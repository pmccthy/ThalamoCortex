{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis of models trained on MNIST classification**\n",
    "Author: patrick.mccarthy@dtc.ox.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import colormaps as cm\n",
    "from scipy.special import softmax\n",
    "from scipy.linalg import svd\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from thalamocortex.models import CTCNet\n",
    "from thalamocortex.utils import create_data_loaders, activation_hook, get_neuron_weights, plot_receptive_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['legend.fontsize'] = 8  \n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "plt.rcParams['figure.facecolor'] = 'w'\n",
    "plt.rcParams['axes.facecolor'] = 'w'  \n",
    "plt.rcParams['savefig.dpi'] = 300  # High-quality images\n",
    "plt.rcParams['savefig.bbox'] = 'tight'  \n",
    "plt.rcParams['savefig.pad_inches'] = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set RC params for plot consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/Users/patmccarthy/Documents/phd/rotation1/results_11_03_25/mnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_paths = {\n",
    "    \"no feedback\": \"/Users/patmccarthy/Documents/thalamocortex/results/06_03_25_feedforward_mnist/0_CTCNet_TC_none\",\n",
    "    # \"driver\": \"/Users/patmccarthy/Documents/thalamocortex/results/11_03_25_driver_mnist/0_CTCNet_TC_add_reciprocal_readout\",\n",
    "    # \"mod1\": \"/Users/patmccarthy/Documents/thalamocortex/results/11_03_25_mod1_mnist/...\",\n",
    "    # \"mod2\": \"/Users/patmccarthy/Documents/thalamocortex/results/11_03_25_mod2_mnist/...\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m     val_top1_accs\u001b[38;5;241m.\u001b[39mappend(learning[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_topk_accs\u001b[39m\u001b[38;5;124m\"\u001b[39m][epoch][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     35\u001b[0m results[model_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_top1_accs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_top1_accs)\n\u001b[0;32m---> 36\u001b[0m results[model_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_top1_accs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_top1_accs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load models, learning stats, results \n",
    "results = {}\n",
    "for model_name, path in results_paths.items():\n",
    "    \n",
    "    # NOTE: note loading trained models because can instantiate from final weights\n",
    "\n",
    "    # hyperparameters\n",
    "    with open(Path(f\"{path}\", \"hyperparams.pkl\"), \"rb\") as handle:\n",
    "        hp = pickle.load(handle)\n",
    "\n",
    "    # learning progress\n",
    "    with open(Path(f\"{path}\", \"learning.pkl\"), \"rb\") as handle:\n",
    "        learning = pickle.load(handle)\n",
    "\n",
    "    # store results and params in dict\n",
    "    results[model_name] = {\"val_losses\": learning[\"val_losses\"],\n",
    "                           \"train_losses\": learning[\"train_losses\"],\n",
    "                           \"val_topk_accs\": learning[\"val_topk_accs\"],\n",
    "                           \"train_topk_accs\": learning[\"train_topk_accs\"],\n",
    "                           \"train_time\": learning[\"train_time\"],\n",
    "                           \"state_dicts\": learning[\"state_dicts\"],\n",
    "                           \"hyperparams\": hp}\n",
    "    \n",
    "    # get number of epochs to train for\n",
    "    n_epochs = len(learning[\"train_topk_accs\"])\n",
    "\n",
    "    # get top-1 accuracies in more convenient form for plotting\n",
    "    train_top1_accs = []\n",
    "    val_top1_accs = []\n",
    "\n",
    "    # store training info\n",
    "    for epoch in range(n_epochs):\n",
    "        train_top1_accs.append(learning[\"train_topk_accs\"][epoch][1])\n",
    "        val_top1_accs.append(learning[\"val_topk_accs\"][epoch][1])\n",
    "    results[model_name][\"train_top1_accs\"] = np.array(train_top1_accs)\n",
    "    results[model_name][\"val_top1_accs\"] = np.array(val_top1_accs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Learning progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plot_list = [\"ff_MNIST\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss through time\n",
    "n_epochs = len(results[model_plot_list[0]][\"val_losses\"])\n",
    "colours = [\"r\", \"g\", \"b\", \"m\"]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "models_plotted = []\n",
    "models_plotted_idx = 0\n",
    "for _, (model_name, model_results) in enumerate(results.items()):\n",
    "    if model_name in model_plot_list:\n",
    "        models_plotted.append(model_name)\n",
    "        models_plotted_idx += 1\n",
    "\n",
    "        n_epochs = len(model_results[\"val_losses\"])\n",
    "\n",
    "        ax.plot(np.arange(n_epochs), np.median(np.array(model_results[\"val_losses\"]), axis=-1), ls=\"--\", linewidth=1, label=f\"{model_name} val.\", c=colours[models_plotted_idx-1])\n",
    "        ax.plot(np.arange(n_epochs), np.median(np.array(model_results[\"train_losses\"]), axis=-1), ls=\"-\", linewidth=1, label=f\"{model_name} train.\", c=colours[models_plotted_idx-1])\n",
    "\n",
    "# ax.set_xticks(range(1, len(models_plotted)+1), models_plotted)\n",
    "ax.set_ylabel(\"cross entropy\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_xlim(0, n_epochs)\n",
    "ax.set_ylim(0, 2.5)\n",
    "ax.legend(loc=\"upper right\")\n",
    "fig.savefig(Path(save_path, \"loss_curve.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy through time\n",
    "n_epochs = len(results[model_plot_list[0]][\"val_losses\"])\n",
    "colours = [\"r\", \"g\", \"b\", \"m\"]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 3), layout=\"constrained\")\n",
    "models_plotted = []\n",
    "models_plotted_idx = 0\n",
    "for _, (model_name, model_results) in enumerate(results.items()):\n",
    "    if model_name in model_plot_list:\n",
    "        models_plotted.append(model_name)\n",
    "        models_plotted_idx += 1\n",
    "\n",
    "        n_epochs = len(model_results[\"val_losses\"])\n",
    "\n",
    "        # ax.plot(np.arange(n_epochs), np.array(model_results[\"val_top1_accs\"]),ls=\"--\", label=f\"{model_name} val.\", linewidth=1, c=colours[models_plotted_idx-1])\n",
    "        ax.plot(np.arange(n_epochs), np.array(model_results[\"train_top1_accs\"]) * 100, ls=\"-\", label=f\"{model_name} train\", linewidth=1, c=colours[models_plotted_idx-1])\n",
    "\n",
    "# ax.set_xticks(range(1, len(models_plotted)+1), models_plotted)\n",
    "ax.set_ylabel(\"top-1 accuracy\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_xlim(0, n_epochs)\n",
    "ax.legend(loc=\"lower right\")\n",
    "fig.savefig(Path(save_path, \"accuracy_curve.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy before and after convergence\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "ax.axhline(0.1, ls=\"--\", c=\"k\", label=\"chance\")\n",
    "for _, (model_name, model_results) in enumerate(results.items()):\n",
    "    if model_name in model_plot_list:\n",
    "        ax.plot([0, 1], [model_results[\"train_top1_accs\"][0] * 100, model_results[\"train_top1_accs\"][-1] * 100], c=colours[models_plotted_idx-1], marker=\"o\", markersize=10, linewidth=5, label=model_name)\n",
    "ax.set_ylabel(\"test top-1 accuracy (%)\")\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_xlim(-0.25, 1.25)\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels([\"before\", \"after\"])\n",
    "ax.legend(loc=\"upper left\")\n",
    "fig.savefig(Path(save_path, \"accuracy_prepostlearning.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Trained model analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_selected = [\"ff_MNIST\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch of trained model weights to use \n",
    "epoch_trained = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loaders\n",
    "trainset_loader, testset_loader, metadata = create_data_loaders(dataset=results[models_selected[0]][\"hyperparams\"][\"dataset\"],\n",
    "                                                                norm=results[models_selected[0]][\"hyperparams\"][\"norm\"],\n",
    "                                                                batch_size=32,\n",
    "                                                                save_path=\"/Users/patmccarthy/Documents/ThalamoCortex/data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full test set\n",
    "X_all = []\n",
    "y_all = []\n",
    "for X, y in iter(testset_loader):\n",
    "    # X_all.append(X.detach().numpy()[:, 0, :, :])\n",
    "    # y_all.append(y.detach().numpy()[:])\n",
    "    X_all.append(X[:, :, :])\n",
    "    y_all.append(y[:])\n",
    "if results[models_selected[0]][\"hyperparams\"][\"dataset\"] in [\"BinaryMNIST\", \"LeftRightMNIST\"]:\n",
    "    # Concatenate along the first axis (num_samples)\n",
    "    X_all_arr = np.concatenate(X_all, axis=0)  # Shape: (num_samples, 1, 28, 28)\n",
    "    y_all_reshaped = np.concatenate(y_all, axis=0)  # Shape: (num_samples,)\n",
    "\n",
    "    # Reshape X to [samples, features]\n",
    "    X_all_reshaped = X_all_arr.reshape(X_all_arr.shape[0], -1)  # Shape: (num_samples, 28*28)\n",
    "else:\n",
    "    X_all_tensor = torch.cat(X_all, dim=0)  # Shape: [num_samples, 1, 28, 28]\n",
    "    y_all_tensor = torch.cat(y_all, dim=0)  # Shape: [num_samples]\n",
    "\n",
    "    # Convert to NumPy\n",
    "    X_all_arr = X_all_tensor.numpy()  # Shape: (num_samples, 1, 28, 28)\n",
    "    y_all_reshaped = y_all_tensor.numpy()  # Shape: (num_samples,)\n",
    "\n",
    "    # Reshape X to [samples, features]\n",
    "    X_all_reshaped = X_all_arr.reshape(X_all_arr.shape[0], -1)  # Shape: (num_samples, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â inference on full test set using models trained to various epochs\n",
    "epochs_range = np.arange(0, 800, 25)\n",
    "activations = {}\n",
    "for model_selected in models_selected:\n",
    "    activations[model_selected] = {}\n",
    "\n",
    "    # instantiate model\n",
    "    model = CTCNet(input_size=results[model_selected][\"hyperparams\"][\"input_size\"],\n",
    "                   output_size=results[model_selected][\"hyperparams\"][\"output_size\"],\n",
    "                   ctx_layer_size=results[model_selected][\"hyperparams\"][\"ctx_layer_size\"],\n",
    "                   thal_layer_size=results[model_selected][\"hyperparams\"][\"thal_layer_size\"],\n",
    "                   thalamocortical_type=results[model_selected][\"hyperparams\"][\"thalamocortical_type\"],\n",
    "                   thal_reciprocal=results[model_selected][\"hyperparams\"][\"thal_reciprocal\"],\n",
    "                   thal_to_readout=results[model_selected][\"hyperparams\"][\"thal_to_readout\"], \n",
    "                   thal_per_layer=results[model_selected][\"hyperparams\"][\"thal_per_layer\"])\n",
    "    \n",
    "    for epoch in epochs_range:\n",
    "        activations[model_selected][epoch] = {}\n",
    "\n",
    "        # get model trained to specified epoch\n",
    "        weights = results[model_selected][\"state_dicts\"][epoch]\n",
    "\n",
    "        # set model weights\n",
    "        model.load_state_dict(weights)\n",
    "\n",
    "        # Register hooks for specific layers\n",
    "        hook_handles = []\n",
    "        activations_this_epoch = {}\n",
    "        for name, layer in model.named_modules():\n",
    "            handle = layer.register_forward_hook(lambda module, input, output: activation_hook(module, input, output, activations_this_epoch))\n",
    "            hook_handles.append(handle)\n",
    "        \n",
    "        # inference (on full dataset)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            y_est_logits = model(torch.Tensor(X_all_reshaped))\n",
    "            y_est_prob = softmax(y_est_logits.detach().numpy())\n",
    "            y_est = np.argmax(y_est_prob, axis=1)\n",
    "\n",
    "            # Remove hooks after use\n",
    "            for handle in hook_handles:\n",
    "                handle.remove()\n",
    "        \n",
    "        activations[model_selected][epoch] = copy.deepcopy(activations_this_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define readable names for connections of interest\n",
    "# NOTE: always double check these before usimng\n",
    "# readable_names = {\"ctx1\": list(activations[\"ff_MNIST\"][0].keys())[2],\n",
    "#                   \"ctx2\": list(activations[\"ff_MNIST\"][0].keys())[5],\n",
    "#                   \"ctx_readout\": list(activations[\"ff_MNIST\"][0].keys())[7]\n",
    "#                 #   \"thal\": list(activations[\"ff_MNIST\"][0].keys())[10], # TODO: figure out why thal layer not showing up in activations dict\n",
    "# }\n",
    "readable_layer_idxs = {\"ctx1\": 2,\n",
    "                       \"ctx2\": 5,\n",
    "                       \"ctx_readout\": 7\n",
    "                #   \"thal\": 10, # TODO: figure out why thal layer not showing up in activations dict\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activations decoding analysis \n",
    "layers_selected = [\"ctx1\", \"ctx2\"]\n",
    "train_test_split = 0.8\n",
    "accuracies = {}\n",
    "for layer_selected in layers_selected:\n",
    "    accuracies[layer_selected] = {}\n",
    "    for model_selected in models_selected:\n",
    "        print(f\"Decoding for {layer_selected}, {model_selected}\")\n",
    "        accuracies[layer_selected][model_selected] = {}\n",
    "        for epoch in epochs_range:\n",
    "\n",
    "            # select layer activations to decode from\n",
    "            features = activations[model_selected][epoch][list(activations[model_selected][epoch].keys())[readable_layer_idxs[layer_selected]]].detach().numpy()\n",
    "\n",
    "            # split into train and test set\n",
    "            test_cutoff = int(len(features) * train_test_split)\n",
    "            X_train = features[:test_cutoff, :]\n",
    "            X_test = features[test_cutoff:, :]\n",
    "            y_train = y_all_reshaped[:test_cutoff]\n",
    "            y_test = y_all_reshaped[test_cutoff:]\n",
    "\n",
    "            # TODO: perform cross-validation\n",
    "            # TODO: try replacing with linear classifier\n",
    "            \n",
    "            # train SVM classifier\n",
    "            clf = svm.SVC(kernel=\"linear\")\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            # test SVM classifier\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            # compute classification accuracy\n",
    "            correct = 0\n",
    "            for samp_idx in range(y_pred.shape[0]):\n",
    "                if y_pred[samp_idx] == y_test[samp_idx]:\n",
    "                    correct += 1\n",
    "            accuracy = correct / y_pred.shape[0]\n",
    "            print(f\"epoch: {epoch}, accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "            accuracies[layer_selected][model_selected][epoch] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in accuracies.keys():\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "    for model_idx, model_selected in enumerate(accuracies[layer].keys()):\n",
    "        ax.plot(accuracies[layer][model_selected].keys(), np.array(list(accuracies[layer][model_selected].values())) * 100, c=colours[model_idx], label=model_selected)\n",
    "    ax.axhline(10, ls=\"--\", c=\"k\", label=\"chance\")\n",
    "    ax.set_title(f\"SVM decoding MNIST from {layer} activations\")\n",
    "    ax.set_ylabel(\"classification accuracy (%)\")\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_xlim(list(accuracies[layer][model_selected].keys())[0], list(accuracies[layer][model_selected].keys())[-1])\n",
    "    ax.legend(loc=\"center right\")\n",
    "    fig.savefig(Path(save_path, f\"svm_decoding_{layer}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activations analysis\n",
    "reduction_method = \"PCA\"\n",
    "kernel = \"rbf\"\n",
    "epochs = {\"before\": 0,\n",
    "          \"after\": 775}\n",
    "\n",
    "# generate colourmap\n",
    "jet_cmap = plt.colormaps[\"jet\"]  \n",
    "N = 10\n",
    "colors = jet_cmap(np.linspace(0, 1, N)) \n",
    "discrete_cmap = mcolors.ListedColormap(colors)\n",
    "\n",
    "for layer in accuracies.keys():\n",
    "    fig, ax = plt.subplots(len(accuracies[layer]), len(epochs), figsize=(4 * len(epochs), 4 * len(accuracies[layer])), layout=\"constrained\")\n",
    "    for model_idx, model_selected in enumerate(accuracies[layer].keys()):\n",
    "\n",
    "        for epoch_idx, (epoch_name, epoch) in enumerate(epochs.items()):\n",
    "\n",
    "            features = activations[model_selected][epoch][list(activations[model_selected][epoch].keys())[readable_layer_idxs[layer]]].detach().numpy()\n",
    "\n",
    "            if reduction_method == \"PCA\":\n",
    "                pca = PCA(n_components=2)\n",
    "                activations_2d = pca.fit_transform(features)\n",
    "                title = f\"{reduction_method} on {layer} activations\"\n",
    "                save_tag = \"pca\"\n",
    "            elif reduction_method == \"KernelPCA\":\n",
    "                pca_transformer = KernelPCA(n_components=2, kernel=\"cosine\")\n",
    "                activations_2d = pca_transformer.fit_transform(features)\n",
    "                title = f\"{reduction_method} on {layer} activations\\nwith {kernel} kernel (epoch {epoch})\"\n",
    "                save_tag = \"kernel_pca\"\n",
    "\n",
    "            scatter = ax[epoch_idx].scatter(activations_2d[:, 0], activations_2d[:, 1], c=y_all_reshaped, cmap=discrete_cmap, alpha=0.7, s=0.2)\n",
    "            plt.colorbar(scatter, label=\"class ID\")\n",
    "            ax[epoch_idx].set_title(f\"{model_selected}, {epoch_name}\")\n",
    "            ax[epoch_idx].set_xlabel(\"PC1\")\n",
    "            ax[epoch_idx].set_ylabel(\"PC2\")\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    fig.savefig(Path(save_path, f\"{save_tag}_{layer}_activations_prepostlearning.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define readable names for connections of interest\n",
    "# NOTE: always double check these before using\n",
    "readable_weight_idxs = {\"input_to_ctx1\": 2,\n",
    "                        \"ctx1_to_ctx2\": 4,\n",
    "                #   \"thal\": ?, # TODO: figure out why thal layer not showing up in activations dict\n",
    "}\n",
    "readable_weight_names = {\"input_to_ctx1\": \"ctx1.0.weight\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decomposition analysis\n",
    "weights_selected = [\"input_to_ctx1\", \"ctx1_to_ctx2\"]\n",
    "svds = {}\n",
    "for weights_select in weights_selected:\n",
    "    svds[weights_select] = {}\n",
    "    for model_selected in models_selected:\n",
    "        svds[weights_select][model_selected] = {}\n",
    "        accuracies[layer_selected][model_selected] = {}\n",
    "        activations[model_selected] = {}\n",
    "        for epoch in epochs_range:\n",
    "\n",
    "            # get weights in forward pass direction and compute spectral measures\n",
    "            weights = results[model_selected][\"state_dicts\"][epoch][list(results[model_selected][\"state_dicts\"][epoch].keys())[readable_weight_idxs[weights_select]]]\n",
    "\n",
    "            # SVD on weights\n",
    "            U, s, Vh = svd(weights)\n",
    "            s_norm = s / np.sum(s)\n",
    "            \n",
    "            # compute spectral metrics\n",
    "            spectral_entropy = entropy(s_norm)\n",
    "            spectral_norm = np.max(s)\n",
    "            condition_number = np.max(s) / np.min(s)\n",
    "            \n",
    "            # store\n",
    "            svds[weights_select][model_selected][epoch] = {\"U\": U,\n",
    "                                                           \"s\": s,\n",
    "                                                           \"Vh\": Vh,\n",
    "                                                           \"s_norm\": s_norm,\n",
    "                                                           \"spectral_entropy\": spectral_entropy,\n",
    "                                                           \"spectral_norm\": spectral_norm,\n",
    "                                                           \"condition_number\": condition_number}\n",
    "\n",
    "            # print(f\"{weights.shape=}\")\n",
    "            # print(f\"{Vh.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_select = \"input_to_ctx1\"\n",
    "model_select = \"ff_MNIST\"\n",
    "epochs = [0, 775]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "epochs = {\"before\": 0,\n",
    "          \"after\": 775}\n",
    "for epoch_idx, (epoch_name, epoch) in enumerate(epochs.items()):\n",
    "    ax.plot(svds[weights_select][model_select][epoch][\"s_norm\"], c=colours[epoch_idx], label=epoch_name)\n",
    "ax.set_title(f\"singular value spectrum for {model_select}, {weights_select} weights\")\n",
    "ax.set_ylabel(\"normalised singular value\")\n",
    "ax.set_xlabel(\"rank\")\n",
    "ax.legend()\n",
    "fig.savefig(Path(save_path, f\"sv_spectrum_{model_select}_{weights_select}_prepostlearning.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "for model_idx, model_select in enumerate(models_selected):\n",
    "    ax.plot(svds[weights_select][model_select][epoch][\"s_norm\"], c=colours[model_idx], label=model_select)\n",
    "ax.set_title(f\"singular value spectrum for {weights_select} weights\")\n",
    "ax.set_ylabel(\"normalised singular value\")\n",
    "ax.set_xlabel(\"rank\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "fig.savefig(Path(save_path, f\"norm_sv_spectrum_{model_select}_{weights_select}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify which cells in the input layer the most highly weighted projections correspond to\n",
    "# (reduce weights and plot the )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receptive field analysis\n",
    "cmap = cm.get_cmap(\"seismic\")\n",
    "clims = [-0.2, 0.2]\n",
    "epoch_name = \"trained\"\n",
    "epoch = 775\n",
    "fig, ax = plt.subplots(8, 4, figsize=(5, 10), layout=\"constrained\")\n",
    "\n",
    "for neuron_id in range(32):\n",
    "\n",
    "    row_idx = neuron_id // 4\n",
    "    col_idx = neuron_id % 4\n",
    "\n",
    "    input_layer_weights = results[model_selected][\"state_dicts\"][epoch][list(results[model_selected][\"state_dicts\"][epoch].keys())[readable_weight_idxs[\"input_to_ctx1\"]]]\n",
    "\n",
    "\n",
    "    weights_this_neuron = get_neuron_weights(weights=input_layer_weights,\n",
    "                                             neuron_id=neuron_id,\n",
    "                                             shape=(28, 28))\n",
    "    plot_receptive_field(weights=weights_this_neuron,\n",
    "                         ax=ax[row_idx, col_idx],\n",
    "                         cmap=cmap,\n",
    "                         clims=clims,\n",
    "                         title=neuron_id+1)\n",
    "\n",
    "psm = ax[0, 0].pcolormesh(weights_this_neuron, cmap=cmap, rasterized=True, vmin=clims[0], vmax=clims[1])\n",
    "cbar = fig.colorbar(psm, ax=ax, shrink=0.8, aspect=30)\n",
    "cbar.set_label(\"weight\")\n",
    "fig.suptitle(f\"{model_selected} layer 1 receptive fields ({epoch_name})\")\n",
    "fig.savefig(Path(save_path, f\"norm_sv_spectrum_{model_select}_{weights_select}.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ablation analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy with ablated feedback connections\n",
    "epoch = 775\n",
    "activations = {}\n",
    "accuracies = {}\n",
    "weights_to_zero = \"input_to_ctx1\"\n",
    "for zero_state in [False, True]:\n",
    "    activations[zero_state] = {}\n",
    "    accuracies[zero_state] = {}\n",
    "    for model_selected in models_selected:\n",
    "        activations[zero_state][model_selected] = {}\n",
    "        accuracies[zero_state][model_selected] = {}\n",
    "\n",
    "        # instantiate model\n",
    "        model = CTCNet(input_size=results[model_selected][\"hyperparams\"][\"input_size\"],\n",
    "                        output_size=results[model_selected][\"hyperparams\"][\"output_size\"],\n",
    "                        ctx_layer_size=results[model_selected][\"hyperparams\"][\"ctx_layer_size\"],\n",
    "                        thal_layer_size=results[model_selected][\"hyperparams\"][\"thal_layer_size\"],\n",
    "                        thalamocortical_type=results[model_selected][\"hyperparams\"][\"thalamocortical_type\"],\n",
    "                        thal_reciprocal=results[model_selected][\"hyperparams\"][\"thal_reciprocal\"],\n",
    "                        thal_to_readout=results[model_selected][\"hyperparams\"][\"thal_to_readout\"], \n",
    "                        thal_per_layer=results[model_selected][\"hyperparams\"][\"thal_per_layer\"])\n",
    "\n",
    "        # get model trained to specified epoch\n",
    "        weights = results[model_selected][\"state_dicts\"][epoch]\n",
    "\n",
    "        # set chosen weights to zero\n",
    "        if zero_state:\n",
    "            selected_weights_shape = weights[readable_weight_names[weights_to_zero]].shape\n",
    "            new_weights = weights[readable_weight_names[weights_to_zero]] * 2# torch.zeros(selected_weights_shape)\n",
    "            weights[readable_weight_names[weights_to_zero]] = new_weights\n",
    "        \n",
    "        # set model weights\n",
    "        model.load_state_dict(weights)\n",
    "\n",
    "        # do inference on test set\n",
    "        # register hooks for specific layers\n",
    "        hook_handles = []\n",
    "        activations_this_epoch = {}\n",
    "        for name, layer in model.named_modules():\n",
    "            handle = layer.register_forward_hook(lambda module, input, output: activation_hook(module, input, output, activations_this_epoch))\n",
    "            hook_handles.append(handle)\n",
    "        \n",
    "        # inference (on full dataset)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            y_est_logits = model(torch.Tensor(X_all_reshaped))\n",
    "            y_est_prob = softmax(y_est_logits.detach().numpy())\n",
    "            y_est = np.argmax(y_est_prob, axis=1)\n",
    "\n",
    "            # Remove hooks after use\n",
    "            for handle in hook_handles:\n",
    "                handle.remove()\n",
    "        \n",
    "        # compute classification accuracy\n",
    "        correct = 0\n",
    "        for samp_idx in range(y_est.shape[0]):\n",
    "            if y_est[samp_idx] == y_all_reshaped[samp_idx]:\n",
    "                correct += 1\n",
    "        accuracy = correct / y_est.shape[0]\n",
    "        print(f\"accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        activations[zero_state][model_selected][epoch] = copy.deepcopy(activations_this_epoch)\n",
    "        accuracies[zero_state][model_selected][epoch] = accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activations with ablated feedback\n",
    "reduction_method = \"PCA\"\n",
    "kernel = \"rbf\"\n",
    "epochs = [0, 775]\n",
    "# generate colourmap\n",
    "jet_cmap = plt.colormaps[\"jet\"]  \n",
    "N = 10\n",
    "colors = jet_cmap(np.linspace(0, 1, N)) \n",
    "discrete_cmap = mcolors.ListedColormap(colors)\n",
    "\n",
    "for layer in accuracies.keys():\n",
    "    fig, ax = plt.subplots(len(accuracies[layer]), len(epochs), figsize=(4 * len(epochs), 4 * len(accuracies[layer])), layout=\"constrained\")\n",
    "    for model_idx, model_selected in enumerate(accuracies[layer].keys()):\n",
    "\n",
    "        for zero_state_idx, zero_state in enumerate([False, True]):\n",
    "\n",
    "            features = activations[zero_state][model_selected][epoch][list(activations[zero_state][model_selected][epoch].keys())[readable_layer_idxs[layer]]].detach().numpy()\n",
    "\n",
    "            if reduction_method == \"PCA\":\n",
    "                pca = PCA(n_components=2)\n",
    "                activations_2d = pca.fit_transform(features)\n",
    "                title = f\"{reduction_method} on {layer} activations\"\n",
    "                save_tag = \"pca\"\n",
    "            elif reduction_method == \"KernelPCA\":\n",
    "                pca_transformer = KernelPCA(n_components=2, kernel=\"cosine\")\n",
    "                activations_2d = pca_transformer.fit_transform(features)\n",
    "                title = f\"{reduction_method} on {layer} activations\\nwith {kernel} kernel (epoch {epoch})\"\n",
    "                save_tag = \"kernel_pca\"\n",
    "\n",
    "            scatter = ax[zero_state_idx].scatter(activations_2d[:, 0], activations_2d[:, 1], c=y_all_reshaped, cmap=discrete_cmap, alpha=0.7, s=0.2)\n",
    "            plt.colorbar(scatter, label=\"class ID\")\n",
    "            ax[zero_state_idx].set_title(f\"{model_selected}, epoch {epoch}\")\n",
    "            ax[zero_state_idx].set_xlabel(\"PC1\")\n",
    "            ax[zero_state_idx].set_ylabel(\"PC2\")\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    fig.patch.set_facecolor(\"w\")\n",
    "    fig.savefig(Path(save_path, f\"{save_tag}_{layer}_activations_ablation.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy before and after convergence\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "ax.axhline(0.1, ls=\"--\", c=\"k\", label=\"chance\")\n",
    "for _, (model_name, model_results) in enumerate(results.items()):\n",
    "    if model_name in model_plot_list:\n",
    "        ax.plot([0, 1], [model_results[\"train_top1_accs\"][0] * 100, model_results[\"train_top1_accs\"][-1] * 100], c=colours[models_plotted_idx-1], marker=\"o\", markersize=10, linewidth=5, label=model_name)\n",
    "ax.set_ylabel(\"test top-1 accuracy (%)\")\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_xlim(-0.25, 1.25)\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels([\"full\", \"ablated\"])\n",
    "ax.legend(loc=\"upper left\")\n",
    "fig.savefig(Path(save_path, \"accuracy_prepostlearning.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thalamocortex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
