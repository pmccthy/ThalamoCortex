{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CTCNet\n",
    "from utils import make_grid\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_param_grid\n",
    "model_params = {\"input_size\": [28 * 28],\n",
    "                    \"output_size\": [10],\n",
    "                    \"ctx_layer_size\": [128],\n",
    "                    \"thal_layer_size\": [64],\n",
    "                    \"thalamocortical_type\": [None, \"additive\", \"multi_pre_activation\", \"multi_post_activation\"],\n",
    "                    \"thal_reciprocal\": [True, False],\n",
    "                    \"thal_to_readout\": [True, False],\n",
    "                    \"thal_per_layer\": [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param_grid = make_grid(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 0,\n",
       " 'output_size': 0,\n",
       " 'ctx_layer_size': 0,\n",
       " 'thal_layer_size': 0,\n",
       " 'thalamocortical_type': 0,\n",
       " 'thal_reciprocal': 0,\n",
       " 'thal_to_readout': 0,\n",
       " 'thal_per_layer': 0}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combination_idx = 0\n",
    "for param_name, param_val in model_param_grid.items():\n",
    "\n",
    "    model = CTCNet(input_size=input_size,\n",
    "                output_size=output_size,\n",
    "                ctx_layer_size=ctx_layer_size,\n",
    "                thal_layer_size=thal_layer_size,\n",
    "                thalamocortical_type=\"multiplicative_post_activation\",\n",
    "                thal_reciprocal=True, \n",
    "                thal_to_readout=True, \n",
    "                thal_per_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "ctx_layer_size = 10\n",
    "thal_layer_size = 10\n",
    "thalamocortical_type=None # None, multiplicative, or additive\n",
    "thal_reciprocal=True # True or False\n",
    "thal_to_readout=True # True or False\n",
    "thal_per_layer=False # if no, mixing from cortical layers\n",
    "fan_in_thal=None\n",
    "fan_in_ctx=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CTCNet(input_size=input_size,\n",
    "                output_size=output_size,\n",
    "                ctx_layer_size=ctx_layer_size,\n",
    "                thal_layer_size=thal_layer_size,\n",
    "                thalamocortical_type=\"multiplicative_post_activation\",\n",
    "                thal_reciprocal=True, \n",
    "                thal_to_readout=True, \n",
    "                thal_per_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Linear: 2-1                       210\n",
      "|    └─ReLU: 2-2                         --\n",
      "├─Linear: 1-2                            110\n",
      "├─Linear: 1-3                            110\n",
      "├─Linear: 1-4                            110\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─Linear: 2-3                       110\n",
      "|    └─ReLU: 2-4                         --\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─Linear: 2-5                       110\n",
      "|    └─ReLU: 2-6                         --\n",
      "├─Sequential: 1-7                        --\n",
      "|    └─Linear: 2-7                       110\n",
      "|    └─Softmax: 2-8                      --\n",
      "=================================================================\n",
      "Total params: 870\n",
      "Trainable params: 870\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting backend.\n",
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(dataset, norm,, batch_size, save_path):\n",
    "\n",
    "    # choose normalisation method\n",
    "    if norm == \"normalise\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(), # scale values to lie in range [0, 1]\n",
    "        ])\n",
    "    elif norm == \"standardise\":\n",
    "                transform = transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.), (1.,))  # standarise values to have zero mean and unit SD\n",
    "        ])\n",
    "                \n",
    "    metadata = {}\n",
    "    # load dataset and apply normalisation\n",
    "    if dataset == \"MNIST\":\n",
    "        trainset = datasets.MNIST(root=save_path,\n",
    "                                  train=True,\n",
    "                                  download=True,\n",
    "                                  transform=transform)\n",
    "        testset = datasets.MNIST(root=save_path,\n",
    "                                  train=False,\n",
    "                                  download=True,\n",
    "                                  transform=transform)\n",
    "        metadata[\"classes\"] = trainset.classes\n",
    "    elif dataset == \"FashionMNIST\":\n",
    "        trainset = datasets.FashionMNIST(root=save_path,\n",
    "                                         train=True,\n",
    "                                         download=True,\n",
    "                                         transform=transform)\n",
    "        testset = datasets.FashionMNIST(root=save_path,\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "        \n",
    "        metadata[\"classes\"] = trainset.classes\n",
    "    # create loaders\n",
    "    trainset_loader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                batch_size = batch_size,\n",
    "                                shuffle = True)\n",
    "    testset_loader = torch.utils.data.DataLoader(dataset = testset,\n",
    "                                batch_size = batch_size,\n",
    "                                shuffle = True)\n",
    "\n",
    "    return trainset_loader, testset_loader, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = \"normalise\"\n",
    "dataset = \"FashionMNIST\"\n",
    "save_path = \"/Users/patmccarthy/Documents/BurstCCN/Data2\"\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /Users/patmccarthy/Documents/BurstCCN/Data2/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:01<00:00, 19463890.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/patmccarthy/Documents/BurstCCN/Data2/FashionMNIST/raw/train-images-idx3-ubyte.gz to /Users/patmccarthy/Documents/BurstCCN/Data2/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /Users/patmccarthy/Documents/BurstCCN/Data2/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 1599396.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/patmccarthy/Documents/BurstCCN/Data2/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /Users/patmccarthy/Documents/BurstCCN/Data2/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /Users/patmccarthy/Documents/BurstCCN/Data2/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:00<00:00, 11821870.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/patmccarthy/Documents/BurstCCN/Data2/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /Users/patmccarthy/Documents/BurstCCN/Data2/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /Users/patmccarthy/Documents/BurstCCN/Data2/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 5105764.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/patmccarthy/Documents/BurstCCN/Data2/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /Users/patmccarthy/Documents/BurstCCN/Data2/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainset_loader, testset_loader, metadata = create_data_loaders(dataset=dataset,\n",
    "                                                      norm=norm,\n",
    "                                                      save_path=save_path,\n",
    "                                                      batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': ['T-shirt/top',\n",
       "  'Trouser',\n",
       "  'Pullover',\n",
       "  'Dress',\n",
       "  'Coat',\n",
       "  'Sandal',\n",
       "  'Shirt',\n",
       "  'Sneaker',\n",
       "  'Bag',\n",
       "  'Ankle boot']}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded.\n"
     ]
    }
   ],
   "source": [
    "            \n",
    "mnist_trainset = datasets.MNIST(root=\"/Users/patmccarthy/Documents/BurstCCN/Data2\", train=True, download=True)\n",
    "print(\"Dataset downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - zero',\n",
       " '1 - one',\n",
       " '2 - two',\n",
       " '3 - three',\n",
       " '4 - four',\n",
       " '5 - five',\n",
       " '6 - six',\n",
       " '7 - seven',\n",
       " '8 - eight',\n",
       " '9 - nine']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST\n",
    "print(\"Downloading dataset.\")\n",
    "if norm == \"normalise\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # scale values to lie in range [0, 1]\n",
    "    ])\n",
    "elif norm == \"standardise\":\n",
    "            transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.), (1.,))  # normalise values to \n",
    "    ])\n",
    "            \n",
    "mnist_trainset = datasets.MNIST(root=\"/Users/patmccarthy/Documents/BurstCCN/Data2\", train=True, download=True, transform=transform)\n",
    "print(\"Dataset downloaded.\")\n",
    "\n",
    "# Load MNIST\n",
    "data_loader = torch.utils.data.DataLoader(dataset = mnist_trainset,\n",
    "                                    batch_size = HP[\"BATCH_SIZE\"],\n",
    "                                    shuffle = True)\n",
    "\n",
    "# Initialise model\n",
    "print(\"Initialising model.\")\n",
    "model = AE(encoding_dim=HP[\"ENCODING_DIM\"])\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr = HP[\"LEARNING_RATE\"])\n",
    "print(\"Done initialising.\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train model\n",
    "print(\"Training...\")\n",
    "losses_epochs = []\n",
    "for epoch in range(HP[\"EPOCHS\"]):\n",
    "    print(f\"Beginning epoch {epoch+1}/{HP['EPOCHS']}\")\n",
    "    losses = train(data_loader=data_loader, model=model, loss_fn=loss_function, optimizer=optimizer)\n",
    "    losses_epochs.append(losses)\n",
    "    print(f\"Epoch {epoch+1}/{HP['EPOCHS']} done\")\n",
    "end_time = time.time() - start_time\n",
    "print(f\"Finished training in {end_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss, num_epochs, verbose=True):\n",
    "    \"\"\"Train model and regularly evaluate on validation set.\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"Training...\")\n",
    "    losses_epochs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Beginning epoch {epoch+1}/{num_epochs}\")\n",
    "        losses = train(data_loader=data_loader, model=model, loss_fn=loss_function, optimizer=optimizer)\n",
    "        losses_epochs.append(losses)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} done\")\n",
    "    end_time = time.time() - start_time\n",
    "    print(f\"Finished training in {end_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, trainset_loader, valset_loader, optimizer, loss_fn, num_epochs, device, loss_track_step):\n",
    "    \"\"\"Train model for one epoch.\"\"\"\n",
    "\n",
    "    size = len(data_loader.dataset)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch, (X, _) in enumerate(data_loader):\n",
    "        # move data to device where model will be trained\n",
    "        X = X.to(device)\n",
    "\n",
    "        # compute error\n",
    "        _, X_recon = model(X)\n",
    "        loss = loss_fn(X_recon, X)\n",
    "\n",
    "        # backprop\n",
    "        loss.backward()  # compute gradients\n",
    "        optimizer.step()  # update params\n",
    "        optimizer.zero_grad()  # ensure not tracking gradients fo next iteration\n",
    "\n",
    "        # print loss every Nth batch\n",
    "        if batch % loss_track_step == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(\n",
    "                f\"training batch {batch+1}, loss: {loss:.3f}, {current}/{size} datapoints\"\n",
    "            )\n",
    "            losses.append(loss)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, optimiser, loss\n",
    "model = CTCNet(input_size=input_size,\n",
    "                output_size=output_size,\n",
    "                ctx_layer_size=ctx_layer_size,\n",
    "                thal_layer_size=thal_layer_size,\n",
    "                thalamocortical_type=\"multiplicative_post_activation\",\n",
    "                thal_reciprocal=True, \n",
    "                thal_to_readout=True, \n",
    "                thal_per_layer=False)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model = AE(encoding_dim=HP[\"ENCODING_DIM\"])\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr = HP[\"LEARNING_RATE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Routine to train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {...}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thalamocortex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
