{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from models import CTCNet\n",
    "from utils import create_data_loaders, train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting backend.\n",
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "# Set backend\n",
    "print(\"Setting backend.\")\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/Users/patmccarthy/Documents/thalamocortex/data\"\n",
    "hyperparams = {\n",
    "    # data hyperparams\n",
    "    \"norm\" : \"normalise\",\n",
    "    \"dataset\" : \"FashionMNIST\",\n",
    "    \"save_path\" : \"/Users/patmccarthy/Documents/thalamocortex/data\",\n",
    "    \"batch_size\" : 32,\n",
    "    # model hyperparams\n",
    "    \"input_size\" : 28 * 28,\n",
    "    \"output_size\" : 10,\n",
    "    \"ctx_layer_size\" : 128,\n",
    "    \"thal_layer_size\" : 64,\n",
    "    \"thalamocortical_type\" : None, # None, multiplicative, or additive\n",
    "    \"thal_reciprocal\" : True, # True or False\n",
    "    \"thal_to_readout\" : True, # True or False\n",
    "    \"thal_per_layer\" : False, # if no, mixing from cortical layers\n",
    "    # training hyperparams\n",
    "    \"lr\" : 0.001,\n",
    "    \"loss\" : torch.nn.CrossEntropyLoss(),\n",
    "    \"epochs\": 1,\n",
    "    \"ohe_targets\": True,\n",
    "    \"loss_track_step\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "trainset_loader, testset_loader, metadata = create_data_loaders(dataset=hyperparams[\"dataset\"],\n",
    "                                                                        norm=hyperparams[\"norm\"],\n",
    "                                                                        save_path=hyperparams[\"save_path\"],\n",
    "                                                                        batch_size=hyperparams[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Linear: 2-1                       16,448\n",
      "|    └─ReLU: 2-2                         --\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─Linear: 2-3                       100,480\n",
      "|    └─ReLU: 2-4                         --\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─Linear: 2-5                       16,512\n",
      "|    └─ReLU: 2-6                         --\n",
      "├─Sequential: 1-4                        --\n",
      "|    └─Linear: 2-7                       1,290\n",
      "=================================================================\n",
      "Total params: 134,730\n",
      "Trainable params: 134,730\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = CTCNet(input_size=hyperparams[\"input_size\"],\n",
    "                output_size=hyperparams[\"output_size\"],\n",
    "                ctx_layer_size=hyperparams[\"ctx_layer_size\"],\n",
    "                thal_layer_size=hyperparams[\"thal_layer_size\"],\n",
    "                thalamocortical_type=hyperparams[\"thalamocortical_type\"],\n",
    "                thal_reciprocal=hyperparams[\"thal_reciprocal\"],\n",
    "                thal_to_readout=hyperparams[\"thal_to_readout\"], \n",
    "                thal_per_layer=hyperparams[\"thal_per_layer\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimiser\n",
    "loss_fn = deepcopy(hyperparams[\"loss\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = hyperparams[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Beginning epoch 1/1\n",
      "training batch 1, loss: 2.311, 32/60000 datapoints\n",
      "training batch 201, loss: 0.568, 6432/60000 datapoints\n",
      "training batch 401, loss: 0.610, 12832/60000 datapoints\n",
      "training batch 601, loss: 0.373, 19232/60000 datapoints\n",
      "training batch 801, loss: 0.538, 25632/60000 datapoints\n",
      "training batch 1001, loss: 0.382, 32032/60000 datapoints\n",
      "training batch 1201, loss: 0.570, 38432/60000 datapoints\n",
      "training batch 1401, loss: 0.484, 44832/60000 datapoints\n",
      "training batch 1601, loss: 0.428, 51232/60000 datapoints\n",
      "training batch 1801, loss: 0.401, 57632/60000 datapoints\n",
      "validation batch 1, loss: 0.350, 32/10000 datapoints\n",
      "validation batch 201, loss: 0.606, 6432/10000 datapoints\n",
      "Epoch 1/1 done\n",
      "Finished training in 11.12 seconds.\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "train_losses, val_losses, train_time = train(model=model,\n",
    "                                 trainset_loader=trainset_loader,\n",
    "                                 valset_loader=testset_loader,\n",
    "                                 optimizer=optimizer,\n",
    "                                 loss_fn=loss_fn,\n",
    "                                 ohe_targets=hyperparams[\"ohe_targets\"],\n",
    "                                 num_classes=len(metadata[\"classes\"]),\n",
    "                                 num_epochs=hyperparams[\"epochs\"],\n",
    "                                 device=device,\n",
    "                                 loss_track_step=hyperparams[\"loss_track_step\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation batch 1, loss: 0.279, 32/10000 datapoints\n",
      "validation batch 201, loss: 0.386, 6432/10000 datapoints\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "losses = evaluate(model=model,\n",
    "                  data_loader=testset_loader,\n",
    "                  optimizer=optimizer,\n",
    "                  loss_fn=loss_fn,\n",
    "                  ohe_targets=hyperparams[\"ohe_targets\"],\n",
    "                  num_classes=len(metadata[\"classes\"]),\n",
    "                  device=device,\n",
    "                  loss_track_step=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Done saving.\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "save_path_this_model = Path(save_path, \"model0_05_01_24\")\n",
    "if not os.path.exists(save_path_this_model):\n",
    "    os.mkdir(save_path_this_model)\n",
    "print(\"Saving...\")\n",
    "# model\n",
    "torch.save(model.state_dict(), Path(f\"{save_path_this_model}\", \"model.pth\"))\n",
    "# hyperparams\n",
    "with open(Path(f\"{save_path_this_model}\", \"hyperparams.pkl\"), \"wb\") as handle:\n",
    "    pickle.dump(hyperparams, handle)\n",
    "# learning progress\n",
    "training_stats = {\"train_losses\": train_losses,\n",
    "                  \"val_losses\": val_losses,\n",
    "                  \"final_val_losses\": losses,\n",
    "                  \"train_time\": train_time}\n",
    "with open(Path(f\"{save_path_this_model}\", \"learning.pkl\"), \"wb\") as handle:\n",
    "    pickle.dump(training_stats, handle)\n",
    "print(\"Done saving.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(trainset_loader))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = model(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 2, 0, 6, 7, 1, 3, 4, 5, 5, 6, 9, 8, 5, 4, 9, 5, 4, 9, 2, 7, 1,\n",
       "        6, 3, 1, 1, 7, 9, 1, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 4, 0, 6, 7, 1, 3, 4, 7, 7, 6, 9, 8, 5, 4, 9, 5, 4, 9, 2,\n",
       "       7, 1, 3, 3, 1, 1, 7, 9, 1, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(y_est, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(output[0, :].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thalamocortex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
