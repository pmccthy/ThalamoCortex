2025-03-05 17:35:24,478 - INFO - Running hyperparameter combination 1 of 1
2025-03-05 17:35:24,478 - INFO - 0_CTCNet_finetuning_ThalReadout_2
2025-03-05 17:35:26,580 - INFO - Training...
2025-03-05 17:35:26,581 - INFO - Beginning epoch 1/50
2025-03-05 17:35:26,587 - INFO - training batch 1, loss: 0.976, 32/56000 datapoints
2025-03-05 17:35:26,700 - INFO - training batch 51, loss: 0.823, 1632/56000 datapoints
2025-03-05 17:35:26,795 - INFO - training batch 101, loss: 0.940, 3232/56000 datapoints
2025-03-05 17:35:26,893 - INFO - training batch 151, loss: 0.842, 4832/56000 datapoints
2025-03-05 17:35:26,992 - INFO - training batch 201, loss: 0.943, 6432/56000 datapoints
2025-03-05 17:35:27,083 - INFO - training batch 251, loss: 0.894, 8032/56000 datapoints
2025-03-05 17:35:27,173 - INFO - training batch 301, loss: 0.824, 9632/56000 datapoints
2025-03-05 17:35:27,279 - INFO - training batch 351, loss: 0.893, 11232/56000 datapoints
2025-03-05 17:35:27,367 - INFO - training batch 401, loss: 0.788, 12832/56000 datapoints
2025-03-05 17:35:27,453 - INFO - training batch 451, loss: 0.746, 14432/56000 datapoints
2025-03-05 17:35:27,554 - INFO - training batch 501, loss: 0.778, 16032/56000 datapoints
2025-03-05 17:35:27,645 - INFO - training batch 551, loss: 0.778, 17632/56000 datapoints
2025-03-05 17:35:27,737 - INFO - training batch 601, loss: 0.702, 19232/56000 datapoints
2025-03-05 17:35:27,834 - INFO - training batch 651, loss: 0.771, 20832/56000 datapoints
2025-03-05 17:35:27,925 - INFO - training batch 701, loss: 0.729, 22432/56000 datapoints
2025-03-05 17:35:28,014 - INFO - training batch 751, loss: 0.714, 24032/56000 datapoints
2025-03-05 17:35:28,106 - INFO - training batch 801, loss: 0.686, 25632/56000 datapoints
2025-03-05 17:35:28,202 - INFO - training batch 851, loss: 0.671, 27232/56000 datapoints
2025-03-05 17:35:28,291 - INFO - training batch 901, loss: 0.687, 28832/56000 datapoints
2025-03-05 17:35:28,377 - INFO - training batch 951, loss: 0.680, 30432/56000 datapoints
2025-03-05 17:35:28,471 - INFO - training batch 1001, loss: 0.665, 32032/56000 datapoints
2025-03-05 17:35:28,656 - INFO - training batch 1051, loss: 0.672, 33632/56000 datapoints
2025-03-05 17:35:28,789 - INFO - training batch 1101, loss: 0.584, 35232/56000 datapoints
2025-03-05 17:35:28,906 - INFO - training batch 1151, loss: 0.611, 36832/56000 datapoints
2025-03-05 17:35:29,016 - INFO - training batch 1201, loss: 0.529, 38432/56000 datapoints
2025-03-05 17:35:29,111 - INFO - training batch 1251, loss: 0.593, 40032/56000 datapoints
2025-03-05 17:35:29,206 - INFO - training batch 1301, loss: 0.585, 41632/56000 datapoints
2025-03-05 17:35:29,326 - INFO - training batch 1351, loss: 0.492, 43232/56000 datapoints
2025-03-05 17:35:29,446 - INFO - training batch 1401, loss: 0.532, 44832/56000 datapoints
2025-03-05 17:35:29,580 - INFO - training batch 1451, loss: 0.554, 46432/56000 datapoints
2025-03-05 17:35:29,707 - INFO - training batch 1501, loss: 0.543, 48032/56000 datapoints
2025-03-05 17:35:29,829 - INFO - training batch 1551, loss: 0.427, 49632/56000 datapoints
2025-03-05 17:35:29,933 - INFO - training batch 1601, loss: 0.512, 51232/56000 datapoints
2025-03-05 17:35:30,062 - INFO - training batch 1651, loss: 0.474, 52832/56000 datapoints
2025-03-05 17:35:30,177 - INFO - training batch 1701, loss: 0.466, 54432/56000 datapoints
2025-03-05 17:35:30,298 - INFO - validation batch 1, loss: 0.461, 32/13984 datapoints
2025-03-05 17:35:30,393 - INFO - validation batch 51, loss: 0.424, 1632/13984 datapoints
2025-03-05 17:35:30,491 - INFO - validation batch 101, loss: 0.412, 3232/13984 datapoints
2025-03-05 17:35:30,575 - INFO - validation batch 151, loss: 0.428, 4832/13984 datapoints
2025-03-05 17:35:30,643 - INFO - validation batch 201, loss: 0.453, 6432/13984 datapoints
2025-03-05 17:35:30,718 - INFO - validation batch 251, loss: 0.473, 8032/13984 datapoints
2025-03-05 17:35:30,805 - INFO - validation batch 301, loss: 0.479, 9632/13984 datapoints
2025-03-05 17:35:30,890 - INFO - validation batch 351, loss: 0.451, 11232/13984 datapoints
2025-03-05 17:35:30,951 - INFO - validation batch 401, loss: 0.486, 12832/13984 datapoints
2025-03-05 17:35:31,002 - INFO - Epoch 1/50 done.
2025-03-05 17:35:31,003 - INFO - Final validation performance:
Loss: 0.452, top-1 acc: 0.922top-5 acc: 0.922
2025-03-05 17:35:31,003 - INFO - Epoch 1/50 done
2025-03-05 17:35:31,004 - INFO - Beginning epoch 2/50
2025-03-05 17:35:31,007 - INFO - training batch 1, loss: 0.469, 32/56000 datapoints
2025-03-05 17:35:31,158 - INFO - training batch 51, loss: 0.434, 1632/56000 datapoints
2025-03-05 17:35:31,280 - INFO - training batch 101, loss: 0.448, 3232/56000 datapoints
2025-03-05 17:35:31,409 - INFO - training batch 151, loss: 0.437, 4832/56000 datapoints
2025-03-05 17:35:31,525 - INFO - training batch 201, loss: 0.411, 6432/56000 datapoints
2025-03-05 17:35:31,641 - INFO - training batch 251, loss: 0.405, 8032/56000 datapoints
2025-03-05 17:35:31,750 - INFO - training batch 301, loss: 0.414, 9632/56000 datapoints
2025-03-05 17:35:31,846 - INFO - training batch 351, loss: 0.462, 11232/56000 datapoints
2025-03-05 17:35:31,961 - INFO - training batch 401, loss: 0.442, 12832/56000 datapoints
2025-03-05 17:35:32,077 - INFO - training batch 451, loss: 0.418, 14432/56000 datapoints
2025-03-05 17:35:32,186 - INFO - training batch 501, loss: 0.422, 16032/56000 datapoints
2025-03-05 17:35:32,288 - INFO - training batch 551, loss: 0.351, 17632/56000 datapoints
2025-03-05 17:35:32,395 - INFO - training batch 601, loss: 0.298, 19232/56000 datapoints
2025-03-05 17:35:32,504 - INFO - training batch 651, loss: 0.333, 20832/56000 datapoints
2025-03-05 17:35:32,635 - INFO - training batch 701, loss: 0.354, 22432/56000 datapoints
2025-03-05 17:35:32,752 - INFO - training batch 751, loss: 0.342, 24032/56000 datapoints
2025-03-05 17:35:32,853 - INFO - training batch 801, loss: 0.362, 25632/56000 datapoints
2025-03-05 17:35:32,984 - INFO - training batch 851, loss: 0.303, 27232/56000 datapoints
2025-03-05 17:35:33,123 - INFO - training batch 901, loss: 0.345, 28832/56000 datapoints
2025-03-05 17:35:33,241 - INFO - training batch 951, loss: 0.356, 30432/56000 datapoints
2025-03-05 17:35:33,372 - INFO - training batch 1001, loss: 0.273, 32032/56000 datapoints
2025-03-05 17:35:33,498 - INFO - training batch 1051, loss: 0.315, 33632/56000 datapoints
2025-03-05 17:35:33,640 - INFO - training batch 1101, loss: 0.282, 35232/56000 datapoints
2025-03-05 17:35:33,792 - INFO - training batch 1151, loss: 0.232, 36832/56000 datapoints
2025-03-05 17:35:33,945 - INFO - training batch 1201, loss: 0.267, 38432/56000 datapoints
2025-03-05 17:35:34,198 - INFO - training batch 1251, loss: 0.328, 40032/56000 datapoints
2025-03-05 17:35:34,332 - INFO - training batch 1301, loss: 0.320, 41632/56000 datapoints
2025-03-05 17:35:34,500 - INFO - training batch 1351, loss: 0.208, 43232/56000 datapoints
2025-03-05 17:35:34,676 - INFO - training batch 1401, loss: 0.221, 44832/56000 datapoints
2025-03-05 17:35:34,829 - INFO - training batch 1451, loss: 0.256, 46432/56000 datapoints
2025-03-05 17:35:34,976 - INFO - training batch 1501, loss: 0.385, 48032/56000 datapoints
2025-03-05 17:35:35,229 - INFO - training batch 1551, loss: 0.256, 49632/56000 datapoints
2025-03-05 17:35:35,522 - INFO - training batch 1601, loss: 0.334, 51232/56000 datapoints
2025-03-05 17:35:35,784 - INFO - training batch 1651, loss: 0.247, 52832/56000 datapoints
2025-03-05 17:35:36,015 - INFO - training batch 1701, loss: 0.273, 54432/56000 datapoints
2025-03-05 17:35:36,248 - INFO - validation batch 1, loss: 0.215, 32/13984 datapoints
2025-03-05 17:35:36,401 - INFO - validation batch 51, loss: 0.161, 1632/13984 datapoints
2025-03-05 17:35:36,494 - INFO - validation batch 101, loss: 0.259, 3232/13984 datapoints
2025-03-05 17:35:36,607 - INFO - validation batch 151, loss: 0.178, 4832/13984 datapoints
2025-03-05 17:35:36,718 - INFO - validation batch 201, loss: 0.265, 6432/13984 datapoints
2025-03-05 17:35:36,846 - INFO - validation batch 251, loss: 0.285, 8032/13984 datapoints
2025-03-05 17:35:36,964 - INFO - validation batch 301, loss: 0.369, 9632/13984 datapoints
2025-03-05 17:35:37,066 - INFO - validation batch 351, loss: 0.210, 11232/13984 datapoints
2025-03-05 17:35:37,161 - INFO - validation batch 401, loss: 0.322, 12832/13984 datapoints
2025-03-05 17:35:37,365 - INFO - Epoch 2/50 done.
2025-03-05 17:35:37,366 - INFO - Final validation performance:
Loss: 0.252, top-1 acc: 0.891top-5 acc: 0.891
2025-03-05 17:35:37,366 - INFO - Epoch 2/50 done
2025-03-05 17:35:37,367 - INFO - Beginning epoch 3/50
2025-03-05 17:35:37,411 - INFO - training batch 1, loss: 0.245, 32/56000 datapoints
2025-03-05 17:35:37,617 - INFO - training batch 51, loss: 0.308, 1632/56000 datapoints
2025-03-05 17:35:37,776 - INFO - training batch 101, loss: 0.224, 3232/56000 datapoints
2025-03-05 17:35:37,912 - INFO - training batch 151, loss: 0.287, 4832/56000 datapoints
2025-03-05 17:35:38,131 - INFO - training batch 201, loss: 0.190, 6432/56000 datapoints
2025-03-05 17:35:38,297 - INFO - training batch 251, loss: 0.238, 8032/56000 datapoints
2025-03-05 17:35:38,456 - INFO - training batch 301, loss: 0.253, 9632/56000 datapoints
2025-03-05 17:35:38,606 - INFO - training batch 351, loss: 0.277, 11232/56000 datapoints
2025-03-05 17:35:38,739 - INFO - training batch 401, loss: 0.343, 12832/56000 datapoints
2025-03-05 17:35:38,865 - INFO - training batch 451, loss: 0.320, 14432/56000 datapoints
2025-03-05 17:35:38,992 - INFO - training batch 501, loss: 0.299, 16032/56000 datapoints
2025-03-05 17:35:39,117 - INFO - training batch 551, loss: 0.207, 17632/56000 datapoints
2025-03-05 17:35:39,246 - INFO - training batch 601, loss: 0.170, 19232/56000 datapoints
2025-03-05 17:35:39,367 - INFO - training batch 651, loss: 0.187, 20832/56000 datapoints
2025-03-05 17:35:39,488 - INFO - training batch 701, loss: 0.255, 22432/56000 datapoints
2025-03-05 17:35:39,612 - INFO - training batch 751, loss: 0.225, 24032/56000 datapoints
2025-03-05 17:35:39,742 - INFO - training batch 801, loss: 0.277, 25632/56000 datapoints
2025-03-05 17:35:39,872 - INFO - training batch 851, loss: 0.182, 27232/56000 datapoints
2025-03-05 17:35:40,034 - INFO - training batch 901, loss: 0.247, 28832/56000 datapoints
2025-03-05 17:35:40,161 - INFO - training batch 951, loss: 0.256, 30432/56000 datapoints
2025-03-05 17:35:40,287 - INFO - training batch 1001, loss: 0.154, 32032/56000 datapoints
2025-03-05 17:35:40,413 - INFO - training batch 1051, loss: 0.189, 33632/56000 datapoints
2025-03-05 17:35:40,537 - INFO - training batch 1101, loss: 0.216, 35232/56000 datapoints
2025-03-05 17:35:40,698 - INFO - training batch 1151, loss: 0.122, 36832/56000 datapoints
2025-03-05 17:35:40,835 - INFO - training batch 1201, loss: 0.194, 38432/56000 datapoints
2025-03-05 17:35:40,961 - INFO - training batch 1251, loss: 0.260, 40032/56000 datapoints
2025-03-05 17:35:41,081 - INFO - training batch 1301, loss: 0.241, 41632/56000 datapoints
2025-03-05 17:35:41,205 - INFO - training batch 1351, loss: 0.133, 43232/56000 datapoints
2025-03-05 17:35:41,348 - INFO - training batch 1401, loss: 0.129, 44832/56000 datapoints
2025-03-05 17:35:41,503 - INFO - training batch 1451, loss: 0.162, 46432/56000 datapoints
2025-03-05 17:35:41,635 - INFO - training batch 1501, loss: 0.337, 48032/56000 datapoints
2025-03-05 17:35:41,771 - INFO - training batch 1551, loss: 0.211, 49632/56000 datapoints
2025-03-05 17:35:41,893 - INFO - training batch 1601, loss: 0.277, 51232/56000 datapoints
2025-03-05 17:35:42,010 - INFO - training batch 1651, loss: 0.170, 52832/56000 datapoints
2025-03-05 17:35:42,130 - INFO - training batch 1701, loss: 0.213, 54432/56000 datapoints
2025-03-05 17:35:42,262 - INFO - validation batch 1, loss: 0.138, 32/13984 datapoints
2025-03-05 17:35:42,324 - INFO - validation batch 51, loss: 0.082, 1632/13984 datapoints
2025-03-05 17:35:42,385 - INFO - validation batch 101, loss: 0.215, 3232/13984 datapoints
2025-03-05 17:35:42,450 - INFO - validation batch 151, loss: 0.107, 4832/13984 datapoints
2025-03-05 17:35:42,522 - INFO - validation batch 201, loss: 0.207, 6432/13984 datapoints
2025-03-05 17:35:42,591 - INFO - validation batch 251, loss: 0.224, 8032/13984 datapoints
2025-03-05 17:35:42,660 - INFO - validation batch 301, loss: 0.335, 9632/13984 datapoints
2025-03-05 17:35:42,725 - INFO - validation batch 351, loss: 0.135, 11232/13984 datapoints
2025-03-05 17:35:42,786 - INFO - validation batch 401, loss: 0.268, 12832/13984 datapoints
2025-03-05 17:35:42,831 - INFO - Epoch 3/50 done.
2025-03-05 17:35:42,831 - INFO - Final validation performance:
Loss: 0.190, top-1 acc: 0.889top-5 acc: 0.889
2025-03-05 17:35:42,832 - INFO - Epoch 3/50 done
2025-03-05 17:35:42,832 - INFO - Beginning epoch 4/50
2025-03-05 17:35:42,835 - INFO - training batch 1, loss: 0.186, 32/56000 datapoints
2025-03-05 17:35:42,947 - INFO - training batch 51, loss: 0.274, 1632/56000 datapoints
2025-03-05 17:35:43,084 - INFO - training batch 101, loss: 0.154, 3232/56000 datapoints
2025-03-05 17:35:43,214 - INFO - training batch 151, loss: 0.237, 4832/56000 datapoints
2025-03-05 17:35:43,336 - INFO - training batch 201, loss: 0.123, 6432/56000 datapoints
2025-03-05 17:35:43,449 - INFO - training batch 251, loss: 0.187, 8032/56000 datapoints
2025-03-05 17:35:43,562 - INFO - training batch 301, loss: 0.200, 9632/56000 datapoints
2025-03-05 17:35:43,693 - INFO - training batch 351, loss: 0.215, 11232/56000 datapoints
2025-03-05 17:35:43,806 - INFO - training batch 401, loss: 0.300, 12832/56000 datapoints
2025-03-05 17:35:43,914 - INFO - training batch 451, loss: 0.280, 14432/56000 datapoints
2025-03-05 17:35:44,026 - INFO - training batch 501, loss: 0.253, 16032/56000 datapoints
2025-03-05 17:35:44,136 - INFO - training batch 551, loss: 0.163, 17632/56000 datapoints
2025-03-05 17:35:44,248 - INFO - training batch 601, loss: 0.126, 19232/56000 datapoints
2025-03-05 17:35:44,355 - INFO - training batch 651, loss: 0.138, 20832/56000 datapoints
2025-03-05 17:35:44,462 - INFO - training batch 701, loss: 0.224, 22432/56000 datapoints
2025-03-05 17:35:44,571 - INFO - training batch 751, loss: 0.176, 24032/56000 datapoints
2025-03-05 17:35:44,697 - INFO - training batch 801, loss: 0.239, 25632/56000 datapoints
2025-03-05 17:35:44,823 - INFO - training batch 851, loss: 0.134, 27232/56000 datapoints
2025-03-05 17:35:44,947 - INFO - training batch 901, loss: 0.197, 28832/56000 datapoints
2025-03-05 17:35:45,075 - INFO - training batch 951, loss: 0.210, 30432/56000 datapoints
2025-03-05 17:35:45,237 - INFO - training batch 1001, loss: 0.111, 32032/56000 datapoints
2025-03-05 17:35:45,364 - INFO - training batch 1051, loss: 0.139, 33632/56000 datapoints
2025-03-05 17:35:45,606 - INFO - training batch 1101, loss: 0.188, 35232/56000 datapoints
2025-03-05 17:35:45,770 - INFO - training batch 1151, loss: 0.082, 36832/56000 datapoints
2025-03-05 17:35:45,918 - INFO - training batch 1201, loss: 0.168, 38432/56000 datapoints
2025-03-05 17:35:46,068 - INFO - training batch 1251, loss: 0.233, 40032/56000 datapoints
2025-03-05 17:35:46,217 - INFO - training batch 1301, loss: 0.197, 41632/56000 datapoints
2025-03-05 17:35:46,362 - INFO - training batch 1351, loss: 0.101, 43232/56000 datapoints
2025-03-05 17:35:46,550 - INFO - training batch 1401, loss: 0.094, 44832/56000 datapoints
2025-03-05 17:35:46,778 - INFO - training batch 1451, loss: 0.123, 46432/56000 datapoints
2025-03-05 17:35:47,096 - INFO - training batch 1501, loss: 0.286, 48032/56000 datapoints
2025-03-05 17:35:47,275 - INFO - training batch 1551, loss: 0.169, 49632/56000 datapoints
2025-03-05 17:35:47,420 - INFO - training batch 1601, loss: 0.226, 51232/56000 datapoints
2025-03-05 17:35:47,602 - INFO - training batch 1651, loss: 0.131, 52832/56000 datapoints
2025-03-05 17:35:47,751 - INFO - training batch 1701, loss: 0.177, 54432/56000 datapoints
2025-03-05 17:35:47,884 - INFO - validation batch 1, loss: 0.103, 32/13984 datapoints
2025-03-05 17:35:47,957 - INFO - validation batch 51, loss: 0.055, 1632/13984 datapoints
2025-03-05 17:35:48,058 - INFO - validation batch 101, loss: 0.171, 3232/13984 datapoints
2025-03-05 17:35:48,134 - INFO - validation batch 151, loss: 0.077, 4832/13984 datapoints
2025-03-05 17:35:48,198 - INFO - validation batch 201, loss: 0.169, 6432/13984 datapoints
2025-03-05 17:35:48,269 - INFO - validation batch 251, loss: 0.179, 8032/13984 datapoints
2025-03-05 17:35:48,344 - INFO - validation batch 301, loss: 0.279, 9632/13984 datapoints
2025-03-05 17:35:48,411 - INFO - validation batch 351, loss: 0.102, 11232/13984 datapoints
2025-03-05 17:35:48,472 - INFO - validation batch 401, loss: 0.225, 12832/13984 datapoints
2025-03-05 17:35:48,541 - INFO - Epoch 4/50 done.
2025-03-05 17:35:48,541 - INFO - Final validation performance:
Loss: 0.151, top-1 acc: 0.904top-5 acc: 0.904
2025-03-05 17:35:48,542 - INFO - Epoch 4/50 done
2025-03-05 17:35:48,542 - INFO - Beginning epoch 5/50
2025-03-05 17:35:48,550 - INFO - training batch 1, loss: 0.163, 32/56000 datapoints
2025-03-05 17:35:48,907 - INFO - training batch 51, loss: 0.226, 1632/56000 datapoints
2025-03-05 17:35:49,063 - INFO - training batch 101, loss: 0.118, 3232/56000 datapoints
2025-03-05 17:35:49,211 - INFO - training batch 151, loss: 0.192, 4832/56000 datapoints
2025-03-05 17:35:49,362 - INFO - training batch 201, loss: 0.097, 6432/56000 datapoints
2025-03-05 17:35:49,483 - INFO - training batch 251, loss: 0.149, 8032/56000 datapoints
2025-03-05 17:35:49,603 - INFO - training batch 301, loss: 0.162, 9632/56000 datapoints
2025-03-05 17:35:49,723 - INFO - training batch 351, loss: 0.176, 11232/56000 datapoints
2025-03-05 17:35:49,832 - INFO - training batch 401, loss: 0.251, 12832/56000 datapoints
2025-03-05 17:35:49,956 - INFO - training batch 451, loss: 0.232, 14432/56000 datapoints
2025-03-05 17:35:50,063 - INFO - training batch 501, loss: 0.206, 16032/56000 datapoints
2025-03-05 17:35:50,166 - INFO - training batch 551, loss: 0.137, 17632/56000 datapoints
2025-03-05 17:35:50,270 - INFO - training batch 601, loss: 0.093, 19232/56000 datapoints
2025-03-05 17:35:50,368 - INFO - training batch 651, loss: 0.107, 20832/56000 datapoints
2025-03-05 17:35:50,465 - INFO - training batch 701, loss: 0.204, 22432/56000 datapoints
2025-03-05 17:35:50,568 - INFO - training batch 751, loss: 0.136, 24032/56000 datapoints
2025-03-05 17:35:50,673 - INFO - training batch 801, loss: 0.199, 25632/56000 datapoints
2025-03-05 17:35:50,775 - INFO - training batch 851, loss: 0.102, 27232/56000 datapoints
2025-03-05 17:35:50,893 - INFO - training batch 901, loss: 0.151, 28832/56000 datapoints
2025-03-05 17:35:50,993 - INFO - training batch 951, loss: 0.167, 30432/56000 datapoints
2025-03-05 17:35:51,095 - INFO - training batch 1001, loss: 0.084, 32032/56000 datapoints
2025-03-05 17:35:51,212 - INFO - training batch 1051, loss: 0.106, 33632/56000 datapoints
2025-03-05 17:35:51,386 - INFO - training batch 1101, loss: 0.162, 35232/56000 datapoints
2025-03-05 17:35:51,507 - INFO - training batch 1151, loss: 0.058, 36832/56000 datapoints
2025-03-05 17:35:51,618 - INFO - training batch 1201, loss: 0.147, 38432/56000 datapoints
2025-03-05 17:35:51,752 - INFO - training batch 1251, loss: 0.216, 40032/56000 datapoints
2025-03-05 17:35:51,865 - INFO - training batch 1301, loss: 0.154, 41632/56000 datapoints
2025-03-05 17:35:51,974 - INFO - training batch 1351, loss: 0.078, 43232/56000 datapoints
2025-03-05 17:35:52,078 - INFO - training batch 1401, loss: 0.073, 44832/56000 datapoints
2025-03-05 17:35:52,180 - INFO - training batch 1451, loss: 0.094, 46432/56000 datapoints
2025-03-05 17:35:52,289 - INFO - training batch 1501, loss: 0.231, 48032/56000 datapoints
2025-03-05 17:35:52,414 - INFO - training batch 1551, loss: 0.128, 49632/56000 datapoints
2025-03-05 17:35:52,514 - INFO - training batch 1601, loss: 0.177, 51232/56000 datapoints
2025-03-05 17:35:52,652 - INFO - training batch 1651, loss: 0.103, 52832/56000 datapoints
2025-03-05 17:35:52,780 - INFO - training batch 1701, loss: 0.145, 54432/56000 datapoints
2025-03-05 17:35:52,904 - INFO - validation batch 1, loss: 0.077, 32/13984 datapoints
2025-03-05 17:35:53,070 - INFO - validation batch 51, loss: 0.041, 1632/13984 datapoints
2025-03-05 17:35:53,165 - INFO - validation batch 101, loss: 0.131, 3232/13984 datapoints
2025-03-05 17:35:53,255 - INFO - validation batch 151, loss: 0.058, 4832/13984 datapoints
2025-03-05 17:35:53,331 - INFO - validation batch 201, loss: 0.135, 6432/13984 datapoints
2025-03-05 17:35:53,617 - INFO - validation batch 251, loss: 0.140, 8032/13984 datapoints
2025-03-05 17:35:53,855 - INFO - validation batch 301, loss: 0.218, 9632/13984 datapoints
2025-03-05 17:35:54,017 - INFO - validation batch 351, loss: 0.080, 11232/13984 datapoints
2025-03-05 17:35:54,149 - INFO - validation batch 401, loss: 0.186, 12832/13984 datapoints
2025-03-05 17:35:54,210 - INFO - Epoch 5/50 done.
2025-03-05 17:35:54,220 - INFO - Final validation performance:
Loss: 0.118, top-1 acc: 0.952top-5 acc: 0.952
2025-03-05 17:35:54,221 - INFO - Epoch 5/50 done
2025-03-05 17:35:54,221 - INFO - Beginning epoch 6/50
2025-03-05 17:35:54,226 - INFO - training batch 1, loss: 0.148, 32/56000 datapoints
2025-03-05 17:35:54,492 - INFO - training batch 51, loss: 0.177, 1632/56000 datapoints
2025-03-05 17:35:54,761 - INFO - training batch 101, loss: 0.093, 3232/56000 datapoints
2025-03-05 17:35:54,935 - INFO - training batch 151, loss: 0.152, 4832/56000 datapoints
2025-03-05 17:35:55,089 - INFO - training batch 201, loss: 0.080, 6432/56000 datapoints
2025-03-05 17:35:55,210 - INFO - training batch 251, loss: 0.113, 8032/56000 datapoints
2025-03-05 17:35:55,358 - INFO - training batch 301, loss: 0.128, 9632/56000 datapoints
2025-03-05 17:35:55,491 - INFO - training batch 351, loss: 0.144, 11232/56000 datapoints
2025-03-05 17:35:55,642 - INFO - training batch 401, loss: 0.209, 12832/56000 datapoints
2025-03-05 17:35:55,762 - INFO - training batch 451, loss: 0.185, 14432/56000 datapoints
2025-03-05 17:35:55,876 - INFO - training batch 501, loss: 0.161, 16032/56000 datapoints
2025-03-05 17:35:55,990 - INFO - training batch 551, loss: 0.119, 17632/56000 datapoints
2025-03-05 17:35:56,102 - INFO - training batch 601, loss: 0.067, 19232/56000 datapoints
2025-03-05 17:35:56,210 - INFO - training batch 651, loss: 0.083, 20832/56000 datapoints
2025-03-05 17:35:56,334 - INFO - training batch 701, loss: 0.193, 22432/56000 datapoints
2025-03-05 17:35:56,456 - INFO - training batch 751, loss: 0.105, 24032/56000 datapoints
2025-03-05 17:35:56,577 - INFO - training batch 801, loss: 0.161, 25632/56000 datapoints
2025-03-05 17:35:56,700 - INFO - training batch 851, loss: 0.078, 27232/56000 datapoints
2025-03-05 17:35:56,818 - INFO - training batch 901, loss: 0.114, 28832/56000 datapoints
2025-03-05 17:35:56,934 - INFO - training batch 951, loss: 0.129, 30432/56000 datapoints
2025-03-05 17:35:57,053 - INFO - training batch 1001, loss: 0.065, 32032/56000 datapoints
2025-03-05 17:35:57,176 - INFO - training batch 1051, loss: 0.084, 33632/56000 datapoints
2025-03-05 17:35:57,308 - INFO - training batch 1101, loss: 0.139, 35232/56000 datapoints
2025-03-05 17:35:57,482 - INFO - training batch 1151, loss: 0.039, 36832/56000 datapoints
2025-03-05 17:35:57,624 - INFO - training batch 1201, loss: 0.129, 38432/56000 datapoints
2025-03-05 17:35:57,864 - INFO - training batch 1251, loss: 0.209, 40032/56000 datapoints
2025-03-05 17:35:58,066 - INFO - training batch 1301, loss: 0.118, 41632/56000 datapoints
2025-03-05 17:35:58,234 - INFO - training batch 1351, loss: 0.060, 43232/56000 datapoints
2025-03-05 17:35:58,417 - INFO - training batch 1401, loss: 0.059, 44832/56000 datapoints
2025-03-05 17:35:58,601 - INFO - training batch 1451, loss: 0.072, 46432/56000 datapoints
2025-03-05 17:35:58,782 - INFO - training batch 1501, loss: 0.187, 48032/56000 datapoints
2025-03-05 17:35:58,935 - INFO - training batch 1551, loss: 0.096, 49632/56000 datapoints
2025-03-05 17:35:59,079 - INFO - training batch 1601, loss: 0.138, 51232/56000 datapoints
2025-03-05 17:35:59,245 - INFO - training batch 1651, loss: 0.080, 52832/56000 datapoints
2025-03-05 17:35:59,412 - INFO - training batch 1701, loss: 0.120, 54432/56000 datapoints
2025-03-05 17:35:59,558 - INFO - validation batch 1, loss: 0.057, 32/13984 datapoints
2025-03-05 17:35:59,641 - INFO - validation batch 51, loss: 0.033, 1632/13984 datapoints
2025-03-05 17:35:59,720 - INFO - validation batch 101, loss: 0.100, 3232/13984 datapoints
2025-03-05 17:35:59,799 - INFO - validation batch 151, loss: 0.047, 4832/13984 datapoints
2025-03-05 17:35:59,861 - INFO - validation batch 201, loss: 0.107, 6432/13984 datapoints
2025-03-05 17:35:59,925 - INFO - validation batch 251, loss: 0.111, 8032/13984 datapoints
2025-03-05 17:35:59,993 - INFO - validation batch 301, loss: 0.172, 9632/13984 datapoints
2025-03-05 17:36:00,062 - INFO - validation batch 351, loss: 0.064, 11232/13984 datapoints
2025-03-05 17:36:00,143 - INFO - validation batch 401, loss: 0.160, 12832/13984 datapoints
2025-03-05 17:36:00,203 - INFO - Epoch 6/50 done.
2025-03-05 17:36:00,203 - INFO - Final validation performance:
Loss: 0.094, top-1 acc: 0.978top-5 acc: 0.978
2025-03-05 17:36:00,204 - INFO - Epoch 6/50 done
2025-03-05 17:36:00,204 - INFO - Beginning epoch 7/50
2025-03-05 17:36:00,208 - INFO - training batch 1, loss: 0.141, 32/56000 datapoints
2025-03-05 17:36:00,366 - INFO - training batch 51, loss: 0.138, 1632/56000 datapoints
2025-03-05 17:36:00,495 - INFO - training batch 101, loss: 0.076, 3232/56000 datapoints
2025-03-05 17:36:00,914 - INFO - training batch 151, loss: 0.123, 4832/56000 datapoints
2025-03-05 17:36:01,051 - INFO - training batch 201, loss: 0.069, 6432/56000 datapoints
2025-03-05 17:36:01,171 - INFO - training batch 251, loss: 0.087, 8032/56000 datapoints
2025-03-05 17:36:01,291 - INFO - training batch 301, loss: 0.103, 9632/56000 datapoints
2025-03-05 17:36:01,416 - INFO - training batch 351, loss: 0.121, 11232/56000 datapoints
2025-03-05 17:36:01,535 - INFO - training batch 401, loss: 0.177, 12832/56000 datapoints
2025-03-05 17:36:01,660 - INFO - training batch 451, loss: 0.152, 14432/56000 datapoints
2025-03-05 17:36:01,807 - INFO - training batch 501, loss: 0.130, 16032/56000 datapoints
2025-03-05 17:36:01,934 - INFO - training batch 551, loss: 0.109, 17632/56000 datapoints
2025-03-05 17:36:02,046 - INFO - training batch 601, loss: 0.051, 19232/56000 datapoints
2025-03-05 17:36:02,149 - INFO - training batch 651, loss: 0.066, 20832/56000 datapoints
2025-03-05 17:36:02,276 - INFO - training batch 701, loss: 0.195, 22432/56000 datapoints
2025-03-05 17:36:02,431 - INFO - training batch 751, loss: 0.084, 24032/56000 datapoints
2025-03-05 17:36:02,643 - INFO - training batch 801, loss: 0.131, 25632/56000 datapoints
2025-03-05 17:36:02,827 - INFO - training batch 851, loss: 0.061, 27232/56000 datapoints
2025-03-05 17:36:03,089 - INFO - training batch 901, loss: 0.094, 28832/56000 datapoints
2025-03-05 17:36:03,271 - INFO - training batch 951, loss: 0.104, 30432/56000 datapoints
2025-03-05 17:36:03,773 - INFO - training batch 1001, loss: 0.052, 32032/56000 datapoints
2025-03-05 17:36:04,105 - INFO - training batch 1051, loss: 0.068, 33632/56000 datapoints
2025-03-05 17:36:04,508 - INFO - training batch 1101, loss: 0.125, 35232/56000 datapoints
2025-03-05 17:36:04,704 - INFO - training batch 1151, loss: 0.027, 36832/56000 datapoints
2025-03-05 17:36:05,269 - INFO - training batch 1201, loss: 0.114, 38432/56000 datapoints
2025-03-05 17:36:06,007 - INFO - training batch 1251, loss: 0.210, 40032/56000 datapoints
2025-03-05 17:36:06,152 - INFO - training batch 1301, loss: 0.093, 41632/56000 datapoints
2025-03-05 17:36:06,804 - INFO - training batch 1351, loss: 0.049, 43232/56000 datapoints
2025-03-05 17:36:07,198 - INFO - training batch 1401, loss: 0.050, 44832/56000 datapoints
2025-03-05 17:36:07,370 - INFO - training batch 1451, loss: 0.057, 46432/56000 datapoints
2025-03-05 17:36:07,569 - INFO - training batch 1501, loss: 0.160, 48032/56000 datapoints
2025-03-05 17:36:07,693 - INFO - training batch 1551, loss: 0.079, 49632/56000 datapoints
2025-03-05 17:36:07,797 - INFO - training batch 1601, loss: 0.115, 51232/56000 datapoints
2025-03-05 17:36:07,910 - INFO - training batch 1651, loss: 0.065, 52832/56000 datapoints
2025-03-05 17:36:08,010 - INFO - training batch 1701, loss: 0.106, 54432/56000 datapoints
2025-03-05 17:36:08,108 - INFO - validation batch 1, loss: 0.042, 32/13984 datapoints
2025-03-05 17:36:08,163 - INFO - validation batch 51, loss: 0.027, 1632/13984 datapoints
2025-03-05 17:36:08,215 - INFO - validation batch 101, loss: 0.082, 3232/13984 datapoints
2025-03-05 17:36:08,276 - INFO - validation batch 151, loss: 0.039, 4832/13984 datapoints
2025-03-05 17:36:08,331 - INFO - validation batch 201, loss: 0.087, 6432/13984 datapoints
2025-03-05 17:36:08,386 - INFO - validation batch 251, loss: 0.097, 8032/13984 datapoints
2025-03-05 17:36:08,439 - INFO - validation batch 301, loss: 0.144, 9632/13984 datapoints
2025-03-05 17:36:08,495 - INFO - validation batch 351, loss: 0.054, 11232/13984 datapoints
2025-03-05 17:36:08,548 - INFO - validation batch 401, loss: 0.146, 12832/13984 datapoints
2025-03-05 17:36:08,587 - INFO - Epoch 7/50 done.
2025-03-05 17:36:08,587 - INFO - Final validation performance:
Loss: 0.080, top-1 acc: 0.985top-5 acc: 0.985
2025-03-05 17:36:08,588 - INFO - Epoch 7/50 done
2025-03-05 17:36:08,588 - INFO - Beginning epoch 8/50
2025-03-05 17:36:08,591 - INFO - training batch 1, loss: 0.137, 32/56000 datapoints
2025-03-05 17:36:08,693 - INFO - training batch 51, loss: 0.114, 1632/56000 datapoints
2025-03-05 17:36:08,821 - INFO - training batch 101, loss: 0.065, 3232/56000 datapoints
2025-03-05 17:36:08,932 - INFO - training batch 151, loss: 0.106, 4832/56000 datapoints
2025-03-05 17:36:09,031 - INFO - training batch 201, loss: 0.062, 6432/56000 datapoints
2025-03-05 17:36:09,127 - INFO - training batch 251, loss: 0.070, 8032/56000 datapoints
2025-03-05 17:36:09,233 - INFO - training batch 301, loss: 0.086, 9632/56000 datapoints
2025-03-05 17:36:09,336 - INFO - training batch 351, loss: 0.106, 11232/56000 datapoints
2025-03-05 17:36:09,453 - INFO - training batch 401, loss: 0.160, 12832/56000 datapoints
2025-03-05 17:36:09,553 - INFO - training batch 451, loss: 0.134, 14432/56000 datapoints
2025-03-05 17:36:10,231 - INFO - training batch 501, loss: 0.112, 16032/56000 datapoints
2025-03-05 17:36:10,488 - INFO - training batch 551, loss: 0.104, 17632/56000 datapoints
2025-03-05 17:36:10,665 - INFO - training batch 601, loss: 0.043, 19232/56000 datapoints
2025-03-05 17:36:10,854 - INFO - training batch 651, loss: 0.054, 20832/56000 datapoints
2025-03-05 17:36:10,997 - INFO - training batch 701, loss: 0.205, 22432/56000 datapoints
2025-03-05 17:36:11,114 - INFO - training batch 751, loss: 0.074, 24032/56000 datapoints
2025-03-05 17:36:11,239 - INFO - training batch 801, loss: 0.112, 25632/56000 datapoints
2025-03-05 17:36:11,367 - INFO - training batch 851, loss: 0.052, 27232/56000 datapoints
2025-03-05 17:36:11,489 - INFO - training batch 901, loss: 0.084, 28832/56000 datapoints
2025-03-05 17:36:11,634 - INFO - training batch 951, loss: 0.089, 30432/56000 datapoints
2025-03-05 17:36:11,858 - INFO - training batch 1001, loss: 0.043, 32032/56000 datapoints
2025-03-05 17:36:11,966 - INFO - training batch 1051, loss: 0.059, 33632/56000 datapoints
2025-03-05 17:36:12,082 - INFO - training batch 1101, loss: 0.118, 35232/56000 datapoints
2025-03-05 17:36:12,192 - INFO - training batch 1151, loss: 0.021, 36832/56000 datapoints
2025-03-05 17:36:12,302 - INFO - training batch 1201, loss: 0.102, 38432/56000 datapoints
2025-03-05 17:36:12,422 - INFO - training batch 1251, loss: 0.216, 40032/56000 datapoints
2025-03-05 17:36:12,537 - INFO - training batch 1301, loss: 0.077, 41632/56000 datapoints
2025-03-05 17:36:12,633 - INFO - training batch 1351, loss: 0.042, 43232/56000 datapoints
2025-03-05 17:36:12,788 - INFO - training batch 1401, loss: 0.044, 44832/56000 datapoints
2025-03-05 17:36:12,953 - INFO - training batch 1451, loss: 0.047, 46432/56000 datapoints
2025-03-05 17:36:13,063 - INFO - training batch 1501, loss: 0.144, 48032/56000 datapoints
2025-03-05 17:36:13,174 - INFO - training batch 1551, loss: 0.069, 49632/56000 datapoints
2025-03-05 17:36:13,286 - INFO - training batch 1601, loss: 0.101, 51232/56000 datapoints
2025-03-05 17:36:13,397 - INFO - training batch 1651, loss: 0.055, 52832/56000 datapoints
2025-03-05 17:36:13,515 - INFO - training batch 1701, loss: 0.097, 54432/56000 datapoints
2025-03-05 17:36:13,638 - INFO - validation batch 1, loss: 0.033, 32/13984 datapoints
2025-03-05 17:36:13,817 - INFO - validation batch 51, loss: 0.023, 1632/13984 datapoints
2025-03-05 17:36:13,873 - INFO - validation batch 101, loss: 0.072, 3232/13984 datapoints
2025-03-05 17:36:13,941 - INFO - validation batch 151, loss: 0.034, 4832/13984 datapoints
2025-03-05 17:36:13,991 - INFO - validation batch 201, loss: 0.073, 6432/13984 datapoints
2025-03-05 17:36:14,041 - INFO - validation batch 251, loss: 0.086, 8032/13984 datapoints
2025-03-05 17:36:14,091 - INFO - validation batch 301, loss: 0.127, 9632/13984 datapoints
2025-03-05 17:36:14,140 - INFO - validation batch 351, loss: 0.048, 11232/13984 datapoints
2025-03-05 17:36:14,210 - INFO - validation batch 401, loss: 0.139, 12832/13984 datapoints
2025-03-05 17:36:14,250 - INFO - Epoch 8/50 done.
2025-03-05 17:36:14,250 - INFO - Final validation performance:
Loss: 0.071, top-1 acc: 0.988top-5 acc: 0.988
2025-03-05 17:36:14,251 - INFO - Epoch 8/50 done
2025-03-05 17:36:14,251 - INFO - Beginning epoch 9/50
2025-03-05 17:36:14,255 - INFO - training batch 1, loss: 0.135, 32/56000 datapoints
2025-03-05 17:36:14,354 - INFO - training batch 51, loss: 0.098, 1632/56000 datapoints
2025-03-05 17:36:14,458 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-05 17:36:14,554 - INFO - training batch 151, loss: 0.097, 4832/56000 datapoints
2025-03-05 17:36:14,651 - INFO - training batch 201, loss: 0.057, 6432/56000 datapoints
2025-03-05 17:36:14,748 - INFO - training batch 251, loss: 0.060, 8032/56000 datapoints
2025-03-05 17:36:14,875 - INFO - training batch 301, loss: 0.074, 9632/56000 datapoints
2025-03-05 17:36:14,987 - INFO - training batch 351, loss: 0.098, 11232/56000 datapoints
2025-03-05 17:36:15,080 - INFO - training batch 401, loss: 0.149, 12832/56000 datapoints
2025-03-05 17:36:15,247 - INFO - training batch 451, loss: 0.122, 14432/56000 datapoints
2025-03-05 17:36:15,355 - INFO - training batch 501, loss: 0.099, 16032/56000 datapoints
2025-03-05 17:36:15,487 - INFO - training batch 551, loss: 0.100, 17632/56000 datapoints
2025-03-05 17:36:15,655 - INFO - training batch 601, loss: 0.038, 19232/56000 datapoints
2025-03-05 17:36:15,753 - INFO - training batch 651, loss: 0.046, 20832/56000 datapoints
2025-03-05 17:36:15,846 - INFO - training batch 701, loss: 0.214, 22432/56000 datapoints
2025-03-05 17:36:15,943 - INFO - training batch 751, loss: 0.066, 24032/56000 datapoints
2025-03-05 17:36:16,038 - INFO - training batch 801, loss: 0.097, 25632/56000 datapoints
2025-03-05 17:36:16,139 - INFO - training batch 851, loss: 0.045, 27232/56000 datapoints
2025-03-05 17:36:16,240 - INFO - training batch 901, loss: 0.076, 28832/56000 datapoints
2025-03-05 17:36:16,332 - INFO - training batch 951, loss: 0.079, 30432/56000 datapoints
2025-03-05 17:36:16,437 - INFO - training batch 1001, loss: 0.037, 32032/56000 datapoints
2025-03-05 17:36:16,530 - INFO - training batch 1051, loss: 0.053, 33632/56000 datapoints
2025-03-05 17:36:16,640 - INFO - training batch 1101, loss: 0.113, 35232/56000 datapoints
2025-03-05 17:36:16,731 - INFO - training batch 1151, loss: 0.017, 36832/56000 datapoints
2025-03-05 17:36:16,842 - INFO - training batch 1201, loss: 0.091, 38432/56000 datapoints
2025-03-05 17:36:16,950 - INFO - training batch 1251, loss: 0.220, 40032/56000 datapoints
2025-03-05 17:36:17,070 - INFO - training batch 1301, loss: 0.067, 41632/56000 datapoints
2025-03-05 17:36:17,194 - INFO - training batch 1351, loss: 0.036, 43232/56000 datapoints
2025-03-05 17:36:17,325 - INFO - training batch 1401, loss: 0.039, 44832/56000 datapoints
2025-03-05 17:36:17,439 - INFO - training batch 1451, loss: 0.040, 46432/56000 datapoints
2025-03-05 17:36:17,555 - INFO - training batch 1501, loss: 0.131, 48032/56000 datapoints
2025-03-05 17:36:17,839 - INFO - training batch 1551, loss: 0.061, 49632/56000 datapoints
2025-03-05 17:36:17,935 - INFO - training batch 1601, loss: 0.090, 51232/56000 datapoints
2025-03-05 17:36:18,034 - INFO - training batch 1651, loss: 0.048, 52832/56000 datapoints
2025-03-05 17:36:18,131 - INFO - training batch 1701, loss: 0.091, 54432/56000 datapoints
2025-03-05 17:36:18,223 - INFO - validation batch 1, loss: 0.027, 32/13984 datapoints
2025-03-05 17:36:18,272 - INFO - validation batch 51, loss: 0.021, 1632/13984 datapoints
2025-03-05 17:36:18,325 - INFO - validation batch 101, loss: 0.062, 3232/13984 datapoints
2025-03-05 17:36:18,375 - INFO - validation batch 151, loss: 0.032, 4832/13984 datapoints
2025-03-05 17:36:18,738 - INFO - validation batch 201, loss: 0.062, 6432/13984 datapoints
2025-03-05 17:36:18,800 - INFO - validation batch 251, loss: 0.077, 8032/13984 datapoints
2025-03-05 17:36:18,869 - INFO - validation batch 301, loss: 0.113, 9632/13984 datapoints
2025-03-05 17:36:18,937 - INFO - validation batch 351, loss: 0.043, 11232/13984 datapoints
2025-03-05 17:36:18,992 - INFO - validation batch 401, loss: 0.133, 12832/13984 datapoints
2025-03-05 17:36:19,031 - INFO - Epoch 9/50 done.
2025-03-05 17:36:19,032 - INFO - Final validation performance:
Loss: 0.063, top-1 acc: 0.989top-5 acc: 0.989
2025-03-05 17:36:19,032 - INFO - Epoch 9/50 done
2025-03-05 17:36:19,032 - INFO - Beginning epoch 10/50
2025-03-05 17:36:19,035 - INFO - training batch 1, loss: 0.134, 32/56000 datapoints
2025-03-05 17:36:19,132 - INFO - training batch 51, loss: 0.085, 1632/56000 datapoints
2025-03-05 17:36:19,235 - INFO - training batch 101, loss: 0.057, 3232/56000 datapoints
2025-03-05 17:36:19,340 - INFO - training batch 151, loss: 0.090, 4832/56000 datapoints
2025-03-05 17:36:19,445 - INFO - training batch 201, loss: 0.055, 6432/56000 datapoints
2025-03-05 17:36:19,563 - INFO - training batch 251, loss: 0.053, 8032/56000 datapoints
2025-03-05 17:36:19,668 - INFO - training batch 301, loss: 0.066, 9632/56000 datapoints
2025-03-05 17:36:19,784 - INFO - training batch 351, loss: 0.091, 11232/56000 datapoints
2025-03-05 17:36:19,898 - INFO - training batch 401, loss: 0.139, 12832/56000 datapoints
2025-03-05 17:36:20,000 - INFO - training batch 451, loss: 0.110, 14432/56000 datapoints
2025-03-05 17:36:20,092 - INFO - training batch 501, loss: 0.087, 16032/56000 datapoints
2025-03-05 17:36:20,200 - INFO - training batch 551, loss: 0.098, 17632/56000 datapoints
2025-03-05 17:36:20,300 - INFO - training batch 601, loss: 0.033, 19232/56000 datapoints
2025-03-05 17:36:20,710 - INFO - training batch 651, loss: 0.041, 20832/56000 datapoints
2025-03-05 17:36:20,960 - INFO - training batch 701, loss: 0.222, 22432/56000 datapoints
2025-03-05 17:36:21,102 - INFO - training batch 751, loss: 0.060, 24032/56000 datapoints
2025-03-05 17:36:21,274 - INFO - training batch 801, loss: 0.085, 25632/56000 datapoints
2025-03-05 17:36:21,396 - INFO - training batch 851, loss: 0.040, 27232/56000 datapoints
2025-03-05 17:36:21,491 - INFO - training batch 901, loss: 0.068, 28832/56000 datapoints
2025-03-05 17:36:21,591 - INFO - training batch 951, loss: 0.070, 30432/56000 datapoints
2025-03-05 17:36:21,699 - INFO - training batch 1001, loss: 0.033, 32032/56000 datapoints
2025-03-05 17:36:21,828 - INFO - training batch 1051, loss: 0.047, 33632/56000 datapoints
2025-03-05 17:36:21,959 - INFO - training batch 1101, loss: 0.108, 35232/56000 datapoints
2025-03-05 17:36:22,076 - INFO - training batch 1151, loss: 0.014, 36832/56000 datapoints
2025-03-05 17:36:22,208 - INFO - training batch 1201, loss: 0.082, 38432/56000 datapoints
2025-03-05 17:36:22,328 - INFO - training batch 1251, loss: 0.224, 40032/56000 datapoints
2025-03-05 17:36:22,446 - INFO - training batch 1301, loss: 0.059, 41632/56000 datapoints
2025-03-05 17:36:22,547 - INFO - training batch 1351, loss: 0.032, 43232/56000 datapoints
2025-03-05 17:36:22,659 - INFO - training batch 1401, loss: 0.035, 44832/56000 datapoints
2025-03-05 17:36:22,755 - INFO - training batch 1451, loss: 0.034, 46432/56000 datapoints
2025-03-05 17:36:22,854 - INFO - training batch 1501, loss: 0.120, 48032/56000 datapoints
2025-03-05 17:36:22,947 - INFO - training batch 1551, loss: 0.054, 49632/56000 datapoints
2025-03-05 17:36:23,041 - INFO - training batch 1601, loss: 0.079, 51232/56000 datapoints
2025-03-05 17:36:23,141 - INFO - training batch 1651, loss: 0.043, 52832/56000 datapoints
2025-03-05 17:36:23,242 - INFO - training batch 1701, loss: 0.087, 54432/56000 datapoints
2025-03-05 17:36:23,353 - INFO - validation batch 1, loss: 0.023, 32/13984 datapoints
2025-03-05 17:36:23,404 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-05 17:36:23,457 - INFO - validation batch 101, loss: 0.054, 3232/13984 datapoints
2025-03-05 17:36:23,508 - INFO - validation batch 151, loss: 0.030, 4832/13984 datapoints
2025-03-05 17:36:23,562 - INFO - validation batch 201, loss: 0.053, 6432/13984 datapoints
2025-03-05 17:36:23,632 - INFO - validation batch 251, loss: 0.068, 8032/13984 datapoints
2025-03-05 17:36:23,699 - INFO - validation batch 301, loss: 0.101, 9632/13984 datapoints
2025-03-05 17:36:23,804 - INFO - validation batch 351, loss: 0.040, 11232/13984 datapoints
2025-03-05 17:36:23,880 - INFO - validation batch 401, loss: 0.127, 12832/13984 datapoints
2025-03-05 17:36:23,922 - INFO - Epoch 10/50 done.
2025-03-05 17:36:23,923 - INFO - Final validation performance:
Loss: 0.057, top-1 acc: 0.990top-5 acc: 0.990
2025-03-05 17:36:23,923 - INFO - Epoch 10/50 done
2025-03-05 17:36:23,924 - INFO - Beginning epoch 11/50
2025-03-05 17:36:23,927 - INFO - training batch 1, loss: 0.135, 32/56000 datapoints
2025-03-05 17:36:24,038 - INFO - training batch 51, loss: 0.074, 1632/56000 datapoints
2025-03-05 17:36:24,184 - INFO - training batch 101, loss: 0.056, 3232/56000 datapoints
2025-03-05 17:36:24,321 - INFO - training batch 151, loss: 0.085, 4832/56000 datapoints
2025-03-05 17:36:24,445 - INFO - training batch 201, loss: 0.053, 6432/56000 datapoints
2025-03-05 17:36:24,569 - INFO - training batch 251, loss: 0.047, 8032/56000 datapoints
2025-03-05 17:36:24,672 - INFO - training batch 301, loss: 0.059, 9632/56000 datapoints
2025-03-05 17:36:24,770 - INFO - training batch 351, loss: 0.087, 11232/56000 datapoints
2025-03-05 17:36:24,872 - INFO - training batch 401, loss: 0.130, 12832/56000 datapoints
2025-03-05 17:36:24,968 - INFO - training batch 451, loss: 0.100, 14432/56000 datapoints
2025-03-05 17:36:25,066 - INFO - training batch 501, loss: 0.077, 16032/56000 datapoints
2025-03-05 17:36:25,164 - INFO - training batch 551, loss: 0.095, 17632/56000 datapoints
2025-03-05 17:36:25,259 - INFO - training batch 601, loss: 0.029, 19232/56000 datapoints
2025-03-05 17:36:25,372 - INFO - training batch 651, loss: 0.036, 20832/56000 datapoints
2025-03-05 17:36:25,484 - INFO - training batch 701, loss: 0.227, 22432/56000 datapoints
2025-03-05 17:36:25,716 - INFO - training batch 751, loss: 0.054, 24032/56000 datapoints
2025-03-05 17:36:25,837 - INFO - training batch 801, loss: 0.076, 25632/56000 datapoints
2025-03-05 17:36:25,945 - INFO - training batch 851, loss: 0.036, 27232/56000 datapoints
2025-03-05 17:36:26,046 - INFO - training batch 901, loss: 0.061, 28832/56000 datapoints
2025-03-05 17:36:26,150 - INFO - training batch 951, loss: 0.063, 30432/56000 datapoints
2025-03-05 17:36:26,248 - INFO - training batch 1001, loss: 0.029, 32032/56000 datapoints
2025-03-05 17:36:26,347 - INFO - training batch 1051, loss: 0.043, 33632/56000 datapoints
2025-03-05 17:36:26,444 - INFO - training batch 1101, loss: 0.104, 35232/56000 datapoints
2025-03-05 17:36:26,540 - INFO - training batch 1151, loss: 0.012, 36832/56000 datapoints
2025-03-05 17:36:26,638 - INFO - training batch 1201, loss: 0.074, 38432/56000 datapoints
2025-03-05 17:36:26,733 - INFO - training batch 1251, loss: 0.226, 40032/56000 datapoints
2025-03-05 17:36:26,834 - INFO - training batch 1301, loss: 0.052, 41632/56000 datapoints
2025-03-05 17:36:26,930 - INFO - training batch 1351, loss: 0.028, 43232/56000 datapoints
2025-03-05 17:36:27,030 - INFO - training batch 1401, loss: 0.032, 44832/56000 datapoints
2025-03-05 17:36:27,138 - INFO - training batch 1451, loss: 0.029, 46432/56000 datapoints
2025-03-05 17:36:27,234 - INFO - training batch 1501, loss: 0.110, 48032/56000 datapoints
2025-03-05 17:36:27,332 - INFO - training batch 1551, loss: 0.048, 49632/56000 datapoints
2025-03-05 17:36:27,428 - INFO - training batch 1601, loss: 0.070, 51232/56000 datapoints
2025-03-05 17:36:27,532 - INFO - training batch 1651, loss: 0.038, 52832/56000 datapoints
2025-03-05 17:36:27,634 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 17:36:27,731 - INFO - validation batch 1, loss: 0.020, 32/13984 datapoints
2025-03-05 17:36:27,785 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-05 17:36:27,836 - INFO - validation batch 101, loss: 0.047, 3232/13984 datapoints
2025-03-05 17:36:27,887 - INFO - validation batch 151, loss: 0.028, 4832/13984 datapoints
2025-03-05 17:36:27,937 - INFO - validation batch 201, loss: 0.046, 6432/13984 datapoints
2025-03-05 17:36:27,989 - INFO - validation batch 251, loss: 0.061, 8032/13984 datapoints
2025-03-05 17:36:28,043 - INFO - validation batch 301, loss: 0.089, 9632/13984 datapoints
2025-03-05 17:36:28,099 - INFO - validation batch 351, loss: 0.037, 11232/13984 datapoints
2025-03-05 17:36:28,150 - INFO - validation batch 401, loss: 0.122, 12832/13984 datapoints
2025-03-05 17:36:28,188 - INFO - Epoch 11/50 done.
2025-03-05 17:36:28,189 - INFO - Final validation performance:
Loss: 0.052, top-1 acc: 0.991top-5 acc: 0.991
2025-03-05 17:36:28,189 - INFO - Epoch 11/50 done
2025-03-05 17:36:28,190 - INFO - Beginning epoch 12/50
2025-03-05 17:36:28,193 - INFO - training batch 1, loss: 0.136, 32/56000 datapoints
2025-03-05 17:36:28,292 - INFO - training batch 51, loss: 0.065, 1632/56000 datapoints
2025-03-05 17:36:28,398 - INFO - training batch 101, loss: 0.055, 3232/56000 datapoints
2025-03-05 17:36:28,497 - INFO - training batch 151, loss: 0.080, 4832/56000 datapoints
2025-03-05 17:36:28,593 - INFO - training batch 201, loss: 0.052, 6432/56000 datapoints
2025-03-05 17:36:28,693 - INFO - training batch 251, loss: 0.042, 8032/56000 datapoints
2025-03-05 17:36:28,791 - INFO - training batch 301, loss: 0.053, 9632/56000 datapoints
2025-03-05 17:36:28,898 - INFO - training batch 351, loss: 0.083, 11232/56000 datapoints
2025-03-05 17:36:28,999 - INFO - training batch 401, loss: 0.122, 12832/56000 datapoints
2025-03-05 17:36:29,098 - INFO - training batch 451, loss: 0.090, 14432/56000 datapoints
2025-03-05 17:36:29,203 - INFO - training batch 501, loss: 0.068, 16032/56000 datapoints
2025-03-05 17:36:29,336 - INFO - training batch 551, loss: 0.093, 17632/56000 datapoints
2025-03-05 17:36:29,460 - INFO - training batch 601, loss: 0.025, 19232/56000 datapoints
2025-03-05 17:36:29,585 - INFO - training batch 651, loss: 0.033, 20832/56000 datapoints
2025-03-05 17:36:29,713 - INFO - training batch 701, loss: 0.231, 22432/56000 datapoints
2025-03-05 17:36:29,843 - INFO - training batch 751, loss: 0.048, 24032/56000 datapoints
2025-03-05 17:36:29,966 - INFO - training batch 801, loss: 0.067, 25632/56000 datapoints
2025-03-05 17:36:30,088 - INFO - training batch 851, loss: 0.032, 27232/56000 datapoints
2025-03-05 17:36:30,215 - INFO - training batch 901, loss: 0.055, 28832/56000 datapoints
2025-03-05 17:36:30,320 - INFO - training batch 951, loss: 0.056, 30432/56000 datapoints
2025-03-05 17:36:30,421 - INFO - training batch 1001, loss: 0.026, 32032/56000 datapoints
2025-03-05 17:36:30,529 - INFO - training batch 1051, loss: 0.039, 33632/56000 datapoints
2025-03-05 17:36:30,677 - INFO - training batch 1101, loss: 0.100, 35232/56000 datapoints
2025-03-05 17:36:30,792 - INFO - training batch 1151, loss: 0.010, 36832/56000 datapoints
2025-03-05 17:36:30,969 - INFO - training batch 1201, loss: 0.067, 38432/56000 datapoints
2025-03-05 17:36:31,457 - INFO - training batch 1251, loss: 0.228, 40032/56000 datapoints
2025-03-05 17:36:31,693 - INFO - training batch 1301, loss: 0.046, 41632/56000 datapoints
2025-03-05 17:36:31,805 - INFO - training batch 1351, loss: 0.024, 43232/56000 datapoints
2025-03-05 17:36:32,264 - INFO - training batch 1401, loss: 0.029, 44832/56000 datapoints
2025-03-05 17:36:32,506 - INFO - training batch 1451, loss: 0.025, 46432/56000 datapoints
2025-03-05 17:36:32,639 - INFO - training batch 1501, loss: 0.101, 48032/56000 datapoints
2025-03-05 17:36:32,799 - INFO - training batch 1551, loss: 0.042, 49632/56000 datapoints
2025-03-05 17:36:32,911 - INFO - training batch 1601, loss: 0.062, 51232/56000 datapoints
2025-03-05 17:36:33,076 - INFO - training batch 1651, loss: 0.035, 52832/56000 datapoints
2025-03-05 17:36:33,215 - INFO - training batch 1701, loss: 0.083, 54432/56000 datapoints
2025-03-05 17:36:33,323 - INFO - validation batch 1, loss: 0.018, 32/13984 datapoints
2025-03-05 17:36:33,405 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-05 17:36:33,485 - INFO - validation batch 101, loss: 0.040, 3232/13984 datapoints
2025-03-05 17:36:34,005 - INFO - validation batch 151, loss: 0.026, 4832/13984 datapoints
2025-03-05 17:36:34,326 - INFO - validation batch 201, loss: 0.040, 6432/13984 datapoints
2025-03-05 17:36:34,435 - INFO - validation batch 251, loss: 0.054, 8032/13984 datapoints
2025-03-05 17:36:34,511 - INFO - validation batch 301, loss: 0.079, 9632/13984 datapoints
2025-03-05 17:36:34,577 - INFO - validation batch 351, loss: 0.034, 11232/13984 datapoints
2025-03-05 17:36:34,673 - INFO - validation batch 401, loss: 0.118, 12832/13984 datapoints
2025-03-05 17:36:34,714 - INFO - Epoch 12/50 done.
2025-03-05 17:36:34,714 - INFO - Final validation performance:
Loss: 0.047, top-1 acc: 0.991top-5 acc: 0.991
2025-03-05 17:36:34,715 - INFO - Epoch 12/50 done
2025-03-05 17:36:34,715 - INFO - Beginning epoch 13/50
2025-03-05 17:36:34,718 - INFO - training batch 1, loss: 0.137, 32/56000 datapoints
2025-03-05 17:36:34,854 - INFO - training batch 51, loss: 0.057, 1632/56000 datapoints
2025-03-05 17:36:35,253 - INFO - training batch 101, loss: 0.054, 3232/56000 datapoints
2025-03-05 17:36:35,616 - INFO - training batch 151, loss: 0.076, 4832/56000 datapoints
2025-03-05 17:36:35,987 - INFO - training batch 201, loss: 0.051, 6432/56000 datapoints
2025-03-05 17:36:36,190 - INFO - training batch 251, loss: 0.037, 8032/56000 datapoints
2025-03-05 17:36:36,327 - INFO - training batch 301, loss: 0.047, 9632/56000 datapoints
2025-03-05 17:36:36,461 - INFO - training batch 351, loss: 0.081, 11232/56000 datapoints
2025-03-05 17:36:36,584 - INFO - training batch 401, loss: 0.115, 12832/56000 datapoints
2025-03-05 17:36:36,704 - INFO - training batch 451, loss: 0.082, 14432/56000 datapoints
2025-03-05 17:36:36,813 - INFO - training batch 501, loss: 0.059, 16032/56000 datapoints
2025-03-05 17:36:36,939 - INFO - training batch 551, loss: 0.091, 17632/56000 datapoints
2025-03-05 17:36:37,048 - INFO - training batch 601, loss: 0.021, 19232/56000 datapoints
2025-03-05 17:36:37,155 - INFO - training batch 651, loss: 0.030, 20832/56000 datapoints
2025-03-05 17:36:37,261 - INFO - training batch 701, loss: 0.234, 22432/56000 datapoints
2025-03-05 17:36:37,369 - INFO - training batch 751, loss: 0.043, 24032/56000 datapoints
2025-03-05 17:36:37,520 - INFO - training batch 801, loss: 0.060, 25632/56000 datapoints
2025-03-05 17:36:37,656 - INFO - training batch 851, loss: 0.029, 27232/56000 datapoints
2025-03-05 17:36:37,787 - INFO - training batch 901, loss: 0.049, 28832/56000 datapoints
2025-03-05 17:36:37,915 - INFO - training batch 951, loss: 0.050, 30432/56000 datapoints
2025-03-05 17:36:38,035 - INFO - training batch 1001, loss: 0.024, 32032/56000 datapoints
2025-03-05 17:36:38,160 - INFO - training batch 1051, loss: 0.036, 33632/56000 datapoints
2025-03-05 17:36:38,280 - INFO - training batch 1101, loss: 0.096, 35232/56000 datapoints
2025-03-05 17:36:38,389 - INFO - training batch 1151, loss: 0.009, 36832/56000 datapoints
2025-03-05 17:36:38,515 - INFO - training batch 1201, loss: 0.061, 38432/56000 datapoints
2025-03-05 17:36:38,639 - INFO - training batch 1251, loss: 0.230, 40032/56000 datapoints
2025-03-05 17:36:39,323 - INFO - training batch 1301, loss: 0.041, 41632/56000 datapoints
2025-03-05 17:36:39,493 - INFO - training batch 1351, loss: 0.022, 43232/56000 datapoints
2025-03-05 17:36:39,722 - INFO - training batch 1401, loss: 0.026, 44832/56000 datapoints
2025-03-05 17:36:39,869 - INFO - training batch 1451, loss: 0.022, 46432/56000 datapoints
2025-03-05 17:36:40,004 - INFO - training batch 1501, loss: 0.093, 48032/56000 datapoints
2025-03-05 17:36:40,119 - INFO - training batch 1551, loss: 0.037, 49632/56000 datapoints
2025-03-05 17:36:40,235 - INFO - training batch 1601, loss: 0.054, 51232/56000 datapoints
2025-03-05 17:36:40,336 - INFO - training batch 1651, loss: 0.032, 52832/56000 datapoints
2025-03-05 17:36:40,458 - INFO - training batch 1701, loss: 0.083, 54432/56000 datapoints
2025-03-05 17:36:40,609 - INFO - validation batch 1, loss: 0.016, 32/13984 datapoints
2025-03-05 17:36:40,690 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-05 17:36:41,001 - INFO - validation batch 101, loss: 0.035, 3232/13984 datapoints
2025-03-05 17:36:41,083 - INFO - validation batch 151, loss: 0.025, 4832/13984 datapoints
2025-03-05 17:36:41,143 - INFO - validation batch 201, loss: 0.036, 6432/13984 datapoints
2025-03-05 17:36:41,205 - INFO - validation batch 251, loss: 0.047, 8032/13984 datapoints
2025-03-05 17:36:41,273 - INFO - validation batch 301, loss: 0.069, 9632/13984 datapoints
2025-03-05 17:36:41,333 - INFO - validation batch 351, loss: 0.031, 11232/13984 datapoints
2025-03-05 17:36:41,394 - INFO - validation batch 401, loss: 0.113, 12832/13984 datapoints
2025-03-05 17:36:41,435 - INFO - Epoch 13/50 done.
2025-03-05 17:36:41,435 - INFO - Final validation performance:
Loss: 0.043, top-1 acc: 0.992top-5 acc: 0.992
2025-03-05 17:36:41,436 - INFO - Epoch 13/50 done
2025-03-05 17:36:41,436 - INFO - Beginning epoch 14/50
2025-03-05 17:36:41,439 - INFO - training batch 1, loss: 0.138, 32/56000 datapoints
2025-03-05 17:36:41,546 - INFO - training batch 51, loss: 0.050, 1632/56000 datapoints
2025-03-05 17:36:41,651 - INFO - training batch 101, loss: 0.054, 3232/56000 datapoints
2025-03-05 17:36:41,753 - INFO - training batch 151, loss: 0.072, 4832/56000 datapoints
2025-03-05 17:36:41,857 - INFO - training batch 201, loss: 0.050, 6432/56000 datapoints
2025-03-05 17:36:41,962 - INFO - training batch 251, loss: 0.033, 8032/56000 datapoints
2025-03-05 17:36:42,063 - INFO - training batch 301, loss: 0.043, 9632/56000 datapoints
2025-03-05 17:36:42,163 - INFO - training batch 351, loss: 0.079, 11232/56000 datapoints
2025-03-05 17:36:42,264 - INFO - training batch 401, loss: 0.108, 12832/56000 datapoints
2025-03-05 17:36:42,367 - INFO - training batch 451, loss: 0.074, 14432/56000 datapoints
2025-03-05 17:36:42,448 - ERROR - Traceback (most recent call last):
2025-03-05 17:36:42,448 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/train_scripts/train_ff_finetune.py", line 190, in <module>
    train_losses, val_losses, train_topk_accs, val_topk_accs, state_dicts, train_time = train_thalreadout(model=model_thal,
2025-03-05 17:36:42,448 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 491, in train_thalreadout
    train_losses, train_topk_accs, state_dict = train_one_epoch_thalreadout(
2025-03-05 17:36:42,448 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 429, in train_one_epoch_thalreadout
    _, y_est = model(X)
2025-03-05 17:36:42,448 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
2025-03-05 17:36:42,448 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/models.py", line 296, in forward
    input = input.view(input.size(0), -1) # reshape input to flatten and remove second dimension (using view rather than assigning more memory)
2025-03-05 17:36:42,449 - ERROR - KeyboardInterrupt
2025-03-05 17:36:42,449 - ERROR - Traceback (most recent call last):
  File "/Users/patmccarthy/Documents/ThalamoCortex/train_scripts/train_ff_finetune.py", line 190, in <module>
    train_losses, val_losses, train_topk_accs, val_topk_accs, state_dicts, train_time = train_thalreadout(model=model_thal,
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 491, in train_thalreadout
    train_losses, train_topk_accs, state_dict = train_one_epoch_thalreadout(
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 429, in train_one_epoch_thalreadout
    _, y_est = model(X)
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/models.py", line 296, in forward
    input = input.view(input.size(0), -1) # reshape input to flatten and remove second dimension (using view rather than assigning more memory)
KeyboardInterrupt