2025-03-04 14:49:54,268 - INFO - Running hyperparameter combination 1 of 1
2025-03-04 14:49:54,269 - INFO - 0_CTCNet_finetuning_ThalReadout_2
2025-03-04 14:50:05,764 - INFO - Training...
2025-03-04 14:50:05,768 - INFO - Beginning epoch 1/50
2025-03-04 14:50:06,062 - INFO - training batch 1, loss: 0.210, 32/56000 datapoints
2025-03-04 14:50:06,480 - INFO - training batch 51, loss: 0.463, 1632/56000 datapoints
2025-03-04 14:50:07,063 - INFO - training batch 101, loss: 0.174, 3232/56000 datapoints
2025-03-04 14:50:07,833 - INFO - training batch 151, loss: 0.350, 4832/56000 datapoints
2025-03-04 14:50:08,212 - INFO - training batch 201, loss: 0.115, 6432/56000 datapoints
2025-03-04 14:50:08,675 - INFO - training batch 251, loss: 0.250, 8032/56000 datapoints
2025-03-04 14:50:09,484 - INFO - training batch 301, loss: 0.281, 9632/56000 datapoints
2025-03-04 14:50:10,076 - INFO - training batch 351, loss: 0.277, 11232/56000 datapoints
2025-03-04 14:50:10,605 - INFO - training batch 401, loss: 0.457, 12832/56000 datapoints
2025-03-04 14:50:11,112 - INFO - training batch 451, loss: 0.419, 14432/56000 datapoints
2025-03-04 14:50:11,476 - INFO - training batch 501, loss: 0.388, 16032/56000 datapoints
2025-03-04 14:50:11,663 - INFO - training batch 551, loss: 0.197, 17632/56000 datapoints
2025-03-04 14:50:11,871 - INFO - training batch 601, loss: 0.218, 19232/56000 datapoints
2025-03-04 14:50:12,133 - INFO - training batch 651, loss: 0.176, 20832/56000 datapoints
2025-03-04 14:50:12,513 - INFO - training batch 701, loss: 0.305, 22432/56000 datapoints
2025-03-04 14:50:12,732 - INFO - training batch 751, loss: 0.253, 24032/56000 datapoints
2025-03-04 14:50:12,887 - INFO - training batch 801, loss: 0.310, 25632/56000 datapoints
2025-03-04 14:50:13,933 - INFO - training batch 851, loss: 0.182, 27232/56000 datapoints
2025-03-04 14:50:14,163 - INFO - training batch 901, loss: 0.311, 28832/56000 datapoints
2025-03-04 14:50:14,436 - INFO - training batch 951, loss: 0.308, 30432/56000 datapoints
2025-03-04 14:50:14,949 - INFO - training batch 1001, loss: 0.137, 32032/56000 datapoints
2025-03-04 14:50:15,607 - INFO - training batch 1051, loss: 0.164, 33632/56000 datapoints
2025-03-04 14:50:16,513 - INFO - training batch 1101, loss: 0.247, 35232/56000 datapoints
2025-03-04 14:50:17,414 - INFO - training batch 1151, loss: 0.101, 36832/56000 datapoints
2025-03-04 14:50:18,119 - INFO - training batch 1201, loss: 0.174, 38432/56000 datapoints
2025-03-04 14:50:18,405 - INFO - training batch 1251, loss: 0.267, 40032/56000 datapoints
2025-03-04 14:50:18,610 - INFO - training batch 1301, loss: 0.299, 41632/56000 datapoints
2025-03-04 14:50:18,825 - INFO - training batch 1351, loss: 0.126, 43232/56000 datapoints
2025-03-04 14:50:19,460 - INFO - training batch 1401, loss: 0.119, 44832/56000 datapoints
2025-03-04 14:50:19,817 - INFO - training batch 1451, loss: 0.153, 46432/56000 datapoints
2025-03-04 14:50:20,040 - INFO - training batch 1501, loss: 0.386, 48032/56000 datapoints
2025-03-04 14:50:20,426 - INFO - training batch 1551, loss: 0.238, 49632/56000 datapoints
2025-03-04 14:50:20,697 - INFO - training batch 1601, loss: 0.305, 51232/56000 datapoints
2025-03-04 14:50:20,906 - INFO - training batch 1651, loss: 0.149, 52832/56000 datapoints
2025-03-04 14:50:21,076 - INFO - training batch 1701, loss: 0.203, 54432/56000 datapoints
2025-03-04 14:50:21,244 - INFO - validation batch 1, loss: 0.127, 32/13984 datapoints
2025-03-04 14:50:21,304 - INFO - validation batch 51, loss: 0.056, 1632/13984 datapoints
2025-03-04 14:50:21,357 - INFO - validation batch 101, loss: 0.259, 3232/13984 datapoints
2025-03-04 14:50:21,413 - INFO - validation batch 151, loss: 0.100, 4832/13984 datapoints
2025-03-04 14:50:21,471 - INFO - validation batch 201, loss: 0.210, 6432/13984 datapoints
2025-03-04 14:50:21,527 - INFO - validation batch 251, loss: 0.252, 8032/13984 datapoints
2025-03-04 14:50:21,589 - INFO - validation batch 301, loss: 0.404, 9632/13984 datapoints
2025-03-04 14:50:21,671 - INFO - validation batch 351, loss: 0.119, 11232/13984 datapoints
2025-03-04 14:50:21,745 - INFO - validation batch 401, loss: 0.301, 12832/13984 datapoints
2025-03-04 14:50:21,824 - INFO - Epoch 1/50 done
2025-03-04 14:50:21,825 - INFO - Beginning epoch 2/50
2025-03-04 14:50:21,831 - INFO - training batch 1, loss: 0.176, 32/56000 datapoints
2025-03-04 14:50:22,134 - INFO - training batch 51, loss: 0.344, 1632/56000 datapoints
2025-03-04 14:50:22,343 - INFO - training batch 101, loss: 0.133, 3232/56000 datapoints
2025-03-04 14:50:22,558 - INFO - training batch 151, loss: 0.258, 4832/56000 datapoints
2025-03-04 14:50:22,715 - INFO - training batch 201, loss: 0.094, 6432/56000 datapoints
2025-03-04 14:50:22,866 - INFO - training batch 251, loss: 0.185, 8032/56000 datapoints
2025-03-04 14:50:23,105 - INFO - training batch 301, loss: 0.215, 9632/56000 datapoints
2025-03-04 14:50:23,274 - INFO - training batch 351, loss: 0.222, 11232/56000 datapoints
2025-03-04 14:50:23,532 - INFO - training batch 401, loss: 0.336, 12832/56000 datapoints
2025-03-04 14:50:23,748 - INFO - training batch 451, loss: 0.318, 14432/56000 datapoints
2025-03-04 14:50:23,925 - INFO - training batch 501, loss: 0.289, 16032/56000 datapoints
2025-03-04 14:50:24,051 - INFO - training batch 551, loss: 0.164, 17632/56000 datapoints
2025-03-04 14:50:24,180 - INFO - training batch 601, loss: 0.152, 19232/56000 datapoints
2025-03-04 14:50:24,652 - INFO - training batch 651, loss: 0.133, 20832/56000 datapoints
2025-03-04 14:50:24,892 - INFO - training batch 701, loss: 0.245, 22432/56000 datapoints
2025-03-04 14:50:25,014 - INFO - training batch 751, loss: 0.187, 24032/56000 datapoints
2025-03-04 14:50:25,122 - INFO - training batch 801, loss: 0.231, 25632/56000 datapoints
2025-03-04 14:50:25,241 - INFO - training batch 851, loss: 0.134, 27232/56000 datapoints
2025-03-04 14:50:25,358 - INFO - training batch 901, loss: 0.223, 28832/56000 datapoints
2025-03-04 14:50:25,455 - INFO - training batch 951, loss: 0.231, 30432/56000 datapoints
2025-03-04 14:50:25,585 - INFO - training batch 1001, loss: 0.103, 32032/56000 datapoints
2025-03-04 14:50:25,683 - INFO - training batch 1051, loss: 0.121, 33632/56000 datapoints
2025-03-04 14:50:25,822 - INFO - training batch 1101, loss: 0.190, 35232/56000 datapoints
2025-03-04 14:50:25,937 - INFO - training batch 1151, loss: 0.072, 36832/56000 datapoints
2025-03-04 14:50:26,046 - INFO - training batch 1201, loss: 0.138, 38432/56000 datapoints
2025-03-04 14:50:26,152 - INFO - training batch 1251, loss: 0.228, 40032/56000 datapoints
2025-03-04 14:50:26,262 - INFO - training batch 1301, loss: 0.211, 41632/56000 datapoints
2025-03-04 14:50:26,374 - INFO - training batch 1351, loss: 0.090, 43232/56000 datapoints
2025-03-04 14:50:26,483 - INFO - training batch 1401, loss: 0.093, 44832/56000 datapoints
2025-03-04 14:50:26,607 - INFO - training batch 1451, loss: 0.117, 46432/56000 datapoints
2025-03-04 14:50:26,834 - INFO - training batch 1501, loss: 0.286, 48032/56000 datapoints
2025-03-04 14:50:27,033 - INFO - training batch 1551, loss: 0.162, 49632/56000 datapoints
2025-03-04 14:50:27,216 - INFO - training batch 1601, loss: 0.215, 51232/56000 datapoints
2025-03-04 14:50:27,463 - INFO - training batch 1651, loss: 0.113, 52832/56000 datapoints
2025-03-04 14:50:27,637 - INFO - training batch 1701, loss: 0.152, 54432/56000 datapoints
2025-03-04 14:50:27,843 - INFO - validation batch 1, loss: 0.098, 32/13984 datapoints
2025-03-04 14:50:27,923 - INFO - validation batch 51, loss: 0.048, 1632/13984 datapoints
2025-03-04 14:50:27,976 - INFO - validation batch 101, loss: 0.174, 3232/13984 datapoints
2025-03-04 14:50:28,078 - INFO - validation batch 151, loss: 0.074, 4832/13984 datapoints
2025-03-04 14:50:28,127 - INFO - validation batch 201, loss: 0.158, 6432/13984 datapoints
2025-03-04 14:50:28,174 - INFO - validation batch 251, loss: 0.176, 8032/13984 datapoints
2025-03-04 14:50:28,223 - INFO - validation batch 301, loss: 0.280, 9632/13984 datapoints
2025-03-04 14:50:28,272 - INFO - validation batch 351, loss: 0.091, 11232/13984 datapoints
2025-03-04 14:50:28,324 - INFO - validation batch 401, loss: 0.225, 12832/13984 datapoints
2025-03-04 14:50:28,366 - INFO - Epoch 2/50 done
2025-03-04 14:50:28,367 - INFO - Beginning epoch 3/50
2025-03-04 14:50:28,373 - INFO - training batch 1, loss: 0.155, 32/56000 datapoints
2025-03-04 14:50:28,595 - INFO - training batch 51, loss: 0.235, 1632/56000 datapoints
2025-03-04 14:50:28,745 - INFO - training batch 101, loss: 0.100, 3232/56000 datapoints
2025-03-04 14:50:29,031 - INFO - training batch 151, loss: 0.179, 4832/56000 datapoints
2025-03-04 14:50:29,238 - INFO - training batch 201, loss: 0.081, 6432/56000 datapoints
2025-03-04 14:50:29,459 - INFO - training batch 251, loss: 0.128, 8032/56000 datapoints
2025-03-04 14:50:29,732 - INFO - training batch 301, loss: 0.156, 9632/56000 datapoints
2025-03-04 14:50:30,023 - INFO - training batch 351, loss: 0.173, 11232/56000 datapoints
2025-03-04 14:50:30,179 - INFO - training batch 401, loss: 0.241, 12832/56000 datapoints
2025-03-04 14:50:30,348 - INFO - training batch 451, loss: 0.229, 14432/56000 datapoints
2025-03-04 14:50:30,514 - INFO - training batch 501, loss: 0.202, 16032/56000 datapoints
2025-03-04 14:50:30,703 - INFO - training batch 551, loss: 0.137, 17632/56000 datapoints
2025-03-04 14:50:31,039 - INFO - training batch 601, loss: 0.100, 19232/56000 datapoints
2025-03-04 14:50:31,241 - INFO - training batch 651, loss: 0.097, 20832/56000 datapoints
2025-03-04 14:50:31,523 - INFO - training batch 701, loss: 0.203, 22432/56000 datapoints
2025-03-04 14:50:31,694 - INFO - training batch 751, loss: 0.130, 24032/56000 datapoints
2025-03-04 14:50:31,880 - INFO - training batch 801, loss: 0.165, 25632/56000 datapoints
2025-03-04 14:50:32,019 - INFO - training batch 851, loss: 0.094, 27232/56000 datapoints
2025-03-04 14:50:32,154 - INFO - training batch 901, loss: 0.152, 28832/56000 datapoints
2025-03-04 14:50:32,292 - INFO - training batch 951, loss: 0.165, 30432/56000 datapoints
2025-03-04 14:50:32,430 - INFO - training batch 1001, loss: 0.076, 32032/56000 datapoints
2025-03-04 14:50:32,581 - INFO - training batch 1051, loss: 0.089, 33632/56000 datapoints
2025-03-04 14:50:32,772 - INFO - training batch 1101, loss: 0.142, 35232/56000 datapoints
2025-03-04 14:50:32,897 - INFO - training batch 1151, loss: 0.047, 36832/56000 datapoints
2025-03-04 14:50:33,058 - INFO - training batch 1201, loss: 0.109, 38432/56000 datapoints
2025-03-04 14:50:33,311 - INFO - training batch 1251, loss: 0.204, 40032/56000 datapoints
2025-03-04 14:50:33,537 - INFO - training batch 1301, loss: 0.139, 41632/56000 datapoints
2025-03-04 14:50:33,748 - INFO - training batch 1351, loss: 0.062, 43232/56000 datapoints
2025-03-04 14:50:33,869 - INFO - training batch 1401, loss: 0.071, 44832/56000 datapoints
2025-03-04 14:50:34,053 - INFO - training batch 1451, loss: 0.085, 46432/56000 datapoints
2025-03-04 14:50:34,179 - INFO - training batch 1501, loss: 0.205, 48032/56000 datapoints
2025-03-04 14:50:34,364 - INFO - training batch 1551, loss: 0.105, 49632/56000 datapoints
2025-03-04 14:50:34,537 - INFO - training batch 1601, loss: 0.145, 51232/56000 datapoints
2025-03-04 14:50:34,712 - INFO - training batch 1651, loss: 0.085, 52832/56000 datapoints
2025-03-04 14:50:34,856 - INFO - training batch 1701, loss: 0.115, 54432/56000 datapoints
2025-03-04 14:50:34,985 - INFO - validation batch 1, loss: 0.072, 32/13984 datapoints
2025-03-04 14:50:35,042 - INFO - validation batch 51, loss: 0.042, 1632/13984 datapoints
2025-03-04 14:50:35,105 - INFO - validation batch 101, loss: 0.112, 3232/13984 datapoints
2025-03-04 14:50:35,170 - INFO - validation batch 151, loss: 0.053, 4832/13984 datapoints
2025-03-04 14:50:35,229 - INFO - validation batch 201, loss: 0.115, 6432/13984 datapoints
2025-03-04 14:50:35,308 - INFO - validation batch 251, loss: 0.119, 8032/13984 datapoints
2025-03-04 14:50:35,382 - INFO - validation batch 301, loss: 0.187, 9632/13984 datapoints
2025-03-04 14:50:35,481 - INFO - validation batch 351, loss: 0.067, 11232/13984 datapoints
2025-03-04 14:50:35,566 - INFO - validation batch 401, loss: 0.169, 12832/13984 datapoints
2025-03-04 14:50:35,622 - INFO - Epoch 3/50 done
2025-03-04 14:50:35,622 - INFO - Beginning epoch 4/50
2025-03-04 14:50:35,626 - INFO - training batch 1, loss: 0.144, 32/56000 datapoints
2025-03-04 14:50:35,765 - INFO - training batch 51, loss: 0.154, 1632/56000 datapoints
2025-03-04 14:50:35,881 - INFO - training batch 101, loss: 0.079, 3232/56000 datapoints
2025-03-04 14:50:36,014 - INFO - training batch 151, loss: 0.125, 4832/56000 datapoints
2025-03-04 14:50:36,150 - INFO - training batch 201, loss: 0.071, 6432/56000 datapoints
2025-03-04 14:50:36,272 - INFO - training batch 251, loss: 0.086, 8032/56000 datapoints
2025-03-04 14:50:36,418 - INFO - training batch 301, loss: 0.112, 9632/56000 datapoints
2025-03-04 14:50:36,562 - INFO - training batch 351, loss: 0.134, 11232/56000 datapoints
2025-03-04 14:50:36,688 - INFO - training batch 401, loss: 0.174, 12832/56000 datapoints
2025-03-04 14:50:36,817 - INFO - training batch 451, loss: 0.164, 14432/56000 datapoints
2025-03-04 14:50:36,941 - INFO - training batch 501, loss: 0.141, 16032/56000 datapoints
2025-03-04 14:50:37,061 - INFO - training batch 551, loss: 0.116, 17632/56000 datapoints
2025-03-04 14:50:37,168 - INFO - training batch 601, loss: 0.062, 19232/56000 datapoints
2025-03-04 14:50:37,268 - INFO - training batch 651, loss: 0.068, 20832/56000 datapoints
2025-03-04 14:50:37,373 - INFO - training batch 701, loss: 0.182, 22432/56000 datapoints
2025-03-04 14:50:37,484 - INFO - training batch 751, loss: 0.090, 24032/56000 datapoints
2025-03-04 14:50:37,591 - INFO - training batch 801, loss: 0.119, 25632/56000 datapoints
2025-03-04 14:50:37,680 - INFO - training batch 851, loss: 0.065, 27232/56000 datapoints
2025-03-04 14:50:37,766 - INFO - training batch 901, loss: 0.104, 28832/56000 datapoints
2025-03-04 14:50:37,862 - INFO - training batch 951, loss: 0.115, 30432/56000 datapoints
2025-03-04 14:50:37,982 - INFO - training batch 1001, loss: 0.055, 32032/56000 datapoints
2025-03-04 14:50:38,091 - INFO - training batch 1051, loss: 0.067, 33632/56000 datapoints
2025-03-04 14:50:38,193 - INFO - training batch 1101, loss: 0.109, 35232/56000 datapoints
2025-03-04 14:50:38,334 - INFO - training batch 1151, loss: 0.029, 36832/56000 datapoints
2025-03-04 14:50:38,461 - INFO - training batch 1201, loss: 0.088, 38432/56000 datapoints
2025-03-04 14:50:38,613 - INFO - training batch 1251, loss: 0.191, 40032/56000 datapoints
2025-03-04 14:50:38,767 - INFO - training batch 1301, loss: 0.091, 41632/56000 datapoints
2025-03-04 14:50:39,670 - INFO - training batch 1351, loss: 0.043, 43232/56000 datapoints
2025-03-04 14:50:39,881 - INFO - training batch 1401, loss: 0.054, 44832/56000 datapoints
2025-03-04 14:50:40,064 - INFO - training batch 1451, loss: 0.060, 46432/56000 datapoints
2025-03-04 14:50:40,202 - INFO - training batch 1501, loss: 0.148, 48032/56000 datapoints
2025-03-04 14:50:40,325 - INFO - training batch 1551, loss: 0.067, 49632/56000 datapoints
2025-03-04 14:50:40,463 - INFO - training batch 1601, loss: 0.098, 51232/56000 datapoints
2025-03-04 14:50:40,561 - INFO - training batch 1651, loss: 0.064, 52832/56000 datapoints
2025-03-04 14:50:40,655 - INFO - training batch 1701, loss: 0.096, 54432/56000 datapoints
2025-03-04 14:50:40,758 - INFO - validation batch 1, loss: 0.053, 32/13984 datapoints
2025-03-04 14:50:40,799 - INFO - validation batch 51, loss: 0.037, 1632/13984 datapoints
2025-03-04 14:50:40,833 - INFO - validation batch 101, loss: 0.071, 3232/13984 datapoints
2025-03-04 14:50:40,866 - INFO - validation batch 151, loss: 0.039, 4832/13984 datapoints
2025-03-04 14:50:40,898 - INFO - validation batch 201, loss: 0.084, 6432/13984 datapoints
2025-03-04 14:50:40,937 - INFO - validation batch 251, loss: 0.081, 8032/13984 datapoints
2025-03-04 14:50:40,967 - INFO - validation batch 301, loss: 0.124, 9632/13984 datapoints
2025-03-04 14:50:40,992 - INFO - validation batch 351, loss: 0.050, 11232/13984 datapoints
2025-03-04 14:50:41,020 - INFO - validation batch 401, loss: 0.135, 12832/13984 datapoints
2025-03-04 14:50:41,045 - INFO - Epoch 4/50 done
2025-03-04 14:50:41,046 - INFO - Beginning epoch 5/50
2025-03-04 14:50:41,048 - INFO - training batch 1, loss: 0.139, 32/56000 datapoints
2025-03-04 14:50:41,173 - INFO - training batch 51, loss: 0.101, 1632/56000 datapoints
2025-03-04 14:50:41,264 - INFO - training batch 101, loss: 0.066, 3232/56000 datapoints
2025-03-04 14:50:41,386 - INFO - training batch 151, loss: 0.095, 4832/56000 datapoints
2025-03-04 14:50:41,521 - INFO - training batch 201, loss: 0.065, 6432/56000 datapoints
2025-03-04 14:50:41,649 - INFO - training batch 251, loss: 0.059, 8032/56000 datapoints
2025-03-04 14:50:41,762 - INFO - training batch 301, loss: 0.082, 9632/56000 datapoints
2025-03-04 14:50:41,883 - INFO - training batch 351, loss: 0.108, 11232/56000 datapoints
2025-03-04 14:50:42,144 - INFO - training batch 401, loss: 0.133, 12832/56000 datapoints
2025-03-04 14:50:42,280 - INFO - training batch 451, loss: 0.121, 14432/56000 datapoints
2025-03-04 14:50:42,419 - INFO - training batch 501, loss: 0.100, 16032/56000 datapoints
2025-03-04 14:50:42,538 - INFO - training batch 551, loss: 0.101, 17632/56000 datapoints
2025-03-04 14:50:42,637 - INFO - training batch 601, loss: 0.038, 19232/56000 datapoints
2025-03-04 14:50:42,742 - INFO - training batch 651, loss: 0.049, 20832/56000 datapoints
2025-03-04 14:50:42,862 - INFO - training batch 701, loss: 0.181, 22432/56000 datapoints
2025-03-04 14:50:42,955 - INFO - training batch 751, loss: 0.063, 24032/56000 datapoints
2025-03-04 14:50:43,039 - INFO - training batch 801, loss: 0.088, 25632/56000 datapoints
2025-03-04 14:50:43,137 - INFO - training batch 851, loss: 0.045, 27232/56000 datapoints
2025-03-04 14:50:43,243 - INFO - training batch 901, loss: 0.072, 28832/56000 datapoints
2025-03-04 14:50:43,340 - INFO - training batch 951, loss: 0.081, 30432/56000 datapoints
2025-03-04 14:50:43,436 - INFO - training batch 1001, loss: 0.040, 32032/56000 datapoints
2025-03-04 14:50:43,527 - INFO - training batch 1051, loss: 0.052, 33632/56000 datapoints
2025-03-04 14:50:43,660 - INFO - training batch 1101, loss: 0.089, 35232/56000 datapoints
2025-03-04 14:50:43,775 - INFO - training batch 1151, loss: 0.018, 36832/56000 datapoints
2025-03-04 14:50:43,867 - INFO - training batch 1201, loss: 0.073, 38432/56000 datapoints
2025-03-04 14:50:43,975 - INFO - training batch 1251, loss: 0.187, 40032/56000 datapoints
2025-03-04 14:50:44,063 - INFO - training batch 1301, loss: 0.063, 41632/56000 datapoints
2025-03-04 14:50:44,154 - INFO - training batch 1351, loss: 0.031, 43232/56000 datapoints
2025-03-04 14:50:44,257 - INFO - training batch 1401, loss: 0.041, 44832/56000 datapoints
2025-03-04 14:50:44,359 - INFO - training batch 1451, loss: 0.042, 46432/56000 datapoints
2025-03-04 14:50:44,475 - INFO - training batch 1501, loss: 0.110, 48032/56000 datapoints
2025-03-04 14:50:44,715 - INFO - training batch 1551, loss: 0.045, 49632/56000 datapoints
2025-03-04 14:50:45,757 - INFO - training batch 1601, loss: 0.068, 51232/56000 datapoints
2025-03-04 14:50:46,271 - INFO - training batch 1651, loss: 0.050, 52832/56000 datapoints
2025-03-04 14:50:46,872 - INFO - training batch 1701, loss: 0.088, 54432/56000 datapoints
2025-03-04 14:50:47,355 - INFO - validation batch 1, loss: 0.039, 32/13984 datapoints
2025-03-04 14:50:47,415 - INFO - validation batch 51, loss: 0.033, 1632/13984 datapoints
2025-03-04 14:50:47,480 - INFO - validation batch 101, loss: 0.046, 3232/13984 datapoints
2025-03-04 14:50:47,556 - INFO - validation batch 151, loss: 0.031, 4832/13984 datapoints
2025-03-04 14:50:47,609 - INFO - validation batch 201, loss: 0.062, 6432/13984 datapoints
2025-03-04 14:50:47,658 - INFO - validation batch 251, loss: 0.057, 8032/13984 datapoints
2025-03-04 14:50:47,727 - INFO - validation batch 301, loss: 0.084, 9632/13984 datapoints
2025-03-04 14:50:47,773 - INFO - validation batch 351, loss: 0.039, 11232/13984 datapoints
2025-03-04 14:50:47,831 - INFO - validation batch 401, loss: 0.116, 12832/13984 datapoints
2025-03-04 14:50:47,881 - INFO - Epoch 5/50 done
2025-03-04 14:50:47,882 - INFO - Beginning epoch 6/50
2025-03-04 14:50:47,887 - INFO - training batch 1, loss: 0.137, 32/56000 datapoints
2025-03-04 14:50:48,168 - INFO - training batch 51, loss: 0.068, 1632/56000 datapoints
2025-03-04 14:50:48,860 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:50:49,314 - INFO - training batch 151, loss: 0.078, 4832/56000 datapoints
2025-03-04 14:50:49,585 - INFO - training batch 201, loss: 0.060, 6432/56000 datapoints
2025-03-04 14:50:49,748 - INFO - training batch 251, loss: 0.042, 8032/56000 datapoints
2025-03-04 14:50:50,003 - INFO - training batch 301, loss: 0.062, 9632/56000 datapoints
2025-03-04 14:50:50,284 - INFO - training batch 351, loss: 0.092, 11232/56000 datapoints
2025-03-04 14:50:50,435 - INFO - training batch 401, loss: 0.109, 12832/56000 datapoints
2025-03-04 14:50:50,599 - INFO - training batch 451, loss: 0.093, 14432/56000 datapoints
2025-03-04 14:50:50,724 - INFO - training batch 501, loss: 0.073, 16032/56000 datapoints
2025-03-04 14:50:50,865 - INFO - training batch 551, loss: 0.092, 17632/56000 datapoints
2025-03-04 14:50:50,997 - INFO - training batch 601, loss: 0.024, 19232/56000 datapoints
2025-03-04 14:50:51,116 - INFO - training batch 651, loss: 0.036, 20832/56000 datapoints
2025-03-04 14:50:51,229 - INFO - training batch 701, loss: 0.188, 22432/56000 datapoints
2025-03-04 14:50:51,349 - INFO - training batch 751, loss: 0.046, 24032/56000 datapoints
2025-03-04 14:50:51,494 - INFO - training batch 801, loss: 0.068, 25632/56000 datapoints
2025-03-04 14:50:51,635 - INFO - training batch 851, loss: 0.032, 27232/56000 datapoints
2025-03-04 14:50:51,757 - INFO - training batch 901, loss: 0.053, 28832/56000 datapoints
2025-03-04 14:50:51,857 - INFO - training batch 951, loss: 0.059, 30432/56000 datapoints
2025-03-04 14:50:52,097 - INFO - training batch 1001, loss: 0.030, 32032/56000 datapoints
2025-03-04 14:50:52,786 - INFO - training batch 1051, loss: 0.040, 33632/56000 datapoints
2025-03-04 14:50:53,416 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:50:53,789 - INFO - training batch 1151, loss: 0.011, 36832/56000 datapoints
2025-03-04 14:50:53,972 - INFO - training batch 1201, loss: 0.062, 38432/56000 datapoints
2025-03-04 14:50:54,138 - INFO - training batch 1251, loss: 0.190, 40032/56000 datapoints
2025-03-04 14:50:54,257 - INFO - training batch 1301, loss: 0.046, 41632/56000 datapoints
2025-03-04 14:50:54,385 - INFO - training batch 1351, loss: 0.023, 43232/56000 datapoints
2025-03-04 14:50:54,498 - INFO - training batch 1401, loss: 0.032, 44832/56000 datapoints
2025-03-04 14:50:54,620 - INFO - training batch 1451, loss: 0.029, 46432/56000 datapoints
2025-03-04 14:50:54,749 - INFO - training batch 1501, loss: 0.086, 48032/56000 datapoints
2025-03-04 14:50:54,884 - INFO - training batch 1551, loss: 0.032, 49632/56000 datapoints
2025-03-04 14:50:55,135 - INFO - training batch 1601, loss: 0.049, 51232/56000 datapoints
2025-03-04 14:50:55,318 - INFO - training batch 1651, loss: 0.040, 52832/56000 datapoints
2025-03-04 14:50:55,537 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-04 14:50:55,679 - INFO - validation batch 1, loss: 0.030, 32/13984 datapoints
2025-03-04 14:50:55,728 - INFO - validation batch 51, loss: 0.030, 1632/13984 datapoints
2025-03-04 14:50:55,768 - INFO - validation batch 101, loss: 0.032, 3232/13984 datapoints
2025-03-04 14:50:55,810 - INFO - validation batch 151, loss: 0.025, 4832/13984 datapoints
2025-03-04 14:50:55,886 - INFO - validation batch 201, loss: 0.047, 6432/13984 datapoints
2025-03-04 14:50:55,950 - INFO - validation batch 251, loss: 0.042, 8032/13984 datapoints
2025-03-04 14:50:56,000 - INFO - validation batch 301, loss: 0.059, 9632/13984 datapoints
2025-03-04 14:50:56,054 - INFO - validation batch 351, loss: 0.031, 11232/13984 datapoints
2025-03-04 14:50:56,109 - INFO - validation batch 401, loss: 0.107, 12832/13984 datapoints
2025-03-04 14:50:56,159 - INFO - Epoch 6/50 done
2025-03-04 14:50:56,160 - INFO - Beginning epoch 7/50
2025-03-04 14:50:56,163 - INFO - training batch 1, loss: 0.137, 32/56000 datapoints
2025-03-04 14:50:56,325 - INFO - training batch 51, loss: 0.048, 1632/56000 datapoints
2025-03-04 14:50:56,491 - INFO - training batch 101, loss: 0.057, 3232/56000 datapoints
2025-03-04 14:50:56,614 - INFO - training batch 151, loss: 0.071, 4832/56000 datapoints
2025-03-04 14:50:56,771 - INFO - training batch 201, loss: 0.057, 6432/56000 datapoints
2025-03-04 14:50:56,915 - INFO - training batch 251, loss: 0.031, 8032/56000 datapoints
2025-03-04 14:50:57,022 - INFO - training batch 301, loss: 0.048, 9632/56000 datapoints
2025-03-04 14:50:57,135 - INFO - training batch 351, loss: 0.083, 11232/56000 datapoints
2025-03-04 14:50:57,261 - INFO - training batch 401, loss: 0.096, 12832/56000 datapoints
2025-03-04 14:50:57,372 - INFO - training batch 451, loss: 0.076, 14432/56000 datapoints
2025-03-04 14:50:57,467 - INFO - training batch 501, loss: 0.056, 16032/56000 datapoints
2025-03-04 14:50:57,594 - INFO - training batch 551, loss: 0.087, 17632/56000 datapoints
2025-03-04 14:50:58,208 - INFO - training batch 601, loss: 0.016, 19232/56000 datapoints
2025-03-04 14:50:58,330 - INFO - training batch 651, loss: 0.028, 20832/56000 datapoints
2025-03-04 14:50:58,448 - INFO - training batch 701, loss: 0.199, 22432/56000 datapoints
2025-03-04 14:50:58,552 - INFO - training batch 751, loss: 0.035, 24032/56000 datapoints
2025-03-04 14:50:58,651 - INFO - training batch 801, loss: 0.055, 25632/56000 datapoints
2025-03-04 14:50:58,747 - INFO - training batch 851, loss: 0.023, 27232/56000 datapoints
2025-03-04 14:50:58,834 - INFO - training batch 901, loss: 0.041, 28832/56000 datapoints
2025-03-04 14:50:58,920 - INFO - training batch 951, loss: 0.044, 30432/56000 datapoints
2025-03-04 14:50:58,994 - INFO - training batch 1001, loss: 0.023, 32032/56000 datapoints
2025-03-04 14:50:59,080 - INFO - training batch 1051, loss: 0.033, 33632/56000 datapoints
2025-03-04 14:50:59,159 - INFO - training batch 1101, loss: 0.072, 35232/56000 datapoints
2025-03-04 14:50:59,249 - INFO - training batch 1151, loss: 0.008, 36832/56000 datapoints
2025-03-04 14:50:59,345 - INFO - training batch 1201, loss: 0.054, 38432/56000 datapoints
2025-03-04 14:50:59,440 - INFO - training batch 1251, loss: 0.195, 40032/56000 datapoints
2025-03-04 14:50:59,533 - INFO - training batch 1301, loss: 0.037, 41632/56000 datapoints
2025-03-04 14:50:59,609 - INFO - training batch 1351, loss: 0.018, 43232/56000 datapoints
2025-03-04 14:50:59,687 - INFO - training batch 1401, loss: 0.026, 44832/56000 datapoints
2025-03-04 14:50:59,765 - INFO - training batch 1451, loss: 0.021, 46432/56000 datapoints
2025-03-04 14:50:59,847 - INFO - training batch 1501, loss: 0.072, 48032/56000 datapoints
2025-03-04 14:50:59,933 - INFO - training batch 1551, loss: 0.024, 49632/56000 datapoints
2025-03-04 14:51:00,016 - INFO - training batch 1601, loss: 0.038, 51232/56000 datapoints
2025-03-04 14:51:00,699 - INFO - training batch 1651, loss: 0.034, 52832/56000 datapoints
2025-03-04 14:51:00,784 - INFO - training batch 1701, loss: 0.084, 54432/56000 datapoints
2025-03-04 14:51:00,876 - INFO - validation batch 1, loss: 0.023, 32/13984 datapoints
2025-03-04 14:51:00,914 - INFO - validation batch 51, loss: 0.028, 1632/13984 datapoints
2025-03-04 14:51:00,948 - INFO - validation batch 101, loss: 0.024, 3232/13984 datapoints
2025-03-04 14:51:00,985 - INFO - validation batch 151, loss: 0.021, 4832/13984 datapoints
2025-03-04 14:51:01,025 - INFO - validation batch 201, loss: 0.037, 6432/13984 datapoints
2025-03-04 14:51:01,057 - INFO - validation batch 251, loss: 0.034, 8032/13984 datapoints
2025-03-04 14:51:01,109 - INFO - validation batch 301, loss: 0.045, 9632/13984 datapoints
2025-03-04 14:51:01,149 - INFO - validation batch 351, loss: 0.025, 11232/13984 datapoints
2025-03-04 14:51:01,179 - INFO - validation batch 401, loss: 0.104, 12832/13984 datapoints
2025-03-04 14:51:01,205 - INFO - Epoch 7/50 done
2025-03-04 14:51:01,206 - INFO - Beginning epoch 8/50
2025-03-04 14:51:01,214 - INFO - training batch 1, loss: 0.137, 32/56000 datapoints
2025-03-04 14:51:01,320 - INFO - training batch 51, loss: 0.036, 1632/56000 datapoints
2025-03-04 14:51:01,432 - INFO - training batch 101, loss: 0.056, 3232/56000 datapoints
2025-03-04 14:51:01,540 - INFO - training batch 151, loss: 0.067, 4832/56000 datapoints
2025-03-04 14:51:01,639 - INFO - training batch 201, loss: 0.053, 6432/56000 datapoints
2025-03-04 14:51:01,739 - INFO - training batch 251, loss: 0.025, 8032/56000 datapoints
2025-03-04 14:51:01,835 - INFO - training batch 301, loss: 0.040, 9632/56000 datapoints
2025-03-04 14:51:01,942 - INFO - training batch 351, loss: 0.078, 11232/56000 datapoints
2025-03-04 14:51:02,039 - INFO - training batch 401, loss: 0.089, 12832/56000 datapoints
2025-03-04 14:51:02,150 - INFO - training batch 451, loss: 0.065, 14432/56000 datapoints
2025-03-04 14:51:02,284 - INFO - training batch 501, loss: 0.044, 16032/56000 datapoints
2025-03-04 14:51:02,426 - INFO - training batch 551, loss: 0.085, 17632/56000 datapoints
2025-03-04 14:51:02,590 - INFO - training batch 601, loss: 0.012, 19232/56000 datapoints
2025-03-04 14:51:02,713 - INFO - training batch 651, loss: 0.023, 20832/56000 datapoints
2025-03-04 14:51:02,864 - INFO - training batch 701, loss: 0.211, 22432/56000 datapoints
2025-03-04 14:51:02,998 - INFO - training batch 751, loss: 0.028, 24032/56000 datapoints
2025-03-04 14:51:03,081 - INFO - training batch 801, loss: 0.047, 25632/56000 datapoints
2025-03-04 14:51:03,171 - INFO - training batch 851, loss: 0.019, 27232/56000 datapoints
2025-03-04 14:51:03,262 - INFO - training batch 901, loss: 0.035, 28832/56000 datapoints
2025-03-04 14:51:03,349 - INFO - training batch 951, loss: 0.036, 30432/56000 datapoints
2025-03-04 14:51:03,431 - INFO - training batch 1001, loss: 0.018, 32032/56000 datapoints
2025-03-04 14:51:03,512 - INFO - training batch 1051, loss: 0.027, 33632/56000 datapoints
2025-03-04 14:51:03,585 - INFO - training batch 1101, loss: 0.071, 35232/56000 datapoints
2025-03-04 14:51:03,668 - INFO - training batch 1151, loss: 0.006, 36832/56000 datapoints
2025-03-04 14:51:03,750 - INFO - training batch 1201, loss: 0.048, 38432/56000 datapoints
2025-03-04 14:51:03,841 - INFO - training batch 1251, loss: 0.200, 40032/56000 datapoints
2025-03-04 14:51:03,957 - INFO - training batch 1301, loss: 0.031, 41632/56000 datapoints
2025-03-04 14:51:04,052 - INFO - training batch 1351, loss: 0.015, 43232/56000 datapoints
2025-03-04 14:51:04,181 - INFO - training batch 1401, loss: 0.021, 44832/56000 datapoints
2025-03-04 14:51:04,275 - INFO - training batch 1451, loss: 0.016, 46432/56000 datapoints
2025-03-04 14:51:04,361 - INFO - training batch 1501, loss: 0.063, 48032/56000 datapoints
2025-03-04 14:51:04,433 - INFO - training batch 1551, loss: 0.020, 49632/56000 datapoints
2025-03-04 14:51:04,504 - INFO - training batch 1601, loss: 0.031, 51232/56000 datapoints
2025-03-04 14:51:04,580 - INFO - training batch 1651, loss: 0.030, 52832/56000 datapoints
2025-03-04 14:51:04,668 - INFO - training batch 1701, loss: 0.084, 54432/56000 datapoints
2025-03-04 14:51:04,760 - INFO - validation batch 1, loss: 0.019, 32/13984 datapoints
2025-03-04 14:51:04,786 - INFO - validation batch 51, loss: 0.026, 1632/13984 datapoints
2025-03-04 14:51:04,813 - INFO - validation batch 101, loss: 0.019, 3232/13984 datapoints
2025-03-04 14:51:04,838 - INFO - validation batch 151, loss: 0.019, 4832/13984 datapoints
2025-03-04 14:51:04,867 - INFO - validation batch 201, loss: 0.031, 6432/13984 datapoints
2025-03-04 14:51:04,911 - INFO - validation batch 251, loss: 0.028, 8032/13984 datapoints
2025-03-04 14:51:04,943 - INFO - validation batch 301, loss: 0.036, 9632/13984 datapoints
2025-03-04 14:51:04,969 - INFO - validation batch 351, loss: 0.021, 11232/13984 datapoints
2025-03-04 14:51:04,995 - INFO - validation batch 401, loss: 0.104, 12832/13984 datapoints
2025-03-04 14:51:05,023 - INFO - Epoch 8/50 done
2025-03-04 14:51:05,023 - INFO - Beginning epoch 9/50
2025-03-04 14:51:05,026 - INFO - training batch 1, loss: 0.137, 32/56000 datapoints
2025-03-04 14:51:05,140 - INFO - training batch 51, loss: 0.029, 1632/56000 datapoints
2025-03-04 14:51:05,231 - INFO - training batch 101, loss: 0.056, 3232/56000 datapoints
2025-03-04 14:51:05,368 - INFO - training batch 151, loss: 0.066, 4832/56000 datapoints
2025-03-04 14:51:05,496 - INFO - training batch 201, loss: 0.051, 6432/56000 datapoints
2025-03-04 14:51:05,615 - INFO - training batch 251, loss: 0.021, 8032/56000 datapoints
2025-03-04 14:51:05,759 - INFO - training batch 301, loss: 0.035, 9632/56000 datapoints
2025-03-04 14:51:05,905 - INFO - training batch 351, loss: 0.076, 11232/56000 datapoints
2025-03-04 14:51:06,080 - INFO - training batch 401, loss: 0.085, 12832/56000 datapoints
2025-03-04 14:51:06,165 - INFO - training batch 451, loss: 0.058, 14432/56000 datapoints
2025-03-04 14:51:06,254 - INFO - training batch 501, loss: 0.036, 16032/56000 datapoints
2025-03-04 14:51:06,329 - INFO - training batch 551, loss: 0.083, 17632/56000 datapoints
2025-03-04 14:51:06,414 - INFO - training batch 601, loss: 0.009, 19232/56000 datapoints
2025-03-04 14:51:06,494 - INFO - training batch 651, loss: 0.020, 20832/56000 datapoints
2025-03-04 14:51:06,566 - INFO - training batch 701, loss: 0.220, 22432/56000 datapoints
2025-03-04 14:51:06,635 - INFO - training batch 751, loss: 0.024, 24032/56000 datapoints
2025-03-04 14:51:06,714 - INFO - training batch 801, loss: 0.042, 25632/56000 datapoints
2025-03-04 14:51:06,806 - INFO - training batch 851, loss: 0.015, 27232/56000 datapoints
2025-03-04 14:51:06,897 - INFO - training batch 901, loss: 0.030, 28832/56000 datapoints
2025-03-04 14:51:07,006 - INFO - training batch 951, loss: 0.030, 30432/56000 datapoints
2025-03-04 14:51:07,080 - INFO - training batch 1001, loss: 0.015, 32032/56000 datapoints
2025-03-04 14:51:07,159 - INFO - training batch 1051, loss: 0.023, 33632/56000 datapoints
2025-03-04 14:51:07,239 - INFO - training batch 1101, loss: 0.070, 35232/56000 datapoints
2025-03-04 14:51:07,313 - INFO - training batch 1151, loss: 0.004, 36832/56000 datapoints
2025-03-04 14:51:07,394 - INFO - training batch 1201, loss: 0.044, 38432/56000 datapoints
2025-03-04 14:51:07,479 - INFO - training batch 1251, loss: 0.205, 40032/56000 datapoints
2025-03-04 14:51:07,583 - INFO - training batch 1301, loss: 0.027, 41632/56000 datapoints
2025-03-04 14:51:07,677 - INFO - training batch 1351, loss: 0.013, 43232/56000 datapoints
2025-03-04 14:51:07,783 - INFO - training batch 1401, loss: 0.018, 44832/56000 datapoints
2025-03-04 14:51:07,887 - INFO - training batch 1451, loss: 0.013, 46432/56000 datapoints
2025-03-04 14:51:08,010 - INFO - training batch 1501, loss: 0.058, 48032/56000 datapoints
2025-03-04 14:51:08,087 - INFO - training batch 1551, loss: 0.017, 49632/56000 datapoints
2025-03-04 14:51:08,183 - INFO - training batch 1601, loss: 0.027, 51232/56000 datapoints
2025-03-04 14:51:08,309 - INFO - training batch 1651, loss: 0.027, 52832/56000 datapoints
2025-03-04 14:51:08,410 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-04 14:51:08,509 - INFO - validation batch 1, loss: 0.016, 32/13984 datapoints
2025-03-04 14:51:08,545 - INFO - validation batch 51, loss: 0.025, 1632/13984 datapoints
2025-03-04 14:51:08,573 - INFO - validation batch 101, loss: 0.016, 3232/13984 datapoints
2025-03-04 14:51:08,604 - INFO - validation batch 151, loss: 0.017, 4832/13984 datapoints
2025-03-04 14:51:08,635 - INFO - validation batch 201, loss: 0.026, 6432/13984 datapoints
2025-03-04 14:51:08,664 - INFO - validation batch 251, loss: 0.024, 8032/13984 datapoints
2025-03-04 14:51:08,695 - INFO - validation batch 301, loss: 0.030, 9632/13984 datapoints
2025-03-04 14:51:08,722 - INFO - validation batch 351, loss: 0.018, 11232/13984 datapoints
2025-03-04 14:51:08,755 - INFO - validation batch 401, loss: 0.105, 12832/13984 datapoints
2025-03-04 14:51:08,772 - INFO - Epoch 9/50 done
2025-03-04 14:51:08,772 - INFO - Beginning epoch 10/50
2025-03-04 14:51:08,776 - INFO - training batch 1, loss: 0.139, 32/56000 datapoints
2025-03-04 14:51:08,863 - INFO - training batch 51, loss: 0.024, 1632/56000 datapoints
2025-03-04 14:51:08,954 - INFO - training batch 101, loss: 0.056, 3232/56000 datapoints
2025-03-04 14:51:09,034 - INFO - training batch 151, loss: 0.065, 4832/56000 datapoints
2025-03-04 14:51:09,104 - INFO - training batch 201, loss: 0.048, 6432/56000 datapoints
2025-03-04 14:51:09,182 - INFO - training batch 251, loss: 0.018, 8032/56000 datapoints
2025-03-04 14:51:09,268 - INFO - training batch 301, loss: 0.032, 9632/56000 datapoints
2025-03-04 14:51:09,338 - INFO - training batch 351, loss: 0.074, 11232/56000 datapoints
2025-03-04 14:51:09,408 - INFO - training batch 401, loss: 0.083, 12832/56000 datapoints
2025-03-04 14:51:09,478 - INFO - training batch 451, loss: 0.053, 14432/56000 datapoints
2025-03-04 14:51:09,551 - INFO - training batch 501, loss: 0.031, 16032/56000 datapoints
2025-03-04 14:51:09,621 - INFO - training batch 551, loss: 0.083, 17632/56000 datapoints
2025-03-04 14:51:09,692 - INFO - training batch 601, loss: 0.008, 19232/56000 datapoints
2025-03-04 14:51:09,765 - INFO - training batch 651, loss: 0.018, 20832/56000 datapoints
2025-03-04 14:51:09,836 - INFO - training batch 701, loss: 0.228, 22432/56000 datapoints
2025-03-04 14:51:09,921 - INFO - training batch 751, loss: 0.020, 24032/56000 datapoints
2025-03-04 14:51:09,993 - INFO - training batch 801, loss: 0.038, 25632/56000 datapoints
2025-03-04 14:51:10,074 - INFO - training batch 851, loss: 0.013, 27232/56000 datapoints
2025-03-04 14:51:10,173 - INFO - training batch 901, loss: 0.027, 28832/56000 datapoints
2025-03-04 14:51:10,268 - INFO - training batch 951, loss: 0.026, 30432/56000 datapoints
2025-03-04 14:51:10,354 - INFO - training batch 1001, loss: 0.013, 32032/56000 datapoints
2025-03-04 14:51:10,432 - INFO - training batch 1051, loss: 0.021, 33632/56000 datapoints
2025-03-04 14:51:10,508 - INFO - training batch 1101, loss: 0.070, 35232/56000 datapoints
2025-03-04 14:51:10,587 - INFO - training batch 1151, loss: 0.004, 36832/56000 datapoints
2025-03-04 14:51:10,663 - INFO - training batch 1201, loss: 0.040, 38432/56000 datapoints
2025-03-04 14:51:10,735 - INFO - training batch 1251, loss: 0.209, 40032/56000 datapoints
2025-03-04 14:51:10,812 - INFO - training batch 1301, loss: 0.025, 41632/56000 datapoints
2025-03-04 14:51:10,884 - INFO - training batch 1351, loss: 0.011, 43232/56000 datapoints
2025-03-04 14:51:11,450 - INFO - training batch 1401, loss: 0.016, 44832/56000 datapoints
2025-03-04 14:51:11,936 - INFO - training batch 1451, loss: 0.011, 46432/56000 datapoints
2025-03-04 14:51:12,019 - INFO - training batch 1501, loss: 0.053, 48032/56000 datapoints
2025-03-04 14:51:12,098 - INFO - training batch 1551, loss: 0.015, 49632/56000 datapoints
2025-03-04 14:51:12,176 - INFO - training batch 1601, loss: 0.023, 51232/56000 datapoints
2025-03-04 14:51:12,267 - INFO - training batch 1651, loss: 0.025, 52832/56000 datapoints
2025-03-04 14:51:12,342 - INFO - training batch 1701, loss: 0.086, 54432/56000 datapoints
2025-03-04 14:51:12,418 - INFO - validation batch 1, loss: 0.014, 32/13984 datapoints
2025-03-04 14:51:12,442 - INFO - validation batch 51, loss: 0.024, 1632/13984 datapoints
2025-03-04 14:51:12,469 - INFO - validation batch 101, loss: 0.013, 3232/13984 datapoints
2025-03-04 14:51:12,499 - INFO - validation batch 151, loss: 0.015, 4832/13984 datapoints
2025-03-04 14:51:12,525 - INFO - validation batch 201, loss: 0.023, 6432/13984 datapoints
2025-03-04 14:51:12,550 - INFO - validation batch 251, loss: 0.021, 8032/13984 datapoints
2025-03-04 14:51:12,577 - INFO - validation batch 301, loss: 0.026, 9632/13984 datapoints
2025-03-04 14:51:12,609 - INFO - validation batch 351, loss: 0.016, 11232/13984 datapoints
2025-03-04 14:51:12,635 - INFO - validation batch 401, loss: 0.105, 12832/13984 datapoints
2025-03-04 14:51:12,665 - INFO - Epoch 10/50 done
2025-03-04 14:51:12,666 - INFO - Beginning epoch 11/50
2025-03-04 14:51:12,669 - INFO - training batch 1, loss: 0.140, 32/56000 datapoints
2025-03-04 14:51:12,807 - INFO - training batch 51, loss: 0.021, 1632/56000 datapoints
2025-03-04 14:51:12,912 - INFO - training batch 101, loss: 0.057, 3232/56000 datapoints
2025-03-04 14:51:12,996 - INFO - training batch 151, loss: 0.065, 4832/56000 datapoints
2025-03-04 14:51:13,071 - INFO - training batch 201, loss: 0.047, 6432/56000 datapoints
2025-03-04 14:51:13,147 - INFO - training batch 251, loss: 0.016, 8032/56000 datapoints
2025-03-04 14:51:13,237 - INFO - training batch 301, loss: 0.029, 9632/56000 datapoints
2025-03-04 14:51:13,324 - INFO - training batch 351, loss: 0.074, 11232/56000 datapoints
2025-03-04 14:51:13,402 - INFO - training batch 401, loss: 0.081, 12832/56000 datapoints
2025-03-04 14:51:13,481 - INFO - training batch 451, loss: 0.049, 14432/56000 datapoints
2025-03-04 14:51:13,575 - INFO - training batch 501, loss: 0.027, 16032/56000 datapoints
2025-03-04 14:51:13,683 - INFO - training batch 551, loss: 0.082, 17632/56000 datapoints
2025-03-04 14:51:13,774 - INFO - training batch 601, loss: 0.006, 19232/56000 datapoints
2025-03-04 14:51:13,859 - INFO - training batch 651, loss: 0.016, 20832/56000 datapoints
2025-03-04 14:51:13,961 - INFO - training batch 701, loss: 0.234, 22432/56000 datapoints
2025-03-04 14:51:14,037 - INFO - training batch 751, loss: 0.018, 24032/56000 datapoints
2025-03-04 14:51:14,152 - INFO - training batch 801, loss: 0.035, 25632/56000 datapoints
2025-03-04 14:51:14,239 - INFO - training batch 851, loss: 0.011, 27232/56000 datapoints
2025-03-04 14:51:14,340 - INFO - training batch 901, loss: 0.024, 28832/56000 datapoints
2025-03-04 14:51:14,473 - INFO - training batch 951, loss: 0.023, 30432/56000 datapoints
2025-03-04 14:51:14,569 - INFO - training batch 1001, loss: 0.012, 32032/56000 datapoints
2025-03-04 14:51:14,660 - INFO - training batch 1051, loss: 0.019, 33632/56000 datapoints
2025-03-04 14:51:14,736 - INFO - training batch 1101, loss: 0.071, 35232/56000 datapoints
2025-03-04 14:51:14,827 - INFO - training batch 1151, loss: 0.003, 36832/56000 datapoints
2025-03-04 14:51:14,925 - INFO - training batch 1201, loss: 0.037, 38432/56000 datapoints
2025-03-04 14:51:15,004 - INFO - training batch 1251, loss: 0.211, 40032/56000 datapoints
2025-03-04 14:51:15,082 - INFO - training batch 1301, loss: 0.023, 41632/56000 datapoints
2025-03-04 14:51:15,157 - INFO - training batch 1351, loss: 0.010, 43232/56000 datapoints
2025-03-04 14:51:15,231 - INFO - training batch 1401, loss: 0.014, 44832/56000 datapoints
2025-03-04 14:51:15,327 - INFO - training batch 1451, loss: 0.009, 46432/56000 datapoints
2025-03-04 14:51:15,422 - INFO - training batch 1501, loss: 0.050, 48032/56000 datapoints
2025-03-04 14:51:15,510 - INFO - training batch 1551, loss: 0.014, 49632/56000 datapoints
2025-03-04 14:51:15,599 - INFO - training batch 1601, loss: 0.021, 51232/56000 datapoints
2025-03-04 14:51:15,684 - INFO - training batch 1651, loss: 0.024, 52832/56000 datapoints
2025-03-04 14:51:15,771 - INFO - training batch 1701, loss: 0.087, 54432/56000 datapoints
2025-03-04 14:51:15,862 - INFO - validation batch 1, loss: 0.013, 32/13984 datapoints
2025-03-04 14:51:15,894 - INFO - validation batch 51, loss: 0.023, 1632/13984 datapoints
2025-03-04 14:51:15,933 - INFO - validation batch 101, loss: 0.011, 3232/13984 datapoints
2025-03-04 14:51:15,963 - INFO - validation batch 151, loss: 0.014, 4832/13984 datapoints
2025-03-04 14:51:16,003 - INFO - validation batch 201, loss: 0.020, 6432/13984 datapoints
2025-03-04 14:51:16,036 - INFO - validation batch 251, loss: 0.018, 8032/13984 datapoints
2025-03-04 14:51:16,075 - INFO - validation batch 301, loss: 0.023, 9632/13984 datapoints
2025-03-04 14:51:16,117 - INFO - validation batch 351, loss: 0.015, 11232/13984 datapoints
2025-03-04 14:51:16,144 - INFO - validation batch 401, loss: 0.106, 12832/13984 datapoints
2025-03-04 14:51:16,167 - INFO - Epoch 11/50 done
2025-03-04 14:51:16,167 - INFO - Beginning epoch 12/50
2025-03-04 14:51:16,170 - INFO - training batch 1, loss: 0.142, 32/56000 datapoints
2025-03-04 14:51:16,263 - INFO - training batch 51, loss: 0.019, 1632/56000 datapoints
2025-03-04 14:51:16,356 - INFO - training batch 101, loss: 0.058, 3232/56000 datapoints
2025-03-04 14:51:16,445 - INFO - training batch 151, loss: 0.064, 4832/56000 datapoints
2025-03-04 14:51:16,533 - INFO - training batch 201, loss: 0.045, 6432/56000 datapoints
2025-03-04 14:51:16,624 - INFO - training batch 251, loss: 0.015, 8032/56000 datapoints
2025-03-04 14:51:16,713 - INFO - training batch 301, loss: 0.027, 9632/56000 datapoints
2025-03-04 14:51:16,807 - INFO - training batch 351, loss: 0.073, 11232/56000 datapoints
2025-03-04 14:51:16,923 - INFO - training batch 401, loss: 0.080, 12832/56000 datapoints
2025-03-04 14:51:17,022 - INFO - training batch 451, loss: 0.046, 14432/56000 datapoints
2025-03-04 14:51:17,100 - INFO - training batch 501, loss: 0.024, 16032/56000 datapoints
2025-03-04 14:51:17,177 - INFO - training batch 551, loss: 0.082, 17632/56000 datapoints
2025-03-04 14:51:17,260 - INFO - training batch 601, loss: 0.006, 19232/56000 datapoints
2025-03-04 14:51:17,330 - INFO - training batch 651, loss: 0.015, 20832/56000 datapoints
2025-03-04 14:51:17,403 - INFO - training batch 701, loss: 0.239, 22432/56000 datapoints
2025-03-04 14:51:17,484 - INFO - training batch 751, loss: 0.016, 24032/56000 datapoints
2025-03-04 14:51:17,561 - INFO - training batch 801, loss: 0.032, 25632/56000 datapoints
2025-03-04 14:51:17,631 - INFO - training batch 851, loss: 0.010, 27232/56000 datapoints
2025-03-04 14:51:17,708 - INFO - training batch 901, loss: 0.022, 28832/56000 datapoints
2025-03-04 14:51:17,777 - INFO - training batch 951, loss: 0.021, 30432/56000 datapoints
2025-03-04 14:51:17,866 - INFO - training batch 1001, loss: 0.010, 32032/56000 datapoints
2025-03-04 14:51:17,997 - INFO - training batch 1051, loss: 0.017, 33632/56000 datapoints
2025-03-04 14:51:18,119 - INFO - training batch 1101, loss: 0.072, 35232/56000 datapoints
2025-03-04 14:51:18,315 - INFO - training batch 1151, loss: 0.003, 36832/56000 datapoints
2025-03-04 14:51:18,473 - INFO - training batch 1201, loss: 0.034, 38432/56000 datapoints
2025-03-04 14:51:18,703 - INFO - training batch 1251, loss: 0.213, 40032/56000 datapoints
2025-03-04 14:51:18,816 - INFO - training batch 1301, loss: 0.022, 41632/56000 datapoints
2025-03-04 14:51:18,999 - INFO - training batch 1351, loss: 0.008, 43232/56000 datapoints
2025-03-04 14:51:19,151 - INFO - training batch 1401, loss: 0.012, 44832/56000 datapoints
2025-03-04 14:51:19,297 - INFO - training batch 1451, loss: 0.008, 46432/56000 datapoints
2025-03-04 14:51:19,411 - INFO - training batch 1501, loss: 0.048, 48032/56000 datapoints
2025-03-04 14:51:19,531 - INFO - training batch 1551, loss: 0.012, 49632/56000 datapoints
2025-03-04 14:51:19,628 - INFO - training batch 1601, loss: 0.019, 51232/56000 datapoints
2025-03-04 14:51:19,724 - INFO - training batch 1651, loss: 0.023, 52832/56000 datapoints
2025-03-04 14:51:19,822 - INFO - training batch 1701, loss: 0.089, 54432/56000 datapoints
2025-03-04 14:51:19,931 - INFO - validation batch 1, loss: 0.012, 32/13984 datapoints
2025-03-04 14:51:19,992 - INFO - validation batch 51, loss: 0.023, 1632/13984 datapoints
2025-03-04 14:51:20,037 - INFO - validation batch 101, loss: 0.010, 3232/13984 datapoints
2025-03-04 14:51:20,083 - INFO - validation batch 151, loss: 0.014, 4832/13984 datapoints
2025-03-04 14:51:20,154 - INFO - validation batch 201, loss: 0.018, 6432/13984 datapoints
2025-03-04 14:51:20,205 - INFO - validation batch 251, loss: 0.016, 8032/13984 datapoints
2025-03-04 14:51:20,326 - INFO - validation batch 301, loss: 0.021, 9632/13984 datapoints
2025-03-04 14:51:20,366 - INFO - validation batch 351, loss: 0.013, 11232/13984 datapoints
2025-03-04 14:51:20,439 - INFO - validation batch 401, loss: 0.107, 12832/13984 datapoints
2025-03-04 14:51:20,516 - INFO - Epoch 12/50 done
2025-03-04 14:51:20,519 - INFO - Beginning epoch 13/50
2025-03-04 14:51:20,545 - INFO - training batch 1, loss: 0.143, 32/56000 datapoints
2025-03-04 14:51:20,731 - INFO - training batch 51, loss: 0.017, 1632/56000 datapoints
2025-03-04 14:51:20,909 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:51:21,046 - INFO - training batch 151, loss: 0.064, 4832/56000 datapoints
2025-03-04 14:51:21,226 - INFO - training batch 201, loss: 0.044, 6432/56000 datapoints
2025-03-04 14:51:21,508 - INFO - training batch 251, loss: 0.014, 8032/56000 datapoints
2025-03-04 14:51:21,730 - INFO - training batch 301, loss: 0.025, 9632/56000 datapoints
2025-03-04 14:51:22,039 - INFO - training batch 351, loss: 0.073, 11232/56000 datapoints
2025-03-04 14:51:22,359 - INFO - training batch 401, loss: 0.079, 12832/56000 datapoints
2025-03-04 14:51:22,519 - INFO - training batch 451, loss: 0.044, 14432/56000 datapoints
2025-03-04 14:51:22,674 - INFO - training batch 501, loss: 0.021, 16032/56000 datapoints
2025-03-04 14:51:22,804 - INFO - training batch 551, loss: 0.081, 17632/56000 datapoints
2025-03-04 14:51:22,994 - INFO - training batch 601, loss: 0.005, 19232/56000 datapoints
2025-03-04 14:51:23,182 - INFO - training batch 651, loss: 0.014, 20832/56000 datapoints
2025-03-04 14:51:23,323 - INFO - training batch 701, loss: 0.243, 22432/56000 datapoints
2025-03-04 14:51:23,461 - INFO - training batch 751, loss: 0.015, 24032/56000 datapoints
2025-03-04 14:51:23,607 - INFO - training batch 801, loss: 0.030, 25632/56000 datapoints
2025-03-04 14:51:23,977 - INFO - training batch 851, loss: 0.009, 27232/56000 datapoints
2025-03-04 14:51:24,190 - INFO - training batch 901, loss: 0.021, 28832/56000 datapoints
2025-03-04 14:51:24,307 - INFO - training batch 951, loss: 0.019, 30432/56000 datapoints
2025-03-04 14:51:24,418 - INFO - training batch 1001, loss: 0.009, 32032/56000 datapoints
2025-03-04 14:51:24,525 - INFO - training batch 1051, loss: 0.016, 33632/56000 datapoints
2025-03-04 14:51:24,611 - INFO - training batch 1101, loss: 0.072, 35232/56000 datapoints
2025-03-04 14:51:24,698 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-04 14:51:24,773 - INFO - training batch 1201, loss: 0.032, 38432/56000 datapoints
2025-03-04 14:51:24,853 - INFO - training batch 1251, loss: 0.215, 40032/56000 datapoints
2025-03-04 14:51:24,951 - INFO - training batch 1301, loss: 0.021, 41632/56000 datapoints
2025-03-04 14:51:25,032 - INFO - training batch 1351, loss: 0.008, 43232/56000 datapoints
2025-03-04 14:51:25,111 - INFO - training batch 1401, loss: 0.011, 44832/56000 datapoints
2025-03-04 14:51:25,216 - INFO - training batch 1451, loss: 0.007, 46432/56000 datapoints
2025-03-04 14:51:25,333 - INFO - training batch 1501, loss: 0.046, 48032/56000 datapoints
2025-03-04 14:51:25,436 - INFO - training batch 1551, loss: 0.011, 49632/56000 datapoints
2025-03-04 14:51:25,517 - INFO - training batch 1601, loss: 0.017, 51232/56000 datapoints
2025-03-04 14:51:25,594 - INFO - training batch 1651, loss: 0.022, 52832/56000 datapoints
2025-03-04 14:51:25,669 - INFO - training batch 1701, loss: 0.089, 54432/56000 datapoints
2025-03-04 14:51:25,745 - INFO - validation batch 1, loss: 0.011, 32/13984 datapoints
2025-03-04 14:51:25,767 - INFO - validation batch 51, loss: 0.022, 1632/13984 datapoints
2025-03-04 14:51:25,796 - INFO - validation batch 101, loss: 0.009, 3232/13984 datapoints
2025-03-04 14:51:25,822 - INFO - validation batch 151, loss: 0.013, 4832/13984 datapoints
2025-03-04 14:51:25,847 - INFO - validation batch 201, loss: 0.017, 6432/13984 datapoints
2025-03-04 14:51:25,878 - INFO - validation batch 251, loss: 0.015, 8032/13984 datapoints
2025-03-04 14:51:25,905 - INFO - validation batch 301, loss: 0.019, 9632/13984 datapoints
2025-03-04 14:51:25,932 - INFO - validation batch 351, loss: 0.012, 11232/13984 datapoints
2025-03-04 14:51:25,958 - INFO - validation batch 401, loss: 0.107, 12832/13984 datapoints
2025-03-04 14:51:25,986 - INFO - Epoch 13/50 done
2025-03-04 14:51:25,988 - INFO - Beginning epoch 14/50
2025-03-04 14:51:25,991 - INFO - training batch 1, loss: 0.144, 32/56000 datapoints
2025-03-04 14:51:26,100 - INFO - training batch 51, loss: 0.015, 1632/56000 datapoints
2025-03-04 14:51:26,205 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-04 14:51:26,307 - INFO - training batch 151, loss: 0.064, 4832/56000 datapoints
2025-03-04 14:51:26,412 - INFO - training batch 201, loss: 0.042, 6432/56000 datapoints
2025-03-04 14:51:26,512 - INFO - training batch 251, loss: 0.013, 8032/56000 datapoints
2025-03-04 14:51:26,613 - INFO - training batch 301, loss: 0.024, 9632/56000 datapoints
2025-03-04 14:51:26,707 - INFO - training batch 351, loss: 0.072, 11232/56000 datapoints
2025-03-04 14:51:26,807 - INFO - training batch 401, loss: 0.078, 12832/56000 datapoints
2025-03-04 14:51:26,905 - INFO - training batch 451, loss: 0.042, 14432/56000 datapoints
2025-03-04 14:51:26,992 - INFO - training batch 501, loss: 0.019, 16032/56000 datapoints
2025-03-04 14:51:27,072 - INFO - training batch 551, loss: 0.081, 17632/56000 datapoints
2025-03-04 14:51:27,155 - INFO - training batch 601, loss: 0.004, 19232/56000 datapoints
2025-03-04 14:51:27,245 - INFO - training batch 651, loss: 0.014, 20832/56000 datapoints
2025-03-04 14:51:27,323 - INFO - training batch 701, loss: 0.246, 22432/56000 datapoints
2025-03-04 14:51:27,401 - INFO - training batch 751, loss: 0.014, 24032/56000 datapoints
2025-03-04 14:51:27,476 - INFO - training batch 801, loss: 0.028, 25632/56000 datapoints
2025-03-04 14:51:27,553 - INFO - training batch 851, loss: 0.008, 27232/56000 datapoints
2025-03-04 14:51:27,638 - INFO - training batch 901, loss: 0.019, 28832/56000 datapoints
2025-03-04 14:51:27,712 - INFO - training batch 951, loss: 0.018, 30432/56000 datapoints
2025-03-04 14:51:27,788 - INFO - training batch 1001, loss: 0.009, 32032/56000 datapoints
2025-03-04 14:51:27,861 - INFO - training batch 1051, loss: 0.014, 33632/56000 datapoints
2025-03-04 14:51:27,965 - INFO - training batch 1101, loss: 0.073, 35232/56000 datapoints
2025-03-04 14:51:28,040 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-04 14:51:28,117 - INFO - training batch 1201, loss: 0.030, 38432/56000 datapoints
2025-03-04 14:51:28,201 - INFO - training batch 1251, loss: 0.215, 40032/56000 datapoints
2025-03-04 14:51:28,277 - INFO - training batch 1301, loss: 0.020, 41632/56000 datapoints
2025-03-04 14:51:28,353 - INFO - training batch 1351, loss: 0.007, 43232/56000 datapoints
2025-03-04 14:51:28,430 - INFO - training batch 1401, loss: 0.010, 44832/56000 datapoints
2025-03-04 14:51:28,504 - INFO - training batch 1451, loss: 0.007, 46432/56000 datapoints
2025-03-04 14:51:28,577 - INFO - training batch 1501, loss: 0.044, 48032/56000 datapoints
2025-03-04 14:51:28,650 - INFO - training batch 1551, loss: 0.011, 49632/56000 datapoints
2025-03-04 14:51:28,756 - INFO - training batch 1601, loss: 0.016, 51232/56000 datapoints
2025-03-04 14:51:28,858 - INFO - training batch 1651, loss: 0.022, 52832/56000 datapoints
2025-03-04 14:51:28,943 - INFO - training batch 1701, loss: 0.090, 54432/56000 datapoints
2025-03-04 14:51:29,028 - INFO - validation batch 1, loss: 0.010, 32/13984 datapoints
2025-03-04 14:51:29,054 - INFO - validation batch 51, loss: 0.022, 1632/13984 datapoints
2025-03-04 14:51:29,077 - INFO - validation batch 101, loss: 0.008, 3232/13984 datapoints
2025-03-04 14:51:29,104 - INFO - validation batch 151, loss: 0.012, 4832/13984 datapoints
2025-03-04 14:51:29,129 - INFO - validation batch 201, loss: 0.016, 6432/13984 datapoints
2025-03-04 14:51:29,162 - INFO - validation batch 251, loss: 0.013, 8032/13984 datapoints
2025-03-04 14:51:29,193 - INFO - validation batch 301, loss: 0.018, 9632/13984 datapoints
2025-03-04 14:51:29,221 - INFO - validation batch 351, loss: 0.011, 11232/13984 datapoints
2025-03-04 14:51:29,246 - INFO - validation batch 401, loss: 0.108, 12832/13984 datapoints
2025-03-04 14:51:29,270 - INFO - Epoch 14/50 done
2025-03-04 14:51:29,270 - INFO - Beginning epoch 15/50
2025-03-04 14:51:29,274 - INFO - training batch 1, loss: 0.146, 32/56000 datapoints
2025-03-04 14:51:29,366 - INFO - training batch 51, loss: 0.014, 1632/56000 datapoints
2025-03-04 14:51:29,457 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-04 14:51:29,546 - INFO - training batch 151, loss: 0.063, 4832/56000 datapoints
2025-03-04 14:51:29,649 - INFO - training batch 201, loss: 0.041, 6432/56000 datapoints
2025-03-04 14:51:29,800 - INFO - training batch 251, loss: 0.013, 8032/56000 datapoints
2025-03-04 14:51:30,178 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-04 14:51:30,834 - INFO - training batch 351, loss: 0.072, 11232/56000 datapoints
2025-03-04 14:51:31,431 - INFO - training batch 401, loss: 0.077, 12832/56000 datapoints
2025-03-04 14:51:31,804 - INFO - training batch 451, loss: 0.040, 14432/56000 datapoints
2025-03-04 14:51:32,106 - INFO - training batch 501, loss: 0.017, 16032/56000 datapoints
2025-03-04 14:51:32,246 - INFO - training batch 551, loss: 0.080, 17632/56000 datapoints
2025-03-04 14:51:32,368 - INFO - training batch 601, loss: 0.004, 19232/56000 datapoints
2025-03-04 14:51:32,515 - INFO - training batch 651, loss: 0.013, 20832/56000 datapoints
2025-03-04 14:51:32,630 - INFO - training batch 701, loss: 0.249, 22432/56000 datapoints
2025-03-04 14:51:33,288 - INFO - training batch 751, loss: 0.013, 24032/56000 datapoints
2025-03-04 14:51:33,600 - INFO - training batch 801, loss: 0.027, 25632/56000 datapoints
2025-03-04 14:51:33,755 - INFO - training batch 851, loss: 0.008, 27232/56000 datapoints
2025-03-04 14:51:33,928 - INFO - training batch 901, loss: 0.018, 28832/56000 datapoints
2025-03-04 14:51:34,073 - INFO - training batch 951, loss: 0.017, 30432/56000 datapoints
2025-03-04 14:51:34,286 - INFO - training batch 1001, loss: 0.008, 32032/56000 datapoints
2025-03-04 14:51:34,716 - INFO - training batch 1051, loss: 0.014, 33632/56000 datapoints
2025-03-04 14:51:34,838 - INFO - training batch 1101, loss: 0.073, 35232/56000 datapoints
2025-03-04 14:51:34,983 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-04 14:51:35,118 - INFO - training batch 1201, loss: 0.029, 38432/56000 datapoints
2025-03-04 14:51:35,239 - INFO - training batch 1251, loss: 0.216, 40032/56000 datapoints
2025-03-04 14:51:35,399 - INFO - training batch 1301, loss: 0.020, 41632/56000 datapoints
2025-03-04 14:51:35,567 - INFO - training batch 1351, loss: 0.006, 43232/56000 datapoints
2025-03-04 14:51:35,710 - INFO - training batch 1401, loss: 0.009, 44832/56000 datapoints
2025-03-04 14:51:35,817 - INFO - training batch 1451, loss: 0.006, 46432/56000 datapoints
2025-03-04 14:51:35,902 - INFO - training batch 1501, loss: 0.042, 48032/56000 datapoints
2025-03-04 14:51:35,989 - INFO - training batch 1551, loss: 0.010, 49632/56000 datapoints
2025-03-04 14:51:36,074 - INFO - training batch 1601, loss: 0.015, 51232/56000 datapoints
2025-03-04 14:51:36,161 - INFO - training batch 1651, loss: 0.021, 52832/56000 datapoints
2025-03-04 14:51:36,255 - INFO - training batch 1701, loss: 0.091, 54432/56000 datapoints
2025-03-04 14:51:36,348 - INFO - validation batch 1, loss: 0.009, 32/13984 datapoints
2025-03-04 14:51:36,384 - INFO - validation batch 51, loss: 0.022, 1632/13984 datapoints
2025-03-04 14:51:36,416 - INFO - validation batch 101, loss: 0.008, 3232/13984 datapoints
2025-03-04 14:51:36,453 - INFO - validation batch 151, loss: 0.012, 4832/13984 datapoints
2025-03-04 14:51:36,496 - INFO - validation batch 201, loss: 0.015, 6432/13984 datapoints
2025-03-04 14:51:36,534 - INFO - validation batch 251, loss: 0.012, 8032/13984 datapoints
2025-03-04 14:51:36,578 - INFO - validation batch 301, loss: 0.016, 9632/13984 datapoints
2025-03-04 14:51:36,611 - INFO - validation batch 351, loss: 0.011, 11232/13984 datapoints
2025-03-04 14:51:36,640 - INFO - validation batch 401, loss: 0.108, 12832/13984 datapoints
2025-03-04 14:51:36,660 - INFO - Epoch 15/50 done
2025-03-04 14:51:36,660 - INFO - Beginning epoch 16/50
2025-03-04 14:51:36,663 - INFO - training batch 1, loss: 0.147, 32/56000 datapoints
2025-03-04 14:51:36,752 - INFO - training batch 51, loss: 0.013, 1632/56000 datapoints
2025-03-04 14:51:36,839 - INFO - training batch 101, loss: 0.061, 3232/56000 datapoints
2025-03-04 14:51:36,938 - INFO - training batch 151, loss: 0.063, 4832/56000 datapoints
2025-03-04 14:51:37,026 - INFO - training batch 201, loss: 0.040, 6432/56000 datapoints
2025-03-04 14:51:37,103 - INFO - training batch 251, loss: 0.012, 8032/56000 datapoints
2025-03-04 14:51:37,182 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-04 14:51:37,257 - INFO - training batch 351, loss: 0.072, 11232/56000 datapoints
2025-03-04 14:51:37,342 - INFO - training batch 401, loss: 0.076, 12832/56000 datapoints
2025-03-04 14:51:37,426 - INFO - training batch 451, loss: 0.039, 14432/56000 datapoints
2025-03-04 14:51:37,506 - INFO - training batch 501, loss: 0.016, 16032/56000 datapoints
2025-03-04 14:51:37,588 - INFO - training batch 551, loss: 0.080, 17632/56000 datapoints
2025-03-04 14:51:37,677 - INFO - training batch 601, loss: 0.004, 19232/56000 datapoints
2025-03-04 14:51:37,765 - INFO - training batch 651, loss: 0.013, 20832/56000 datapoints
2025-03-04 14:51:37,838 - INFO - training batch 701, loss: 0.251, 22432/56000 datapoints
2025-03-04 14:51:37,912 - INFO - training batch 751, loss: 0.012, 24032/56000 datapoints
2025-03-04 14:51:37,991 - INFO - training batch 801, loss: 0.026, 25632/56000 datapoints
2025-03-04 14:51:38,074 - INFO - training batch 851, loss: 0.007, 27232/56000 datapoints
2025-03-04 14:51:38,152 - INFO - training batch 901, loss: 0.017, 28832/56000 datapoints
2025-03-04 14:51:38,223 - INFO - training batch 951, loss: 0.016, 30432/56000 datapoints
2025-03-04 14:51:38,302 - INFO - training batch 1001, loss: 0.008, 32032/56000 datapoints
2025-03-04 14:51:38,382 - INFO - training batch 1051, loss: 0.013, 33632/56000 datapoints
2025-03-04 14:51:38,457 - INFO - training batch 1101, loss: 0.074, 35232/56000 datapoints
2025-03-04 14:51:38,549 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-04 14:51:38,631 - INFO - training batch 1201, loss: 0.027, 38432/56000 datapoints
2025-03-04 14:51:38,724 - INFO - training batch 1251, loss: 0.216, 40032/56000 datapoints
2025-03-04 14:51:38,819 - INFO - training batch 1301, loss: 0.019, 41632/56000 datapoints
2025-03-04 14:51:38,932 - INFO - training batch 1351, loss: 0.006, 43232/56000 datapoints
2025-03-04 14:51:39,036 - INFO - training batch 1401, loss: 0.009, 44832/56000 datapoints
2025-03-04 14:51:39,134 - INFO - training batch 1451, loss: 0.006, 46432/56000 datapoints
2025-03-04 14:51:39,229 - INFO - training batch 1501, loss: 0.041, 48032/56000 datapoints
2025-03-04 14:51:39,334 - INFO - training batch 1551, loss: 0.009, 49632/56000 datapoints
2025-03-04 14:51:39,416 - INFO - training batch 1601, loss: 0.014, 51232/56000 datapoints
2025-03-04 14:51:39,487 - INFO - training batch 1651, loss: 0.021, 52832/56000 datapoints
2025-03-04 14:51:39,558 - INFO - training batch 1701, loss: 0.092, 54432/56000 datapoints
2025-03-04 14:51:39,625 - INFO - validation batch 1, loss: 0.009, 32/13984 datapoints
2025-03-04 14:51:39,646 - INFO - validation batch 51, loss: 0.021, 1632/13984 datapoints
2025-03-04 14:51:39,671 - INFO - validation batch 101, loss: 0.007, 3232/13984 datapoints
2025-03-04 14:51:39,692 - INFO - validation batch 151, loss: 0.011, 4832/13984 datapoints
2025-03-04 14:51:39,721 - INFO - validation batch 201, loss: 0.014, 6432/13984 datapoints
2025-03-04 14:51:39,747 - INFO - validation batch 251, loss: 0.011, 8032/13984 datapoints
2025-03-04 14:51:39,781 - INFO - validation batch 301, loss: 0.015, 9632/13984 datapoints
2025-03-04 14:51:39,808 - INFO - validation batch 351, loss: 0.010, 11232/13984 datapoints
2025-03-04 14:51:39,863 - INFO - validation batch 401, loss: 0.109, 12832/13984 datapoints
2025-03-04 14:51:39,880 - INFO - Epoch 16/50 done
2025-03-04 14:51:39,881 - INFO - Beginning epoch 17/50
2025-03-04 14:51:39,883 - INFO - training batch 1, loss: 0.148, 32/56000 datapoints
2025-03-04 14:51:39,994 - INFO - training batch 51, loss: 0.012, 1632/56000 datapoints
2025-03-04 14:51:40,090 - INFO - training batch 101, loss: 0.061, 3232/56000 datapoints
2025-03-04 14:51:40,175 - INFO - training batch 151, loss: 0.063, 4832/56000 datapoints
2025-03-04 14:51:40,258 - INFO - training batch 201, loss: 0.039, 6432/56000 datapoints
2025-03-04 14:51:40,345 - INFO - training batch 251, loss: 0.012, 8032/56000 datapoints
2025-03-04 14:51:40,445 - INFO - training batch 301, loss: 0.022, 9632/56000 datapoints
2025-03-04 14:51:40,535 - INFO - training batch 351, loss: 0.072, 11232/56000 datapoints
2025-03-04 14:51:40,622 - INFO - training batch 401, loss: 0.075, 12832/56000 datapoints
2025-03-04 14:51:40,714 - INFO - training batch 451, loss: 0.037, 14432/56000 datapoints
2025-03-04 14:51:40,803 - INFO - training batch 501, loss: 0.015, 16032/56000 datapoints
2025-03-04 14:51:40,921 - INFO - training batch 551, loss: 0.079, 17632/56000 datapoints
2025-03-04 14:51:41,073 - INFO - training batch 601, loss: 0.003, 19232/56000 datapoints
2025-03-04 14:51:41,177 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:51:41,294 - INFO - training batch 701, loss: 0.253, 22432/56000 datapoints
2025-03-04 14:51:41,388 - INFO - training batch 751, loss: 0.011, 24032/56000 datapoints
2025-03-04 14:51:41,490 - INFO - training batch 801, loss: 0.025, 25632/56000 datapoints
2025-03-04 14:51:41,677 - INFO - training batch 851, loss: 0.007, 27232/56000 datapoints
2025-03-04 14:51:41,815 - INFO - training batch 901, loss: 0.017, 28832/56000 datapoints
2025-03-04 14:51:42,007 - INFO - training batch 951, loss: 0.015, 30432/56000 datapoints
2025-03-04 14:51:42,174 - INFO - training batch 1001, loss: 0.007, 32032/56000 datapoints
2025-03-04 14:51:42,335 - INFO - training batch 1051, loss: 0.012, 33632/56000 datapoints
2025-03-04 14:51:42,547 - INFO - training batch 1101, loss: 0.075, 35232/56000 datapoints
2025-03-04 14:51:42,929 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-04 14:51:43,061 - INFO - training batch 1201, loss: 0.026, 38432/56000 datapoints
2025-03-04 14:51:43,198 - INFO - training batch 1251, loss: 0.215, 40032/56000 datapoints
2025-03-04 14:51:43,362 - INFO - training batch 1301, loss: 0.019, 41632/56000 datapoints
2025-03-04 14:51:43,466 - INFO - training batch 1351, loss: 0.005, 43232/56000 datapoints
2025-03-04 14:51:43,579 - INFO - training batch 1401, loss: 0.008, 44832/56000 datapoints
2025-03-04 14:51:43,739 - INFO - training batch 1451, loss: 0.005, 46432/56000 datapoints
2025-03-04 14:51:43,866 - INFO - training batch 1501, loss: 0.040, 48032/56000 datapoints
2025-03-04 14:51:43,966 - INFO - training batch 1551, loss: 0.009, 49632/56000 datapoints
2025-03-04 14:51:44,049 - INFO - training batch 1601, loss: 0.013, 51232/56000 datapoints
2025-03-04 14:51:44,136 - INFO - training batch 1651, loss: 0.020, 52832/56000 datapoints
2025-03-04 14:51:44,231 - INFO - training batch 1701, loss: 0.092, 54432/56000 datapoints
2025-03-04 14:51:44,334 - INFO - validation batch 1, loss: 0.008, 32/13984 datapoints
2025-03-04 14:51:44,366 - INFO - validation batch 51, loss: 0.021, 1632/13984 datapoints
2025-03-04 14:51:44,397 - INFO - validation batch 101, loss: 0.007, 3232/13984 datapoints
2025-03-04 14:51:44,432 - INFO - validation batch 151, loss: 0.011, 4832/13984 datapoints
2025-03-04 14:51:44,458 - INFO - validation batch 201, loss: 0.013, 6432/13984 datapoints
2025-03-04 14:51:44,492 - INFO - validation batch 251, loss: 0.011, 8032/13984 datapoints
2025-03-04 14:51:44,543 - INFO - validation batch 301, loss: 0.015, 9632/13984 datapoints
2025-03-04 14:51:44,573 - INFO - validation batch 351, loss: 0.009, 11232/13984 datapoints
2025-03-04 14:51:44,605 - INFO - validation batch 401, loss: 0.109, 12832/13984 datapoints
2025-03-04 14:51:44,627 - INFO - Epoch 17/50 done
2025-03-04 14:51:44,627 - INFO - Beginning epoch 18/50
2025-03-04 14:51:44,631 - INFO - training batch 1, loss: 0.148, 32/56000 datapoints
2025-03-04 14:51:44,723 - INFO - training batch 51, loss: 0.011, 1632/56000 datapoints
2025-03-04 14:51:44,846 - INFO - training batch 101, loss: 0.062, 3232/56000 datapoints
2025-03-04 14:51:44,941 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-04 14:51:45,053 - INFO - training batch 201, loss: 0.038, 6432/56000 datapoints
2025-03-04 14:51:45,166 - INFO - training batch 251, loss: 0.012, 8032/56000 datapoints
2025-03-04 14:51:45,303 - INFO - training batch 301, loss: 0.022, 9632/56000 datapoints
2025-03-04 14:51:45,469 - INFO - training batch 351, loss: 0.071, 11232/56000 datapoints
2025-03-04 14:51:45,625 - INFO - training batch 401, loss: 0.074, 12832/56000 datapoints
2025-03-04 14:51:45,732 - INFO - training batch 451, loss: 0.036, 14432/56000 datapoints
2025-03-04 14:51:45,830 - INFO - training batch 501, loss: 0.014, 16032/56000 datapoints
2025-03-04 14:51:45,947 - INFO - training batch 551, loss: 0.078, 17632/56000 datapoints
2025-03-04 14:51:46,040 - INFO - training batch 601, loss: 0.003, 19232/56000 datapoints
2025-03-04 14:51:46,123 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:51:46,201 - INFO - training batch 701, loss: 0.254, 22432/56000 datapoints
2025-03-04 14:51:46,276 - INFO - training batch 751, loss: 0.011, 24032/56000 datapoints
2025-03-04 14:51:46,355 - INFO - training batch 801, loss: 0.024, 25632/56000 datapoints
2025-03-04 14:51:46,432 - INFO - training batch 851, loss: 0.006, 27232/56000 datapoints
2025-03-04 14:51:46,509 - INFO - training batch 901, loss: 0.016, 28832/56000 datapoints
2025-03-04 14:51:46,589 - INFO - training batch 951, loss: 0.014, 30432/56000 datapoints
2025-03-04 14:51:46,667 - INFO - training batch 1001, loss: 0.007, 32032/56000 datapoints
2025-03-04 14:51:46,745 - INFO - training batch 1051, loss: 0.012, 33632/56000 datapoints
2025-03-04 14:51:46,835 - INFO - training batch 1101, loss: 0.075, 35232/56000 datapoints
2025-03-04 14:51:46,931 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-04 14:51:47,020 - INFO - training batch 1201, loss: 0.025, 38432/56000 datapoints
2025-03-04 14:51:47,109 - INFO - training batch 1251, loss: 0.215, 40032/56000 datapoints
2025-03-04 14:51:47,230 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-04 14:51:47,335 - INFO - training batch 1351, loss: 0.005, 43232/56000 datapoints
2025-03-04 14:51:47,466 - INFO - training batch 1401, loss: 0.008, 44832/56000 datapoints
2025-03-04 14:51:47,888 - INFO - training batch 1451, loss: 0.005, 46432/56000 datapoints
2025-03-04 14:51:48,338 - INFO - training batch 1501, loss: 0.039, 48032/56000 datapoints
2025-03-04 14:51:49,235 - INFO - training batch 1551, loss: 0.009, 49632/56000 datapoints
2025-03-04 14:51:49,717 - INFO - training batch 1601, loss: 0.013, 51232/56000 datapoints
2025-03-04 14:51:50,374 - INFO - training batch 1651, loss: 0.020, 52832/56000 datapoints
2025-03-04 14:51:50,495 - INFO - training batch 1701, loss: 0.093, 54432/56000 datapoints
2025-03-04 14:51:50,657 - INFO - validation batch 1, loss: 0.008, 32/13984 datapoints
2025-03-04 14:51:50,727 - INFO - validation batch 51, loss: 0.021, 1632/13984 datapoints
2025-03-04 14:51:50,784 - INFO - validation batch 101, loss: 0.006, 3232/13984 datapoints
2025-03-04 14:51:50,833 - INFO - validation batch 151, loss: 0.010, 4832/13984 datapoints
2025-03-04 14:51:50,887 - INFO - validation batch 201, loss: 0.012, 6432/13984 datapoints
2025-03-04 14:51:50,960 - INFO - validation batch 251, loss: 0.010, 8032/13984 datapoints
2025-03-04 14:51:51,020 - INFO - validation batch 301, loss: 0.014, 9632/13984 datapoints
2025-03-04 14:51:51,103 - INFO - validation batch 351, loss: 0.009, 11232/13984 datapoints
2025-03-04 14:51:51,226 - INFO - validation batch 401, loss: 0.109, 12832/13984 datapoints
2025-03-04 14:51:51,328 - INFO - Epoch 18/50 done
2025-03-04 14:51:51,329 - INFO - Beginning epoch 19/50
2025-03-04 14:51:51,334 - INFO - training batch 1, loss: 0.149, 32/56000 datapoints
2025-03-04 14:51:51,523 - INFO - training batch 51, loss: 0.011, 1632/56000 datapoints
2025-03-04 14:51:51,667 - INFO - training batch 101, loss: 0.062, 3232/56000 datapoints
2025-03-04 14:51:51,792 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-04 14:51:51,917 - INFO - training batch 201, loss: 0.037, 6432/56000 datapoints
2025-03-04 14:51:52,047 - INFO - training batch 251, loss: 0.012, 8032/56000 datapoints
2025-03-04 14:51:52,162 - INFO - training batch 301, loss: 0.021, 9632/56000 datapoints
2025-03-04 14:51:52,275 - INFO - training batch 351, loss: 0.071, 11232/56000 datapoints
2025-03-04 14:51:52,388 - INFO - training batch 401, loss: 0.073, 12832/56000 datapoints
2025-03-04 14:51:52,510 - INFO - training batch 451, loss: 0.035, 14432/56000 datapoints
2025-03-04 14:51:52,632 - INFO - training batch 501, loss: 0.013, 16032/56000 datapoints
2025-03-04 14:51:52,837 - INFO - training batch 551, loss: 0.077, 17632/56000 datapoints
2025-03-04 14:51:52,961 - INFO - training batch 601, loss: 0.003, 19232/56000 datapoints
2025-03-04 14:51:53,079 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:51:53,216 - INFO - training batch 701, loss: 0.255, 22432/56000 datapoints
2025-03-04 14:51:53,367 - INFO - training batch 751, loss: 0.010, 24032/56000 datapoints
2025-03-04 14:51:53,505 - INFO - training batch 801, loss: 0.023, 25632/56000 datapoints
2025-03-04 14:51:53,613 - INFO - training batch 851, loss: 0.006, 27232/56000 datapoints
2025-03-04 14:51:53,737 - INFO - training batch 901, loss: 0.015, 28832/56000 datapoints
2025-03-04 14:51:53,851 - INFO - training batch 951, loss: 0.014, 30432/56000 datapoints
2025-03-04 14:51:53,962 - INFO - training batch 1001, loss: 0.006, 32032/56000 datapoints
2025-03-04 14:51:54,064 - INFO - training batch 1051, loss: 0.011, 33632/56000 datapoints
2025-03-04 14:51:54,161 - INFO - training batch 1101, loss: 0.076, 35232/56000 datapoints
2025-03-04 14:51:54,245 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-04 14:51:54,358 - INFO - training batch 1201, loss: 0.024, 38432/56000 datapoints
2025-03-04 14:51:54,441 - INFO - training batch 1251, loss: 0.214, 40032/56000 datapoints
2025-03-04 14:51:54,516 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-04 14:51:54,593 - INFO - training batch 1351, loss: 0.005, 43232/56000 datapoints
2025-03-04 14:51:54,692 - INFO - training batch 1401, loss: 0.007, 44832/56000 datapoints
2025-03-04 14:51:54,784 - INFO - training batch 1451, loss: 0.005, 46432/56000 datapoints
2025-03-04 14:51:54,869 - INFO - training batch 1501, loss: 0.039, 48032/56000 datapoints
2025-03-04 14:51:54,994 - INFO - training batch 1551, loss: 0.008, 49632/56000 datapoints
2025-03-04 14:51:55,094 - INFO - training batch 1601, loss: 0.012, 51232/56000 datapoints
2025-03-04 14:51:55,186 - INFO - training batch 1651, loss: 0.020, 52832/56000 datapoints
2025-03-04 14:51:55,267 - INFO - training batch 1701, loss: 0.093, 54432/56000 datapoints
2025-03-04 14:51:55,348 - INFO - validation batch 1, loss: 0.007, 32/13984 datapoints
2025-03-04 14:51:55,373 - INFO - validation batch 51, loss: 0.021, 1632/13984 datapoints
2025-03-04 14:51:55,402 - INFO - validation batch 101, loss: 0.006, 3232/13984 datapoints
2025-03-04 14:51:55,434 - INFO - validation batch 151, loss: 0.010, 4832/13984 datapoints
2025-03-04 14:51:55,467 - INFO - validation batch 201, loss: 0.012, 6432/13984 datapoints
2025-03-04 14:51:55,496 - INFO - validation batch 251, loss: 0.009, 8032/13984 datapoints
2025-03-04 14:51:55,525 - INFO - validation batch 301, loss: 0.013, 9632/13984 datapoints
2025-03-04 14:51:55,557 - INFO - validation batch 351, loss: 0.008, 11232/13984 datapoints
2025-03-04 14:51:55,587 - INFO - validation batch 401, loss: 0.110, 12832/13984 datapoints
2025-03-04 14:51:55,608 - INFO - Epoch 19/50 done
2025-03-04 14:51:55,608 - INFO - Beginning epoch 20/50
2025-03-04 14:51:55,612 - INFO - training batch 1, loss: 0.150, 32/56000 datapoints
2025-03-04 14:51:55,706 - INFO - training batch 51, loss: 0.010, 1632/56000 datapoints
2025-03-04 14:51:55,806 - INFO - training batch 101, loss: 0.062, 3232/56000 datapoints
2025-03-04 14:51:55,892 - INFO - training batch 151, loss: 0.061, 4832/56000 datapoints
2025-03-04 14:51:55,979 - INFO - training batch 201, loss: 0.037, 6432/56000 datapoints
2025-03-04 14:51:56,095 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:51:56,218 - INFO - training batch 301, loss: 0.021, 9632/56000 datapoints
2025-03-04 14:51:56,304 - INFO - training batch 351, loss: 0.071, 11232/56000 datapoints
2025-03-04 14:51:56,386 - INFO - training batch 401, loss: 0.073, 12832/56000 datapoints
2025-03-04 14:51:56,463 - INFO - training batch 451, loss: 0.035, 14432/56000 datapoints
2025-03-04 14:51:56,541 - INFO - training batch 501, loss: 0.012, 16032/56000 datapoints
2025-03-04 14:51:56,628 - INFO - training batch 551, loss: 0.076, 17632/56000 datapoints
2025-03-04 14:51:56,713 - INFO - training batch 601, loss: 0.003, 19232/56000 datapoints
2025-03-04 14:51:56,784 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:51:56,863 - INFO - training batch 701, loss: 0.255, 22432/56000 datapoints
2025-03-04 14:51:56,944 - INFO - training batch 751, loss: 0.010, 24032/56000 datapoints
2025-03-04 14:51:57,027 - INFO - training batch 801, loss: 0.022, 25632/56000 datapoints
2025-03-04 14:51:57,116 - INFO - training batch 851, loss: 0.006, 27232/56000 datapoints
2025-03-04 14:51:57,195 - INFO - training batch 901, loss: 0.015, 28832/56000 datapoints
2025-03-04 14:51:57,287 - INFO - training batch 951, loss: 0.013, 30432/56000 datapoints
2025-03-04 14:51:57,389 - INFO - training batch 1001, loss: 0.006, 32032/56000 datapoints
2025-03-04 14:51:57,482 - INFO - training batch 1051, loss: 0.011, 33632/56000 datapoints
2025-03-04 14:51:57,572 - INFO - training batch 1101, loss: 0.076, 35232/56000 datapoints
2025-03-04 14:51:57,664 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-04 14:51:57,747 - INFO - training batch 1201, loss: 0.023, 38432/56000 datapoints
2025-03-04 14:51:57,818 - INFO - training batch 1251, loss: 0.213, 40032/56000 datapoints
2025-03-04 14:51:57,887 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-04 14:51:57,968 - INFO - training batch 1351, loss: 0.004, 43232/56000 datapoints
2025-03-04 14:51:58,039 - INFO - training batch 1401, loss: 0.007, 44832/56000 datapoints
2025-03-04 14:51:58,113 - INFO - training batch 1451, loss: 0.004, 46432/56000 datapoints
2025-03-04 14:51:58,201 - INFO - training batch 1501, loss: 0.038, 48032/56000 datapoints
2025-03-04 14:51:58,289 - INFO - training batch 1551, loss: 0.008, 49632/56000 datapoints
2025-03-04 14:51:58,369 - INFO - training batch 1601, loss: 0.012, 51232/56000 datapoints
2025-03-04 14:51:58,451 - INFO - training batch 1651, loss: 0.019, 52832/56000 datapoints
2025-03-04 14:51:58,530 - INFO - training batch 1701, loss: 0.093, 54432/56000 datapoints
2025-03-04 14:51:58,616 - INFO - validation batch 1, loss: 0.007, 32/13984 datapoints
2025-03-04 14:51:58,642 - INFO - validation batch 51, loss: 0.021, 1632/13984 datapoints
2025-03-04 14:51:58,672 - INFO - validation batch 101, loss: 0.006, 3232/13984 datapoints
2025-03-04 14:51:58,702 - INFO - validation batch 151, loss: 0.010, 4832/13984 datapoints
2025-03-04 14:51:58,733 - INFO - validation batch 201, loss: 0.011, 6432/13984 datapoints
2025-03-04 14:51:58,766 - INFO - validation batch 251, loss: 0.009, 8032/13984 datapoints
2025-03-04 14:51:58,801 - INFO - validation batch 301, loss: 0.013, 9632/13984 datapoints
2025-03-04 14:51:58,831 - INFO - validation batch 351, loss: 0.008, 11232/13984 datapoints
2025-03-04 14:51:58,857 - INFO - validation batch 401, loss: 0.110, 12832/13984 datapoints
2025-03-04 14:51:58,875 - INFO - Epoch 20/50 done
2025-03-04 14:51:58,875 - INFO - Beginning epoch 21/50
2025-03-04 14:51:58,878 - INFO - training batch 1, loss: 0.151, 32/56000 datapoints
2025-03-04 14:51:58,963 - INFO - training batch 51, loss: 0.010, 1632/56000 datapoints
2025-03-04 14:51:59,054 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-04 14:51:59,132 - INFO - training batch 151, loss: 0.061, 4832/56000 datapoints
2025-03-04 14:51:59,223 - INFO - training batch 201, loss: 0.036, 6432/56000 datapoints
2025-03-04 14:51:59,306 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:51:59,377 - INFO - training batch 301, loss: 0.021, 9632/56000 datapoints
2025-03-04 14:51:59,453 - INFO - training batch 351, loss: 0.071, 11232/56000 datapoints
2025-03-04 14:51:59,544 - INFO - training batch 401, loss: 0.072, 12832/56000 datapoints
2025-03-04 14:51:59,615 - INFO - training batch 451, loss: 0.034, 14432/56000 datapoints
2025-03-04 14:51:59,695 - INFO - training batch 501, loss: 0.012, 16032/56000 datapoints
2025-03-04 14:51:59,794 - INFO - training batch 551, loss: 0.076, 17632/56000 datapoints
2025-03-04 14:52:00,225 - INFO - training batch 601, loss: 0.003, 19232/56000 datapoints
2025-03-04 14:52:00,500 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:52:00,730 - INFO - training batch 701, loss: 0.256, 22432/56000 datapoints
2025-03-04 14:52:01,027 - INFO - training batch 751, loss: 0.009, 24032/56000 datapoints
2025-03-04 14:52:01,281 - INFO - training batch 801, loss: 0.021, 25632/56000 datapoints
2025-03-04 14:52:02,276 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-04 14:52:03,175 - INFO - training batch 901, loss: 0.014, 28832/56000 datapoints
2025-03-04 14:52:03,367 - INFO - training batch 951, loss: 0.013, 30432/56000 datapoints
2025-03-04 14:52:03,526 - INFO - training batch 1001, loss: 0.006, 32032/56000 datapoints
2025-03-04 14:52:03,802 - INFO - training batch 1051, loss: 0.010, 33632/56000 datapoints
2025-03-04 14:52:04,120 - INFO - training batch 1101, loss: 0.077, 35232/56000 datapoints
2025-03-04 14:52:04,523 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-04 14:52:05,271 - INFO - training batch 1201, loss: 0.022, 38432/56000 datapoints
2025-03-04 14:52:06,209 - INFO - training batch 1251, loss: 0.212, 40032/56000 datapoints
2025-03-04 14:52:06,380 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-04 14:52:06,559 - INFO - training batch 1351, loss: 0.004, 43232/56000 datapoints
2025-03-04 14:52:06,696 - INFO - training batch 1401, loss: 0.007, 44832/56000 datapoints
2025-03-04 14:52:06,801 - INFO - training batch 1451, loss: 0.004, 46432/56000 datapoints
2025-03-04 14:52:07,435 - INFO - training batch 1501, loss: 0.037, 48032/56000 datapoints
2025-03-04 14:52:08,054 - INFO - training batch 1551, loss: 0.008, 49632/56000 datapoints
2025-03-04 14:52:08,429 - INFO - training batch 1601, loss: 0.011, 51232/56000 datapoints
2025-03-04 14:52:08,559 - INFO - training batch 1651, loss: 0.019, 52832/56000 datapoints
2025-03-04 14:52:08,695 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:52:08,851 - INFO - validation batch 1, loss: 0.007, 32/13984 datapoints
2025-03-04 14:52:09,112 - INFO - validation batch 51, loss: 0.021, 1632/13984 datapoints
2025-03-04 14:52:09,218 - INFO - validation batch 101, loss: 0.005, 3232/13984 datapoints
2025-03-04 14:52:09,288 - INFO - validation batch 151, loss: 0.010, 4832/13984 datapoints
2025-03-04 14:52:09,419 - INFO - validation batch 201, loss: 0.011, 6432/13984 datapoints
2025-03-04 14:52:09,491 - INFO - validation batch 251, loss: 0.009, 8032/13984 datapoints
2025-03-04 14:52:09,586 - INFO - validation batch 301, loss: 0.012, 9632/13984 datapoints
2025-03-04 14:52:09,660 - INFO - validation batch 351, loss: 0.008, 11232/13984 datapoints
2025-03-04 14:52:09,721 - INFO - validation batch 401, loss: 0.110, 12832/13984 datapoints
2025-03-04 14:52:09,778 - INFO - Epoch 21/50 done
2025-03-04 14:52:09,778 - INFO - Beginning epoch 22/50
2025-03-04 14:52:09,782 - INFO - training batch 1, loss: 0.151, 32/56000 datapoints
2025-03-04 14:52:10,001 - INFO - training batch 51, loss: 0.009, 1632/56000 datapoints
2025-03-04 14:52:10,141 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-04 14:52:10,288 - INFO - training batch 151, loss: 0.061, 4832/56000 datapoints
2025-03-04 14:52:10,434 - INFO - training batch 201, loss: 0.035, 6432/56000 datapoints
2025-03-04 14:52:10,576 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:52:10,719 - INFO - training batch 301, loss: 0.021, 9632/56000 datapoints
2025-03-04 14:52:10,864 - INFO - training batch 351, loss: 0.070, 11232/56000 datapoints
2025-03-04 14:52:11,349 - INFO - training batch 401, loss: 0.071, 12832/56000 datapoints
2025-03-04 14:52:11,474 - INFO - training batch 451, loss: 0.033, 14432/56000 datapoints
2025-03-04 14:52:11,616 - INFO - training batch 501, loss: 0.011, 16032/56000 datapoints
2025-03-04 14:52:11,782 - INFO - training batch 551, loss: 0.075, 17632/56000 datapoints
2025-03-04 14:52:11,942 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:52:12,069 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:52:12,199 - INFO - training batch 701, loss: 0.256, 22432/56000 datapoints
2025-03-04 14:52:12,328 - INFO - training batch 751, loss: 0.009, 24032/56000 datapoints
2025-03-04 14:52:12,428 - INFO - training batch 801, loss: 0.020, 25632/56000 datapoints
2025-03-04 14:52:12,532 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-04 14:52:12,655 - INFO - training batch 901, loss: 0.014, 28832/56000 datapoints
2025-03-04 14:52:12,790 - INFO - training batch 951, loss: 0.013, 30432/56000 datapoints
2025-03-04 14:52:12,914 - INFO - training batch 1001, loss: 0.006, 32032/56000 datapoints
2025-03-04 14:52:13,026 - INFO - training batch 1051, loss: 0.010, 33632/56000 datapoints
2025-03-04 14:52:13,137 - INFO - training batch 1101, loss: 0.077, 35232/56000 datapoints
2025-03-04 14:52:13,235 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-04 14:52:13,335 - INFO - training batch 1201, loss: 0.021, 38432/56000 datapoints
2025-03-04 14:52:13,429 - INFO - training batch 1251, loss: 0.210, 40032/56000 datapoints
2025-03-04 14:52:13,512 - INFO - training batch 1301, loss: 0.017, 41632/56000 datapoints
2025-03-04 14:52:13,591 - INFO - training batch 1351, loss: 0.004, 43232/56000 datapoints
2025-03-04 14:52:13,669 - INFO - training batch 1401, loss: 0.006, 44832/56000 datapoints
2025-03-04 14:52:13,747 - INFO - training batch 1451, loss: 0.004, 46432/56000 datapoints
2025-03-04 14:52:13,825 - INFO - training batch 1501, loss: 0.036, 48032/56000 datapoints
2025-03-04 14:52:13,914 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-04 14:52:13,988 - INFO - training batch 1601, loss: 0.011, 51232/56000 datapoints
2025-03-04 14:52:14,067 - INFO - training batch 1651, loss: 0.019, 52832/56000 datapoints
2025-03-04 14:52:14,159 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:52:14,238 - INFO - validation batch 1, loss: 0.007, 32/13984 datapoints
2025-03-04 14:52:14,430 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-04 14:52:14,511 - INFO - validation batch 101, loss: 0.005, 3232/13984 datapoints
2025-03-04 14:52:14,538 - INFO - validation batch 151, loss: 0.009, 4832/13984 datapoints
2025-03-04 14:52:14,566 - INFO - validation batch 201, loss: 0.011, 6432/13984 datapoints
2025-03-04 14:52:14,620 - INFO - validation batch 251, loss: 0.008, 8032/13984 datapoints
2025-03-04 14:52:14,651 - INFO - validation batch 301, loss: 0.012, 9632/13984 datapoints
2025-03-04 14:52:14,688 - INFO - validation batch 351, loss: 0.007, 11232/13984 datapoints
2025-03-04 14:52:14,723 - INFO - validation batch 401, loss: 0.110, 12832/13984 datapoints
2025-03-04 14:52:14,747 - INFO - Epoch 22/50 done
2025-03-04 14:52:14,748 - INFO - Beginning epoch 23/50
2025-03-04 14:52:14,751 - INFO - training batch 1, loss: 0.152, 32/56000 datapoints
2025-03-04 14:52:14,851 - INFO - training batch 51, loss: 0.009, 1632/56000 datapoints
2025-03-04 14:52:14,969 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-04 14:52:15,069 - INFO - training batch 151, loss: 0.060, 4832/56000 datapoints
2025-03-04 14:52:15,413 - INFO - training batch 201, loss: 0.034, 6432/56000 datapoints
2025-03-04 14:52:15,512 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:52:15,599 - INFO - training batch 301, loss: 0.020, 9632/56000 datapoints
2025-03-04 14:52:15,683 - INFO - training batch 351, loss: 0.070, 11232/56000 datapoints
2025-03-04 14:52:15,776 - INFO - training batch 401, loss: 0.070, 12832/56000 datapoints
2025-03-04 14:52:15,878 - INFO - training batch 451, loss: 0.032, 14432/56000 datapoints
2025-03-04 14:52:15,986 - INFO - training batch 501, loss: 0.011, 16032/56000 datapoints
2025-03-04 14:52:16,084 - INFO - training batch 551, loss: 0.074, 17632/56000 datapoints
2025-03-04 14:52:16,180 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:52:16,279 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:52:16,388 - INFO - training batch 701, loss: 0.256, 22432/56000 datapoints
2025-03-04 14:52:16,509 - INFO - training batch 751, loss: 0.009, 24032/56000 datapoints
2025-03-04 14:52:16,611 - INFO - training batch 801, loss: 0.020, 25632/56000 datapoints
2025-03-04 14:52:16,766 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-04 14:52:16,896 - INFO - training batch 901, loss: 0.013, 28832/56000 datapoints
2025-03-04 14:52:16,991 - INFO - training batch 951, loss: 0.012, 30432/56000 datapoints
2025-03-04 14:52:17,091 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-04 14:52:17,186 - INFO - training batch 1051, loss: 0.010, 33632/56000 datapoints
2025-03-04 14:52:17,305 - INFO - training batch 1101, loss: 0.077, 35232/56000 datapoints
2025-03-04 14:52:17,394 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-04 14:52:17,472 - INFO - training batch 1201, loss: 0.021, 38432/56000 datapoints
2025-03-04 14:52:17,548 - INFO - training batch 1251, loss: 0.209, 40032/56000 datapoints
2025-03-04 14:52:17,624 - INFO - training batch 1301, loss: 0.017, 41632/56000 datapoints
2025-03-04 14:52:17,700 - INFO - training batch 1351, loss: 0.004, 43232/56000 datapoints
2025-03-04 14:52:17,776 - INFO - training batch 1401, loss: 0.006, 44832/56000 datapoints
2025-03-04 14:52:17,858 - INFO - training batch 1451, loss: 0.004, 46432/56000 datapoints
2025-03-04 14:52:17,948 - INFO - training batch 1501, loss: 0.036, 48032/56000 datapoints
2025-03-04 14:52:18,077 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-04 14:52:18,203 - INFO - training batch 1601, loss: 0.011, 51232/56000 datapoints
2025-03-04 14:52:18,304 - INFO - training batch 1651, loss: 0.019, 52832/56000 datapoints
2025-03-04 14:52:18,412 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:52:18,514 - INFO - validation batch 1, loss: 0.006, 32/13984 datapoints
2025-03-04 14:52:18,547 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-04 14:52:18,580 - INFO - validation batch 101, loss: 0.005, 3232/13984 datapoints
2025-03-04 14:52:18,612 - INFO - validation batch 151, loss: 0.009, 4832/13984 datapoints
2025-03-04 14:52:18,651 - INFO - validation batch 201, loss: 0.010, 6432/13984 datapoints
2025-03-04 14:52:18,685 - INFO - validation batch 251, loss: 0.008, 8032/13984 datapoints
2025-03-04 14:52:18,713 - INFO - validation batch 301, loss: 0.011, 9632/13984 datapoints
2025-03-04 14:52:18,752 - INFO - validation batch 351, loss: 0.007, 11232/13984 datapoints
2025-03-04 14:52:18,794 - INFO - validation batch 401, loss: 0.110, 12832/13984 datapoints
2025-03-04 14:52:18,835 - INFO - Epoch 23/50 done
2025-03-04 14:52:18,838 - INFO - Beginning epoch 24/50
2025-03-04 14:52:18,844 - INFO - training batch 1, loss: 0.153, 32/56000 datapoints
2025-03-04 14:52:19,033 - INFO - training batch 51, loss: 0.009, 1632/56000 datapoints
2025-03-04 14:52:19,290 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-04 14:52:19,437 - INFO - training batch 151, loss: 0.060, 4832/56000 datapoints
2025-03-04 14:52:20,344 - INFO - training batch 201, loss: 0.034, 6432/56000 datapoints
2025-03-04 14:52:21,166 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:52:22,742 - INFO - training batch 301, loss: 0.020, 9632/56000 datapoints
2025-03-04 14:52:23,593 - INFO - training batch 351, loss: 0.070, 11232/56000 datapoints
2025-03-04 14:52:24,506 - INFO - training batch 401, loss: 0.069, 12832/56000 datapoints
2025-03-04 14:52:25,037 - INFO - training batch 451, loss: 0.032, 14432/56000 datapoints
2025-03-04 14:52:25,286 - INFO - training batch 501, loss: 0.010, 16032/56000 datapoints
2025-03-04 14:52:25,790 - INFO - training batch 551, loss: 0.073, 17632/56000 datapoints
2025-03-04 14:52:26,072 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:52:26,353 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:52:27,571 - INFO - training batch 701, loss: 0.256, 22432/56000 datapoints
2025-03-04 14:52:28,049 - INFO - training batch 751, loss: 0.009, 24032/56000 datapoints
2025-03-04 14:52:28,306 - INFO - training batch 801, loss: 0.019, 25632/56000 datapoints
2025-03-04 14:52:28,555 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-04 14:52:28,714 - INFO - training batch 901, loss: 0.013, 28832/56000 datapoints
2025-03-04 14:52:29,047 - INFO - training batch 951, loss: 0.012, 30432/56000 datapoints
2025-03-04 14:52:29,202 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-04 14:52:29,361 - INFO - training batch 1051, loss: 0.009, 33632/56000 datapoints
2025-03-04 14:52:29,521 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:52:29,656 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-04 14:52:29,809 - INFO - training batch 1201, loss: 0.020, 38432/56000 datapoints
2025-03-04 14:52:29,960 - INFO - training batch 1251, loss: 0.208, 40032/56000 datapoints
2025-03-04 14:52:30,083 - INFO - training batch 1301, loss: 0.017, 41632/56000 datapoints
2025-03-04 14:52:30,195 - INFO - training batch 1351, loss: 0.004, 43232/56000 datapoints
2025-03-04 14:52:30,311 - INFO - training batch 1401, loss: 0.006, 44832/56000 datapoints
2025-03-04 14:52:30,436 - INFO - training batch 1451, loss: 0.004, 46432/56000 datapoints
2025-03-04 14:52:30,557 - INFO - training batch 1501, loss: 0.035, 48032/56000 datapoints
2025-03-04 14:52:30,673 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-04 14:52:30,784 - INFO - training batch 1601, loss: 0.010, 51232/56000 datapoints
2025-03-04 14:52:30,874 - INFO - training batch 1651, loss: 0.019, 52832/56000 datapoints
2025-03-04 14:52:30,981 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:52:31,084 - INFO - validation batch 1, loss: 0.006, 32/13984 datapoints
2025-03-04 14:52:31,130 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-04 14:52:31,174 - INFO - validation batch 101, loss: 0.005, 3232/13984 datapoints
2025-03-04 14:52:31,224 - INFO - validation batch 151, loss: 0.009, 4832/13984 datapoints
2025-03-04 14:52:31,276 - INFO - validation batch 201, loss: 0.010, 6432/13984 datapoints
2025-03-04 14:52:31,317 - INFO - validation batch 251, loss: 0.007, 8032/13984 datapoints
2025-03-04 14:52:31,358 - INFO - validation batch 301, loss: 0.011, 9632/13984 datapoints
2025-03-04 14:52:31,402 - INFO - validation batch 351, loss: 0.007, 11232/13984 datapoints
2025-03-04 14:52:31,446 - INFO - validation batch 401, loss: 0.110, 12832/13984 datapoints
2025-03-04 14:52:31,482 - INFO - Epoch 24/50 done
2025-03-04 14:52:31,483 - INFO - Beginning epoch 25/50
2025-03-04 14:52:31,487 - INFO - training batch 1, loss: 0.153, 32/56000 datapoints
2025-03-04 14:52:31,601 - INFO - training batch 51, loss: 0.008, 1632/56000 datapoints
2025-03-04 14:52:31,689 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:52:31,790 - INFO - training batch 151, loss: 0.059, 4832/56000 datapoints
2025-03-04 14:52:31,889 - INFO - training batch 201, loss: 0.033, 6432/56000 datapoints
2025-03-04 14:52:32,006 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:52:32,122 - INFO - training batch 301, loss: 0.020, 9632/56000 datapoints
2025-03-04 14:52:32,226 - INFO - training batch 351, loss: 0.069, 11232/56000 datapoints
2025-03-04 14:52:32,374 - INFO - training batch 401, loss: 0.069, 12832/56000 datapoints
2025-03-04 14:52:32,484 - INFO - training batch 451, loss: 0.031, 14432/56000 datapoints
2025-03-04 14:52:32,587 - INFO - training batch 501, loss: 0.010, 16032/56000 datapoints
2025-03-04 14:52:32,684 - INFO - training batch 551, loss: 0.072, 17632/56000 datapoints
2025-03-04 14:52:32,783 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:52:32,880 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:52:32,983 - INFO - training batch 701, loss: 0.255, 22432/56000 datapoints
2025-03-04 14:52:33,079 - INFO - training batch 751, loss: 0.008, 24032/56000 datapoints
2025-03-04 14:52:33,168 - INFO - training batch 801, loss: 0.019, 25632/56000 datapoints
2025-03-04 14:52:33,258 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-04 14:52:33,372 - INFO - training batch 901, loss: 0.013, 28832/56000 datapoints
2025-03-04 14:52:33,465 - INFO - training batch 951, loss: 0.012, 30432/56000 datapoints
2025-03-04 14:52:33,633 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-04 14:52:33,734 - INFO - training batch 1051, loss: 0.009, 33632/56000 datapoints
2025-03-04 14:52:33,835 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:52:33,944 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:52:34,042 - INFO - training batch 1201, loss: 0.020, 38432/56000 datapoints
2025-03-04 14:52:34,148 - INFO - training batch 1251, loss: 0.207, 40032/56000 datapoints
2025-03-04 14:52:34,252 - INFO - training batch 1301, loss: 0.017, 41632/56000 datapoints
2025-03-04 14:52:34,369 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-04 14:52:34,469 - INFO - training batch 1401, loss: 0.006, 44832/56000 datapoints
2025-03-04 14:52:34,575 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:52:34,710 - INFO - training batch 1501, loss: 0.035, 48032/56000 datapoints
2025-03-04 14:52:34,814 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-04 14:52:34,934 - INFO - training batch 1601, loss: 0.010, 51232/56000 datapoints
2025-03-04 14:52:35,024 - INFO - training batch 1651, loss: 0.018, 52832/56000 datapoints
2025-03-04 14:52:35,104 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:52:35,181 - INFO - validation batch 1, loss: 0.006, 32/13984 datapoints
2025-03-04 14:52:35,207 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-04 14:52:35,239 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-04 14:52:35,266 - INFO - validation batch 151, loss: 0.009, 4832/13984 datapoints
2025-03-04 14:52:35,300 - INFO - validation batch 201, loss: 0.010, 6432/13984 datapoints
2025-03-04 14:52:35,325 - INFO - validation batch 251, loss: 0.007, 8032/13984 datapoints
2025-03-04 14:52:35,354 - INFO - validation batch 301, loss: 0.011, 9632/13984 datapoints
2025-03-04 14:52:35,377 - INFO - validation batch 351, loss: 0.006, 11232/13984 datapoints
2025-03-04 14:52:35,402 - INFO - validation batch 401, loss: 0.110, 12832/13984 datapoints
2025-03-04 14:52:35,424 - INFO - Epoch 25/50 done
2025-03-04 14:52:35,424 - INFO - Beginning epoch 26/50
2025-03-04 14:52:35,427 - INFO - training batch 1, loss: 0.154, 32/56000 datapoints
2025-03-04 14:52:35,514 - INFO - training batch 51, loss: 0.008, 1632/56000 datapoints
2025-03-04 14:52:35,600 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:52:35,679 - INFO - training batch 151, loss: 0.059, 4832/56000 datapoints
2025-03-04 14:52:35,758 - INFO - training batch 201, loss: 0.032, 6432/56000 datapoints
2025-03-04 14:52:35,834 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:52:35,914 - INFO - training batch 301, loss: 0.020, 9632/56000 datapoints
2025-03-04 14:52:35,994 - INFO - training batch 351, loss: 0.069, 11232/56000 datapoints
2025-03-04 14:52:36,073 - INFO - training batch 401, loss: 0.068, 12832/56000 datapoints
2025-03-04 14:52:36,156 - INFO - training batch 451, loss: 0.031, 14432/56000 datapoints
2025-03-04 14:52:36,235 - INFO - training batch 501, loss: 0.009, 16032/56000 datapoints
2025-03-04 14:52:36,318 - INFO - training batch 551, loss: 0.071, 17632/56000 datapoints
2025-03-04 14:52:36,409 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:52:36,485 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:52:36,574 - INFO - training batch 701, loss: 0.255, 22432/56000 datapoints
2025-03-04 14:52:36,658 - INFO - training batch 751, loss: 0.008, 24032/56000 datapoints
2025-03-04 14:52:36,752 - INFO - training batch 801, loss: 0.018, 25632/56000 datapoints
2025-03-04 14:52:36,838 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-04 14:52:36,926 - INFO - training batch 901, loss: 0.012, 28832/56000 datapoints
2025-03-04 14:52:37,027 - INFO - training batch 951, loss: 0.012, 30432/56000 datapoints
2025-03-04 14:52:37,123 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-04 14:52:37,206 - INFO - training batch 1051, loss: 0.009, 33632/56000 datapoints
2025-03-04 14:52:37,317 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:52:37,408 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:52:37,621 - INFO - training batch 1201, loss: 0.019, 38432/56000 datapoints
2025-03-04 14:52:37,864 - INFO - training batch 1251, loss: 0.205, 40032/56000 datapoints
2025-03-04 14:52:38,170 - INFO - training batch 1301, loss: 0.017, 41632/56000 datapoints
2025-03-04 14:52:38,622 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-04 14:52:38,953 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-04 14:52:39,147 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:52:39,237 - INFO - training batch 1501, loss: 0.034, 48032/56000 datapoints
2025-03-04 14:52:39,335 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-04 14:52:39,433 - INFO - training batch 1601, loss: 0.010, 51232/56000 datapoints
2025-03-04 14:52:39,540 - INFO - training batch 1651, loss: 0.018, 52832/56000 datapoints
2025-03-04 14:52:39,643 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:52:39,744 - INFO - validation batch 1, loss: 0.006, 32/13984 datapoints
2025-03-04 14:52:39,815 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-04 14:52:39,854 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-04 14:52:39,910 - INFO - validation batch 151, loss: 0.009, 4832/13984 datapoints
2025-03-04 14:52:40,009 - INFO - validation batch 201, loss: 0.009, 6432/13984 datapoints
2025-03-04 14:52:40,061 - INFO - validation batch 251, loss: 0.007, 8032/13984 datapoints
2025-03-04 14:52:40,116 - INFO - validation batch 301, loss: 0.011, 9632/13984 datapoints
2025-03-04 14:52:40,156 - INFO - validation batch 351, loss: 0.006, 11232/13984 datapoints
2025-03-04 14:52:40,194 - INFO - validation batch 401, loss: 0.110, 12832/13984 datapoints
2025-03-04 14:52:40,230 - INFO - Epoch 26/50 done
2025-03-04 14:52:40,231 - INFO - Beginning epoch 27/50
2025-03-04 14:52:40,234 - INFO - training batch 1, loss: 0.154, 32/56000 datapoints
2025-03-04 14:52:40,336 - INFO - training batch 51, loss: 0.008, 1632/56000 datapoints
2025-03-04 14:52:40,436 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:52:40,544 - INFO - training batch 151, loss: 0.058, 4832/56000 datapoints
2025-03-04 14:52:40,652 - INFO - training batch 201, loss: 0.032, 6432/56000 datapoints
2025-03-04 14:52:40,750 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:52:40,845 - INFO - training batch 301, loss: 0.020, 9632/56000 datapoints
2025-03-04 14:52:40,955 - INFO - training batch 351, loss: 0.069, 11232/56000 datapoints
2025-03-04 14:52:41,080 - INFO - training batch 401, loss: 0.067, 12832/56000 datapoints
2025-03-04 14:52:41,194 - INFO - training batch 451, loss: 0.030, 14432/56000 datapoints
2025-03-04 14:52:41,320 - INFO - training batch 501, loss: 0.009, 16032/56000 datapoints
2025-03-04 14:52:41,410 - INFO - training batch 551, loss: 0.070, 17632/56000 datapoints
2025-03-04 14:52:41,536 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:52:41,644 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:52:41,748 - INFO - training batch 701, loss: 0.255, 22432/56000 datapoints
2025-03-04 14:52:41,968 - INFO - training batch 751, loss: 0.008, 24032/56000 datapoints
2025-03-04 14:52:42,158 - INFO - training batch 801, loss: 0.018, 25632/56000 datapoints
2025-03-04 14:52:42,326 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-04 14:52:42,438 - INFO - training batch 901, loss: 0.012, 28832/56000 datapoints
2025-03-04 14:52:42,606 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-04 14:52:43,057 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-04 14:52:43,323 - INFO - training batch 1051, loss: 0.009, 33632/56000 datapoints
2025-03-04 14:52:43,463 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:52:43,566 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:52:43,676 - INFO - training batch 1201, loss: 0.019, 38432/56000 datapoints
2025-03-04 14:52:43,774 - INFO - training batch 1251, loss: 0.204, 40032/56000 datapoints
2025-03-04 14:52:43,880 - INFO - training batch 1301, loss: 0.017, 41632/56000 datapoints
2025-03-04 14:52:44,049 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-04 14:52:44,163 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-04 14:52:44,272 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:52:44,424 - INFO - training batch 1501, loss: 0.034, 48032/56000 datapoints
2025-03-04 14:52:44,534 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-04 14:52:44,665 - INFO - training batch 1601, loss: 0.010, 51232/56000 datapoints
2025-03-04 14:52:44,784 - INFO - training batch 1651, loss: 0.018, 52832/56000 datapoints
2025-03-04 14:52:44,895 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:52:45,010 - INFO - validation batch 1, loss: 0.006, 32/13984 datapoints
2025-03-04 14:52:45,046 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-04 14:52:45,097 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-04 14:52:45,132 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-04 14:52:45,175 - INFO - validation batch 201, loss: 0.009, 6432/13984 datapoints
2025-03-04 14:52:45,219 - INFO - validation batch 251, loss: 0.007, 8032/13984 datapoints
2025-03-04 14:52:45,256 - INFO - validation batch 301, loss: 0.010, 9632/13984 datapoints
2025-03-04 14:52:45,297 - INFO - validation batch 351, loss: 0.006, 11232/13984 datapoints
2025-03-04 14:52:45,351 - INFO - validation batch 401, loss: 0.110, 12832/13984 datapoints
2025-03-04 14:52:45,406 - INFO - Epoch 27/50 done
2025-03-04 14:52:45,407 - INFO - Beginning epoch 28/50
2025-03-04 14:52:45,413 - INFO - training batch 1, loss: 0.155, 32/56000 datapoints
2025-03-04 14:52:46,620 - INFO - training batch 51, loss: 0.008, 1632/56000 datapoints
2025-03-04 14:52:48,641 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:52:50,061 - INFO - training batch 151, loss: 0.058, 4832/56000 datapoints
2025-03-04 14:52:50,359 - INFO - training batch 201, loss: 0.031, 6432/56000 datapoints
2025-03-04 14:52:50,704 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:52:51,433 - INFO - training batch 301, loss: 0.020, 9632/56000 datapoints
2025-03-04 14:52:52,230 - INFO - training batch 351, loss: 0.068, 11232/56000 datapoints
2025-03-04 14:52:52,530 - INFO - training batch 401, loss: 0.066, 12832/56000 datapoints
2025-03-04 14:52:52,911 - INFO - training batch 451, loss: 0.030, 14432/56000 datapoints
2025-03-04 14:52:53,206 - INFO - training batch 501, loss: 0.009, 16032/56000 datapoints
2025-03-04 14:52:53,382 - INFO - training batch 551, loss: 0.069, 17632/56000 datapoints
2025-03-04 14:52:53,609 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:52:54,283 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:52:55,288 - INFO - training batch 701, loss: 0.254, 22432/56000 datapoints
2025-03-04 14:52:55,506 - INFO - training batch 751, loss: 0.008, 24032/56000 datapoints
2025-03-04 14:52:55,669 - INFO - training batch 801, loss: 0.017, 25632/56000 datapoints
2025-03-04 14:52:55,834 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:52:55,979 - INFO - training batch 901, loss: 0.012, 28832/56000 datapoints
2025-03-04 14:52:56,120 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-04 14:52:56,233 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-04 14:52:56,344 - INFO - training batch 1051, loss: 0.009, 33632/56000 datapoints
2025-03-04 14:52:56,465 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:52:56,617 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:52:56,848 - INFO - training batch 1201, loss: 0.018, 38432/56000 datapoints
2025-03-04 14:52:57,021 - INFO - training batch 1251, loss: 0.202, 40032/56000 datapoints
2025-03-04 14:52:57,159 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:52:57,284 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-04 14:52:57,501 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-04 14:52:57,680 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:52:57,854 - INFO - training batch 1501, loss: 0.034, 48032/56000 datapoints
2025-03-04 14:52:58,029 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-04 14:52:58,180 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-04 14:52:58,326 - INFO - training batch 1651, loss: 0.018, 52832/56000 datapoints
2025-03-04 14:52:58,443 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:52:58,566 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-04 14:52:58,624 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-04 14:52:58,685 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-04 14:52:58,746 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-04 14:52:58,813 - INFO - validation batch 201, loss: 0.009, 6432/13984 datapoints
2025-03-04 14:52:58,861 - INFO - validation batch 251, loss: 0.006, 8032/13984 datapoints
2025-03-04 14:52:59,158 - INFO - validation batch 301, loss: 0.010, 9632/13984 datapoints
2025-03-04 14:52:59,769 - INFO - validation batch 351, loss: 0.006, 11232/13984 datapoints
2025-03-04 14:52:59,870 - INFO - validation batch 401, loss: 0.109, 12832/13984 datapoints
2025-03-04 14:52:59,971 - INFO - Epoch 28/50 done
2025-03-04 14:52:59,972 - INFO - Beginning epoch 29/50
2025-03-04 14:52:59,988 - INFO - training batch 1, loss: 0.155, 32/56000 datapoints
2025-03-04 14:53:00,330 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-04 14:53:00,579 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:53:00,783 - INFO - training batch 151, loss: 0.058, 4832/56000 datapoints
2025-03-04 14:53:01,171 - INFO - training batch 201, loss: 0.031, 6432/56000 datapoints
2025-03-04 14:53:01,414 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:53:01,685 - INFO - training batch 301, loss: 0.020, 9632/56000 datapoints
2025-03-04 14:53:02,925 - INFO - training batch 351, loss: 0.068, 11232/56000 datapoints
2025-03-04 14:53:03,262 - INFO - training batch 401, loss: 0.066, 12832/56000 datapoints
2025-03-04 14:53:03,415 - INFO - training batch 451, loss: 0.030, 14432/56000 datapoints
2025-03-04 14:53:03,571 - INFO - training batch 501, loss: 0.009, 16032/56000 datapoints
2025-03-04 14:53:03,717 - INFO - training batch 551, loss: 0.068, 17632/56000 datapoints
2025-03-04 14:53:03,863 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:53:04,170 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:53:04,411 - INFO - training batch 701, loss: 0.253, 22432/56000 datapoints
2025-03-04 14:53:04,616 - INFO - training batch 751, loss: 0.008, 24032/56000 datapoints
2025-03-04 14:53:04,783 - INFO - training batch 801, loss: 0.017, 25632/56000 datapoints
2025-03-04 14:53:04,947 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:53:05,763 - INFO - training batch 901, loss: 0.012, 28832/56000 datapoints
2025-03-04 14:53:06,628 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-04 14:53:07,863 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-04 14:53:08,129 - INFO - training batch 1051, loss: 0.008, 33632/56000 datapoints
2025-03-04 14:53:08,736 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:53:08,979 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:53:09,201 - INFO - training batch 1201, loss: 0.018, 38432/56000 datapoints
2025-03-04 14:53:09,333 - INFO - training batch 1251, loss: 0.201, 40032/56000 datapoints
2025-03-04 14:53:09,467 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:53:09,633 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-04 14:53:09,786 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-04 14:53:09,983 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:53:10,155 - INFO - training batch 1501, loss: 0.033, 48032/56000 datapoints
2025-03-04 14:53:10,335 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-04 14:53:11,202 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-04 14:53:11,680 - INFO - training batch 1651, loss: 0.018, 52832/56000 datapoints
2025-03-04 14:53:11,884 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:53:12,286 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-04 14:53:12,718 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-04 14:53:13,087 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-04 14:53:13,367 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-04 14:53:13,572 - INFO - validation batch 201, loss: 0.009, 6432/13984 datapoints
2025-03-04 14:53:13,885 - INFO - validation batch 251, loss: 0.006, 8032/13984 datapoints
2025-03-04 14:53:14,189 - INFO - validation batch 301, loss: 0.010, 9632/13984 datapoints
2025-03-04 14:53:14,358 - INFO - validation batch 351, loss: 0.006, 11232/13984 datapoints
2025-03-04 14:53:14,525 - INFO - validation batch 401, loss: 0.109, 12832/13984 datapoints
2025-03-04 14:53:14,654 - INFO - Epoch 29/50 done
2025-03-04 14:53:14,656 - INFO - Beginning epoch 30/50
2025-03-04 14:53:14,661 - INFO - training batch 1, loss: 0.155, 32/56000 datapoints
2025-03-04 14:53:14,897 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-04 14:53:15,056 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:53:15,213 - INFO - training batch 151, loss: 0.057, 4832/56000 datapoints
2025-03-04 14:53:15,431 - INFO - training batch 201, loss: 0.030, 6432/56000 datapoints
2025-03-04 14:53:16,261 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:53:16,829 - INFO - training batch 301, loss: 0.021, 9632/56000 datapoints
2025-03-04 14:53:18,043 - INFO - training batch 351, loss: 0.068, 11232/56000 datapoints
2025-03-04 14:53:19,033 - INFO - training batch 401, loss: 0.065, 12832/56000 datapoints
2025-03-04 14:53:19,621 - INFO - training batch 451, loss: 0.029, 14432/56000 datapoints
2025-03-04 14:53:20,071 - INFO - training batch 501, loss: 0.008, 16032/56000 datapoints
2025-03-04 14:53:20,247 - INFO - training batch 551, loss: 0.067, 17632/56000 datapoints
2025-03-04 14:53:20,435 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:53:20,626 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:53:20,969 - INFO - training batch 701, loss: 0.253, 22432/56000 datapoints
2025-03-04 14:53:21,182 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-04 14:53:21,358 - INFO - training batch 801, loss: 0.017, 25632/56000 datapoints
2025-03-04 14:53:21,516 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:53:22,294 - INFO - training batch 901, loss: 0.011, 28832/56000 datapoints
2025-03-04 14:53:22,889 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-04 14:53:23,304 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-04 14:53:23,681 - INFO - training batch 1051, loss: 0.008, 33632/56000 datapoints
2025-03-04 14:53:24,013 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:53:24,356 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:53:25,457 - INFO - training batch 1201, loss: 0.017, 38432/56000 datapoints
2025-03-04 14:53:25,662 - INFO - training batch 1251, loss: 0.199, 40032/56000 datapoints
2025-03-04 14:53:26,239 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:53:26,877 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-04 14:53:28,044 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-04 14:53:29,080 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:53:29,434 - INFO - training batch 1501, loss: 0.033, 48032/56000 datapoints
2025-03-04 14:53:29,814 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:53:30,123 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-04 14:53:30,272 - INFO - training batch 1651, loss: 0.018, 52832/56000 datapoints
2025-03-04 14:53:30,412 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:53:30,554 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-04 14:53:30,629 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-04 14:53:30,714 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-04 14:53:30,825 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-04 14:53:31,507 - INFO - validation batch 201, loss: 0.009, 6432/13984 datapoints
2025-03-04 14:53:31,627 - INFO - validation batch 251, loss: 0.006, 8032/13984 datapoints
2025-03-04 14:53:31,786 - INFO - validation batch 301, loss: 0.010, 9632/13984 datapoints
2025-03-04 14:53:31,930 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-04 14:53:32,027 - INFO - validation batch 401, loss: 0.109, 12832/13984 datapoints
2025-03-04 14:53:32,091 - INFO - Epoch 30/50 done
2025-03-04 14:53:32,091 - INFO - Beginning epoch 31/50
2025-03-04 14:53:32,105 - INFO - training batch 1, loss: 0.156, 32/56000 datapoints
2025-03-04 14:53:32,321 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-04 14:53:32,549 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:53:32,771 - INFO - training batch 151, loss: 0.057, 4832/56000 datapoints
2025-03-04 14:53:33,163 - INFO - training batch 201, loss: 0.030, 6432/56000 datapoints
2025-03-04 14:53:33,462 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:53:33,714 - INFO - training batch 301, loss: 0.021, 9632/56000 datapoints
2025-03-04 14:53:33,930 - INFO - training batch 351, loss: 0.067, 11232/56000 datapoints
2025-03-04 14:53:34,070 - INFO - training batch 401, loss: 0.064, 12832/56000 datapoints
2025-03-04 14:53:35,531 - INFO - training batch 451, loss: 0.029, 14432/56000 datapoints
2025-03-04 14:53:36,280 - INFO - training batch 501, loss: 0.008, 16032/56000 datapoints
2025-03-04 14:53:37,185 - INFO - training batch 551, loss: 0.066, 17632/56000 datapoints
2025-03-04 14:53:38,816 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:53:39,867 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:53:40,862 - INFO - training batch 701, loss: 0.252, 22432/56000 datapoints
2025-03-04 14:53:41,663 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-04 14:53:42,234 - INFO - training batch 801, loss: 0.016, 25632/56000 datapoints
2025-03-04 14:53:42,474 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:53:42,612 - INFO - training batch 901, loss: 0.011, 28832/56000 datapoints
2025-03-04 14:53:42,770 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-04 14:53:42,980 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:53:43,264 - INFO - training batch 1051, loss: 0.008, 33632/56000 datapoints
2025-03-04 14:53:43,940 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:53:44,169 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:53:44,712 - INFO - training batch 1201, loss: 0.017, 38432/56000 datapoints
2025-03-04 14:53:45,077 - INFO - training batch 1251, loss: 0.198, 40032/56000 datapoints
2025-03-04 14:53:45,310 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:53:45,624 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-04 14:53:45,884 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-04 14:53:46,070 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:53:46,382 - INFO - training batch 1501, loss: 0.032, 48032/56000 datapoints
2025-03-04 14:53:46,582 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:53:46,795 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-04 14:53:47,217 - INFO - training batch 1651, loss: 0.017, 52832/56000 datapoints
2025-03-04 14:53:48,163 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:53:48,649 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-04 14:53:48,716 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-04 14:53:48,780 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-04 14:53:48,896 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-04 14:53:49,186 - INFO - validation batch 201, loss: 0.009, 6432/13984 datapoints
2025-03-04 14:53:49,357 - INFO - validation batch 251, loss: 0.006, 8032/13984 datapoints
2025-03-04 14:53:49,528 - INFO - validation batch 301, loss: 0.010, 9632/13984 datapoints
2025-03-04 14:53:49,632 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-04 14:53:49,865 - INFO - validation batch 401, loss: 0.108, 12832/13984 datapoints
2025-03-04 14:53:50,057 - INFO - Epoch 31/50 done
2025-03-04 14:53:50,057 - INFO - Beginning epoch 32/50
2025-03-04 14:53:50,088 - INFO - training batch 1, loss: 0.156, 32/56000 datapoints
2025-03-04 14:53:50,610 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-04 14:53:51,182 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:53:51,517 - INFO - training batch 151, loss: 0.056, 4832/56000 datapoints
2025-03-04 14:53:52,470 - INFO - training batch 201, loss: 0.029, 6432/56000 datapoints
2025-03-04 14:53:52,825 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:53:53,235 - INFO - training batch 301, loss: 0.021, 9632/56000 datapoints
2025-03-04 14:53:53,460 - INFO - training batch 351, loss: 0.067, 11232/56000 datapoints
2025-03-04 14:53:53,634 - INFO - training batch 401, loss: 0.063, 12832/56000 datapoints
2025-03-04 14:53:53,885 - INFO - training batch 451, loss: 0.028, 14432/56000 datapoints
2025-03-04 14:53:54,291 - INFO - training batch 501, loss: 0.008, 16032/56000 datapoints
2025-03-04 14:53:54,580 - INFO - training batch 551, loss: 0.065, 17632/56000 datapoints
2025-03-04 14:53:54,916 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:53:55,266 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:53:55,685 - INFO - training batch 701, loss: 0.251, 22432/56000 datapoints
2025-03-04 14:53:55,838 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-04 14:53:56,331 - INFO - training batch 801, loss: 0.016, 25632/56000 datapoints
2025-03-04 14:53:56,600 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:53:57,086 - INFO - training batch 901, loss: 0.011, 28832/56000 datapoints
2025-03-04 14:53:57,266 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-04 14:53:57,484 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:53:57,632 - INFO - training batch 1051, loss: 0.008, 33632/56000 datapoints
2025-03-04 14:53:57,817 - INFO - training batch 1101, loss: 0.079, 35232/56000 datapoints
2025-03-04 14:53:58,082 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:53:58,669 - INFO - training batch 1201, loss: 0.017, 38432/56000 datapoints
2025-03-04 14:53:58,934 - INFO - training batch 1251, loss: 0.196, 40032/56000 datapoints
2025-03-04 14:53:59,177 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:53:59,427 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-04 14:53:59,585 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-04 14:53:59,805 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:54:00,118 - INFO - training batch 1501, loss: 0.032, 48032/56000 datapoints
2025-03-04 14:54:00,382 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:54:00,528 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-04 14:54:00,665 - INFO - training batch 1651, loss: 0.017, 52832/56000 datapoints
2025-03-04 14:54:00,806 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:54:00,985 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-04 14:54:01,036 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-04 14:54:01,086 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-04 14:54:01,132 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-04 14:54:01,183 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-04 14:54:01,233 - INFO - validation batch 251, loss: 0.006, 8032/13984 datapoints
2025-03-04 14:54:01,287 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-04 14:54:01,372 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-04 14:54:01,428 - INFO - validation batch 401, loss: 0.108, 12832/13984 datapoints
2025-03-04 14:54:01,478 - INFO - Epoch 32/50 done
2025-03-04 14:54:01,481 - INFO - Beginning epoch 33/50
2025-03-04 14:54:01,484 - INFO - training batch 1, loss: 0.157, 32/56000 datapoints
2025-03-04 14:54:01,653 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-04 14:54:01,816 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:54:02,082 - INFO - training batch 151, loss: 0.056, 4832/56000 datapoints
2025-03-04 14:54:02,431 - INFO - training batch 201, loss: 0.029, 6432/56000 datapoints
2025-03-04 14:54:02,787 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:54:03,003 - INFO - training batch 301, loss: 0.021, 9632/56000 datapoints
2025-03-04 14:54:03,135 - INFO - training batch 351, loss: 0.067, 11232/56000 datapoints
2025-03-04 14:54:03,283 - INFO - training batch 401, loss: 0.063, 12832/56000 datapoints
2025-03-04 14:54:03,421 - INFO - training batch 451, loss: 0.028, 14432/56000 datapoints
2025-03-04 14:54:03,538 - INFO - training batch 501, loss: 0.008, 16032/56000 datapoints
2025-03-04 14:54:03,654 - INFO - training batch 551, loss: 0.064, 17632/56000 datapoints
2025-03-04 14:54:03,772 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:54:03,976 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:54:04,071 - INFO - training batch 701, loss: 0.250, 22432/56000 datapoints
2025-03-04 14:54:04,183 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-04 14:54:04,280 - INFO - training batch 801, loss: 0.016, 25632/56000 datapoints
2025-03-04 14:54:04,378 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:54:04,475 - INFO - training batch 901, loss: 0.011, 28832/56000 datapoints
2025-03-04 14:54:04,587 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-04 14:54:04,702 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:54:04,880 - INFO - training batch 1051, loss: 0.008, 33632/56000 datapoints
2025-03-04 14:54:05,079 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:54:05,245 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:54:05,474 - INFO - training batch 1201, loss: 0.016, 38432/56000 datapoints
2025-03-04 14:54:05,638 - INFO - training batch 1251, loss: 0.194, 40032/56000 datapoints
2025-03-04 14:54:05,784 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:54:05,898 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-04 14:54:06,116 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:54:06,221 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:54:06,334 - INFO - training batch 1501, loss: 0.032, 48032/56000 datapoints
2025-03-04 14:54:06,448 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:54:06,565 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-04 14:54:06,699 - INFO - training batch 1651, loss: 0.017, 52832/56000 datapoints
2025-03-04 14:54:06,813 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:54:06,927 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-04 14:54:06,973 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-04 14:54:07,015 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-04 14:54:07,054 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-04 14:54:07,093 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-04 14:54:07,148 - INFO - validation batch 251, loss: 0.006, 8032/13984 datapoints
2025-03-04 14:54:07,239 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-04 14:54:07,311 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-04 14:54:07,606 - INFO - validation batch 401, loss: 0.108, 12832/13984 datapoints
2025-03-04 14:54:07,704 - INFO - Epoch 33/50 done
2025-03-04 14:54:07,704 - INFO - Beginning epoch 34/50
2025-03-04 14:54:07,710 - INFO - training batch 1, loss: 0.157, 32/56000 datapoints
2025-03-04 14:54:09,130 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-04 14:54:09,649 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:54:11,772 - INFO - training batch 151, loss: 0.056, 4832/56000 datapoints
2025-03-04 14:54:12,848 - INFO - training batch 201, loss: 0.029, 6432/56000 datapoints
2025-03-04 14:54:13,252 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:54:13,424 - INFO - training batch 301, loss: 0.021, 9632/56000 datapoints
2025-03-04 14:54:13,577 - INFO - training batch 351, loss: 0.066, 11232/56000 datapoints
2025-03-04 14:54:14,007 - INFO - training batch 401, loss: 0.062, 12832/56000 datapoints
2025-03-04 14:54:14,269 - INFO - training batch 451, loss: 0.028, 14432/56000 datapoints
2025-03-04 14:54:14,390 - INFO - training batch 501, loss: 0.008, 16032/56000 datapoints
2025-03-04 14:54:14,611 - INFO - training batch 551, loss: 0.064, 17632/56000 datapoints
2025-03-04 14:54:14,869 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:54:15,024 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:54:15,173 - INFO - training batch 701, loss: 0.250, 22432/56000 datapoints
2025-03-04 14:54:15,336 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-04 14:54:15,529 - INFO - training batch 801, loss: 0.015, 25632/56000 datapoints
2025-03-04 14:54:16,027 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:54:16,197 - INFO - training batch 901, loss: 0.010, 28832/56000 datapoints
2025-03-04 14:54:16,364 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-04 14:54:16,689 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:54:16,818 - INFO - training batch 1051, loss: 0.008, 33632/56000 datapoints
2025-03-04 14:54:16,997 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:54:17,147 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:54:17,290 - INFO - training batch 1201, loss: 0.016, 38432/56000 datapoints
2025-03-04 14:54:17,425 - INFO - training batch 1251, loss: 0.193, 40032/56000 datapoints
2025-03-04 14:54:17,945 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:54:18,120 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-04 14:54:18,362 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:54:18,500 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:54:18,726 - INFO - training batch 1501, loss: 0.032, 48032/56000 datapoints
2025-03-04 14:54:18,881 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:54:19,060 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-04 14:54:19,182 - INFO - training batch 1651, loss: 0.017, 52832/56000 datapoints
2025-03-04 14:54:19,309 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:54:19,468 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-04 14:54:19,535 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-04 14:54:19,578 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-04 14:54:19,631 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-04 14:54:19,678 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-04 14:54:19,727 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-04 14:54:19,776 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-04 14:54:19,828 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-04 14:54:19,882 - INFO - validation batch 401, loss: 0.107, 12832/13984 datapoints
2025-03-04 14:54:19,948 - INFO - Epoch 34/50 done
2025-03-04 14:54:19,949 - INFO - Beginning epoch 35/50
2025-03-04 14:54:19,953 - INFO - training batch 1, loss: 0.157, 32/56000 datapoints
2025-03-04 14:54:20,088 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-04 14:54:20,225 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:54:20,471 - INFO - training batch 151, loss: 0.055, 4832/56000 datapoints
2025-03-04 14:54:20,622 - INFO - training batch 201, loss: 0.028, 6432/56000 datapoints
2025-03-04 14:54:20,827 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:54:21,275 - INFO - training batch 301, loss: 0.021, 9632/56000 datapoints
2025-03-04 14:54:21,622 - INFO - training batch 351, loss: 0.066, 11232/56000 datapoints
2025-03-04 14:54:22,198 - INFO - training batch 401, loss: 0.061, 12832/56000 datapoints
2025-03-04 14:54:23,012 - INFO - training batch 451, loss: 0.027, 14432/56000 datapoints
2025-03-04 14:54:23,335 - INFO - training batch 501, loss: 0.008, 16032/56000 datapoints
2025-03-04 14:54:23,490 - INFO - training batch 551, loss: 0.063, 17632/56000 datapoints
2025-03-04 14:54:23,603 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:54:23,764 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:54:24,328 - INFO - training batch 701, loss: 0.249, 22432/56000 datapoints
2025-03-04 14:54:24,635 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-04 14:54:24,779 - INFO - training batch 801, loss: 0.015, 25632/56000 datapoints
2025-03-04 14:54:25,132 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:54:25,323 - INFO - training batch 901, loss: 0.010, 28832/56000 datapoints
2025-03-04 14:54:25,441 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-04 14:54:25,587 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:54:25,730 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-04 14:54:25,897 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:54:26,232 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:54:27,122 - INFO - training batch 1201, loss: 0.016, 38432/56000 datapoints
2025-03-04 14:54:28,079 - INFO - training batch 1251, loss: 0.191, 40032/56000 datapoints
2025-03-04 14:54:29,514 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:54:30,526 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-04 14:54:31,000 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:54:31,509 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:54:32,006 - INFO - training batch 1501, loss: 0.031, 48032/56000 datapoints
2025-03-04 14:54:32,432 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:54:32,710 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-04 14:54:33,041 - INFO - training batch 1651, loss: 0.017, 52832/56000 datapoints
2025-03-04 14:54:33,279 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:54:33,508 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-04 14:54:33,606 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-04 14:54:33,731 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-04 14:54:33,865 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-04 14:54:34,044 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-04 14:54:34,108 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-04 14:54:34,181 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-04 14:54:34,234 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-04 14:54:34,299 - INFO - validation batch 401, loss: 0.107, 12832/13984 datapoints
2025-03-04 14:54:34,346 - INFO - Epoch 35/50 done
2025-03-04 14:54:34,346 - INFO - Beginning epoch 36/50
2025-03-04 14:54:34,354 - INFO - training batch 1, loss: 0.158, 32/56000 datapoints
2025-03-04 14:54:34,490 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-04 14:54:36,369 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:54:36,837 - INFO - training batch 151, loss: 0.055, 4832/56000 datapoints
2025-03-04 14:54:37,003 - INFO - training batch 201, loss: 0.028, 6432/56000 datapoints
2025-03-04 14:54:37,261 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:54:37,511 - INFO - training batch 301, loss: 0.021, 9632/56000 datapoints
2025-03-04 14:54:37,680 - INFO - training batch 351, loss: 0.066, 11232/56000 datapoints
2025-03-04 14:54:37,803 - INFO - training batch 401, loss: 0.060, 12832/56000 datapoints
2025-03-04 14:54:37,965 - INFO - training batch 451, loss: 0.027, 14432/56000 datapoints
2025-03-04 14:54:38,172 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-04 14:54:38,409 - INFO - training batch 551, loss: 0.062, 17632/56000 datapoints
2025-03-04 14:54:38,552 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-04 14:54:38,683 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:54:38,883 - INFO - training batch 701, loss: 0.248, 22432/56000 datapoints
2025-03-04 14:54:39,041 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-04 14:54:39,162 - INFO - training batch 801, loss: 0.015, 25632/56000 datapoints
2025-03-04 14:54:39,290 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:54:39,474 - INFO - training batch 901, loss: 0.010, 28832/56000 datapoints
2025-03-04 14:54:39,614 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-04 14:54:39,725 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:54:39,837 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-04 14:54:39,964 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:54:40,070 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:54:40,175 - INFO - training batch 1201, loss: 0.016, 38432/56000 datapoints
2025-03-04 14:54:40,279 - INFO - training batch 1251, loss: 0.190, 40032/56000 datapoints
2025-03-04 14:54:40,384 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:54:40,496 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-04 14:54:40,601 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:54:41,014 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:54:41,139 - INFO - training batch 1501, loss: 0.031, 48032/56000 datapoints
2025-03-04 14:54:41,258 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:54:41,374 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-04 14:54:41,479 - INFO - training batch 1651, loss: 0.017, 52832/56000 datapoints
2025-03-04 14:54:41,587 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:54:41,700 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-04 14:54:41,749 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-04 14:54:41,794 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:54:41,830 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:54:41,883 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-04 14:54:41,921 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-04 14:54:41,952 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-04 14:54:41,983 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-04 14:54:42,021 - INFO - validation batch 401, loss: 0.106, 12832/13984 datapoints
2025-03-04 14:54:42,053 - INFO - Epoch 36/50 done
2025-03-04 14:54:42,062 - INFO - Beginning epoch 37/50
2025-03-04 14:54:42,066 - INFO - training batch 1, loss: 0.158, 32/56000 datapoints
2025-03-04 14:54:42,189 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-04 14:54:42,286 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:54:42,414 - INFO - training batch 151, loss: 0.055, 4832/56000 datapoints
2025-03-04 14:54:42,540 - INFO - training batch 201, loss: 0.027, 6432/56000 datapoints
2025-03-04 14:54:42,629 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:54:42,706 - INFO - training batch 301, loss: 0.022, 9632/56000 datapoints
2025-03-04 14:54:42,783 - INFO - training batch 351, loss: 0.066, 11232/56000 datapoints
2025-03-04 14:54:42,929 - INFO - training batch 401, loss: 0.060, 12832/56000 datapoints
2025-03-04 14:54:43,005 - INFO - training batch 451, loss: 0.027, 14432/56000 datapoints
2025-03-04 14:54:43,078 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-04 14:54:43,154 - INFO - training batch 551, loss: 0.061, 17632/56000 datapoints
2025-03-04 14:54:43,221 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:54:43,303 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:54:43,465 - INFO - training batch 701, loss: 0.247, 22432/56000 datapoints
2025-03-04 14:54:43,610 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-04 14:54:43,713 - INFO - training batch 801, loss: 0.015, 25632/56000 datapoints
2025-03-04 14:54:43,803 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:54:43,910 - INFO - training batch 901, loss: 0.010, 28832/56000 datapoints
2025-03-04 14:54:44,009 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-04 14:54:44,120 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:54:44,240 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-04 14:54:44,626 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:54:44,723 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:54:44,810 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-04 14:54:44,909 - INFO - training batch 1251, loss: 0.188, 40032/56000 datapoints
2025-03-04 14:54:45,012 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:54:45,136 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:54:45,222 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:54:45,316 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:54:45,417 - INFO - training batch 1501, loss: 0.031, 48032/56000 datapoints
2025-03-04 14:54:45,503 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:54:45,592 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-04 14:54:45,695 - INFO - training batch 1651, loss: 0.017, 52832/56000 datapoints
2025-03-04 14:54:45,792 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:54:45,884 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:54:45,922 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-04 14:54:45,955 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:54:45,980 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:54:46,008 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-04 14:54:46,031 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-04 14:54:46,056 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-04 14:54:46,078 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-04 14:54:46,099 - INFO - validation batch 401, loss: 0.106, 12832/13984 datapoints
2025-03-04 14:54:46,115 - INFO - Epoch 37/50 done
2025-03-04 14:54:46,115 - INFO - Beginning epoch 38/50
2025-03-04 14:54:46,117 - INFO - training batch 1, loss: 0.159, 32/56000 datapoints
2025-03-04 14:54:46,184 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-04 14:54:46,257 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:54:46,346 - INFO - training batch 151, loss: 0.054, 4832/56000 datapoints
2025-03-04 14:54:46,443 - INFO - training batch 201, loss: 0.027, 6432/56000 datapoints
2025-03-04 14:54:46,537 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:54:46,620 - INFO - training batch 301, loss: 0.022, 9632/56000 datapoints
2025-03-04 14:54:46,693 - INFO - training batch 351, loss: 0.065, 11232/56000 datapoints
2025-03-04 14:54:46,772 - INFO - training batch 401, loss: 0.059, 12832/56000 datapoints
2025-03-04 14:54:46,843 - INFO - training batch 451, loss: 0.026, 14432/56000 datapoints
2025-03-04 14:54:46,934 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-04 14:54:47,279 - INFO - training batch 551, loss: 0.060, 17632/56000 datapoints
2025-03-04 14:54:47,665 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:54:49,321 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:54:49,790 - INFO - training batch 701, loss: 0.245, 22432/56000 datapoints
2025-03-04 14:54:50,336 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-04 14:54:50,707 - INFO - training batch 801, loss: 0.014, 25632/56000 datapoints
2025-03-04 14:54:51,007 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:54:51,178 - INFO - training batch 901, loss: 0.009, 28832/56000 datapoints
2025-03-04 14:54:51,321 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-04 14:54:51,521 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:54:51,736 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-04 14:54:51,861 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:54:52,077 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:54:52,200 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-04 14:54:52,350 - INFO - training batch 1251, loss: 0.186, 40032/56000 datapoints
2025-03-04 14:54:52,496 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:54:52,620 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:54:52,748 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:54:52,875 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:54:53,041 - INFO - training batch 1501, loss: 0.030, 48032/56000 datapoints
2025-03-04 14:54:54,256 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:54:54,445 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-04 14:54:54,668 - INFO - training batch 1651, loss: 0.016, 52832/56000 datapoints
2025-03-04 14:54:55,051 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:54:55,536 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:54:55,597 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-04 14:54:55,656 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:54:55,708 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:54:55,748 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-04 14:54:55,791 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-04 14:54:55,830 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-04 14:54:55,872 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-04 14:54:55,927 - INFO - validation batch 401, loss: 0.105, 12832/13984 datapoints
2025-03-04 14:54:55,985 - INFO - Epoch 38/50 done
2025-03-04 14:54:55,989 - INFO - Beginning epoch 39/50
2025-03-04 14:54:55,997 - INFO - training batch 1, loss: 0.159, 32/56000 datapoints
2025-03-04 14:54:56,348 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-04 14:54:57,319 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:54:57,618 - INFO - training batch 151, loss: 0.054, 4832/56000 datapoints
2025-03-04 14:54:57,725 - INFO - training batch 201, loss: 0.027, 6432/56000 datapoints
2025-03-04 14:54:57,841 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:54:57,962 - INFO - training batch 301, loss: 0.022, 9632/56000 datapoints
2025-03-04 14:54:58,087 - INFO - training batch 351, loss: 0.065, 11232/56000 datapoints
2025-03-04 14:54:58,191 - INFO - training batch 401, loss: 0.058, 12832/56000 datapoints
2025-03-04 14:54:58,303 - INFO - training batch 451, loss: 0.026, 14432/56000 datapoints
2025-03-04 14:54:58,549 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-04 14:54:58,659 - INFO - training batch 551, loss: 0.059, 17632/56000 datapoints
2025-03-04 14:54:58,757 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:54:58,860 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:54:58,966 - INFO - training batch 701, loss: 0.244, 22432/56000 datapoints
2025-03-04 14:54:59,070 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-04 14:54:59,160 - INFO - training batch 801, loss: 0.014, 25632/56000 datapoints
2025-03-04 14:54:59,258 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:54:59,364 - INFO - training batch 901, loss: 0.009, 28832/56000 datapoints
2025-03-04 14:54:59,473 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-04 14:54:59,576 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:54:59,679 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-04 14:54:59,776 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:54:59,881 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:54:59,976 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-04 14:55:00,092 - INFO - training batch 1251, loss: 0.185, 40032/56000 datapoints
2025-03-04 14:55:00,205 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:55:00,319 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:55:00,410 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:55:00,503 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:55:00,651 - INFO - training batch 1501, loss: 0.030, 48032/56000 datapoints
2025-03-04 14:55:00,774 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:55:00,876 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-04 14:55:00,984 - INFO - training batch 1651, loss: 0.016, 52832/56000 datapoints
2025-03-04 14:55:01,094 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:55:01,210 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:01,258 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-04 14:55:01,300 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:01,341 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:55:01,383 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-04 14:55:01,423 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-04 14:55:01,464 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-04 14:55:01,505 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:01,541 - INFO - validation batch 401, loss: 0.104, 12832/13984 datapoints
2025-03-04 14:55:01,570 - INFO - Epoch 39/50 done
2025-03-04 14:55:01,573 - INFO - Beginning epoch 40/50
2025-03-04 14:55:01,576 - INFO - training batch 1, loss: 0.159, 32/56000 datapoints
2025-03-04 14:55:01,663 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-04 14:55:02,795 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:55:02,886 - INFO - training batch 151, loss: 0.054, 4832/56000 datapoints
2025-03-04 14:55:02,981 - INFO - training batch 201, loss: 0.027, 6432/56000 datapoints
2025-03-04 14:55:03,074 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:55:03,180 - INFO - training batch 301, loss: 0.022, 9632/56000 datapoints
2025-03-04 14:55:03,320 - INFO - training batch 351, loss: 0.065, 11232/56000 datapoints
2025-03-04 14:55:03,416 - INFO - training batch 401, loss: 0.057, 12832/56000 datapoints
2025-03-04 14:55:03,501 - INFO - training batch 451, loss: 0.026, 14432/56000 datapoints
2025-03-04 14:55:03,576 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-04 14:55:03,659 - INFO - training batch 551, loss: 0.058, 17632/56000 datapoints
2025-03-04 14:55:03,768 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:55:03,953 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:55:04,118 - INFO - training batch 701, loss: 0.243, 22432/56000 datapoints
2025-03-04 14:55:04,268 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-04 14:55:04,632 - INFO - training batch 801, loss: 0.014, 25632/56000 datapoints
2025-03-04 14:55:04,780 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:55:04,935 - INFO - training batch 901, loss: 0.009, 28832/56000 datapoints
2025-03-04 14:55:05,018 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-04 14:55:05,105 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:55:05,187 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-04 14:55:05,271 - INFO - training batch 1101, loss: 0.078, 35232/56000 datapoints
2025-03-04 14:55:05,360 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:55:05,453 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-04 14:55:05,538 - INFO - training batch 1251, loss: 0.183, 40032/56000 datapoints
2025-03-04 14:55:05,631 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:55:05,743 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:55:05,856 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:55:05,972 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:55:06,069 - INFO - training batch 1501, loss: 0.030, 48032/56000 datapoints
2025-03-04 14:55:06,218 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:55:06,313 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-04 14:55:06,395 - INFO - training batch 1651, loss: 0.016, 52832/56000 datapoints
2025-03-04 14:55:06,485 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:55:06,575 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:06,601 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-04 14:55:06,625 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:06,648 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:55:06,672 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-04 14:55:06,696 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-04 14:55:06,727 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-04 14:55:06,759 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:06,791 - INFO - validation batch 401, loss: 0.104, 12832/13984 datapoints
2025-03-04 14:55:06,814 - INFO - Epoch 40/50 done
2025-03-04 14:55:06,814 - INFO - Beginning epoch 41/50
2025-03-04 14:55:06,817 - INFO - training batch 1, loss: 0.160, 32/56000 datapoints
2025-03-04 14:55:06,921 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-04 14:55:07,050 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:55:07,136 - INFO - training batch 151, loss: 0.053, 4832/56000 datapoints
2025-03-04 14:55:07,260 - INFO - training batch 201, loss: 0.026, 6432/56000 datapoints
2025-03-04 14:55:07,344 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:55:07,439 - INFO - training batch 301, loss: 0.022, 9632/56000 datapoints
2025-03-04 14:55:07,524 - INFO - training batch 351, loss: 0.065, 11232/56000 datapoints
2025-03-04 14:55:07,601 - INFO - training batch 401, loss: 0.057, 12832/56000 datapoints
2025-03-04 14:55:07,684 - INFO - training batch 451, loss: 0.025, 14432/56000 datapoints
2025-03-04 14:55:07,765 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-04 14:55:07,848 - INFO - training batch 551, loss: 0.058, 17632/56000 datapoints
2025-03-04 14:55:07,937 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:55:08,028 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:55:08,121 - INFO - training batch 701, loss: 0.242, 22432/56000 datapoints
2025-03-04 14:55:08,238 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-04 14:55:08,341 - INFO - training batch 801, loss: 0.014, 25632/56000 datapoints
2025-03-04 14:55:08,428 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-04 14:55:08,511 - INFO - training batch 901, loss: 0.009, 28832/56000 datapoints
2025-03-04 14:55:08,595 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-04 14:55:08,687 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:55:08,778 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-04 14:55:08,880 - INFO - training batch 1101, loss: 0.077, 35232/56000 datapoints
2025-03-04 14:55:08,994 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:55:09,082 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-04 14:55:09,164 - INFO - training batch 1251, loss: 0.181, 40032/56000 datapoints
2025-03-04 14:55:09,242 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:55:09,314 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:55:09,390 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:55:09,469 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-04 14:55:09,544 - INFO - training batch 1501, loss: 0.030, 48032/56000 datapoints
2025-03-04 14:55:09,623 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:55:09,710 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-04 14:55:09,779 - INFO - training batch 1651, loss: 0.016, 52832/56000 datapoints
2025-03-04 14:55:09,858 - INFO - training batch 1701, loss: 0.094, 54432/56000 datapoints
2025-03-04 14:55:09,950 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:09,972 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-04 14:55:09,991 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:10,015 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:55:10,036 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-04 14:55:10,056 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-04 14:55:10,076 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-04 14:55:10,095 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:10,118 - INFO - validation batch 401, loss: 0.103, 12832/13984 datapoints
2025-03-04 14:55:10,132 - INFO - Epoch 41/50 done
2025-03-04 14:55:10,132 - INFO - Beginning epoch 42/50
2025-03-04 14:55:10,135 - INFO - training batch 1, loss: 0.160, 32/56000 datapoints
2025-03-04 14:55:10,206 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-04 14:55:10,277 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:55:10,360 - INFO - training batch 151, loss: 0.053, 4832/56000 datapoints
2025-03-04 14:55:10,440 - INFO - training batch 201, loss: 0.026, 6432/56000 datapoints
2025-03-04 14:55:10,515 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:55:10,586 - INFO - training batch 301, loss: 0.022, 9632/56000 datapoints
2025-03-04 14:55:10,662 - INFO - training batch 351, loss: 0.065, 11232/56000 datapoints
2025-03-04 14:55:10,734 - INFO - training batch 401, loss: 0.056, 12832/56000 datapoints
2025-03-04 14:55:10,809 - INFO - training batch 451, loss: 0.025, 14432/56000 datapoints
2025-03-04 14:55:10,879 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-04 14:55:11,141 - INFO - training batch 551, loss: 0.057, 17632/56000 datapoints
2025-03-04 14:55:11,439 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:55:11,593 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:55:11,735 - INFO - training batch 701, loss: 0.241, 22432/56000 datapoints
2025-03-04 14:55:11,881 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-04 14:55:12,044 - INFO - training batch 801, loss: 0.013, 25632/56000 datapoints
2025-03-04 14:55:12,218 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-04 14:55:12,368 - INFO - training batch 901, loss: 0.009, 28832/56000 datapoints
2025-03-04 14:55:12,497 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-04 14:55:12,636 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:55:12,772 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-04 14:55:12,894 - INFO - training batch 1101, loss: 0.077, 35232/56000 datapoints
2025-03-04 14:55:13,060 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:55:13,181 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-04 14:55:13,309 - INFO - training batch 1251, loss: 0.180, 40032/56000 datapoints
2025-03-04 14:55:13,507 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:55:13,616 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:55:13,728 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:55:13,851 - INFO - training batch 1451, loss: 0.002, 46432/56000 datapoints
2025-03-04 14:55:13,999 - INFO - training batch 1501, loss: 0.029, 48032/56000 datapoints
2025-03-04 14:55:14,183 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:55:14,306 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-04 14:55:14,452 - INFO - training batch 1651, loss: 0.016, 52832/56000 datapoints
2025-03-04 14:55:14,559 - INFO - training batch 1701, loss: 0.093, 54432/56000 datapoints
2025-03-04 14:55:14,858 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:14,934 - INFO - validation batch 51, loss: 0.017, 1632/13984 datapoints
2025-03-04 14:55:15,006 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:15,070 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:55:15,123 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-04 14:55:15,178 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-04 14:55:15,350 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-04 14:55:15,535 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:15,648 - INFO - validation batch 401, loss: 0.103, 12832/13984 datapoints
2025-03-04 14:55:15,704 - INFO - Epoch 42/50 done
2025-03-04 14:55:15,705 - INFO - Beginning epoch 43/50
2025-03-04 14:55:15,709 - INFO - training batch 1, loss: 0.160, 32/56000 datapoints
2025-03-04 14:55:15,896 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-04 14:55:16,392 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:55:16,537 - INFO - training batch 151, loss: 0.053, 4832/56000 datapoints
2025-03-04 14:55:16,670 - INFO - training batch 201, loss: 0.026, 6432/56000 datapoints
2025-03-04 14:55:16,862 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:55:17,108 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-04 14:55:17,309 - INFO - training batch 351, loss: 0.064, 11232/56000 datapoints
2025-03-04 14:55:17,464 - INFO - training batch 401, loss: 0.055, 12832/56000 datapoints
2025-03-04 14:55:17,739 - INFO - training batch 451, loss: 0.025, 14432/56000 datapoints
2025-03-04 14:55:17,855 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-04 14:55:17,977 - INFO - training batch 551, loss: 0.056, 17632/56000 datapoints
2025-03-04 14:55:18,079 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:55:18,181 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:55:18,280 - INFO - training batch 701, loss: 0.240, 22432/56000 datapoints
2025-03-04 14:55:18,380 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-04 14:55:18,490 - INFO - training batch 801, loss: 0.013, 25632/56000 datapoints
2025-03-04 14:55:18,592 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-04 14:55:18,693 - INFO - training batch 901, loss: 0.009, 28832/56000 datapoints
2025-03-04 14:55:18,831 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-04 14:55:18,985 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:55:19,098 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-04 14:55:19,208 - INFO - training batch 1101, loss: 0.077, 35232/56000 datapoints
2025-03-04 14:55:19,335 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:55:19,496 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-04 14:55:19,622 - INFO - training batch 1251, loss: 0.178, 40032/56000 datapoints
2025-03-04 14:55:19,738 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:55:19,842 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:55:19,955 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:55:20,067 - INFO - training batch 1451, loss: 0.002, 46432/56000 datapoints
2025-03-04 14:55:20,163 - INFO - training batch 1501, loss: 0.029, 48032/56000 datapoints
2025-03-04 14:55:20,271 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:55:20,367 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-04 14:55:20,459 - INFO - training batch 1651, loss: 0.016, 52832/56000 datapoints
2025-03-04 14:55:20,538 - INFO - training batch 1701, loss: 0.093, 54432/56000 datapoints
2025-03-04 14:55:20,614 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:20,634 - INFO - validation batch 51, loss: 0.017, 1632/13984 datapoints
2025-03-04 14:55:20,656 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:20,677 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:55:20,700 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-04 14:55:20,722 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-04 14:55:20,743 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-04 14:55:20,765 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:20,784 - INFO - validation batch 401, loss: 0.102, 12832/13984 datapoints
2025-03-04 14:55:20,804 - INFO - Epoch 43/50 done
2025-03-04 14:55:20,805 - INFO - Beginning epoch 44/50
2025-03-04 14:55:20,808 - INFO - training batch 1, loss: 0.161, 32/56000 datapoints
2025-03-04 14:55:20,890 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-04 14:55:20,977 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:55:21,079 - INFO - training batch 151, loss: 0.053, 4832/56000 datapoints
2025-03-04 14:55:21,153 - INFO - training batch 201, loss: 0.025, 6432/56000 datapoints
2025-03-04 14:55:21,225 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:55:21,292 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-04 14:55:21,365 - INFO - training batch 351, loss: 0.064, 11232/56000 datapoints
2025-03-04 14:55:21,439 - INFO - training batch 401, loss: 0.055, 12832/56000 datapoints
2025-03-04 14:55:21,509 - INFO - training batch 451, loss: 0.025, 14432/56000 datapoints
2025-03-04 14:55:21,584 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-04 14:55:21,656 - INFO - training batch 551, loss: 0.055, 17632/56000 datapoints
2025-03-04 14:55:21,723 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:55:21,794 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:55:21,862 - INFO - training batch 701, loss: 0.239, 22432/56000 datapoints
2025-03-04 14:55:21,949 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-04 14:55:22,091 - INFO - training batch 801, loss: 0.013, 25632/56000 datapoints
2025-03-04 14:55:22,172 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-04 14:55:22,257 - INFO - training batch 901, loss: 0.008, 28832/56000 datapoints
2025-03-04 14:55:22,334 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-04 14:55:22,419 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:55:22,486 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-04 14:55:22,561 - INFO - training batch 1101, loss: 0.077, 35232/56000 datapoints
2025-03-04 14:55:22,664 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:55:22,750 - INFO - training batch 1201, loss: 0.014, 38432/56000 datapoints
2025-03-04 14:55:22,828 - INFO - training batch 1251, loss: 0.177, 40032/56000 datapoints
2025-03-04 14:55:22,917 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:55:22,989 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:55:23,071 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:55:23,160 - INFO - training batch 1451, loss: 0.002, 46432/56000 datapoints
2025-03-04 14:55:23,242 - INFO - training batch 1501, loss: 0.029, 48032/56000 datapoints
2025-03-04 14:55:23,324 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:55:23,411 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-04 14:55:23,487 - INFO - training batch 1651, loss: 0.015, 52832/56000 datapoints
2025-03-04 14:55:23,576 - INFO - training batch 1701, loss: 0.093, 54432/56000 datapoints
2025-03-04 14:55:23,658 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:23,681 - INFO - validation batch 51, loss: 0.017, 1632/13984 datapoints
2025-03-04 14:55:23,707 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:23,732 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:55:23,755 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-04 14:55:23,839 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-04 14:55:23,883 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-04 14:55:23,915 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:23,943 - INFO - validation batch 401, loss: 0.102, 12832/13984 datapoints
2025-03-04 14:55:23,961 - INFO - Epoch 44/50 done
2025-03-04 14:55:23,962 - INFO - Beginning epoch 45/50
2025-03-04 14:55:23,963 - INFO - training batch 1, loss: 0.161, 32/56000 datapoints
2025-03-04 14:55:24,046 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-04 14:55:24,115 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:55:24,198 - INFO - training batch 151, loss: 0.052, 4832/56000 datapoints
2025-03-04 14:55:24,278 - INFO - training batch 201, loss: 0.025, 6432/56000 datapoints
2025-03-04 14:55:24,358 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:55:24,435 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-04 14:55:24,514 - INFO - training batch 351, loss: 0.064, 11232/56000 datapoints
2025-03-04 14:55:24,595 - INFO - training batch 401, loss: 0.054, 12832/56000 datapoints
2025-03-04 14:55:24,657 - INFO - training batch 451, loss: 0.024, 14432/56000 datapoints
2025-03-04 14:55:24,722 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-04 14:55:24,788 - INFO - training batch 551, loss: 0.054, 17632/56000 datapoints
2025-03-04 14:55:24,853 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:55:24,922 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:55:24,993 - INFO - training batch 701, loss: 0.239, 22432/56000 datapoints
2025-03-04 14:55:25,060 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-04 14:55:25,121 - INFO - training batch 801, loss: 0.013, 25632/56000 datapoints
2025-03-04 14:55:25,185 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-04 14:55:25,246 - INFO - training batch 901, loss: 0.008, 28832/56000 datapoints
2025-03-04 14:55:25,315 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-04 14:55:25,378 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:55:25,442 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-04 14:55:25,501 - INFO - training batch 1101, loss: 0.077, 35232/56000 datapoints
2025-03-04 14:55:25,561 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:55:25,618 - INFO - training batch 1201, loss: 0.014, 38432/56000 datapoints
2025-03-04 14:55:25,676 - INFO - training batch 1251, loss: 0.175, 40032/56000 datapoints
2025-03-04 14:55:25,735 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:55:25,794 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:55:25,854 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:55:25,916 - INFO - training batch 1451, loss: 0.002, 46432/56000 datapoints
2025-03-04 14:55:25,979 - INFO - training batch 1501, loss: 0.029, 48032/56000 datapoints
2025-03-04 14:55:26,037 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:55:26,102 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-04 14:55:26,163 - INFO - training batch 1651, loss: 0.015, 52832/56000 datapoints
2025-03-04 14:55:26,222 - INFO - training batch 1701, loss: 0.093, 54432/56000 datapoints
2025-03-04 14:55:26,284 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:26,304 - INFO - validation batch 51, loss: 0.017, 1632/13984 datapoints
2025-03-04 14:55:26,322 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:26,339 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:55:26,358 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-04 14:55:26,375 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-04 14:55:26,392 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-04 14:55:26,411 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:26,428 - INFO - validation batch 401, loss: 0.101, 12832/13984 datapoints
2025-03-04 14:55:26,441 - INFO - Epoch 45/50 done
2025-03-04 14:55:26,441 - INFO - Beginning epoch 46/50
2025-03-04 14:55:26,443 - INFO - training batch 1, loss: 0.161, 32/56000 datapoints
2025-03-04 14:55:26,505 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-04 14:55:26,568 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:55:26,627 - INFO - training batch 151, loss: 0.052, 4832/56000 datapoints
2025-03-04 14:55:26,686 - INFO - training batch 201, loss: 0.025, 6432/56000 datapoints
2025-03-04 14:55:26,744 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:55:26,803 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-04 14:55:26,864 - INFO - training batch 351, loss: 0.064, 11232/56000 datapoints
2025-03-04 14:55:26,926 - INFO - training batch 401, loss: 0.053, 12832/56000 datapoints
2025-03-04 14:55:26,990 - INFO - training batch 451, loss: 0.024, 14432/56000 datapoints
2025-03-04 14:55:27,058 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-04 14:55:27,122 - INFO - training batch 551, loss: 0.053, 17632/56000 datapoints
2025-03-04 14:55:27,180 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:55:27,237 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:55:27,297 - INFO - training batch 701, loss: 0.238, 22432/56000 datapoints
2025-03-04 14:55:27,356 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-04 14:55:27,420 - INFO - training batch 801, loss: 0.013, 25632/56000 datapoints
2025-03-04 14:55:27,483 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-04 14:55:27,543 - INFO - training batch 901, loss: 0.008, 28832/56000 datapoints
2025-03-04 14:55:27,610 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-04 14:55:27,666 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-04 14:55:27,724 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-04 14:55:27,780 - INFO - training batch 1101, loss: 0.076, 35232/56000 datapoints
2025-03-04 14:55:27,857 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:55:27,926 - INFO - training batch 1201, loss: 0.014, 38432/56000 datapoints
2025-03-04 14:55:27,986 - INFO - training batch 1251, loss: 0.174, 40032/56000 datapoints
2025-03-04 14:55:28,042 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:55:28,100 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:55:28,156 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:55:28,211 - INFO - training batch 1451, loss: 0.002, 46432/56000 datapoints
2025-03-04 14:55:28,269 - INFO - training batch 1501, loss: 0.028, 48032/56000 datapoints
2025-03-04 14:55:28,328 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-04 14:55:28,385 - INFO - training batch 1601, loss: 0.007, 51232/56000 datapoints
2025-03-04 14:55:28,441 - INFO - training batch 1651, loss: 0.015, 52832/56000 datapoints
2025-03-04 14:55:28,501 - INFO - training batch 1701, loss: 0.093, 54432/56000 datapoints
2025-03-04 14:55:28,557 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:28,574 - INFO - validation batch 51, loss: 0.017, 1632/13984 datapoints
2025-03-04 14:55:28,596 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:28,614 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:55:28,632 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-04 14:55:28,649 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-04 14:55:28,666 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-04 14:55:28,683 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:28,700 - INFO - validation batch 401, loss: 0.101, 12832/13984 datapoints
2025-03-04 14:55:28,713 - INFO - Epoch 46/50 done
2025-03-04 14:55:28,713 - INFO - Beginning epoch 47/50
2025-03-04 14:55:28,715 - INFO - training batch 1, loss: 0.161, 32/56000 datapoints
2025-03-04 14:55:28,771 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-04 14:55:28,832 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:55:28,889 - INFO - training batch 151, loss: 0.052, 4832/56000 datapoints
2025-03-04 14:55:28,950 - INFO - training batch 201, loss: 0.024, 6432/56000 datapoints
2025-03-04 14:55:29,008 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:55:29,066 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-04 14:55:29,133 - INFO - training batch 351, loss: 0.063, 11232/56000 datapoints
2025-03-04 14:55:29,191 - INFO - training batch 401, loss: 0.053, 12832/56000 datapoints
2025-03-04 14:55:29,249 - INFO - training batch 451, loss: 0.024, 14432/56000 datapoints
2025-03-04 14:55:29,306 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-04 14:55:29,363 - INFO - training batch 551, loss: 0.053, 17632/56000 datapoints
2025-03-04 14:55:29,425 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:55:29,482 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:55:29,539 - INFO - training batch 701, loss: 0.238, 22432/56000 datapoints
2025-03-04 14:55:29,594 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-04 14:55:29,654 - INFO - training batch 801, loss: 0.012, 25632/56000 datapoints
2025-03-04 14:55:29,711 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-04 14:55:29,767 - INFO - training batch 901, loss: 0.008, 28832/56000 datapoints
2025-03-04 14:55:29,826 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-04 14:55:29,882 - INFO - training batch 1001, loss: 0.003, 32032/56000 datapoints
2025-03-04 14:55:29,945 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-04 14:55:30,002 - INFO - training batch 1101, loss: 0.076, 35232/56000 datapoints
2025-03-04 14:55:30,059 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:55:30,116 - INFO - training batch 1201, loss: 0.014, 38432/56000 datapoints
2025-03-04 14:55:30,173 - INFO - training batch 1251, loss: 0.172, 40032/56000 datapoints
2025-03-04 14:55:30,229 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:55:30,284 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:55:30,341 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:55:30,398 - INFO - training batch 1451, loss: 0.002, 46432/56000 datapoints
2025-03-04 14:55:30,455 - INFO - training batch 1501, loss: 0.028, 48032/56000 datapoints
2025-03-04 14:55:30,513 - INFO - training batch 1551, loss: 0.005, 49632/56000 datapoints
2025-03-04 14:55:30,568 - INFO - training batch 1601, loss: 0.007, 51232/56000 datapoints
2025-03-04 14:55:30,628 - INFO - training batch 1651, loss: 0.015, 52832/56000 datapoints
2025-03-04 14:55:30,683 - INFO - training batch 1701, loss: 0.093, 54432/56000 datapoints
2025-03-04 14:55:30,740 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:30,759 - INFO - validation batch 51, loss: 0.016, 1632/13984 datapoints
2025-03-04 14:55:30,776 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:30,792 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:55:30,811 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-04 14:55:30,829 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-04 14:55:30,847 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-04 14:55:30,864 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:30,880 - INFO - validation batch 401, loss: 0.101, 12832/13984 datapoints
2025-03-04 14:55:30,895 - INFO - Epoch 47/50 done
2025-03-04 14:55:30,895 - INFO - Beginning epoch 48/50
2025-03-04 14:55:30,897 - INFO - training batch 1, loss: 0.161, 32/56000 datapoints
2025-03-04 14:55:30,972 - INFO - training batch 51, loss: 0.005, 1632/56000 datapoints
2025-03-04 14:55:31,029 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:55:31,085 - INFO - training batch 151, loss: 0.052, 4832/56000 datapoints
2025-03-04 14:55:31,150 - INFO - training batch 201, loss: 0.024, 6432/56000 datapoints
2025-03-04 14:55:31,205 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:55:31,262 - INFO - training batch 301, loss: 0.024, 9632/56000 datapoints
2025-03-04 14:55:31,325 - INFO - training batch 351, loss: 0.063, 11232/56000 datapoints
2025-03-04 14:55:31,383 - INFO - training batch 401, loss: 0.052, 12832/56000 datapoints
2025-03-04 14:55:31,454 - INFO - training batch 451, loss: 0.023, 14432/56000 datapoints
2025-03-04 14:55:31,513 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-04 14:55:31,572 - INFO - training batch 551, loss: 0.052, 17632/56000 datapoints
2025-03-04 14:55:31,640 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:55:31,723 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:55:31,827 - INFO - training batch 701, loss: 0.237, 22432/56000 datapoints
2025-03-04 14:55:31,923 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-04 14:55:32,027 - INFO - training batch 801, loss: 0.012, 25632/56000 datapoints
2025-03-04 14:55:32,144 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-04 14:55:32,277 - INFO - training batch 901, loss: 0.008, 28832/56000 datapoints
2025-03-04 14:55:32,407 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-04 14:55:32,474 - INFO - training batch 1001, loss: 0.003, 32032/56000 datapoints
2025-03-04 14:55:32,548 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-04 14:55:32,620 - INFO - training batch 1101, loss: 0.076, 35232/56000 datapoints
2025-03-04 14:55:32,689 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:55:32,763 - INFO - training batch 1201, loss: 0.014, 38432/56000 datapoints
2025-03-04 14:55:32,838 - INFO - training batch 1251, loss: 0.171, 40032/56000 datapoints
2025-03-04 14:55:32,910 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:55:32,982 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:55:33,059 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:55:33,125 - INFO - training batch 1451, loss: 0.002, 46432/56000 datapoints
2025-03-04 14:55:33,198 - INFO - training batch 1501, loss: 0.028, 48032/56000 datapoints
2025-03-04 14:55:33,265 - INFO - training batch 1551, loss: 0.005, 49632/56000 datapoints
2025-03-04 14:55:33,338 - INFO - training batch 1601, loss: 0.007, 51232/56000 datapoints
2025-03-04 14:55:33,402 - INFO - training batch 1651, loss: 0.015, 52832/56000 datapoints
2025-03-04 14:55:33,481 - INFO - training batch 1701, loss: 0.092, 54432/56000 datapoints
2025-03-04 14:55:33,561 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:33,582 - INFO - validation batch 51, loss: 0.016, 1632/13984 datapoints
2025-03-04 14:55:33,605 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:33,627 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:55:33,649 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-04 14:55:33,682 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-04 14:55:33,712 - INFO - validation batch 301, loss: 0.007, 9632/13984 datapoints
2025-03-04 14:55:33,735 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:33,758 - INFO - validation batch 401, loss: 0.100, 12832/13984 datapoints
2025-03-04 14:55:33,776 - INFO - Epoch 48/50 done
2025-03-04 14:55:33,776 - INFO - Beginning epoch 49/50
2025-03-04 14:55:33,778 - INFO - training batch 1, loss: 0.161, 32/56000 datapoints
2025-03-04 14:55:33,853 - INFO - training batch 51, loss: 0.005, 1632/56000 datapoints
2025-03-04 14:55:34,101 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:55:34,170 - INFO - training batch 151, loss: 0.052, 4832/56000 datapoints
2025-03-04 14:55:34,242 - INFO - training batch 201, loss: 0.024, 6432/56000 datapoints
2025-03-04 14:55:34,315 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:55:34,385 - INFO - training batch 301, loss: 0.024, 9632/56000 datapoints
2025-03-04 14:55:34,447 - INFO - training batch 351, loss: 0.063, 11232/56000 datapoints
2025-03-04 14:55:34,521 - INFO - training batch 401, loss: 0.052, 12832/56000 datapoints
2025-03-04 14:55:34,581 - INFO - training batch 451, loss: 0.023, 14432/56000 datapoints
2025-03-04 14:55:34,640 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-04 14:55:34,702 - INFO - training batch 551, loss: 0.051, 17632/56000 datapoints
2025-03-04 14:55:34,761 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:55:34,820 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:55:34,879 - INFO - training batch 701, loss: 0.237, 22432/56000 datapoints
2025-03-04 14:55:34,939 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-04 14:55:34,996 - INFO - training batch 801, loss: 0.012, 25632/56000 datapoints
2025-03-04 14:55:35,053 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-04 14:55:35,109 - INFO - training batch 901, loss: 0.008, 28832/56000 datapoints
2025-03-04 14:55:35,169 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-04 14:55:35,233 - INFO - training batch 1001, loss: 0.003, 32032/56000 datapoints
2025-03-04 14:55:35,293 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-04 14:55:35,356 - INFO - training batch 1101, loss: 0.076, 35232/56000 datapoints
2025-03-04 14:55:35,414 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:55:35,477 - INFO - training batch 1201, loss: 0.014, 38432/56000 datapoints
2025-03-04 14:55:35,538 - INFO - training batch 1251, loss: 0.170, 40032/56000 datapoints
2025-03-04 14:55:35,594 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:55:35,651 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:55:35,712 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:55:35,770 - INFO - training batch 1451, loss: 0.002, 46432/56000 datapoints
2025-03-04 14:55:35,828 - INFO - training batch 1501, loss: 0.028, 48032/56000 datapoints
2025-03-04 14:55:35,887 - INFO - training batch 1551, loss: 0.005, 49632/56000 datapoints
2025-03-04 14:55:35,957 - INFO - training batch 1601, loss: 0.007, 51232/56000 datapoints
2025-03-04 14:55:36,021 - INFO - training batch 1651, loss: 0.015, 52832/56000 datapoints
2025-03-04 14:55:36,079 - INFO - training batch 1701, loss: 0.092, 54432/56000 datapoints
2025-03-04 14:55:36,141 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:36,162 - INFO - validation batch 51, loss: 0.016, 1632/13984 datapoints
2025-03-04 14:55:36,179 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:36,201 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-04 14:55:36,218 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-04 14:55:36,235 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-04 14:55:36,252 - INFO - validation batch 301, loss: 0.007, 9632/13984 datapoints
2025-03-04 14:55:36,270 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:36,289 - INFO - validation batch 401, loss: 0.100, 12832/13984 datapoints
2025-03-04 14:55:36,304 - INFO - Epoch 49/50 done
2025-03-04 14:55:36,305 - INFO - Beginning epoch 50/50
2025-03-04 14:55:36,307 - INFO - training batch 1, loss: 0.162, 32/56000 datapoints
2025-03-04 14:55:36,373 - INFO - training batch 51, loss: 0.005, 1632/56000 datapoints
2025-03-04 14:55:36,435 - INFO - training batch 101, loss: 0.064, 3232/56000 datapoints
2025-03-04 14:55:36,494 - INFO - training batch 151, loss: 0.051, 4832/56000 datapoints
2025-03-04 14:55:36,551 - INFO - training batch 201, loss: 0.024, 6432/56000 datapoints
2025-03-04 14:55:36,608 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-04 14:55:36,668 - INFO - training batch 301, loss: 0.024, 9632/56000 datapoints
2025-03-04 14:55:36,733 - INFO - training batch 351, loss: 0.063, 11232/56000 datapoints
2025-03-04 14:55:36,792 - INFO - training batch 401, loss: 0.051, 12832/56000 datapoints
2025-03-04 14:55:36,853 - INFO - training batch 451, loss: 0.023, 14432/56000 datapoints
2025-03-04 14:55:36,918 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-04 14:55:36,976 - INFO - training batch 551, loss: 0.050, 17632/56000 datapoints
2025-03-04 14:55:37,035 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-04 14:55:37,096 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-04 14:55:37,155 - INFO - training batch 701, loss: 0.236, 22432/56000 datapoints
2025-03-04 14:55:37,216 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-04 14:55:37,273 - INFO - training batch 801, loss: 0.012, 25632/56000 datapoints
2025-03-04 14:55:37,329 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-04 14:55:37,388 - INFO - training batch 901, loss: 0.007, 28832/56000 datapoints
2025-03-04 14:55:37,454 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-04 14:55:37,512 - INFO - training batch 1001, loss: 0.003, 32032/56000 datapoints
2025-03-04 14:55:37,571 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-04 14:55:37,629 - INFO - training batch 1101, loss: 0.076, 35232/56000 datapoints
2025-03-04 14:55:37,689 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-04 14:55:37,750 - INFO - training batch 1201, loss: 0.014, 38432/56000 datapoints
2025-03-04 14:55:37,808 - INFO - training batch 1251, loss: 0.168, 40032/56000 datapoints
2025-03-04 14:55:37,868 - INFO - training batch 1301, loss: 0.016, 41632/56000 datapoints
2025-03-04 14:55:37,928 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-04 14:55:37,985 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-04 14:55:38,043 - INFO - training batch 1451, loss: 0.002, 46432/56000 datapoints
2025-03-04 14:55:38,100 - INFO - training batch 1501, loss: 0.028, 48032/56000 datapoints
2025-03-04 14:55:38,162 - INFO - training batch 1551, loss: 0.005, 49632/56000 datapoints
2025-03-04 14:55:38,221 - INFO - training batch 1601, loss: 0.007, 51232/56000 datapoints
2025-03-04 14:55:38,279 - INFO - training batch 1651, loss: 0.015, 52832/56000 datapoints
2025-03-04 14:55:38,339 - INFO - training batch 1701, loss: 0.092, 54432/56000 datapoints
2025-03-04 14:55:38,400 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:38,418 - INFO - validation batch 51, loss: 0.016, 1632/13984 datapoints
2025-03-04 14:55:38,436 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:38,455 - INFO - validation batch 151, loss: 0.006, 4832/13984 datapoints
2025-03-04 14:55:38,472 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-04 14:55:38,489 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-04 14:55:38,508 - INFO - validation batch 301, loss: 0.007, 9632/13984 datapoints
2025-03-04 14:55:38,526 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:38,545 - INFO - validation batch 401, loss: 0.100, 12832/13984 datapoints
2025-03-04 14:55:38,558 - INFO - Epoch 50/50 done
2025-03-04 14:55:38,559 - INFO - Finished training in 332.80 seconds.
2025-03-04 14:55:38,559 - INFO - Evaluating model...
2025-03-04 14:55:38,562 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-04 14:55:38,580 - INFO - validation batch 51, loss: 0.016, 1632/13984 datapoints
2025-03-04 14:55:38,598 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-04 14:55:38,615 - INFO - validation batch 151, loss: 0.006, 4832/13984 datapoints
2025-03-04 14:55:38,633 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-04 14:55:38,650 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-04 14:55:38,667 - INFO - validation batch 301, loss: 0.007, 9632/13984 datapoints
2025-03-04 14:55:38,684 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-04 14:55:38,701 - INFO - validation batch 401, loss: 0.100, 12832/13984 datapoints
2025-03-04 14:55:38,725 - INFO - Done evaluating.
2025-03-04 14:55:38,727 - INFO - Average final validation loss: 0.017
2025-03-04 14:55:38,727 - INFO - Saving...
2025-03-04 14:55:39,720 - INFO - Done saving.
2025-03-04 14:55:39,720 - INFO - Successfully completed hyperparameter combination 1 of 1