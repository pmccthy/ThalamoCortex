2025-03-04 14:45:29,569 - INFO - Running hyperparameter combination 1 of 1
2025-03-04 14:45:29,569 - INFO - 0_CTCNet_finetuning_ThalReadout_2
2025-03-04 14:45:32,565 - INFO - Training...
2025-03-04 14:45:32,566 - INFO - Beginning epoch 1/50
2025-03-04 14:45:32,633 - INFO - training batch 1, loss: 3.470, 32/56000 datapoints
2025-03-04 14:45:32,730 - INFO - training batch 51, loss: 3.348, 1632/56000 datapoints
2025-03-04 14:45:32,819 - INFO - training batch 101, loss: 3.690, 3232/56000 datapoints
2025-03-04 14:45:32,901 - INFO - training batch 151, loss: 3.270, 4832/56000 datapoints
2025-03-04 14:45:32,990 - INFO - training batch 201, loss: 3.365, 6432/56000 datapoints
2025-03-04 14:45:33,063 - INFO - training batch 251, loss: 3.422, 8032/56000 datapoints
2025-03-04 14:45:33,139 - INFO - training batch 301, loss: 3.299, 9632/56000 datapoints
2025-03-04 14:45:33,230 - INFO - training batch 351, loss: 2.952, 11232/56000 datapoints
2025-03-04 14:45:33,326 - INFO - training batch 401, loss: 2.539, 12832/56000 datapoints
2025-03-04 14:45:33,422 - INFO - training batch 451, loss: 2.818, 14432/56000 datapoints
2025-03-04 14:45:33,515 - INFO - training batch 501, loss: 2.403, 16032/56000 datapoints
2025-03-04 14:45:33,618 - INFO - training batch 551, loss: 2.935, 17632/56000 datapoints
2025-03-04 14:45:33,758 - INFO - training batch 601, loss: 3.207, 19232/56000 datapoints
2025-03-04 14:45:33,971 - INFO - training batch 651, loss: 2.903, 20832/56000 datapoints
2025-03-04 14:45:34,092 - INFO - training batch 701, loss: 2.700, 22432/56000 datapoints
2025-03-04 14:45:34,398 - INFO - training batch 751, loss: 2.337, 24032/56000 datapoints
2025-03-04 14:45:34,705 - INFO - training batch 801, loss: 2.382, 25632/56000 datapoints
2025-03-04 14:45:35,130 - INFO - training batch 851, loss: 2.593, 27232/56000 datapoints
2025-03-04 14:45:35,381 - INFO - training batch 901, loss: 2.151, 28832/56000 datapoints
2025-03-04 14:45:36,356 - INFO - training batch 951, loss: 2.247, 30432/56000 datapoints
2025-03-04 14:45:36,499 - INFO - training batch 1001, loss: 2.373, 32032/56000 datapoints
2025-03-04 14:45:36,645 - INFO - training batch 1051, loss: 2.097, 33632/56000 datapoints
2025-03-04 14:45:36,828 - INFO - training batch 1101, loss: 2.229, 35232/56000 datapoints
2025-03-04 14:45:37,345 - INFO - training batch 1151, loss: 2.224, 36832/56000 datapoints
2025-03-04 14:45:38,462 - INFO - training batch 1201, loss: 2.310, 38432/56000 datapoints
2025-03-04 14:45:40,454 - INFO - training batch 1251, loss: 2.057, 40032/56000 datapoints
2025-03-04 14:45:42,380 - INFO - training batch 1301, loss: 1.782, 41632/56000 datapoints
2025-03-04 14:45:43,088 - INFO - training batch 1351, loss: 2.122, 43232/56000 datapoints
2025-03-04 14:45:44,919 - INFO - training batch 1401, loss: 1.813, 44832/56000 datapoints
2025-03-04 14:45:46,303 - INFO - training batch 1451, loss: 1.708, 46432/56000 datapoints
2025-03-04 14:45:47,145 - INFO - training batch 1501, loss: 1.537, 48032/56000 datapoints
2025-03-04 14:45:48,233 - INFO - training batch 1551, loss: 2.164, 49632/56000 datapoints
2025-03-04 14:45:49,946 - INFO - training batch 1601, loss: 1.465, 51232/56000 datapoints
2025-03-04 14:45:52,078 - INFO - training batch 1651, loss: 1.740, 52832/56000 datapoints
2025-03-04 14:45:52,578 - INFO - training batch 1701, loss: 1.701, 54432/56000 datapoints
2025-03-04 14:45:53,223 - INFO - validation batch 1, loss: 1.549, 32/13984 datapoints
2025-03-04 14:45:53,469 - INFO - validation batch 51, loss: 1.691, 1632/13984 datapoints
2025-03-04 14:45:53,594 - INFO - validation batch 101, loss: 1.695, 3232/13984 datapoints
2025-03-04 14:45:53,700 - INFO - validation batch 151, loss: 1.536, 4832/13984 datapoints
2025-03-04 14:45:53,758 - INFO - validation batch 201, loss: 1.590, 6432/13984 datapoints
2025-03-04 14:45:53,843 - INFO - validation batch 251, loss: 1.533, 8032/13984 datapoints
2025-03-04 14:45:53,958 - INFO - validation batch 301, loss: 1.568, 9632/13984 datapoints
2025-03-04 14:45:54,052 - INFO - validation batch 351, loss: 1.427, 11232/13984 datapoints
2025-03-04 14:45:54,391 - INFO - validation batch 401, loss: 1.468, 12832/13984 datapoints
2025-03-04 14:45:54,770 - INFO - Epoch 1/50 done
2025-03-04 14:45:54,774 - INFO - Beginning epoch 2/50
2025-03-04 14:45:54,781 - INFO - training batch 1, loss: 1.472, 32/56000 datapoints
2025-03-04 14:45:55,210 - INFO - training batch 51, loss: 1.410, 1632/56000 datapoints
2025-03-04 14:45:55,757 - INFO - training batch 101, loss: 1.529, 3232/56000 datapoints
2025-03-04 14:45:56,722 - INFO - training batch 151, loss: 1.423, 4832/56000 datapoints
2025-03-04 14:45:57,148 - INFO - training batch 201, loss: 1.332, 6432/56000 datapoints
2025-03-04 14:45:57,318 - INFO - training batch 251, loss: 1.307, 8032/56000 datapoints
2025-03-04 14:45:57,496 - INFO - training batch 301, loss: 1.393, 9632/56000 datapoints
2025-03-04 14:45:57,675 - INFO - training batch 351, loss: 1.161, 11232/56000 datapoints
2025-03-04 14:45:57,943 - INFO - training batch 401, loss: 1.047, 12832/56000 datapoints
2025-03-04 14:45:58,143 - INFO - training batch 451, loss: 1.249, 14432/56000 datapoints
2025-03-04 14:45:58,605 - INFO - training batch 501, loss: 1.000, 16032/56000 datapoints
2025-03-04 14:45:59,459 - INFO - training batch 551, loss: 1.135, 17632/56000 datapoints
2025-03-04 14:45:59,796 - INFO - training batch 601, loss: 1.256, 19232/56000 datapoints
2025-03-04 14:46:00,002 - INFO - training batch 651, loss: 1.070, 20832/56000 datapoints
2025-03-04 14:46:00,474 - INFO - training batch 701, loss: 1.077, 22432/56000 datapoints
2025-03-04 14:46:01,130 - INFO - training batch 751, loss: 0.905, 24032/56000 datapoints
2025-03-04 14:46:01,581 - INFO - training batch 801, loss: 0.945, 25632/56000 datapoints
2025-03-04 14:46:02,172 - INFO - training batch 851, loss: 1.007, 27232/56000 datapoints
2025-03-04 14:46:02,463 - INFO - training batch 901, loss: 0.868, 28832/56000 datapoints
2025-03-04 14:46:02,638 - INFO - training batch 951, loss: 0.907, 30432/56000 datapoints
2025-03-04 14:46:02,846 - INFO - training batch 1001, loss: 0.875, 32032/56000 datapoints
2025-03-04 14:46:03,046 - INFO - training batch 1051, loss: 0.814, 33632/56000 datapoints
2025-03-04 14:46:03,316 - INFO - training batch 1101, loss: 0.862, 35232/56000 datapoints
2025-03-04 14:46:03,636 - INFO - training batch 1151, loss: 0.831, 36832/56000 datapoints
2025-03-04 14:46:03,897 - INFO - training batch 1201, loss: 0.885, 38432/56000 datapoints
2025-03-04 14:46:04,203 - INFO - training batch 1251, loss: 0.864, 40032/56000 datapoints
2025-03-04 14:46:04,832 - INFO - training batch 1301, loss: 0.751, 41632/56000 datapoints
2025-03-04 14:46:05,039 - INFO - training batch 1351, loss: 0.755, 43232/56000 datapoints
2025-03-04 14:46:05,270 - INFO - training batch 1401, loss: 0.676, 44832/56000 datapoints
2025-03-04 14:46:05,674 - INFO - training batch 1451, loss: 0.673, 46432/56000 datapoints
2025-03-04 14:46:06,100 - INFO - training batch 1501, loss: 0.722, 48032/56000 datapoints
2025-03-04 14:46:06,274 - INFO - training batch 1551, loss: 0.842, 49632/56000 datapoints
2025-03-04 14:46:06,476 - INFO - training batch 1601, loss: 0.654, 51232/56000 datapoints
2025-03-04 14:46:06,653 - INFO - training batch 1651, loss: 0.647, 52832/56000 datapoints
2025-03-04 14:46:06,788 - INFO - training batch 1701, loss: 0.716, 54432/56000 datapoints
2025-03-04 14:46:06,946 - INFO - validation batch 1, loss: 0.629, 32/13984 datapoints
2025-03-04 14:46:07,014 - INFO - validation batch 51, loss: 0.613, 1632/13984 datapoints
2025-03-04 14:46:07,082 - INFO - validation batch 101, loss: 0.723, 3232/13984 datapoints
2025-03-04 14:46:07,141 - INFO - validation batch 151, loss: 0.573, 4832/13984 datapoints
2025-03-04 14:46:07,197 - INFO - validation batch 201, loss: 0.704, 6432/13984 datapoints
2025-03-04 14:46:07,250 - INFO - validation batch 251, loss: 0.720, 8032/13984 datapoints
2025-03-04 14:46:07,299 - INFO - validation batch 301, loss: 0.774, 9632/13984 datapoints
2025-03-04 14:46:07,348 - INFO - validation batch 351, loss: 0.584, 11232/13984 datapoints
2025-03-04 14:46:07,401 - INFO - validation batch 401, loss: 0.697, 12832/13984 datapoints
2025-03-04 14:46:07,442 - INFO - Epoch 2/50 done
2025-03-04 14:46:07,443 - INFO - Beginning epoch 3/50
2025-03-04 14:46:07,448 - INFO - training batch 1, loss: 0.658, 32/56000 datapoints
2025-03-04 14:46:07,566 - INFO - training batch 51, loss: 0.675, 1632/56000 datapoints
2025-03-04 14:46:07,720 - INFO - training batch 101, loss: 0.671, 3232/56000 datapoints
2025-03-04 14:46:07,855 - INFO - training batch 151, loss: 0.675, 4832/56000 datapoints
2025-03-04 14:46:08,008 - INFO - training batch 201, loss: 0.572, 6432/56000 datapoints
2025-03-04 14:46:08,121 - INFO - training batch 251, loss: 0.544, 8032/56000 datapoints
2025-03-04 14:46:08,219 - INFO - training batch 301, loss: 0.636, 9632/56000 datapoints
2025-03-04 14:46:08,342 - INFO - training batch 351, loss: 0.558, 11232/56000 datapoints
2025-03-04 14:46:08,468 - INFO - training batch 401, loss: 0.556, 12832/56000 datapoints
2025-03-04 14:46:08,585 - INFO - training batch 451, loss: 0.656, 14432/56000 datapoints
2025-03-04 14:46:08,710 - INFO - training batch 501, loss: 0.578, 16032/56000 datapoints
2025-03-04 14:46:08,819 - INFO - training batch 551, loss: 0.518, 17632/56000 datapoints
2025-03-04 14:46:08,934 - INFO - training batch 601, loss: 0.551, 19232/56000 datapoints
2025-03-04 14:46:09,033 - INFO - training batch 651, loss: 0.487, 20832/56000 datapoints
2025-03-04 14:46:09,156 - INFO - training batch 701, loss: 0.530, 22432/56000 datapoints
2025-03-04 14:46:09,266 - INFO - training batch 751, loss: 0.475, 24032/56000 datapoints
2025-03-04 14:46:09,374 - INFO - training batch 801, loss: 0.518, 25632/56000 datapoints
2025-03-04 14:46:09,466 - INFO - training batch 851, loss: 0.478, 27232/56000 datapoints
2025-03-04 14:46:09,552 - INFO - training batch 901, loss: 0.497, 28832/56000 datapoints
2025-03-04 14:46:09,636 - INFO - training batch 951, loss: 0.498, 30432/56000 datapoints
2025-03-04 14:46:09,729 - INFO - training batch 1001, loss: 0.442, 32032/56000 datapoints
2025-03-04 14:46:09,935 - INFO - training batch 1051, loss: 0.437, 33632/56000 datapoints
2025-03-04 14:46:10,121 - INFO - training batch 1101, loss: 0.476, 35232/56000 datapoints
2025-03-04 14:46:10,254 - INFO - training batch 1151, loss: 0.396, 36832/56000 datapoints
2025-03-04 14:46:10,349 - INFO - training batch 1201, loss: 0.466, 38432/56000 datapoints
2025-03-04 14:46:10,451 - INFO - training batch 1251, loss: 0.490, 40032/56000 datapoints
2025-03-04 14:46:10,648 - INFO - training batch 1301, loss: 0.457, 41632/56000 datapoints
2025-03-04 14:46:10,782 - INFO - training batch 1351, loss: 0.396, 43232/56000 datapoints
2025-03-04 14:46:10,932 - INFO - training batch 1401, loss: 0.367, 44832/56000 datapoints
2025-03-04 14:46:11,307 - INFO - training batch 1451, loss: 0.390, 46432/56000 datapoints
2025-03-04 14:46:11,467 - INFO - training batch 1501, loss: 0.511, 48032/56000 datapoints
2025-03-04 14:46:11,665 - INFO - training batch 1551, loss: 0.461, 49632/56000 datapoints
2025-03-04 14:46:11,788 - INFO - training batch 1601, loss: 0.460, 51232/56000 datapoints
2025-03-04 14:46:12,205 - INFO - training batch 1651, loss: 0.371, 52832/56000 datapoints
2025-03-04 14:46:12,524 - INFO - training batch 1701, loss: 0.428, 54432/56000 datapoints
2025-03-04 14:46:12,686 - INFO - validation batch 1, loss: 0.366, 32/13984 datapoints
2025-03-04 14:46:12,748 - INFO - validation batch 51, loss: 0.317, 1632/13984 datapoints
2025-03-04 14:46:12,815 - INFO - validation batch 101, loss: 0.432, 3232/13984 datapoints
2025-03-04 14:46:12,880 - INFO - validation batch 151, loss: 0.315, 4832/13984 datapoints
2025-03-04 14:46:12,943 - INFO - validation batch 201, loss: 0.426, 6432/13984 datapoints
2025-03-04 14:46:13,004 - INFO - validation batch 251, loss: 0.467, 8032/13984 datapoints
2025-03-04 14:46:13,160 - INFO - validation batch 301, loss: 0.540, 9632/13984 datapoints
2025-03-04 14:46:13,240 - INFO - validation batch 351, loss: 0.346, 11232/13984 datapoints
2025-03-04 14:46:13,339 - INFO - validation batch 401, loss: 0.485, 12832/13984 datapoints
2025-03-04 14:46:13,381 - INFO - Epoch 3/50 done
2025-03-04 14:46:13,382 - INFO - Beginning epoch 4/50
2025-03-04 14:46:13,424 - INFO - training batch 1, loss: 0.404, 32/56000 datapoints
2025-03-04 14:46:13,681 - INFO - training batch 51, loss: 0.476, 1632/56000 datapoints
2025-03-04 14:46:14,280 - INFO - training batch 101, loss: 0.387, 3232/56000 datapoints
2025-03-04 14:46:14,592 - INFO - training batch 151, loss: 0.447, 4832/56000 datapoints
2025-03-04 14:46:14,994 - INFO - training batch 201, loss: 0.343, 6432/56000 datapoints
2025-03-04 14:46:15,196 - INFO - training batch 251, loss: 0.364, 8032/56000 datapoints
2025-03-04 14:46:15,323 - INFO - training batch 301, loss: 0.407, 9632/56000 datapoints
2025-03-04 14:46:15,425 - INFO - training batch 351, loss: 0.373, 11232/56000 datapoints
2025-03-04 14:46:15,555 - INFO - training batch 401, loss: 0.437, 12832/56000 datapoints
2025-03-04 14:46:15,666 - INFO - training batch 451, loss: 0.482, 14432/56000 datapoints
2025-03-04 14:46:15,827 - INFO - training batch 501, loss: 0.438, 16032/56000 datapoints
2025-03-04 14:46:15,973 - INFO - training batch 551, loss: 0.336, 17632/56000 datapoints
2025-03-04 14:46:16,092 - INFO - training batch 601, loss: 0.341, 19232/56000 datapoints
2025-03-04 14:46:16,200 - INFO - training batch 651, loss: 0.305, 20832/56000 datapoints
2025-03-04 14:46:16,306 - INFO - training batch 701, loss: 0.373, 22432/56000 datapoints
2025-03-04 14:46:16,390 - INFO - training batch 751, loss: 0.355, 24032/56000 datapoints
2025-03-04 14:46:16,495 - INFO - training batch 801, loss: 0.404, 25632/56000 datapoints
2025-03-04 14:46:16,603 - INFO - training batch 851, loss: 0.320, 27232/56000 datapoints
2025-03-04 14:46:16,739 - INFO - training batch 901, loss: 0.378, 28832/56000 datapoints
2025-03-04 14:46:16,835 - INFO - training batch 951, loss: 0.380, 30432/56000 datapoints
2025-03-04 14:46:16,962 - INFO - training batch 1001, loss: 0.293, 32032/56000 datapoints
2025-03-04 14:46:17,051 - INFO - training batch 1051, loss: 0.310, 33632/56000 datapoints
2025-03-04 14:46:17,127 - INFO - training batch 1101, loss: 0.356, 35232/56000 datapoints
2025-03-04 14:46:17,214 - INFO - training batch 1151, loss: 0.255, 36832/56000 datapoints
2025-03-04 14:46:17,330 - INFO - training batch 1201, loss: 0.345, 38432/56000 datapoints
2025-03-04 14:46:17,456 - INFO - training batch 1251, loss: 0.380, 40032/56000 datapoints
2025-03-04 14:46:17,575 - INFO - training batch 1301, loss: 0.354, 41632/56000 datapoints
2025-03-04 14:46:17,695 - INFO - training batch 1351, loss: 0.279, 43232/56000 datapoints
2025-03-04 14:46:17,782 - INFO - training batch 1401, loss: 0.250, 44832/56000 datapoints
2025-03-04 14:46:17,862 - INFO - training batch 1451, loss: 0.289, 46432/56000 datapoints
2025-03-04 14:46:17,946 - INFO - training batch 1501, loss: 0.445, 48032/56000 datapoints
2025-03-04 14:46:18,027 - INFO - training batch 1551, loss: 0.360, 49632/56000 datapoints
2025-03-04 14:46:18,103 - INFO - training batch 1601, loss: 0.389, 51232/56000 datapoints
2025-03-04 14:46:18,182 - INFO - training batch 1651, loss: 0.276, 52832/56000 datapoints
2025-03-04 14:46:18,259 - INFO - training batch 1701, loss: 0.339, 54432/56000 datapoints
2025-03-04 14:46:18,336 - INFO - validation batch 1, loss: 0.253, 32/13984 datapoints
2025-03-04 14:46:18,362 - INFO - validation batch 51, loss: 0.200, 1632/13984 datapoints
2025-03-04 14:46:18,387 - INFO - validation batch 101, loss: 0.346, 3232/13984 datapoints
2025-03-04 14:46:18,412 - INFO - validation batch 151, loss: 0.216, 4832/13984 datapoints
2025-03-04 14:46:18,441 - INFO - validation batch 201, loss: 0.325, 6432/13984 datapoints
2025-03-04 14:46:18,466 - INFO - validation batch 251, loss: 0.383, 8032/13984 datapoints
2025-03-04 14:46:18,494 - INFO - validation batch 301, loss: 0.478, 9632/13984 datapoints
2025-03-04 14:46:18,523 - INFO - validation batch 351, loss: 0.251, 11232/13984 datapoints
2025-03-04 14:46:18,548 - INFO - validation batch 401, loss: 0.409, 12832/13984 datapoints
2025-03-04 14:46:18,570 - INFO - Epoch 4/50 done
2025-03-04 14:46:18,572 - INFO - Beginning epoch 5/50
2025-03-04 14:46:18,575 - INFO - training batch 1, loss: 0.308, 32/56000 datapoints
2025-03-04 14:46:18,653 - INFO - training batch 51, loss: 0.411, 1632/56000 datapoints
2025-03-04 14:46:18,740 - INFO - training batch 101, loss: 0.280, 3232/56000 datapoints
2025-03-04 14:46:18,817 - INFO - training batch 151, loss: 0.374, 4832/56000 datapoints
2025-03-04 14:46:18,892 - INFO - training batch 201, loss: 0.239, 6432/56000 datapoints
2025-03-04 14:46:18,976 - INFO - training batch 251, loss: 0.288, 8032/56000 datapoints
2025-03-04 14:46:19,051 - INFO - training batch 301, loss: 0.324, 9632/56000 datapoints
2025-03-04 14:46:19,143 - INFO - training batch 351, loss: 0.298, 11232/56000 datapoints
2025-03-04 14:46:19,229 - INFO - training batch 401, loss: 0.394, 12832/56000 datapoints
2025-03-04 14:46:19,317 - INFO - training batch 451, loss: 0.424, 14432/56000 datapoints
2025-03-04 14:46:19,403 - INFO - training batch 501, loss: 0.387, 16032/56000 datapoints
2025-03-04 14:46:19,487 - INFO - training batch 551, loss: 0.255, 17632/56000 datapoints
2025-03-04 14:46:19,570 - INFO - training batch 601, loss: 0.257, 19232/56000 datapoints
2025-03-04 14:46:19,653 - INFO - training batch 651, loss: 0.227, 20832/56000 datapoints
2025-03-04 14:46:19,738 - INFO - training batch 701, loss: 0.312, 22432/56000 datapoints
2025-03-04 14:46:19,831 - INFO - training batch 751, loss: 0.291, 24032/56000 datapoints
2025-03-04 14:46:19,928 - INFO - training batch 801, loss: 0.359, 25632/56000 datapoints
2025-03-04 14:46:20,013 - INFO - training batch 851, loss: 0.245, 27232/56000 datapoints
2025-03-04 14:46:20,091 - INFO - training batch 901, loss: 0.322, 28832/56000 datapoints
2025-03-04 14:46:20,180 - INFO - training batch 951, loss: 0.331, 30432/56000 datapoints
2025-03-04 14:46:20,273 - INFO - training batch 1001, loss: 0.216, 32032/56000 datapoints
2025-03-04 14:46:20,368 - INFO - training batch 1051, loss: 0.238, 33632/56000 datapoints
2025-03-04 14:46:20,454 - INFO - training batch 1101, loss: 0.306, 35232/56000 datapoints
2025-03-04 14:46:20,534 - INFO - training batch 1151, loss: 0.178, 36832/56000 datapoints
2025-03-04 14:46:20,620 - INFO - training batch 1201, loss: 0.286, 38432/56000 datapoints
2025-03-04 14:46:20,719 - INFO - training batch 1251, loss: 0.327, 40032/56000 datapoints
2025-03-04 14:46:20,826 - INFO - training batch 1301, loss: 0.304, 41632/56000 datapoints
2025-03-04 14:46:20,946 - INFO - training batch 1351, loss: 0.215, 43232/56000 datapoints
2025-03-04 14:46:21,043 - INFO - training batch 1401, loss: 0.183, 44832/56000 datapoints
2025-03-04 14:46:21,137 - INFO - training batch 1451, loss: 0.228, 46432/56000 datapoints
2025-03-04 14:46:21,231 - INFO - training batch 1501, loss: 0.414, 48032/56000 datapoints
2025-03-04 14:46:21,314 - INFO - training batch 1551, loss: 0.311, 49632/56000 datapoints
2025-03-04 14:46:21,400 - INFO - training batch 1601, loss: 0.350, 51232/56000 datapoints
2025-03-04 14:46:21,480 - INFO - training batch 1651, loss: 0.215, 52832/56000 datapoints
2025-03-04 14:46:21,560 - INFO - training batch 1701, loss: 0.288, 54432/56000 datapoints
2025-03-04 14:46:21,648 - INFO - validation batch 1, loss: 0.187, 32/13984 datapoints
2025-03-04 14:46:21,675 - INFO - validation batch 51, loss: 0.128, 1632/13984 datapoints
2025-03-04 14:46:21,700 - INFO - validation batch 101, loss: 0.306, 3232/13984 datapoints
2025-03-04 14:46:21,725 - INFO - validation batch 151, loss: 0.156, 4832/13984 datapoints
2025-03-04 14:46:21,754 - INFO - validation batch 201, loss: 0.272, 6432/13984 datapoints
2025-03-04 14:46:21,781 - INFO - validation batch 251, loss: 0.339, 8032/13984 datapoints
2025-03-04 14:46:21,812 - INFO - validation batch 301, loss: 0.453, 9632/13984 datapoints
2025-03-04 14:46:21,837 - INFO - validation batch 351, loss: 0.193, 11232/13984 datapoints
2025-03-04 14:46:21,871 - INFO - validation batch 401, loss: 0.371, 12832/13984 datapoints
2025-03-04 14:46:21,901 - INFO - Epoch 5/50 done
2025-03-04 14:46:21,902 - INFO - Beginning epoch 6/50
2025-03-04 14:46:21,906 - INFO - training batch 1, loss: 0.257, 32/56000 datapoints
2025-03-04 14:46:21,988 - INFO - training batch 51, loss: 0.380, 1632/56000 datapoints
2025-03-04 14:46:22,067 - INFO - training batch 101, loss: 0.220, 3232/56000 datapoints
2025-03-04 14:46:22,147 - INFO - training batch 151, loss: 0.332, 4832/56000 datapoints
2025-03-04 14:46:22,251 - INFO - training batch 201, loss: 0.177, 6432/56000 datapoints
2025-03-04 14:46:22,361 - INFO - training batch 251, loss: 0.245, 8032/56000 datapoints
2025-03-04 14:46:22,530 - INFO - training batch 301, loss: 0.275, 9632/56000 datapoints
2025-03-04 14:46:22,690 - INFO - training batch 351, loss: 0.253, 11232/56000 datapoints
2025-03-04 14:46:23,197 - INFO - training batch 401, loss: 0.371, 12832/56000 datapoints
2025-03-04 14:46:23,573 - INFO - training batch 451, loss: 0.396, 14432/56000 datapoints
2025-03-04 14:46:23,745 - INFO - training batch 501, loss: 0.360, 16032/56000 datapoints
2025-03-04 14:46:23,970 - INFO - training batch 551, loss: 0.211, 17632/56000 datapoints
2025-03-04 14:46:24,208 - INFO - training batch 601, loss: 0.210, 19232/56000 datapoints
2025-03-04 14:46:24,342 - INFO - training batch 651, loss: 0.183, 20832/56000 datapoints
2025-03-04 14:46:24,744 - INFO - training batch 701, loss: 0.279, 22432/56000 datapoints
2025-03-04 14:46:25,140 - INFO - training batch 751, loss: 0.249, 24032/56000 datapoints
2025-03-04 14:46:25,264 - INFO - training batch 801, loss: 0.336, 25632/56000 datapoints
2025-03-04 14:46:25,404 - INFO - training batch 851, loss: 0.199, 27232/56000 datapoints
2025-03-04 14:46:25,538 - INFO - training batch 901, loss: 0.290, 28832/56000 datapoints
2025-03-04 14:46:25,684 - INFO - training batch 951, loss: 0.299, 30432/56000 datapoints
2025-03-04 14:46:25,883 - INFO - training batch 1001, loss: 0.171, 32032/56000 datapoints
2025-03-04 14:46:26,192 - INFO - training batch 1051, loss: 0.192, 33632/56000 datapoints
2025-03-04 14:46:26,506 - INFO - training batch 1101, loss: 0.279, 35232/56000 datapoints
2025-03-04 14:46:27,197 - INFO - training batch 1151, loss: 0.132, 36832/56000 datapoints
2025-03-04 14:46:27,580 - INFO - training batch 1201, loss: 0.255, 38432/56000 datapoints
2025-03-04 14:46:27,962 - INFO - training batch 1251, loss: 0.298, 40032/56000 datapoints
2025-03-04 14:46:28,144 - INFO - training batch 1301, loss: 0.274, 41632/56000 datapoints
2025-03-04 14:46:28,546 - INFO - training batch 1351, loss: 0.179, 43232/56000 datapoints
2025-03-04 14:46:28,821 - INFO - training batch 1401, loss: 0.142, 44832/56000 datapoints
2025-03-04 14:46:29,177 - INFO - training batch 1451, loss: 0.190, 46432/56000 datapoints
2025-03-04 14:46:29,339 - INFO - training batch 1501, loss: 0.389, 48032/56000 datapoints
2025-03-04 14:46:29,573 - INFO - training batch 1551, loss: 0.277, 49632/56000 datapoints
2025-03-04 14:46:30,344 - INFO - training batch 1601, loss: 0.320, 51232/56000 datapoints
2025-03-04 14:46:30,695 - INFO - training batch 1651, loss: 0.177, 52832/56000 datapoints
2025-03-04 14:46:30,863 - INFO - training batch 1701, loss: 0.254, 54432/56000 datapoints
2025-03-04 14:46:31,052 - INFO - validation batch 1, loss: 0.147, 32/13984 datapoints
2025-03-04 14:46:31,108 - INFO - validation batch 51, loss: 0.087, 1632/13984 datapoints
2025-03-04 14:46:31,159 - INFO - validation batch 101, loss: 0.276, 3232/13984 datapoints
2025-03-04 14:46:31,204 - INFO - validation batch 151, loss: 0.119, 4832/13984 datapoints
2025-03-04 14:46:31,234 - INFO - validation batch 201, loss: 0.239, 6432/13984 datapoints
2025-03-04 14:46:31,267 - INFO - validation batch 251, loss: 0.308, 8032/13984 datapoints
2025-03-04 14:46:31,358 - INFO - validation batch 301, loss: 0.430, 9632/13984 datapoints
2025-03-04 14:46:31,440 - INFO - validation batch 351, loss: 0.158, 11232/13984 datapoints
2025-03-04 14:46:31,513 - INFO - validation batch 401, loss: 0.344, 12832/13984 datapoints
2025-03-04 14:46:31,535 - INFO - Epoch 6/50 done
2025-03-04 14:46:31,535 - INFO - Beginning epoch 7/50
2025-03-04 14:46:31,542 - INFO - training batch 1, loss: 0.230, 32/56000 datapoints
2025-03-04 14:46:31,647 - INFO - training batch 51, loss: 0.353, 1632/56000 datapoints
2025-03-04 14:46:31,732 - INFO - training batch 101, loss: 0.182, 3232/56000 datapoints
2025-03-04 14:46:31,875 - INFO - training batch 151, loss: 0.300, 4832/56000 datapoints
2025-03-04 14:46:32,223 - INFO - training batch 201, loss: 0.142, 6432/56000 datapoints
2025-03-04 14:46:33,519 - INFO - training batch 251, loss: 0.215, 8032/56000 datapoints
2025-03-04 14:46:33,621 - INFO - training batch 301, loss: 0.241, 9632/56000 datapoints
2025-03-04 14:46:33,727 - INFO - training batch 351, loss: 0.223, 11232/56000 datapoints
2025-03-04 14:46:33,905 - INFO - training batch 401, loss: 0.347, 12832/56000 datapoints
2025-03-04 14:46:34,054 - INFO - training batch 451, loss: 0.368, 14432/56000 datapoints
2025-03-04 14:46:34,176 - INFO - training batch 501, loss: 0.335, 16032/56000 datapoints
2025-03-04 14:46:34,285 - INFO - training batch 551, loss: 0.184, 17632/56000 datapoints
2025-03-04 14:46:34,385 - INFO - training batch 601, loss: 0.178, 19232/56000 datapoints
2025-03-04 14:46:34,487 - INFO - training batch 651, loss: 0.155, 20832/56000 datapoints
2025-03-04 14:46:34,582 - INFO - training batch 701, loss: 0.257, 22432/56000 datapoints
2025-03-04 14:46:34,662 - INFO - training batch 751, loss: 0.216, 24032/56000 datapoints
2025-03-04 14:46:34,735 - INFO - training batch 801, loss: 0.316, 25632/56000 datapoints
2025-03-04 14:46:34,824 - INFO - training batch 851, loss: 0.164, 27232/56000 datapoints
2025-03-04 14:46:34,938 - INFO - training batch 901, loss: 0.258, 28832/56000 datapoints
2025-03-04 14:46:35,049 - INFO - training batch 951, loss: 0.269, 30432/56000 datapoints
2025-03-04 14:46:35,156 - INFO - training batch 1001, loss: 0.140, 32032/56000 datapoints
2025-03-04 14:46:35,254 - INFO - training batch 1051, loss: 0.163, 33632/56000 datapoints
2025-03-04 14:46:35,353 - INFO - training batch 1101, loss: 0.254, 35232/56000 datapoints
2025-03-04 14:46:35,476 - INFO - training batch 1151, loss: 0.101, 36832/56000 datapoints
2025-03-04 14:46:35,587 - INFO - training batch 1201, loss: 0.237, 38432/56000 datapoints
2025-03-04 14:46:35,704 - INFO - training batch 1251, loss: 0.276, 40032/56000 datapoints
2025-03-04 14:46:35,803 - INFO - training batch 1301, loss: 0.244, 41632/56000 datapoints
2025-03-04 14:46:35,906 - INFO - training batch 1351, loss: 0.154, 43232/56000 datapoints
2025-03-04 14:46:36,021 - INFO - training batch 1401, loss: 0.109, 44832/56000 datapoints
2025-03-04 14:46:36,132 - INFO - training batch 1451, loss: 0.162, 46432/56000 datapoints
2025-03-04 14:46:36,280 - INFO - training batch 1501, loss: 0.331, 48032/56000 datapoints
2025-03-04 14:46:36,401 - INFO - training batch 1551, loss: 0.229, 49632/56000 datapoints
2025-03-04 14:46:36,517 - INFO - training batch 1601, loss: 0.260, 51232/56000 datapoints
2025-03-04 14:46:36,777 - INFO - training batch 1651, loss: 0.138, 52832/56000 datapoints
2025-03-04 14:46:37,108 - INFO - training batch 1701, loss: 0.214, 54432/56000 datapoints
2025-03-04 14:46:37,813 - INFO - validation batch 1, loss: 0.112, 32/13984 datapoints
2025-03-04 14:46:38,362 - INFO - validation batch 51, loss: 0.065, 1632/13984 datapoints
2025-03-04 14:46:38,974 - INFO - validation batch 101, loss: 0.209, 3232/13984 datapoints
2025-03-04 14:46:40,436 - INFO - validation batch 151, loss: 0.089, 4832/13984 datapoints
2025-03-04 14:46:41,037 - INFO - validation batch 201, loss: 0.205, 6432/13984 datapoints
2025-03-04 14:46:41,729 - INFO - validation batch 251, loss: 0.257, 8032/13984 datapoints
2025-03-04 14:46:41,974 - INFO - validation batch 301, loss: 0.352, 9632/13984 datapoints
2025-03-04 14:46:42,756 - INFO - validation batch 351, loss: 0.130, 11232/13984 datapoints
2025-03-04 14:46:43,621 - INFO - validation batch 401, loss: 0.289, 12832/13984 datapoints
2025-03-04 14:46:43,969 - INFO - Epoch 7/50 done
2025-03-04 14:46:43,970 - INFO - Beginning epoch 8/50
2025-03-04 14:46:43,999 - INFO - training batch 1, loss: 0.213, 32/56000 datapoints
2025-03-04 14:46:44,373 - INFO - training batch 51, loss: 0.274, 1632/56000 datapoints
2025-03-04 14:46:44,657 - INFO - training batch 101, loss: 0.141, 3232/56000 datapoints
2025-03-04 14:46:45,440 - INFO - training batch 151, loss: 0.237, 4832/56000 datapoints
2025-03-04 14:46:46,476 - INFO - training batch 201, loss: 0.119, 6432/56000 datapoints
2025-03-04 14:46:46,861 - INFO - training batch 251, loss: 0.162, 8032/56000 datapoints
2025-03-04 14:46:47,096 - INFO - training batch 301, loss: 0.183, 9632/56000 datapoints
2025-03-04 14:46:47,312 - INFO - training batch 351, loss: 0.170, 11232/56000 datapoints
2025-03-04 14:46:47,448 - INFO - training batch 401, loss: 0.268, 12832/56000 datapoints
2025-03-04 14:46:47,588 - INFO - training batch 451, loss: 0.290, 14432/56000 datapoints
2025-03-04 14:46:47,735 - INFO - training batch 501, loss: 0.268, 16032/56000 datapoints
2025-03-04 14:46:47,911 - INFO - training batch 551, loss: 0.140, 17632/56000 datapoints
2025-03-04 14:46:48,189 - INFO - training batch 601, loss: 0.121, 19232/56000 datapoints
2025-03-04 14:46:48,910 - INFO - training batch 651, loss: 0.111, 20832/56000 datapoints
2025-03-04 14:46:49,144 - INFO - training batch 701, loss: 0.208, 22432/56000 datapoints
2025-03-04 14:46:49,665 - INFO - training batch 751, loss: 0.136, 24032/56000 datapoints
2025-03-04 14:46:49,827 - INFO - training batch 801, loss: 0.266, 25632/56000 datapoints
2025-03-04 14:46:49,973 - INFO - training batch 851, loss: 0.115, 27232/56000 datapoints
2025-03-04 14:46:50,169 - INFO - training batch 901, loss: 0.169, 28832/56000 datapoints
2025-03-04 14:46:50,288 - INFO - training batch 951, loss: 0.181, 30432/56000 datapoints
2025-03-04 14:46:50,414 - INFO - training batch 1001, loss: 0.105, 32032/56000 datapoints
2025-03-04 14:46:50,834 - INFO - training batch 1051, loss: 0.124, 33632/56000 datapoints
2025-03-04 14:46:51,297 - INFO - training batch 1101, loss: 0.198, 35232/56000 datapoints
2025-03-04 14:46:51,584 - INFO - training batch 1151, loss: 0.064, 36832/56000 datapoints
2025-03-04 14:46:53,001 - INFO - training batch 1201, loss: 0.232, 38432/56000 datapoints
2025-03-04 14:46:53,651 - INFO - training batch 1251, loss: 0.246, 40032/56000 datapoints
2025-03-04 14:46:53,813 - INFO - training batch 1301, loss: 0.154, 41632/56000 datapoints
2025-03-04 14:46:54,062 - INFO - training batch 1351, loss: 0.117, 43232/56000 datapoints
2025-03-04 14:46:54,220 - INFO - training batch 1401, loss: 0.077, 44832/56000 datapoints
2025-03-04 14:46:54,376 - INFO - training batch 1451, loss: 0.117, 46432/56000 datapoints
2025-03-04 14:46:54,495 - INFO - training batch 1501, loss: 0.234, 48032/56000 datapoints
2025-03-04 14:46:54,606 - INFO - training batch 1551, loss: 0.142, 49632/56000 datapoints
2025-03-04 14:46:54,719 - INFO - training batch 1601, loss: 0.164, 51232/56000 datapoints
2025-03-04 14:46:54,834 - INFO - training batch 1651, loss: 0.099, 52832/56000 datapoints
2025-03-04 14:46:54,952 - INFO - training batch 1701, loss: 0.166, 54432/56000 datapoints
2025-03-04 14:46:55,071 - INFO - validation batch 1, loss: 0.076, 32/13984 datapoints
2025-03-04 14:46:55,121 - INFO - validation batch 51, loss: 0.052, 1632/13984 datapoints
2025-03-04 14:46:55,181 - INFO - validation batch 101, loss: 0.125, 3232/13984 datapoints
2025-03-04 14:46:55,254 - INFO - validation batch 151, loss: 0.066, 4832/13984 datapoints
2025-03-04 14:46:55,317 - INFO - validation batch 201, loss: 0.145, 6432/13984 datapoints
2025-03-04 14:46:55,373 - INFO - validation batch 251, loss: 0.198, 8032/13984 datapoints
2025-03-04 14:46:55,425 - INFO - validation batch 301, loss: 0.223, 9632/13984 datapoints
2025-03-04 14:46:55,493 - INFO - validation batch 351, loss: 0.102, 11232/13984 datapoints
2025-03-04 14:46:55,552 - INFO - validation batch 401, loss: 0.234, 12832/13984 datapoints
2025-03-04 14:46:55,587 - INFO - Epoch 8/50 done
2025-03-04 14:46:55,588 - INFO - Beginning epoch 9/50
2025-03-04 14:46:55,593 - INFO - training batch 1, loss: 0.194, 32/56000 datapoints
2025-03-04 14:46:55,693 - INFO - training batch 51, loss: 0.173, 1632/56000 datapoints
2025-03-04 14:46:55,812 - INFO - training batch 101, loss: 0.106, 3232/56000 datapoints
2025-03-04 14:46:55,928 - INFO - training batch 151, loss: 0.160, 4832/56000 datapoints
2025-03-04 14:46:56,066 - INFO - training batch 201, loss: 0.102, 6432/56000 datapoints
2025-03-04 14:46:56,183 - INFO - training batch 251, loss: 0.106, 8032/56000 datapoints
2025-03-04 14:46:56,313 - INFO - training batch 301, loss: 0.125, 9632/56000 datapoints
2025-03-04 14:46:56,417 - INFO - training batch 351, loss: 0.126, 11232/56000 datapoints
2025-03-04 14:46:56,521 - INFO - training batch 401, loss: 0.190, 12832/56000 datapoints
2025-03-04 14:46:56,617 - INFO - training batch 451, loss: 0.214, 14432/56000 datapoints
2025-03-04 14:46:56,721 - INFO - training batch 501, loss: 0.190, 16032/56000 datapoints
2025-03-04 14:46:56,822 - INFO - training batch 551, loss: 0.111, 17632/56000 datapoints
2025-03-04 14:46:56,925 - INFO - training batch 601, loss: 0.074, 19232/56000 datapoints
2025-03-04 14:46:57,049 - INFO - training batch 651, loss: 0.080, 20832/56000 datapoints
2025-03-04 14:46:57,143 - INFO - training batch 701, loss: 0.180, 22432/56000 datapoints
2025-03-04 14:46:57,223 - INFO - training batch 751, loss: 0.088, 24032/56000 datapoints
2025-03-04 14:46:57,297 - INFO - training batch 801, loss: 0.208, 25632/56000 datapoints
2025-03-04 14:46:57,371 - INFO - training batch 851, loss: 0.079, 27232/56000 datapoints
2025-03-04 14:46:57,449 - INFO - training batch 901, loss: 0.109, 28832/56000 datapoints
2025-03-04 14:46:57,534 - INFO - training batch 951, loss: 0.125, 30432/56000 datapoints
2025-03-04 14:46:57,606 - INFO - training batch 1001, loss: 0.078, 32032/56000 datapoints
2025-03-04 14:46:57,683 - INFO - training batch 1051, loss: 0.094, 33632/56000 datapoints
2025-03-04 14:46:57,760 - INFO - training batch 1101, loss: 0.150, 35232/56000 datapoints
2025-03-04 14:46:57,833 - INFO - training batch 1151, loss: 0.040, 36832/56000 datapoints
2025-03-04 14:46:57,921 - INFO - training batch 1201, loss: 0.205, 38432/56000 datapoints
2025-03-04 14:46:58,079 - INFO - training batch 1251, loss: 0.226, 40032/56000 datapoints
2025-03-04 14:46:58,173 - INFO - training batch 1301, loss: 0.099, 41632/56000 datapoints
2025-03-04 14:46:58,259 - INFO - training batch 1351, loss: 0.086, 43232/56000 datapoints
2025-03-04 14:46:58,337 - INFO - training batch 1401, loss: 0.054, 44832/56000 datapoints
2025-03-04 14:46:58,410 - INFO - training batch 1451, loss: 0.084, 46432/56000 datapoints
2025-03-04 14:46:58,493 - INFO - training batch 1501, loss: 0.167, 48032/56000 datapoints
2025-03-04 14:46:58,569 - INFO - training batch 1551, loss: 0.088, 49632/56000 datapoints
2025-03-04 14:46:58,642 - INFO - training batch 1601, loss: 0.102, 51232/56000 datapoints
2025-03-04 14:46:58,793 - INFO - training batch 1651, loss: 0.072, 52832/56000 datapoints
2025-03-04 14:46:58,888 - INFO - training batch 1701, loss: 0.135, 54432/56000 datapoints
2025-03-04 14:46:59,008 - INFO - validation batch 1, loss: 0.054, 32/13984 datapoints
2025-03-04 14:46:59,033 - INFO - validation batch 51, loss: 0.043, 1632/13984 datapoints
2025-03-04 14:46:59,068 - INFO - validation batch 101, loss: 0.077, 3232/13984 datapoints
2025-03-04 14:46:59,094 - INFO - validation batch 151, loss: 0.052, 4832/13984 datapoints
2025-03-04 14:46:59,116 - INFO - validation batch 201, loss: 0.100, 6432/13984 datapoints
2025-03-04 14:46:59,141 - INFO - validation batch 251, loss: 0.148, 8032/13984 datapoints
2025-03-04 14:46:59,165 - INFO - validation batch 301, loss: 0.142, 9632/13984 datapoints
2025-03-04 14:46:59,188 - INFO - validation batch 351, loss: 0.077, 11232/13984 datapoints
2025-03-04 14:46:59,212 - INFO - validation batch 401, loss: 0.196, 12832/13984 datapoints
2025-03-04 14:46:59,227 - INFO - Epoch 9/50 done
2025-03-04 14:46:59,227 - INFO - Beginning epoch 10/50
2025-03-04 14:46:59,232 - INFO - training batch 1, loss: 0.181, 32/56000 datapoints
2025-03-04 14:46:59,324 - INFO - training batch 51, loss: 0.111, 1632/56000 datapoints
2025-03-04 14:46:59,422 - INFO - training batch 101, loss: 0.082, 3232/56000 datapoints
2025-03-04 14:46:59,525 - INFO - training batch 151, loss: 0.116, 4832/56000 datapoints
2025-03-04 14:46:59,613 - INFO - training batch 201, loss: 0.092, 6432/56000 datapoints
2025-03-04 14:46:59,707 - INFO - training batch 251, loss: 0.071, 8032/56000 datapoints
2025-03-04 14:46:59,781 - INFO - training batch 301, loss: 0.085, 9632/56000 datapoints
2025-03-04 14:46:59,863 - INFO - training batch 351, loss: 0.102, 11232/56000 datapoints
2025-03-04 14:46:59,984 - INFO - training batch 401, loss: 0.144, 12832/56000 datapoints
2025-03-04 14:47:00,134 - INFO - training batch 451, loss: 0.160, 14432/56000 datapoints
2025-03-04 14:47:00,375 - INFO - training batch 501, loss: 0.138, 16032/56000 datapoints
2025-03-04 14:47:00,609 - INFO - training batch 551, loss: 0.097, 17632/56000 datapoints
2025-03-04 14:47:01,011 - INFO - training batch 601, loss: 0.045, 19232/56000 datapoints
2025-03-04 14:47:01,179 - INFO - training batch 651, loss: 0.059, 20832/56000 datapoints
2025-03-04 14:47:01,376 - INFO - training batch 701, loss: 0.170, 22432/56000 datapoints
2025-03-04 14:47:01,649 - INFO - training batch 751, loss: 0.061, 24032/56000 datapoints
2025-03-04 14:47:01,785 - INFO - training batch 801, loss: 0.165, 25632/56000 datapoints
2025-03-04 14:47:01,890 - INFO - training batch 851, loss: 0.054, 27232/56000 datapoints
2025-03-04 14:47:02,045 - INFO - training batch 901, loss: 0.072, 28832/56000 datapoints
2025-03-04 14:47:02,150 - INFO - training batch 951, loss: 0.089, 30432/56000 datapoints
2025-03-04 14:47:02,251 - INFO - training batch 1001, loss: 0.060, 32032/56000 datapoints
2025-03-04 14:47:02,418 - INFO - training batch 1051, loss: 0.074, 33632/56000 datapoints
2025-03-04 14:47:02,551 - INFO - training batch 1101, loss: 0.120, 35232/56000 datapoints
2025-03-04 14:47:02,667 - INFO - training batch 1151, loss: 0.025, 36832/56000 datapoints
2025-03-04 14:47:02,798 - INFO - training batch 1201, loss: 0.183, 38432/56000 datapoints
2025-03-04 14:47:02,923 - INFO - training batch 1251, loss: 0.218, 40032/56000 datapoints
2025-03-04 14:47:03,137 - INFO - training batch 1301, loss: 0.067, 41632/56000 datapoints
2025-03-04 14:47:03,248 - INFO - training batch 1351, loss: 0.063, 43232/56000 datapoints
2025-03-04 14:47:03,348 - INFO - training batch 1401, loss: 0.038, 44832/56000 datapoints
2025-03-04 14:47:03,444 - INFO - training batch 1451, loss: 0.059, 46432/56000 datapoints
2025-03-04 14:47:03,550 - INFO - training batch 1501, loss: 0.127, 48032/56000 datapoints
2025-03-04 14:47:03,643 - INFO - training batch 1551, loss: 0.059, 49632/56000 datapoints
2025-03-04 14:47:03,729 - INFO - training batch 1601, loss: 0.068, 51232/56000 datapoints
2025-03-04 14:47:03,832 - INFO - training batch 1651, loss: 0.055, 52832/56000 datapoints
2025-03-04 14:47:03,919 - INFO - training batch 1701, loss: 0.117, 54432/56000 datapoints
2025-03-04 14:47:04,002 - INFO - validation batch 1, loss: 0.040, 32/13984 datapoints
2025-03-04 14:47:04,035 - INFO - validation batch 51, loss: 0.037, 1632/13984 datapoints
2025-03-04 14:47:04,060 - INFO - validation batch 101, loss: 0.050, 3232/13984 datapoints
2025-03-04 14:47:04,086 - INFO - validation batch 151, loss: 0.045, 4832/13984 datapoints
2025-03-04 14:47:04,111 - INFO - validation batch 201, loss: 0.070, 6432/13984 datapoints
2025-03-04 14:47:04,144 - INFO - validation batch 251, loss: 0.117, 8032/13984 datapoints
2025-03-04 14:47:04,174 - INFO - validation batch 301, loss: 0.095, 9632/13984 datapoints
2025-03-04 14:47:04,200 - INFO - validation batch 351, loss: 0.060, 11232/13984 datapoints
2025-03-04 14:47:04,234 - INFO - validation batch 401, loss: 0.172, 12832/13984 datapoints
2025-03-04 14:47:04,254 - INFO - Epoch 10/50 done
2025-03-04 14:47:04,254 - INFO - Beginning epoch 11/50
2025-03-04 14:47:04,258 - INFO - training batch 1, loss: 0.174, 32/56000 datapoints
2025-03-04 14:47:04,380 - INFO - training batch 51, loss: 0.076, 1632/56000 datapoints
2025-03-04 14:47:04,543 - INFO - training batch 101, loss: 0.069, 3232/56000 datapoints
2025-03-04 14:47:04,673 - INFO - training batch 151, loss: 0.093, 4832/56000 datapoints
2025-03-04 14:47:04,807 - INFO - training batch 201, loss: 0.087, 6432/56000 datapoints
2025-03-04 14:47:04,942 - INFO - training batch 251, loss: 0.051, 8032/56000 datapoints
2025-03-04 14:47:05,058 - INFO - training batch 301, loss: 0.061, 9632/56000 datapoints
2025-03-04 14:47:05,179 - INFO - training batch 351, loss: 0.090, 11232/56000 datapoints
2025-03-04 14:47:05,292 - INFO - training batch 401, loss: 0.117, 12832/56000 datapoints
2025-03-04 14:47:05,413 - INFO - training batch 451, loss: 0.128, 14432/56000 datapoints
2025-03-04 14:47:05,562 - INFO - training batch 501, loss: 0.104, 16032/56000 datapoints
2025-03-04 14:47:05,808 - INFO - training batch 551, loss: 0.092, 17632/56000 datapoints
2025-03-04 14:47:05,913 - INFO - training batch 601, loss: 0.029, 19232/56000 datapoints
2025-03-04 14:47:06,021 - INFO - training batch 651, loss: 0.046, 20832/56000 datapoints
2025-03-04 14:47:06,125 - INFO - training batch 701, loss: 0.171, 22432/56000 datapoints
2025-03-04 14:47:06,215 - INFO - training batch 751, loss: 0.045, 24032/56000 datapoints
2025-03-04 14:47:06,307 - INFO - training batch 801, loss: 0.137, 25632/56000 datapoints
2025-03-04 14:47:06,393 - INFO - training batch 851, loss: 0.038, 27232/56000 datapoints
2025-03-04 14:47:06,519 - INFO - training batch 901, loss: 0.051, 28832/56000 datapoints
2025-03-04 14:47:06,769 - INFO - training batch 951, loss: 0.066, 30432/56000 datapoints
2025-03-04 14:47:07,028 - INFO - training batch 1001, loss: 0.048, 32032/56000 datapoints
2025-03-04 14:47:07,682 - INFO - training batch 1051, loss: 0.061, 33632/56000 datapoints
2025-03-04 14:47:07,924 - INFO - training batch 1101, loss: 0.101, 35232/56000 datapoints
2025-03-04 14:47:08,063 - INFO - training batch 1151, loss: 0.016, 36832/56000 datapoints
2025-03-04 14:47:08,180 - INFO - training batch 1201, loss: 0.166, 38432/56000 datapoints
2025-03-04 14:47:08,771 - INFO - training batch 1251, loss: 0.217, 40032/56000 datapoints
2025-03-04 14:47:08,909 - INFO - training batch 1301, loss: 0.048, 41632/56000 datapoints
2025-03-04 14:47:09,029 - INFO - training batch 1351, loss: 0.048, 43232/56000 datapoints
2025-03-04 14:47:09,230 - INFO - training batch 1401, loss: 0.028, 44832/56000 datapoints
2025-03-04 14:47:09,743 - INFO - training batch 1451, loss: 0.043, 46432/56000 datapoints
2025-03-04 14:47:10,397 - INFO - training batch 1501, loss: 0.102, 48032/56000 datapoints
2025-03-04 14:47:10,564 - INFO - training batch 1551, loss: 0.043, 49632/56000 datapoints
2025-03-04 14:47:11,235 - INFO - training batch 1601, loss: 0.048, 51232/56000 datapoints
2025-03-04 14:47:11,422 - INFO - training batch 1651, loss: 0.044, 52832/56000 datapoints
2025-03-04 14:47:11,545 - INFO - training batch 1701, loss: 0.106, 54432/56000 datapoints
2025-03-04 14:47:11,664 - INFO - validation batch 1, loss: 0.031, 32/13984 datapoints
2025-03-04 14:47:11,715 - INFO - validation batch 51, loss: 0.033, 1632/13984 datapoints
2025-03-04 14:47:11,768 - INFO - validation batch 101, loss: 0.034, 3232/13984 datapoints
2025-03-04 14:47:11,822 - INFO - validation batch 151, loss: 0.040, 4832/13984 datapoints
2025-03-04 14:47:12,214 - INFO - validation batch 201, loss: 0.051, 6432/13984 datapoints
2025-03-04 14:47:12,385 - INFO - validation batch 251, loss: 0.097, 8032/13984 datapoints
2025-03-04 14:47:12,533 - INFO - validation batch 301, loss: 0.067, 9632/13984 datapoints
2025-03-04 14:47:12,854 - INFO - validation batch 351, loss: 0.047, 11232/13984 datapoints
2025-03-04 14:47:13,054 - INFO - validation batch 401, loss: 0.160, 12832/13984 datapoints
2025-03-04 14:47:13,261 - INFO - Epoch 11/50 done
2025-03-04 14:47:13,263 - INFO - Beginning epoch 12/50
2025-03-04 14:47:13,268 - INFO - training batch 1, loss: 0.171, 32/56000 datapoints
2025-03-04 14:47:13,460 - INFO - training batch 51, loss: 0.056, 1632/56000 datapoints
2025-03-04 14:47:13,665 - INFO - training batch 101, loss: 0.062, 3232/56000 datapoints
2025-03-04 14:47:13,875 - INFO - training batch 151, loss: 0.081, 4832/56000 datapoints
2025-03-04 14:47:14,075 - INFO - training batch 201, loss: 0.083, 6432/56000 datapoints
2025-03-04 14:47:14,314 - INFO - training batch 251, loss: 0.039, 8032/56000 datapoints
2025-03-04 14:47:14,541 - INFO - training batch 301, loss: 0.046, 9632/56000 datapoints
2025-03-04 14:47:14,741 - INFO - training batch 351, loss: 0.085, 11232/56000 datapoints
2025-03-04 14:47:14,894 - INFO - training batch 401, loss: 0.101, 12832/56000 datapoints
2025-03-04 14:47:15,027 - INFO - training batch 451, loss: 0.107, 14432/56000 datapoints
2025-03-04 14:47:15,145 - INFO - training batch 501, loss: 0.081, 16032/56000 datapoints
2025-03-04 14:47:15,265 - INFO - training batch 551, loss: 0.090, 17632/56000 datapoints
2025-03-04 14:47:15,390 - INFO - training batch 601, loss: 0.020, 19232/56000 datapoints
2025-03-04 14:47:15,514 - INFO - training batch 651, loss: 0.037, 20832/56000 datapoints
2025-03-04 14:47:15,636 - INFO - training batch 701, loss: 0.177, 22432/56000 datapoints
2025-03-04 14:47:15,736 - INFO - training batch 751, loss: 0.036, 24032/56000 datapoints
2025-03-04 14:47:15,851 - INFO - training batch 801, loss: 0.117, 25632/56000 datapoints
2025-03-04 14:47:15,955 - INFO - training batch 851, loss: 0.028, 27232/56000 datapoints
2025-03-04 14:47:16,093 - INFO - training batch 901, loss: 0.038, 28832/56000 datapoints
2025-03-04 14:47:16,198 - INFO - training batch 951, loss: 0.051, 30432/56000 datapoints
2025-03-04 14:47:16,304 - INFO - training batch 1001, loss: 0.039, 32032/56000 datapoints
2025-03-04 14:47:16,423 - INFO - training batch 1051, loss: 0.052, 33632/56000 datapoints
2025-03-04 14:47:16,531 - INFO - training batch 1101, loss: 0.090, 35232/56000 datapoints
2025-03-04 14:47:16,641 - INFO - training batch 1151, loss: 0.011, 36832/56000 datapoints
2025-03-04 14:47:16,729 - INFO - training batch 1201, loss: 0.151, 38432/56000 datapoints
2025-03-04 14:47:16,805 - INFO - training batch 1251, loss: 0.219, 40032/56000 datapoints
2025-03-04 14:47:16,888 - INFO - training batch 1301, loss: 0.038, 41632/56000 datapoints
2025-03-04 14:47:16,968 - INFO - training batch 1351, loss: 0.037, 43232/56000 datapoints
2025-03-04 14:47:17,041 - INFO - training batch 1401, loss: 0.021, 44832/56000 datapoints
2025-03-04 14:47:17,111 - INFO - training batch 1451, loss: 0.032, 46432/56000 datapoints
2025-03-04 14:47:17,181 - INFO - training batch 1501, loss: 0.088, 48032/56000 datapoints
2025-03-04 14:47:17,252 - INFO - training batch 1551, loss: 0.032, 49632/56000 datapoints
2025-03-04 14:47:17,330 - INFO - training batch 1601, loss: 0.036, 51232/56000 datapoints
2025-03-04 14:47:17,401 - INFO - training batch 1651, loss: 0.036, 52832/56000 datapoints
2025-03-04 14:47:17,474 - INFO - training batch 1701, loss: 0.100, 54432/56000 datapoints
2025-03-04 14:47:17,579 - INFO - validation batch 1, loss: 0.025, 32/13984 datapoints
2025-03-04 14:47:17,606 - INFO - validation batch 51, loss: 0.030, 1632/13984 datapoints
2025-03-04 14:47:17,631 - INFO - validation batch 101, loss: 0.025, 3232/13984 datapoints
2025-03-04 14:47:17,652 - INFO - validation batch 151, loss: 0.037, 4832/13984 datapoints
2025-03-04 14:47:17,678 - INFO - validation batch 201, loss: 0.040, 6432/13984 datapoints
2025-03-04 14:47:17,706 - INFO - validation batch 251, loss: 0.084, 8032/13984 datapoints
2025-03-04 14:47:17,731 - INFO - validation batch 301, loss: 0.050, 9632/13984 datapoints
2025-03-04 14:47:17,754 - INFO - validation batch 351, loss: 0.038, 11232/13984 datapoints
2025-03-04 14:47:17,780 - INFO - validation batch 401, loss: 0.152, 12832/13984 datapoints
2025-03-04 14:47:17,797 - INFO - Epoch 12/50 done
2025-03-04 14:47:17,798 - INFO - Beginning epoch 13/50
2025-03-04 14:47:17,800 - INFO - training batch 1, loss: 0.170, 32/56000 datapoints
2025-03-04 14:47:17,887 - INFO - training batch 51, loss: 0.043, 1632/56000 datapoints
2025-03-04 14:47:17,966 - INFO - training batch 101, loss: 0.058, 3232/56000 datapoints
2025-03-04 14:47:18,043 - INFO - training batch 151, loss: 0.075, 4832/56000 datapoints
2025-03-04 14:47:18,134 - INFO - training batch 201, loss: 0.080, 6432/56000 datapoints
2025-03-04 14:47:18,214 - INFO - training batch 251, loss: 0.031, 8032/56000 datapoints
2025-03-04 14:47:18,292 - INFO - training batch 301, loss: 0.036, 9632/56000 datapoints
2025-03-04 14:47:18,361 - INFO - training batch 351, loss: 0.083, 11232/56000 datapoints
2025-03-04 14:47:18,430 - INFO - training batch 401, loss: 0.091, 12832/56000 datapoints
2025-03-04 14:47:18,498 - INFO - training batch 451, loss: 0.092, 14432/56000 datapoints
2025-03-04 14:47:18,572 - INFO - training batch 501, loss: 0.066, 16032/56000 datapoints
2025-03-04 14:47:18,645 - INFO - training batch 551, loss: 0.091, 17632/56000 datapoints
2025-03-04 14:47:18,711 - INFO - training batch 601, loss: 0.015, 19232/56000 datapoints
2025-03-04 14:47:18,784 - INFO - training batch 651, loss: 0.030, 20832/56000 datapoints
2025-03-04 14:47:18,852 - INFO - training batch 701, loss: 0.184, 22432/56000 datapoints
2025-03-04 14:47:18,930 - INFO - training batch 751, loss: 0.031, 24032/56000 datapoints
2025-03-04 14:47:18,998 - INFO - training batch 801, loss: 0.101, 25632/56000 datapoints
2025-03-04 14:47:19,075 - INFO - training batch 851, loss: 0.021, 27232/56000 datapoints
2025-03-04 14:47:19,153 - INFO - training batch 901, loss: 0.029, 28832/56000 datapoints
2025-03-04 14:47:19,246 - INFO - training batch 951, loss: 0.042, 30432/56000 datapoints
2025-03-04 14:47:19,352 - INFO - training batch 1001, loss: 0.033, 32032/56000 datapoints
2025-03-04 14:47:19,449 - INFO - training batch 1051, loss: 0.045, 33632/56000 datapoints
2025-03-04 14:47:19,541 - INFO - training batch 1101, loss: 0.084, 35232/56000 datapoints
2025-03-04 14:47:19,634 - INFO - training batch 1151, loss: 0.008, 36832/56000 datapoints
2025-03-04 14:47:19,738 - INFO - training batch 1201, loss: 0.139, 38432/56000 datapoints
2025-03-04 14:47:19,837 - INFO - training batch 1251, loss: 0.223, 40032/56000 datapoints
2025-03-04 14:47:19,927 - INFO - training batch 1301, loss: 0.031, 41632/56000 datapoints
2025-03-04 14:47:20,003 - INFO - training batch 1351, loss: 0.030, 43232/56000 datapoints
2025-03-04 14:47:20,080 - INFO - training batch 1401, loss: 0.017, 44832/56000 datapoints
2025-03-04 14:47:20,163 - INFO - training batch 1451, loss: 0.025, 46432/56000 datapoints
2025-03-04 14:47:20,243 - INFO - training batch 1501, loss: 0.079, 48032/56000 datapoints
2025-03-04 14:47:20,320 - INFO - training batch 1551, loss: 0.026, 49632/56000 datapoints
2025-03-04 14:47:20,396 - INFO - training batch 1601, loss: 0.028, 51232/56000 datapoints
2025-03-04 14:47:20,474 - INFO - training batch 1651, loss: 0.031, 52832/56000 datapoints
2025-03-04 14:47:20,547 - INFO - training batch 1701, loss: 0.096, 54432/56000 datapoints
2025-03-04 14:47:20,615 - INFO - validation batch 1, loss: 0.021, 32/13984 datapoints
2025-03-04 14:47:20,643 - INFO - validation batch 51, loss: 0.028, 1632/13984 datapoints
2025-03-04 14:47:20,671 - INFO - validation batch 101, loss: 0.020, 3232/13984 datapoints
2025-03-04 14:47:20,699 - INFO - validation batch 151, loss: 0.035, 4832/13984 datapoints
2025-03-04 14:47:20,727 - INFO - validation batch 201, loss: 0.032, 6432/13984 datapoints
2025-03-04 14:47:20,750 - INFO - validation batch 251, loss: 0.073, 8032/13984 datapoints
2025-03-04 14:47:20,774 - INFO - validation batch 301, loss: 0.039, 9632/13984 datapoints
2025-03-04 14:47:20,816 - INFO - validation batch 351, loss: 0.032, 11232/13984 datapoints
2025-03-04 14:47:20,842 - INFO - validation batch 401, loss: 0.148, 12832/13984 datapoints
2025-03-04 14:47:20,858 - INFO - Epoch 13/50 done
2025-03-04 14:47:20,859 - INFO - Beginning epoch 14/50
2025-03-04 14:47:20,861 - INFO - training batch 1, loss: 0.169, 32/56000 datapoints
2025-03-04 14:47:20,949 - INFO - training batch 51, loss: 0.035, 1632/56000 datapoints
2025-03-04 14:47:21,020 - INFO - training batch 101, loss: 0.057, 3232/56000 datapoints
2025-03-04 14:47:21,103 - INFO - training batch 151, loss: 0.071, 4832/56000 datapoints
2025-03-04 14:47:21,189 - INFO - training batch 201, loss: 0.077, 6432/56000 datapoints
2025-03-04 14:47:21,291 - INFO - training batch 251, loss: 0.027, 8032/56000 datapoints
2025-03-04 14:47:21,380 - INFO - training batch 301, loss: 0.030, 9632/56000 datapoints
2025-03-04 14:47:21,454 - INFO - training batch 351, loss: 0.082, 11232/56000 datapoints
2025-03-04 14:47:21,530 - INFO - training batch 401, loss: 0.084, 12832/56000 datapoints
2025-03-04 14:47:21,601 - INFO - training batch 451, loss: 0.080, 14432/56000 datapoints
2025-03-04 14:47:21,673 - INFO - training batch 501, loss: 0.055, 16032/56000 datapoints
2025-03-04 14:47:21,741 - INFO - training batch 551, loss: 0.092, 17632/56000 datapoints
2025-03-04 14:47:21,812 - INFO - training batch 601, loss: 0.011, 19232/56000 datapoints
2025-03-04 14:47:21,881 - INFO - training batch 651, loss: 0.026, 20832/56000 datapoints
2025-03-04 14:47:21,954 - INFO - training batch 701, loss: 0.191, 22432/56000 datapoints
2025-03-04 14:47:22,021 - INFO - training batch 751, loss: 0.027, 24032/56000 datapoints
2025-03-04 14:47:22,090 - INFO - training batch 801, loss: 0.089, 25632/56000 datapoints
2025-03-04 14:47:22,165 - INFO - training batch 851, loss: 0.017, 27232/56000 datapoints
2025-03-04 14:47:22,234 - INFO - training batch 901, loss: 0.024, 28832/56000 datapoints
2025-03-04 14:47:22,301 - INFO - training batch 951, loss: 0.036, 30432/56000 datapoints
2025-03-04 14:47:22,386 - INFO - training batch 1001, loss: 0.028, 32032/56000 datapoints
2025-03-04 14:47:22,463 - INFO - training batch 1051, loss: 0.040, 33632/56000 datapoints
2025-03-04 14:47:22,538 - INFO - training batch 1101, loss: 0.080, 35232/56000 datapoints
2025-03-04 14:47:22,614 - INFO - training batch 1151, loss: 0.006, 36832/56000 datapoints
2025-03-04 14:47:22,686 - INFO - training batch 1201, loss: 0.128, 38432/56000 datapoints
2025-03-04 14:47:22,762 - INFO - training batch 1251, loss: 0.226, 40032/56000 datapoints
2025-03-04 14:47:22,833 - INFO - training batch 1301, loss: 0.027, 41632/56000 datapoints
2025-03-04 14:47:22,914 - INFO - training batch 1351, loss: 0.025, 43232/56000 datapoints
2025-03-04 14:47:23,027 - INFO - training batch 1401, loss: 0.014, 44832/56000 datapoints
2025-03-04 14:47:23,093 - INFO - training batch 1451, loss: 0.020, 46432/56000 datapoints
2025-03-04 14:47:23,157 - INFO - training batch 1501, loss: 0.074, 48032/56000 datapoints
2025-03-04 14:47:23,227 - INFO - training batch 1551, loss: 0.021, 49632/56000 datapoints
2025-03-04 14:47:23,296 - INFO - training batch 1601, loss: 0.023, 51232/56000 datapoints
2025-03-04 14:47:23,384 - INFO - training batch 1651, loss: 0.027, 52832/56000 datapoints
2025-03-04 14:47:23,468 - INFO - training batch 1701, loss: 0.093, 54432/56000 datapoints
2025-03-04 14:47:23,547 - INFO - validation batch 1, loss: 0.018, 32/13984 datapoints
2025-03-04 14:47:23,574 - INFO - validation batch 51, loss: 0.026, 1632/13984 datapoints
2025-03-04 14:47:23,601 - INFO - validation batch 101, loss: 0.016, 3232/13984 datapoints
2025-03-04 14:47:23,624 - INFO - validation batch 151, loss: 0.033, 4832/13984 datapoints
2025-03-04 14:47:23,650 - INFO - validation batch 201, loss: 0.027, 6432/13984 datapoints
2025-03-04 14:47:23,675 - INFO - validation batch 251, loss: 0.064, 8032/13984 datapoints
2025-03-04 14:47:23,703 - INFO - validation batch 301, loss: 0.032, 9632/13984 datapoints
2025-03-04 14:47:23,727 - INFO - validation batch 351, loss: 0.027, 11232/13984 datapoints
2025-03-04 14:47:23,755 - INFO - validation batch 401, loss: 0.146, 12832/13984 datapoints
2025-03-04 14:47:23,790 - INFO - Epoch 14/50 done
2025-03-04 14:47:23,797 - INFO - Beginning epoch 15/50
2025-03-04 14:47:23,799 - INFO - training batch 1, loss: 0.169, 32/56000 datapoints
2025-03-04 14:47:23,884 - INFO - training batch 51, loss: 0.029, 1632/56000 datapoints
2025-03-04 14:47:23,977 - INFO - training batch 101, loss: 0.057, 3232/56000 datapoints
2025-03-04 14:47:24,064 - INFO - training batch 151, loss: 0.069, 4832/56000 datapoints
2025-03-04 14:47:24,164 - INFO - training batch 201, loss: 0.075, 6432/56000 datapoints
2025-03-04 14:47:24,262 - INFO - training batch 251, loss: 0.023, 8032/56000 datapoints
2025-03-04 14:47:24,345 - INFO - training batch 301, loss: 0.026, 9632/56000 datapoints
2025-03-04 14:47:24,422 - INFO - training batch 351, loss: 0.081, 11232/56000 datapoints
2025-03-04 14:47:24,502 - INFO - training batch 401, loss: 0.080, 12832/56000 datapoints
2025-03-04 14:47:24,582 - INFO - training batch 451, loss: 0.071, 14432/56000 datapoints
2025-03-04 14:47:24,656 - INFO - training batch 501, loss: 0.046, 16032/56000 datapoints
2025-03-04 14:47:24,724 - INFO - training batch 551, loss: 0.093, 17632/56000 datapoints
2025-03-04 14:47:24,790 - INFO - training batch 601, loss: 0.009, 19232/56000 datapoints
2025-03-04 14:47:24,856 - INFO - training batch 651, loss: 0.022, 20832/56000 datapoints
2025-03-04 14:47:24,932 - INFO - training batch 701, loss: 0.197, 22432/56000 datapoints
2025-03-04 14:47:24,996 - INFO - training batch 751, loss: 0.025, 24032/56000 datapoints
2025-03-04 14:47:25,348 - INFO - training batch 801, loss: 0.078, 25632/56000 datapoints
2025-03-04 14:47:25,472 - INFO - training batch 851, loss: 0.014, 27232/56000 datapoints
2025-03-04 14:47:25,542 - INFO - training batch 901, loss: 0.020, 28832/56000 datapoints
2025-03-04 14:47:25,610 - INFO - training batch 951, loss: 0.031, 30432/56000 datapoints
2025-03-04 14:47:25,677 - INFO - training batch 1001, loss: 0.025, 32032/56000 datapoints
2025-03-04 14:47:25,758 - INFO - training batch 1051, loss: 0.036, 33632/56000 datapoints
2025-03-04 14:47:25,852 - INFO - training batch 1101, loss: 0.077, 35232/56000 datapoints
2025-03-04 14:47:25,941 - INFO - training batch 1151, loss: 0.005, 36832/56000 datapoints
2025-03-04 14:47:26,298 - INFO - training batch 1201, loss: 0.118, 38432/56000 datapoints
2025-03-04 14:47:26,378 - INFO - training batch 1251, loss: 0.230, 40032/56000 datapoints
2025-03-04 14:47:26,456 - INFO - training batch 1301, loss: 0.024, 41632/56000 datapoints
2025-03-04 14:47:26,533 - INFO - training batch 1351, loss: 0.021, 43232/56000 datapoints
2025-03-04 14:47:26,607 - INFO - training batch 1401, loss: 0.012, 44832/56000 datapoints
2025-03-04 14:47:26,678 - INFO - training batch 1451, loss: 0.017, 46432/56000 datapoints
2025-03-04 14:47:26,744 - INFO - training batch 1501, loss: 0.071, 48032/56000 datapoints
2025-03-04 14:47:26,821 - INFO - training batch 1551, loss: 0.018, 49632/56000 datapoints
2025-03-04 14:47:26,902 - INFO - training batch 1601, loss: 0.020, 51232/56000 datapoints
2025-03-04 14:47:26,991 - INFO - training batch 1651, loss: 0.024, 52832/56000 datapoints
2025-03-04 14:47:27,072 - INFO - training batch 1701, loss: 0.091, 54432/56000 datapoints
2025-03-04 14:47:27,148 - INFO - validation batch 1, loss: 0.016, 32/13984 datapoints
2025-03-04 14:47:27,170 - INFO - validation batch 51, loss: 0.024, 1632/13984 datapoints
2025-03-04 14:47:27,200 - INFO - validation batch 101, loss: 0.014, 3232/13984 datapoints
2025-03-04 14:47:27,227 - INFO - validation batch 151, loss: 0.032, 4832/13984 datapoints
2025-03-04 14:47:27,253 - INFO - validation batch 201, loss: 0.023, 6432/13984 datapoints
2025-03-04 14:47:27,279 - INFO - validation batch 251, loss: 0.057, 8032/13984 datapoints
2025-03-04 14:47:27,303 - INFO - validation batch 301, loss: 0.026, 9632/13984 datapoints
2025-03-04 14:47:27,331 - INFO - validation batch 351, loss: 0.023, 11232/13984 datapoints
2025-03-04 14:47:27,355 - INFO - validation batch 401, loss: 0.144, 12832/13984 datapoints
2025-03-04 14:47:27,373 - INFO - Epoch 15/50 done
2025-03-04 14:47:27,373 - INFO - Beginning epoch 16/50
2025-03-04 14:47:27,376 - INFO - training batch 1, loss: 0.169, 32/56000 datapoints
2025-03-04 14:47:27,461 - INFO - training batch 51, loss: 0.025, 1632/56000 datapoints
2025-03-04 14:47:27,542 - INFO - training batch 101, loss: 0.058, 3232/56000 datapoints
2025-03-04 14:47:27,615 - INFO - training batch 151, loss: 0.067, 4832/56000 datapoints
2025-03-04 14:47:27,686 - INFO - training batch 201, loss: 0.073, 6432/56000 datapoints
2025-03-04 14:47:27,755 - INFO - training batch 251, loss: 0.021, 8032/56000 datapoints
2025-03-04 14:47:27,828 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-04 14:47:27,896 - INFO - training batch 351, loss: 0.081, 11232/56000 datapoints
2025-03-04 14:47:27,972 - INFO - training batch 401, loss: 0.078, 12832/56000 datapoints
2025-03-04 14:47:28,036 - INFO - training batch 451, loss: 0.064, 14432/56000 datapoints
2025-03-04 14:47:28,102 - INFO - training batch 501, loss: 0.040, 16032/56000 datapoints
2025-03-04 14:47:28,173 - INFO - training batch 551, loss: 0.094, 17632/56000 datapoints
2025-03-04 14:47:28,258 - INFO - training batch 601, loss: 0.008, 19232/56000 datapoints
2025-03-04 14:47:28,339 - INFO - training batch 651, loss: 0.020, 20832/56000 datapoints
2025-03-04 14:47:28,412 - INFO - training batch 701, loss: 0.202, 22432/56000 datapoints
2025-03-04 14:47:28,520 - INFO - training batch 751, loss: 0.023, 24032/56000 datapoints
2025-03-04 14:47:28,602 - INFO - training batch 801, loss: 0.070, 25632/56000 datapoints
2025-03-04 14:47:28,676 - INFO - training batch 851, loss: 0.012, 27232/56000 datapoints
2025-03-04 14:47:28,751 - INFO - training batch 901, loss: 0.017, 28832/56000 datapoints
2025-03-04 14:47:28,819 - INFO - training batch 951, loss: 0.028, 30432/56000 datapoints
2025-03-04 14:47:28,887 - INFO - training batch 1001, loss: 0.022, 32032/56000 datapoints
2025-03-04 14:47:28,970 - INFO - training batch 1051, loss: 0.033, 33632/56000 datapoints
2025-03-04 14:47:29,046 - INFO - training batch 1101, loss: 0.076, 35232/56000 datapoints
2025-03-04 14:47:29,112 - INFO - training batch 1151, loss: 0.004, 36832/56000 datapoints
2025-03-04 14:47:29,186 - INFO - training batch 1201, loss: 0.108, 38432/56000 datapoints
2025-03-04 14:47:29,255 - INFO - training batch 1251, loss: 0.233, 40032/56000 datapoints
2025-03-04 14:47:29,323 - INFO - training batch 1301, loss: 0.021, 41632/56000 datapoints
2025-03-04 14:47:29,389 - INFO - training batch 1351, loss: 0.018, 43232/56000 datapoints
2025-03-04 14:47:29,455 - INFO - training batch 1401, loss: 0.011, 44832/56000 datapoints
2025-03-04 14:47:29,524 - INFO - training batch 1451, loss: 0.015, 46432/56000 datapoints
2025-03-04 14:47:29,590 - INFO - training batch 1501, loss: 0.069, 48032/56000 datapoints
2025-03-04 14:47:29,656 - INFO - training batch 1551, loss: 0.016, 49632/56000 datapoints
2025-03-04 14:47:29,729 - INFO - training batch 1601, loss: 0.018, 51232/56000 datapoints
2025-03-04 14:47:29,800 - INFO - training batch 1651, loss: 0.022, 52832/56000 datapoints
2025-03-04 14:47:29,873 - INFO - training batch 1701, loss: 0.090, 54432/56000 datapoints
2025-03-04 14:47:29,951 - INFO - validation batch 1, loss: 0.015, 32/13984 datapoints
2025-03-04 14:47:29,974 - INFO - validation batch 51, loss: 0.022, 1632/13984 datapoints
2025-03-04 14:47:30,002 - INFO - validation batch 101, loss: 0.012, 3232/13984 datapoints
2025-03-04 14:47:30,023 - INFO - validation batch 151, loss: 0.030, 4832/13984 datapoints
2025-03-04 14:47:30,048 - INFO - validation batch 201, loss: 0.020, 6432/13984 datapoints
2025-03-04 14:47:30,076 - INFO - validation batch 251, loss: 0.050, 8032/13984 datapoints
2025-03-04 14:47:30,100 - INFO - validation batch 301, loss: 0.023, 9632/13984 datapoints
2025-03-04 14:47:30,122 - INFO - validation batch 351, loss: 0.020, 11232/13984 datapoints
2025-03-04 14:47:30,145 - INFO - validation batch 401, loss: 0.143, 12832/13984 datapoints
2025-03-04 14:47:30,164 - INFO - Epoch 16/50 done
2025-03-04 14:47:30,165 - INFO - Beginning epoch 17/50
2025-03-04 14:47:30,168 - INFO - training batch 1, loss: 0.168, 32/56000 datapoints
2025-03-04 14:47:30,268 - INFO - training batch 51, loss: 0.022, 1632/56000 datapoints
2025-03-04 14:47:30,341 - INFO - training batch 101, loss: 0.058, 3232/56000 datapoints
2025-03-04 14:47:30,416 - INFO - training batch 151, loss: 0.065, 4832/56000 datapoints
2025-03-04 14:47:30,499 - INFO - training batch 201, loss: 0.070, 6432/56000 datapoints
2025-03-04 14:47:30,575 - INFO - training batch 251, loss: 0.019, 8032/56000 datapoints
2025-03-04 14:47:30,647 - INFO - training batch 301, loss: 0.021, 9632/56000 datapoints
2025-03-04 14:47:30,721 - INFO - training batch 351, loss: 0.080, 11232/56000 datapoints
2025-03-04 14:47:30,795 - INFO - training batch 401, loss: 0.076, 12832/56000 datapoints
2025-03-04 14:47:30,872 - INFO - training batch 451, loss: 0.059, 14432/56000 datapoints
2025-03-04 14:47:30,943 - INFO - training batch 501, loss: 0.035, 16032/56000 datapoints
2025-03-04 14:47:31,020 - INFO - training batch 551, loss: 0.096, 17632/56000 datapoints
2025-03-04 14:47:31,089 - INFO - training batch 601, loss: 0.006, 19232/56000 datapoints
2025-03-04 14:47:31,157 - INFO - training batch 651, loss: 0.018, 20832/56000 datapoints
2025-03-04 14:47:31,226 - INFO - training batch 701, loss: 0.206, 22432/56000 datapoints
2025-03-04 14:47:31,298 - INFO - training batch 751, loss: 0.021, 24032/56000 datapoints
2025-03-04 14:47:31,374 - INFO - training batch 801, loss: 0.062, 25632/56000 datapoints
2025-03-04 14:47:31,449 - INFO - training batch 851, loss: 0.010, 27232/56000 datapoints
2025-03-04 14:47:31,522 - INFO - training batch 901, loss: 0.015, 28832/56000 datapoints
2025-03-04 14:47:31,592 - INFO - training batch 951, loss: 0.025, 30432/56000 datapoints
2025-03-04 14:47:31,662 - INFO - training batch 1001, loss: 0.019, 32032/56000 datapoints
2025-03-04 14:47:31,730 - INFO - training batch 1051, loss: 0.031, 33632/56000 datapoints
2025-03-04 14:47:31,800 - INFO - training batch 1101, loss: 0.075, 35232/56000 datapoints
2025-03-04 14:47:31,866 - INFO - training batch 1151, loss: 0.004, 36832/56000 datapoints
2025-03-04 14:47:31,943 - INFO - training batch 1201, loss: 0.100, 38432/56000 datapoints
2025-03-04 14:47:32,019 - INFO - training batch 1251, loss: 0.235, 40032/56000 datapoints
2025-03-04 14:47:32,087 - INFO - training batch 1301, loss: 0.020, 41632/56000 datapoints
2025-03-04 14:47:32,154 - INFO - training batch 1351, loss: 0.016, 43232/56000 datapoints
2025-03-04 14:47:32,231 - INFO - training batch 1401, loss: 0.010, 44832/56000 datapoints
2025-03-04 14:47:32,300 - INFO - training batch 1451, loss: 0.013, 46432/56000 datapoints
2025-03-04 14:47:32,370 - INFO - training batch 1501, loss: 0.067, 48032/56000 datapoints
2025-03-04 14:47:32,435 - INFO - training batch 1551, loss: 0.014, 49632/56000 datapoints
2025-03-04 14:47:32,507 - INFO - training batch 1601, loss: 0.016, 51232/56000 datapoints
2025-03-04 14:47:32,575 - INFO - training batch 1651, loss: 0.020, 52832/56000 datapoints
2025-03-04 14:47:32,644 - INFO - training batch 1701, loss: 0.089, 54432/56000 datapoints
2025-03-04 14:47:32,724 - INFO - validation batch 1, loss: 0.013, 32/13984 datapoints
2025-03-04 14:47:32,744 - INFO - validation batch 51, loss: 0.021, 1632/13984 datapoints
2025-03-04 14:47:32,774 - INFO - validation batch 101, loss: 0.011, 3232/13984 datapoints
2025-03-04 14:47:32,804 - INFO - validation batch 151, loss: 0.029, 4832/13984 datapoints
2025-03-04 14:47:32,827 - INFO - validation batch 201, loss: 0.018, 6432/13984 datapoints
2025-03-04 14:47:32,853 - INFO - validation batch 251, loss: 0.045, 8032/13984 datapoints
2025-03-04 14:47:32,874 - INFO - validation batch 301, loss: 0.020, 9632/13984 datapoints
2025-03-04 14:47:32,907 - INFO - validation batch 351, loss: 0.018, 11232/13984 datapoints
2025-03-04 14:47:32,928 - INFO - validation batch 401, loss: 0.141, 12832/13984 datapoints
2025-03-04 14:47:32,944 - INFO - Epoch 17/50 done
2025-03-04 14:47:32,944 - INFO - Beginning epoch 18/50
2025-03-04 14:47:32,947 - INFO - training batch 1, loss: 0.168, 32/56000 datapoints
2025-03-04 14:47:33,045 - INFO - training batch 51, loss: 0.019, 1632/56000 datapoints
2025-03-04 14:47:33,117 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:47:33,184 - INFO - training batch 151, loss: 0.064, 4832/56000 datapoints
2025-03-04 14:47:33,267 - INFO - training batch 201, loss: 0.068, 6432/56000 datapoints
2025-03-04 14:47:33,372 - INFO - training batch 251, loss: 0.018, 8032/56000 datapoints
2025-03-04 14:47:33,466 - INFO - training batch 301, loss: 0.019, 9632/56000 datapoints
2025-03-04 14:47:33,594 - INFO - training batch 351, loss: 0.080, 11232/56000 datapoints
2025-03-04 14:47:33,694 - INFO - training batch 401, loss: 0.075, 12832/56000 datapoints
2025-03-04 14:47:33,785 - INFO - training batch 451, loss: 0.054, 14432/56000 datapoints
2025-03-04 14:47:33,884 - INFO - training batch 501, loss: 0.031, 16032/56000 datapoints
2025-03-04 14:47:34,233 - INFO - training batch 551, loss: 0.097, 17632/56000 datapoints
2025-03-04 14:47:34,724 - INFO - training batch 601, loss: 0.006, 19232/56000 datapoints
2025-03-04 14:47:34,957 - INFO - training batch 651, loss: 0.016, 20832/56000 datapoints
2025-03-04 14:47:35,358 - INFO - training batch 701, loss: 0.210, 22432/56000 datapoints
2025-03-04 14:47:35,534 - INFO - training batch 751, loss: 0.019, 24032/56000 datapoints
2025-03-04 14:47:35,688 - INFO - training batch 801, loss: 0.056, 25632/56000 datapoints
2025-03-04 14:47:35,825 - INFO - training batch 851, loss: 0.009, 27232/56000 datapoints
2025-03-04 14:47:35,971 - INFO - training batch 901, loss: 0.013, 28832/56000 datapoints
2025-03-04 14:47:36,079 - INFO - training batch 951, loss: 0.024, 30432/56000 datapoints
2025-03-04 14:47:36,209 - INFO - training batch 1001, loss: 0.017, 32032/56000 datapoints
2025-03-04 14:47:36,321 - INFO - training batch 1051, loss: 0.028, 33632/56000 datapoints
2025-03-04 14:47:36,424 - INFO - training batch 1101, loss: 0.075, 35232/56000 datapoints
2025-03-04 14:47:36,539 - INFO - training batch 1151, loss: 0.003, 36832/56000 datapoints
2025-03-04 14:47:36,680 - INFO - training batch 1201, loss: 0.092, 38432/56000 datapoints
2025-03-04 14:47:36,790 - INFO - training batch 1251, loss: 0.237, 40032/56000 datapoints
2025-03-04 14:47:36,934 - INFO - training batch 1301, loss: 0.019, 41632/56000 datapoints
2025-03-04 14:47:37,062 - INFO - training batch 1351, loss: 0.014, 43232/56000 datapoints
2025-03-04 14:47:37,256 - INFO - training batch 1401, loss: 0.009, 44832/56000 datapoints
2025-03-04 14:47:37,353 - INFO - training batch 1451, loss: 0.011, 46432/56000 datapoints
2025-03-04 14:47:37,464 - INFO - training batch 1501, loss: 0.066, 48032/56000 datapoints
2025-03-04 14:47:37,572 - INFO - training batch 1551, loss: 0.013, 49632/56000 datapoints
2025-03-04 14:47:37,672 - INFO - training batch 1601, loss: 0.015, 51232/56000 datapoints
2025-03-04 14:47:37,771 - INFO - training batch 1651, loss: 0.018, 52832/56000 datapoints
2025-03-04 14:47:37,862 - INFO - training batch 1701, loss: 0.089, 54432/56000 datapoints
2025-03-04 14:47:37,965 - INFO - validation batch 1, loss: 0.012, 32/13984 datapoints
2025-03-04 14:47:37,995 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-04 14:47:38,028 - INFO - validation batch 101, loss: 0.010, 3232/13984 datapoints
2025-03-04 14:47:38,077 - INFO - validation batch 151, loss: 0.027, 4832/13984 datapoints
2025-03-04 14:47:38,109 - INFO - validation batch 201, loss: 0.016, 6432/13984 datapoints
2025-03-04 14:47:38,163 - INFO - validation batch 251, loss: 0.040, 8032/13984 datapoints
2025-03-04 14:47:38,213 - INFO - validation batch 301, loss: 0.018, 9632/13984 datapoints
2025-03-04 14:47:38,268 - INFO - validation batch 351, loss: 0.016, 11232/13984 datapoints
2025-03-04 14:47:38,346 - INFO - validation batch 401, loss: 0.140, 12832/13984 datapoints
2025-03-04 14:47:38,393 - INFO - Epoch 18/50 done
2025-03-04 14:47:38,394 - INFO - Beginning epoch 19/50
2025-03-04 14:47:38,400 - INFO - training batch 1, loss: 0.168, 32/56000 datapoints
2025-03-04 14:47:39,318 - INFO - training batch 51, loss: 0.018, 1632/56000 datapoints
2025-03-04 14:47:39,876 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:47:40,085 - INFO - training batch 151, loss: 0.064, 4832/56000 datapoints
2025-03-04 14:47:40,233 - ERROR - Traceback (most recent call last):
2025-03-04 14:47:40,233 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/train_scripts/train_ff_finetune.py", line 190, in <module>
    train_losses, val_losses, state_dicts, train_time = train_thalreadout(model=model_thal,
2025-03-04 14:47:40,233 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 420, in train_thalreadout
    train_losses, state_dict = train_one_epoch_thalreadout(
2025-03-04 14:47:40,233 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 377, in train_one_epoch_thalreadout
    loss.backward()  # compute gradients
2025-03-04 14:47:40,234 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
2025-03-04 14:47:40,234 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-03-04 14:47:40,234 - ERROR - KeyboardInterrupt
2025-03-04 14:47:40,240 - ERROR - Traceback (most recent call last):
  File "/Users/patmccarthy/Documents/ThalamoCortex/train_scripts/train_ff_finetune.py", line 190, in <module>
    train_losses, val_losses, state_dicts, train_time = train_thalreadout(model=model_thal,
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 420, in train_thalreadout
    train_losses, state_dict = train_one_epoch_thalreadout(
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 377, in train_one_epoch_thalreadout
    loss.backward()  # compute gradients
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt