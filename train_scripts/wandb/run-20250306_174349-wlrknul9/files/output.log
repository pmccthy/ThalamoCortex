2025-03-06 17:43:53,256 - INFO - Running hyperparameter combination 1 of 1
2025-03-06 17:43:53,256 - INFO - 0_CTCNet_TC_none
2025-03-06 17:43:53,257 - INFO - Loading data...
2025-03-06 17:43:53,392 - INFO - Done loading.
2025-03-06 17:43:53,392 - INFO - Building model and optimiser...
2025-03-06 17:43:53,400 - INFO - =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Sequential: 1-1                        --
|    └─Linear: 2-1                       1,040
|    └─ReLU: 2-2                         --
├─Sequential: 1-2                        --
|    └─Linear: 2-3                       25,120
|    └─ReLU: 2-4                         --
├─Sequential: 1-3                        --
|    └─Linear: 2-5                       1,056
|    └─ReLU: 2-6                         --
├─Sequential: 1-4                        --
|    └─Linear: 2-7                       330
=================================================================
Total params: 27,546
Trainable params: 27,546
Non-trainable params: 0
=================================================================
2025-03-06 17:43:53,400 - INFO - Done.
2025-03-06 17:43:53,400 - INFO - Training...
2025-03-06 17:43:53,400 - INFO - Beginning epoch 1/800
2025-03-06 17:43:53,496 - INFO - training batch 1, loss: 2.278, 32/60000 datapoints
2025-03-06 17:43:53,708 - INFO - training batch 51, loss: 2.306, 1632/60000 datapoints
2025-03-06 17:43:53,909 - INFO - training batch 101, loss: 2.316, 3232/60000 datapoints
2025-03-06 17:43:54,112 - INFO - training batch 151, loss: 2.323, 4832/60000 datapoints
2025-03-06 17:43:54,314 - INFO - training batch 201, loss: 2.314, 6432/60000 datapoints
2025-03-06 17:43:54,513 - INFO - training batch 251, loss: 2.338, 8032/60000 datapoints
2025-03-06 17:43:54,716 - INFO - training batch 301, loss: 2.285, 9632/60000 datapoints
2025-03-06 17:43:54,925 - INFO - training batch 351, loss: 2.309, 11232/60000 datapoints
2025-03-06 17:43:55,124 - INFO - training batch 401, loss: 2.334, 12832/60000 datapoints
2025-03-06 17:43:55,414 - INFO - training batch 451, loss: 2.303, 14432/60000 datapoints
2025-03-06 17:43:55,625 - INFO - training batch 501, loss: 2.322, 16032/60000 datapoints
2025-03-06 17:43:56,008 - INFO - training batch 551, loss: 2.304, 17632/60000 datapoints
2025-03-06 17:43:56,227 - INFO - training batch 601, loss: 2.320, 19232/60000 datapoints
2025-03-06 17:43:56,438 - INFO - training batch 651, loss: 2.272, 20832/60000 datapoints
2025-03-06 17:43:56,739 - INFO - training batch 701, loss: 2.297, 22432/60000 datapoints
2025-03-06 17:43:57,076 - INFO - training batch 751, loss: 2.313, 24032/60000 datapoints
2025-03-06 17:43:57,401 - INFO - training batch 801, loss: 2.326, 25632/60000 datapoints
2025-03-06 17:43:57,926 - INFO - training batch 851, loss: 2.295, 27232/60000 datapoints
2025-03-06 17:43:58,308 - INFO - training batch 901, loss: 2.281, 28832/60000 datapoints
2025-03-06 17:43:58,661 - INFO - training batch 951, loss: 2.312, 30432/60000 datapoints
2025-03-06 17:43:58,932 - INFO - training batch 1001, loss: 2.298, 32032/60000 datapoints
2025-03-06 17:43:59,164 - INFO - training batch 1051, loss: 2.319, 33632/60000 datapoints
2025-03-06 17:43:59,408 - INFO - training batch 1101, loss: 2.315, 35232/60000 datapoints
2025-03-06 17:43:59,645 - INFO - training batch 1151, loss: 2.329, 36832/60000 datapoints
2025-03-06 17:43:59,886 - INFO - training batch 1201, loss: 2.340, 38432/60000 datapoints
2025-03-06 17:44:00,112 - INFO - training batch 1251, loss: 2.305, 40032/60000 datapoints
2025-03-06 17:44:00,380 - INFO - training batch 1301, loss: 2.289, 41632/60000 datapoints
2025-03-06 17:44:00,663 - INFO - training batch 1351, loss: 2.289, 43232/60000 datapoints
2025-03-06 17:44:00,875 - INFO - training batch 1401, loss: 2.283, 44832/60000 datapoints
2025-03-06 17:44:01,104 - INFO - training batch 1451, loss: 2.315, 46432/60000 datapoints
2025-03-06 17:44:01,347 - INFO - training batch 1501, loss: 2.271, 48032/60000 datapoints
2025-03-06 17:44:01,546 - INFO - training batch 1551, loss: 2.298, 49632/60000 datapoints
2025-03-06 17:44:01,752 - INFO - training batch 1601, loss: 2.278, 51232/60000 datapoints
2025-03-06 17:44:01,950 - INFO - training batch 1651, loss: 2.293, 52832/60000 datapoints
2025-03-06 17:44:02,172 - INFO - training batch 1701, loss: 2.323, 54432/60000 datapoints
2025-03-06 17:44:02,385 - INFO - training batch 1751, loss: 2.277, 56032/60000 datapoints
2025-03-06 17:44:02,600 - INFO - training batch 1801, loss: 2.292, 57632/60000 datapoints
2025-03-06 17:44:02,813 - INFO - training batch 1851, loss: 2.300, 59232/60000 datapoints
2025-03-06 17:44:02,943 - INFO - validation batch 1, loss: 2.277, 32/10016 datapoints
2025-03-06 17:44:03,113 - INFO - validation batch 51, loss: 2.292, 1632/10016 datapoints
2025-03-06 17:44:03,274 - INFO - validation batch 101, loss: 2.333, 3232/10016 datapoints
2025-03-06 17:44:03,436 - INFO - validation batch 151, loss: 2.320, 4832/10016 datapoints
2025-03-06 17:44:03,604 - INFO - validation batch 201, loss: 2.293, 6432/10016 datapoints
2025-03-06 17:44:03,760 - INFO - validation batch 251, loss: 2.292, 8032/10016 datapoints
2025-03-06 17:44:03,919 - INFO - validation batch 301, loss: 2.301, 9632/10016 datapoints
2025-03-06 17:44:03,958 - INFO - Epoch 1/800 done.
2025-03-06 17:44:03,958 - INFO - Final validation performance:
Loss: 2.301, top-1 acc: 0.142top-5 acc: 0.142
2025-03-06 17:44:03,959 - INFO - Beginning epoch 2/800
2025-03-06 17:44:03,965 - INFO - training batch 1, loss: 2.343, 32/60000 datapoints
2025-03-06 17:44:04,168 - INFO - training batch 51, loss: 2.258, 1632/60000 datapoints
2025-03-06 17:44:04,373 - INFO - training batch 101, loss: 2.343, 3232/60000 datapoints
2025-03-06 17:44:04,572 - INFO - training batch 151, loss: 2.288, 4832/60000 datapoints
2025-03-06 17:44:04,777 - INFO - training batch 201, loss: 2.276, 6432/60000 datapoints
2025-03-06 17:44:04,999 - INFO - training batch 251, loss: 2.286, 8032/60000 datapoints
2025-03-06 17:44:05,222 - INFO - training batch 301, loss: 2.266, 9632/60000 datapoints
2025-03-06 17:44:05,456 - INFO - training batch 351, loss: 2.327, 11232/60000 datapoints
2025-03-06 17:44:05,714 - INFO - training batch 401, loss: 2.274, 12832/60000 datapoints
2025-03-06 17:44:05,914 - INFO - training batch 451, loss: 2.280, 14432/60000 datapoints
2025-03-06 17:44:06,131 - INFO - training batch 501, loss: 2.302, 16032/60000 datapoints
2025-03-06 17:44:06,345 - INFO - training batch 551, loss: 2.267, 17632/60000 datapoints
2025-03-06 17:44:06,554 - INFO - training batch 601, loss: 2.300, 19232/60000 datapoints
2025-03-06 17:44:06,759 - INFO - training batch 651, loss: 2.299, 20832/60000 datapoints
2025-03-06 17:44:06,961 - INFO - training batch 701, loss: 2.315, 22432/60000 datapoints
2025-03-06 17:44:07,161 - INFO - training batch 751, loss: 2.296, 24032/60000 datapoints
2025-03-06 17:44:07,369 - INFO - training batch 801, loss: 2.320, 25632/60000 datapoints
2025-03-06 17:44:07,577 - INFO - training batch 851, loss: 2.293, 27232/60000 datapoints
2025-03-06 17:44:07,794 - INFO - training batch 901, loss: 2.283, 28832/60000 datapoints
2025-03-06 17:44:07,997 - INFO - training batch 951, loss: 2.293, 30432/60000 datapoints
2025-03-06 17:44:08,204 - INFO - training batch 1001, loss: 2.318, 32032/60000 datapoints
2025-03-06 17:44:08,419 - INFO - training batch 1051, loss: 2.269, 33632/60000 datapoints
2025-03-06 17:44:08,623 - INFO - training batch 1101, loss: 2.305, 35232/60000 datapoints
2025-03-06 17:44:08,832 - INFO - training batch 1151, loss: 2.286, 36832/60000 datapoints
2025-03-06 17:44:09,058 - INFO - training batch 1201, loss: 2.319, 38432/60000 datapoints
2025-03-06 17:44:09,255 - INFO - training batch 1251, loss: 2.291, 40032/60000 datapoints
2025-03-06 17:44:09,456 - INFO - training batch 1301, loss: 2.279, 41632/60000 datapoints
2025-03-06 17:44:09,655 - INFO - training batch 1351, loss: 2.302, 43232/60000 datapoints
2025-03-06 17:44:09,854 - INFO - training batch 1401, loss: 2.269, 44832/60000 datapoints
2025-03-06 17:44:10,057 - INFO - training batch 1451, loss: 2.311, 46432/60000 datapoints
2025-03-06 17:44:10,255 - INFO - training batch 1501, loss: 2.280, 48032/60000 datapoints
2025-03-06 17:44:10,452 - INFO - training batch 1551, loss: 2.298, 49632/60000 datapoints
2025-03-06 17:44:10,655 - INFO - training batch 1601, loss: 2.288, 51232/60000 datapoints
2025-03-06 17:44:10,853 - INFO - training batch 1651, loss: 2.268, 52832/60000 datapoints
2025-03-06 17:44:11,058 - INFO - training batch 1701, loss: 2.285, 54432/60000 datapoints
2025-03-06 17:44:11,254 - INFO - training batch 1751, loss: 2.270, 56032/60000 datapoints
2025-03-06 17:44:11,463 - INFO - training batch 1801, loss: 2.261, 57632/60000 datapoints
2025-03-06 17:44:11,673 - INFO - training batch 1851, loss: 2.293, 59232/60000 datapoints
2025-03-06 17:44:11,778 - INFO - validation batch 1, loss: 2.272, 32/10016 datapoints
2025-03-06 17:44:11,935 - INFO - validation batch 51, loss: 2.264, 1632/10016 datapoints
2025-03-06 17:44:12,094 - INFO - validation batch 101, loss: 2.307, 3232/10016 datapoints
2025-03-06 17:44:12,252 - INFO - validation batch 151, loss: 2.289, 4832/10016 datapoints
2025-03-06 17:44:12,416 - INFO - validation batch 201, loss: 2.291, 6432/10016 datapoints
2025-03-06 17:44:12,579 - INFO - validation batch 251, loss: 2.275, 8032/10016 datapoints
2025-03-06 17:44:12,737 - INFO - validation batch 301, loss: 2.248, 9632/10016 datapoints
2025-03-06 17:44:12,775 - INFO - Epoch 2/800 done.
2025-03-06 17:44:12,776 - INFO - Final validation performance:
Loss: 2.278, top-1 acc: 0.195top-5 acc: 0.195
2025-03-06 17:44:12,776 - INFO - Beginning epoch 3/800
2025-03-06 17:44:12,782 - INFO - training batch 1, loss: 2.265, 32/60000 datapoints
2025-03-06 17:44:12,990 - INFO - training batch 51, loss: 2.279, 1632/60000 datapoints
2025-03-06 17:44:13,195 - INFO - training batch 101, loss: 2.309, 3232/60000 datapoints
2025-03-06 17:44:13,396 - INFO - training batch 151, loss: 2.266, 4832/60000 datapoints
2025-03-06 17:44:13,612 - INFO - training batch 201, loss: 2.296, 6432/60000 datapoints
2025-03-06 17:44:13,813 - INFO - training batch 251, loss: 2.269, 8032/60000 datapoints
2025-03-06 17:44:14,017 - INFO - training batch 301, loss: 2.249, 9632/60000 datapoints
2025-03-06 17:44:14,219 - INFO - training batch 351, loss: 2.274, 11232/60000 datapoints
2025-03-06 17:44:14,430 - INFO - training batch 401, loss: 2.277, 12832/60000 datapoints
2025-03-06 17:44:14,641 - INFO - training batch 451, loss: 2.302, 14432/60000 datapoints
2025-03-06 17:44:14,843 - INFO - training batch 501, loss: 2.254, 16032/60000 datapoints
2025-03-06 17:44:15,044 - INFO - training batch 551, loss: 2.283, 17632/60000 datapoints
2025-03-06 17:44:15,244 - INFO - training batch 601, loss: 2.239, 19232/60000 datapoints
2025-03-06 17:44:15,445 - INFO - training batch 651, loss: 2.275, 20832/60000 datapoints
2025-03-06 17:44:15,647 - INFO - training batch 701, loss: 2.263, 22432/60000 datapoints
2025-03-06 17:44:15,843 - INFO - training batch 751, loss: 2.251, 24032/60000 datapoints
2025-03-06 17:44:16,043 - INFO - training batch 801, loss: 2.292, 25632/60000 datapoints
2025-03-06 17:44:16,242 - INFO - training batch 851, loss: 2.318, 27232/60000 datapoints
2025-03-06 17:44:16,443 - INFO - training batch 901, loss: 2.285, 28832/60000 datapoints
2025-03-06 17:44:16,659 - INFO - training batch 951, loss: 2.278, 30432/60000 datapoints
2025-03-06 17:44:16,859 - INFO - training batch 1001, loss: 2.310, 32032/60000 datapoints
2025-03-06 17:44:17,059 - INFO - training batch 1051, loss: 2.266, 33632/60000 datapoints
2025-03-06 17:44:17,257 - INFO - training batch 1101, loss: 2.288, 35232/60000 datapoints
2025-03-06 17:44:17,460 - INFO - training batch 1151, loss: 2.285, 36832/60000 datapoints
2025-03-06 17:44:17,662 - INFO - training batch 1201, loss: 2.295, 38432/60000 datapoints
2025-03-06 17:44:17,874 - INFO - training batch 1251, loss: 2.281, 40032/60000 datapoints
2025-03-06 17:44:18,073 - INFO - training batch 1301, loss: 2.300, 41632/60000 datapoints
2025-03-06 17:44:18,288 - INFO - training batch 1351, loss: 2.304, 43232/60000 datapoints
2025-03-06 17:44:18,489 - INFO - training batch 1401, loss: 2.274, 44832/60000 datapoints
2025-03-06 17:44:18,690 - INFO - training batch 1451, loss: 2.308, 46432/60000 datapoints
2025-03-06 17:44:18,891 - INFO - training batch 1501, loss: 2.268, 48032/60000 datapoints
2025-03-06 17:44:19,112 - INFO - training batch 1551, loss: 2.265, 49632/60000 datapoints
2025-03-06 17:44:19,311 - INFO - training batch 1601, loss: 2.245, 51232/60000 datapoints
2025-03-06 17:44:19,517 - INFO - training batch 1651, loss: 2.252, 52832/60000 datapoints
2025-03-06 17:44:19,718 - INFO - training batch 1701, loss: 2.255, 54432/60000 datapoints
2025-03-06 17:44:19,919 - INFO - training batch 1751, loss: 2.263, 56032/60000 datapoints
2025-03-06 17:44:20,117 - INFO - training batch 1801, loss: 2.258, 57632/60000 datapoints
2025-03-06 17:44:20,313 - INFO - training batch 1851, loss: 2.237, 59232/60000 datapoints
2025-03-06 17:44:20,419 - INFO - validation batch 1, loss: 2.275, 32/10016 datapoints
2025-03-06 17:44:20,573 - INFO - validation batch 51, loss: 2.270, 1632/10016 datapoints
2025-03-06 17:44:20,728 - INFO - validation batch 101, loss: 2.284, 3232/10016 datapoints
2025-03-06 17:44:20,882 - INFO - validation batch 151, loss: 2.266, 4832/10016 datapoints
2025-03-06 17:44:21,043 - INFO - validation batch 201, loss: 2.243, 6432/10016 datapoints
2025-03-06 17:44:21,200 - INFO - validation batch 251, loss: 2.278, 8032/10016 datapoints
2025-03-06 17:44:21,357 - INFO - validation batch 301, loss: 2.251, 9632/10016 datapoints
2025-03-06 17:44:21,397 - INFO - Epoch 3/800 done.
2025-03-06 17:44:21,397 - INFO - Final validation performance:
Loss: 2.267, top-1 acc: 0.239top-5 acc: 0.239
2025-03-06 17:44:21,398 - INFO - Beginning epoch 4/800
2025-03-06 17:44:21,404 - INFO - training batch 1, loss: 2.284, 32/60000 datapoints
2025-03-06 17:44:21,615 - INFO - training batch 51, loss: 2.278, 1632/60000 datapoints
2025-03-06 17:44:21,813 - INFO - training batch 101, loss: 2.276, 3232/60000 datapoints
2025-03-06 17:44:22,012 - INFO - training batch 151, loss: 2.228, 4832/60000 datapoints
2025-03-06 17:44:22,210 - INFO - training batch 201, loss: 2.259, 6432/60000 datapoints
2025-03-06 17:44:22,407 - INFO - training batch 251, loss: 2.269, 8032/60000 datapoints
2025-03-06 17:44:22,609 - INFO - training batch 301, loss: 2.270, 9632/60000 datapoints
2025-03-06 17:44:22,806 - INFO - training batch 351, loss: 2.249, 11232/60000 datapoints
2025-03-06 17:44:23,002 - INFO - training batch 401, loss: 2.251, 12832/60000 datapoints
2025-03-06 17:44:23,198 - INFO - training batch 451, loss: 2.259, 14432/60000 datapoints
2025-03-06 17:44:23,399 - INFO - training batch 501, loss: 2.266, 16032/60000 datapoints
2025-03-06 17:44:23,608 - INFO - training batch 551, loss: 2.273, 17632/60000 datapoints
2025-03-06 17:44:23,804 - INFO - training batch 601, loss: 2.270, 19232/60000 datapoints
2025-03-06 17:44:24,002 - INFO - training batch 651, loss: 2.237, 20832/60000 datapoints
2025-03-06 17:44:24,201 - INFO - training batch 701, loss: 2.237, 22432/60000 datapoints
2025-03-06 17:44:24,402 - INFO - training batch 751, loss: 2.244, 24032/60000 datapoints
2025-03-06 17:44:24,605 - INFO - training batch 801, loss: 2.295, 25632/60000 datapoints
2025-03-06 17:44:24,808 - INFO - training batch 851, loss: 2.232, 27232/60000 datapoints
2025-03-06 17:44:25,006 - INFO - training batch 901, loss: 2.280, 28832/60000 datapoints
2025-03-06 17:44:25,203 - INFO - training batch 951, loss: 2.239, 30432/60000 datapoints
2025-03-06 17:44:25,402 - INFO - training batch 1001, loss: 2.246, 32032/60000 datapoints
2025-03-06 17:44:25,605 - INFO - training batch 1051, loss: 2.279, 33632/60000 datapoints
2025-03-06 17:44:25,800 - INFO - training batch 1101, loss: 2.269, 35232/60000 datapoints
2025-03-06 17:44:26,002 - INFO - training batch 1151, loss: 2.271, 36832/60000 datapoints
2025-03-06 17:44:26,214 - INFO - training batch 1201, loss: 2.237, 38432/60000 datapoints
2025-03-06 17:44:26,416 - INFO - training batch 1251, loss: 2.233, 40032/60000 datapoints
2025-03-06 17:44:26,620 - INFO - training batch 1301, loss: 2.253, 41632/60000 datapoints
2025-03-06 17:44:26,817 - INFO - training batch 1351, loss: 2.252, 43232/60000 datapoints
2025-03-06 17:44:27,016 - INFO - training batch 1401, loss: 2.265, 44832/60000 datapoints
2025-03-06 17:44:27,213 - INFO - training batch 1451, loss: 2.270, 46432/60000 datapoints
2025-03-06 17:44:27,413 - INFO - training batch 1501, loss: 2.221, 48032/60000 datapoints
2025-03-06 17:44:27,614 - INFO - training batch 1551, loss: 2.277, 49632/60000 datapoints
2025-03-06 17:44:27,813 - INFO - training batch 1601, loss: 2.232, 51232/60000 datapoints
2025-03-06 17:44:28,013 - INFO - training batch 1651, loss: 2.262, 52832/60000 datapoints
2025-03-06 17:44:28,212 - INFO - training batch 1701, loss: 2.250, 54432/60000 datapoints
2025-03-06 17:44:28,410 - INFO - training batch 1751, loss: 2.252, 56032/60000 datapoints
2025-03-06 17:44:28,620 - INFO - training batch 1801, loss: 2.249, 57632/60000 datapoints
2025-03-06 17:44:28,856 - INFO - training batch 1851, loss: 2.247, 59232/60000 datapoints
2025-03-06 17:44:28,960 - INFO - validation batch 1, loss: 2.222, 32/10016 datapoints
2025-03-06 17:44:29,138 - INFO - validation batch 51, loss: 2.251, 1632/10016 datapoints
2025-03-06 17:44:29,295 - INFO - validation batch 101, loss: 2.221, 3232/10016 datapoints
2025-03-06 17:44:29,454 - INFO - validation batch 151, loss: 2.260, 4832/10016 datapoints
2025-03-06 17:44:29,612 - INFO - validation batch 201, loss: 2.226, 6432/10016 datapoints
2025-03-06 17:44:29,765 - INFO - validation batch 251, loss: 2.242, 8032/10016 datapoints
2025-03-06 17:44:29,920 - INFO - validation batch 301, loss: 2.241, 9632/10016 datapoints
2025-03-06 17:44:29,961 - INFO - Epoch 4/800 done.
2025-03-06 17:44:29,961 - INFO - Final validation performance:
Loss: 2.238, top-1 acc: 0.263top-5 acc: 0.263
2025-03-06 17:44:29,961 - INFO - Beginning epoch 5/800
2025-03-06 17:44:29,967 - INFO - training batch 1, loss: 2.240, 32/60000 datapoints
2025-03-06 17:44:30,167 - INFO - training batch 51, loss: 2.261, 1632/60000 datapoints
2025-03-06 17:44:30,364 - INFO - training batch 101, loss: 2.240, 3232/60000 datapoints
2025-03-06 17:44:30,564 - INFO - training batch 151, loss: 2.262, 4832/60000 datapoints
2025-03-06 17:44:30,764 - INFO - training batch 201, loss: 2.226, 6432/60000 datapoints
2025-03-06 17:44:30,961 - INFO - training batch 251, loss: 2.237, 8032/60000 datapoints
2025-03-06 17:44:31,157 - INFO - training batch 301, loss: 2.240, 9632/60000 datapoints
2025-03-06 17:44:31,359 - INFO - training batch 351, loss: 2.249, 11232/60000 datapoints
2025-03-06 17:44:31,559 - INFO - training batch 401, loss: 2.216, 12832/60000 datapoints
2025-03-06 17:44:31,764 - INFO - training batch 451, loss: 2.230, 14432/60000 datapoints
2025-03-06 17:44:31,963 - INFO - training batch 501, loss: 2.222, 16032/60000 datapoints
2025-03-06 17:44:32,164 - INFO - training batch 551, loss: 2.278, 17632/60000 datapoints
2025-03-06 17:44:32,359 - INFO - training batch 601, loss: 2.253, 19232/60000 datapoints
2025-03-06 17:44:32,558 - INFO - training batch 651, loss: 2.218, 20832/60000 datapoints
2025-03-06 17:44:32,757 - INFO - training batch 701, loss: 2.232, 22432/60000 datapoints
2025-03-06 17:44:32,956 - INFO - training batch 751, loss: 2.212, 24032/60000 datapoints
2025-03-06 17:44:33,154 - INFO - training batch 801, loss: 2.244, 25632/60000 datapoints
2025-03-06 17:44:33,352 - INFO - training batch 851, loss: 2.283, 27232/60000 datapoints
2025-03-06 17:44:33,553 - INFO - training batch 901, loss: 2.253, 28832/60000 datapoints
2025-03-06 17:44:33,756 - INFO - training batch 951, loss: 2.228, 30432/60000 datapoints
2025-03-06 17:44:33,954 - INFO - training batch 1001, loss: 2.242, 32032/60000 datapoints
2025-03-06 17:44:34,152 - INFO - training batch 1051, loss: 2.220, 33632/60000 datapoints
2025-03-06 17:44:34,351 - INFO - training batch 1101, loss: 2.222, 35232/60000 datapoints
2025-03-06 17:44:34,554 - INFO - training batch 1151, loss: 2.224, 36832/60000 datapoints
2025-03-06 17:44:34,758 - INFO - training batch 1201, loss: 2.185, 38432/60000 datapoints
2025-03-06 17:44:34,962 - INFO - training batch 1251, loss: 2.225, 40032/60000 datapoints
2025-03-06 17:44:35,161 - INFO - training batch 1301, loss: 2.226, 41632/60000 datapoints
2025-03-06 17:44:35,361 - INFO - training batch 1351, loss: 2.226, 43232/60000 datapoints
2025-03-06 17:44:35,561 - INFO - training batch 1401, loss: 2.234, 44832/60000 datapoints
2025-03-06 17:44:35,768 - INFO - training batch 1451, loss: 2.222, 46432/60000 datapoints
2025-03-06 17:44:35,970 - INFO - training batch 1501, loss: 2.208, 48032/60000 datapoints
2025-03-06 17:44:36,166 - INFO - training batch 1551, loss: 2.212, 49632/60000 datapoints
2025-03-06 17:44:36,364 - INFO - training batch 1601, loss: 2.222, 51232/60000 datapoints
2025-03-06 17:44:36,562 - INFO - training batch 1651, loss: 2.220, 52832/60000 datapoints
2025-03-06 17:44:36,764 - INFO - training batch 1701, loss: 2.240, 54432/60000 datapoints
2025-03-06 17:44:36,963 - INFO - training batch 1751, loss: 2.241, 56032/60000 datapoints
2025-03-06 17:44:37,162 - INFO - training batch 1801, loss: 2.189, 57632/60000 datapoints
2025-03-06 17:44:37,360 - INFO - training batch 1851, loss: 2.220, 59232/60000 datapoints
2025-03-06 17:44:37,466 - INFO - validation batch 1, loss: 2.216, 32/10016 datapoints
2025-03-06 17:44:37,650 - INFO - validation batch 51, loss: 2.249, 1632/10016 datapoints
2025-03-06 17:44:37,805 - INFO - validation batch 101, loss: 2.220, 3232/10016 datapoints
2025-03-06 17:44:37,961 - INFO - validation batch 151, loss: 2.162, 4832/10016 datapoints
2025-03-06 17:44:38,114 - INFO - validation batch 201, loss: 2.213, 6432/10016 datapoints
2025-03-06 17:44:38,268 - INFO - validation batch 251, loss: 2.189, 8032/10016 datapoints
2025-03-06 17:44:38,421 - INFO - validation batch 301, loss: 2.201, 9632/10016 datapoints
2025-03-06 17:44:38,462 - INFO - Epoch 5/800 done.
2025-03-06 17:44:38,463 - INFO - Final validation performance:
Loss: 2.207, top-1 acc: 0.278top-5 acc: 0.278
2025-03-06 17:44:38,463 - INFO - Beginning epoch 6/800
2025-03-06 17:44:38,470 - INFO - training batch 1, loss: 2.202, 32/60000 datapoints
2025-03-06 17:44:38,674 - INFO - training batch 51, loss: 2.239, 1632/60000 datapoints
2025-03-06 17:44:38,878 - INFO - training batch 101, loss: 2.220, 3232/60000 datapoints
2025-03-06 17:44:39,079 - INFO - training batch 151, loss: 2.212, 4832/60000 datapoints
2025-03-06 17:44:39,295 - INFO - training batch 201, loss: 2.261, 6432/60000 datapoints
2025-03-06 17:44:39,496 - INFO - training batch 251, loss: 2.243, 8032/60000 datapoints
2025-03-06 17:44:39,698 - INFO - training batch 301, loss: 2.208, 9632/60000 datapoints
2025-03-06 17:44:39,894 - INFO - training batch 351, loss: 2.249, 11232/60000 datapoints
2025-03-06 17:44:40,094 - INFO - training batch 401, loss: 2.148, 12832/60000 datapoints
2025-03-06 17:44:40,292 - INFO - training batch 451, loss: 2.232, 14432/60000 datapoints
2025-03-06 17:44:40,490 - INFO - training batch 501, loss: 2.240, 16032/60000 datapoints
2025-03-06 17:44:40,693 - INFO - training batch 551, loss: 2.189, 17632/60000 datapoints
2025-03-06 17:44:40,893 - INFO - training batch 601, loss: 2.218, 19232/60000 datapoints
2025-03-06 17:44:41,090 - INFO - training batch 651, loss: 2.223, 20832/60000 datapoints
2025-03-06 17:44:41,285 - INFO - training batch 701, loss: 2.183, 22432/60000 datapoints
2025-03-06 17:44:41,487 - INFO - training batch 751, loss: 2.218, 24032/60000 datapoints
2025-03-06 17:44:41,689 - INFO - training batch 801, loss: 2.236, 25632/60000 datapoints
2025-03-06 17:44:41,889 - INFO - training batch 851, loss: 2.237, 27232/60000 datapoints
2025-03-06 17:44:42,088 - INFO - training batch 901, loss: 2.202, 28832/60000 datapoints
2025-03-06 17:44:42,283 - INFO - training batch 951, loss: 2.217, 30432/60000 datapoints
2025-03-06 17:44:42,484 - INFO - training batch 1001, loss: 2.231, 32032/60000 datapoints
2025-03-06 17:44:42,689 - INFO - training batch 1051, loss: 2.218, 33632/60000 datapoints
2025-03-06 17:44:42,889 - INFO - training batch 1101, loss: 2.208, 35232/60000 datapoints
2025-03-06 17:44:43,089 - INFO - training batch 1151, loss: 2.204, 36832/60000 datapoints
2025-03-06 17:44:43,284 - INFO - training batch 1201, loss: 2.216, 38432/60000 datapoints
2025-03-06 17:44:43,483 - INFO - training batch 1251, loss: 2.296, 40032/60000 datapoints
2025-03-06 17:44:43,686 - INFO - training batch 1301, loss: 2.213, 41632/60000 datapoints
2025-03-06 17:44:43,883 - INFO - training batch 1351, loss: 2.199, 43232/60000 datapoints
2025-03-06 17:44:44,084 - INFO - training batch 1401, loss: 2.202, 44832/60000 datapoints
2025-03-06 17:44:44,279 - INFO - training batch 1451, loss: 2.210, 46432/60000 datapoints
2025-03-06 17:44:44,479 - INFO - training batch 1501, loss: 2.225, 48032/60000 datapoints
2025-03-06 17:44:44,680 - INFO - training batch 1551, loss: 2.171, 49632/60000 datapoints
2025-03-06 17:44:44,882 - INFO - training batch 1601, loss: 2.184, 51232/60000 datapoints
2025-03-06 17:44:45,083 - INFO - training batch 1651, loss: 2.254, 52832/60000 datapoints
2025-03-06 17:44:45,281 - INFO - training batch 1701, loss: 2.237, 54432/60000 datapoints
2025-03-06 17:44:45,483 - INFO - training batch 1751, loss: 2.182, 56032/60000 datapoints
2025-03-06 17:44:45,704 - INFO - training batch 1801, loss: 2.197, 57632/60000 datapoints
2025-03-06 17:44:45,987 - INFO - training batch 1851, loss: 2.217, 59232/60000 datapoints
2025-03-06 17:44:46,100 - INFO - validation batch 1, loss: 2.184, 32/10016 datapoints
2025-03-06 17:44:46,263 - INFO - validation batch 51, loss: 2.248, 1632/10016 datapoints
2025-03-06 17:44:46,431 - INFO - validation batch 101, loss: 2.191, 3232/10016 datapoints
2025-03-06 17:44:46,619 - INFO - validation batch 151, loss: 2.183, 4832/10016 datapoints
2025-03-06 17:44:46,785 - INFO - validation batch 201, loss: 2.157, 6432/10016 datapoints
2025-03-06 17:44:46,950 - INFO - validation batch 251, loss: 2.203, 8032/10016 datapoints
2025-03-06 17:44:47,133 - INFO - validation batch 301, loss: 2.129, 9632/10016 datapoints
2025-03-06 17:44:47,179 - INFO - Epoch 6/800 done.
2025-03-06 17:44:47,179 - INFO - Final validation performance:
Loss: 2.185, top-1 acc: 0.290top-5 acc: 0.290
2025-03-06 17:44:47,180 - INFO - Beginning epoch 7/800
2025-03-06 17:44:47,187 - INFO - training batch 1, loss: 2.257, 32/60000 datapoints
2025-03-06 17:44:47,409 - INFO - training batch 51, loss: 2.199, 1632/60000 datapoints
2025-03-06 17:44:47,633 - INFO - training batch 101, loss: 2.197, 3232/60000 datapoints
2025-03-06 17:44:47,851 - INFO - training batch 151, loss: 2.223, 4832/60000 datapoints
2025-03-06 17:44:48,070 - INFO - training batch 201, loss: 2.194, 6432/60000 datapoints
2025-03-06 17:44:48,281 - INFO - training batch 251, loss: 2.214, 8032/60000 datapoints
2025-03-06 17:44:48,500 - INFO - training batch 301, loss: 2.207, 9632/60000 datapoints
2025-03-06 17:44:48,799 - INFO - training batch 351, loss: 2.211, 11232/60000 datapoints
2025-03-06 17:44:49,091 - INFO - training batch 401, loss: 2.218, 12832/60000 datapoints
2025-03-06 17:44:49,443 - INFO - training batch 451, loss: 2.209, 14432/60000 datapoints
2025-03-06 17:44:50,009 - INFO - training batch 501, loss: 2.186, 16032/60000 datapoints
2025-03-06 17:44:50,269 - INFO - training batch 551, loss: 2.180, 17632/60000 datapoints
2025-03-06 17:44:50,520 - INFO - training batch 601, loss: 2.189, 19232/60000 datapoints
2025-03-06 17:44:50,777 - INFO - training batch 651, loss: 2.183, 20832/60000 datapoints
2025-03-06 17:44:51,030 - INFO - training batch 701, loss: 2.252, 22432/60000 datapoints
2025-03-06 17:44:51,273 - INFO - training batch 751, loss: 2.163, 24032/60000 datapoints
2025-03-06 17:44:51,510 - INFO - training batch 801, loss: 2.189, 25632/60000 datapoints
2025-03-06 17:44:51,750 - INFO - training batch 851, loss: 2.184, 27232/60000 datapoints
2025-03-06 17:44:51,975 - INFO - training batch 901, loss: 2.188, 28832/60000 datapoints
2025-03-06 17:44:52,208 - INFO - training batch 951, loss: 2.212, 30432/60000 datapoints
2025-03-06 17:44:52,430 - INFO - training batch 1001, loss: 2.167, 32032/60000 datapoints
2025-03-06 17:44:52,661 - INFO - training batch 1051, loss: 2.216, 33632/60000 datapoints
2025-03-06 17:44:52,886 - INFO - training batch 1101, loss: 2.169, 35232/60000 datapoints
2025-03-06 17:44:53,111 - INFO - training batch 1151, loss: 2.179, 36832/60000 datapoints
2025-03-06 17:44:53,324 - INFO - training batch 1201, loss: 2.170, 38432/60000 datapoints
2025-03-06 17:44:53,539 - INFO - training batch 1251, loss: 2.139, 40032/60000 datapoints
2025-03-06 17:44:53,752 - INFO - training batch 1301, loss: 2.239, 41632/60000 datapoints
2025-03-06 17:44:53,962 - INFO - training batch 1351, loss: 2.148, 43232/60000 datapoints
2025-03-06 17:44:54,171 - INFO - training batch 1401, loss: 2.192, 44832/60000 datapoints
2025-03-06 17:44:54,384 - INFO - training batch 1451, loss: 2.183, 46432/60000 datapoints
2025-03-06 17:44:54,739 - INFO - training batch 1501, loss: 2.230, 48032/60000 datapoints
2025-03-06 17:44:54,957 - INFO - training batch 1551, loss: 2.175, 49632/60000 datapoints
2025-03-06 17:44:55,165 - INFO - training batch 1601, loss: 2.135, 51232/60000 datapoints
2025-03-06 17:44:55,375 - INFO - training batch 1651, loss: 2.182, 52832/60000 datapoints
2025-03-06 17:44:55,621 - INFO - training batch 1701, loss: 2.174, 54432/60000 datapoints
2025-03-06 17:44:55,842 - INFO - training batch 1751, loss: 2.175, 56032/60000 datapoints
2025-03-06 17:44:56,051 - INFO - training batch 1801, loss: 2.176, 57632/60000 datapoints
2025-03-06 17:44:56,254 - INFO - training batch 1851, loss: 2.226, 59232/60000 datapoints
2025-03-06 17:44:56,360 - INFO - validation batch 1, loss: 2.152, 32/10016 datapoints
2025-03-06 17:44:56,524 - INFO - validation batch 51, loss: 2.183, 1632/10016 datapoints
2025-03-06 17:44:56,711 - INFO - validation batch 101, loss: 2.211, 3232/10016 datapoints
2025-03-06 17:44:56,875 - INFO - validation batch 151, loss: 2.131, 4832/10016 datapoints
2025-03-06 17:44:57,037 - INFO - validation batch 201, loss: 2.228, 6432/10016 datapoints
2025-03-06 17:44:57,200 - INFO - validation batch 251, loss: 2.188, 8032/10016 datapoints
2025-03-06 17:44:57,363 - INFO - validation batch 301, loss: 2.219, 9632/10016 datapoints
2025-03-06 17:44:57,408 - INFO - Epoch 7/800 done.
2025-03-06 17:44:57,409 - INFO - Final validation performance:
Loss: 2.187, top-1 acc: 0.305top-5 acc: 0.305
2025-03-06 17:44:57,409 - INFO - Beginning epoch 8/800
2025-03-06 17:44:57,415 - INFO - training batch 1, loss: 2.157, 32/60000 datapoints
2025-03-06 17:44:57,626 - INFO - training batch 51, loss: 2.160, 1632/60000 datapoints
2025-03-06 17:44:57,834 - INFO - training batch 101, loss: 2.229, 3232/60000 datapoints
2025-03-06 17:44:58,044 - INFO - training batch 151, loss: 2.155, 4832/60000 datapoints
2025-03-06 17:44:58,253 - INFO - training batch 201, loss: 2.188, 6432/60000 datapoints
2025-03-06 17:44:58,462 - INFO - training batch 251, loss: 2.155, 8032/60000 datapoints
2025-03-06 17:44:58,677 - INFO - training batch 301, loss: 2.177, 9632/60000 datapoints
2025-03-06 17:44:58,885 - INFO - training batch 351, loss: 2.160, 11232/60000 datapoints
2025-03-06 17:44:59,093 - INFO - training batch 401, loss: 2.185, 12832/60000 datapoints
2025-03-06 17:44:59,317 - INFO - training batch 451, loss: 2.202, 14432/60000 datapoints
2025-03-06 17:44:59,547 - INFO - training batch 501, loss: 2.153, 16032/60000 datapoints
2025-03-06 17:44:59,762 - INFO - training batch 551, loss: 2.195, 17632/60000 datapoints
2025-03-06 17:44:59,975 - INFO - training batch 601, loss: 2.167, 19232/60000 datapoints
2025-03-06 17:45:00,182 - INFO - training batch 651, loss: 2.167, 20832/60000 datapoints
2025-03-06 17:45:00,416 - INFO - training batch 701, loss: 2.148, 22432/60000 datapoints
2025-03-06 17:45:00,646 - INFO - training batch 751, loss: 2.143, 24032/60000 datapoints
2025-03-06 17:45:00,864 - INFO - training batch 801, loss: 2.131, 25632/60000 datapoints
2025-03-06 17:45:01,101 - INFO - training batch 851, loss: 2.182, 27232/60000 datapoints
2025-03-06 17:45:01,328 - INFO - training batch 901, loss: 2.150, 28832/60000 datapoints
2025-03-06 17:45:01,548 - INFO - training batch 951, loss: 2.179, 30432/60000 datapoints
2025-03-06 17:45:01,781 - INFO - training batch 1001, loss: 2.127, 32032/60000 datapoints
2025-03-06 17:45:01,998 - INFO - training batch 1051, loss: 2.159, 33632/60000 datapoints
2025-03-06 17:45:02,232 - INFO - training batch 1101, loss: 2.126, 35232/60000 datapoints
2025-03-06 17:45:02,460 - INFO - training batch 1151, loss: 2.127, 36832/60000 datapoints
2025-03-06 17:45:02,679 - INFO - training batch 1201, loss: 2.168, 38432/60000 datapoints
2025-03-06 17:45:02,906 - INFO - training batch 1251, loss: 2.189, 40032/60000 datapoints
2025-03-06 17:45:03,171 - INFO - training batch 1301, loss: 2.161, 41632/60000 datapoints
2025-03-06 17:45:03,408 - INFO - training batch 1351, loss: 2.162, 43232/60000 datapoints
2025-03-06 17:45:03,643 - INFO - training batch 1401, loss: 2.152, 44832/60000 datapoints
2025-03-06 17:45:03,871 - INFO - training batch 1451, loss: 2.152, 46432/60000 datapoints
2025-03-06 17:45:04,098 - INFO - training batch 1501, loss: 2.164, 48032/60000 datapoints
2025-03-06 17:45:04,319 - INFO - training batch 1551, loss: 2.160, 49632/60000 datapoints
2025-03-06 17:45:04,537 - INFO - training batch 1601, loss: 2.140, 51232/60000 datapoints
2025-03-06 17:45:04,777 - INFO - training batch 1651, loss: 2.145, 52832/60000 datapoints
2025-03-06 17:45:05,007 - INFO - training batch 1701, loss: 2.139, 54432/60000 datapoints
2025-03-06 17:45:05,227 - INFO - training batch 1751, loss: 2.152, 56032/60000 datapoints
2025-03-06 17:45:05,441 - INFO - training batch 1801, loss: 2.114, 57632/60000 datapoints
2025-03-06 17:45:05,657 - INFO - training batch 1851, loss: 2.221, 59232/60000 datapoints
2025-03-06 17:45:05,767 - INFO - validation batch 1, loss: 2.184, 32/10016 datapoints
2025-03-06 17:45:05,932 - INFO - validation batch 51, loss: 2.176, 1632/10016 datapoints
2025-03-06 17:45:06,095 - INFO - validation batch 101, loss: 2.133, 3232/10016 datapoints
2025-03-06 17:45:06,268 - INFO - validation batch 151, loss: 2.149, 4832/10016 datapoints
2025-03-06 17:45:06,431 - INFO - validation batch 201, loss: 2.198, 6432/10016 datapoints
2025-03-06 17:45:06,601 - INFO - validation batch 251, loss: 2.201, 8032/10016 datapoints
2025-03-06 17:45:06,769 - INFO - validation batch 301, loss: 2.159, 9632/10016 datapoints
2025-03-06 17:45:06,812 - INFO - Epoch 8/800 done.
2025-03-06 17:45:06,812 - INFO - Final validation performance:
Loss: 2.171, top-1 acc: 0.327top-5 acc: 0.327
2025-03-06 17:45:06,812 - INFO - Beginning epoch 9/800
2025-03-06 17:45:06,819 - INFO - training batch 1, loss: 2.139, 32/60000 datapoints
2025-03-06 17:45:07,020 - INFO - training batch 51, loss: 2.129, 1632/60000 datapoints
2025-03-06 17:45:07,223 - INFO - training batch 101, loss: 2.195, 3232/60000 datapoints
2025-03-06 17:45:07,425 - INFO - training batch 151, loss: 2.150, 4832/60000 datapoints
2025-03-06 17:45:07,629 - INFO - training batch 201, loss: 2.210, 6432/60000 datapoints
2025-03-06 17:45:07,829 - INFO - training batch 251, loss: 2.166, 8032/60000 datapoints
2025-03-06 17:45:08,030 - INFO - training batch 301, loss: 2.155, 9632/60000 datapoints
2025-03-06 17:45:08,233 - INFO - training batch 351, loss: 2.140, 11232/60000 datapoints
2025-03-06 17:45:08,432 - INFO - training batch 401, loss: 2.192, 12832/60000 datapoints
2025-03-06 17:45:08,632 - INFO - training batch 451, loss: 2.146, 14432/60000 datapoints
2025-03-06 17:45:08,834 - INFO - training batch 501, loss: 2.103, 16032/60000 datapoints
2025-03-06 17:45:09,034 - INFO - training batch 551, loss: 2.132, 17632/60000 datapoints
2025-03-06 17:45:09,230 - INFO - training batch 601, loss: 2.209, 19232/60000 datapoints
2025-03-06 17:45:09,447 - INFO - training batch 651, loss: 2.130, 20832/60000 datapoints
2025-03-06 17:45:09,649 - INFO - training batch 701, loss: 2.100, 22432/60000 datapoints
2025-03-06 17:45:09,851 - INFO - training batch 751, loss: 2.156, 24032/60000 datapoints
2025-03-06 17:45:10,051 - INFO - training batch 801, loss: 2.149, 25632/60000 datapoints
2025-03-06 17:45:10,251 - INFO - training batch 851, loss: 2.206, 27232/60000 datapoints
2025-03-06 17:45:10,451 - INFO - training batch 901, loss: 2.158, 28832/60000 datapoints
2025-03-06 17:45:10,651 - INFO - training batch 951, loss: 2.151, 30432/60000 datapoints
2025-03-06 17:45:10,854 - INFO - training batch 1001, loss: 2.118, 32032/60000 datapoints
2025-03-06 17:45:11,051 - INFO - training batch 1051, loss: 2.099, 33632/60000 datapoints
2025-03-06 17:45:11,246 - INFO - training batch 1101, loss: 2.189, 35232/60000 datapoints
2025-03-06 17:45:11,445 - INFO - training batch 1151, loss: 2.169, 36832/60000 datapoints
2025-03-06 17:45:11,662 - INFO - training batch 1201, loss: 2.097, 38432/60000 datapoints
2025-03-06 17:45:11,867 - INFO - training batch 1251, loss: 2.122, 40032/60000 datapoints
2025-03-06 17:45:12,066 - INFO - training batch 1301, loss: 2.152, 41632/60000 datapoints
2025-03-06 17:45:12,266 - INFO - training batch 1351, loss: 2.177, 43232/60000 datapoints
2025-03-06 17:45:12,467 - INFO - training batch 1401, loss: 2.190, 44832/60000 datapoints
2025-03-06 17:45:12,671 - INFO - training batch 1451, loss: 2.125, 46432/60000 datapoints
2025-03-06 17:45:12,873 - INFO - training batch 1501, loss: 2.120, 48032/60000 datapoints
2025-03-06 17:45:13,072 - INFO - training batch 1551, loss: 2.137, 49632/60000 datapoints
2025-03-06 17:45:13,272 - INFO - training batch 1601, loss: 2.119, 51232/60000 datapoints
2025-03-06 17:45:13,475 - INFO - training batch 1651, loss: 2.132, 52832/60000 datapoints
2025-03-06 17:45:13,679 - INFO - training batch 1701, loss: 2.163, 54432/60000 datapoints
2025-03-06 17:45:13,888 - INFO - training batch 1751, loss: 2.136, 56032/60000 datapoints
2025-03-06 17:45:14,089 - INFO - training batch 1801, loss: 2.114, 57632/60000 datapoints
2025-03-06 17:45:14,287 - INFO - training batch 1851, loss: 2.115, 59232/60000 datapoints
2025-03-06 17:45:14,392 - INFO - validation batch 1, loss: 2.169, 32/10016 datapoints
2025-03-06 17:45:14,551 - INFO - validation batch 51, loss: 2.144, 1632/10016 datapoints
2025-03-06 17:45:14,716 - INFO - validation batch 101, loss: 2.111, 3232/10016 datapoints
2025-03-06 17:45:14,881 - INFO - validation batch 151, loss: 2.139, 4832/10016 datapoints
2025-03-06 17:45:15,039 - INFO - validation batch 201, loss: 2.164, 6432/10016 datapoints
2025-03-06 17:45:15,197 - INFO - validation batch 251, loss: 2.118, 8032/10016 datapoints
2025-03-06 17:45:15,357 - INFO - validation batch 301, loss: 2.093, 9632/10016 datapoints
2025-03-06 17:45:15,396 - INFO - Epoch 9/800 done.
2025-03-06 17:45:15,396 - INFO - Final validation performance:
Loss: 2.134, top-1 acc: 0.350top-5 acc: 0.350
2025-03-06 17:45:15,397 - INFO - Beginning epoch 10/800
2025-03-06 17:45:15,402 - INFO - training batch 1, loss: 2.125, 32/60000 datapoints
2025-03-06 17:45:15,604 - INFO - training batch 51, loss: 2.102, 1632/60000 datapoints
2025-03-06 17:45:15,804 - INFO - training batch 101, loss: 2.141, 3232/60000 datapoints
2025-03-06 17:45:16,001 - INFO - training batch 151, loss: 2.084, 4832/60000 datapoints
2025-03-06 17:45:16,206 - INFO - training batch 201, loss: 2.141, 6432/60000 datapoints
2025-03-06 17:45:16,408 - INFO - training batch 251, loss: 2.137, 8032/60000 datapoints
2025-03-06 17:45:16,610 - INFO - training batch 301, loss: 2.151, 9632/60000 datapoints
2025-03-06 17:45:16,810 - INFO - training batch 351, loss: 2.110, 11232/60000 datapoints
2025-03-06 17:45:17,009 - INFO - training batch 401, loss: 2.149, 12832/60000 datapoints
2025-03-06 17:45:17,205 - INFO - training batch 451, loss: 2.066, 14432/60000 datapoints
2025-03-06 17:45:17,410 - INFO - training batch 501, loss: 2.144, 16032/60000 datapoints
2025-03-06 17:45:17,615 - INFO - training batch 551, loss: 2.162, 17632/60000 datapoints
2025-03-06 17:45:17,815 - INFO - training batch 601, loss: 2.065, 19232/60000 datapoints
2025-03-06 17:45:18,014 - INFO - training batch 651, loss: 2.081, 20832/60000 datapoints
2025-03-06 17:45:18,212 - INFO - training batch 701, loss: 2.116, 22432/60000 datapoints
2025-03-06 17:45:18,413 - INFO - training batch 751, loss: 2.183, 24032/60000 datapoints
2025-03-06 17:45:18,618 - INFO - training batch 801, loss: 2.144, 25632/60000 datapoints
2025-03-06 17:45:18,817 - INFO - training batch 851, loss: 2.171, 27232/60000 datapoints
2025-03-06 17:45:19,018 - INFO - training batch 901, loss: 2.118, 28832/60000 datapoints
2025-03-06 17:45:19,215 - INFO - training batch 951, loss: 2.128, 30432/60000 datapoints
2025-03-06 17:45:19,413 - INFO - training batch 1001, loss: 2.162, 32032/60000 datapoints
2025-03-06 17:45:19,634 - INFO - training batch 1051, loss: 2.113, 33632/60000 datapoints
2025-03-06 17:45:19,835 - INFO - training batch 1101, loss: 2.118, 35232/60000 datapoints
2025-03-06 17:45:20,035 - INFO - training batch 1151, loss: 2.097, 36832/60000 datapoints
2025-03-06 17:45:20,236 - INFO - training batch 1201, loss: 2.082, 38432/60000 datapoints
2025-03-06 17:45:20,438 - INFO - training batch 1251, loss: 2.111, 40032/60000 datapoints
2025-03-06 17:45:20,640 - INFO - training batch 1301, loss: 2.153, 41632/60000 datapoints
2025-03-06 17:45:20,845 - INFO - training batch 1351, loss: 2.100, 43232/60000 datapoints
2025-03-06 17:45:21,055 - INFO - training batch 1401, loss: 2.122, 44832/60000 datapoints
2025-03-06 17:45:21,254 - INFO - training batch 1451, loss: 2.123, 46432/60000 datapoints
2025-03-06 17:45:21,457 - INFO - training batch 1501, loss: 2.085, 48032/60000 datapoints
2025-03-06 17:45:21,663 - INFO - training batch 1551, loss: 2.104, 49632/60000 datapoints
2025-03-06 17:45:21,865 - INFO - training batch 1601, loss: 2.151, 51232/60000 datapoints
2025-03-06 17:45:22,066 - INFO - training batch 1651, loss: 2.134, 52832/60000 datapoints
2025-03-06 17:45:22,263 - INFO - training batch 1701, loss: 2.075, 54432/60000 datapoints
2025-03-06 17:45:22,468 - INFO - training batch 1751, loss: 2.124, 56032/60000 datapoints
2025-03-06 17:45:22,674 - INFO - training batch 1801, loss: 2.116, 57632/60000 datapoints
2025-03-06 17:45:22,874 - INFO - training batch 1851, loss: 2.063, 59232/60000 datapoints
2025-03-06 17:45:22,978 - INFO - validation batch 1, loss: 2.054, 32/10016 datapoints
2025-03-06 17:45:23,134 - INFO - validation batch 51, loss: 2.041, 1632/10016 datapoints
2025-03-06 17:45:23,290 - INFO - validation batch 101, loss: 2.083, 3232/10016 datapoints
2025-03-06 17:45:23,444 - INFO - validation batch 151, loss: 2.102, 4832/10016 datapoints
2025-03-06 17:45:23,606 - INFO - validation batch 201, loss: 2.037, 6432/10016 datapoints
2025-03-06 17:45:23,761 - INFO - validation batch 251, loss: 2.165, 8032/10016 datapoints
2025-03-06 17:45:23,919 - INFO - validation batch 301, loss: 2.099, 9632/10016 datapoints
2025-03-06 17:45:23,959 - INFO - Epoch 10/800 done.
2025-03-06 17:45:23,959 - INFO - Final validation performance:
Loss: 2.083, top-1 acc: 0.372top-5 acc: 0.372
2025-03-06 17:45:23,959 - INFO - Beginning epoch 11/800
2025-03-06 17:45:23,966 - INFO - training batch 1, loss: 2.133, 32/60000 datapoints
2025-03-06 17:45:24,168 - INFO - training batch 51, loss: 2.047, 1632/60000 datapoints
2025-03-06 17:45:24,364 - INFO - training batch 101, loss: 2.118, 3232/60000 datapoints
2025-03-06 17:45:24,566 - INFO - training batch 151, loss: 2.068, 4832/60000 datapoints
2025-03-06 17:45:24,769 - INFO - training batch 201, loss: 2.113, 6432/60000 datapoints
2025-03-06 17:45:24,976 - INFO - training batch 251, loss: 2.078, 8032/60000 datapoints
2025-03-06 17:45:25,175 - INFO - training batch 301, loss: 2.031, 9632/60000 datapoints
2025-03-06 17:45:25,373 - INFO - training batch 351, loss: 2.167, 11232/60000 datapoints
2025-03-06 17:45:25,575 - INFO - training batch 401, loss: 2.139, 12832/60000 datapoints
2025-03-06 17:45:25,777 - INFO - training batch 451, loss: 2.103, 14432/60000 datapoints
2025-03-06 17:45:25,974 - INFO - training batch 501, loss: 2.106, 16032/60000 datapoints
2025-03-06 17:45:26,176 - INFO - training batch 551, loss: 2.085, 17632/60000 datapoints
2025-03-06 17:45:26,376 - INFO - training batch 601, loss: 2.070, 19232/60000 datapoints
2025-03-06 17:45:26,578 - INFO - training batch 651, loss: 2.083, 20832/60000 datapoints
2025-03-06 17:45:26,777 - INFO - training batch 701, loss: 2.075, 22432/60000 datapoints
2025-03-06 17:45:26,978 - INFO - training batch 751, loss: 2.075, 24032/60000 datapoints
2025-03-06 17:45:27,180 - INFO - training batch 801, loss: 2.042, 25632/60000 datapoints
2025-03-06 17:45:27,382 - INFO - training batch 851, loss: 2.093, 27232/60000 datapoints
2025-03-06 17:45:27,586 - INFO - training batch 901, loss: 2.008, 28832/60000 datapoints
2025-03-06 17:45:27,792 - INFO - training batch 951, loss: 2.059, 30432/60000 datapoints
2025-03-06 17:45:28,004 - INFO - training batch 1001, loss: 2.100, 32032/60000 datapoints
2025-03-06 17:45:28,204 - INFO - training batch 1051, loss: 2.091, 33632/60000 datapoints
2025-03-06 17:45:28,407 - INFO - training batch 1101, loss: 2.083, 35232/60000 datapoints
2025-03-06 17:45:28,616 - INFO - training batch 1151, loss: 2.017, 36832/60000 datapoints
2025-03-06 17:45:28,818 - INFO - training batch 1201, loss: 2.126, 38432/60000 datapoints
2025-03-06 17:45:29,021 - INFO - training batch 1251, loss: 2.098, 40032/60000 datapoints
2025-03-06 17:45:29,221 - INFO - training batch 1301, loss: 2.089, 41632/60000 datapoints
2025-03-06 17:45:29,422 - INFO - training batch 1351, loss: 2.092, 43232/60000 datapoints
2025-03-06 17:45:29,644 - INFO - training batch 1401, loss: 2.119, 44832/60000 datapoints
2025-03-06 17:45:29,846 - INFO - training batch 1451, loss: 2.082, 46432/60000 datapoints
2025-03-06 17:45:30,050 - INFO - training batch 1501, loss: 2.077, 48032/60000 datapoints
2025-03-06 17:45:30,247 - INFO - training batch 1551, loss: 2.091, 49632/60000 datapoints
2025-03-06 17:45:30,446 - INFO - training batch 1601, loss: 2.078, 51232/60000 datapoints
2025-03-06 17:45:30,649 - INFO - training batch 1651, loss: 2.044, 52832/60000 datapoints
2025-03-06 17:45:30,845 - INFO - training batch 1701, loss: 2.078, 54432/60000 datapoints
2025-03-06 17:45:31,042 - INFO - training batch 1751, loss: 2.059, 56032/60000 datapoints
2025-03-06 17:45:31,239 - INFO - training batch 1801, loss: 2.091, 57632/60000 datapoints
2025-03-06 17:45:31,434 - INFO - training batch 1851, loss: 2.009, 59232/60000 datapoints
2025-03-06 17:45:31,537 - INFO - validation batch 1, loss: 2.114, 32/10016 datapoints
2025-03-06 17:45:31,696 - INFO - validation batch 51, loss: 2.095, 1632/10016 datapoints
2025-03-06 17:45:31,855 - INFO - validation batch 101, loss: 2.092, 3232/10016 datapoints
2025-03-06 17:45:32,009 - INFO - validation batch 151, loss: 2.045, 4832/10016 datapoints
2025-03-06 17:45:32,174 - INFO - validation batch 201, loss: 2.058, 6432/10016 datapoints
2025-03-06 17:45:32,330 - INFO - validation batch 251, loss: 2.062, 8032/10016 datapoints
2025-03-06 17:45:32,485 - INFO - validation batch 301, loss: 2.051, 9632/10016 datapoints
2025-03-06 17:45:32,525 - INFO - Epoch 11/800 done.
2025-03-06 17:45:32,525 - INFO - Final validation performance:
Loss: 2.074, top-1 acc: 0.393top-5 acc: 0.393
2025-03-06 17:45:32,525 - INFO - Beginning epoch 12/800
2025-03-06 17:45:32,533 - INFO - training batch 1, loss: 2.116, 32/60000 datapoints
2025-03-06 17:45:32,738 - INFO - training batch 51, loss: 2.073, 1632/60000 datapoints
2025-03-06 17:45:32,936 - INFO - training batch 101, loss: 2.027, 3232/60000 datapoints
2025-03-06 17:45:33,133 - INFO - training batch 151, loss: 2.024, 4832/60000 datapoints
2025-03-06 17:45:33,329 - INFO - training batch 201, loss: 2.048, 6432/60000 datapoints
2025-03-06 17:45:33,526 - INFO - training batch 251, loss: 2.089, 8032/60000 datapoints
2025-03-06 17:45:33,731 - INFO - training batch 301, loss: 2.069, 9632/60000 datapoints
2025-03-06 17:45:33,928 - INFO - training batch 351, loss: 2.103, 11232/60000 datapoints
2025-03-06 17:45:34,123 - INFO - training batch 401, loss: 2.013, 12832/60000 datapoints
2025-03-06 17:45:34,318 - INFO - training batch 451, loss: 2.077, 14432/60000 datapoints
2025-03-06 17:45:34,515 - INFO - training batch 501, loss: 2.087, 16032/60000 datapoints
2025-03-06 17:45:34,727 - INFO - training batch 551, loss: 2.111, 17632/60000 datapoints
2025-03-06 17:45:34,945 - INFO - training batch 601, loss: 2.059, 19232/60000 datapoints
2025-03-06 17:45:35,236 - INFO - training batch 651, loss: 2.085, 20832/60000 datapoints
2025-03-06 17:45:35,467 - INFO - training batch 701, loss: 2.083, 22432/60000 datapoints
2025-03-06 17:45:35,676 - INFO - training batch 751, loss: 2.049, 24032/60000 datapoints
2025-03-06 17:45:35,875 - INFO - training batch 801, loss: 2.091, 25632/60000 datapoints
2025-03-06 17:45:36,079 - INFO - training batch 851, loss: 2.024, 27232/60000 datapoints
2025-03-06 17:45:36,282 - INFO - training batch 901, loss: 2.042, 28832/60000 datapoints
2025-03-06 17:45:36,492 - INFO - training batch 951, loss: 2.103, 30432/60000 datapoints
2025-03-06 17:45:36,699 - INFO - training batch 1001, loss: 2.018, 32032/60000 datapoints
2025-03-06 17:45:36,899 - INFO - training batch 1051, loss: 2.111, 33632/60000 datapoints
2025-03-06 17:45:37,100 - INFO - training batch 1101, loss: 2.116, 35232/60000 datapoints
2025-03-06 17:45:37,302 - INFO - training batch 1151, loss: 2.063, 36832/60000 datapoints
2025-03-06 17:45:37,500 - INFO - training batch 1201, loss: 2.019, 38432/60000 datapoints
2025-03-06 17:45:37,733 - INFO - training batch 1251, loss: 2.064, 40032/60000 datapoints
2025-03-06 17:45:37,931 - INFO - training batch 1301, loss: 2.058, 41632/60000 datapoints
2025-03-06 17:45:38,132 - INFO - training batch 1351, loss: 2.108, 43232/60000 datapoints
2025-03-06 17:45:38,332 - INFO - training batch 1401, loss: 2.066, 44832/60000 datapoints
2025-03-06 17:45:38,529 - INFO - training batch 1451, loss: 2.060, 46432/60000 datapoints
2025-03-06 17:45:38,730 - INFO - training batch 1501, loss: 2.119, 48032/60000 datapoints
2025-03-06 17:45:38,927 - INFO - training batch 1551, loss: 2.073, 49632/60000 datapoints
2025-03-06 17:45:39,126 - INFO - training batch 1601, loss: 2.082, 51232/60000 datapoints
2025-03-06 17:45:39,321 - INFO - training batch 1651, loss: 2.112, 52832/60000 datapoints
2025-03-06 17:45:39,516 - INFO - training batch 1701, loss: 2.095, 54432/60000 datapoints
2025-03-06 17:45:39,745 - INFO - training batch 1751, loss: 2.073, 56032/60000 datapoints
2025-03-06 17:45:39,942 - INFO - training batch 1801, loss: 2.037, 57632/60000 datapoints
2025-03-06 17:45:40,296 - INFO - training batch 1851, loss: 2.050, 59232/60000 datapoints
2025-03-06 17:45:40,427 - INFO - validation batch 1, loss: 2.020, 32/10016 datapoints
2025-03-06 17:45:40,583 - INFO - validation batch 51, loss: 2.056, 1632/10016 datapoints
2025-03-06 17:45:40,745 - INFO - validation batch 101, loss: 2.029, 3232/10016 datapoints
2025-03-06 17:45:40,901 - INFO - validation batch 151, loss: 2.049, 4832/10016 datapoints
2025-03-06 17:45:41,057 - INFO - validation batch 201, loss: 2.046, 6432/10016 datapoints
2025-03-06 17:45:41,214 - INFO - validation batch 251, loss: 2.060, 8032/10016 datapoints
2025-03-06 17:45:41,370 - INFO - validation batch 301, loss: 1.965, 9632/10016 datapoints
2025-03-06 17:45:41,409 - INFO - Epoch 12/800 done.
2025-03-06 17:45:41,409 - INFO - Final validation performance:
Loss: 2.032, top-1 acc: 0.415top-5 acc: 0.415
2025-03-06 17:45:41,409 - INFO - Beginning epoch 13/800
2025-03-06 17:45:41,415 - INFO - training batch 1, loss: 2.063, 32/60000 datapoints
2025-03-06 17:45:41,626 - INFO - training batch 51, loss: 2.013, 1632/60000 datapoints
2025-03-06 17:45:41,833 - INFO - training batch 101, loss: 2.078, 3232/60000 datapoints
2025-03-06 17:45:42,036 - INFO - training batch 151, loss: 1.991, 4832/60000 datapoints
2025-03-06 17:45:42,233 - INFO - training batch 201, loss: 2.044, 6432/60000 datapoints
2025-03-06 17:45:42,430 - INFO - training batch 251, loss: 2.035, 8032/60000 datapoints
2025-03-06 17:45:42,632 - INFO - training batch 301, loss: 1.997, 9632/60000 datapoints
2025-03-06 17:45:42,829 - INFO - training batch 351, loss: 2.104, 11232/60000 datapoints
2025-03-06 17:45:43,027 - INFO - training batch 401, loss: 2.072, 12832/60000 datapoints
2025-03-06 17:45:43,223 - INFO - training batch 451, loss: 2.013, 14432/60000 datapoints
2025-03-06 17:45:43,416 - INFO - training batch 501, loss: 2.014, 16032/60000 datapoints
2025-03-06 17:45:43,619 - INFO - training batch 551, loss: 2.097, 17632/60000 datapoints
2025-03-06 17:45:43,817 - INFO - training batch 601, loss: 2.100, 19232/60000 datapoints
2025-03-06 17:45:44,012 - INFO - training batch 651, loss: 2.007, 20832/60000 datapoints
2025-03-06 17:45:44,210 - INFO - training batch 701, loss: 2.054, 22432/60000 datapoints
2025-03-06 17:45:44,407 - INFO - training batch 751, loss: 2.050, 24032/60000 datapoints
2025-03-06 17:45:44,605 - INFO - training batch 801, loss: 2.034, 25632/60000 datapoints
2025-03-06 17:45:44,808 - INFO - training batch 851, loss: 2.110, 27232/60000 datapoints
2025-03-06 17:45:45,008 - INFO - training batch 901, loss: 2.104, 28832/60000 datapoints
2025-03-06 17:45:45,206 - INFO - training batch 951, loss: 2.036, 30432/60000 datapoints
2025-03-06 17:45:45,407 - INFO - training batch 1001, loss: 2.024, 32032/60000 datapoints
2025-03-06 17:45:45,608 - INFO - training batch 1051, loss: 2.030, 33632/60000 datapoints
2025-03-06 17:45:45,805 - INFO - training batch 1101, loss: 2.012, 35232/60000 datapoints
2025-03-06 17:45:46,001 - INFO - training batch 1151, loss: 2.055, 36832/60000 datapoints
2025-03-06 17:45:46,203 - INFO - training batch 1201, loss: 2.041, 38432/60000 datapoints
2025-03-06 17:45:46,399 - INFO - training batch 1251, loss: 2.054, 40032/60000 datapoints
2025-03-06 17:45:46,602 - INFO - training batch 1301, loss: 2.043, 41632/60000 datapoints
2025-03-06 17:45:46,800 - INFO - training batch 1351, loss: 2.071, 43232/60000 datapoints
2025-03-06 17:45:46,996 - INFO - training batch 1401, loss: 2.053, 44832/60000 datapoints
2025-03-06 17:45:47,191 - INFO - training batch 1451, loss: 2.060, 46432/60000 datapoints
2025-03-06 17:45:47,387 - INFO - training batch 1501, loss: 2.073, 48032/60000 datapoints
2025-03-06 17:45:47,581 - INFO - training batch 1551, loss: 2.050, 49632/60000 datapoints
2025-03-06 17:45:47,783 - INFO - training batch 1601, loss: 2.073, 51232/60000 datapoints
2025-03-06 17:45:47,977 - INFO - training batch 1651, loss: 2.010, 52832/60000 datapoints
2025-03-06 17:45:48,174 - INFO - training batch 1701, loss: 2.062, 54432/60000 datapoints
2025-03-06 17:45:48,370 - INFO - training batch 1751, loss: 2.056, 56032/60000 datapoints
2025-03-06 17:45:48,569 - INFO - training batch 1801, loss: 2.038, 57632/60000 datapoints
2025-03-06 17:45:48,765 - INFO - training batch 1851, loss: 2.030, 59232/60000 datapoints
2025-03-06 17:45:48,866 - INFO - validation batch 1, loss: 2.001, 32/10016 datapoints
2025-03-06 17:45:49,019 - INFO - validation batch 51, loss: 1.964, 1632/10016 datapoints
2025-03-06 17:45:49,174 - INFO - validation batch 101, loss: 1.960, 3232/10016 datapoints
2025-03-06 17:45:49,326 - INFO - validation batch 151, loss: 1.990, 4832/10016 datapoints
2025-03-06 17:45:49,479 - INFO - validation batch 201, loss: 2.082, 6432/10016 datapoints
2025-03-06 17:45:49,636 - INFO - validation batch 251, loss: 2.022, 8032/10016 datapoints
2025-03-06 17:45:49,821 - INFO - validation batch 301, loss: 2.014, 9632/10016 datapoints
2025-03-06 17:45:49,859 - INFO - Epoch 13/800 done.
2025-03-06 17:45:49,860 - INFO - Final validation performance:
Loss: 2.005, top-1 acc: 0.436top-5 acc: 0.436
2025-03-06 17:45:49,860 - INFO - Beginning epoch 14/800
2025-03-06 17:45:49,866 - INFO - training batch 1, loss: 1.993, 32/60000 datapoints
2025-03-06 17:45:50,060 - INFO - training batch 51, loss: 1.996, 1632/60000 datapoints
2025-03-06 17:45:50,261 - INFO - training batch 101, loss: 1.948, 3232/60000 datapoints
2025-03-06 17:45:50,455 - INFO - training batch 151, loss: 2.060, 4832/60000 datapoints
2025-03-06 17:45:50,657 - INFO - training batch 201, loss: 1.989, 6432/60000 datapoints
2025-03-06 17:45:50,857 - INFO - training batch 251, loss: 2.022, 8032/60000 datapoints
2025-03-06 17:45:51,055 - INFO - training batch 301, loss: 2.014, 9632/60000 datapoints
2025-03-06 17:45:51,252 - INFO - training batch 351, loss: 2.061, 11232/60000 datapoints
2025-03-06 17:45:51,447 - INFO - training batch 401, loss: 1.962, 12832/60000 datapoints
2025-03-06 17:45:51,649 - INFO - training batch 451, loss: 1.997, 14432/60000 datapoints
2025-03-06 17:45:51,846 - INFO - training batch 501, loss: 1.958, 16032/60000 datapoints
2025-03-06 17:45:52,039 - INFO - training batch 551, loss: 2.029, 17632/60000 datapoints
2025-03-06 17:45:52,234 - INFO - training batch 601, loss: 1.892, 19232/60000 datapoints
2025-03-06 17:45:52,428 - INFO - training batch 651, loss: 2.018, 20832/60000 datapoints
2025-03-06 17:45:52,635 - INFO - training batch 701, loss: 1.983, 22432/60000 datapoints
2025-03-06 17:45:52,832 - INFO - training batch 751, loss: 2.011, 24032/60000 datapoints
2025-03-06 17:45:53,027 - INFO - training batch 801, loss: 1.978, 25632/60000 datapoints
2025-03-06 17:45:53,224 - INFO - training batch 851, loss: 2.043, 27232/60000 datapoints
2025-03-06 17:45:53,418 - INFO - training batch 901, loss: 2.046, 28832/60000 datapoints
2025-03-06 17:45:53,619 - INFO - training batch 951, loss: 2.043, 30432/60000 datapoints
2025-03-06 17:45:53,816 - INFO - training batch 1001, loss: 2.012, 32032/60000 datapoints
2025-03-06 17:45:54,009 - INFO - training batch 1051, loss: 2.057, 33632/60000 datapoints
2025-03-06 17:45:54,202 - INFO - training batch 1101, loss: 2.047, 35232/60000 datapoints
2025-03-06 17:45:54,399 - INFO - training batch 1151, loss: 2.045, 36832/60000 datapoints
2025-03-06 17:45:54,601 - INFO - training batch 1201, loss: 1.930, 38432/60000 datapoints
2025-03-06 17:45:54,806 - INFO - training batch 1251, loss: 2.013, 40032/60000 datapoints
2025-03-06 17:45:55,011 - INFO - training batch 1301, loss: 1.912, 41632/60000 datapoints
2025-03-06 17:45:55,212 - INFO - training batch 1351, loss: 2.017, 43232/60000 datapoints
2025-03-06 17:45:55,409 - INFO - training batch 1401, loss: 2.065, 44832/60000 datapoints
2025-03-06 17:45:55,621 - INFO - training batch 1451, loss: 1.994, 46432/60000 datapoints
2025-03-06 17:45:55,832 - INFO - training batch 1501, loss: 2.007, 48032/60000 datapoints
2025-03-06 17:45:56,030 - INFO - training batch 1551, loss: 1.938, 49632/60000 datapoints
2025-03-06 17:45:56,225 - INFO - training batch 1601, loss: 2.015, 51232/60000 datapoints
2025-03-06 17:45:56,518 - INFO - training batch 1651, loss: 2.004, 52832/60000 datapoints
2025-03-06 17:45:56,750 - INFO - training batch 1701, loss: 2.024, 54432/60000 datapoints
2025-03-06 17:45:56,946 - INFO - training batch 1751, loss: 1.926, 56032/60000 datapoints
2025-03-06 17:45:57,141 - INFO - training batch 1801, loss: 1.958, 57632/60000 datapoints
2025-03-06 17:45:57,337 - INFO - training batch 1851, loss: 2.040, 59232/60000 datapoints
2025-03-06 17:45:57,437 - INFO - validation batch 1, loss: 1.920, 32/10016 datapoints
2025-03-06 17:45:57,594 - INFO - validation batch 51, loss: 1.981, 1632/10016 datapoints
2025-03-06 17:45:57,752 - INFO - validation batch 101, loss: 1.948, 3232/10016 datapoints
2025-03-06 17:45:57,905 - INFO - validation batch 151, loss: 1.992, 4832/10016 datapoints
2025-03-06 17:45:58,064 - INFO - validation batch 201, loss: 2.020, 6432/10016 datapoints
2025-03-06 17:45:58,215 - INFO - validation batch 251, loss: 1.930, 8032/10016 datapoints
2025-03-06 17:45:58,368 - INFO - validation batch 301, loss: 1.953, 9632/10016 datapoints
2025-03-06 17:45:58,409 - INFO - Epoch 14/800 done.
2025-03-06 17:45:58,409 - INFO - Final validation performance:
Loss: 1.963, top-1 acc: 0.455top-5 acc: 0.455
2025-03-06 17:45:58,410 - INFO - Beginning epoch 15/800
2025-03-06 17:45:58,416 - INFO - training batch 1, loss: 1.955, 32/60000 datapoints
2025-03-06 17:45:58,614 - INFO - training batch 51, loss: 1.966, 1632/60000 datapoints
2025-03-06 17:45:58,810 - INFO - training batch 101, loss: 2.018, 3232/60000 datapoints
2025-03-06 17:45:59,002 - INFO - training batch 151, loss: 1.957, 4832/60000 datapoints
2025-03-06 17:45:59,199 - INFO - training batch 201, loss: 2.036, 6432/60000 datapoints
2025-03-06 17:45:59,396 - INFO - training batch 251, loss: 1.917, 8032/60000 datapoints
2025-03-06 17:45:59,592 - INFO - training batch 301, loss: 1.985, 9632/60000 datapoints
2025-03-06 17:45:59,791 - INFO - training batch 351, loss: 1.875, 11232/60000 datapoints
2025-03-06 17:46:00,003 - INFO - training batch 401, loss: 1.951, 12832/60000 datapoints
2025-03-06 17:46:00,203 - INFO - training batch 451, loss: 2.001, 14432/60000 datapoints
2025-03-06 17:46:00,408 - INFO - training batch 501, loss: 1.995, 16032/60000 datapoints
2025-03-06 17:46:00,610 - INFO - training batch 551, loss: 1.962, 17632/60000 datapoints
2025-03-06 17:46:00,812 - INFO - training batch 601, loss: 1.990, 19232/60000 datapoints
2025-03-06 17:46:01,006 - INFO - training batch 651, loss: 1.970, 20832/60000 datapoints
2025-03-06 17:46:01,200 - INFO - training batch 701, loss: 2.039, 22432/60000 datapoints
2025-03-06 17:46:01,393 - INFO - training batch 751, loss: 1.981, 24032/60000 datapoints
2025-03-06 17:46:01,591 - INFO - training batch 801, loss: 2.042, 25632/60000 datapoints
2025-03-06 17:46:01,789 - INFO - training batch 851, loss: 2.020, 27232/60000 datapoints
2025-03-06 17:46:01,985 - INFO - training batch 901, loss: 1.997, 28832/60000 datapoints
2025-03-06 17:46:02,178 - INFO - training batch 951, loss: 1.969, 30432/60000 datapoints
2025-03-06 17:46:02,371 - INFO - training batch 1001, loss: 1.907, 32032/60000 datapoints
2025-03-06 17:46:02,567 - INFO - training batch 1051, loss: 1.941, 33632/60000 datapoints
2025-03-06 17:46:02,772 - INFO - training batch 1101, loss: 1.948, 35232/60000 datapoints
2025-03-06 17:46:02,966 - INFO - training batch 1151, loss: 2.031, 36832/60000 datapoints
2025-03-06 17:46:03,164 - INFO - training batch 1201, loss: 2.037, 38432/60000 datapoints
2025-03-06 17:46:03,371 - INFO - training batch 1251, loss: 2.004, 40032/60000 datapoints
2025-03-06 17:46:03,564 - INFO - training batch 1301, loss: 1.898, 41632/60000 datapoints
2025-03-06 17:46:03,771 - INFO - training batch 1351, loss: 1.967, 43232/60000 datapoints
2025-03-06 17:46:03,963 - INFO - training batch 1401, loss: 1.973, 44832/60000 datapoints
2025-03-06 17:46:04,159 - INFO - training batch 1451, loss: 2.022, 46432/60000 datapoints
2025-03-06 17:46:04,354 - INFO - training batch 1501, loss: 2.083, 48032/60000 datapoints
2025-03-06 17:46:04,550 - INFO - training batch 1551, loss: 1.952, 49632/60000 datapoints
2025-03-06 17:46:04,750 - INFO - training batch 1601, loss: 1.994, 51232/60000 datapoints
2025-03-06 17:46:04,955 - INFO - training batch 1651, loss: 1.946, 52832/60000 datapoints
2025-03-06 17:46:05,154 - INFO - training batch 1701, loss: 2.013, 54432/60000 datapoints
2025-03-06 17:46:05,355 - INFO - training batch 1751, loss: 1.956, 56032/60000 datapoints
2025-03-06 17:46:05,550 - INFO - training batch 1801, loss: 1.999, 57632/60000 datapoints
2025-03-06 17:46:05,749 - INFO - training batch 1851, loss: 1.945, 59232/60000 datapoints
2025-03-06 17:46:05,851 - INFO - validation batch 1, loss: 1.939, 32/10016 datapoints
2025-03-06 17:46:06,006 - INFO - validation batch 51, loss: 1.968, 1632/10016 datapoints
2025-03-06 17:46:06,160 - INFO - validation batch 101, loss: 1.935, 3232/10016 datapoints
2025-03-06 17:46:06,315 - INFO - validation batch 151, loss: 1.947, 4832/10016 datapoints
2025-03-06 17:46:06,468 - INFO - validation batch 201, loss: 1.926, 6432/10016 datapoints
2025-03-06 17:46:06,624 - INFO - validation batch 251, loss: 1.947, 8032/10016 datapoints
2025-03-06 17:46:06,776 - INFO - validation batch 301, loss: 1.993, 9632/10016 datapoints
2025-03-06 17:46:06,812 - INFO - Epoch 15/800 done.
2025-03-06 17:46:06,812 - INFO - Final validation performance:
Loss: 1.951, top-1 acc: 0.476top-5 acc: 0.476
2025-03-06 17:46:06,813 - INFO - Beginning epoch 16/800
2025-03-06 17:46:06,818 - INFO - training batch 1, loss: 1.955, 32/60000 datapoints
2025-03-06 17:46:07,013 - INFO - training batch 51, loss: 1.891, 1632/60000 datapoints
2025-03-06 17:46:07,206 - INFO - training batch 101, loss: 1.899, 3232/60000 datapoints
2025-03-06 17:46:07,414 - INFO - training batch 151, loss: 2.021, 4832/60000 datapoints
2025-03-06 17:46:07,612 - INFO - training batch 201, loss: 1.906, 6432/60000 datapoints
2025-03-06 17:46:07,810 - INFO - training batch 251, loss: 1.946, 8032/60000 datapoints
2025-03-06 17:46:08,004 - INFO - training batch 301, loss: 2.008, 9632/60000 datapoints
2025-03-06 17:46:08,224 - INFO - training batch 351, loss: 1.983, 11232/60000 datapoints
2025-03-06 17:46:08,419 - INFO - training batch 401, loss: 1.963, 12832/60000 datapoints
2025-03-06 17:46:08,612 - INFO - training batch 451, loss: 2.003, 14432/60000 datapoints
2025-03-06 17:46:08,808 - INFO - training batch 501, loss: 2.030, 16032/60000 datapoints
2025-03-06 17:46:09,005 - INFO - training batch 551, loss: 1.946, 17632/60000 datapoints
2025-03-06 17:46:09,200 - INFO - training batch 601, loss: 1.997, 19232/60000 datapoints
2025-03-06 17:46:09,398 - INFO - training batch 651, loss: 1.953, 20832/60000 datapoints
2025-03-06 17:46:09,594 - INFO - training batch 701, loss: 2.040, 22432/60000 datapoints
2025-03-06 17:46:09,791 - INFO - training batch 751, loss: 1.999, 24032/60000 datapoints
2025-03-06 17:46:10,004 - INFO - training batch 801, loss: 1.963, 25632/60000 datapoints
2025-03-06 17:46:10,201 - INFO - training batch 851, loss: 1.979, 27232/60000 datapoints
2025-03-06 17:46:10,397 - INFO - training batch 901, loss: 1.998, 28832/60000 datapoints
2025-03-06 17:46:10,599 - INFO - training batch 951, loss: 2.051, 30432/60000 datapoints
2025-03-06 17:46:10,798 - INFO - training batch 1001, loss: 1.936, 32032/60000 datapoints
2025-03-06 17:46:10,997 - INFO - training batch 1051, loss: 1.998, 33632/60000 datapoints
2025-03-06 17:46:11,198 - INFO - training batch 1101, loss: 1.968, 35232/60000 datapoints
2025-03-06 17:46:11,393 - INFO - training batch 1151, loss: 1.970, 36832/60000 datapoints
2025-03-06 17:46:11,593 - INFO - training batch 1201, loss: 1.880, 38432/60000 datapoints
2025-03-06 17:46:11,791 - INFO - training batch 1251, loss: 1.962, 40032/60000 datapoints
2025-03-06 17:46:11,987 - INFO - training batch 1301, loss: 1.982, 41632/60000 datapoints
2025-03-06 17:46:12,180 - INFO - training batch 1351, loss: 1.977, 43232/60000 datapoints
2025-03-06 17:46:12,373 - INFO - training batch 1401, loss: 1.879, 44832/60000 datapoints
2025-03-06 17:46:12,568 - INFO - training batch 1451, loss: 2.026, 46432/60000 datapoints
2025-03-06 17:46:12,766 - INFO - training batch 1501, loss: 1.861, 48032/60000 datapoints
2025-03-06 17:46:12,963 - INFO - training batch 1551, loss: 1.914, 49632/60000 datapoints
2025-03-06 17:46:13,160 - INFO - training batch 1601, loss: 1.921, 51232/60000 datapoints
2025-03-06 17:46:13,355 - INFO - training batch 1651, loss: 1.879, 52832/60000 datapoints
2025-03-06 17:46:13,549 - INFO - training batch 1701, loss: 2.034, 54432/60000 datapoints
2025-03-06 17:46:13,748 - INFO - training batch 1751, loss: 2.008, 56032/60000 datapoints
2025-03-06 17:46:13,941 - INFO - training batch 1801, loss: 1.966, 57632/60000 datapoints
2025-03-06 17:46:14,135 - INFO - training batch 1851, loss: 1.926, 59232/60000 datapoints
2025-03-06 17:46:14,241 - INFO - validation batch 1, loss: 1.944, 32/10016 datapoints
2025-03-06 17:46:14,393 - INFO - validation batch 51, loss: 1.933, 1632/10016 datapoints
2025-03-06 17:46:14,547 - INFO - validation batch 101, loss: 1.864, 3232/10016 datapoints
2025-03-06 17:46:14,701 - INFO - validation batch 151, loss: 1.852, 4832/10016 datapoints
2025-03-06 17:46:14,862 - INFO - validation batch 201, loss: 1.946, 6432/10016 datapoints
2025-03-06 17:46:15,017 - INFO - validation batch 251, loss: 1.962, 8032/10016 datapoints
2025-03-06 17:46:15,170 - INFO - validation batch 301, loss: 1.964, 9632/10016 datapoints
2025-03-06 17:46:15,208 - INFO - Epoch 16/800 done.
2025-03-06 17:46:15,208 - INFO - Final validation performance:
Loss: 1.924, top-1 acc: 0.496top-5 acc: 0.496
2025-03-06 17:46:15,208 - INFO - Beginning epoch 17/800
2025-03-06 17:46:15,214 - INFO - training batch 1, loss: 1.957, 32/60000 datapoints
2025-03-06 17:46:15,413 - INFO - training batch 51, loss: 1.937, 1632/60000 datapoints
2025-03-06 17:46:15,611 - INFO - training batch 101, loss: 1.971, 3232/60000 datapoints
2025-03-06 17:46:15,809 - INFO - training batch 151, loss: 1.940, 4832/60000 datapoints
2025-03-06 17:46:16,000 - INFO - training batch 201, loss: 1.932, 6432/60000 datapoints
2025-03-06 17:46:16,202 - INFO - training batch 251, loss: 1.900, 8032/60000 datapoints
2025-03-06 17:46:16,392 - INFO - training batch 301, loss: 1.881, 9632/60000 datapoints
2025-03-06 17:46:16,586 - INFO - training batch 351, loss: 1.878, 11232/60000 datapoints
2025-03-06 17:46:16,781 - INFO - training batch 401, loss: 1.826, 12832/60000 datapoints
2025-03-06 17:46:16,980 - INFO - training batch 451, loss: 1.857, 14432/60000 datapoints
2025-03-06 17:46:17,174 - INFO - training batch 501, loss: 1.951, 16032/60000 datapoints
2025-03-06 17:46:17,366 - INFO - training batch 551, loss: 1.913, 17632/60000 datapoints
2025-03-06 17:46:17,566 - INFO - training batch 601, loss: 1.944, 19232/60000 datapoints
2025-03-06 17:46:17,767 - INFO - training batch 651, loss: 1.934, 20832/60000 datapoints
2025-03-06 17:46:17,960 - INFO - training batch 701, loss: 1.823, 22432/60000 datapoints
2025-03-06 17:46:18,157 - INFO - training batch 751, loss: 1.916, 24032/60000 datapoints
2025-03-06 17:46:18,352 - INFO - training batch 801, loss: 1.908, 25632/60000 datapoints
2025-03-06 17:46:18,547 - INFO - training batch 851, loss: 1.882, 27232/60000 datapoints
2025-03-06 17:46:18,743 - INFO - training batch 901, loss: 1.979, 28832/60000 datapoints
2025-03-06 17:46:18,938 - INFO - training batch 951, loss: 1.981, 30432/60000 datapoints
2025-03-06 17:46:19,134 - INFO - training batch 1001, loss: 1.905, 32032/60000 datapoints
2025-03-06 17:46:19,327 - INFO - training batch 1051, loss: 1.912, 33632/60000 datapoints
2025-03-06 17:46:19,521 - INFO - training batch 1101, loss: 1.902, 35232/60000 datapoints
2025-03-06 17:46:19,722 - INFO - training batch 1151, loss: 1.941, 36832/60000 datapoints
2025-03-06 17:46:19,918 - INFO - training batch 1201, loss: 1.953, 38432/60000 datapoints
2025-03-06 17:46:20,139 - INFO - training batch 1251, loss: 1.934, 40032/60000 datapoints
2025-03-06 17:46:20,334 - INFO - training batch 1301, loss: 1.928, 41632/60000 datapoints
2025-03-06 17:46:20,528 - INFO - training batch 1351, loss: 1.986, 43232/60000 datapoints
2025-03-06 17:46:20,723 - INFO - training batch 1401, loss: 1.867, 44832/60000 datapoints
2025-03-06 17:46:20,918 - INFO - training batch 1451, loss: 1.992, 46432/60000 datapoints
2025-03-06 17:46:21,114 - INFO - training batch 1501, loss: 1.933, 48032/60000 datapoints
2025-03-06 17:46:21,311 - INFO - training batch 1551, loss: 1.887, 49632/60000 datapoints
2025-03-06 17:46:21,503 - INFO - training batch 1601, loss: 1.896, 51232/60000 datapoints
2025-03-06 17:46:21,702 - INFO - training batch 1651, loss: 1.944, 52832/60000 datapoints
2025-03-06 17:46:21,901 - INFO - training batch 1701, loss: 1.931, 54432/60000 datapoints
2025-03-06 17:46:22,096 - INFO - training batch 1751, loss: 1.909, 56032/60000 datapoints
2025-03-06 17:46:22,291 - INFO - training batch 1801, loss: 1.952, 57632/60000 datapoints
2025-03-06 17:46:22,484 - INFO - training batch 1851, loss: 1.904, 59232/60000 datapoints
2025-03-06 17:46:22,586 - INFO - validation batch 1, loss: 1.882, 32/10016 datapoints
2025-03-06 17:46:22,742 - INFO - validation batch 51, loss: 1.851, 1632/10016 datapoints
2025-03-06 17:46:22,896 - INFO - validation batch 101, loss: 1.804, 3232/10016 datapoints
2025-03-06 17:46:23,049 - INFO - validation batch 151, loss: 1.852, 4832/10016 datapoints
2025-03-06 17:46:23,202 - INFO - validation batch 201, loss: 1.875, 6432/10016 datapoints
2025-03-06 17:46:23,356 - INFO - validation batch 251, loss: 1.950, 8032/10016 datapoints
2025-03-06 17:46:23,509 - INFO - validation batch 301, loss: 1.873, 9632/10016 datapoints
2025-03-06 17:46:23,544 - INFO - Epoch 17/800 done.
2025-03-06 17:46:23,545 - INFO - Final validation performance:
Loss: 1.870, top-1 acc: 0.514top-5 acc: 0.514
2025-03-06 17:46:23,545 - INFO - Beginning epoch 18/800
2025-03-06 17:46:23,552 - INFO - training batch 1, loss: 1.862, 32/60000 datapoints
2025-03-06 17:46:23,756 - INFO - training batch 51, loss: 1.973, 1632/60000 datapoints
2025-03-06 17:46:23,950 - INFO - training batch 101, loss: 1.962, 3232/60000 datapoints
2025-03-06 17:46:24,148 - INFO - training batch 151, loss: 1.929, 4832/60000 datapoints
2025-03-06 17:46:24,348 - INFO - training batch 201, loss: 1.942, 6432/60000 datapoints
2025-03-06 17:46:24,542 - INFO - training batch 251, loss: 1.832, 8032/60000 datapoints
2025-03-06 17:46:24,740 - INFO - training batch 301, loss: 1.846, 9632/60000 datapoints
2025-03-06 17:46:24,944 - INFO - training batch 351, loss: 1.976, 11232/60000 datapoints
2025-03-06 17:46:25,141 - INFO - training batch 401, loss: 1.896, 12832/60000 datapoints
2025-03-06 17:46:25,337 - INFO - training batch 451, loss: 1.886, 14432/60000 datapoints
2025-03-06 17:46:25,562 - INFO - training batch 501, loss: 1.914, 16032/60000 datapoints
2025-03-06 17:46:25,762 - INFO - training batch 551, loss: 1.951, 17632/60000 datapoints
2025-03-06 17:46:25,955 - INFO - training batch 601, loss: 1.956, 19232/60000 datapoints
2025-03-06 17:46:26,152 - INFO - training batch 651, loss: 1.850, 20832/60000 datapoints
2025-03-06 17:46:26,347 - INFO - training batch 701, loss: 1.926, 22432/60000 datapoints
2025-03-06 17:46:26,540 - INFO - training batch 751, loss: 1.941, 24032/60000 datapoints
2025-03-06 17:46:26,735 - INFO - training batch 801, loss: 1.900, 25632/60000 datapoints
2025-03-06 17:46:26,931 - INFO - training batch 851, loss: 1.897, 27232/60000 datapoints
2025-03-06 17:46:27,126 - INFO - training batch 901, loss: 1.882, 28832/60000 datapoints
2025-03-06 17:46:27,322 - INFO - training batch 951, loss: 1.922, 30432/60000 datapoints
2025-03-06 17:46:27,517 - INFO - training batch 1001, loss: 1.852, 32032/60000 datapoints
2025-03-06 17:46:27,715 - INFO - training batch 1051, loss: 1.974, 33632/60000 datapoints
2025-03-06 17:46:27,912 - INFO - training batch 1101, loss: 1.949, 35232/60000 datapoints
2025-03-06 17:46:28,104 - INFO - training batch 1151, loss: 1.802, 36832/60000 datapoints
2025-03-06 17:46:28,298 - INFO - training batch 1201, loss: 1.796, 38432/60000 datapoints
2025-03-06 17:46:28,492 - INFO - training batch 1251, loss: 1.884, 40032/60000 datapoints
2025-03-06 17:46:28,691 - INFO - training batch 1301, loss: 1.815, 41632/60000 datapoints
2025-03-06 17:46:28,886 - INFO - training batch 1351, loss: 1.940, 43232/60000 datapoints
2025-03-06 17:46:29,077 - INFO - training batch 1401, loss: 1.808, 44832/60000 datapoints
2025-03-06 17:46:29,272 - INFO - training batch 1451, loss: 1.880, 46432/60000 datapoints
2025-03-06 17:46:29,467 - INFO - training batch 1501, loss: 1.934, 48032/60000 datapoints
2025-03-06 17:46:29,669 - INFO - training batch 1551, loss: 1.884, 49632/60000 datapoints
2025-03-06 17:46:29,868 - INFO - training batch 1601, loss: 1.909, 51232/60000 datapoints
2025-03-06 17:46:30,064 - INFO - training batch 1651, loss: 1.822, 52832/60000 datapoints
2025-03-06 17:46:30,285 - INFO - training batch 1701, loss: 1.809, 54432/60000 datapoints
2025-03-06 17:46:30,481 - INFO - training batch 1751, loss: 1.824, 56032/60000 datapoints
2025-03-06 17:46:30,679 - INFO - training batch 1801, loss: 1.846, 57632/60000 datapoints
2025-03-06 17:46:30,874 - INFO - training batch 1851, loss: 1.865, 59232/60000 datapoints
2025-03-06 17:46:30,974 - INFO - validation batch 1, loss: 1.822, 32/10016 datapoints
2025-03-06 17:46:31,127 - INFO - validation batch 51, loss: 1.804, 1632/10016 datapoints
2025-03-06 17:46:31,281 - INFO - validation batch 101, loss: 1.883, 3232/10016 datapoints
2025-03-06 17:46:31,435 - INFO - validation batch 151, loss: 1.835, 4832/10016 datapoints
2025-03-06 17:46:31,591 - INFO - validation batch 201, loss: 1.878, 6432/10016 datapoints
2025-03-06 17:46:31,750 - INFO - validation batch 251, loss: 1.819, 8032/10016 datapoints
2025-03-06 17:46:31,906 - INFO - validation batch 301, loss: 1.828, 9632/10016 datapoints
2025-03-06 17:46:31,942 - INFO - Epoch 18/800 done.
2025-03-06 17:46:31,942 - INFO - Final validation performance:
Loss: 1.838, top-1 acc: 0.530top-5 acc: 0.530
2025-03-06 17:46:31,943 - INFO - Beginning epoch 19/800
2025-03-06 17:46:31,949 - INFO - training batch 1, loss: 1.816, 32/60000 datapoints
2025-03-06 17:46:32,143 - INFO - training batch 51, loss: 1.813, 1632/60000 datapoints
2025-03-06 17:46:32,338 - INFO - training batch 101, loss: 1.856, 3232/60000 datapoints
2025-03-06 17:46:32,532 - INFO - training batch 151, loss: 1.878, 4832/60000 datapoints
2025-03-06 17:46:32,730 - INFO - training batch 201, loss: 1.806, 6432/60000 datapoints
2025-03-06 17:46:32,927 - INFO - training batch 251, loss: 1.800, 8032/60000 datapoints
2025-03-06 17:46:33,124 - INFO - training batch 301, loss: 1.972, 9632/60000 datapoints
2025-03-06 17:46:33,321 - INFO - training batch 351, loss: 1.869, 11232/60000 datapoints
2025-03-06 17:46:33,514 - INFO - training batch 401, loss: 1.773, 12832/60000 datapoints
2025-03-06 17:46:33,714 - INFO - training batch 451, loss: 1.878, 14432/60000 datapoints
2025-03-06 17:46:33,912 - INFO - training batch 501, loss: 1.806, 16032/60000 datapoints
2025-03-06 17:46:34,108 - INFO - training batch 551, loss: 1.868, 17632/60000 datapoints
2025-03-06 17:46:34,304 - INFO - training batch 601, loss: 1.919, 19232/60000 datapoints
2025-03-06 17:46:34,504 - INFO - training batch 651, loss: 1.778, 20832/60000 datapoints
2025-03-06 17:46:34,702 - INFO - training batch 701, loss: 1.844, 22432/60000 datapoints
2025-03-06 17:46:34,907 - INFO - training batch 751, loss: 1.789, 24032/60000 datapoints
2025-03-06 17:46:35,107 - INFO - training batch 801, loss: 1.886, 25632/60000 datapoints
2025-03-06 17:46:35,305 - INFO - training batch 851, loss: 1.819, 27232/60000 datapoints
2025-03-06 17:46:35,501 - INFO - training batch 901, loss: 1.791, 28832/60000 datapoints
2025-03-06 17:46:35,697 - INFO - training batch 951, loss: 1.894, 30432/60000 datapoints
2025-03-06 17:46:35,896 - INFO - training batch 1001, loss: 1.874, 32032/60000 datapoints
2025-03-06 17:46:36,088 - INFO - training batch 1051, loss: 1.864, 33632/60000 datapoints
2025-03-06 17:46:36,288 - INFO - training batch 1101, loss: 1.746, 35232/60000 datapoints
2025-03-06 17:46:36,484 - INFO - training batch 1151, loss: 1.878, 36832/60000 datapoints
2025-03-06 17:46:36,680 - INFO - training batch 1201, loss: 1.814, 38432/60000 datapoints
2025-03-06 17:46:36,875 - INFO - training batch 1251, loss: 1.776, 40032/60000 datapoints
2025-03-06 17:46:37,068 - INFO - training batch 1301, loss: 1.780, 41632/60000 datapoints
2025-03-06 17:46:37,266 - INFO - training batch 1351, loss: 1.787, 43232/60000 datapoints
2025-03-06 17:46:37,460 - INFO - training batch 1401, loss: 1.783, 44832/60000 datapoints
2025-03-06 17:46:37,708 - INFO - training batch 1451, loss: 1.869, 46432/60000 datapoints
2025-03-06 17:46:37,933 - INFO - training batch 1501, loss: 1.951, 48032/60000 datapoints
2025-03-06 17:46:38,125 - INFO - training batch 1551, loss: 1.891, 49632/60000 datapoints
2025-03-06 17:46:38,320 - INFO - training batch 1601, loss: 1.908, 51232/60000 datapoints
2025-03-06 17:46:38,519 - INFO - training batch 1651, loss: 1.821, 52832/60000 datapoints
2025-03-06 17:46:38,720 - INFO - training batch 1701, loss: 1.710, 54432/60000 datapoints
2025-03-06 17:46:38,916 - INFO - training batch 1751, loss: 1.787, 56032/60000 datapoints
2025-03-06 17:46:39,115 - INFO - training batch 1801, loss: 1.850, 57632/60000 datapoints
2025-03-06 17:46:39,312 - INFO - training batch 1851, loss: 1.809, 59232/60000 datapoints
2025-03-06 17:46:39,412 - INFO - validation batch 1, loss: 1.895, 32/10016 datapoints
2025-03-06 17:46:39,568 - INFO - validation batch 51, loss: 1.800, 1632/10016 datapoints
2025-03-06 17:46:39,726 - INFO - validation batch 101, loss: 1.826, 3232/10016 datapoints
2025-03-06 17:46:39,885 - INFO - validation batch 151, loss: 1.895, 4832/10016 datapoints
2025-03-06 17:46:40,039 - INFO - validation batch 201, loss: 1.796, 6432/10016 datapoints
2025-03-06 17:46:40,194 - INFO - validation batch 251, loss: 1.826, 8032/10016 datapoints
2025-03-06 17:46:40,367 - INFO - validation batch 301, loss: 1.820, 9632/10016 datapoints
2025-03-06 17:46:40,405 - INFO - Epoch 19/800 done.
2025-03-06 17:46:40,405 - INFO - Final validation performance:
Loss: 1.837, top-1 acc: 0.541top-5 acc: 0.541
2025-03-06 17:46:40,405 - INFO - Beginning epoch 20/800
2025-03-06 17:46:40,412 - INFO - training batch 1, loss: 1.805, 32/60000 datapoints
2025-03-06 17:46:40,613 - INFO - training batch 51, loss: 1.815, 1632/60000 datapoints
2025-03-06 17:46:40,811 - INFO - training batch 101, loss: 1.874, 3232/60000 datapoints
2025-03-06 17:46:41,005 - INFO - training batch 151, loss: 2.019, 4832/60000 datapoints
2025-03-06 17:46:41,197 - INFO - training batch 201, loss: 1.772, 6432/60000 datapoints
2025-03-06 17:46:41,397 - INFO - training batch 251, loss: 1.843, 8032/60000 datapoints
2025-03-06 17:46:41,596 - INFO - training batch 301, loss: 1.834, 9632/60000 datapoints
2025-03-06 17:46:41,793 - INFO - training batch 351, loss: 1.801, 11232/60000 datapoints
2025-03-06 17:46:41,987 - INFO - training batch 401, loss: 1.781, 12832/60000 datapoints
2025-03-06 17:46:42,179 - INFO - training batch 451, loss: 1.849, 14432/60000 datapoints
2025-03-06 17:46:42,374 - INFO - training batch 501, loss: 1.774, 16032/60000 datapoints
2025-03-06 17:46:42,567 - INFO - training batch 551, loss: 1.834, 17632/60000 datapoints
2025-03-06 17:46:42,763 - INFO - training batch 601, loss: 1.832, 19232/60000 datapoints
2025-03-06 17:46:42,967 - INFO - training batch 651, loss: 1.823, 20832/60000 datapoints
2025-03-06 17:46:43,159 - INFO - training batch 701, loss: 1.840, 22432/60000 datapoints
2025-03-06 17:46:43,351 - INFO - training batch 751, loss: 1.938, 24032/60000 datapoints
2025-03-06 17:46:43,545 - INFO - training batch 801, loss: 1.806, 25632/60000 datapoints
2025-03-06 17:46:43,741 - INFO - training batch 851, loss: 1.873, 27232/60000 datapoints
2025-03-06 17:46:43,939 - INFO - training batch 901, loss: 1.890, 28832/60000 datapoints
2025-03-06 17:46:44,137 - INFO - training batch 951, loss: 1.842, 30432/60000 datapoints
2025-03-06 17:46:44,331 - INFO - training batch 1001, loss: 1.888, 32032/60000 datapoints
2025-03-06 17:46:44,523 - INFO - training batch 1051, loss: 1.893, 33632/60000 datapoints
2025-03-06 17:46:44,718 - INFO - training batch 1101, loss: 1.725, 35232/60000 datapoints
2025-03-06 17:46:44,921 - INFO - training batch 1151, loss: 1.829, 36832/60000 datapoints
2025-03-06 17:46:45,123 - INFO - training batch 1201, loss: 1.828, 38432/60000 datapoints
2025-03-06 17:46:45,328 - INFO - training batch 1251, loss: 1.776, 40032/60000 datapoints
2025-03-06 17:46:45,527 - INFO - training batch 1301, loss: 1.860, 41632/60000 datapoints
2025-03-06 17:46:45,723 - INFO - training batch 1351, loss: 1.911, 43232/60000 datapoints
2025-03-06 17:46:45,920 - INFO - training batch 1401, loss: 1.799, 44832/60000 datapoints
2025-03-06 17:46:46,112 - INFO - training batch 1451, loss: 1.779, 46432/60000 datapoints
2025-03-06 17:46:46,303 - INFO - training batch 1501, loss: 1.844, 48032/60000 datapoints
2025-03-06 17:46:46,498 - INFO - training batch 1551, loss: 1.811, 49632/60000 datapoints
2025-03-06 17:46:46,693 - INFO - training batch 1601, loss: 1.838, 51232/60000 datapoints
2025-03-06 17:46:46,887 - INFO - training batch 1651, loss: 1.714, 52832/60000 datapoints
2025-03-06 17:46:47,080 - INFO - training batch 1701, loss: 1.653, 54432/60000 datapoints
2025-03-06 17:46:47,274 - INFO - training batch 1751, loss: 1.772, 56032/60000 datapoints
2025-03-06 17:46:47,470 - INFO - training batch 1801, loss: 1.932, 57632/60000 datapoints
2025-03-06 17:46:47,667 - INFO - training batch 1851, loss: 1.972, 59232/60000 datapoints
2025-03-06 17:46:47,769 - INFO - validation batch 1, loss: 1.692, 32/10016 datapoints
2025-03-06 17:46:47,926 - INFO - validation batch 51, loss: 1.867, 1632/10016 datapoints
2025-03-06 17:46:48,079 - INFO - validation batch 101, loss: 1.811, 3232/10016 datapoints
2025-03-06 17:46:48,230 - INFO - validation batch 151, loss: 1.837, 4832/10016 datapoints
2025-03-06 17:46:48,383 - INFO - validation batch 201, loss: 1.803, 6432/10016 datapoints
2025-03-06 17:46:48,537 - INFO - validation batch 251, loss: 1.767, 8032/10016 datapoints
2025-03-06 17:46:48,691 - INFO - validation batch 301, loss: 1.793, 9632/10016 datapoints
2025-03-06 17:46:48,728 - INFO - Epoch 20/800 done.
2025-03-06 17:46:48,729 - INFO - Final validation performance:
Loss: 1.796, top-1 acc: 0.555top-5 acc: 0.555
2025-03-06 17:46:48,729 - INFO - Beginning epoch 21/800
2025-03-06 17:46:48,736 - INFO - training batch 1, loss: 1.794, 32/60000 datapoints
2025-03-06 17:46:48,930 - INFO - training batch 51, loss: 1.749, 1632/60000 datapoints
2025-03-06 17:46:49,124 - INFO - training batch 101, loss: 1.813, 3232/60000 datapoints
2025-03-06 17:46:49,317 - INFO - training batch 151, loss: 1.835, 4832/60000 datapoints
2025-03-06 17:46:49,512 - INFO - training batch 201, loss: 1.805, 6432/60000 datapoints
2025-03-06 17:46:49,711 - INFO - training batch 251, loss: 1.841, 8032/60000 datapoints
2025-03-06 17:46:49,910 - INFO - training batch 301, loss: 1.828, 9632/60000 datapoints
2025-03-06 17:46:50,109 - INFO - training batch 351, loss: 1.782, 11232/60000 datapoints
2025-03-06 17:46:50,311 - INFO - training batch 401, loss: 1.908, 12832/60000 datapoints
2025-03-06 17:46:50,541 - INFO - training batch 451, loss: 1.845, 14432/60000 datapoints
2025-03-06 17:46:50,737 - INFO - training batch 501, loss: 1.810, 16032/60000 datapoints
2025-03-06 17:46:50,930 - INFO - training batch 551, loss: 1.702, 17632/60000 datapoints
2025-03-06 17:46:51,127 - INFO - training batch 601, loss: 1.892, 19232/60000 datapoints
2025-03-06 17:46:51,325 - INFO - training batch 651, loss: 1.814, 20832/60000 datapoints
2025-03-06 17:46:51,522 - INFO - training batch 701, loss: 1.728, 22432/60000 datapoints
2025-03-06 17:46:51,720 - INFO - training batch 751, loss: 1.821, 24032/60000 datapoints
2025-03-06 17:46:51,919 - INFO - training batch 801, loss: 1.691, 25632/60000 datapoints
2025-03-06 17:46:52,113 - INFO - training batch 851, loss: 1.800, 27232/60000 datapoints
2025-03-06 17:46:52,309 - INFO - training batch 901, loss: 1.870, 28832/60000 datapoints
2025-03-06 17:46:52,503 - INFO - training batch 951, loss: 1.780, 30432/60000 datapoints
2025-03-06 17:46:52,698 - INFO - training batch 1001, loss: 1.734, 32032/60000 datapoints
2025-03-06 17:46:52,894 - INFO - training batch 1051, loss: 1.735, 33632/60000 datapoints
2025-03-06 17:46:53,089 - INFO - training batch 1101, loss: 1.610, 35232/60000 datapoints
2025-03-06 17:46:53,282 - INFO - training batch 1151, loss: 1.732, 36832/60000 datapoints
2025-03-06 17:46:53,476 - INFO - training batch 1201, loss: 1.730, 38432/60000 datapoints
2025-03-06 17:46:53,674 - INFO - training batch 1251, loss: 1.763, 40032/60000 datapoints
2025-03-06 17:46:53,871 - INFO - training batch 1301, loss: 1.797, 41632/60000 datapoints
2025-03-06 17:46:54,065 - INFO - training batch 1351, loss: 1.796, 43232/60000 datapoints
2025-03-06 17:46:54,258 - INFO - training batch 1401, loss: 1.812, 44832/60000 datapoints
2025-03-06 17:46:54,456 - INFO - training batch 1451, loss: 1.695, 46432/60000 datapoints
2025-03-06 17:46:54,653 - INFO - training batch 1501, loss: 1.688, 48032/60000 datapoints
2025-03-06 17:46:54,854 - INFO - training batch 1551, loss: 1.828, 49632/60000 datapoints
2025-03-06 17:46:55,052 - INFO - training batch 1601, loss: 1.738, 51232/60000 datapoints
2025-03-06 17:46:55,247 - INFO - training batch 1651, loss: 1.711, 52832/60000 datapoints
2025-03-06 17:46:55,443 - INFO - training batch 1701, loss: 1.837, 54432/60000 datapoints
2025-03-06 17:46:55,645 - INFO - training batch 1751, loss: 1.738, 56032/60000 datapoints
2025-03-06 17:46:55,842 - INFO - training batch 1801, loss: 1.748, 57632/60000 datapoints
2025-03-06 17:46:56,039 - INFO - training batch 1851, loss: 1.745, 59232/60000 datapoints
2025-03-06 17:46:56,140 - INFO - validation batch 1, loss: 1.722, 32/10016 datapoints
2025-03-06 17:46:56,293 - INFO - validation batch 51, loss: 1.835, 1632/10016 datapoints
2025-03-06 17:46:56,445 - INFO - validation batch 101, loss: 1.644, 3232/10016 datapoints
2025-03-06 17:46:56,600 - INFO - validation batch 151, loss: 1.794, 4832/10016 datapoints
2025-03-06 17:46:56,752 - INFO - validation batch 201, loss: 1.879, 6432/10016 datapoints
2025-03-06 17:46:56,904 - INFO - validation batch 251, loss: 1.738, 8032/10016 datapoints
2025-03-06 17:46:57,056 - INFO - validation batch 301, loss: 1.843, 9632/10016 datapoints
2025-03-06 17:46:57,092 - INFO - Epoch 21/800 done.
2025-03-06 17:46:57,092 - INFO - Final validation performance:
Loss: 1.779, top-1 acc: 0.567top-5 acc: 0.567
2025-03-06 17:46:57,092 - INFO - Beginning epoch 22/800
2025-03-06 17:46:57,099 - INFO - training batch 1, loss: 1.803, 32/60000 datapoints
2025-03-06 17:46:57,518 - INFO - training batch 51, loss: 1.736, 1632/60000 datapoints
2025-03-06 17:46:57,712 - INFO - training batch 101, loss: 1.735, 3232/60000 datapoints
2025-03-06 17:46:57,909 - INFO - training batch 151, loss: 1.723, 4832/60000 datapoints
2025-03-06 17:46:58,101 - INFO - training batch 201, loss: 1.903, 6432/60000 datapoints
2025-03-06 17:46:58,294 - INFO - training batch 251, loss: 1.685, 8032/60000 datapoints
2025-03-06 17:46:58,488 - INFO - training batch 301, loss: 1.756, 9632/60000 datapoints
2025-03-06 17:46:58,684 - INFO - training batch 351, loss: 1.838, 11232/60000 datapoints
2025-03-06 17:46:58,880 - INFO - training batch 401, loss: 1.724, 12832/60000 datapoints
2025-03-06 17:46:59,094 - INFO - training batch 451, loss: 1.767, 14432/60000 datapoints
2025-03-06 17:46:59,289 - INFO - training batch 501, loss: 1.795, 16032/60000 datapoints
2025-03-06 17:46:59,486 - INFO - training batch 551, loss: 1.793, 17632/60000 datapoints
2025-03-06 17:46:59,770 - INFO - training batch 601, loss: 1.876, 19232/60000 datapoints
2025-03-06 17:47:00,039 - INFO - training batch 651, loss: 1.724, 20832/60000 datapoints
2025-03-06 17:47:00,298 - INFO - training batch 701, loss: 1.738, 22432/60000 datapoints
2025-03-06 17:47:00,568 - INFO - training batch 751, loss: 1.672, 24032/60000 datapoints
2025-03-06 17:47:00,788 - INFO - training batch 801, loss: 1.705, 25632/60000 datapoints
2025-03-06 17:47:01,014 - INFO - training batch 851, loss: 1.717, 27232/60000 datapoints
2025-03-06 17:47:01,229 - INFO - training batch 901, loss: 1.677, 28832/60000 datapoints
2025-03-06 17:47:01,442 - INFO - training batch 951, loss: 1.792, 30432/60000 datapoints
2025-03-06 17:47:01,672 - INFO - training batch 1001, loss: 1.695, 32032/60000 datapoints
2025-03-06 17:47:01,900 - INFO - training batch 1051, loss: 1.737, 33632/60000 datapoints
2025-03-06 17:47:02,125 - INFO - training batch 1101, loss: 1.728, 35232/60000 datapoints
2025-03-06 17:47:02,350 - INFO - training batch 1151, loss: 1.722, 36832/60000 datapoints
2025-03-06 17:47:02,571 - INFO - training batch 1201, loss: 1.724, 38432/60000 datapoints
2025-03-06 17:47:02,799 - INFO - training batch 1251, loss: 1.790, 40032/60000 datapoints
2025-03-06 17:47:03,052 - INFO - training batch 1301, loss: 1.797, 41632/60000 datapoints
2025-03-06 17:47:03,275 - INFO - training batch 1351, loss: 1.668, 43232/60000 datapoints
2025-03-06 17:47:03,484 - INFO - training batch 1401, loss: 1.732, 44832/60000 datapoints
2025-03-06 17:47:03,702 - INFO - training batch 1451, loss: 1.740, 46432/60000 datapoints
2025-03-06 17:47:03,925 - INFO - training batch 1501, loss: 1.809, 48032/60000 datapoints
2025-03-06 17:47:04,163 - INFO - training batch 1551, loss: 1.867, 49632/60000 datapoints
2025-03-06 17:47:04,391 - INFO - training batch 1601, loss: 1.804, 51232/60000 datapoints
2025-03-06 17:47:04,632 - INFO - training batch 1651, loss: 1.711, 52832/60000 datapoints
2025-03-06 17:47:04,844 - INFO - training batch 1701, loss: 1.754, 54432/60000 datapoints
2025-03-06 17:47:05,068 - INFO - training batch 1751, loss: 1.777, 56032/60000 datapoints
2025-03-06 17:47:05,280 - INFO - training batch 1801, loss: 1.812, 57632/60000 datapoints
2025-03-06 17:47:05,497 - INFO - training batch 1851, loss: 1.718, 59232/60000 datapoints
2025-03-06 17:47:05,613 - INFO - validation batch 1, loss: 1.767, 32/10016 datapoints
2025-03-06 17:47:05,780 - INFO - validation batch 51, loss: 1.716, 1632/10016 datapoints
2025-03-06 17:47:05,945 - INFO - validation batch 101, loss: 1.758, 3232/10016 datapoints
2025-03-06 17:47:06,109 - INFO - validation batch 151, loss: 1.796, 4832/10016 datapoints
2025-03-06 17:47:06,275 - INFO - validation batch 201, loss: 1.647, 6432/10016 datapoints
2025-03-06 17:47:06,442 - INFO - validation batch 251, loss: 1.701, 8032/10016 datapoints
2025-03-06 17:47:06,609 - INFO - validation batch 301, loss: 1.698, 9632/10016 datapoints
2025-03-06 17:47:06,647 - INFO - Epoch 22/800 done.
2025-03-06 17:47:06,648 - INFO - Final validation performance:
Loss: 1.726, top-1 acc: 0.577top-5 acc: 0.577
2025-03-06 17:47:06,648 - INFO - Beginning epoch 23/800
2025-03-06 17:47:06,655 - INFO - training batch 1, loss: 1.718, 32/60000 datapoints
2025-03-06 17:47:06,863 - INFO - training batch 51, loss: 1.761, 1632/60000 datapoints
2025-03-06 17:47:07,071 - INFO - training batch 101, loss: 1.836, 3232/60000 datapoints
2025-03-06 17:47:07,275 - INFO - training batch 151, loss: 1.700, 4832/60000 datapoints
2025-03-06 17:47:07,487 - INFO - training batch 201, loss: 1.701, 6432/60000 datapoints
2025-03-06 17:47:07,730 - INFO - training batch 251, loss: 1.692, 8032/60000 datapoints
2025-03-06 17:47:07,952 - INFO - training batch 301, loss: 1.695, 9632/60000 datapoints
2025-03-06 17:47:08,173 - INFO - training batch 351, loss: 1.714, 11232/60000 datapoints
2025-03-06 17:47:08,387 - INFO - training batch 401, loss: 1.670, 12832/60000 datapoints
2025-03-06 17:47:08,606 - INFO - training batch 451, loss: 1.717, 14432/60000 datapoints
2025-03-06 17:47:08,817 - INFO - training batch 501, loss: 1.720, 16032/60000 datapoints
2025-03-06 17:47:09,034 - INFO - training batch 551, loss: 1.731, 17632/60000 datapoints
2025-03-06 17:47:09,253 - INFO - training batch 601, loss: 1.640, 19232/60000 datapoints
2025-03-06 17:47:09,468 - INFO - training batch 651, loss: 1.707, 20832/60000 datapoints
2025-03-06 17:47:09,681 - INFO - training batch 701, loss: 1.784, 22432/60000 datapoints
2025-03-06 17:47:09,885 - INFO - training batch 751, loss: 1.798, 24032/60000 datapoints
2025-03-06 17:47:10,093 - INFO - training batch 801, loss: 1.573, 25632/60000 datapoints
2025-03-06 17:47:10,310 - INFO - training batch 851, loss: 1.702, 27232/60000 datapoints
2025-03-06 17:47:10,543 - INFO - training batch 901, loss: 1.793, 28832/60000 datapoints
2025-03-06 17:47:10,750 - INFO - training batch 951, loss: 1.702, 30432/60000 datapoints
2025-03-06 17:47:10,953 - INFO - training batch 1001, loss: 1.681, 32032/60000 datapoints
2025-03-06 17:47:11,152 - INFO - training batch 1051, loss: 1.727, 33632/60000 datapoints
2025-03-06 17:47:11,354 - INFO - training batch 1101, loss: 1.723, 35232/60000 datapoints
2025-03-06 17:47:11,551 - INFO - training batch 1151, loss: 1.772, 36832/60000 datapoints
2025-03-06 17:47:11,767 - INFO - training batch 1201, loss: 1.630, 38432/60000 datapoints
2025-03-06 17:47:11,976 - INFO - training batch 1251, loss: 1.870, 40032/60000 datapoints
2025-03-06 17:47:12,183 - INFO - training batch 1301, loss: 1.595, 41632/60000 datapoints
2025-03-06 17:47:12,385 - INFO - training batch 1351, loss: 1.734, 43232/60000 datapoints
2025-03-06 17:47:12,600 - INFO - training batch 1401, loss: 1.815, 44832/60000 datapoints
2025-03-06 17:47:12,810 - INFO - training batch 1451, loss: 1.771, 46432/60000 datapoints
2025-03-06 17:47:13,022 - INFO - training batch 1501, loss: 1.753, 48032/60000 datapoints
2025-03-06 17:47:13,284 - INFO - training batch 1551, loss: 1.743, 49632/60000 datapoints
2025-03-06 17:47:13,536 - INFO - training batch 1601, loss: 1.762, 51232/60000 datapoints
2025-03-06 17:47:13,747 - INFO - training batch 1651, loss: 1.783, 52832/60000 datapoints
2025-03-06 17:47:13,956 - INFO - training batch 1701, loss: 1.665, 54432/60000 datapoints
2025-03-06 17:47:14,181 - INFO - training batch 1751, loss: 1.654, 56032/60000 datapoints
2025-03-06 17:47:14,423 - INFO - training batch 1801, loss: 1.656, 57632/60000 datapoints
2025-03-06 17:47:14,653 - INFO - training batch 1851, loss: 1.705, 59232/60000 datapoints
2025-03-06 17:47:14,780 - INFO - validation batch 1, loss: 1.632, 32/10016 datapoints
2025-03-06 17:47:14,986 - INFO - validation batch 51, loss: 1.719, 1632/10016 datapoints
2025-03-06 17:47:15,210 - INFO - validation batch 101, loss: 1.733, 3232/10016 datapoints
2025-03-06 17:47:15,376 - INFO - validation batch 151, loss: 1.660, 4832/10016 datapoints
2025-03-06 17:47:15,541 - INFO - validation batch 201, loss: 1.507, 6432/10016 datapoints
2025-03-06 17:47:15,710 - INFO - validation batch 251, loss: 1.655, 8032/10016 datapoints
2025-03-06 17:47:15,873 - INFO - validation batch 301, loss: 1.647, 9632/10016 datapoints
2025-03-06 17:47:15,916 - INFO - Epoch 23/800 done.
2025-03-06 17:47:15,916 - INFO - Final validation performance:
Loss: 1.650, top-1 acc: 0.587top-5 acc: 0.587
2025-03-06 17:47:15,917 - INFO - Beginning epoch 24/800
2025-03-06 17:47:15,923 - INFO - training batch 1, loss: 1.746, 32/60000 datapoints
2025-03-06 17:47:16,131 - INFO - training batch 51, loss: 1.752, 1632/60000 datapoints
2025-03-06 17:47:16,332 - INFO - training batch 101, loss: 1.607, 3232/60000 datapoints
2025-03-06 17:47:16,529 - INFO - training batch 151, loss: 1.677, 4832/60000 datapoints
2025-03-06 17:47:16,732 - INFO - training batch 201, loss: 1.607, 6432/60000 datapoints
2025-03-06 17:47:16,932 - INFO - training batch 251, loss: 1.673, 8032/60000 datapoints
2025-03-06 17:47:17,133 - INFO - training batch 301, loss: 1.680, 9632/60000 datapoints
2025-03-06 17:47:17,327 - INFO - training batch 351, loss: 1.615, 11232/60000 datapoints
2025-03-06 17:47:17,521 - INFO - training batch 401, loss: 1.691, 12832/60000 datapoints
2025-03-06 17:47:17,719 - INFO - training batch 451, loss: 1.671, 14432/60000 datapoints
2025-03-06 17:47:17,917 - INFO - training batch 501, loss: 1.629, 16032/60000 datapoints
2025-03-06 17:47:18,112 - INFO - training batch 551, loss: 1.616, 17632/60000 datapoints
2025-03-06 17:47:18,309 - INFO - training batch 601, loss: 1.798, 19232/60000 datapoints
2025-03-06 17:47:18,503 - INFO - training batch 651, loss: 1.709, 20832/60000 datapoints
2025-03-06 17:47:18,700 - INFO - training batch 701, loss: 1.802, 22432/60000 datapoints
2025-03-06 17:47:18,893 - INFO - training batch 751, loss: 1.603, 24032/60000 datapoints
2025-03-06 17:47:19,205 - INFO - training batch 801, loss: 1.675, 25632/60000 datapoints
2025-03-06 17:47:19,420 - INFO - training batch 851, loss: 1.608, 27232/60000 datapoints
2025-03-06 17:47:19,619 - INFO - training batch 901, loss: 1.675, 28832/60000 datapoints
2025-03-06 17:47:19,822 - INFO - training batch 951, loss: 1.574, 30432/60000 datapoints
2025-03-06 17:47:20,019 - INFO - training batch 1001, loss: 1.625, 32032/60000 datapoints
2025-03-06 17:47:20,216 - INFO - training batch 1051, loss: 1.651, 33632/60000 datapoints
2025-03-06 17:47:20,412 - INFO - training batch 1101, loss: 1.682, 35232/60000 datapoints
2025-03-06 17:47:20,623 - INFO - training batch 1151, loss: 1.775, 36832/60000 datapoints
2025-03-06 17:47:20,853 - INFO - training batch 1201, loss: 1.689, 38432/60000 datapoints
2025-03-06 17:47:21,057 - INFO - training batch 1251, loss: 1.650, 40032/60000 datapoints
2025-03-06 17:47:21,251 - INFO - training batch 1301, loss: 1.691, 41632/60000 datapoints
2025-03-06 17:47:21,443 - INFO - training batch 1351, loss: 1.690, 43232/60000 datapoints
2025-03-06 17:47:21,639 - INFO - training batch 1401, loss: 1.711, 44832/60000 datapoints
2025-03-06 17:47:21,839 - INFO - training batch 1451, loss: 1.625, 46432/60000 datapoints
2025-03-06 17:47:22,035 - INFO - training batch 1501, loss: 1.672, 48032/60000 datapoints
2025-03-06 17:47:22,227 - INFO - training batch 1551, loss: 1.604, 49632/60000 datapoints
2025-03-06 17:47:22,420 - INFO - training batch 1601, loss: 1.648, 51232/60000 datapoints
2025-03-06 17:47:22,619 - INFO - training batch 1651, loss: 1.752, 52832/60000 datapoints
2025-03-06 17:47:22,814 - INFO - training batch 1701, loss: 1.752, 54432/60000 datapoints
2025-03-06 17:47:23,008 - INFO - training batch 1751, loss: 1.611, 56032/60000 datapoints
2025-03-06 17:47:23,203 - INFO - training batch 1801, loss: 1.648, 57632/60000 datapoints
2025-03-06 17:47:23,399 - INFO - training batch 1851, loss: 1.720, 59232/60000 datapoints
2025-03-06 17:47:23,500 - INFO - validation batch 1, loss: 1.718, 32/10016 datapoints
2025-03-06 17:47:23,653 - INFO - validation batch 51, loss: 1.716, 1632/10016 datapoints
2025-03-06 17:47:23,809 - INFO - validation batch 101, loss: 1.659, 3232/10016 datapoints
2025-03-06 17:47:23,963 - INFO - validation batch 151, loss: 1.702, 4832/10016 datapoints
2025-03-06 17:47:24,115 - INFO - validation batch 201, loss: 1.666, 6432/10016 datapoints
2025-03-06 17:47:24,268 - INFO - validation batch 251, loss: 1.507, 8032/10016 datapoints
2025-03-06 17:47:24,421 - INFO - validation batch 301, loss: 1.536, 9632/10016 datapoints
2025-03-06 17:47:24,459 - INFO - Epoch 24/800 done.
2025-03-06 17:47:24,460 - INFO - Final validation performance:
Loss: 1.644, top-1 acc: 0.596top-5 acc: 0.596
2025-03-06 17:47:24,460 - INFO - Beginning epoch 25/800
2025-03-06 17:47:24,465 - INFO - training batch 1, loss: 1.676, 32/60000 datapoints
2025-03-06 17:47:24,661 - INFO - training batch 51, loss: 1.696, 1632/60000 datapoints
2025-03-06 17:47:24,881 - INFO - training batch 101, loss: 1.588, 3232/60000 datapoints
2025-03-06 17:47:25,077 - INFO - training batch 151, loss: 1.653, 4832/60000 datapoints
2025-03-06 17:47:25,273 - INFO - training batch 201, loss: 1.798, 6432/60000 datapoints
2025-03-06 17:47:25,469 - INFO - training batch 251, loss: 1.734, 8032/60000 datapoints
2025-03-06 17:47:25,670 - INFO - training batch 301, loss: 1.572, 9632/60000 datapoints
2025-03-06 17:47:25,868 - INFO - training batch 351, loss: 1.657, 11232/60000 datapoints
2025-03-06 17:47:26,065 - INFO - training batch 401, loss: 1.573, 12832/60000 datapoints
2025-03-06 17:47:26,261 - INFO - training batch 451, loss: 1.590, 14432/60000 datapoints
2025-03-06 17:47:26,457 - INFO - training batch 501, loss: 1.661, 16032/60000 datapoints
2025-03-06 17:47:26,654 - INFO - training batch 551, loss: 1.632, 17632/60000 datapoints
2025-03-06 17:47:26,853 - INFO - training batch 601, loss: 1.615, 19232/60000 datapoints
2025-03-06 17:47:27,046 - INFO - training batch 651, loss: 1.655, 20832/60000 datapoints
2025-03-06 17:47:27,242 - INFO - training batch 701, loss: 1.659, 22432/60000 datapoints
2025-03-06 17:47:27,438 - INFO - training batch 751, loss: 1.731, 24032/60000 datapoints
2025-03-06 17:47:27,636 - INFO - training batch 801, loss: 1.501, 25632/60000 datapoints
2025-03-06 17:47:27,830 - INFO - training batch 851, loss: 1.642, 27232/60000 datapoints
2025-03-06 17:47:28,026 - INFO - training batch 901, loss: 1.626, 28832/60000 datapoints
2025-03-06 17:47:28,218 - INFO - training batch 951, loss: 1.528, 30432/60000 datapoints
2025-03-06 17:47:28,412 - INFO - training batch 1001, loss: 1.680, 32032/60000 datapoints
2025-03-06 17:47:28,608 - INFO - training batch 1051, loss: 1.660, 33632/60000 datapoints
2025-03-06 17:47:28,802 - INFO - training batch 1101, loss: 1.680, 35232/60000 datapoints
2025-03-06 17:47:28,996 - INFO - training batch 1151, loss: 1.584, 36832/60000 datapoints
2025-03-06 17:47:29,190 - INFO - training batch 1201, loss: 1.615, 38432/60000 datapoints
2025-03-06 17:47:29,384 - INFO - training batch 1251, loss: 1.703, 40032/60000 datapoints
2025-03-06 17:47:29,574 - INFO - training batch 1301, loss: 1.668, 41632/60000 datapoints
2025-03-06 17:47:29,768 - INFO - training batch 1351, loss: 1.616, 43232/60000 datapoints
2025-03-06 17:47:29,966 - INFO - training batch 1401, loss: 1.638, 44832/60000 datapoints
2025-03-06 17:47:30,159 - INFO - training batch 1451, loss: 1.606, 46432/60000 datapoints
2025-03-06 17:47:30,353 - INFO - training batch 1501, loss: 1.564, 48032/60000 datapoints
2025-03-06 17:47:30,546 - INFO - training batch 1551, loss: 1.598, 49632/60000 datapoints
2025-03-06 17:47:30,761 - INFO - training batch 1601, loss: 1.625, 51232/60000 datapoints
2025-03-06 17:47:30,954 - INFO - training batch 1651, loss: 1.718, 52832/60000 datapoints
2025-03-06 17:47:31,153 - INFO - training batch 1701, loss: 1.608, 54432/60000 datapoints
2025-03-06 17:47:31,357 - INFO - training batch 1751, loss: 1.694, 56032/60000 datapoints
2025-03-06 17:47:31,548 - INFO - training batch 1801, loss: 1.619, 57632/60000 datapoints
2025-03-06 17:47:31,747 - INFO - training batch 1851, loss: 1.680, 59232/60000 datapoints
2025-03-06 17:47:31,849 - INFO - validation batch 1, loss: 1.448, 32/10016 datapoints
2025-03-06 17:47:32,003 - INFO - validation batch 51, loss: 1.715, 1632/10016 datapoints
2025-03-06 17:47:32,155 - INFO - validation batch 101, loss: 1.624, 3232/10016 datapoints
2025-03-06 17:47:32,307 - INFO - validation batch 151, loss: 1.555, 4832/10016 datapoints
2025-03-06 17:47:32,463 - INFO - validation batch 201, loss: 1.684, 6432/10016 datapoints
2025-03-06 17:47:32,618 - INFO - validation batch 251, loss: 1.598, 8032/10016 datapoints
2025-03-06 17:47:32,770 - INFO - validation batch 301, loss: 1.565, 9632/10016 datapoints
2025-03-06 17:47:32,807 - INFO - Epoch 25/800 done.
2025-03-06 17:47:32,808 - INFO - Final validation performance:
Loss: 1.598, top-1 acc: 0.610top-5 acc: 0.610
2025-03-06 17:47:32,808 - INFO - Beginning epoch 26/800
2025-03-06 17:47:32,814 - INFO - training batch 1, loss: 1.609, 32/60000 datapoints
2025-03-06 17:47:33,042 - INFO - training batch 51, loss: 1.697, 1632/60000 datapoints
2025-03-06 17:47:33,265 - INFO - training batch 101, loss: 1.633, 3232/60000 datapoints
2025-03-06 17:47:33,473 - INFO - training batch 151, loss: 1.640, 4832/60000 datapoints
2025-03-06 17:47:33,685 - INFO - training batch 201, loss: 1.586, 6432/60000 datapoints
2025-03-06 17:47:33,912 - INFO - training batch 251, loss: 1.588, 8032/60000 datapoints
2025-03-06 17:47:34,128 - INFO - training batch 301, loss: 1.567, 9632/60000 datapoints
2025-03-06 17:47:34,329 - INFO - training batch 351, loss: 1.591, 11232/60000 datapoints
2025-03-06 17:47:34,524 - INFO - training batch 401, loss: 1.616, 12832/60000 datapoints
2025-03-06 17:47:34,720 - INFO - training batch 451, loss: 1.711, 14432/60000 datapoints
2025-03-06 17:47:34,918 - INFO - training batch 501, loss: 1.644, 16032/60000 datapoints
2025-03-06 17:47:35,117 - INFO - training batch 551, loss: 1.585, 17632/60000 datapoints
2025-03-06 17:47:35,315 - INFO - training batch 601, loss: 1.611, 19232/60000 datapoints
2025-03-06 17:47:35,514 - INFO - training batch 651, loss: 1.515, 20832/60000 datapoints
2025-03-06 17:47:35,707 - INFO - training batch 701, loss: 1.732, 22432/60000 datapoints
2025-03-06 17:47:35,899 - INFO - training batch 751, loss: 1.577, 24032/60000 datapoints
2025-03-06 17:47:36,094 - INFO - training batch 801, loss: 1.544, 25632/60000 datapoints
2025-03-06 17:47:36,293 - INFO - training batch 851, loss: 1.589, 27232/60000 datapoints
2025-03-06 17:47:36,487 - INFO - training batch 901, loss: 1.553, 28832/60000 datapoints
2025-03-06 17:47:36,688 - INFO - training batch 951, loss: 1.517, 30432/60000 datapoints
2025-03-06 17:47:36,880 - INFO - training batch 1001, loss: 1.697, 32032/60000 datapoints
2025-03-06 17:47:37,085 - INFO - training batch 1051, loss: 1.531, 33632/60000 datapoints
2025-03-06 17:47:37,282 - INFO - training batch 1101, loss: 1.589, 35232/60000 datapoints
2025-03-06 17:47:37,482 - INFO - training batch 1151, loss: 1.612, 36832/60000 datapoints
2025-03-06 17:47:37,709 - INFO - training batch 1201, loss: 1.702, 38432/60000 datapoints
2025-03-06 17:47:37,917 - INFO - training batch 1251, loss: 1.780, 40032/60000 datapoints
2025-03-06 17:47:38,119 - INFO - training batch 1301, loss: 1.442, 41632/60000 datapoints
2025-03-06 17:47:38,323 - INFO - training batch 1351, loss: 1.463, 43232/60000 datapoints
2025-03-06 17:47:38,537 - INFO - training batch 1401, loss: 1.550, 44832/60000 datapoints
2025-03-06 17:47:38,752 - INFO - training batch 1451, loss: 1.704, 46432/60000 datapoints
2025-03-06 17:47:38,952 - INFO - training batch 1501, loss: 1.590, 48032/60000 datapoints
2025-03-06 17:47:39,157 - INFO - training batch 1551, loss: 1.673, 49632/60000 datapoints
2025-03-06 17:47:39,351 - INFO - training batch 1601, loss: 1.600, 51232/60000 datapoints
2025-03-06 17:47:39,551 - INFO - training batch 1651, loss: 1.567, 52832/60000 datapoints
2025-03-06 17:47:39,773 - INFO - training batch 1701, loss: 1.524, 54432/60000 datapoints
2025-03-06 17:47:39,974 - INFO - training batch 1751, loss: 1.567, 56032/60000 datapoints
2025-03-06 17:47:40,175 - INFO - training batch 1801, loss: 1.558, 57632/60000 datapoints
2025-03-06 17:47:40,374 - INFO - training batch 1851, loss: 1.605, 59232/60000 datapoints
2025-03-06 17:47:40,486 - INFO - validation batch 1, loss: 1.449, 32/10016 datapoints
2025-03-06 17:47:40,644 - INFO - validation batch 51, loss: 1.531, 1632/10016 datapoints
2025-03-06 17:47:40,825 - INFO - validation batch 101, loss: 1.680, 3232/10016 datapoints
2025-03-06 17:47:40,988 - INFO - validation batch 151, loss: 1.570, 4832/10016 datapoints
2025-03-06 17:47:41,144 - INFO - validation batch 201, loss: 1.589, 6432/10016 datapoints
2025-03-06 17:47:41,300 - INFO - validation batch 251, loss: 1.502, 8032/10016 datapoints
2025-03-06 17:47:41,451 - INFO - validation batch 301, loss: 1.524, 9632/10016 datapoints
2025-03-06 17:47:41,489 - INFO - Epoch 26/800 done.
2025-03-06 17:47:41,489 - INFO - Final validation performance:
Loss: 1.549, top-1 acc: 0.629top-5 acc: 0.629
2025-03-06 17:47:41,490 - INFO - Beginning epoch 27/800
2025-03-06 17:47:41,496 - INFO - training batch 1, loss: 1.523, 32/60000 datapoints
2025-03-06 17:47:41,692 - INFO - training batch 51, loss: 1.595, 1632/60000 datapoints
2025-03-06 17:47:41,887 - INFO - training batch 101, loss: 1.615, 3232/60000 datapoints
2025-03-06 17:47:42,088 - INFO - training batch 151, loss: 1.658, 4832/60000 datapoints
2025-03-06 17:47:42,281 - INFO - training batch 201, loss: 1.563, 6432/60000 datapoints
2025-03-06 17:47:42,475 - INFO - training batch 251, loss: 1.564, 8032/60000 datapoints
2025-03-06 17:47:42,672 - INFO - training batch 301, loss: 1.666, 9632/60000 datapoints
2025-03-06 17:47:42,867 - INFO - training batch 351, loss: 1.576, 11232/60000 datapoints
2025-03-06 17:47:43,066 - INFO - training batch 401, loss: 1.655, 12832/60000 datapoints
2025-03-06 17:47:43,283 - INFO - training batch 451, loss: 1.519, 14432/60000 datapoints
2025-03-06 17:47:43,492 - INFO - training batch 501, loss: 1.543, 16032/60000 datapoints
2025-03-06 17:47:43,700 - INFO - training batch 551, loss: 1.486, 17632/60000 datapoints
2025-03-06 17:47:43,926 - INFO - training batch 601, loss: 1.568, 19232/60000 datapoints
2025-03-06 17:47:44,144 - INFO - training batch 651, loss: 1.611, 20832/60000 datapoints
2025-03-06 17:47:44,340 - INFO - training batch 701, loss: 1.553, 22432/60000 datapoints
2025-03-06 17:47:44,541 - INFO - training batch 751, loss: 1.509, 24032/60000 datapoints
2025-03-06 17:47:44,736 - INFO - training batch 801, loss: 1.554, 25632/60000 datapoints
2025-03-06 17:47:44,934 - INFO - training batch 851, loss: 1.597, 27232/60000 datapoints
2025-03-06 17:47:45,128 - INFO - training batch 901, loss: 1.565, 28832/60000 datapoints
2025-03-06 17:47:45,327 - INFO - training batch 951, loss: 1.621, 30432/60000 datapoints
2025-03-06 17:47:45,523 - INFO - training batch 1001, loss: 1.653, 32032/60000 datapoints
2025-03-06 17:47:45,717 - INFO - training batch 1051, loss: 1.687, 33632/60000 datapoints
2025-03-06 17:47:45,910 - INFO - training batch 1101, loss: 1.622, 35232/60000 datapoints
2025-03-06 17:47:46,107 - INFO - training batch 1151, loss: 1.549, 36832/60000 datapoints
2025-03-06 17:47:46,302 - INFO - training batch 1201, loss: 1.519, 38432/60000 datapoints
2025-03-06 17:47:46,495 - INFO - training batch 1251, loss: 1.463, 40032/60000 datapoints
2025-03-06 17:47:46,694 - INFO - training batch 1301, loss: 1.663, 41632/60000 datapoints
2025-03-06 17:47:46,889 - INFO - training batch 1351, loss: 1.496, 43232/60000 datapoints
2025-03-06 17:47:47,084 - INFO - training batch 1401, loss: 1.430, 44832/60000 datapoints
2025-03-06 17:47:47,278 - INFO - training batch 1451, loss: 1.545, 46432/60000 datapoints
2025-03-06 17:47:47,474 - INFO - training batch 1501, loss: 1.497, 48032/60000 datapoints
2025-03-06 17:47:47,670 - INFO - training batch 1551, loss: 1.517, 49632/60000 datapoints
2025-03-06 17:47:47,863 - INFO - training batch 1601, loss: 1.459, 51232/60000 datapoints
2025-03-06 17:47:48,061 - INFO - training batch 1651, loss: 1.564, 52832/60000 datapoints
2025-03-06 17:47:48,253 - INFO - training batch 1701, loss: 1.594, 54432/60000 datapoints
2025-03-06 17:47:48,448 - INFO - training batch 1751, loss: 1.578, 56032/60000 datapoints
2025-03-06 17:47:48,643 - INFO - training batch 1801, loss: 1.467, 57632/60000 datapoints
2025-03-06 17:47:48,836 - INFO - training batch 1851, loss: 1.497, 59232/60000 datapoints
2025-03-06 17:47:48,937 - INFO - validation batch 1, loss: 1.511, 32/10016 datapoints
2025-03-06 17:47:49,089 - INFO - validation batch 51, loss: 1.498, 1632/10016 datapoints
2025-03-06 17:47:49,241 - INFO - validation batch 101, loss: 1.499, 3232/10016 datapoints
2025-03-06 17:47:49,392 - INFO - validation batch 151, loss: 1.529, 4832/10016 datapoints
2025-03-06 17:47:49,547 - INFO - validation batch 201, loss: 1.651, 6432/10016 datapoints
2025-03-06 17:47:49,702 - INFO - validation batch 251, loss: 1.512, 8032/10016 datapoints
2025-03-06 17:47:49,854 - INFO - validation batch 301, loss: 1.597, 9632/10016 datapoints
2025-03-06 17:47:49,890 - INFO - Epoch 27/800 done.
2025-03-06 17:47:49,891 - INFO - Final validation performance:
Loss: 1.542, top-1 acc: 0.645top-5 acc: 0.645
2025-03-06 17:47:49,891 - INFO - Beginning epoch 28/800
2025-03-06 17:47:49,897 - INFO - training batch 1, loss: 1.497, 32/60000 datapoints
2025-03-06 17:47:50,095 - INFO - training batch 51, loss: 1.536, 1632/60000 datapoints
2025-03-06 17:47:50,291 - INFO - training batch 101, loss: 1.618, 3232/60000 datapoints
2025-03-06 17:47:50,484 - INFO - training batch 151, loss: 1.632, 4832/60000 datapoints
2025-03-06 17:47:50,675 - INFO - training batch 201, loss: 1.482, 6432/60000 datapoints
2025-03-06 17:47:50,874 - INFO - training batch 251, loss: 1.591, 8032/60000 datapoints
2025-03-06 17:47:51,081 - INFO - training batch 301, loss: 1.683, 9632/60000 datapoints
2025-03-06 17:47:51,273 - INFO - training batch 351, loss: 1.493, 11232/60000 datapoints
2025-03-06 17:47:51,464 - INFO - training batch 401, loss: 1.510, 12832/60000 datapoints
2025-03-06 17:47:51,657 - INFO - training batch 451, loss: 1.673, 14432/60000 datapoints
2025-03-06 17:47:51,851 - INFO - training batch 501, loss: 1.584, 16032/60000 datapoints
2025-03-06 17:47:52,046 - INFO - training batch 551, loss: 1.595, 17632/60000 datapoints
2025-03-06 17:47:52,238 - INFO - training batch 601, loss: 1.532, 19232/60000 datapoints
2025-03-06 17:47:52,430 - INFO - training batch 651, loss: 1.483, 20832/60000 datapoints
2025-03-06 17:47:52,625 - INFO - training batch 701, loss: 1.551, 22432/60000 datapoints
2025-03-06 17:47:52,814 - INFO - training batch 751, loss: 1.552, 24032/60000 datapoints
2025-03-06 17:47:53,006 - INFO - training batch 801, loss: 1.619, 25632/60000 datapoints
2025-03-06 17:47:53,199 - INFO - training batch 851, loss: 1.602, 27232/60000 datapoints
2025-03-06 17:47:53,388 - INFO - training batch 901, loss: 1.530, 28832/60000 datapoints
2025-03-06 17:47:53,579 - INFO - training batch 951, loss: 1.480, 30432/60000 datapoints
2025-03-06 17:47:53,772 - INFO - training batch 1001, loss: 1.572, 32032/60000 datapoints
2025-03-06 17:47:53,962 - INFO - training batch 1051, loss: 1.417, 33632/60000 datapoints
2025-03-06 17:47:54,170 - INFO - training batch 1101, loss: 1.614, 35232/60000 datapoints
2025-03-06 17:47:54,378 - INFO - training batch 1151, loss: 1.553, 36832/60000 datapoints
2025-03-06 17:47:54,604 - INFO - training batch 1201, loss: 1.508, 38432/60000 datapoints
2025-03-06 17:47:54,833 - INFO - training batch 1251, loss: 1.429, 40032/60000 datapoints
2025-03-06 17:47:55,044 - INFO - training batch 1301, loss: 1.582, 41632/60000 datapoints
2025-03-06 17:47:55,238 - INFO - training batch 1351, loss: 1.493, 43232/60000 datapoints
2025-03-06 17:47:55,430 - INFO - training batch 1401, loss: 1.497, 44832/60000 datapoints
2025-03-06 17:47:55,631 - INFO - training batch 1451, loss: 1.624, 46432/60000 datapoints
2025-03-06 17:47:55,827 - INFO - training batch 1501, loss: 1.532, 48032/60000 datapoints
2025-03-06 17:47:56,047 - INFO - training batch 1551, loss: 1.547, 49632/60000 datapoints
2025-03-06 17:47:56,249 - INFO - training batch 1601, loss: 1.592, 51232/60000 datapoints
2025-03-06 17:47:56,443 - INFO - training batch 1651, loss: 1.545, 52832/60000 datapoints
2025-03-06 17:47:56,637 - INFO - training batch 1701, loss: 1.566, 54432/60000 datapoints
2025-03-06 17:47:56,828 - INFO - training batch 1751, loss: 1.517, 56032/60000 datapoints
2025-03-06 17:47:57,019 - INFO - training batch 1801, loss: 1.490, 57632/60000 datapoints
2025-03-06 17:47:57,212 - INFO - training batch 1851, loss: 1.527, 59232/60000 datapoints
2025-03-06 17:47:57,315 - INFO - validation batch 1, loss: 1.495, 32/10016 datapoints
2025-03-06 17:47:57,464 - INFO - validation batch 51, loss: 1.587, 1632/10016 datapoints
2025-03-06 17:47:57,621 - INFO - validation batch 101, loss: 1.586, 3232/10016 datapoints
2025-03-06 17:47:57,776 - INFO - validation batch 151, loss: 1.459, 4832/10016 datapoints
2025-03-06 17:47:57,933 - INFO - validation batch 201, loss: 1.634, 6432/10016 datapoints
2025-03-06 17:47:58,086 - INFO - validation batch 251, loss: 1.525, 8032/10016 datapoints
2025-03-06 17:47:58,239 - INFO - validation batch 301, loss: 1.573, 9632/10016 datapoints
2025-03-06 17:47:58,276 - INFO - Epoch 28/800 done.
2025-03-06 17:47:58,277 - INFO - Final validation performance:
Loss: 1.551, top-1 acc: 0.668top-5 acc: 0.668
2025-03-06 17:47:58,277 - INFO - Beginning epoch 29/800
2025-03-06 17:47:58,283 - INFO - training batch 1, loss: 1.648, 32/60000 datapoints
2025-03-06 17:47:58,490 - INFO - training batch 51, loss: 1.570, 1632/60000 datapoints
2025-03-06 17:47:58,690 - INFO - training batch 101, loss: 1.489, 3232/60000 datapoints
2025-03-06 17:47:58,888 - INFO - training batch 151, loss: 1.521, 4832/60000 datapoints
2025-03-06 17:47:59,082 - INFO - training batch 201, loss: 1.515, 6432/60000 datapoints
2025-03-06 17:47:59,278 - INFO - training batch 251, loss: 1.450, 8032/60000 datapoints
2025-03-06 17:47:59,471 - INFO - training batch 301, loss: 1.534, 9632/60000 datapoints
2025-03-06 17:47:59,670 - INFO - training batch 351, loss: 1.588, 11232/60000 datapoints
2025-03-06 17:47:59,866 - INFO - training batch 401, loss: 1.434, 12832/60000 datapoints
2025-03-06 17:48:00,063 - INFO - training batch 451, loss: 1.487, 14432/60000 datapoints
2025-03-06 17:48:00,264 - INFO - training batch 501, loss: 1.511, 16032/60000 datapoints
2025-03-06 17:48:00,462 - INFO - training batch 551, loss: 1.553, 17632/60000 datapoints
2025-03-06 17:48:00,660 - INFO - training batch 601, loss: 1.578, 19232/60000 datapoints
2025-03-06 17:48:00,855 - INFO - training batch 651, loss: 1.406, 20832/60000 datapoints
2025-03-06 17:48:01,070 - INFO - training batch 701, loss: 1.476, 22432/60000 datapoints
2025-03-06 17:48:01,266 - INFO - training batch 751, loss: 1.465, 24032/60000 datapoints
2025-03-06 17:48:01,461 - INFO - training batch 801, loss: 1.485, 25632/60000 datapoints
2025-03-06 17:48:01,660 - INFO - training batch 851, loss: 1.466, 27232/60000 datapoints
2025-03-06 17:48:01,853 - INFO - training batch 901, loss: 1.342, 28832/60000 datapoints
2025-03-06 17:48:02,046 - INFO - training batch 951, loss: 1.477, 30432/60000 datapoints
2025-03-06 17:48:02,243 - INFO - training batch 1001, loss: 1.507, 32032/60000 datapoints
2025-03-06 17:48:02,438 - INFO - training batch 1051, loss: 1.467, 33632/60000 datapoints
2025-03-06 17:48:02,634 - INFO - training batch 1101, loss: 1.519, 35232/60000 datapoints
2025-03-06 17:48:02,829 - INFO - training batch 1151, loss: 1.474, 36832/60000 datapoints
2025-03-06 17:48:03,032 - INFO - training batch 1201, loss: 1.496, 38432/60000 datapoints
2025-03-06 17:48:03,232 - INFO - training batch 1251, loss: 1.408, 40032/60000 datapoints
2025-03-06 17:48:03,434 - INFO - training batch 1301, loss: 1.492, 41632/60000 datapoints
2025-03-06 17:48:03,636 - INFO - training batch 1351, loss: 1.398, 43232/60000 datapoints
2025-03-06 17:48:03,839 - INFO - training batch 1401, loss: 1.632, 44832/60000 datapoints
2025-03-06 17:48:04,044 - INFO - training batch 1451, loss: 1.508, 46432/60000 datapoints
2025-03-06 17:48:04,248 - INFO - training batch 1501, loss: 1.518, 48032/60000 datapoints
2025-03-06 17:48:04,452 - INFO - training batch 1551, loss: 1.517, 49632/60000 datapoints
2025-03-06 17:48:04,652 - INFO - training batch 1601, loss: 1.662, 51232/60000 datapoints
2025-03-06 17:48:04,852 - INFO - training batch 1651, loss: 1.393, 52832/60000 datapoints
2025-03-06 17:48:05,077 - INFO - training batch 1701, loss: 1.234, 54432/60000 datapoints
2025-03-06 17:48:05,303 - INFO - training batch 1751, loss: 1.437, 56032/60000 datapoints
2025-03-06 17:48:05,525 - INFO - training batch 1801, loss: 1.583, 57632/60000 datapoints
2025-03-06 17:48:05,751 - INFO - training batch 1851, loss: 1.492, 59232/60000 datapoints
2025-03-06 17:48:05,865 - INFO - validation batch 1, loss: 1.382, 32/10016 datapoints
2025-03-06 17:48:06,035 - INFO - validation batch 51, loss: 1.597, 1632/10016 datapoints
2025-03-06 17:48:06,200 - INFO - validation batch 101, loss: 1.419, 3232/10016 datapoints
2025-03-06 17:48:06,362 - INFO - validation batch 151, loss: 1.450, 4832/10016 datapoints
2025-03-06 17:48:06,523 - INFO - validation batch 201, loss: 1.443, 6432/10016 datapoints
2025-03-06 17:48:06,684 - INFO - validation batch 251, loss: 1.555, 8032/10016 datapoints
2025-03-06 17:48:06,844 - INFO - validation batch 301, loss: 1.534, 9632/10016 datapoints
2025-03-06 17:48:06,880 - INFO - Epoch 29/800 done.
2025-03-06 17:48:06,880 - INFO - Final validation performance:
Loss: 1.483, top-1 acc: 0.694top-5 acc: 0.694
2025-03-06 17:48:06,881 - INFO - Beginning epoch 30/800
2025-03-06 17:48:06,886 - INFO - training batch 1, loss: 1.447, 32/60000 datapoints
2025-03-06 17:48:07,085 - INFO - training batch 51, loss: 1.553, 1632/60000 datapoints
2025-03-06 17:48:07,288 - INFO - training batch 101, loss: 1.475, 3232/60000 datapoints
2025-03-06 17:48:07,480 - INFO - training batch 151, loss: 1.554, 4832/60000 datapoints
2025-03-06 17:48:07,682 - INFO - training batch 201, loss: 1.516, 6432/60000 datapoints
2025-03-06 17:48:07,879 - INFO - training batch 251, loss: 1.505, 8032/60000 datapoints
2025-03-06 17:48:08,084 - INFO - training batch 301, loss: 1.391, 9632/60000 datapoints
2025-03-06 17:48:08,289 - INFO - training batch 351, loss: 1.407, 11232/60000 datapoints
2025-03-06 17:48:08,487 - INFO - training batch 401, loss: 1.462, 12832/60000 datapoints
2025-03-06 17:48:08,686 - INFO - training batch 451, loss: 1.405, 14432/60000 datapoints
2025-03-06 17:48:08,882 - INFO - training batch 501, loss: 1.574, 16032/60000 datapoints
2025-03-06 17:48:09,082 - INFO - training batch 551, loss: 1.546, 17632/60000 datapoints
2025-03-06 17:48:09,280 - INFO - training batch 601, loss: 1.527, 19232/60000 datapoints
2025-03-06 17:48:09,474 - INFO - training batch 651, loss: 1.402, 20832/60000 datapoints
2025-03-06 17:48:09,671 - INFO - training batch 701, loss: 1.586, 22432/60000 datapoints
2025-03-06 17:48:09,871 - INFO - training batch 751, loss: 1.584, 24032/60000 datapoints
2025-03-06 17:48:10,067 - INFO - training batch 801, loss: 1.514, 25632/60000 datapoints
2025-03-06 17:48:10,263 - INFO - training batch 851, loss: 1.526, 27232/60000 datapoints
2025-03-06 17:48:10,469 - INFO - training batch 901, loss: 1.549, 28832/60000 datapoints
2025-03-06 17:48:10,667 - INFO - training batch 951, loss: 1.400, 30432/60000 datapoints
2025-03-06 17:48:10,863 - INFO - training batch 1001, loss: 1.549, 32032/60000 datapoints
2025-03-06 17:48:11,065 - INFO - training batch 1051, loss: 1.441, 33632/60000 datapoints
2025-03-06 17:48:11,273 - INFO - training batch 1101, loss: 1.468, 35232/60000 datapoints
2025-03-06 17:48:11,471 - INFO - training batch 1151, loss: 1.524, 36832/60000 datapoints
2025-03-06 17:48:11,667 - INFO - training batch 1201, loss: 1.429, 38432/60000 datapoints
2025-03-06 17:48:11,864 - INFO - training batch 1251, loss: 1.650, 40032/60000 datapoints
2025-03-06 17:48:12,059 - INFO - training batch 1301, loss: 1.451, 41632/60000 datapoints
2025-03-06 17:48:12,260 - INFO - training batch 1351, loss: 1.400, 43232/60000 datapoints
2025-03-06 17:48:12,457 - INFO - training batch 1401, loss: 1.475, 44832/60000 datapoints
2025-03-06 17:48:12,653 - INFO - training batch 1451, loss: 1.472, 46432/60000 datapoints
2025-03-06 17:48:12,848 - INFO - training batch 1501, loss: 1.451, 48032/60000 datapoints
2025-03-06 17:48:13,045 - INFO - training batch 1551, loss: 1.507, 49632/60000 datapoints
2025-03-06 17:48:13,242 - INFO - training batch 1601, loss: 1.423, 51232/60000 datapoints
2025-03-06 17:48:13,441 - INFO - training batch 1651, loss: 1.424, 52832/60000 datapoints
2025-03-06 17:48:13,645 - INFO - training batch 1701, loss: 1.410, 54432/60000 datapoints
2025-03-06 17:48:13,843 - INFO - training batch 1751, loss: 1.540, 56032/60000 datapoints
2025-03-06 17:48:14,041 - INFO - training batch 1801, loss: 1.415, 57632/60000 datapoints
2025-03-06 17:48:14,241 - INFO - training batch 1851, loss: 1.506, 59232/60000 datapoints
2025-03-06 17:48:14,345 - INFO - validation batch 1, loss: 1.441, 32/10016 datapoints
2025-03-06 17:48:14,497 - INFO - validation batch 51, loss: 1.482, 1632/10016 datapoints
2025-03-06 17:48:14,652 - INFO - validation batch 101, loss: 1.550, 3232/10016 datapoints
2025-03-06 17:48:14,817 - INFO - validation batch 151, loss: 1.350, 4832/10016 datapoints
2025-03-06 17:48:14,974 - INFO - validation batch 201, loss: 1.361, 6432/10016 datapoints
2025-03-06 17:48:15,127 - INFO - validation batch 251, loss: 1.515, 8032/10016 datapoints
2025-03-06 17:48:15,282 - INFO - validation batch 301, loss: 1.515, 9632/10016 datapoints
2025-03-06 17:48:15,321 - INFO - Epoch 30/800 done.
2025-03-06 17:48:15,321 - INFO - Final validation performance:
Loss: 1.459, top-1 acc: 0.715top-5 acc: 0.715
2025-03-06 17:48:15,322 - INFO - Beginning epoch 31/800
2025-03-06 17:48:15,327 - INFO - training batch 1, loss: 1.413, 32/60000 datapoints
2025-03-06 17:48:15,523 - INFO - training batch 51, loss: 1.530, 1632/60000 datapoints
2025-03-06 17:48:15,719 - INFO - training batch 101, loss: 1.368, 3232/60000 datapoints
2025-03-06 17:48:15,915 - INFO - training batch 151, loss: 1.652, 4832/60000 datapoints
2025-03-06 17:48:16,110 - INFO - training batch 201, loss: 1.431, 6432/60000 datapoints
2025-03-06 17:48:16,313 - INFO - training batch 251, loss: 1.494, 8032/60000 datapoints
2025-03-06 17:48:16,510 - INFO - training batch 301, loss: 1.486, 9632/60000 datapoints
2025-03-06 17:48:16,709 - INFO - training batch 351, loss: 1.485, 11232/60000 datapoints
2025-03-06 17:48:16,905 - INFO - training batch 401, loss: 1.715, 12832/60000 datapoints
2025-03-06 17:48:17,101 - INFO - training batch 451, loss: 1.570, 14432/60000 datapoints
2025-03-06 17:48:17,295 - INFO - training batch 501, loss: 1.414, 16032/60000 datapoints
2025-03-06 17:48:17,492 - INFO - training batch 551, loss: 1.432, 17632/60000 datapoints
2025-03-06 17:48:17,688 - INFO - training batch 601, loss: 1.555, 19232/60000 datapoints
2025-03-06 17:48:17,883 - INFO - training batch 651, loss: 1.507, 20832/60000 datapoints
2025-03-06 17:48:18,080 - INFO - training batch 701, loss: 1.454, 22432/60000 datapoints
2025-03-06 17:48:18,275 - INFO - training batch 751, loss: 1.409, 24032/60000 datapoints
2025-03-06 17:48:18,471 - INFO - training batch 801, loss: 1.424, 25632/60000 datapoints
2025-03-06 17:48:18,667 - INFO - training batch 851, loss: 1.461, 27232/60000 datapoints
2025-03-06 17:48:18,862 - INFO - training batch 901, loss: 1.416, 28832/60000 datapoints
2025-03-06 17:48:19,056 - INFO - training batch 951, loss: 1.382, 30432/60000 datapoints
2025-03-06 17:48:19,250 - INFO - training batch 1001, loss: 1.357, 32032/60000 datapoints
2025-03-06 17:48:19,443 - INFO - training batch 1051, loss: 1.436, 33632/60000 datapoints
2025-03-06 17:48:19,640 - INFO - training batch 1101, loss: 1.543, 35232/60000 datapoints
2025-03-06 17:48:19,835 - INFO - training batch 1151, loss: 1.509, 36832/60000 datapoints
2025-03-06 17:48:20,029 - INFO - training batch 1201, loss: 1.395, 38432/60000 datapoints
2025-03-06 17:48:20,225 - INFO - training batch 1251, loss: 1.406, 40032/60000 datapoints
2025-03-06 17:48:20,422 - INFO - training batch 1301, loss: 1.428, 41632/60000 datapoints
2025-03-06 17:48:20,616 - INFO - training batch 1351, loss: 1.462, 43232/60000 datapoints
2025-03-06 17:48:20,810 - INFO - training batch 1401, loss: 1.420, 44832/60000 datapoints
2025-03-06 17:48:21,009 - INFO - training batch 1451, loss: 1.459, 46432/60000 datapoints
2025-03-06 17:48:21,223 - INFO - training batch 1501, loss: 1.380, 48032/60000 datapoints
2025-03-06 17:48:21,421 - INFO - training batch 1551, loss: 1.428, 49632/60000 datapoints
2025-03-06 17:48:21,615 - INFO - training batch 1601, loss: 1.531, 51232/60000 datapoints
2025-03-06 17:48:21,812 - INFO - training batch 1651, loss: 1.597, 52832/60000 datapoints
2025-03-06 17:48:22,005 - INFO - training batch 1701, loss: 1.497, 54432/60000 datapoints
2025-03-06 17:48:22,202 - INFO - training batch 1751, loss: 1.475, 56032/60000 datapoints
2025-03-06 17:48:22,406 - INFO - training batch 1801, loss: 1.424, 57632/60000 datapoints
2025-03-06 17:48:22,602 - INFO - training batch 1851, loss: 1.457, 59232/60000 datapoints
2025-03-06 17:48:22,702 - INFO - validation batch 1, loss: 1.289, 32/10016 datapoints
2025-03-06 17:48:22,854 - INFO - validation batch 51, loss: 1.482, 1632/10016 datapoints
2025-03-06 17:48:23,009 - INFO - validation batch 101, loss: 1.454, 3232/10016 datapoints
2025-03-06 17:48:23,160 - INFO - validation batch 151, loss: 1.479, 4832/10016 datapoints
2025-03-06 17:48:23,313 - INFO - validation batch 201, loss: 1.282, 6432/10016 datapoints
2025-03-06 17:48:23,466 - INFO - validation batch 251, loss: 1.448, 8032/10016 datapoints
2025-03-06 17:48:23,620 - INFO - validation batch 301, loss: 1.519, 9632/10016 datapoints
2025-03-06 17:48:23,656 - INFO - Epoch 31/800 done.
2025-03-06 17:48:23,656 - INFO - Final validation performance:
Loss: 1.422, top-1 acc: 0.733top-5 acc: 0.733
2025-03-06 17:48:23,657 - INFO - Beginning epoch 32/800
2025-03-06 17:48:23,664 - INFO - training batch 1, loss: 1.546, 32/60000 datapoints
2025-03-06 17:48:23,864 - INFO - training batch 51, loss: 1.320, 1632/60000 datapoints
2025-03-06 17:48:24,058 - INFO - training batch 101, loss: 1.543, 3232/60000 datapoints
2025-03-06 17:48:24,255 - INFO - training batch 151, loss: 1.504, 4832/60000 datapoints
2025-03-06 17:48:24,452 - INFO - training batch 201, loss: 1.545, 6432/60000 datapoints
2025-03-06 17:48:24,655 - INFO - training batch 251, loss: 1.362, 8032/60000 datapoints
2025-03-06 17:48:24,860 - INFO - training batch 301, loss: 1.311, 9632/60000 datapoints
2025-03-06 17:48:25,055 - INFO - training batch 351, loss: 1.561, 11232/60000 datapoints
2025-03-06 17:48:25,256 - INFO - training batch 401, loss: 1.312, 12832/60000 datapoints
2025-03-06 17:48:25,457 - INFO - training batch 451, loss: 1.566, 14432/60000 datapoints
2025-03-06 17:48:25,655 - INFO - training batch 501, loss: 1.318, 16032/60000 datapoints
2025-03-06 17:48:25,850 - INFO - training batch 551, loss: 1.373, 17632/60000 datapoints
2025-03-06 17:48:26,046 - INFO - training batch 601, loss: 1.402, 19232/60000 datapoints
2025-03-06 17:48:26,245 - INFO - training batch 651, loss: 1.436, 20832/60000 datapoints
2025-03-06 17:48:26,442 - INFO - training batch 701, loss: 1.450, 22432/60000 datapoints
2025-03-06 17:48:26,635 - INFO - training batch 751, loss: 1.426, 24032/60000 datapoints
2025-03-06 17:48:26,831 - INFO - training batch 801, loss: 1.448, 25632/60000 datapoints
2025-03-06 17:48:27,024 - INFO - training batch 851, loss: 1.316, 27232/60000 datapoints
2025-03-06 17:48:27,216 - INFO - training batch 901, loss: 1.338, 28832/60000 datapoints
2025-03-06 17:48:27,409 - INFO - training batch 951, loss: 1.424, 30432/60000 datapoints
2025-03-06 17:48:27,608 - INFO - training batch 1001, loss: 1.513, 32032/60000 datapoints
2025-03-06 17:48:27,802 - INFO - training batch 1051, loss: 1.457, 33632/60000 datapoints
2025-03-06 17:48:27,998 - INFO - training batch 1101, loss: 1.323, 35232/60000 datapoints
2025-03-06 17:48:28,193 - INFO - training batch 1151, loss: 1.576, 36832/60000 datapoints
2025-03-06 17:48:28,385 - INFO - training batch 1201, loss: 1.526, 38432/60000 datapoints
2025-03-06 17:48:28,579 - INFO - training batch 1251, loss: 1.335, 40032/60000 datapoints
2025-03-06 17:48:28,776 - INFO - training batch 1301, loss: 1.429, 41632/60000 datapoints
2025-03-06 17:48:28,970 - INFO - training batch 1351, loss: 1.427, 43232/60000 datapoints
2025-03-06 17:48:29,163 - INFO - training batch 1401, loss: 1.573, 44832/60000 datapoints
2025-03-06 17:48:29,356 - INFO - training batch 1451, loss: 1.280, 46432/60000 datapoints
2025-03-06 17:48:29,550 - INFO - training batch 1501, loss: 1.430, 48032/60000 datapoints
2025-03-06 17:48:29,745 - INFO - training batch 1551, loss: 1.199, 49632/60000 datapoints
2025-03-06 17:48:29,939 - INFO - training batch 1601, loss: 1.362, 51232/60000 datapoints
2025-03-06 17:48:30,133 - INFO - training batch 1651, loss: 1.407, 52832/60000 datapoints
2025-03-06 17:48:30,330 - INFO - training batch 1701, loss: 1.453, 54432/60000 datapoints
2025-03-06 17:48:30,525 - INFO - training batch 1751, loss: 1.647, 56032/60000 datapoints
2025-03-06 17:48:30,720 - INFO - training batch 1801, loss: 1.287, 57632/60000 datapoints
2025-03-06 17:48:30,913 - INFO - training batch 1851, loss: 1.490, 59232/60000 datapoints
2025-03-06 17:48:31,014 - INFO - validation batch 1, loss: 1.440, 32/10016 datapoints
2025-03-06 17:48:31,166 - INFO - validation batch 51, loss: 1.333, 1632/10016 datapoints
2025-03-06 17:48:31,341 - INFO - validation batch 101, loss: 1.369, 3232/10016 datapoints
2025-03-06 17:48:31,495 - INFO - validation batch 151, loss: 1.340, 4832/10016 datapoints
2025-03-06 17:48:31,650 - INFO - validation batch 201, loss: 1.429, 6432/10016 datapoints
2025-03-06 17:48:31,802 - INFO - validation batch 251, loss: 1.414, 8032/10016 datapoints
2025-03-06 17:48:31,955 - INFO - validation batch 301, loss: 1.299, 9632/10016 datapoints
2025-03-06 17:48:31,992 - INFO - Epoch 32/800 done.
2025-03-06 17:48:31,992 - INFO - Final validation performance:
Loss: 1.375, top-1 acc: 0.744top-5 acc: 0.744
2025-03-06 17:48:31,992 - INFO - Beginning epoch 33/800
2025-03-06 17:48:31,999 - INFO - training batch 1, loss: 1.263, 32/60000 datapoints
2025-03-06 17:48:32,193 - INFO - training batch 51, loss: 1.490, 1632/60000 datapoints
2025-03-06 17:48:32,382 - INFO - training batch 101, loss: 1.442, 3232/60000 datapoints
2025-03-06 17:48:32,572 - INFO - training batch 151, loss: 1.374, 4832/60000 datapoints
2025-03-06 17:48:32,765 - INFO - training batch 201, loss: 1.431, 6432/60000 datapoints
2025-03-06 17:48:32,959 - INFO - training batch 251, loss: 1.331, 8032/60000 datapoints
2025-03-06 17:48:33,151 - INFO - training batch 301, loss: 1.410, 9632/60000 datapoints
2025-03-06 17:48:33,343 - INFO - training batch 351, loss: 1.411, 11232/60000 datapoints
2025-03-06 17:48:33,533 - INFO - training batch 401, loss: 1.401, 12832/60000 datapoints
2025-03-06 17:48:33,726 - INFO - training batch 451, loss: 1.504, 14432/60000 datapoints
2025-03-06 17:48:33,917 - INFO - training batch 501, loss: 1.464, 16032/60000 datapoints
2025-03-06 17:48:34,109 - INFO - training batch 551, loss: 1.368, 17632/60000 datapoints
2025-03-06 17:48:34,302 - INFO - training batch 601, loss: 1.362, 19232/60000 datapoints
2025-03-06 17:48:34,492 - INFO - training batch 651, loss: 1.468, 20832/60000 datapoints
2025-03-06 17:48:34,685 - INFO - training batch 701, loss: 1.493, 22432/60000 datapoints
2025-03-06 17:48:34,881 - INFO - training batch 751, loss: 1.416, 24032/60000 datapoints
2025-03-06 17:48:35,075 - INFO - training batch 801, loss: 1.445, 25632/60000 datapoints
2025-03-06 17:48:35,268 - INFO - training batch 851, loss: 1.374, 27232/60000 datapoints
2025-03-06 17:48:35,464 - INFO - training batch 901, loss: 1.663, 28832/60000 datapoints
2025-03-06 17:48:35,658 - INFO - training batch 951, loss: 1.437, 30432/60000 datapoints
2025-03-06 17:48:35,848 - INFO - training batch 1001, loss: 1.379, 32032/60000 datapoints
2025-03-06 17:48:36,040 - INFO - training batch 1051, loss: 1.318, 33632/60000 datapoints
2025-03-06 17:48:36,237 - INFO - training batch 1101, loss: 1.305, 35232/60000 datapoints
2025-03-06 17:48:36,430 - INFO - training batch 1151, loss: 1.331, 36832/60000 datapoints
2025-03-06 17:48:36,622 - INFO - training batch 1201, loss: 1.359, 38432/60000 datapoints
2025-03-06 17:48:36,813 - INFO - training batch 1251, loss: 1.354, 40032/60000 datapoints
2025-03-06 17:48:37,005 - INFO - training batch 1301, loss: 1.378, 41632/60000 datapoints
2025-03-06 17:48:37,197 - INFO - training batch 1351, loss: 1.454, 43232/60000 datapoints
2025-03-06 17:48:37,388 - INFO - training batch 1401, loss: 1.399, 44832/60000 datapoints
2025-03-06 17:48:37,578 - INFO - training batch 1451, loss: 1.442, 46432/60000 datapoints
2025-03-06 17:48:37,790 - INFO - training batch 1501, loss: 1.448, 48032/60000 datapoints
2025-03-06 17:48:37,982 - INFO - training batch 1551, loss: 1.255, 49632/60000 datapoints
2025-03-06 17:48:38,182 - INFO - training batch 1601, loss: 1.494, 51232/60000 datapoints
2025-03-06 17:48:38,379 - INFO - training batch 1651, loss: 1.485, 52832/60000 datapoints
2025-03-06 17:48:38,575 - INFO - training batch 1701, loss: 1.329, 54432/60000 datapoints
2025-03-06 17:48:38,769 - INFO - training batch 1751, loss: 1.237, 56032/60000 datapoints
2025-03-06 17:48:38,964 - INFO - training batch 1801, loss: 1.430, 57632/60000 datapoints
2025-03-06 17:48:39,158 - INFO - training batch 1851, loss: 1.485, 59232/60000 datapoints
2025-03-06 17:48:39,260 - INFO - validation batch 1, loss: 1.394, 32/10016 datapoints
2025-03-06 17:48:39,411 - INFO - validation batch 51, loss: 1.250, 1632/10016 datapoints
2025-03-06 17:48:39,564 - INFO - validation batch 101, loss: 1.366, 3232/10016 datapoints
2025-03-06 17:48:39,718 - INFO - validation batch 151, loss: 1.317, 4832/10016 datapoints
2025-03-06 17:48:39,870 - INFO - validation batch 201, loss: 1.530, 6432/10016 datapoints
2025-03-06 17:48:40,024 - INFO - validation batch 251, loss: 1.556, 8032/10016 datapoints
2025-03-06 17:48:40,180 - INFO - validation batch 301, loss: 1.335, 9632/10016 datapoints
2025-03-06 17:48:40,218 - INFO - Epoch 33/800 done.
2025-03-06 17:48:40,218 - INFO - Final validation performance:
Loss: 1.393, top-1 acc: 0.755top-5 acc: 0.755
2025-03-06 17:48:40,218 - INFO - Beginning epoch 34/800
2025-03-06 17:48:40,226 - INFO - training batch 1, loss: 1.412, 32/60000 datapoints
2025-03-06 17:48:40,419 - INFO - training batch 51, loss: 1.439, 1632/60000 datapoints
2025-03-06 17:48:40,619 - INFO - training batch 101, loss: 1.504, 3232/60000 datapoints
2025-03-06 17:48:40,815 - INFO - training batch 151, loss: 1.355, 4832/60000 datapoints
2025-03-06 17:48:41,012 - INFO - training batch 201, loss: 1.418, 6432/60000 datapoints
2025-03-06 17:48:41,205 - INFO - training batch 251, loss: 1.392, 8032/60000 datapoints
2025-03-06 17:48:41,423 - INFO - training batch 301, loss: 1.441, 9632/60000 datapoints
2025-03-06 17:48:41,619 - INFO - training batch 351, loss: 1.479, 11232/60000 datapoints
2025-03-06 17:48:41,812 - INFO - training batch 401, loss: 1.370, 12832/60000 datapoints
2025-03-06 17:48:42,004 - INFO - training batch 451, loss: 1.320, 14432/60000 datapoints
2025-03-06 17:48:42,197 - INFO - training batch 501, loss: 1.454, 16032/60000 datapoints
2025-03-06 17:48:42,394 - INFO - training batch 551, loss: 1.265, 17632/60000 datapoints
2025-03-06 17:48:42,586 - INFO - training batch 601, loss: 1.313, 19232/60000 datapoints
2025-03-06 17:48:42,780 - INFO - training batch 651, loss: 1.276, 20832/60000 datapoints
2025-03-06 17:48:42,973 - INFO - training batch 701, loss: 1.383, 22432/60000 datapoints
2025-03-06 17:48:43,166 - INFO - training batch 751, loss: 1.357, 24032/60000 datapoints
2025-03-06 17:48:43,359 - INFO - training batch 801, loss: 1.400, 25632/60000 datapoints
2025-03-06 17:48:43,551 - INFO - training batch 851, loss: 1.376, 27232/60000 datapoints
2025-03-06 17:48:43,756 - INFO - training batch 901, loss: 1.292, 28832/60000 datapoints
2025-03-06 17:48:43,949 - INFO - training batch 951, loss: 1.466, 30432/60000 datapoints
2025-03-06 17:48:44,144 - INFO - training batch 1001, loss: 1.403, 32032/60000 datapoints
2025-03-06 17:48:44,340 - INFO - training batch 1051, loss: 1.266, 33632/60000 datapoints
2025-03-06 17:48:44,534 - INFO - training batch 1101, loss: 1.302, 35232/60000 datapoints
2025-03-06 17:48:44,730 - INFO - training batch 1151, loss: 1.429, 36832/60000 datapoints
2025-03-06 17:48:44,928 - INFO - training batch 1201, loss: 1.330, 38432/60000 datapoints
2025-03-06 17:48:45,122 - INFO - training batch 1251, loss: 1.186, 40032/60000 datapoints
2025-03-06 17:48:45,315 - INFO - training batch 1301, loss: 1.322, 41632/60000 datapoints
2025-03-06 17:48:45,514 - INFO - training batch 1351, loss: 1.183, 43232/60000 datapoints
2025-03-06 17:48:45,709 - INFO - training batch 1401, loss: 1.321, 44832/60000 datapoints
2025-03-06 17:48:45,903 - INFO - training batch 1451, loss: 1.290, 46432/60000 datapoints
2025-03-06 17:48:46,099 - INFO - training batch 1501, loss: 1.270, 48032/60000 datapoints
2025-03-06 17:48:46,298 - INFO - training batch 1551, loss: 1.385, 49632/60000 datapoints
2025-03-06 17:48:46,491 - INFO - training batch 1601, loss: 1.377, 51232/60000 datapoints
2025-03-06 17:48:46,689 - INFO - training batch 1651, loss: 1.361, 52832/60000 datapoints
2025-03-06 17:48:46,882 - INFO - training batch 1701, loss: 1.311, 54432/60000 datapoints
2025-03-06 17:48:47,076 - INFO - training batch 1751, loss: 1.277, 56032/60000 datapoints
2025-03-06 17:48:47,269 - INFO - training batch 1801, loss: 1.340, 57632/60000 datapoints
2025-03-06 17:48:47,462 - INFO - training batch 1851, loss: 1.220, 59232/60000 datapoints
2025-03-06 17:48:47,562 - INFO - validation batch 1, loss: 1.201, 32/10016 datapoints
2025-03-06 17:48:47,716 - INFO - validation batch 51, loss: 1.261, 1632/10016 datapoints
2025-03-06 17:48:47,867 - INFO - validation batch 101, loss: 1.268, 3232/10016 datapoints
2025-03-06 17:48:48,019 - INFO - validation batch 151, loss: 1.243, 4832/10016 datapoints
2025-03-06 17:48:48,172 - INFO - validation batch 201, loss: 1.205, 6432/10016 datapoints
2025-03-06 17:48:48,325 - INFO - validation batch 251, loss: 1.228, 8032/10016 datapoints
2025-03-06 17:48:48,477 - INFO - validation batch 301, loss: 1.391, 9632/10016 datapoints
2025-03-06 17:48:48,516 - INFO - Epoch 34/800 done.
2025-03-06 17:48:48,517 - INFO - Final validation performance:
Loss: 1.257, top-1 acc: 0.766top-5 acc: 0.766
2025-03-06 17:48:48,517 - INFO - Beginning epoch 35/800
2025-03-06 17:48:48,522 - INFO - training batch 1, loss: 1.363, 32/60000 datapoints
2025-03-06 17:48:48,718 - INFO - training batch 51, loss: 1.312, 1632/60000 datapoints
2025-03-06 17:48:48,911 - INFO - training batch 101, loss: 1.269, 3232/60000 datapoints
2025-03-06 17:48:49,105 - INFO - training batch 151, loss: 1.351, 4832/60000 datapoints
2025-03-06 17:48:49,303 - INFO - training batch 201, loss: 1.312, 6432/60000 datapoints
2025-03-06 17:48:49,495 - INFO - training batch 251, loss: 1.338, 8032/60000 datapoints
2025-03-06 17:48:49,692 - INFO - training batch 301, loss: 1.330, 9632/60000 datapoints
2025-03-06 17:48:49,885 - INFO - training batch 351, loss: 1.320, 11232/60000 datapoints
2025-03-06 17:48:50,079 - INFO - training batch 401, loss: 1.293, 12832/60000 datapoints
2025-03-06 17:48:50,275 - INFO - training batch 451, loss: 1.322, 14432/60000 datapoints
2025-03-06 17:48:50,469 - INFO - training batch 501, loss: 1.205, 16032/60000 datapoints
2025-03-06 17:48:50,664 - INFO - training batch 551, loss: 1.415, 17632/60000 datapoints
2025-03-06 17:48:50,858 - INFO - training batch 601, loss: 1.299, 19232/60000 datapoints
2025-03-06 17:48:51,050 - INFO - training batch 651, loss: 1.315, 20832/60000 datapoints
2025-03-06 17:48:51,244 - INFO - training batch 701, loss: 1.356, 22432/60000 datapoints
2025-03-06 17:48:51,449 - INFO - training batch 751, loss: 1.312, 24032/60000 datapoints
2025-03-06 17:48:51,652 - INFO - training batch 801, loss: 1.422, 25632/60000 datapoints
2025-03-06 17:48:51,849 - INFO - training batch 851, loss: 1.248, 27232/60000 datapoints
2025-03-06 17:48:52,041 - INFO - training batch 901, loss: 1.389, 28832/60000 datapoints
2025-03-06 17:48:52,233 - INFO - training batch 951, loss: 1.453, 30432/60000 datapoints
2025-03-06 17:48:52,429 - INFO - training batch 1001, loss: 1.348, 32032/60000 datapoints
2025-03-06 17:48:52,624 - INFO - training batch 1051, loss: 1.324, 33632/60000 datapoints
2025-03-06 17:48:52,819 - INFO - training batch 1101, loss: 1.406, 35232/60000 datapoints
2025-03-06 17:48:53,012 - INFO - training batch 1151, loss: 1.416, 36832/60000 datapoints
2025-03-06 17:48:53,203 - INFO - training batch 1201, loss: 1.391, 38432/60000 datapoints
2025-03-06 17:48:53,398 - INFO - training batch 1251, loss: 1.323, 40032/60000 datapoints
2025-03-06 17:48:53,594 - INFO - training batch 1301, loss: 1.302, 41632/60000 datapoints
2025-03-06 17:48:53,788 - INFO - training batch 1351, loss: 1.329, 43232/60000 datapoints
2025-03-06 17:48:53,980 - INFO - training batch 1401, loss: 1.237, 44832/60000 datapoints
2025-03-06 17:48:54,171 - INFO - training batch 1451, loss: 1.193, 46432/60000 datapoints
2025-03-06 17:48:54,368 - INFO - training batch 1501, loss: 1.225, 48032/60000 datapoints
2025-03-06 17:48:54,561 - INFO - training batch 1551, loss: 1.351, 49632/60000 datapoints
2025-03-06 17:48:54,757 - INFO - training batch 1601, loss: 1.312, 51232/60000 datapoints
2025-03-06 17:48:54,957 - INFO - training batch 1651, loss: 1.356, 52832/60000 datapoints
2025-03-06 17:48:55,151 - INFO - training batch 1701, loss: 1.340, 54432/60000 datapoints
2025-03-06 17:48:55,346 - INFO - training batch 1751, loss: 1.356, 56032/60000 datapoints
2025-03-06 17:48:55,546 - INFO - training batch 1801, loss: 1.260, 57632/60000 datapoints
2025-03-06 17:48:55,744 - INFO - training batch 1851, loss: 1.408, 59232/60000 datapoints
2025-03-06 17:48:55,849 - INFO - validation batch 1, loss: 1.306, 32/10016 datapoints
2025-03-06 17:48:56,000 - INFO - validation batch 51, loss: 1.276, 1632/10016 datapoints
2025-03-06 17:48:56,152 - INFO - validation batch 101, loss: 1.193, 3232/10016 datapoints
2025-03-06 17:48:56,307 - INFO - validation batch 151, loss: 1.162, 4832/10016 datapoints
2025-03-06 17:48:56,459 - INFO - validation batch 201, loss: 1.294, 6432/10016 datapoints
2025-03-06 17:48:56,612 - INFO - validation batch 251, loss: 1.326, 8032/10016 datapoints
2025-03-06 17:48:56,764 - INFO - validation batch 301, loss: 1.289, 9632/10016 datapoints
2025-03-06 17:48:56,804 - INFO - Epoch 35/800 done.
2025-03-06 17:48:56,804 - INFO - Final validation performance:
Loss: 1.264, top-1 acc: 0.772top-5 acc: 0.772
2025-03-06 17:48:56,804 - INFO - Beginning epoch 36/800
2025-03-06 17:48:56,810 - INFO - training batch 1, loss: 1.344, 32/60000 datapoints
2025-03-06 17:48:57,000 - INFO - training batch 51, loss: 1.373, 1632/60000 datapoints
2025-03-06 17:48:57,189 - INFO - training batch 101, loss: 1.269, 3232/60000 datapoints
2025-03-06 17:48:57,382 - INFO - training batch 151, loss: 1.266, 4832/60000 datapoints
2025-03-06 17:48:57,571 - INFO - training batch 201, loss: 1.351, 6432/60000 datapoints
2025-03-06 17:48:57,772 - INFO - training batch 251, loss: 1.295, 8032/60000 datapoints
2025-03-06 17:48:57,967 - INFO - training batch 301, loss: 1.382, 9632/60000 datapoints
2025-03-06 17:48:58,160 - INFO - training batch 351, loss: 1.343, 11232/60000 datapoints
2025-03-06 17:48:58,361 - INFO - training batch 401, loss: 1.274, 12832/60000 datapoints
2025-03-06 17:48:58,558 - INFO - training batch 451, loss: 1.251, 14432/60000 datapoints
2025-03-06 17:48:58,761 - INFO - training batch 501, loss: 1.296, 16032/60000 datapoints
2025-03-06 17:48:58,956 - INFO - training batch 551, loss: 1.329, 17632/60000 datapoints
2025-03-06 17:48:59,148 - INFO - training batch 601, loss: 1.245, 19232/60000 datapoints
2025-03-06 17:48:59,345 - INFO - training batch 651, loss: 1.290, 20832/60000 datapoints
2025-03-06 17:48:59,536 - INFO - training batch 701, loss: 1.135, 22432/60000 datapoints
2025-03-06 17:48:59,730 - INFO - training batch 751, loss: 1.307, 24032/60000 datapoints
2025-03-06 17:48:59,923 - INFO - training batch 801, loss: 1.254, 25632/60000 datapoints
2025-03-06 17:49:00,118 - INFO - training batch 851, loss: 1.343, 27232/60000 datapoints
2025-03-06 17:49:00,315 - INFO - training batch 901, loss: 1.305, 28832/60000 datapoints
2025-03-06 17:49:00,509 - INFO - training batch 951, loss: 1.366, 30432/60000 datapoints
2025-03-06 17:49:00,707 - INFO - training batch 1001, loss: 1.604, 32032/60000 datapoints
2025-03-06 17:49:00,902 - INFO - training batch 1051, loss: 1.239, 33632/60000 datapoints
2025-03-06 17:49:01,096 - INFO - training batch 1101, loss: 1.369, 35232/60000 datapoints
2025-03-06 17:49:01,292 - INFO - training batch 1151, loss: 1.364, 36832/60000 datapoints
2025-03-06 17:49:01,488 - INFO - training batch 1201, loss: 1.284, 38432/60000 datapoints
2025-03-06 17:49:01,704 - INFO - training batch 1251, loss: 1.165, 40032/60000 datapoints
2025-03-06 17:49:01,901 - INFO - training batch 1301, loss: 1.215, 41632/60000 datapoints
2025-03-06 17:49:02,096 - INFO - training batch 1351, loss: 1.263, 43232/60000 datapoints
2025-03-06 17:49:02,293 - INFO - training batch 1401, loss: 1.308, 44832/60000 datapoints
2025-03-06 17:49:02,488 - INFO - training batch 1451, loss: 1.075, 46432/60000 datapoints
2025-03-06 17:49:02,683 - INFO - training batch 1501, loss: 1.394, 48032/60000 datapoints
2025-03-06 17:49:02,878 - INFO - training batch 1551, loss: 1.135, 49632/60000 datapoints
2025-03-06 17:49:03,070 - INFO - training batch 1601, loss: 1.396, 51232/60000 datapoints
2025-03-06 17:49:03,263 - INFO - training batch 1651, loss: 1.302, 52832/60000 datapoints
2025-03-06 17:49:03,461 - INFO - training batch 1701, loss: 1.507, 54432/60000 datapoints
2025-03-06 17:49:03,656 - INFO - training batch 1751, loss: 1.183, 56032/60000 datapoints
2025-03-06 17:49:03,853 - INFO - training batch 1801, loss: 1.206, 57632/60000 datapoints
2025-03-06 17:49:04,050 - INFO - training batch 1851, loss: 1.275, 59232/60000 datapoints
2025-03-06 17:49:04,150 - INFO - validation batch 1, loss: 1.226, 32/10016 datapoints
2025-03-06 17:49:04,305 - INFO - validation batch 51, loss: 1.265, 1632/10016 datapoints
2025-03-06 17:49:04,459 - INFO - validation batch 101, loss: 1.351, 3232/10016 datapoints
2025-03-06 17:49:04,614 - INFO - validation batch 151, loss: 1.286, 4832/10016 datapoints
2025-03-06 17:49:04,767 - INFO - validation batch 201, loss: 1.172, 6432/10016 datapoints
2025-03-06 17:49:04,924 - INFO - validation batch 251, loss: 1.269, 8032/10016 datapoints
2025-03-06 17:49:05,079 - INFO - validation batch 301, loss: 1.256, 9632/10016 datapoints
2025-03-06 17:49:05,118 - INFO - Epoch 36/800 done.
2025-03-06 17:49:05,118 - INFO - Final validation performance:
Loss: 1.261, top-1 acc: 0.779top-5 acc: 0.779
2025-03-06 17:49:05,118 - INFO - Beginning epoch 37/800
2025-03-06 17:49:05,124 - INFO - training batch 1, loss: 1.334, 32/60000 datapoints
2025-03-06 17:49:05,320 - INFO - training batch 51, loss: 1.091, 1632/60000 datapoints
2025-03-06 17:49:05,521 - INFO - training batch 101, loss: 1.336, 3232/60000 datapoints
2025-03-06 17:49:05,719 - INFO - training batch 151, loss: 1.277, 4832/60000 datapoints
2025-03-06 17:49:05,914 - INFO - training batch 201, loss: 1.359, 6432/60000 datapoints
2025-03-06 17:49:06,106 - INFO - training batch 251, loss: 1.460, 8032/60000 datapoints
2025-03-06 17:49:06,301 - INFO - training batch 301, loss: 1.309, 9632/60000 datapoints
2025-03-06 17:49:06,501 - INFO - training batch 351, loss: 1.236, 11232/60000 datapoints
2025-03-06 17:49:06,697 - INFO - training batch 401, loss: 1.261, 12832/60000 datapoints
2025-03-06 17:49:06,893 - INFO - training batch 451, loss: 1.182, 14432/60000 datapoints
2025-03-06 17:49:07,087 - INFO - training batch 501, loss: 1.159, 16032/60000 datapoints
2025-03-06 17:49:07,278 - INFO - training batch 551, loss: 1.178, 17632/60000 datapoints
2025-03-06 17:49:07,471 - INFO - training batch 601, loss: 1.314, 19232/60000 datapoints
2025-03-06 17:49:07,667 - INFO - training batch 651, loss: 1.418, 20832/60000 datapoints
2025-03-06 17:49:07,858 - INFO - training batch 701, loss: 1.212, 22432/60000 datapoints
2025-03-06 17:49:08,058 - INFO - training batch 751, loss: 1.363, 24032/60000 datapoints
2025-03-06 17:49:08,251 - INFO - training batch 801, loss: 1.385, 25632/60000 datapoints
2025-03-06 17:49:08,447 - INFO - training batch 851, loss: 1.356, 27232/60000 datapoints
2025-03-06 17:49:08,642 - INFO - training batch 901, loss: 1.297, 28832/60000 datapoints
2025-03-06 17:49:08,838 - INFO - training batch 951, loss: 1.224, 30432/60000 datapoints
2025-03-06 17:49:09,038 - INFO - training batch 1001, loss: 1.390, 32032/60000 datapoints
2025-03-06 17:49:09,232 - INFO - training batch 1051, loss: 1.318, 33632/60000 datapoints
2025-03-06 17:49:09,427 - INFO - training batch 1101, loss: 1.269, 35232/60000 datapoints
2025-03-06 17:49:09,624 - INFO - training batch 1151, loss: 1.257, 36832/60000 datapoints
2025-03-06 17:49:09,816 - INFO - training batch 1201, loss: 1.434, 38432/60000 datapoints
2025-03-06 17:49:10,011 - INFO - training batch 1251, loss: 1.256, 40032/60000 datapoints
2025-03-06 17:49:10,204 - INFO - training batch 1301, loss: 1.211, 41632/60000 datapoints
2025-03-06 17:49:10,402 - INFO - training batch 1351, loss: 1.267, 43232/60000 datapoints
2025-03-06 17:49:10,602 - INFO - training batch 1401, loss: 1.343, 44832/60000 datapoints
2025-03-06 17:49:10,796 - INFO - training batch 1451, loss: 1.347, 46432/60000 datapoints
2025-03-06 17:49:10,998 - INFO - training batch 1501, loss: 1.392, 48032/60000 datapoints
2025-03-06 17:49:11,199 - INFO - training batch 1551, loss: 1.417, 49632/60000 datapoints
2025-03-06 17:49:11,392 - INFO - training batch 1601, loss: 1.430, 51232/60000 datapoints
2025-03-06 17:49:11,616 - INFO - training batch 1651, loss: 1.302, 52832/60000 datapoints
2025-03-06 17:49:11,835 - INFO - training batch 1701, loss: 1.151, 54432/60000 datapoints
2025-03-06 17:49:12,031 - INFO - training batch 1751, loss: 1.235, 56032/60000 datapoints
2025-03-06 17:49:12,222 - INFO - training batch 1801, loss: 1.200, 57632/60000 datapoints
2025-03-06 17:49:12,417 - INFO - training batch 1851, loss: 1.204, 59232/60000 datapoints
2025-03-06 17:49:12,522 - INFO - validation batch 1, loss: 1.216, 32/10016 datapoints
2025-03-06 17:49:12,676 - INFO - validation batch 51, loss: 1.257, 1632/10016 datapoints
2025-03-06 17:49:12,829 - INFO - validation batch 101, loss: 1.128, 3232/10016 datapoints
2025-03-06 17:49:12,984 - INFO - validation batch 151, loss: 1.175, 4832/10016 datapoints
2025-03-06 17:49:13,136 - INFO - validation batch 201, loss: 1.149, 6432/10016 datapoints
2025-03-06 17:49:13,288 - INFO - validation batch 251, loss: 1.264, 8032/10016 datapoints
2025-03-06 17:49:13,441 - INFO - validation batch 301, loss: 1.244, 9632/10016 datapoints
2025-03-06 17:49:13,480 - INFO - Epoch 37/800 done.
2025-03-06 17:49:13,480 - INFO - Final validation performance:
Loss: 1.205, top-1 acc: 0.783top-5 acc: 0.783
2025-03-06 17:49:13,480 - INFO - Beginning epoch 38/800
2025-03-06 17:49:13,486 - INFO - training batch 1, loss: 1.235, 32/60000 datapoints
2025-03-06 17:49:13,684 - INFO - training batch 51, loss: 1.337, 1632/60000 datapoints
2025-03-06 17:49:13,878 - INFO - training batch 101, loss: 1.298, 3232/60000 datapoints
2025-03-06 17:49:14,073 - INFO - training batch 151, loss: 1.190, 4832/60000 datapoints
2025-03-06 17:49:14,268 - INFO - training batch 201, loss: 1.294, 6432/60000 datapoints
2025-03-06 17:49:14,465 - INFO - training batch 251, loss: 1.175, 8032/60000 datapoints
2025-03-06 17:49:14,663 - INFO - training batch 301, loss: 1.143, 9632/60000 datapoints
2025-03-06 17:49:14,862 - INFO - training batch 351, loss: 1.216, 11232/60000 datapoints
2025-03-06 17:49:15,058 - INFO - training batch 401, loss: 1.205, 12832/60000 datapoints
2025-03-06 17:49:15,256 - INFO - training batch 451, loss: 1.277, 14432/60000 datapoints
2025-03-06 17:49:15,455 - INFO - training batch 501, loss: 1.005, 16032/60000 datapoints
2025-03-06 17:49:15,657 - INFO - training batch 551, loss: 1.488, 17632/60000 datapoints
2025-03-06 17:49:15,859 - INFO - training batch 601, loss: 1.245, 19232/60000 datapoints
2025-03-06 17:49:16,054 - INFO - training batch 651, loss: 1.249, 20832/60000 datapoints
2025-03-06 17:49:16,249 - INFO - training batch 701, loss: 1.236, 22432/60000 datapoints
2025-03-06 17:49:16,445 - INFO - training batch 751, loss: 1.307, 24032/60000 datapoints
2025-03-06 17:49:16,647 - INFO - training batch 801, loss: 1.414, 25632/60000 datapoints
2025-03-06 17:49:16,840 - INFO - training batch 851, loss: 1.250, 27232/60000 datapoints
2025-03-06 17:49:17,034 - INFO - training batch 901, loss: 1.169, 28832/60000 datapoints
2025-03-06 17:49:17,228 - INFO - training batch 951, loss: 1.195, 30432/60000 datapoints
2025-03-06 17:49:17,419 - INFO - training batch 1001, loss: 1.167, 32032/60000 datapoints
2025-03-06 17:49:17,616 - INFO - training batch 1051, loss: 1.366, 33632/60000 datapoints
2025-03-06 17:49:17,807 - INFO - training batch 1101, loss: 1.348, 35232/60000 datapoints
2025-03-06 17:49:17,999 - INFO - training batch 1151, loss: 1.252, 36832/60000 datapoints
2025-03-06 17:49:18,193 - INFO - training batch 1201, loss: 1.289, 38432/60000 datapoints
2025-03-06 17:49:18,390 - INFO - training batch 1251, loss: 1.266, 40032/60000 datapoints
2025-03-06 17:49:18,585 - INFO - training batch 1301, loss: 1.134, 41632/60000 datapoints
2025-03-06 17:49:18,784 - INFO - training batch 1351, loss: 1.407, 43232/60000 datapoints
2025-03-06 17:49:18,979 - INFO - training batch 1401, loss: 1.306, 44832/60000 datapoints
2025-03-06 17:49:19,174 - INFO - training batch 1451, loss: 1.188, 46432/60000 datapoints
2025-03-06 17:49:19,367 - INFO - training batch 1501, loss: 1.146, 48032/60000 datapoints
2025-03-06 17:49:19,561 - INFO - training batch 1551, loss: 1.253, 49632/60000 datapoints
2025-03-06 17:49:19,757 - INFO - training batch 1601, loss: 1.297, 51232/60000 datapoints
2025-03-06 17:49:19,951 - INFO - training batch 1651, loss: 1.220, 52832/60000 datapoints
2025-03-06 17:49:20,145 - INFO - training batch 1701, loss: 1.138, 54432/60000 datapoints
2025-03-06 17:49:20,338 - INFO - training batch 1751, loss: 1.143, 56032/60000 datapoints
2025-03-06 17:49:20,535 - INFO - training batch 1801, loss: 1.449, 57632/60000 datapoints
2025-03-06 17:49:20,730 - INFO - training batch 1851, loss: 1.213, 59232/60000 datapoints
2025-03-06 17:49:20,832 - INFO - validation batch 1, loss: 1.348, 32/10016 datapoints
2025-03-06 17:49:20,984 - INFO - validation batch 51, loss: 1.256, 1632/10016 datapoints
2025-03-06 17:49:21,137 - INFO - validation batch 101, loss: 1.237, 3232/10016 datapoints
2025-03-06 17:49:21,289 - INFO - validation batch 151, loss: 1.451, 4832/10016 datapoints
2025-03-06 17:49:21,441 - INFO - validation batch 201, loss: 1.164, 6432/10016 datapoints
2025-03-06 17:49:21,596 - INFO - validation batch 251, loss: 1.134, 8032/10016 datapoints
2025-03-06 17:49:21,766 - INFO - validation batch 301, loss: 1.270, 9632/10016 datapoints
2025-03-06 17:49:21,803 - INFO - Epoch 38/800 done.
2025-03-06 17:49:21,803 - INFO - Final validation performance:
Loss: 1.266, top-1 acc: 0.788top-5 acc: 0.788
2025-03-06 17:49:21,803 - INFO - Beginning epoch 39/800
2025-03-06 17:49:21,810 - INFO - training batch 1, loss: 1.251, 32/60000 datapoints
2025-03-06 17:49:22,001 - INFO - training batch 51, loss: 1.175, 1632/60000 datapoints
2025-03-06 17:49:22,195 - INFO - training batch 101, loss: 0.968, 3232/60000 datapoints
2025-03-06 17:49:22,387 - INFO - training batch 151, loss: 1.241, 4832/60000 datapoints
2025-03-06 17:49:22,581 - INFO - training batch 201, loss: 1.379, 6432/60000 datapoints
2025-03-06 17:49:22,779 - INFO - training batch 251, loss: 1.259, 8032/60000 datapoints
2025-03-06 17:49:22,968 - INFO - training batch 301, loss: 1.269, 9632/60000 datapoints
2025-03-06 17:49:23,160 - INFO - training batch 351, loss: 1.306, 11232/60000 datapoints
2025-03-06 17:49:23,352 - INFO - training batch 401, loss: 1.204, 12832/60000 datapoints
2025-03-06 17:49:23,544 - INFO - training batch 451, loss: 1.207, 14432/60000 datapoints
2025-03-06 17:49:23,737 - INFO - training batch 501, loss: 1.181, 16032/60000 datapoints
2025-03-06 17:49:23,928 - INFO - training batch 551, loss: 1.307, 17632/60000 datapoints
2025-03-06 17:49:24,121 - INFO - training batch 601, loss: 1.133, 19232/60000 datapoints
2025-03-06 17:49:24,311 - INFO - training batch 651, loss: 1.081, 20832/60000 datapoints
2025-03-06 17:49:24,504 - INFO - training batch 701, loss: 1.208, 22432/60000 datapoints
2025-03-06 17:49:24,697 - INFO - training batch 751, loss: 1.232, 24032/60000 datapoints
2025-03-06 17:49:24,891 - INFO - training batch 801, loss: 1.275, 25632/60000 datapoints
2025-03-06 17:49:25,081 - INFO - training batch 851, loss: 1.175, 27232/60000 datapoints
2025-03-06 17:49:25,272 - INFO - training batch 901, loss: 1.178, 28832/60000 datapoints
2025-03-06 17:49:25,467 - INFO - training batch 951, loss: 1.275, 30432/60000 datapoints
2025-03-06 17:49:25,678 - INFO - training batch 1001, loss: 1.492, 32032/60000 datapoints
2025-03-06 17:49:25,869 - INFO - training batch 1051, loss: 1.193, 33632/60000 datapoints
2025-03-06 17:49:26,059 - INFO - training batch 1101, loss: 1.275, 35232/60000 datapoints
2025-03-06 17:49:26,253 - INFO - training batch 1151, loss: 1.218, 36832/60000 datapoints
2025-03-06 17:49:26,445 - INFO - training batch 1201, loss: 1.244, 38432/60000 datapoints
2025-03-06 17:49:26,637 - INFO - training batch 1251, loss: 1.271, 40032/60000 datapoints
2025-03-06 17:49:26,828 - INFO - training batch 1301, loss: 1.257, 41632/60000 datapoints
2025-03-06 17:49:27,018 - INFO - training batch 1351, loss: 1.397, 43232/60000 datapoints
2025-03-06 17:49:27,208 - INFO - training batch 1401, loss: 1.188, 44832/60000 datapoints
2025-03-06 17:49:27,399 - INFO - training batch 1451, loss: 1.240, 46432/60000 datapoints
2025-03-06 17:49:27,592 - INFO - training batch 1501, loss: 1.139, 48032/60000 datapoints
2025-03-06 17:49:27,785 - INFO - training batch 1551, loss: 1.275, 49632/60000 datapoints
2025-03-06 17:49:27,977 - INFO - training batch 1601, loss: 1.254, 51232/60000 datapoints
2025-03-06 17:49:28,168 - INFO - training batch 1651, loss: 1.502, 52832/60000 datapoints
2025-03-06 17:49:28,358 - INFO - training batch 1701, loss: 1.226, 54432/60000 datapoints
2025-03-06 17:49:28,549 - INFO - training batch 1751, loss: 1.230, 56032/60000 datapoints
2025-03-06 17:49:28,742 - INFO - training batch 1801, loss: 1.111, 57632/60000 datapoints
2025-03-06 17:49:28,933 - INFO - training batch 1851, loss: 1.132, 59232/60000 datapoints
2025-03-06 17:49:29,030 - INFO - validation batch 1, loss: 1.285, 32/10016 datapoints
2025-03-06 17:49:29,182 - INFO - validation batch 51, loss: 1.209, 1632/10016 datapoints
2025-03-06 17:49:29,330 - INFO - validation batch 101, loss: 1.101, 3232/10016 datapoints
2025-03-06 17:49:29,479 - INFO - validation batch 151, loss: 1.168, 4832/10016 datapoints
2025-03-06 17:49:29,630 - INFO - validation batch 201, loss: 1.045, 6432/10016 datapoints
2025-03-06 17:49:29,779 - INFO - validation batch 251, loss: 1.425, 8032/10016 datapoints
2025-03-06 17:49:29,928 - INFO - validation batch 301, loss: 1.115, 9632/10016 datapoints
2025-03-06 17:49:29,963 - INFO - Epoch 39/800 done.
2025-03-06 17:49:29,964 - INFO - Final validation performance:
Loss: 1.193, top-1 acc: 0.791top-5 acc: 0.791
2025-03-06 17:49:29,964 - INFO - Beginning epoch 40/800
2025-03-06 17:49:29,969 - INFO - training batch 1, loss: 1.342, 32/60000 datapoints
2025-03-06 17:49:30,161 - INFO - training batch 51, loss: 1.064, 1632/60000 datapoints
2025-03-06 17:49:30,354 - INFO - training batch 101, loss: 1.264, 3232/60000 datapoints
2025-03-06 17:49:30,547 - INFO - training batch 151, loss: 1.398, 4832/60000 datapoints
2025-03-06 17:49:30,741 - INFO - training batch 201, loss: 1.226, 6432/60000 datapoints
2025-03-06 17:49:30,931 - INFO - training batch 251, loss: 1.238, 8032/60000 datapoints
2025-03-06 17:49:31,125 - INFO - training batch 301, loss: 1.225, 9632/60000 datapoints
2025-03-06 17:49:31,317 - INFO - training batch 351, loss: 1.139, 11232/60000 datapoints
2025-03-06 17:49:31,507 - INFO - training batch 401, loss: 1.221, 12832/60000 datapoints
2025-03-06 17:49:31,747 - INFO - training batch 451, loss: 1.140, 14432/60000 datapoints
2025-03-06 17:49:31,952 - INFO - training batch 501, loss: 1.128, 16032/60000 datapoints
2025-03-06 17:49:32,144 - INFO - training batch 551, loss: 1.204, 17632/60000 datapoints
2025-03-06 17:49:32,337 - INFO - training batch 601, loss: 1.278, 19232/60000 datapoints
2025-03-06 17:49:32,530 - INFO - training batch 651, loss: 1.320, 20832/60000 datapoints
2025-03-06 17:49:32,723 - INFO - training batch 701, loss: 1.129, 22432/60000 datapoints
2025-03-06 17:49:32,914 - INFO - training batch 751, loss: 1.160, 24032/60000 datapoints
2025-03-06 17:49:33,105 - INFO - training batch 801, loss: 1.126, 25632/60000 datapoints
2025-03-06 17:49:33,296 - INFO - training batch 851, loss: 1.053, 27232/60000 datapoints
2025-03-06 17:49:33,486 - INFO - training batch 901, loss: 1.114, 28832/60000 datapoints
2025-03-06 17:49:33,678 - INFO - training batch 951, loss: 1.332, 30432/60000 datapoints
2025-03-06 17:49:33,870 - INFO - training batch 1001, loss: 1.116, 32032/60000 datapoints
2025-03-06 17:49:34,058 - INFO - training batch 1051, loss: 1.238, 33632/60000 datapoints
2025-03-06 17:49:34,248 - INFO - training batch 1101, loss: 1.227, 35232/60000 datapoints
2025-03-06 17:49:34,441 - INFO - training batch 1151, loss: 1.184, 36832/60000 datapoints
2025-03-06 17:49:34,634 - INFO - training batch 1201, loss: 1.334, 38432/60000 datapoints
2025-03-06 17:49:34,827 - INFO - training batch 1251, loss: 1.048, 40032/60000 datapoints
2025-03-06 17:49:35,021 - INFO - training batch 1301, loss: 1.237, 41632/60000 datapoints
2025-03-06 17:49:35,212 - INFO - training batch 1351, loss: 1.311, 43232/60000 datapoints
2025-03-06 17:49:35,404 - INFO - training batch 1401, loss: 1.086, 44832/60000 datapoints
2025-03-06 17:49:35,600 - INFO - training batch 1451, loss: 1.270, 46432/60000 datapoints
2025-03-06 17:49:35,794 - INFO - training batch 1501, loss: 1.293, 48032/60000 datapoints
2025-03-06 17:49:35,984 - INFO - training batch 1551, loss: 1.103, 49632/60000 datapoints
2025-03-06 17:49:36,176 - INFO - training batch 1601, loss: 1.197, 51232/60000 datapoints
2025-03-06 17:49:36,369 - INFO - training batch 1651, loss: 1.258, 52832/60000 datapoints
2025-03-06 17:49:36,563 - INFO - training batch 1701, loss: 1.183, 54432/60000 datapoints
2025-03-06 17:49:36,757 - INFO - training batch 1751, loss: 1.106, 56032/60000 datapoints
2025-03-06 17:49:36,948 - INFO - training batch 1801, loss: 1.138, 57632/60000 datapoints
2025-03-06 17:49:37,138 - INFO - training batch 1851, loss: 1.134, 59232/60000 datapoints
2025-03-06 17:49:37,235 - INFO - validation batch 1, loss: 1.172, 32/10016 datapoints
2025-03-06 17:49:37,384 - INFO - validation batch 51, loss: 0.961, 1632/10016 datapoints
2025-03-06 17:49:37,533 - INFO - validation batch 101, loss: 1.012, 3232/10016 datapoints
2025-03-06 17:49:37,704 - INFO - validation batch 151, loss: 1.176, 4832/10016 datapoints
2025-03-06 17:49:37,854 - INFO - validation batch 201, loss: 1.153, 6432/10016 datapoints
2025-03-06 17:49:38,003 - INFO - validation batch 251, loss: 0.956, 8032/10016 datapoints
2025-03-06 17:49:38,153 - INFO - validation batch 301, loss: 1.011, 9632/10016 datapoints
2025-03-06 17:49:38,188 - INFO - Epoch 40/800 done.
2025-03-06 17:49:38,188 - INFO - Final validation performance:
Loss: 1.063, top-1 acc: 0.794top-5 acc: 0.794
2025-03-06 17:49:38,188 - INFO - Beginning epoch 41/800
2025-03-06 17:49:38,194 - INFO - training batch 1, loss: 1.278, 32/60000 datapoints
2025-03-06 17:49:38,386 - INFO - training batch 51, loss: 1.174, 1632/60000 datapoints
2025-03-06 17:49:38,585 - INFO - training batch 101, loss: 1.220, 3232/60000 datapoints
2025-03-06 17:49:38,784 - INFO - training batch 151, loss: 1.293, 4832/60000 datapoints
2025-03-06 17:49:38,982 - INFO - training batch 201, loss: 1.083, 6432/60000 datapoints
2025-03-06 17:49:39,178 - INFO - training batch 251, loss: 1.139, 8032/60000 datapoints
2025-03-06 17:49:39,372 - INFO - training batch 301, loss: 1.266, 9632/60000 datapoints
2025-03-06 17:49:39,566 - INFO - training batch 351, loss: 1.087, 11232/60000 datapoints
2025-03-06 17:49:39,763 - INFO - training batch 401, loss: 1.296, 12832/60000 datapoints
2025-03-06 17:49:39,961 - INFO - training batch 451, loss: 1.109, 14432/60000 datapoints
2025-03-06 17:49:40,154 - INFO - training batch 501, loss: 1.260, 16032/60000 datapoints
2025-03-06 17:49:40,350 - INFO - training batch 551, loss: 1.256, 17632/60000 datapoints
2025-03-06 17:49:40,545 - INFO - training batch 601, loss: 1.209, 19232/60000 datapoints
2025-03-06 17:49:40,740 - INFO - training batch 651, loss: 1.203, 20832/60000 datapoints
2025-03-06 17:49:40,935 - INFO - training batch 701, loss: 0.989, 22432/60000 datapoints
2025-03-06 17:49:41,127 - INFO - training batch 751, loss: 1.258, 24032/60000 datapoints
2025-03-06 17:49:41,324 - INFO - training batch 801, loss: 1.089, 25632/60000 datapoints
2025-03-06 17:49:41,516 - INFO - training batch 851, loss: 1.111, 27232/60000 datapoints
2025-03-06 17:49:41,712 - INFO - training batch 901, loss: 1.191, 28832/60000 datapoints
2025-03-06 17:49:41,923 - INFO - training batch 951, loss: 1.045, 30432/60000 datapoints
2025-03-06 17:49:42,118 - INFO - training batch 1001, loss: 1.120, 32032/60000 datapoints
2025-03-06 17:49:42,313 - INFO - training batch 1051, loss: 1.189, 33632/60000 datapoints
2025-03-06 17:49:42,510 - INFO - training batch 1101, loss: 0.978, 35232/60000 datapoints
2025-03-06 17:49:42,707 - INFO - training batch 1151, loss: 1.111, 36832/60000 datapoints
2025-03-06 17:49:42,900 - INFO - training batch 1201, loss: 0.968, 38432/60000 datapoints
2025-03-06 17:49:43,096 - INFO - training batch 1251, loss: 1.181, 40032/60000 datapoints
2025-03-06 17:49:43,290 - INFO - training batch 1301, loss: 1.024, 41632/60000 datapoints
2025-03-06 17:49:43,485 - INFO - training batch 1351, loss: 1.338, 43232/60000 datapoints
2025-03-06 17:49:43,683 - INFO - training batch 1401, loss: 1.209, 44832/60000 datapoints
2025-03-06 17:49:43,880 - INFO - training batch 1451, loss: 1.038, 46432/60000 datapoints
2025-03-06 17:49:44,074 - INFO - training batch 1501, loss: 1.028, 48032/60000 datapoints
2025-03-06 17:49:44,267 - INFO - training batch 1551, loss: 1.180, 49632/60000 datapoints
2025-03-06 17:49:44,462 - INFO - training batch 1601, loss: 1.253, 51232/60000 datapoints
2025-03-06 17:49:44,656 - INFO - training batch 1651, loss: 1.221, 52832/60000 datapoints
2025-03-06 17:49:44,858 - INFO - training batch 1701, loss: 0.993, 54432/60000 datapoints
2025-03-06 17:49:45,050 - INFO - training batch 1751, loss: 1.285, 56032/60000 datapoints
2025-03-06 17:49:45,244 - INFO - training batch 1801, loss: 1.177, 57632/60000 datapoints
2025-03-06 17:49:45,437 - INFO - training batch 1851, loss: 1.158, 59232/60000 datapoints
2025-03-06 17:49:45,540 - INFO - validation batch 1, loss: 0.960, 32/10016 datapoints
2025-03-06 17:49:45,696 - INFO - validation batch 51, loss: 1.055, 1632/10016 datapoints
2025-03-06 17:49:45,848 - INFO - validation batch 101, loss: 1.153, 3232/10016 datapoints
2025-03-06 17:49:45,999 - INFO - validation batch 151, loss: 1.236, 4832/10016 datapoints
2025-03-06 17:49:46,155 - INFO - validation batch 201, loss: 1.262, 6432/10016 datapoints
2025-03-06 17:49:46,308 - INFO - validation batch 251, loss: 1.102, 8032/10016 datapoints
2025-03-06 17:49:46,464 - INFO - validation batch 301, loss: 1.083, 9632/10016 datapoints
2025-03-06 17:49:46,501 - INFO - Epoch 41/800 done.
2025-03-06 17:49:46,501 - INFO - Final validation performance:
Loss: 1.122, top-1 acc: 0.796top-5 acc: 0.796
2025-03-06 17:49:46,501 - INFO - Beginning epoch 42/800
2025-03-06 17:49:46,507 - INFO - training batch 1, loss: 1.106, 32/60000 datapoints
2025-03-06 17:49:46,701 - INFO - training batch 51, loss: 1.062, 1632/60000 datapoints
2025-03-06 17:49:46,894 - INFO - training batch 101, loss: 1.139, 3232/60000 datapoints
2025-03-06 17:49:47,085 - INFO - training batch 151, loss: 0.972, 4832/60000 datapoints
2025-03-06 17:49:47,274 - INFO - training batch 201, loss: 1.091, 6432/60000 datapoints
2025-03-06 17:49:47,468 - INFO - training batch 251, loss: 1.085, 8032/60000 datapoints
2025-03-06 17:49:47,659 - INFO - training batch 301, loss: 1.132, 9632/60000 datapoints
2025-03-06 17:49:47,849 - INFO - training batch 351, loss: 1.112, 11232/60000 datapoints
2025-03-06 17:49:48,041 - INFO - training batch 401, loss: 1.034, 12832/60000 datapoints
2025-03-06 17:49:48,231 - INFO - training batch 451, loss: 1.175, 14432/60000 datapoints
2025-03-06 17:49:48,423 - INFO - training batch 501, loss: 1.093, 16032/60000 datapoints
2025-03-06 17:49:48,616 - INFO - training batch 551, loss: 1.070, 17632/60000 datapoints
2025-03-06 17:49:48,806 - INFO - training batch 601, loss: 1.005, 19232/60000 datapoints
2025-03-06 17:49:48,999 - INFO - training batch 651, loss: 1.248, 20832/60000 datapoints
2025-03-06 17:49:49,188 - INFO - training batch 701, loss: 1.189, 22432/60000 datapoints
2025-03-06 17:49:49,378 - INFO - training batch 751, loss: 1.127, 24032/60000 datapoints
2025-03-06 17:49:49,568 - INFO - training batch 801, loss: 1.162, 25632/60000 datapoints
2025-03-06 17:49:49,764 - INFO - training batch 851, loss: 1.222, 27232/60000 datapoints
2025-03-06 17:49:49,955 - INFO - training batch 901, loss: 1.188, 28832/60000 datapoints
2025-03-06 17:49:50,145 - INFO - training batch 951, loss: 1.166, 30432/60000 datapoints
2025-03-06 17:49:50,336 - INFO - training batch 1001, loss: 1.027, 32032/60000 datapoints
2025-03-06 17:49:50,530 - INFO - training batch 1051, loss: 1.003, 33632/60000 datapoints
2025-03-06 17:49:50,724 - INFO - training batch 1101, loss: 1.277, 35232/60000 datapoints
2025-03-06 17:49:50,915 - INFO - training batch 1151, loss: 0.972, 36832/60000 datapoints
2025-03-06 17:49:51,106 - INFO - training batch 1201, loss: 1.158, 38432/60000 datapoints
2025-03-06 17:49:51,296 - INFO - training batch 1251, loss: 1.024, 40032/60000 datapoints
2025-03-06 17:49:51,488 - INFO - training batch 1301, loss: 1.139, 41632/60000 datapoints
2025-03-06 17:49:51,682 - INFO - training batch 1351, loss: 1.285, 43232/60000 datapoints
2025-03-06 17:49:51,872 - INFO - training batch 1401, loss: 1.240, 44832/60000 datapoints
2025-03-06 17:49:52,084 - INFO - training batch 1451, loss: 1.121, 46432/60000 datapoints
2025-03-06 17:49:52,274 - INFO - training batch 1501, loss: 0.868, 48032/60000 datapoints
2025-03-06 17:49:52,466 - INFO - training batch 1551, loss: 1.101, 49632/60000 datapoints
2025-03-06 17:49:52,659 - INFO - training batch 1601, loss: 1.259, 51232/60000 datapoints
2025-03-06 17:49:52,850 - INFO - training batch 1651, loss: 1.175, 52832/60000 datapoints
2025-03-06 17:49:53,041 - INFO - training batch 1701, loss: 1.111, 54432/60000 datapoints
2025-03-06 17:49:53,232 - INFO - training batch 1751, loss: 1.273, 56032/60000 datapoints
2025-03-06 17:49:53,423 - INFO - training batch 1801, loss: 1.132, 57632/60000 datapoints
2025-03-06 17:49:53,615 - INFO - training batch 1851, loss: 1.103, 59232/60000 datapoints
2025-03-06 17:49:53,712 - INFO - validation batch 1, loss: 1.090, 32/10016 datapoints
2025-03-06 17:49:53,864 - INFO - validation batch 51, loss: 1.161, 1632/10016 datapoints
2025-03-06 17:49:54,016 - INFO - validation batch 101, loss: 1.154, 3232/10016 datapoints
2025-03-06 17:49:54,167 - INFO - validation batch 151, loss: 0.948, 4832/10016 datapoints
2025-03-06 17:49:54,316 - INFO - validation batch 201, loss: 1.161, 6432/10016 datapoints
2025-03-06 17:49:54,465 - INFO - validation batch 251, loss: 1.058, 8032/10016 datapoints
2025-03-06 17:49:54,619 - INFO - validation batch 301, loss: 1.215, 9632/10016 datapoints
2025-03-06 17:49:54,655 - INFO - Epoch 42/800 done.
2025-03-06 17:49:54,655 - INFO - Final validation performance:
Loss: 1.113, top-1 acc: 0.799top-5 acc: 0.799
2025-03-06 17:49:54,655 - INFO - Beginning epoch 43/800
2025-03-06 17:49:54,661 - INFO - training batch 1, loss: 1.029, 32/60000 datapoints
2025-03-06 17:49:54,854 - INFO - training batch 51, loss: 1.183, 1632/60000 datapoints
2025-03-06 17:49:55,050 - INFO - training batch 101, loss: 1.137, 3232/60000 datapoints
2025-03-06 17:49:55,241 - INFO - training batch 151, loss: 1.186, 4832/60000 datapoints
2025-03-06 17:49:55,433 - INFO - training batch 201, loss: 1.195, 6432/60000 datapoints
2025-03-06 17:49:55,636 - INFO - training batch 251, loss: 1.318, 8032/60000 datapoints
2025-03-06 17:49:55,830 - INFO - training batch 301, loss: 0.963, 9632/60000 datapoints
2025-03-06 17:49:56,022 - INFO - training batch 351, loss: 1.099, 11232/60000 datapoints
2025-03-06 17:49:56,216 - INFO - training batch 401, loss: 1.218, 12832/60000 datapoints
2025-03-06 17:49:56,406 - INFO - training batch 451, loss: 1.082, 14432/60000 datapoints
2025-03-06 17:49:56,602 - INFO - training batch 501, loss: 1.134, 16032/60000 datapoints
2025-03-06 17:49:56,794 - INFO - training batch 551, loss: 1.071, 17632/60000 datapoints
2025-03-06 17:49:56,986 - INFO - training batch 601, loss: 1.187, 19232/60000 datapoints
2025-03-06 17:49:57,179 - INFO - training batch 651, loss: 1.355, 20832/60000 datapoints
2025-03-06 17:49:57,370 - INFO - training batch 701, loss: 1.098, 22432/60000 datapoints
2025-03-06 17:49:57,561 - INFO - training batch 751, loss: 1.043, 24032/60000 datapoints
2025-03-06 17:49:57,755 - INFO - training batch 801, loss: 0.924, 25632/60000 datapoints
2025-03-06 17:49:57,946 - INFO - training batch 851, loss: 1.101, 27232/60000 datapoints
2025-03-06 17:49:58,139 - INFO - training batch 901, loss: 1.100, 28832/60000 datapoints
2025-03-06 17:49:58,330 - INFO - training batch 951, loss: 1.160, 30432/60000 datapoints
2025-03-06 17:49:58,530 - INFO - training batch 1001, loss: 1.139, 32032/60000 datapoints
2025-03-06 17:49:58,728 - INFO - training batch 1051, loss: 1.151, 33632/60000 datapoints
2025-03-06 17:49:58,931 - INFO - training batch 1101, loss: 1.357, 35232/60000 datapoints
2025-03-06 17:49:59,124 - INFO - training batch 1151, loss: 1.032, 36832/60000 datapoints
2025-03-06 17:49:59,317 - INFO - training batch 1201, loss: 1.101, 38432/60000 datapoints
2025-03-06 17:49:59,510 - INFO - training batch 1251, loss: 1.135, 40032/60000 datapoints
2025-03-06 17:49:59,706 - INFO - training batch 1301, loss: 1.275, 41632/60000 datapoints
2025-03-06 17:49:59,902 - INFO - training batch 1351, loss: 1.200, 43232/60000 datapoints
2025-03-06 17:50:00,097 - INFO - training batch 1401, loss: 1.220, 44832/60000 datapoints
2025-03-06 17:50:00,291 - INFO - training batch 1451, loss: 1.030, 46432/60000 datapoints
2025-03-06 17:50:00,483 - INFO - training batch 1501, loss: 0.984, 48032/60000 datapoints
2025-03-06 17:50:00,681 - INFO - training batch 1551, loss: 1.079, 49632/60000 datapoints
2025-03-06 17:50:00,883 - INFO - training batch 1601, loss: 1.160, 51232/60000 datapoints
2025-03-06 17:50:01,084 - INFO - training batch 1651, loss: 1.215, 52832/60000 datapoints
2025-03-06 17:50:01,279 - INFO - training batch 1701, loss: 1.277, 54432/60000 datapoints
2025-03-06 17:50:01,473 - INFO - training batch 1751, loss: 1.204, 56032/60000 datapoints
2025-03-06 17:50:01,673 - INFO - training batch 1801, loss: 1.135, 57632/60000 datapoints
2025-03-06 17:50:01,869 - INFO - training batch 1851, loss: 1.031, 59232/60000 datapoints
2025-03-06 17:50:02,003 - INFO - validation batch 1, loss: 1.161, 32/10016 datapoints
2025-03-06 17:50:02,174 - INFO - validation batch 51, loss: 1.059, 1632/10016 datapoints
2025-03-06 17:50:02,328 - INFO - validation batch 101, loss: 1.106, 3232/10016 datapoints
2025-03-06 17:50:02,480 - INFO - validation batch 151, loss: 1.163, 4832/10016 datapoints
2025-03-06 17:50:02,638 - INFO - validation batch 201, loss: 1.142, 6432/10016 datapoints
2025-03-06 17:50:02,794 - INFO - validation batch 251, loss: 1.252, 8032/10016 datapoints
2025-03-06 17:50:02,949 - INFO - validation batch 301, loss: 0.910, 9632/10016 datapoints
2025-03-06 17:50:02,987 - INFO - Epoch 43/800 done.
2025-03-06 17:50:02,987 - INFO - Final validation performance:
Loss: 1.113, top-1 acc: 0.802top-5 acc: 0.802
2025-03-06 17:50:02,987 - INFO - Beginning epoch 44/800
2025-03-06 17:50:02,994 - INFO - training batch 1, loss: 1.096, 32/60000 datapoints
2025-03-06 17:50:03,192 - INFO - training batch 51, loss: 1.044, 1632/60000 datapoints
2025-03-06 17:50:03,385 - INFO - training batch 101, loss: 1.086, 3232/60000 datapoints
2025-03-06 17:50:03,580 - INFO - training batch 151, loss: 1.193, 4832/60000 datapoints
2025-03-06 17:50:03,780 - INFO - training batch 201, loss: 1.015, 6432/60000 datapoints
2025-03-06 17:50:03,973 - INFO - training batch 251, loss: 0.857, 8032/60000 datapoints
2025-03-06 17:50:04,170 - INFO - training batch 301, loss: 1.193, 9632/60000 datapoints
2025-03-06 17:50:04,365 - INFO - training batch 351, loss: 1.134, 11232/60000 datapoints
2025-03-06 17:50:04,561 - INFO - training batch 401, loss: 1.286, 12832/60000 datapoints
2025-03-06 17:50:04,760 - INFO - training batch 451, loss: 1.012, 14432/60000 datapoints
2025-03-06 17:50:04,961 - INFO - training batch 501, loss: 1.131, 16032/60000 datapoints
2025-03-06 17:50:05,158 - INFO - training batch 551, loss: 1.058, 17632/60000 datapoints
2025-03-06 17:50:05,353 - INFO - training batch 601, loss: 1.100, 19232/60000 datapoints
2025-03-06 17:50:05,550 - INFO - training batch 651, loss: 1.177, 20832/60000 datapoints
2025-03-06 17:50:05,750 - INFO - training batch 701, loss: 1.123, 22432/60000 datapoints
2025-03-06 17:50:05,946 - INFO - training batch 751, loss: 0.932, 24032/60000 datapoints
2025-03-06 17:50:06,140 - INFO - training batch 801, loss: 1.029, 25632/60000 datapoints
2025-03-06 17:50:06,337 - INFO - training batch 851, loss: 1.153, 27232/60000 datapoints
2025-03-06 17:50:06,534 - INFO - training batch 901, loss: 1.157, 28832/60000 datapoints
2025-03-06 17:50:06,730 - INFO - training batch 951, loss: 1.038, 30432/60000 datapoints
2025-03-06 17:50:06,927 - INFO - training batch 1001, loss: 1.128, 32032/60000 datapoints
2025-03-06 17:50:07,120 - INFO - training batch 1051, loss: 1.146, 33632/60000 datapoints
2025-03-06 17:50:07,315 - INFO - training batch 1101, loss: 1.210, 35232/60000 datapoints
2025-03-06 17:50:07,508 - INFO - training batch 1151, loss: 0.973, 36832/60000 datapoints
2025-03-06 17:50:07,711 - INFO - training batch 1201, loss: 0.990, 38432/60000 datapoints
2025-03-06 17:50:07,906 - INFO - training batch 1251, loss: 0.978, 40032/60000 datapoints
2025-03-06 17:50:08,121 - INFO - training batch 1301, loss: 1.072, 41632/60000 datapoints
2025-03-06 17:50:08,314 - INFO - training batch 1351, loss: 1.237, 43232/60000 datapoints
2025-03-06 17:50:08,507 - INFO - training batch 1401, loss: 1.221, 44832/60000 datapoints
2025-03-06 17:50:08,706 - INFO - training batch 1451, loss: 1.243, 46432/60000 datapoints
2025-03-06 17:50:08,902 - INFO - training batch 1501, loss: 1.009, 48032/60000 datapoints
2025-03-06 17:50:09,101 - INFO - training batch 1551, loss: 0.990, 49632/60000 datapoints
2025-03-06 17:50:09,297 - INFO - training batch 1601, loss: 1.275, 51232/60000 datapoints
2025-03-06 17:50:09,489 - INFO - training batch 1651, loss: 1.018, 52832/60000 datapoints
2025-03-06 17:50:09,685 - INFO - training batch 1701, loss: 1.102, 54432/60000 datapoints
2025-03-06 17:50:09,878 - INFO - training batch 1751, loss: 1.198, 56032/60000 datapoints
2025-03-06 17:50:10,073 - INFO - training batch 1801, loss: 0.956, 57632/60000 datapoints
2025-03-06 17:50:10,270 - INFO - training batch 1851, loss: 0.913, 59232/60000 datapoints
2025-03-06 17:50:10,370 - INFO - validation batch 1, loss: 1.205, 32/10016 datapoints
2025-03-06 17:50:10,522 - INFO - validation batch 51, loss: 1.229, 1632/10016 datapoints
2025-03-06 17:50:10,682 - INFO - validation batch 101, loss: 1.017, 3232/10016 datapoints
2025-03-06 17:50:10,837 - INFO - validation batch 151, loss: 0.834, 4832/10016 datapoints
2025-03-06 17:50:10,989 - INFO - validation batch 201, loss: 1.015, 6432/10016 datapoints
2025-03-06 17:50:11,142 - INFO - validation batch 251, loss: 0.919, 8032/10016 datapoints
2025-03-06 17:50:11,294 - INFO - validation batch 301, loss: 1.013, 9632/10016 datapoints
2025-03-06 17:50:11,331 - INFO - Epoch 44/800 done.
2025-03-06 17:50:11,331 - INFO - Final validation performance:
Loss: 1.033, top-1 acc: 0.805top-5 acc: 0.805
2025-03-06 17:50:11,331 - INFO - Beginning epoch 45/800
2025-03-06 17:50:11,337 - INFO - training batch 1, loss: 0.897, 32/60000 datapoints
2025-03-06 17:50:11,530 - INFO - training batch 51, loss: 1.101, 1632/60000 datapoints
2025-03-06 17:50:11,726 - INFO - training batch 101, loss: 1.000, 3232/60000 datapoints
2025-03-06 17:50:11,923 - INFO - training batch 151, loss: 1.282, 4832/60000 datapoints
2025-03-06 17:50:12,152 - INFO - training batch 201, loss: 0.964, 6432/60000 datapoints
2025-03-06 17:50:12,347 - INFO - training batch 251, loss: 1.083, 8032/60000 datapoints
2025-03-06 17:50:12,542 - INFO - training batch 301, loss: 1.084, 9632/60000 datapoints
2025-03-06 17:50:12,740 - INFO - training batch 351, loss: 1.072, 11232/60000 datapoints
2025-03-06 17:50:12,935 - INFO - training batch 401, loss: 1.173, 12832/60000 datapoints
2025-03-06 17:50:13,132 - INFO - training batch 451, loss: 1.042, 14432/60000 datapoints
2025-03-06 17:50:13,329 - INFO - training batch 501, loss: 0.973, 16032/60000 datapoints
2025-03-06 17:50:13,525 - INFO - training batch 551, loss: 1.096, 17632/60000 datapoints
2025-03-06 17:50:13,723 - INFO - training batch 601, loss: 1.152, 19232/60000 datapoints
2025-03-06 17:50:13,921 - INFO - training batch 651, loss: 0.988, 20832/60000 datapoints
2025-03-06 17:50:14,117 - INFO - training batch 701, loss: 1.130, 22432/60000 datapoints
2025-03-06 17:50:14,314 - INFO - training batch 751, loss: 1.097, 24032/60000 datapoints
2025-03-06 17:50:14,508 - INFO - training batch 801, loss: 1.334, 25632/60000 datapoints
2025-03-06 17:50:14,707 - INFO - training batch 851, loss: 1.019, 27232/60000 datapoints
2025-03-06 17:50:14,908 - INFO - training batch 901, loss: 1.239, 28832/60000 datapoints
2025-03-06 17:50:15,104 - INFO - training batch 951, loss: 1.080, 30432/60000 datapoints
2025-03-06 17:50:15,297 - INFO - training batch 1001, loss: 1.154, 32032/60000 datapoints
2025-03-06 17:50:15,489 - INFO - training batch 1051, loss: 1.034, 33632/60000 datapoints
2025-03-06 17:50:15,687 - INFO - training batch 1101, loss: 0.988, 35232/60000 datapoints
2025-03-06 17:50:15,883 - INFO - training batch 1151, loss: 1.045, 36832/60000 datapoints
2025-03-06 17:50:16,079 - INFO - training batch 1201, loss: 0.987, 38432/60000 datapoints
2025-03-06 17:50:16,276 - INFO - training batch 1251, loss: 1.124, 40032/60000 datapoints
2025-03-06 17:50:16,468 - INFO - training batch 1301, loss: 1.161, 41632/60000 datapoints
2025-03-06 17:50:16,668 - INFO - training batch 1351, loss: 0.969, 43232/60000 datapoints
2025-03-06 17:50:16,873 - INFO - training batch 1401, loss: 1.051, 44832/60000 datapoints
2025-03-06 17:50:17,069 - INFO - training batch 1451, loss: 1.161, 46432/60000 datapoints
2025-03-06 17:50:17,263 - INFO - training batch 1501, loss: 1.145, 48032/60000 datapoints
2025-03-06 17:50:17,460 - INFO - training batch 1551, loss: 0.949, 49632/60000 datapoints
2025-03-06 17:50:17,656 - INFO - training batch 1601, loss: 0.919, 51232/60000 datapoints
2025-03-06 17:50:17,856 - INFO - training batch 1651, loss: 1.045, 52832/60000 datapoints
2025-03-06 17:50:18,052 - INFO - training batch 1701, loss: 1.066, 54432/60000 datapoints
2025-03-06 17:50:18,246 - INFO - training batch 1751, loss: 1.062, 56032/60000 datapoints
2025-03-06 17:50:18,442 - INFO - training batch 1801, loss: 1.128, 57632/60000 datapoints
2025-03-06 17:50:18,641 - INFO - training batch 1851, loss: 0.973, 59232/60000 datapoints
2025-03-06 17:50:18,744 - INFO - validation batch 1, loss: 0.897, 32/10016 datapoints
2025-03-06 17:50:18,897 - INFO - validation batch 51, loss: 0.935, 1632/10016 datapoints
2025-03-06 17:50:19,048 - INFO - validation batch 101, loss: 0.962, 3232/10016 datapoints
2025-03-06 17:50:19,202 - INFO - validation batch 151, loss: 1.037, 4832/10016 datapoints
2025-03-06 17:50:19,355 - INFO - validation batch 201, loss: 1.132, 6432/10016 datapoints
2025-03-06 17:50:19,508 - INFO - validation batch 251, loss: 1.187, 8032/10016 datapoints
2025-03-06 17:50:19,663 - INFO - validation batch 301, loss: 1.161, 9632/10016 datapoints
2025-03-06 17:50:19,700 - INFO - Epoch 45/800 done.
2025-03-06 17:50:19,700 - INFO - Final validation performance:
Loss: 1.044, top-1 acc: 0.807top-5 acc: 0.807
2025-03-06 17:50:19,700 - INFO - Beginning epoch 46/800
2025-03-06 17:50:19,707 - INFO - training batch 1, loss: 1.290, 32/60000 datapoints
2025-03-06 17:50:19,906 - INFO - training batch 51, loss: 1.041, 1632/60000 datapoints
2025-03-06 17:50:20,102 - INFO - training batch 101, loss: 1.154, 3232/60000 datapoints
2025-03-06 17:50:20,295 - INFO - training batch 151, loss: 1.217, 4832/60000 datapoints
2025-03-06 17:50:20,489 - INFO - training batch 201, loss: 1.077, 6432/60000 datapoints
2025-03-06 17:50:20,704 - INFO - training batch 251, loss: 1.195, 8032/60000 datapoints
2025-03-06 17:50:20,900 - INFO - training batch 301, loss: 1.117, 9632/60000 datapoints
2025-03-06 17:50:21,094 - INFO - training batch 351, loss: 0.983, 11232/60000 datapoints
2025-03-06 17:50:21,287 - INFO - training batch 401, loss: 1.102, 12832/60000 datapoints
2025-03-06 17:50:21,481 - INFO - training batch 451, loss: 1.043, 14432/60000 datapoints
2025-03-06 17:50:21,679 - INFO - training batch 501, loss: 1.165, 16032/60000 datapoints
2025-03-06 17:50:21,876 - INFO - training batch 551, loss: 1.099, 17632/60000 datapoints
2025-03-06 17:50:22,071 - INFO - training batch 601, loss: 1.113, 19232/60000 datapoints
2025-03-06 17:50:22,285 - INFO - training batch 651, loss: 0.960, 20832/60000 datapoints
2025-03-06 17:50:22,480 - INFO - training batch 701, loss: 1.203, 22432/60000 datapoints
2025-03-06 17:50:22,678 - INFO - training batch 751, loss: 1.139, 24032/60000 datapoints
2025-03-06 17:50:22,877 - INFO - training batch 801, loss: 1.118, 25632/60000 datapoints
2025-03-06 17:50:23,075 - INFO - training batch 851, loss: 1.030, 27232/60000 datapoints
2025-03-06 17:50:23,269 - INFO - training batch 901, loss: 1.041, 28832/60000 datapoints
2025-03-06 17:50:23,465 - INFO - training batch 951, loss: 0.987, 30432/60000 datapoints
2025-03-06 17:50:23,661 - INFO - training batch 1001, loss: 1.299, 32032/60000 datapoints
2025-03-06 17:50:23,856 - INFO - training batch 1051, loss: 0.981, 33632/60000 datapoints
2025-03-06 17:50:24,056 - INFO - training batch 1101, loss: 1.015, 35232/60000 datapoints
2025-03-06 17:50:24,250 - INFO - training batch 1151, loss: 1.056, 36832/60000 datapoints
2025-03-06 17:50:24,446 - INFO - training batch 1201, loss: 1.063, 38432/60000 datapoints
2025-03-06 17:50:24,644 - INFO - training batch 1251, loss: 1.093, 40032/60000 datapoints
2025-03-06 17:50:24,842 - INFO - training batch 1301, loss: 0.883, 41632/60000 datapoints
2025-03-06 17:50:25,037 - INFO - training batch 1351, loss: 1.063, 43232/60000 datapoints
2025-03-06 17:50:25,232 - INFO - training batch 1401, loss: 1.063, 44832/60000 datapoints
2025-03-06 17:50:25,429 - INFO - training batch 1451, loss: 1.041, 46432/60000 datapoints
2025-03-06 17:50:25,632 - INFO - training batch 1501, loss: 1.102, 48032/60000 datapoints
2025-03-06 17:50:25,825 - INFO - training batch 1551, loss: 1.190, 49632/60000 datapoints
2025-03-06 17:50:26,027 - INFO - training batch 1601, loss: 1.133, 51232/60000 datapoints
2025-03-06 17:50:26,226 - INFO - training batch 1651, loss: 0.984, 52832/60000 datapoints
2025-03-06 17:50:26,422 - INFO - training batch 1701, loss: 1.067, 54432/60000 datapoints
2025-03-06 17:50:26,621 - INFO - training batch 1751, loss: 1.129, 56032/60000 datapoints
2025-03-06 17:50:26,815 - INFO - training batch 1801, loss: 0.969, 57632/60000 datapoints
2025-03-06 17:50:27,015 - INFO - training batch 1851, loss: 1.191, 59232/60000 datapoints
2025-03-06 17:50:27,118 - INFO - validation batch 1, loss: 0.980, 32/10016 datapoints
2025-03-06 17:50:27,270 - INFO - validation batch 51, loss: 0.941, 1632/10016 datapoints
2025-03-06 17:50:27,425 - INFO - validation batch 101, loss: 1.014, 3232/10016 datapoints
2025-03-06 17:50:27,578 - INFO - validation batch 151, loss: 1.062, 4832/10016 datapoints
2025-03-06 17:50:27,734 - INFO - validation batch 201, loss: 1.041, 6432/10016 datapoints
2025-03-06 17:50:27,886 - INFO - validation batch 251, loss: 1.037, 8032/10016 datapoints
2025-03-06 17:50:28,040 - INFO - validation batch 301, loss: 1.088, 9632/10016 datapoints
2025-03-06 17:50:28,079 - INFO - Epoch 46/800 done.
2025-03-06 17:50:28,080 - INFO - Final validation performance:
Loss: 1.023, top-1 acc: 0.809top-5 acc: 0.809
2025-03-06 17:50:28,080 - INFO - Beginning epoch 47/800
2025-03-06 17:50:28,085 - INFO - training batch 1, loss: 0.963, 32/60000 datapoints
2025-03-06 17:50:28,279 - INFO - training batch 51, loss: 0.967, 1632/60000 datapoints
2025-03-06 17:50:28,476 - INFO - training batch 101, loss: 1.067, 3232/60000 datapoints
2025-03-06 17:50:28,676 - INFO - training batch 151, loss: 0.977, 4832/60000 datapoints
2025-03-06 17:50:28,873 - INFO - training batch 201, loss: 1.160, 6432/60000 datapoints
2025-03-06 17:50:29,070 - INFO - training batch 251, loss: 1.151, 8032/60000 datapoints
2025-03-06 17:50:29,266 - INFO - training batch 301, loss: 1.071, 9632/60000 datapoints
2025-03-06 17:50:29,460 - INFO - training batch 351, loss: 0.972, 11232/60000 datapoints
2025-03-06 17:50:29,658 - INFO - training batch 401, loss: 0.916, 12832/60000 datapoints
2025-03-06 17:50:29,854 - INFO - training batch 451, loss: 0.872, 14432/60000 datapoints
2025-03-06 17:50:30,051 - INFO - training batch 501, loss: 0.964, 16032/60000 datapoints
2025-03-06 17:50:30,245 - INFO - training batch 551, loss: 0.926, 17632/60000 datapoints
2025-03-06 17:50:30,440 - INFO - training batch 601, loss: 1.059, 19232/60000 datapoints
2025-03-06 17:50:30,639 - INFO - training batch 651, loss: 1.028, 20832/60000 datapoints
2025-03-06 17:50:30,834 - INFO - training batch 701, loss: 0.930, 22432/60000 datapoints
2025-03-06 17:50:31,032 - INFO - training batch 751, loss: 1.075, 24032/60000 datapoints
2025-03-06 17:50:31,227 - INFO - training batch 801, loss: 1.071, 25632/60000 datapoints
2025-03-06 17:50:31,421 - INFO - training batch 851, loss: 0.992, 27232/60000 datapoints
2025-03-06 17:50:31,619 - INFO - training batch 901, loss: 0.935, 28832/60000 datapoints
2025-03-06 17:50:31,814 - INFO - training batch 951, loss: 0.915, 30432/60000 datapoints
2025-03-06 17:50:32,012 - INFO - training batch 1001, loss: 1.063, 32032/60000 datapoints
2025-03-06 17:50:32,209 - INFO - training batch 1051, loss: 0.950, 33632/60000 datapoints
2025-03-06 17:50:32,421 - INFO - training batch 1101, loss: 0.905, 35232/60000 datapoints
2025-03-06 17:50:32,621 - INFO - training batch 1151, loss: 1.036, 36832/60000 datapoints
2025-03-06 17:50:32,815 - INFO - training batch 1201, loss: 0.970, 38432/60000 datapoints
2025-03-06 17:50:33,011 - INFO - training batch 1251, loss: 1.033, 40032/60000 datapoints
2025-03-06 17:50:33,207 - INFO - training batch 1301, loss: 0.811, 41632/60000 datapoints
2025-03-06 17:50:33,401 - INFO - training batch 1351, loss: 0.960, 43232/60000 datapoints
2025-03-06 17:50:33,598 - INFO - training batch 1401, loss: 1.036, 44832/60000 datapoints
2025-03-06 17:50:33,788 - INFO - training batch 1451, loss: 1.006, 46432/60000 datapoints
2025-03-06 17:50:33,983 - INFO - training batch 1501, loss: 1.029, 48032/60000 datapoints
2025-03-06 17:50:34,177 - INFO - training batch 1551, loss: 0.836, 49632/60000 datapoints
2025-03-06 17:50:34,370 - INFO - training batch 1601, loss: 0.964, 51232/60000 datapoints
2025-03-06 17:50:34,564 - INFO - training batch 1651, loss: 0.986, 52832/60000 datapoints
2025-03-06 17:50:34,762 - INFO - training batch 1701, loss: 0.947, 54432/60000 datapoints
2025-03-06 17:50:34,961 - INFO - training batch 1751, loss: 1.216, 56032/60000 datapoints
2025-03-06 17:50:35,157 - INFO - training batch 1801, loss: 0.850, 57632/60000 datapoints
2025-03-06 17:50:35,351 - INFO - training batch 1851, loss: 1.084, 59232/60000 datapoints
2025-03-06 17:50:35,452 - INFO - validation batch 1, loss: 1.154, 32/10016 datapoints
2025-03-06 17:50:35,612 - INFO - validation batch 51, loss: 0.987, 1632/10016 datapoints
2025-03-06 17:50:35,764 - INFO - validation batch 101, loss: 0.869, 3232/10016 datapoints
2025-03-06 17:50:35,917 - INFO - validation batch 151, loss: 0.880, 4832/10016 datapoints
2025-03-06 17:50:36,078 - INFO - validation batch 201, loss: 1.105, 6432/10016 datapoints
2025-03-06 17:50:36,229 - INFO - validation batch 251, loss: 0.846, 8032/10016 datapoints
2025-03-06 17:50:36,383 - INFO - validation batch 301, loss: 0.941, 9632/10016 datapoints
2025-03-06 17:50:36,420 - INFO - Epoch 47/800 done.
2025-03-06 17:50:36,420 - INFO - Final validation performance:
Loss: 0.969, top-1 acc: 0.811top-5 acc: 0.811
2025-03-06 17:50:36,420 - INFO - Beginning epoch 48/800
2025-03-06 17:50:36,426 - INFO - training batch 1, loss: 0.915, 32/60000 datapoints
2025-03-06 17:50:36,624 - INFO - training batch 51, loss: 1.028, 1632/60000 datapoints
2025-03-06 17:50:36,818 - INFO - training batch 101, loss: 0.981, 3232/60000 datapoints
2025-03-06 17:50:37,011 - INFO - training batch 151, loss: 1.111, 4832/60000 datapoints
2025-03-06 17:50:37,205 - INFO - training batch 201, loss: 1.019, 6432/60000 datapoints
2025-03-06 17:50:37,400 - INFO - training batch 251, loss: 1.086, 8032/60000 datapoints
2025-03-06 17:50:37,604 - INFO - training batch 301, loss: 1.106, 9632/60000 datapoints
2025-03-06 17:50:37,799 - INFO - training batch 351, loss: 1.078, 11232/60000 datapoints
2025-03-06 17:50:37,992 - INFO - training batch 401, loss: 1.029, 12832/60000 datapoints
2025-03-06 17:50:38,191 - INFO - training batch 451, loss: 0.900, 14432/60000 datapoints
2025-03-06 17:50:38,385 - INFO - training batch 501, loss: 0.841, 16032/60000 datapoints
2025-03-06 17:50:38,579 - INFO - training batch 551, loss: 1.049, 17632/60000 datapoints
2025-03-06 17:50:38,778 - INFO - training batch 601, loss: 0.978, 19232/60000 datapoints
2025-03-06 17:50:38,984 - INFO - training batch 651, loss: 1.139, 20832/60000 datapoints
2025-03-06 17:50:39,181 - INFO - training batch 701, loss: 1.029, 22432/60000 datapoints
2025-03-06 17:50:39,379 - INFO - training batch 751, loss: 0.921, 24032/60000 datapoints
2025-03-06 17:50:39,572 - INFO - training batch 801, loss: 1.027, 25632/60000 datapoints
2025-03-06 17:50:39,768 - INFO - training batch 851, loss: 1.031, 27232/60000 datapoints
2025-03-06 17:50:39,963 - INFO - training batch 901, loss: 0.911, 28832/60000 datapoints
2025-03-06 17:50:40,157 - INFO - training batch 951, loss: 0.914, 30432/60000 datapoints
2025-03-06 17:50:40,352 - INFO - training batch 1001, loss: 1.111, 32032/60000 datapoints
2025-03-06 17:50:40,547 - INFO - training batch 1051, loss: 1.012, 33632/60000 datapoints
2025-03-06 17:50:40,747 - INFO - training batch 1101, loss: 0.941, 35232/60000 datapoints
2025-03-06 17:50:40,942 - INFO - training batch 1151, loss: 1.027, 36832/60000 datapoints
2025-03-06 17:50:41,137 - INFO - training batch 1201, loss: 1.055, 38432/60000 datapoints
2025-03-06 17:50:41,332 - INFO - training batch 1251, loss: 0.989, 40032/60000 datapoints
2025-03-06 17:50:41,523 - INFO - training batch 1301, loss: 1.032, 41632/60000 datapoints
2025-03-06 17:50:41,725 - INFO - training batch 1351, loss: 1.042, 43232/60000 datapoints
2025-03-06 17:50:41,917 - INFO - training batch 1401, loss: 0.942, 44832/60000 datapoints
2025-03-06 17:50:42,113 - INFO - training batch 1451, loss: 0.989, 46432/60000 datapoints
2025-03-06 17:50:42,307 - INFO - training batch 1501, loss: 0.930, 48032/60000 datapoints
2025-03-06 17:50:42,520 - INFO - training batch 1551, loss: 1.020, 49632/60000 datapoints
2025-03-06 17:50:42,720 - INFO - training batch 1601, loss: 1.068, 51232/60000 datapoints
2025-03-06 17:50:42,914 - INFO - training batch 1651, loss: 0.971, 52832/60000 datapoints
2025-03-06 17:50:43,109 - INFO - training batch 1701, loss: 1.057, 54432/60000 datapoints
2025-03-06 17:50:43,303 - INFO - training batch 1751, loss: 1.095, 56032/60000 datapoints
2025-03-06 17:50:43,497 - INFO - training batch 1801, loss: 0.972, 57632/60000 datapoints
2025-03-06 17:50:43,695 - INFO - training batch 1851, loss: 1.072, 59232/60000 datapoints
2025-03-06 17:50:43,796 - INFO - validation batch 1, loss: 1.003, 32/10016 datapoints
2025-03-06 17:50:43,947 - INFO - validation batch 51, loss: 0.842, 1632/10016 datapoints
2025-03-06 17:50:44,099 - INFO - validation batch 101, loss: 0.914, 3232/10016 datapoints
2025-03-06 17:50:44,253 - INFO - validation batch 151, loss: 0.967, 4832/10016 datapoints
2025-03-06 17:50:44,408 - INFO - validation batch 201, loss: 1.061, 6432/10016 datapoints
2025-03-06 17:50:44,560 - INFO - validation batch 251, loss: 0.988, 8032/10016 datapoints
2025-03-06 17:50:44,718 - INFO - validation batch 301, loss: 0.898, 9632/10016 datapoints
2025-03-06 17:50:44,754 - INFO - Epoch 48/800 done.
2025-03-06 17:50:44,754 - INFO - Final validation performance:
Loss: 0.953, top-1 acc: 0.813top-5 acc: 0.813
2025-03-06 17:50:44,755 - INFO - Beginning epoch 49/800
2025-03-06 17:50:44,763 - INFO - training batch 1, loss: 0.904, 32/60000 datapoints
2025-03-06 17:50:44,961 - INFO - training batch 51, loss: 1.145, 1632/60000 datapoints
2025-03-06 17:50:45,156 - INFO - training batch 101, loss: 0.993, 3232/60000 datapoints
2025-03-06 17:50:45,350 - INFO - training batch 151, loss: 0.976, 4832/60000 datapoints
2025-03-06 17:50:45,544 - INFO - training batch 201, loss: 1.058, 6432/60000 datapoints
2025-03-06 17:50:45,745 - INFO - training batch 251, loss: 0.952, 8032/60000 datapoints
2025-03-06 17:50:45,940 - INFO - training batch 301, loss: 0.917, 9632/60000 datapoints
2025-03-06 17:50:46,135 - INFO - training batch 351, loss: 1.057, 11232/60000 datapoints
2025-03-06 17:50:46,333 - INFO - training batch 401, loss: 0.819, 12832/60000 datapoints
2025-03-06 17:50:46,527 - INFO - training batch 451, loss: 1.191, 14432/60000 datapoints
2025-03-06 17:50:46,726 - INFO - training batch 501, loss: 1.002, 16032/60000 datapoints
2025-03-06 17:50:46,920 - INFO - training batch 551, loss: 0.843, 17632/60000 datapoints
2025-03-06 17:50:47,115 - INFO - training batch 601, loss: 0.963, 19232/60000 datapoints
2025-03-06 17:50:47,312 - INFO - training batch 651, loss: 0.976, 20832/60000 datapoints
2025-03-06 17:50:47,507 - INFO - training batch 701, loss: 0.863, 22432/60000 datapoints
2025-03-06 17:50:47,706 - INFO - training batch 751, loss: 0.912, 24032/60000 datapoints
2025-03-06 17:50:47,900 - INFO - training batch 801, loss: 0.934, 25632/60000 datapoints
2025-03-06 17:50:48,095 - INFO - training batch 851, loss: 1.029, 27232/60000 datapoints
2025-03-06 17:50:48,290 - INFO - training batch 901, loss: 1.067, 28832/60000 datapoints
2025-03-06 17:50:48,484 - INFO - training batch 951, loss: 0.996, 30432/60000 datapoints
2025-03-06 17:50:48,683 - INFO - training batch 1001, loss: 1.006, 32032/60000 datapoints
2025-03-06 17:50:48,878 - INFO - training batch 1051, loss: 0.905, 33632/60000 datapoints
2025-03-06 17:50:49,072 - INFO - training batch 1101, loss: 0.980, 35232/60000 datapoints
2025-03-06 17:50:49,266 - INFO - training batch 1151, loss: 1.013, 36832/60000 datapoints
2025-03-06 17:50:49,458 - INFO - training batch 1201, loss: 1.087, 38432/60000 datapoints
2025-03-06 17:50:49,655 - INFO - training batch 1251, loss: 0.976, 40032/60000 datapoints
2025-03-06 17:50:49,851 - INFO - training batch 1301, loss: 1.129, 41632/60000 datapoints
2025-03-06 17:50:50,044 - INFO - training batch 1351, loss: 0.966, 43232/60000 datapoints
2025-03-06 17:50:50,238 - INFO - training batch 1401, loss: 0.909, 44832/60000 datapoints
2025-03-06 17:50:50,433 - INFO - training batch 1451, loss: 0.860, 46432/60000 datapoints
2025-03-06 17:50:50,628 - INFO - training batch 1501, loss: 0.913, 48032/60000 datapoints
2025-03-06 17:50:50,824 - INFO - training batch 1551, loss: 0.856, 49632/60000 datapoints
2025-03-06 17:50:51,018 - INFO - training batch 1601, loss: 1.049, 51232/60000 datapoints
2025-03-06 17:50:51,211 - INFO - training batch 1651, loss: 0.924, 52832/60000 datapoints
2025-03-06 17:50:51,408 - INFO - training batch 1701, loss: 0.891, 54432/60000 datapoints
2025-03-06 17:50:51,601 - INFO - training batch 1751, loss: 0.969, 56032/60000 datapoints
2025-03-06 17:50:51,794 - INFO - training batch 1801, loss: 1.077, 57632/60000 datapoints
2025-03-06 17:50:51,988 - INFO - training batch 1851, loss: 0.926, 59232/60000 datapoints
2025-03-06 17:50:52,088 - INFO - validation batch 1, loss: 1.065, 32/10016 datapoints
2025-03-06 17:50:52,244 - INFO - validation batch 51, loss: 1.023, 1632/10016 datapoints
2025-03-06 17:50:52,395 - INFO - validation batch 101, loss: 1.055, 3232/10016 datapoints
2025-03-06 17:50:52,567 - INFO - validation batch 151, loss: 1.076, 4832/10016 datapoints
2025-03-06 17:50:52,724 - INFO - validation batch 201, loss: 0.885, 6432/10016 datapoints
2025-03-06 17:50:52,879 - INFO - validation batch 251, loss: 0.811, 8032/10016 datapoints
2025-03-06 17:50:53,032 - INFO - validation batch 301, loss: 0.926, 9632/10016 datapoints
2025-03-06 17:50:53,069 - INFO - Epoch 49/800 done.
2025-03-06 17:50:53,070 - INFO - Final validation performance:
Loss: 0.977, top-1 acc: 0.815top-5 acc: 0.815
2025-03-06 17:50:53,070 - INFO - Beginning epoch 50/800
2025-03-06 17:50:53,075 - INFO - training batch 1, loss: 0.978, 32/60000 datapoints
2025-03-06 17:50:53,271 - INFO - training batch 51, loss: 0.945, 1632/60000 datapoints
2025-03-06 17:50:53,462 - INFO - training batch 101, loss: 1.014, 3232/60000 datapoints
2025-03-06 17:50:53,656 - INFO - training batch 151, loss: 1.018, 4832/60000 datapoints
2025-03-06 17:50:53,849 - INFO - training batch 201, loss: 1.004, 6432/60000 datapoints
2025-03-06 17:50:54,041 - INFO - training batch 251, loss: 1.042, 8032/60000 datapoints
2025-03-06 17:50:54,237 - INFO - training batch 301, loss: 0.963, 9632/60000 datapoints
2025-03-06 17:50:54,431 - INFO - training batch 351, loss: 1.045, 11232/60000 datapoints
2025-03-06 17:50:54,626 - INFO - training batch 401, loss: 0.922, 12832/60000 datapoints
2025-03-06 17:50:54,823 - INFO - training batch 451, loss: 1.077, 14432/60000 datapoints
2025-03-06 17:50:55,026 - INFO - training batch 501, loss: 0.905, 16032/60000 datapoints
2025-03-06 17:50:55,221 - INFO - training batch 551, loss: 1.074, 17632/60000 datapoints
2025-03-06 17:50:55,416 - INFO - training batch 601, loss: 1.098, 19232/60000 datapoints
2025-03-06 17:50:55,612 - INFO - training batch 651, loss: 1.027, 20832/60000 datapoints
2025-03-06 17:50:55,809 - INFO - training batch 701, loss: 0.996, 22432/60000 datapoints
2025-03-06 17:50:56,003 - INFO - training batch 751, loss: 0.821, 24032/60000 datapoints
2025-03-06 17:50:56,200 - INFO - training batch 801, loss: 1.036, 25632/60000 datapoints
2025-03-06 17:50:56,393 - INFO - training batch 851, loss: 0.929, 27232/60000 datapoints
2025-03-06 17:50:56,588 - INFO - training batch 901, loss: 0.931, 28832/60000 datapoints
2025-03-06 17:50:56,787 - INFO - training batch 951, loss: 1.055, 30432/60000 datapoints
2025-03-06 17:50:56,982 - INFO - training batch 1001, loss: 0.831, 32032/60000 datapoints
2025-03-06 17:50:57,177 - INFO - training batch 1051, loss: 0.859, 33632/60000 datapoints
2025-03-06 17:50:57,371 - INFO - training batch 1101, loss: 0.970, 35232/60000 datapoints
2025-03-06 17:50:57,563 - INFO - training batch 1151, loss: 1.086, 36832/60000 datapoints
2025-03-06 17:50:57,758 - INFO - training batch 1201, loss: 0.903, 38432/60000 datapoints
2025-03-06 17:50:57,953 - INFO - training batch 1251, loss: 0.923, 40032/60000 datapoints
2025-03-06 17:50:58,146 - INFO - training batch 1301, loss: 1.091, 41632/60000 datapoints
2025-03-06 17:50:58,340 - INFO - training batch 1351, loss: 1.026, 43232/60000 datapoints
2025-03-06 17:50:58,536 - INFO - training batch 1401, loss: 0.978, 44832/60000 datapoints
2025-03-06 17:50:58,738 - INFO - training batch 1451, loss: 1.121, 46432/60000 datapoints
2025-03-06 17:50:58,940 - INFO - training batch 1501, loss: 1.088, 48032/60000 datapoints
2025-03-06 17:50:59,137 - INFO - training batch 1551, loss: 1.104, 49632/60000 datapoints
2025-03-06 17:50:59,331 - INFO - training batch 1601, loss: 1.104, 51232/60000 datapoints
2025-03-06 17:50:59,524 - INFO - training batch 1651, loss: 0.977, 52832/60000 datapoints
2025-03-06 17:50:59,720 - INFO - training batch 1701, loss: 1.077, 54432/60000 datapoints
2025-03-06 17:50:59,915 - INFO - training batch 1751, loss: 0.906, 56032/60000 datapoints
2025-03-06 17:51:00,109 - INFO - training batch 1801, loss: 0.892, 57632/60000 datapoints
2025-03-06 17:51:00,302 - INFO - training batch 1851, loss: 1.058, 59232/60000 datapoints
2025-03-06 17:51:00,403 - INFO - validation batch 1, loss: 1.012, 32/10016 datapoints
2025-03-06 17:51:00,554 - INFO - validation batch 51, loss: 1.049, 1632/10016 datapoints
2025-03-06 17:51:00,711 - INFO - validation batch 101, loss: 1.241, 3232/10016 datapoints
2025-03-06 17:51:00,864 - INFO - validation batch 151, loss: 0.996, 4832/10016 datapoints
2025-03-06 17:51:01,015 - INFO - validation batch 201, loss: 0.907, 6432/10016 datapoints
2025-03-06 17:51:01,166 - INFO - validation batch 251, loss: 1.027, 8032/10016 datapoints
2025-03-06 17:51:01,322 - INFO - validation batch 301, loss: 0.764, 9632/10016 datapoints
2025-03-06 17:51:01,358 - INFO - Epoch 50/800 done.
2025-03-06 17:51:01,358 - INFO - Final validation performance:
Loss: 0.999, top-1 acc: 0.816top-5 acc: 0.816
2025-03-06 17:51:01,358 - INFO - Beginning epoch 51/800
2025-03-06 17:51:01,364 - INFO - training batch 1, loss: 0.903, 32/60000 datapoints
2025-03-06 17:51:01,560 - INFO - training batch 51, loss: 1.097, 1632/60000 datapoints
2025-03-06 17:51:01,757 - INFO - training batch 101, loss: 0.809, 3232/60000 datapoints
2025-03-06 17:51:01,953 - INFO - training batch 151, loss: 0.867, 4832/60000 datapoints
2025-03-06 17:51:02,147 - INFO - training batch 201, loss: 1.112, 6432/60000 datapoints
2025-03-06 17:51:02,340 - INFO - training batch 251, loss: 1.228, 8032/60000 datapoints
2025-03-06 17:51:02,552 - INFO - training batch 301, loss: 1.023, 9632/60000 datapoints
2025-03-06 17:51:02,749 - INFO - training batch 351, loss: 0.930, 11232/60000 datapoints
2025-03-06 17:51:02,943 - INFO - training batch 401, loss: 0.846, 12832/60000 datapoints
2025-03-06 17:51:03,136 - INFO - training batch 451, loss: 0.962, 14432/60000 datapoints
2025-03-06 17:51:03,329 - INFO - training batch 501, loss: 1.016, 16032/60000 datapoints
2025-03-06 17:51:03,524 - INFO - training batch 551, loss: 1.098, 17632/60000 datapoints
2025-03-06 17:51:03,719 - INFO - training batch 601, loss: 0.931, 19232/60000 datapoints
2025-03-06 17:51:03,913 - INFO - training batch 651, loss: 0.804, 20832/60000 datapoints
2025-03-06 17:51:04,107 - INFO - training batch 701, loss: 0.946, 22432/60000 datapoints
2025-03-06 17:51:04,300 - INFO - training batch 751, loss: 0.843, 24032/60000 datapoints
2025-03-06 17:51:04,494 - INFO - training batch 801, loss: 0.851, 25632/60000 datapoints
2025-03-06 17:51:04,690 - INFO - training batch 851, loss: 0.831, 27232/60000 datapoints
2025-03-06 17:51:04,891 - INFO - training batch 901, loss: 0.853, 28832/60000 datapoints
2025-03-06 17:51:05,086 - INFO - training batch 951, loss: 0.816, 30432/60000 datapoints
2025-03-06 17:51:05,279 - INFO - training batch 1001, loss: 1.103, 32032/60000 datapoints
2025-03-06 17:51:05,474 - INFO - training batch 1051, loss: 0.909, 33632/60000 datapoints
2025-03-06 17:51:05,671 - INFO - training batch 1101, loss: 0.989, 35232/60000 datapoints
2025-03-06 17:51:05,864 - INFO - training batch 1151, loss: 0.873, 36832/60000 datapoints
2025-03-06 17:51:06,058 - INFO - training batch 1201, loss: 1.136, 38432/60000 datapoints
2025-03-06 17:51:06,255 - INFO - training batch 1251, loss: 0.951, 40032/60000 datapoints
2025-03-06 17:51:06,449 - INFO - training batch 1301, loss: 1.149, 41632/60000 datapoints
2025-03-06 17:51:06,643 - INFO - training batch 1351, loss: 0.904, 43232/60000 datapoints
2025-03-06 17:51:06,838 - INFO - training batch 1401, loss: 1.025, 44832/60000 datapoints
2025-03-06 17:51:07,038 - INFO - training batch 1451, loss: 0.914, 46432/60000 datapoints
2025-03-06 17:51:07,234 - INFO - training batch 1501, loss: 0.974, 48032/60000 datapoints
2025-03-06 17:51:07,428 - INFO - training batch 1551, loss: 0.811, 49632/60000 datapoints
2025-03-06 17:51:07,623 - INFO - training batch 1601, loss: 0.862, 51232/60000 datapoints
2025-03-06 17:51:07,817 - INFO - training batch 1651, loss: 0.972, 52832/60000 datapoints
2025-03-06 17:51:08,013 - INFO - training batch 1701, loss: 0.846, 54432/60000 datapoints
2025-03-06 17:51:08,213 - INFO - training batch 1751, loss: 0.757, 56032/60000 datapoints
2025-03-06 17:51:08,408 - INFO - training batch 1801, loss: 0.913, 57632/60000 datapoints
2025-03-06 17:51:08,605 - INFO - training batch 1851, loss: 0.999, 59232/60000 datapoints
2025-03-06 17:51:08,705 - INFO - validation batch 1, loss: 1.115, 32/10016 datapoints
2025-03-06 17:51:08,860 - INFO - validation batch 51, loss: 0.962, 1632/10016 datapoints
2025-03-06 17:51:09,016 - INFO - validation batch 101, loss: 1.004, 3232/10016 datapoints
2025-03-06 17:51:09,167 - INFO - validation batch 151, loss: 0.935, 4832/10016 datapoints
2025-03-06 17:51:09,319 - INFO - validation batch 201, loss: 0.951, 6432/10016 datapoints
2025-03-06 17:51:09,472 - INFO - validation batch 251, loss: 0.815, 8032/10016 datapoints
2025-03-06 17:51:09,625 - INFO - validation batch 301, loss: 0.916, 9632/10016 datapoints
2025-03-06 17:51:09,662 - INFO - Epoch 51/800 done.
2025-03-06 17:51:09,662 - INFO - Final validation performance:
Loss: 0.957, top-1 acc: 0.819top-5 acc: 0.819
2025-03-06 17:51:09,663 - INFO - Beginning epoch 52/800
2025-03-06 17:51:09,668 - INFO - training batch 1, loss: 0.933, 32/60000 datapoints
2025-03-06 17:51:09,862 - INFO - training batch 51, loss: 0.922, 1632/60000 datapoints
2025-03-06 17:51:10,059 - INFO - training batch 101, loss: 0.782, 3232/60000 datapoints
2025-03-06 17:51:10,253 - INFO - training batch 151, loss: 0.818, 4832/60000 datapoints
2025-03-06 17:51:10,450 - INFO - training batch 201, loss: 0.904, 6432/60000 datapoints
2025-03-06 17:51:10,648 - INFO - training batch 251, loss: 0.935, 8032/60000 datapoints
2025-03-06 17:51:10,843 - INFO - training batch 301, loss: 0.856, 9632/60000 datapoints
2025-03-06 17:51:11,037 - INFO - training batch 351, loss: 0.870, 11232/60000 datapoints
2025-03-06 17:51:11,230 - INFO - training batch 401, loss: 0.898, 12832/60000 datapoints
2025-03-06 17:51:11,426 - INFO - training batch 451, loss: 0.838, 14432/60000 datapoints
2025-03-06 17:51:11,623 - INFO - training batch 501, loss: 1.102, 16032/60000 datapoints
2025-03-06 17:51:11,815 - INFO - training batch 551, loss: 0.958, 17632/60000 datapoints
2025-03-06 17:51:12,011 - INFO - training batch 601, loss: 0.957, 19232/60000 datapoints
2025-03-06 17:51:12,206 - INFO - training batch 651, loss: 0.926, 20832/60000 datapoints
2025-03-06 17:51:12,401 - INFO - training batch 701, loss: 1.102, 22432/60000 datapoints
2025-03-06 17:51:12,614 - INFO - training batch 751, loss: 0.912, 24032/60000 datapoints
2025-03-06 17:51:12,821 - INFO - training batch 801, loss: 1.195, 25632/60000 datapoints
2025-03-06 17:51:13,017 - INFO - training batch 851, loss: 0.975, 27232/60000 datapoints
2025-03-06 17:51:13,221 - INFO - training batch 901, loss: 1.053, 28832/60000 datapoints
2025-03-06 17:51:13,417 - INFO - training batch 951, loss: 0.901, 30432/60000 datapoints
2025-03-06 17:51:13,616 - INFO - training batch 1001, loss: 0.838, 32032/60000 datapoints
2025-03-06 17:51:13,808 - INFO - training batch 1051, loss: 0.916, 33632/60000 datapoints
2025-03-06 17:51:14,002 - INFO - training batch 1101, loss: 1.056, 35232/60000 datapoints
2025-03-06 17:51:14,194 - INFO - training batch 1151, loss: 0.985, 36832/60000 datapoints
2025-03-06 17:51:14,387 - INFO - training batch 1201, loss: 0.893, 38432/60000 datapoints
2025-03-06 17:51:14,582 - INFO - training batch 1251, loss: 0.925, 40032/60000 datapoints
2025-03-06 17:51:14,777 - INFO - training batch 1301, loss: 1.052, 41632/60000 datapoints
2025-03-06 17:51:14,975 - INFO - training batch 1351, loss: 0.827, 43232/60000 datapoints
2025-03-06 17:51:15,169 - INFO - training batch 1401, loss: 0.965, 44832/60000 datapoints
2025-03-06 17:51:15,372 - INFO - training batch 1451, loss: 0.861, 46432/60000 datapoints
2025-03-06 17:51:15,569 - INFO - training batch 1501, loss: 0.992, 48032/60000 datapoints
2025-03-06 17:51:15,769 - INFO - training batch 1551, loss: 0.987, 49632/60000 datapoints
2025-03-06 17:51:15,962 - INFO - training batch 1601, loss: 1.006, 51232/60000 datapoints
2025-03-06 17:51:16,158 - INFO - training batch 1651, loss: 0.777, 52832/60000 datapoints
2025-03-06 17:51:16,352 - INFO - training batch 1701, loss: 0.999, 54432/60000 datapoints
2025-03-06 17:51:16,546 - INFO - training batch 1751, loss: 1.184, 56032/60000 datapoints
2025-03-06 17:51:16,741 - INFO - training batch 1801, loss: 0.880, 57632/60000 datapoints
2025-03-06 17:51:16,936 - INFO - training batch 1851, loss: 0.712, 59232/60000 datapoints
2025-03-06 17:51:17,039 - INFO - validation batch 1, loss: 0.986, 32/10016 datapoints
2025-03-06 17:51:17,192 - INFO - validation batch 51, loss: 0.851, 1632/10016 datapoints
2025-03-06 17:51:17,358 - INFO - validation batch 101, loss: 0.836, 3232/10016 datapoints
2025-03-06 17:51:17,512 - INFO - validation batch 151, loss: 0.939, 4832/10016 datapoints
2025-03-06 17:51:17,665 - INFO - validation batch 201, loss: 0.929, 6432/10016 datapoints
2025-03-06 17:51:17,817 - INFO - validation batch 251, loss: 1.001, 8032/10016 datapoints
2025-03-06 17:51:18,070 - INFO - validation batch 301, loss: 0.921, 9632/10016 datapoints
2025-03-06 17:51:18,113 - INFO - Epoch 52/800 done.
2025-03-06 17:51:18,129 - INFO - Final validation performance:
Loss: 0.923, top-1 acc: 0.820top-5 acc: 0.820
2025-03-06 17:51:18,130 - INFO - Beginning epoch 53/800
2025-03-06 17:51:18,156 - INFO - training batch 1, loss: 1.116, 32/60000 datapoints
2025-03-06 17:51:18,398 - INFO - training batch 51, loss: 0.831, 1632/60000 datapoints
2025-03-06 17:51:18,616 - INFO - training batch 101, loss: 0.917, 3232/60000 datapoints
2025-03-06 17:51:18,811 - INFO - training batch 151, loss: 0.895, 4832/60000 datapoints
2025-03-06 17:51:19,004 - INFO - training batch 201, loss: 0.874, 6432/60000 datapoints
2025-03-06 17:51:19,199 - INFO - training batch 251, loss: 1.001, 8032/60000 datapoints
2025-03-06 17:51:19,394 - INFO - training batch 301, loss: 0.852, 9632/60000 datapoints
2025-03-06 17:51:19,589 - INFO - training batch 351, loss: 0.766, 11232/60000 datapoints
2025-03-06 17:51:19,785 - INFO - training batch 401, loss: 0.852, 12832/60000 datapoints
2025-03-06 17:51:19,979 - INFO - training batch 451, loss: 0.831, 14432/60000 datapoints
2025-03-06 17:51:20,173 - INFO - training batch 501, loss: 0.921, 16032/60000 datapoints
2025-03-06 17:51:20,368 - INFO - training batch 551, loss: 0.937, 17632/60000 datapoints
2025-03-06 17:51:20,563 - INFO - training batch 601, loss: 0.917, 19232/60000 datapoints
2025-03-06 17:51:20,757 - INFO - training batch 651, loss: 0.934, 20832/60000 datapoints
2025-03-06 17:51:20,954 - INFO - training batch 701, loss: 1.013, 22432/60000 datapoints
2025-03-06 17:51:21,146 - INFO - training batch 751, loss: 0.742, 24032/60000 datapoints
2025-03-06 17:51:21,339 - INFO - training batch 801, loss: 0.991, 25632/60000 datapoints
2025-03-06 17:51:21,533 - INFO - training batch 851, loss: 0.956, 27232/60000 datapoints
2025-03-06 17:51:21,729 - INFO - training batch 901, loss: 0.952, 28832/60000 datapoints
2025-03-06 17:51:21,925 - INFO - training batch 951, loss: 0.991, 30432/60000 datapoints
2025-03-06 17:51:22,123 - INFO - training batch 1001, loss: 0.841, 32032/60000 datapoints
2025-03-06 17:51:22,321 - INFO - training batch 1051, loss: 0.748, 33632/60000 datapoints
2025-03-06 17:51:22,515 - INFO - training batch 1101, loss: 1.002, 35232/60000 datapoints
2025-03-06 17:51:22,731 - INFO - training batch 1151, loss: 0.984, 36832/60000 datapoints
2025-03-06 17:51:22,929 - INFO - training batch 1201, loss: 0.877, 38432/60000 datapoints
2025-03-06 17:51:23,126 - INFO - training batch 1251, loss: 0.962, 40032/60000 datapoints
2025-03-06 17:51:23,319 - INFO - training batch 1301, loss: 0.979, 41632/60000 datapoints
2025-03-06 17:51:23,515 - INFO - training batch 1351, loss: 0.865, 43232/60000 datapoints
2025-03-06 17:51:23,712 - INFO - training batch 1401, loss: 0.843, 44832/60000 datapoints
2025-03-06 17:51:23,906 - INFO - training batch 1451, loss: 1.045, 46432/60000 datapoints
2025-03-06 17:51:24,099 - INFO - training batch 1501, loss: 0.974, 48032/60000 datapoints
2025-03-06 17:51:24,292 - INFO - training batch 1551, loss: 0.928, 49632/60000 datapoints
2025-03-06 17:51:24,486 - INFO - training batch 1601, loss: 0.982, 51232/60000 datapoints
2025-03-06 17:51:24,681 - INFO - training batch 1651, loss: 0.984, 52832/60000 datapoints
2025-03-06 17:51:24,881 - INFO - training batch 1701, loss: 0.911, 54432/60000 datapoints
2025-03-06 17:51:25,073 - INFO - training batch 1751, loss: 0.778, 56032/60000 datapoints
2025-03-06 17:51:25,266 - INFO - training batch 1801, loss: 0.817, 57632/60000 datapoints
2025-03-06 17:51:25,463 - INFO - training batch 1851, loss: 0.873, 59232/60000 datapoints
2025-03-06 17:51:25,566 - INFO - validation batch 1, loss: 0.841, 32/10016 datapoints
2025-03-06 17:51:25,724 - INFO - validation batch 51, loss: 0.980, 1632/10016 datapoints
2025-03-06 17:51:25,880 - INFO - validation batch 101, loss: 0.767, 3232/10016 datapoints
2025-03-06 17:51:26,031 - INFO - validation batch 151, loss: 0.824, 4832/10016 datapoints
2025-03-06 17:51:26,187 - INFO - validation batch 201, loss: 0.890, 6432/10016 datapoints
2025-03-06 17:51:26,338 - INFO - validation batch 251, loss: 0.952, 8032/10016 datapoints
2025-03-06 17:51:26,496 - INFO - validation batch 301, loss: 0.783, 9632/10016 datapoints
2025-03-06 17:51:26,533 - INFO - Epoch 53/800 done.
2025-03-06 17:51:26,534 - INFO - Final validation performance:
Loss: 0.862, top-1 acc: 0.823top-5 acc: 0.823
2025-03-06 17:51:26,534 - INFO - Beginning epoch 54/800
2025-03-06 17:51:26,539 - INFO - training batch 1, loss: 0.707, 32/60000 datapoints
2025-03-06 17:51:26,738 - INFO - training batch 51, loss: 0.921, 1632/60000 datapoints
2025-03-06 17:51:26,934 - INFO - training batch 101, loss: 0.876, 3232/60000 datapoints
2025-03-06 17:51:27,130 - INFO - training batch 151, loss: 1.015, 4832/60000 datapoints
2025-03-06 17:51:27,326 - INFO - training batch 201, loss: 0.882, 6432/60000 datapoints
2025-03-06 17:51:27,529 - INFO - training batch 251, loss: 0.923, 8032/60000 datapoints
2025-03-06 17:51:27,727 - INFO - training batch 301, loss: 0.789, 9632/60000 datapoints
2025-03-06 17:51:27,923 - INFO - training batch 351, loss: 0.930, 11232/60000 datapoints
2025-03-06 17:51:28,116 - INFO - training batch 401, loss: 1.048, 12832/60000 datapoints
2025-03-06 17:51:28,313 - INFO - training batch 451, loss: 0.908, 14432/60000 datapoints
2025-03-06 17:51:28,507 - INFO - training batch 501, loss: 0.753, 16032/60000 datapoints
2025-03-06 17:51:28,702 - INFO - training batch 551, loss: 0.966, 17632/60000 datapoints
2025-03-06 17:51:28,899 - INFO - training batch 601, loss: 1.003, 19232/60000 datapoints
2025-03-06 17:51:29,093 - INFO - training batch 651, loss: 0.803, 20832/60000 datapoints
2025-03-06 17:51:29,287 - INFO - training batch 701, loss: 1.030, 22432/60000 datapoints
2025-03-06 17:51:29,479 - INFO - training batch 751, loss: 0.836, 24032/60000 datapoints
2025-03-06 17:51:29,674 - INFO - training batch 801, loss: 0.986, 25632/60000 datapoints
2025-03-06 17:51:29,867 - INFO - training batch 851, loss: 0.734, 27232/60000 datapoints
2025-03-06 17:51:30,060 - INFO - training batch 901, loss: 0.948, 28832/60000 datapoints
2025-03-06 17:51:30,256 - INFO - training batch 951, loss: 0.758, 30432/60000 datapoints
2025-03-06 17:51:30,449 - INFO - training batch 1001, loss: 1.005, 32032/60000 datapoints
2025-03-06 17:51:30,645 - INFO - training batch 1051, loss: 1.020, 33632/60000 datapoints
2025-03-06 17:51:30,842 - INFO - training batch 1101, loss: 0.742, 35232/60000 datapoints
2025-03-06 17:51:31,034 - INFO - training batch 1151, loss: 0.970, 36832/60000 datapoints
2025-03-06 17:51:31,227 - INFO - training batch 1201, loss: 1.012, 38432/60000 datapoints
2025-03-06 17:51:31,422 - INFO - training batch 1251, loss: 0.808, 40032/60000 datapoints
2025-03-06 17:51:31,620 - INFO - training batch 1301, loss: 0.836, 41632/60000 datapoints
2025-03-06 17:51:31,814 - INFO - training batch 1351, loss: 0.860, 43232/60000 datapoints
2025-03-06 17:51:32,008 - INFO - training batch 1401, loss: 1.093, 44832/60000 datapoints
2025-03-06 17:51:32,201 - INFO - training batch 1451, loss: 1.053, 46432/60000 datapoints
2025-03-06 17:51:32,394 - INFO - training batch 1501, loss: 0.893, 48032/60000 datapoints
2025-03-06 17:51:32,588 - INFO - training batch 1551, loss: 0.753, 49632/60000 datapoints
2025-03-06 17:51:32,802 - INFO - training batch 1601, loss: 0.994, 51232/60000 datapoints
2025-03-06 17:51:32,998 - INFO - training batch 1651, loss: 0.882, 52832/60000 datapoints
2025-03-06 17:51:33,192 - INFO - training batch 1701, loss: 0.850, 54432/60000 datapoints
2025-03-06 17:51:33,387 - INFO - training batch 1751, loss: 0.705, 56032/60000 datapoints
2025-03-06 17:51:33,579 - INFO - training batch 1801, loss: 0.813, 57632/60000 datapoints
2025-03-06 17:51:33,774 - INFO - training batch 1851, loss: 0.998, 59232/60000 datapoints
2025-03-06 17:51:33,875 - INFO - validation batch 1, loss: 0.921, 32/10016 datapoints
2025-03-06 17:51:34,026 - INFO - validation batch 51, loss: 0.955, 1632/10016 datapoints
2025-03-06 17:51:34,179 - INFO - validation batch 101, loss: 0.807, 3232/10016 datapoints
2025-03-06 17:51:34,332 - INFO - validation batch 151, loss: 0.723, 4832/10016 datapoints
2025-03-06 17:51:34,483 - INFO - validation batch 201, loss: 0.793, 6432/10016 datapoints
2025-03-06 17:51:34,637 - INFO - validation batch 251, loss: 0.745, 8032/10016 datapoints
2025-03-06 17:51:34,790 - INFO - validation batch 301, loss: 0.827, 9632/10016 datapoints
2025-03-06 17:51:34,833 - INFO - Epoch 54/800 done.
2025-03-06 17:51:34,833 - INFO - Final validation performance:
Loss: 0.825, top-1 acc: 0.823top-5 acc: 0.823
2025-03-06 17:51:34,833 - INFO - Beginning epoch 55/800
2025-03-06 17:51:34,842 - INFO - training batch 1, loss: 0.996, 32/60000 datapoints
2025-03-06 17:51:35,036 - INFO - training batch 51, loss: 0.828, 1632/60000 datapoints
2025-03-06 17:51:35,230 - INFO - training batch 101, loss: 0.875, 3232/60000 datapoints
2025-03-06 17:51:35,424 - INFO - training batch 151, loss: 0.809, 4832/60000 datapoints
2025-03-06 17:51:35,621 - INFO - training batch 201, loss: 0.751, 6432/60000 datapoints
2025-03-06 17:51:35,823 - INFO - training batch 251, loss: 0.828, 8032/60000 datapoints
2025-03-06 17:51:36,015 - INFO - training batch 301, loss: 0.981, 9632/60000 datapoints
2025-03-06 17:51:36,210 - INFO - training batch 351, loss: 0.960, 11232/60000 datapoints
2025-03-06 17:51:36,403 - INFO - training batch 401, loss: 0.816, 12832/60000 datapoints
2025-03-06 17:51:36,599 - INFO - training batch 451, loss: 0.824, 14432/60000 datapoints
2025-03-06 17:51:36,792 - INFO - training batch 501, loss: 1.091, 16032/60000 datapoints
2025-03-06 17:51:36,989 - INFO - training batch 551, loss: 0.992, 17632/60000 datapoints
2025-03-06 17:51:37,182 - INFO - training batch 601, loss: 0.725, 19232/60000 datapoints
2025-03-06 17:51:37,375 - INFO - training batch 651, loss: 0.865, 20832/60000 datapoints
2025-03-06 17:51:37,566 - INFO - training batch 701, loss: 1.073, 22432/60000 datapoints
2025-03-06 17:51:37,784 - INFO - training batch 751, loss: 0.747, 24032/60000 datapoints
2025-03-06 17:51:37,979 - INFO - training batch 801, loss: 1.049, 25632/60000 datapoints
2025-03-06 17:51:38,180 - INFO - training batch 851, loss: 0.769, 27232/60000 datapoints
2025-03-06 17:51:38,377 - INFO - training batch 901, loss: 0.771, 28832/60000 datapoints
2025-03-06 17:51:38,570 - INFO - training batch 951, loss: 0.943, 30432/60000 datapoints
2025-03-06 17:51:38,764 - INFO - training batch 1001, loss: 0.836, 32032/60000 datapoints
2025-03-06 17:51:38,961 - INFO - training batch 1051, loss: 0.779, 33632/60000 datapoints
2025-03-06 17:51:39,155 - INFO - training batch 1101, loss: 0.763, 35232/60000 datapoints
2025-03-06 17:51:39,352 - INFO - training batch 1151, loss: 1.067, 36832/60000 datapoints
2025-03-06 17:51:39,545 - INFO - training batch 1201, loss: 0.846, 38432/60000 datapoints
2025-03-06 17:51:39,741 - INFO - training batch 1251, loss: 0.946, 40032/60000 datapoints
2025-03-06 17:51:39,936 - INFO - training batch 1301, loss: 0.984, 41632/60000 datapoints
2025-03-06 17:51:40,129 - INFO - training batch 1351, loss: 0.898, 43232/60000 datapoints
2025-03-06 17:51:40,327 - INFO - training batch 1401, loss: 0.827, 44832/60000 datapoints
2025-03-06 17:51:40,525 - INFO - training batch 1451, loss: 0.760, 46432/60000 datapoints
2025-03-06 17:51:40,723 - INFO - training batch 1501, loss: 0.847, 48032/60000 datapoints
2025-03-06 17:51:40,919 - INFO - training batch 1551, loss: 0.796, 49632/60000 datapoints
2025-03-06 17:51:41,112 - INFO - training batch 1601, loss: 0.889, 51232/60000 datapoints
2025-03-06 17:51:41,306 - INFO - training batch 1651, loss: 0.966, 52832/60000 datapoints
2025-03-06 17:51:41,500 - INFO - training batch 1701, loss: 0.777, 54432/60000 datapoints
2025-03-06 17:51:41,694 - INFO - training batch 1751, loss: 1.046, 56032/60000 datapoints
2025-03-06 17:51:41,887 - INFO - training batch 1801, loss: 0.804, 57632/60000 datapoints
2025-03-06 17:51:42,080 - INFO - training batch 1851, loss: 0.841, 59232/60000 datapoints
2025-03-06 17:51:42,181 - INFO - validation batch 1, loss: 0.790, 32/10016 datapoints
2025-03-06 17:51:42,337 - INFO - validation batch 51, loss: 0.843, 1632/10016 datapoints
2025-03-06 17:51:42,488 - INFO - validation batch 101, loss: 0.963, 3232/10016 datapoints
2025-03-06 17:51:42,641 - INFO - validation batch 151, loss: 0.800, 4832/10016 datapoints
2025-03-06 17:51:42,793 - INFO - validation batch 201, loss: 0.929, 6432/10016 datapoints
2025-03-06 17:51:42,967 - INFO - validation batch 251, loss: 0.829, 8032/10016 datapoints
2025-03-06 17:51:43,120 - INFO - validation batch 301, loss: 0.741, 9632/10016 datapoints
2025-03-06 17:51:43,158 - INFO - Epoch 55/800 done.
2025-03-06 17:51:43,158 - INFO - Final validation performance:
Loss: 0.842, top-1 acc: 0.825top-5 acc: 0.825
2025-03-06 17:51:43,159 - INFO - Beginning epoch 56/800
2025-03-06 17:51:43,165 - INFO - training batch 1, loss: 0.830, 32/60000 datapoints
2025-03-06 17:51:43,359 - INFO - training batch 51, loss: 0.885, 1632/60000 datapoints
2025-03-06 17:51:43,552 - INFO - training batch 101, loss: 0.880, 3232/60000 datapoints
2025-03-06 17:51:43,745 - INFO - training batch 151, loss: 0.800, 4832/60000 datapoints
2025-03-06 17:51:43,938 - INFO - training batch 201, loss: 1.006, 6432/60000 datapoints
2025-03-06 17:51:44,132 - INFO - training batch 251, loss: 0.773, 8032/60000 datapoints
2025-03-06 17:51:44,322 - INFO - training batch 301, loss: 0.821, 9632/60000 datapoints
2025-03-06 17:51:44,513 - INFO - training batch 351, loss: 0.885, 11232/60000 datapoints
2025-03-06 17:51:44,705 - INFO - training batch 401, loss: 1.066, 12832/60000 datapoints
2025-03-06 17:51:44,904 - INFO - training batch 451, loss: 0.983, 14432/60000 datapoints
2025-03-06 17:51:45,094 - INFO - training batch 501, loss: 0.717, 16032/60000 datapoints
2025-03-06 17:51:45,286 - INFO - training batch 551, loss: 1.131, 17632/60000 datapoints
2025-03-06 17:51:45,478 - INFO - training batch 601, loss: 1.049, 19232/60000 datapoints
2025-03-06 17:51:45,672 - INFO - training batch 651, loss: 0.914, 20832/60000 datapoints
2025-03-06 17:51:45,867 - INFO - training batch 701, loss: 0.865, 22432/60000 datapoints
2025-03-06 17:51:46,058 - INFO - training batch 751, loss: 0.801, 24032/60000 datapoints
2025-03-06 17:51:46,253 - INFO - training batch 801, loss: 1.007, 25632/60000 datapoints
2025-03-06 17:51:46,446 - INFO - training batch 851, loss: 0.919, 27232/60000 datapoints
2025-03-06 17:51:46,641 - INFO - training batch 901, loss: 0.794, 28832/60000 datapoints
2025-03-06 17:51:46,831 - INFO - training batch 951, loss: 0.800, 30432/60000 datapoints
2025-03-06 17:51:47,024 - INFO - training batch 1001, loss: 0.879, 32032/60000 datapoints
2025-03-06 17:51:47,214 - INFO - training batch 1051, loss: 0.818, 33632/60000 datapoints
2025-03-06 17:51:47,406 - INFO - training batch 1101, loss: 0.879, 35232/60000 datapoints
2025-03-06 17:51:47,598 - INFO - training batch 1151, loss: 0.723, 36832/60000 datapoints
2025-03-06 17:51:47,791 - INFO - training batch 1201, loss: 0.785, 38432/60000 datapoints
2025-03-06 17:51:47,982 - INFO - training batch 1251, loss: 0.978, 40032/60000 datapoints
2025-03-06 17:51:48,175 - INFO - training batch 1301, loss: 0.893, 41632/60000 datapoints
2025-03-06 17:51:48,370 - INFO - training batch 1351, loss: 0.685, 43232/60000 datapoints
2025-03-06 17:51:48,560 - INFO - training batch 1401, loss: 0.798, 44832/60000 datapoints
2025-03-06 17:51:48,751 - INFO - training batch 1451, loss: 1.094, 46432/60000 datapoints
2025-03-06 17:51:48,947 - INFO - training batch 1501, loss: 0.885, 48032/60000 datapoints
2025-03-06 17:51:49,138 - INFO - training batch 1551, loss: 0.898, 49632/60000 datapoints
2025-03-06 17:51:49,329 - INFO - training batch 1601, loss: 0.729, 51232/60000 datapoints
2025-03-06 17:51:49,528 - INFO - training batch 1651, loss: 0.850, 52832/60000 datapoints
2025-03-06 17:51:49,721 - INFO - training batch 1701, loss: 0.812, 54432/60000 datapoints
2025-03-06 17:51:49,912 - INFO - training batch 1751, loss: 0.809, 56032/60000 datapoints
2025-03-06 17:51:50,102 - INFO - training batch 1801, loss: 1.080, 57632/60000 datapoints
2025-03-06 17:51:50,293 - INFO - training batch 1851, loss: 0.854, 59232/60000 datapoints
2025-03-06 17:51:50,392 - INFO - validation batch 1, loss: 0.958, 32/10016 datapoints
2025-03-06 17:51:50,543 - INFO - validation batch 51, loss: 0.785, 1632/10016 datapoints
2025-03-06 17:51:50,695 - INFO - validation batch 101, loss: 0.773, 3232/10016 datapoints
2025-03-06 17:51:50,844 - INFO - validation batch 151, loss: 0.926, 4832/10016 datapoints
2025-03-06 17:51:50,995 - INFO - validation batch 201, loss: 0.711, 6432/10016 datapoints
2025-03-06 17:51:51,145 - INFO - validation batch 251, loss: 0.753, 8032/10016 datapoints
2025-03-06 17:51:51,293 - INFO - validation batch 301, loss: 0.951, 9632/10016 datapoints
2025-03-06 17:51:51,329 - INFO - Epoch 56/800 done.
2025-03-06 17:51:51,329 - INFO - Final validation performance:
Loss: 0.837, top-1 acc: 0.826top-5 acc: 0.826
2025-03-06 17:51:51,330 - INFO - Beginning epoch 57/800
2025-03-06 17:51:51,335 - INFO - training batch 1, loss: 0.863, 32/60000 datapoints
2025-03-06 17:51:51,528 - INFO - training batch 51, loss: 1.121, 1632/60000 datapoints
2025-03-06 17:51:51,723 - INFO - training batch 101, loss: 0.666, 3232/60000 datapoints
2025-03-06 17:51:51,914 - INFO - training batch 151, loss: 0.785, 4832/60000 datapoints
2025-03-06 17:51:52,107 - INFO - training batch 201, loss: 0.781, 6432/60000 datapoints
2025-03-06 17:51:52,298 - INFO - training batch 251, loss: 0.876, 8032/60000 datapoints
2025-03-06 17:51:52,490 - INFO - training batch 301, loss: 1.111, 9632/60000 datapoints
2025-03-06 17:51:52,685 - INFO - training batch 351, loss: 0.896, 11232/60000 datapoints
2025-03-06 17:51:52,874 - INFO - training batch 401, loss: 0.873, 12832/60000 datapoints
2025-03-06 17:51:53,086 - INFO - training batch 451, loss: 0.745, 14432/60000 datapoints
2025-03-06 17:51:53,286 - INFO - training batch 501, loss: 0.756, 16032/60000 datapoints
2025-03-06 17:51:53,482 - INFO - training batch 551, loss: 0.912, 17632/60000 datapoints
2025-03-06 17:51:53,677 - INFO - training batch 601, loss: 0.829, 19232/60000 datapoints
2025-03-06 17:51:53,868 - INFO - training batch 651, loss: 0.814, 20832/60000 datapoints
2025-03-06 17:51:54,060 - INFO - training batch 701, loss: 0.971, 22432/60000 datapoints
2025-03-06 17:51:54,253 - INFO - training batch 751, loss: 0.781, 24032/60000 datapoints
2025-03-06 17:51:54,445 - INFO - training batch 801, loss: 0.945, 25632/60000 datapoints
2025-03-06 17:51:54,639 - INFO - training batch 851, loss: 0.854, 27232/60000 datapoints
2025-03-06 17:51:54,832 - INFO - training batch 901, loss: 0.884, 28832/60000 datapoints
2025-03-06 17:51:55,029 - INFO - training batch 951, loss: 0.827, 30432/60000 datapoints
2025-03-06 17:51:55,221 - INFO - training batch 1001, loss: 0.892, 32032/60000 datapoints
2025-03-06 17:51:55,414 - INFO - training batch 1051, loss: 0.689, 33632/60000 datapoints
2025-03-06 17:51:55,610 - INFO - training batch 1101, loss: 0.856, 35232/60000 datapoints
2025-03-06 17:51:55,804 - INFO - training batch 1151, loss: 0.778, 36832/60000 datapoints
2025-03-06 17:51:55,996 - INFO - training batch 1201, loss: 0.984, 38432/60000 datapoints
2025-03-06 17:51:56,190 - INFO - training batch 1251, loss: 0.973, 40032/60000 datapoints
2025-03-06 17:51:56,383 - INFO - training batch 1301, loss: 0.978, 41632/60000 datapoints
2025-03-06 17:51:56,578 - INFO - training batch 1351, loss: 0.710, 43232/60000 datapoints
2025-03-06 17:51:56,772 - INFO - training batch 1401, loss: 0.821, 44832/60000 datapoints
2025-03-06 17:51:56,969 - INFO - training batch 1451, loss: 0.851, 46432/60000 datapoints
2025-03-06 17:51:57,160 - INFO - training batch 1501, loss: 0.958, 48032/60000 datapoints
2025-03-06 17:51:57,351 - INFO - training batch 1551, loss: 0.933, 49632/60000 datapoints
2025-03-06 17:51:57,542 - INFO - training batch 1601, loss: 0.665, 51232/60000 datapoints
2025-03-06 17:51:57,735 - INFO - training batch 1651, loss: 0.693, 52832/60000 datapoints
2025-03-06 17:51:57,927 - INFO - training batch 1701, loss: 0.805, 54432/60000 datapoints
2025-03-06 17:51:58,119 - INFO - training batch 1751, loss: 0.675, 56032/60000 datapoints
2025-03-06 17:51:58,310 - INFO - training batch 1801, loss: 0.905, 57632/60000 datapoints
2025-03-06 17:51:58,503 - INFO - training batch 1851, loss: 0.855, 59232/60000 datapoints
2025-03-06 17:51:58,604 - INFO - validation batch 1, loss: 0.983, 32/10016 datapoints
2025-03-06 17:51:58,753 - INFO - validation batch 51, loss: 0.758, 1632/10016 datapoints
2025-03-06 17:51:58,905 - INFO - validation batch 101, loss: 0.811, 3232/10016 datapoints
2025-03-06 17:51:59,059 - INFO - validation batch 151, loss: 0.827, 4832/10016 datapoints
2025-03-06 17:51:59,208 - INFO - validation batch 201, loss: 0.839, 6432/10016 datapoints
2025-03-06 17:51:59,358 - INFO - validation batch 251, loss: 0.703, 8032/10016 datapoints
2025-03-06 17:51:59,511 - INFO - validation batch 301, loss: 0.953, 9632/10016 datapoints
2025-03-06 17:51:59,548 - INFO - Epoch 57/800 done.
2025-03-06 17:51:59,549 - INFO - Final validation performance:
Loss: 0.839, top-1 acc: 0.828top-5 acc: 0.828
2025-03-06 17:51:59,549 - INFO - Beginning epoch 58/800
2025-03-06 17:51:59,555 - INFO - training batch 1, loss: 0.722, 32/60000 datapoints
2025-03-06 17:51:59,752 - INFO - training batch 51, loss: 0.777, 1632/60000 datapoints
2025-03-06 17:51:59,945 - INFO - training batch 101, loss: 0.768, 3232/60000 datapoints
2025-03-06 17:52:00,141 - INFO - training batch 151, loss: 0.632, 4832/60000 datapoints
2025-03-06 17:52:00,336 - INFO - training batch 201, loss: 0.772, 6432/60000 datapoints
2025-03-06 17:52:00,533 - INFO - training batch 251, loss: 0.925, 8032/60000 datapoints
2025-03-06 17:52:00,733 - INFO - training batch 301, loss: 0.728, 9632/60000 datapoints
2025-03-06 17:52:00,927 - INFO - training batch 351, loss: 0.891, 11232/60000 datapoints
2025-03-06 17:52:01,124 - INFO - training batch 401, loss: 0.844, 12832/60000 datapoints
2025-03-06 17:52:01,316 - INFO - training batch 451, loss: 0.818, 14432/60000 datapoints
2025-03-06 17:52:01,508 - INFO - training batch 501, loss: 0.908, 16032/60000 datapoints
2025-03-06 17:52:01,707 - INFO - training batch 551, loss: 1.064, 17632/60000 datapoints
2025-03-06 17:52:01,917 - INFO - training batch 601, loss: 0.766, 19232/60000 datapoints
2025-03-06 17:52:02,142 - INFO - training batch 651, loss: 0.797, 20832/60000 datapoints
2025-03-06 17:52:02,352 - INFO - training batch 701, loss: 0.984, 22432/60000 datapoints
2025-03-06 17:52:02,563 - INFO - training batch 751, loss: 0.822, 24032/60000 datapoints
2025-03-06 17:52:02,764 - INFO - training batch 801, loss: 0.821, 25632/60000 datapoints
2025-03-06 17:52:02,966 - INFO - training batch 851, loss: 0.715, 27232/60000 datapoints
2025-03-06 17:52:03,183 - INFO - training batch 901, loss: 1.043, 28832/60000 datapoints
2025-03-06 17:52:03,382 - INFO - training batch 951, loss: 0.759, 30432/60000 datapoints
2025-03-06 17:52:03,577 - INFO - training batch 1001, loss: 0.783, 32032/60000 datapoints
2025-03-06 17:52:03,778 - INFO - training batch 1051, loss: 0.779, 33632/60000 datapoints
2025-03-06 17:52:03,973 - INFO - training batch 1101, loss: 0.935, 35232/60000 datapoints
2025-03-06 17:52:04,166 - INFO - training batch 1151, loss: 0.851, 36832/60000 datapoints
2025-03-06 17:52:04,358 - INFO - training batch 1201, loss: 0.987, 38432/60000 datapoints
2025-03-06 17:52:04,549 - INFO - training batch 1251, loss: 0.680, 40032/60000 datapoints
2025-03-06 17:52:04,744 - INFO - training batch 1301, loss: 0.805, 41632/60000 datapoints
2025-03-06 17:52:04,943 - INFO - training batch 1351, loss: 0.811, 43232/60000 datapoints
2025-03-06 17:52:05,139 - INFO - training batch 1401, loss: 0.770, 44832/60000 datapoints
2025-03-06 17:52:05,332 - INFO - training batch 1451, loss: 0.866, 46432/60000 datapoints
2025-03-06 17:52:05,524 - INFO - training batch 1501, loss: 0.889, 48032/60000 datapoints
2025-03-06 17:52:05,722 - INFO - training batch 1551, loss: 0.706, 49632/60000 datapoints
2025-03-06 17:52:05,918 - INFO - training batch 1601, loss: 0.748, 51232/60000 datapoints
2025-03-06 17:52:06,112 - INFO - training batch 1651, loss: 0.683, 52832/60000 datapoints
2025-03-06 17:52:06,309 - INFO - training batch 1701, loss: 0.992, 54432/60000 datapoints
2025-03-06 17:52:06,503 - INFO - training batch 1751, loss: 0.751, 56032/60000 datapoints
2025-03-06 17:52:06,701 - INFO - training batch 1801, loss: 0.803, 57632/60000 datapoints
2025-03-06 17:52:06,897 - INFO - training batch 1851, loss: 0.786, 59232/60000 datapoints
2025-03-06 17:52:07,002 - INFO - validation batch 1, loss: 0.779, 32/10016 datapoints
2025-03-06 17:52:07,154 - INFO - validation batch 51, loss: 0.959, 1632/10016 datapoints
2025-03-06 17:52:07,306 - INFO - validation batch 101, loss: 0.951, 3232/10016 datapoints
2025-03-06 17:52:07,458 - INFO - validation batch 151, loss: 0.858, 4832/10016 datapoints
2025-03-06 17:52:07,613 - INFO - validation batch 201, loss: 0.865, 6432/10016 datapoints
2025-03-06 17:52:07,768 - INFO - validation batch 251, loss: 0.836, 8032/10016 datapoints
2025-03-06 17:52:07,920 - INFO - validation batch 301, loss: 0.868, 9632/10016 datapoints
2025-03-06 17:52:07,957 - INFO - Epoch 58/800 done.
2025-03-06 17:52:07,957 - INFO - Final validation performance:
Loss: 0.874, top-1 acc: 0.829top-5 acc: 0.829
2025-03-06 17:52:07,957 - INFO - Beginning epoch 59/800
2025-03-06 17:52:07,964 - INFO - training batch 1, loss: 1.151, 32/60000 datapoints
2025-03-06 17:52:08,162 - INFO - training batch 51, loss: 0.798, 1632/60000 datapoints
2025-03-06 17:52:08,355 - INFO - training batch 101, loss: 0.802, 3232/60000 datapoints
2025-03-06 17:52:08,547 - INFO - training batch 151, loss: 0.742, 4832/60000 datapoints
2025-03-06 17:52:08,740 - INFO - training batch 201, loss: 0.903, 6432/60000 datapoints
2025-03-06 17:52:08,940 - INFO - training batch 251, loss: 0.678, 8032/60000 datapoints
2025-03-06 17:52:09,138 - INFO - training batch 301, loss: 0.717, 9632/60000 datapoints
2025-03-06 17:52:09,331 - INFO - training batch 351, loss: 0.863, 11232/60000 datapoints
2025-03-06 17:52:09,520 - INFO - training batch 401, loss: 0.816, 12832/60000 datapoints
2025-03-06 17:52:09,713 - INFO - training batch 451, loss: 0.853, 14432/60000 datapoints
2025-03-06 17:52:09,906 - INFO - training batch 501, loss: 0.760, 16032/60000 datapoints
2025-03-06 17:52:10,096 - INFO - training batch 551, loss: 0.944, 17632/60000 datapoints
2025-03-06 17:52:10,289 - INFO - training batch 601, loss: 0.960, 19232/60000 datapoints
2025-03-06 17:52:10,482 - INFO - training batch 651, loss: 0.648, 20832/60000 datapoints
2025-03-06 17:52:10,678 - INFO - training batch 701, loss: 0.862, 22432/60000 datapoints
2025-03-06 17:52:10,871 - INFO - training batch 751, loss: 0.623, 24032/60000 datapoints
2025-03-06 17:52:11,065 - INFO - training batch 801, loss: 0.747, 25632/60000 datapoints
2025-03-06 17:52:11,256 - INFO - training batch 851, loss: 0.770, 27232/60000 datapoints
2025-03-06 17:52:11,447 - INFO - training batch 901, loss: 0.778, 28832/60000 datapoints
2025-03-06 17:52:11,639 - INFO - training batch 951, loss: 0.730, 30432/60000 datapoints
2025-03-06 17:52:11,832 - INFO - training batch 1001, loss: 0.766, 32032/60000 datapoints
2025-03-06 17:52:12,023 - INFO - training batch 1051, loss: 0.792, 33632/60000 datapoints
2025-03-06 17:52:12,216 - INFO - training batch 1101, loss: 0.746, 35232/60000 datapoints
2025-03-06 17:52:12,407 - INFO - training batch 1151, loss: 0.821, 36832/60000 datapoints
2025-03-06 17:52:12,600 - INFO - training batch 1201, loss: 0.577, 38432/60000 datapoints
2025-03-06 17:52:12,792 - INFO - training batch 1251, loss: 0.766, 40032/60000 datapoints
2025-03-06 17:52:12,984 - INFO - training batch 1301, loss: 0.771, 41632/60000 datapoints
2025-03-06 17:52:13,196 - INFO - training batch 1351, loss: 0.933, 43232/60000 datapoints
2025-03-06 17:52:13,388 - INFO - training batch 1401, loss: 0.684, 44832/60000 datapoints
2025-03-06 17:52:13,580 - INFO - training batch 1451, loss: 0.834, 46432/60000 datapoints
2025-03-06 17:52:13,774 - INFO - training batch 1501, loss: 0.717, 48032/60000 datapoints
2025-03-06 17:52:13,966 - INFO - training batch 1551, loss: 0.655, 49632/60000 datapoints
2025-03-06 17:52:14,160 - INFO - training batch 1601, loss: 0.657, 51232/60000 datapoints
2025-03-06 17:52:14,353 - INFO - training batch 1651, loss: 0.606, 52832/60000 datapoints
2025-03-06 17:52:14,545 - INFO - training batch 1701, loss: 0.784, 54432/60000 datapoints
2025-03-06 17:52:14,738 - INFO - training batch 1751, loss: 1.083, 56032/60000 datapoints
2025-03-06 17:52:14,934 - INFO - training batch 1801, loss: 0.773, 57632/60000 datapoints
2025-03-06 17:52:15,128 - INFO - training batch 1851, loss: 0.838, 59232/60000 datapoints
2025-03-06 17:52:15,227 - INFO - validation batch 1, loss: 0.859, 32/10016 datapoints
2025-03-06 17:52:15,375 - INFO - validation batch 51, loss: 1.046, 1632/10016 datapoints
2025-03-06 17:52:15,524 - INFO - validation batch 101, loss: 0.855, 3232/10016 datapoints
2025-03-06 17:52:15,677 - INFO - validation batch 151, loss: 0.837, 4832/10016 datapoints
2025-03-06 17:52:15,832 - INFO - validation batch 201, loss: 0.691, 6432/10016 datapoints
2025-03-06 17:52:15,983 - INFO - validation batch 251, loss: 0.678, 8032/10016 datapoints
2025-03-06 17:52:16,134 - INFO - validation batch 301, loss: 0.712, 9632/10016 datapoints
2025-03-06 17:52:16,172 - INFO - Epoch 59/800 done.
2025-03-06 17:52:16,172 - INFO - Final validation performance:
Loss: 0.811, top-1 acc: 0.831top-5 acc: 0.831
2025-03-06 17:52:16,172 - INFO - Beginning epoch 60/800
2025-03-06 17:52:16,178 - INFO - training batch 1, loss: 0.878, 32/60000 datapoints
2025-03-06 17:52:16,370 - INFO - training batch 51, loss: 0.712, 1632/60000 datapoints
2025-03-06 17:52:16,559 - INFO - training batch 101, loss: 0.783, 3232/60000 datapoints
2025-03-06 17:52:16,754 - INFO - training batch 151, loss: 0.897, 4832/60000 datapoints
2025-03-06 17:52:16,945 - INFO - training batch 201, loss: 0.800, 6432/60000 datapoints
2025-03-06 17:52:17,138 - INFO - training batch 251, loss: 0.839, 8032/60000 datapoints
2025-03-06 17:52:17,329 - INFO - training batch 301, loss: 0.752, 9632/60000 datapoints
2025-03-06 17:52:17,519 - INFO - training batch 351, loss: 0.878, 11232/60000 datapoints
2025-03-06 17:52:17,715 - INFO - training batch 401, loss: 0.891, 12832/60000 datapoints
2025-03-06 17:52:17,906 - INFO - training batch 451, loss: 0.794, 14432/60000 datapoints
2025-03-06 17:52:18,098 - INFO - training batch 501, loss: 0.817, 16032/60000 datapoints
2025-03-06 17:52:18,290 - INFO - training batch 551, loss: 0.812, 17632/60000 datapoints
2025-03-06 17:52:18,480 - INFO - training batch 601, loss: 0.688, 19232/60000 datapoints
2025-03-06 17:52:18,674 - INFO - training batch 651, loss: 0.725, 20832/60000 datapoints
2025-03-06 17:52:18,895 - INFO - training batch 701, loss: 1.016, 22432/60000 datapoints
2025-03-06 17:52:19,089 - INFO - training batch 751, loss: 0.682, 24032/60000 datapoints
2025-03-06 17:52:19,283 - INFO - training batch 801, loss: 0.743, 25632/60000 datapoints
2025-03-06 17:52:19,473 - INFO - training batch 851, loss: 0.798, 27232/60000 datapoints
2025-03-06 17:52:19,673 - INFO - training batch 901, loss: 0.889, 28832/60000 datapoints
2025-03-06 17:52:19,870 - INFO - training batch 951, loss: 0.883, 30432/60000 datapoints
2025-03-06 17:52:20,078 - INFO - training batch 1001, loss: 0.754, 32032/60000 datapoints
2025-03-06 17:52:20,277 - INFO - training batch 1051, loss: 0.829, 33632/60000 datapoints
2025-03-06 17:52:20,471 - INFO - training batch 1101, loss: 0.708, 35232/60000 datapoints
2025-03-06 17:52:20,680 - INFO - training batch 1151, loss: 0.741, 36832/60000 datapoints
2025-03-06 17:52:20,885 - INFO - training batch 1201, loss: 0.733, 38432/60000 datapoints
2025-03-06 17:52:21,082 - INFO - training batch 1251, loss: 0.801, 40032/60000 datapoints
2025-03-06 17:52:21,276 - INFO - training batch 1301, loss: 0.775, 41632/60000 datapoints
2025-03-06 17:52:21,472 - INFO - training batch 1351, loss: 0.803, 43232/60000 datapoints
2025-03-06 17:52:21,669 - INFO - training batch 1401, loss: 0.758, 44832/60000 datapoints
2025-03-06 17:52:21,863 - INFO - training batch 1451, loss: 0.675, 46432/60000 datapoints
2025-03-06 17:52:22,057 - INFO - training batch 1501, loss: 0.881, 48032/60000 datapoints
2025-03-06 17:52:22,252 - INFO - training batch 1551, loss: 0.776, 49632/60000 datapoints
2025-03-06 17:52:22,447 - INFO - training batch 1601, loss: 1.045, 51232/60000 datapoints
2025-03-06 17:52:22,643 - INFO - training batch 1651, loss: 0.791, 52832/60000 datapoints
2025-03-06 17:52:22,841 - INFO - training batch 1701, loss: 0.770, 54432/60000 datapoints
2025-03-06 17:52:23,036 - INFO - training batch 1751, loss: 0.737, 56032/60000 datapoints
2025-03-06 17:52:23,251 - INFO - training batch 1801, loss: 0.710, 57632/60000 datapoints
2025-03-06 17:52:23,447 - INFO - training batch 1851, loss: 0.711, 59232/60000 datapoints
2025-03-06 17:52:23,548 - INFO - validation batch 1, loss: 0.805, 32/10016 datapoints
2025-03-06 17:52:23,701 - INFO - validation batch 51, loss: 0.836, 1632/10016 datapoints
2025-03-06 17:52:23,861 - INFO - validation batch 101, loss: 0.750, 3232/10016 datapoints
2025-03-06 17:52:24,013 - INFO - validation batch 151, loss: 0.997, 4832/10016 datapoints
2025-03-06 17:52:24,168 - INFO - validation batch 201, loss: 0.709, 6432/10016 datapoints
2025-03-06 17:52:24,320 - INFO - validation batch 251, loss: 0.932, 8032/10016 datapoints
2025-03-06 17:52:24,474 - INFO - validation batch 301, loss: 0.670, 9632/10016 datapoints
2025-03-06 17:52:24,512 - INFO - Epoch 60/800 done.
2025-03-06 17:52:24,512 - INFO - Final validation performance:
Loss: 0.814, top-1 acc: 0.833top-5 acc: 0.833
2025-03-06 17:52:24,512 - INFO - Beginning epoch 61/800
2025-03-06 17:52:24,518 - INFO - training batch 1, loss: 1.035, 32/60000 datapoints
2025-03-06 17:52:24,714 - INFO - training batch 51, loss: 0.828, 1632/60000 datapoints
2025-03-06 17:52:24,915 - INFO - training batch 101, loss: 0.884, 3232/60000 datapoints
2025-03-06 17:52:25,111 - INFO - training batch 151, loss: 0.645, 4832/60000 datapoints
2025-03-06 17:52:25,304 - INFO - training batch 201, loss: 0.626, 6432/60000 datapoints
2025-03-06 17:52:25,500 - INFO - training batch 251, loss: 0.802, 8032/60000 datapoints
2025-03-06 17:52:25,696 - INFO - training batch 301, loss: 0.552, 9632/60000 datapoints
2025-03-06 17:52:25,894 - INFO - training batch 351, loss: 0.835, 11232/60000 datapoints
2025-03-06 17:52:26,089 - INFO - training batch 401, loss: 0.817, 12832/60000 datapoints
2025-03-06 17:52:26,287 - INFO - training batch 451, loss: 0.783, 14432/60000 datapoints
2025-03-06 17:52:26,481 - INFO - training batch 501, loss: 0.889, 16032/60000 datapoints
2025-03-06 17:52:26,679 - INFO - training batch 551, loss: 0.743, 17632/60000 datapoints
2025-03-06 17:52:26,881 - INFO - training batch 601, loss: 0.900, 19232/60000 datapoints
2025-03-06 17:52:27,078 - INFO - training batch 651, loss: 0.748, 20832/60000 datapoints
2025-03-06 17:52:27,272 - INFO - training batch 701, loss: 0.793, 22432/60000 datapoints
2025-03-06 17:52:27,470 - INFO - training batch 751, loss: 0.794, 24032/60000 datapoints
2025-03-06 17:52:27,669 - INFO - training batch 801, loss: 0.723, 25632/60000 datapoints
2025-03-06 17:52:27,866 - INFO - training batch 851, loss: 0.662, 27232/60000 datapoints
2025-03-06 17:52:28,059 - INFO - training batch 901, loss: 0.656, 28832/60000 datapoints
2025-03-06 17:52:28,255 - INFO - training batch 951, loss: 0.915, 30432/60000 datapoints
2025-03-06 17:52:28,453 - INFO - training batch 1001, loss: 0.893, 32032/60000 datapoints
2025-03-06 17:52:28,647 - INFO - training batch 1051, loss: 0.886, 33632/60000 datapoints
2025-03-06 17:52:28,843 - INFO - training batch 1101, loss: 0.868, 35232/60000 datapoints
2025-03-06 17:52:29,043 - INFO - training batch 1151, loss: 0.728, 36832/60000 datapoints
2025-03-06 17:52:29,236 - INFO - training batch 1201, loss: 0.732, 38432/60000 datapoints
2025-03-06 17:52:29,432 - INFO - training batch 1251, loss: 0.632, 40032/60000 datapoints
2025-03-06 17:52:29,628 - INFO - training batch 1301, loss: 0.803, 41632/60000 datapoints
2025-03-06 17:52:29,823 - INFO - training batch 1351, loss: 0.764, 43232/60000 datapoints
2025-03-06 17:52:30,019 - INFO - training batch 1401, loss: 0.761, 44832/60000 datapoints
2025-03-06 17:52:30,215 - INFO - training batch 1451, loss: 0.740, 46432/60000 datapoints
2025-03-06 17:52:30,412 - INFO - training batch 1501, loss: 0.684, 48032/60000 datapoints
2025-03-06 17:52:30,608 - INFO - training batch 1551, loss: 0.676, 49632/60000 datapoints
2025-03-06 17:52:30,803 - INFO - training batch 1601, loss: 0.634, 51232/60000 datapoints
2025-03-06 17:52:30,997 - INFO - training batch 1651, loss: 0.766, 52832/60000 datapoints
2025-03-06 17:52:31,194 - INFO - training batch 1701, loss: 0.769, 54432/60000 datapoints
2025-03-06 17:52:31,392 - INFO - training batch 1751, loss: 0.916, 56032/60000 datapoints
2025-03-06 17:52:31,585 - INFO - training batch 1801, loss: 0.786, 57632/60000 datapoints
2025-03-06 17:52:31,784 - INFO - training batch 1851, loss: 0.763, 59232/60000 datapoints
2025-03-06 17:52:31,885 - INFO - validation batch 1, loss: 0.811, 32/10016 datapoints
2025-03-06 17:52:32,039 - INFO - validation batch 51, loss: 0.610, 1632/10016 datapoints
2025-03-06 17:52:32,193 - INFO - validation batch 101, loss: 0.813, 3232/10016 datapoints
2025-03-06 17:52:32,343 - INFO - validation batch 151, loss: 0.768, 4832/10016 datapoints
2025-03-06 17:52:32,498 - INFO - validation batch 201, loss: 0.762, 6432/10016 datapoints
2025-03-06 17:52:32,651 - INFO - validation batch 251, loss: 0.574, 8032/10016 datapoints
2025-03-06 17:52:32,804 - INFO - validation batch 301, loss: 0.780, 9632/10016 datapoints
2025-03-06 17:52:32,841 - INFO - Epoch 61/800 done.
2025-03-06 17:52:32,841 - INFO - Final validation performance:
Loss: 0.731, top-1 acc: 0.835top-5 acc: 0.835
2025-03-06 17:52:32,842 - INFO - Beginning epoch 62/800
2025-03-06 17:52:32,849 - INFO - training batch 1, loss: 0.716, 32/60000 datapoints
2025-03-06 17:52:33,061 - INFO - training batch 51, loss: 0.726, 1632/60000 datapoints
2025-03-06 17:52:33,282 - INFO - training batch 101, loss: 0.611, 3232/60000 datapoints
2025-03-06 17:52:33,479 - INFO - training batch 151, loss: 0.662, 4832/60000 datapoints
2025-03-06 17:52:33,674 - INFO - training batch 201, loss: 0.600, 6432/60000 datapoints
2025-03-06 17:52:33,869 - INFO - training batch 251, loss: 0.804, 8032/60000 datapoints
2025-03-06 17:52:34,062 - INFO - training batch 301, loss: 0.630, 9632/60000 datapoints
2025-03-06 17:52:34,254 - INFO - training batch 351, loss: 0.791, 11232/60000 datapoints
2025-03-06 17:52:34,449 - INFO - training batch 401, loss: 0.648, 12832/60000 datapoints
2025-03-06 17:52:34,646 - INFO - training batch 451, loss: 0.785, 14432/60000 datapoints
2025-03-06 17:52:34,842 - INFO - training batch 501, loss: 0.703, 16032/60000 datapoints
2025-03-06 17:52:35,037 - INFO - training batch 551, loss: 0.765, 17632/60000 datapoints
2025-03-06 17:52:35,233 - INFO - training batch 601, loss: 0.734, 19232/60000 datapoints
2025-03-06 17:52:35,426 - INFO - training batch 651, loss: 0.821, 20832/60000 datapoints
2025-03-06 17:52:35,620 - INFO - training batch 701, loss: 0.993, 22432/60000 datapoints
2025-03-06 17:52:35,813 - INFO - training batch 751, loss: 0.776, 24032/60000 datapoints
2025-03-06 17:52:36,013 - INFO - training batch 801, loss: 0.770, 25632/60000 datapoints
2025-03-06 17:52:36,207 - INFO - training batch 851, loss: 0.693, 27232/60000 datapoints
2025-03-06 17:52:36,401 - INFO - training batch 901, loss: 0.854, 28832/60000 datapoints
2025-03-06 17:52:36,597 - INFO - training batch 951, loss: 0.916, 30432/60000 datapoints
2025-03-06 17:52:36,793 - INFO - training batch 1001, loss: 0.770, 32032/60000 datapoints
2025-03-06 17:52:36,987 - INFO - training batch 1051, loss: 0.732, 33632/60000 datapoints
2025-03-06 17:52:37,183 - INFO - training batch 1101, loss: 0.903, 35232/60000 datapoints
2025-03-06 17:52:37,375 - INFO - training batch 1151, loss: 0.733, 36832/60000 datapoints
2025-03-06 17:52:37,570 - INFO - training batch 1201, loss: 0.872, 38432/60000 datapoints
2025-03-06 17:52:37,780 - INFO - training batch 1251, loss: 0.905, 40032/60000 datapoints
2025-03-06 17:52:37,974 - INFO - training batch 1301, loss: 0.645, 41632/60000 datapoints
2025-03-06 17:52:38,169 - INFO - training batch 1351, loss: 0.554, 43232/60000 datapoints
2025-03-06 17:52:38,363 - INFO - training batch 1401, loss: 0.857, 44832/60000 datapoints
2025-03-06 17:52:38,559 - INFO - training batch 1451, loss: 0.797, 46432/60000 datapoints
2025-03-06 17:52:38,754 - INFO - training batch 1501, loss: 0.738, 48032/60000 datapoints
2025-03-06 17:52:38,950 - INFO - training batch 1551, loss: 0.947, 49632/60000 datapoints
2025-03-06 17:52:39,155 - INFO - training batch 1601, loss: 0.725, 51232/60000 datapoints
2025-03-06 17:52:39,351 - INFO - training batch 1651, loss: 0.667, 52832/60000 datapoints
2025-03-06 17:52:39,548 - INFO - training batch 1701, loss: 0.757, 54432/60000 datapoints
2025-03-06 17:52:39,746 - INFO - training batch 1751, loss: 0.750, 56032/60000 datapoints
2025-03-06 17:52:39,941 - INFO - training batch 1801, loss: 0.813, 57632/60000 datapoints
2025-03-06 17:52:40,137 - INFO - training batch 1851, loss: 0.631, 59232/60000 datapoints
2025-03-06 17:52:40,240 - INFO - validation batch 1, loss: 0.776, 32/10016 datapoints
2025-03-06 17:52:40,392 - INFO - validation batch 51, loss: 0.819, 1632/10016 datapoints
2025-03-06 17:52:40,548 - INFO - validation batch 101, loss: 0.877, 3232/10016 datapoints
2025-03-06 17:52:40,705 - INFO - validation batch 151, loss: 0.797, 4832/10016 datapoints
2025-03-06 17:52:40,859 - INFO - validation batch 201, loss: 0.782, 6432/10016 datapoints
2025-03-06 17:52:41,016 - INFO - validation batch 251, loss: 0.636, 8032/10016 datapoints
2025-03-06 17:52:41,169 - INFO - validation batch 301, loss: 0.540, 9632/10016 datapoints
2025-03-06 17:52:41,206 - INFO - Epoch 62/800 done.
2025-03-06 17:52:41,206 - INFO - Final validation performance:
Loss: 0.747, top-1 acc: 0.837top-5 acc: 0.837
2025-03-06 17:52:41,207 - INFO - Beginning epoch 63/800
2025-03-06 17:52:41,213 - INFO - training batch 1, loss: 0.795, 32/60000 datapoints
2025-03-06 17:52:41,405 - INFO - training batch 51, loss: 0.676, 1632/60000 datapoints
2025-03-06 17:52:41,599 - INFO - training batch 101, loss: 0.949, 3232/60000 datapoints
2025-03-06 17:52:41,789 - INFO - training batch 151, loss: 0.887, 4832/60000 datapoints
2025-03-06 17:52:41,979 - INFO - training batch 201, loss: 0.632, 6432/60000 datapoints
2025-03-06 17:52:42,176 - INFO - training batch 251, loss: 0.812, 8032/60000 datapoints
2025-03-06 17:52:42,366 - INFO - training batch 301, loss: 0.756, 9632/60000 datapoints
2025-03-06 17:52:42,556 - INFO - training batch 351, loss: 0.756, 11232/60000 datapoints
2025-03-06 17:52:42,749 - INFO - training batch 401, loss: 0.853, 12832/60000 datapoints
2025-03-06 17:52:42,942 - INFO - training batch 451, loss: 0.788, 14432/60000 datapoints
2025-03-06 17:52:43,138 - INFO - training batch 501, loss: 0.709, 16032/60000 datapoints
2025-03-06 17:52:43,340 - INFO - training batch 551, loss: 0.703, 17632/60000 datapoints
2025-03-06 17:52:43,542 - INFO - training batch 601, loss: 0.795, 19232/60000 datapoints
2025-03-06 17:52:43,736 - INFO - training batch 651, loss: 0.666, 20832/60000 datapoints
2025-03-06 17:52:43,926 - INFO - training batch 701, loss: 0.803, 22432/60000 datapoints
2025-03-06 17:52:44,119 - INFO - training batch 751, loss: 0.689, 24032/60000 datapoints
2025-03-06 17:52:44,309 - INFO - training batch 801, loss: 0.833, 25632/60000 datapoints
2025-03-06 17:52:44,500 - INFO - training batch 851, loss: 0.802, 27232/60000 datapoints
2025-03-06 17:52:44,693 - INFO - training batch 901, loss: 0.865, 28832/60000 datapoints
2025-03-06 17:52:44,890 - INFO - training batch 951, loss: 0.645, 30432/60000 datapoints
2025-03-06 17:52:45,083 - INFO - training batch 1001, loss: 0.796, 32032/60000 datapoints
2025-03-06 17:52:45,277 - INFO - training batch 1051, loss: 0.748, 33632/60000 datapoints
2025-03-06 17:52:45,471 - INFO - training batch 1101, loss: 0.861, 35232/60000 datapoints
2025-03-06 17:52:45,664 - INFO - training batch 1151, loss: 0.886, 36832/60000 datapoints
2025-03-06 17:52:45,856 - INFO - training batch 1201, loss: 0.802, 38432/60000 datapoints
2025-03-06 17:52:46,050 - INFO - training batch 1251, loss: 0.848, 40032/60000 datapoints
2025-03-06 17:52:46,244 - INFO - training batch 1301, loss: 0.721, 41632/60000 datapoints
2025-03-06 17:52:46,437 - INFO - training batch 1351, loss: 0.735, 43232/60000 datapoints
2025-03-06 17:52:46,632 - INFO - training batch 1401, loss: 0.990, 44832/60000 datapoints
2025-03-06 17:52:46,825 - INFO - training batch 1451, loss: 0.893, 46432/60000 datapoints
2025-03-06 17:52:47,018 - INFO - training batch 1501, loss: 0.829, 48032/60000 datapoints
2025-03-06 17:52:47,212 - INFO - training batch 1551, loss: 0.950, 49632/60000 datapoints
2025-03-06 17:52:47,402 - INFO - training batch 1601, loss: 0.564, 51232/60000 datapoints
2025-03-06 17:52:47,596 - INFO - training batch 1651, loss: 0.802, 52832/60000 datapoints
2025-03-06 17:52:47,786 - INFO - training batch 1701, loss: 1.005, 54432/60000 datapoints
2025-03-06 17:52:47,981 - INFO - training batch 1751, loss: 0.685, 56032/60000 datapoints
2025-03-06 17:52:48,175 - INFO - training batch 1801, loss: 0.945, 57632/60000 datapoints
2025-03-06 17:52:48,368 - INFO - training batch 1851, loss: 0.602, 59232/60000 datapoints
2025-03-06 17:52:48,467 - INFO - validation batch 1, loss: 0.886, 32/10016 datapoints
2025-03-06 17:52:48,619 - INFO - validation batch 51, loss: 0.717, 1632/10016 datapoints
2025-03-06 17:52:48,769 - INFO - validation batch 101, loss: 0.619, 3232/10016 datapoints
2025-03-06 17:52:48,920 - INFO - validation batch 151, loss: 0.737, 4832/10016 datapoints
2025-03-06 17:52:49,070 - INFO - validation batch 201, loss: 0.657, 6432/10016 datapoints
2025-03-06 17:52:49,223 - INFO - validation batch 251, loss: 0.729, 8032/10016 datapoints
2025-03-06 17:52:49,374 - INFO - validation batch 301, loss: 0.868, 9632/10016 datapoints
2025-03-06 17:52:49,414 - INFO - Epoch 63/800 done.
2025-03-06 17:52:49,415 - INFO - Final validation performance:
Loss: 0.745, top-1 acc: 0.838top-5 acc: 0.838
2025-03-06 17:52:49,415 - INFO - Beginning epoch 64/800
2025-03-06 17:52:49,421 - INFO - training batch 1, loss: 0.955, 32/60000 datapoints
2025-03-06 17:52:49,620 - INFO - training batch 51, loss: 0.607, 1632/60000 datapoints
2025-03-06 17:52:49,829 - INFO - training batch 101, loss: 0.775, 3232/60000 datapoints
2025-03-06 17:52:50,021 - INFO - training batch 151, loss: 0.843, 4832/60000 datapoints
2025-03-06 17:52:50,214 - INFO - training batch 201, loss: 0.928, 6432/60000 datapoints
2025-03-06 17:52:50,407 - INFO - training batch 251, loss: 0.888, 8032/60000 datapoints
2025-03-06 17:52:50,600 - INFO - training batch 301, loss: 0.826, 9632/60000 datapoints
2025-03-06 17:52:50,794 - INFO - training batch 351, loss: 0.707, 11232/60000 datapoints
2025-03-06 17:52:50,986 - INFO - training batch 401, loss: 0.784, 12832/60000 datapoints
2025-03-06 17:52:51,180 - INFO - training batch 451, loss: 0.641, 14432/60000 datapoints
2025-03-06 17:52:51,371 - INFO - training batch 501, loss: 0.603, 16032/60000 datapoints
2025-03-06 17:52:51,562 - INFO - training batch 551, loss: 0.629, 17632/60000 datapoints
2025-03-06 17:52:51,755 - INFO - training batch 601, loss: 0.903, 19232/60000 datapoints
2025-03-06 17:52:51,948 - INFO - training batch 651, loss: 0.679, 20832/60000 datapoints
2025-03-06 17:52:52,139 - INFO - training batch 701, loss: 0.862, 22432/60000 datapoints
2025-03-06 17:52:52,329 - INFO - training batch 751, loss: 0.695, 24032/60000 datapoints
2025-03-06 17:52:52,520 - INFO - training batch 801, loss: 0.799, 25632/60000 datapoints
2025-03-06 17:52:52,715 - INFO - training batch 851, loss: 0.609, 27232/60000 datapoints
2025-03-06 17:52:52,905 - INFO - training batch 901, loss: 0.697, 28832/60000 datapoints
2025-03-06 17:52:53,094 - INFO - training batch 951, loss: 0.859, 30432/60000 datapoints
2025-03-06 17:52:53,289 - INFO - training batch 1001, loss: 0.815, 32032/60000 datapoints
2025-03-06 17:52:53,501 - INFO - training batch 1051, loss: 0.832, 33632/60000 datapoints
2025-03-06 17:52:53,693 - INFO - training batch 1101, loss: 0.674, 35232/60000 datapoints
2025-03-06 17:52:53,883 - INFO - training batch 1151, loss: 0.835, 36832/60000 datapoints
2025-03-06 17:52:54,077 - INFO - training batch 1201, loss: 0.904, 38432/60000 datapoints
2025-03-06 17:52:54,268 - INFO - training batch 1251, loss: 0.700, 40032/60000 datapoints
2025-03-06 17:52:54,463 - INFO - training batch 1301, loss: 0.862, 41632/60000 datapoints
2025-03-06 17:52:54,669 - INFO - training batch 1351, loss: 0.704, 43232/60000 datapoints
2025-03-06 17:52:54,866 - INFO - training batch 1401, loss: 0.749, 44832/60000 datapoints
2025-03-06 17:52:55,060 - INFO - training batch 1451, loss: 0.645, 46432/60000 datapoints
2025-03-06 17:52:55,253 - INFO - training batch 1501, loss: 0.724, 48032/60000 datapoints
2025-03-06 17:52:55,447 - INFO - training batch 1551, loss: 0.730, 49632/60000 datapoints
2025-03-06 17:52:55,642 - INFO - training batch 1601, loss: 0.949, 51232/60000 datapoints
2025-03-06 17:52:55,836 - INFO - training batch 1651, loss: 0.701, 52832/60000 datapoints
2025-03-06 17:52:56,034 - INFO - training batch 1701, loss: 0.821, 54432/60000 datapoints
2025-03-06 17:52:56,229 - INFO - training batch 1751, loss: 0.900, 56032/60000 datapoints
2025-03-06 17:52:56,420 - INFO - training batch 1801, loss: 0.853, 57632/60000 datapoints
2025-03-06 17:52:56,616 - INFO - training batch 1851, loss: 0.664, 59232/60000 datapoints
2025-03-06 17:52:56,716 - INFO - validation batch 1, loss: 0.672, 32/10016 datapoints
2025-03-06 17:52:56,865 - INFO - validation batch 51, loss: 0.610, 1632/10016 datapoints
2025-03-06 17:52:57,016 - INFO - validation batch 101, loss: 0.680, 3232/10016 datapoints
2025-03-06 17:52:57,167 - INFO - validation batch 151, loss: 0.766, 4832/10016 datapoints
2025-03-06 17:52:57,318 - INFO - validation batch 201, loss: 0.818, 6432/10016 datapoints
2025-03-06 17:52:57,486 - INFO - validation batch 251, loss: 0.796, 8032/10016 datapoints
2025-03-06 17:52:57,638 - INFO - validation batch 301, loss: 0.931, 9632/10016 datapoints
2025-03-06 17:52:57,673 - INFO - Epoch 64/800 done.
2025-03-06 17:52:57,674 - INFO - Final validation performance:
Loss: 0.753, top-1 acc: 0.840top-5 acc: 0.840
2025-03-06 17:52:57,674 - INFO - Beginning epoch 65/800
2025-03-06 17:52:57,679 - INFO - training batch 1, loss: 0.785, 32/60000 datapoints
2025-03-06 17:52:57,871 - INFO - training batch 51, loss: 0.748, 1632/60000 datapoints
2025-03-06 17:52:58,063 - INFO - training batch 101, loss: 0.853, 3232/60000 datapoints
2025-03-06 17:52:58,256 - INFO - training batch 151, loss: 0.952, 4832/60000 datapoints
2025-03-06 17:52:58,449 - INFO - training batch 201, loss: 0.549, 6432/60000 datapoints
2025-03-06 17:52:58,644 - INFO - training batch 251, loss: 0.874, 8032/60000 datapoints
2025-03-06 17:52:58,835 - INFO - training batch 301, loss: 0.802, 9632/60000 datapoints
2025-03-06 17:52:59,027 - INFO - training batch 351, loss: 0.581, 11232/60000 datapoints
2025-03-06 17:52:59,223 - INFO - training batch 401, loss: 0.931, 12832/60000 datapoints
2025-03-06 17:52:59,415 - INFO - training batch 451, loss: 0.676, 14432/60000 datapoints
2025-03-06 17:52:59,609 - INFO - training batch 501, loss: 0.725, 16032/60000 datapoints
2025-03-06 17:52:59,801 - INFO - training batch 551, loss: 0.757, 17632/60000 datapoints
2025-03-06 17:52:59,998 - INFO - training batch 601, loss: 0.926, 19232/60000 datapoints
2025-03-06 17:53:00,192 - INFO - training batch 651, loss: 0.624, 20832/60000 datapoints
2025-03-06 17:53:00,387 - INFO - training batch 701, loss: 0.648, 22432/60000 datapoints
2025-03-06 17:53:00,583 - INFO - training batch 751, loss: 0.835, 24032/60000 datapoints
2025-03-06 17:53:00,781 - INFO - training batch 801, loss: 0.965, 25632/60000 datapoints
2025-03-06 17:53:00,975 - INFO - training batch 851, loss: 0.601, 27232/60000 datapoints
2025-03-06 17:53:01,173 - INFO - training batch 901, loss: 0.773, 28832/60000 datapoints
2025-03-06 17:53:01,367 - INFO - training batch 951, loss: 0.630, 30432/60000 datapoints
2025-03-06 17:53:01,561 - INFO - training batch 1001, loss: 0.879, 32032/60000 datapoints
2025-03-06 17:53:01,758 - INFO - training batch 1051, loss: 0.898, 33632/60000 datapoints
2025-03-06 17:53:01,951 - INFO - training batch 1101, loss: 0.643, 35232/60000 datapoints
2025-03-06 17:53:02,144 - INFO - training batch 1151, loss: 0.587, 36832/60000 datapoints
2025-03-06 17:53:02,339 - INFO - training batch 1201, loss: 0.907, 38432/60000 datapoints
2025-03-06 17:53:02,533 - INFO - training batch 1251, loss: 0.690, 40032/60000 datapoints
2025-03-06 17:53:02,728 - INFO - training batch 1301, loss: 0.843, 41632/60000 datapoints
2025-03-06 17:53:02,923 - INFO - training batch 1351, loss: 0.697, 43232/60000 datapoints
2025-03-06 17:53:03,115 - INFO - training batch 1401, loss: 0.625, 44832/60000 datapoints
2025-03-06 17:53:03,310 - INFO - training batch 1451, loss: 0.698, 46432/60000 datapoints
2025-03-06 17:53:03,516 - INFO - training batch 1501, loss: 0.821, 48032/60000 datapoints
2025-03-06 17:53:03,715 - INFO - training batch 1551, loss: 0.836, 49632/60000 datapoints
2025-03-06 17:53:03,910 - INFO - training batch 1601, loss: 0.769, 51232/60000 datapoints
2025-03-06 17:53:04,105 - INFO - training batch 1651, loss: 0.917, 52832/60000 datapoints
2025-03-06 17:53:04,301 - INFO - training batch 1701, loss: 0.614, 54432/60000 datapoints
2025-03-06 17:53:04,494 - INFO - training batch 1751, loss: 0.745, 56032/60000 datapoints
2025-03-06 17:53:04,690 - INFO - training batch 1801, loss: 0.848, 57632/60000 datapoints
2025-03-06 17:53:04,889 - INFO - training batch 1851, loss: 0.728, 59232/60000 datapoints
2025-03-06 17:53:04,991 - INFO - validation batch 1, loss: 0.625, 32/10016 datapoints
2025-03-06 17:53:05,145 - INFO - validation batch 51, loss: 0.582, 1632/10016 datapoints
2025-03-06 17:53:05,299 - INFO - validation batch 101, loss: 0.637, 3232/10016 datapoints
2025-03-06 17:53:05,451 - INFO - validation batch 151, loss: 0.750, 4832/10016 datapoints
2025-03-06 17:53:05,606 - INFO - validation batch 201, loss: 0.740, 6432/10016 datapoints
2025-03-06 17:53:05,758 - INFO - validation batch 251, loss: 0.740, 8032/10016 datapoints
2025-03-06 17:53:05,910 - INFO - validation batch 301, loss: 0.917, 9632/10016 datapoints
2025-03-06 17:53:05,946 - INFO - Epoch 65/800 done.
2025-03-06 17:53:05,946 - INFO - Final validation performance:
Loss: 0.713, top-1 acc: 0.841top-5 acc: 0.841
2025-03-06 17:53:05,946 - INFO - Beginning epoch 66/800
2025-03-06 17:53:05,954 - INFO - training batch 1, loss: 0.664, 32/60000 datapoints
2025-03-06 17:53:06,152 - INFO - training batch 51, loss: 0.625, 1632/60000 datapoints
2025-03-06 17:53:06,345 - INFO - training batch 101, loss: 0.820, 3232/60000 datapoints
2025-03-06 17:53:06,537 - INFO - training batch 151, loss: 0.641, 4832/60000 datapoints
2025-03-06 17:53:06,731 - INFO - training batch 201, loss: 0.804, 6432/60000 datapoints
2025-03-06 17:53:06,926 - INFO - training batch 251, loss: 0.899, 8032/60000 datapoints
2025-03-06 17:53:07,117 - INFO - training batch 301, loss: 0.678, 9632/60000 datapoints
2025-03-06 17:53:07,314 - INFO - training batch 351, loss: 0.579, 11232/60000 datapoints
2025-03-06 17:53:07,506 - INFO - training batch 401, loss: 0.753, 12832/60000 datapoints
2025-03-06 17:53:07,702 - INFO - training batch 451, loss: 0.940, 14432/60000 datapoints
2025-03-06 17:53:07,897 - INFO - training batch 501, loss: 0.828, 16032/60000 datapoints
2025-03-06 17:53:08,097 - INFO - training batch 551, loss: 0.768, 17632/60000 datapoints
2025-03-06 17:53:08,291 - INFO - training batch 601, loss: 0.699, 19232/60000 datapoints
2025-03-06 17:53:08,484 - INFO - training batch 651, loss: 0.806, 20832/60000 datapoints
2025-03-06 17:53:08,678 - INFO - training batch 701, loss: 0.751, 22432/60000 datapoints
2025-03-06 17:53:08,872 - INFO - training batch 751, loss: 0.556, 24032/60000 datapoints
2025-03-06 17:53:09,065 - INFO - training batch 801, loss: 0.690, 25632/60000 datapoints
2025-03-06 17:53:09,259 - INFO - training batch 851, loss: 0.824, 27232/60000 datapoints
2025-03-06 17:53:09,452 - INFO - training batch 901, loss: 0.735, 28832/60000 datapoints
2025-03-06 17:53:09,648 - INFO - training batch 951, loss: 0.737, 30432/60000 datapoints
2025-03-06 17:53:09,841 - INFO - training batch 1001, loss: 0.771, 32032/60000 datapoints
2025-03-06 17:53:10,039 - INFO - training batch 1051, loss: 0.667, 33632/60000 datapoints
2025-03-06 17:53:10,235 - INFO - training batch 1101, loss: 1.018, 35232/60000 datapoints
2025-03-06 17:53:10,432 - INFO - training batch 1151, loss: 0.839, 36832/60000 datapoints
2025-03-06 17:53:10,626 - INFO - training batch 1201, loss: 0.730, 38432/60000 datapoints
2025-03-06 17:53:10,820 - INFO - training batch 1251, loss: 0.827, 40032/60000 datapoints
2025-03-06 17:53:11,014 - INFO - training batch 1301, loss: 0.627, 41632/60000 datapoints
2025-03-06 17:53:11,210 - INFO - training batch 1351, loss: 0.738, 43232/60000 datapoints
2025-03-06 17:53:11,404 - INFO - training batch 1401, loss: 0.819, 44832/60000 datapoints
2025-03-06 17:53:11,599 - INFO - training batch 1451, loss: 0.638, 46432/60000 datapoints
2025-03-06 17:53:11,791 - INFO - training batch 1501, loss: 0.562, 48032/60000 datapoints
2025-03-06 17:53:11,983 - INFO - training batch 1551, loss: 0.779, 49632/60000 datapoints
2025-03-06 17:53:12,175 - INFO - training batch 1601, loss: 0.699, 51232/60000 datapoints
2025-03-06 17:53:12,366 - INFO - training batch 1651, loss: 0.684, 52832/60000 datapoints
2025-03-06 17:53:12,560 - INFO - training batch 1701, loss: 0.849, 54432/60000 datapoints
2025-03-06 17:53:12,757 - INFO - training batch 1751, loss: 0.793, 56032/60000 datapoints
2025-03-06 17:53:12,953 - INFO - training batch 1801, loss: 0.655, 57632/60000 datapoints
2025-03-06 17:53:13,147 - INFO - training batch 1851, loss: 0.546, 59232/60000 datapoints
2025-03-06 17:53:13,250 - INFO - validation batch 1, loss: 0.738, 32/10016 datapoints
2025-03-06 17:53:13,402 - INFO - validation batch 51, loss: 0.732, 1632/10016 datapoints
2025-03-06 17:53:13,559 - INFO - validation batch 101, loss: 0.737, 3232/10016 datapoints
2025-03-06 17:53:13,727 - INFO - validation batch 151, loss: 0.713, 4832/10016 datapoints
2025-03-06 17:53:13,880 - INFO - validation batch 201, loss: 0.805, 6432/10016 datapoints
2025-03-06 17:53:14,034 - INFO - validation batch 251, loss: 0.671, 8032/10016 datapoints
2025-03-06 17:53:14,186 - INFO - validation batch 301, loss: 0.642, 9632/10016 datapoints
2025-03-06 17:53:14,224 - INFO - Epoch 66/800 done.
2025-03-06 17:53:14,224 - INFO - Final validation performance:
Loss: 0.720, top-1 acc: 0.843top-5 acc: 0.843
2025-03-06 17:53:14,224 - INFO - Beginning epoch 67/800
2025-03-06 17:53:14,230 - INFO - training batch 1, loss: 0.772, 32/60000 datapoints
2025-03-06 17:53:14,424 - INFO - training batch 51, loss: 0.660, 1632/60000 datapoints
2025-03-06 17:53:14,618 - INFO - training batch 101, loss: 0.679, 3232/60000 datapoints
2025-03-06 17:53:14,813 - INFO - training batch 151, loss: 0.775, 4832/60000 datapoints
2025-03-06 17:53:15,009 - INFO - training batch 201, loss: 0.608, 6432/60000 datapoints
2025-03-06 17:53:15,208 - INFO - training batch 251, loss: 0.461, 8032/60000 datapoints
2025-03-06 17:53:15,402 - INFO - training batch 301, loss: 0.658, 9632/60000 datapoints
2025-03-06 17:53:15,601 - INFO - training batch 351, loss: 0.660, 11232/60000 datapoints
2025-03-06 17:53:15,796 - INFO - training batch 401, loss: 0.819, 12832/60000 datapoints
2025-03-06 17:53:15,992 - INFO - training batch 451, loss: 0.827, 14432/60000 datapoints
2025-03-06 17:53:16,194 - INFO - training batch 501, loss: 0.762, 16032/60000 datapoints
2025-03-06 17:53:16,387 - INFO - training batch 551, loss: 0.727, 17632/60000 datapoints
2025-03-06 17:53:16,581 - INFO - training batch 601, loss: 0.904, 19232/60000 datapoints
2025-03-06 17:53:16,776 - INFO - training batch 651, loss: 0.610, 20832/60000 datapoints
2025-03-06 17:53:16,970 - INFO - training batch 701, loss: 0.725, 22432/60000 datapoints
2025-03-06 17:53:17,164 - INFO - training batch 751, loss: 0.883, 24032/60000 datapoints
2025-03-06 17:53:17,360 - INFO - training batch 801, loss: 0.700, 25632/60000 datapoints
2025-03-06 17:53:17,555 - INFO - training batch 851, loss: 0.730, 27232/60000 datapoints
2025-03-06 17:53:17,751 - INFO - training batch 901, loss: 0.716, 28832/60000 datapoints
2025-03-06 17:53:17,944 - INFO - training batch 951, loss: 0.828, 30432/60000 datapoints
2025-03-06 17:53:18,142 - INFO - training batch 1001, loss: 0.788, 32032/60000 datapoints
2025-03-06 17:53:18,338 - INFO - training batch 1051, loss: 0.703, 33632/60000 datapoints
2025-03-06 17:53:18,531 - INFO - training batch 1101, loss: 0.844, 35232/60000 datapoints
2025-03-06 17:53:18,725 - INFO - training batch 1151, loss: 0.672, 36832/60000 datapoints
2025-03-06 17:53:18,919 - INFO - training batch 1201, loss: 0.744, 38432/60000 datapoints
2025-03-06 17:53:19,116 - INFO - training batch 1251, loss: 0.769, 40032/60000 datapoints
2025-03-06 17:53:19,342 - INFO - training batch 1301, loss: 0.818, 41632/60000 datapoints
2025-03-06 17:53:19,541 - INFO - training batch 1351, loss: 0.628, 43232/60000 datapoints
2025-03-06 17:53:19,738 - INFO - training batch 1401, loss: 0.875, 44832/60000 datapoints
2025-03-06 17:53:19,932 - INFO - training batch 1451, loss: 0.529, 46432/60000 datapoints
2025-03-06 17:53:20,125 - INFO - training batch 1501, loss: 0.707, 48032/60000 datapoints
2025-03-06 17:53:20,319 - INFO - training batch 1551, loss: 0.590, 49632/60000 datapoints
2025-03-06 17:53:20,511 - INFO - training batch 1601, loss: 0.572, 51232/60000 datapoints
2025-03-06 17:53:20,707 - INFO - training batch 1651, loss: 0.782, 52832/60000 datapoints
2025-03-06 17:53:20,902 - INFO - training batch 1701, loss: 0.757, 54432/60000 datapoints
2025-03-06 17:53:21,095 - INFO - training batch 1751, loss: 0.767, 56032/60000 datapoints
2025-03-06 17:53:21,302 - INFO - training batch 1801, loss: 1.021, 57632/60000 datapoints
2025-03-06 17:53:21,496 - INFO - training batch 1851, loss: 0.862, 59232/60000 datapoints
2025-03-06 17:53:21,599 - INFO - validation batch 1, loss: 0.712, 32/10016 datapoints
2025-03-06 17:53:21,754 - INFO - validation batch 51, loss: 0.755, 1632/10016 datapoints
2025-03-06 17:53:21,906 - INFO - validation batch 101, loss: 0.619, 3232/10016 datapoints
2025-03-06 17:53:22,059 - INFO - validation batch 151, loss: 0.938, 4832/10016 datapoints
2025-03-06 17:53:22,213 - INFO - validation batch 201, loss: 0.809, 6432/10016 datapoints
2025-03-06 17:53:22,365 - INFO - validation batch 251, loss: 0.681, 8032/10016 datapoints
2025-03-06 17:53:22,518 - INFO - validation batch 301, loss: 0.694, 9632/10016 datapoints
2025-03-06 17:53:22,554 - INFO - Epoch 67/800 done.
2025-03-06 17:53:22,554 - INFO - Final validation performance:
Loss: 0.744, top-1 acc: 0.844top-5 acc: 0.844
2025-03-06 17:53:22,554 - INFO - Beginning epoch 68/800
2025-03-06 17:53:22,561 - INFO - training batch 1, loss: 0.714, 32/60000 datapoints
2025-03-06 17:53:22,755 - INFO - training batch 51, loss: 0.670, 1632/60000 datapoints
2025-03-06 17:53:22,944 - INFO - training batch 101, loss: 0.744, 3232/60000 datapoints
2025-03-06 17:53:23,137 - INFO - training batch 151, loss: 0.562, 4832/60000 datapoints
2025-03-06 17:53:23,332 - INFO - training batch 201, loss: 0.938, 6432/60000 datapoints
2025-03-06 17:53:23,526 - INFO - training batch 251, loss: 0.691, 8032/60000 datapoints
2025-03-06 17:53:23,739 - INFO - training batch 301, loss: 0.739, 9632/60000 datapoints
2025-03-06 17:53:23,928 - INFO - training batch 351, loss: 0.801, 11232/60000 datapoints
2025-03-06 17:53:24,121 - INFO - training batch 401, loss: 0.647, 12832/60000 datapoints
2025-03-06 17:53:24,313 - INFO - training batch 451, loss: 0.909, 14432/60000 datapoints
2025-03-06 17:53:24,503 - INFO - training batch 501, loss: 0.605, 16032/60000 datapoints
2025-03-06 17:53:24,744 - INFO - training batch 551, loss: 0.704, 17632/60000 datapoints
2025-03-06 17:53:24,937 - INFO - training batch 601, loss: 0.592, 19232/60000 datapoints
2025-03-06 17:53:25,128 - INFO - training batch 651, loss: 0.630, 20832/60000 datapoints
2025-03-06 17:53:25,323 - INFO - training batch 701, loss: 0.684, 22432/60000 datapoints
2025-03-06 17:53:25,513 - INFO - training batch 751, loss: 0.716, 24032/60000 datapoints
2025-03-06 17:53:25,709 - INFO - training batch 801, loss: 0.779, 25632/60000 datapoints
2025-03-06 17:53:25,900 - INFO - training batch 851, loss: 0.667, 27232/60000 datapoints
2025-03-06 17:53:26,092 - INFO - training batch 901, loss: 0.629, 28832/60000 datapoints
2025-03-06 17:53:26,285 - INFO - training batch 951, loss: 0.717, 30432/60000 datapoints
2025-03-06 17:53:26,474 - INFO - training batch 1001, loss: 0.537, 32032/60000 datapoints
2025-03-06 17:53:26,668 - INFO - training batch 1051, loss: 0.706, 33632/60000 datapoints
2025-03-06 17:53:26,858 - INFO - training batch 1101, loss: 0.756, 35232/60000 datapoints
2025-03-06 17:53:27,050 - INFO - training batch 1151, loss: 0.465, 36832/60000 datapoints
2025-03-06 17:53:27,248 - INFO - training batch 1201, loss: 0.670, 38432/60000 datapoints
2025-03-06 17:53:27,438 - INFO - training batch 1251, loss: 0.560, 40032/60000 datapoints
2025-03-06 17:53:27,630 - INFO - training batch 1301, loss: 0.692, 41632/60000 datapoints
2025-03-06 17:53:27,822 - INFO - training batch 1351, loss: 0.569, 43232/60000 datapoints
2025-03-06 17:53:28,010 - INFO - training batch 1401, loss: 0.724, 44832/60000 datapoints
2025-03-06 17:53:28,201 - INFO - training batch 1451, loss: 0.718, 46432/60000 datapoints
2025-03-06 17:53:28,392 - INFO - training batch 1501, loss: 0.773, 48032/60000 datapoints
2025-03-06 17:53:28,582 - INFO - training batch 1551, loss: 0.584, 49632/60000 datapoints
2025-03-06 17:53:28,776 - INFO - training batch 1601, loss: 0.911, 51232/60000 datapoints
2025-03-06 17:53:28,967 - INFO - training batch 1651, loss: 0.889, 52832/60000 datapoints
2025-03-06 17:53:29,159 - INFO - training batch 1701, loss: 0.618, 54432/60000 datapoints
2025-03-06 17:53:29,352 - INFO - training batch 1751, loss: 0.609, 56032/60000 datapoints
2025-03-06 17:53:29,541 - INFO - training batch 1801, loss: 0.671, 57632/60000 datapoints
2025-03-06 17:53:29,735 - INFO - training batch 1851, loss: 0.582, 59232/60000 datapoints
2025-03-06 17:53:29,833 - INFO - validation batch 1, loss: 0.609, 32/10016 datapoints
2025-03-06 17:53:29,981 - INFO - validation batch 51, loss: 0.666, 1632/10016 datapoints
2025-03-06 17:53:30,131 - INFO - validation batch 101, loss: 0.902, 3232/10016 datapoints
2025-03-06 17:53:30,281 - INFO - validation batch 151, loss: 0.997, 4832/10016 datapoints
2025-03-06 17:53:30,431 - INFO - validation batch 201, loss: 0.768, 6432/10016 datapoints
2025-03-06 17:53:30,579 - INFO - validation batch 251, loss: 0.632, 8032/10016 datapoints
2025-03-06 17:53:30,733 - INFO - validation batch 301, loss: 0.675, 9632/10016 datapoints
2025-03-06 17:53:30,770 - INFO - Epoch 68/800 done.
2025-03-06 17:53:30,770 - INFO - Final validation performance:
Loss: 0.750, top-1 acc: 0.845top-5 acc: 0.845
2025-03-06 17:53:30,770 - INFO - Beginning epoch 69/800
2025-03-06 17:53:30,776 - INFO - training batch 1, loss: 0.716, 32/60000 datapoints
2025-03-06 17:53:30,967 - INFO - training batch 51, loss: 0.648, 1632/60000 datapoints
2025-03-06 17:53:31,159 - INFO - training batch 101, loss: 0.660, 3232/60000 datapoints
2025-03-06 17:53:31,352 - INFO - training batch 151, loss: 0.678, 4832/60000 datapoints
2025-03-06 17:53:31,544 - INFO - training batch 201, loss: 0.890, 6432/60000 datapoints
2025-03-06 17:53:31,738 - INFO - training batch 251, loss: 0.665, 8032/60000 datapoints
2025-03-06 17:53:31,930 - INFO - training batch 301, loss: 0.773, 9632/60000 datapoints
2025-03-06 17:53:32,123 - INFO - training batch 351, loss: 0.551, 11232/60000 datapoints
2025-03-06 17:53:32,316 - INFO - training batch 401, loss: 0.778, 12832/60000 datapoints
2025-03-06 17:53:32,507 - INFO - training batch 451, loss: 0.745, 14432/60000 datapoints
2025-03-06 17:53:32,701 - INFO - training batch 501, loss: 0.638, 16032/60000 datapoints
2025-03-06 17:53:32,891 - INFO - training batch 551, loss: 0.612, 17632/60000 datapoints
2025-03-06 17:53:33,080 - INFO - training batch 601, loss: 0.561, 19232/60000 datapoints
2025-03-06 17:53:33,272 - INFO - training batch 651, loss: 0.613, 20832/60000 datapoints
2025-03-06 17:53:33,462 - INFO - training batch 701, loss: 0.662, 22432/60000 datapoints
2025-03-06 17:53:33,658 - INFO - training batch 751, loss: 0.779, 24032/60000 datapoints
2025-03-06 17:53:33,869 - INFO - training batch 801, loss: 0.779, 25632/60000 datapoints
2025-03-06 17:53:34,059 - INFO - training batch 851, loss: 0.869, 27232/60000 datapoints
2025-03-06 17:53:34,248 - INFO - training batch 901, loss: 0.688, 28832/60000 datapoints
2025-03-06 17:53:34,437 - INFO - training batch 951, loss: 0.733, 30432/60000 datapoints
2025-03-06 17:53:34,630 - INFO - training batch 1001, loss: 0.723, 32032/60000 datapoints
2025-03-06 17:53:34,826 - INFO - training batch 1051, loss: 0.731, 33632/60000 datapoints
2025-03-06 17:53:35,016 - INFO - training batch 1101, loss: 0.559, 35232/60000 datapoints
2025-03-06 17:53:35,208 - INFO - training batch 1151, loss: 0.997, 36832/60000 datapoints
2025-03-06 17:53:35,399 - INFO - training batch 1201, loss: 0.612, 38432/60000 datapoints
2025-03-06 17:53:35,589 - INFO - training batch 1251, loss: 0.527, 40032/60000 datapoints
2025-03-06 17:53:35,784 - INFO - training batch 1301, loss: 0.701, 41632/60000 datapoints
2025-03-06 17:53:35,976 - INFO - training batch 1351, loss: 0.516, 43232/60000 datapoints
2025-03-06 17:53:36,173 - INFO - training batch 1401, loss: 0.726, 44832/60000 datapoints
2025-03-06 17:53:36,366 - INFO - training batch 1451, loss: 0.781, 46432/60000 datapoints
2025-03-06 17:53:36,556 - INFO - training batch 1501, loss: 0.638, 48032/60000 datapoints
2025-03-06 17:53:36,749 - INFO - training batch 1551, loss: 0.583, 49632/60000 datapoints
2025-03-06 17:53:36,938 - INFO - training batch 1601, loss: 0.584, 51232/60000 datapoints
2025-03-06 17:53:37,129 - INFO - training batch 1651, loss: 0.594, 52832/60000 datapoints
2025-03-06 17:53:37,324 - INFO - training batch 1701, loss: 0.791, 54432/60000 datapoints
2025-03-06 17:53:37,513 - INFO - training batch 1751, loss: 0.962, 56032/60000 datapoints
2025-03-06 17:53:37,718 - INFO - training batch 1801, loss: 0.566, 57632/60000 datapoints
2025-03-06 17:53:37,909 - INFO - training batch 1851, loss: 0.669, 59232/60000 datapoints
2025-03-06 17:53:38,006 - INFO - validation batch 1, loss: 1.004, 32/10016 datapoints
2025-03-06 17:53:38,153 - INFO - validation batch 51, loss: 0.559, 1632/10016 datapoints
2025-03-06 17:53:38,305 - INFO - validation batch 101, loss: 0.897, 3232/10016 datapoints
2025-03-06 17:53:38,454 - INFO - validation batch 151, loss: 0.899, 4832/10016 datapoints
2025-03-06 17:53:38,604 - INFO - validation batch 201, loss: 0.938, 6432/10016 datapoints
2025-03-06 17:53:38,755 - INFO - validation batch 251, loss: 0.821, 8032/10016 datapoints
2025-03-06 17:53:38,905 - INFO - validation batch 301, loss: 0.674, 9632/10016 datapoints
2025-03-06 17:53:38,940 - INFO - Epoch 69/800 done.
2025-03-06 17:53:38,940 - INFO - Final validation performance:
Loss: 0.827, top-1 acc: 0.847top-5 acc: 0.847
2025-03-06 17:53:38,940 - INFO - Beginning epoch 70/800
2025-03-06 17:53:38,946 - INFO - training batch 1, loss: 0.703, 32/60000 datapoints
2025-03-06 17:53:39,139 - INFO - training batch 51, loss: 0.847, 1632/60000 datapoints
2025-03-06 17:53:39,339 - INFO - training batch 101, loss: 0.736, 3232/60000 datapoints
2025-03-06 17:53:39,528 - INFO - training batch 151, loss: 0.717, 4832/60000 datapoints
2025-03-06 17:53:39,721 - INFO - training batch 201, loss: 0.657, 6432/60000 datapoints
2025-03-06 17:53:39,911 - INFO - training batch 251, loss: 0.759, 8032/60000 datapoints
2025-03-06 17:53:40,102 - INFO - training batch 301, loss: 0.686, 9632/60000 datapoints
2025-03-06 17:53:40,299 - INFO - training batch 351, loss: 0.691, 11232/60000 datapoints
2025-03-06 17:53:40,492 - INFO - training batch 401, loss: 0.556, 12832/60000 datapoints
2025-03-06 17:53:40,694 - INFO - training batch 451, loss: 0.610, 14432/60000 datapoints
2025-03-06 17:53:40,894 - INFO - training batch 501, loss: 0.853, 16032/60000 datapoints
2025-03-06 17:53:41,087 - INFO - training batch 551, loss: 0.471, 17632/60000 datapoints
2025-03-06 17:53:41,281 - INFO - training batch 601, loss: 0.632, 19232/60000 datapoints
2025-03-06 17:53:41,477 - INFO - training batch 651, loss: 0.659, 20832/60000 datapoints
2025-03-06 17:53:41,670 - INFO - training batch 701, loss: 0.703, 22432/60000 datapoints
2025-03-06 17:53:41,872 - INFO - training batch 751, loss: 0.707, 24032/60000 datapoints
2025-03-06 17:53:42,075 - INFO - training batch 801, loss: 0.568, 25632/60000 datapoints
2025-03-06 17:53:42,268 - INFO - training batch 851, loss: 0.580, 27232/60000 datapoints
2025-03-06 17:53:42,463 - INFO - training batch 901, loss: 0.931, 28832/60000 datapoints
2025-03-06 17:53:42,659 - INFO - training batch 951, loss: 0.773, 30432/60000 datapoints
2025-03-06 17:53:42,852 - INFO - training batch 1001, loss: 0.679, 32032/60000 datapoints
2025-03-06 17:53:43,044 - INFO - training batch 1051, loss: 0.679, 33632/60000 datapoints
2025-03-06 17:53:43,239 - INFO - training batch 1101, loss: 0.580, 35232/60000 datapoints
2025-03-06 17:53:43,434 - INFO - training batch 1151, loss: 0.588, 36832/60000 datapoints
2025-03-06 17:53:43,630 - INFO - training batch 1201, loss: 0.736, 38432/60000 datapoints
2025-03-06 17:53:43,831 - INFO - training batch 1251, loss: 0.796, 40032/60000 datapoints
2025-03-06 17:53:44,035 - INFO - training batch 1301, loss: 0.695, 41632/60000 datapoints
2025-03-06 17:53:44,228 - INFO - training batch 1351, loss: 0.590, 43232/60000 datapoints
2025-03-06 17:53:44,424 - INFO - training batch 1401, loss: 0.706, 44832/60000 datapoints
2025-03-06 17:53:44,618 - INFO - training batch 1451, loss: 0.666, 46432/60000 datapoints
2025-03-06 17:53:44,810 - INFO - training batch 1501, loss: 0.855, 48032/60000 datapoints
2025-03-06 17:53:45,011 - INFO - training batch 1551, loss: 0.658, 49632/60000 datapoints
2025-03-06 17:53:45,206 - INFO - training batch 1601, loss: 0.542, 51232/60000 datapoints
2025-03-06 17:53:45,402 - INFO - training batch 1651, loss: 0.770, 52832/60000 datapoints
2025-03-06 17:53:45,596 - INFO - training batch 1701, loss: 0.558, 54432/60000 datapoints
2025-03-06 17:53:45,789 - INFO - training batch 1751, loss: 0.981, 56032/60000 datapoints
2025-03-06 17:53:45,985 - INFO - training batch 1801, loss: 0.777, 57632/60000 datapoints
2025-03-06 17:53:46,187 - INFO - training batch 1851, loss: 0.788, 59232/60000 datapoints
2025-03-06 17:53:46,289 - INFO - validation batch 1, loss: 0.487, 32/10016 datapoints
2025-03-06 17:53:46,442 - INFO - validation batch 51, loss: 0.431, 1632/10016 datapoints
2025-03-06 17:53:46,595 - INFO - validation batch 101, loss: 0.752, 3232/10016 datapoints
2025-03-06 17:53:46,746 - INFO - validation batch 151, loss: 0.912, 4832/10016 datapoints
2025-03-06 17:53:46,899 - INFO - validation batch 201, loss: 0.792, 6432/10016 datapoints
2025-03-06 17:53:47,054 - INFO - validation batch 251, loss: 0.537, 8032/10016 datapoints
2025-03-06 17:53:47,209 - INFO - validation batch 301, loss: 0.775, 9632/10016 datapoints
2025-03-06 17:53:47,245 - INFO - Epoch 70/800 done.
2025-03-06 17:53:47,245 - INFO - Final validation performance:
Loss: 0.669, top-1 acc: 0.848top-5 acc: 0.848
2025-03-06 17:53:47,246 - INFO - Beginning epoch 71/800
2025-03-06 17:53:47,253 - INFO - training batch 1, loss: 0.692, 32/60000 datapoints
2025-03-06 17:53:47,454 - INFO - training batch 51, loss: 0.498, 1632/60000 datapoints
2025-03-06 17:53:47,649 - INFO - training batch 101, loss: 0.502, 3232/60000 datapoints
2025-03-06 17:53:47,844 - INFO - training batch 151, loss: 0.905, 4832/60000 datapoints
2025-03-06 17:53:48,038 - INFO - training batch 201, loss: 0.528, 6432/60000 datapoints
2025-03-06 17:53:48,232 - INFO - training batch 251, loss: 0.599, 8032/60000 datapoints
2025-03-06 17:53:48,430 - INFO - training batch 301, loss: 0.603, 9632/60000 datapoints
2025-03-06 17:53:48,628 - INFO - training batch 351, loss: 0.746, 11232/60000 datapoints
2025-03-06 17:53:48,823 - INFO - training batch 401, loss: 0.690, 12832/60000 datapoints
2025-03-06 17:53:49,015 - INFO - training batch 451, loss: 0.565, 14432/60000 datapoints
2025-03-06 17:53:49,208 - INFO - training batch 501, loss: 0.664, 16032/60000 datapoints
2025-03-06 17:53:49,407 - INFO - training batch 551, loss: 0.827, 17632/60000 datapoints
2025-03-06 17:53:49,604 - INFO - training batch 601, loss: 0.562, 19232/60000 datapoints
2025-03-06 17:53:49,799 - INFO - training batch 651, loss: 0.558, 20832/60000 datapoints
2025-03-06 17:53:49,993 - INFO - training batch 701, loss: 0.580, 22432/60000 datapoints
2025-03-06 17:53:50,185 - INFO - training batch 751, loss: 0.858, 24032/60000 datapoints
2025-03-06 17:53:50,379 - INFO - training batch 801, loss: 0.905, 25632/60000 datapoints
2025-03-06 17:53:50,572 - INFO - training batch 851, loss: 0.697, 27232/60000 datapoints
2025-03-06 17:53:50,768 - INFO - training batch 901, loss: 0.797, 28832/60000 datapoints
2025-03-06 17:53:50,964 - INFO - training batch 951, loss: 0.677, 30432/60000 datapoints
2025-03-06 17:53:51,159 - INFO - training batch 1001, loss: 0.482, 32032/60000 datapoints
2025-03-06 17:53:51,354 - INFO - training batch 1051, loss: 0.583, 33632/60000 datapoints
2025-03-06 17:53:51,547 - INFO - training batch 1101, loss: 0.633, 35232/60000 datapoints
2025-03-06 17:53:51,741 - INFO - training batch 1151, loss: 0.793, 36832/60000 datapoints
2025-03-06 17:53:51,934 - INFO - training batch 1201, loss: 0.687, 38432/60000 datapoints
2025-03-06 17:53:52,127 - INFO - training batch 1251, loss: 0.446, 40032/60000 datapoints
2025-03-06 17:53:52,320 - INFO - training batch 1301, loss: 0.654, 41632/60000 datapoints
2025-03-06 17:53:52,517 - INFO - training batch 1351, loss: 0.797, 43232/60000 datapoints
2025-03-06 17:53:52,732 - INFO - training batch 1401, loss: 0.647, 44832/60000 datapoints
2025-03-06 17:53:52,937 - INFO - training batch 1451, loss: 0.685, 46432/60000 datapoints
2025-03-06 17:53:53,133 - INFO - training batch 1501, loss: 0.618, 48032/60000 datapoints
2025-03-06 17:53:53,326 - INFO - training batch 1551, loss: 0.553, 49632/60000 datapoints
2025-03-06 17:53:53,523 - INFO - training batch 1601, loss: 0.927, 51232/60000 datapoints
2025-03-06 17:53:53,717 - INFO - training batch 1651, loss: 0.549, 52832/60000 datapoints
2025-03-06 17:53:53,922 - INFO - training batch 1701, loss: 0.559, 54432/60000 datapoints
2025-03-06 17:53:54,121 - INFO - training batch 1751, loss: 0.587, 56032/60000 datapoints
2025-03-06 17:53:54,315 - INFO - training batch 1801, loss: 0.640, 57632/60000 datapoints
2025-03-06 17:53:54,509 - INFO - training batch 1851, loss: 0.822, 59232/60000 datapoints
2025-03-06 17:53:54,611 - INFO - validation batch 1, loss: 0.534, 32/10016 datapoints
2025-03-06 17:53:54,762 - INFO - validation batch 51, loss: 0.366, 1632/10016 datapoints
2025-03-06 17:53:54,917 - INFO - validation batch 101, loss: 0.719, 3232/10016 datapoints
2025-03-06 17:53:55,072 - INFO - validation batch 151, loss: 0.807, 4832/10016 datapoints
2025-03-06 17:53:55,221 - INFO - validation batch 201, loss: 0.491, 6432/10016 datapoints
2025-03-06 17:53:55,376 - INFO - validation batch 251, loss: 0.715, 8032/10016 datapoints
2025-03-06 17:53:55,530 - INFO - validation batch 301, loss: 0.613, 9632/10016 datapoints
2025-03-06 17:53:55,566 - INFO - Epoch 71/800 done.
2025-03-06 17:53:55,566 - INFO - Final validation performance:
Loss: 0.606, top-1 acc: 0.849top-5 acc: 0.849
2025-03-06 17:53:55,567 - INFO - Beginning epoch 72/800
2025-03-06 17:53:55,572 - INFO - training batch 1, loss: 0.644, 32/60000 datapoints
2025-03-06 17:53:55,769 - INFO - training batch 51, loss: 0.547, 1632/60000 datapoints
2025-03-06 17:53:55,966 - INFO - training batch 101, loss: 0.711, 3232/60000 datapoints
2025-03-06 17:53:56,171 - INFO - training batch 151, loss: 0.717, 4832/60000 datapoints
2025-03-06 17:53:56,367 - INFO - training batch 201, loss: 0.729, 6432/60000 datapoints
2025-03-06 17:53:56,563 - INFO - training batch 251, loss: 0.847, 8032/60000 datapoints
2025-03-06 17:53:56,758 - INFO - training batch 301, loss: 0.608, 9632/60000 datapoints
2025-03-06 17:53:56,952 - INFO - training batch 351, loss: 0.888, 11232/60000 datapoints
2025-03-06 17:53:57,147 - INFO - training batch 401, loss: 0.689, 12832/60000 datapoints
2025-03-06 17:53:57,345 - INFO - training batch 451, loss: 0.688, 14432/60000 datapoints
2025-03-06 17:53:57,543 - INFO - training batch 501, loss: 0.768, 16032/60000 datapoints
2025-03-06 17:53:57,739 - INFO - training batch 551, loss: 0.563, 17632/60000 datapoints
2025-03-06 17:53:57,932 - INFO - training batch 601, loss: 0.746, 19232/60000 datapoints
2025-03-06 17:53:58,126 - INFO - training batch 651, loss: 0.728, 20832/60000 datapoints
2025-03-06 17:53:58,317 - INFO - training batch 701, loss: 0.643, 22432/60000 datapoints
2025-03-06 17:53:58,512 - INFO - training batch 751, loss: 0.615, 24032/60000 datapoints
2025-03-06 17:53:58,710 - INFO - training batch 801, loss: 0.757, 25632/60000 datapoints
2025-03-06 17:53:58,903 - INFO - training batch 851, loss: 0.672, 27232/60000 datapoints
2025-03-06 17:53:59,096 - INFO - training batch 901, loss: 0.826, 28832/60000 datapoints
2025-03-06 17:53:59,288 - INFO - training batch 951, loss: 0.699, 30432/60000 datapoints
2025-03-06 17:53:59,492 - INFO - training batch 1001, loss: 0.773, 32032/60000 datapoints
2025-03-06 17:53:59,685 - INFO - training batch 1051, loss: 0.725, 33632/60000 datapoints
2025-03-06 17:53:59,879 - INFO - training batch 1101, loss: 0.825, 35232/60000 datapoints
2025-03-06 17:54:00,074 - INFO - training batch 1151, loss: 0.592, 36832/60000 datapoints
2025-03-06 17:54:00,267 - INFO - training batch 1201, loss: 0.621, 38432/60000 datapoints
2025-03-06 17:54:00,460 - INFO - training batch 1251, loss: 0.595, 40032/60000 datapoints
2025-03-06 17:54:00,660 - INFO - training batch 1301, loss: 0.626, 41632/60000 datapoints
2025-03-06 17:54:00,854 - INFO - training batch 1351, loss: 0.565, 43232/60000 datapoints
2025-03-06 17:54:01,048 - INFO - training batch 1401, loss: 0.540, 44832/60000 datapoints
2025-03-06 17:54:01,241 - INFO - training batch 1451, loss: 0.577, 46432/60000 datapoints
2025-03-06 17:54:01,439 - INFO - training batch 1501, loss: 0.598, 48032/60000 datapoints
2025-03-06 17:54:01,641 - INFO - training batch 1551, loss: 0.745, 49632/60000 datapoints
2025-03-06 17:54:01,873 - INFO - training batch 1601, loss: 0.851, 51232/60000 datapoints
2025-03-06 17:54:02,068 - INFO - training batch 1651, loss: 0.539, 52832/60000 datapoints
2025-03-06 17:54:02,261 - INFO - training batch 1701, loss: 0.854, 54432/60000 datapoints
2025-03-06 17:54:02,453 - INFO - training batch 1751, loss: 0.645, 56032/60000 datapoints
2025-03-06 17:54:02,649 - INFO - training batch 1801, loss: 0.653, 57632/60000 datapoints
2025-03-06 17:54:02,841 - INFO - training batch 1851, loss: 0.806, 59232/60000 datapoints
2025-03-06 17:54:02,943 - INFO - validation batch 1, loss: 0.976, 32/10016 datapoints
2025-03-06 17:54:03,095 - INFO - validation batch 51, loss: 0.594, 1632/10016 datapoints
2025-03-06 17:54:03,246 - INFO - validation batch 101, loss: 0.763, 3232/10016 datapoints
2025-03-06 17:54:03,400 - INFO - validation batch 151, loss: 0.531, 4832/10016 datapoints
2025-03-06 17:54:03,554 - INFO - validation batch 201, loss: 0.683, 6432/10016 datapoints
2025-03-06 17:54:03,708 - INFO - validation batch 251, loss: 0.638, 8032/10016 datapoints
2025-03-06 17:54:03,861 - INFO - validation batch 301, loss: 0.505, 9632/10016 datapoints
2025-03-06 17:54:03,896 - INFO - Epoch 72/800 done.
2025-03-06 17:54:03,896 - INFO - Final validation performance:
Loss: 0.670, top-1 acc: 0.850top-5 acc: 0.850
2025-03-06 17:54:03,896 - INFO - Beginning epoch 73/800
2025-03-06 17:54:03,904 - INFO - training batch 1, loss: 0.769, 32/60000 datapoints
2025-03-06 17:54:04,123 - INFO - training batch 51, loss: 0.683, 1632/60000 datapoints
2025-03-06 17:54:04,314 - INFO - training batch 101, loss: 0.668, 3232/60000 datapoints
2025-03-06 17:54:04,508 - INFO - training batch 151, loss: 0.606, 4832/60000 datapoints
2025-03-06 17:54:04,709 - INFO - training batch 201, loss: 0.781, 6432/60000 datapoints
2025-03-06 17:54:04,907 - INFO - training batch 251, loss: 0.800, 8032/60000 datapoints
2025-03-06 17:54:05,102 - INFO - training batch 301, loss: 0.546, 9632/60000 datapoints
2025-03-06 17:54:05,296 - INFO - training batch 351, loss: 0.879, 11232/60000 datapoints
2025-03-06 17:54:05,492 - INFO - training batch 401, loss: 0.797, 12832/60000 datapoints
2025-03-06 17:54:05,688 - INFO - training batch 451, loss: 0.642, 14432/60000 datapoints
2025-03-06 17:54:05,884 - INFO - training batch 501, loss: 0.974, 16032/60000 datapoints
2025-03-06 17:54:06,080 - INFO - training batch 551, loss: 0.498, 17632/60000 datapoints
2025-03-06 17:54:06,280 - INFO - training batch 601, loss: 0.610, 19232/60000 datapoints
2025-03-06 17:54:06,474 - INFO - training batch 651, loss: 0.657, 20832/60000 datapoints
2025-03-06 17:54:06,670 - INFO - training batch 701, loss: 0.704, 22432/60000 datapoints
2025-03-06 17:54:06,864 - INFO - training batch 751, loss: 0.705, 24032/60000 datapoints
2025-03-06 17:54:07,058 - INFO - training batch 801, loss: 0.654, 25632/60000 datapoints
2025-03-06 17:54:07,251 - INFO - training batch 851, loss: 0.649, 27232/60000 datapoints
2025-03-06 17:54:07,445 - INFO - training batch 901, loss: 0.656, 28832/60000 datapoints
2025-03-06 17:54:07,642 - INFO - training batch 951, loss: 0.821, 30432/60000 datapoints
2025-03-06 17:54:07,838 - INFO - training batch 1001, loss: 0.645, 32032/60000 datapoints
2025-03-06 17:54:08,033 - INFO - training batch 1051, loss: 0.665, 33632/60000 datapoints
2025-03-06 17:54:08,252 - INFO - training batch 1101, loss: 0.522, 35232/60000 datapoints
2025-03-06 17:54:08,447 - INFO - training batch 1151, loss: 0.716, 36832/60000 datapoints
2025-03-06 17:54:08,643 - INFO - training batch 1201, loss: 0.798, 38432/60000 datapoints
2025-03-06 17:54:08,837 - INFO - training batch 1251, loss: 0.672, 40032/60000 datapoints
2025-03-06 17:54:09,035 - INFO - training batch 1301, loss: 0.667, 41632/60000 datapoints
2025-03-06 17:54:09,229 - INFO - training batch 1351, loss: 0.510, 43232/60000 datapoints
2025-03-06 17:54:09,425 - INFO - training batch 1401, loss: 0.556, 44832/60000 datapoints
2025-03-06 17:54:09,620 - INFO - training batch 1451, loss: 0.584, 46432/60000 datapoints
2025-03-06 17:54:09,812 - INFO - training batch 1501, loss: 0.912, 48032/60000 datapoints
2025-03-06 17:54:10,004 - INFO - training batch 1551, loss: 0.824, 49632/60000 datapoints
2025-03-06 17:54:10,199 - INFO - training batch 1601, loss: 0.762, 51232/60000 datapoints
2025-03-06 17:54:10,392 - INFO - training batch 1651, loss: 0.559, 52832/60000 datapoints
2025-03-06 17:54:10,586 - INFO - training batch 1701, loss: 0.461, 54432/60000 datapoints
2025-03-06 17:54:10,782 - INFO - training batch 1751, loss: 0.591, 56032/60000 datapoints
2025-03-06 17:54:10,975 - INFO - training batch 1801, loss: 0.742, 57632/60000 datapoints
2025-03-06 17:54:11,169 - INFO - training batch 1851, loss: 0.755, 59232/60000 datapoints
2025-03-06 17:54:11,271 - INFO - validation batch 1, loss: 0.456, 32/10016 datapoints
2025-03-06 17:54:11,425 - INFO - validation batch 51, loss: 0.434, 1632/10016 datapoints
2025-03-06 17:54:11,581 - INFO - validation batch 101, loss: 0.729, 3232/10016 datapoints
2025-03-06 17:54:11,734 - INFO - validation batch 151, loss: 0.608, 4832/10016 datapoints
2025-03-06 17:54:11,888 - INFO - validation batch 201, loss: 0.743, 6432/10016 datapoints
2025-03-06 17:54:12,039 - INFO - validation batch 251, loss: 0.773, 8032/10016 datapoints
2025-03-06 17:54:12,192 - INFO - validation batch 301, loss: 0.596, 9632/10016 datapoints
2025-03-06 17:54:12,229 - INFO - Epoch 73/800 done.
2025-03-06 17:54:12,229 - INFO - Final validation performance:
Loss: 0.620, top-1 acc: 0.851top-5 acc: 0.851
2025-03-06 17:54:12,229 - INFO - Beginning epoch 74/800
2025-03-06 17:54:12,235 - INFO - training batch 1, loss: 0.412, 32/60000 datapoints
2025-03-06 17:54:12,428 - INFO - training batch 51, loss: 0.865, 1632/60000 datapoints
2025-03-06 17:54:12,623 - INFO - training batch 101, loss: 0.649, 3232/60000 datapoints
2025-03-06 17:54:12,819 - INFO - training batch 151, loss: 0.525, 4832/60000 datapoints
2025-03-06 17:54:13,013 - INFO - training batch 201, loss: 0.816, 6432/60000 datapoints
2025-03-06 17:54:13,210 - INFO - training batch 251, loss: 0.898, 8032/60000 datapoints
2025-03-06 17:54:13,408 - INFO - training batch 301, loss: 0.646, 9632/60000 datapoints
2025-03-06 17:54:13,607 - INFO - training batch 351, loss: 0.713, 11232/60000 datapoints
2025-03-06 17:54:13,803 - INFO - training batch 401, loss: 0.691, 12832/60000 datapoints
2025-03-06 17:54:13,997 - INFO - training batch 451, loss: 0.735, 14432/60000 datapoints
2025-03-06 17:54:14,212 - INFO - training batch 501, loss: 0.589, 16032/60000 datapoints
2025-03-06 17:54:14,405 - INFO - training batch 551, loss: 0.665, 17632/60000 datapoints
2025-03-06 17:54:14,599 - INFO - training batch 601, loss: 0.457, 19232/60000 datapoints
2025-03-06 17:54:14,792 - INFO - training batch 651, loss: 0.903, 20832/60000 datapoints
2025-03-06 17:54:14,990 - INFO - training batch 701, loss: 0.703, 22432/60000 datapoints
2025-03-06 17:54:15,184 - INFO - training batch 751, loss: 0.698, 24032/60000 datapoints
2025-03-06 17:54:15,377 - INFO - training batch 801, loss: 0.817, 25632/60000 datapoints
2025-03-06 17:54:15,573 - INFO - training batch 851, loss: 0.916, 27232/60000 datapoints
2025-03-06 17:54:15,767 - INFO - training batch 901, loss: 0.752, 28832/60000 datapoints
2025-03-06 17:54:15,961 - INFO - training batch 951, loss: 0.806, 30432/60000 datapoints
2025-03-06 17:54:16,158 - INFO - training batch 1001, loss: 0.681, 32032/60000 datapoints
2025-03-06 17:54:16,351 - INFO - training batch 1051, loss: 0.788, 33632/60000 datapoints
2025-03-06 17:54:16,544 - INFO - training batch 1101, loss: 0.687, 35232/60000 datapoints
2025-03-06 17:54:16,741 - INFO - training batch 1151, loss: 0.768, 36832/60000 datapoints
2025-03-06 17:54:16,934 - INFO - training batch 1201, loss: 0.721, 38432/60000 datapoints
2025-03-06 17:54:17,127 - INFO - training batch 1251, loss: 0.616, 40032/60000 datapoints
2025-03-06 17:54:17,324 - INFO - training batch 1301, loss: 0.630, 41632/60000 datapoints
2025-03-06 17:54:17,520 - INFO - training batch 1351, loss: 0.470, 43232/60000 datapoints
2025-03-06 17:54:17,716 - INFO - training batch 1401, loss: 0.767, 44832/60000 datapoints
2025-03-06 17:54:17,910 - INFO - training batch 1451, loss: 0.800, 46432/60000 datapoints
2025-03-06 17:54:18,107 - INFO - training batch 1501, loss: 0.577, 48032/60000 datapoints
2025-03-06 17:54:18,301 - INFO - training batch 1551, loss: 0.856, 49632/60000 datapoints
2025-03-06 17:54:18,497 - INFO - training batch 1601, loss: 0.913, 51232/60000 datapoints
2025-03-06 17:54:18,698 - INFO - training batch 1651, loss: 0.773, 52832/60000 datapoints
2025-03-06 17:54:18,892 - INFO - training batch 1701, loss: 0.707, 54432/60000 datapoints
2025-03-06 17:54:19,084 - INFO - training batch 1751, loss: 0.682, 56032/60000 datapoints
2025-03-06 17:54:19,279 - INFO - training batch 1801, loss: 0.583, 57632/60000 datapoints
2025-03-06 17:54:19,474 - INFO - training batch 1851, loss: 0.756, 59232/60000 datapoints
2025-03-06 17:54:19,597 - INFO - validation batch 1, loss: 0.723, 32/10016 datapoints
2025-03-06 17:54:19,760 - INFO - validation batch 51, loss: 0.718, 1632/10016 datapoints
2025-03-06 17:54:19,912 - INFO - validation batch 101, loss: 0.734, 3232/10016 datapoints
2025-03-06 17:54:20,066 - INFO - validation batch 151, loss: 0.496, 4832/10016 datapoints
2025-03-06 17:54:20,218 - INFO - validation batch 201, loss: 0.626, 6432/10016 datapoints
2025-03-06 17:54:20,370 - INFO - validation batch 251, loss: 0.608, 8032/10016 datapoints
2025-03-06 17:54:20,521 - INFO - validation batch 301, loss: 0.493, 9632/10016 datapoints
2025-03-06 17:54:20,557 - INFO - Epoch 74/800 done.
2025-03-06 17:54:20,557 - INFO - Final validation performance:
Loss: 0.628, top-1 acc: 0.853top-5 acc: 0.853
2025-03-06 17:54:20,558 - INFO - Beginning epoch 75/800
2025-03-06 17:54:20,565 - INFO - training batch 1, loss: 0.630, 32/60000 datapoints
2025-03-06 17:54:20,763 - INFO - training batch 51, loss: 0.810, 1632/60000 datapoints
2025-03-06 17:54:20,959 - INFO - training batch 101, loss: 0.867, 3232/60000 datapoints
2025-03-06 17:54:21,151 - INFO - training batch 151, loss: 0.739, 4832/60000 datapoints
2025-03-06 17:54:21,345 - INFO - training batch 201, loss: 0.620, 6432/60000 datapoints
2025-03-06 17:54:21,542 - INFO - training batch 251, loss: 0.559, 8032/60000 datapoints
2025-03-06 17:54:21,739 - INFO - training batch 301, loss: 0.684, 9632/60000 datapoints
2025-03-06 17:54:21,934 - INFO - training batch 351, loss: 0.694, 11232/60000 datapoints
2025-03-06 17:54:22,127 - INFO - training batch 401, loss: 0.551, 12832/60000 datapoints
2025-03-06 17:54:22,324 - INFO - training batch 451, loss: 0.556, 14432/60000 datapoints
2025-03-06 17:54:22,516 - INFO - training batch 501, loss: 0.422, 16032/60000 datapoints
2025-03-06 17:54:22,713 - INFO - training batch 551, loss: 0.583, 17632/60000 datapoints
2025-03-06 17:54:22,907 - INFO - training batch 601, loss: 0.805, 19232/60000 datapoints
2025-03-06 17:54:23,102 - INFO - training batch 651, loss: 0.388, 20832/60000 datapoints
2025-03-06 17:54:23,297 - INFO - training batch 701, loss: 0.531, 22432/60000 datapoints
2025-03-06 17:54:23,495 - INFO - training batch 751, loss: 0.615, 24032/60000 datapoints
2025-03-06 17:54:23,691 - INFO - training batch 801, loss: 0.659, 25632/60000 datapoints
2025-03-06 17:54:23,884 - INFO - training batch 851, loss: 0.769, 27232/60000 datapoints
2025-03-06 17:54:24,078 - INFO - training batch 901, loss: 0.655, 28832/60000 datapoints
2025-03-06 17:54:24,291 - INFO - training batch 951, loss: 0.957, 30432/60000 datapoints
2025-03-06 17:54:24,485 - INFO - training batch 1001, loss: 0.833, 32032/60000 datapoints
2025-03-06 17:54:24,683 - INFO - training batch 1051, loss: 0.447, 33632/60000 datapoints
2025-03-06 17:54:24,883 - INFO - training batch 1101, loss: 0.632, 35232/60000 datapoints
2025-03-06 17:54:25,076 - INFO - training batch 1151, loss: 0.828, 36832/60000 datapoints
2025-03-06 17:54:25,269 - INFO - training batch 1201, loss: 0.591, 38432/60000 datapoints
2025-03-06 17:54:25,468 - INFO - training batch 1251, loss: 0.523, 40032/60000 datapoints
2025-03-06 17:54:25,670 - INFO - training batch 1301, loss: 0.720, 41632/60000 datapoints
2025-03-06 17:54:25,865 - INFO - training batch 1351, loss: 0.619, 43232/60000 datapoints
2025-03-06 17:54:26,058 - INFO - training batch 1401, loss: 0.706, 44832/60000 datapoints
2025-03-06 17:54:26,256 - INFO - training batch 1451, loss: 0.596, 46432/60000 datapoints
2025-03-06 17:54:26,450 - INFO - training batch 1501, loss: 0.492, 48032/60000 datapoints
2025-03-06 17:54:26,643 - INFO - training batch 1551, loss: 0.719, 49632/60000 datapoints
2025-03-06 17:54:26,840 - INFO - training batch 1601, loss: 0.750, 51232/60000 datapoints
2025-03-06 17:54:27,033 - INFO - training batch 1651, loss: 0.538, 52832/60000 datapoints
2025-03-06 17:54:27,228 - INFO - training batch 1701, loss: 0.507, 54432/60000 datapoints
2025-03-06 17:54:27,421 - INFO - training batch 1751, loss: 0.706, 56032/60000 datapoints
2025-03-06 17:54:27,626 - INFO - training batch 1801, loss: 0.582, 57632/60000 datapoints
2025-03-06 17:54:27,820 - INFO - training batch 1851, loss: 0.598, 59232/60000 datapoints
2025-03-06 17:54:27,921 - INFO - validation batch 1, loss: 0.587, 32/10016 datapoints
2025-03-06 17:54:28,073 - INFO - validation batch 51, loss: 0.510, 1632/10016 datapoints
2025-03-06 17:54:28,228 - INFO - validation batch 101, loss: 0.541, 3232/10016 datapoints
2025-03-06 17:54:28,380 - INFO - validation batch 151, loss: 0.673, 4832/10016 datapoints
2025-03-06 17:54:28,532 - INFO - validation batch 201, loss: 0.533, 6432/10016 datapoints
2025-03-06 17:54:28,688 - INFO - validation batch 251, loss: 0.561, 8032/10016 datapoints
2025-03-06 17:54:28,842 - INFO - validation batch 301, loss: 0.679, 9632/10016 datapoints
2025-03-06 17:54:28,880 - INFO - Epoch 75/800 done.
2025-03-06 17:54:28,880 - INFO - Final validation performance:
Loss: 0.583, top-1 acc: 0.853top-5 acc: 0.853
2025-03-06 17:54:28,880 - INFO - Beginning epoch 76/800
2025-03-06 17:54:28,886 - INFO - training batch 1, loss: 0.653, 32/60000 datapoints
2025-03-06 17:54:29,080 - INFO - training batch 51, loss: 0.604, 1632/60000 datapoints
2025-03-06 17:54:29,273 - INFO - training batch 101, loss: 0.510, 3232/60000 datapoints
2025-03-06 17:54:29,471 - INFO - training batch 151, loss: 0.577, 4832/60000 datapoints
2025-03-06 17:54:29,665 - INFO - training batch 201, loss: 0.797, 6432/60000 datapoints
2025-03-06 17:54:29,861 - INFO - training batch 251, loss: 0.647, 8032/60000 datapoints
2025-03-06 17:54:30,054 - INFO - training batch 301, loss: 0.577, 9632/60000 datapoints
2025-03-06 17:54:30,248 - INFO - training batch 351, loss: 0.833, 11232/60000 datapoints
2025-03-06 17:54:30,442 - INFO - training batch 401, loss: 0.662, 12832/60000 datapoints
2025-03-06 17:54:30,636 - INFO - training batch 451, loss: 0.687, 14432/60000 datapoints
2025-03-06 17:54:30,828 - INFO - training batch 501, loss: 0.703, 16032/60000 datapoints
2025-03-06 17:54:31,024 - INFO - training batch 551, loss: 0.643, 17632/60000 datapoints
2025-03-06 17:54:31,218 - INFO - training batch 601, loss: 0.532, 19232/60000 datapoints
2025-03-06 17:54:31,412 - INFO - training batch 651, loss: 0.747, 20832/60000 datapoints
2025-03-06 17:54:31,610 - INFO - training batch 701, loss: 0.452, 22432/60000 datapoints
2025-03-06 17:54:31,807 - INFO - training batch 751, loss: 0.549, 24032/60000 datapoints
2025-03-06 17:54:32,000 - INFO - training batch 801, loss: 0.577, 25632/60000 datapoints
2025-03-06 17:54:32,194 - INFO - training batch 851, loss: 0.684, 27232/60000 datapoints
2025-03-06 17:54:32,388 - INFO - training batch 901, loss: 0.485, 28832/60000 datapoints
2025-03-06 17:54:32,583 - INFO - training batch 951, loss: 0.555, 30432/60000 datapoints
2025-03-06 17:54:32,780 - INFO - training batch 1001, loss: 0.967, 32032/60000 datapoints
2025-03-06 17:54:32,976 - INFO - training batch 1051, loss: 0.585, 33632/60000 datapoints
2025-03-06 17:54:33,168 - INFO - training batch 1101, loss: 0.570, 35232/60000 datapoints
2025-03-06 17:54:33,363 - INFO - training batch 1151, loss: 0.528, 36832/60000 datapoints
2025-03-06 17:54:33,558 - INFO - training batch 1201, loss: 0.608, 38432/60000 datapoints
2025-03-06 17:54:33,754 - INFO - training batch 1251, loss: 0.904, 40032/60000 datapoints
2025-03-06 17:54:33,949 - INFO - training batch 1301, loss: 0.528, 41632/60000 datapoints
2025-03-06 17:54:34,140 - INFO - training batch 1351, loss: 0.791, 43232/60000 datapoints
2025-03-06 17:54:34,352 - INFO - training batch 1401, loss: 0.659, 44832/60000 datapoints
2025-03-06 17:54:34,545 - INFO - training batch 1451, loss: 0.600, 46432/60000 datapoints
2025-03-06 17:54:34,741 - INFO - training batch 1501, loss: 0.764, 48032/60000 datapoints
2025-03-06 17:54:34,939 - INFO - training batch 1551, loss: 0.806, 49632/60000 datapoints
2025-03-06 17:54:35,134 - INFO - training batch 1601, loss: 0.751, 51232/60000 datapoints
2025-03-06 17:54:35,328 - INFO - training batch 1651, loss: 0.703, 52832/60000 datapoints
2025-03-06 17:54:35,525 - INFO - training batch 1701, loss: 0.562, 54432/60000 datapoints
2025-03-06 17:54:35,720 - INFO - training batch 1751, loss: 0.511, 56032/60000 datapoints
2025-03-06 17:54:35,916 - INFO - training batch 1801, loss: 0.766, 57632/60000 datapoints
2025-03-06 17:54:36,110 - INFO - training batch 1851, loss: 0.525, 59232/60000 datapoints
2025-03-06 17:54:36,214 - INFO - validation batch 1, loss: 0.553, 32/10016 datapoints
2025-03-06 17:54:36,367 - INFO - validation batch 51, loss: 0.858, 1632/10016 datapoints
2025-03-06 17:54:36,518 - INFO - validation batch 101, loss: 0.624, 3232/10016 datapoints
2025-03-06 17:54:36,671 - INFO - validation batch 151, loss: 0.505, 4832/10016 datapoints
2025-03-06 17:54:36,823 - INFO - validation batch 201, loss: 0.596, 6432/10016 datapoints
2025-03-06 17:54:36,978 - INFO - validation batch 251, loss: 0.555, 8032/10016 datapoints
2025-03-06 17:54:37,128 - INFO - validation batch 301, loss: 0.572, 9632/10016 datapoints
2025-03-06 17:54:37,165 - INFO - Epoch 76/800 done.
2025-03-06 17:54:37,165 - INFO - Final validation performance:
Loss: 0.609, top-1 acc: 0.853top-5 acc: 0.853
2025-03-06 17:54:37,165 - INFO - Beginning epoch 77/800
2025-03-06 17:54:37,171 - INFO - training batch 1, loss: 0.778, 32/60000 datapoints
2025-03-06 17:54:37,366 - INFO - training batch 51, loss: 0.845, 1632/60000 datapoints
2025-03-06 17:54:37,563 - INFO - training batch 101, loss: 0.677, 3232/60000 datapoints
2025-03-06 17:54:37,774 - INFO - training batch 151, loss: 0.752, 4832/60000 datapoints
2025-03-06 17:54:37,968 - INFO - training batch 201, loss: 0.483, 6432/60000 datapoints
2025-03-06 17:54:38,162 - INFO - training batch 251, loss: 0.756, 8032/60000 datapoints
2025-03-06 17:54:38,356 - INFO - training batch 301, loss: 0.715, 9632/60000 datapoints
2025-03-06 17:54:38,550 - INFO - training batch 351, loss: 0.587, 11232/60000 datapoints
2025-03-06 17:54:38,747 - INFO - training batch 401, loss: 0.856, 12832/60000 datapoints
2025-03-06 17:54:38,941 - INFO - training batch 451, loss: 0.517, 14432/60000 datapoints
2025-03-06 17:54:39,137 - INFO - training batch 501, loss: 0.633, 16032/60000 datapoints
2025-03-06 17:54:39,329 - INFO - training batch 551, loss: 0.492, 17632/60000 datapoints
2025-03-06 17:54:39,526 - INFO - training batch 601, loss: 0.618, 19232/60000 datapoints
2025-03-06 17:54:39,723 - INFO - training batch 651, loss: 0.675, 20832/60000 datapoints
2025-03-06 17:54:39,918 - INFO - training batch 701, loss: 0.659, 22432/60000 datapoints
2025-03-06 17:54:40,113 - INFO - training batch 751, loss: 0.640, 24032/60000 datapoints
2025-03-06 17:54:40,307 - INFO - training batch 801, loss: 0.598, 25632/60000 datapoints
2025-03-06 17:54:40,504 - INFO - training batch 851, loss: 0.499, 27232/60000 datapoints
2025-03-06 17:54:40,701 - INFO - training batch 901, loss: 0.647, 28832/60000 datapoints
2025-03-06 17:54:40,894 - INFO - training batch 951, loss: 0.535, 30432/60000 datapoints
2025-03-06 17:54:41,088 - INFO - training batch 1001, loss: 0.529, 32032/60000 datapoints
2025-03-06 17:54:41,281 - INFO - training batch 1051, loss: 0.377, 33632/60000 datapoints
2025-03-06 17:54:41,477 - INFO - training batch 1101, loss: 0.691, 35232/60000 datapoints
2025-03-06 17:54:41,674 - INFO - training batch 1151, loss: 0.523, 36832/60000 datapoints
2025-03-06 17:54:41,870 - INFO - training batch 1201, loss: 0.561, 38432/60000 datapoints
2025-03-06 17:54:42,065 - INFO - training batch 1251, loss: 0.681, 40032/60000 datapoints
2025-03-06 17:54:42,261 - INFO - training batch 1301, loss: 0.753, 41632/60000 datapoints
2025-03-06 17:54:42,454 - INFO - training batch 1351, loss: 0.687, 43232/60000 datapoints
2025-03-06 17:54:42,650 - INFO - training batch 1401, loss: 0.595, 44832/60000 datapoints
2025-03-06 17:54:42,841 - INFO - training batch 1451, loss: 0.751, 46432/60000 datapoints
2025-03-06 17:54:43,034 - INFO - training batch 1501, loss: 0.589, 48032/60000 datapoints
2025-03-06 17:54:43,228 - INFO - training batch 1551, loss: 0.557, 49632/60000 datapoints
2025-03-06 17:54:43,420 - INFO - training batch 1601, loss: 0.704, 51232/60000 datapoints
2025-03-06 17:54:43,620 - INFO - training batch 1651, loss: 0.581, 52832/60000 datapoints
2025-03-06 17:54:43,812 - INFO - training batch 1701, loss: 0.747, 54432/60000 datapoints
2025-03-06 17:54:44,005 - INFO - training batch 1751, loss: 0.746, 56032/60000 datapoints
2025-03-06 17:54:44,199 - INFO - training batch 1801, loss: 0.540, 57632/60000 datapoints
2025-03-06 17:54:44,410 - INFO - training batch 1851, loss: 0.480, 59232/60000 datapoints
2025-03-06 17:54:44,510 - INFO - validation batch 1, loss: 0.748, 32/10016 datapoints
2025-03-06 17:54:44,662 - INFO - validation batch 51, loss: 0.904, 1632/10016 datapoints
2025-03-06 17:54:44,813 - INFO - validation batch 101, loss: 0.688, 3232/10016 datapoints
2025-03-06 17:54:44,972 - INFO - validation batch 151, loss: 0.472, 4832/10016 datapoints
2025-03-06 17:54:45,127 - INFO - validation batch 201, loss: 0.598, 6432/10016 datapoints
2025-03-06 17:54:45,280 - INFO - validation batch 251, loss: 0.513, 8032/10016 datapoints
2025-03-06 17:54:45,432 - INFO - validation batch 301, loss: 0.762, 9632/10016 datapoints
2025-03-06 17:54:45,470 - INFO - Epoch 77/800 done.
2025-03-06 17:54:45,470 - INFO - Final validation performance:
Loss: 0.669, top-1 acc: 0.854top-5 acc: 0.854
2025-03-06 17:54:45,471 - INFO - Beginning epoch 78/800
2025-03-06 17:54:45,476 - INFO - training batch 1, loss: 0.648, 32/60000 datapoints
2025-03-06 17:54:45,673 - INFO - training batch 51, loss: 0.623, 1632/60000 datapoints
2025-03-06 17:54:45,866 - INFO - training batch 101, loss: 0.652, 3232/60000 datapoints
2025-03-06 17:54:46,061 - INFO - training batch 151, loss: 0.764, 4832/60000 datapoints
2025-03-06 17:54:46,259 - INFO - training batch 201, loss: 0.727, 6432/60000 datapoints
2025-03-06 17:54:46,452 - INFO - training batch 251, loss: 0.622, 8032/60000 datapoints
2025-03-06 17:54:46,649 - INFO - training batch 301, loss: 0.421, 9632/60000 datapoints
2025-03-06 17:54:46,843 - INFO - training batch 351, loss: 0.789, 11232/60000 datapoints
2025-03-06 17:54:47,037 - INFO - training batch 401, loss: 0.751, 12832/60000 datapoints
2025-03-06 17:54:47,231 - INFO - training batch 451, loss: 0.617, 14432/60000 datapoints
2025-03-06 17:54:47,426 - INFO - training batch 501, loss: 0.512, 16032/60000 datapoints
2025-03-06 17:54:47,625 - INFO - training batch 551, loss: 0.353, 17632/60000 datapoints
2025-03-06 17:54:47,819 - INFO - training batch 601, loss: 0.676, 19232/60000 datapoints
2025-03-06 17:54:48,013 - INFO - training batch 651, loss: 0.646, 20832/60000 datapoints
2025-03-06 17:54:48,204 - INFO - training batch 701, loss: 0.554, 22432/60000 datapoints
2025-03-06 17:54:48,397 - INFO - training batch 751, loss: 0.852, 24032/60000 datapoints
2025-03-06 17:54:48,592 - INFO - training batch 801, loss: 0.792, 25632/60000 datapoints
2025-03-06 17:54:48,786 - INFO - training batch 851, loss: 0.677, 27232/60000 datapoints
2025-03-06 17:54:48,987 - INFO - training batch 901, loss: 0.709, 28832/60000 datapoints
2025-03-06 17:54:49,183 - INFO - training batch 951, loss: 0.510, 30432/60000 datapoints
2025-03-06 17:54:49,374 - INFO - training batch 1001, loss: 0.670, 32032/60000 datapoints
2025-03-06 17:54:49,570 - INFO - training batch 1051, loss: 0.645, 33632/60000 datapoints
2025-03-06 17:54:49,766 - INFO - training batch 1101, loss: 0.693, 35232/60000 datapoints
2025-03-06 17:54:49,961 - INFO - training batch 1151, loss: 0.607, 36832/60000 datapoints
2025-03-06 17:54:50,157 - INFO - training batch 1201, loss: 0.597, 38432/60000 datapoints
2025-03-06 17:54:50,350 - INFO - training batch 1251, loss: 0.633, 40032/60000 datapoints
2025-03-06 17:54:50,543 - INFO - training batch 1301, loss: 1.042, 41632/60000 datapoints
2025-03-06 17:54:50,737 - INFO - training batch 1351, loss: 0.770, 43232/60000 datapoints
2025-03-06 17:54:50,932 - INFO - training batch 1401, loss: 0.600, 44832/60000 datapoints
2025-03-06 17:54:51,126 - INFO - training batch 1451, loss: 0.458, 46432/60000 datapoints
2025-03-06 17:54:51,319 - INFO - training batch 1501, loss: 0.380, 48032/60000 datapoints
2025-03-06 17:54:51,512 - INFO - training batch 1551, loss: 0.537, 49632/60000 datapoints
2025-03-06 17:54:51,718 - INFO - training batch 1601, loss: 0.553, 51232/60000 datapoints
2025-03-06 17:54:51,913 - INFO - training batch 1651, loss: 0.549, 52832/60000 datapoints
2025-03-06 17:54:52,106 - INFO - training batch 1701, loss: 0.744, 54432/60000 datapoints
2025-03-06 17:54:52,299 - INFO - training batch 1751, loss: 0.744, 56032/60000 datapoints
2025-03-06 17:54:52,493 - INFO - training batch 1801, loss: 0.758, 57632/60000 datapoints
2025-03-06 17:54:52,692 - INFO - training batch 1851, loss: 0.536, 59232/60000 datapoints
2025-03-06 17:54:52,794 - INFO - validation batch 1, loss: 0.561, 32/10016 datapoints
2025-03-06 17:54:52,947 - INFO - validation batch 51, loss: 0.764, 1632/10016 datapoints
2025-03-06 17:54:53,102 - INFO - validation batch 101, loss: 0.799, 3232/10016 datapoints
2025-03-06 17:54:53,254 - INFO - validation batch 151, loss: 0.697, 4832/10016 datapoints
2025-03-06 17:54:53,410 - INFO - validation batch 201, loss: 0.585, 6432/10016 datapoints
2025-03-06 17:54:53,563 - INFO - validation batch 251, loss: 0.472, 8032/10016 datapoints
2025-03-06 17:54:53,718 - INFO - validation batch 301, loss: 0.671, 9632/10016 datapoints
2025-03-06 17:54:53,754 - INFO - Epoch 78/800 done.
2025-03-06 17:54:53,754 - INFO - Final validation performance:
Loss: 0.650, top-1 acc: 0.855top-5 acc: 0.855
2025-03-06 17:54:53,755 - INFO - Beginning epoch 79/800
2025-03-06 17:54:53,761 - INFO - training batch 1, loss: 0.463, 32/60000 datapoints
2025-03-06 17:54:53,951 - INFO - training batch 51, loss: 0.746, 1632/60000 datapoints
2025-03-06 17:54:54,144 - INFO - training batch 101, loss: 0.696, 3232/60000 datapoints
2025-03-06 17:54:54,335 - INFO - training batch 151, loss: 0.575, 4832/60000 datapoints
2025-03-06 17:54:54,541 - INFO - training batch 201, loss: 0.624, 6432/60000 datapoints
2025-03-06 17:54:54,737 - INFO - training batch 251, loss: 0.745, 8032/60000 datapoints
2025-03-06 17:54:54,932 - INFO - training batch 301, loss: 0.413, 9632/60000 datapoints
2025-03-06 17:54:55,124 - INFO - training batch 351, loss: 0.766, 11232/60000 datapoints
2025-03-06 17:54:55,314 - INFO - training batch 401, loss: 0.784, 12832/60000 datapoints
2025-03-06 17:54:55,505 - INFO - training batch 451, loss: 1.127, 14432/60000 datapoints
2025-03-06 17:54:55,702 - INFO - training batch 501, loss: 0.628, 16032/60000 datapoints
2025-03-06 17:54:55,894 - INFO - training batch 551, loss: 0.974, 17632/60000 datapoints
2025-03-06 17:54:56,086 - INFO - training batch 601, loss: 0.629, 19232/60000 datapoints
2025-03-06 17:54:56,284 - INFO - training batch 651, loss: 0.530, 20832/60000 datapoints
2025-03-06 17:54:56,474 - INFO - training batch 701, loss: 0.563, 22432/60000 datapoints
2025-03-06 17:54:56,668 - INFO - training batch 751, loss: 0.965, 24032/60000 datapoints
2025-03-06 17:54:56,857 - INFO - training batch 801, loss: 0.496, 25632/60000 datapoints
2025-03-06 17:54:57,048 - INFO - training batch 851, loss: 0.722, 27232/60000 datapoints
2025-03-06 17:54:57,241 - INFO - training batch 901, loss: 0.624, 28832/60000 datapoints
2025-03-06 17:54:57,432 - INFO - training batch 951, loss: 0.661, 30432/60000 datapoints
2025-03-06 17:54:57,633 - INFO - training batch 1001, loss: 0.641, 32032/60000 datapoints
2025-03-06 17:54:57,824 - INFO - training batch 1051, loss: 0.616, 33632/60000 datapoints
2025-03-06 17:54:58,016 - INFO - training batch 1101, loss: 0.718, 35232/60000 datapoints
2025-03-06 17:54:58,208 - INFO - training batch 1151, loss: 0.546, 36832/60000 datapoints
2025-03-06 17:54:58,399 - INFO - training batch 1201, loss: 0.610, 38432/60000 datapoints
2025-03-06 17:54:58,594 - INFO - training batch 1251, loss: 0.666, 40032/60000 datapoints
2025-03-06 17:54:58,790 - INFO - training batch 1301, loss: 0.730, 41632/60000 datapoints
2025-03-06 17:54:58,984 - INFO - training batch 1351, loss: 0.429, 43232/60000 datapoints
2025-03-06 17:54:59,175 - INFO - training batch 1401, loss: 0.926, 44832/60000 datapoints
2025-03-06 17:54:59,365 - INFO - training batch 1451, loss: 0.745, 46432/60000 datapoints
2025-03-06 17:54:59,555 - INFO - training batch 1501, loss: 0.591, 48032/60000 datapoints
2025-03-06 17:54:59,749 - INFO - training batch 1551, loss: 0.461, 49632/60000 datapoints
2025-03-06 17:54:59,940 - INFO - training batch 1601, loss: 0.559, 51232/60000 datapoints
2025-03-06 17:55:00,133 - INFO - training batch 1651, loss: 0.878, 52832/60000 datapoints
2025-03-06 17:55:00,325 - INFO - training batch 1701, loss: 0.398, 54432/60000 datapoints
2025-03-06 17:55:00,513 - INFO - training batch 1751, loss: 0.591, 56032/60000 datapoints
2025-03-06 17:55:00,719 - INFO - training batch 1801, loss: 0.505, 57632/60000 datapoints
2025-03-06 17:55:00,914 - INFO - training batch 1851, loss: 0.649, 59232/60000 datapoints
2025-03-06 17:55:01,013 - INFO - validation batch 1, loss: 0.690, 32/10016 datapoints
2025-03-06 17:55:01,166 - INFO - validation batch 51, loss: 0.514, 1632/10016 datapoints
2025-03-06 17:55:01,317 - INFO - validation batch 101, loss: 0.794, 3232/10016 datapoints
2025-03-06 17:55:01,471 - INFO - validation batch 151, loss: 0.692, 4832/10016 datapoints
2025-03-06 17:55:01,629 - INFO - validation batch 201, loss: 0.456, 6432/10016 datapoints
2025-03-06 17:55:01,781 - INFO - validation batch 251, loss: 0.561, 8032/10016 datapoints
2025-03-06 17:55:01,935 - INFO - validation batch 301, loss: 0.863, 9632/10016 datapoints
2025-03-06 17:55:01,971 - INFO - Epoch 79/800 done.
2025-03-06 17:55:01,971 - INFO - Final validation performance:
Loss: 0.653, top-1 acc: 0.856top-5 acc: 0.856
2025-03-06 17:55:01,972 - INFO - Beginning epoch 80/800
2025-03-06 17:55:01,977 - INFO - training batch 1, loss: 0.564, 32/60000 datapoints
2025-03-06 17:55:02,175 - INFO - training batch 51, loss: 0.641, 1632/60000 datapoints
2025-03-06 17:55:02,369 - INFO - training batch 101, loss: 0.584, 3232/60000 datapoints
2025-03-06 17:55:02,564 - INFO - training batch 151, loss: 0.457, 4832/60000 datapoints
2025-03-06 17:55:02,760 - INFO - training batch 201, loss: 0.507, 6432/60000 datapoints
2025-03-06 17:55:02,956 - INFO - training batch 251, loss: 0.534, 8032/60000 datapoints
2025-03-06 17:55:03,148 - INFO - training batch 301, loss: 0.658, 9632/60000 datapoints
2025-03-06 17:55:03,342 - INFO - training batch 351, loss: 0.662, 11232/60000 datapoints
2025-03-06 17:55:03,536 - INFO - training batch 401, loss: 1.020, 12832/60000 datapoints
2025-03-06 17:55:03,737 - INFO - training batch 451, loss: 0.512, 14432/60000 datapoints
2025-03-06 17:55:03,930 - INFO - training batch 501, loss: 0.523, 16032/60000 datapoints
2025-03-06 17:55:04,125 - INFO - training batch 551, loss: 0.699, 17632/60000 datapoints
2025-03-06 17:55:04,320 - INFO - training batch 601, loss: 0.723, 19232/60000 datapoints
2025-03-06 17:55:04,531 - INFO - training batch 651, loss: 0.605, 20832/60000 datapoints
2025-03-06 17:55:04,732 - INFO - training batch 701, loss: 0.548, 22432/60000 datapoints
2025-03-06 17:55:04,930 - INFO - training batch 751, loss: 0.851, 24032/60000 datapoints
2025-03-06 17:55:05,124 - INFO - training batch 801, loss: 0.392, 25632/60000 datapoints
2025-03-06 17:55:05,320 - INFO - training batch 851, loss: 0.811, 27232/60000 datapoints
2025-03-06 17:55:05,511 - INFO - training batch 901, loss: 0.538, 28832/60000 datapoints
2025-03-06 17:55:05,710 - INFO - training batch 951, loss: 0.560, 30432/60000 datapoints
2025-03-06 17:55:05,906 - INFO - training batch 1001, loss: 0.516, 32032/60000 datapoints
2025-03-06 17:55:06,100 - INFO - training batch 1051, loss: 0.637, 33632/60000 datapoints
2025-03-06 17:55:06,300 - INFO - training batch 1101, loss: 0.270, 35232/60000 datapoints
2025-03-06 17:55:06,492 - INFO - training batch 1151, loss: 0.746, 36832/60000 datapoints
2025-03-06 17:55:06,687 - INFO - training batch 1201, loss: 0.723, 38432/60000 datapoints
2025-03-06 17:55:06,882 - INFO - training batch 1251, loss: 0.629, 40032/60000 datapoints
2025-03-06 17:55:07,074 - INFO - training batch 1301, loss: 0.738, 41632/60000 datapoints
2025-03-06 17:55:07,272 - INFO - training batch 1351, loss: 0.610, 43232/60000 datapoints
2025-03-06 17:55:07,466 - INFO - training batch 1401, loss: 0.590, 44832/60000 datapoints
2025-03-06 17:55:07,667 - INFO - training batch 1451, loss: 0.639, 46432/60000 datapoints
2025-03-06 17:55:07,861 - INFO - training batch 1501, loss: 0.623, 48032/60000 datapoints
2025-03-06 17:55:08,067 - INFO - training batch 1551, loss: 0.741, 49632/60000 datapoints
2025-03-06 17:55:08,289 - INFO - training batch 1601, loss: 0.543, 51232/60000 datapoints
2025-03-06 17:55:08,489 - INFO - training batch 1651, loss: 0.681, 52832/60000 datapoints
2025-03-06 17:55:08,683 - INFO - training batch 1701, loss: 0.721, 54432/60000 datapoints
2025-03-06 17:55:08,877 - INFO - training batch 1751, loss: 0.726, 56032/60000 datapoints
2025-03-06 17:55:09,072 - INFO - training batch 1801, loss: 0.755, 57632/60000 datapoints
2025-03-06 17:55:09,266 - INFO - training batch 1851, loss: 0.567, 59232/60000 datapoints
2025-03-06 17:55:09,367 - INFO - validation batch 1, loss: 0.763, 32/10016 datapoints
2025-03-06 17:55:09,519 - INFO - validation batch 51, loss: 0.568, 1632/10016 datapoints
2025-03-06 17:55:09,675 - INFO - validation batch 101, loss: 0.644, 3232/10016 datapoints
2025-03-06 17:55:09,829 - INFO - validation batch 151, loss: 1.089, 4832/10016 datapoints
2025-03-06 17:55:09,981 - INFO - validation batch 201, loss: 0.613, 6432/10016 datapoints
2025-03-06 17:55:10,138 - INFO - validation batch 251, loss: 0.715, 8032/10016 datapoints
2025-03-06 17:55:10,292 - INFO - validation batch 301, loss: 0.512, 9632/10016 datapoints
2025-03-06 17:55:10,330 - INFO - Epoch 80/800 done.
2025-03-06 17:55:10,330 - INFO - Final validation performance:
Loss: 0.701, top-1 acc: 0.857top-5 acc: 0.857
2025-03-06 17:55:10,330 - INFO - Beginning epoch 81/800
2025-03-06 17:55:10,336 - INFO - training batch 1, loss: 0.661, 32/60000 datapoints
2025-03-06 17:55:10,530 - INFO - training batch 51, loss: 0.723, 1632/60000 datapoints
2025-03-06 17:55:10,726 - INFO - training batch 101, loss: 0.553, 3232/60000 datapoints
2025-03-06 17:55:10,921 - INFO - training batch 151, loss: 0.664, 4832/60000 datapoints
2025-03-06 17:55:11,114 - INFO - training batch 201, loss: 0.600, 6432/60000 datapoints
2025-03-06 17:55:11,308 - INFO - training batch 251, loss: 0.682, 8032/60000 datapoints
2025-03-06 17:55:11,502 - INFO - training batch 301, loss: 0.578, 9632/60000 datapoints
2025-03-06 17:55:11,708 - INFO - training batch 351, loss: 0.702, 11232/60000 datapoints
2025-03-06 17:55:11,906 - INFO - training batch 401, loss: 0.571, 12832/60000 datapoints
2025-03-06 17:55:12,101 - INFO - training batch 451, loss: 0.762, 14432/60000 datapoints
2025-03-06 17:55:12,294 - INFO - training batch 501, loss: 0.556, 16032/60000 datapoints
2025-03-06 17:55:12,488 - INFO - training batch 551, loss: 0.641, 17632/60000 datapoints
2025-03-06 17:55:12,683 - INFO - training batch 601, loss: 0.733, 19232/60000 datapoints
2025-03-06 17:55:12,879 - INFO - training batch 651, loss: 0.670, 20832/60000 datapoints
2025-03-06 17:55:13,077 - INFO - training batch 701, loss: 0.808, 22432/60000 datapoints
2025-03-06 17:55:13,273 - INFO - training batch 751, loss: 0.640, 24032/60000 datapoints
2025-03-06 17:55:13,467 - INFO - training batch 801, loss: 0.653, 25632/60000 datapoints
2025-03-06 17:55:13,666 - INFO - training batch 851, loss: 0.770, 27232/60000 datapoints
2025-03-06 17:55:13,859 - INFO - training batch 901, loss: 0.718, 28832/60000 datapoints
2025-03-06 17:55:14,052 - INFO - training batch 951, loss: 0.548, 30432/60000 datapoints
2025-03-06 17:55:14,246 - INFO - training batch 1001, loss: 0.572, 32032/60000 datapoints
2025-03-06 17:55:14,440 - INFO - training batch 1051, loss: 0.691, 33632/60000 datapoints
2025-03-06 17:55:14,652 - INFO - training batch 1101, loss: 0.692, 35232/60000 datapoints
2025-03-06 17:55:14,850 - INFO - training batch 1151, loss: 0.476, 36832/60000 datapoints
2025-03-06 17:55:15,047 - INFO - training batch 1201, loss: 0.545, 38432/60000 datapoints
2025-03-06 17:55:15,240 - INFO - training batch 1251, loss: 0.544, 40032/60000 datapoints
2025-03-06 17:55:15,435 - INFO - training batch 1301, loss: 0.544, 41632/60000 datapoints
2025-03-06 17:55:15,633 - INFO - training batch 1351, loss: 0.673, 43232/60000 datapoints
2025-03-06 17:55:15,830 - INFO - training batch 1401, loss: 1.035, 44832/60000 datapoints
2025-03-06 17:55:16,023 - INFO - training batch 1451, loss: 0.821, 46432/60000 datapoints
2025-03-06 17:55:16,221 - INFO - training batch 1501, loss: 0.768, 48032/60000 datapoints
2025-03-06 17:55:16,420 - INFO - training batch 1551, loss: 0.579, 49632/60000 datapoints
2025-03-06 17:55:16,615 - INFO - training batch 1601, loss: 0.529, 51232/60000 datapoints
2025-03-06 17:55:16,810 - INFO - training batch 1651, loss: 0.733, 52832/60000 datapoints
2025-03-06 17:55:17,007 - INFO - training batch 1701, loss: 0.638, 54432/60000 datapoints
2025-03-06 17:55:17,201 - INFO - training batch 1751, loss: 0.683, 56032/60000 datapoints
2025-03-06 17:55:17,394 - INFO - training batch 1801, loss: 0.661, 57632/60000 datapoints
2025-03-06 17:55:17,584 - INFO - training batch 1851, loss: 0.681, 59232/60000 datapoints
2025-03-06 17:55:17,688 - INFO - validation batch 1, loss: 0.433, 32/10016 datapoints
2025-03-06 17:55:17,840 - INFO - validation batch 51, loss: 0.710, 1632/10016 datapoints
2025-03-06 17:55:17,993 - INFO - validation batch 101, loss: 0.557, 3232/10016 datapoints
2025-03-06 17:55:18,146 - INFO - validation batch 151, loss: 0.466, 4832/10016 datapoints
2025-03-06 17:55:18,298 - INFO - validation batch 201, loss: 0.675, 6432/10016 datapoints
2025-03-06 17:55:18,450 - INFO - validation batch 251, loss: 0.687, 8032/10016 datapoints
2025-03-06 17:55:18,602 - INFO - validation batch 301, loss: 0.346, 9632/10016 datapoints
2025-03-06 17:55:18,639 - INFO - Epoch 81/800 done.
2025-03-06 17:55:18,640 - INFO - Final validation performance:
Loss: 0.554, top-1 acc: 0.858top-5 acc: 0.858
2025-03-06 17:55:18,640 - INFO - Beginning epoch 82/800
2025-03-06 17:55:18,646 - INFO - training batch 1, loss: 0.568, 32/60000 datapoints
2025-03-06 17:55:18,840 - INFO - training batch 51, loss: 0.647, 1632/60000 datapoints
2025-03-06 17:55:19,040 - INFO - training batch 101, loss: 0.647, 3232/60000 datapoints
2025-03-06 17:55:19,234 - INFO - training batch 151, loss: 0.453, 4832/60000 datapoints
2025-03-06 17:55:19,427 - INFO - training batch 201, loss: 0.758, 6432/60000 datapoints
2025-03-06 17:55:19,622 - INFO - training batch 251, loss: 0.435, 8032/60000 datapoints
2025-03-06 17:55:19,823 - INFO - training batch 301, loss: 0.553, 9632/60000 datapoints
2025-03-06 17:55:20,046 - INFO - training batch 351, loss: 0.560, 11232/60000 datapoints
2025-03-06 17:55:20,249 - INFO - training batch 401, loss: 0.568, 12832/60000 datapoints
2025-03-06 17:55:20,445 - INFO - training batch 451, loss: 0.965, 14432/60000 datapoints
2025-03-06 17:55:20,641 - INFO - training batch 501, loss: 0.633, 16032/60000 datapoints
2025-03-06 17:55:20,833 - INFO - training batch 551, loss: 0.700, 17632/60000 datapoints
2025-03-06 17:55:21,029 - INFO - training batch 601, loss: 0.627, 19232/60000 datapoints
2025-03-06 17:55:21,222 - INFO - training batch 651, loss: 0.710, 20832/60000 datapoints
2025-03-06 17:55:21,415 - INFO - training batch 701, loss: 0.575, 22432/60000 datapoints
2025-03-06 17:55:21,610 - INFO - training batch 751, loss: 0.564, 24032/60000 datapoints
2025-03-06 17:55:21,805 - INFO - training batch 801, loss: 0.924, 25632/60000 datapoints
2025-03-06 17:55:22,002 - INFO - training batch 851, loss: 0.409, 27232/60000 datapoints
2025-03-06 17:55:22,195 - INFO - training batch 901, loss: 0.585, 28832/60000 datapoints
2025-03-06 17:55:22,388 - INFO - training batch 951, loss: 0.855, 30432/60000 datapoints
2025-03-06 17:55:22,581 - INFO - training batch 1001, loss: 0.671, 32032/60000 datapoints
2025-03-06 17:55:22,778 - INFO - training batch 1051, loss: 0.548, 33632/60000 datapoints
2025-03-06 17:55:22,973 - INFO - training batch 1101, loss: 0.402, 35232/60000 datapoints
2025-03-06 17:55:23,170 - INFO - training batch 1151, loss: 0.579, 36832/60000 datapoints
2025-03-06 17:55:23,363 - INFO - training batch 1201, loss: 0.619, 38432/60000 datapoints
2025-03-06 17:55:23,559 - INFO - training batch 1251, loss: 0.957, 40032/60000 datapoints
2025-03-06 17:55:23,757 - INFO - training batch 1301, loss: 0.720, 41632/60000 datapoints
2025-03-06 17:55:23,953 - INFO - training batch 1351, loss: 0.713, 43232/60000 datapoints
2025-03-06 17:55:24,146 - INFO - training batch 1401, loss: 0.614, 44832/60000 datapoints
2025-03-06 17:55:24,341 - INFO - training batch 1451, loss: 0.788, 46432/60000 datapoints
2025-03-06 17:55:24,533 - INFO - training batch 1501, loss: 0.563, 48032/60000 datapoints
2025-03-06 17:55:24,749 - INFO - training batch 1551, loss: 0.375, 49632/60000 datapoints
2025-03-06 17:55:24,946 - INFO - training batch 1601, loss: 0.525, 51232/60000 datapoints
2025-03-06 17:55:25,140 - INFO - training batch 1651, loss: 0.584, 52832/60000 datapoints
2025-03-06 17:55:25,334 - INFO - training batch 1701, loss: 0.420, 54432/60000 datapoints
2025-03-06 17:55:25,528 - INFO - training batch 1751, loss: 0.658, 56032/60000 datapoints
2025-03-06 17:55:25,727 - INFO - training batch 1801, loss: 0.598, 57632/60000 datapoints
2025-03-06 17:55:25,918 - INFO - training batch 1851, loss: 0.427, 59232/60000 datapoints
2025-03-06 17:55:26,021 - INFO - validation batch 1, loss: 0.620, 32/10016 datapoints
2025-03-06 17:55:26,178 - INFO - validation batch 51, loss: 0.408, 1632/10016 datapoints
2025-03-06 17:55:26,333 - INFO - validation batch 101, loss: 0.563, 3232/10016 datapoints
2025-03-06 17:55:26,487 - INFO - validation batch 151, loss: 0.617, 4832/10016 datapoints
2025-03-06 17:55:26,639 - INFO - validation batch 201, loss: 0.666, 6432/10016 datapoints
2025-03-06 17:55:26,792 - INFO - validation batch 251, loss: 0.597, 8032/10016 datapoints
2025-03-06 17:55:26,942 - INFO - validation batch 301, loss: 0.535, 9632/10016 datapoints
2025-03-06 17:55:26,981 - INFO - Epoch 82/800 done.
2025-03-06 17:55:26,981 - INFO - Final validation performance:
Loss: 0.572, top-1 acc: 0.859top-5 acc: 0.859
2025-03-06 17:55:26,982 - INFO - Beginning epoch 83/800
2025-03-06 17:55:26,987 - INFO - training batch 1, loss: 0.610, 32/60000 datapoints
2025-03-06 17:55:27,181 - INFO - training batch 51, loss: 0.691, 1632/60000 datapoints
2025-03-06 17:55:27,372 - INFO - training batch 101, loss: 0.624, 3232/60000 datapoints
2025-03-06 17:55:27,565 - INFO - training batch 151, loss: 0.617, 4832/60000 datapoints
2025-03-06 17:55:27,763 - INFO - training batch 201, loss: 0.647, 6432/60000 datapoints
2025-03-06 17:55:27,960 - INFO - training batch 251, loss: 0.412, 8032/60000 datapoints
2025-03-06 17:55:28,160 - INFO - training batch 301, loss: 0.692, 9632/60000 datapoints
2025-03-06 17:55:28,353 - INFO - training batch 351, loss: 0.368, 11232/60000 datapoints
2025-03-06 17:55:28,549 - INFO - training batch 401, loss: 0.549, 12832/60000 datapoints
2025-03-06 17:55:28,746 - INFO - training batch 451, loss: 0.649, 14432/60000 datapoints
2025-03-06 17:55:28,939 - INFO - training batch 501, loss: 0.720, 16032/60000 datapoints
2025-03-06 17:55:29,135 - INFO - training batch 551, loss: 0.422, 17632/60000 datapoints
2025-03-06 17:55:29,330 - INFO - training batch 601, loss: 0.538, 19232/60000 datapoints
2025-03-06 17:55:29,523 - INFO - training batch 651, loss: 0.604, 20832/60000 datapoints
2025-03-06 17:55:29,720 - INFO - training batch 701, loss: 0.914, 22432/60000 datapoints
2025-03-06 17:55:29,914 - INFO - training batch 751, loss: 0.578, 24032/60000 datapoints
2025-03-06 17:55:30,108 - INFO - training batch 801, loss: 0.654, 25632/60000 datapoints
2025-03-06 17:55:30,302 - INFO - training batch 851, loss: 0.628, 27232/60000 datapoints
2025-03-06 17:55:30,495 - INFO - training batch 901, loss: 0.562, 28832/60000 datapoints
2025-03-06 17:55:30,692 - INFO - training batch 951, loss: 0.686, 30432/60000 datapoints
2025-03-06 17:55:30,883 - INFO - training batch 1001, loss: 0.630, 32032/60000 datapoints
2025-03-06 17:55:31,078 - INFO - training batch 1051, loss: 0.437, 33632/60000 datapoints
2025-03-06 17:55:31,272 - INFO - training batch 1101, loss: 0.613, 35232/60000 datapoints
2025-03-06 17:55:31,464 - INFO - training batch 1151, loss: 0.523, 36832/60000 datapoints
2025-03-06 17:55:31,666 - INFO - training batch 1201, loss: 0.540, 38432/60000 datapoints
2025-03-06 17:55:31,867 - INFO - training batch 1251, loss: 0.552, 40032/60000 datapoints
2025-03-06 17:55:32,061 - INFO - training batch 1301, loss: 0.523, 41632/60000 datapoints
2025-03-06 17:55:32,254 - INFO - training batch 1351, loss: 0.693, 43232/60000 datapoints
2025-03-06 17:55:32,447 - INFO - training batch 1401, loss: 0.826, 44832/60000 datapoints
2025-03-06 17:55:32,642 - INFO - training batch 1451, loss: 0.486, 46432/60000 datapoints
2025-03-06 17:55:32,835 - INFO - training batch 1501, loss: 0.534, 48032/60000 datapoints
2025-03-06 17:55:33,031 - INFO - training batch 1551, loss: 0.658, 49632/60000 datapoints
2025-03-06 17:55:33,223 - INFO - training batch 1601, loss: 0.537, 51232/60000 datapoints
2025-03-06 17:55:33,416 - INFO - training batch 1651, loss: 0.551, 52832/60000 datapoints
2025-03-06 17:55:33,613 - INFO - training batch 1701, loss: 0.388, 54432/60000 datapoints
2025-03-06 17:55:33,808 - INFO - training batch 1751, loss: 0.586, 56032/60000 datapoints
2025-03-06 17:55:34,003 - INFO - training batch 1801, loss: 0.530, 57632/60000 datapoints
2025-03-06 17:55:34,197 - INFO - training batch 1851, loss: 0.471, 59232/60000 datapoints
2025-03-06 17:55:34,297 - INFO - validation batch 1, loss: 0.585, 32/10016 datapoints
2025-03-06 17:55:34,449 - INFO - validation batch 51, loss: 0.570, 1632/10016 datapoints
2025-03-06 17:55:34,606 - INFO - validation batch 101, loss: 0.799, 3232/10016 datapoints
2025-03-06 17:55:34,774 - INFO - validation batch 151, loss: 0.672, 4832/10016 datapoints
2025-03-06 17:55:34,933 - INFO - validation batch 201, loss: 0.655, 6432/10016 datapoints
2025-03-06 17:55:35,084 - INFO - validation batch 251, loss: 0.678, 8032/10016 datapoints
2025-03-06 17:55:35,235 - INFO - validation batch 301, loss: 0.759, 9632/10016 datapoints
2025-03-06 17:55:35,274 - INFO - Epoch 83/800 done.
2025-03-06 17:55:35,274 - INFO - Final validation performance:
Loss: 0.674, top-1 acc: 0.860top-5 acc: 0.860
2025-03-06 17:55:35,274 - INFO - Beginning epoch 84/800
2025-03-06 17:55:35,280 - INFO - training batch 1, loss: 0.530, 32/60000 datapoints
2025-03-06 17:55:35,472 - INFO - training batch 51, loss: 0.646, 1632/60000 datapoints
2025-03-06 17:55:35,669 - INFO - training batch 101, loss: 0.426, 3232/60000 datapoints
2025-03-06 17:55:35,866 - INFO - training batch 151, loss: 0.468, 4832/60000 datapoints
2025-03-06 17:55:36,058 - INFO - training batch 201, loss: 0.662, 6432/60000 datapoints
2025-03-06 17:55:36,255 - INFO - training batch 251, loss: 0.538, 8032/60000 datapoints
2025-03-06 17:55:36,451 - INFO - training batch 301, loss: 0.859, 9632/60000 datapoints
2025-03-06 17:55:36,646 - INFO - training batch 351, loss: 0.418, 11232/60000 datapoints
2025-03-06 17:55:36,837 - INFO - training batch 401, loss: 0.781, 12832/60000 datapoints
2025-03-06 17:55:37,030 - INFO - training batch 451, loss: 0.478, 14432/60000 datapoints
2025-03-06 17:55:37,225 - INFO - training batch 501, loss: 0.476, 16032/60000 datapoints
2025-03-06 17:55:37,418 - INFO - training batch 551, loss: 0.508, 17632/60000 datapoints
2025-03-06 17:55:37,629 - INFO - training batch 601, loss: 0.521, 19232/60000 datapoints
2025-03-06 17:55:37,823 - INFO - training batch 651, loss: 0.567, 20832/60000 datapoints
2025-03-06 17:55:38,018 - INFO - training batch 701, loss: 0.433, 22432/60000 datapoints
2025-03-06 17:55:38,212 - INFO - training batch 751, loss: 0.627, 24032/60000 datapoints
2025-03-06 17:55:38,406 - INFO - training batch 801, loss: 0.623, 25632/60000 datapoints
2025-03-06 17:55:38,602 - INFO - training batch 851, loss: 0.558, 27232/60000 datapoints
2025-03-06 17:55:38,796 - INFO - training batch 901, loss: 0.543, 28832/60000 datapoints
2025-03-06 17:55:38,993 - INFO - training batch 951, loss: 0.535, 30432/60000 datapoints
2025-03-06 17:55:39,189 - INFO - training batch 1001, loss: 0.454, 32032/60000 datapoints
2025-03-06 17:55:39,381 - INFO - training batch 1051, loss: 0.420, 33632/60000 datapoints
2025-03-06 17:55:39,573 - INFO - training batch 1101, loss: 0.774, 35232/60000 datapoints
2025-03-06 17:55:39,771 - INFO - training batch 1151, loss: 0.598, 36832/60000 datapoints
2025-03-06 17:55:39,965 - INFO - training batch 1201, loss: 0.691, 38432/60000 datapoints
2025-03-06 17:55:40,159 - INFO - training batch 1251, loss: 0.795, 40032/60000 datapoints
2025-03-06 17:55:40,353 - INFO - training batch 1301, loss: 0.556, 41632/60000 datapoints
2025-03-06 17:55:40,545 - INFO - training batch 1351, loss: 0.521, 43232/60000 datapoints
2025-03-06 17:55:40,743 - INFO - training batch 1401, loss: 0.640, 44832/60000 datapoints
2025-03-06 17:55:40,936 - INFO - training batch 1451, loss: 0.417, 46432/60000 datapoints
2025-03-06 17:55:41,130 - INFO - training batch 1501, loss: 0.457, 48032/60000 datapoints
2025-03-06 17:55:41,324 - INFO - training batch 1551, loss: 0.507, 49632/60000 datapoints
2025-03-06 17:55:41,516 - INFO - training batch 1601, loss: 0.541, 51232/60000 datapoints
2025-03-06 17:55:41,711 - INFO - training batch 1651, loss: 0.516, 52832/60000 datapoints
2025-03-06 17:55:41,905 - INFO - training batch 1701, loss: 0.450, 54432/60000 datapoints
2025-03-06 17:55:42,099 - INFO - training batch 1751, loss: 0.617, 56032/60000 datapoints
2025-03-06 17:55:42,291 - INFO - training batch 1801, loss: 0.733, 57632/60000 datapoints
2025-03-06 17:55:42,483 - INFO - training batch 1851, loss: 0.770, 59232/60000 datapoints
2025-03-06 17:55:42,584 - INFO - validation batch 1, loss: 0.729, 32/10016 datapoints
2025-03-06 17:55:42,740 - INFO - validation batch 51, loss: 0.575, 1632/10016 datapoints
2025-03-06 17:55:42,893 - INFO - validation batch 101, loss: 0.378, 3232/10016 datapoints
2025-03-06 17:55:43,045 - INFO - validation batch 151, loss: 0.613, 4832/10016 datapoints
2025-03-06 17:55:43,200 - INFO - validation batch 201, loss: 0.593, 6432/10016 datapoints
2025-03-06 17:55:43,353 - INFO - validation batch 251, loss: 0.610, 8032/10016 datapoints
2025-03-06 17:55:43,504 - INFO - validation batch 301, loss: 0.580, 9632/10016 datapoints
2025-03-06 17:55:43,542 - INFO - Epoch 84/800 done.
2025-03-06 17:55:43,542 - INFO - Final validation performance:
Loss: 0.583, top-1 acc: 0.861top-5 acc: 0.861
2025-03-06 17:55:43,542 - INFO - Beginning epoch 85/800
2025-03-06 17:55:43,549 - INFO - training batch 1, loss: 0.574, 32/60000 datapoints
2025-03-06 17:55:43,751 - INFO - training batch 51, loss: 0.746, 1632/60000 datapoints
2025-03-06 17:55:43,947 - INFO - training batch 101, loss: 0.898, 3232/60000 datapoints
2025-03-06 17:55:44,141 - INFO - training batch 151, loss: 0.599, 4832/60000 datapoints
2025-03-06 17:55:44,337 - INFO - training batch 201, loss: 0.732, 6432/60000 datapoints
2025-03-06 17:55:44,530 - INFO - training batch 251, loss: 0.675, 8032/60000 datapoints
2025-03-06 17:55:44,724 - INFO - training batch 301, loss: 0.557, 9632/60000 datapoints
2025-03-06 17:55:44,944 - INFO - training batch 351, loss: 0.599, 11232/60000 datapoints
2025-03-06 17:55:45,139 - INFO - training batch 401, loss: 0.292, 12832/60000 datapoints
2025-03-06 17:55:45,336 - INFO - training batch 451, loss: 0.524, 14432/60000 datapoints
2025-03-06 17:55:45,530 - INFO - training batch 501, loss: 0.469, 16032/60000 datapoints
2025-03-06 17:55:45,726 - INFO - training batch 551, loss: 0.648, 17632/60000 datapoints
2025-03-06 17:55:45,921 - INFO - training batch 601, loss: 0.441, 19232/60000 datapoints
2025-03-06 17:55:46,114 - INFO - training batch 651, loss: 0.525, 20832/60000 datapoints
2025-03-06 17:55:46,310 - INFO - training batch 701, loss: 0.627, 22432/60000 datapoints
2025-03-06 17:55:46,506 - INFO - training batch 751, loss: 0.654, 24032/60000 datapoints
2025-03-06 17:55:46,703 - INFO - training batch 801, loss: 0.999, 25632/60000 datapoints
2025-03-06 17:55:46,896 - INFO - training batch 851, loss: 0.550, 27232/60000 datapoints
2025-03-06 17:55:47,090 - INFO - training batch 901, loss: 0.493, 28832/60000 datapoints
2025-03-06 17:55:47,288 - INFO - training batch 951, loss: 0.514, 30432/60000 datapoints
2025-03-06 17:55:47,483 - INFO - training batch 1001, loss: 0.631, 32032/60000 datapoints
2025-03-06 17:55:47,680 - INFO - training batch 1051, loss: 0.663, 33632/60000 datapoints
2025-03-06 17:55:47,879 - INFO - training batch 1101, loss: 0.759, 35232/60000 datapoints
2025-03-06 17:55:48,073 - INFO - training batch 1151, loss: 0.553, 36832/60000 datapoints
2025-03-06 17:55:48,269 - INFO - training batch 1201, loss: 0.378, 38432/60000 datapoints
2025-03-06 17:55:48,464 - INFO - training batch 1251, loss: 0.655, 40032/60000 datapoints
2025-03-06 17:55:48,662 - INFO - training batch 1301, loss: 0.730, 41632/60000 datapoints
2025-03-06 17:55:48,857 - INFO - training batch 1351, loss: 0.496, 43232/60000 datapoints
2025-03-06 17:55:49,050 - INFO - training batch 1401, loss: 0.569, 44832/60000 datapoints
2025-03-06 17:55:49,247 - INFO - training batch 1451, loss: 0.581, 46432/60000 datapoints
2025-03-06 17:55:49,440 - INFO - training batch 1501, loss: 0.533, 48032/60000 datapoints
2025-03-06 17:55:49,638 - INFO - training batch 1551, loss: 0.504, 49632/60000 datapoints
2025-03-06 17:55:49,834 - INFO - training batch 1601, loss: 0.692, 51232/60000 datapoints
2025-03-06 17:55:50,028 - INFO - training batch 1651, loss: 0.661, 52832/60000 datapoints
2025-03-06 17:55:50,222 - INFO - training batch 1701, loss: 0.563, 54432/60000 datapoints
2025-03-06 17:55:50,419 - INFO - training batch 1751, loss: 0.620, 56032/60000 datapoints
2025-03-06 17:55:50,615 - INFO - training batch 1801, loss: 0.652, 57632/60000 datapoints
2025-03-06 17:55:50,811 - INFO - training batch 1851, loss: 0.779, 59232/60000 datapoints
2025-03-06 17:55:50,911 - INFO - validation batch 1, loss: 0.746, 32/10016 datapoints
2025-03-06 17:55:51,062 - INFO - validation batch 51, loss: 0.457, 1632/10016 datapoints
2025-03-06 17:55:51,216 - INFO - validation batch 101, loss: 0.469, 3232/10016 datapoints
2025-03-06 17:55:51,368 - INFO - validation batch 151, loss: 0.500, 4832/10016 datapoints
2025-03-06 17:55:51,520 - INFO - validation batch 201, loss: 0.505, 6432/10016 datapoints
2025-03-06 17:55:51,675 - INFO - validation batch 251, loss: 0.741, 8032/10016 datapoints
2025-03-06 17:55:51,831 - INFO - validation batch 301, loss: 0.438, 9632/10016 datapoints
2025-03-06 17:55:51,868 - INFO - Epoch 85/800 done.
2025-03-06 17:55:51,868 - INFO - Final validation performance:
Loss: 0.551, top-1 acc: 0.862top-5 acc: 0.862
2025-03-06 17:55:51,869 - INFO - Beginning epoch 86/800
2025-03-06 17:55:51,874 - INFO - training batch 1, loss: 0.731, 32/60000 datapoints
2025-03-06 17:55:52,066 - INFO - training batch 51, loss: 0.517, 1632/60000 datapoints
2025-03-06 17:55:52,261 - INFO - training batch 101, loss: 0.534, 3232/60000 datapoints
2025-03-06 17:55:52,455 - INFO - training batch 151, loss: 0.548, 4832/60000 datapoints
2025-03-06 17:55:52,650 - INFO - training batch 201, loss: 0.573, 6432/60000 datapoints
2025-03-06 17:55:52,843 - INFO - training batch 251, loss: 0.773, 8032/60000 datapoints
2025-03-06 17:55:53,036 - INFO - training batch 301, loss: 0.472, 9632/60000 datapoints
2025-03-06 17:55:53,231 - INFO - training batch 351, loss: 0.486, 11232/60000 datapoints
2025-03-06 17:55:53,424 - INFO - training batch 401, loss: 0.538, 12832/60000 datapoints
2025-03-06 17:55:53,622 - INFO - training batch 451, loss: 0.586, 14432/60000 datapoints
2025-03-06 17:55:53,819 - INFO - training batch 501, loss: 0.614, 16032/60000 datapoints
2025-03-06 17:55:54,011 - INFO - training batch 551, loss: 0.426, 17632/60000 datapoints
2025-03-06 17:55:54,203 - INFO - training batch 601, loss: 0.520, 19232/60000 datapoints
2025-03-06 17:55:54,398 - INFO - training batch 651, loss: 0.511, 20832/60000 datapoints
2025-03-06 17:55:54,592 - INFO - training batch 701, loss: 0.610, 22432/60000 datapoints
2025-03-06 17:55:54,784 - INFO - training batch 751, loss: 0.583, 24032/60000 datapoints
2025-03-06 17:55:55,002 - INFO - training batch 801, loss: 0.679, 25632/60000 datapoints
2025-03-06 17:55:55,196 - INFO - training batch 851, loss: 0.384, 27232/60000 datapoints
2025-03-06 17:55:55,391 - INFO - training batch 901, loss: 0.603, 28832/60000 datapoints
2025-03-06 17:55:55,583 - INFO - training batch 951, loss: 0.585, 30432/60000 datapoints
2025-03-06 17:55:55,778 - INFO - training batch 1001, loss: 0.372, 32032/60000 datapoints
2025-03-06 17:55:55,974 - INFO - training batch 1051, loss: 0.635, 33632/60000 datapoints
2025-03-06 17:55:56,170 - INFO - training batch 1101, loss: 0.799, 35232/60000 datapoints
2025-03-06 17:55:56,366 - INFO - training batch 1151, loss: 0.607, 36832/60000 datapoints
2025-03-06 17:55:56,560 - INFO - training batch 1201, loss: 0.532, 38432/60000 datapoints
2025-03-06 17:55:56,754 - INFO - training batch 1251, loss: 0.563, 40032/60000 datapoints
2025-03-06 17:55:56,950 - INFO - training batch 1301, loss: 0.678, 41632/60000 datapoints
2025-03-06 17:55:57,142 - INFO - training batch 1351, loss: 0.393, 43232/60000 datapoints
2025-03-06 17:55:57,335 - INFO - training batch 1401, loss: 0.782, 44832/60000 datapoints
2025-03-06 17:55:57,527 - INFO - training batch 1451, loss: 0.650, 46432/60000 datapoints
2025-03-06 17:55:57,723 - INFO - training batch 1501, loss: 0.523, 48032/60000 datapoints
2025-03-06 17:55:57,918 - INFO - training batch 1551, loss: 0.421, 49632/60000 datapoints
2025-03-06 17:55:58,116 - INFO - training batch 1601, loss: 0.492, 51232/60000 datapoints
2025-03-06 17:55:58,310 - INFO - training batch 1651, loss: 0.518, 52832/60000 datapoints
2025-03-06 17:55:58,505 - INFO - training batch 1701, loss: 0.913, 54432/60000 datapoints
2025-03-06 17:55:58,701 - INFO - training batch 1751, loss: 0.706, 56032/60000 datapoints
2025-03-06 17:55:58,894 - INFO - training batch 1801, loss: 0.499, 57632/60000 datapoints
2025-03-06 17:55:59,085 - INFO - training batch 1851, loss: 0.746, 59232/60000 datapoints
2025-03-06 17:55:59,189 - INFO - validation batch 1, loss: 0.521, 32/10016 datapoints
2025-03-06 17:55:59,342 - INFO - validation batch 51, loss: 0.771, 1632/10016 datapoints
2025-03-06 17:55:59,495 - INFO - validation batch 101, loss: 0.502, 3232/10016 datapoints
2025-03-06 17:55:59,648 - INFO - validation batch 151, loss: 0.556, 4832/10016 datapoints
2025-03-06 17:55:59,800 - INFO - validation batch 201, loss: 0.417, 6432/10016 datapoints
2025-03-06 17:55:59,955 - INFO - validation batch 251, loss: 0.707, 8032/10016 datapoints
2025-03-06 17:56:00,108 - INFO - validation batch 301, loss: 0.660, 9632/10016 datapoints
2025-03-06 17:56:00,144 - INFO - Epoch 86/800 done.
2025-03-06 17:56:00,144 - INFO - Final validation performance:
Loss: 0.591, top-1 acc: 0.862top-5 acc: 0.862
2025-03-06 17:56:00,144 - INFO - Beginning epoch 87/800
2025-03-06 17:56:00,151 - INFO - training batch 1, loss: 0.378, 32/60000 datapoints
2025-03-06 17:56:00,353 - INFO - training batch 51, loss: 0.425, 1632/60000 datapoints
2025-03-06 17:56:00,544 - INFO - training batch 101, loss: 0.680, 3232/60000 datapoints
2025-03-06 17:56:00,740 - INFO - training batch 151, loss: 0.769, 4832/60000 datapoints
2025-03-06 17:56:00,933 - INFO - training batch 201, loss: 0.547, 6432/60000 datapoints
2025-03-06 17:56:01,127 - INFO - training batch 251, loss: 0.694, 8032/60000 datapoints
2025-03-06 17:56:01,321 - INFO - training batch 301, loss: 0.664, 9632/60000 datapoints
2025-03-06 17:56:01,514 - INFO - training batch 351, loss: 0.777, 11232/60000 datapoints
2025-03-06 17:56:01,710 - INFO - training batch 401, loss: 0.651, 12832/60000 datapoints
2025-03-06 17:56:01,907 - INFO - training batch 451, loss: 0.575, 14432/60000 datapoints
2025-03-06 17:56:02,100 - INFO - training batch 501, loss: 0.575, 16032/60000 datapoints
2025-03-06 17:56:02,295 - INFO - training batch 551, loss: 0.581, 17632/60000 datapoints
2025-03-06 17:56:02,489 - INFO - training batch 601, loss: 0.473, 19232/60000 datapoints
2025-03-06 17:56:02,683 - INFO - training batch 651, loss: 0.543, 20832/60000 datapoints
2025-03-06 17:56:02,877 - INFO - training batch 701, loss: 0.335, 22432/60000 datapoints
2025-03-06 17:56:03,070 - INFO - training batch 751, loss: 0.532, 24032/60000 datapoints
2025-03-06 17:56:03,265 - INFO - training batch 801, loss: 0.571, 25632/60000 datapoints
2025-03-06 17:56:03,457 - INFO - training batch 851, loss: 0.513, 27232/60000 datapoints
2025-03-06 17:56:03,653 - INFO - training batch 901, loss: 0.537, 28832/60000 datapoints
2025-03-06 17:56:03,849 - INFO - training batch 951, loss: 0.604, 30432/60000 datapoints
2025-03-06 17:56:04,042 - INFO - training batch 1001, loss: 0.529, 32032/60000 datapoints
2025-03-06 17:56:04,235 - INFO - training batch 1051, loss: 0.649, 33632/60000 datapoints
2025-03-06 17:56:04,428 - INFO - training batch 1101, loss: 0.484, 35232/60000 datapoints
2025-03-06 17:56:04,626 - INFO - training batch 1151, loss: 0.557, 36832/60000 datapoints
2025-03-06 17:56:04,822 - INFO - training batch 1201, loss: 0.568, 38432/60000 datapoints
2025-03-06 17:56:05,036 - INFO - training batch 1251, loss: 0.656, 40032/60000 datapoints
2025-03-06 17:56:05,232 - INFO - training batch 1301, loss: 0.418, 41632/60000 datapoints
2025-03-06 17:56:05,427 - INFO - training batch 1351, loss: 0.515, 43232/60000 datapoints
2025-03-06 17:56:05,622 - INFO - training batch 1401, loss: 0.512, 44832/60000 datapoints
2025-03-06 17:56:05,825 - INFO - training batch 1451, loss: 0.600, 46432/60000 datapoints
2025-03-06 17:56:06,023 - INFO - training batch 1501, loss: 0.478, 48032/60000 datapoints
2025-03-06 17:56:06,217 - INFO - training batch 1551, loss: 0.516, 49632/60000 datapoints
2025-03-06 17:56:06,415 - INFO - training batch 1601, loss: 0.514, 51232/60000 datapoints
2025-03-06 17:56:06,609 - INFO - training batch 1651, loss: 0.489, 52832/60000 datapoints
2025-03-06 17:56:06,800 - INFO - training batch 1701, loss: 0.447, 54432/60000 datapoints
2025-03-06 17:56:06,993 - INFO - training batch 1751, loss: 0.569, 56032/60000 datapoints
2025-03-06 17:56:07,185 - INFO - training batch 1801, loss: 0.541, 57632/60000 datapoints
2025-03-06 17:56:07,377 - INFO - training batch 1851, loss: 0.648, 59232/60000 datapoints
2025-03-06 17:56:07,479 - INFO - validation batch 1, loss: 0.471, 32/10016 datapoints
2025-03-06 17:56:07,636 - INFO - validation batch 51, loss: 0.706, 1632/10016 datapoints
2025-03-06 17:56:07,788 - INFO - validation batch 101, loss: 0.513, 3232/10016 datapoints
2025-03-06 17:56:07,942 - INFO - validation batch 151, loss: 0.538, 4832/10016 datapoints
2025-03-06 17:56:08,098 - INFO - validation batch 201, loss: 0.425, 6432/10016 datapoints
2025-03-06 17:56:08,251 - INFO - validation batch 251, loss: 0.406, 8032/10016 datapoints
2025-03-06 17:56:08,402 - INFO - validation batch 301, loss: 0.592, 9632/10016 datapoints
2025-03-06 17:56:08,441 - INFO - Epoch 87/800 done.
2025-03-06 17:56:08,441 - INFO - Final validation performance:
Loss: 0.521, top-1 acc: 0.863top-5 acc: 0.863
2025-03-06 17:56:08,442 - INFO - Beginning epoch 88/800
2025-03-06 17:56:08,447 - INFO - training batch 1, loss: 0.665, 32/60000 datapoints
2025-03-06 17:56:08,645 - INFO - training batch 51, loss: 0.406, 1632/60000 datapoints
2025-03-06 17:56:08,838 - INFO - training batch 101, loss: 0.685, 3232/60000 datapoints
2025-03-06 17:56:09,034 - INFO - training batch 151, loss: 0.475, 4832/60000 datapoints
2025-03-06 17:56:09,229 - INFO - training batch 201, loss: 0.795, 6432/60000 datapoints
2025-03-06 17:56:09,423 - INFO - training batch 251, loss: 0.433, 8032/60000 datapoints
2025-03-06 17:56:09,623 - INFO - training batch 301, loss: 0.474, 9632/60000 datapoints
2025-03-06 17:56:09,815 - INFO - training batch 351, loss: 0.641, 11232/60000 datapoints
2025-03-06 17:56:10,012 - INFO - training batch 401, loss: 0.499, 12832/60000 datapoints
2025-03-06 17:56:10,204 - INFO - training batch 451, loss: 0.640, 14432/60000 datapoints
2025-03-06 17:56:10,399 - INFO - training batch 501, loss: 0.445, 16032/60000 datapoints
2025-03-06 17:56:10,595 - INFO - training batch 551, loss: 0.729, 17632/60000 datapoints
2025-03-06 17:56:10,789 - INFO - training batch 601, loss: 0.474, 19232/60000 datapoints
2025-03-06 17:56:10,983 - INFO - training batch 651, loss: 0.701, 20832/60000 datapoints
2025-03-06 17:56:11,175 - INFO - training batch 701, loss: 0.539, 22432/60000 datapoints
2025-03-06 17:56:11,368 - INFO - training batch 751, loss: 0.535, 24032/60000 datapoints
2025-03-06 17:56:11,560 - INFO - training batch 801, loss: 0.736, 25632/60000 datapoints
2025-03-06 17:56:11,755 - INFO - training batch 851, loss: 0.461, 27232/60000 datapoints
2025-03-06 17:56:11,950 - INFO - training batch 901, loss: 0.553, 28832/60000 datapoints
2025-03-06 17:56:12,144 - INFO - training batch 951, loss: 0.439, 30432/60000 datapoints
2025-03-06 17:56:12,338 - INFO - training batch 1001, loss: 0.542, 32032/60000 datapoints
2025-03-06 17:56:12,530 - INFO - training batch 1051, loss: 0.601, 33632/60000 datapoints
2025-03-06 17:56:12,723 - INFO - training batch 1101, loss: 0.609, 35232/60000 datapoints
2025-03-06 17:56:12,916 - INFO - training batch 1151, loss: 0.607, 36832/60000 datapoints
2025-03-06 17:56:13,121 - INFO - training batch 1201, loss: 0.574, 38432/60000 datapoints
2025-03-06 17:56:13,316 - INFO - training batch 1251, loss: 0.659, 40032/60000 datapoints
2025-03-06 17:56:13,536 - INFO - training batch 1301, loss: 0.466, 41632/60000 datapoints
2025-03-06 17:56:13,730 - INFO - training batch 1351, loss: 0.821, 43232/60000 datapoints
2025-03-06 17:56:13,924 - INFO - training batch 1401, loss: 0.538, 44832/60000 datapoints
2025-03-06 17:56:14,118 - INFO - training batch 1451, loss: 0.500, 46432/60000 datapoints
2025-03-06 17:56:14,310 - INFO - training batch 1501, loss: 0.496, 48032/60000 datapoints
2025-03-06 17:56:14,506 - INFO - training batch 1551, loss: 0.487, 49632/60000 datapoints
2025-03-06 17:56:14,699 - INFO - training batch 1601, loss: 0.591, 51232/60000 datapoints
2025-03-06 17:56:14,894 - INFO - training batch 1651, loss: 0.523, 52832/60000 datapoints
2025-03-06 17:56:15,108 - INFO - training batch 1701, loss: 0.692, 54432/60000 datapoints
2025-03-06 17:56:15,302 - INFO - training batch 1751, loss: 0.467, 56032/60000 datapoints
2025-03-06 17:56:15,496 - INFO - training batch 1801, loss: 0.581, 57632/60000 datapoints
2025-03-06 17:56:15,692 - INFO - training batch 1851, loss: 0.587, 59232/60000 datapoints
2025-03-06 17:56:15,792 - INFO - validation batch 1, loss: 0.439, 32/10016 datapoints
2025-03-06 17:56:15,946 - INFO - validation batch 51, loss: 0.580, 1632/10016 datapoints
2025-03-06 17:56:16,099 - INFO - validation batch 101, loss: 0.419, 3232/10016 datapoints
2025-03-06 17:56:16,254 - INFO - validation batch 151, loss: 0.415, 4832/10016 datapoints
2025-03-06 17:56:16,405 - INFO - validation batch 201, loss: 0.770, 6432/10016 datapoints
2025-03-06 17:56:16,562 - INFO - validation batch 251, loss: 0.510, 8032/10016 datapoints
2025-03-06 17:56:16,715 - INFO - validation batch 301, loss: 0.712, 9632/10016 datapoints
2025-03-06 17:56:16,751 - INFO - Epoch 88/800 done.
2025-03-06 17:56:16,751 - INFO - Final validation performance:
Loss: 0.550, top-1 acc: 0.863top-5 acc: 0.863
2025-03-06 17:56:16,751 - INFO - Beginning epoch 89/800
2025-03-06 17:56:16,758 - INFO - training batch 1, loss: 0.574, 32/60000 datapoints
2025-03-06 17:56:16,958 - INFO - training batch 51, loss: 0.844, 1632/60000 datapoints
2025-03-06 17:56:17,154 - INFO - training batch 101, loss: 0.573, 3232/60000 datapoints
2025-03-06 17:56:17,352 - INFO - training batch 151, loss: 0.456, 4832/60000 datapoints
2025-03-06 17:56:17,547 - INFO - training batch 201, loss: 0.626, 6432/60000 datapoints
2025-03-06 17:56:17,742 - INFO - training batch 251, loss: 0.836, 8032/60000 datapoints
2025-03-06 17:56:17,938 - INFO - training batch 301, loss: 0.406, 9632/60000 datapoints
2025-03-06 17:56:18,134 - INFO - training batch 351, loss: 0.449, 11232/60000 datapoints
2025-03-06 17:56:18,328 - INFO - training batch 401, loss: 0.469, 12832/60000 datapoints
2025-03-06 17:56:18,522 - INFO - training batch 451, loss: 0.518, 14432/60000 datapoints
2025-03-06 17:56:18,718 - INFO - training batch 501, loss: 0.606, 16032/60000 datapoints
2025-03-06 17:56:18,913 - INFO - training batch 551, loss: 0.489, 17632/60000 datapoints
2025-03-06 17:56:19,110 - INFO - training batch 601, loss: 0.682, 19232/60000 datapoints
2025-03-06 17:56:19,308 - INFO - training batch 651, loss: 0.484, 20832/60000 datapoints
2025-03-06 17:56:19,505 - INFO - training batch 701, loss: 0.629, 22432/60000 datapoints
2025-03-06 17:56:19,703 - INFO - training batch 751, loss: 0.499, 24032/60000 datapoints
2025-03-06 17:56:19,899 - INFO - training batch 801, loss: 0.475, 25632/60000 datapoints
2025-03-06 17:56:20,095 - INFO - training batch 851, loss: 0.712, 27232/60000 datapoints
2025-03-06 17:56:20,290 - INFO - training batch 901, loss: 0.419, 28832/60000 datapoints
2025-03-06 17:56:20,520 - INFO - training batch 951, loss: 0.562, 30432/60000 datapoints
2025-03-06 17:56:20,714 - INFO - training batch 1001, loss: 0.748, 32032/60000 datapoints
2025-03-06 17:56:20,909 - INFO - training batch 1051, loss: 0.613, 33632/60000 datapoints
2025-03-06 17:56:21,104 - INFO - training batch 1101, loss: 0.512, 35232/60000 datapoints
2025-03-06 17:56:21,297 - INFO - training batch 1151, loss: 0.491, 36832/60000 datapoints
2025-03-06 17:56:21,490 - INFO - training batch 1201, loss: 0.616, 38432/60000 datapoints
2025-03-06 17:56:21,692 - INFO - training batch 1251, loss: 0.671, 40032/60000 datapoints
2025-03-06 17:56:21,886 - INFO - training batch 1301, loss: 0.513, 41632/60000 datapoints
2025-03-06 17:56:22,081 - INFO - training batch 1351, loss: 0.371, 43232/60000 datapoints
2025-03-06 17:56:22,273 - INFO - training batch 1401, loss: 0.476, 44832/60000 datapoints
2025-03-06 17:56:22,475 - INFO - training batch 1451, loss: 0.689, 46432/60000 datapoints
2025-03-06 17:56:22,676 - INFO - training batch 1501, loss: 0.390, 48032/60000 datapoints
2025-03-06 17:56:22,868 - INFO - training batch 1551, loss: 0.708, 49632/60000 datapoints
2025-03-06 17:56:23,061 - INFO - training batch 1601, loss: 0.647, 51232/60000 datapoints
2025-03-06 17:56:23,256 - INFO - training batch 1651, loss: 0.693, 52832/60000 datapoints
2025-03-06 17:56:23,451 - INFO - training batch 1701, loss: 0.338, 54432/60000 datapoints
2025-03-06 17:56:23,649 - INFO - training batch 1751, loss: 0.599, 56032/60000 datapoints
2025-03-06 17:56:23,843 - INFO - training batch 1801, loss: 0.377, 57632/60000 datapoints
2025-03-06 17:56:24,038 - INFO - training batch 1851, loss: 0.689, 59232/60000 datapoints
2025-03-06 17:56:24,139 - INFO - validation batch 1, loss: 0.780, 32/10016 datapoints
2025-03-06 17:56:24,293 - INFO - validation batch 51, loss: 0.544, 1632/10016 datapoints
2025-03-06 17:56:24,447 - INFO - validation batch 101, loss: 0.737, 3232/10016 datapoints
2025-03-06 17:56:24,602 - INFO - validation batch 151, loss: 0.466, 4832/10016 datapoints
2025-03-06 17:56:24,754 - INFO - validation batch 201, loss: 0.449, 6432/10016 datapoints
2025-03-06 17:56:24,908 - INFO - validation batch 251, loss: 0.524, 8032/10016 datapoints
2025-03-06 17:56:25,061 - INFO - validation batch 301, loss: 0.443, 9632/10016 datapoints
2025-03-06 17:56:25,097 - INFO - Epoch 89/800 done.
2025-03-06 17:56:25,097 - INFO - Final validation performance:
Loss: 0.563, top-1 acc: 0.865top-5 acc: 0.865
2025-03-06 17:56:25,097 - INFO - Beginning epoch 90/800
2025-03-06 17:56:25,104 - INFO - training batch 1, loss: 0.494, 32/60000 datapoints
2025-03-06 17:56:25,320 - INFO - training batch 51, loss: 0.549, 1632/60000 datapoints
2025-03-06 17:56:25,516 - INFO - training batch 101, loss: 0.726, 3232/60000 datapoints
2025-03-06 17:56:25,712 - INFO - training batch 151, loss: 0.604, 4832/60000 datapoints
2025-03-06 17:56:25,913 - INFO - training batch 201, loss: 0.538, 6432/60000 datapoints
2025-03-06 17:56:26,107 - INFO - training batch 251, loss: 0.560, 8032/60000 datapoints
2025-03-06 17:56:26,304 - INFO - training batch 301, loss: 0.430, 9632/60000 datapoints
2025-03-06 17:56:26,501 - INFO - training batch 351, loss: 0.558, 11232/60000 datapoints
2025-03-06 17:56:26,699 - INFO - training batch 401, loss: 0.884, 12832/60000 datapoints
2025-03-06 17:56:26,891 - INFO - training batch 451, loss: 0.556, 14432/60000 datapoints
2025-03-06 17:56:27,087 - INFO - training batch 501, loss: 0.482, 16032/60000 datapoints
2025-03-06 17:56:27,283 - INFO - training batch 551, loss: 0.377, 17632/60000 datapoints
2025-03-06 17:56:27,480 - INFO - training batch 601, loss: 0.447, 19232/60000 datapoints
2025-03-06 17:56:27,677 - INFO - training batch 651, loss: 0.697, 20832/60000 datapoints
2025-03-06 17:56:27,871 - INFO - training batch 701, loss: 0.833, 22432/60000 datapoints
2025-03-06 17:56:28,067 - INFO - training batch 751, loss: 0.602, 24032/60000 datapoints
2025-03-06 17:56:28,263 - INFO - training batch 801, loss: 0.584, 25632/60000 datapoints
2025-03-06 17:56:28,461 - INFO - training batch 851, loss: 0.501, 27232/60000 datapoints
2025-03-06 17:56:28,659 - INFO - training batch 901, loss: 0.596, 28832/60000 datapoints
2025-03-06 17:56:28,855 - INFO - training batch 951, loss: 0.467, 30432/60000 datapoints
2025-03-06 17:56:29,050 - INFO - training batch 1001, loss: 0.443, 32032/60000 datapoints
2025-03-06 17:56:29,247 - INFO - training batch 1051, loss: 0.557, 33632/60000 datapoints
2025-03-06 17:56:29,444 - INFO - training batch 1101, loss: 0.469, 35232/60000 datapoints
2025-03-06 17:56:29,641 - INFO - training batch 1151, loss: 0.374, 36832/60000 datapoints
2025-03-06 17:56:29,837 - INFO - training batch 1201, loss: 0.509, 38432/60000 datapoints
2025-03-06 17:56:30,036 - INFO - training batch 1251, loss: 0.726, 40032/60000 datapoints
2025-03-06 17:56:30,230 - INFO - training batch 1301, loss: 0.630, 41632/60000 datapoints
2025-03-06 17:56:30,424 - INFO - training batch 1351, loss: 0.495, 43232/60000 datapoints
2025-03-06 17:56:30,620 - INFO - training batch 1401, loss: 0.393, 44832/60000 datapoints
2025-03-06 17:56:30,815 - INFO - training batch 1451, loss: 0.606, 46432/60000 datapoints
2025-03-06 17:56:31,009 - INFO - training batch 1501, loss: 0.640, 48032/60000 datapoints
2025-03-06 17:56:31,205 - INFO - training batch 1551, loss: 0.409, 49632/60000 datapoints
2025-03-06 17:56:31,400 - INFO - training batch 1601, loss: 0.597, 51232/60000 datapoints
2025-03-06 17:56:31,597 - INFO - training batch 1651, loss: 0.587, 52832/60000 datapoints
2025-03-06 17:56:31,792 - INFO - training batch 1701, loss: 0.595, 54432/60000 datapoints
2025-03-06 17:56:31,988 - INFO - training batch 1751, loss: 0.497, 56032/60000 datapoints
2025-03-06 17:56:32,182 - INFO - training batch 1801, loss: 0.609, 57632/60000 datapoints
2025-03-06 17:56:32,377 - INFO - training batch 1851, loss: 0.623, 59232/60000 datapoints
2025-03-06 17:56:32,480 - INFO - validation batch 1, loss: 0.575, 32/10016 datapoints
2025-03-06 17:56:32,634 - INFO - validation batch 51, loss: 0.541, 1632/10016 datapoints
2025-03-06 17:56:32,786 - INFO - validation batch 101, loss: 0.557, 3232/10016 datapoints
2025-03-06 17:56:32,939 - INFO - validation batch 151, loss: 0.418, 4832/10016 datapoints
2025-03-06 17:56:33,091 - INFO - validation batch 201, loss: 0.535, 6432/10016 datapoints
2025-03-06 17:56:33,245 - INFO - validation batch 251, loss: 0.536, 8032/10016 datapoints
2025-03-06 17:56:33,398 - INFO - validation batch 301, loss: 0.783, 9632/10016 datapoints
2025-03-06 17:56:33,436 - INFO - Epoch 90/800 done.
2025-03-06 17:56:33,436 - INFO - Final validation performance:
Loss: 0.564, top-1 acc: 0.866top-5 acc: 0.866
2025-03-06 17:56:33,436 - INFO - Beginning epoch 91/800
2025-03-06 17:56:33,442 - INFO - training batch 1, loss: 0.350, 32/60000 datapoints
2025-03-06 17:56:33,639 - INFO - training batch 51, loss: 0.652, 1632/60000 datapoints
2025-03-06 17:56:33,834 - INFO - training batch 101, loss: 0.393, 3232/60000 datapoints
2025-03-06 17:56:34,033 - INFO - training batch 151, loss: 0.568, 4832/60000 datapoints
2025-03-06 17:56:34,227 - INFO - training batch 201, loss: 0.644, 6432/60000 datapoints
2025-03-06 17:56:34,420 - INFO - training batch 251, loss: 0.552, 8032/60000 datapoints
2025-03-06 17:56:34,620 - INFO - training batch 301, loss: 0.705, 9632/60000 datapoints
2025-03-06 17:56:34,816 - INFO - training batch 351, loss: 0.534, 11232/60000 datapoints
2025-03-06 17:56:35,012 - INFO - training batch 401, loss: 0.491, 12832/60000 datapoints
2025-03-06 17:56:35,212 - INFO - training batch 451, loss: 0.441, 14432/60000 datapoints
2025-03-06 17:56:35,424 - INFO - training batch 501, loss: 0.566, 16032/60000 datapoints
2025-03-06 17:56:35,620 - INFO - training batch 551, loss: 0.461, 17632/60000 datapoints
2025-03-06 17:56:35,815 - INFO - training batch 601, loss: 0.705, 19232/60000 datapoints
2025-03-06 17:56:36,010 - INFO - training batch 651, loss: 0.446, 20832/60000 datapoints
2025-03-06 17:56:36,206 - INFO - training batch 701, loss: 0.554, 22432/60000 datapoints
2025-03-06 17:56:36,402 - INFO - training batch 751, loss: 0.743, 24032/60000 datapoints
2025-03-06 17:56:36,601 - INFO - training batch 801, loss: 0.794, 25632/60000 datapoints
2025-03-06 17:56:36,795 - INFO - training batch 851, loss: 0.472, 27232/60000 datapoints
2025-03-06 17:56:36,990 - INFO - training batch 901, loss: 1.063, 28832/60000 datapoints
2025-03-06 17:56:37,183 - INFO - training batch 951, loss: 0.524, 30432/60000 datapoints
2025-03-06 17:56:37,377 - INFO - training batch 1001, loss: 0.549, 32032/60000 datapoints
2025-03-06 17:56:37,572 - INFO - training batch 1051, loss: 0.639, 33632/60000 datapoints
2025-03-06 17:56:37,789 - INFO - training batch 1101, loss: 0.545, 35232/60000 datapoints
2025-03-06 17:56:37,990 - INFO - training batch 1151, loss: 0.685, 36832/60000 datapoints
2025-03-06 17:56:38,184 - INFO - training batch 1201, loss: 0.553, 38432/60000 datapoints
2025-03-06 17:56:38,379 - INFO - training batch 1251, loss: 0.368, 40032/60000 datapoints
2025-03-06 17:56:38,573 - INFO - training batch 1301, loss: 0.397, 41632/60000 datapoints
2025-03-06 17:56:38,770 - INFO - training batch 1351, loss: 0.611, 43232/60000 datapoints
2025-03-06 17:56:38,967 - INFO - training batch 1401, loss: 0.516, 44832/60000 datapoints
2025-03-06 17:56:39,162 - INFO - training batch 1451, loss: 0.395, 46432/60000 datapoints
2025-03-06 17:56:39,356 - INFO - training batch 1501, loss: 0.601, 48032/60000 datapoints
2025-03-06 17:56:39,551 - INFO - training batch 1551, loss: 0.514, 49632/60000 datapoints
2025-03-06 17:56:39,749 - INFO - training batch 1601, loss: 0.518, 51232/60000 datapoints
2025-03-06 17:56:39,964 - INFO - training batch 1651, loss: 0.381, 52832/60000 datapoints
2025-03-06 17:56:40,157 - INFO - training batch 1701, loss: 0.451, 54432/60000 datapoints
2025-03-06 17:56:40,350 - INFO - training batch 1751, loss: 0.588, 56032/60000 datapoints
2025-03-06 17:56:40,544 - INFO - training batch 1801, loss: 0.404, 57632/60000 datapoints
2025-03-06 17:56:40,741 - INFO - training batch 1851, loss: 0.504, 59232/60000 datapoints
2025-03-06 17:56:40,841 - INFO - validation batch 1, loss: 0.543, 32/10016 datapoints
2025-03-06 17:56:40,995 - INFO - validation batch 51, loss: 0.554, 1632/10016 datapoints
2025-03-06 17:56:41,151 - INFO - validation batch 101, loss: 0.472, 3232/10016 datapoints
2025-03-06 17:56:41,305 - INFO - validation batch 151, loss: 0.484, 4832/10016 datapoints
2025-03-06 17:56:41,460 - INFO - validation batch 201, loss: 0.509, 6432/10016 datapoints
2025-03-06 17:56:41,612 - INFO - validation batch 251, loss: 0.564, 8032/10016 datapoints
2025-03-06 17:56:41,765 - INFO - validation batch 301, loss: 0.522, 9632/10016 datapoints
2025-03-06 17:56:41,801 - INFO - Epoch 91/800 done.
2025-03-06 17:56:41,802 - INFO - Final validation performance:
Loss: 0.521, top-1 acc: 0.867top-5 acc: 0.867
2025-03-06 17:56:41,802 - INFO - Beginning epoch 92/800
2025-03-06 17:56:41,810 - INFO - training batch 1, loss: 0.469, 32/60000 datapoints
2025-03-06 17:56:42,005 - INFO - training batch 51, loss: 0.383, 1632/60000 datapoints
2025-03-06 17:56:42,199 - INFO - training batch 101, loss: 0.618, 3232/60000 datapoints
2025-03-06 17:56:42,389 - INFO - training batch 151, loss: 0.771, 4832/60000 datapoints
2025-03-06 17:56:42,579 - INFO - training batch 201, loss: 0.426, 6432/60000 datapoints
2025-03-06 17:56:42,774 - INFO - training batch 251, loss: 0.506, 8032/60000 datapoints
2025-03-06 17:56:42,966 - INFO - training batch 301, loss: 0.590, 9632/60000 datapoints
2025-03-06 17:56:43,158 - INFO - training batch 351, loss: 0.555, 11232/60000 datapoints
2025-03-06 17:56:43,350 - INFO - training batch 401, loss: 0.523, 12832/60000 datapoints
2025-03-06 17:56:43,543 - INFO - training batch 451, loss: 0.674, 14432/60000 datapoints
2025-03-06 17:56:43,736 - INFO - training batch 501, loss: 0.444, 16032/60000 datapoints
2025-03-06 17:56:43,929 - INFO - training batch 551, loss: 0.483, 17632/60000 datapoints
2025-03-06 17:56:44,121 - INFO - training batch 601, loss: 0.491, 19232/60000 datapoints
2025-03-06 17:56:44,313 - INFO - training batch 651, loss: 0.501, 20832/60000 datapoints
2025-03-06 17:56:44,506 - INFO - training batch 701, loss: 0.461, 22432/60000 datapoints
2025-03-06 17:56:44,700 - INFO - training batch 751, loss: 0.728, 24032/60000 datapoints
2025-03-06 17:56:44,897 - INFO - training batch 801, loss: 0.791, 25632/60000 datapoints
2025-03-06 17:56:45,089 - INFO - training batch 851, loss: 0.654, 27232/60000 datapoints
2025-03-06 17:56:45,283 - INFO - training batch 901, loss: 0.765, 28832/60000 datapoints
2025-03-06 17:56:45,492 - INFO - training batch 951, loss: 0.459, 30432/60000 datapoints
2025-03-06 17:56:45,684 - INFO - training batch 1001, loss: 0.455, 32032/60000 datapoints
2025-03-06 17:56:45,883 - INFO - training batch 1051, loss: 0.389, 33632/60000 datapoints
2025-03-06 17:56:46,083 - INFO - training batch 1101, loss: 0.596, 35232/60000 datapoints
2025-03-06 17:56:46,281 - INFO - training batch 1151, loss: 0.472, 36832/60000 datapoints
2025-03-06 17:56:46,473 - INFO - training batch 1201, loss: 0.470, 38432/60000 datapoints
2025-03-06 17:56:46,668 - INFO - training batch 1251, loss: 0.617, 40032/60000 datapoints
2025-03-06 17:56:46,860 - INFO - training batch 1301, loss: 0.609, 41632/60000 datapoints
2025-03-06 17:56:47,050 - INFO - training batch 1351, loss: 0.667, 43232/60000 datapoints
2025-03-06 17:56:47,252 - INFO - training batch 1401, loss: 0.467, 44832/60000 datapoints
2025-03-06 17:56:47,452 - INFO - training batch 1451, loss: 0.551, 46432/60000 datapoints
2025-03-06 17:56:47,646 - INFO - training batch 1501, loss: 0.493, 48032/60000 datapoints
2025-03-06 17:56:47,836 - INFO - training batch 1551, loss: 0.780, 49632/60000 datapoints
2025-03-06 17:56:48,031 - INFO - training batch 1601, loss: 0.546, 51232/60000 datapoints
2025-03-06 17:56:48,238 - INFO - training batch 1651, loss: 0.518, 52832/60000 datapoints
2025-03-06 17:56:48,445 - INFO - training batch 1701, loss: 0.593, 54432/60000 datapoints
2025-03-06 17:56:48,642 - INFO - training batch 1751, loss: 0.703, 56032/60000 datapoints
2025-03-06 17:56:48,837 - INFO - training batch 1801, loss: 0.432, 57632/60000 datapoints
2025-03-06 17:56:49,027 - INFO - training batch 1851, loss: 0.455, 59232/60000 datapoints
2025-03-06 17:56:49,125 - INFO - validation batch 1, loss: 0.490, 32/10016 datapoints
2025-03-06 17:56:49,274 - INFO - validation batch 51, loss: 0.797, 1632/10016 datapoints
2025-03-06 17:56:49,426 - INFO - validation batch 101, loss: 0.636, 3232/10016 datapoints
2025-03-06 17:56:49,575 - INFO - validation batch 151, loss: 0.427, 4832/10016 datapoints
2025-03-06 17:56:49,731 - INFO - validation batch 201, loss: 0.722, 6432/10016 datapoints
2025-03-06 17:56:49,887 - INFO - validation batch 251, loss: 0.644, 8032/10016 datapoints
2025-03-06 17:56:50,039 - INFO - validation batch 301, loss: 0.616, 9632/10016 datapoints
2025-03-06 17:56:50,075 - INFO - Epoch 92/800 done.
2025-03-06 17:56:50,075 - INFO - Final validation performance:
Loss: 0.619, top-1 acc: 0.867top-5 acc: 0.867
2025-03-06 17:56:50,075 - INFO - Beginning epoch 93/800
2025-03-06 17:56:50,082 - INFO - training batch 1, loss: 0.663, 32/60000 datapoints
2025-03-06 17:56:50,274 - INFO - training batch 51, loss: 0.427, 1632/60000 datapoints
2025-03-06 17:56:50,465 - INFO - training batch 101, loss: 0.628, 3232/60000 datapoints
2025-03-06 17:56:50,655 - INFO - training batch 151, loss: 0.424, 4832/60000 datapoints
2025-03-06 17:56:50,845 - INFO - training batch 201, loss: 0.614, 6432/60000 datapoints
2025-03-06 17:56:51,039 - INFO - training batch 251, loss: 0.755, 8032/60000 datapoints
2025-03-06 17:56:51,230 - INFO - training batch 301, loss: 0.563, 9632/60000 datapoints
2025-03-06 17:56:51,423 - INFO - training batch 351, loss: 0.630, 11232/60000 datapoints
2025-03-06 17:56:51,617 - INFO - training batch 401, loss: 0.468, 12832/60000 datapoints
2025-03-06 17:56:51,810 - INFO - training batch 451, loss: 0.232, 14432/60000 datapoints
2025-03-06 17:56:52,004 - INFO - training batch 501, loss: 0.363, 16032/60000 datapoints
2025-03-06 17:56:52,194 - INFO - training batch 551, loss: 0.619, 17632/60000 datapoints
2025-03-06 17:56:52,384 - INFO - training batch 601, loss: 0.643, 19232/60000 datapoints
2025-03-06 17:56:52,581 - INFO - training batch 651, loss: 0.465, 20832/60000 datapoints
2025-03-06 17:56:52,773 - INFO - training batch 701, loss: 0.510, 22432/60000 datapoints
2025-03-06 17:56:52,962 - INFO - training batch 751, loss: 0.370, 24032/60000 datapoints
2025-03-06 17:56:53,152 - INFO - training batch 801, loss: 0.518, 25632/60000 datapoints
2025-03-06 17:56:53,344 - INFO - training batch 851, loss: 0.501, 27232/60000 datapoints
2025-03-06 17:56:53,535 - INFO - training batch 901, loss: 0.545, 28832/60000 datapoints
2025-03-06 17:56:53,730 - INFO - training batch 951, loss: 0.603, 30432/60000 datapoints
2025-03-06 17:56:53,922 - INFO - training batch 1001, loss: 0.418, 32032/60000 datapoints
2025-03-06 17:56:54,117 - INFO - training batch 1051, loss: 0.421, 33632/60000 datapoints
2025-03-06 17:56:54,307 - INFO - training batch 1101, loss: 0.493, 35232/60000 datapoints
2025-03-06 17:56:54,502 - INFO - training batch 1151, loss: 0.640, 36832/60000 datapoints
2025-03-06 17:56:54,694 - INFO - training batch 1201, loss: 0.408, 38432/60000 datapoints
2025-03-06 17:56:54,891 - INFO - training batch 1251, loss: 0.564, 40032/60000 datapoints
2025-03-06 17:56:55,083 - INFO - training batch 1301, loss: 0.446, 41632/60000 datapoints
2025-03-06 17:56:55,272 - INFO - training batch 1351, loss: 0.551, 43232/60000 datapoints
2025-03-06 17:56:55,485 - INFO - training batch 1401, loss: 0.479, 44832/60000 datapoints
2025-03-06 17:56:55,683 - INFO - training batch 1451, loss: 0.683, 46432/60000 datapoints
2025-03-06 17:56:55,873 - INFO - training batch 1501, loss: 0.566, 48032/60000 datapoints
2025-03-06 17:56:56,066 - INFO - training batch 1551, loss: 0.694, 49632/60000 datapoints
2025-03-06 17:56:56,260 - INFO - training batch 1601, loss: 0.577, 51232/60000 datapoints
2025-03-06 17:56:56,450 - INFO - training batch 1651, loss: 0.482, 52832/60000 datapoints
2025-03-06 17:56:56,648 - INFO - training batch 1701, loss: 0.647, 54432/60000 datapoints
2025-03-06 17:56:56,839 - INFO - training batch 1751, loss: 0.635, 56032/60000 datapoints
2025-03-06 17:56:57,030 - INFO - training batch 1801, loss: 0.440, 57632/60000 datapoints
2025-03-06 17:56:57,221 - INFO - training batch 1851, loss: 0.425, 59232/60000 datapoints
2025-03-06 17:56:57,317 - INFO - validation batch 1, loss: 0.487, 32/10016 datapoints
2025-03-06 17:56:57,466 - INFO - validation batch 51, loss: 0.642, 1632/10016 datapoints
2025-03-06 17:56:57,623 - INFO - validation batch 101, loss: 0.437, 3232/10016 datapoints
2025-03-06 17:56:57,798 - INFO - validation batch 151, loss: 0.583, 4832/10016 datapoints
2025-03-06 17:56:57,947 - INFO - validation batch 201, loss: 0.650, 6432/10016 datapoints
2025-03-06 17:56:58,102 - INFO - validation batch 251, loss: 0.467, 8032/10016 datapoints
2025-03-06 17:56:58,252 - INFO - validation batch 301, loss: 0.558, 9632/10016 datapoints
2025-03-06 17:56:58,287 - INFO - Epoch 93/800 done.
2025-03-06 17:56:58,288 - INFO - Final validation performance:
Loss: 0.546, top-1 acc: 0.868top-5 acc: 0.868
2025-03-06 17:56:58,288 - INFO - Beginning epoch 94/800
2025-03-06 17:56:58,293 - INFO - training batch 1, loss: 0.562, 32/60000 datapoints
2025-03-06 17:56:58,492 - INFO - training batch 51, loss: 0.803, 1632/60000 datapoints
2025-03-06 17:56:58,687 - INFO - training batch 101, loss: 0.680, 3232/60000 datapoints
2025-03-06 17:56:58,879 - INFO - training batch 151, loss: 0.628, 4832/60000 datapoints
2025-03-06 17:56:59,071 - INFO - training batch 201, loss: 0.523, 6432/60000 datapoints
2025-03-06 17:56:59,262 - INFO - training batch 251, loss: 0.482, 8032/60000 datapoints
2025-03-06 17:56:59,454 - INFO - training batch 301, loss: 0.713, 9632/60000 datapoints
2025-03-06 17:56:59,651 - INFO - training batch 351, loss: 0.307, 11232/60000 datapoints
2025-03-06 17:56:59,846 - INFO - training batch 401, loss: 0.859, 12832/60000 datapoints
2025-03-06 17:57:00,040 - INFO - training batch 451, loss: 0.577, 14432/60000 datapoints
2025-03-06 17:57:00,232 - INFO - training batch 501, loss: 0.465, 16032/60000 datapoints
2025-03-06 17:57:00,424 - INFO - training batch 551, loss: 0.476, 17632/60000 datapoints
2025-03-06 17:57:00,634 - INFO - training batch 601, loss: 0.597, 19232/60000 datapoints
2025-03-06 17:57:00,846 - INFO - training batch 651, loss: 0.486, 20832/60000 datapoints
2025-03-06 17:57:01,037 - INFO - training batch 701, loss: 0.441, 22432/60000 datapoints
2025-03-06 17:57:01,230 - INFO - training batch 751, loss: 0.663, 24032/60000 datapoints
2025-03-06 17:57:01,423 - INFO - training batch 801, loss: 0.404, 25632/60000 datapoints
2025-03-06 17:57:01,626 - INFO - training batch 851, loss: 0.674, 27232/60000 datapoints
2025-03-06 17:57:01,821 - INFO - training batch 901, loss: 0.400, 28832/60000 datapoints
2025-03-06 17:57:02,016 - INFO - training batch 951, loss: 0.472, 30432/60000 datapoints
2025-03-06 17:57:02,215 - INFO - training batch 1001, loss: 0.592, 32032/60000 datapoints
2025-03-06 17:57:02,410 - INFO - training batch 1051, loss: 0.559, 33632/60000 datapoints
2025-03-06 17:57:02,608 - INFO - training batch 1101, loss: 0.585, 35232/60000 datapoints
2025-03-06 17:57:02,803 - INFO - training batch 1151, loss: 0.332, 36832/60000 datapoints
2025-03-06 17:57:02,994 - INFO - training batch 1201, loss: 0.494, 38432/60000 datapoints
2025-03-06 17:57:03,190 - INFO - training batch 1251, loss: 0.637, 40032/60000 datapoints
2025-03-06 17:57:03,384 - INFO - training batch 1301, loss: 0.324, 41632/60000 datapoints
2025-03-06 17:57:03,584 - INFO - training batch 1351, loss: 0.515, 43232/60000 datapoints
2025-03-06 17:57:03,800 - INFO - training batch 1401, loss: 0.868, 44832/60000 datapoints
2025-03-06 17:57:04,028 - INFO - training batch 1451, loss: 0.707, 46432/60000 datapoints
2025-03-06 17:57:04,249 - INFO - training batch 1501, loss: 0.341, 48032/60000 datapoints
2025-03-06 17:57:04,462 - INFO - training batch 1551, loss: 0.505, 49632/60000 datapoints
2025-03-06 17:57:04,661 - INFO - training batch 1601, loss: 0.577, 51232/60000 datapoints
2025-03-06 17:57:04,860 - INFO - training batch 1651, loss: 0.744, 52832/60000 datapoints
2025-03-06 17:57:05,059 - INFO - training batch 1701, loss: 0.504, 54432/60000 datapoints
2025-03-06 17:57:05,259 - INFO - training batch 1751, loss: 0.486, 56032/60000 datapoints
2025-03-06 17:57:05,454 - INFO - training batch 1801, loss: 0.449, 57632/60000 datapoints
2025-03-06 17:57:05,672 - INFO - training batch 1851, loss: 0.520, 59232/60000 datapoints
2025-03-06 17:57:05,772 - INFO - validation batch 1, loss: 0.390, 32/10016 datapoints
2025-03-06 17:57:05,928 - INFO - validation batch 51, loss: 0.498, 1632/10016 datapoints
2025-03-06 17:57:06,086 - INFO - validation batch 101, loss: 0.452, 3232/10016 datapoints
2025-03-06 17:57:06,241 - INFO - validation batch 151, loss: 0.400, 4832/10016 datapoints
2025-03-06 17:57:06,397 - INFO - validation batch 201, loss: 0.488, 6432/10016 datapoints
2025-03-06 17:57:06,548 - INFO - validation batch 251, loss: 0.547, 8032/10016 datapoints
2025-03-06 17:57:06,708 - INFO - validation batch 301, loss: 0.530, 9632/10016 datapoints
2025-03-06 17:57:06,744 - INFO - Epoch 94/800 done.
2025-03-06 17:57:06,744 - INFO - Final validation performance:
Loss: 0.472, top-1 acc: 0.869top-5 acc: 0.869
2025-03-06 17:57:06,744 - INFO - Beginning epoch 95/800
2025-03-06 17:57:06,752 - INFO - training batch 1, loss: 0.688, 32/60000 datapoints
2025-03-06 17:57:06,951 - INFO - training batch 51, loss: 0.481, 1632/60000 datapoints
2025-03-06 17:57:07,143 - INFO - training batch 101, loss: 0.358, 3232/60000 datapoints
2025-03-06 17:57:07,337 - INFO - training batch 151, loss: 0.487, 4832/60000 datapoints
2025-03-06 17:57:07,531 - INFO - training batch 201, loss: 0.480, 6432/60000 datapoints
2025-03-06 17:57:07,734 - INFO - training batch 251, loss: 0.435, 8032/60000 datapoints
2025-03-06 17:57:07,932 - INFO - training batch 301, loss: 0.429, 9632/60000 datapoints
2025-03-06 17:57:08,137 - INFO - training batch 351, loss: 0.426, 11232/60000 datapoints
2025-03-06 17:57:08,333 - INFO - training batch 401, loss: 0.555, 12832/60000 datapoints
2025-03-06 17:57:08,528 - INFO - training batch 451, loss: 0.506, 14432/60000 datapoints
2025-03-06 17:57:08,726 - INFO - training batch 501, loss: 0.498, 16032/60000 datapoints
2025-03-06 17:57:08,921 - INFO - training batch 551, loss: 0.500, 17632/60000 datapoints
2025-03-06 17:57:09,118 - INFO - training batch 601, loss: 0.622, 19232/60000 datapoints
2025-03-06 17:57:09,311 - INFO - training batch 651, loss: 0.481, 20832/60000 datapoints
2025-03-06 17:57:09,506 - INFO - training batch 701, loss: 0.459, 22432/60000 datapoints
2025-03-06 17:57:09,704 - INFO - training batch 751, loss: 0.684, 24032/60000 datapoints
2025-03-06 17:57:09,899 - INFO - training batch 801, loss: 0.643, 25632/60000 datapoints
2025-03-06 17:57:10,099 - INFO - training batch 851, loss: 0.332, 27232/60000 datapoints
2025-03-06 17:57:10,293 - INFO - training batch 901, loss: 0.432, 28832/60000 datapoints
2025-03-06 17:57:10,488 - INFO - training batch 951, loss: 0.711, 30432/60000 datapoints
2025-03-06 17:57:10,685 - INFO - training batch 1001, loss: 0.464, 32032/60000 datapoints
2025-03-06 17:57:10,878 - INFO - training batch 1051, loss: 0.662, 33632/60000 datapoints
2025-03-06 17:57:11,072 - INFO - training batch 1101, loss: 0.609, 35232/60000 datapoints
2025-03-06 17:57:11,270 - INFO - training batch 1151, loss: 0.472, 36832/60000 datapoints
2025-03-06 17:57:11,465 - INFO - training batch 1201, loss: 0.420, 38432/60000 datapoints
2025-03-06 17:57:11,661 - INFO - training batch 1251, loss: 0.539, 40032/60000 datapoints
2025-03-06 17:57:11,855 - INFO - training batch 1301, loss: 0.431, 41632/60000 datapoints
2025-03-06 17:57:12,057 - INFO - training batch 1351, loss: 0.479, 43232/60000 datapoints
2025-03-06 17:57:12,253 - INFO - training batch 1401, loss: 0.693, 44832/60000 datapoints
2025-03-06 17:57:12,449 - INFO - training batch 1451, loss: 0.613, 46432/60000 datapoints
2025-03-06 17:57:12,647 - INFO - training batch 1501, loss: 0.607, 48032/60000 datapoints
2025-03-06 17:57:12,842 - INFO - training batch 1551, loss: 0.747, 49632/60000 datapoints
2025-03-06 17:57:13,036 - INFO - training batch 1601, loss: 0.552, 51232/60000 datapoints
2025-03-06 17:57:13,237 - INFO - training batch 1651, loss: 0.463, 52832/60000 datapoints
2025-03-06 17:57:13,431 - INFO - training batch 1701, loss: 0.551, 54432/60000 datapoints
2025-03-06 17:57:13,629 - INFO - training batch 1751, loss: 0.506, 56032/60000 datapoints
2025-03-06 17:57:13,824 - INFO - training batch 1801, loss: 0.490, 57632/60000 datapoints
2025-03-06 17:57:14,021 - INFO - training batch 1851, loss: 0.505, 59232/60000 datapoints
2025-03-06 17:57:14,124 - INFO - validation batch 1, loss: 0.391, 32/10016 datapoints
2025-03-06 17:57:14,278 - INFO - validation batch 51, loss: 0.444, 1632/10016 datapoints
2025-03-06 17:57:14,430 - INFO - validation batch 101, loss: 0.700, 3232/10016 datapoints
2025-03-06 17:57:14,584 - INFO - validation batch 151, loss: 0.443, 4832/10016 datapoints
2025-03-06 17:57:14,740 - INFO - validation batch 201, loss: 0.487, 6432/10016 datapoints
2025-03-06 17:57:14,896 - INFO - validation batch 251, loss: 0.456, 8032/10016 datapoints
2025-03-06 17:57:15,049 - INFO - validation batch 301, loss: 0.350, 9632/10016 datapoints
2025-03-06 17:57:15,086 - INFO - Epoch 95/800 done.
2025-03-06 17:57:15,086 - INFO - Final validation performance:
Loss: 0.467, top-1 acc: 0.869top-5 acc: 0.869
2025-03-06 17:57:15,087 - INFO - Beginning epoch 96/800
2025-03-06 17:57:15,092 - INFO - training batch 1, loss: 0.649, 32/60000 datapoints
2025-03-06 17:57:15,333 - INFO - training batch 51, loss: 0.548, 1632/60000 datapoints
2025-03-06 17:57:15,530 - INFO - training batch 101, loss: 0.584, 3232/60000 datapoints
2025-03-06 17:57:15,749 - INFO - training batch 151, loss: 0.565, 4832/60000 datapoints
2025-03-06 17:57:15,943 - INFO - training batch 201, loss: 0.378, 6432/60000 datapoints
2025-03-06 17:57:16,140 - INFO - training batch 251, loss: 0.514, 8032/60000 datapoints
2025-03-06 17:57:16,337 - INFO - training batch 301, loss: 0.688, 9632/60000 datapoints
2025-03-06 17:57:16,531 - INFO - training batch 351, loss: 0.904, 11232/60000 datapoints
2025-03-06 17:57:16,735 - INFO - training batch 401, loss: 0.347, 12832/60000 datapoints
2025-03-06 17:57:16,930 - INFO - training batch 451, loss: 0.911, 14432/60000 datapoints
2025-03-06 17:57:17,125 - INFO - training batch 501, loss: 0.566, 16032/60000 datapoints
2025-03-06 17:57:17,321 - INFO - training batch 551, loss: 0.345, 17632/60000 datapoints
2025-03-06 17:57:17,515 - INFO - training batch 601, loss: 0.480, 19232/60000 datapoints
2025-03-06 17:57:17,712 - INFO - training batch 651, loss: 0.847, 20832/60000 datapoints
2025-03-06 17:57:17,906 - INFO - training batch 701, loss: 0.492, 22432/60000 datapoints
2025-03-06 17:57:18,102 - INFO - training batch 751, loss: 0.771, 24032/60000 datapoints
2025-03-06 17:57:18,296 - INFO - training batch 801, loss: 0.641, 25632/60000 datapoints
2025-03-06 17:57:18,491 - INFO - training batch 851, loss: 0.509, 27232/60000 datapoints
2025-03-06 17:57:18,688 - INFO - training batch 901, loss: 0.527, 28832/60000 datapoints
2025-03-06 17:57:18,885 - INFO - training batch 951, loss: 0.596, 30432/60000 datapoints
2025-03-06 17:57:19,080 - INFO - training batch 1001, loss: 0.514, 32032/60000 datapoints
2025-03-06 17:57:19,274 - INFO - training batch 1051, loss: 0.668, 33632/60000 datapoints
2025-03-06 17:57:19,469 - INFO - training batch 1101, loss: 0.500, 35232/60000 datapoints
2025-03-06 17:57:19,665 - INFO - training batch 1151, loss: 0.427, 36832/60000 datapoints
2025-03-06 17:57:19,868 - INFO - training batch 1201, loss: 0.533, 38432/60000 datapoints
2025-03-06 17:57:20,062 - INFO - training batch 1251, loss: 0.626, 40032/60000 datapoints
2025-03-06 17:57:20,257 - INFO - training batch 1301, loss: 0.583, 41632/60000 datapoints
2025-03-06 17:57:20,454 - INFO - training batch 1351, loss: 0.470, 43232/60000 datapoints
2025-03-06 17:57:20,650 - INFO - training batch 1401, loss: 0.684, 44832/60000 datapoints
2025-03-06 17:57:20,849 - INFO - training batch 1451, loss: 0.591, 46432/60000 datapoints
2025-03-06 17:57:21,043 - INFO - training batch 1501, loss: 0.571, 48032/60000 datapoints
2025-03-06 17:57:21,236 - INFO - training batch 1551, loss: 0.445, 49632/60000 datapoints
2025-03-06 17:57:21,431 - INFO - training batch 1601, loss: 0.744, 51232/60000 datapoints
2025-03-06 17:57:21,627 - INFO - training batch 1651, loss: 0.684, 52832/60000 datapoints
2025-03-06 17:57:21,822 - INFO - training batch 1701, loss: 0.609, 54432/60000 datapoints
2025-03-06 17:57:22,015 - INFO - training batch 1751, loss: 0.688, 56032/60000 datapoints
2025-03-06 17:57:22,209 - INFO - training batch 1801, loss: 0.399, 57632/60000 datapoints
2025-03-06 17:57:22,402 - INFO - training batch 1851, loss: 0.561, 59232/60000 datapoints
2025-03-06 17:57:22,503 - INFO - validation batch 1, loss: 0.292, 32/10016 datapoints
2025-03-06 17:57:22,658 - INFO - validation batch 51, loss: 0.561, 1632/10016 datapoints
2025-03-06 17:57:22,811 - INFO - validation batch 101, loss: 0.579, 3232/10016 datapoints
2025-03-06 17:57:22,964 - INFO - validation batch 151, loss: 0.406, 4832/10016 datapoints
2025-03-06 17:57:23,116 - INFO - validation batch 201, loss: 0.677, 6432/10016 datapoints
2025-03-06 17:57:23,268 - INFO - validation batch 251, loss: 0.327, 8032/10016 datapoints
2025-03-06 17:57:23,423 - INFO - validation batch 301, loss: 0.530, 9632/10016 datapoints
2025-03-06 17:57:23,460 - INFO - Epoch 96/800 done.
2025-03-06 17:57:23,461 - INFO - Final validation performance:
Loss: 0.482, top-1 acc: 0.869top-5 acc: 0.869
2025-03-06 17:57:23,461 - INFO - Beginning epoch 97/800
2025-03-06 17:57:23,467 - INFO - training batch 1, loss: 0.707, 32/60000 datapoints
2025-03-06 17:57:23,664 - INFO - training batch 51, loss: 0.481, 1632/60000 datapoints
2025-03-06 17:57:23,861 - INFO - training batch 101, loss: 0.777, 3232/60000 datapoints
2025-03-06 17:57:24,056 - INFO - training batch 151, loss: 0.724, 4832/60000 datapoints
2025-03-06 17:57:24,253 - INFO - training batch 201, loss: 0.582, 6432/60000 datapoints
2025-03-06 17:57:24,448 - INFO - training batch 251, loss: 0.766, 8032/60000 datapoints
2025-03-06 17:57:24,643 - INFO - training batch 301, loss: 0.623, 9632/60000 datapoints
2025-03-06 17:57:24,859 - INFO - training batch 351, loss: 0.435, 11232/60000 datapoints
2025-03-06 17:57:25,054 - INFO - training batch 401, loss: 0.624, 12832/60000 datapoints
2025-03-06 17:57:25,248 - INFO - training batch 451, loss: 0.353, 14432/60000 datapoints
2025-03-06 17:57:25,441 - INFO - training batch 501, loss: 0.362, 16032/60000 datapoints
2025-03-06 17:57:25,638 - INFO - training batch 551, loss: 0.596, 17632/60000 datapoints
2025-03-06 17:57:25,853 - INFO - training batch 601, loss: 0.515, 19232/60000 datapoints
2025-03-06 17:57:26,049 - INFO - training batch 651, loss: 0.775, 20832/60000 datapoints
2025-03-06 17:57:26,249 - INFO - training batch 701, loss: 0.345, 22432/60000 datapoints
2025-03-06 17:57:26,443 - INFO - training batch 751, loss: 0.390, 24032/60000 datapoints
2025-03-06 17:57:26,640 - INFO - training batch 801, loss: 0.275, 25632/60000 datapoints
2025-03-06 17:57:26,835 - INFO - training batch 851, loss: 0.491, 27232/60000 datapoints
2025-03-06 17:57:27,029 - INFO - training batch 901, loss: 0.669, 28832/60000 datapoints
2025-03-06 17:57:27,221 - INFO - training batch 951, loss: 0.540, 30432/60000 datapoints
2025-03-06 17:57:27,414 - INFO - training batch 1001, loss: 0.610, 32032/60000 datapoints
2025-03-06 17:57:27,609 - INFO - training batch 1051, loss: 0.513, 33632/60000 datapoints
2025-03-06 17:57:27,806 - INFO - training batch 1101, loss: 0.438, 35232/60000 datapoints
2025-03-06 17:57:28,002 - INFO - training batch 1151, loss: 0.525, 36832/60000 datapoints
2025-03-06 17:57:28,198 - INFO - training batch 1201, loss: 0.594, 38432/60000 datapoints
2025-03-06 17:57:28,391 - INFO - training batch 1251, loss: 0.655, 40032/60000 datapoints
2025-03-06 17:57:28,585 - INFO - training batch 1301, loss: 0.368, 41632/60000 datapoints
2025-03-06 17:57:28,787 - INFO - training batch 1351, loss: 0.471, 43232/60000 datapoints
2025-03-06 17:57:28,984 - INFO - training batch 1401, loss: 0.394, 44832/60000 datapoints
2025-03-06 17:57:29,179 - INFO - training batch 1451, loss: 0.432, 46432/60000 datapoints
2025-03-06 17:57:29,376 - INFO - training batch 1501, loss: 0.365, 48032/60000 datapoints
2025-03-06 17:57:29,571 - INFO - training batch 1551, loss: 0.846, 49632/60000 datapoints
2025-03-06 17:57:29,766 - INFO - training batch 1601, loss: 0.495, 51232/60000 datapoints
2025-03-06 17:57:29,960 - INFO - training batch 1651, loss: 0.797, 52832/60000 datapoints
2025-03-06 17:57:30,156 - INFO - training batch 1701, loss: 0.482, 54432/60000 datapoints
2025-03-06 17:57:30,352 - INFO - training batch 1751, loss: 0.692, 56032/60000 datapoints
2025-03-06 17:57:30,545 - INFO - training batch 1801, loss: 0.564, 57632/60000 datapoints
2025-03-06 17:57:30,741 - INFO - training batch 1851, loss: 0.400, 59232/60000 datapoints
2025-03-06 17:57:30,843 - INFO - validation batch 1, loss: 0.259, 32/10016 datapoints
2025-03-06 17:57:30,998 - INFO - validation batch 51, loss: 0.709, 1632/10016 datapoints
2025-03-06 17:57:31,150 - INFO - validation batch 101, loss: 0.410, 3232/10016 datapoints
2025-03-06 17:57:31,305 - INFO - validation batch 151, loss: 0.547, 4832/10016 datapoints
2025-03-06 17:57:31,459 - INFO - validation batch 201, loss: 0.490, 6432/10016 datapoints
2025-03-06 17:57:31,614 - INFO - validation batch 251, loss: 0.608, 8032/10016 datapoints
2025-03-06 17:57:31,769 - INFO - validation batch 301, loss: 0.406, 9632/10016 datapoints
2025-03-06 17:57:31,806 - INFO - Epoch 97/800 done.
2025-03-06 17:57:31,806 - INFO - Final validation performance:
Loss: 0.490, top-1 acc: 0.870top-5 acc: 0.870
2025-03-06 17:57:31,807 - INFO - Beginning epoch 98/800
2025-03-06 17:57:31,813 - INFO - training batch 1, loss: 0.438, 32/60000 datapoints
2025-03-06 17:57:32,005 - INFO - training batch 51, loss: 0.316, 1632/60000 datapoints
2025-03-06 17:57:32,199 - INFO - training batch 101, loss: 0.563, 3232/60000 datapoints
2025-03-06 17:57:32,388 - INFO - training batch 151, loss: 0.896, 4832/60000 datapoints
2025-03-06 17:57:32,581 - INFO - training batch 201, loss: 0.638, 6432/60000 datapoints
2025-03-06 17:57:32,777 - INFO - training batch 251, loss: 0.344, 8032/60000 datapoints
2025-03-06 17:57:32,971 - INFO - training batch 301, loss: 0.514, 9632/60000 datapoints
2025-03-06 17:57:33,162 - INFO - training batch 351, loss: 0.447, 11232/60000 datapoints
2025-03-06 17:57:33,353 - INFO - training batch 401, loss: 0.552, 12832/60000 datapoints
2025-03-06 17:57:33,547 - INFO - training batch 451, loss: 0.504, 14432/60000 datapoints
2025-03-06 17:57:33,742 - INFO - training batch 501, loss: 0.327, 16032/60000 datapoints
2025-03-06 17:57:33,934 - INFO - training batch 551, loss: 0.288, 17632/60000 datapoints
2025-03-06 17:57:34,125 - INFO - training batch 601, loss: 0.379, 19232/60000 datapoints
2025-03-06 17:57:34,318 - INFO - training batch 651, loss: 0.554, 20832/60000 datapoints
2025-03-06 17:57:34,509 - INFO - training batch 701, loss: 0.464, 22432/60000 datapoints
2025-03-06 17:57:34,703 - INFO - training batch 751, loss: 0.597, 24032/60000 datapoints
2025-03-06 17:57:34,915 - INFO - training batch 801, loss: 0.727, 25632/60000 datapoints
2025-03-06 17:57:35,116 - INFO - training batch 851, loss: 0.267, 27232/60000 datapoints
2025-03-06 17:57:35,314 - INFO - training batch 901, loss: 0.579, 28832/60000 datapoints
2025-03-06 17:57:35,505 - INFO - training batch 951, loss: 0.514, 30432/60000 datapoints
2025-03-06 17:57:35,698 - INFO - training batch 1001, loss: 0.372, 32032/60000 datapoints
2025-03-06 17:57:35,907 - INFO - training batch 1051, loss: 0.477, 33632/60000 datapoints
2025-03-06 17:57:36,098 - INFO - training batch 1101, loss: 0.445, 35232/60000 datapoints
2025-03-06 17:57:36,297 - INFO - training batch 1151, loss: 0.461, 36832/60000 datapoints
2025-03-06 17:57:36,491 - INFO - training batch 1201, loss: 0.462, 38432/60000 datapoints
2025-03-06 17:57:36,690 - INFO - training batch 1251, loss: 0.436, 40032/60000 datapoints
2025-03-06 17:57:36,883 - INFO - training batch 1301, loss: 0.519, 41632/60000 datapoints
2025-03-06 17:57:37,074 - INFO - training batch 1351, loss: 0.425, 43232/60000 datapoints
2025-03-06 17:57:37,265 - INFO - training batch 1401, loss: 0.264, 44832/60000 datapoints
2025-03-06 17:57:37,455 - INFO - training batch 1451, loss: 0.379, 46432/60000 datapoints
2025-03-06 17:57:37,663 - INFO - training batch 1501, loss: 0.795, 48032/60000 datapoints
2025-03-06 17:57:37,855 - INFO - training batch 1551, loss: 0.521, 49632/60000 datapoints
2025-03-06 17:57:38,046 - INFO - training batch 1601, loss: 0.643, 51232/60000 datapoints
2025-03-06 17:57:38,239 - INFO - training batch 1651, loss: 0.460, 52832/60000 datapoints
2025-03-06 17:57:38,430 - INFO - training batch 1701, loss: 0.545, 54432/60000 datapoints
2025-03-06 17:57:38,625 - INFO - training batch 1751, loss: 0.566, 56032/60000 datapoints
2025-03-06 17:57:38,818 - INFO - training batch 1801, loss: 0.644, 57632/60000 datapoints
2025-03-06 17:57:39,012 - INFO - training batch 1851, loss: 0.621, 59232/60000 datapoints
2025-03-06 17:57:39,110 - INFO - validation batch 1, loss: 0.429, 32/10016 datapoints
2025-03-06 17:57:39,259 - INFO - validation batch 51, loss: 0.554, 1632/10016 datapoints
2025-03-06 17:57:39,408 - INFO - validation batch 101, loss: 0.318, 3232/10016 datapoints
2025-03-06 17:57:39,559 - INFO - validation batch 151, loss: 0.581, 4832/10016 datapoints
2025-03-06 17:57:39,712 - INFO - validation batch 201, loss: 0.410, 6432/10016 datapoints
2025-03-06 17:57:39,862 - INFO - validation batch 251, loss: 0.367, 8032/10016 datapoints
2025-03-06 17:57:40,023 - INFO - validation batch 301, loss: 0.485, 9632/10016 datapoints
2025-03-06 17:57:40,059 - INFO - Epoch 98/800 done.
2025-03-06 17:57:40,059 - INFO - Final validation performance:
Loss: 0.449, top-1 acc: 0.871top-5 acc: 0.871
2025-03-06 17:57:40,060 - INFO - Beginning epoch 99/800
2025-03-06 17:57:40,065 - INFO - training batch 1, loss: 0.465, 32/60000 datapoints
2025-03-06 17:57:40,258 - INFO - training batch 51, loss: 0.408, 1632/60000 datapoints
2025-03-06 17:57:40,448 - INFO - training batch 101, loss: 0.429, 3232/60000 datapoints
2025-03-06 17:57:40,645 - INFO - training batch 151, loss: 0.534, 4832/60000 datapoints
2025-03-06 17:57:40,837 - INFO - training batch 201, loss: 0.531, 6432/60000 datapoints
2025-03-06 17:57:41,029 - INFO - training batch 251, loss: 0.451, 8032/60000 datapoints
2025-03-06 17:57:41,221 - INFO - training batch 301, loss: 0.407, 9632/60000 datapoints
2025-03-06 17:57:41,411 - INFO - training batch 351, loss: 0.572, 11232/60000 datapoints
2025-03-06 17:57:41,611 - INFO - training batch 401, loss: 0.510, 12832/60000 datapoints
2025-03-06 17:57:41,805 - INFO - training batch 451, loss: 0.393, 14432/60000 datapoints
2025-03-06 17:57:42,000 - INFO - training batch 501, loss: 0.521, 16032/60000 datapoints
2025-03-06 17:57:42,198 - INFO - training batch 551, loss: 0.620, 17632/60000 datapoints
2025-03-06 17:57:42,391 - INFO - training batch 601, loss: 0.576, 19232/60000 datapoints
2025-03-06 17:57:42,585 - INFO - training batch 651, loss: 0.410, 20832/60000 datapoints
2025-03-06 17:57:42,783 - INFO - training batch 701, loss: 0.511, 22432/60000 datapoints
2025-03-06 17:57:42,977 - INFO - training batch 751, loss: 0.416, 24032/60000 datapoints
2025-03-06 17:57:43,172 - INFO - training batch 801, loss: 0.436, 25632/60000 datapoints
2025-03-06 17:57:43,367 - INFO - training batch 851, loss: 0.560, 27232/60000 datapoints
2025-03-06 17:57:43,562 - INFO - training batch 901, loss: 0.438, 28832/60000 datapoints
2025-03-06 17:57:43,757 - INFO - training batch 951, loss: 0.401, 30432/60000 datapoints
2025-03-06 17:57:43,951 - INFO - training batch 1001, loss: 0.509, 32032/60000 datapoints
2025-03-06 17:57:44,146 - INFO - training batch 1051, loss: 0.600, 33632/60000 datapoints
2025-03-06 17:57:44,342 - INFO - training batch 1101, loss: 0.734, 35232/60000 datapoints
2025-03-06 17:57:44,535 - INFO - training batch 1151, loss: 0.627, 36832/60000 datapoints
2025-03-06 17:57:44,730 - INFO - training batch 1201, loss: 0.381, 38432/60000 datapoints
2025-03-06 17:57:44,927 - INFO - training batch 1251, loss: 0.492, 40032/60000 datapoints
2025-03-06 17:57:45,120 - INFO - training batch 1301, loss: 0.571, 41632/60000 datapoints
2025-03-06 17:57:45,314 - INFO - training batch 1351, loss: 0.491, 43232/60000 datapoints
2025-03-06 17:57:45,506 - INFO - training batch 1401, loss: 0.464, 44832/60000 datapoints
2025-03-06 17:57:45,704 - INFO - training batch 1451, loss: 0.460, 46432/60000 datapoints
2025-03-06 17:57:45,918 - INFO - training batch 1501, loss: 0.454, 48032/60000 datapoints
2025-03-06 17:57:46,120 - INFO - training batch 1551, loss: 0.391, 49632/60000 datapoints
2025-03-06 17:57:46,319 - INFO - training batch 1601, loss: 0.255, 51232/60000 datapoints
2025-03-06 17:57:46,513 - INFO - training batch 1651, loss: 0.418, 52832/60000 datapoints
2025-03-06 17:57:46,713 - INFO - training batch 1701, loss: 0.493, 54432/60000 datapoints
2025-03-06 17:57:46,908 - INFO - training batch 1751, loss: 0.564, 56032/60000 datapoints
2025-03-06 17:57:47,100 - INFO - training batch 1801, loss: 0.493, 57632/60000 datapoints
2025-03-06 17:57:47,295 - INFO - training batch 1851, loss: 0.574, 59232/60000 datapoints
2025-03-06 17:57:47,395 - INFO - validation batch 1, loss: 0.632, 32/10016 datapoints
2025-03-06 17:57:47,546 - INFO - validation batch 51, loss: 0.463, 1632/10016 datapoints
2025-03-06 17:57:47,702 - INFO - validation batch 101, loss: 0.497, 3232/10016 datapoints
2025-03-06 17:57:47,855 - INFO - validation batch 151, loss: 0.324, 4832/10016 datapoints
2025-03-06 17:57:48,006 - INFO - validation batch 201, loss: 0.682, 6432/10016 datapoints
2025-03-06 17:57:48,159 - INFO - validation batch 251, loss: 0.588, 8032/10016 datapoints
2025-03-06 17:57:48,312 - INFO - validation batch 301, loss: 0.380, 9632/10016 datapoints
2025-03-06 17:57:48,348 - INFO - Epoch 99/800 done.
2025-03-06 17:57:48,349 - INFO - Final validation performance:
Loss: 0.509, top-1 acc: 0.872top-5 acc: 0.872
2025-03-06 17:57:48,349 - INFO - Beginning epoch 100/800
2025-03-06 17:57:48,355 - INFO - training batch 1, loss: 0.536, 32/60000 datapoints
2025-03-06 17:57:48,550 - INFO - training batch 51, loss: 0.358, 1632/60000 datapoints
2025-03-06 17:57:48,745 - INFO - training batch 101, loss: 0.415, 3232/60000 datapoints
2025-03-06 17:57:48,936 - INFO - training batch 151, loss: 0.640, 4832/60000 datapoints
2025-03-06 17:57:49,127 - INFO - training batch 201, loss: 0.309, 6432/60000 datapoints
2025-03-06 17:57:49,332 - INFO - training batch 251, loss: 0.489, 8032/60000 datapoints
2025-03-06 17:57:49,526 - INFO - training batch 301, loss: 0.505, 9632/60000 datapoints
2025-03-06 17:57:49,720 - INFO - training batch 351, loss: 0.294, 11232/60000 datapoints
2025-03-06 17:57:49,911 - INFO - training batch 401, loss: 0.621, 12832/60000 datapoints
2025-03-06 17:57:50,101 - INFO - training batch 451, loss: 0.517, 14432/60000 datapoints
2025-03-06 17:57:50,296 - INFO - training batch 501, loss: 0.561, 16032/60000 datapoints
2025-03-06 17:57:50,488 - INFO - training batch 551, loss: 0.420, 17632/60000 datapoints
2025-03-06 17:57:50,681 - INFO - training batch 601, loss: 0.430, 19232/60000 datapoints
2025-03-06 17:57:50,873 - INFO - training batch 651, loss: 0.242, 20832/60000 datapoints
2025-03-06 17:57:51,063 - INFO - training batch 701, loss: 0.342, 22432/60000 datapoints
2025-03-06 17:57:51,256 - INFO - training batch 751, loss: 0.490, 24032/60000 datapoints
2025-03-06 17:57:51,449 - INFO - training batch 801, loss: 0.481, 25632/60000 datapoints
2025-03-06 17:57:51,644 - INFO - training batch 851, loss: 0.372, 27232/60000 datapoints
2025-03-06 17:57:51,837 - INFO - training batch 901, loss: 0.818, 28832/60000 datapoints
2025-03-06 17:57:52,030 - INFO - training batch 951, loss: 0.486, 30432/60000 datapoints
2025-03-06 17:57:52,223 - INFO - training batch 1001, loss: 0.394, 32032/60000 datapoints
2025-03-06 17:57:52,414 - INFO - training batch 1051, loss: 0.340, 33632/60000 datapoints
2025-03-06 17:57:52,609 - INFO - training batch 1101, loss: 0.305, 35232/60000 datapoints
2025-03-06 17:57:52,800 - INFO - training batch 1151, loss: 0.744, 36832/60000 datapoints
2025-03-06 17:57:52,991 - INFO - training batch 1201, loss: 0.868, 38432/60000 datapoints
2025-03-06 17:57:53,181 - INFO - training batch 1251, loss: 0.657, 40032/60000 datapoints
2025-03-06 17:57:53,371 - INFO - training batch 1301, loss: 0.528, 41632/60000 datapoints
2025-03-06 17:57:53,564 - INFO - training batch 1351, loss: 0.402, 43232/60000 datapoints
2025-03-06 17:57:53,759 - INFO - training batch 1401, loss: 0.285, 44832/60000 datapoints
2025-03-06 17:57:53,950 - INFO - training batch 1451, loss: 0.690, 46432/60000 datapoints
2025-03-06 17:57:54,140 - INFO - training batch 1501, loss: 0.425, 48032/60000 datapoints
2025-03-06 17:57:54,340 - INFO - training batch 1551, loss: 0.323, 49632/60000 datapoints
2025-03-06 17:57:54,531 - INFO - training batch 1601, loss: 0.375, 51232/60000 datapoints
2025-03-06 17:57:54,725 - INFO - training batch 1651, loss: 0.413, 52832/60000 datapoints
2025-03-06 17:57:54,921 - INFO - training batch 1701, loss: 0.703, 54432/60000 datapoints
2025-03-06 17:57:55,113 - INFO - training batch 1751, loss: 0.550, 56032/60000 datapoints
2025-03-06 17:57:55,304 - INFO - training batch 1801, loss: 0.520, 57632/60000 datapoints
2025-03-06 17:57:55,496 - INFO - training batch 1851, loss: 0.606, 59232/60000 datapoints
2025-03-06 17:57:55,597 - INFO - validation batch 1, loss: 0.418, 32/10016 datapoints
2025-03-06 17:57:55,765 - INFO - validation batch 51, loss: 0.478, 1632/10016 datapoints
2025-03-06 17:57:55,929 - INFO - validation batch 101, loss: 0.395, 3232/10016 datapoints
2025-03-06 17:57:56,081 - INFO - validation batch 151, loss: 0.461, 4832/10016 datapoints
2025-03-06 17:57:56,236 - INFO - validation batch 201, loss: 0.414, 6432/10016 datapoints
2025-03-06 17:57:56,386 - INFO - validation batch 251, loss: 0.492, 8032/10016 datapoints
2025-03-06 17:57:56,535 - INFO - validation batch 301, loss: 0.555, 9632/10016 datapoints
2025-03-06 17:57:56,569 - INFO - Epoch 100/800 done.
2025-03-06 17:57:56,570 - INFO - Final validation performance:
Loss: 0.459, top-1 acc: 0.873top-5 acc: 0.873
2025-03-06 17:57:56,570 - INFO - Beginning epoch 101/800
2025-03-06 17:57:56,575 - INFO - training batch 1, loss: 0.525, 32/60000 datapoints
2025-03-06 17:57:56,773 - INFO - training batch 51, loss: 0.424, 1632/60000 datapoints
2025-03-06 17:57:56,963 - INFO - training batch 101, loss: 0.248, 3232/60000 datapoints
2025-03-06 17:57:57,153 - INFO - training batch 151, loss: 0.417, 4832/60000 datapoints
2025-03-06 17:57:57,343 - INFO - training batch 201, loss: 0.800, 6432/60000 datapoints
2025-03-06 17:57:57,532 - INFO - training batch 251, loss: 0.518, 8032/60000 datapoints
2025-03-06 17:57:57,727 - INFO - training batch 301, loss: 0.433, 9632/60000 datapoints
2025-03-06 17:57:57,919 - INFO - training batch 351, loss: 0.346, 11232/60000 datapoints
2025-03-06 17:57:58,108 - INFO - training batch 401, loss: 0.514, 12832/60000 datapoints
2025-03-06 17:57:58,303 - INFO - training batch 451, loss: 0.707, 14432/60000 datapoints
2025-03-06 17:57:58,495 - INFO - training batch 501, loss: 0.565, 16032/60000 datapoints
2025-03-06 17:57:58,691 - INFO - training batch 551, loss: 0.558, 17632/60000 datapoints
2025-03-06 17:57:58,890 - INFO - training batch 601, loss: 0.600, 19232/60000 datapoints
2025-03-06 17:57:59,081 - INFO - training batch 651, loss: 0.362, 20832/60000 datapoints
2025-03-06 17:57:59,270 - INFO - training batch 701, loss: 0.520, 22432/60000 datapoints
2025-03-06 17:57:59,462 - INFO - training batch 751, loss: 0.530, 24032/60000 datapoints
2025-03-06 17:57:59,655 - INFO - training batch 801, loss: 0.333, 25632/60000 datapoints
2025-03-06 17:57:59,845 - INFO - training batch 851, loss: 0.429, 27232/60000 datapoints
2025-03-06 17:58:00,039 - INFO - training batch 901, loss: 0.373, 28832/60000 datapoints
2025-03-06 17:58:00,231 - INFO - training batch 951, loss: 0.514, 30432/60000 datapoints
2025-03-06 17:58:00,429 - INFO - training batch 1001, loss: 0.766, 32032/60000 datapoints
2025-03-06 17:58:00,635 - INFO - training batch 1051, loss: 0.605, 33632/60000 datapoints
2025-03-06 17:58:00,828 - INFO - training batch 1101, loss: 0.404, 35232/60000 datapoints
2025-03-06 17:58:01,023 - INFO - training batch 1151, loss: 0.462, 36832/60000 datapoints
2025-03-06 17:58:01,217 - INFO - training batch 1201, loss: 0.430, 38432/60000 datapoints
2025-03-06 17:58:01,409 - INFO - training batch 1251, loss: 0.519, 40032/60000 datapoints
2025-03-06 17:58:01,609 - INFO - training batch 1301, loss: 0.576, 41632/60000 datapoints
2025-03-06 17:58:01,804 - INFO - training batch 1351, loss: 0.617, 43232/60000 datapoints
2025-03-06 17:58:01,999 - INFO - training batch 1401, loss: 0.546, 44832/60000 datapoints
2025-03-06 17:58:02,194 - INFO - training batch 1451, loss: 0.524, 46432/60000 datapoints
2025-03-06 17:58:02,392 - INFO - training batch 1501, loss: 0.677, 48032/60000 datapoints
2025-03-06 17:58:02,586 - INFO - training batch 1551, loss: 0.571, 49632/60000 datapoints
2025-03-06 17:58:02,783 - INFO - training batch 1601, loss: 0.658, 51232/60000 datapoints
2025-03-06 17:58:02,979 - INFO - training batch 1651, loss: 0.492, 52832/60000 datapoints
2025-03-06 17:58:03,172 - INFO - training batch 1701, loss: 0.736, 54432/60000 datapoints
2025-03-06 17:58:03,366 - INFO - training batch 1751, loss: 0.423, 56032/60000 datapoints
2025-03-06 17:58:03,558 - INFO - training batch 1801, loss: 0.453, 57632/60000 datapoints
2025-03-06 17:58:03,755 - INFO - training batch 1851, loss: 0.644, 59232/60000 datapoints
2025-03-06 17:58:03,858 - INFO - validation batch 1, loss: 0.691, 32/10016 datapoints
2025-03-06 17:58:04,011 - INFO - validation batch 51, loss: 0.444, 1632/10016 datapoints
2025-03-06 17:58:04,162 - INFO - validation batch 101, loss: 0.440, 3232/10016 datapoints
2025-03-06 17:58:04,316 - INFO - validation batch 151, loss: 0.492, 4832/10016 datapoints
2025-03-06 17:58:04,470 - INFO - validation batch 201, loss: 0.423, 6432/10016 datapoints
2025-03-06 17:58:04,625 - INFO - validation batch 251, loss: 0.430, 8032/10016 datapoints
2025-03-06 17:58:04,776 - INFO - validation batch 301, loss: 0.808, 9632/10016 datapoints
2025-03-06 17:58:04,815 - INFO - Epoch 101/800 done.
2025-03-06 17:58:04,815 - INFO - Final validation performance:
Loss: 0.533, top-1 acc: 0.874top-5 acc: 0.874
2025-03-06 17:58:04,815 - INFO - Beginning epoch 102/800
2025-03-06 17:58:04,823 - INFO - training batch 1, loss: 0.266, 32/60000 datapoints
2025-03-06 17:58:05,021 - INFO - training batch 51, loss: 0.563, 1632/60000 datapoints
2025-03-06 17:58:05,216 - INFO - training batch 101, loss: 0.402, 3232/60000 datapoints
2025-03-06 17:58:05,410 - INFO - training batch 151, loss: 0.583, 4832/60000 datapoints
2025-03-06 17:58:05,605 - INFO - training batch 201, loss: 0.563, 6432/60000 datapoints
2025-03-06 17:58:05,798 - INFO - training batch 251, loss: 0.459, 8032/60000 datapoints
2025-03-06 17:58:06,005 - INFO - training batch 301, loss: 0.433, 9632/60000 datapoints
2025-03-06 17:58:06,212 - INFO - training batch 351, loss: 0.449, 11232/60000 datapoints
2025-03-06 17:58:06,409 - INFO - training batch 401, loss: 0.491, 12832/60000 datapoints
2025-03-06 17:58:06,604 - INFO - training batch 451, loss: 0.419, 14432/60000 datapoints
2025-03-06 17:58:06,798 - INFO - training batch 501, loss: 0.541, 16032/60000 datapoints
2025-03-06 17:58:06,997 - INFO - training batch 551, loss: 0.438, 17632/60000 datapoints
2025-03-06 17:58:07,191 - INFO - training batch 601, loss: 0.314, 19232/60000 datapoints
2025-03-06 17:58:07,386 - INFO - training batch 651, loss: 0.628, 20832/60000 datapoints
2025-03-06 17:58:07,580 - INFO - training batch 701, loss: 0.301, 22432/60000 datapoints
2025-03-06 17:58:07,777 - INFO - training batch 751, loss: 0.520, 24032/60000 datapoints
2025-03-06 17:58:07,971 - INFO - training batch 801, loss: 0.520, 25632/60000 datapoints
2025-03-06 17:58:08,173 - INFO - training batch 851, loss: 0.519, 27232/60000 datapoints
2025-03-06 17:58:08,367 - INFO - training batch 901, loss: 0.729, 28832/60000 datapoints
2025-03-06 17:58:08,562 - INFO - training batch 951, loss: 0.688, 30432/60000 datapoints
2025-03-06 17:58:08,757 - INFO - training batch 1001, loss: 0.510, 32032/60000 datapoints
2025-03-06 17:58:08,953 - INFO - training batch 1051, loss: 0.462, 33632/60000 datapoints
2025-03-06 17:58:09,148 - INFO - training batch 1101, loss: 0.587, 35232/60000 datapoints
2025-03-06 17:58:09,341 - INFO - training batch 1151, loss: 0.673, 36832/60000 datapoints
2025-03-06 17:58:09,534 - INFO - training batch 1201, loss: 0.356, 38432/60000 datapoints
2025-03-06 17:58:09,730 - INFO - training batch 1251, loss: 0.589, 40032/60000 datapoints
2025-03-06 17:58:09,925 - INFO - training batch 1301, loss: 0.340, 41632/60000 datapoints
2025-03-06 17:58:10,118 - INFO - training batch 1351, loss: 0.293, 43232/60000 datapoints
2025-03-06 17:58:10,314 - INFO - training batch 1401, loss: 0.319, 44832/60000 datapoints
2025-03-06 17:58:10,508 - INFO - training batch 1451, loss: 0.757, 46432/60000 datapoints
2025-03-06 17:58:10,705 - INFO - training batch 1501, loss: 0.418, 48032/60000 datapoints
2025-03-06 17:58:10,899 - INFO - training batch 1551, loss: 0.375, 49632/60000 datapoints
2025-03-06 17:58:11,094 - INFO - training batch 1601, loss: 0.522, 51232/60000 datapoints
2025-03-06 17:58:11,290 - INFO - training batch 1651, loss: 0.409, 52832/60000 datapoints
2025-03-06 17:58:11,484 - INFO - training batch 1701, loss: 0.652, 54432/60000 datapoints
2025-03-06 17:58:11,680 - INFO - training batch 1751, loss: 0.379, 56032/60000 datapoints
2025-03-06 17:58:11,873 - INFO - training batch 1801, loss: 0.351, 57632/60000 datapoints
2025-03-06 17:58:12,067 - INFO - training batch 1851, loss: 0.621, 59232/60000 datapoints
2025-03-06 17:58:12,167 - INFO - validation batch 1, loss: 0.481, 32/10016 datapoints
2025-03-06 17:58:12,323 - INFO - validation batch 51, loss: 0.364, 1632/10016 datapoints
2025-03-06 17:58:12,475 - INFO - validation batch 101, loss: 0.271, 3232/10016 datapoints
2025-03-06 17:58:12,629 - INFO - validation batch 151, loss: 0.559, 4832/10016 datapoints
2025-03-06 17:58:12,781 - INFO - validation batch 201, loss: 0.466, 6432/10016 datapoints
2025-03-06 17:58:12,934 - INFO - validation batch 251, loss: 0.541, 8032/10016 datapoints
2025-03-06 17:58:13,087 - INFO - validation batch 301, loss: 0.374, 9632/10016 datapoints
2025-03-06 17:58:13,124 - INFO - Epoch 102/800 done.
2025-03-06 17:58:13,125 - INFO - Final validation performance:
Loss: 0.437, top-1 acc: 0.875top-5 acc: 0.875
2025-03-06 17:58:13,125 - INFO - Beginning epoch 103/800
2025-03-06 17:58:13,131 - INFO - training batch 1, loss: 0.343, 32/60000 datapoints
2025-03-06 17:58:13,328 - INFO - training batch 51, loss: 0.478, 1632/60000 datapoints
2025-03-06 17:58:13,524 - INFO - training batch 101, loss: 0.322, 3232/60000 datapoints
2025-03-06 17:58:13,723 - INFO - training batch 151, loss: 0.510, 4832/60000 datapoints
2025-03-06 17:58:13,916 - INFO - training batch 201, loss: 0.523, 6432/60000 datapoints
2025-03-06 17:58:14,113 - INFO - training batch 251, loss: 0.447, 8032/60000 datapoints
2025-03-06 17:58:14,310 - INFO - training batch 301, loss: 0.742, 9632/60000 datapoints
2025-03-06 17:58:14,503 - INFO - training batch 351, loss: 0.367, 11232/60000 datapoints
2025-03-06 17:58:14,701 - INFO - training batch 401, loss: 0.418, 12832/60000 datapoints
2025-03-06 17:58:14,896 - INFO - training batch 451, loss: 0.800, 14432/60000 datapoints
2025-03-06 17:58:15,093 - INFO - training batch 501, loss: 0.845, 16032/60000 datapoints
2025-03-06 17:58:15,285 - INFO - training batch 551, loss: 0.410, 17632/60000 datapoints
2025-03-06 17:58:15,478 - INFO - training batch 601, loss: 0.535, 19232/60000 datapoints
2025-03-06 17:58:15,678 - INFO - training batch 651, loss: 0.353, 20832/60000 datapoints
2025-03-06 17:58:15,871 - INFO - training batch 701, loss: 0.478, 22432/60000 datapoints
2025-03-06 17:58:16,072 - INFO - training batch 751, loss: 0.523, 24032/60000 datapoints
2025-03-06 17:58:16,278 - INFO - training batch 801, loss: 0.410, 25632/60000 datapoints
2025-03-06 17:58:16,472 - INFO - training batch 851, loss: 0.653, 27232/60000 datapoints
2025-03-06 17:58:16,668 - INFO - training batch 901, loss: 0.549, 28832/60000 datapoints
2025-03-06 17:58:16,867 - INFO - training batch 951, loss: 0.448, 30432/60000 datapoints
2025-03-06 17:58:17,060 - INFO - training batch 1001, loss: 0.621, 32032/60000 datapoints
2025-03-06 17:58:17,253 - INFO - training batch 1051, loss: 0.690, 33632/60000 datapoints
2025-03-06 17:58:17,447 - INFO - training batch 1101, loss: 0.522, 35232/60000 datapoints
2025-03-06 17:58:17,646 - INFO - training batch 1151, loss: 0.368, 36832/60000 datapoints
2025-03-06 17:58:17,840 - INFO - training batch 1201, loss: 0.729, 38432/60000 datapoints
2025-03-06 17:58:18,034 - INFO - training batch 1251, loss: 0.519, 40032/60000 datapoints
2025-03-06 17:58:18,228 - INFO - training batch 1301, loss: 0.195, 41632/60000 datapoints
2025-03-06 17:58:18,424 - INFO - training batch 1351, loss: 0.827, 43232/60000 datapoints
2025-03-06 17:58:18,620 - INFO - training batch 1401, loss: 0.591, 44832/60000 datapoints
2025-03-06 17:58:18,813 - INFO - training batch 1451, loss: 0.324, 46432/60000 datapoints
2025-03-06 17:58:19,007 - INFO - training batch 1501, loss: 0.401, 48032/60000 datapoints
2025-03-06 17:58:19,200 - INFO - training batch 1551, loss: 0.485, 49632/60000 datapoints
2025-03-06 17:58:19,392 - INFO - training batch 1601, loss: 0.458, 51232/60000 datapoints
2025-03-06 17:58:19,583 - INFO - training batch 1651, loss: 0.432, 52832/60000 datapoints
2025-03-06 17:58:19,779 - INFO - training batch 1701, loss: 0.631, 54432/60000 datapoints
2025-03-06 17:58:19,974 - INFO - training batch 1751, loss: 0.506, 56032/60000 datapoints
2025-03-06 17:58:20,174 - INFO - training batch 1801, loss: 0.407, 57632/60000 datapoints
2025-03-06 17:58:20,371 - INFO - training batch 1851, loss: 0.342, 59232/60000 datapoints
2025-03-06 17:58:20,471 - INFO - validation batch 1, loss: 0.366, 32/10016 datapoints
2025-03-06 17:58:20,624 - INFO - validation batch 51, loss: 0.463, 1632/10016 datapoints
2025-03-06 17:58:20,775 - INFO - validation batch 101, loss: 0.409, 3232/10016 datapoints
2025-03-06 17:58:20,927 - INFO - validation batch 151, loss: 0.693, 4832/10016 datapoints
2025-03-06 17:58:21,080 - INFO - validation batch 201, loss: 0.391, 6432/10016 datapoints
2025-03-06 17:58:21,259 - INFO - validation batch 251, loss: 0.396, 8032/10016 datapoints
2025-03-06 17:58:21,411 - INFO - validation batch 301, loss: 0.609, 9632/10016 datapoints
2025-03-06 17:58:21,446 - INFO - Epoch 103/800 done.
2025-03-06 17:58:21,447 - INFO - Final validation performance:
Loss: 0.475, top-1 acc: 0.875top-5 acc: 0.875
2025-03-06 17:58:21,447 - INFO - Beginning epoch 104/800
2025-03-06 17:58:21,454 - INFO - training batch 1, loss: 0.347, 32/60000 datapoints
2025-03-06 17:58:21,658 - INFO - training batch 51, loss: 0.456, 1632/60000 datapoints
2025-03-06 17:58:21,852 - INFO - training batch 101, loss: 0.443, 3232/60000 datapoints
2025-03-06 17:58:22,044 - INFO - training batch 151, loss: 0.294, 4832/60000 datapoints
2025-03-06 17:58:22,240 - INFO - training batch 201, loss: 0.489, 6432/60000 datapoints
2025-03-06 17:58:22,436 - INFO - training batch 251, loss: 0.414, 8032/60000 datapoints
2025-03-06 17:58:22,632 - INFO - training batch 301, loss: 0.473, 9632/60000 datapoints
2025-03-06 17:58:22,826 - INFO - training batch 351, loss: 0.434, 11232/60000 datapoints
2025-03-06 17:58:23,021 - INFO - training batch 401, loss: 0.339, 12832/60000 datapoints
2025-03-06 17:58:23,216 - INFO - training batch 451, loss: 0.553, 14432/60000 datapoints
2025-03-06 17:58:23,411 - INFO - training batch 501, loss: 0.372, 16032/60000 datapoints
2025-03-06 17:58:23,610 - INFO - training batch 551, loss: 0.618, 17632/60000 datapoints
2025-03-06 17:58:23,805 - INFO - training batch 601, loss: 0.582, 19232/60000 datapoints
2025-03-06 17:58:23,999 - INFO - training batch 651, loss: 0.501, 20832/60000 datapoints
2025-03-06 17:58:24,194 - INFO - training batch 701, loss: 0.431, 22432/60000 datapoints
2025-03-06 17:58:24,389 - INFO - training batch 751, loss: 0.497, 24032/60000 datapoints
2025-03-06 17:58:24,582 - INFO - training batch 801, loss: 0.382, 25632/60000 datapoints
2025-03-06 17:58:24,778 - INFO - training batch 851, loss: 0.418, 27232/60000 datapoints
2025-03-06 17:58:24,974 - INFO - training batch 901, loss: 0.702, 28832/60000 datapoints
2025-03-06 17:58:25,171 - INFO - training batch 951, loss: 0.582, 30432/60000 datapoints
2025-03-06 17:58:25,365 - INFO - training batch 1001, loss: 0.377, 32032/60000 datapoints
2025-03-06 17:58:25,559 - INFO - training batch 1051, loss: 0.444, 33632/60000 datapoints
2025-03-06 17:58:25,755 - INFO - training batch 1101, loss: 0.437, 35232/60000 datapoints
2025-03-06 17:58:25,948 - INFO - training batch 1151, loss: 0.466, 36832/60000 datapoints
2025-03-06 17:58:26,153 - INFO - training batch 1201, loss: 0.398, 38432/60000 datapoints
2025-03-06 17:58:26,365 - INFO - training batch 1251, loss: 0.385, 40032/60000 datapoints
2025-03-06 17:58:26,559 - INFO - training batch 1301, loss: 0.387, 41632/60000 datapoints
2025-03-06 17:58:26,754 - INFO - training batch 1351, loss: 0.528, 43232/60000 datapoints
2025-03-06 17:58:26,948 - INFO - training batch 1401, loss: 0.677, 44832/60000 datapoints
2025-03-06 17:58:27,142 - INFO - training batch 1451, loss: 0.729, 46432/60000 datapoints
2025-03-06 17:58:27,337 - INFO - training batch 1501, loss: 0.502, 48032/60000 datapoints
2025-03-06 17:58:27,530 - INFO - training batch 1551, loss: 0.362, 49632/60000 datapoints
2025-03-06 17:58:27,725 - INFO - training batch 1601, loss: 0.512, 51232/60000 datapoints
2025-03-06 17:58:27,917 - INFO - training batch 1651, loss: 0.668, 52832/60000 datapoints
2025-03-06 17:58:28,110 - INFO - training batch 1701, loss: 0.551, 54432/60000 datapoints
2025-03-06 17:58:28,304 - INFO - training batch 1751, loss: 0.465, 56032/60000 datapoints
2025-03-06 17:58:28,500 - INFO - training batch 1801, loss: 0.585, 57632/60000 datapoints
2025-03-06 17:58:28,698 - INFO - training batch 1851, loss: 0.605, 59232/60000 datapoints
2025-03-06 17:58:28,799 - INFO - validation batch 1, loss: 0.504, 32/10016 datapoints
2025-03-06 17:58:28,950 - INFO - validation batch 51, loss: 0.407, 1632/10016 datapoints
2025-03-06 17:58:29,103 - INFO - validation batch 101, loss: 0.518, 3232/10016 datapoints
2025-03-06 17:58:29,264 - INFO - validation batch 151, loss: 0.458, 4832/10016 datapoints
2025-03-06 17:58:29,415 - INFO - validation batch 201, loss: 0.598, 6432/10016 datapoints
2025-03-06 17:58:29,566 - INFO - validation batch 251, loss: 0.383, 8032/10016 datapoints
2025-03-06 17:58:29,720 - INFO - validation batch 301, loss: 0.412, 9632/10016 datapoints
2025-03-06 17:58:29,759 - INFO - Epoch 104/800 done.
2025-03-06 17:58:29,759 - INFO - Final validation performance:
Loss: 0.469, top-1 acc: 0.876top-5 acc: 0.876
2025-03-06 17:58:29,760 - INFO - Beginning epoch 105/800
2025-03-06 17:58:29,766 - INFO - training batch 1, loss: 0.455, 32/60000 datapoints
2025-03-06 17:58:29,957 - INFO - training batch 51, loss: 0.517, 1632/60000 datapoints
2025-03-06 17:58:30,150 - INFO - training batch 101, loss: 0.390, 3232/60000 datapoints
2025-03-06 17:58:30,344 - INFO - training batch 151, loss: 0.461, 4832/60000 datapoints
2025-03-06 17:58:30,538 - INFO - training batch 201, loss: 0.303, 6432/60000 datapoints
2025-03-06 17:58:30,731 - INFO - training batch 251, loss: 0.545, 8032/60000 datapoints
2025-03-06 17:58:30,927 - INFO - training batch 301, loss: 0.579, 9632/60000 datapoints
2025-03-06 17:58:31,117 - INFO - training batch 351, loss: 0.537, 11232/60000 datapoints
2025-03-06 17:58:31,310 - INFO - training batch 401, loss: 0.405, 12832/60000 datapoints
2025-03-06 17:58:31,501 - INFO - training batch 451, loss: 0.619, 14432/60000 datapoints
2025-03-06 17:58:31,695 - INFO - training batch 501, loss: 0.584, 16032/60000 datapoints
2025-03-06 17:58:31,886 - INFO - training batch 551, loss: 0.383, 17632/60000 datapoints
2025-03-06 17:58:32,077 - INFO - training batch 601, loss: 0.370, 19232/60000 datapoints
2025-03-06 17:58:32,268 - INFO - training batch 651, loss: 0.548, 20832/60000 datapoints
2025-03-06 17:58:32,460 - INFO - training batch 701, loss: 0.490, 22432/60000 datapoints
2025-03-06 17:58:32,653 - INFO - training batch 751, loss: 0.453, 24032/60000 datapoints
2025-03-06 17:58:32,846 - INFO - training batch 801, loss: 0.362, 25632/60000 datapoints
2025-03-06 17:58:33,037 - INFO - training batch 851, loss: 0.642, 27232/60000 datapoints
2025-03-06 17:58:33,229 - INFO - training batch 901, loss: 0.324, 28832/60000 datapoints
2025-03-06 17:58:33,420 - INFO - training batch 951, loss: 0.408, 30432/60000 datapoints
2025-03-06 17:58:33,613 - INFO - training batch 1001, loss: 0.521, 32032/60000 datapoints
2025-03-06 17:58:33,804 - INFO - training batch 1051, loss: 0.519, 33632/60000 datapoints
2025-03-06 17:58:33,996 - INFO - training batch 1101, loss: 0.546, 35232/60000 datapoints
2025-03-06 17:58:34,189 - INFO - training batch 1151, loss: 0.466, 36832/60000 datapoints
2025-03-06 17:58:34,384 - INFO - training batch 1201, loss: 0.827, 38432/60000 datapoints
2025-03-06 17:58:34,573 - INFO - training batch 1251, loss: 0.329, 40032/60000 datapoints
2025-03-06 17:58:34,766 - INFO - training batch 1301, loss: 0.501, 41632/60000 datapoints
2025-03-06 17:58:34,959 - INFO - training batch 1351, loss: 0.563, 43232/60000 datapoints
2025-03-06 17:58:35,150 - INFO - training batch 1401, loss: 0.583, 44832/60000 datapoints
2025-03-06 17:58:35,342 - INFO - training batch 1451, loss: 0.565, 46432/60000 datapoints
2025-03-06 17:58:35,533 - INFO - training batch 1501, loss: 0.549, 48032/60000 datapoints
2025-03-06 17:58:35,726 - INFO - training batch 1551, loss: 0.450, 49632/60000 datapoints
2025-03-06 17:58:35,917 - INFO - training batch 1601, loss: 0.417, 51232/60000 datapoints
2025-03-06 17:58:36,107 - INFO - training batch 1651, loss: 0.372, 52832/60000 datapoints
2025-03-06 17:58:36,321 - INFO - training batch 1701, loss: 0.374, 54432/60000 datapoints
2025-03-06 17:58:36,513 - INFO - training batch 1751, loss: 0.487, 56032/60000 datapoints
2025-03-06 17:58:36,704 - INFO - training batch 1801, loss: 0.471, 57632/60000 datapoints
2025-03-06 17:58:36,899 - INFO - training batch 1851, loss: 0.347, 59232/60000 datapoints
2025-03-06 17:58:36,997 - INFO - validation batch 1, loss: 0.398, 32/10016 datapoints
2025-03-06 17:58:37,146 - INFO - validation batch 51, loss: 0.590, 1632/10016 datapoints
2025-03-06 17:58:37,296 - INFO - validation batch 101, loss: 0.391, 3232/10016 datapoints
2025-03-06 17:58:37,445 - INFO - validation batch 151, loss: 0.419, 4832/10016 datapoints
2025-03-06 17:58:37,605 - INFO - validation batch 201, loss: 0.418, 6432/10016 datapoints
2025-03-06 17:58:37,755 - INFO - validation batch 251, loss: 0.640, 8032/10016 datapoints
2025-03-06 17:58:37,904 - INFO - validation batch 301, loss: 0.306, 9632/10016 datapoints
2025-03-06 17:58:37,939 - INFO - Epoch 105/800 done.
2025-03-06 17:58:37,939 - INFO - Final validation performance:
Loss: 0.452, top-1 acc: 0.876top-5 acc: 0.876
2025-03-06 17:58:37,939 - INFO - Beginning epoch 106/800
2025-03-06 17:58:37,945 - INFO - training batch 1, loss: 0.575, 32/60000 datapoints
2025-03-06 17:58:38,135 - INFO - training batch 51, loss: 0.867, 1632/60000 datapoints
2025-03-06 17:58:38,328 - INFO - training batch 101, loss: 0.863, 3232/60000 datapoints
2025-03-06 17:58:38,524 - INFO - training batch 151, loss: 0.681, 4832/60000 datapoints
2025-03-06 17:58:38,717 - INFO - training batch 201, loss: 0.686, 6432/60000 datapoints
2025-03-06 17:58:38,908 - INFO - training batch 251, loss: 0.500, 8032/60000 datapoints
2025-03-06 17:58:39,102 - INFO - training batch 301, loss: 0.496, 9632/60000 datapoints
2025-03-06 17:58:39,293 - INFO - training batch 351, loss: 0.435, 11232/60000 datapoints
2025-03-06 17:58:39,483 - INFO - training batch 401, loss: 0.450, 12832/60000 datapoints
2025-03-06 17:58:39,677 - INFO - training batch 451, loss: 0.429, 14432/60000 datapoints
2025-03-06 17:58:39,867 - INFO - training batch 501, loss: 0.563, 16032/60000 datapoints
2025-03-06 17:58:40,058 - INFO - training batch 551, loss: 0.406, 17632/60000 datapoints
2025-03-06 17:58:40,247 - INFO - training batch 601, loss: 0.621, 19232/60000 datapoints
2025-03-06 17:58:40,440 - INFO - training batch 651, loss: 0.712, 20832/60000 datapoints
2025-03-06 17:58:40,633 - INFO - training batch 701, loss: 0.489, 22432/60000 datapoints
2025-03-06 17:58:40,823 - INFO - training batch 751, loss: 0.622, 24032/60000 datapoints
2025-03-06 17:58:41,012 - INFO - training batch 801, loss: 0.569, 25632/60000 datapoints
2025-03-06 17:58:41,203 - INFO - training batch 851, loss: 0.636, 27232/60000 datapoints
2025-03-06 17:58:41,393 - INFO - training batch 901, loss: 0.682, 28832/60000 datapoints
2025-03-06 17:58:41,583 - INFO - training batch 951, loss: 0.622, 30432/60000 datapoints
2025-03-06 17:58:41,782 - INFO - training batch 1001, loss: 0.376, 32032/60000 datapoints
2025-03-06 17:58:41,977 - INFO - training batch 1051, loss: 0.612, 33632/60000 datapoints
2025-03-06 17:58:42,171 - INFO - training batch 1101, loss: 0.412, 35232/60000 datapoints
2025-03-06 17:58:42,367 - INFO - training batch 1151, loss: 0.394, 36832/60000 datapoints
2025-03-06 17:58:42,562 - INFO - training batch 1201, loss: 0.600, 38432/60000 datapoints
2025-03-06 17:58:42,755 - INFO - training batch 1251, loss: 0.673, 40032/60000 datapoints
2025-03-06 17:58:42,950 - INFO - training batch 1301, loss: 0.700, 41632/60000 datapoints
2025-03-06 17:58:43,144 - INFO - training batch 1351, loss: 0.278, 43232/60000 datapoints
2025-03-06 17:58:43,339 - INFO - training batch 1401, loss: 0.634, 44832/60000 datapoints
2025-03-06 17:58:43,530 - INFO - training batch 1451, loss: 0.652, 46432/60000 datapoints
2025-03-06 17:58:43,729 - INFO - training batch 1501, loss: 0.591, 48032/60000 datapoints
2025-03-06 17:58:43,923 - INFO - training batch 1551, loss: 0.520, 49632/60000 datapoints
2025-03-06 17:58:44,118 - INFO - training batch 1601, loss: 0.462, 51232/60000 datapoints
2025-03-06 17:58:44,313 - INFO - training batch 1651, loss: 0.384, 52832/60000 datapoints
2025-03-06 17:58:44,508 - INFO - training batch 1701, loss: 0.400, 54432/60000 datapoints
2025-03-06 17:58:44,703 - INFO - training batch 1751, loss: 0.477, 56032/60000 datapoints
2025-03-06 17:58:44,906 - INFO - training batch 1801, loss: 0.455, 57632/60000 datapoints
2025-03-06 17:58:45,101 - INFO - training batch 1851, loss: 0.403, 59232/60000 datapoints
2025-03-06 17:58:45,203 - INFO - validation batch 1, loss: 0.581, 32/10016 datapoints
2025-03-06 17:58:45,358 - INFO - validation batch 51, loss: 0.629, 1632/10016 datapoints
2025-03-06 17:58:45,510 - INFO - validation batch 101, loss: 0.537, 3232/10016 datapoints
2025-03-06 17:58:45,664 - INFO - validation batch 151, loss: 0.414, 4832/10016 datapoints
2025-03-06 17:58:45,816 - INFO - validation batch 201, loss: 0.752, 6432/10016 datapoints
2025-03-06 17:58:45,967 - INFO - validation batch 251, loss: 0.495, 8032/10016 datapoints
2025-03-06 17:58:46,120 - INFO - validation batch 301, loss: 0.413, 9632/10016 datapoints
2025-03-06 17:58:46,159 - INFO - Epoch 106/800 done.
2025-03-06 17:58:46,159 - INFO - Final validation performance:
Loss: 0.546, top-1 acc: 0.877top-5 acc: 0.877
2025-03-06 17:58:46,160 - INFO - Beginning epoch 107/800
2025-03-06 17:58:46,166 - INFO - training batch 1, loss: 0.795, 32/60000 datapoints
2025-03-06 17:58:46,380 - INFO - training batch 51, loss: 0.434, 1632/60000 datapoints
2025-03-06 17:58:46,573 - INFO - training batch 101, loss: 0.242, 3232/60000 datapoints
2025-03-06 17:58:46,766 - INFO - training batch 151, loss: 0.853, 4832/60000 datapoints
2025-03-06 17:58:46,961 - INFO - training batch 201, loss: 0.318, 6432/60000 datapoints
2025-03-06 17:58:47,154 - INFO - training batch 251, loss: 0.343, 8032/60000 datapoints
2025-03-06 17:58:47,345 - INFO - training batch 301, loss: 0.404, 9632/60000 datapoints
2025-03-06 17:58:47,536 - INFO - training batch 351, loss: 0.443, 11232/60000 datapoints
2025-03-06 17:58:47,728 - INFO - training batch 401, loss: 0.598, 12832/60000 datapoints
2025-03-06 17:58:47,918 - INFO - training batch 451, loss: 0.614, 14432/60000 datapoints
2025-03-06 17:58:48,108 - INFO - training batch 501, loss: 0.521, 16032/60000 datapoints
2025-03-06 17:58:48,297 - INFO - training batch 551, loss: 0.388, 17632/60000 datapoints
2025-03-06 17:58:48,491 - INFO - training batch 601, loss: 0.395, 19232/60000 datapoints
2025-03-06 17:58:48,684 - INFO - training batch 651, loss: 0.322, 20832/60000 datapoints
2025-03-06 17:58:48,875 - INFO - training batch 701, loss: 0.494, 22432/60000 datapoints
2025-03-06 17:58:49,067 - INFO - training batch 751, loss: 0.491, 24032/60000 datapoints
2025-03-06 17:58:49,256 - INFO - training batch 801, loss: 0.327, 25632/60000 datapoints
2025-03-06 17:58:49,447 - INFO - training batch 851, loss: 0.600, 27232/60000 datapoints
2025-03-06 17:58:49,640 - INFO - training batch 901, loss: 0.659, 28832/60000 datapoints
2025-03-06 17:58:49,829 - INFO - training batch 951, loss: 0.475, 30432/60000 datapoints
2025-03-06 17:58:50,021 - INFO - training batch 1001, loss: 0.304, 32032/60000 datapoints
2025-03-06 17:58:50,213 - INFO - training batch 1051, loss: 0.509, 33632/60000 datapoints
2025-03-06 17:58:50,403 - INFO - training batch 1101, loss: 0.637, 35232/60000 datapoints
2025-03-06 17:58:50,596 - INFO - training batch 1151, loss: 0.581, 36832/60000 datapoints
2025-03-06 17:58:50,786 - INFO - training batch 1201, loss: 0.375, 38432/60000 datapoints
2025-03-06 17:58:50,977 - INFO - training batch 1251, loss: 0.471, 40032/60000 datapoints
2025-03-06 17:58:51,167 - INFO - training batch 1301, loss: 0.400, 41632/60000 datapoints
2025-03-06 17:58:51,360 - INFO - training batch 1351, loss: 0.371, 43232/60000 datapoints
2025-03-06 17:58:51,551 - INFO - training batch 1401, loss: 0.508, 44832/60000 datapoints
2025-03-06 17:58:51,743 - INFO - training batch 1451, loss: 0.537, 46432/60000 datapoints
2025-03-06 17:58:51,934 - INFO - training batch 1501, loss: 0.635, 48032/60000 datapoints
2025-03-06 17:58:52,125 - INFO - training batch 1551, loss: 0.749, 49632/60000 datapoints
2025-03-06 17:58:52,315 - INFO - training batch 1601, loss: 0.501, 51232/60000 datapoints
2025-03-06 17:58:52,509 - INFO - training batch 1651, loss: 0.633, 52832/60000 datapoints
2025-03-06 17:58:52,703 - INFO - training batch 1701, loss: 0.410, 54432/60000 datapoints
2025-03-06 17:58:52,893 - INFO - training batch 1751, loss: 0.451, 56032/60000 datapoints
2025-03-06 17:58:53,085 - INFO - training batch 1801, loss: 0.252, 57632/60000 datapoints
2025-03-06 17:58:53,276 - INFO - training batch 1851, loss: 0.331, 59232/60000 datapoints
2025-03-06 17:58:53,375 - INFO - validation batch 1, loss: 0.275, 32/10016 datapoints
2025-03-06 17:58:53,524 - INFO - validation batch 51, loss: 0.465, 1632/10016 datapoints
2025-03-06 17:58:53,676 - INFO - validation batch 101, loss: 0.466, 3232/10016 datapoints
2025-03-06 17:58:53,825 - INFO - validation batch 151, loss: 0.429, 4832/10016 datapoints
2025-03-06 17:58:53,975 - INFO - validation batch 201, loss: 0.389, 6432/10016 datapoints
2025-03-06 17:58:54,125 - INFO - validation batch 251, loss: 0.369, 8032/10016 datapoints
2025-03-06 17:58:54,273 - INFO - validation batch 301, loss: 0.438, 9632/10016 datapoints
2025-03-06 17:58:54,308 - INFO - Epoch 107/800 done.
2025-03-06 17:58:54,309 - INFO - Final validation performance:
Loss: 0.404, top-1 acc: 0.878top-5 acc: 0.878
2025-03-06 17:58:54,309 - INFO - Beginning epoch 108/800
2025-03-06 17:58:54,314 - INFO - training batch 1, loss: 0.417, 32/60000 datapoints
2025-03-06 17:58:54,512 - INFO - training batch 51, loss: 0.383, 1632/60000 datapoints
2025-03-06 17:58:54,705 - INFO - training batch 101, loss: 0.505, 3232/60000 datapoints
2025-03-06 17:58:54,901 - INFO - training batch 151, loss: 0.388, 4832/60000 datapoints
2025-03-06 17:58:55,093 - INFO - training batch 201, loss: 0.590, 6432/60000 datapoints
2025-03-06 17:58:55,289 - INFO - training batch 251, loss: 0.555, 8032/60000 datapoints
2025-03-06 17:58:55,481 - INFO - training batch 301, loss: 0.561, 9632/60000 datapoints
2025-03-06 17:58:55,677 - INFO - training batch 351, loss: 0.493, 11232/60000 datapoints
2025-03-06 17:58:55,867 - INFO - training batch 401, loss: 0.726, 12832/60000 datapoints
2025-03-06 17:58:56,061 - INFO - training batch 451, loss: 0.419, 14432/60000 datapoints
2025-03-06 17:58:56,256 - INFO - training batch 501, loss: 0.587, 16032/60000 datapoints
2025-03-06 17:58:56,474 - INFO - training batch 551, loss: 0.579, 17632/60000 datapoints
2025-03-06 17:58:56,666 - INFO - training batch 601, loss: 0.454, 19232/60000 datapoints
2025-03-06 17:58:56,861 - INFO - training batch 651, loss: 0.358, 20832/60000 datapoints
2025-03-06 17:58:57,055 - INFO - training batch 701, loss: 0.571, 22432/60000 datapoints
2025-03-06 17:58:57,247 - INFO - training batch 751, loss: 0.670, 24032/60000 datapoints
2025-03-06 17:58:57,439 - INFO - training batch 801, loss: 0.235, 25632/60000 datapoints
2025-03-06 17:58:57,632 - INFO - training batch 851, loss: 0.516, 27232/60000 datapoints
2025-03-06 17:58:57,825 - INFO - training batch 901, loss: 0.366, 28832/60000 datapoints
2025-03-06 17:58:58,015 - INFO - training batch 951, loss: 0.451, 30432/60000 datapoints
2025-03-06 17:58:58,207 - INFO - training batch 1001, loss: 0.820, 32032/60000 datapoints
2025-03-06 17:58:58,399 - INFO - training batch 1051, loss: 0.548, 33632/60000 datapoints
2025-03-06 17:58:58,596 - INFO - training batch 1101, loss: 0.486, 35232/60000 datapoints
2025-03-06 17:58:58,788 - INFO - training batch 1151, loss: 0.582, 36832/60000 datapoints
2025-03-06 17:58:58,979 - INFO - training batch 1201, loss: 0.375, 38432/60000 datapoints
2025-03-06 17:58:59,179 - INFO - training batch 1251, loss: 0.511, 40032/60000 datapoints
2025-03-06 17:58:59,373 - INFO - training batch 1301, loss: 0.430, 41632/60000 datapoints
2025-03-06 17:58:59,565 - INFO - training batch 1351, loss: 0.351, 43232/60000 datapoints
2025-03-06 17:58:59,759 - INFO - training batch 1401, loss: 0.635, 44832/60000 datapoints
2025-03-06 17:58:59,951 - INFO - training batch 1451, loss: 0.636, 46432/60000 datapoints
2025-03-06 17:59:00,146 - INFO - training batch 1501, loss: 0.759, 48032/60000 datapoints
2025-03-06 17:59:00,339 - INFO - training batch 1551, loss: 0.233, 49632/60000 datapoints
2025-03-06 17:59:00,532 - INFO - training batch 1601, loss: 0.404, 51232/60000 datapoints
2025-03-06 17:59:00,723 - INFO - training batch 1651, loss: 0.754, 52832/60000 datapoints
2025-03-06 17:59:00,914 - INFO - training batch 1701, loss: 0.473, 54432/60000 datapoints
2025-03-06 17:59:01,105 - INFO - training batch 1751, loss: 0.673, 56032/60000 datapoints
2025-03-06 17:59:01,299 - INFO - training batch 1801, loss: 0.546, 57632/60000 datapoints
2025-03-06 17:59:01,492 - INFO - training batch 1851, loss: 0.509, 59232/60000 datapoints
2025-03-06 17:59:01,594 - INFO - validation batch 1, loss: 0.653, 32/10016 datapoints
2025-03-06 17:59:01,751 - INFO - validation batch 51, loss: 0.640, 1632/10016 datapoints
2025-03-06 17:59:01,901 - INFO - validation batch 101, loss: 0.488, 3232/10016 datapoints
2025-03-06 17:59:02,054 - INFO - validation batch 151, loss: 0.446, 4832/10016 datapoints
2025-03-06 17:59:02,207 - INFO - validation batch 201, loss: 0.461, 6432/10016 datapoints
2025-03-06 17:59:02,360 - INFO - validation batch 251, loss: 0.411, 8032/10016 datapoints
2025-03-06 17:59:02,517 - INFO - validation batch 301, loss: 0.656, 9632/10016 datapoints
2025-03-06 17:59:02,554 - INFO - Epoch 108/800 done.
2025-03-06 17:59:02,554 - INFO - Final validation performance:
Loss: 0.536, top-1 acc: 0.878top-5 acc: 0.878
2025-03-06 17:59:02,555 - INFO - Beginning epoch 109/800
2025-03-06 17:59:02,561 - INFO - training batch 1, loss: 0.472, 32/60000 datapoints
2025-03-06 17:59:02,758 - INFO - training batch 51, loss: 0.557, 1632/60000 datapoints
2025-03-06 17:59:02,953 - INFO - training batch 101, loss: 0.297, 3232/60000 datapoints
2025-03-06 17:59:03,148 - INFO - training batch 151, loss: 0.373, 4832/60000 datapoints
2025-03-06 17:59:03,343 - INFO - training batch 201, loss: 0.499, 6432/60000 datapoints
2025-03-06 17:59:03,538 - INFO - training batch 251, loss: 0.617, 8032/60000 datapoints
2025-03-06 17:59:03,737 - INFO - training batch 301, loss: 0.402, 9632/60000 datapoints
2025-03-06 17:59:03,928 - INFO - training batch 351, loss: 0.423, 11232/60000 datapoints
2025-03-06 17:59:04,125 - INFO - training batch 401, loss: 0.464, 12832/60000 datapoints
2025-03-06 17:59:04,323 - INFO - training batch 451, loss: 0.447, 14432/60000 datapoints
2025-03-06 17:59:04,527 - INFO - training batch 501, loss: 0.471, 16032/60000 datapoints
2025-03-06 17:59:04,723 - INFO - training batch 551, loss: 0.455, 17632/60000 datapoints
2025-03-06 17:59:04,933 - INFO - training batch 601, loss: 0.432, 19232/60000 datapoints
2025-03-06 17:59:05,128 - INFO - training batch 651, loss: 0.596, 20832/60000 datapoints
2025-03-06 17:59:05,325 - INFO - training batch 701, loss: 0.659, 22432/60000 datapoints
2025-03-06 17:59:05,528 - INFO - training batch 751, loss: 0.454, 24032/60000 datapoints
2025-03-06 17:59:05,773 - INFO - training batch 801, loss: 0.676, 25632/60000 datapoints
2025-03-06 17:59:05,967 - INFO - training batch 851, loss: 0.446, 27232/60000 datapoints
2025-03-06 17:59:06,163 - INFO - training batch 901, loss: 0.571, 28832/60000 datapoints
2025-03-06 17:59:06,358 - INFO - training batch 951, loss: 0.542, 30432/60000 datapoints
2025-03-06 17:59:06,571 - INFO - training batch 1001, loss: 0.539, 32032/60000 datapoints
2025-03-06 17:59:06,765 - INFO - training batch 1051, loss: 0.710, 33632/60000 datapoints
2025-03-06 17:59:06,964 - INFO - training batch 1101, loss: 0.471, 35232/60000 datapoints
2025-03-06 17:59:07,161 - INFO - training batch 1151, loss: 0.688, 36832/60000 datapoints
2025-03-06 17:59:07,355 - INFO - training batch 1201, loss: 0.468, 38432/60000 datapoints
2025-03-06 17:59:07,551 - INFO - training batch 1251, loss: 0.462, 40032/60000 datapoints
2025-03-06 17:59:07,753 - INFO - training batch 1301, loss: 0.401, 41632/60000 datapoints
2025-03-06 17:59:07,947 - INFO - training batch 1351, loss: 0.364, 43232/60000 datapoints
2025-03-06 17:59:08,147 - INFO - training batch 1401, loss: 0.695, 44832/60000 datapoints
2025-03-06 17:59:08,343 - INFO - training batch 1451, loss: 0.519, 46432/60000 datapoints
2025-03-06 17:59:08,538 - INFO - training batch 1501, loss: 0.392, 48032/60000 datapoints
2025-03-06 17:59:08,735 - INFO - training batch 1551, loss: 0.345, 49632/60000 datapoints
2025-03-06 17:59:08,927 - INFO - training batch 1601, loss: 0.576, 51232/60000 datapoints
2025-03-06 17:59:09,128 - INFO - training batch 1651, loss: 0.267, 52832/60000 datapoints
2025-03-06 17:59:09,324 - INFO - training batch 1701, loss: 0.281, 54432/60000 datapoints
2025-03-06 17:59:09,516 - INFO - training batch 1751, loss: 0.447, 56032/60000 datapoints
2025-03-06 17:59:09,714 - INFO - training batch 1801, loss: 0.320, 57632/60000 datapoints
2025-03-06 17:59:09,908 - INFO - training batch 1851, loss: 0.528, 59232/60000 datapoints
2025-03-06 17:59:10,009 - INFO - validation batch 1, loss: 0.339, 32/10016 datapoints
2025-03-06 17:59:10,161 - INFO - validation batch 51, loss: 0.360, 1632/10016 datapoints
2025-03-06 17:59:10,314 - INFO - validation batch 101, loss: 0.469, 3232/10016 datapoints
2025-03-06 17:59:10,466 - INFO - validation batch 151, loss: 0.368, 4832/10016 datapoints
2025-03-06 17:59:10,623 - INFO - validation batch 201, loss: 0.759, 6432/10016 datapoints
2025-03-06 17:59:10,776 - INFO - validation batch 251, loss: 0.440, 8032/10016 datapoints
2025-03-06 17:59:10,929 - INFO - validation batch 301, loss: 0.543, 9632/10016 datapoints
2025-03-06 17:59:10,966 - INFO - Epoch 109/800 done.
2025-03-06 17:59:10,966 - INFO - Final validation performance:
Loss: 0.468, top-1 acc: 0.879top-5 acc: 0.879
2025-03-06 17:59:10,967 - INFO - Beginning epoch 110/800
2025-03-06 17:59:10,972 - INFO - training batch 1, loss: 0.853, 32/60000 datapoints
2025-03-06 17:59:11,168 - INFO - training batch 51, loss: 0.490, 1632/60000 datapoints
2025-03-06 17:59:11,363 - INFO - training batch 101, loss: 0.536, 3232/60000 datapoints
2025-03-06 17:59:11,560 - INFO - training batch 151, loss: 0.563, 4832/60000 datapoints
2025-03-06 17:59:11,759 - INFO - training batch 201, loss: 0.415, 6432/60000 datapoints
2025-03-06 17:59:11,952 - INFO - training batch 251, loss: 0.408, 8032/60000 datapoints
2025-03-06 17:59:12,143 - INFO - training batch 301, loss: 0.497, 9632/60000 datapoints
2025-03-06 17:59:12,336 - INFO - training batch 351, loss: 0.537, 11232/60000 datapoints
2025-03-06 17:59:12,533 - INFO - training batch 401, loss: 0.525, 12832/60000 datapoints
2025-03-06 17:59:12,729 - INFO - training batch 451, loss: 0.338, 14432/60000 datapoints
2025-03-06 17:59:12,922 - INFO - training batch 501, loss: 0.333, 16032/60000 datapoints
2025-03-06 17:59:13,118 - INFO - training batch 551, loss: 0.865, 17632/60000 datapoints
2025-03-06 17:59:13,314 - INFO - training batch 601, loss: 0.735, 19232/60000 datapoints
2025-03-06 17:59:13,507 - INFO - training batch 651, loss: 0.513, 20832/60000 datapoints
2025-03-06 17:59:13,705 - INFO - training batch 701, loss: 0.444, 22432/60000 datapoints
2025-03-06 17:59:13,901 - INFO - training batch 751, loss: 0.398, 24032/60000 datapoints
2025-03-06 17:59:14,095 - INFO - training batch 801, loss: 0.380, 25632/60000 datapoints
2025-03-06 17:59:14,291 - INFO - training batch 851, loss: 0.436, 27232/60000 datapoints
2025-03-06 17:59:14,485 - INFO - training batch 901, loss: 0.275, 28832/60000 datapoints
2025-03-06 17:59:14,685 - INFO - training batch 951, loss: 0.705, 30432/60000 datapoints
2025-03-06 17:59:14,883 - INFO - training batch 1001, loss: 0.498, 32032/60000 datapoints
2025-03-06 17:59:15,076 - INFO - training batch 1051, loss: 0.488, 33632/60000 datapoints
2025-03-06 17:59:15,267 - INFO - training batch 1101, loss: 0.394, 35232/60000 datapoints
2025-03-06 17:59:15,463 - INFO - training batch 1151, loss: 0.417, 36832/60000 datapoints
2025-03-06 17:59:15,680 - INFO - training batch 1201, loss: 0.480, 38432/60000 datapoints
2025-03-06 17:59:15,878 - INFO - training batch 1251, loss: 0.348, 40032/60000 datapoints
2025-03-06 17:59:16,074 - INFO - training batch 1301, loss: 0.586, 41632/60000 datapoints
2025-03-06 17:59:16,271 - INFO - training batch 1351, loss: 0.361, 43232/60000 datapoints
2025-03-06 17:59:16,466 - INFO - training batch 1401, loss: 0.450, 44832/60000 datapoints
2025-03-06 17:59:16,681 - INFO - training batch 1451, loss: 0.522, 46432/60000 datapoints
2025-03-06 17:59:16,881 - INFO - training batch 1501, loss: 0.375, 48032/60000 datapoints
2025-03-06 17:59:17,074 - INFO - training batch 1551, loss: 0.541, 49632/60000 datapoints
2025-03-06 17:59:17,269 - INFO - training batch 1601, loss: 0.297, 51232/60000 datapoints
2025-03-06 17:59:17,464 - INFO - training batch 1651, loss: 0.570, 52832/60000 datapoints
2025-03-06 17:59:17,659 - INFO - training batch 1701, loss: 0.304, 54432/60000 datapoints
2025-03-06 17:59:17,853 - INFO - training batch 1751, loss: 0.495, 56032/60000 datapoints
2025-03-06 17:59:18,045 - INFO - training batch 1801, loss: 0.693, 57632/60000 datapoints
2025-03-06 17:59:18,239 - INFO - training batch 1851, loss: 0.863, 59232/60000 datapoints
2025-03-06 17:59:18,342 - INFO - validation batch 1, loss: 0.475, 32/10016 datapoints
2025-03-06 17:59:18,495 - INFO - validation batch 51, loss: 0.261, 1632/10016 datapoints
2025-03-06 17:59:18,652 - INFO - validation batch 101, loss: 0.380, 3232/10016 datapoints
2025-03-06 17:59:18,805 - INFO - validation batch 151, loss: 0.368, 4832/10016 datapoints
2025-03-06 17:59:18,958 - INFO - validation batch 201, loss: 0.491, 6432/10016 datapoints
2025-03-06 17:59:19,112 - INFO - validation batch 251, loss: 0.498, 8032/10016 datapoints
2025-03-06 17:59:19,264 - INFO - validation batch 301, loss: 0.372, 9632/10016 datapoints
2025-03-06 17:59:19,300 - INFO - Epoch 110/800 done.
2025-03-06 17:59:19,301 - INFO - Final validation performance:
Loss: 0.406, top-1 acc: 0.879top-5 acc: 0.879
2025-03-06 17:59:19,301 - INFO - Beginning epoch 111/800
2025-03-06 17:59:19,307 - INFO - training batch 1, loss: 0.667, 32/60000 datapoints
2025-03-06 17:59:19,499 - INFO - training batch 51, loss: 0.281, 1632/60000 datapoints
2025-03-06 17:59:19,694 - INFO - training batch 101, loss: 0.589, 3232/60000 datapoints
2025-03-06 17:59:19,889 - INFO - training batch 151, loss: 0.585, 4832/60000 datapoints
2025-03-06 17:59:20,081 - INFO - training batch 201, loss: 0.641, 6432/60000 datapoints
2025-03-06 17:59:20,275 - INFO - training batch 251, loss: 0.532, 8032/60000 datapoints
2025-03-06 17:59:20,475 - INFO - training batch 301, loss: 0.454, 9632/60000 datapoints
2025-03-06 17:59:20,672 - INFO - training batch 351, loss: 0.496, 11232/60000 datapoints
2025-03-06 17:59:20,864 - INFO - training batch 401, loss: 0.495, 12832/60000 datapoints
2025-03-06 17:59:21,055 - INFO - training batch 451, loss: 0.545, 14432/60000 datapoints
2025-03-06 17:59:21,246 - INFO - training batch 501, loss: 0.521, 16032/60000 datapoints
2025-03-06 17:59:21,443 - INFO - training batch 551, loss: 0.409, 17632/60000 datapoints
2025-03-06 17:59:21,661 - INFO - training batch 601, loss: 0.537, 19232/60000 datapoints
2025-03-06 17:59:21,852 - INFO - training batch 651, loss: 0.531, 20832/60000 datapoints
2025-03-06 17:59:22,049 - INFO - training batch 701, loss: 0.682, 22432/60000 datapoints
2025-03-06 17:59:22,241 - INFO - training batch 751, loss: 0.433, 24032/60000 datapoints
2025-03-06 17:59:22,436 - INFO - training batch 801, loss: 0.333, 25632/60000 datapoints
2025-03-06 17:59:22,634 - INFO - training batch 851, loss: 0.379, 27232/60000 datapoints
2025-03-06 17:59:22,828 - INFO - training batch 901, loss: 0.450, 28832/60000 datapoints
2025-03-06 17:59:23,022 - INFO - training batch 951, loss: 0.605, 30432/60000 datapoints
2025-03-06 17:59:23,215 - INFO - training batch 1001, loss: 0.654, 32032/60000 datapoints
2025-03-06 17:59:23,409 - INFO - training batch 1051, loss: 0.848, 33632/60000 datapoints
2025-03-06 17:59:23,605 - INFO - training batch 1101, loss: 0.513, 35232/60000 datapoints
2025-03-06 17:59:23,800 - INFO - training batch 1151, loss: 0.802, 36832/60000 datapoints
2025-03-06 17:59:23,992 - INFO - training batch 1201, loss: 0.462, 38432/60000 datapoints
2025-03-06 17:59:24,188 - INFO - training batch 1251, loss: 0.575, 40032/60000 datapoints
2025-03-06 17:59:24,382 - INFO - training batch 1301, loss: 0.694, 41632/60000 datapoints
2025-03-06 17:59:24,579 - INFO - training batch 1351, loss: 0.313, 43232/60000 datapoints
2025-03-06 17:59:24,774 - INFO - training batch 1401, loss: 0.381, 44832/60000 datapoints
2025-03-06 17:59:24,977 - INFO - training batch 1451, loss: 0.555, 46432/60000 datapoints
2025-03-06 17:59:25,171 - INFO - training batch 1501, loss: 0.481, 48032/60000 datapoints
2025-03-06 17:59:25,366 - INFO - training batch 1551, loss: 0.438, 49632/60000 datapoints
2025-03-06 17:59:25,562 - INFO - training batch 1601, loss: 0.325, 51232/60000 datapoints
2025-03-06 17:59:25,760 - INFO - training batch 1651, loss: 0.474, 52832/60000 datapoints
2025-03-06 17:59:25,972 - INFO - training batch 1701, loss: 0.481, 54432/60000 datapoints
2025-03-06 17:59:26,168 - INFO - training batch 1751, loss: 0.638, 56032/60000 datapoints
2025-03-06 17:59:26,364 - INFO - training batch 1801, loss: 0.336, 57632/60000 datapoints
2025-03-06 17:59:26,559 - INFO - training batch 1851, loss: 0.596, 59232/60000 datapoints
2025-03-06 17:59:26,677 - INFO - validation batch 1, loss: 0.296, 32/10016 datapoints
2025-03-06 17:59:26,837 - INFO - validation batch 51, loss: 0.593, 1632/10016 datapoints
2025-03-06 17:59:26,996 - INFO - validation batch 101, loss: 0.350, 3232/10016 datapoints
2025-03-06 17:59:27,150 - INFO - validation batch 151, loss: 0.443, 4832/10016 datapoints
2025-03-06 17:59:27,303 - INFO - validation batch 201, loss: 0.380, 6432/10016 datapoints
2025-03-06 17:59:27,457 - INFO - validation batch 251, loss: 0.569, 8032/10016 datapoints
2025-03-06 17:59:27,610 - INFO - validation batch 301, loss: 0.295, 9632/10016 datapoints
2025-03-06 17:59:27,646 - INFO - Epoch 111/800 done.
2025-03-06 17:59:27,646 - INFO - Final validation performance:
Loss: 0.418, top-1 acc: 0.880top-5 acc: 0.880
2025-03-06 17:59:27,646 - INFO - Beginning epoch 112/800
2025-03-06 17:59:27,653 - INFO - training batch 1, loss: 0.568, 32/60000 datapoints
2025-03-06 17:59:27,853 - INFO - training batch 51, loss: 0.430, 1632/60000 datapoints
2025-03-06 17:59:28,045 - INFO - training batch 101, loss: 0.547, 3232/60000 datapoints
2025-03-06 17:59:28,240 - INFO - training batch 151, loss: 0.404, 4832/60000 datapoints
2025-03-06 17:59:28,435 - INFO - training batch 201, loss: 0.729, 6432/60000 datapoints
2025-03-06 17:59:28,636 - INFO - training batch 251, loss: 0.679, 8032/60000 datapoints
2025-03-06 17:59:28,830 - INFO - training batch 301, loss: 0.321, 9632/60000 datapoints
2025-03-06 17:59:29,023 - INFO - training batch 351, loss: 0.588, 11232/60000 datapoints
2025-03-06 17:59:29,216 - INFO - training batch 401, loss: 0.310, 12832/60000 datapoints
2025-03-06 17:59:29,409 - INFO - training batch 451, loss: 0.486, 14432/60000 datapoints
2025-03-06 17:59:29,614 - INFO - training batch 501, loss: 0.403, 16032/60000 datapoints
2025-03-06 17:59:29,808 - INFO - training batch 551, loss: 0.580, 17632/60000 datapoints
2025-03-06 17:59:30,003 - INFO - training batch 601, loss: 0.476, 19232/60000 datapoints
2025-03-06 17:59:30,198 - INFO - training batch 651, loss: 0.588, 20832/60000 datapoints
2025-03-06 17:59:30,394 - INFO - training batch 701, loss: 0.495, 22432/60000 datapoints
2025-03-06 17:59:30,590 - INFO - training batch 751, loss: 0.284, 24032/60000 datapoints
2025-03-06 17:59:30,788 - INFO - training batch 801, loss: 0.646, 25632/60000 datapoints
2025-03-06 17:59:30,982 - INFO - training batch 851, loss: 0.466, 27232/60000 datapoints
2025-03-06 17:59:31,175 - INFO - training batch 901, loss: 0.704, 28832/60000 datapoints
2025-03-06 17:59:31,369 - INFO - training batch 951, loss: 0.488, 30432/60000 datapoints
2025-03-06 17:59:31,563 - INFO - training batch 1001, loss: 0.449, 32032/60000 datapoints
2025-03-06 17:59:31,763 - INFO - training batch 1051, loss: 0.498, 33632/60000 datapoints
2025-03-06 17:59:31,958 - INFO - training batch 1101, loss: 0.322, 35232/60000 datapoints
2025-03-06 17:59:32,151 - INFO - training batch 1151, loss: 0.499, 36832/60000 datapoints
2025-03-06 17:59:32,344 - INFO - training batch 1201, loss: 0.537, 38432/60000 datapoints
2025-03-06 17:59:32,538 - INFO - training batch 1251, loss: 0.446, 40032/60000 datapoints
2025-03-06 17:59:32,736 - INFO - training batch 1301, loss: 0.269, 41632/60000 datapoints
2025-03-06 17:59:32,931 - INFO - training batch 1351, loss: 0.407, 43232/60000 datapoints
2025-03-06 17:59:33,125 - INFO - training batch 1401, loss: 0.618, 44832/60000 datapoints
2025-03-06 17:59:33,318 - INFO - training batch 1451, loss: 0.729, 46432/60000 datapoints
2025-03-06 17:59:33,511 - INFO - training batch 1501, loss: 0.473, 48032/60000 datapoints
2025-03-06 17:59:33,708 - INFO - training batch 1551, loss: 0.486, 49632/60000 datapoints
2025-03-06 17:59:33,904 - INFO - training batch 1601, loss: 0.379, 51232/60000 datapoints
2025-03-06 17:59:34,098 - INFO - training batch 1651, loss: 0.297, 52832/60000 datapoints
2025-03-06 17:59:34,291 - INFO - training batch 1701, loss: 0.513, 54432/60000 datapoints
2025-03-06 17:59:34,484 - INFO - training batch 1751, loss: 0.539, 56032/60000 datapoints
2025-03-06 17:59:34,682 - INFO - training batch 1801, loss: 0.312, 57632/60000 datapoints
2025-03-06 17:59:34,881 - INFO - training batch 1851, loss: 0.587, 59232/60000 datapoints
2025-03-06 17:59:34,983 - INFO - validation batch 1, loss: 0.468, 32/10016 datapoints
2025-03-06 17:59:35,137 - INFO - validation batch 51, loss: 0.788, 1632/10016 datapoints
2025-03-06 17:59:35,293 - INFO - validation batch 101, loss: 0.335, 3232/10016 datapoints
2025-03-06 17:59:35,446 - INFO - validation batch 151, loss: 0.565, 4832/10016 datapoints
2025-03-06 17:59:35,601 - INFO - validation batch 201, loss: 0.328, 6432/10016 datapoints
2025-03-06 17:59:35,754 - INFO - validation batch 251, loss: 0.429, 8032/10016 datapoints
2025-03-06 17:59:35,905 - INFO - validation batch 301, loss: 0.338, 9632/10016 datapoints
2025-03-06 17:59:35,942 - INFO - Epoch 112/800 done.
2025-03-06 17:59:35,942 - INFO - Final validation performance:
Loss: 0.465, top-1 acc: 0.881top-5 acc: 0.881
2025-03-06 17:59:35,942 - INFO - Beginning epoch 113/800
2025-03-06 17:59:35,951 - INFO - training batch 1, loss: 0.388, 32/60000 datapoints
2025-03-06 17:59:36,146 - INFO - training batch 51, loss: 0.524, 1632/60000 datapoints
2025-03-06 17:59:36,342 - INFO - training batch 101, loss: 0.643, 3232/60000 datapoints
2025-03-06 17:59:36,538 - INFO - training batch 151, loss: 0.336, 4832/60000 datapoints
2025-03-06 17:59:36,747 - INFO - training batch 201, loss: 0.663, 6432/60000 datapoints
2025-03-06 17:59:36,956 - INFO - training batch 251, loss: 0.358, 8032/60000 datapoints
2025-03-06 17:59:37,149 - INFO - training batch 301, loss: 0.546, 9632/60000 datapoints
2025-03-06 17:59:37,342 - INFO - training batch 351, loss: 0.390, 11232/60000 datapoints
2025-03-06 17:59:37,534 - INFO - training batch 401, loss: 0.507, 12832/60000 datapoints
2025-03-06 17:59:37,736 - INFO - training batch 451, loss: 0.481, 14432/60000 datapoints
2025-03-06 17:59:37,928 - INFO - training batch 501, loss: 0.695, 16032/60000 datapoints
2025-03-06 17:59:38,122 - INFO - training batch 551, loss: 0.421, 17632/60000 datapoints
2025-03-06 17:59:38,316 - INFO - training batch 601, loss: 0.460, 19232/60000 datapoints
2025-03-06 17:59:38,509 - INFO - training batch 651, loss: 0.446, 20832/60000 datapoints
2025-03-06 17:59:38,706 - INFO - training batch 701, loss: 0.470, 22432/60000 datapoints
2025-03-06 17:59:38,902 - INFO - training batch 751, loss: 0.375, 24032/60000 datapoints
2025-03-06 17:59:39,097 - INFO - training batch 801, loss: 0.456, 25632/60000 datapoints
2025-03-06 17:59:39,291 - INFO - training batch 851, loss: 0.330, 27232/60000 datapoints
2025-03-06 17:59:39,484 - INFO - training batch 901, loss: 0.370, 28832/60000 datapoints
2025-03-06 17:59:39,681 - INFO - training batch 951, loss: 0.474, 30432/60000 datapoints
2025-03-06 17:59:39,875 - INFO - training batch 1001, loss: 0.301, 32032/60000 datapoints
2025-03-06 17:59:40,069 - INFO - training batch 1051, loss: 0.637, 33632/60000 datapoints
2025-03-06 17:59:40,268 - INFO - training batch 1101, loss: 0.289, 35232/60000 datapoints
2025-03-06 17:59:40,465 - INFO - training batch 1151, loss: 0.530, 36832/60000 datapoints
2025-03-06 17:59:40,668 - INFO - training batch 1201, loss: 0.362, 38432/60000 datapoints
2025-03-06 17:59:40,861 - INFO - training batch 1251, loss: 0.380, 40032/60000 datapoints
2025-03-06 17:59:41,055 - INFO - training batch 1301, loss: 0.353, 41632/60000 datapoints
2025-03-06 17:59:41,250 - INFO - training batch 1351, loss: 0.773, 43232/60000 datapoints
2025-03-06 17:59:41,445 - INFO - training batch 1401, loss: 0.675, 44832/60000 datapoints
2025-03-06 17:59:41,640 - INFO - training batch 1451, loss: 0.506, 46432/60000 datapoints
2025-03-06 17:59:41,832 - INFO - training batch 1501, loss: 0.691, 48032/60000 datapoints
2025-03-06 17:59:42,027 - INFO - training batch 1551, loss: 0.402, 49632/60000 datapoints
2025-03-06 17:59:42,221 - INFO - training batch 1601, loss: 0.362, 51232/60000 datapoints
2025-03-06 17:59:42,414 - INFO - training batch 1651, loss: 0.404, 52832/60000 datapoints
2025-03-06 17:59:42,612 - INFO - training batch 1701, loss: 0.587, 54432/60000 datapoints
2025-03-06 17:59:42,805 - INFO - training batch 1751, loss: 0.365, 56032/60000 datapoints
2025-03-06 17:59:42,999 - INFO - training batch 1801, loss: 0.694, 57632/60000 datapoints
2025-03-06 17:59:43,193 - INFO - training batch 1851, loss: 0.448, 59232/60000 datapoints
2025-03-06 17:59:43,294 - INFO - validation batch 1, loss: 0.329, 32/10016 datapoints
2025-03-06 17:59:43,445 - INFO - validation batch 51, loss: 0.492, 1632/10016 datapoints
2025-03-06 17:59:43,602 - INFO - validation batch 101, loss: 0.319, 3232/10016 datapoints
2025-03-06 17:59:43,753 - INFO - validation batch 151, loss: 0.566, 4832/10016 datapoints
2025-03-06 17:59:43,908 - INFO - validation batch 201, loss: 0.501, 6432/10016 datapoints
2025-03-06 17:59:44,060 - INFO - validation batch 251, loss: 0.364, 8032/10016 datapoints
2025-03-06 17:59:44,212 - INFO - validation batch 301, loss: 0.454, 9632/10016 datapoints
2025-03-06 17:59:44,247 - INFO - Epoch 113/800 done.
2025-03-06 17:59:44,248 - INFO - Final validation performance:
Loss: 0.432, top-1 acc: 0.881top-5 acc: 0.881
2025-03-06 17:59:44,248 - INFO - Beginning epoch 114/800
2025-03-06 17:59:44,254 - INFO - training batch 1, loss: 0.503, 32/60000 datapoints
2025-03-06 17:59:44,453 - INFO - training batch 51, loss: 0.379, 1632/60000 datapoints
2025-03-06 17:59:44,655 - INFO - training batch 101, loss: 0.499, 3232/60000 datapoints
2025-03-06 17:59:44,849 - INFO - training batch 151, loss: 0.364, 4832/60000 datapoints
2025-03-06 17:59:45,049 - INFO - training batch 201, loss: 0.393, 6432/60000 datapoints
2025-03-06 17:59:45,245 - INFO - training batch 251, loss: 0.419, 8032/60000 datapoints
2025-03-06 17:59:45,438 - INFO - training batch 301, loss: 0.487, 9632/60000 datapoints
2025-03-06 17:59:45,643 - INFO - training batch 351, loss: 0.371, 11232/60000 datapoints
2025-03-06 17:59:45,837 - INFO - training batch 401, loss: 0.480, 12832/60000 datapoints
2025-03-06 17:59:46,028 - INFO - training batch 451, loss: 0.760, 14432/60000 datapoints
2025-03-06 17:59:46,224 - INFO - training batch 501, loss: 0.430, 16032/60000 datapoints
2025-03-06 17:59:46,420 - INFO - training batch 551, loss: 0.265, 17632/60000 datapoints
2025-03-06 17:59:46,619 - INFO - training batch 601, loss: 0.289, 19232/60000 datapoints
2025-03-06 17:59:46,813 - INFO - training batch 651, loss: 0.272, 20832/60000 datapoints
2025-03-06 17:59:47,026 - INFO - training batch 701, loss: 0.365, 22432/60000 datapoints
2025-03-06 17:59:47,221 - INFO - training batch 751, loss: 0.417, 24032/60000 datapoints
2025-03-06 17:59:47,417 - INFO - training batch 801, loss: 0.662, 25632/60000 datapoints
2025-03-06 17:59:47,615 - INFO - training batch 851, loss: 0.425, 27232/60000 datapoints
2025-03-06 17:59:47,808 - INFO - training batch 901, loss: 0.472, 28832/60000 datapoints
2025-03-06 17:59:48,001 - INFO - training batch 951, loss: 0.424, 30432/60000 datapoints
2025-03-06 17:59:48,196 - INFO - training batch 1001, loss: 0.406, 32032/60000 datapoints
2025-03-06 17:59:48,390 - INFO - training batch 1051, loss: 0.276, 33632/60000 datapoints
2025-03-06 17:59:48,583 - INFO - training batch 1101, loss: 0.446, 35232/60000 datapoints
2025-03-06 17:59:48,782 - INFO - training batch 1151, loss: 0.873, 36832/60000 datapoints
2025-03-06 17:59:48,976 - INFO - training batch 1201, loss: 0.694, 38432/60000 datapoints
2025-03-06 17:59:49,170 - INFO - training batch 1251, loss: 0.439, 40032/60000 datapoints
2025-03-06 17:59:49,364 - INFO - training batch 1301, loss: 0.292, 41632/60000 datapoints
2025-03-06 17:59:49,560 - INFO - training batch 1351, loss: 0.482, 43232/60000 datapoints
2025-03-06 17:59:49,779 - INFO - training batch 1401, loss: 0.455, 44832/60000 datapoints
2025-03-06 17:59:50,004 - INFO - training batch 1451, loss: 0.868, 46432/60000 datapoints
2025-03-06 17:59:50,198 - INFO - training batch 1501, loss: 0.395, 48032/60000 datapoints
2025-03-06 17:59:50,390 - INFO - training batch 1551, loss: 0.424, 49632/60000 datapoints
2025-03-06 17:59:50,583 - INFO - training batch 1601, loss: 0.515, 51232/60000 datapoints
2025-03-06 17:59:50,782 - INFO - training batch 1651, loss: 0.489, 52832/60000 datapoints
2025-03-06 17:59:50,976 - INFO - training batch 1701, loss: 0.421, 54432/60000 datapoints
2025-03-06 17:59:51,170 - INFO - training batch 1751, loss: 0.800, 56032/60000 datapoints
2025-03-06 17:59:51,367 - INFO - training batch 1801, loss: 0.572, 57632/60000 datapoints
2025-03-06 17:59:51,562 - INFO - training batch 1851, loss: 0.387, 59232/60000 datapoints
2025-03-06 17:59:51,666 - INFO - validation batch 1, loss: 0.211, 32/10016 datapoints
2025-03-06 17:59:51,819 - INFO - validation batch 51, loss: 0.507, 1632/10016 datapoints
2025-03-06 17:59:51,971 - INFO - validation batch 101, loss: 0.391, 3232/10016 datapoints
2025-03-06 17:59:52,122 - INFO - validation batch 151, loss: 0.522, 4832/10016 datapoints
2025-03-06 17:59:52,274 - INFO - validation batch 201, loss: 0.617, 6432/10016 datapoints
2025-03-06 17:59:52,428 - INFO - validation batch 251, loss: 0.372, 8032/10016 datapoints
2025-03-06 17:59:52,578 - INFO - validation batch 301, loss: 0.412, 9632/10016 datapoints
2025-03-06 17:59:52,619 - INFO - Epoch 114/800 done.
2025-03-06 17:59:52,619 - INFO - Final validation performance:
Loss: 0.433, top-1 acc: 0.881top-5 acc: 0.881
2025-03-06 17:59:52,619 - INFO - Beginning epoch 115/800
2025-03-06 17:59:52,625 - INFO - training batch 1, loss: 0.482, 32/60000 datapoints
2025-03-06 17:59:52,823 - INFO - training batch 51, loss: 0.792, 1632/60000 datapoints
2025-03-06 17:59:53,016 - INFO - training batch 101, loss: 0.396, 3232/60000 datapoints
2025-03-06 17:59:53,210 - INFO - training batch 151, loss: 0.397, 4832/60000 datapoints
2025-03-06 17:59:53,405 - INFO - training batch 201, loss: 0.483, 6432/60000 datapoints
2025-03-06 17:59:53,602 - INFO - training batch 251, loss: 0.643, 8032/60000 datapoints
2025-03-06 17:59:53,798 - INFO - training batch 301, loss: 0.349, 9632/60000 datapoints
2025-03-06 17:59:53,992 - INFO - training batch 351, loss: 0.269, 11232/60000 datapoints
2025-03-06 17:59:54,185 - INFO - training batch 401, loss: 0.433, 12832/60000 datapoints
2025-03-06 17:59:54,378 - INFO - training batch 451, loss: 0.573, 14432/60000 datapoints
2025-03-06 17:59:54,572 - INFO - training batch 501, loss: 0.459, 16032/60000 datapoints
2025-03-06 17:59:54,770 - INFO - training batch 551, loss: 0.396, 17632/60000 datapoints
2025-03-06 17:59:54,968 - INFO - training batch 601, loss: 0.352, 19232/60000 datapoints
2025-03-06 17:59:55,161 - INFO - training batch 651, loss: 0.673, 20832/60000 datapoints
2025-03-06 17:59:55,355 - INFO - training batch 701, loss: 0.373, 22432/60000 datapoints
2025-03-06 17:59:55,548 - INFO - training batch 751, loss: 0.513, 24032/60000 datapoints
2025-03-06 17:59:55,745 - INFO - training batch 801, loss: 0.652, 25632/60000 datapoints
2025-03-06 17:59:55,937 - INFO - training batch 851, loss: 0.922, 27232/60000 datapoints
2025-03-06 17:59:56,130 - INFO - training batch 901, loss: 0.528, 28832/60000 datapoints
2025-03-06 17:59:56,327 - INFO - training batch 951, loss: 0.490, 30432/60000 datapoints
2025-03-06 17:59:56,521 - INFO - training batch 1001, loss: 0.520, 32032/60000 datapoints
2025-03-06 17:59:56,719 - INFO - training batch 1051, loss: 0.434, 33632/60000 datapoints
2025-03-06 17:59:56,921 - INFO - training batch 1101, loss: 0.516, 35232/60000 datapoints
2025-03-06 17:59:57,127 - INFO - training batch 1151, loss: 0.476, 36832/60000 datapoints
2025-03-06 17:59:57,323 - INFO - training batch 1201, loss: 0.316, 38432/60000 datapoints
2025-03-06 17:59:57,518 - INFO - training batch 1251, loss: 0.538, 40032/60000 datapoints
2025-03-06 17:59:57,716 - INFO - training batch 1301, loss: 0.419, 41632/60000 datapoints
2025-03-06 17:59:57,917 - INFO - training batch 1351, loss: 0.367, 43232/60000 datapoints
2025-03-06 17:59:58,110 - INFO - training batch 1401, loss: 0.445, 44832/60000 datapoints
2025-03-06 17:59:58,302 - INFO - training batch 1451, loss: 0.539, 46432/60000 datapoints
2025-03-06 17:59:58,498 - INFO - training batch 1501, loss: 0.366, 48032/60000 datapoints
2025-03-06 17:59:58,698 - INFO - training batch 1551, loss: 0.408, 49632/60000 datapoints
2025-03-06 17:59:58,895 - INFO - training batch 1601, loss: 0.439, 51232/60000 datapoints
2025-03-06 17:59:59,087 - INFO - training batch 1651, loss: 0.464, 52832/60000 datapoints
2025-03-06 17:59:59,282 - INFO - training batch 1701, loss: 0.407, 54432/60000 datapoints
2025-03-06 17:59:59,478 - INFO - training batch 1751, loss: 0.448, 56032/60000 datapoints
2025-03-06 17:59:59,674 - INFO - training batch 1801, loss: 0.488, 57632/60000 datapoints
2025-03-06 17:59:59,868 - INFO - training batch 1851, loss: 0.528, 59232/60000 datapoints
2025-03-06 17:59:59,969 - INFO - validation batch 1, loss: 0.546, 32/10016 datapoints
2025-03-06 18:00:00,122 - INFO - validation batch 51, loss: 0.279, 1632/10016 datapoints
2025-03-06 18:00:00,278 - INFO - validation batch 101, loss: 0.461, 3232/10016 datapoints
2025-03-06 18:00:00,434 - INFO - validation batch 151, loss: 0.216, 4832/10016 datapoints
2025-03-06 18:00:00,587 - INFO - validation batch 201, loss: 0.500, 6432/10016 datapoints
2025-03-06 18:00:00,743 - INFO - validation batch 251, loss: 0.567, 8032/10016 datapoints
2025-03-06 18:00:00,900 - INFO - validation batch 301, loss: 0.476, 9632/10016 datapoints
2025-03-06 18:00:00,938 - INFO - Epoch 115/800 done.
2025-03-06 18:00:00,938 - INFO - Final validation performance:
Loss: 0.435, top-1 acc: 0.881top-5 acc: 0.881
2025-03-06 18:00:00,939 - INFO - Beginning epoch 116/800
2025-03-06 18:00:00,944 - INFO - training batch 1, loss: 0.547, 32/60000 datapoints
2025-03-06 18:00:01,138 - INFO - training batch 51, loss: 0.344, 1632/60000 datapoints
2025-03-06 18:00:01,334 - INFO - training batch 101, loss: 0.518, 3232/60000 datapoints
2025-03-06 18:00:01,529 - INFO - training batch 151, loss: 0.476, 4832/60000 datapoints
2025-03-06 18:00:01,724 - INFO - training batch 201, loss: 0.303, 6432/60000 datapoints
2025-03-06 18:00:01,919 - INFO - training batch 251, loss: 0.581, 8032/60000 datapoints
2025-03-06 18:00:02,112 - INFO - training batch 301, loss: 0.394, 9632/60000 datapoints
2025-03-06 18:00:02,306 - INFO - training batch 351, loss: 0.395, 11232/60000 datapoints
2025-03-06 18:00:02,501 - INFO - training batch 401, loss: 0.276, 12832/60000 datapoints
2025-03-06 18:00:02,699 - INFO - training batch 451, loss: 0.343, 14432/60000 datapoints
2025-03-06 18:00:02,894 - INFO - training batch 501, loss: 0.412, 16032/60000 datapoints
2025-03-06 18:00:03,088 - INFO - training batch 551, loss: 0.456, 17632/60000 datapoints
2025-03-06 18:00:03,282 - INFO - training batch 601, loss: 0.752, 19232/60000 datapoints
2025-03-06 18:00:03,479 - INFO - training batch 651, loss: 0.242, 20832/60000 datapoints
2025-03-06 18:00:03,674 - INFO - training batch 701, loss: 0.483, 22432/60000 datapoints
2025-03-06 18:00:03,870 - INFO - training batch 751, loss: 0.494, 24032/60000 datapoints
2025-03-06 18:00:04,063 - INFO - training batch 801, loss: 0.501, 25632/60000 datapoints
2025-03-06 18:00:04,256 - INFO - training batch 851, loss: 0.435, 27232/60000 datapoints
2025-03-06 18:00:04,450 - INFO - training batch 901, loss: 0.497, 28832/60000 datapoints
2025-03-06 18:00:04,645 - INFO - training batch 951, loss: 0.309, 30432/60000 datapoints
2025-03-06 18:00:04,847 - INFO - training batch 1001, loss: 1.138, 32032/60000 datapoints
2025-03-06 18:00:05,040 - INFO - training batch 1051, loss: 0.812, 33632/60000 datapoints
2025-03-06 18:00:05,233 - INFO - training batch 1101, loss: 0.429, 35232/60000 datapoints
2025-03-06 18:00:05,428 - INFO - training batch 1151, loss: 0.314, 36832/60000 datapoints
2025-03-06 18:00:05,625 - INFO - training batch 1201, loss: 0.472, 38432/60000 datapoints
2025-03-06 18:00:05,816 - INFO - training batch 1251, loss: 0.417, 40032/60000 datapoints
2025-03-06 18:00:06,012 - INFO - training batch 1301, loss: 0.559, 41632/60000 datapoints
2025-03-06 18:00:06,208 - INFO - training batch 1351, loss: 0.510, 43232/60000 datapoints
2025-03-06 18:00:06,404 - INFO - training batch 1401, loss: 0.352, 44832/60000 datapoints
2025-03-06 18:00:06,599 - INFO - training batch 1451, loss: 0.466, 46432/60000 datapoints
2025-03-06 18:00:06,795 - INFO - training batch 1501, loss: 0.659, 48032/60000 datapoints
2025-03-06 18:00:06,998 - INFO - training batch 1551, loss: 0.395, 49632/60000 datapoints
2025-03-06 18:00:07,205 - INFO - training batch 1601, loss: 0.472, 51232/60000 datapoints
2025-03-06 18:00:07,399 - INFO - training batch 1651, loss: 0.238, 52832/60000 datapoints
2025-03-06 18:00:07,595 - INFO - training batch 1701, loss: 0.536, 54432/60000 datapoints
2025-03-06 18:00:07,791 - INFO - training batch 1751, loss: 0.613, 56032/60000 datapoints
2025-03-06 18:00:07,982 - INFO - training batch 1801, loss: 0.457, 57632/60000 datapoints
2025-03-06 18:00:08,201 - INFO - training batch 1851, loss: 0.366, 59232/60000 datapoints
2025-03-06 18:00:08,306 - INFO - validation batch 1, loss: 0.386, 32/10016 datapoints
2025-03-06 18:00:08,459 - INFO - validation batch 51, loss: 0.700, 1632/10016 datapoints
2025-03-06 18:00:08,612 - INFO - validation batch 101, loss: 0.337, 3232/10016 datapoints
2025-03-06 18:00:08,767 - INFO - validation batch 151, loss: 0.520, 4832/10016 datapoints
2025-03-06 18:00:08,920 - INFO - validation batch 201, loss: 0.477, 6432/10016 datapoints
2025-03-06 18:00:09,073 - INFO - validation batch 251, loss: 0.536, 8032/10016 datapoints
2025-03-06 18:00:09,226 - INFO - validation batch 301, loss: 0.304, 9632/10016 datapoints
2025-03-06 18:00:09,262 - INFO - Epoch 116/800 done.
2025-03-06 18:00:09,263 - INFO - Final validation performance:
Loss: 0.466, top-1 acc: 0.882top-5 acc: 0.882
2025-03-06 18:00:09,263 - INFO - Beginning epoch 117/800
2025-03-06 18:00:09,269 - INFO - training batch 1, loss: 0.480, 32/60000 datapoints
2025-03-06 18:00:09,464 - INFO - training batch 51, loss: 0.593, 1632/60000 datapoints
2025-03-06 18:00:09,662 - INFO - training batch 101, loss: 0.479, 3232/60000 datapoints
2025-03-06 18:00:09,856 - INFO - training batch 151, loss: 0.590, 4832/60000 datapoints
2025-03-06 18:00:10,050 - INFO - training batch 201, loss: 0.667, 6432/60000 datapoints
2025-03-06 18:00:10,244 - INFO - training batch 251, loss: 0.677, 8032/60000 datapoints
2025-03-06 18:00:10,438 - INFO - training batch 301, loss: 0.385, 9632/60000 datapoints
2025-03-06 18:00:10,635 - INFO - training batch 351, loss: 0.245, 11232/60000 datapoints
2025-03-06 18:00:10,832 - INFO - training batch 401, loss: 0.694, 12832/60000 datapoints
2025-03-06 18:00:11,026 - INFO - training batch 451, loss: 0.600, 14432/60000 datapoints
2025-03-06 18:00:11,218 - INFO - training batch 501, loss: 0.642, 16032/60000 datapoints
2025-03-06 18:00:11,413 - INFO - training batch 551, loss: 0.543, 17632/60000 datapoints
2025-03-06 18:00:11,610 - INFO - training batch 601, loss: 0.450, 19232/60000 datapoints
2025-03-06 18:00:11,804 - INFO - training batch 651, loss: 0.278, 20832/60000 datapoints
2025-03-06 18:00:11,998 - INFO - training batch 701, loss: 0.496, 22432/60000 datapoints
2025-03-06 18:00:12,191 - INFO - training batch 751, loss: 0.497, 24032/60000 datapoints
2025-03-06 18:00:12,389 - INFO - training batch 801, loss: 0.436, 25632/60000 datapoints
2025-03-06 18:00:12,584 - INFO - training batch 851, loss: 0.438, 27232/60000 datapoints
2025-03-06 18:00:12,780 - INFO - training batch 901, loss: 0.410, 28832/60000 datapoints
2025-03-06 18:00:12,974 - INFO - training batch 951, loss: 0.276, 30432/60000 datapoints
2025-03-06 18:00:13,168 - INFO - training batch 1001, loss: 0.487, 32032/60000 datapoints
2025-03-06 18:00:13,362 - INFO - training batch 1051, loss: 0.682, 33632/60000 datapoints
2025-03-06 18:00:13,557 - INFO - training batch 1101, loss: 0.474, 35232/60000 datapoints
2025-03-06 18:00:13,753 - INFO - training batch 1151, loss: 0.631, 36832/60000 datapoints
2025-03-06 18:00:13,948 - INFO - training batch 1201, loss: 0.408, 38432/60000 datapoints
2025-03-06 18:00:14,143 - INFO - training batch 1251, loss: 0.330, 40032/60000 datapoints
2025-03-06 18:00:14,336 - INFO - training batch 1301, loss: 0.516, 41632/60000 datapoints
2025-03-06 18:00:14,530 - INFO - training batch 1351, loss: 0.552, 43232/60000 datapoints
2025-03-06 18:00:14,727 - INFO - training batch 1401, loss: 0.288, 44832/60000 datapoints
2025-03-06 18:00:14,926 - INFO - training batch 1451, loss: 0.640, 46432/60000 datapoints
2025-03-06 18:00:15,123 - INFO - training batch 1501, loss: 0.572, 48032/60000 datapoints
2025-03-06 18:00:15,317 - INFO - training batch 1551, loss: 0.288, 49632/60000 datapoints
2025-03-06 18:00:15,514 - INFO - training batch 1601, loss: 0.503, 51232/60000 datapoints
2025-03-06 18:00:15,709 - INFO - training batch 1651, loss: 0.401, 52832/60000 datapoints
2025-03-06 18:00:15,900 - INFO - training batch 1701, loss: 0.463, 54432/60000 datapoints
2025-03-06 18:00:16,094 - INFO - training batch 1751, loss: 0.696, 56032/60000 datapoints
2025-03-06 18:00:16,289 - INFO - training batch 1801, loss: 0.503, 57632/60000 datapoints
2025-03-06 18:00:16,488 - INFO - training batch 1851, loss: 0.439, 59232/60000 datapoints
2025-03-06 18:00:16,587 - INFO - validation batch 1, loss: 0.627, 32/10016 datapoints
2025-03-06 18:00:16,745 - INFO - validation batch 51, loss: 0.254, 1632/10016 datapoints
2025-03-06 18:00:16,900 - INFO - validation batch 101, loss: 0.547, 3232/10016 datapoints
2025-03-06 18:00:17,058 - INFO - validation batch 151, loss: 0.290, 4832/10016 datapoints
2025-03-06 18:00:17,227 - INFO - validation batch 201, loss: 0.319, 6432/10016 datapoints
2025-03-06 18:00:17,380 - INFO - validation batch 251, loss: 0.337, 8032/10016 datapoints
2025-03-06 18:00:17,533 - INFO - validation batch 301, loss: 0.449, 9632/10016 datapoints
2025-03-06 18:00:17,571 - INFO - Epoch 117/800 done.
2025-03-06 18:00:17,571 - INFO - Final validation performance:
Loss: 0.403, top-1 acc: 0.883top-5 acc: 0.883
2025-03-06 18:00:17,571 - INFO - Beginning epoch 118/800
2025-03-06 18:00:17,577 - INFO - training batch 1, loss: 0.411, 32/60000 datapoints
2025-03-06 18:00:17,773 - INFO - training batch 51, loss: 0.302, 1632/60000 datapoints
2025-03-06 18:00:17,967 - INFO - training batch 101, loss: 0.440, 3232/60000 datapoints
2025-03-06 18:00:18,162 - INFO - training batch 151, loss: 0.478, 4832/60000 datapoints
2025-03-06 18:00:18,355 - INFO - training batch 201, loss: 0.598, 6432/60000 datapoints
2025-03-06 18:00:18,552 - INFO - training batch 251, loss: 0.294, 8032/60000 datapoints
2025-03-06 18:00:18,750 - INFO - training batch 301, loss: 0.468, 9632/60000 datapoints
2025-03-06 18:00:18,945 - INFO - training batch 351, loss: 0.334, 11232/60000 datapoints
2025-03-06 18:00:19,139 - INFO - training batch 401, loss: 0.617, 12832/60000 datapoints
2025-03-06 18:00:19,334 - INFO - training batch 451, loss: 0.684, 14432/60000 datapoints
2025-03-06 18:00:19,529 - INFO - training batch 501, loss: 0.396, 16032/60000 datapoints
2025-03-06 18:00:19,725 - INFO - training batch 551, loss: 0.423, 17632/60000 datapoints
2025-03-06 18:00:19,920 - INFO - training batch 601, loss: 0.376, 19232/60000 datapoints
2025-03-06 18:00:20,116 - INFO - training batch 651, loss: 0.438, 20832/60000 datapoints
2025-03-06 18:00:20,309 - INFO - training batch 701, loss: 0.590, 22432/60000 datapoints
2025-03-06 18:00:20,502 - INFO - training batch 751, loss: 0.603, 24032/60000 datapoints
2025-03-06 18:00:20,697 - INFO - training batch 801, loss: 0.365, 25632/60000 datapoints
2025-03-06 18:00:20,900 - INFO - training batch 851, loss: 0.611, 27232/60000 datapoints
2025-03-06 18:00:21,096 - INFO - training batch 901, loss: 0.252, 28832/60000 datapoints
2025-03-06 18:00:21,287 - INFO - training batch 951, loss: 0.264, 30432/60000 datapoints
2025-03-06 18:00:21,482 - INFO - training batch 1001, loss: 0.518, 32032/60000 datapoints
2025-03-06 18:00:21,679 - INFO - training batch 1051, loss: 0.367, 33632/60000 datapoints
2025-03-06 18:00:21,891 - INFO - training batch 1101, loss: 0.322, 35232/60000 datapoints
2025-03-06 18:00:22,098 - INFO - training batch 1151, loss: 0.308, 36832/60000 datapoints
2025-03-06 18:00:22,291 - INFO - training batch 1201, loss: 0.330, 38432/60000 datapoints
2025-03-06 18:00:22,484 - INFO - training batch 1251, loss: 0.570, 40032/60000 datapoints
2025-03-06 18:00:22,682 - INFO - training batch 1301, loss: 0.393, 41632/60000 datapoints
2025-03-06 18:00:22,880 - INFO - training batch 1351, loss: 0.357, 43232/60000 datapoints
2025-03-06 18:00:23,077 - INFO - training batch 1401, loss: 0.547, 44832/60000 datapoints
2025-03-06 18:00:23,271 - INFO - training batch 1451, loss: 0.475, 46432/60000 datapoints
2025-03-06 18:00:23,465 - INFO - training batch 1501, loss: 0.450, 48032/60000 datapoints
2025-03-06 18:00:23,660 - INFO - training batch 1551, loss: 0.500, 49632/60000 datapoints
2025-03-06 18:00:23,855 - INFO - training batch 1601, loss: 0.603, 51232/60000 datapoints
2025-03-06 18:00:24,049 - INFO - training batch 1651, loss: 0.390, 52832/60000 datapoints
2025-03-06 18:00:24,244 - INFO - training batch 1701, loss: 0.443, 54432/60000 datapoints
2025-03-06 18:00:24,436 - INFO - training batch 1751, loss: 0.394, 56032/60000 datapoints
2025-03-06 18:00:24,630 - INFO - training batch 1801, loss: 0.427, 57632/60000 datapoints
2025-03-06 18:00:24,828 - INFO - training batch 1851, loss: 0.758, 59232/60000 datapoints
2025-03-06 18:00:24,931 - INFO - validation batch 1, loss: 0.473, 32/10016 datapoints
2025-03-06 18:00:25,085 - INFO - validation batch 51, loss: 0.540, 1632/10016 datapoints
2025-03-06 18:00:25,236 - INFO - validation batch 101, loss: 0.255, 3232/10016 datapoints
2025-03-06 18:00:25,390 - INFO - validation batch 151, loss: 0.424, 4832/10016 datapoints
2025-03-06 18:00:25,543 - INFO - validation batch 201, loss: 0.304, 6432/10016 datapoints
2025-03-06 18:00:25,701 - INFO - validation batch 251, loss: 0.388, 8032/10016 datapoints
2025-03-06 18:00:25,854 - INFO - validation batch 301, loss: 0.516, 9632/10016 datapoints
2025-03-06 18:00:25,891 - INFO - Epoch 118/800 done.
2025-03-06 18:00:25,891 - INFO - Final validation performance:
Loss: 0.414, top-1 acc: 0.883top-5 acc: 0.883
2025-03-06 18:00:25,891 - INFO - Beginning epoch 119/800
2025-03-06 18:00:25,897 - INFO - training batch 1, loss: 0.373, 32/60000 datapoints
2025-03-06 18:00:26,091 - INFO - training batch 51, loss: 0.304, 1632/60000 datapoints
2025-03-06 18:00:26,284 - INFO - training batch 101, loss: 0.341, 3232/60000 datapoints
2025-03-06 18:00:26,492 - INFO - training batch 151, loss: 0.747, 4832/60000 datapoints
2025-03-06 18:00:26,688 - INFO - training batch 201, loss: 0.407, 6432/60000 datapoints
2025-03-06 18:00:26,884 - INFO - training batch 251, loss: 0.218, 8032/60000 datapoints
2025-03-06 18:00:27,079 - INFO - training batch 301, loss: 0.506, 9632/60000 datapoints
2025-03-06 18:00:27,293 - INFO - training batch 351, loss: 0.423, 11232/60000 datapoints
2025-03-06 18:00:27,487 - INFO - training batch 401, loss: 0.333, 12832/60000 datapoints
2025-03-06 18:00:27,681 - INFO - training batch 451, loss: 0.445, 14432/60000 datapoints
2025-03-06 18:00:27,876 - INFO - training batch 501, loss: 0.602, 16032/60000 datapoints
2025-03-06 18:00:28,069 - INFO - training batch 551, loss: 0.411, 17632/60000 datapoints
2025-03-06 18:00:28,262 - INFO - training batch 601, loss: 0.363, 19232/60000 datapoints
2025-03-06 18:00:28,454 - INFO - training batch 651, loss: 0.382, 20832/60000 datapoints
2025-03-06 18:00:28,652 - INFO - training batch 701, loss: 0.654, 22432/60000 datapoints
2025-03-06 18:00:28,861 - INFO - training batch 751, loss: 0.303, 24032/60000 datapoints
2025-03-06 18:00:29,054 - INFO - training batch 801, loss: 0.416, 25632/60000 datapoints
2025-03-06 18:00:29,250 - INFO - training batch 851, loss: 0.536, 27232/60000 datapoints
2025-03-06 18:00:29,442 - INFO - training batch 901, loss: 0.360, 28832/60000 datapoints
2025-03-06 18:00:29,637 - INFO - training batch 951, loss: 0.442, 30432/60000 datapoints
2025-03-06 18:00:29,831 - INFO - training batch 1001, loss: 0.413, 32032/60000 datapoints
2025-03-06 18:00:30,027 - INFO - training batch 1051, loss: 0.246, 33632/60000 datapoints
2025-03-06 18:00:30,222 - INFO - training batch 1101, loss: 0.334, 35232/60000 datapoints
2025-03-06 18:00:30,415 - INFO - training batch 1151, loss: 0.427, 36832/60000 datapoints
2025-03-06 18:00:30,612 - INFO - training batch 1201, loss: 0.476, 38432/60000 datapoints
2025-03-06 18:00:30,808 - INFO - training batch 1251, loss: 0.576, 40032/60000 datapoints
2025-03-06 18:00:31,003 - INFO - training batch 1301, loss: 0.620, 41632/60000 datapoints
2025-03-06 18:00:31,197 - INFO - training batch 1351, loss: 0.545, 43232/60000 datapoints
2025-03-06 18:00:31,391 - INFO - training batch 1401, loss: 0.360, 44832/60000 datapoints
2025-03-06 18:00:31,583 - INFO - training batch 1451, loss: 0.357, 46432/60000 datapoints
2025-03-06 18:00:31,783 - INFO - training batch 1501, loss: 0.324, 48032/60000 datapoints
2025-03-06 18:00:31,978 - INFO - training batch 1551, loss: 0.515, 49632/60000 datapoints
2025-03-06 18:00:32,173 - INFO - training batch 1601, loss: 0.398, 51232/60000 datapoints
2025-03-06 18:00:32,366 - INFO - training batch 1651, loss: 0.567, 52832/60000 datapoints
2025-03-06 18:00:32,559 - INFO - training batch 1701, loss: 0.346, 54432/60000 datapoints
2025-03-06 18:00:32,757 - INFO - training batch 1751, loss: 0.672, 56032/60000 datapoints
2025-03-06 18:00:32,952 - INFO - training batch 1801, loss: 0.357, 57632/60000 datapoints
2025-03-06 18:00:33,145 - INFO - training batch 1851, loss: 0.353, 59232/60000 datapoints
2025-03-06 18:00:33,245 - INFO - validation batch 1, loss: 0.547, 32/10016 datapoints
2025-03-06 18:00:33,396 - INFO - validation batch 51, loss: 0.488, 1632/10016 datapoints
2025-03-06 18:00:33,548 - INFO - validation batch 101, loss: 0.442, 3232/10016 datapoints
2025-03-06 18:00:33,705 - INFO - validation batch 151, loss: 0.411, 4832/10016 datapoints
2025-03-06 18:00:33,858 - INFO - validation batch 201, loss: 0.432, 6432/10016 datapoints
2025-03-06 18:00:34,012 - INFO - validation batch 251, loss: 0.376, 8032/10016 datapoints
2025-03-06 18:00:34,165 - INFO - validation batch 301, loss: 0.543, 9632/10016 datapoints
2025-03-06 18:00:34,202 - INFO - Epoch 119/800 done.
2025-03-06 18:00:34,202 - INFO - Final validation performance:
Loss: 0.463, top-1 acc: 0.883top-5 acc: 0.883
2025-03-06 18:00:34,202 - INFO - Beginning epoch 120/800
2025-03-06 18:00:34,208 - INFO - training batch 1, loss: 0.313, 32/60000 datapoints
2025-03-06 18:00:34,402 - INFO - training batch 51, loss: 0.531, 1632/60000 datapoints
2025-03-06 18:00:34,595 - INFO - training batch 101, loss: 0.239, 3232/60000 datapoints
2025-03-06 18:00:34,791 - INFO - training batch 151, loss: 0.434, 4832/60000 datapoints
2025-03-06 18:00:34,991 - INFO - training batch 201, loss: 0.373, 6432/60000 datapoints
2025-03-06 18:00:35,181 - INFO - training batch 251, loss: 0.373, 8032/60000 datapoints
2025-03-06 18:00:35,375 - INFO - training batch 301, loss: 0.278, 9632/60000 datapoints
2025-03-06 18:00:35,566 - INFO - training batch 351, loss: 0.414, 11232/60000 datapoints
2025-03-06 18:00:35,758 - INFO - training batch 401, loss: 0.473, 12832/60000 datapoints
2025-03-06 18:00:35,949 - INFO - training batch 451, loss: 0.368, 14432/60000 datapoints
2025-03-06 18:00:36,139 - INFO - training batch 501, loss: 0.358, 16032/60000 datapoints
2025-03-06 18:00:36,331 - INFO - training batch 551, loss: 0.691, 17632/60000 datapoints
2025-03-06 18:00:36,522 - INFO - training batch 601, loss: 0.519, 19232/60000 datapoints
2025-03-06 18:00:36,713 - INFO - training batch 651, loss: 0.489, 20832/60000 datapoints
2025-03-06 18:00:36,906 - INFO - training batch 701, loss: 0.536, 22432/60000 datapoints
2025-03-06 18:00:37,100 - INFO - training batch 751, loss: 0.557, 24032/60000 datapoints
2025-03-06 18:00:37,308 - INFO - training batch 801, loss: 0.451, 25632/60000 datapoints
2025-03-06 18:00:37,498 - INFO - training batch 851, loss: 0.667, 27232/60000 datapoints
2025-03-06 18:00:37,708 - INFO - training batch 901, loss: 0.623, 28832/60000 datapoints
2025-03-06 18:00:37,900 - INFO - training batch 951, loss: 0.326, 30432/60000 datapoints
2025-03-06 18:00:38,091 - INFO - training batch 1001, loss: 0.500, 32032/60000 datapoints
2025-03-06 18:00:38,283 - INFO - training batch 1051, loss: 0.470, 33632/60000 datapoints
2025-03-06 18:00:38,476 - INFO - training batch 1101, loss: 0.530, 35232/60000 datapoints
2025-03-06 18:00:38,669 - INFO - training batch 1151, loss: 0.397, 36832/60000 datapoints
2025-03-06 18:00:38,864 - INFO - training batch 1201, loss: 0.447, 38432/60000 datapoints
2025-03-06 18:00:39,058 - INFO - training batch 1251, loss: 0.698, 40032/60000 datapoints
2025-03-06 18:00:39,250 - INFO - training batch 1301, loss: 0.586, 41632/60000 datapoints
2025-03-06 18:00:39,442 - INFO - training batch 1351, loss: 0.430, 43232/60000 datapoints
2025-03-06 18:00:39,636 - INFO - training batch 1401, loss: 0.415, 44832/60000 datapoints
2025-03-06 18:00:39,828 - INFO - training batch 1451, loss: 0.459, 46432/60000 datapoints
2025-03-06 18:00:40,021 - INFO - training batch 1501, loss: 0.366, 48032/60000 datapoints
2025-03-06 18:00:40,212 - INFO - training batch 1551, loss: 0.746, 49632/60000 datapoints
2025-03-06 18:00:40,405 - INFO - training batch 1601, loss: 0.215, 51232/60000 datapoints
2025-03-06 18:00:40,599 - INFO - training batch 1651, loss: 0.457, 52832/60000 datapoints
2025-03-06 18:00:40,789 - INFO - training batch 1701, loss: 0.314, 54432/60000 datapoints
2025-03-06 18:00:40,982 - INFO - training batch 1751, loss: 0.402, 56032/60000 datapoints
2025-03-06 18:00:41,172 - INFO - training batch 1801, loss: 0.670, 57632/60000 datapoints
2025-03-06 18:00:41,363 - INFO - training batch 1851, loss: 0.422, 59232/60000 datapoints
2025-03-06 18:00:41,461 - INFO - validation batch 1, loss: 0.321, 32/10016 datapoints
2025-03-06 18:00:41,611 - INFO - validation batch 51, loss: 0.672, 1632/10016 datapoints
2025-03-06 18:00:41,761 - INFO - validation batch 101, loss: 0.288, 3232/10016 datapoints
2025-03-06 18:00:41,911 - INFO - validation batch 151, loss: 0.669, 4832/10016 datapoints
2025-03-06 18:00:42,061 - INFO - validation batch 201, loss: 0.494, 6432/10016 datapoints
2025-03-06 18:00:42,210 - INFO - validation batch 251, loss: 0.301, 8032/10016 datapoints
2025-03-06 18:00:42,360 - INFO - validation batch 301, loss: 0.522, 9632/10016 datapoints
2025-03-06 18:00:42,395 - INFO - Epoch 120/800 done.
2025-03-06 18:00:42,395 - INFO - Final validation performance:
Loss: 0.467, top-1 acc: 0.884top-5 acc: 0.884
2025-03-06 18:00:42,395 - INFO - Beginning epoch 121/800
2025-03-06 18:00:42,401 - INFO - training batch 1, loss: 0.391, 32/60000 datapoints
2025-03-06 18:00:42,600 - INFO - training batch 51, loss: 0.445, 1632/60000 datapoints
2025-03-06 18:00:42,795 - INFO - training batch 101, loss: 0.401, 3232/60000 datapoints
2025-03-06 18:00:42,993 - INFO - training batch 151, loss: 0.489, 4832/60000 datapoints
2025-03-06 18:00:43,189 - INFO - training batch 201, loss: 0.496, 6432/60000 datapoints
2025-03-06 18:00:43,384 - INFO - training batch 251, loss: 0.649, 8032/60000 datapoints
2025-03-06 18:00:43,580 - INFO - training batch 301, loss: 0.503, 9632/60000 datapoints
2025-03-06 18:00:43,778 - INFO - training batch 351, loss: 0.454, 11232/60000 datapoints
2025-03-06 18:00:43,973 - INFO - training batch 401, loss: 0.367, 12832/60000 datapoints
2025-03-06 18:00:44,170 - INFO - training batch 451, loss: 0.457, 14432/60000 datapoints
2025-03-06 18:00:44,365 - INFO - training batch 501, loss: 0.296, 16032/60000 datapoints
2025-03-06 18:00:44,560 - INFO - training batch 551, loss: 0.377, 17632/60000 datapoints
2025-03-06 18:00:44,753 - INFO - training batch 601, loss: 0.332, 19232/60000 datapoints
2025-03-06 18:00:44,955 - INFO - training batch 651, loss: 0.677, 20832/60000 datapoints
2025-03-06 18:00:45,149 - INFO - training batch 701, loss: 0.340, 22432/60000 datapoints
2025-03-06 18:00:45,342 - INFO - training batch 751, loss: 0.390, 24032/60000 datapoints
2025-03-06 18:00:45,534 - INFO - training batch 801, loss: 0.490, 25632/60000 datapoints
2025-03-06 18:00:45,729 - INFO - training batch 851, loss: 0.252, 27232/60000 datapoints
2025-03-06 18:00:45,922 - INFO - training batch 901, loss: 0.602, 28832/60000 datapoints
2025-03-06 18:00:46,115 - INFO - training batch 951, loss: 0.368, 30432/60000 datapoints
2025-03-06 18:00:46,310 - INFO - training batch 1001, loss: 0.572, 32032/60000 datapoints
2025-03-06 18:00:46,513 - INFO - training batch 1051, loss: 0.408, 33632/60000 datapoints
2025-03-06 18:00:46,710 - INFO - training batch 1101, loss: 0.671, 35232/60000 datapoints
2025-03-06 18:00:46,909 - INFO - training batch 1151, loss: 0.354, 36832/60000 datapoints
2025-03-06 18:00:47,109 - INFO - training batch 1201, loss: 0.457, 38432/60000 datapoints
2025-03-06 18:00:47,308 - INFO - training batch 1251, loss: 0.450, 40032/60000 datapoints
2025-03-06 18:00:47,515 - INFO - training batch 1301, loss: 0.291, 41632/60000 datapoints
2025-03-06 18:00:47,710 - INFO - training batch 1351, loss: 0.370, 43232/60000 datapoints
2025-03-06 18:00:47,904 - INFO - training batch 1401, loss: 0.680, 44832/60000 datapoints
2025-03-06 18:00:48,097 - INFO - training batch 1451, loss: 0.619, 46432/60000 datapoints
2025-03-06 18:00:48,289 - INFO - training batch 1501, loss: 0.395, 48032/60000 datapoints
2025-03-06 18:00:48,484 - INFO - training batch 1551, loss: 0.407, 49632/60000 datapoints
2025-03-06 18:00:48,679 - INFO - training batch 1601, loss: 0.443, 51232/60000 datapoints
2025-03-06 18:00:48,876 - INFO - training batch 1651, loss: 0.315, 52832/60000 datapoints
2025-03-06 18:00:49,070 - INFO - training batch 1701, loss: 0.426, 54432/60000 datapoints
2025-03-06 18:00:49,263 - INFO - training batch 1751, loss: 0.478, 56032/60000 datapoints
2025-03-06 18:00:49,458 - INFO - training batch 1801, loss: 0.577, 57632/60000 datapoints
2025-03-06 18:00:49,656 - INFO - training batch 1851, loss: 0.438, 59232/60000 datapoints
2025-03-06 18:00:49,757 - INFO - validation batch 1, loss: 0.348, 32/10016 datapoints
2025-03-06 18:00:49,911 - INFO - validation batch 51, loss: 0.375, 1632/10016 datapoints
2025-03-06 18:00:50,065 - INFO - validation batch 101, loss: 0.365, 3232/10016 datapoints
2025-03-06 18:00:50,216 - INFO - validation batch 151, loss: 0.197, 4832/10016 datapoints
2025-03-06 18:00:50,369 - INFO - validation batch 201, loss: 0.349, 6432/10016 datapoints
2025-03-06 18:00:50,521 - INFO - validation batch 251, loss: 0.511, 8032/10016 datapoints
2025-03-06 18:00:50,677 - INFO - validation batch 301, loss: 0.405, 9632/10016 datapoints
2025-03-06 18:00:50,714 - INFO - Epoch 121/800 done.
2025-03-06 18:00:50,714 - INFO - Final validation performance:
Loss: 0.364, top-1 acc: 0.885top-5 acc: 0.885
2025-03-06 18:00:50,714 - INFO - Beginning epoch 122/800
2025-03-06 18:00:50,721 - INFO - training batch 1, loss: 0.359, 32/60000 datapoints
2025-03-06 18:00:50,918 - INFO - training batch 51, loss: 0.397, 1632/60000 datapoints
2025-03-06 18:00:51,112 - INFO - training batch 101, loss: 0.422, 3232/60000 datapoints
2025-03-06 18:00:51,306 - INFO - training batch 151, loss: 0.268, 4832/60000 datapoints
2025-03-06 18:00:51,500 - INFO - training batch 201, loss: 0.421, 6432/60000 datapoints
2025-03-06 18:00:51,696 - INFO - training batch 251, loss: 0.606, 8032/60000 datapoints
2025-03-06 18:00:51,890 - INFO - training batch 301, loss: 0.434, 9632/60000 datapoints
2025-03-06 18:00:52,084 - INFO - training batch 351, loss: 0.246, 11232/60000 datapoints
2025-03-06 18:00:52,279 - INFO - training batch 401, loss: 0.635, 12832/60000 datapoints
2025-03-06 18:00:52,472 - INFO - training batch 451, loss: 0.605, 14432/60000 datapoints
2025-03-06 18:00:52,671 - INFO - training batch 501, loss: 0.317, 16032/60000 datapoints
2025-03-06 18:00:52,867 - INFO - training batch 551, loss: 0.368, 17632/60000 datapoints
2025-03-06 18:00:53,063 - INFO - training batch 601, loss: 0.234, 19232/60000 datapoints
2025-03-06 18:00:53,257 - INFO - training batch 651, loss: 0.456, 20832/60000 datapoints
2025-03-06 18:00:53,450 - INFO - training batch 701, loss: 0.444, 22432/60000 datapoints
2025-03-06 18:00:53,648 - INFO - training batch 751, loss: 0.516, 24032/60000 datapoints
2025-03-06 18:00:53,843 - INFO - training batch 801, loss: 0.660, 25632/60000 datapoints
2025-03-06 18:00:54,037 - INFO - training batch 851, loss: 0.457, 27232/60000 datapoints
2025-03-06 18:00:54,230 - INFO - training batch 901, loss: 0.568, 28832/60000 datapoints
2025-03-06 18:00:54,425 - INFO - training batch 951, loss: 0.514, 30432/60000 datapoints
2025-03-06 18:00:54,620 - INFO - training batch 1001, loss: 0.568, 32032/60000 datapoints
2025-03-06 18:00:54,811 - INFO - training batch 1051, loss: 0.329, 33632/60000 datapoints
2025-03-06 18:00:55,010 - INFO - training batch 1101, loss: 0.727, 35232/60000 datapoints
2025-03-06 18:00:55,203 - INFO - training batch 1151, loss: 0.360, 36832/60000 datapoints
2025-03-06 18:00:55,395 - INFO - training batch 1201, loss: 0.693, 38432/60000 datapoints
2025-03-06 18:00:55,590 - INFO - training batch 1251, loss: 0.355, 40032/60000 datapoints
2025-03-06 18:00:55,785 - INFO - training batch 1301, loss: 0.360, 41632/60000 datapoints
2025-03-06 18:00:55,978 - INFO - training batch 1351, loss: 0.513, 43232/60000 datapoints
2025-03-06 18:00:56,174 - INFO - training batch 1401, loss: 0.469, 44832/60000 datapoints
2025-03-06 18:00:56,366 - INFO - training batch 1451, loss: 0.291, 46432/60000 datapoints
2025-03-06 18:00:56,560 - INFO - training batch 1501, loss: 0.321, 48032/60000 datapoints
2025-03-06 18:00:56,752 - INFO - training batch 1551, loss: 0.506, 49632/60000 datapoints
2025-03-06 18:00:56,947 - INFO - training batch 1601, loss: 0.548, 51232/60000 datapoints
2025-03-06 18:00:57,146 - INFO - training batch 1651, loss: 0.569, 52832/60000 datapoints
2025-03-06 18:00:57,340 - INFO - training batch 1701, loss: 0.357, 54432/60000 datapoints
2025-03-06 18:00:57,554 - INFO - training batch 1751, loss: 0.320, 56032/60000 datapoints
2025-03-06 18:00:57,750 - INFO - training batch 1801, loss: 0.411, 57632/60000 datapoints
2025-03-06 18:00:57,945 - INFO - training batch 1851, loss: 0.477, 59232/60000 datapoints
2025-03-06 18:00:58,046 - INFO - validation batch 1, loss: 0.473, 32/10016 datapoints
2025-03-06 18:00:58,197 - INFO - validation batch 51, loss: 0.478, 1632/10016 datapoints
2025-03-06 18:00:58,347 - INFO - validation batch 101, loss: 0.646, 3232/10016 datapoints
2025-03-06 18:00:58,501 - INFO - validation batch 151, loss: 0.456, 4832/10016 datapoints
2025-03-06 18:00:58,660 - INFO - validation batch 201, loss: 0.362, 6432/10016 datapoints
2025-03-06 18:00:58,819 - INFO - validation batch 251, loss: 0.318, 8032/10016 datapoints
2025-03-06 18:00:58,977 - INFO - validation batch 301, loss: 0.536, 9632/10016 datapoints
2025-03-06 18:00:59,020 - INFO - Epoch 122/800 done.
2025-03-06 18:00:59,021 - INFO - Final validation performance:
Loss: 0.467, top-1 acc: 0.885top-5 acc: 0.885
2025-03-06 18:00:59,021 - INFO - Beginning epoch 123/800
2025-03-06 18:00:59,026 - INFO - training batch 1, loss: 0.343, 32/60000 datapoints
2025-03-06 18:00:59,225 - INFO - training batch 51, loss: 0.359, 1632/60000 datapoints
2025-03-06 18:00:59,422 - INFO - training batch 101, loss: 0.409, 3232/60000 datapoints
2025-03-06 18:00:59,627 - INFO - training batch 151, loss: 0.433, 4832/60000 datapoints
2025-03-06 18:00:59,832 - INFO - training batch 201, loss: 0.366, 6432/60000 datapoints
2025-03-06 18:01:00,029 - INFO - training batch 251, loss: 0.427, 8032/60000 datapoints
2025-03-06 18:01:00,223 - INFO - training batch 301, loss: 0.637, 9632/60000 datapoints
2025-03-06 18:01:00,416 - INFO - training batch 351, loss: 0.309, 11232/60000 datapoints
2025-03-06 18:01:00,617 - INFO - training batch 401, loss: 0.442, 12832/60000 datapoints
2025-03-06 18:01:00,809 - INFO - training batch 451, loss: 0.610, 14432/60000 datapoints
2025-03-06 18:01:01,004 - INFO - training batch 501, loss: 0.531, 16032/60000 datapoints
2025-03-06 18:01:01,197 - INFO - training batch 551, loss: 0.380, 17632/60000 datapoints
2025-03-06 18:01:01,393 - INFO - training batch 601, loss: 0.589, 19232/60000 datapoints
2025-03-06 18:01:01,595 - INFO - training batch 651, loss: 0.452, 20832/60000 datapoints
2025-03-06 18:01:01,790 - INFO - training batch 701, loss: 0.439, 22432/60000 datapoints
2025-03-06 18:01:01,983 - INFO - training batch 751, loss: 0.320, 24032/60000 datapoints
2025-03-06 18:01:02,186 - INFO - training batch 801, loss: 0.261, 25632/60000 datapoints
2025-03-06 18:01:02,384 - INFO - training batch 851, loss: 0.346, 27232/60000 datapoints
2025-03-06 18:01:02,578 - INFO - training batch 901, loss: 0.459, 28832/60000 datapoints
2025-03-06 18:01:02,777 - INFO - training batch 951, loss: 0.397, 30432/60000 datapoints
2025-03-06 18:01:02,973 - INFO - training batch 1001, loss: 0.572, 32032/60000 datapoints
2025-03-06 18:01:03,167 - INFO - training batch 1051, loss: 0.557, 33632/60000 datapoints
2025-03-06 18:01:03,360 - INFO - training batch 1101, loss: 0.514, 35232/60000 datapoints
2025-03-06 18:01:03,559 - INFO - training batch 1151, loss: 0.468, 36832/60000 datapoints
2025-03-06 18:01:03,783 - INFO - training batch 1201, loss: 0.539, 38432/60000 datapoints
2025-03-06 18:01:03,987 - INFO - training batch 1251, loss: 0.312, 40032/60000 datapoints
2025-03-06 18:01:04,247 - INFO - training batch 1301, loss: 0.391, 41632/60000 datapoints
2025-03-06 18:01:04,459 - INFO - training batch 1351, loss: 0.449, 43232/60000 datapoints
2025-03-06 18:01:04,674 - INFO - training batch 1401, loss: 0.432, 44832/60000 datapoints
2025-03-06 18:01:04,875 - INFO - training batch 1451, loss: 0.414, 46432/60000 datapoints
2025-03-06 18:01:05,070 - INFO - training batch 1501, loss: 0.389, 48032/60000 datapoints
2025-03-06 18:01:05,263 - INFO - training batch 1551, loss: 0.530, 49632/60000 datapoints
2025-03-06 18:01:05,454 - INFO - training batch 1601, loss: 0.436, 51232/60000 datapoints
2025-03-06 18:01:05,647 - INFO - training batch 1651, loss: 0.385, 52832/60000 datapoints
2025-03-06 18:01:05,840 - INFO - training batch 1701, loss: 0.455, 54432/60000 datapoints
2025-03-06 18:01:06,031 - INFO - training batch 1751, loss: 0.740, 56032/60000 datapoints
2025-03-06 18:01:06,230 - INFO - training batch 1801, loss: 0.466, 57632/60000 datapoints
2025-03-06 18:01:06,424 - INFO - training batch 1851, loss: 0.347, 59232/60000 datapoints
2025-03-06 18:01:06,525 - INFO - validation batch 1, loss: 0.403, 32/10016 datapoints
2025-03-06 18:01:06,680 - INFO - validation batch 51, loss: 0.575, 1632/10016 datapoints
2025-03-06 18:01:06,831 - INFO - validation batch 101, loss: 0.568, 3232/10016 datapoints
2025-03-06 18:01:06,986 - INFO - validation batch 151, loss: 0.671, 4832/10016 datapoints
2025-03-06 18:01:07,150 - INFO - validation batch 201, loss: 0.331, 6432/10016 datapoints
2025-03-06 18:01:07,301 - INFO - validation batch 251, loss: 0.311, 8032/10016 datapoints
2025-03-06 18:01:07,456 - INFO - validation batch 301, loss: 0.587, 9632/10016 datapoints
2025-03-06 18:01:07,499 - INFO - Epoch 123/800 done.
2025-03-06 18:01:07,499 - INFO - Final validation performance:
Loss: 0.492, top-1 acc: 0.886top-5 acc: 0.886
2025-03-06 18:01:07,500 - INFO - Beginning epoch 124/800
2025-03-06 18:01:07,509 - INFO - training batch 1, loss: 0.354, 32/60000 datapoints
2025-03-06 18:01:07,716 - INFO - training batch 51, loss: 0.227, 1632/60000 datapoints
2025-03-06 18:01:07,909 - INFO - training batch 101, loss: 0.320, 3232/60000 datapoints
2025-03-06 18:01:08,106 - INFO - training batch 151, loss: 0.358, 4832/60000 datapoints
2025-03-06 18:01:08,300 - INFO - training batch 201, loss: 0.284, 6432/60000 datapoints
2025-03-06 18:01:08,492 - INFO - training batch 251, loss: 0.390, 8032/60000 datapoints
2025-03-06 18:01:08,687 - INFO - training batch 301, loss: 0.545, 9632/60000 datapoints
2025-03-06 18:01:08,880 - INFO - training batch 351, loss: 0.637, 11232/60000 datapoints
2025-03-06 18:01:09,084 - INFO - training batch 401, loss: 0.543, 12832/60000 datapoints
2025-03-06 18:01:09,279 - INFO - training batch 451, loss: 0.344, 14432/60000 datapoints
2025-03-06 18:01:09,471 - INFO - training batch 501, loss: 0.282, 16032/60000 datapoints
2025-03-06 18:01:09,665 - INFO - training batch 551, loss: 0.337, 17632/60000 datapoints
2025-03-06 18:01:09,856 - INFO - training batch 601, loss: 0.387, 19232/60000 datapoints
2025-03-06 18:01:10,049 - INFO - training batch 651, loss: 0.410, 20832/60000 datapoints
2025-03-06 18:01:10,240 - INFO - training batch 701, loss: 0.541, 22432/60000 datapoints
2025-03-06 18:01:10,433 - INFO - training batch 751, loss: 0.380, 24032/60000 datapoints
2025-03-06 18:01:10,628 - INFO - training batch 801, loss: 0.436, 25632/60000 datapoints
2025-03-06 18:01:10,819 - INFO - training batch 851, loss: 0.319, 27232/60000 datapoints
2025-03-06 18:01:11,011 - INFO - training batch 901, loss: 0.523, 28832/60000 datapoints
2025-03-06 18:01:11,200 - INFO - training batch 951, loss: 0.362, 30432/60000 datapoints
2025-03-06 18:01:11,392 - INFO - training batch 1001, loss: 0.342, 32032/60000 datapoints
2025-03-06 18:01:11,584 - INFO - training batch 1051, loss: 0.615, 33632/60000 datapoints
2025-03-06 18:01:11,777 - INFO - training batch 1101, loss: 0.299, 35232/60000 datapoints
2025-03-06 18:01:11,968 - INFO - training batch 1151, loss: 0.483, 36832/60000 datapoints
2025-03-06 18:01:12,157 - INFO - training batch 1201, loss: 0.417, 38432/60000 datapoints
2025-03-06 18:01:12,346 - INFO - training batch 1251, loss: 0.364, 40032/60000 datapoints
2025-03-06 18:01:12,536 - INFO - training batch 1301, loss: 0.371, 41632/60000 datapoints
2025-03-06 18:01:12,734 - INFO - training batch 1351, loss: 0.361, 43232/60000 datapoints
2025-03-06 18:01:12,925 - INFO - training batch 1401, loss: 0.260, 44832/60000 datapoints
2025-03-06 18:01:13,117 - INFO - training batch 1451, loss: 0.373, 46432/60000 datapoints
2025-03-06 18:01:13,308 - INFO - training batch 1501, loss: 0.327, 48032/60000 datapoints
2025-03-06 18:01:13,501 - INFO - training batch 1551, loss: 0.546, 49632/60000 datapoints
2025-03-06 18:01:13,696 - INFO - training batch 1601, loss: 0.633, 51232/60000 datapoints
2025-03-06 18:01:13,890 - INFO - training batch 1651, loss: 0.646, 52832/60000 datapoints
2025-03-06 18:01:14,081 - INFO - training batch 1701, loss: 0.524, 54432/60000 datapoints
2025-03-06 18:01:14,273 - INFO - training batch 1751, loss: 0.463, 56032/60000 datapoints
2025-03-06 18:01:14,466 - INFO - training batch 1801, loss: 0.478, 57632/60000 datapoints
2025-03-06 18:01:14,660 - INFO - training batch 1851, loss: 0.476, 59232/60000 datapoints
2025-03-06 18:01:14,762 - INFO - validation batch 1, loss: 0.227, 32/10016 datapoints
2025-03-06 18:01:14,913 - INFO - validation batch 51, loss: 0.569, 1632/10016 datapoints
2025-03-06 18:01:15,066 - INFO - validation batch 101, loss: 0.295, 3232/10016 datapoints
2025-03-06 18:01:15,215 - INFO - validation batch 151, loss: 0.452, 4832/10016 datapoints
2025-03-06 18:01:15,365 - INFO - validation batch 201, loss: 0.549, 6432/10016 datapoints
2025-03-06 18:01:15,516 - INFO - validation batch 251, loss: 0.336, 8032/10016 datapoints
2025-03-06 18:01:15,669 - INFO - validation batch 301, loss: 0.412, 9632/10016 datapoints
2025-03-06 18:01:15,704 - INFO - Epoch 124/800 done.
2025-03-06 18:01:15,704 - INFO - Final validation performance:
Loss: 0.406, top-1 acc: 0.886top-5 acc: 0.886
2025-03-06 18:01:15,705 - INFO - Beginning epoch 125/800
2025-03-06 18:01:15,710 - INFO - training batch 1, loss: 0.350, 32/60000 datapoints
2025-03-06 18:01:15,903 - INFO - training batch 51, loss: 0.372, 1632/60000 datapoints
2025-03-06 18:01:16,094 - INFO - training batch 101, loss: 0.522, 3232/60000 datapoints
2025-03-06 18:01:16,291 - INFO - training batch 151, loss: 0.382, 4832/60000 datapoints
2025-03-06 18:01:16,483 - INFO - training batch 201, loss: 0.399, 6432/60000 datapoints
2025-03-06 18:01:16,694 - INFO - training batch 251, loss: 0.340, 8032/60000 datapoints
2025-03-06 18:01:16,910 - INFO - training batch 301, loss: 0.472, 9632/60000 datapoints
2025-03-06 18:01:17,105 - INFO - training batch 351, loss: 0.440, 11232/60000 datapoints
2025-03-06 18:01:17,318 - INFO - training batch 401, loss: 0.370, 12832/60000 datapoints
2025-03-06 18:01:17,523 - INFO - training batch 451, loss: 0.447, 14432/60000 datapoints
2025-03-06 18:01:17,738 - INFO - training batch 501, loss: 0.349, 16032/60000 datapoints
2025-03-06 18:01:17,932 - INFO - training batch 551, loss: 0.345, 17632/60000 datapoints
2025-03-06 18:01:18,138 - INFO - training batch 601, loss: 0.446, 19232/60000 datapoints
2025-03-06 18:01:18,328 - INFO - training batch 651, loss: 0.273, 20832/60000 datapoints
2025-03-06 18:01:18,522 - INFO - training batch 701, loss: 0.439, 22432/60000 datapoints
2025-03-06 18:01:18,715 - INFO - training batch 751, loss: 0.468, 24032/60000 datapoints
2025-03-06 18:01:18,909 - INFO - training batch 801, loss: 0.300, 25632/60000 datapoints
2025-03-06 18:01:19,102 - INFO - training batch 851, loss: 0.465, 27232/60000 datapoints
2025-03-06 18:01:19,295 - INFO - training batch 901, loss: 0.488, 28832/60000 datapoints
2025-03-06 18:01:19,484 - INFO - training batch 951, loss: 0.326, 30432/60000 datapoints
2025-03-06 18:01:19,678 - INFO - training batch 1001, loss: 0.295, 32032/60000 datapoints
2025-03-06 18:01:19,871 - INFO - training batch 1051, loss: 0.344, 33632/60000 datapoints
2025-03-06 18:01:20,063 - INFO - training batch 1101, loss: 0.363, 35232/60000 datapoints
2025-03-06 18:01:20,253 - INFO - training batch 1151, loss: 0.480, 36832/60000 datapoints
2025-03-06 18:01:20,445 - INFO - training batch 1201, loss: 0.322, 38432/60000 datapoints
2025-03-06 18:01:20,637 - INFO - training batch 1251, loss: 0.361, 40032/60000 datapoints
2025-03-06 18:01:20,828 - INFO - training batch 1301, loss: 0.583, 41632/60000 datapoints
2025-03-06 18:01:21,023 - INFO - training batch 1351, loss: 0.601, 43232/60000 datapoints
2025-03-06 18:01:21,211 - INFO - training batch 1401, loss: 0.311, 44832/60000 datapoints
2025-03-06 18:01:21,407 - INFO - training batch 1451, loss: 0.418, 46432/60000 datapoints
2025-03-06 18:01:21,598 - INFO - training batch 1501, loss: 0.368, 48032/60000 datapoints
2025-03-06 18:01:21,789 - INFO - training batch 1551, loss: 0.770, 49632/60000 datapoints
2025-03-06 18:01:21,980 - INFO - training batch 1601, loss: 0.429, 51232/60000 datapoints
2025-03-06 18:01:22,169 - INFO - training batch 1651, loss: 0.636, 52832/60000 datapoints
2025-03-06 18:01:22,361 - INFO - training batch 1701, loss: 0.420, 54432/60000 datapoints
2025-03-06 18:01:22,551 - INFO - training batch 1751, loss: 0.602, 56032/60000 datapoints
2025-03-06 18:01:22,753 - INFO - training batch 1801, loss: 0.298, 57632/60000 datapoints
2025-03-06 18:01:22,947 - INFO - training batch 1851, loss: 0.473, 59232/60000 datapoints
2025-03-06 18:01:23,052 - INFO - validation batch 1, loss: 0.252, 32/10016 datapoints
2025-03-06 18:01:23,206 - INFO - validation batch 51, loss: 0.222, 1632/10016 datapoints
2025-03-06 18:01:23,361 - INFO - validation batch 101, loss: 0.385, 3232/10016 datapoints
2025-03-06 18:01:23,512 - INFO - validation batch 151, loss: 0.361, 4832/10016 datapoints
2025-03-06 18:01:23,667 - INFO - validation batch 201, loss: 0.223, 6432/10016 datapoints
2025-03-06 18:01:23,824 - INFO - validation batch 251, loss: 0.619, 8032/10016 datapoints
2025-03-06 18:01:23,977 - INFO - validation batch 301, loss: 0.322, 9632/10016 datapoints
2025-03-06 18:01:24,014 - INFO - Epoch 125/800 done.
2025-03-06 18:01:24,014 - INFO - Final validation performance:
Loss: 0.341, top-1 acc: 0.887top-5 acc: 0.887
2025-03-06 18:01:24,015 - INFO - Beginning epoch 126/800
2025-03-06 18:01:24,020 - INFO - training batch 1, loss: 0.457, 32/60000 datapoints
2025-03-06 18:01:24,214 - INFO - training batch 51, loss: 0.520, 1632/60000 datapoints
2025-03-06 18:01:24,410 - INFO - training batch 101, loss: 0.443, 3232/60000 datapoints
2025-03-06 18:01:24,605 - INFO - training batch 151, loss: 0.534, 4832/60000 datapoints
2025-03-06 18:01:24,800 - INFO - training batch 201, loss: 0.627, 6432/60000 datapoints
2025-03-06 18:01:25,003 - INFO - training batch 251, loss: 0.488, 8032/60000 datapoints
2025-03-06 18:01:25,198 - INFO - training batch 301, loss: 0.473, 9632/60000 datapoints
2025-03-06 18:01:25,394 - INFO - training batch 351, loss: 0.338, 11232/60000 datapoints
2025-03-06 18:01:25,587 - INFO - training batch 401, loss: 0.274, 12832/60000 datapoints
2025-03-06 18:01:25,784 - INFO - training batch 451, loss: 0.394, 14432/60000 datapoints
2025-03-06 18:01:25,980 - INFO - training batch 501, loss: 0.555, 16032/60000 datapoints
2025-03-06 18:01:26,175 - INFO - training batch 551, loss: 0.373, 17632/60000 datapoints
2025-03-06 18:01:26,369 - INFO - training batch 601, loss: 0.457, 19232/60000 datapoints
2025-03-06 18:01:26,561 - INFO - training batch 651, loss: 0.354, 20832/60000 datapoints
2025-03-06 18:01:26,755 - INFO - training batch 701, loss: 1.059, 22432/60000 datapoints
2025-03-06 18:01:26,950 - INFO - training batch 751, loss: 0.382, 24032/60000 datapoints
2025-03-06 18:01:27,146 - INFO - training batch 801, loss: 0.471, 25632/60000 datapoints
2025-03-06 18:01:27,343 - INFO - training batch 851, loss: 0.283, 27232/60000 datapoints
2025-03-06 18:01:27,535 - INFO - training batch 901, loss: 0.374, 28832/60000 datapoints
2025-03-06 18:01:27,751 - INFO - training batch 951, loss: 0.381, 30432/60000 datapoints
2025-03-06 18:01:27,945 - INFO - training batch 1001, loss: 0.249, 32032/60000 datapoints
2025-03-06 18:01:28,138 - INFO - training batch 1051, loss: 0.605, 33632/60000 datapoints
2025-03-06 18:01:28,330 - INFO - training batch 1101, loss: 0.493, 35232/60000 datapoints
2025-03-06 18:01:28,526 - INFO - training batch 1151, loss: 0.778, 36832/60000 datapoints
2025-03-06 18:01:28,721 - INFO - training batch 1201, loss: 0.450, 38432/60000 datapoints
2025-03-06 18:01:28,916 - INFO - training batch 1251, loss: 0.495, 40032/60000 datapoints
2025-03-06 18:01:29,113 - INFO - training batch 1301, loss: 0.221, 41632/60000 datapoints
2025-03-06 18:01:29,308 - INFO - training batch 1351, loss: 0.651, 43232/60000 datapoints
2025-03-06 18:01:29,502 - INFO - training batch 1401, loss: 0.274, 44832/60000 datapoints
2025-03-06 18:01:29,697 - INFO - training batch 1451, loss: 0.586, 46432/60000 datapoints
2025-03-06 18:01:29,892 - INFO - training batch 1501, loss: 0.459, 48032/60000 datapoints
2025-03-06 18:01:30,086 - INFO - training batch 1551, loss: 0.418, 49632/60000 datapoints
2025-03-06 18:01:30,281 - INFO - training batch 1601, loss: 0.365, 51232/60000 datapoints
2025-03-06 18:01:30,478 - INFO - training batch 1651, loss: 0.509, 52832/60000 datapoints
2025-03-06 18:01:30,677 - INFO - training batch 1701, loss: 0.300, 54432/60000 datapoints
2025-03-06 18:01:30,872 - INFO - training batch 1751, loss: 0.310, 56032/60000 datapoints
2025-03-06 18:01:31,071 - INFO - training batch 1801, loss: 0.501, 57632/60000 datapoints
2025-03-06 18:01:31,262 - INFO - training batch 1851, loss: 0.619, 59232/60000 datapoints
2025-03-06 18:01:31,364 - INFO - validation batch 1, loss: 0.547, 32/10016 datapoints
2025-03-06 18:01:31,520 - INFO - validation batch 51, loss: 0.298, 1632/10016 datapoints
2025-03-06 18:01:31,675 - INFO - validation batch 101, loss: 0.318, 3232/10016 datapoints
2025-03-06 18:01:31,827 - INFO - validation batch 151, loss: 0.683, 4832/10016 datapoints
2025-03-06 18:01:31,981 - INFO - validation batch 201, loss: 0.623, 6432/10016 datapoints
2025-03-06 18:01:32,133 - INFO - validation batch 251, loss: 0.295, 8032/10016 datapoints
2025-03-06 18:01:32,283 - INFO - validation batch 301, loss: 0.384, 9632/10016 datapoints
2025-03-06 18:01:32,321 - INFO - Epoch 126/800 done.
2025-03-06 18:01:32,321 - INFO - Final validation performance:
Loss: 0.450, top-1 acc: 0.887top-5 acc: 0.887
2025-03-06 18:01:32,322 - INFO - Beginning epoch 127/800
2025-03-06 18:01:32,327 - INFO - training batch 1, loss: 0.317, 32/60000 datapoints
2025-03-06 18:01:32,523 - INFO - training batch 51, loss: 0.414, 1632/60000 datapoints
2025-03-06 18:01:32,717 - INFO - training batch 101, loss: 0.553, 3232/60000 datapoints
2025-03-06 18:01:32,910 - INFO - training batch 151, loss: 0.359, 4832/60000 datapoints
2025-03-06 18:01:33,107 - INFO - training batch 201, loss: 0.334, 6432/60000 datapoints
2025-03-06 18:01:33,303 - INFO - training batch 251, loss: 0.569, 8032/60000 datapoints
2025-03-06 18:01:33,496 - INFO - training batch 301, loss: 0.292, 9632/60000 datapoints
2025-03-06 18:01:33,706 - INFO - training batch 351, loss: 0.508, 11232/60000 datapoints
2025-03-06 18:01:33,920 - INFO - training batch 401, loss: 0.478, 12832/60000 datapoints
2025-03-06 18:01:34,116 - INFO - training batch 451, loss: 0.345, 14432/60000 datapoints
2025-03-06 18:01:34,310 - INFO - training batch 501, loss: 0.478, 16032/60000 datapoints
2025-03-06 18:01:34,504 - INFO - training batch 551, loss: 0.468, 17632/60000 datapoints
2025-03-06 18:01:34,699 - INFO - training batch 601, loss: 0.331, 19232/60000 datapoints
2025-03-06 18:01:34,898 - INFO - training batch 651, loss: 0.524, 20832/60000 datapoints
2025-03-06 18:01:35,096 - INFO - training batch 701, loss: 0.363, 22432/60000 datapoints
2025-03-06 18:01:35,290 - INFO - training batch 751, loss: 0.299, 24032/60000 datapoints
2025-03-06 18:01:35,484 - INFO - training batch 801, loss: 0.434, 25632/60000 datapoints
2025-03-06 18:01:35,680 - INFO - training batch 851, loss: 0.545, 27232/60000 datapoints
2025-03-06 18:01:35,876 - INFO - training batch 901, loss: 0.360, 28832/60000 datapoints
2025-03-06 18:01:36,070 - INFO - training batch 951, loss: 0.592, 30432/60000 datapoints
2025-03-06 18:01:36,267 - INFO - training batch 1001, loss: 0.612, 32032/60000 datapoints
2025-03-06 18:01:36,462 - INFO - training batch 1051, loss: 0.364, 33632/60000 datapoints
2025-03-06 18:01:36,657 - INFO - training batch 1101, loss: 0.380, 35232/60000 datapoints
2025-03-06 18:01:36,853 - INFO - training batch 1151, loss: 0.334, 36832/60000 datapoints
2025-03-06 18:01:37,048 - INFO - training batch 1201, loss: 0.184, 38432/60000 datapoints
2025-03-06 18:01:37,249 - INFO - training batch 1251, loss: 0.749, 40032/60000 datapoints
2025-03-06 18:01:37,444 - INFO - training batch 1301, loss: 0.481, 41632/60000 datapoints
2025-03-06 18:01:37,654 - INFO - training batch 1351, loss: 0.418, 43232/60000 datapoints
2025-03-06 18:01:37,873 - INFO - training batch 1401, loss: 0.389, 44832/60000 datapoints
2025-03-06 18:01:38,069 - INFO - training batch 1451, loss: 0.333, 46432/60000 datapoints
2025-03-06 18:01:38,265 - INFO - training batch 1501, loss: 0.524, 48032/60000 datapoints
2025-03-06 18:01:38,458 - INFO - training batch 1551, loss: 0.371, 49632/60000 datapoints
2025-03-06 18:01:38,654 - INFO - training batch 1601, loss: 0.239, 51232/60000 datapoints
2025-03-06 18:01:38,847 - INFO - training batch 1651, loss: 0.548, 52832/60000 datapoints
2025-03-06 18:01:39,048 - INFO - training batch 1701, loss: 0.645, 54432/60000 datapoints
2025-03-06 18:01:39,244 - INFO - training batch 1751, loss: 0.599, 56032/60000 datapoints
2025-03-06 18:01:39,440 - INFO - training batch 1801, loss: 0.349, 57632/60000 datapoints
2025-03-06 18:01:39,635 - INFO - training batch 1851, loss: 0.474, 59232/60000 datapoints
2025-03-06 18:01:39,736 - INFO - validation batch 1, loss: 0.383, 32/10016 datapoints
2025-03-06 18:01:39,888 - INFO - validation batch 51, loss: 0.460, 1632/10016 datapoints
2025-03-06 18:01:40,042 - INFO - validation batch 101, loss: 0.292, 3232/10016 datapoints
2025-03-06 18:01:40,192 - INFO - validation batch 151, loss: 0.577, 4832/10016 datapoints
2025-03-06 18:01:40,344 - INFO - validation batch 201, loss: 0.383, 6432/10016 datapoints
2025-03-06 18:01:40,496 - INFO - validation batch 251, loss: 0.579, 8032/10016 datapoints
2025-03-06 18:01:40,653 - INFO - validation batch 301, loss: 0.587, 9632/10016 datapoints
2025-03-06 18:01:40,690 - INFO - Epoch 127/800 done.
2025-03-06 18:01:40,691 - INFO - Final validation performance:
Loss: 0.466, top-1 acc: 0.888top-5 acc: 0.888
2025-03-06 18:01:40,691 - INFO - Beginning epoch 128/800
2025-03-06 18:01:40,696 - INFO - training batch 1, loss: 0.347, 32/60000 datapoints
2025-03-06 18:01:40,891 - INFO - training batch 51, loss: 0.330, 1632/60000 datapoints
2025-03-06 18:01:41,092 - INFO - training batch 101, loss: 0.411, 3232/60000 datapoints
2025-03-06 18:01:41,286 - INFO - training batch 151, loss: 0.258, 4832/60000 datapoints
2025-03-06 18:01:41,480 - INFO - training batch 201, loss: 0.391, 6432/60000 datapoints
2025-03-06 18:01:41,675 - INFO - training batch 251, loss: 0.687, 8032/60000 datapoints
2025-03-06 18:01:41,871 - INFO - training batch 301, loss: 0.409, 9632/60000 datapoints
2025-03-06 18:01:42,066 - INFO - training batch 351, loss: 0.528, 11232/60000 datapoints
2025-03-06 18:01:42,261 - INFO - training batch 401, loss: 0.798, 12832/60000 datapoints
2025-03-06 18:01:42,455 - INFO - training batch 451, loss: 0.325, 14432/60000 datapoints
2025-03-06 18:01:42,651 - INFO - training batch 501, loss: 0.409, 16032/60000 datapoints
2025-03-06 18:01:42,844 - INFO - training batch 551, loss: 0.382, 17632/60000 datapoints
2025-03-06 18:01:43,039 - INFO - training batch 601, loss: 0.316, 19232/60000 datapoints
2025-03-06 18:01:43,239 - INFO - training batch 651, loss: 0.474, 20832/60000 datapoints
2025-03-06 18:01:43,432 - INFO - training batch 701, loss: 0.447, 22432/60000 datapoints
2025-03-06 18:01:43,628 - INFO - training batch 751, loss: 0.656, 24032/60000 datapoints
2025-03-06 18:01:43,823 - INFO - training batch 801, loss: 0.566, 25632/60000 datapoints
2025-03-06 18:01:44,017 - INFO - training batch 851, loss: 0.402, 27232/60000 datapoints
2025-03-06 18:01:44,213 - INFO - training batch 901, loss: 0.327, 28832/60000 datapoints
2025-03-06 18:01:44,408 - INFO - training batch 951, loss: 0.485, 30432/60000 datapoints
2025-03-06 18:01:44,604 - INFO - training batch 1001, loss: 0.394, 32032/60000 datapoints
2025-03-06 18:01:44,799 - INFO - training batch 1051, loss: 0.696, 33632/60000 datapoints
2025-03-06 18:01:44,997 - INFO - training batch 1101, loss: 0.414, 35232/60000 datapoints
2025-03-06 18:01:45,193 - INFO - training batch 1151, loss: 0.308, 36832/60000 datapoints
2025-03-06 18:01:45,386 - INFO - training batch 1201, loss: 0.543, 38432/60000 datapoints
2025-03-06 18:01:45,579 - INFO - training batch 1251, loss: 0.573, 40032/60000 datapoints
2025-03-06 18:01:45,777 - INFO - training batch 1301, loss: 0.882, 41632/60000 datapoints
2025-03-06 18:01:45,971 - INFO - training batch 1351, loss: 0.585, 43232/60000 datapoints
2025-03-06 18:01:46,167 - INFO - training batch 1401, loss: 0.509, 44832/60000 datapoints
2025-03-06 18:01:46,361 - INFO - training batch 1451, loss: 0.659, 46432/60000 datapoints
2025-03-06 18:01:46,554 - INFO - training batch 1501, loss: 0.308, 48032/60000 datapoints
2025-03-06 18:01:46,752 - INFO - training batch 1551, loss: 0.400, 49632/60000 datapoints
2025-03-06 18:01:46,947 - INFO - training batch 1601, loss: 0.310, 51232/60000 datapoints
2025-03-06 18:01:47,143 - INFO - training batch 1651, loss: 0.274, 52832/60000 datapoints
2025-03-06 18:01:47,340 - INFO - training batch 1701, loss: 0.264, 54432/60000 datapoints
2025-03-06 18:01:47,533 - INFO - training batch 1751, loss: 0.556, 56032/60000 datapoints
2025-03-06 18:01:47,731 - INFO - training batch 1801, loss: 0.455, 57632/60000 datapoints
2025-03-06 18:01:47,942 - INFO - training batch 1851, loss: 0.373, 59232/60000 datapoints
2025-03-06 18:01:48,043 - INFO - validation batch 1, loss: 0.547, 32/10016 datapoints
2025-03-06 18:01:48,199 - INFO - validation batch 51, loss: 0.583, 1632/10016 datapoints
2025-03-06 18:01:48,369 - INFO - validation batch 101, loss: 0.366, 3232/10016 datapoints
2025-03-06 18:01:48,521 - INFO - validation batch 151, loss: 0.389, 4832/10016 datapoints
2025-03-06 18:01:48,676 - INFO - validation batch 201, loss: 0.272, 6432/10016 datapoints
2025-03-06 18:01:48,827 - INFO - validation batch 251, loss: 0.375, 8032/10016 datapoints
2025-03-06 18:01:48,982 - INFO - validation batch 301, loss: 0.289, 9632/10016 datapoints
2025-03-06 18:01:49,020 - INFO - Epoch 128/800 done.
2025-03-06 18:01:49,021 - INFO - Final validation performance:
Loss: 0.403, top-1 acc: 0.888top-5 acc: 0.888
2025-03-06 18:01:49,021 - INFO - Beginning epoch 129/800
2025-03-06 18:01:49,026 - INFO - training batch 1, loss: 0.417, 32/60000 datapoints
2025-03-06 18:01:49,227 - INFO - training batch 51, loss: 0.574, 1632/60000 datapoints
2025-03-06 18:01:49,425 - INFO - training batch 101, loss: 0.548, 3232/60000 datapoints
2025-03-06 18:01:49,623 - INFO - training batch 151, loss: 0.406, 4832/60000 datapoints
2025-03-06 18:01:49,817 - INFO - training batch 201, loss: 0.279, 6432/60000 datapoints
2025-03-06 18:01:50,013 - INFO - training batch 251, loss: 0.359, 8032/60000 datapoints
2025-03-06 18:01:50,209 - INFO - training batch 301, loss: 0.384, 9632/60000 datapoints
2025-03-06 18:01:50,402 - INFO - training batch 351, loss: 0.453, 11232/60000 datapoints
2025-03-06 18:01:50,597 - INFO - training batch 401, loss: 0.447, 12832/60000 datapoints
2025-03-06 18:01:50,790 - INFO - training batch 451, loss: 0.452, 14432/60000 datapoints
2025-03-06 18:01:50,983 - INFO - training batch 501, loss: 0.709, 16032/60000 datapoints
2025-03-06 18:01:51,180 - INFO - training batch 551, loss: 0.320, 17632/60000 datapoints
2025-03-06 18:01:51,372 - INFO - training batch 601, loss: 0.733, 19232/60000 datapoints
2025-03-06 18:01:51,566 - INFO - training batch 651, loss: 0.412, 20832/60000 datapoints
2025-03-06 18:01:51,763 - INFO - training batch 701, loss: 0.404, 22432/60000 datapoints
2025-03-06 18:01:51,958 - INFO - training batch 751, loss: 0.405, 24032/60000 datapoints
2025-03-06 18:01:52,151 - INFO - training batch 801, loss: 0.301, 25632/60000 datapoints
2025-03-06 18:01:52,344 - INFO - training batch 851, loss: 0.403, 27232/60000 datapoints
2025-03-06 18:01:52,536 - INFO - training batch 901, loss: 0.360, 28832/60000 datapoints
2025-03-06 18:01:52,732 - INFO - training batch 951, loss: 0.428, 30432/60000 datapoints
2025-03-06 18:01:52,925 - INFO - training batch 1001, loss: 0.333, 32032/60000 datapoints
2025-03-06 18:01:53,121 - INFO - training batch 1051, loss: 0.471, 33632/60000 datapoints
2025-03-06 18:01:53,313 - INFO - training batch 1101, loss: 0.473, 35232/60000 datapoints
2025-03-06 18:01:53,508 - INFO - training batch 1151, loss: 0.228, 36832/60000 datapoints
2025-03-06 18:01:53,705 - INFO - training batch 1201, loss: 0.400, 38432/60000 datapoints
2025-03-06 18:01:53,898 - INFO - training batch 1251, loss: 0.400, 40032/60000 datapoints
2025-03-06 18:01:54,093 - INFO - training batch 1301, loss: 0.381, 41632/60000 datapoints
2025-03-06 18:01:54,287 - INFO - training batch 1351, loss: 0.437, 43232/60000 datapoints
2025-03-06 18:01:54,482 - INFO - training batch 1401, loss: 0.554, 44832/60000 datapoints
2025-03-06 18:01:54,677 - INFO - training batch 1451, loss: 0.497, 46432/60000 datapoints
2025-03-06 18:01:54,873 - INFO - training batch 1501, loss: 0.719, 48032/60000 datapoints
2025-03-06 18:01:55,068 - INFO - training batch 1551, loss: 0.472, 49632/60000 datapoints
2025-03-06 18:01:55,264 - INFO - training batch 1601, loss: 0.358, 51232/60000 datapoints
2025-03-06 18:01:55,458 - INFO - training batch 1651, loss: 0.396, 52832/60000 datapoints
2025-03-06 18:01:55,655 - INFO - training batch 1701, loss: 0.454, 54432/60000 datapoints
2025-03-06 18:01:55,851 - INFO - training batch 1751, loss: 0.448, 56032/60000 datapoints
2025-03-06 18:01:56,045 - INFO - training batch 1801, loss: 0.524, 57632/60000 datapoints
2025-03-06 18:01:56,242 - INFO - training batch 1851, loss: 0.432, 59232/60000 datapoints
2025-03-06 18:01:56,343 - INFO - validation batch 1, loss: 0.654, 32/10016 datapoints
2025-03-06 18:01:56,497 - INFO - validation batch 51, loss: 0.512, 1632/10016 datapoints
2025-03-06 18:01:56,650 - INFO - validation batch 101, loss: 0.351, 3232/10016 datapoints
2025-03-06 18:01:56,803 - INFO - validation batch 151, loss: 0.402, 4832/10016 datapoints
2025-03-06 18:01:56,956 - INFO - validation batch 201, loss: 0.596, 6432/10016 datapoints
2025-03-06 18:01:57,109 - INFO - validation batch 251, loss: 0.411, 8032/10016 datapoints
2025-03-06 18:01:57,267 - INFO - validation batch 301, loss: 0.441, 9632/10016 datapoints
2025-03-06 18:01:57,304 - INFO - Epoch 129/800 done.
2025-03-06 18:01:57,304 - INFO - Final validation performance:
Loss: 0.481, top-1 acc: 0.888top-5 acc: 0.888
2025-03-06 18:01:57,305 - INFO - Beginning epoch 130/800
2025-03-06 18:01:57,311 - INFO - training batch 1, loss: 0.378, 32/60000 datapoints
2025-03-06 18:01:57,503 - INFO - training batch 51, loss: 0.463, 1632/60000 datapoints
2025-03-06 18:01:57,695 - INFO - training batch 101, loss: 0.384, 3232/60000 datapoints
2025-03-06 18:01:57,893 - INFO - training batch 151, loss: 0.335, 4832/60000 datapoints
2025-03-06 18:01:58,097 - INFO - training batch 201, loss: 0.273, 6432/60000 datapoints
2025-03-06 18:01:58,290 - INFO - training batch 251, loss: 0.542, 8032/60000 datapoints
2025-03-06 18:01:58,483 - INFO - training batch 301, loss: 0.432, 9632/60000 datapoints
2025-03-06 18:01:58,677 - INFO - training batch 351, loss: 0.498, 11232/60000 datapoints
2025-03-06 18:01:58,869 - INFO - training batch 401, loss: 0.333, 12832/60000 datapoints
2025-03-06 18:01:59,060 - INFO - training batch 451, loss: 0.492, 14432/60000 datapoints
2025-03-06 18:01:59,255 - INFO - training batch 501, loss: 0.629, 16032/60000 datapoints
2025-03-06 18:01:59,447 - INFO - training batch 551, loss: 0.364, 17632/60000 datapoints
2025-03-06 18:01:59,642 - INFO - training batch 601, loss: 0.260, 19232/60000 datapoints
2025-03-06 18:01:59,853 - INFO - training batch 651, loss: 0.468, 20832/60000 datapoints
2025-03-06 18:02:00,048 - INFO - training batch 701, loss: 0.444, 22432/60000 datapoints
2025-03-06 18:02:00,239 - INFO - training batch 751, loss: 0.520, 24032/60000 datapoints
2025-03-06 18:02:00,438 - INFO - training batch 801, loss: 0.471, 25632/60000 datapoints
2025-03-06 18:02:00,639 - INFO - training batch 851, loss: 0.545, 27232/60000 datapoints
2025-03-06 18:02:00,839 - INFO - training batch 901, loss: 0.417, 28832/60000 datapoints
2025-03-06 18:02:01,032 - INFO - training batch 951, loss: 0.467, 30432/60000 datapoints
2025-03-06 18:02:01,225 - INFO - training batch 1001, loss: 0.448, 32032/60000 datapoints
2025-03-06 18:02:01,417 - INFO - training batch 1051, loss: 0.418, 33632/60000 datapoints
2025-03-06 18:02:01,617 - INFO - training batch 1101, loss: 0.292, 35232/60000 datapoints
2025-03-06 18:02:01,809 - INFO - training batch 1151, loss: 0.446, 36832/60000 datapoints
2025-03-06 18:02:02,000 - INFO - training batch 1201, loss: 0.621, 38432/60000 datapoints
2025-03-06 18:02:02,191 - INFO - training batch 1251, loss: 0.432, 40032/60000 datapoints
2025-03-06 18:02:02,384 - INFO - training batch 1301, loss: 0.341, 41632/60000 datapoints
2025-03-06 18:02:02,576 - INFO - training batch 1351, loss: 0.483, 43232/60000 datapoints
2025-03-06 18:02:02,768 - INFO - training batch 1401, loss: 0.465, 44832/60000 datapoints
2025-03-06 18:02:02,966 - INFO - training batch 1451, loss: 0.426, 46432/60000 datapoints
2025-03-06 18:02:03,164 - INFO - training batch 1501, loss: 0.534, 48032/60000 datapoints
2025-03-06 18:02:03,359 - INFO - training batch 1551, loss: 0.331, 49632/60000 datapoints
2025-03-06 18:02:03,554 - INFO - training batch 1601, loss: 0.526, 51232/60000 datapoints
2025-03-06 18:02:03,748 - INFO - training batch 1651, loss: 0.694, 52832/60000 datapoints
2025-03-06 18:02:03,942 - INFO - training batch 1701, loss: 0.273, 54432/60000 datapoints
2025-03-06 18:02:04,136 - INFO - training batch 1751, loss: 0.409, 56032/60000 datapoints
2025-03-06 18:02:04,332 - INFO - training batch 1801, loss: 0.512, 57632/60000 datapoints
2025-03-06 18:02:04,528 - INFO - training batch 1851, loss: 0.453, 59232/60000 datapoints
2025-03-06 18:02:04,632 - INFO - validation batch 1, loss: 0.245, 32/10016 datapoints
2025-03-06 18:02:04,784 - INFO - validation batch 51, loss: 0.324, 1632/10016 datapoints
2025-03-06 18:02:04,942 - INFO - validation batch 101, loss: 0.287, 3232/10016 datapoints
2025-03-06 18:02:05,094 - INFO - validation batch 151, loss: 0.304, 4832/10016 datapoints
2025-03-06 18:02:05,248 - INFO - validation batch 201, loss: 0.517, 6432/10016 datapoints
2025-03-06 18:02:05,401 - INFO - validation batch 251, loss: 0.216, 8032/10016 datapoints
2025-03-06 18:02:05,553 - INFO - validation batch 301, loss: 0.465, 9632/10016 datapoints
2025-03-06 18:02:05,590 - INFO - Epoch 130/800 done.
2025-03-06 18:02:05,591 - INFO - Final validation performance:
Loss: 0.337, top-1 acc: 0.889top-5 acc: 0.889
2025-03-06 18:02:05,591 - INFO - Beginning epoch 131/800
2025-03-06 18:02:05,599 - INFO - training batch 1, loss: 0.363, 32/60000 datapoints
2025-03-06 18:02:05,810 - INFO - training batch 51, loss: 0.341, 1632/60000 datapoints
2025-03-06 18:02:06,027 - INFO - training batch 101, loss: 0.365, 3232/60000 datapoints
2025-03-06 18:02:06,251 - INFO - training batch 151, loss: 0.483, 4832/60000 datapoints
2025-03-06 18:02:06,450 - INFO - training batch 201, loss: 0.335, 6432/60000 datapoints
2025-03-06 18:02:06,645 - INFO - training batch 251, loss: 0.528, 8032/60000 datapoints
2025-03-06 18:02:06,839 - INFO - training batch 301, loss: 0.426, 9632/60000 datapoints
2025-03-06 18:02:07,034 - INFO - training batch 351, loss: 0.378, 11232/60000 datapoints
2025-03-06 18:02:07,232 - INFO - training batch 401, loss: 0.452, 12832/60000 datapoints
2025-03-06 18:02:07,431 - INFO - training batch 451, loss: 0.464, 14432/60000 datapoints
2025-03-06 18:02:07,631 - INFO - training batch 501, loss: 0.421, 16032/60000 datapoints
2025-03-06 18:02:07,824 - INFO - training batch 551, loss: 0.319, 17632/60000 datapoints
2025-03-06 18:02:08,040 - INFO - training batch 601, loss: 0.326, 19232/60000 datapoints
2025-03-06 18:02:08,237 - INFO - training batch 651, loss: 0.805, 20832/60000 datapoints
2025-03-06 18:02:08,432 - INFO - training batch 701, loss: 0.330, 22432/60000 datapoints
2025-03-06 18:02:08,628 - INFO - training batch 751, loss: 0.374, 24032/60000 datapoints
2025-03-06 18:02:08,822 - INFO - training batch 801, loss: 0.344, 25632/60000 datapoints
2025-03-06 18:02:09,020 - INFO - training batch 851, loss: 0.302, 27232/60000 datapoints
2025-03-06 18:02:09,218 - INFO - training batch 901, loss: 0.566, 28832/60000 datapoints
2025-03-06 18:02:09,413 - INFO - training batch 951, loss: 0.462, 30432/60000 datapoints
2025-03-06 18:02:09,609 - INFO - training batch 1001, loss: 0.330, 32032/60000 datapoints
2025-03-06 18:02:09,803 - INFO - training batch 1051, loss: 0.343, 33632/60000 datapoints
2025-03-06 18:02:10,001 - INFO - training batch 1101, loss: 0.382, 35232/60000 datapoints
2025-03-06 18:02:10,193 - INFO - training batch 1151, loss: 0.341, 36832/60000 datapoints
2025-03-06 18:02:10,388 - INFO - training batch 1201, loss: 0.472, 38432/60000 datapoints
2025-03-06 18:02:10,580 - INFO - training batch 1251, loss: 0.427, 40032/60000 datapoints
2025-03-06 18:02:10,777 - INFO - training batch 1301, loss: 0.569, 41632/60000 datapoints
2025-03-06 18:02:10,972 - INFO - training batch 1351, loss: 0.274, 43232/60000 datapoints
2025-03-06 18:02:11,168 - INFO - training batch 1401, loss: 0.282, 44832/60000 datapoints
2025-03-06 18:02:11,362 - INFO - training batch 1451, loss: 0.437, 46432/60000 datapoints
2025-03-06 18:02:11,555 - INFO - training batch 1501, loss: 0.298, 48032/60000 datapoints
2025-03-06 18:02:11,751 - INFO - training batch 1551, loss: 0.585, 49632/60000 datapoints
2025-03-06 18:02:11,944 - INFO - training batch 1601, loss: 0.300, 51232/60000 datapoints
2025-03-06 18:02:12,136 - INFO - training batch 1651, loss: 0.261, 52832/60000 datapoints
2025-03-06 18:02:12,330 - INFO - training batch 1701, loss: 0.834, 54432/60000 datapoints
2025-03-06 18:02:12,523 - INFO - training batch 1751, loss: 0.247, 56032/60000 datapoints
2025-03-06 18:02:12,718 - INFO - training batch 1801, loss: 0.340, 57632/60000 datapoints
2025-03-06 18:02:12,912 - INFO - training batch 1851, loss: 0.281, 59232/60000 datapoints
2025-03-06 18:02:13,013 - INFO - validation batch 1, loss: 0.543, 32/10016 datapoints
2025-03-06 18:02:13,166 - INFO - validation batch 51, loss: 0.487, 1632/10016 datapoints
2025-03-06 18:02:13,320 - INFO - validation batch 101, loss: 0.286, 3232/10016 datapoints
2025-03-06 18:02:13,471 - INFO - validation batch 151, loss: 0.695, 4832/10016 datapoints
2025-03-06 18:02:13,625 - INFO - validation batch 201, loss: 0.316, 6432/10016 datapoints
2025-03-06 18:02:13,777 - INFO - validation batch 251, loss: 0.308, 8032/10016 datapoints
2025-03-06 18:02:13,932 - INFO - validation batch 301, loss: 0.235, 9632/10016 datapoints
2025-03-06 18:02:13,970 - INFO - Epoch 131/800 done.
2025-03-06 18:02:13,970 - INFO - Final validation performance:
Loss: 0.410, top-1 acc: 0.889top-5 acc: 0.889
2025-03-06 18:02:13,970 - INFO - Beginning epoch 132/800
2025-03-06 18:02:13,976 - INFO - training batch 1, loss: 0.738, 32/60000 datapoints
2025-03-06 18:02:14,170 - INFO - training batch 51, loss: 0.308, 1632/60000 datapoints
2025-03-06 18:02:14,364 - INFO - training batch 101, loss: 0.667, 3232/60000 datapoints
2025-03-06 18:02:14,561 - INFO - training batch 151, loss: 0.462, 4832/60000 datapoints
2025-03-06 18:02:14,762 - INFO - training batch 201, loss: 0.524, 6432/60000 datapoints
2025-03-06 18:02:14,963 - INFO - training batch 251, loss: 0.245, 8032/60000 datapoints
2025-03-06 18:02:15,156 - INFO - training batch 301, loss: 0.468, 9632/60000 datapoints
2025-03-06 18:02:15,352 - INFO - training batch 351, loss: 0.488, 11232/60000 datapoints
2025-03-06 18:02:15,548 - INFO - training batch 401, loss: 0.358, 12832/60000 datapoints
2025-03-06 18:02:15,746 - INFO - training batch 451, loss: 0.333, 14432/60000 datapoints
2025-03-06 18:02:15,942 - INFO - training batch 501, loss: 0.713, 16032/60000 datapoints
2025-03-06 18:02:16,136 - INFO - training batch 551, loss: 0.753, 17632/60000 datapoints
2025-03-06 18:02:16,331 - INFO - training batch 601, loss: 0.433, 19232/60000 datapoints
2025-03-06 18:02:16,528 - INFO - training batch 651, loss: 0.226, 20832/60000 datapoints
2025-03-06 18:02:16,729 - INFO - training batch 701, loss: 0.338, 22432/60000 datapoints
2025-03-06 18:02:16,924 - INFO - training batch 751, loss: 0.334, 24032/60000 datapoints
2025-03-06 18:02:17,120 - INFO - training batch 801, loss: 0.440, 25632/60000 datapoints
2025-03-06 18:02:17,318 - INFO - training batch 851, loss: 0.384, 27232/60000 datapoints
2025-03-06 18:02:17,513 - INFO - training batch 901, loss: 0.670, 28832/60000 datapoints
2025-03-06 18:02:17,710 - INFO - training batch 951, loss: 0.211, 30432/60000 datapoints
2025-03-06 18:02:17,903 - INFO - training batch 1001, loss: 0.579, 32032/60000 datapoints
2025-03-06 18:02:18,120 - INFO - training batch 1051, loss: 0.311, 33632/60000 datapoints
2025-03-06 18:02:18,312 - INFO - training batch 1101, loss: 0.717, 35232/60000 datapoints
2025-03-06 18:02:18,505 - INFO - training batch 1151, loss: 0.318, 36832/60000 datapoints
2025-03-06 18:02:18,700 - INFO - training batch 1201, loss: 0.361, 38432/60000 datapoints
2025-03-06 18:02:18,894 - INFO - training batch 1251, loss: 0.431, 40032/60000 datapoints
2025-03-06 18:02:19,088 - INFO - training batch 1301, loss: 0.455, 41632/60000 datapoints
2025-03-06 18:02:19,284 - INFO - training batch 1351, loss: 0.468, 43232/60000 datapoints
2025-03-06 18:02:19,477 - INFO - training batch 1401, loss: 0.395, 44832/60000 datapoints
2025-03-06 18:02:19,674 - INFO - training batch 1451, loss: 0.645, 46432/60000 datapoints
2025-03-06 18:02:19,872 - INFO - training batch 1501, loss: 0.280, 48032/60000 datapoints
2025-03-06 18:02:20,068 - INFO - training batch 1551, loss: 0.298, 49632/60000 datapoints
2025-03-06 18:02:20,263 - INFO - training batch 1601, loss: 0.436, 51232/60000 datapoints
2025-03-06 18:02:20,457 - INFO - training batch 1651, loss: 0.299, 52832/60000 datapoints
2025-03-06 18:02:20,653 - INFO - training batch 1701, loss: 0.522, 54432/60000 datapoints
2025-03-06 18:02:20,845 - INFO - training batch 1751, loss: 0.450, 56032/60000 datapoints
2025-03-06 18:02:21,041 - INFO - training batch 1801, loss: 0.303, 57632/60000 datapoints
2025-03-06 18:02:21,236 - INFO - training batch 1851, loss: 0.478, 59232/60000 datapoints
2025-03-06 18:02:21,335 - INFO - validation batch 1, loss: 0.428, 32/10016 datapoints
2025-03-06 18:02:21,487 - INFO - validation batch 51, loss: 0.407, 1632/10016 datapoints
2025-03-06 18:02:21,648 - INFO - validation batch 101, loss: 0.259, 3232/10016 datapoints
2025-03-06 18:02:21,804 - INFO - validation batch 151, loss: 0.726, 4832/10016 datapoints
2025-03-06 18:02:21,954 - INFO - validation batch 201, loss: 0.314, 6432/10016 datapoints
2025-03-06 18:02:22,106 - INFO - validation batch 251, loss: 0.443, 8032/10016 datapoints
2025-03-06 18:02:22,258 - INFO - validation batch 301, loss: 0.428, 9632/10016 datapoints
2025-03-06 18:02:22,294 - INFO - Epoch 132/800 done.
2025-03-06 18:02:22,294 - INFO - Final validation performance:
Loss: 0.429, top-1 acc: 0.890top-5 acc: 0.890
2025-03-06 18:02:22,294 - INFO - Beginning epoch 133/800
2025-03-06 18:02:22,302 - INFO - training batch 1, loss: 0.337, 32/60000 datapoints
2025-03-06 18:02:22,497 - INFO - training batch 51, loss: 0.264, 1632/60000 datapoints
2025-03-06 18:02:22,713 - INFO - training batch 101, loss: 0.640, 3232/60000 datapoints
2025-03-06 18:02:22,912 - INFO - training batch 151, loss: 0.944, 4832/60000 datapoints
2025-03-06 18:02:23,105 - INFO - training batch 201, loss: 0.359, 6432/60000 datapoints
2025-03-06 18:02:23,306 - INFO - training batch 251, loss: 0.480, 8032/60000 datapoints
2025-03-06 18:02:23,505 - INFO - training batch 301, loss: 0.564, 9632/60000 datapoints
2025-03-06 18:02:23,703 - INFO - training batch 351, loss: 0.512, 11232/60000 datapoints
2025-03-06 18:02:23,896 - INFO - training batch 401, loss: 0.391, 12832/60000 datapoints
2025-03-06 18:02:24,092 - INFO - training batch 451, loss: 0.597, 14432/60000 datapoints
2025-03-06 18:02:24,285 - INFO - training batch 501, loss: 0.374, 16032/60000 datapoints
2025-03-06 18:02:24,480 - INFO - training batch 551, loss: 0.243, 17632/60000 datapoints
2025-03-06 18:02:24,678 - INFO - training batch 601, loss: 0.589, 19232/60000 datapoints
2025-03-06 18:02:24,886 - INFO - training batch 651, loss: 0.498, 20832/60000 datapoints
2025-03-06 18:02:25,084 - INFO - training batch 701, loss: 0.675, 22432/60000 datapoints
2025-03-06 18:02:25,280 - INFO - training batch 751, loss: 0.315, 24032/60000 datapoints
2025-03-06 18:02:25,475 - INFO - training batch 801, loss: 0.358, 25632/60000 datapoints
2025-03-06 18:02:25,674 - INFO - training batch 851, loss: 0.276, 27232/60000 datapoints
2025-03-06 18:02:25,867 - INFO - training batch 901, loss: 0.431, 28832/60000 datapoints
2025-03-06 18:02:26,062 - INFO - training batch 951, loss: 0.465, 30432/60000 datapoints
2025-03-06 18:02:26,258 - INFO - training batch 1001, loss: 0.677, 32032/60000 datapoints
2025-03-06 18:02:26,452 - INFO - training batch 1051, loss: 0.508, 33632/60000 datapoints
2025-03-06 18:02:26,651 - INFO - training batch 1101, loss: 0.401, 35232/60000 datapoints
2025-03-06 18:02:26,859 - INFO - training batch 1151, loss: 0.458, 36832/60000 datapoints
2025-03-06 18:02:27,051 - INFO - training batch 1201, loss: 0.414, 38432/60000 datapoints
2025-03-06 18:02:27,248 - INFO - training batch 1251, loss: 0.299, 40032/60000 datapoints
2025-03-06 18:02:27,444 - INFO - training batch 1301, loss: 0.813, 41632/60000 datapoints
2025-03-06 18:02:27,642 - INFO - training batch 1351, loss: 0.598, 43232/60000 datapoints
2025-03-06 18:02:27,834 - INFO - training batch 1401, loss: 0.385, 44832/60000 datapoints
2025-03-06 18:02:28,029 - INFO - training batch 1451, loss: 0.485, 46432/60000 datapoints
2025-03-06 18:02:28,240 - INFO - training batch 1501, loss: 0.370, 48032/60000 datapoints
2025-03-06 18:02:28,433 - INFO - training batch 1551, loss: 0.621, 49632/60000 datapoints
2025-03-06 18:02:28,630 - INFO - training batch 1601, loss: 0.390, 51232/60000 datapoints
2025-03-06 18:02:28,824 - INFO - training batch 1651, loss: 0.785, 52832/60000 datapoints
2025-03-06 18:02:29,018 - INFO - training batch 1701, loss: 0.461, 54432/60000 datapoints
2025-03-06 18:02:29,218 - INFO - training batch 1751, loss: 0.377, 56032/60000 datapoints
2025-03-06 18:02:29,416 - INFO - training batch 1801, loss: 0.607, 57632/60000 datapoints
2025-03-06 18:02:29,609 - INFO - training batch 1851, loss: 0.453, 59232/60000 datapoints
2025-03-06 18:02:29,712 - INFO - validation batch 1, loss: 0.205, 32/10016 datapoints
2025-03-06 18:02:29,863 - INFO - validation batch 51, loss: 0.533, 1632/10016 datapoints
2025-03-06 18:02:30,015 - INFO - validation batch 101, loss: 0.358, 3232/10016 datapoints
2025-03-06 18:02:30,168 - INFO - validation batch 151, loss: 0.335, 4832/10016 datapoints
2025-03-06 18:02:30,322 - INFO - validation batch 201, loss: 0.536, 6432/10016 datapoints
2025-03-06 18:02:30,474 - INFO - validation batch 251, loss: 0.435, 8032/10016 datapoints
2025-03-06 18:02:30,629 - INFO - validation batch 301, loss: 0.288, 9632/10016 datapoints
2025-03-06 18:02:30,676 - INFO - Epoch 133/800 done.
2025-03-06 18:02:30,678 - INFO - Final validation performance:
Loss: 0.384, top-1 acc: 0.890top-5 acc: 0.890
2025-03-06 18:02:30,679 - INFO - Beginning epoch 134/800
2025-03-06 18:02:30,684 - INFO - training batch 1, loss: 0.652, 32/60000 datapoints
2025-03-06 18:02:30,887 - INFO - training batch 51, loss: 0.623, 1632/60000 datapoints
2025-03-06 18:02:31,093 - INFO - training batch 101, loss: 0.477, 3232/60000 datapoints
2025-03-06 18:02:31,291 - INFO - training batch 151, loss: 0.512, 4832/60000 datapoints
2025-03-06 18:02:31,486 - INFO - training batch 201, loss: 0.516, 6432/60000 datapoints
2025-03-06 18:02:31,686 - INFO - training batch 251, loss: 0.530, 8032/60000 datapoints
2025-03-06 18:02:31,881 - INFO - training batch 301, loss: 0.453, 9632/60000 datapoints
2025-03-06 18:02:32,076 - INFO - training batch 351, loss: 0.384, 11232/60000 datapoints
2025-03-06 18:02:32,270 - INFO - training batch 401, loss: 0.533, 12832/60000 datapoints
2025-03-06 18:02:32,467 - INFO - training batch 451, loss: 0.348, 14432/60000 datapoints
2025-03-06 18:02:32,662 - INFO - training batch 501, loss: 0.275, 16032/60000 datapoints
2025-03-06 18:02:32,856 - INFO - training batch 551, loss: 0.356, 17632/60000 datapoints
2025-03-06 18:02:33,051 - INFO - training batch 601, loss: 0.539, 19232/60000 datapoints
2025-03-06 18:02:33,249 - INFO - training batch 651, loss: 0.517, 20832/60000 datapoints
2025-03-06 18:02:33,444 - INFO - training batch 701, loss: 0.343, 22432/60000 datapoints
2025-03-06 18:02:33,641 - INFO - training batch 751, loss: 0.617, 24032/60000 datapoints
2025-03-06 18:02:33,836 - INFO - training batch 801, loss: 0.429, 25632/60000 datapoints
2025-03-06 18:02:34,030 - INFO - training batch 851, loss: 0.614, 27232/60000 datapoints
2025-03-06 18:02:34,224 - INFO - training batch 901, loss: 0.452, 28832/60000 datapoints
2025-03-06 18:02:34,418 - INFO - training batch 951, loss: 0.337, 30432/60000 datapoints
2025-03-06 18:02:34,615 - INFO - training batch 1001, loss: 0.590, 32032/60000 datapoints
2025-03-06 18:02:34,810 - INFO - training batch 1051, loss: 0.526, 33632/60000 datapoints
2025-03-06 18:02:35,012 - INFO - training batch 1101, loss: 0.452, 35232/60000 datapoints
2025-03-06 18:02:35,208 - INFO - training batch 1151, loss: 0.182, 36832/60000 datapoints
2025-03-06 18:02:35,404 - INFO - training batch 1201, loss: 0.454, 38432/60000 datapoints
2025-03-06 18:02:35,601 - INFO - training batch 1251, loss: 0.507, 40032/60000 datapoints
2025-03-06 18:02:35,794 - INFO - training batch 1301, loss: 0.306, 41632/60000 datapoints
2025-03-06 18:02:35,989 - INFO - training batch 1351, loss: 0.660, 43232/60000 datapoints
2025-03-06 18:02:36,187 - INFO - training batch 1401, loss: 0.299, 44832/60000 datapoints
2025-03-06 18:02:36,380 - INFO - training batch 1451, loss: 0.688, 46432/60000 datapoints
2025-03-06 18:02:36,587 - INFO - training batch 1501, loss: 0.658, 48032/60000 datapoints
2025-03-06 18:02:36,796 - INFO - training batch 1551, loss: 0.464, 49632/60000 datapoints
2025-03-06 18:02:36,989 - INFO - training batch 1601, loss: 0.556, 51232/60000 datapoints
2025-03-06 18:02:37,182 - INFO - training batch 1651, loss: 0.434, 52832/60000 datapoints
2025-03-06 18:02:37,386 - INFO - training batch 1701, loss: 0.347, 54432/60000 datapoints
2025-03-06 18:02:37,582 - INFO - training batch 1751, loss: 0.294, 56032/60000 datapoints
2025-03-06 18:02:37,795 - INFO - training batch 1801, loss: 0.252, 57632/60000 datapoints
2025-03-06 18:02:37,990 - INFO - training batch 1851, loss: 0.228, 59232/60000 datapoints
2025-03-06 18:02:38,092 - INFO - validation batch 1, loss: 0.296, 32/10016 datapoints
2025-03-06 18:02:38,263 - INFO - validation batch 51, loss: 0.627, 1632/10016 datapoints
2025-03-06 18:02:38,417 - INFO - validation batch 101, loss: 0.483, 3232/10016 datapoints
2025-03-06 18:02:38,568 - INFO - validation batch 151, loss: 0.276, 4832/10016 datapoints
2025-03-06 18:02:38,723 - INFO - validation batch 201, loss: 0.194, 6432/10016 datapoints
2025-03-06 18:02:38,874 - INFO - validation batch 251, loss: 0.552, 8032/10016 datapoints
2025-03-06 18:02:39,029 - INFO - validation batch 301, loss: 0.311, 9632/10016 datapoints
2025-03-06 18:02:39,066 - INFO - Epoch 134/800 done.
2025-03-06 18:02:39,066 - INFO - Final validation performance:
Loss: 0.391, top-1 acc: 0.890top-5 acc: 0.890
2025-03-06 18:02:39,067 - INFO - Beginning epoch 135/800
2025-03-06 18:02:39,072 - INFO - training batch 1, loss: 0.421, 32/60000 datapoints
2025-03-06 18:02:39,271 - INFO - training batch 51, loss: 0.337, 1632/60000 datapoints
2025-03-06 18:02:39,464 - INFO - training batch 101, loss: 0.267, 3232/60000 datapoints
2025-03-06 18:02:39,659 - INFO - training batch 151, loss: 0.600, 4832/60000 datapoints
2025-03-06 18:02:39,853 - INFO - training batch 201, loss: 0.425, 6432/60000 datapoints
2025-03-06 18:02:40,049 - INFO - training batch 251, loss: 0.505, 8032/60000 datapoints
2025-03-06 18:02:40,241 - INFO - training batch 301, loss: 0.219, 9632/60000 datapoints
2025-03-06 18:02:40,435 - INFO - training batch 351, loss: 0.373, 11232/60000 datapoints
2025-03-06 18:02:40,631 - INFO - training batch 401, loss: 0.293, 12832/60000 datapoints
2025-03-06 18:02:40,824 - INFO - training batch 451, loss: 0.512, 14432/60000 datapoints
2025-03-06 18:02:41,017 - INFO - training batch 501, loss: 0.830, 16032/60000 datapoints
2025-03-06 18:02:41,210 - INFO - training batch 551, loss: 0.387, 17632/60000 datapoints
2025-03-06 18:02:41,406 - INFO - training batch 601, loss: 0.245, 19232/60000 datapoints
2025-03-06 18:02:41,603 - INFO - training batch 651, loss: 0.518, 20832/60000 datapoints
2025-03-06 18:02:41,796 - INFO - training batch 701, loss: 0.691, 22432/60000 datapoints
2025-03-06 18:02:41,993 - INFO - training batch 751, loss: 0.317, 24032/60000 datapoints
2025-03-06 18:02:42,186 - INFO - training batch 801, loss: 0.383, 25632/60000 datapoints
2025-03-06 18:02:42,379 - INFO - training batch 851, loss: 0.544, 27232/60000 datapoints
2025-03-06 18:02:42,573 - INFO - training batch 901, loss: 0.378, 28832/60000 datapoints
2025-03-06 18:02:42,766 - INFO - training batch 951, loss: 0.354, 30432/60000 datapoints
2025-03-06 18:02:42,959 - INFO - training batch 1001, loss: 0.373, 32032/60000 datapoints
2025-03-06 18:02:43,152 - INFO - training batch 1051, loss: 0.226, 33632/60000 datapoints
2025-03-06 18:02:43,350 - INFO - training batch 1101, loss: 0.308, 35232/60000 datapoints
2025-03-06 18:02:43,546 - INFO - training batch 1151, loss: 0.179, 36832/60000 datapoints
2025-03-06 18:02:43,744 - INFO - training batch 1201, loss: 0.347, 38432/60000 datapoints
2025-03-06 18:02:43,937 - INFO - training batch 1251, loss: 0.389, 40032/60000 datapoints
2025-03-06 18:02:44,133 - INFO - training batch 1301, loss: 0.714, 41632/60000 datapoints
2025-03-06 18:02:44,328 - INFO - training batch 1351, loss: 0.506, 43232/60000 datapoints
2025-03-06 18:02:44,524 - INFO - training batch 1401, loss: 0.808, 44832/60000 datapoints
2025-03-06 18:02:44,719 - INFO - training batch 1451, loss: 0.377, 46432/60000 datapoints
2025-03-06 18:02:44,918 - INFO - training batch 1501, loss: 0.419, 48032/60000 datapoints
2025-03-06 18:02:45,112 - INFO - training batch 1551, loss: 0.397, 49632/60000 datapoints
2025-03-06 18:02:45,310 - INFO - training batch 1601, loss: 0.323, 51232/60000 datapoints
2025-03-06 18:02:45,504 - INFO - training batch 1651, loss: 0.302, 52832/60000 datapoints
2025-03-06 18:02:45,704 - INFO - training batch 1701, loss: 0.542, 54432/60000 datapoints
2025-03-06 18:02:45,898 - INFO - training batch 1751, loss: 0.668, 56032/60000 datapoints
2025-03-06 18:02:46,090 - INFO - training batch 1801, loss: 0.417, 57632/60000 datapoints
2025-03-06 18:02:46,286 - INFO - training batch 1851, loss: 0.733, 59232/60000 datapoints
2025-03-06 18:02:46,388 - INFO - validation batch 1, loss: 0.290, 32/10016 datapoints
2025-03-06 18:02:46,541 - INFO - validation batch 51, loss: 0.251, 1632/10016 datapoints
2025-03-06 18:02:46,702 - INFO - validation batch 101, loss: 0.511, 3232/10016 datapoints
2025-03-06 18:02:46,855 - INFO - validation batch 151, loss: 0.460, 4832/10016 datapoints
2025-03-06 18:02:47,010 - INFO - validation batch 201, loss: 0.293, 6432/10016 datapoints
2025-03-06 18:02:47,162 - INFO - validation batch 251, loss: 0.781, 8032/10016 datapoints
2025-03-06 18:02:47,318 - INFO - validation batch 301, loss: 0.280, 9632/10016 datapoints
2025-03-06 18:02:47,355 - INFO - Epoch 135/800 done.
2025-03-06 18:02:47,355 - INFO - Final validation performance:
Loss: 0.409, top-1 acc: 0.891top-5 acc: 0.891
2025-03-06 18:02:47,355 - INFO - Beginning epoch 136/800
2025-03-06 18:02:47,362 - INFO - training batch 1, loss: 0.690, 32/60000 datapoints
2025-03-06 18:02:47,557 - INFO - training batch 51, loss: 0.396, 1632/60000 datapoints
2025-03-06 18:02:47,748 - INFO - training batch 101, loss: 0.312, 3232/60000 datapoints
2025-03-06 18:02:47,941 - INFO - training batch 151, loss: 0.532, 4832/60000 datapoints
2025-03-06 18:02:48,132 - INFO - training batch 201, loss: 0.558, 6432/60000 datapoints
2025-03-06 18:02:48,342 - INFO - training batch 251, loss: 0.434, 8032/60000 datapoints
2025-03-06 18:02:48,536 - INFO - training batch 301, loss: 0.431, 9632/60000 datapoints
2025-03-06 18:02:48,727 - INFO - training batch 351, loss: 0.442, 11232/60000 datapoints
2025-03-06 18:02:48,929 - INFO - training batch 401, loss: 0.457, 12832/60000 datapoints
2025-03-06 18:02:49,118 - INFO - training batch 451, loss: 0.361, 14432/60000 datapoints
2025-03-06 18:02:49,311 - INFO - training batch 501, loss: 0.331, 16032/60000 datapoints
2025-03-06 18:02:49,502 - INFO - training batch 551, loss: 0.636, 17632/60000 datapoints
2025-03-06 18:02:49,696 - INFO - training batch 601, loss: 0.540, 19232/60000 datapoints
2025-03-06 18:02:49,886 - INFO - training batch 651, loss: 0.332, 20832/60000 datapoints
2025-03-06 18:02:50,077 - INFO - training batch 701, loss: 0.318, 22432/60000 datapoints
2025-03-06 18:02:50,266 - INFO - training batch 751, loss: 0.355, 24032/60000 datapoints
2025-03-06 18:02:50,458 - INFO - training batch 801, loss: 0.463, 25632/60000 datapoints
2025-03-06 18:02:50,654 - INFO - training batch 851, loss: 0.553, 27232/60000 datapoints
2025-03-06 18:02:50,846 - INFO - training batch 901, loss: 0.254, 28832/60000 datapoints
2025-03-06 18:02:51,036 - INFO - training batch 951, loss: 0.455, 30432/60000 datapoints
2025-03-06 18:02:51,228 - INFO - training batch 1001, loss: 0.574, 32032/60000 datapoints
2025-03-06 18:02:51,420 - INFO - training batch 1051, loss: 0.272, 33632/60000 datapoints
2025-03-06 18:02:51,614 - INFO - training batch 1101, loss: 0.403, 35232/60000 datapoints
2025-03-06 18:02:51,809 - INFO - training batch 1151, loss: 0.267, 36832/60000 datapoints
2025-03-06 18:02:52,000 - INFO - training batch 1201, loss: 0.306, 38432/60000 datapoints
2025-03-06 18:02:52,191 - INFO - training batch 1251, loss: 0.376, 40032/60000 datapoints
2025-03-06 18:02:52,383 - INFO - training batch 1301, loss: 0.217, 41632/60000 datapoints
2025-03-06 18:02:52,573 - INFO - training batch 1351, loss: 0.535, 43232/60000 datapoints
2025-03-06 18:02:52,766 - INFO - training batch 1401, loss: 0.454, 44832/60000 datapoints
2025-03-06 18:02:52,956 - INFO - training batch 1451, loss: 0.261, 46432/60000 datapoints
2025-03-06 18:02:53,145 - INFO - training batch 1501, loss: 0.429, 48032/60000 datapoints
2025-03-06 18:02:53,339 - INFO - training batch 1551, loss: 0.490, 49632/60000 datapoints
2025-03-06 18:02:53,531 - INFO - training batch 1601, loss: 0.438, 51232/60000 datapoints
2025-03-06 18:02:53,722 - INFO - training batch 1651, loss: 0.527, 52832/60000 datapoints
2025-03-06 18:02:53,914 - INFO - training batch 1701, loss: 0.362, 54432/60000 datapoints
2025-03-06 18:02:54,106 - INFO - training batch 1751, loss: 0.427, 56032/60000 datapoints
2025-03-06 18:02:54,298 - INFO - training batch 1801, loss: 0.715, 57632/60000 datapoints
2025-03-06 18:02:54,491 - INFO - training batch 1851, loss: 0.353, 59232/60000 datapoints
2025-03-06 18:02:54,589 - INFO - validation batch 1, loss: 0.351, 32/10016 datapoints
2025-03-06 18:02:54,738 - INFO - validation batch 51, loss: 0.309, 1632/10016 datapoints
2025-03-06 18:02:54,890 - INFO - validation batch 101, loss: 0.488, 3232/10016 datapoints
2025-03-06 18:02:55,041 - INFO - validation batch 151, loss: 0.541, 4832/10016 datapoints
2025-03-06 18:02:55,190 - INFO - validation batch 201, loss: 0.768, 6432/10016 datapoints
2025-03-06 18:02:55,341 - INFO - validation batch 251, loss: 0.476, 8032/10016 datapoints
2025-03-06 18:02:55,492 - INFO - validation batch 301, loss: 0.588, 9632/10016 datapoints
2025-03-06 18:02:55,527 - INFO - Epoch 136/800 done.
2025-03-06 18:02:55,528 - INFO - Final validation performance:
Loss: 0.503, top-1 acc: 0.891top-5 acc: 0.891
2025-03-06 18:02:55,528 - INFO - Beginning epoch 137/800
2025-03-06 18:02:55,534 - INFO - training batch 1, loss: 0.341, 32/60000 datapoints
2025-03-06 18:02:55,727 - INFO - training batch 51, loss: 0.391, 1632/60000 datapoints
2025-03-06 18:02:55,924 - INFO - training batch 101, loss: 0.363, 3232/60000 datapoints
2025-03-06 18:02:56,117 - INFO - training batch 151, loss: 0.567, 4832/60000 datapoints
2025-03-06 18:02:56,311 - INFO - training batch 201, loss: 0.378, 6432/60000 datapoints
2025-03-06 18:02:56,502 - INFO - training batch 251, loss: 0.477, 8032/60000 datapoints
2025-03-06 18:02:56,695 - INFO - training batch 301, loss: 0.484, 9632/60000 datapoints
2025-03-06 18:02:56,884 - INFO - training batch 351, loss: 0.381, 11232/60000 datapoints
2025-03-06 18:02:57,075 - INFO - training batch 401, loss: 0.664, 12832/60000 datapoints
2025-03-06 18:02:57,265 - INFO - training batch 451, loss: 0.461, 14432/60000 datapoints
2025-03-06 18:02:57,462 - INFO - training batch 501, loss: 0.420, 16032/60000 datapoints
2025-03-06 18:02:57,655 - INFO - training batch 551, loss: 0.512, 17632/60000 datapoints
2025-03-06 18:02:57,845 - INFO - training batch 601, loss: 0.441, 19232/60000 datapoints
2025-03-06 18:02:58,036 - INFO - training batch 651, loss: 0.574, 20832/60000 datapoints
2025-03-06 18:02:58,226 - INFO - training batch 701, loss: 0.930, 22432/60000 datapoints
2025-03-06 18:02:58,434 - INFO - training batch 751, loss: 0.436, 24032/60000 datapoints
2025-03-06 18:02:58,628 - INFO - training batch 801, loss: 0.403, 25632/60000 datapoints
2025-03-06 18:02:58,819 - INFO - training batch 851, loss: 0.242, 27232/60000 datapoints
2025-03-06 18:02:59,009 - INFO - training batch 901, loss: 0.312, 28832/60000 datapoints
2025-03-06 18:02:59,200 - INFO - training batch 951, loss: 0.181, 30432/60000 datapoints
2025-03-06 18:02:59,393 - INFO - training batch 1001, loss: 0.300, 32032/60000 datapoints
2025-03-06 18:02:59,585 - INFO - training batch 1051, loss: 0.210, 33632/60000 datapoints
2025-03-06 18:02:59,778 - INFO - training batch 1101, loss: 0.467, 35232/60000 datapoints
2025-03-06 18:02:59,970 - INFO - training batch 1151, loss: 0.335, 36832/60000 datapoints
2025-03-06 18:03:00,164 - INFO - training batch 1201, loss: 0.240, 38432/60000 datapoints
2025-03-06 18:03:00,355 - INFO - training batch 1251, loss: 0.462, 40032/60000 datapoints
2025-03-06 18:03:00,546 - INFO - training batch 1301, loss: 0.247, 41632/60000 datapoints
2025-03-06 18:03:00,741 - INFO - training batch 1351, loss: 0.399, 43232/60000 datapoints
2025-03-06 18:03:00,934 - INFO - training batch 1401, loss: 0.359, 44832/60000 datapoints
2025-03-06 18:03:01,124 - INFO - training batch 1451, loss: 0.383, 46432/60000 datapoints
2025-03-06 18:03:01,314 - INFO - training batch 1501, loss: 0.512, 48032/60000 datapoints
2025-03-06 18:03:01,507 - INFO - training batch 1551, loss: 0.308, 49632/60000 datapoints
2025-03-06 18:03:01,713 - INFO - training batch 1601, loss: 0.301, 51232/60000 datapoints
2025-03-06 18:03:01,904 - INFO - training batch 1651, loss: 0.418, 52832/60000 datapoints
2025-03-06 18:03:02,096 - INFO - training batch 1701, loss: 0.266, 54432/60000 datapoints
2025-03-06 18:03:02,287 - INFO - training batch 1751, loss: 0.688, 56032/60000 datapoints
2025-03-06 18:03:02,481 - INFO - training batch 1801, loss: 0.644, 57632/60000 datapoints
2025-03-06 18:03:02,752 - INFO - training batch 1851, loss: 0.397, 59232/60000 datapoints
2025-03-06 18:03:02,855 - INFO - validation batch 1, loss: 0.358, 32/10016 datapoints
2025-03-06 18:03:03,005 - INFO - validation batch 51, loss: 0.507, 1632/10016 datapoints
2025-03-06 18:03:03,155 - INFO - validation batch 101, loss: 0.441, 3232/10016 datapoints
2025-03-06 18:03:03,306 - INFO - validation batch 151, loss: 0.341, 4832/10016 datapoints
2025-03-06 18:03:03,463 - INFO - validation batch 201, loss: 0.445, 6432/10016 datapoints
2025-03-06 18:03:03,618 - INFO - validation batch 251, loss: 0.337, 8032/10016 datapoints
2025-03-06 18:03:03,774 - INFO - validation batch 301, loss: 0.359, 9632/10016 datapoints
2025-03-06 18:03:03,810 - INFO - Epoch 137/800 done.
2025-03-06 18:03:03,810 - INFO - Final validation performance:
Loss: 0.398, top-1 acc: 0.891top-5 acc: 0.891
2025-03-06 18:03:03,810 - INFO - Beginning epoch 138/800
2025-03-06 18:03:03,817 - INFO - training batch 1, loss: 0.411, 32/60000 datapoints
2025-03-06 18:03:04,016 - INFO - training batch 51, loss: 0.271, 1632/60000 datapoints
2025-03-06 18:03:04,211 - INFO - training batch 101, loss: 0.356, 3232/60000 datapoints
2025-03-06 18:03:04,405 - INFO - training batch 151, loss: 0.441, 4832/60000 datapoints
2025-03-06 18:03:04,602 - INFO - training batch 201, loss: 0.520, 6432/60000 datapoints
2025-03-06 18:03:04,800 - INFO - training batch 251, loss: 0.333, 8032/60000 datapoints
2025-03-06 18:03:05,000 - INFO - training batch 301, loss: 0.468, 9632/60000 datapoints
2025-03-06 18:03:05,195 - INFO - training batch 351, loss: 0.439, 11232/60000 datapoints
2025-03-06 18:03:05,391 - INFO - training batch 401, loss: 0.214, 12832/60000 datapoints
2025-03-06 18:03:05,587 - INFO - training batch 451, loss: 0.473, 14432/60000 datapoints
2025-03-06 18:03:05,787 - INFO - training batch 501, loss: 0.205, 16032/60000 datapoints
2025-03-06 18:03:05,981 - INFO - training batch 551, loss: 0.242, 17632/60000 datapoints
2025-03-06 18:03:06,179 - INFO - training batch 601, loss: 0.223, 19232/60000 datapoints
2025-03-06 18:03:06,373 - INFO - training batch 651, loss: 0.352, 20832/60000 datapoints
2025-03-06 18:03:06,570 - INFO - training batch 701, loss: 0.543, 22432/60000 datapoints
2025-03-06 18:03:06,763 - INFO - training batch 751, loss: 0.244, 24032/60000 datapoints
2025-03-06 18:03:06,958 - INFO - training batch 801, loss: 0.336, 25632/60000 datapoints
2025-03-06 18:03:07,153 - INFO - training batch 851, loss: 0.379, 27232/60000 datapoints
2025-03-06 18:03:07,347 - INFO - training batch 901, loss: 0.513, 28832/60000 datapoints
2025-03-06 18:03:07,544 - INFO - training batch 951, loss: 0.427, 30432/60000 datapoints
2025-03-06 18:03:07,742 - INFO - training batch 1001, loss: 0.387, 32032/60000 datapoints
2025-03-06 18:03:07,935 - INFO - training batch 1051, loss: 0.697, 33632/60000 datapoints
2025-03-06 18:03:08,136 - INFO - training batch 1101, loss: 0.242, 35232/60000 datapoints
2025-03-06 18:03:08,330 - INFO - training batch 1151, loss: 0.528, 36832/60000 datapoints
2025-03-06 18:03:08,548 - INFO - training batch 1201, loss: 0.936, 38432/60000 datapoints
2025-03-06 18:03:08,742 - INFO - training batch 1251, loss: 0.404, 40032/60000 datapoints
2025-03-06 18:03:08,936 - INFO - training batch 1301, loss: 0.527, 41632/60000 datapoints
2025-03-06 18:03:09,134 - INFO - training batch 1351, loss: 0.614, 43232/60000 datapoints
2025-03-06 18:03:09,328 - INFO - training batch 1401, loss: 0.574, 44832/60000 datapoints
2025-03-06 18:03:09,524 - INFO - training batch 1451, loss: 0.477, 46432/60000 datapoints
2025-03-06 18:03:09,721 - INFO - training batch 1501, loss: 0.318, 48032/60000 datapoints
2025-03-06 18:03:09,916 - INFO - training batch 1551, loss: 0.358, 49632/60000 datapoints
2025-03-06 18:03:10,111 - INFO - training batch 1601, loss: 0.383, 51232/60000 datapoints
2025-03-06 18:03:10,306 - INFO - training batch 1651, loss: 0.313, 52832/60000 datapoints
2025-03-06 18:03:10,500 - INFO - training batch 1701, loss: 0.252, 54432/60000 datapoints
2025-03-06 18:03:10,700 - INFO - training batch 1751, loss: 0.368, 56032/60000 datapoints
2025-03-06 18:03:10,893 - INFO - training batch 1801, loss: 0.589, 57632/60000 datapoints
2025-03-06 18:03:11,087 - INFO - training batch 1851, loss: 0.426, 59232/60000 datapoints
2025-03-06 18:03:11,188 - INFO - validation batch 1, loss: 0.427, 32/10016 datapoints
2025-03-06 18:03:11,340 - INFO - validation batch 51, loss: 0.242, 1632/10016 datapoints
2025-03-06 18:03:11,493 - INFO - validation batch 101, loss: 0.386, 3232/10016 datapoints
2025-03-06 18:03:11,648 - INFO - validation batch 151, loss: 0.527, 4832/10016 datapoints
2025-03-06 18:03:11,801 - INFO - validation batch 201, loss: 0.298, 6432/10016 datapoints
2025-03-06 18:03:11,952 - INFO - validation batch 251, loss: 0.371, 8032/10016 datapoints
2025-03-06 18:03:12,105 - INFO - validation batch 301, loss: 0.330, 9632/10016 datapoints
2025-03-06 18:03:12,143 - INFO - Epoch 138/800 done.
2025-03-06 18:03:12,143 - INFO - Final validation performance:
Loss: 0.369, top-1 acc: 0.891top-5 acc: 0.891
2025-03-06 18:03:12,143 - INFO - Beginning epoch 139/800
2025-03-06 18:03:12,149 - INFO - training batch 1, loss: 0.517, 32/60000 datapoints
2025-03-06 18:03:12,347 - INFO - training batch 51, loss: 0.290, 1632/60000 datapoints
2025-03-06 18:03:12,541 - INFO - training batch 101, loss: 0.543, 3232/60000 datapoints
2025-03-06 18:03:12,738 - INFO - training batch 151, loss: 0.492, 4832/60000 datapoints
2025-03-06 18:03:12,930 - INFO - training batch 201, loss: 0.185, 6432/60000 datapoints
2025-03-06 18:03:13,124 - INFO - training batch 251, loss: 0.555, 8032/60000 datapoints
2025-03-06 18:03:13,320 - INFO - training batch 301, loss: 0.384, 9632/60000 datapoints
2025-03-06 18:03:13,516 - INFO - training batch 351, loss: 0.437, 11232/60000 datapoints
2025-03-06 18:03:13,714 - INFO - training batch 401, loss: 0.396, 12832/60000 datapoints
2025-03-06 18:03:13,908 - INFO - training batch 451, loss: 0.604, 14432/60000 datapoints
2025-03-06 18:03:14,103 - INFO - training batch 501, loss: 0.427, 16032/60000 datapoints
2025-03-06 18:03:14,295 - INFO - training batch 551, loss: 0.449, 17632/60000 datapoints
2025-03-06 18:03:14,488 - INFO - training batch 601, loss: 0.375, 19232/60000 datapoints
2025-03-06 18:03:14,683 - INFO - training batch 651, loss: 0.345, 20832/60000 datapoints
2025-03-06 18:03:14,879 - INFO - training batch 701, loss: 0.362, 22432/60000 datapoints
2025-03-06 18:03:15,072 - INFO - training batch 751, loss: 0.294, 24032/60000 datapoints
2025-03-06 18:03:15,267 - INFO - training batch 801, loss: 0.759, 25632/60000 datapoints
2025-03-06 18:03:15,463 - INFO - training batch 851, loss: 0.422, 27232/60000 datapoints
2025-03-06 18:03:15,660 - INFO - training batch 901, loss: 0.358, 28832/60000 datapoints
2025-03-06 18:03:15,852 - INFO - training batch 951, loss: 0.226, 30432/60000 datapoints
2025-03-06 18:03:16,046 - INFO - training batch 1001, loss: 0.301, 32032/60000 datapoints
2025-03-06 18:03:16,244 - INFO - training batch 1051, loss: 0.669, 33632/60000 datapoints
2025-03-06 18:03:16,438 - INFO - training batch 1101, loss: 0.444, 35232/60000 datapoints
2025-03-06 18:03:16,635 - INFO - training batch 1151, loss: 0.521, 36832/60000 datapoints
2025-03-06 18:03:16,827 - INFO - training batch 1201, loss: 0.423, 38432/60000 datapoints
2025-03-06 18:03:17,020 - INFO - training batch 1251, loss: 0.362, 40032/60000 datapoints
2025-03-06 18:03:17,213 - INFO - training batch 1301, loss: 0.206, 41632/60000 datapoints
2025-03-06 18:03:17,406 - INFO - training batch 1351, loss: 0.260, 43232/60000 datapoints
2025-03-06 18:03:17,606 - INFO - training batch 1401, loss: 0.283, 44832/60000 datapoints
2025-03-06 18:03:17,799 - INFO - training batch 1451, loss: 0.454, 46432/60000 datapoints
2025-03-06 18:03:17,992 - INFO - training batch 1501, loss: 0.245, 48032/60000 datapoints
2025-03-06 18:03:18,186 - INFO - training batch 1551, loss: 0.737, 49632/60000 datapoints
2025-03-06 18:03:18,380 - INFO - training batch 1601, loss: 0.526, 51232/60000 datapoints
2025-03-06 18:03:18,586 - INFO - training batch 1651, loss: 0.723, 52832/60000 datapoints
2025-03-06 18:03:18,788 - INFO - training batch 1701, loss: 0.529, 54432/60000 datapoints
2025-03-06 18:03:18,980 - INFO - training batch 1751, loss: 0.469, 56032/60000 datapoints
2025-03-06 18:03:19,176 - INFO - training batch 1801, loss: 0.363, 57632/60000 datapoints
2025-03-06 18:03:19,368 - INFO - training batch 1851, loss: 0.782, 59232/60000 datapoints
2025-03-06 18:03:19,473 - INFO - validation batch 1, loss: 0.405, 32/10016 datapoints
2025-03-06 18:03:19,627 - INFO - validation batch 51, loss: 0.549, 1632/10016 datapoints
2025-03-06 18:03:19,778 - INFO - validation batch 101, loss: 0.230, 3232/10016 datapoints
2025-03-06 18:03:19,929 - INFO - validation batch 151, loss: 0.473, 4832/10016 datapoints
2025-03-06 18:03:20,080 - INFO - validation batch 201, loss: 0.409, 6432/10016 datapoints
2025-03-06 18:03:20,235 - INFO - validation batch 251, loss: 0.492, 8032/10016 datapoints
2025-03-06 18:03:20,389 - INFO - validation batch 301, loss: 0.372, 9632/10016 datapoints
2025-03-06 18:03:20,426 - INFO - Epoch 139/800 done.
2025-03-06 18:03:20,426 - INFO - Final validation performance:
Loss: 0.418, top-1 acc: 0.892top-5 acc: 0.892
2025-03-06 18:03:20,426 - INFO - Beginning epoch 140/800
2025-03-06 18:03:20,432 - INFO - training batch 1, loss: 0.492, 32/60000 datapoints
2025-03-06 18:03:20,626 - INFO - training batch 51, loss: 0.352, 1632/60000 datapoints
2025-03-06 18:03:20,821 - INFO - training batch 101, loss: 0.199, 3232/60000 datapoints
2025-03-06 18:03:21,016 - INFO - training batch 151, loss: 0.434, 4832/60000 datapoints
2025-03-06 18:03:21,213 - INFO - training batch 201, loss: 0.402, 6432/60000 datapoints
2025-03-06 18:03:21,409 - INFO - training batch 251, loss: 0.487, 8032/60000 datapoints
2025-03-06 18:03:21,610 - INFO - training batch 301, loss: 0.244, 9632/60000 datapoints
2025-03-06 18:03:21,806 - INFO - training batch 351, loss: 0.363, 11232/60000 datapoints
2025-03-06 18:03:21,997 - INFO - training batch 401, loss: 0.313, 12832/60000 datapoints
2025-03-06 18:03:22,191 - INFO - training batch 451, loss: 0.185, 14432/60000 datapoints
2025-03-06 18:03:22,394 - INFO - training batch 501, loss: 0.291, 16032/60000 datapoints
2025-03-06 18:03:22,587 - INFO - training batch 551, loss: 0.287, 17632/60000 datapoints
2025-03-06 18:03:22,784 - INFO - training batch 601, loss: 0.409, 19232/60000 datapoints
2025-03-06 18:03:22,977 - INFO - training batch 651, loss: 0.338, 20832/60000 datapoints
2025-03-06 18:03:23,170 - INFO - training batch 701, loss: 0.463, 22432/60000 datapoints
2025-03-06 18:03:23,401 - INFO - training batch 751, loss: 0.280, 24032/60000 datapoints
2025-03-06 18:03:23,598 - INFO - training batch 801, loss: 0.315, 25632/60000 datapoints
2025-03-06 18:03:23,790 - INFO - training batch 851, loss: 0.397, 27232/60000 datapoints
2025-03-06 18:03:23,983 - INFO - training batch 901, loss: 0.440, 28832/60000 datapoints
2025-03-06 18:03:24,176 - INFO - training batch 951, loss: 0.639, 30432/60000 datapoints
2025-03-06 18:03:24,371 - INFO - training batch 1001, loss: 0.286, 32032/60000 datapoints
2025-03-06 18:03:24,564 - INFO - training batch 1051, loss: 0.352, 33632/60000 datapoints
2025-03-06 18:03:24,761 - INFO - training batch 1101, loss: 0.264, 35232/60000 datapoints
2025-03-06 18:03:24,959 - INFO - training batch 1151, loss: 0.450, 36832/60000 datapoints
2025-03-06 18:03:25,152 - INFO - training batch 1201, loss: 0.345, 38432/60000 datapoints
2025-03-06 18:03:25,348 - INFO - training batch 1251, loss: 0.491, 40032/60000 datapoints
2025-03-06 18:03:25,546 - INFO - training batch 1301, loss: 0.706, 41632/60000 datapoints
2025-03-06 18:03:25,744 - INFO - training batch 1351, loss: 0.583, 43232/60000 datapoints
2025-03-06 18:03:25,936 - INFO - training batch 1401, loss: 0.580, 44832/60000 datapoints
2025-03-06 18:03:26,129 - INFO - training batch 1451, loss: 0.451, 46432/60000 datapoints
2025-03-06 18:03:26,327 - INFO - training batch 1501, loss: 0.401, 48032/60000 datapoints
2025-03-06 18:03:26,519 - INFO - training batch 1551, loss: 0.441, 49632/60000 datapoints
2025-03-06 18:03:26,712 - INFO - training batch 1601, loss: 0.591, 51232/60000 datapoints
2025-03-06 18:03:26,907 - INFO - training batch 1651, loss: 0.410, 52832/60000 datapoints
2025-03-06 18:03:27,099 - INFO - training batch 1701, loss: 0.222, 54432/60000 datapoints
2025-03-06 18:03:27,298 - INFO - training batch 1751, loss: 0.339, 56032/60000 datapoints
2025-03-06 18:03:27,507 - INFO - training batch 1801, loss: 0.456, 57632/60000 datapoints
2025-03-06 18:03:27,705 - INFO - training batch 1851, loss: 0.416, 59232/60000 datapoints
2025-03-06 18:03:27,808 - INFO - validation batch 1, loss: 0.486, 32/10016 datapoints
2025-03-06 18:03:27,960 - INFO - validation batch 51, loss: 0.419, 1632/10016 datapoints
2025-03-06 18:03:28,112 - INFO - validation batch 101, loss: 0.747, 3232/10016 datapoints
2025-03-06 18:03:28,265 - INFO - validation batch 151, loss: 0.762, 4832/10016 datapoints
2025-03-06 18:03:28,416 - INFO - validation batch 201, loss: 0.332, 6432/10016 datapoints
2025-03-06 18:03:28,568 - INFO - validation batch 251, loss: 0.447, 8032/10016 datapoints
2025-03-06 18:03:28,740 - INFO - validation batch 301, loss: 0.346, 9632/10016 datapoints
2025-03-06 18:03:28,781 - INFO - Epoch 140/800 done.
2025-03-06 18:03:28,782 - INFO - Final validation performance:
Loss: 0.506, top-1 acc: 0.892top-5 acc: 0.892
2025-03-06 18:03:28,782 - INFO - Beginning epoch 141/800
2025-03-06 18:03:28,788 - INFO - training batch 1, loss: 0.380, 32/60000 datapoints
2025-03-06 18:03:28,991 - INFO - training batch 51, loss: 0.376, 1632/60000 datapoints
2025-03-06 18:03:29,184 - INFO - training batch 101, loss: 0.384, 3232/60000 datapoints
2025-03-06 18:03:29,380 - INFO - training batch 151, loss: 0.484, 4832/60000 datapoints
2025-03-06 18:03:29,578 - INFO - training batch 201, loss: 0.688, 6432/60000 datapoints
2025-03-06 18:03:29,774 - INFO - training batch 251, loss: 0.241, 8032/60000 datapoints
2025-03-06 18:03:29,968 - INFO - training batch 301, loss: 0.519, 9632/60000 datapoints
2025-03-06 18:03:30,163 - INFO - training batch 351, loss: 0.317, 11232/60000 datapoints
2025-03-06 18:03:30,360 - INFO - training batch 401, loss: 0.280, 12832/60000 datapoints
2025-03-06 18:03:30,551 - INFO - training batch 451, loss: 0.430, 14432/60000 datapoints
2025-03-06 18:03:30,744 - INFO - training batch 501, loss: 0.449, 16032/60000 datapoints
2025-03-06 18:03:30,937 - INFO - training batch 551, loss: 0.304, 17632/60000 datapoints
2025-03-06 18:03:31,126 - INFO - training batch 601, loss: 0.541, 19232/60000 datapoints
2025-03-06 18:03:31,328 - INFO - training batch 651, loss: 0.214, 20832/60000 datapoints
2025-03-06 18:03:31,528 - INFO - training batch 701, loss: 0.501, 22432/60000 datapoints
2025-03-06 18:03:31,728 - INFO - training batch 751, loss: 0.791, 24032/60000 datapoints
2025-03-06 18:03:31,924 - INFO - training batch 801, loss: 0.351, 25632/60000 datapoints
2025-03-06 18:03:32,118 - INFO - training batch 851, loss: 0.290, 27232/60000 datapoints
2025-03-06 18:03:32,311 - INFO - training batch 901, loss: 0.394, 28832/60000 datapoints
2025-03-06 18:03:32,505 - INFO - training batch 951, loss: 0.344, 30432/60000 datapoints
2025-03-06 18:03:32,701 - INFO - training batch 1001, loss: 0.298, 32032/60000 datapoints
2025-03-06 18:03:32,898 - INFO - training batch 1051, loss: 0.262, 33632/60000 datapoints
2025-03-06 18:03:33,092 - INFO - training batch 1101, loss: 0.349, 35232/60000 datapoints
2025-03-06 18:03:33,285 - INFO - training batch 1151, loss: 0.374, 36832/60000 datapoints
2025-03-06 18:03:33,482 - INFO - training batch 1201, loss: 0.156, 38432/60000 datapoints
2025-03-06 18:03:33,678 - INFO - training batch 1251, loss: 0.542, 40032/60000 datapoints
2025-03-06 18:03:33,873 - INFO - training batch 1301, loss: 0.483, 41632/60000 datapoints
2025-03-06 18:03:34,072 - INFO - training batch 1351, loss: 0.376, 43232/60000 datapoints
2025-03-06 18:03:34,266 - INFO - training batch 1401, loss: 0.415, 44832/60000 datapoints
2025-03-06 18:03:34,461 - INFO - training batch 1451, loss: 0.412, 46432/60000 datapoints
2025-03-06 18:03:34,657 - INFO - training batch 1501, loss: 0.285, 48032/60000 datapoints
2025-03-06 18:03:34,855 - INFO - training batch 1551, loss: 0.322, 49632/60000 datapoints
2025-03-06 18:03:35,050 - INFO - training batch 1601, loss: 0.484, 51232/60000 datapoints
2025-03-06 18:03:35,247 - INFO - training batch 1651, loss: 0.394, 52832/60000 datapoints
2025-03-06 18:03:35,443 - INFO - training batch 1701, loss: 0.435, 54432/60000 datapoints
2025-03-06 18:03:35,641 - INFO - training batch 1751, loss: 0.468, 56032/60000 datapoints
2025-03-06 18:03:35,834 - INFO - training batch 1801, loss: 0.387, 57632/60000 datapoints
2025-03-06 18:03:36,029 - INFO - training batch 1851, loss: 0.321, 59232/60000 datapoints
2025-03-06 18:03:36,132 - INFO - validation batch 1, loss: 0.444, 32/10016 datapoints
2025-03-06 18:03:36,288 - INFO - validation batch 51, loss: 0.356, 1632/10016 datapoints
2025-03-06 18:03:36,444 - INFO - validation batch 101, loss: 0.405, 3232/10016 datapoints
2025-03-06 18:03:36,598 - INFO - validation batch 151, loss: 0.279, 4832/10016 datapoints
2025-03-06 18:03:36,750 - INFO - validation batch 201, loss: 0.299, 6432/10016 datapoints
2025-03-06 18:03:36,902 - INFO - validation batch 251, loss: 0.337, 8032/10016 datapoints
2025-03-06 18:03:37,054 - INFO - validation batch 301, loss: 0.553, 9632/10016 datapoints
2025-03-06 18:03:37,091 - INFO - Epoch 141/800 done.
2025-03-06 18:03:37,091 - INFO - Final validation performance:
Loss: 0.382, top-1 acc: 0.892top-5 acc: 0.892
2025-03-06 18:03:37,091 - INFO - Beginning epoch 142/800
2025-03-06 18:03:37,097 - INFO - training batch 1, loss: 0.413, 32/60000 datapoints
2025-03-06 18:03:37,291 - INFO - training batch 51, loss: 0.459, 1632/60000 datapoints
2025-03-06 18:03:37,490 - INFO - training batch 101, loss: 0.324, 3232/60000 datapoints
2025-03-06 18:03:37,705 - INFO - training batch 151, loss: 0.383, 4832/60000 datapoints
2025-03-06 18:03:37,898 - INFO - training batch 201, loss: 0.316, 6432/60000 datapoints
2025-03-06 18:03:38,090 - INFO - training batch 251, loss: 0.546, 8032/60000 datapoints
2025-03-06 18:03:38,283 - INFO - training batch 301, loss: 0.335, 9632/60000 datapoints
2025-03-06 18:03:38,478 - INFO - training batch 351, loss: 0.570, 11232/60000 datapoints
2025-03-06 18:03:38,674 - INFO - training batch 401, loss: 0.416, 12832/60000 datapoints
2025-03-06 18:03:38,885 - INFO - training batch 451, loss: 0.452, 14432/60000 datapoints
2025-03-06 18:03:39,083 - INFO - training batch 501, loss: 0.289, 16032/60000 datapoints
2025-03-06 18:03:39,276 - INFO - training batch 551, loss: 0.240, 17632/60000 datapoints
2025-03-06 18:03:39,472 - INFO - training batch 601, loss: 0.385, 19232/60000 datapoints
2025-03-06 18:03:39,670 - INFO - training batch 651, loss: 0.239, 20832/60000 datapoints
2025-03-06 18:03:39,864 - INFO - training batch 701, loss: 0.277, 22432/60000 datapoints
2025-03-06 18:03:40,060 - INFO - training batch 751, loss: 0.328, 24032/60000 datapoints
2025-03-06 18:03:40,254 - INFO - training batch 801, loss: 0.507, 25632/60000 datapoints
2025-03-06 18:03:40,450 - INFO - training batch 851, loss: 0.376, 27232/60000 datapoints
2025-03-06 18:03:40,646 - INFO - training batch 901, loss: 0.398, 28832/60000 datapoints
2025-03-06 18:03:40,839 - INFO - training batch 951, loss: 0.328, 30432/60000 datapoints
2025-03-06 18:03:41,032 - INFO - training batch 1001, loss: 0.493, 32032/60000 datapoints
2025-03-06 18:03:41,224 - INFO - training batch 1051, loss: 0.455, 33632/60000 datapoints
2025-03-06 18:03:41,418 - INFO - training batch 1101, loss: 0.278, 35232/60000 datapoints
2025-03-06 18:03:41,620 - INFO - training batch 1151, loss: 0.396, 36832/60000 datapoints
2025-03-06 18:03:41,814 - INFO - training batch 1201, loss: 0.334, 38432/60000 datapoints
2025-03-06 18:03:42,007 - INFO - training batch 1251, loss: 0.355, 40032/60000 datapoints
2025-03-06 18:03:42,201 - INFO - training batch 1301, loss: 0.168, 41632/60000 datapoints
2025-03-06 18:03:42,393 - INFO - training batch 1351, loss: 0.651, 43232/60000 datapoints
2025-03-06 18:03:42,588 - INFO - training batch 1401, loss: 0.464, 44832/60000 datapoints
2025-03-06 18:03:42,783 - INFO - training batch 1451, loss: 0.432, 46432/60000 datapoints
2025-03-06 18:03:42,974 - INFO - training batch 1501, loss: 0.721, 48032/60000 datapoints
2025-03-06 18:03:43,167 - INFO - training batch 1551, loss: 0.366, 49632/60000 datapoints
2025-03-06 18:03:43,362 - INFO - training batch 1601, loss: 0.220, 51232/60000 datapoints
2025-03-06 18:03:43,557 - INFO - training batch 1651, loss: 0.493, 52832/60000 datapoints
2025-03-06 18:03:43,755 - INFO - training batch 1701, loss: 0.354, 54432/60000 datapoints
2025-03-06 18:03:43,948 - INFO - training batch 1751, loss: 0.378, 56032/60000 datapoints
2025-03-06 18:03:44,141 - INFO - training batch 1801, loss: 0.186, 57632/60000 datapoints
2025-03-06 18:03:44,335 - INFO - training batch 1851, loss: 0.252, 59232/60000 datapoints
2025-03-06 18:03:44,436 - INFO - validation batch 1, loss: 0.164, 32/10016 datapoints
2025-03-06 18:03:44,589 - INFO - validation batch 51, loss: 0.358, 1632/10016 datapoints
2025-03-06 18:03:44,742 - INFO - validation batch 101, loss: 0.432, 3232/10016 datapoints
2025-03-06 18:03:44,897 - INFO - validation batch 151, loss: 0.421, 4832/10016 datapoints
2025-03-06 18:03:45,050 - INFO - validation batch 201, loss: 0.294, 6432/10016 datapoints
2025-03-06 18:03:45,203 - INFO - validation batch 251, loss: 0.476, 8032/10016 datapoints
2025-03-06 18:03:45,354 - INFO - validation batch 301, loss: 0.421, 9632/10016 datapoints
2025-03-06 18:03:45,390 - INFO - Epoch 142/800 done.
2025-03-06 18:03:45,391 - INFO - Final validation performance:
Loss: 0.367, top-1 acc: 0.893top-5 acc: 0.893
2025-03-06 18:03:45,391 - INFO - Beginning epoch 143/800
2025-03-06 18:03:45,397 - INFO - training batch 1, loss: 0.337, 32/60000 datapoints
2025-03-06 18:03:45,595 - INFO - training batch 51, loss: 0.283, 1632/60000 datapoints
2025-03-06 18:03:45,792 - INFO - training batch 101, loss: 0.415, 3232/60000 datapoints
2025-03-06 18:03:45,986 - INFO - training batch 151, loss: 0.466, 4832/60000 datapoints
2025-03-06 18:03:46,181 - INFO - training batch 201, loss: 0.292, 6432/60000 datapoints
2025-03-06 18:03:46,375 - INFO - training batch 251, loss: 0.403, 8032/60000 datapoints
2025-03-06 18:03:46,570 - INFO - training batch 301, loss: 0.722, 9632/60000 datapoints
2025-03-06 18:03:46,769 - INFO - training batch 351, loss: 0.476, 11232/60000 datapoints
2025-03-06 18:03:46,963 - INFO - training batch 401, loss: 0.397, 12832/60000 datapoints
2025-03-06 18:03:47,159 - INFO - training batch 451, loss: 0.637, 14432/60000 datapoints
2025-03-06 18:03:47,351 - INFO - training batch 501, loss: 0.384, 16032/60000 datapoints
2025-03-06 18:03:47,550 - INFO - training batch 551, loss: 0.367, 17632/60000 datapoints
2025-03-06 18:03:47,750 - INFO - training batch 601, loss: 0.639, 19232/60000 datapoints
2025-03-06 18:03:47,942 - INFO - training batch 651, loss: 0.452, 20832/60000 datapoints
2025-03-06 18:03:48,135 - INFO - training batch 701, loss: 0.515, 22432/60000 datapoints
2025-03-06 18:03:48,328 - INFO - training batch 751, loss: 0.422, 24032/60000 datapoints
2025-03-06 18:03:48,531 - INFO - training batch 801, loss: 0.407, 25632/60000 datapoints
2025-03-06 18:03:48,726 - INFO - training batch 851, loss: 0.260, 27232/60000 datapoints
2025-03-06 18:03:48,936 - INFO - training batch 901, loss: 0.678, 28832/60000 datapoints
2025-03-06 18:03:49,132 - INFO - training batch 951, loss: 0.240, 30432/60000 datapoints
2025-03-06 18:03:49,326 - INFO - training batch 1001, loss: 0.351, 32032/60000 datapoints
2025-03-06 18:03:49,519 - INFO - training batch 1051, loss: 0.551, 33632/60000 datapoints
2025-03-06 18:03:49,720 - INFO - training batch 1101, loss: 0.561, 35232/60000 datapoints
2025-03-06 18:03:49,914 - INFO - training batch 1151, loss: 0.533, 36832/60000 datapoints
2025-03-06 18:03:50,112 - INFO - training batch 1201, loss: 0.445, 38432/60000 datapoints
2025-03-06 18:03:50,306 - INFO - training batch 1251, loss: 0.356, 40032/60000 datapoints
2025-03-06 18:03:50,500 - INFO - training batch 1301, loss: 0.362, 41632/60000 datapoints
2025-03-06 18:03:50,695 - INFO - training batch 1351, loss: 0.283, 43232/60000 datapoints
2025-03-06 18:03:50,889 - INFO - training batch 1401, loss: 0.375, 44832/60000 datapoints
2025-03-06 18:03:51,082 - INFO - training batch 1451, loss: 0.515, 46432/60000 datapoints
2025-03-06 18:03:51,275 - INFO - training batch 1501, loss: 0.663, 48032/60000 datapoints
2025-03-06 18:03:51,469 - INFO - training batch 1551, loss: 0.460, 49632/60000 datapoints
2025-03-06 18:03:51,672 - INFO - training batch 1601, loss: 0.438, 51232/60000 datapoints
2025-03-06 18:03:51,868 - INFO - training batch 1651, loss: 0.442, 52832/60000 datapoints
2025-03-06 18:03:52,061 - INFO - training batch 1701, loss: 0.496, 54432/60000 datapoints
2025-03-06 18:03:52,254 - INFO - training batch 1751, loss: 0.475, 56032/60000 datapoints
2025-03-06 18:03:52,447 - INFO - training batch 1801, loss: 0.525, 57632/60000 datapoints
2025-03-06 18:03:52,644 - INFO - training batch 1851, loss: 0.317, 59232/60000 datapoints
2025-03-06 18:03:52,744 - INFO - validation batch 1, loss: 0.267, 32/10016 datapoints
2025-03-06 18:03:52,896 - INFO - validation batch 51, loss: 0.347, 1632/10016 datapoints
2025-03-06 18:03:53,048 - INFO - validation batch 101, loss: 0.326, 3232/10016 datapoints
2025-03-06 18:03:53,200 - INFO - validation batch 151, loss: 0.480, 4832/10016 datapoints
2025-03-06 18:03:53,351 - INFO - validation batch 201, loss: 0.582, 6432/10016 datapoints
2025-03-06 18:03:53,504 - INFO - validation batch 251, loss: 0.530, 8032/10016 datapoints
2025-03-06 18:03:53,661 - INFO - validation batch 301, loss: 0.474, 9632/10016 datapoints
2025-03-06 18:03:53,696 - INFO - Epoch 143/800 done.
2025-03-06 18:03:53,696 - INFO - Final validation performance:
Loss: 0.430, top-1 acc: 0.893top-5 acc: 0.893
2025-03-06 18:03:53,696 - INFO - Beginning epoch 144/800
2025-03-06 18:03:53,703 - INFO - training batch 1, loss: 0.625, 32/60000 datapoints
2025-03-06 18:03:53,898 - INFO - training batch 51, loss: 0.281, 1632/60000 datapoints
2025-03-06 18:03:54,095 - INFO - training batch 101, loss: 0.272, 3232/60000 datapoints
2025-03-06 18:03:54,288 - INFO - training batch 151, loss: 0.681, 4832/60000 datapoints
2025-03-06 18:03:54,480 - INFO - training batch 201, loss: 0.546, 6432/60000 datapoints
2025-03-06 18:03:54,677 - INFO - training batch 251, loss: 0.406, 8032/60000 datapoints
2025-03-06 18:03:54,874 - INFO - training batch 301, loss: 0.447, 9632/60000 datapoints
2025-03-06 18:03:55,070 - INFO - training batch 351, loss: 0.398, 11232/60000 datapoints
2025-03-06 18:03:55,265 - INFO - training batch 401, loss: 0.248, 12832/60000 datapoints
2025-03-06 18:03:55,458 - INFO - training batch 451, loss: 0.335, 14432/60000 datapoints
2025-03-06 18:03:55,661 - INFO - training batch 501, loss: 0.504, 16032/60000 datapoints
2025-03-06 18:03:55,855 - INFO - training batch 551, loss: 0.331, 17632/60000 datapoints
2025-03-06 18:03:56,047 - INFO - training batch 601, loss: 0.439, 19232/60000 datapoints
2025-03-06 18:03:56,242 - INFO - training batch 651, loss: 0.435, 20832/60000 datapoints
2025-03-06 18:03:56,437 - INFO - training batch 701, loss: 0.392, 22432/60000 datapoints
2025-03-06 18:03:56,634 - INFO - training batch 751, loss: 0.607, 24032/60000 datapoints
2025-03-06 18:03:56,829 - INFO - training batch 801, loss: 0.208, 25632/60000 datapoints
2025-03-06 18:03:57,023 - INFO - training batch 851, loss: 0.302, 27232/60000 datapoints
2025-03-06 18:03:57,216 - INFO - training batch 901, loss: 0.320, 28832/60000 datapoints
2025-03-06 18:03:57,408 - INFO - training batch 951, loss: 0.527, 30432/60000 datapoints
2025-03-06 18:03:57,607 - INFO - training batch 1001, loss: 0.351, 32032/60000 datapoints
2025-03-06 18:03:57,802 - INFO - training batch 1051, loss: 0.240, 33632/60000 datapoints
2025-03-06 18:03:57,993 - INFO - training batch 1101, loss: 0.292, 35232/60000 datapoints
2025-03-06 18:03:58,188 - INFO - training batch 1151, loss: 0.507, 36832/60000 datapoints
2025-03-06 18:03:58,379 - INFO - training batch 1201, loss: 0.171, 38432/60000 datapoints
2025-03-06 18:03:58,574 - INFO - training batch 1251, loss: 0.561, 40032/60000 datapoints
2025-03-06 18:03:58,769 - INFO - training batch 1301, loss: 0.241, 41632/60000 datapoints
2025-03-06 18:03:58,980 - INFO - training batch 1351, loss: 0.621, 43232/60000 datapoints
2025-03-06 18:03:59,173 - INFO - training batch 1401, loss: 0.310, 44832/60000 datapoints
2025-03-06 18:03:59,366 - INFO - training batch 1451, loss: 0.449, 46432/60000 datapoints
2025-03-06 18:03:59,563 - INFO - training batch 1501, loss: 0.453, 48032/60000 datapoints
2025-03-06 18:03:59,759 - INFO - training batch 1551, loss: 0.536, 49632/60000 datapoints
2025-03-06 18:03:59,954 - INFO - training batch 1601, loss: 0.314, 51232/60000 datapoints
2025-03-06 18:04:00,149 - INFO - training batch 1651, loss: 0.425, 52832/60000 datapoints
2025-03-06 18:04:00,341 - INFO - training batch 1701, loss: 0.214, 54432/60000 datapoints
2025-03-06 18:04:00,535 - INFO - training batch 1751, loss: 0.366, 56032/60000 datapoints
2025-03-06 18:04:00,730 - INFO - training batch 1801, loss: 0.386, 57632/60000 datapoints
2025-03-06 18:04:00,922 - INFO - training batch 1851, loss: 0.517, 59232/60000 datapoints
2025-03-06 18:04:01,025 - INFO - validation batch 1, loss: 0.177, 32/10016 datapoints
2025-03-06 18:04:01,177 - INFO - validation batch 51, loss: 0.472, 1632/10016 datapoints
2025-03-06 18:04:01,330 - INFO - validation batch 101, loss: 0.253, 3232/10016 datapoints
2025-03-06 18:04:01,482 - INFO - validation batch 151, loss: 0.215, 4832/10016 datapoints
2025-03-06 18:04:01,641 - INFO - validation batch 201, loss: 0.373, 6432/10016 datapoints
2025-03-06 18:04:01,798 - INFO - validation batch 251, loss: 0.325, 8032/10016 datapoints
2025-03-06 18:04:01,952 - INFO - validation batch 301, loss: 0.220, 9632/10016 datapoints
2025-03-06 18:04:01,991 - INFO - Epoch 144/800 done.
2025-03-06 18:04:01,991 - INFO - Final validation performance:
Loss: 0.291, top-1 acc: 0.893top-5 acc: 0.893
2025-03-06 18:04:01,991 - INFO - Beginning epoch 145/800
2025-03-06 18:04:01,997 - INFO - training batch 1, loss: 0.576, 32/60000 datapoints
2025-03-06 18:04:02,191 - INFO - training batch 51, loss: 0.355, 1632/60000 datapoints
2025-03-06 18:04:02,385 - INFO - training batch 101, loss: 0.411, 3232/60000 datapoints
2025-03-06 18:04:02,578 - INFO - training batch 151, loss: 0.424, 4832/60000 datapoints
2025-03-06 18:04:02,774 - INFO - training batch 201, loss: 0.404, 6432/60000 datapoints
2025-03-06 18:04:02,967 - INFO - training batch 251, loss: 0.217, 8032/60000 datapoints
2025-03-06 18:04:03,168 - INFO - training batch 301, loss: 0.417, 9632/60000 datapoints
2025-03-06 18:04:03,365 - INFO - training batch 351, loss: 0.312, 11232/60000 datapoints
2025-03-06 18:04:03,559 - INFO - training batch 401, loss: 0.548, 12832/60000 datapoints
2025-03-06 18:04:03,758 - INFO - training batch 451, loss: 0.287, 14432/60000 datapoints
2025-03-06 18:04:03,956 - INFO - training batch 501, loss: 0.496, 16032/60000 datapoints
2025-03-06 18:04:04,152 - INFO - training batch 551, loss: 0.679, 17632/60000 datapoints
2025-03-06 18:04:04,345 - INFO - training batch 601, loss: 0.238, 19232/60000 datapoints
2025-03-06 18:04:04,538 - INFO - training batch 651, loss: 0.243, 20832/60000 datapoints
2025-03-06 18:04:04,735 - INFO - training batch 701, loss: 0.455, 22432/60000 datapoints
2025-03-06 18:04:04,933 - INFO - training batch 751, loss: 0.343, 24032/60000 datapoints
2025-03-06 18:04:05,131 - INFO - training batch 801, loss: 0.429, 25632/60000 datapoints
2025-03-06 18:04:05,324 - INFO - training batch 851, loss: 0.436, 27232/60000 datapoints
2025-03-06 18:04:05,517 - INFO - training batch 901, loss: 0.486, 28832/60000 datapoints
2025-03-06 18:04:05,719 - INFO - training batch 951, loss: 0.294, 30432/60000 datapoints
2025-03-06 18:04:05,926 - INFO - training batch 1001, loss: 0.544, 32032/60000 datapoints
2025-03-06 18:04:06,122 - INFO - training batch 1051, loss: 0.555, 33632/60000 datapoints
2025-03-06 18:04:06,320 - INFO - training batch 1101, loss: 0.382, 35232/60000 datapoints
2025-03-06 18:04:06,526 - INFO - training batch 1151, loss: 0.404, 36832/60000 datapoints
2025-03-06 18:04:06,724 - INFO - training batch 1201, loss: 0.638, 38432/60000 datapoints
2025-03-06 18:04:06,927 - INFO - training batch 1251, loss: 0.267, 40032/60000 datapoints
2025-03-06 18:04:07,119 - INFO - training batch 1301, loss: 0.308, 41632/60000 datapoints
2025-03-06 18:04:07,313 - INFO - training batch 1351, loss: 0.370, 43232/60000 datapoints
2025-03-06 18:04:07,508 - INFO - training batch 1401, loss: 0.291, 44832/60000 datapoints
2025-03-06 18:04:07,712 - INFO - training batch 1451, loss: 0.427, 46432/60000 datapoints
2025-03-06 18:04:07,905 - INFO - training batch 1501, loss: 0.513, 48032/60000 datapoints
2025-03-06 18:04:08,109 - INFO - training batch 1551, loss: 0.419, 49632/60000 datapoints
2025-03-06 18:04:08,303 - INFO - training batch 1601, loss: 0.374, 51232/60000 datapoints
2025-03-06 18:04:08,498 - INFO - training batch 1651, loss: 0.537, 52832/60000 datapoints
2025-03-06 18:04:08,695 - INFO - training batch 1701, loss: 0.310, 54432/60000 datapoints
2025-03-06 18:04:08,888 - INFO - training batch 1751, loss: 0.451, 56032/60000 datapoints
2025-03-06 18:04:09,111 - INFO - training batch 1801, loss: 0.639, 57632/60000 datapoints
2025-03-06 18:04:09,304 - INFO - training batch 1851, loss: 0.448, 59232/60000 datapoints
2025-03-06 18:04:09,404 - INFO - validation batch 1, loss: 0.378, 32/10016 datapoints
2025-03-06 18:04:09,556 - INFO - validation batch 51, loss: 0.627, 1632/10016 datapoints
2025-03-06 18:04:09,711 - INFO - validation batch 101, loss: 0.331, 3232/10016 datapoints
2025-03-06 18:04:09,864 - INFO - validation batch 151, loss: 0.639, 4832/10016 datapoints
2025-03-06 18:04:10,016 - INFO - validation batch 201, loss: 0.528, 6432/10016 datapoints
2025-03-06 18:04:10,167 - INFO - validation batch 251, loss: 0.374, 8032/10016 datapoints
2025-03-06 18:04:10,321 - INFO - validation batch 301, loss: 0.269, 9632/10016 datapoints
2025-03-06 18:04:10,358 - INFO - Epoch 145/800 done.
2025-03-06 18:04:10,359 - INFO - Final validation performance:
Loss: 0.449, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:04:10,359 - INFO - Beginning epoch 146/800
2025-03-06 18:04:10,365 - INFO - training batch 1, loss: 0.718, 32/60000 datapoints
2025-03-06 18:04:10,558 - INFO - training batch 51, loss: 0.572, 1632/60000 datapoints
2025-03-06 18:04:10,754 - INFO - training batch 101, loss: 0.265, 3232/60000 datapoints
2025-03-06 18:04:10,945 - INFO - training batch 151, loss: 0.528, 4832/60000 datapoints
2025-03-06 18:04:11,134 - INFO - training batch 201, loss: 0.313, 6432/60000 datapoints
2025-03-06 18:04:11,329 - INFO - training batch 251, loss: 0.500, 8032/60000 datapoints
2025-03-06 18:04:11,521 - INFO - training batch 301, loss: 0.604, 9632/60000 datapoints
2025-03-06 18:04:11,717 - INFO - training batch 351, loss: 0.377, 11232/60000 datapoints
2025-03-06 18:04:11,910 - INFO - training batch 401, loss: 0.381, 12832/60000 datapoints
2025-03-06 18:04:12,100 - INFO - training batch 451, loss: 0.526, 14432/60000 datapoints
2025-03-06 18:04:12,291 - INFO - training batch 501, loss: 0.403, 16032/60000 datapoints
2025-03-06 18:04:12,484 - INFO - training batch 551, loss: 0.227, 17632/60000 datapoints
2025-03-06 18:04:12,677 - INFO - training batch 601, loss: 0.288, 19232/60000 datapoints
2025-03-06 18:04:12,870 - INFO - training batch 651, loss: 0.550, 20832/60000 datapoints
2025-03-06 18:04:13,065 - INFO - training batch 701, loss: 0.395, 22432/60000 datapoints
2025-03-06 18:04:13,258 - INFO - training batch 751, loss: 0.298, 24032/60000 datapoints
2025-03-06 18:04:13,450 - INFO - training batch 801, loss: 0.239, 25632/60000 datapoints
2025-03-06 18:04:13,646 - INFO - training batch 851, loss: 0.382, 27232/60000 datapoints
2025-03-06 18:04:13,838 - INFO - training batch 901, loss: 0.346, 28832/60000 datapoints
2025-03-06 18:04:14,026 - INFO - training batch 951, loss: 0.330, 30432/60000 datapoints
2025-03-06 18:04:14,216 - INFO - training batch 1001, loss: 0.472, 32032/60000 datapoints
2025-03-06 18:04:14,408 - INFO - training batch 1051, loss: 0.373, 33632/60000 datapoints
2025-03-06 18:04:14,599 - INFO - training batch 1101, loss: 0.684, 35232/60000 datapoints
2025-03-06 18:04:14,790 - INFO - training batch 1151, loss: 0.418, 36832/60000 datapoints
2025-03-06 18:04:14,988 - INFO - training batch 1201, loss: 0.278, 38432/60000 datapoints
2025-03-06 18:04:15,179 - INFO - training batch 1251, loss: 0.242, 40032/60000 datapoints
2025-03-06 18:04:15,370 - INFO - training batch 1301, loss: 0.610, 41632/60000 datapoints
2025-03-06 18:04:15,560 - INFO - training batch 1351, loss: 0.301, 43232/60000 datapoints
2025-03-06 18:04:15,755 - INFO - training batch 1401, loss: 0.467, 44832/60000 datapoints
2025-03-06 18:04:15,947 - INFO - training batch 1451, loss: 0.732, 46432/60000 datapoints
2025-03-06 18:04:16,135 - INFO - training batch 1501, loss: 0.306, 48032/60000 datapoints
2025-03-06 18:04:16,331 - INFO - training batch 1551, loss: 0.313, 49632/60000 datapoints
2025-03-06 18:04:16,529 - INFO - training batch 1601, loss: 0.412, 51232/60000 datapoints
2025-03-06 18:04:16,726 - INFO - training batch 1651, loss: 0.351, 52832/60000 datapoints
2025-03-06 18:04:16,918 - INFO - training batch 1701, loss: 0.470, 54432/60000 datapoints
2025-03-06 18:04:17,109 - INFO - training batch 1751, loss: 0.224, 56032/60000 datapoints
2025-03-06 18:04:17,298 - INFO - training batch 1801, loss: 0.489, 57632/60000 datapoints
2025-03-06 18:04:17,493 - INFO - training batch 1851, loss: 0.362, 59232/60000 datapoints
2025-03-06 18:04:17,591 - INFO - validation batch 1, loss: 0.287, 32/10016 datapoints
2025-03-06 18:04:17,747 - INFO - validation batch 51, loss: 0.356, 1632/10016 datapoints
2025-03-06 18:04:17,897 - INFO - validation batch 101, loss: 0.278, 3232/10016 datapoints
2025-03-06 18:04:18,047 - INFO - validation batch 151, loss: 0.631, 4832/10016 datapoints
2025-03-06 18:04:18,196 - INFO - validation batch 201, loss: 0.337, 6432/10016 datapoints
2025-03-06 18:04:18,349 - INFO - validation batch 251, loss: 0.249, 8032/10016 datapoints
2025-03-06 18:04:18,498 - INFO - validation batch 301, loss: 0.452, 9632/10016 datapoints
2025-03-06 18:04:18,533 - INFO - Epoch 146/800 done.
2025-03-06 18:04:18,533 - INFO - Final validation performance:
Loss: 0.370, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:04:18,534 - INFO - Beginning epoch 147/800
2025-03-06 18:04:18,539 - INFO - training batch 1, loss: 0.363, 32/60000 datapoints
2025-03-06 18:04:18,732 - INFO - training batch 51, loss: 0.324, 1632/60000 datapoints
2025-03-06 18:04:18,924 - INFO - training batch 101, loss: 0.392, 3232/60000 datapoints
2025-03-06 18:04:19,133 - INFO - training batch 151, loss: 0.438, 4832/60000 datapoints
2025-03-06 18:04:19,324 - INFO - training batch 201, loss: 0.306, 6432/60000 datapoints
2025-03-06 18:04:19,516 - INFO - training batch 251, loss: 0.450, 8032/60000 datapoints
2025-03-06 18:04:19,714 - INFO - training batch 301, loss: 0.403, 9632/60000 datapoints
2025-03-06 18:04:19,907 - INFO - training batch 351, loss: 0.587, 11232/60000 datapoints
2025-03-06 18:04:20,101 - INFO - training batch 401, loss: 0.468, 12832/60000 datapoints
2025-03-06 18:04:20,290 - INFO - training batch 451, loss: 0.351, 14432/60000 datapoints
2025-03-06 18:04:20,482 - INFO - training batch 501, loss: 0.290, 16032/60000 datapoints
2025-03-06 18:04:20,676 - INFO - training batch 551, loss: 0.681, 17632/60000 datapoints
2025-03-06 18:04:20,866 - INFO - training batch 601, loss: 0.543, 19232/60000 datapoints
2025-03-06 18:04:21,059 - INFO - training batch 651, loss: 0.737, 20832/60000 datapoints
2025-03-06 18:04:21,249 - INFO - training batch 701, loss: 0.364, 22432/60000 datapoints
2025-03-06 18:04:21,440 - INFO - training batch 751, loss: 0.275, 24032/60000 datapoints
2025-03-06 18:04:21,632 - INFO - training batch 801, loss: 0.362, 25632/60000 datapoints
2025-03-06 18:04:21,827 - INFO - training batch 851, loss: 0.284, 27232/60000 datapoints
2025-03-06 18:04:22,020 - INFO - training batch 901, loss: 0.320, 28832/60000 datapoints
2025-03-06 18:04:22,209 - INFO - training batch 951, loss: 0.350, 30432/60000 datapoints
2025-03-06 18:04:22,404 - INFO - training batch 1001, loss: 0.246, 32032/60000 datapoints
2025-03-06 18:04:22,599 - INFO - training batch 1051, loss: 0.499, 33632/60000 datapoints
2025-03-06 18:04:22,795 - INFO - training batch 1101, loss: 0.167, 35232/60000 datapoints
2025-03-06 18:04:22,991 - INFO - training batch 1151, loss: 0.360, 36832/60000 datapoints
2025-03-06 18:04:23,180 - INFO - training batch 1201, loss: 0.659, 38432/60000 datapoints
2025-03-06 18:04:23,371 - INFO - training batch 1251, loss: 0.483, 40032/60000 datapoints
2025-03-06 18:04:23,561 - INFO - training batch 1301, loss: 0.372, 41632/60000 datapoints
2025-03-06 18:04:23,757 - INFO - training batch 1351, loss: 0.385, 43232/60000 datapoints
2025-03-06 18:04:23,984 - INFO - training batch 1401, loss: 0.566, 44832/60000 datapoints
2025-03-06 18:04:24,178 - INFO - training batch 1451, loss: 0.582, 46432/60000 datapoints
2025-03-06 18:04:24,372 - INFO - training batch 1501, loss: 0.284, 48032/60000 datapoints
2025-03-06 18:04:24,569 - INFO - training batch 1551, loss: 0.473, 49632/60000 datapoints
2025-03-06 18:04:24,764 - INFO - training batch 1601, loss: 0.169, 51232/60000 datapoints
2025-03-06 18:04:24,964 - INFO - training batch 1651, loss: 0.502, 52832/60000 datapoints
2025-03-06 18:04:25,159 - INFO - training batch 1701, loss: 0.470, 54432/60000 datapoints
2025-03-06 18:04:25,352 - INFO - training batch 1751, loss: 0.375, 56032/60000 datapoints
2025-03-06 18:04:25,548 - INFO - training batch 1801, loss: 0.276, 57632/60000 datapoints
2025-03-06 18:04:25,748 - INFO - training batch 1851, loss: 0.368, 59232/60000 datapoints
2025-03-06 18:04:25,848 - INFO - validation batch 1, loss: 0.211, 32/10016 datapoints
2025-03-06 18:04:26,001 - INFO - validation batch 51, loss: 0.512, 1632/10016 datapoints
2025-03-06 18:04:26,155 - INFO - validation batch 101, loss: 0.527, 3232/10016 datapoints
2025-03-06 18:04:26,309 - INFO - validation batch 151, loss: 0.426, 4832/10016 datapoints
2025-03-06 18:04:26,461 - INFO - validation batch 201, loss: 0.339, 6432/10016 datapoints
2025-03-06 18:04:26,615 - INFO - validation batch 251, loss: 0.214, 8032/10016 datapoints
2025-03-06 18:04:26,768 - INFO - validation batch 301, loss: 0.362, 9632/10016 datapoints
2025-03-06 18:04:26,805 - INFO - Epoch 147/800 done.
2025-03-06 18:04:26,805 - INFO - Final validation performance:
Loss: 0.370, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:04:26,806 - INFO - Beginning epoch 148/800
2025-03-06 18:04:26,812 - INFO - training batch 1, loss: 0.360, 32/60000 datapoints
2025-03-06 18:04:27,006 - INFO - training batch 51, loss: 0.342, 1632/60000 datapoints
2025-03-06 18:04:27,197 - INFO - training batch 101, loss: 0.330, 3232/60000 datapoints
2025-03-06 18:04:27,388 - INFO - training batch 151, loss: 0.486, 4832/60000 datapoints
2025-03-06 18:04:27,578 - INFO - training batch 201, loss: 0.481, 6432/60000 datapoints
2025-03-06 18:04:27,783 - INFO - training batch 251, loss: 0.563, 8032/60000 datapoints
2025-03-06 18:04:27,975 - INFO - training batch 301, loss: 0.643, 9632/60000 datapoints
2025-03-06 18:04:28,170 - INFO - training batch 351, loss: 0.727, 11232/60000 datapoints
2025-03-06 18:04:28,361 - INFO - training batch 401, loss: 0.524, 12832/60000 datapoints
2025-03-06 18:04:28,552 - INFO - training batch 451, loss: 0.286, 14432/60000 datapoints
2025-03-06 18:04:28,746 - INFO - training batch 501, loss: 0.297, 16032/60000 datapoints
2025-03-06 18:04:28,937 - INFO - training batch 551, loss: 0.335, 17632/60000 datapoints
2025-03-06 18:04:29,144 - INFO - training batch 601, loss: 0.212, 19232/60000 datapoints
2025-03-06 18:04:29,341 - INFO - training batch 651, loss: 0.343, 20832/60000 datapoints
2025-03-06 18:04:29,534 - INFO - training batch 701, loss: 0.269, 22432/60000 datapoints
2025-03-06 18:04:29,730 - INFO - training batch 751, loss: 0.481, 24032/60000 datapoints
2025-03-06 18:04:29,922 - INFO - training batch 801, loss: 0.357, 25632/60000 datapoints
2025-03-06 18:04:30,114 - INFO - training batch 851, loss: 0.318, 27232/60000 datapoints
2025-03-06 18:04:30,305 - INFO - training batch 901, loss: 0.442, 28832/60000 datapoints
2025-03-06 18:04:30,497 - INFO - training batch 951, loss: 0.366, 30432/60000 datapoints
2025-03-06 18:04:30,692 - INFO - training batch 1001, loss: 0.361, 32032/60000 datapoints
2025-03-06 18:04:30,885 - INFO - training batch 1051, loss: 0.217, 33632/60000 datapoints
2025-03-06 18:04:31,077 - INFO - training batch 1101, loss: 0.651, 35232/60000 datapoints
2025-03-06 18:04:31,270 - INFO - training batch 1151, loss: 0.246, 36832/60000 datapoints
2025-03-06 18:04:31,461 - INFO - training batch 1201, loss: 0.373, 38432/60000 datapoints
2025-03-06 18:04:31,657 - INFO - training batch 1251, loss: 0.309, 40032/60000 datapoints
2025-03-06 18:04:31,859 - INFO - training batch 1301, loss: 0.296, 41632/60000 datapoints
2025-03-06 18:04:32,049 - INFO - training batch 1351, loss: 0.388, 43232/60000 datapoints
2025-03-06 18:04:32,242 - INFO - training batch 1401, loss: 0.380, 44832/60000 datapoints
2025-03-06 18:04:32,431 - INFO - training batch 1451, loss: 0.319, 46432/60000 datapoints
2025-03-06 18:04:32,628 - INFO - training batch 1501, loss: 0.398, 48032/60000 datapoints
2025-03-06 18:04:32,833 - INFO - training batch 1551, loss: 0.413, 49632/60000 datapoints
2025-03-06 18:04:33,025 - INFO - training batch 1601, loss: 0.369, 51232/60000 datapoints
2025-03-06 18:04:33,217 - INFO - training batch 1651, loss: 0.302, 52832/60000 datapoints
2025-03-06 18:04:33,409 - INFO - training batch 1701, loss: 0.324, 54432/60000 datapoints
2025-03-06 18:04:33,604 - INFO - training batch 1751, loss: 0.331, 56032/60000 datapoints
2025-03-06 18:04:33,798 - INFO - training batch 1801, loss: 0.548, 57632/60000 datapoints
2025-03-06 18:04:33,996 - INFO - training batch 1851, loss: 0.314, 59232/60000 datapoints
2025-03-06 18:04:34,095 - INFO - validation batch 1, loss: 0.315, 32/10016 datapoints
2025-03-06 18:04:34,246 - INFO - validation batch 51, loss: 0.272, 1632/10016 datapoints
2025-03-06 18:04:34,395 - INFO - validation batch 101, loss: 0.391, 3232/10016 datapoints
2025-03-06 18:04:34,545 - INFO - validation batch 151, loss: 0.528, 4832/10016 datapoints
2025-03-06 18:04:34,698 - INFO - validation batch 201, loss: 0.340, 6432/10016 datapoints
2025-03-06 18:04:34,854 - INFO - validation batch 251, loss: 0.249, 8032/10016 datapoints
2025-03-06 18:04:35,003 - INFO - validation batch 301, loss: 0.382, 9632/10016 datapoints
2025-03-06 18:04:35,041 - INFO - Epoch 148/800 done.
2025-03-06 18:04:35,041 - INFO - Final validation performance:
Loss: 0.354, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:04:35,041 - INFO - Beginning epoch 149/800
2025-03-06 18:04:35,047 - INFO - training batch 1, loss: 0.315, 32/60000 datapoints
2025-03-06 18:04:35,238 - INFO - training batch 51, loss: 0.422, 1632/60000 datapoints
2025-03-06 18:04:35,429 - INFO - training batch 101, loss: 0.303, 3232/60000 datapoints
2025-03-06 18:04:35,625 - INFO - training batch 151, loss: 0.326, 4832/60000 datapoints
2025-03-06 18:04:35,817 - INFO - training batch 201, loss: 0.521, 6432/60000 datapoints
2025-03-06 18:04:36,016 - INFO - training batch 251, loss: 0.449, 8032/60000 datapoints
2025-03-06 18:04:36,211 - INFO - training batch 301, loss: 0.509, 9632/60000 datapoints
2025-03-06 18:04:36,403 - INFO - training batch 351, loss: 0.397, 11232/60000 datapoints
2025-03-06 18:04:36,595 - INFO - training batch 401, loss: 0.473, 12832/60000 datapoints
2025-03-06 18:04:36,788 - INFO - training batch 451, loss: 0.370, 14432/60000 datapoints
2025-03-06 18:04:36,982 - INFO - training batch 501, loss: 0.443, 16032/60000 datapoints
2025-03-06 18:04:37,173 - INFO - training batch 551, loss: 0.263, 17632/60000 datapoints
2025-03-06 18:04:37,364 - INFO - training batch 601, loss: 0.484, 19232/60000 datapoints
2025-03-06 18:04:37,556 - INFO - training batch 651, loss: 0.438, 20832/60000 datapoints
2025-03-06 18:04:37,773 - INFO - training batch 701, loss: 0.687, 22432/60000 datapoints
2025-03-06 18:04:37,963 - INFO - training batch 751, loss: 0.326, 24032/60000 datapoints
2025-03-06 18:04:38,156 - INFO - training batch 801, loss: 0.477, 25632/60000 datapoints
2025-03-06 18:04:38,347 - INFO - training batch 851, loss: 0.363, 27232/60000 datapoints
2025-03-06 18:04:38,537 - INFO - training batch 901, loss: 0.275, 28832/60000 datapoints
2025-03-06 18:04:38,731 - INFO - training batch 951, loss: 0.346, 30432/60000 datapoints
2025-03-06 18:04:38,922 - INFO - training batch 1001, loss: 0.610, 32032/60000 datapoints
2025-03-06 18:04:39,115 - INFO - training batch 1051, loss: 0.423, 33632/60000 datapoints
2025-03-06 18:04:39,326 - INFO - training batch 1101, loss: 0.597, 35232/60000 datapoints
2025-03-06 18:04:39,517 - INFO - training batch 1151, loss: 0.174, 36832/60000 datapoints
2025-03-06 18:04:39,714 - INFO - training batch 1201, loss: 0.524, 38432/60000 datapoints
2025-03-06 18:04:39,907 - INFO - training batch 1251, loss: 0.428, 40032/60000 datapoints
2025-03-06 18:04:40,100 - INFO - training batch 1301, loss: 0.554, 41632/60000 datapoints
2025-03-06 18:04:40,290 - INFO - training batch 1351, loss: 0.423, 43232/60000 datapoints
2025-03-06 18:04:40,482 - INFO - training batch 1401, loss: 0.442, 44832/60000 datapoints
2025-03-06 18:04:40,679 - INFO - training batch 1451, loss: 0.240, 46432/60000 datapoints
2025-03-06 18:04:40,869 - INFO - training batch 1501, loss: 0.312, 48032/60000 datapoints
2025-03-06 18:04:41,061 - INFO - training batch 1551, loss: 0.542, 49632/60000 datapoints
2025-03-06 18:04:41,252 - INFO - training batch 1601, loss: 0.312, 51232/60000 datapoints
2025-03-06 18:04:41,442 - INFO - training batch 1651, loss: 0.419, 52832/60000 datapoints
2025-03-06 18:04:41,634 - INFO - training batch 1701, loss: 0.645, 54432/60000 datapoints
2025-03-06 18:04:41,828 - INFO - training batch 1751, loss: 0.292, 56032/60000 datapoints
2025-03-06 18:04:42,019 - INFO - training batch 1801, loss: 0.640, 57632/60000 datapoints
2025-03-06 18:04:42,211 - INFO - training batch 1851, loss: 0.505, 59232/60000 datapoints
2025-03-06 18:04:42,310 - INFO - validation batch 1, loss: 0.279, 32/10016 datapoints
2025-03-06 18:04:42,457 - INFO - validation batch 51, loss: 0.197, 1632/10016 datapoints
2025-03-06 18:04:42,607 - INFO - validation batch 101, loss: 0.437, 3232/10016 datapoints
2025-03-06 18:04:42,758 - INFO - validation batch 151, loss: 0.437, 4832/10016 datapoints
2025-03-06 18:04:42,906 - INFO - validation batch 201, loss: 0.315, 6432/10016 datapoints
2025-03-06 18:04:43,056 - INFO - validation batch 251, loss: 0.382, 8032/10016 datapoints
2025-03-06 18:04:43,206 - INFO - validation batch 301, loss: 0.534, 9632/10016 datapoints
2025-03-06 18:04:43,243 - INFO - Epoch 149/800 done.
2025-03-06 18:04:43,243 - INFO - Final validation performance:
Loss: 0.369, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:04:43,244 - INFO - Beginning epoch 150/800
2025-03-06 18:04:43,249 - INFO - training batch 1, loss: 0.402, 32/60000 datapoints
2025-03-06 18:04:43,437 - INFO - training batch 51, loss: 0.308, 1632/60000 datapoints
2025-03-06 18:04:43,633 - INFO - training batch 101, loss: 0.228, 3232/60000 datapoints
2025-03-06 18:04:43,827 - INFO - training batch 151, loss: 0.245, 4832/60000 datapoints
2025-03-06 18:04:44,024 - INFO - training batch 201, loss: 0.446, 6432/60000 datapoints
2025-03-06 18:04:44,217 - INFO - training batch 251, loss: 0.391, 8032/60000 datapoints
2025-03-06 18:04:44,413 - INFO - training batch 301, loss: 0.611, 9632/60000 datapoints
2025-03-06 18:04:44,610 - INFO - training batch 351, loss: 0.545, 11232/60000 datapoints
2025-03-06 18:04:44,803 - INFO - training batch 401, loss: 0.366, 12832/60000 datapoints
2025-03-06 18:04:45,001 - INFO - training batch 451, loss: 0.333, 14432/60000 datapoints
2025-03-06 18:04:45,197 - INFO - training batch 501, loss: 0.305, 16032/60000 datapoints
2025-03-06 18:04:45,391 - INFO - training batch 551, loss: 0.250, 17632/60000 datapoints
2025-03-06 18:04:45,585 - INFO - training batch 601, loss: 0.433, 19232/60000 datapoints
2025-03-06 18:04:45,785 - INFO - training batch 651, loss: 0.508, 20832/60000 datapoints
2025-03-06 18:04:45,980 - INFO - training batch 701, loss: 0.477, 22432/60000 datapoints
2025-03-06 18:04:46,176 - INFO - training batch 751, loss: 0.346, 24032/60000 datapoints
2025-03-06 18:04:46,370 - INFO - training batch 801, loss: 0.546, 25632/60000 datapoints
2025-03-06 18:04:46,564 - INFO - training batch 851, loss: 0.193, 27232/60000 datapoints
2025-03-06 18:04:46,764 - INFO - training batch 901, loss: 0.350, 28832/60000 datapoints
2025-03-06 18:04:46,959 - INFO - training batch 951, loss: 0.651, 30432/60000 datapoints
2025-03-06 18:04:47,153 - INFO - training batch 1001, loss: 0.345, 32032/60000 datapoints
2025-03-06 18:04:47,346 - INFO - training batch 1051, loss: 0.444, 33632/60000 datapoints
2025-03-06 18:04:47,539 - INFO - training batch 1101, loss: 0.525, 35232/60000 datapoints
2025-03-06 18:04:47,740 - INFO - training batch 1151, loss: 0.423, 36832/60000 datapoints
2025-03-06 18:04:47,933 - INFO - training batch 1201, loss: 0.364, 38432/60000 datapoints
2025-03-06 18:04:48,127 - INFO - training batch 1251, loss: 0.451, 40032/60000 datapoints
2025-03-06 18:04:48,321 - INFO - training batch 1301, loss: 0.767, 41632/60000 datapoints
2025-03-06 18:04:48,513 - INFO - training batch 1351, loss: 0.258, 43232/60000 datapoints
2025-03-06 18:04:48,708 - INFO - training batch 1401, loss: 0.464, 44832/60000 datapoints
2025-03-06 18:04:48,901 - INFO - training batch 1451, loss: 0.531, 46432/60000 datapoints
2025-03-06 18:04:49,097 - INFO - training batch 1501, loss: 0.224, 48032/60000 datapoints
2025-03-06 18:04:49,311 - INFO - training batch 1551, loss: 0.742, 49632/60000 datapoints
2025-03-06 18:04:49,506 - INFO - training batch 1601, loss: 0.376, 51232/60000 datapoints
2025-03-06 18:04:49,701 - INFO - training batch 1651, loss: 0.411, 52832/60000 datapoints
2025-03-06 18:04:49,900 - INFO - training batch 1701, loss: 0.173, 54432/60000 datapoints
2025-03-06 18:04:50,096 - INFO - training batch 1751, loss: 0.390, 56032/60000 datapoints
2025-03-06 18:04:50,290 - INFO - training batch 1801, loss: 0.758, 57632/60000 datapoints
2025-03-06 18:04:50,484 - INFO - training batch 1851, loss: 0.534, 59232/60000 datapoints
2025-03-06 18:04:50,585 - INFO - validation batch 1, loss: 0.414, 32/10016 datapoints
2025-03-06 18:04:50,740 - INFO - validation batch 51, loss: 0.407, 1632/10016 datapoints
2025-03-06 18:04:50,892 - INFO - validation batch 101, loss: 0.425, 3232/10016 datapoints
2025-03-06 18:04:51,045 - INFO - validation batch 151, loss: 0.400, 4832/10016 datapoints
2025-03-06 18:04:51,195 - INFO - validation batch 201, loss: 0.491, 6432/10016 datapoints
2025-03-06 18:04:51,393 - INFO - validation batch 251, loss: 0.274, 8032/10016 datapoints
2025-03-06 18:04:51,545 - INFO - validation batch 301, loss: 0.367, 9632/10016 datapoints
2025-03-06 18:04:51,582 - INFO - Epoch 150/800 done.
2025-03-06 18:04:51,582 - INFO - Final validation performance:
Loss: 0.397, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:04:51,583 - INFO - Beginning epoch 151/800
2025-03-06 18:04:51,588 - INFO - training batch 1, loss: 0.434, 32/60000 datapoints
2025-03-06 18:04:51,787 - INFO - training batch 51, loss: 0.386, 1632/60000 datapoints
2025-03-06 18:04:51,982 - INFO - training batch 101, loss: 0.338, 3232/60000 datapoints
2025-03-06 18:04:52,176 - INFO - training batch 151, loss: 0.417, 4832/60000 datapoints
2025-03-06 18:04:52,371 - INFO - training batch 201, loss: 0.479, 6432/60000 datapoints
2025-03-06 18:04:52,564 - INFO - training batch 251, loss: 0.536, 8032/60000 datapoints
2025-03-06 18:04:52,763 - INFO - training batch 301, loss: 0.396, 9632/60000 datapoints
2025-03-06 18:04:52,957 - INFO - training batch 351, loss: 0.432, 11232/60000 datapoints
2025-03-06 18:04:53,154 - INFO - training batch 401, loss: 0.443, 12832/60000 datapoints
2025-03-06 18:04:53,349 - INFO - training batch 451, loss: 0.365, 14432/60000 datapoints
2025-03-06 18:04:53,542 - INFO - training batch 501, loss: 0.347, 16032/60000 datapoints
2025-03-06 18:04:53,737 - INFO - training batch 551, loss: 0.471, 17632/60000 datapoints
2025-03-06 18:04:53,935 - INFO - training batch 601, loss: 0.321, 19232/60000 datapoints
2025-03-06 18:04:54,129 - INFO - training batch 651, loss: 0.341, 20832/60000 datapoints
2025-03-06 18:04:54,323 - INFO - training batch 701, loss: 0.202, 22432/60000 datapoints
2025-03-06 18:04:54,516 - INFO - training batch 751, loss: 0.621, 24032/60000 datapoints
2025-03-06 18:04:54,712 - INFO - training batch 801, loss: 0.441, 25632/60000 datapoints
2025-03-06 18:04:54,911 - INFO - training batch 851, loss: 0.414, 27232/60000 datapoints
2025-03-06 18:04:55,107 - INFO - training batch 901, loss: 0.606, 28832/60000 datapoints
2025-03-06 18:04:55,300 - INFO - training batch 951, loss: 0.367, 30432/60000 datapoints
2025-03-06 18:04:55,496 - INFO - training batch 1001, loss: 0.257, 32032/60000 datapoints
2025-03-06 18:04:55,693 - INFO - training batch 1051, loss: 0.367, 33632/60000 datapoints
2025-03-06 18:04:55,889 - INFO - training batch 1101, loss: 0.299, 35232/60000 datapoints
2025-03-06 18:04:56,083 - INFO - training batch 1151, loss: 0.345, 36832/60000 datapoints
2025-03-06 18:04:56,280 - INFO - training batch 1201, loss: 0.437, 38432/60000 datapoints
2025-03-06 18:04:56,473 - INFO - training batch 1251, loss: 0.468, 40032/60000 datapoints
2025-03-06 18:04:56,668 - INFO - training batch 1301, loss: 0.227, 41632/60000 datapoints
2025-03-06 18:04:56,865 - INFO - training batch 1351, loss: 0.346, 43232/60000 datapoints
2025-03-06 18:04:57,060 - INFO - training batch 1401, loss: 0.278, 44832/60000 datapoints
2025-03-06 18:04:57,253 - INFO - training batch 1451, loss: 0.224, 46432/60000 datapoints
2025-03-06 18:04:57,446 - INFO - training batch 1501, loss: 0.461, 48032/60000 datapoints
2025-03-06 18:04:57,640 - INFO - training batch 1551, loss: 0.456, 49632/60000 datapoints
2025-03-06 18:04:57,838 - INFO - training batch 1601, loss: 0.368, 51232/60000 datapoints
2025-03-06 18:04:58,031 - INFO - training batch 1651, loss: 0.251, 52832/60000 datapoints
2025-03-06 18:04:58,224 - INFO - training batch 1701, loss: 0.462, 54432/60000 datapoints
2025-03-06 18:04:58,418 - INFO - training batch 1751, loss: 0.565, 56032/60000 datapoints
2025-03-06 18:04:58,619 - INFO - training batch 1801, loss: 0.106, 57632/60000 datapoints
2025-03-06 18:04:58,812 - INFO - training batch 1851, loss: 0.286, 59232/60000 datapoints
2025-03-06 18:04:58,916 - INFO - validation batch 1, loss: 0.275, 32/10016 datapoints
2025-03-06 18:04:59,069 - INFO - validation batch 51, loss: 0.321, 1632/10016 datapoints
2025-03-06 18:04:59,222 - INFO - validation batch 101, loss: 0.561, 3232/10016 datapoints
2025-03-06 18:04:59,395 - INFO - validation batch 151, loss: 0.204, 4832/10016 datapoints
2025-03-06 18:04:59,548 - INFO - validation batch 201, loss: 0.242, 6432/10016 datapoints
2025-03-06 18:04:59,703 - INFO - validation batch 251, loss: 0.202, 8032/10016 datapoints
2025-03-06 18:04:59,857 - INFO - validation batch 301, loss: 0.692, 9632/10016 datapoints
2025-03-06 18:04:59,896 - INFO - Epoch 151/800 done.
2025-03-06 18:04:59,896 - INFO - Final validation performance:
Loss: 0.357, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:04:59,896 - INFO - Beginning epoch 152/800
2025-03-06 18:04:59,902 - INFO - training batch 1, loss: 0.269, 32/60000 datapoints
2025-03-06 18:05:00,098 - INFO - training batch 51, loss: 0.596, 1632/60000 datapoints
2025-03-06 18:05:00,289 - INFO - training batch 101, loss: 0.319, 3232/60000 datapoints
2025-03-06 18:05:00,484 - INFO - training batch 151, loss: 0.468, 4832/60000 datapoints
2025-03-06 18:05:00,678 - INFO - training batch 201, loss: 0.695, 6432/60000 datapoints
2025-03-06 18:05:00,871 - INFO - training batch 251, loss: 0.608, 8032/60000 datapoints
2025-03-06 18:05:01,065 - INFO - training batch 301, loss: 0.474, 9632/60000 datapoints
2025-03-06 18:05:01,257 - INFO - training batch 351, loss: 0.576, 11232/60000 datapoints
2025-03-06 18:05:01,449 - INFO - training batch 401, loss: 0.752, 12832/60000 datapoints
2025-03-06 18:05:01,646 - INFO - training batch 451, loss: 0.439, 14432/60000 datapoints
2025-03-06 18:05:01,842 - INFO - training batch 501, loss: 0.451, 16032/60000 datapoints
2025-03-06 18:05:02,032 - INFO - training batch 551, loss: 0.524, 17632/60000 datapoints
2025-03-06 18:05:02,222 - INFO - training batch 601, loss: 0.626, 19232/60000 datapoints
2025-03-06 18:05:02,411 - INFO - training batch 651, loss: 0.340, 20832/60000 datapoints
2025-03-06 18:05:02,603 - INFO - training batch 701, loss: 0.458, 22432/60000 datapoints
2025-03-06 18:05:02,796 - INFO - training batch 751, loss: 0.401, 24032/60000 datapoints
2025-03-06 18:05:02,988 - INFO - training batch 801, loss: 0.496, 25632/60000 datapoints
2025-03-06 18:05:03,187 - INFO - training batch 851, loss: 0.400, 27232/60000 datapoints
2025-03-06 18:05:03,381 - INFO - training batch 901, loss: 0.307, 28832/60000 datapoints
2025-03-06 18:05:03,572 - INFO - training batch 951, loss: 0.354, 30432/60000 datapoints
2025-03-06 18:05:03,768 - INFO - training batch 1001, loss: 0.525, 32032/60000 datapoints
2025-03-06 18:05:03,962 - INFO - training batch 1051, loss: 0.507, 33632/60000 datapoints
2025-03-06 18:05:04,154 - INFO - training batch 1101, loss: 0.322, 35232/60000 datapoints
2025-03-06 18:05:04,346 - INFO - training batch 1151, loss: 0.405, 36832/60000 datapoints
2025-03-06 18:05:04,538 - INFO - training batch 1201, loss: 0.306, 38432/60000 datapoints
2025-03-06 18:05:04,737 - INFO - training batch 1251, loss: 0.333, 40032/60000 datapoints
2025-03-06 18:05:04,935 - INFO - training batch 1301, loss: 0.355, 41632/60000 datapoints
2025-03-06 18:05:05,128 - INFO - training batch 1351, loss: 0.428, 43232/60000 datapoints
2025-03-06 18:05:05,318 - INFO - training batch 1401, loss: 0.610, 44832/60000 datapoints
2025-03-06 18:05:05,512 - INFO - training batch 1451, loss: 0.410, 46432/60000 datapoints
2025-03-06 18:05:05,705 - INFO - training batch 1501, loss: 0.657, 48032/60000 datapoints
2025-03-06 18:05:05,900 - INFO - training batch 1551, loss: 0.295, 49632/60000 datapoints
2025-03-06 18:05:06,093 - INFO - training batch 1601, loss: 0.228, 51232/60000 datapoints
2025-03-06 18:05:06,288 - INFO - training batch 1651, loss: 0.366, 52832/60000 datapoints
2025-03-06 18:05:06,478 - INFO - training batch 1701, loss: 0.253, 54432/60000 datapoints
2025-03-06 18:05:06,671 - INFO - training batch 1751, loss: 0.661, 56032/60000 datapoints
2025-03-06 18:05:06,867 - INFO - training batch 1801, loss: 0.336, 57632/60000 datapoints
2025-03-06 18:05:07,061 - INFO - training batch 1851, loss: 0.265, 59232/60000 datapoints
2025-03-06 18:05:07,158 - INFO - validation batch 1, loss: 0.347, 32/10016 datapoints
2025-03-06 18:05:07,308 - INFO - validation batch 51, loss: 0.243, 1632/10016 datapoints
2025-03-06 18:05:07,457 - INFO - validation batch 101, loss: 0.274, 3232/10016 datapoints
2025-03-06 18:05:07,610 - INFO - validation batch 151, loss: 0.379, 4832/10016 datapoints
2025-03-06 18:05:07,762 - INFO - validation batch 201, loss: 0.561, 6432/10016 datapoints
2025-03-06 18:05:07,916 - INFO - validation batch 251, loss: 0.280, 8032/10016 datapoints
2025-03-06 18:05:08,071 - INFO - validation batch 301, loss: 0.338, 9632/10016 datapoints
2025-03-06 18:05:08,106 - INFO - Epoch 152/800 done.
2025-03-06 18:05:08,106 - INFO - Final validation performance:
Loss: 0.346, top-1 acc: 0.895top-5 acc: 0.895
2025-03-06 18:05:08,106 - INFO - Beginning epoch 153/800
2025-03-06 18:05:08,112 - INFO - training batch 1, loss: 0.296, 32/60000 datapoints
2025-03-06 18:05:08,307 - INFO - training batch 51, loss: 0.591, 1632/60000 datapoints
2025-03-06 18:05:08,502 - INFO - training batch 101, loss: 0.409, 3232/60000 datapoints
2025-03-06 18:05:08,706 - INFO - training batch 151, loss: 0.672, 4832/60000 datapoints
2025-03-06 18:05:08,924 - INFO - training batch 201, loss: 0.344, 6432/60000 datapoints
2025-03-06 18:05:09,117 - INFO - training batch 251, loss: 0.567, 8032/60000 datapoints
2025-03-06 18:05:09,308 - INFO - training batch 301, loss: 0.246, 9632/60000 datapoints
2025-03-06 18:05:09,520 - INFO - training batch 351, loss: 0.304, 11232/60000 datapoints
2025-03-06 18:05:09,715 - INFO - training batch 401, loss: 0.280, 12832/60000 datapoints
2025-03-06 18:05:09,906 - INFO - training batch 451, loss: 0.395, 14432/60000 datapoints
2025-03-06 18:05:10,098 - INFO - training batch 501, loss: 0.353, 16032/60000 datapoints
2025-03-06 18:05:10,289 - INFO - training batch 551, loss: 0.454, 17632/60000 datapoints
2025-03-06 18:05:10,483 - INFO - training batch 601, loss: 0.204, 19232/60000 datapoints
2025-03-06 18:05:10,680 - INFO - training batch 651, loss: 0.374, 20832/60000 datapoints
2025-03-06 18:05:10,871 - INFO - training batch 701, loss: 0.352, 22432/60000 datapoints
2025-03-06 18:05:11,062 - INFO - training batch 751, loss: 0.314, 24032/60000 datapoints
2025-03-06 18:05:11,254 - INFO - training batch 801, loss: 0.481, 25632/60000 datapoints
2025-03-06 18:05:11,444 - INFO - training batch 851, loss: 0.412, 27232/60000 datapoints
2025-03-06 18:05:11,638 - INFO - training batch 901, loss: 0.483, 28832/60000 datapoints
2025-03-06 18:05:11,833 - INFO - training batch 951, loss: 0.419, 30432/60000 datapoints
2025-03-06 18:05:12,026 - INFO - training batch 1001, loss: 0.196, 32032/60000 datapoints
2025-03-06 18:05:12,219 - INFO - training batch 1051, loss: 0.398, 33632/60000 datapoints
2025-03-06 18:05:12,409 - INFO - training batch 1101, loss: 0.303, 35232/60000 datapoints
2025-03-06 18:05:12,602 - INFO - training batch 1151, loss: 0.255, 36832/60000 datapoints
2025-03-06 18:05:12,794 - INFO - training batch 1201, loss: 0.600, 38432/60000 datapoints
2025-03-06 18:05:12,984 - INFO - training batch 1251, loss: 0.626, 40032/60000 datapoints
2025-03-06 18:05:13,176 - INFO - training batch 1301, loss: 0.696, 41632/60000 datapoints
2025-03-06 18:05:13,367 - INFO - training batch 1351, loss: 0.490, 43232/60000 datapoints
2025-03-06 18:05:13,558 - INFO - training batch 1401, loss: 0.445, 44832/60000 datapoints
2025-03-06 18:05:13,751 - INFO - training batch 1451, loss: 0.316, 46432/60000 datapoints
2025-03-06 18:05:13,944 - INFO - training batch 1501, loss: 0.564, 48032/60000 datapoints
2025-03-06 18:05:14,137 - INFO - training batch 1551, loss: 0.528, 49632/60000 datapoints
2025-03-06 18:05:14,328 - INFO - training batch 1601, loss: 0.198, 51232/60000 datapoints
2025-03-06 18:05:14,517 - INFO - training batch 1651, loss: 0.191, 52832/60000 datapoints
2025-03-06 18:05:14,711 - INFO - training batch 1701, loss: 0.491, 54432/60000 datapoints
2025-03-06 18:05:14,906 - INFO - training batch 1751, loss: 0.326, 56032/60000 datapoints
2025-03-06 18:05:15,100 - INFO - training batch 1801, loss: 0.447, 57632/60000 datapoints
2025-03-06 18:05:15,289 - INFO - training batch 1851, loss: 0.272, 59232/60000 datapoints
2025-03-06 18:05:15,388 - INFO - validation batch 1, loss: 0.439, 32/10016 datapoints
2025-03-06 18:05:15,538 - INFO - validation batch 51, loss: 0.485, 1632/10016 datapoints
2025-03-06 18:05:15,691 - INFO - validation batch 101, loss: 0.227, 3232/10016 datapoints
2025-03-06 18:05:15,843 - INFO - validation batch 151, loss: 0.416, 4832/10016 datapoints
2025-03-06 18:05:15,992 - INFO - validation batch 201, loss: 0.289, 6432/10016 datapoints
2025-03-06 18:05:16,144 - INFO - validation batch 251, loss: 0.272, 8032/10016 datapoints
2025-03-06 18:05:16,295 - INFO - validation batch 301, loss: 0.309, 9632/10016 datapoints
2025-03-06 18:05:16,331 - INFO - Epoch 153/800 done.
2025-03-06 18:05:16,331 - INFO - Final validation performance:
Loss: 0.348, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:05:16,332 - INFO - Beginning epoch 154/800
2025-03-06 18:05:16,337 - INFO - training batch 1, loss: 0.387, 32/60000 datapoints
2025-03-06 18:05:16,532 - INFO - training batch 51, loss: 0.308, 1632/60000 datapoints
2025-03-06 18:05:16,734 - INFO - training batch 101, loss: 0.229, 3232/60000 datapoints
2025-03-06 18:05:16,929 - INFO - training batch 151, loss: 0.303, 4832/60000 datapoints
2025-03-06 18:05:17,125 - INFO - training batch 201, loss: 0.342, 6432/60000 datapoints
2025-03-06 18:05:17,324 - INFO - training batch 251, loss: 0.336, 8032/60000 datapoints
2025-03-06 18:05:17,516 - INFO - training batch 301, loss: 0.485, 9632/60000 datapoints
2025-03-06 18:05:17,714 - INFO - training batch 351, loss: 0.354, 11232/60000 datapoints
2025-03-06 18:05:17,912 - INFO - training batch 401, loss: 0.328, 12832/60000 datapoints
2025-03-06 18:05:18,105 - INFO - training batch 451, loss: 0.341, 14432/60000 datapoints
2025-03-06 18:05:18,300 - INFO - training batch 501, loss: 0.398, 16032/60000 datapoints
2025-03-06 18:05:18,492 - INFO - training batch 551, loss: 0.326, 17632/60000 datapoints
2025-03-06 18:05:18,689 - INFO - training batch 601, loss: 0.334, 19232/60000 datapoints
2025-03-06 18:05:18,882 - INFO - training batch 651, loss: 0.595, 20832/60000 datapoints
2025-03-06 18:05:19,075 - INFO - training batch 701, loss: 0.540, 22432/60000 datapoints
2025-03-06 18:05:19,271 - INFO - training batch 751, loss: 0.377, 24032/60000 datapoints
2025-03-06 18:05:19,464 - INFO - training batch 801, loss: 0.373, 25632/60000 datapoints
2025-03-06 18:05:19,686 - INFO - training batch 851, loss: 0.454, 27232/60000 datapoints
2025-03-06 18:05:19,883 - INFO - training batch 901, loss: 0.355, 28832/60000 datapoints
2025-03-06 18:05:20,079 - INFO - training batch 951, loss: 0.448, 30432/60000 datapoints
2025-03-06 18:05:20,277 - INFO - training batch 1001, loss: 0.375, 32032/60000 datapoints
2025-03-06 18:05:20,472 - INFO - training batch 1051, loss: 0.218, 33632/60000 datapoints
2025-03-06 18:05:20,668 - INFO - training batch 1101, loss: 0.540, 35232/60000 datapoints
2025-03-06 18:05:20,864 - INFO - training batch 1151, loss: 0.438, 36832/60000 datapoints
2025-03-06 18:05:21,059 - INFO - training batch 1201, loss: 0.438, 38432/60000 datapoints
2025-03-06 18:05:21,256 - INFO - training batch 1251, loss: 0.672, 40032/60000 datapoints
2025-03-06 18:05:21,447 - INFO - training batch 1301, loss: 0.717, 41632/60000 datapoints
2025-03-06 18:05:21,641 - INFO - training batch 1351, loss: 0.335, 43232/60000 datapoints
2025-03-06 18:05:21,837 - INFO - training batch 1401, loss: 0.188, 44832/60000 datapoints
2025-03-06 18:05:22,034 - INFO - training batch 1451, loss: 0.745, 46432/60000 datapoints
2025-03-06 18:05:22,227 - INFO - training batch 1501, loss: 0.514, 48032/60000 datapoints
2025-03-06 18:05:22,419 - INFO - training batch 1551, loss: 0.229, 49632/60000 datapoints
2025-03-06 18:05:22,615 - INFO - training batch 1601, loss: 0.189, 51232/60000 datapoints
2025-03-06 18:05:22,810 - INFO - training batch 1651, loss: 0.602, 52832/60000 datapoints
2025-03-06 18:05:23,006 - INFO - training batch 1701, loss: 0.361, 54432/60000 datapoints
2025-03-06 18:05:23,205 - INFO - training batch 1751, loss: 0.305, 56032/60000 datapoints
2025-03-06 18:05:23,399 - INFO - training batch 1801, loss: 0.442, 57632/60000 datapoints
2025-03-06 18:05:23,596 - INFO - training batch 1851, loss: 0.277, 59232/60000 datapoints
2025-03-06 18:05:23,697 - INFO - validation batch 1, loss: 0.391, 32/10016 datapoints
2025-03-06 18:05:23,849 - INFO - validation batch 51, loss: 0.442, 1632/10016 datapoints
2025-03-06 18:05:24,005 - INFO - validation batch 101, loss: 0.452, 3232/10016 datapoints
2025-03-06 18:05:24,156 - INFO - validation batch 151, loss: 0.348, 4832/10016 datapoints
2025-03-06 18:05:24,340 - INFO - validation batch 201, loss: 0.201, 6432/10016 datapoints
2025-03-06 18:05:24,492 - INFO - validation batch 251, loss: 0.382, 8032/10016 datapoints
2025-03-06 18:05:24,648 - INFO - validation batch 301, loss: 0.360, 9632/10016 datapoints
2025-03-06 18:05:24,684 - INFO - Epoch 154/800 done.
2025-03-06 18:05:24,684 - INFO - Final validation performance:
Loss: 0.368, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:05:24,685 - INFO - Beginning epoch 155/800
2025-03-06 18:05:24,690 - INFO - training batch 1, loss: 0.374, 32/60000 datapoints
2025-03-06 18:05:24,889 - INFO - training batch 51, loss: 0.268, 1632/60000 datapoints
2025-03-06 18:05:25,083 - INFO - training batch 101, loss: 0.407, 3232/60000 datapoints
2025-03-06 18:05:25,278 - INFO - training batch 151, loss: 0.319, 4832/60000 datapoints
2025-03-06 18:05:25,473 - INFO - training batch 201, loss: 0.325, 6432/60000 datapoints
2025-03-06 18:05:25,672 - INFO - training batch 251, loss: 0.381, 8032/60000 datapoints
2025-03-06 18:05:25,870 - INFO - training batch 301, loss: 0.265, 9632/60000 datapoints
2025-03-06 18:05:26,064 - INFO - training batch 351, loss: 0.272, 11232/60000 datapoints
2025-03-06 18:05:26,264 - INFO - training batch 401, loss: 0.416, 12832/60000 datapoints
2025-03-06 18:05:26,456 - INFO - training batch 451, loss: 0.642, 14432/60000 datapoints
2025-03-06 18:05:26,650 - INFO - training batch 501, loss: 0.159, 16032/60000 datapoints
2025-03-06 18:05:26,847 - INFO - training batch 551, loss: 0.262, 17632/60000 datapoints
2025-03-06 18:05:27,041 - INFO - training batch 601, loss: 0.408, 19232/60000 datapoints
2025-03-06 18:05:27,234 - INFO - training batch 651, loss: 0.344, 20832/60000 datapoints
2025-03-06 18:05:27,427 - INFO - training batch 701, loss: 0.159, 22432/60000 datapoints
2025-03-06 18:05:27,622 - INFO - training batch 751, loss: 0.348, 24032/60000 datapoints
2025-03-06 18:05:27,819 - INFO - training batch 801, loss: 0.468, 25632/60000 datapoints
2025-03-06 18:05:28,016 - INFO - training batch 851, loss: 0.300, 27232/60000 datapoints
2025-03-06 18:05:28,213 - INFO - training batch 901, loss: 0.232, 28832/60000 datapoints
2025-03-06 18:05:28,409 - INFO - training batch 951, loss: 0.727, 30432/60000 datapoints
2025-03-06 18:05:28,606 - INFO - training batch 1001, loss: 0.368, 32032/60000 datapoints
2025-03-06 18:05:28,801 - INFO - training batch 1051, loss: 0.279, 33632/60000 datapoints
2025-03-06 18:05:28,995 - INFO - training batch 1101, loss: 0.406, 35232/60000 datapoints
2025-03-06 18:05:29,189 - INFO - training batch 1151, loss: 0.427, 36832/60000 datapoints
2025-03-06 18:05:29,385 - INFO - training batch 1201, loss: 0.511, 38432/60000 datapoints
2025-03-06 18:05:29,580 - INFO - training batch 1251, loss: 0.291, 40032/60000 datapoints
2025-03-06 18:05:29,792 - INFO - training batch 1301, loss: 0.543, 41632/60000 datapoints
2025-03-06 18:05:29,989 - INFO - training batch 1351, loss: 0.257, 43232/60000 datapoints
2025-03-06 18:05:30,184 - INFO - training batch 1401, loss: 0.504, 44832/60000 datapoints
2025-03-06 18:05:30,381 - INFO - training batch 1451, loss: 0.641, 46432/60000 datapoints
2025-03-06 18:05:30,573 - INFO - training batch 1501, loss: 0.416, 48032/60000 datapoints
2025-03-06 18:05:30,769 - INFO - training batch 1551, loss: 0.331, 49632/60000 datapoints
2025-03-06 18:05:30,964 - INFO - training batch 1601, loss: 0.440, 51232/60000 datapoints
2025-03-06 18:05:31,155 - INFO - training batch 1651, loss: 0.269, 52832/60000 datapoints
2025-03-06 18:05:31,351 - INFO - training batch 1701, loss: 0.369, 54432/60000 datapoints
2025-03-06 18:05:31,546 - INFO - training batch 1751, loss: 0.609, 56032/60000 datapoints
2025-03-06 18:05:31,745 - INFO - training batch 1801, loss: 0.426, 57632/60000 datapoints
2025-03-06 18:05:31,944 - INFO - training batch 1851, loss: 0.353, 59232/60000 datapoints
2025-03-06 18:05:32,045 - INFO - validation batch 1, loss: 0.148, 32/10016 datapoints
2025-03-06 18:05:32,197 - INFO - validation batch 51, loss: 0.441, 1632/10016 datapoints
2025-03-06 18:05:32,353 - INFO - validation batch 101, loss: 0.202, 3232/10016 datapoints
2025-03-06 18:05:32,506 - INFO - validation batch 151, loss: 0.225, 4832/10016 datapoints
2025-03-06 18:05:32,661 - INFO - validation batch 201, loss: 0.404, 6432/10016 datapoints
2025-03-06 18:05:32,815 - INFO - validation batch 251, loss: 0.497, 8032/10016 datapoints
2025-03-06 18:05:32,970 - INFO - validation batch 301, loss: 0.302, 9632/10016 datapoints
2025-03-06 18:05:33,007 - INFO - Epoch 155/800 done.
2025-03-06 18:05:33,007 - INFO - Final validation performance:
Loss: 0.317, top-1 acc: 0.895top-5 acc: 0.895
2025-03-06 18:05:33,008 - INFO - Beginning epoch 156/800
2025-03-06 18:05:33,014 - INFO - training batch 1, loss: 0.474, 32/60000 datapoints
2025-03-06 18:05:33,207 - INFO - training batch 51, loss: 0.327, 1632/60000 datapoints
2025-03-06 18:05:33,403 - INFO - training batch 101, loss: 0.377, 3232/60000 datapoints
2025-03-06 18:05:33,596 - INFO - training batch 151, loss: 0.311, 4832/60000 datapoints
2025-03-06 18:05:33,786 - INFO - training batch 201, loss: 0.185, 6432/60000 datapoints
2025-03-06 18:05:33,983 - INFO - training batch 251, loss: 0.259, 8032/60000 datapoints
2025-03-06 18:05:34,171 - INFO - training batch 301, loss: 0.241, 9632/60000 datapoints
2025-03-06 18:05:34,364 - INFO - training batch 351, loss: 0.191, 11232/60000 datapoints
2025-03-06 18:05:34,553 - INFO - training batch 401, loss: 0.164, 12832/60000 datapoints
2025-03-06 18:05:34,749 - INFO - training batch 451, loss: 0.441, 14432/60000 datapoints
2025-03-06 18:05:34,946 - INFO - training batch 501, loss: 0.535, 16032/60000 datapoints
2025-03-06 18:05:35,139 - INFO - training batch 551, loss: 0.577, 17632/60000 datapoints
2025-03-06 18:05:35,333 - INFO - training batch 601, loss: 0.224, 19232/60000 datapoints
2025-03-06 18:05:35,523 - INFO - training batch 651, loss: 0.612, 20832/60000 datapoints
2025-03-06 18:05:35,716 - INFO - training batch 701, loss: 0.384, 22432/60000 datapoints
2025-03-06 18:05:35,911 - INFO - training batch 751, loss: 0.444, 24032/60000 datapoints
2025-03-06 18:05:36,103 - INFO - training batch 801, loss: 0.300, 25632/60000 datapoints
2025-03-06 18:05:36,299 - INFO - training batch 851, loss: 0.291, 27232/60000 datapoints
2025-03-06 18:05:36,491 - INFO - training batch 901, loss: 0.317, 28832/60000 datapoints
2025-03-06 18:05:36,688 - INFO - training batch 951, loss: 0.243, 30432/60000 datapoints
2025-03-06 18:05:36,882 - INFO - training batch 1001, loss: 0.419, 32032/60000 datapoints
2025-03-06 18:05:37,074 - INFO - training batch 1051, loss: 0.353, 33632/60000 datapoints
2025-03-06 18:05:37,277 - INFO - training batch 1101, loss: 0.277, 35232/60000 datapoints
2025-03-06 18:05:37,471 - INFO - training batch 1151, loss: 0.439, 36832/60000 datapoints
2025-03-06 18:05:37,681 - INFO - training batch 1201, loss: 0.356, 38432/60000 datapoints
2025-03-06 18:05:37,881 - INFO - training batch 1251, loss: 0.319, 40032/60000 datapoints
2025-03-06 18:05:38,076 - INFO - training batch 1301, loss: 0.724, 41632/60000 datapoints
2025-03-06 18:05:38,272 - INFO - training batch 1351, loss: 0.270, 43232/60000 datapoints
2025-03-06 18:05:38,466 - INFO - training batch 1401, loss: 0.400, 44832/60000 datapoints
2025-03-06 18:05:38,662 - INFO - training batch 1451, loss: 0.310, 46432/60000 datapoints
2025-03-06 18:05:38,856 - INFO - training batch 1501, loss: 0.180, 48032/60000 datapoints
2025-03-06 18:05:39,054 - INFO - training batch 1551, loss: 0.250, 49632/60000 datapoints
2025-03-06 18:05:39,249 - INFO - training batch 1601, loss: 0.516, 51232/60000 datapoints
2025-03-06 18:05:39,444 - INFO - training batch 1651, loss: 0.352, 52832/60000 datapoints
2025-03-06 18:05:39,641 - INFO - training batch 1701, loss: 0.210, 54432/60000 datapoints
2025-03-06 18:05:39,855 - INFO - training batch 1751, loss: 0.513, 56032/60000 datapoints
2025-03-06 18:05:40,051 - INFO - training batch 1801, loss: 0.236, 57632/60000 datapoints
2025-03-06 18:05:40,245 - INFO - training batch 1851, loss: 0.642, 59232/60000 datapoints
2025-03-06 18:05:40,347 - INFO - validation batch 1, loss: 0.283, 32/10016 datapoints
2025-03-06 18:05:40,498 - INFO - validation batch 51, loss: 0.572, 1632/10016 datapoints
2025-03-06 18:05:40,654 - INFO - validation batch 101, loss: 0.331, 3232/10016 datapoints
2025-03-06 18:05:40,805 - INFO - validation batch 151, loss: 0.221, 4832/10016 datapoints
2025-03-06 18:05:40,958 - INFO - validation batch 201, loss: 0.558, 6432/10016 datapoints
2025-03-06 18:05:41,111 - INFO - validation batch 251, loss: 0.584, 8032/10016 datapoints
2025-03-06 18:05:41,267 - INFO - validation batch 301, loss: 0.412, 9632/10016 datapoints
2025-03-06 18:05:41,304 - INFO - Epoch 156/800 done.
2025-03-06 18:05:41,304 - INFO - Final validation performance:
Loss: 0.423, top-1 acc: 0.895top-5 acc: 0.895
2025-03-06 18:05:41,305 - INFO - Beginning epoch 157/800
2025-03-06 18:05:41,311 - INFO - training batch 1, loss: 0.575, 32/60000 datapoints
2025-03-06 18:05:41,504 - INFO - training batch 51, loss: 0.389, 1632/60000 datapoints
2025-03-06 18:05:41,695 - INFO - training batch 101, loss: 0.530, 3232/60000 datapoints
2025-03-06 18:05:41,889 - INFO - training batch 151, loss: 0.570, 4832/60000 datapoints
2025-03-06 18:05:42,083 - INFO - training batch 201, loss: 0.290, 6432/60000 datapoints
2025-03-06 18:05:42,277 - INFO - training batch 251, loss: 0.495, 8032/60000 datapoints
2025-03-06 18:05:42,469 - INFO - training batch 301, loss: 0.509, 9632/60000 datapoints
2025-03-06 18:05:42,660 - INFO - training batch 351, loss: 0.490, 11232/60000 datapoints
2025-03-06 18:05:42,852 - INFO - training batch 401, loss: 0.215, 12832/60000 datapoints
2025-03-06 18:05:43,044 - INFO - training batch 451, loss: 0.443, 14432/60000 datapoints
2025-03-06 18:05:43,234 - INFO - training batch 501, loss: 0.427, 16032/60000 datapoints
2025-03-06 18:05:43,429 - INFO - training batch 551, loss: 0.378, 17632/60000 datapoints
2025-03-06 18:05:43,622 - INFO - training batch 601, loss: 0.369, 19232/60000 datapoints
2025-03-06 18:05:43,812 - INFO - training batch 651, loss: 0.334, 20832/60000 datapoints
2025-03-06 18:05:44,005 - INFO - training batch 701, loss: 0.375, 22432/60000 datapoints
2025-03-06 18:05:44,196 - INFO - training batch 751, loss: 0.456, 24032/60000 datapoints
2025-03-06 18:05:44,387 - INFO - training batch 801, loss: 0.239, 25632/60000 datapoints
2025-03-06 18:05:44,578 - INFO - training batch 851, loss: 0.257, 27232/60000 datapoints
2025-03-06 18:05:44,772 - INFO - training batch 901, loss: 0.579, 28832/60000 datapoints
2025-03-06 18:05:44,968 - INFO - training batch 951, loss: 0.490, 30432/60000 datapoints
2025-03-06 18:05:45,160 - INFO - training batch 1001, loss: 0.820, 32032/60000 datapoints
2025-03-06 18:05:45,352 - INFO - training batch 1051, loss: 0.346, 33632/60000 datapoints
2025-03-06 18:05:45,544 - INFO - training batch 1101, loss: 0.432, 35232/60000 datapoints
2025-03-06 18:05:45,738 - INFO - training batch 1151, loss: 0.809, 36832/60000 datapoints
2025-03-06 18:05:45,933 - INFO - training batch 1201, loss: 0.725, 38432/60000 datapoints
2025-03-06 18:05:46,127 - INFO - training batch 1251, loss: 0.440, 40032/60000 datapoints
2025-03-06 18:05:46,320 - INFO - training batch 1301, loss: 0.179, 41632/60000 datapoints
2025-03-06 18:05:46,515 - INFO - training batch 1351, loss: 0.604, 43232/60000 datapoints
2025-03-06 18:05:46,710 - INFO - training batch 1401, loss: 0.648, 44832/60000 datapoints
2025-03-06 18:05:46,902 - INFO - training batch 1451, loss: 0.280, 46432/60000 datapoints
2025-03-06 18:05:47,095 - INFO - training batch 1501, loss: 0.468, 48032/60000 datapoints
2025-03-06 18:05:47,286 - INFO - training batch 1551, loss: 0.323, 49632/60000 datapoints
2025-03-06 18:05:47,476 - INFO - training batch 1601, loss: 0.295, 51232/60000 datapoints
2025-03-06 18:05:47,668 - INFO - training batch 1651, loss: 0.282, 52832/60000 datapoints
2025-03-06 18:05:47,859 - INFO - training batch 1701, loss: 0.551, 54432/60000 datapoints
2025-03-06 18:05:48,053 - INFO - training batch 1751, loss: 0.630, 56032/60000 datapoints
2025-03-06 18:05:48,244 - INFO - training batch 1801, loss: 0.363, 57632/60000 datapoints
2025-03-06 18:05:48,437 - INFO - training batch 1851, loss: 0.514, 59232/60000 datapoints
2025-03-06 18:05:48,536 - INFO - validation batch 1, loss: 0.310, 32/10016 datapoints
2025-03-06 18:05:48,687 - INFO - validation batch 51, loss: 0.522, 1632/10016 datapoints
2025-03-06 18:05:48,838 - INFO - validation batch 101, loss: 0.522, 3232/10016 datapoints
2025-03-06 18:05:48,987 - INFO - validation batch 151, loss: 0.464, 4832/10016 datapoints
2025-03-06 18:05:49,137 - INFO - validation batch 201, loss: 0.491, 6432/10016 datapoints
2025-03-06 18:05:49,285 - INFO - validation batch 251, loss: 0.577, 8032/10016 datapoints
2025-03-06 18:05:49,436 - INFO - validation batch 301, loss: 0.610, 9632/10016 datapoints
2025-03-06 18:05:49,472 - INFO - Epoch 157/800 done.
2025-03-06 18:05:49,472 - INFO - Final validation performance:
Loss: 0.500, top-1 acc: 0.895top-5 acc: 0.895
2025-03-06 18:05:49,473 - INFO - Beginning epoch 158/800
2025-03-06 18:05:49,478 - INFO - training batch 1, loss: 0.593, 32/60000 datapoints
2025-03-06 18:05:49,671 - INFO - training batch 51, loss: 0.390, 1632/60000 datapoints
2025-03-06 18:05:49,883 - INFO - training batch 101, loss: 0.344, 3232/60000 datapoints
2025-03-06 18:05:50,078 - INFO - training batch 151, loss: 0.361, 4832/60000 datapoints
2025-03-06 18:05:50,268 - INFO - training batch 201, loss: 0.384, 6432/60000 datapoints
2025-03-06 18:05:50,459 - INFO - training batch 251, loss: 0.358, 8032/60000 datapoints
2025-03-06 18:05:50,654 - INFO - training batch 301, loss: 0.456, 9632/60000 datapoints
2025-03-06 18:05:50,845 - INFO - training batch 351, loss: 0.237, 11232/60000 datapoints
2025-03-06 18:05:51,037 - INFO - training batch 401, loss: 0.248, 12832/60000 datapoints
2025-03-06 18:05:51,228 - INFO - training batch 451, loss: 0.358, 14432/60000 datapoints
2025-03-06 18:05:51,419 - INFO - training batch 501, loss: 0.511, 16032/60000 datapoints
2025-03-06 18:05:51,613 - INFO - training batch 551, loss: 0.494, 17632/60000 datapoints
2025-03-06 18:05:51,805 - INFO - training batch 601, loss: 0.232, 19232/60000 datapoints
2025-03-06 18:05:51,998 - INFO - training batch 651, loss: 0.752, 20832/60000 datapoints
2025-03-06 18:05:52,189 - INFO - training batch 701, loss: 0.318, 22432/60000 datapoints
2025-03-06 18:05:52,379 - INFO - training batch 751, loss: 0.264, 24032/60000 datapoints
2025-03-06 18:05:52,569 - INFO - training batch 801, loss: 0.383, 25632/60000 datapoints
2025-03-06 18:05:52,771 - INFO - training batch 851, loss: 0.442, 27232/60000 datapoints
2025-03-06 18:05:52,961 - INFO - training batch 901, loss: 0.643, 28832/60000 datapoints
2025-03-06 18:05:53,152 - INFO - training batch 951, loss: 0.512, 30432/60000 datapoints
2025-03-06 18:05:53,344 - INFO - training batch 1001, loss: 0.510, 32032/60000 datapoints
2025-03-06 18:05:53,535 - INFO - training batch 1051, loss: 0.453, 33632/60000 datapoints
2025-03-06 18:05:53,726 - INFO - training batch 1101, loss: 0.181, 35232/60000 datapoints
2025-03-06 18:05:53,917 - INFO - training batch 1151, loss: 0.298, 36832/60000 datapoints
2025-03-06 18:05:54,110 - INFO - training batch 1201, loss: 0.327, 38432/60000 datapoints
2025-03-06 18:05:54,301 - INFO - training batch 1251, loss: 0.291, 40032/60000 datapoints
2025-03-06 18:05:54,491 - INFO - training batch 1301, loss: 0.383, 41632/60000 datapoints
2025-03-06 18:05:54,686 - INFO - training batch 1351, loss: 0.396, 43232/60000 datapoints
2025-03-06 18:05:54,880 - INFO - training batch 1401, loss: 0.292, 44832/60000 datapoints
2025-03-06 18:05:55,071 - INFO - training batch 1451, loss: 0.371, 46432/60000 datapoints
2025-03-06 18:05:55,261 - INFO - training batch 1501, loss: 0.324, 48032/60000 datapoints
2025-03-06 18:05:55,452 - INFO - training batch 1551, loss: 0.206, 49632/60000 datapoints
2025-03-06 18:05:55,646 - INFO - training batch 1601, loss: 0.209, 51232/60000 datapoints
2025-03-06 18:05:55,838 - INFO - training batch 1651, loss: 0.472, 52832/60000 datapoints
2025-03-06 18:05:56,032 - INFO - training batch 1701, loss: 0.323, 54432/60000 datapoints
2025-03-06 18:05:56,224 - INFO - training batch 1751, loss: 0.463, 56032/60000 datapoints
2025-03-06 18:05:56,415 - INFO - training batch 1801, loss: 0.264, 57632/60000 datapoints
2025-03-06 18:05:56,618 - INFO - training batch 1851, loss: 0.499, 59232/60000 datapoints
2025-03-06 18:05:56,719 - INFO - validation batch 1, loss: 0.411, 32/10016 datapoints
2025-03-06 18:05:56,872 - INFO - validation batch 51, loss: 0.263, 1632/10016 datapoints
2025-03-06 18:05:57,026 - INFO - validation batch 101, loss: 0.347, 3232/10016 datapoints
2025-03-06 18:05:57,180 - INFO - validation batch 151, loss: 0.475, 4832/10016 datapoints
2025-03-06 18:05:57,333 - INFO - validation batch 201, loss: 0.238, 6432/10016 datapoints
2025-03-06 18:05:57,485 - INFO - validation batch 251, loss: 0.401, 8032/10016 datapoints
2025-03-06 18:05:57,641 - INFO - validation batch 301, loss: 0.479, 9632/10016 datapoints
2025-03-06 18:05:57,679 - INFO - Epoch 158/800 done.
2025-03-06 18:05:57,679 - INFO - Final validation performance:
Loss: 0.373, top-1 acc: 0.895top-5 acc: 0.895
2025-03-06 18:05:57,679 - INFO - Beginning epoch 159/800
2025-03-06 18:05:57,685 - INFO - training batch 1, loss: 0.251, 32/60000 datapoints
2025-03-06 18:05:57,880 - INFO - training batch 51, loss: 0.389, 1632/60000 datapoints
2025-03-06 18:05:58,080 - INFO - training batch 101, loss: 0.755, 3232/60000 datapoints
2025-03-06 18:05:58,273 - INFO - training batch 151, loss: 0.308, 4832/60000 datapoints
2025-03-06 18:05:58,468 - INFO - training batch 201, loss: 0.472, 6432/60000 datapoints
2025-03-06 18:05:58,665 - INFO - training batch 251, loss: 0.335, 8032/60000 datapoints
2025-03-06 18:05:58,858 - INFO - training batch 301, loss: 0.528, 9632/60000 datapoints
2025-03-06 18:05:59,052 - INFO - training batch 351, loss: 0.560, 11232/60000 datapoints
2025-03-06 18:05:59,244 - INFO - training batch 401, loss: 0.282, 12832/60000 datapoints
2025-03-06 18:05:59,438 - INFO - training batch 451, loss: 0.382, 14432/60000 datapoints
2025-03-06 18:05:59,634 - INFO - training batch 501, loss: 0.386, 16032/60000 datapoints
2025-03-06 18:05:59,831 - INFO - training batch 551, loss: 0.370, 17632/60000 datapoints
2025-03-06 18:06:00,048 - INFO - training batch 601, loss: 0.506, 19232/60000 datapoints
2025-03-06 18:06:00,244 - INFO - training batch 651, loss: 0.246, 20832/60000 datapoints
2025-03-06 18:06:00,439 - INFO - training batch 701, loss: 0.410, 22432/60000 datapoints
2025-03-06 18:06:00,635 - INFO - training batch 751, loss: 0.378, 24032/60000 datapoints
2025-03-06 18:06:00,829 - INFO - training batch 801, loss: 0.455, 25632/60000 datapoints
2025-03-06 18:06:01,023 - INFO - training batch 851, loss: 0.689, 27232/60000 datapoints
2025-03-06 18:06:01,218 - INFO - training batch 901, loss: 0.386, 28832/60000 datapoints
2025-03-06 18:06:01,414 - INFO - training batch 951, loss: 0.501, 30432/60000 datapoints
2025-03-06 18:06:01,614 - INFO - training batch 1001, loss: 0.512, 32032/60000 datapoints
2025-03-06 18:06:01,809 - INFO - training batch 1051, loss: 0.319, 33632/60000 datapoints
2025-03-06 18:06:02,010 - INFO - training batch 1101, loss: 0.616, 35232/60000 datapoints
2025-03-06 18:06:02,205 - INFO - training batch 1151, loss: 0.413, 36832/60000 datapoints
2025-03-06 18:06:02,400 - INFO - training batch 1201, loss: 0.457, 38432/60000 datapoints
2025-03-06 18:06:02,598 - INFO - training batch 1251, loss: 0.231, 40032/60000 datapoints
2025-03-06 18:06:02,791 - INFO - training batch 1301, loss: 0.524, 41632/60000 datapoints
2025-03-06 18:06:02,986 - INFO - training batch 1351, loss: 0.255, 43232/60000 datapoints
2025-03-06 18:06:03,182 - INFO - training batch 1401, loss: 0.514, 44832/60000 datapoints
2025-03-06 18:06:03,377 - INFO - training batch 1451, loss: 0.752, 46432/60000 datapoints
2025-03-06 18:06:03,572 - INFO - training batch 1501, loss: 0.493, 48032/60000 datapoints
2025-03-06 18:06:03,768 - INFO - training batch 1551, loss: 0.337, 49632/60000 datapoints
2025-03-06 18:06:03,961 - INFO - training batch 1601, loss: 0.433, 51232/60000 datapoints
2025-03-06 18:06:04,157 - INFO - training batch 1651, loss: 0.443, 52832/60000 datapoints
2025-03-06 18:06:04,353 - INFO - training batch 1701, loss: 0.401, 54432/60000 datapoints
2025-03-06 18:06:04,548 - INFO - training batch 1751, loss: 0.390, 56032/60000 datapoints
2025-03-06 18:06:04,745 - INFO - training batch 1801, loss: 0.516, 57632/60000 datapoints
2025-03-06 18:06:04,943 - INFO - training batch 1851, loss: 0.531, 59232/60000 datapoints
2025-03-06 18:06:05,046 - INFO - validation batch 1, loss: 0.409, 32/10016 datapoints
2025-03-06 18:06:05,200 - INFO - validation batch 51, loss: 0.321, 1632/10016 datapoints
2025-03-06 18:06:05,355 - INFO - validation batch 101, loss: 0.165, 3232/10016 datapoints
2025-03-06 18:06:05,506 - INFO - validation batch 151, loss: 0.381, 4832/10016 datapoints
2025-03-06 18:06:05,662 - INFO - validation batch 201, loss: 0.290, 6432/10016 datapoints
2025-03-06 18:06:05,816 - INFO - validation batch 251, loss: 0.446, 8032/10016 datapoints
2025-03-06 18:06:05,968 - INFO - validation batch 301, loss: 0.665, 9632/10016 datapoints
2025-03-06 18:06:06,004 - INFO - Epoch 159/800 done.
2025-03-06 18:06:06,005 - INFO - Final validation performance:
Loss: 0.382, top-1 acc: 0.896top-5 acc: 0.896
2025-03-06 18:06:06,005 - INFO - Beginning epoch 160/800
2025-03-06 18:06:06,012 - INFO - training batch 1, loss: 0.419, 32/60000 datapoints
2025-03-06 18:06:06,209 - INFO - training batch 51, loss: 0.395, 1632/60000 datapoints
2025-03-06 18:06:06,404 - INFO - training batch 101, loss: 0.246, 3232/60000 datapoints
2025-03-06 18:06:06,597 - INFO - training batch 151, loss: 0.307, 4832/60000 datapoints
2025-03-06 18:06:06,790 - INFO - training batch 201, loss: 0.239, 6432/60000 datapoints
2025-03-06 18:06:06,985 - INFO - training batch 251, loss: 0.404, 8032/60000 datapoints
2025-03-06 18:06:07,177 - INFO - training batch 301, loss: 0.233, 9632/60000 datapoints
2025-03-06 18:06:07,366 - INFO - training batch 351, loss: 0.418, 11232/60000 datapoints
2025-03-06 18:06:07,556 - INFO - training batch 401, loss: 0.408, 12832/60000 datapoints
2025-03-06 18:06:07,751 - INFO - training batch 451, loss: 0.279, 14432/60000 datapoints
2025-03-06 18:06:07,944 - INFO - training batch 501, loss: 0.257, 16032/60000 datapoints
2025-03-06 18:06:08,144 - INFO - training batch 551, loss: 0.517, 17632/60000 datapoints
2025-03-06 18:06:08,335 - INFO - training batch 601, loss: 0.416, 19232/60000 datapoints
2025-03-06 18:06:08,526 - INFO - training batch 651, loss: 0.478, 20832/60000 datapoints
2025-03-06 18:06:08,722 - INFO - training batch 701, loss: 0.377, 22432/60000 datapoints
2025-03-06 18:06:08,913 - INFO - training batch 751, loss: 0.395, 24032/60000 datapoints
2025-03-06 18:06:09,106 - INFO - training batch 801, loss: 0.446, 25632/60000 datapoints
2025-03-06 18:06:09,298 - INFO - training batch 851, loss: 0.520, 27232/60000 datapoints
2025-03-06 18:06:09,488 - INFO - training batch 901, loss: 0.331, 28832/60000 datapoints
2025-03-06 18:06:09,682 - INFO - training batch 951, loss: 0.365, 30432/60000 datapoints
2025-03-06 18:06:09,874 - INFO - training batch 1001, loss: 0.533, 32032/60000 datapoints
2025-03-06 18:06:10,087 - INFO - training batch 1051, loss: 0.199, 33632/60000 datapoints
2025-03-06 18:06:10,279 - INFO - training batch 1101, loss: 0.548, 35232/60000 datapoints
2025-03-06 18:06:10,471 - INFO - training batch 1151, loss: 0.260, 36832/60000 datapoints
2025-03-06 18:06:10,667 - INFO - training batch 1201, loss: 0.276, 38432/60000 datapoints
2025-03-06 18:06:10,859 - INFO - training batch 1251, loss: 0.372, 40032/60000 datapoints
2025-03-06 18:06:11,051 - INFO - training batch 1301, loss: 0.244, 41632/60000 datapoints
2025-03-06 18:06:11,240 - INFO - training batch 1351, loss: 0.388, 43232/60000 datapoints
2025-03-06 18:06:11,428 - INFO - training batch 1401, loss: 0.274, 44832/60000 datapoints
2025-03-06 18:06:11,624 - INFO - training batch 1451, loss: 0.563, 46432/60000 datapoints
2025-03-06 18:06:11,819 - INFO - training batch 1501, loss: 0.647, 48032/60000 datapoints
2025-03-06 18:06:12,017 - INFO - training batch 1551, loss: 0.448, 49632/60000 datapoints
2025-03-06 18:06:12,209 - INFO - training batch 1601, loss: 0.316, 51232/60000 datapoints
2025-03-06 18:06:12,401 - INFO - training batch 1651, loss: 0.360, 52832/60000 datapoints
2025-03-06 18:06:12,594 - INFO - training batch 1701, loss: 0.294, 54432/60000 datapoints
2025-03-06 18:06:12,789 - INFO - training batch 1751, loss: 0.299, 56032/60000 datapoints
2025-03-06 18:06:12,978 - INFO - training batch 1801, loss: 0.392, 57632/60000 datapoints
2025-03-06 18:06:13,166 - INFO - training batch 1851, loss: 0.646, 59232/60000 datapoints
2025-03-06 18:06:13,265 - INFO - validation batch 1, loss: 0.282, 32/10016 datapoints
2025-03-06 18:06:13,419 - INFO - validation batch 51, loss: 0.473, 1632/10016 datapoints
2025-03-06 18:06:13,569 - INFO - validation batch 101, loss: 0.301, 3232/10016 datapoints
2025-03-06 18:06:13,722 - INFO - validation batch 151, loss: 0.460, 4832/10016 datapoints
2025-03-06 18:06:13,872 - INFO - validation batch 201, loss: 0.478, 6432/10016 datapoints
2025-03-06 18:06:14,022 - INFO - validation batch 251, loss: 0.239, 8032/10016 datapoints
2025-03-06 18:06:14,177 - INFO - validation batch 301, loss: 0.587, 9632/10016 datapoints
2025-03-06 18:06:14,213 - INFO - Epoch 160/800 done.
2025-03-06 18:06:14,213 - INFO - Final validation performance:
Loss: 0.403, top-1 acc: 0.896top-5 acc: 0.896
2025-03-06 18:06:14,214 - INFO - Beginning epoch 161/800
2025-03-06 18:06:14,219 - INFO - training batch 1, loss: 0.176, 32/60000 datapoints
2025-03-06 18:06:14,412 - INFO - training batch 51, loss: 0.361, 1632/60000 datapoints
2025-03-06 18:06:14,605 - INFO - training batch 101, loss: 0.198, 3232/60000 datapoints
2025-03-06 18:06:14,799 - INFO - training batch 151, loss: 0.447, 4832/60000 datapoints
2025-03-06 18:06:14,997 - INFO - training batch 201, loss: 0.219, 6432/60000 datapoints
2025-03-06 18:06:15,190 - INFO - training batch 251, loss: 0.397, 8032/60000 datapoints
2025-03-06 18:06:15,382 - INFO - training batch 301, loss: 0.578, 9632/60000 datapoints
2025-03-06 18:06:15,576 - INFO - training batch 351, loss: 0.280, 11232/60000 datapoints
2025-03-06 18:06:15,769 - INFO - training batch 401, loss: 0.243, 12832/60000 datapoints
2025-03-06 18:06:15,962 - INFO - training batch 451, loss: 0.498, 14432/60000 datapoints
2025-03-06 18:06:16,157 - INFO - training batch 501, loss: 0.540, 16032/60000 datapoints
2025-03-06 18:06:16,349 - INFO - training batch 551, loss: 0.223, 17632/60000 datapoints
2025-03-06 18:06:16,540 - INFO - training batch 601, loss: 0.515, 19232/60000 datapoints
2025-03-06 18:06:16,740 - INFO - training batch 651, loss: 0.258, 20832/60000 datapoints
2025-03-06 18:06:16,936 - INFO - training batch 701, loss: 0.309, 22432/60000 datapoints
2025-03-06 18:06:17,136 - INFO - training batch 751, loss: 0.320, 24032/60000 datapoints
2025-03-06 18:06:17,336 - INFO - training batch 801, loss: 0.450, 25632/60000 datapoints
2025-03-06 18:06:17,531 - INFO - training batch 851, loss: 0.272, 27232/60000 datapoints
2025-03-06 18:06:17,729 - INFO - training batch 901, loss: 0.334, 28832/60000 datapoints
2025-03-06 18:06:17,921 - INFO - training batch 951, loss: 0.340, 30432/60000 datapoints
2025-03-06 18:06:18,120 - INFO - training batch 1001, loss: 0.446, 32032/60000 datapoints
2025-03-06 18:06:18,314 - INFO - training batch 1051, loss: 0.607, 33632/60000 datapoints
2025-03-06 18:06:18,507 - INFO - training batch 1101, loss: 0.253, 35232/60000 datapoints
2025-03-06 18:06:18,702 - INFO - training batch 1151, loss: 0.313, 36832/60000 datapoints
2025-03-06 18:06:18,897 - INFO - training batch 1201, loss: 0.232, 38432/60000 datapoints
2025-03-06 18:06:19,092 - INFO - training batch 1251, loss: 0.436, 40032/60000 datapoints
2025-03-06 18:06:19,288 - INFO - training batch 1301, loss: 0.251, 41632/60000 datapoints
2025-03-06 18:06:19,482 - INFO - training batch 1351, loss: 0.379, 43232/60000 datapoints
2025-03-06 18:06:19,680 - INFO - training batch 1401, loss: 0.282, 44832/60000 datapoints
2025-03-06 18:06:19,875 - INFO - training batch 1451, loss: 0.571, 46432/60000 datapoints
2025-03-06 18:06:20,092 - INFO - training batch 1501, loss: 0.376, 48032/60000 datapoints
2025-03-06 18:06:20,288 - INFO - training batch 1551, loss: 0.485, 49632/60000 datapoints
2025-03-06 18:06:20,481 - INFO - training batch 1601, loss: 0.277, 51232/60000 datapoints
2025-03-06 18:06:20,678 - INFO - training batch 1651, loss: 0.240, 52832/60000 datapoints
2025-03-06 18:06:20,872 - INFO - training batch 1701, loss: 0.682, 54432/60000 datapoints
2025-03-06 18:06:21,065 - INFO - training batch 1751, loss: 0.543, 56032/60000 datapoints
2025-03-06 18:06:21,258 - INFO - training batch 1801, loss: 0.506, 57632/60000 datapoints
2025-03-06 18:06:21,456 - INFO - training batch 1851, loss: 0.367, 59232/60000 datapoints
2025-03-06 18:06:21,557 - INFO - validation batch 1, loss: 0.213, 32/10016 datapoints
2025-03-06 18:06:21,712 - INFO - validation batch 51, loss: 0.385, 1632/10016 datapoints
2025-03-06 18:06:21,868 - INFO - validation batch 101, loss: 0.655, 3232/10016 datapoints
2025-03-06 18:06:22,023 - INFO - validation batch 151, loss: 0.544, 4832/10016 datapoints
2025-03-06 18:06:22,179 - INFO - validation batch 201, loss: 0.293, 6432/10016 datapoints
2025-03-06 18:06:22,334 - INFO - validation batch 251, loss: 0.390, 8032/10016 datapoints
2025-03-06 18:06:22,489 - INFO - validation batch 301, loss: 0.410, 9632/10016 datapoints
2025-03-06 18:06:22,525 - INFO - Epoch 161/800 done.
2025-03-06 18:06:22,525 - INFO - Final validation performance:
Loss: 0.413, top-1 acc: 0.896top-5 acc: 0.896
2025-03-06 18:06:22,526 - INFO - Beginning epoch 162/800
2025-03-06 18:06:22,532 - INFO - training batch 1, loss: 0.487, 32/60000 datapoints
2025-03-06 18:06:22,730 - INFO - training batch 51, loss: 0.498, 1632/60000 datapoints
2025-03-06 18:06:22,928 - INFO - training batch 101, loss: 0.290, 3232/60000 datapoints
2025-03-06 18:06:23,122 - INFO - training batch 151, loss: 0.387, 4832/60000 datapoints
2025-03-06 18:06:23,320 - INFO - training batch 201, loss: 0.222, 6432/60000 datapoints
2025-03-06 18:06:23,519 - INFO - training batch 251, loss: 0.362, 8032/60000 datapoints
2025-03-06 18:06:23,723 - INFO - training batch 301, loss: 0.456, 9632/60000 datapoints
2025-03-06 18:06:23,917 - INFO - training batch 351, loss: 0.599, 11232/60000 datapoints
2025-03-06 18:06:24,115 - INFO - training batch 401, loss: 0.389, 12832/60000 datapoints
2025-03-06 18:06:24,309 - INFO - training batch 451, loss: 0.384, 14432/60000 datapoints
2025-03-06 18:06:24,505 - INFO - training batch 501, loss: 0.468, 16032/60000 datapoints
2025-03-06 18:06:24,732 - INFO - training batch 551, loss: 0.710, 17632/60000 datapoints
2025-03-06 18:06:24,930 - INFO - training batch 601, loss: 0.692, 19232/60000 datapoints
2025-03-06 18:06:25,123 - INFO - training batch 651, loss: 0.368, 20832/60000 datapoints
2025-03-06 18:06:25,316 - INFO - training batch 701, loss: 0.362, 22432/60000 datapoints
2025-03-06 18:06:25,512 - INFO - training batch 751, loss: 0.252, 24032/60000 datapoints
2025-03-06 18:06:25,713 - INFO - training batch 801, loss: 0.575, 25632/60000 datapoints
2025-03-06 18:06:25,909 - INFO - training batch 851, loss: 0.373, 27232/60000 datapoints
2025-03-06 18:06:26,108 - INFO - training batch 901, loss: 0.212, 28832/60000 datapoints
2025-03-06 18:06:26,302 - INFO - training batch 951, loss: 0.277, 30432/60000 datapoints
2025-03-06 18:06:26,498 - INFO - training batch 1001, loss: 0.394, 32032/60000 datapoints
2025-03-06 18:06:26,695 - INFO - training batch 1051, loss: 0.186, 33632/60000 datapoints
2025-03-06 18:06:26,887 - INFO - training batch 1101, loss: 0.369, 35232/60000 datapoints
2025-03-06 18:06:27,082 - INFO - training batch 1151, loss: 0.415, 36832/60000 datapoints
2025-03-06 18:06:27,280 - INFO - training batch 1201, loss: 0.920, 38432/60000 datapoints
2025-03-06 18:06:27,475 - INFO - training batch 1251, loss: 0.778, 40032/60000 datapoints
2025-03-06 18:06:27,674 - INFO - training batch 1301, loss: 0.339, 41632/60000 datapoints
2025-03-06 18:06:27,866 - INFO - training batch 1351, loss: 0.382, 43232/60000 datapoints
2025-03-06 18:06:28,065 - INFO - training batch 1401, loss: 0.304, 44832/60000 datapoints
2025-03-06 18:06:28,264 - INFO - training batch 1451, loss: 0.591, 46432/60000 datapoints
2025-03-06 18:06:28,461 - INFO - training batch 1501, loss: 0.221, 48032/60000 datapoints
2025-03-06 18:06:28,661 - INFO - training batch 1551, loss: 0.430, 49632/60000 datapoints
2025-03-06 18:06:28,855 - INFO - training batch 1601, loss: 0.286, 51232/60000 datapoints
2025-03-06 18:06:29,052 - INFO - training batch 1651, loss: 0.692, 52832/60000 datapoints
2025-03-06 18:06:29,247 - INFO - training batch 1701, loss: 0.325, 54432/60000 datapoints
2025-03-06 18:06:29,442 - INFO - training batch 1751, loss: 0.488, 56032/60000 datapoints
2025-03-06 18:06:29,640 - INFO - training batch 1801, loss: 0.341, 57632/60000 datapoints
2025-03-06 18:06:29,834 - INFO - training batch 1851, loss: 0.203, 59232/60000 datapoints
2025-03-06 18:06:29,936 - INFO - validation batch 1, loss: 0.220, 32/10016 datapoints
2025-03-06 18:06:30,099 - INFO - validation batch 51, loss: 0.358, 1632/10016 datapoints
2025-03-06 18:06:30,264 - INFO - validation batch 101, loss: 0.335, 3232/10016 datapoints
2025-03-06 18:06:30,420 - INFO - validation batch 151, loss: 0.261, 4832/10016 datapoints
2025-03-06 18:06:30,574 - INFO - validation batch 201, loss: 0.470, 6432/10016 datapoints
2025-03-06 18:06:30,728 - INFO - validation batch 251, loss: 0.287, 8032/10016 datapoints
2025-03-06 18:06:30,882 - INFO - validation batch 301, loss: 0.254, 9632/10016 datapoints
2025-03-06 18:06:30,920 - INFO - Epoch 162/800 done.
2025-03-06 18:06:30,921 - INFO - Final validation performance:
Loss: 0.312, top-1 acc: 0.896top-5 acc: 0.896
2025-03-06 18:06:30,921 - INFO - Beginning epoch 163/800
2025-03-06 18:06:30,927 - INFO - training batch 1, loss: 0.405, 32/60000 datapoints
2025-03-06 18:06:31,126 - INFO - training batch 51, loss: 0.651, 1632/60000 datapoints
2025-03-06 18:06:31,319 - INFO - training batch 101, loss: 0.330, 3232/60000 datapoints
2025-03-06 18:06:31,514 - INFO - training batch 151, loss: 0.375, 4832/60000 datapoints
2025-03-06 18:06:31,711 - INFO - training batch 201, loss: 0.522, 6432/60000 datapoints
2025-03-06 18:06:31,907 - INFO - training batch 251, loss: 0.422, 8032/60000 datapoints
2025-03-06 18:06:32,102 - INFO - training batch 301, loss: 0.274, 9632/60000 datapoints
2025-03-06 18:06:32,303 - INFO - training batch 351, loss: 0.455, 11232/60000 datapoints
2025-03-06 18:06:32,498 - INFO - training batch 401, loss: 0.572, 12832/60000 datapoints
2025-03-06 18:06:32,717 - INFO - training batch 451, loss: 0.374, 14432/60000 datapoints
2025-03-06 18:06:32,923 - INFO - training batch 501, loss: 0.575, 16032/60000 datapoints
2025-03-06 18:06:33,118 - INFO - training batch 551, loss: 0.133, 17632/60000 datapoints
2025-03-06 18:06:33,313 - INFO - training batch 601, loss: 0.451, 19232/60000 datapoints
2025-03-06 18:06:33,508 - INFO - training batch 651, loss: 0.549, 20832/60000 datapoints
2025-03-06 18:06:33,706 - INFO - training batch 701, loss: 0.291, 22432/60000 datapoints
2025-03-06 18:06:33,903 - INFO - training batch 751, loss: 0.252, 24032/60000 datapoints
2025-03-06 18:06:34,101 - INFO - training batch 801, loss: 0.301, 25632/60000 datapoints
2025-03-06 18:06:34,299 - INFO - training batch 851, loss: 0.688, 27232/60000 datapoints
2025-03-06 18:06:34,493 - INFO - training batch 901, loss: 0.291, 28832/60000 datapoints
2025-03-06 18:06:34,693 - INFO - training batch 951, loss: 0.578, 30432/60000 datapoints
2025-03-06 18:06:34,892 - INFO - training batch 1001, loss: 0.292, 32032/60000 datapoints
2025-03-06 18:06:35,092 - INFO - training batch 1051, loss: 0.328, 33632/60000 datapoints
2025-03-06 18:06:35,288 - INFO - training batch 1101, loss: 0.617, 35232/60000 datapoints
2025-03-06 18:06:35,482 - INFO - training batch 1151, loss: 0.634, 36832/60000 datapoints
2025-03-06 18:06:35,679 - INFO - training batch 1201, loss: 0.500, 38432/60000 datapoints
2025-03-06 18:06:35,874 - INFO - training batch 1251, loss: 0.308, 40032/60000 datapoints
2025-03-06 18:06:36,072 - INFO - training batch 1301, loss: 0.554, 41632/60000 datapoints
2025-03-06 18:06:36,273 - INFO - training batch 1351, loss: 0.474, 43232/60000 datapoints
2025-03-06 18:06:36,467 - INFO - training batch 1401, loss: 0.214, 44832/60000 datapoints
2025-03-06 18:06:36,665 - INFO - training batch 1451, loss: 0.272, 46432/60000 datapoints
2025-03-06 18:06:36,861 - INFO - training batch 1501, loss: 0.276, 48032/60000 datapoints
2025-03-06 18:06:37,053 - INFO - training batch 1551, loss: 0.523, 49632/60000 datapoints
2025-03-06 18:06:37,247 - INFO - training batch 1601, loss: 0.468, 51232/60000 datapoints
2025-03-06 18:06:37,444 - INFO - training batch 1651, loss: 0.259, 52832/60000 datapoints
2025-03-06 18:06:37,662 - INFO - training batch 1701, loss: 0.322, 54432/60000 datapoints
2025-03-06 18:06:37,855 - INFO - training batch 1751, loss: 0.627, 56032/60000 datapoints
2025-03-06 18:06:38,054 - INFO - training batch 1801, loss: 0.360, 57632/60000 datapoints
2025-03-06 18:06:38,250 - INFO - training batch 1851, loss: 0.480, 59232/60000 datapoints
2025-03-06 18:06:38,351 - INFO - validation batch 1, loss: 0.183, 32/10016 datapoints
2025-03-06 18:06:38,507 - INFO - validation batch 51, loss: 0.148, 1632/10016 datapoints
2025-03-06 18:06:38,661 - INFO - validation batch 101, loss: 0.274, 3232/10016 datapoints
2025-03-06 18:06:38,815 - INFO - validation batch 151, loss: 0.400, 4832/10016 datapoints
2025-03-06 18:06:38,969 - INFO - validation batch 201, loss: 0.713, 6432/10016 datapoints
2025-03-06 18:06:39,127 - INFO - validation batch 251, loss: 0.494, 8032/10016 datapoints
2025-03-06 18:06:39,279 - INFO - validation batch 301, loss: 0.396, 9632/10016 datapoints
2025-03-06 18:06:39,317 - INFO - Epoch 163/800 done.
2025-03-06 18:06:39,317 - INFO - Final validation performance:
Loss: 0.373, top-1 acc: 0.896top-5 acc: 0.896
2025-03-06 18:06:39,318 - INFO - Beginning epoch 164/800
2025-03-06 18:06:39,323 - INFO - training batch 1, loss: 0.543, 32/60000 datapoints
2025-03-06 18:06:39,519 - INFO - training batch 51, loss: 0.470, 1632/60000 datapoints
2025-03-06 18:06:39,723 - INFO - training batch 101, loss: 0.432, 3232/60000 datapoints
2025-03-06 18:06:39,917 - INFO - training batch 151, loss: 0.245, 4832/60000 datapoints
2025-03-06 18:06:40,112 - INFO - training batch 201, loss: 0.301, 6432/60000 datapoints
2025-03-06 18:06:40,328 - INFO - training batch 251, loss: 0.496, 8032/60000 datapoints
2025-03-06 18:06:40,523 - INFO - training batch 301, loss: 0.231, 9632/60000 datapoints
2025-03-06 18:06:40,720 - INFO - training batch 351, loss: 0.282, 11232/60000 datapoints
2025-03-06 18:06:40,913 - INFO - training batch 401, loss: 0.311, 12832/60000 datapoints
2025-03-06 18:06:41,108 - INFO - training batch 451, loss: 0.395, 14432/60000 datapoints
2025-03-06 18:06:41,302 - INFO - training batch 501, loss: 0.317, 16032/60000 datapoints
2025-03-06 18:06:41,499 - INFO - training batch 551, loss: 0.315, 17632/60000 datapoints
2025-03-06 18:06:41,696 - INFO - training batch 601, loss: 0.578, 19232/60000 datapoints
2025-03-06 18:06:41,892 - INFO - training batch 651, loss: 0.183, 20832/60000 datapoints
2025-03-06 18:06:42,088 - INFO - training batch 701, loss: 0.345, 22432/60000 datapoints
2025-03-06 18:06:42,286 - INFO - training batch 751, loss: 0.918, 24032/60000 datapoints
2025-03-06 18:06:42,479 - INFO - training batch 801, loss: 0.407, 25632/60000 datapoints
2025-03-06 18:06:42,680 - INFO - training batch 851, loss: 0.258, 27232/60000 datapoints
2025-03-06 18:06:42,878 - INFO - training batch 901, loss: 0.285, 28832/60000 datapoints
2025-03-06 18:06:43,074 - INFO - training batch 951, loss: 0.381, 30432/60000 datapoints
2025-03-06 18:06:43,270 - INFO - training batch 1001, loss: 0.425, 32032/60000 datapoints
2025-03-06 18:06:43,468 - INFO - training batch 1051, loss: 0.358, 33632/60000 datapoints
2025-03-06 18:06:43,670 - INFO - training batch 1101, loss: 0.380, 35232/60000 datapoints
2025-03-06 18:06:43,865 - INFO - training batch 1151, loss: 0.322, 36832/60000 datapoints
2025-03-06 18:06:44,058 - INFO - training batch 1201, loss: 0.497, 38432/60000 datapoints
2025-03-06 18:06:44,255 - INFO - training batch 1251, loss: 0.529, 40032/60000 datapoints
2025-03-06 18:06:44,450 - INFO - training batch 1301, loss: 0.229, 41632/60000 datapoints
2025-03-06 18:06:44,646 - INFO - training batch 1351, loss: 0.502, 43232/60000 datapoints
2025-03-06 18:06:44,841 - INFO - training batch 1401, loss: 0.443, 44832/60000 datapoints
2025-03-06 18:06:45,040 - INFO - training batch 1451, loss: 0.513, 46432/60000 datapoints
2025-03-06 18:06:45,233 - INFO - training batch 1501, loss: 0.438, 48032/60000 datapoints
2025-03-06 18:06:45,426 - INFO - training batch 1551, loss: 0.609, 49632/60000 datapoints
2025-03-06 18:06:45,623 - INFO - training batch 1601, loss: 0.407, 51232/60000 datapoints
2025-03-06 18:06:45,816 - INFO - training batch 1651, loss: 0.264, 52832/60000 datapoints
2025-03-06 18:06:46,013 - INFO - training batch 1701, loss: 0.541, 54432/60000 datapoints
2025-03-06 18:06:46,215 - INFO - training batch 1751, loss: 0.326, 56032/60000 datapoints
2025-03-06 18:06:46,412 - INFO - training batch 1801, loss: 0.340, 57632/60000 datapoints
2025-03-06 18:06:46,607 - INFO - training batch 1851, loss: 0.390, 59232/60000 datapoints
2025-03-06 18:06:46,710 - INFO - validation batch 1, loss: 0.323, 32/10016 datapoints
2025-03-06 18:06:46,863 - INFO - validation batch 51, loss: 0.393, 1632/10016 datapoints
2025-03-06 18:06:47,016 - INFO - validation batch 101, loss: 0.489, 3232/10016 datapoints
2025-03-06 18:06:47,170 - INFO - validation batch 151, loss: 0.256, 4832/10016 datapoints
2025-03-06 18:06:47,325 - INFO - validation batch 201, loss: 0.332, 6432/10016 datapoints
2025-03-06 18:06:47,478 - INFO - validation batch 251, loss: 0.269, 8032/10016 datapoints
2025-03-06 18:06:47,633 - INFO - validation batch 301, loss: 0.483, 9632/10016 datapoints
2025-03-06 18:06:47,673 - INFO - Epoch 164/800 done.
2025-03-06 18:06:47,690 - INFO - Final validation performance:
Loss: 0.363, top-1 acc: 0.896top-5 acc: 0.896
2025-03-06 18:06:47,690 - INFO - Beginning epoch 165/800
2025-03-06 18:06:47,696 - INFO - training batch 1, loss: 0.381, 32/60000 datapoints
2025-03-06 18:06:47,896 - INFO - training batch 51, loss: 0.483, 1632/60000 datapoints
2025-03-06 18:06:48,092 - INFO - training batch 101, loss: 0.504, 3232/60000 datapoints
2025-03-06 18:06:48,291 - INFO - training batch 151, loss: 0.305, 4832/60000 datapoints
2025-03-06 18:06:48,485 - INFO - training batch 201, loss: 0.586, 6432/60000 datapoints
2025-03-06 18:06:48,680 - INFO - training batch 251, loss: 0.298, 8032/60000 datapoints
2025-03-06 18:06:48,875 - INFO - training batch 301, loss: 0.315, 9632/60000 datapoints
2025-03-06 18:06:49,067 - INFO - training batch 351, loss: 0.368, 11232/60000 datapoints
2025-03-06 18:06:49,270 - INFO - training batch 401, loss: 0.258, 12832/60000 datapoints
2025-03-06 18:06:49,470 - INFO - training batch 451, loss: 0.233, 14432/60000 datapoints
2025-03-06 18:06:49,665 - INFO - training batch 501, loss: 0.262, 16032/60000 datapoints
2025-03-06 18:06:49,860 - INFO - training batch 551, loss: 0.346, 17632/60000 datapoints
2025-03-06 18:06:50,055 - INFO - training batch 601, loss: 0.527, 19232/60000 datapoints
2025-03-06 18:06:50,254 - INFO - training batch 651, loss: 0.573, 20832/60000 datapoints
2025-03-06 18:06:50,463 - INFO - training batch 701, loss: 0.495, 22432/60000 datapoints
2025-03-06 18:06:50,661 - INFO - training batch 751, loss: 0.767, 24032/60000 datapoints
2025-03-06 18:06:50,856 - INFO - training batch 801, loss: 0.412, 25632/60000 datapoints
2025-03-06 18:06:51,052 - INFO - training batch 851, loss: 0.278, 27232/60000 datapoints
2025-03-06 18:06:51,247 - INFO - training batch 901, loss: 0.492, 28832/60000 datapoints
2025-03-06 18:06:51,442 - INFO - training batch 951, loss: 0.454, 30432/60000 datapoints
2025-03-06 18:06:51,637 - INFO - training batch 1001, loss: 0.706, 32032/60000 datapoints
2025-03-06 18:06:51,835 - INFO - training batch 1051, loss: 0.442, 33632/60000 datapoints
2025-03-06 18:06:52,031 - INFO - training batch 1101, loss: 0.310, 35232/60000 datapoints
2025-03-06 18:06:52,226 - INFO - training batch 1151, loss: 0.289, 36832/60000 datapoints
2025-03-06 18:06:52,419 - INFO - training batch 1201, loss: 0.292, 38432/60000 datapoints
2025-03-06 18:06:52,615 - INFO - training batch 1251, loss: 0.406, 40032/60000 datapoints
2025-03-06 18:06:52,813 - INFO - training batch 1301, loss: 0.254, 41632/60000 datapoints
2025-03-06 18:06:53,008 - INFO - training batch 1351, loss: 0.627, 43232/60000 datapoints
2025-03-06 18:06:53,201 - INFO - training batch 1401, loss: 0.347, 44832/60000 datapoints
2025-03-06 18:06:53,396 - INFO - training batch 1451, loss: 0.555, 46432/60000 datapoints
2025-03-06 18:06:53,593 - INFO - training batch 1501, loss: 0.330, 48032/60000 datapoints
2025-03-06 18:06:53,788 - INFO - training batch 1551, loss: 0.476, 49632/60000 datapoints
2025-03-06 18:06:53,983 - INFO - training batch 1601, loss: 0.312, 51232/60000 datapoints
2025-03-06 18:06:54,177 - INFO - training batch 1651, loss: 0.469, 52832/60000 datapoints
2025-03-06 18:06:54,377 - INFO - training batch 1701, loss: 0.299, 54432/60000 datapoints
2025-03-06 18:06:54,573 - INFO - training batch 1751, loss: 0.310, 56032/60000 datapoints
2025-03-06 18:06:54,807 - INFO - training batch 1801, loss: 0.395, 57632/60000 datapoints
2025-03-06 18:06:55,008 - INFO - training batch 1851, loss: 0.398, 59232/60000 datapoints
2025-03-06 18:06:55,111 - INFO - validation batch 1, loss: 0.269, 32/10016 datapoints
2025-03-06 18:06:55,265 - INFO - validation batch 51, loss: 0.684, 1632/10016 datapoints
2025-03-06 18:06:55,419 - INFO - validation batch 101, loss: 0.270, 3232/10016 datapoints
2025-03-06 18:06:55,571 - INFO - validation batch 151, loss: 0.456, 4832/10016 datapoints
2025-03-06 18:06:55,729 - INFO - validation batch 201, loss: 0.338, 6432/10016 datapoints
2025-03-06 18:06:55,884 - INFO - validation batch 251, loss: 0.403, 8032/10016 datapoints
2025-03-06 18:06:56,038 - INFO - validation batch 301, loss: 0.352, 9632/10016 datapoints
2025-03-06 18:06:56,076 - INFO - Epoch 165/800 done.
2025-03-06 18:06:56,076 - INFO - Final validation performance:
Loss: 0.396, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 18:06:56,076 - INFO - Beginning epoch 166/800
2025-03-06 18:06:56,082 - INFO - training batch 1, loss: 0.463, 32/60000 datapoints
2025-03-06 18:06:56,286 - INFO - training batch 51, loss: 0.174, 1632/60000 datapoints
2025-03-06 18:06:56,479 - INFO - training batch 101, loss: 0.251, 3232/60000 datapoints
2025-03-06 18:06:56,676 - INFO - training batch 151, loss: 0.213, 4832/60000 datapoints
2025-03-06 18:06:56,874 - INFO - training batch 201, loss: 0.474, 6432/60000 datapoints
2025-03-06 18:06:57,071 - INFO - training batch 251, loss: 0.523, 8032/60000 datapoints
2025-03-06 18:06:57,269 - INFO - training batch 301, loss: 0.368, 9632/60000 datapoints
2025-03-06 18:06:57,469 - INFO - training batch 351, loss: 0.203, 11232/60000 datapoints
2025-03-06 18:06:57,669 - INFO - training batch 401, loss: 0.456, 12832/60000 datapoints
2025-03-06 18:06:57,865 - INFO - training batch 451, loss: 0.569, 14432/60000 datapoints
2025-03-06 18:06:58,068 - INFO - training batch 501, loss: 0.434, 16032/60000 datapoints
2025-03-06 18:06:58,267 - INFO - training batch 551, loss: 0.350, 17632/60000 datapoints
2025-03-06 18:06:58,464 - INFO - training batch 601, loss: 0.825, 19232/60000 datapoints
2025-03-06 18:06:58,661 - INFO - training batch 651, loss: 0.146, 20832/60000 datapoints
2025-03-06 18:06:58,856 - INFO - training batch 701, loss: 0.289, 22432/60000 datapoints
2025-03-06 18:06:59,052 - INFO - training batch 751, loss: 0.351, 24032/60000 datapoints
2025-03-06 18:06:59,247 - INFO - training batch 801, loss: 0.396, 25632/60000 datapoints
2025-03-06 18:06:59,444 - INFO - training batch 851, loss: 0.353, 27232/60000 datapoints
2025-03-06 18:06:59,641 - INFO - training batch 901, loss: 0.298, 28832/60000 datapoints
2025-03-06 18:06:59,835 - INFO - training batch 951, loss: 0.341, 30432/60000 datapoints
2025-03-06 18:07:00,031 - INFO - training batch 1001, loss: 0.462, 32032/60000 datapoints
2025-03-06 18:07:00,228 - INFO - training batch 1051, loss: 0.464, 33632/60000 datapoints
2025-03-06 18:07:00,444 - INFO - training batch 1101, loss: 0.540, 35232/60000 datapoints
2025-03-06 18:07:00,641 - INFO - training batch 1151, loss: 0.558, 36832/60000 datapoints
2025-03-06 18:07:00,835 - INFO - training batch 1201, loss: 0.335, 38432/60000 datapoints
2025-03-06 18:07:01,028 - INFO - training batch 1251, loss: 0.617, 40032/60000 datapoints
2025-03-06 18:07:01,222 - INFO - training batch 1301, loss: 0.530, 41632/60000 datapoints
2025-03-06 18:07:01,416 - INFO - training batch 1351, loss: 0.572, 43232/60000 datapoints
2025-03-06 18:07:01,613 - INFO - training batch 1401, loss: 0.378, 44832/60000 datapoints
2025-03-06 18:07:01,811 - INFO - training batch 1451, loss: 0.314, 46432/60000 datapoints
2025-03-06 18:07:02,003 - INFO - training batch 1501, loss: 0.357, 48032/60000 datapoints
2025-03-06 18:07:02,199 - INFO - training batch 1551, loss: 0.327, 49632/60000 datapoints
2025-03-06 18:07:02,399 - INFO - training batch 1601, loss: 0.475, 51232/60000 datapoints
2025-03-06 18:07:02,591 - INFO - training batch 1651, loss: 0.417, 52832/60000 datapoints
2025-03-06 18:07:02,789 - INFO - training batch 1701, loss: 0.280, 54432/60000 datapoints
2025-03-06 18:07:02,984 - INFO - training batch 1751, loss: 0.327, 56032/60000 datapoints
2025-03-06 18:07:03,179 - INFO - training batch 1801, loss: 0.368, 57632/60000 datapoints
2025-03-06 18:07:03,376 - INFO - training batch 1851, loss: 0.490, 59232/60000 datapoints
2025-03-06 18:07:03,477 - INFO - validation batch 1, loss: 0.315, 32/10016 datapoints
2025-03-06 18:07:03,631 - INFO - validation batch 51, loss: 0.142, 1632/10016 datapoints
2025-03-06 18:07:03,783 - INFO - validation batch 101, loss: 0.699, 3232/10016 datapoints
2025-03-06 18:07:03,938 - INFO - validation batch 151, loss: 0.489, 4832/10016 datapoints
2025-03-06 18:07:04,091 - INFO - validation batch 201, loss: 0.284, 6432/10016 datapoints
2025-03-06 18:07:04,243 - INFO - validation batch 251, loss: 0.356, 8032/10016 datapoints
2025-03-06 18:07:04,402 - INFO - validation batch 301, loss: 0.565, 9632/10016 datapoints
2025-03-06 18:07:04,441 - INFO - Epoch 166/800 done.
2025-03-06 18:07:04,441 - INFO - Final validation performance:
Loss: 0.407, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 18:07:04,441 - INFO - Beginning epoch 167/800
2025-03-06 18:07:04,447 - INFO - training batch 1, loss: 0.383, 32/60000 datapoints
2025-03-06 18:07:04,645 - INFO - training batch 51, loss: 0.295, 1632/60000 datapoints
2025-03-06 18:07:04,845 - INFO - training batch 101, loss: 0.437, 3232/60000 datapoints
2025-03-06 18:07:05,043 - INFO - training batch 151, loss: 0.348, 4832/60000 datapoints
2025-03-06 18:07:05,238 - INFO - training batch 201, loss: 0.402, 6432/60000 datapoints
2025-03-06 18:07:05,434 - INFO - training batch 251, loss: 0.325, 8032/60000 datapoints
2025-03-06 18:07:05,629 - INFO - training batch 301, loss: 0.158, 9632/60000 datapoints
2025-03-06 18:07:05,836 - INFO - training batch 351, loss: 0.403, 11232/60000 datapoints
2025-03-06 18:07:06,032 - INFO - training batch 401, loss: 0.451, 12832/60000 datapoints
2025-03-06 18:07:06,229 - INFO - training batch 451, loss: 0.313, 14432/60000 datapoints
2025-03-06 18:07:06,426 - INFO - training batch 501, loss: 0.285, 16032/60000 datapoints
2025-03-06 18:07:06,622 - INFO - training batch 551, loss: 0.228, 17632/60000 datapoints
2025-03-06 18:07:06,817 - INFO - training batch 601, loss: 0.364, 19232/60000 datapoints
2025-03-06 18:07:07,015 - INFO - training batch 651, loss: 0.229, 20832/60000 datapoints
2025-03-06 18:07:07,209 - INFO - training batch 701, loss: 0.171, 22432/60000 datapoints
2025-03-06 18:07:07,403 - INFO - training batch 751, loss: 0.305, 24032/60000 datapoints
2025-03-06 18:07:07,616 - INFO - training batch 801, loss: 0.265, 25632/60000 datapoints
2025-03-06 18:07:07,836 - INFO - training batch 851, loss: 0.184, 27232/60000 datapoints
2025-03-06 18:07:08,064 - INFO - training batch 901, loss: 0.473, 28832/60000 datapoints
2025-03-06 18:07:08,273 - INFO - training batch 951, loss: 0.462, 30432/60000 datapoints
2025-03-06 18:07:08,470 - INFO - training batch 1001, loss: 0.199, 32032/60000 datapoints
2025-03-06 18:07:08,666 - INFO - training batch 1051, loss: 0.388, 33632/60000 datapoints
2025-03-06 18:07:08,861 - INFO - training batch 1101, loss: 0.470, 35232/60000 datapoints
2025-03-06 18:07:09,061 - INFO - training batch 1151, loss: 0.398, 36832/60000 datapoints
2025-03-06 18:07:09,254 - INFO - training batch 1201, loss: 0.252, 38432/60000 datapoints
2025-03-06 18:07:09,467 - INFO - training batch 1251, loss: 0.234, 40032/60000 datapoints
2025-03-06 18:07:09,665 - INFO - training batch 1301, loss: 0.457, 41632/60000 datapoints
2025-03-06 18:07:09,861 - INFO - training batch 1351, loss: 0.439, 43232/60000 datapoints
2025-03-06 18:07:10,056 - INFO - training batch 1401, loss: 0.346, 44832/60000 datapoints
2025-03-06 18:07:10,249 - INFO - training batch 1451, loss: 0.590, 46432/60000 datapoints
2025-03-06 18:07:10,461 - INFO - training batch 1501, loss: 0.287, 48032/60000 datapoints
2025-03-06 18:07:10,667 - INFO - training batch 1551, loss: 0.480, 49632/60000 datapoints
2025-03-06 18:07:10,864 - INFO - training batch 1601, loss: 0.424, 51232/60000 datapoints
2025-03-06 18:07:11,059 - INFO - training batch 1651, loss: 0.554, 52832/60000 datapoints
2025-03-06 18:07:11,254 - INFO - training batch 1701, loss: 0.216, 54432/60000 datapoints
2025-03-06 18:07:11,451 - INFO - training batch 1751, loss: 0.269, 56032/60000 datapoints
2025-03-06 18:07:11,646 - INFO - training batch 1801, loss: 0.228, 57632/60000 datapoints
2025-03-06 18:07:11,839 - INFO - training batch 1851, loss: 0.570, 59232/60000 datapoints
2025-03-06 18:07:11,942 - INFO - validation batch 1, loss: 0.349, 32/10016 datapoints
2025-03-06 18:07:12,092 - INFO - validation batch 51, loss: 0.506, 1632/10016 datapoints
2025-03-06 18:07:12,245 - INFO - validation batch 101, loss: 0.386, 3232/10016 datapoints
2025-03-06 18:07:12,399 - INFO - validation batch 151, loss: 0.274, 4832/10016 datapoints
2025-03-06 18:07:12,555 - INFO - validation batch 201, loss: 0.544, 6432/10016 datapoints
2025-03-06 18:07:12,710 - INFO - validation batch 251, loss: 0.303, 8032/10016 datapoints
2025-03-06 18:07:12,862 - INFO - validation batch 301, loss: 0.499, 9632/10016 datapoints
2025-03-06 18:07:12,898 - INFO - Epoch 167/800 done.
2025-03-06 18:07:12,898 - INFO - Final validation performance:
Loss: 0.409, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 18:07:12,898 - INFO - Beginning epoch 168/800
2025-03-06 18:07:12,905 - INFO - training batch 1, loss: 0.240, 32/60000 datapoints
2025-03-06 18:07:13,104 - INFO - training batch 51, loss: 0.374, 1632/60000 datapoints
2025-03-06 18:07:13,302 - INFO - training batch 101, loss: 0.508, 3232/60000 datapoints
2025-03-06 18:07:13,498 - INFO - training batch 151, loss: 0.465, 4832/60000 datapoints
2025-03-06 18:07:13,696 - INFO - training batch 201, loss: 0.138, 6432/60000 datapoints
2025-03-06 18:07:13,892 - INFO - training batch 251, loss: 0.362, 8032/60000 datapoints
2025-03-06 18:07:14,088 - INFO - training batch 301, loss: 0.377, 9632/60000 datapoints
2025-03-06 18:07:14,284 - INFO - training batch 351, loss: 0.786, 11232/60000 datapoints
2025-03-06 18:07:14,481 - INFO - training batch 401, loss: 0.596, 12832/60000 datapoints
2025-03-06 18:07:14,678 - INFO - training batch 451, loss: 0.318, 14432/60000 datapoints
2025-03-06 18:07:14,878 - INFO - training batch 501, loss: 0.340, 16032/60000 datapoints
2025-03-06 18:07:15,075 - INFO - training batch 551, loss: 0.487, 17632/60000 datapoints
2025-03-06 18:07:15,269 - INFO - training batch 601, loss: 0.257, 19232/60000 datapoints
2025-03-06 18:07:15,463 - INFO - training batch 651, loss: 0.403, 20832/60000 datapoints
2025-03-06 18:07:15,658 - INFO - training batch 701, loss: 0.250, 22432/60000 datapoints
2025-03-06 18:07:15,852 - INFO - training batch 751, loss: 0.351, 24032/60000 datapoints
2025-03-06 18:07:16,048 - INFO - training batch 801, loss: 0.417, 25632/60000 datapoints
2025-03-06 18:07:16,246 - INFO - training batch 851, loss: 0.288, 27232/60000 datapoints
2025-03-06 18:07:16,443 - INFO - training batch 901, loss: 0.355, 28832/60000 datapoints
2025-03-06 18:07:16,640 - INFO - training batch 951, loss: 0.273, 30432/60000 datapoints
2025-03-06 18:07:16,835 - INFO - training batch 1001, loss: 0.320, 32032/60000 datapoints
2025-03-06 18:07:17,031 - INFO - training batch 1051, loss: 0.167, 33632/60000 datapoints
2025-03-06 18:07:17,226 - INFO - training batch 1101, loss: 0.366, 35232/60000 datapoints
2025-03-06 18:07:17,421 - INFO - training batch 1151, loss: 0.518, 36832/60000 datapoints
2025-03-06 18:07:17,628 - INFO - training batch 1201, loss: 0.434, 38432/60000 datapoints
2025-03-06 18:07:17,823 - INFO - training batch 1251, loss: 0.171, 40032/60000 datapoints
2025-03-06 18:07:18,018 - INFO - training batch 1301, loss: 0.340, 41632/60000 datapoints
2025-03-06 18:07:18,217 - INFO - training batch 1351, loss: 0.580, 43232/60000 datapoints
2025-03-06 18:07:18,414 - INFO - training batch 1401, loss: 0.404, 44832/60000 datapoints
2025-03-06 18:07:18,611 - INFO - training batch 1451, loss: 0.563, 46432/60000 datapoints
2025-03-06 18:07:18,805 - INFO - training batch 1501, loss: 0.345, 48032/60000 datapoints
2025-03-06 18:07:19,001 - INFO - training batch 1551, loss: 0.355, 49632/60000 datapoints
2025-03-06 18:07:19,193 - INFO - training batch 1601, loss: 0.439, 51232/60000 datapoints
2025-03-06 18:07:19,389 - INFO - training batch 1651, loss: 0.427, 52832/60000 datapoints
2025-03-06 18:07:19,585 - INFO - training batch 1701, loss: 0.551, 54432/60000 datapoints
2025-03-06 18:07:19,779 - INFO - training batch 1751, loss: 0.505, 56032/60000 datapoints
2025-03-06 18:07:19,975 - INFO - training batch 1801, loss: 0.261, 57632/60000 datapoints
2025-03-06 18:07:20,170 - INFO - training batch 1851, loss: 0.697, 59232/60000 datapoints
2025-03-06 18:07:20,271 - INFO - validation batch 1, loss: 0.235, 32/10016 datapoints
2025-03-06 18:07:20,427 - INFO - validation batch 51, loss: 0.303, 1632/10016 datapoints
2025-03-06 18:07:20,607 - INFO - validation batch 101, loss: 0.383, 3232/10016 datapoints
2025-03-06 18:07:20,761 - INFO - validation batch 151, loss: 0.242, 4832/10016 datapoints
2025-03-06 18:07:20,917 - INFO - validation batch 201, loss: 0.173, 6432/10016 datapoints
2025-03-06 18:07:21,068 - INFO - validation batch 251, loss: 0.455, 8032/10016 datapoints
2025-03-06 18:07:21,220 - INFO - validation batch 301, loss: 0.375, 9632/10016 datapoints
2025-03-06 18:07:21,255 - INFO - Epoch 168/800 done.
2025-03-06 18:07:21,255 - INFO - Final validation performance:
Loss: 0.309, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 18:07:21,256 - INFO - Beginning epoch 169/800
2025-03-06 18:07:21,262 - INFO - training batch 1, loss: 0.360, 32/60000 datapoints
2025-03-06 18:07:21,455 - INFO - training batch 51, loss: 0.580, 1632/60000 datapoints
2025-03-06 18:07:21,650 - INFO - training batch 101, loss: 0.576, 3232/60000 datapoints
2025-03-06 18:07:21,842 - INFO - training batch 151, loss: 0.330, 4832/60000 datapoints
2025-03-06 18:07:22,034 - INFO - training batch 201, loss: 0.304, 6432/60000 datapoints
2025-03-06 18:07:22,228 - INFO - training batch 251, loss: 0.418, 8032/60000 datapoints
2025-03-06 18:07:22,421 - INFO - training batch 301, loss: 0.195, 9632/60000 datapoints
2025-03-06 18:07:22,614 - INFO - training batch 351, loss: 0.546, 11232/60000 datapoints
2025-03-06 18:07:22,806 - INFO - training batch 401, loss: 0.584, 12832/60000 datapoints
2025-03-06 18:07:22,999 - INFO - training batch 451, loss: 0.323, 14432/60000 datapoints
2025-03-06 18:07:23,191 - INFO - training batch 501, loss: 0.686, 16032/60000 datapoints
2025-03-06 18:07:23,383 - INFO - training batch 551, loss: 0.249, 17632/60000 datapoints
2025-03-06 18:07:23,573 - INFO - training batch 601, loss: 0.386, 19232/60000 datapoints
2025-03-06 18:07:23,765 - INFO - training batch 651, loss: 0.305, 20832/60000 datapoints
2025-03-06 18:07:23,956 - INFO - training batch 701, loss: 0.289, 22432/60000 datapoints
2025-03-06 18:07:24,156 - INFO - training batch 751, loss: 0.403, 24032/60000 datapoints
2025-03-06 18:07:24,350 - INFO - training batch 801, loss: 0.264, 25632/60000 datapoints
2025-03-06 18:07:24,544 - INFO - training batch 851, loss: 0.566, 27232/60000 datapoints
2025-03-06 18:07:24,738 - INFO - training batch 901, loss: 0.519, 28832/60000 datapoints
2025-03-06 18:07:24,937 - INFO - training batch 951, loss: 0.548, 30432/60000 datapoints
2025-03-06 18:07:25,158 - INFO - training batch 1001, loss: 0.433, 32032/60000 datapoints
2025-03-06 18:07:25,352 - INFO - training batch 1051, loss: 0.409, 33632/60000 datapoints
2025-03-06 18:07:25,545 - INFO - training batch 1101, loss: 0.371, 35232/60000 datapoints
2025-03-06 18:07:25,739 - INFO - training batch 1151, loss: 0.409, 36832/60000 datapoints
2025-03-06 18:07:25,932 - INFO - training batch 1201, loss: 0.474, 38432/60000 datapoints
2025-03-06 18:07:26,124 - INFO - training batch 1251, loss: 0.590, 40032/60000 datapoints
2025-03-06 18:07:26,320 - INFO - training batch 1301, loss: 0.407, 41632/60000 datapoints
2025-03-06 18:07:26,517 - INFO - training batch 1351, loss: 0.451, 43232/60000 datapoints
2025-03-06 18:07:26,711 - INFO - training batch 1401, loss: 0.292, 44832/60000 datapoints
2025-03-06 18:07:26,901 - INFO - training batch 1451, loss: 0.206, 46432/60000 datapoints
2025-03-06 18:07:27,091 - INFO - training batch 1501, loss: 0.651, 48032/60000 datapoints
2025-03-06 18:07:27,281 - INFO - training batch 1551, loss: 0.235, 49632/60000 datapoints
2025-03-06 18:07:27,472 - INFO - training batch 1601, loss: 0.317, 51232/60000 datapoints
2025-03-06 18:07:27,670 - INFO - training batch 1651, loss: 0.225, 52832/60000 datapoints
2025-03-06 18:07:27,863 - INFO - training batch 1701, loss: 0.338, 54432/60000 datapoints
2025-03-06 18:07:28,055 - INFO - training batch 1751, loss: 0.429, 56032/60000 datapoints
2025-03-06 18:07:28,253 - INFO - training batch 1801, loss: 0.352, 57632/60000 datapoints
2025-03-06 18:07:28,452 - INFO - training batch 1851, loss: 0.383, 59232/60000 datapoints
2025-03-06 18:07:28,549 - INFO - validation batch 1, loss: 0.239, 32/10016 datapoints
2025-03-06 18:07:28,704 - INFO - validation batch 51, loss: 0.532, 1632/10016 datapoints
2025-03-06 18:07:28,855 - INFO - validation batch 101, loss: 0.203, 3232/10016 datapoints
2025-03-06 18:07:29,007 - INFO - validation batch 151, loss: 0.459, 4832/10016 datapoints
2025-03-06 18:07:29,157 - INFO - validation batch 201, loss: 0.653, 6432/10016 datapoints
2025-03-06 18:07:29,306 - INFO - validation batch 251, loss: 0.627, 8032/10016 datapoints
2025-03-06 18:07:29,458 - INFO - validation batch 301, loss: 0.421, 9632/10016 datapoints
2025-03-06 18:07:29,493 - INFO - Epoch 169/800 done.
2025-03-06 18:07:29,493 - INFO - Final validation performance:
Loss: 0.448, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 18:07:29,494 - INFO - Beginning epoch 170/800
2025-03-06 18:07:29,499 - INFO - training batch 1, loss: 0.497, 32/60000 datapoints
2025-03-06 18:07:29,694 - INFO - training batch 51, loss: 0.299, 1632/60000 datapoints
2025-03-06 18:07:29,886 - INFO - training batch 101, loss: 0.384, 3232/60000 datapoints
2025-03-06 18:07:30,080 - INFO - training batch 151, loss: 0.407, 4832/60000 datapoints
2025-03-06 18:07:30,271 - INFO - training batch 201, loss: 0.193, 6432/60000 datapoints
2025-03-06 18:07:30,466 - INFO - training batch 251, loss: 0.401, 8032/60000 datapoints
2025-03-06 18:07:30,679 - INFO - training batch 301, loss: 0.287, 9632/60000 datapoints
2025-03-06 18:07:30,872 - INFO - training batch 351, loss: 0.254, 11232/60000 datapoints
2025-03-06 18:07:31,068 - INFO - training batch 401, loss: 0.247, 12832/60000 datapoints
2025-03-06 18:07:31,260 - INFO - training batch 451, loss: 0.278, 14432/60000 datapoints
2025-03-06 18:07:31,452 - INFO - training batch 501, loss: 0.334, 16032/60000 datapoints
2025-03-06 18:07:31,647 - INFO - training batch 551, loss: 0.439, 17632/60000 datapoints
2025-03-06 18:07:31,838 - INFO - training batch 601, loss: 0.578, 19232/60000 datapoints
2025-03-06 18:07:32,031 - INFO - training batch 651, loss: 0.338, 20832/60000 datapoints
2025-03-06 18:07:32,225 - INFO - training batch 701, loss: 0.243, 22432/60000 datapoints
2025-03-06 18:07:32,419 - INFO - training batch 751, loss: 0.292, 24032/60000 datapoints
2025-03-06 18:07:32,610 - INFO - training batch 801, loss: 0.337, 25632/60000 datapoints
2025-03-06 18:07:32,803 - INFO - training batch 851, loss: 0.223, 27232/60000 datapoints
2025-03-06 18:07:32,993 - INFO - training batch 901, loss: 0.278, 28832/60000 datapoints
2025-03-06 18:07:33,192 - INFO - training batch 951, loss: 0.144, 30432/60000 datapoints
2025-03-06 18:07:33,383 - INFO - training batch 1001, loss: 0.297, 32032/60000 datapoints
2025-03-06 18:07:33,573 - INFO - training batch 1051, loss: 0.428, 33632/60000 datapoints
2025-03-06 18:07:33,770 - INFO - training batch 1101, loss: 0.476, 35232/60000 datapoints
2025-03-06 18:07:33,961 - INFO - training batch 1151, loss: 0.256, 36832/60000 datapoints
2025-03-06 18:07:34,155 - INFO - training batch 1201, loss: 0.487, 38432/60000 datapoints
2025-03-06 18:07:34,345 - INFO - training batch 1251, loss: 0.562, 40032/60000 datapoints
2025-03-06 18:07:34,540 - INFO - training batch 1301, loss: 0.489, 41632/60000 datapoints
2025-03-06 18:07:34,735 - INFO - training batch 1351, loss: 0.507, 43232/60000 datapoints
2025-03-06 18:07:34,931 - INFO - training batch 1401, loss: 0.332, 44832/60000 datapoints
2025-03-06 18:07:35,127 - INFO - training batch 1451, loss: 0.182, 46432/60000 datapoints
2025-03-06 18:07:35,321 - INFO - training batch 1501, loss: 0.598, 48032/60000 datapoints
2025-03-06 18:07:35,514 - INFO - training batch 1551, loss: 0.439, 49632/60000 datapoints
2025-03-06 18:07:35,709 - INFO - training batch 1601, loss: 0.398, 51232/60000 datapoints
2025-03-06 18:07:35,900 - INFO - training batch 1651, loss: 0.842, 52832/60000 datapoints
2025-03-06 18:07:36,092 - INFO - training batch 1701, loss: 0.661, 54432/60000 datapoints
2025-03-06 18:07:36,290 - INFO - training batch 1751, loss: 0.634, 56032/60000 datapoints
2025-03-06 18:07:36,486 - INFO - training batch 1801, loss: 0.504, 57632/60000 datapoints
2025-03-06 18:07:36,680 - INFO - training batch 1851, loss: 0.305, 59232/60000 datapoints
2025-03-06 18:07:36,780 - INFO - validation batch 1, loss: 0.416, 32/10016 datapoints
2025-03-06 18:07:36,932 - INFO - validation batch 51, loss: 0.344, 1632/10016 datapoints
2025-03-06 18:07:37,081 - INFO - validation batch 101, loss: 0.728, 3232/10016 datapoints
2025-03-06 18:07:37,239 - INFO - validation batch 151, loss: 0.213, 4832/10016 datapoints
2025-03-06 18:07:37,393 - INFO - validation batch 201, loss: 0.707, 6432/10016 datapoints
2025-03-06 18:07:37,559 - INFO - validation batch 251, loss: 0.340, 8032/10016 datapoints
2025-03-06 18:07:37,748 - INFO - validation batch 301, loss: 0.398, 9632/10016 datapoints
2025-03-06 18:07:37,786 - INFO - Epoch 170/800 done.
2025-03-06 18:07:37,787 - INFO - Final validation performance:
Loss: 0.449, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 18:07:37,787 - INFO - Beginning epoch 171/800
2025-03-06 18:07:37,793 - INFO - training batch 1, loss: 0.373, 32/60000 datapoints
2025-03-06 18:07:37,989 - INFO - training batch 51, loss: 0.308, 1632/60000 datapoints
2025-03-06 18:07:38,188 - INFO - training batch 101, loss: 0.526, 3232/60000 datapoints
2025-03-06 18:07:38,389 - INFO - training batch 151, loss: 0.307, 4832/60000 datapoints
2025-03-06 18:07:38,583 - INFO - training batch 201, loss: 0.302, 6432/60000 datapoints
2025-03-06 18:07:38,779 - INFO - training batch 251, loss: 0.156, 8032/60000 datapoints
2025-03-06 18:07:38,977 - INFO - training batch 301, loss: 0.455, 9632/60000 datapoints
2025-03-06 18:07:39,180 - INFO - training batch 351, loss: 0.255, 11232/60000 datapoints
2025-03-06 18:07:39,375 - INFO - training batch 401, loss: 0.601, 12832/60000 datapoints
2025-03-06 18:07:39,571 - INFO - training batch 451, loss: 0.332, 14432/60000 datapoints
2025-03-06 18:07:39,768 - INFO - training batch 501, loss: 0.414, 16032/60000 datapoints
2025-03-06 18:07:39,963 - INFO - training batch 551, loss: 0.405, 17632/60000 datapoints
2025-03-06 18:07:40,160 - INFO - training batch 601, loss: 0.379, 19232/60000 datapoints
2025-03-06 18:07:40,354 - INFO - training batch 651, loss: 0.316, 20832/60000 datapoints
2025-03-06 18:07:40,551 - INFO - training batch 701, loss: 0.424, 22432/60000 datapoints
2025-03-06 18:07:40,771 - INFO - training batch 751, loss: 0.182, 24032/60000 datapoints
2025-03-06 18:07:40,965 - INFO - training batch 801, loss: 0.228, 25632/60000 datapoints
2025-03-06 18:07:41,159 - INFO - training batch 851, loss: 0.482, 27232/60000 datapoints
2025-03-06 18:07:41,355 - INFO - training batch 901, loss: 0.165, 28832/60000 datapoints
2025-03-06 18:07:41,552 - INFO - training batch 951, loss: 0.595, 30432/60000 datapoints
2025-03-06 18:07:41,748 - INFO - training batch 1001, loss: 0.336, 32032/60000 datapoints
2025-03-06 18:07:41,944 - INFO - training batch 1051, loss: 0.281, 33632/60000 datapoints
2025-03-06 18:07:42,138 - INFO - training batch 1101, loss: 0.202, 35232/60000 datapoints
2025-03-06 18:07:42,330 - INFO - training batch 1151, loss: 0.392, 36832/60000 datapoints
2025-03-06 18:07:42,528 - INFO - training batch 1201, loss: 0.433, 38432/60000 datapoints
2025-03-06 18:07:42,728 - INFO - training batch 1251, loss: 0.340, 40032/60000 datapoints
2025-03-06 18:07:42,925 - INFO - training batch 1301, loss: 0.307, 41632/60000 datapoints
2025-03-06 18:07:43,120 - INFO - training batch 1351, loss: 0.419, 43232/60000 datapoints
2025-03-06 18:07:43,316 - INFO - training batch 1401, loss: 0.548, 44832/60000 datapoints
2025-03-06 18:07:43,510 - INFO - training batch 1451, loss: 0.240, 46432/60000 datapoints
2025-03-06 18:07:43,706 - INFO - training batch 1501, loss: 0.463, 48032/60000 datapoints
2025-03-06 18:07:43,905 - INFO - training batch 1551, loss: 0.293, 49632/60000 datapoints
2025-03-06 18:07:44,101 - INFO - training batch 1601, loss: 0.371, 51232/60000 datapoints
2025-03-06 18:07:44,298 - INFO - training batch 1651, loss: 0.560, 52832/60000 datapoints
2025-03-06 18:07:44,495 - INFO - training batch 1701, loss: 0.529, 54432/60000 datapoints
2025-03-06 18:07:44,688 - INFO - training batch 1751, loss: 0.360, 56032/60000 datapoints
2025-03-06 18:07:44,887 - INFO - training batch 1801, loss: 0.431, 57632/60000 datapoints
2025-03-06 18:07:45,086 - INFO - training batch 1851, loss: 0.292, 59232/60000 datapoints
2025-03-06 18:07:45,187 - INFO - validation batch 1, loss: 0.374, 32/10016 datapoints
2025-03-06 18:07:45,344 - INFO - validation batch 51, loss: 0.487, 1632/10016 datapoints
2025-03-06 18:07:45,496 - INFO - validation batch 101, loss: 0.364, 3232/10016 datapoints
2025-03-06 18:07:45,651 - INFO - validation batch 151, loss: 0.407, 4832/10016 datapoints
2025-03-06 18:07:45,804 - INFO - validation batch 201, loss: 0.442, 6432/10016 datapoints
2025-03-06 18:07:45,957 - INFO - validation batch 251, loss: 0.277, 8032/10016 datapoints
2025-03-06 18:07:46,108 - INFO - validation batch 301, loss: 0.490, 9632/10016 datapoints
2025-03-06 18:07:46,146 - INFO - Epoch 171/800 done.
2025-03-06 18:07:46,146 - INFO - Final validation performance:
Loss: 0.406, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 18:07:46,146 - INFO - Beginning epoch 172/800
2025-03-06 18:07:46,153 - INFO - training batch 1, loss: 0.470, 32/60000 datapoints
2025-03-06 18:07:46,355 - INFO - training batch 51, loss: 0.518, 1632/60000 datapoints
2025-03-06 18:07:46,553 - INFO - training batch 101, loss: 0.490, 3232/60000 datapoints
2025-03-06 18:07:46,749 - INFO - training batch 151, loss: 0.721, 4832/60000 datapoints
2025-03-06 18:07:46,947 - INFO - training batch 201, loss: 0.098, 6432/60000 datapoints
2025-03-06 18:07:47,141 - INFO - training batch 251, loss: 0.279, 8032/60000 datapoints
2025-03-06 18:07:47,335 - INFO - training batch 301, loss: 0.293, 9632/60000 datapoints
2025-03-06 18:07:47,528 - INFO - training batch 351, loss: 0.333, 11232/60000 datapoints
2025-03-06 18:07:47,726 - INFO - training batch 401, loss: 0.262, 12832/60000 datapoints
2025-03-06 18:07:47,922 - INFO - training batch 451, loss: 0.323, 14432/60000 datapoints
2025-03-06 18:07:48,135 - INFO - training batch 501, loss: 0.330, 16032/60000 datapoints
2025-03-06 18:07:48,334 - INFO - training batch 551, loss: 0.334, 17632/60000 datapoints
2025-03-06 18:07:48,530 - INFO - training batch 601, loss: 0.917, 19232/60000 datapoints
2025-03-06 18:07:48,724 - INFO - training batch 651, loss: 0.696, 20832/60000 datapoints
2025-03-06 18:07:48,919 - INFO - training batch 701, loss: 0.508, 22432/60000 datapoints
2025-03-06 18:07:49,114 - INFO - training batch 751, loss: 0.420, 24032/60000 datapoints
2025-03-06 18:07:49,309 - INFO - training batch 801, loss: 0.208, 25632/60000 datapoints
2025-03-06 18:07:49,506 - INFO - training batch 851, loss: 0.304, 27232/60000 datapoints
2025-03-06 18:07:49,705 - INFO - training batch 901, loss: 0.354, 28832/60000 datapoints
2025-03-06 18:07:49,901 - INFO - training batch 951, loss: 0.206, 30432/60000 datapoints
2025-03-06 18:07:50,111 - INFO - training batch 1001, loss: 0.460, 32032/60000 datapoints
2025-03-06 18:07:50,318 - INFO - training batch 1051, loss: 0.457, 33632/60000 datapoints
2025-03-06 18:07:50,543 - INFO - training batch 1101, loss: 0.429, 35232/60000 datapoints
2025-03-06 18:07:50,770 - INFO - training batch 1151, loss: 0.515, 36832/60000 datapoints
2025-03-06 18:07:50,987 - INFO - training batch 1201, loss: 0.533, 38432/60000 datapoints
2025-03-06 18:07:51,182 - INFO - training batch 1251, loss: 0.413, 40032/60000 datapoints
2025-03-06 18:07:51,377 - INFO - training batch 1301, loss: 0.323, 41632/60000 datapoints
2025-03-06 18:07:51,570 - INFO - training batch 1351, loss: 0.373, 43232/60000 datapoints
2025-03-06 18:07:51,768 - INFO - training batch 1401, loss: 0.351, 44832/60000 datapoints
2025-03-06 18:07:51,963 - INFO - training batch 1451, loss: 0.406, 46432/60000 datapoints
2025-03-06 18:07:52,161 - INFO - training batch 1501, loss: 0.222, 48032/60000 datapoints
2025-03-06 18:07:52,364 - INFO - training batch 1551, loss: 0.328, 49632/60000 datapoints
2025-03-06 18:07:52,559 - INFO - training batch 1601, loss: 0.550, 51232/60000 datapoints
2025-03-06 18:07:52,760 - INFO - training batch 1651, loss: 0.401, 52832/60000 datapoints
2025-03-06 18:07:52,955 - INFO - training batch 1701, loss: 0.237, 54432/60000 datapoints
2025-03-06 18:07:53,149 - INFO - training batch 1751, loss: 0.480, 56032/60000 datapoints
2025-03-06 18:07:53,341 - INFO - training batch 1801, loss: 0.249, 57632/60000 datapoints
2025-03-06 18:07:53,538 - INFO - training batch 1851, loss: 0.451, 59232/60000 datapoints
2025-03-06 18:07:53,641 - INFO - validation batch 1, loss: 0.382, 32/10016 datapoints
2025-03-06 18:07:53,791 - INFO - validation batch 51, loss: 0.333, 1632/10016 datapoints
2025-03-06 18:07:53,947 - INFO - validation batch 101, loss: 0.420, 3232/10016 datapoints
2025-03-06 18:07:54,098 - INFO - validation batch 151, loss: 0.322, 4832/10016 datapoints
2025-03-06 18:07:54,253 - INFO - validation batch 201, loss: 0.740, 6432/10016 datapoints
2025-03-06 18:07:54,408 - INFO - validation batch 251, loss: 0.942, 8032/10016 datapoints
2025-03-06 18:07:54,563 - INFO - validation batch 301, loss: 0.333, 9632/10016 datapoints
2025-03-06 18:07:54,601 - INFO - Epoch 172/800 done.
2025-03-06 18:07:54,601 - INFO - Final validation performance:
Loss: 0.496, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 18:07:54,602 - INFO - Beginning epoch 173/800
2025-03-06 18:07:54,609 - INFO - training batch 1, loss: 0.209, 32/60000 datapoints
2025-03-06 18:07:54,807 - INFO - training batch 51, loss: 0.551, 1632/60000 datapoints
2025-03-06 18:07:55,004 - INFO - training batch 101, loss: 0.507, 3232/60000 datapoints
2025-03-06 18:07:55,199 - INFO - training batch 151, loss: 0.311, 4832/60000 datapoints
2025-03-06 18:07:55,394 - INFO - training batch 201, loss: 0.426, 6432/60000 datapoints
2025-03-06 18:07:55,588 - INFO - training batch 251, loss: 0.373, 8032/60000 datapoints
2025-03-06 18:07:55,786 - INFO - training batch 301, loss: 0.317, 9632/60000 datapoints
2025-03-06 18:07:55,981 - INFO - training batch 351, loss: 0.393, 11232/60000 datapoints
2025-03-06 18:07:56,178 - INFO - training batch 401, loss: 0.601, 12832/60000 datapoints
2025-03-06 18:07:56,372 - INFO - training batch 451, loss: 0.267, 14432/60000 datapoints
2025-03-06 18:07:56,571 - INFO - training batch 501, loss: 0.473, 16032/60000 datapoints
2025-03-06 18:07:56,767 - INFO - training batch 551, loss: 0.367, 17632/60000 datapoints
2025-03-06 18:07:56,963 - INFO - training batch 601, loss: 0.499, 19232/60000 datapoints
2025-03-06 18:07:57,160 - INFO - training batch 651, loss: 0.370, 20832/60000 datapoints
2025-03-06 18:07:57,354 - INFO - training batch 701, loss: 0.488, 22432/60000 datapoints
2025-03-06 18:07:57,549 - INFO - training batch 751, loss: 0.263, 24032/60000 datapoints
2025-03-06 18:07:57,752 - INFO - training batch 801, loss: 0.374, 25632/60000 datapoints
2025-03-06 18:07:57,946 - INFO - training batch 851, loss: 0.169, 27232/60000 datapoints
2025-03-06 18:07:58,142 - INFO - training batch 901, loss: 0.276, 28832/60000 datapoints
2025-03-06 18:07:58,337 - INFO - training batch 951, loss: 0.566, 30432/60000 datapoints
2025-03-06 18:07:58,535 - INFO - training batch 1001, loss: 0.563, 32032/60000 datapoints
2025-03-06 18:07:58,732 - INFO - training batch 1051, loss: 0.465, 33632/60000 datapoints
2025-03-06 18:07:58,926 - INFO - training batch 1101, loss: 0.439, 35232/60000 datapoints
2025-03-06 18:07:59,129 - INFO - training batch 1151, loss: 0.309, 36832/60000 datapoints
2025-03-06 18:07:59,331 - INFO - training batch 1201, loss: 0.492, 38432/60000 datapoints
2025-03-06 18:07:59,531 - INFO - training batch 1251, loss: 0.285, 40032/60000 datapoints
2025-03-06 18:07:59,728 - INFO - training batch 1301, loss: 0.237, 41632/60000 datapoints
2025-03-06 18:07:59,926 - INFO - training batch 1351, loss: 0.497, 43232/60000 datapoints
2025-03-06 18:08:00,122 - INFO - training batch 1401, loss: 0.246, 44832/60000 datapoints
2025-03-06 18:08:00,320 - INFO - training batch 1451, loss: 0.356, 46432/60000 datapoints
2025-03-06 18:08:00,520 - INFO - training batch 1501, loss: 0.415, 48032/60000 datapoints
2025-03-06 18:08:00,718 - INFO - training batch 1551, loss: 0.260, 49632/60000 datapoints
2025-03-06 18:08:00,920 - INFO - training batch 1601, loss: 0.497, 51232/60000 datapoints
2025-03-06 18:08:01,126 - INFO - training batch 1651, loss: 0.364, 52832/60000 datapoints
2025-03-06 18:08:01,320 - INFO - training batch 1701, loss: 0.414, 54432/60000 datapoints
2025-03-06 18:08:01,517 - INFO - training batch 1751, loss: 0.407, 56032/60000 datapoints
2025-03-06 18:08:01,717 - INFO - training batch 1801, loss: 0.361, 57632/60000 datapoints
2025-03-06 18:08:01,913 - INFO - training batch 1851, loss: 0.185, 59232/60000 datapoints
2025-03-06 18:08:02,015 - INFO - validation batch 1, loss: 0.195, 32/10016 datapoints
2025-03-06 18:08:02,168 - INFO - validation batch 51, loss: 0.231, 1632/10016 datapoints
2025-03-06 18:08:02,321 - INFO - validation batch 101, loss: 0.437, 3232/10016 datapoints
2025-03-06 18:08:02,477 - INFO - validation batch 151, loss: 0.250, 4832/10016 datapoints
2025-03-06 18:08:02,631 - INFO - validation batch 201, loss: 0.321, 6432/10016 datapoints
2025-03-06 18:08:02,783 - INFO - validation batch 251, loss: 0.168, 8032/10016 datapoints
2025-03-06 18:08:02,934 - INFO - validation batch 301, loss: 0.705, 9632/10016 datapoints
2025-03-06 18:08:02,973 - INFO - Epoch 173/800 done.
2025-03-06 18:08:02,973 - INFO - Final validation performance:
Loss: 0.330, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 18:08:02,973 - INFO - Beginning epoch 174/800
2025-03-06 18:08:02,979 - INFO - training batch 1, loss: 0.312, 32/60000 datapoints
2025-03-06 18:08:03,176 - INFO - training batch 51, loss: 0.363, 1632/60000 datapoints
2025-03-06 18:08:03,371 - INFO - training batch 101, loss: 0.240, 3232/60000 datapoints
2025-03-06 18:08:03,570 - INFO - training batch 151, loss: 0.431, 4832/60000 datapoints
2025-03-06 18:08:03,767 - INFO - training batch 201, loss: 0.380, 6432/60000 datapoints
2025-03-06 18:08:03,963 - INFO - training batch 251, loss: 0.462, 8032/60000 datapoints
2025-03-06 18:08:04,159 - INFO - training batch 301, loss: 0.247, 9632/60000 datapoints
2025-03-06 18:08:04,352 - INFO - training batch 351, loss: 0.333, 11232/60000 datapoints
2025-03-06 18:08:04,551 - INFO - training batch 401, loss: 0.433, 12832/60000 datapoints
2025-03-06 18:08:04,747 - INFO - training batch 451, loss: 0.317, 14432/60000 datapoints
2025-03-06 18:08:04,947 - INFO - training batch 501, loss: 0.227, 16032/60000 datapoints
2025-03-06 18:08:05,141 - INFO - training batch 551, loss: 0.661, 17632/60000 datapoints
2025-03-06 18:08:05,336 - INFO - training batch 601, loss: 0.371, 19232/60000 datapoints
2025-03-06 18:08:05,537 - INFO - training batch 651, loss: 0.261, 20832/60000 datapoints
2025-03-06 18:08:05,747 - INFO - training batch 701, loss: 0.725, 22432/60000 datapoints
2025-03-06 18:08:05,941 - INFO - training batch 751, loss: 0.217, 24032/60000 datapoints
2025-03-06 18:08:06,136 - INFO - training batch 801, loss: 0.340, 25632/60000 datapoints
2025-03-06 18:08:06,334 - INFO - training batch 851, loss: 0.332, 27232/60000 datapoints
2025-03-06 18:08:06,530 - INFO - training batch 901, loss: 0.671, 28832/60000 datapoints
2025-03-06 18:08:06,726 - INFO - training batch 951, loss: 0.296, 30432/60000 datapoints
2025-03-06 18:08:06,920 - INFO - training batch 1001, loss: 0.874, 32032/60000 datapoints
2025-03-06 18:08:07,130 - INFO - training batch 1051, loss: 0.407, 33632/60000 datapoints
2025-03-06 18:08:07,323 - INFO - training batch 1101, loss: 0.442, 35232/60000 datapoints
2025-03-06 18:08:07,518 - INFO - training batch 1151, loss: 0.334, 36832/60000 datapoints
2025-03-06 18:08:07,718 - INFO - training batch 1201, loss: 0.247, 38432/60000 datapoints
2025-03-06 18:08:07,913 - INFO - training batch 1251, loss: 0.322, 40032/60000 datapoints
2025-03-06 18:08:08,147 - INFO - training batch 1301, loss: 0.572, 41632/60000 datapoints
2025-03-06 18:08:08,355 - INFO - training batch 1351, loss: 0.470, 43232/60000 datapoints
2025-03-06 18:08:08,550 - INFO - training batch 1401, loss: 0.300, 44832/60000 datapoints
2025-03-06 18:08:08,746 - INFO - training batch 1451, loss: 0.454, 46432/60000 datapoints
2025-03-06 18:08:08,942 - INFO - training batch 1501, loss: 0.285, 48032/60000 datapoints
2025-03-06 18:08:09,142 - INFO - training batch 1551, loss: 0.237, 49632/60000 datapoints
2025-03-06 18:08:09,336 - INFO - training batch 1601, loss: 0.271, 51232/60000 datapoints
2025-03-06 18:08:09,536 - INFO - training batch 1651, loss: 0.226, 52832/60000 datapoints
2025-03-06 18:08:09,732 - INFO - training batch 1701, loss: 0.396, 54432/60000 datapoints
2025-03-06 18:08:09,928 - INFO - training batch 1751, loss: 0.471, 56032/60000 datapoints
2025-03-06 18:08:10,122 - INFO - training batch 1801, loss: 0.410, 57632/60000 datapoints
2025-03-06 18:08:10,319 - INFO - training batch 1851, loss: 0.359, 59232/60000 datapoints
2025-03-06 18:08:10,419 - INFO - validation batch 1, loss: 0.296, 32/10016 datapoints
2025-03-06 18:08:10,575 - INFO - validation batch 51, loss: 0.727, 1632/10016 datapoints
2025-03-06 18:08:10,731 - INFO - validation batch 101, loss: 0.334, 3232/10016 datapoints
2025-03-06 18:08:10,883 - INFO - validation batch 151, loss: 0.341, 4832/10016 datapoints
2025-03-06 18:08:11,056 - INFO - validation batch 201, loss: 0.305, 6432/10016 datapoints
2025-03-06 18:08:11,208 - INFO - validation batch 251, loss: 0.236, 8032/10016 datapoints
2025-03-06 18:08:11,362 - INFO - validation batch 301, loss: 0.207, 9632/10016 datapoints
2025-03-06 18:08:11,398 - INFO - Epoch 174/800 done.
2025-03-06 18:08:11,398 - INFO - Final validation performance:
Loss: 0.349, top-1 acc: 0.899top-5 acc: 0.899
2025-03-06 18:08:11,398 - INFO - Beginning epoch 175/800
2025-03-06 18:08:11,406 - INFO - training batch 1, loss: 0.215, 32/60000 datapoints
2025-03-06 18:08:11,608 - INFO - training batch 51, loss: 0.457, 1632/60000 datapoints
2025-03-06 18:08:11,802 - INFO - training batch 101, loss: 0.602, 3232/60000 datapoints
2025-03-06 18:08:11,998 - INFO - training batch 151, loss: 0.486, 4832/60000 datapoints
2025-03-06 18:08:12,192 - INFO - training batch 201, loss: 0.257, 6432/60000 datapoints
2025-03-06 18:08:12,388 - INFO - training batch 251, loss: 0.353, 8032/60000 datapoints
2025-03-06 18:08:12,586 - INFO - training batch 301, loss: 0.234, 9632/60000 datapoints
2025-03-06 18:08:12,781 - INFO - training batch 351, loss: 0.195, 11232/60000 datapoints
2025-03-06 18:08:12,974 - INFO - training batch 401, loss: 0.226, 12832/60000 datapoints
2025-03-06 18:08:13,167 - INFO - training batch 451, loss: 0.364, 14432/60000 datapoints
2025-03-06 18:08:13,363 - INFO - training batch 501, loss: 0.350, 16032/60000 datapoints
2025-03-06 18:08:13,560 - INFO - training batch 551, loss: 0.405, 17632/60000 datapoints
2025-03-06 18:08:13,757 - INFO - training batch 601, loss: 0.450, 19232/60000 datapoints
2025-03-06 18:08:13,952 - INFO - training batch 651, loss: 0.657, 20832/60000 datapoints
2025-03-06 18:08:14,146 - INFO - training batch 701, loss: 0.445, 22432/60000 datapoints
2025-03-06 18:08:14,341 - INFO - training batch 751, loss: 0.362, 24032/60000 datapoints
2025-03-06 18:08:14,543 - INFO - training batch 801, loss: 0.396, 25632/60000 datapoints
2025-03-06 18:08:14,742 - INFO - training batch 851, loss: 0.475, 27232/60000 datapoints
2025-03-06 18:08:14,940 - INFO - training batch 901, loss: 0.322, 28832/60000 datapoints
2025-03-06 18:08:15,135 - INFO - training batch 951, loss: 0.500, 30432/60000 datapoints
2025-03-06 18:08:15,330 - INFO - training batch 1001, loss: 0.531, 32032/60000 datapoints
2025-03-06 18:08:15,523 - INFO - training batch 1051, loss: 0.349, 33632/60000 datapoints
2025-03-06 18:08:15,721 - INFO - training batch 1101, loss: 0.410, 35232/60000 datapoints
2025-03-06 18:08:15,916 - INFO - training batch 1151, loss: 0.337, 36832/60000 datapoints
2025-03-06 18:08:16,110 - INFO - training batch 1201, loss: 0.319, 38432/60000 datapoints
2025-03-06 18:08:16,308 - INFO - training batch 1251, loss: 0.334, 40032/60000 datapoints
2025-03-06 18:08:16,502 - INFO - training batch 1301, loss: 0.385, 41632/60000 datapoints
2025-03-06 18:08:16,702 - INFO - training batch 1351, loss: 0.346, 43232/60000 datapoints
2025-03-06 18:08:16,897 - INFO - training batch 1401, loss: 0.431, 44832/60000 datapoints
2025-03-06 18:08:17,091 - INFO - training batch 1451, loss: 0.526, 46432/60000 datapoints
2025-03-06 18:08:17,285 - INFO - training batch 1501, loss: 0.370, 48032/60000 datapoints
2025-03-06 18:08:17,480 - INFO - training batch 1551, loss: 0.268, 49632/60000 datapoints
2025-03-06 18:08:17,679 - INFO - training batch 1601, loss: 0.317, 51232/60000 datapoints
2025-03-06 18:08:17,875 - INFO - training batch 1651, loss: 0.263, 52832/60000 datapoints
2025-03-06 18:08:18,069 - INFO - training batch 1701, loss: 0.291, 54432/60000 datapoints
2025-03-06 18:08:18,269 - INFO - training batch 1751, loss: 0.293, 56032/60000 datapoints
2025-03-06 18:08:18,465 - INFO - training batch 1801, loss: 0.354, 57632/60000 datapoints
2025-03-06 18:08:18,664 - INFO - training batch 1851, loss: 0.288, 59232/60000 datapoints
2025-03-06 18:08:18,764 - INFO - validation batch 1, loss: 0.546, 32/10016 datapoints
2025-03-06 18:08:18,916 - INFO - validation batch 51, loss: 0.321, 1632/10016 datapoints
2025-03-06 18:08:19,068 - INFO - validation batch 101, loss: 0.452, 3232/10016 datapoints
2025-03-06 18:08:19,221 - INFO - validation batch 151, loss: 0.591, 4832/10016 datapoints
2025-03-06 18:08:19,373 - INFO - validation batch 201, loss: 0.422, 6432/10016 datapoints
2025-03-06 18:08:19,525 - INFO - validation batch 251, loss: 0.298, 8032/10016 datapoints
2025-03-06 18:08:19,683 - INFO - validation batch 301, loss: 0.243, 9632/10016 datapoints
2025-03-06 18:08:19,722 - INFO - Epoch 175/800 done.
2025-03-06 18:08:19,722 - INFO - Final validation performance:
Loss: 0.410, top-1 acc: 0.899top-5 acc: 0.899
2025-03-06 18:08:19,722 - INFO - Beginning epoch 176/800
2025-03-06 18:08:19,728 - INFO - training batch 1, loss: 0.544, 32/60000 datapoints
2025-03-06 18:08:19,921 - INFO - training batch 51, loss: 0.615, 1632/60000 datapoints
2025-03-06 18:08:20,115 - INFO - training batch 101, loss: 1.036, 3232/60000 datapoints
2025-03-06 18:08:20,311 - INFO - training batch 151, loss: 0.394, 4832/60000 datapoints
2025-03-06 18:08:20,504 - INFO - training batch 201, loss: 0.357, 6432/60000 datapoints
2025-03-06 18:08:20,704 - INFO - training batch 251, loss: 0.364, 8032/60000 datapoints
2025-03-06 18:08:20,897 - INFO - training batch 301, loss: 0.154, 9632/60000 datapoints
2025-03-06 18:08:21,103 - INFO - training batch 351, loss: 0.495, 11232/60000 datapoints
2025-03-06 18:08:21,305 - INFO - training batch 401, loss: 0.360, 12832/60000 datapoints
2025-03-06 18:08:21,497 - INFO - training batch 451, loss: 0.283, 14432/60000 datapoints
2025-03-06 18:08:21,697 - INFO - training batch 501, loss: 0.304, 16032/60000 datapoints
2025-03-06 18:08:21,894 - INFO - training batch 551, loss: 0.323, 17632/60000 datapoints
2025-03-06 18:08:22,088 - INFO - training batch 601, loss: 0.294, 19232/60000 datapoints
2025-03-06 18:08:22,283 - INFO - training batch 651, loss: 0.244, 20832/60000 datapoints
2025-03-06 18:08:22,477 - INFO - training batch 701, loss: 0.335, 22432/60000 datapoints
2025-03-06 18:08:22,677 - INFO - training batch 751, loss: 0.431, 24032/60000 datapoints
2025-03-06 18:08:22,873 - INFO - training batch 801, loss: 0.457, 25632/60000 datapoints
2025-03-06 18:08:23,066 - INFO - training batch 851, loss: 0.285, 27232/60000 datapoints
2025-03-06 18:08:23,260 - INFO - training batch 901, loss: 0.234, 28832/60000 datapoints
2025-03-06 18:08:23,456 - INFO - training batch 951, loss: 0.354, 30432/60000 datapoints
2025-03-06 18:08:23,652 - INFO - training batch 1001, loss: 0.387, 32032/60000 datapoints
2025-03-06 18:08:23,847 - INFO - training batch 1051, loss: 0.665, 33632/60000 datapoints
2025-03-06 18:08:24,042 - INFO - training batch 1101, loss: 0.223, 35232/60000 datapoints
2025-03-06 18:08:24,239 - INFO - training batch 1151, loss: 0.316, 36832/60000 datapoints
2025-03-06 18:08:24,437 - INFO - training batch 1201, loss: 0.344, 38432/60000 datapoints
2025-03-06 18:08:24,636 - INFO - training batch 1251, loss: 0.230, 40032/60000 datapoints
2025-03-06 18:08:24,835 - INFO - training batch 1301, loss: 0.444, 41632/60000 datapoints
2025-03-06 18:08:25,032 - INFO - training batch 1351, loss: 0.919, 43232/60000 datapoints
2025-03-06 18:08:25,227 - INFO - training batch 1401, loss: 0.364, 44832/60000 datapoints
2025-03-06 18:08:25,446 - INFO - training batch 1451, loss: 0.198, 46432/60000 datapoints
2025-03-06 18:08:25,651 - INFO - training batch 1501, loss: 0.534, 48032/60000 datapoints
2025-03-06 18:08:25,846 - INFO - training batch 1551, loss: 0.321, 49632/60000 datapoints
2025-03-06 18:08:26,043 - INFO - training batch 1601, loss: 0.249, 51232/60000 datapoints
2025-03-06 18:08:26,241 - INFO - training batch 1651, loss: 0.327, 52832/60000 datapoints
2025-03-06 18:08:26,439 - INFO - training batch 1701, loss: 0.461, 54432/60000 datapoints
2025-03-06 18:08:26,636 - INFO - training batch 1751, loss: 0.344, 56032/60000 datapoints
2025-03-06 18:08:26,830 - INFO - training batch 1801, loss: 0.540, 57632/60000 datapoints
2025-03-06 18:08:27,025 - INFO - training batch 1851, loss: 0.203, 59232/60000 datapoints
2025-03-06 18:08:27,127 - INFO - validation batch 1, loss: 0.196, 32/10016 datapoints
2025-03-06 18:08:27,281 - INFO - validation batch 51, loss: 0.378, 1632/10016 datapoints
2025-03-06 18:08:27,434 - INFO - validation batch 101, loss: 0.254, 3232/10016 datapoints
2025-03-06 18:08:27,588 - INFO - validation batch 151, loss: 0.347, 4832/10016 datapoints
2025-03-06 18:08:27,745 - INFO - validation batch 201, loss: 0.542, 6432/10016 datapoints
2025-03-06 18:08:27,900 - INFO - validation batch 251, loss: 0.274, 8032/10016 datapoints
2025-03-06 18:08:28,053 - INFO - validation batch 301, loss: 0.562, 9632/10016 datapoints
2025-03-06 18:08:28,090 - INFO - Epoch 176/800 done.
2025-03-06 18:08:28,090 - INFO - Final validation performance:
Loss: 0.365, top-1 acc: 0.899top-5 acc: 0.899
2025-03-06 18:08:28,090 - INFO - Beginning epoch 177/800
2025-03-06 18:08:28,096 - INFO - training batch 1, loss: 0.333, 32/60000 datapoints
2025-03-06 18:08:28,299 - INFO - training batch 51, loss: 0.341, 1632/60000 datapoints
2025-03-06 18:08:28,496 - INFO - training batch 101, loss: 0.476, 3232/60000 datapoints
2025-03-06 18:08:28,695 - INFO - training batch 151, loss: 0.495, 4832/60000 datapoints
2025-03-06 18:08:28,890 - INFO - training batch 201, loss: 0.315, 6432/60000 datapoints
2025-03-06 18:08:29,082 - INFO - training batch 251, loss: 0.326, 8032/60000 datapoints
2025-03-06 18:08:29,276 - INFO - training batch 301, loss: 0.387, 9632/60000 datapoints
2025-03-06 18:08:29,473 - INFO - training batch 351, loss: 0.459, 11232/60000 datapoints
2025-03-06 18:08:29,670 - INFO - training batch 401, loss: 0.364, 12832/60000 datapoints
2025-03-06 18:08:29,869 - INFO - training batch 451, loss: 0.311, 14432/60000 datapoints
2025-03-06 18:08:30,064 - INFO - training batch 501, loss: 0.336, 16032/60000 datapoints
2025-03-06 18:08:30,261 - INFO - training batch 551, loss: 0.293, 17632/60000 datapoints
2025-03-06 18:08:30,455 - INFO - training batch 601, loss: 0.405, 19232/60000 datapoints
2025-03-06 18:08:30,654 - INFO - training batch 651, loss: 0.533, 20832/60000 datapoints
2025-03-06 18:08:30,850 - INFO - training batch 701, loss: 0.281, 22432/60000 datapoints
2025-03-06 18:08:31,046 - INFO - training batch 751, loss: 0.249, 24032/60000 datapoints
2025-03-06 18:08:31,259 - INFO - training batch 801, loss: 0.280, 25632/60000 datapoints
2025-03-06 18:08:31,454 - INFO - training batch 851, loss: 0.483, 27232/60000 datapoints
2025-03-06 18:08:31,650 - INFO - training batch 901, loss: 0.533, 28832/60000 datapoints
2025-03-06 18:08:31,845 - INFO - training batch 951, loss: 0.370, 30432/60000 datapoints
2025-03-06 18:08:32,042 - INFO - training batch 1001, loss: 0.453, 32032/60000 datapoints
2025-03-06 18:08:32,234 - INFO - training batch 1051, loss: 0.509, 33632/60000 datapoints
2025-03-06 18:08:32,429 - INFO - training batch 1101, loss: 0.230, 35232/60000 datapoints
2025-03-06 18:08:32,626 - INFO - training batch 1151, loss: 0.256, 36832/60000 datapoints
2025-03-06 18:08:32,821 - INFO - training batch 1201, loss: 0.274, 38432/60000 datapoints
2025-03-06 18:08:33,013 - INFO - training batch 1251, loss: 0.279, 40032/60000 datapoints
2025-03-06 18:08:33,207 - INFO - training batch 1301, loss: 0.310, 41632/60000 datapoints
2025-03-06 18:08:33,407 - INFO - training batch 1351, loss: 0.423, 43232/60000 datapoints
2025-03-06 18:08:33,604 - INFO - training batch 1401, loss: 0.410, 44832/60000 datapoints
2025-03-06 18:08:33,798 - INFO - training batch 1451, loss: 0.388, 46432/60000 datapoints
2025-03-06 18:08:33,991 - INFO - training batch 1501, loss: 0.390, 48032/60000 datapoints
2025-03-06 18:08:34,185 - INFO - training batch 1551, loss: 0.449, 49632/60000 datapoints
2025-03-06 18:08:34,381 - INFO - training batch 1601, loss: 0.401, 51232/60000 datapoints
2025-03-06 18:08:34,580 - INFO - training batch 1651, loss: 0.359, 52832/60000 datapoints
2025-03-06 18:08:34,780 - INFO - training batch 1701, loss: 0.329, 54432/60000 datapoints
2025-03-06 18:08:34,977 - INFO - training batch 1751, loss: 0.284, 56032/60000 datapoints
2025-03-06 18:08:35,174 - INFO - training batch 1801, loss: 0.573, 57632/60000 datapoints
2025-03-06 18:08:35,368 - INFO - training batch 1851, loss: 0.351, 59232/60000 datapoints
2025-03-06 18:08:35,470 - INFO - validation batch 1, loss: 0.260, 32/10016 datapoints
2025-03-06 18:08:35,623 - INFO - validation batch 51, loss: 0.692, 1632/10016 datapoints
2025-03-06 18:08:35,775 - INFO - validation batch 101, loss: 0.419, 3232/10016 datapoints
2025-03-06 18:08:35,927 - INFO - validation batch 151, loss: 0.388, 4832/10016 datapoints
2025-03-06 18:08:36,079 - INFO - validation batch 201, loss: 0.361, 6432/10016 datapoints
2025-03-06 18:08:36,235 - INFO - validation batch 251, loss: 0.346, 8032/10016 datapoints
2025-03-06 18:08:36,385 - INFO - validation batch 301, loss: 0.326, 9632/10016 datapoints
2025-03-06 18:08:36,423 - INFO - Epoch 177/800 done.
2025-03-06 18:08:36,423 - INFO - Final validation performance:
Loss: 0.399, top-1 acc: 0.899top-5 acc: 0.899
2025-03-06 18:08:36,424 - INFO - Beginning epoch 178/800
2025-03-06 18:08:36,429 - INFO - training batch 1, loss: 0.467, 32/60000 datapoints
2025-03-06 18:08:36,629 - INFO - training batch 51, loss: 0.528, 1632/60000 datapoints
2025-03-06 18:08:36,824 - INFO - training batch 101, loss: 0.387, 3232/60000 datapoints
2025-03-06 18:08:37,023 - INFO - training batch 151, loss: 0.315, 4832/60000 datapoints
2025-03-06 18:08:37,232 - INFO - training batch 201, loss: 0.541, 6432/60000 datapoints
2025-03-06 18:08:37,427 - INFO - training batch 251, loss: 0.340, 8032/60000 datapoints
2025-03-06 18:08:37,640 - INFO - training batch 301, loss: 0.279, 9632/60000 datapoints
2025-03-06 18:08:37,834 - INFO - training batch 351, loss: 0.310, 11232/60000 datapoints
2025-03-06 18:08:38,026 - INFO - training batch 401, loss: 0.330, 12832/60000 datapoints
2025-03-06 18:08:38,220 - INFO - training batch 451, loss: 0.188, 14432/60000 datapoints
2025-03-06 18:08:38,417 - INFO - training batch 501, loss: 0.327, 16032/60000 datapoints
2025-03-06 18:08:38,623 - INFO - training batch 551, loss: 0.528, 17632/60000 datapoints
2025-03-06 18:08:38,816 - INFO - training batch 601, loss: 0.472, 19232/60000 datapoints
2025-03-06 18:08:39,014 - INFO - training batch 651, loss: 0.411, 20832/60000 datapoints
2025-03-06 18:08:39,209 - INFO - training batch 701, loss: 0.407, 22432/60000 datapoints
2025-03-06 18:08:39,404 - INFO - training batch 751, loss: 0.338, 24032/60000 datapoints
2025-03-06 18:08:39,602 - INFO - training batch 801, loss: 0.241, 25632/60000 datapoints
2025-03-06 18:08:39,795 - INFO - training batch 851, loss: 0.197, 27232/60000 datapoints
2025-03-06 18:08:39,991 - INFO - training batch 901, loss: 0.455, 28832/60000 datapoints
2025-03-06 18:08:40,186 - INFO - training batch 951, loss: 0.573, 30432/60000 datapoints
2025-03-06 18:08:40,380 - INFO - training batch 1001, loss: 0.327, 32032/60000 datapoints
2025-03-06 18:08:40,574 - INFO - training batch 1051, loss: 0.228, 33632/60000 datapoints
2025-03-06 18:08:40,771 - INFO - training batch 1101, loss: 0.289, 35232/60000 datapoints
2025-03-06 18:08:40,965 - INFO - training batch 1151, loss: 0.223, 36832/60000 datapoints
2025-03-06 18:08:41,161 - INFO - training batch 1201, loss: 0.575, 38432/60000 datapoints
2025-03-06 18:08:41,375 - INFO - training batch 1251, loss: 0.592, 40032/60000 datapoints
2025-03-06 18:08:41,569 - INFO - training batch 1301, loss: 0.449, 41632/60000 datapoints
2025-03-06 18:08:41,765 - INFO - training batch 1351, loss: 0.242, 43232/60000 datapoints
2025-03-06 18:08:41,959 - INFO - training batch 1401, loss: 0.206, 44832/60000 datapoints
2025-03-06 18:08:42,153 - INFO - training batch 1451, loss: 0.461, 46432/60000 datapoints
2025-03-06 18:08:42,351 - INFO - training batch 1501, loss: 0.225, 48032/60000 datapoints
2025-03-06 18:08:42,544 - INFO - training batch 1551, loss: 0.272, 49632/60000 datapoints
2025-03-06 18:08:42,741 - INFO - training batch 1601, loss: 0.461, 51232/60000 datapoints
2025-03-06 18:08:42,934 - INFO - training batch 1651, loss: 0.487, 52832/60000 datapoints
2025-03-06 18:08:43,129 - INFO - training batch 1701, loss: 0.546, 54432/60000 datapoints
2025-03-06 18:08:43,329 - INFO - training batch 1751, loss: 0.445, 56032/60000 datapoints
2025-03-06 18:08:43,525 - INFO - training batch 1801, loss: 0.425, 57632/60000 datapoints
2025-03-06 18:08:43,728 - INFO - training batch 1851, loss: 0.393, 59232/60000 datapoints
2025-03-06 18:08:43,831 - INFO - validation batch 1, loss: 0.617, 32/10016 datapoints
2025-03-06 18:08:43,985 - INFO - validation batch 51, loss: 0.368, 1632/10016 datapoints
2025-03-06 18:08:44,136 - INFO - validation batch 101, loss: 0.294, 3232/10016 datapoints
2025-03-06 18:08:44,288 - INFO - validation batch 151, loss: 0.468, 4832/10016 datapoints
2025-03-06 18:08:44,440 - INFO - validation batch 201, loss: 0.657, 6432/10016 datapoints
2025-03-06 18:08:44,593 - INFO - validation batch 251, loss: 0.163, 8032/10016 datapoints
2025-03-06 18:08:44,749 - INFO - validation batch 301, loss: 0.347, 9632/10016 datapoints
2025-03-06 18:08:44,787 - INFO - Epoch 178/800 done.
2025-03-06 18:08:44,787 - INFO - Final validation performance:
Loss: 0.416, top-1 acc: 0.899top-5 acc: 0.899
2025-03-06 18:08:44,788 - INFO - Beginning epoch 179/800
2025-03-06 18:08:44,793 - INFO - training batch 1, loss: 0.505, 32/60000 datapoints
2025-03-06 18:08:44,996 - INFO - training batch 51, loss: 0.728, 1632/60000 datapoints
2025-03-06 18:08:45,192 - INFO - training batch 101, loss: 0.292, 3232/60000 datapoints
2025-03-06 18:08:45,384 - INFO - training batch 151, loss: 0.230, 4832/60000 datapoints
2025-03-06 18:08:45,577 - INFO - training batch 201, loss: 0.260, 6432/60000 datapoints
2025-03-06 18:08:45,775 - INFO - training batch 251, loss: 0.318, 8032/60000 datapoints
2025-03-06 18:08:45,967 - INFO - training batch 301, loss: 0.230, 9632/60000 datapoints
2025-03-06 18:08:46,165 - INFO - training batch 351, loss: 0.483, 11232/60000 datapoints
2025-03-06 18:08:46,360 - INFO - training batch 401, loss: 0.393, 12832/60000 datapoints
2025-03-06 18:08:46,555 - INFO - training batch 451, loss: 0.592, 14432/60000 datapoints
2025-03-06 18:08:46,756 - INFO - training batch 501, loss: 0.457, 16032/60000 datapoints
2025-03-06 18:08:46,951 - INFO - training batch 551, loss: 0.249, 17632/60000 datapoints
2025-03-06 18:08:47,148 - INFO - training batch 601, loss: 0.606, 19232/60000 datapoints
2025-03-06 18:08:47,343 - INFO - training batch 651, loss: 0.311, 20832/60000 datapoints
2025-03-06 18:08:47,539 - INFO - training batch 701, loss: 0.400, 22432/60000 datapoints
2025-03-06 18:08:47,737 - INFO - training batch 751, loss: 0.397, 24032/60000 datapoints
2025-03-06 18:08:47,931 - INFO - training batch 801, loss: 0.353, 25632/60000 datapoints
2025-03-06 18:08:48,126 - INFO - training batch 851, loss: 0.275, 27232/60000 datapoints
2025-03-06 18:08:48,321 - INFO - training batch 901, loss: 0.233, 28832/60000 datapoints
2025-03-06 18:08:48,519 - INFO - training batch 951, loss: 0.333, 30432/60000 datapoints
2025-03-06 18:08:48,717 - INFO - training batch 1001, loss: 0.635, 32032/60000 datapoints
2025-03-06 18:08:48,909 - INFO - training batch 1051, loss: 0.547, 33632/60000 datapoints
2025-03-06 18:08:49,104 - INFO - training batch 1101, loss: 0.608, 35232/60000 datapoints
2025-03-06 18:08:49,300 - INFO - training batch 1151, loss: 0.343, 36832/60000 datapoints
2025-03-06 18:08:49,494 - INFO - training batch 1201, loss: 0.318, 38432/60000 datapoints
2025-03-06 18:08:49,688 - INFO - training batch 1251, loss: 0.562, 40032/60000 datapoints
2025-03-06 18:08:49,883 - INFO - training batch 1301, loss: 0.253, 41632/60000 datapoints
2025-03-06 18:08:50,081 - INFO - training batch 1351, loss: 0.565, 43232/60000 datapoints
2025-03-06 18:08:50,274 - INFO - training batch 1401, loss: 0.426, 44832/60000 datapoints
2025-03-06 18:08:50,466 - INFO - training batch 1451, loss: 0.455, 46432/60000 datapoints
2025-03-06 18:08:50,667 - INFO - training batch 1501, loss: 0.899, 48032/60000 datapoints
2025-03-06 18:08:50,859 - INFO - training batch 1551, loss: 0.286, 49632/60000 datapoints
2025-03-06 18:08:51,053 - INFO - training batch 1601, loss: 0.532, 51232/60000 datapoints
2025-03-06 18:08:51,245 - INFO - training batch 1651, loss: 0.170, 52832/60000 datapoints
2025-03-06 18:08:51,455 - INFO - training batch 1701, loss: 0.267, 54432/60000 datapoints
2025-03-06 18:08:51,655 - INFO - training batch 1751, loss: 0.241, 56032/60000 datapoints
2025-03-06 18:08:51,850 - INFO - training batch 1801, loss: 0.308, 57632/60000 datapoints
2025-03-06 18:08:52,092 - INFO - training batch 1851, loss: 0.643, 59232/60000 datapoints
2025-03-06 18:08:52,193 - INFO - validation batch 1, loss: 0.274, 32/10016 datapoints
2025-03-06 18:08:52,344 - INFO - validation batch 51, loss: 0.258, 1632/10016 datapoints
2025-03-06 18:08:52,499 - INFO - validation batch 101, loss: 0.211, 3232/10016 datapoints
2025-03-06 18:08:52,658 - INFO - validation batch 151, loss: 0.247, 4832/10016 datapoints
2025-03-06 18:08:52,810 - INFO - validation batch 201, loss: 0.736, 6432/10016 datapoints
2025-03-06 18:08:52,962 - INFO - validation batch 251, loss: 0.544, 8032/10016 datapoints
2025-03-06 18:08:53,116 - INFO - validation batch 301, loss: 0.587, 9632/10016 datapoints
2025-03-06 18:08:53,152 - INFO - Epoch 179/800 done.
2025-03-06 18:08:53,153 - INFO - Final validation performance:
Loss: 0.408, top-1 acc: 0.899top-5 acc: 0.899
2025-03-06 18:08:53,153 - INFO - Beginning epoch 180/800
2025-03-06 18:08:53,160 - INFO - training batch 1, loss: 0.618, 32/60000 datapoints
2025-03-06 18:08:53,356 - INFO - training batch 51, loss: 0.366, 1632/60000 datapoints
2025-03-06 18:08:53,554 - INFO - training batch 101, loss: 0.508, 3232/60000 datapoints
2025-03-06 18:08:53,748 - INFO - training batch 151, loss: 0.181, 4832/60000 datapoints
2025-03-06 18:08:53,942 - INFO - training batch 201, loss: 0.467, 6432/60000 datapoints
2025-03-06 18:08:54,137 - INFO - training batch 251, loss: 0.448, 8032/60000 datapoints
2025-03-06 18:08:54,330 - INFO - training batch 301, loss: 0.636, 9632/60000 datapoints
2025-03-06 18:08:54,522 - INFO - training batch 351, loss: 0.216, 11232/60000 datapoints
2025-03-06 18:08:54,721 - INFO - training batch 401, loss: 0.355, 12832/60000 datapoints
2025-03-06 18:08:54,919 - INFO - training batch 451, loss: 0.381, 14432/60000 datapoints
2025-03-06 18:08:55,116 - INFO - training batch 501, loss: 0.264, 16032/60000 datapoints
2025-03-06 18:08:55,311 - INFO - training batch 551, loss: 0.482, 17632/60000 datapoints
2025-03-06 18:08:55,505 - INFO - training batch 601, loss: 0.427, 19232/60000 datapoints
2025-03-06 18:08:55,701 - INFO - training batch 651, loss: 0.311, 20832/60000 datapoints
2025-03-06 18:08:55,896 - INFO - training batch 701, loss: 0.346, 22432/60000 datapoints
2025-03-06 18:08:56,090 - INFO - training batch 751, loss: 0.472, 24032/60000 datapoints
2025-03-06 18:08:56,288 - INFO - training batch 801, loss: 0.430, 25632/60000 datapoints
2025-03-06 18:08:56,481 - INFO - training batch 851, loss: 0.243, 27232/60000 datapoints
2025-03-06 18:08:56,680 - INFO - training batch 901, loss: 0.259, 28832/60000 datapoints
2025-03-06 18:08:56,876 - INFO - training batch 951, loss: 0.128, 30432/60000 datapoints
2025-03-06 18:08:57,072 - INFO - training batch 1001, loss: 0.230, 32032/60000 datapoints
2025-03-06 18:08:57,266 - INFO - training batch 1051, loss: 0.472, 33632/60000 datapoints
2025-03-06 18:08:57,460 - INFO - training batch 1101, loss: 0.515, 35232/60000 datapoints
2025-03-06 18:08:57,659 - INFO - training batch 1151, loss: 0.569, 36832/60000 datapoints
2025-03-06 18:08:57,853 - INFO - training batch 1201, loss: 0.403, 38432/60000 datapoints
2025-03-06 18:08:58,049 - INFO - training batch 1251, loss: 0.366, 40032/60000 datapoints
2025-03-06 18:08:58,243 - INFO - training batch 1301, loss: 0.351, 41632/60000 datapoints
2025-03-06 18:08:58,443 - INFO - training batch 1351, loss: 0.211, 43232/60000 datapoints
2025-03-06 18:08:58,642 - INFO - training batch 1401, loss: 0.217, 44832/60000 datapoints
2025-03-06 18:08:58,838 - INFO - training batch 1451, loss: 0.459, 46432/60000 datapoints
2025-03-06 18:08:59,030 - INFO - training batch 1501, loss: 0.351, 48032/60000 datapoints
2025-03-06 18:08:59,224 - INFO - training batch 1551, loss: 0.369, 49632/60000 datapoints
2025-03-06 18:08:59,415 - INFO - training batch 1601, loss: 0.410, 51232/60000 datapoints
2025-03-06 18:08:59,611 - INFO - training batch 1651, loss: 0.438, 52832/60000 datapoints
2025-03-06 18:08:59,805 - INFO - training batch 1701, loss: 0.367, 54432/60000 datapoints
2025-03-06 18:09:00,002 - INFO - training batch 1751, loss: 0.347, 56032/60000 datapoints
2025-03-06 18:09:00,200 - INFO - training batch 1801, loss: 0.439, 57632/60000 datapoints
2025-03-06 18:09:00,394 - INFO - training batch 1851, loss: 0.201, 59232/60000 datapoints
2025-03-06 18:09:00,495 - INFO - validation batch 1, loss: 0.410, 32/10016 datapoints
2025-03-06 18:09:00,649 - INFO - validation batch 51, loss: 0.252, 1632/10016 datapoints
2025-03-06 18:09:00,805 - INFO - validation batch 101, loss: 0.558, 3232/10016 datapoints
2025-03-06 18:09:00,959 - INFO - validation batch 151, loss: 0.526, 4832/10016 datapoints
2025-03-06 18:09:01,110 - INFO - validation batch 201, loss: 0.237, 6432/10016 datapoints
2025-03-06 18:09:01,262 - INFO - validation batch 251, loss: 0.409, 8032/10016 datapoints
2025-03-06 18:09:01,427 - INFO - validation batch 301, loss: 0.279, 9632/10016 datapoints
2025-03-06 18:09:01,470 - INFO - Epoch 180/800 done.
2025-03-06 18:09:01,471 - INFO - Final validation performance:
Loss: 0.382, top-1 acc: 0.900top-5 acc: 0.900
2025-03-06 18:09:01,471 - INFO - Beginning epoch 181/800
2025-03-06 18:09:01,477 - INFO - training batch 1, loss: 0.546, 32/60000 datapoints
2025-03-06 18:09:01,675 - INFO - training batch 51, loss: 0.226, 1632/60000 datapoints
2025-03-06 18:09:01,870 - INFO - training batch 101, loss: 0.224, 3232/60000 datapoints
2025-03-06 18:09:02,064 - INFO - training batch 151, loss: 0.316, 4832/60000 datapoints
2025-03-06 18:09:02,258 - INFO - training batch 201, loss: 0.280, 6432/60000 datapoints
2025-03-06 18:09:02,451 - INFO - training batch 251, loss: 0.439, 8032/60000 datapoints
2025-03-06 18:09:02,646 - INFO - training batch 301, loss: 0.334, 9632/60000 datapoints
2025-03-06 18:09:02,842 - INFO - training batch 351, loss: 0.146, 11232/60000 datapoints
2025-03-06 18:09:03,034 - INFO - training batch 401, loss: 0.372, 12832/60000 datapoints
2025-03-06 18:09:03,229 - INFO - training batch 451, loss: 0.257, 14432/60000 datapoints
2025-03-06 18:09:03,425 - INFO - training batch 501, loss: 0.290, 16032/60000 datapoints
2025-03-06 18:09:03,621 - INFO - training batch 551, loss: 0.451, 17632/60000 datapoints
2025-03-06 18:09:03,822 - INFO - training batch 601, loss: 0.287, 19232/60000 datapoints
2025-03-06 18:09:04,014 - INFO - training batch 651, loss: 0.492, 20832/60000 datapoints
2025-03-06 18:09:04,208 - INFO - training batch 701, loss: 0.400, 22432/60000 datapoints
2025-03-06 18:09:04,400 - INFO - training batch 751, loss: 0.382, 24032/60000 datapoints
2025-03-06 18:09:04,596 - INFO - training batch 801, loss: 0.279, 25632/60000 datapoints
2025-03-06 18:09:04,791 - INFO - training batch 851, loss: 0.679, 27232/60000 datapoints
2025-03-06 18:09:04,986 - INFO - training batch 901, loss: 0.190, 28832/60000 datapoints
2025-03-06 18:09:05,181 - INFO - training batch 951, loss: 0.371, 30432/60000 datapoints
2025-03-06 18:09:05,377 - INFO - training batch 1001, loss: 0.322, 32032/60000 datapoints
2025-03-06 18:09:05,572 - INFO - training batch 1051, loss: 0.254, 33632/60000 datapoints
2025-03-06 18:09:05,769 - INFO - training batch 1101, loss: 0.330, 35232/60000 datapoints
2025-03-06 18:09:05,964 - INFO - training batch 1151, loss: 0.238, 36832/60000 datapoints
2025-03-06 18:09:06,165 - INFO - training batch 1201, loss: 0.571, 38432/60000 datapoints
2025-03-06 18:09:06,361 - INFO - training batch 1251, loss: 0.383, 40032/60000 datapoints
2025-03-06 18:09:06,555 - INFO - training batch 1301, loss: 0.252, 41632/60000 datapoints
2025-03-06 18:09:06,757 - INFO - training batch 1351, loss: 0.336, 43232/60000 datapoints
2025-03-06 18:09:06,955 - INFO - training batch 1401, loss: 0.500, 44832/60000 datapoints
2025-03-06 18:09:07,151 - INFO - training batch 1451, loss: 0.318, 46432/60000 datapoints
2025-03-06 18:09:07,347 - INFO - training batch 1501, loss: 0.495, 48032/60000 datapoints
2025-03-06 18:09:07,550 - INFO - training batch 1551, loss: 0.294, 49632/60000 datapoints
2025-03-06 18:09:07,751 - INFO - training batch 1601, loss: 0.365, 51232/60000 datapoints
2025-03-06 18:09:07,944 - INFO - training batch 1651, loss: 0.394, 52832/60000 datapoints
2025-03-06 18:09:08,141 - INFO - training batch 1701, loss: 0.216, 54432/60000 datapoints
2025-03-06 18:09:08,335 - INFO - training batch 1751, loss: 0.465, 56032/60000 datapoints
2025-03-06 18:09:08,534 - INFO - training batch 1801, loss: 0.282, 57632/60000 datapoints
2025-03-06 18:09:08,733 - INFO - training batch 1851, loss: 0.491, 59232/60000 datapoints
2025-03-06 18:09:08,835 - INFO - validation batch 1, loss: 0.699, 32/10016 datapoints
2025-03-06 18:09:08,989 - INFO - validation batch 51, loss: 0.237, 1632/10016 datapoints
2025-03-06 18:09:09,147 - INFO - validation batch 101, loss: 0.377, 3232/10016 datapoints
2025-03-06 18:09:09,301 - INFO - validation batch 151, loss: 0.105, 4832/10016 datapoints
2025-03-06 18:09:09,454 - INFO - validation batch 201, loss: 0.484, 6432/10016 datapoints
2025-03-06 18:09:09,611 - INFO - validation batch 251, loss: 0.195, 8032/10016 datapoints
2025-03-06 18:09:09,764 - INFO - validation batch 301, loss: 0.368, 9632/10016 datapoints
2025-03-06 18:09:09,800 - INFO - Epoch 181/800 done.
2025-03-06 18:09:09,800 - INFO - Final validation performance:
Loss: 0.352, top-1 acc: 0.900top-5 acc: 0.900
2025-03-06 18:09:09,800 - INFO - Beginning epoch 182/800
2025-03-06 18:09:09,806 - INFO - training batch 1, loss: 0.273, 32/60000 datapoints
2025-03-06 18:09:10,005 - INFO - training batch 51, loss: 0.389, 1632/60000 datapoints
2025-03-06 18:09:10,199 - INFO - training batch 101, loss: 0.392, 3232/60000 datapoints
2025-03-06 18:09:10,394 - INFO - training batch 151, loss: 0.518, 4832/60000 datapoints
2025-03-06 18:09:10,587 - INFO - training batch 201, loss: 0.417, 6432/60000 datapoints
2025-03-06 18:09:10,785 - INFO - training batch 251, loss: 0.314, 8032/60000 datapoints
2025-03-06 18:09:10,980 - INFO - training batch 301, loss: 0.235, 9632/60000 datapoints
2025-03-06 18:09:11,172 - INFO - training batch 351, loss: 0.444, 11232/60000 datapoints
2025-03-06 18:09:11,367 - INFO - training batch 401, loss: 0.309, 12832/60000 datapoints
2025-03-06 18:09:11,580 - INFO - training batch 451, loss: 0.341, 14432/60000 datapoints
2025-03-06 18:09:11,776 - INFO - training batch 501, loss: 0.354, 16032/60000 datapoints
2025-03-06 18:09:11,969 - INFO - training batch 551, loss: 0.414, 17632/60000 datapoints
2025-03-06 18:09:12,164 - INFO - training batch 601, loss: 0.436, 19232/60000 datapoints
2025-03-06 18:09:12,356 - INFO - training batch 651, loss: 0.473, 20832/60000 datapoints
2025-03-06 18:09:12,548 - INFO - training batch 701, loss: 0.281, 22432/60000 datapoints
2025-03-06 18:09:12,744 - INFO - training batch 751, loss: 0.260, 24032/60000 datapoints
2025-03-06 18:09:12,940 - INFO - training batch 801, loss: 0.488, 25632/60000 datapoints
2025-03-06 18:09:13,132 - INFO - training batch 851, loss: 0.405, 27232/60000 datapoints
2025-03-06 18:09:13,328 - INFO - training batch 901, loss: 0.467, 28832/60000 datapoints
2025-03-06 18:09:13,521 - INFO - training batch 951, loss: 0.439, 30432/60000 datapoints
2025-03-06 18:09:13,718 - INFO - training batch 1001, loss: 0.365, 32032/60000 datapoints
2025-03-06 18:09:13,915 - INFO - training batch 1051, loss: 0.436, 33632/60000 datapoints
2025-03-06 18:09:14,115 - INFO - training batch 1101, loss: 0.249, 35232/60000 datapoints
2025-03-06 18:09:14,309 - INFO - training batch 1151, loss: 0.277, 36832/60000 datapoints
2025-03-06 18:09:14,502 - INFO - training batch 1201, loss: 0.315, 38432/60000 datapoints
2025-03-06 18:09:14,699 - INFO - training batch 1251, loss: 0.430, 40032/60000 datapoints
2025-03-06 18:09:14,900 - INFO - training batch 1301, loss: 0.316, 41632/60000 datapoints
2025-03-06 18:09:15,095 - INFO - training batch 1351, loss: 0.272, 43232/60000 datapoints
2025-03-06 18:09:15,290 - INFO - training batch 1401, loss: 0.380, 44832/60000 datapoints
2025-03-06 18:09:15,487 - INFO - training batch 1451, loss: 0.229, 46432/60000 datapoints
2025-03-06 18:09:15,684 - INFO - training batch 1501, loss: 0.466, 48032/60000 datapoints
2025-03-06 18:09:15,876 - INFO - training batch 1551, loss: 0.544, 49632/60000 datapoints
2025-03-06 18:09:16,070 - INFO - training batch 1601, loss: 0.452, 51232/60000 datapoints
2025-03-06 18:09:16,265 - INFO - training batch 1651, loss: 0.325, 52832/60000 datapoints
2025-03-06 18:09:16,461 - INFO - training batch 1701, loss: 0.320, 54432/60000 datapoints
2025-03-06 18:09:16,659 - INFO - training batch 1751, loss: 0.364, 56032/60000 datapoints
2025-03-06 18:09:16,856 - INFO - training batch 1801, loss: 0.369, 57632/60000 datapoints
2025-03-06 18:09:17,049 - INFO - training batch 1851, loss: 0.485, 59232/60000 datapoints
2025-03-06 18:09:17,151 - INFO - validation batch 1, loss: 0.322, 32/10016 datapoints
2025-03-06 18:09:17,303 - INFO - validation batch 51, loss: 0.439, 1632/10016 datapoints
2025-03-06 18:09:17,471 - INFO - validation batch 101, loss: 0.272, 3232/10016 datapoints
2025-03-06 18:09:17,629 - INFO - validation batch 151, loss: 0.168, 4832/10016 datapoints
2025-03-06 18:09:17,781 - INFO - validation batch 201, loss: 0.458, 6432/10016 datapoints
2025-03-06 18:09:17,936 - INFO - validation batch 251, loss: 0.404, 8032/10016 datapoints
2025-03-06 18:09:18,088 - INFO - validation batch 301, loss: 0.131, 9632/10016 datapoints
2025-03-06 18:09:18,126 - INFO - Epoch 182/800 done.
2025-03-06 18:09:18,126 - INFO - Final validation performance:
Loss: 0.313, top-1 acc: 0.900top-5 acc: 0.900
2025-03-06 18:09:18,126 - INFO - Beginning epoch 183/800
2025-03-06 18:09:18,132 - INFO - training batch 1, loss: 0.486, 32/60000 datapoints
2025-03-06 18:09:18,329 - INFO - training batch 51, loss: 0.500, 1632/60000 datapoints
2025-03-06 18:09:18,533 - INFO - training batch 101, loss: 0.270, 3232/60000 datapoints
2025-03-06 18:09:18,729 - INFO - training batch 151, loss: 0.401, 4832/60000 datapoints
2025-03-06 18:09:18,925 - INFO - training batch 201, loss: 0.331, 6432/60000 datapoints
2025-03-06 18:09:19,119 - INFO - training batch 251, loss: 0.153, 8032/60000 datapoints
2025-03-06 18:09:19,312 - INFO - training batch 301, loss: 0.651, 9632/60000 datapoints
2025-03-06 18:09:19,507 - INFO - training batch 351, loss: 0.444, 11232/60000 datapoints
2025-03-06 18:09:19,706 - INFO - training batch 401, loss: 0.438, 12832/60000 datapoints
2025-03-06 18:09:19,900 - INFO - training batch 451, loss: 0.493, 14432/60000 datapoints
2025-03-06 18:09:20,096 - INFO - training batch 501, loss: 0.287, 16032/60000 datapoints
2025-03-06 18:09:20,290 - INFO - training batch 551, loss: 0.332, 17632/60000 datapoints
2025-03-06 18:09:20,486 - INFO - training batch 601, loss: 0.343, 19232/60000 datapoints
2025-03-06 18:09:20,684 - INFO - training batch 651, loss: 0.461, 20832/60000 datapoints
2025-03-06 18:09:20,880 - INFO - training batch 701, loss: 0.609, 22432/60000 datapoints
2025-03-06 18:09:21,074 - INFO - training batch 751, loss: 0.546, 24032/60000 datapoints
2025-03-06 18:09:21,268 - INFO - training batch 801, loss: 0.254, 25632/60000 datapoints
2025-03-06 18:09:21,461 - INFO - training batch 851, loss: 0.425, 27232/60000 datapoints
2025-03-06 18:09:21,677 - INFO - training batch 901, loss: 0.504, 28832/60000 datapoints
2025-03-06 18:09:21,873 - INFO - training batch 951, loss: 0.300, 30432/60000 datapoints
2025-03-06 18:09:22,067 - INFO - training batch 1001, loss: 0.266, 32032/60000 datapoints
2025-03-06 18:09:22,263 - INFO - training batch 1051, loss: 0.657, 33632/60000 datapoints
2025-03-06 18:09:22,456 - INFO - training batch 1101, loss: 0.280, 35232/60000 datapoints
2025-03-06 18:09:22,652 - INFO - training batch 1151, loss: 0.173, 36832/60000 datapoints
2025-03-06 18:09:22,851 - INFO - training batch 1201, loss: 0.633, 38432/60000 datapoints
2025-03-06 18:09:23,045 - INFO - training batch 1251, loss: 0.238, 40032/60000 datapoints
2025-03-06 18:09:23,239 - INFO - training batch 1301, loss: 0.387, 41632/60000 datapoints
2025-03-06 18:09:23,433 - INFO - training batch 1351, loss: 0.408, 43232/60000 datapoints
2025-03-06 18:09:23,628 - INFO - training batch 1401, loss: 0.454, 44832/60000 datapoints
2025-03-06 18:09:23,822 - INFO - training batch 1451, loss: 0.231, 46432/60000 datapoints
2025-03-06 18:09:24,016 - INFO - training batch 1501, loss: 0.536, 48032/60000 datapoints
2025-03-06 18:09:24,209 - INFO - training batch 1551, loss: 0.170, 49632/60000 datapoints
2025-03-06 18:09:24,402 - INFO - training batch 1601, loss: 0.266, 51232/60000 datapoints
2025-03-06 18:09:24,599 - INFO - training batch 1651, loss: 0.400, 52832/60000 datapoints
2025-03-06 18:09:24,802 - INFO - training batch 1701, loss: 0.600, 54432/60000 datapoints
2025-03-06 18:09:25,000 - INFO - training batch 1751, loss: 0.382, 56032/60000 datapoints
2025-03-06 18:09:25,194 - INFO - training batch 1801, loss: 0.376, 57632/60000 datapoints
2025-03-06 18:09:25,386 - INFO - training batch 1851, loss: 0.420, 59232/60000 datapoints
2025-03-06 18:09:25,487 - INFO - validation batch 1, loss: 0.284, 32/10016 datapoints
2025-03-06 18:09:25,643 - INFO - validation batch 51, loss: 0.531, 1632/10016 datapoints
2025-03-06 18:09:25,824 - INFO - validation batch 101, loss: 0.331, 3232/10016 datapoints
2025-03-06 18:09:25,982 - INFO - validation batch 151, loss: 0.554, 4832/10016 datapoints
2025-03-06 18:09:26,134 - INFO - validation batch 201, loss: 0.291, 6432/10016 datapoints
2025-03-06 18:09:26,289 - INFO - validation batch 251, loss: 0.357, 8032/10016 datapoints
2025-03-06 18:09:26,441 - INFO - validation batch 301, loss: 0.484, 9632/10016 datapoints
2025-03-06 18:09:26,478 - INFO - Epoch 183/800 done.
2025-03-06 18:09:26,479 - INFO - Final validation performance:
Loss: 0.405, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 18:09:26,479 - INFO - Beginning epoch 184/800
2025-03-06 18:09:26,485 - INFO - training batch 1, loss: 0.549, 32/60000 datapoints
2025-03-06 18:09:26,678 - INFO - training batch 51, loss: 0.499, 1632/60000 datapoints
2025-03-06 18:09:26,874 - INFO - training batch 101, loss: 0.146, 3232/60000 datapoints
2025-03-06 18:09:27,071 - INFO - training batch 151, loss: 0.290, 4832/60000 datapoints
2025-03-06 18:09:27,267 - INFO - training batch 201, loss: 0.287, 6432/60000 datapoints
2025-03-06 18:09:27,464 - INFO - training batch 251, loss: 0.231, 8032/60000 datapoints
2025-03-06 18:09:27,660 - INFO - training batch 301, loss: 0.255, 9632/60000 datapoints
2025-03-06 18:09:27,855 - INFO - training batch 351, loss: 0.427, 11232/60000 datapoints
2025-03-06 18:09:28,050 - INFO - training batch 401, loss: 0.536, 12832/60000 datapoints
2025-03-06 18:09:28,243 - INFO - training batch 451, loss: 0.320, 14432/60000 datapoints
2025-03-06 18:09:28,438 - INFO - training batch 501, loss: 0.611, 16032/60000 datapoints
2025-03-06 18:09:28,636 - INFO - training batch 551, loss: 0.194, 17632/60000 datapoints
2025-03-06 18:09:28,833 - INFO - training batch 601, loss: 0.277, 19232/60000 datapoints
2025-03-06 18:09:29,026 - INFO - training batch 651, loss: 0.264, 20832/60000 datapoints
2025-03-06 18:09:29,219 - INFO - training batch 701, loss: 0.513, 22432/60000 datapoints
2025-03-06 18:09:29,414 - INFO - training batch 751, loss: 0.204, 24032/60000 datapoints
2025-03-06 18:09:29,611 - INFO - training batch 801, loss: 0.267, 25632/60000 datapoints
2025-03-06 18:09:29,806 - INFO - training batch 851, loss: 0.303, 27232/60000 datapoints
2025-03-06 18:09:30,002 - INFO - training batch 901, loss: 0.243, 28832/60000 datapoints
2025-03-06 18:09:30,196 - INFO - training batch 951, loss: 0.856, 30432/60000 datapoints
2025-03-06 18:09:30,389 - INFO - training batch 1001, loss: 0.524, 32032/60000 datapoints
2025-03-06 18:09:30,583 - INFO - training batch 1051, loss: 0.569, 33632/60000 datapoints
2025-03-06 18:09:30,783 - INFO - training batch 1101, loss: 0.375, 35232/60000 datapoints
2025-03-06 18:09:30,983 - INFO - training batch 1151, loss: 0.451, 36832/60000 datapoints
2025-03-06 18:09:31,178 - INFO - training batch 1201, loss: 0.303, 38432/60000 datapoints
2025-03-06 18:09:31,369 - INFO - training batch 1251, loss: 0.217, 40032/60000 datapoints
2025-03-06 18:09:31,564 - INFO - training batch 1301, loss: 0.263, 41632/60000 datapoints
2025-03-06 18:09:31,778 - INFO - training batch 1351, loss: 0.266, 43232/60000 datapoints
2025-03-06 18:09:31,972 - INFO - training batch 1401, loss: 0.385, 44832/60000 datapoints
2025-03-06 18:09:32,166 - INFO - training batch 1451, loss: 0.446, 46432/60000 datapoints
2025-03-06 18:09:32,361 - INFO - training batch 1501, loss: 0.172, 48032/60000 datapoints
2025-03-06 18:09:32,555 - INFO - training batch 1551, loss: 0.555, 49632/60000 datapoints
2025-03-06 18:09:32,752 - INFO - training batch 1601, loss: 0.470, 51232/60000 datapoints
2025-03-06 18:09:32,948 - INFO - training batch 1651, loss: 0.511, 52832/60000 datapoints
2025-03-06 18:09:33,142 - INFO - training batch 1701, loss: 0.205, 54432/60000 datapoints
2025-03-06 18:09:33,333 - INFO - training batch 1751, loss: 0.313, 56032/60000 datapoints
2025-03-06 18:09:33,529 - INFO - training batch 1801, loss: 0.467, 57632/60000 datapoints
2025-03-06 18:09:33,725 - INFO - training batch 1851, loss: 0.587, 59232/60000 datapoints
2025-03-06 18:09:33,830 - INFO - validation batch 1, loss: 0.222, 32/10016 datapoints
2025-03-06 18:09:33,984 - INFO - validation batch 51, loss: 0.440, 1632/10016 datapoints
2025-03-06 18:09:34,136 - INFO - validation batch 101, loss: 0.341, 3232/10016 datapoints
2025-03-06 18:09:34,290 - INFO - validation batch 151, loss: 0.299, 4832/10016 datapoints
2025-03-06 18:09:34,442 - INFO - validation batch 201, loss: 0.354, 6432/10016 datapoints
2025-03-06 18:09:34,601 - INFO - validation batch 251, loss: 0.410, 8032/10016 datapoints
2025-03-06 18:09:34,753 - INFO - validation batch 301, loss: 0.587, 9632/10016 datapoints
2025-03-06 18:09:34,791 - INFO - Epoch 184/800 done.
2025-03-06 18:09:34,791 - INFO - Final validation performance:
Loss: 0.379, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 18:09:34,791 - INFO - Beginning epoch 185/800
2025-03-06 18:09:34,797 - INFO - training batch 1, loss: 0.363, 32/60000 datapoints
2025-03-06 18:09:34,997 - INFO - training batch 51, loss: 0.276, 1632/60000 datapoints
2025-03-06 18:09:35,196 - INFO - training batch 101, loss: 0.367, 3232/60000 datapoints
2025-03-06 18:09:35,390 - INFO - training batch 151, loss: 0.438, 4832/60000 datapoints
2025-03-06 18:09:35,585 - INFO - training batch 201, loss: 0.344, 6432/60000 datapoints
2025-03-06 18:09:35,782 - INFO - training batch 251, loss: 0.385, 8032/60000 datapoints
2025-03-06 18:09:35,976 - INFO - training batch 301, loss: 0.407, 9632/60000 datapoints
2025-03-06 18:09:36,173 - INFO - training batch 351, loss: 0.312, 11232/60000 datapoints
2025-03-06 18:09:36,369 - INFO - training batch 401, loss: 0.257, 12832/60000 datapoints
2025-03-06 18:09:36,565 - INFO - training batch 451, loss: 0.450, 14432/60000 datapoints
2025-03-06 18:09:36,762 - INFO - training batch 501, loss: 0.263, 16032/60000 datapoints
2025-03-06 18:09:36,959 - INFO - training batch 551, loss: 0.467, 17632/60000 datapoints
2025-03-06 18:09:37,153 - INFO - training batch 601, loss: 0.581, 19232/60000 datapoints
2025-03-06 18:09:37,346 - INFO - training batch 651, loss: 0.428, 20832/60000 datapoints
2025-03-06 18:09:37,539 - INFO - training batch 701, loss: 0.176, 22432/60000 datapoints
2025-03-06 18:09:37,774 - INFO - training batch 751, loss: 0.277, 24032/60000 datapoints
2025-03-06 18:09:37,968 - INFO - training batch 801, loss: 0.597, 25632/60000 datapoints
2025-03-06 18:09:38,165 - INFO - training batch 851, loss: 0.282, 27232/60000 datapoints
2025-03-06 18:09:38,359 - INFO - training batch 901, loss: 0.300, 28832/60000 datapoints
2025-03-06 18:09:38,558 - INFO - training batch 951, loss: 0.761, 30432/60000 datapoints
2025-03-06 18:09:38,756 - INFO - training batch 1001, loss: 0.248, 32032/60000 datapoints
2025-03-06 18:09:38,952 - INFO - training batch 1051, loss: 0.340, 33632/60000 datapoints
2025-03-06 18:09:39,150 - INFO - training batch 1101, loss: 0.213, 35232/60000 datapoints
2025-03-06 18:09:39,342 - INFO - training batch 1151, loss: 0.358, 36832/60000 datapoints
2025-03-06 18:09:39,536 - INFO - training batch 1201, loss: 0.330, 38432/60000 datapoints
2025-03-06 18:09:39,733 - INFO - training batch 1251, loss: 0.513, 40032/60000 datapoints
2025-03-06 18:09:39,930 - INFO - training batch 1301, loss: 0.471, 41632/60000 datapoints
2025-03-06 18:09:40,126 - INFO - training batch 1351, loss: 0.317, 43232/60000 datapoints
2025-03-06 18:09:40,320 - INFO - training batch 1401, loss: 0.390, 44832/60000 datapoints
2025-03-06 18:09:40,514 - INFO - training batch 1451, loss: 0.342, 46432/60000 datapoints
2025-03-06 18:09:40,711 - INFO - training batch 1501, loss: 0.370, 48032/60000 datapoints
2025-03-06 18:09:40,909 - INFO - training batch 1551, loss: 0.650, 49632/60000 datapoints
2025-03-06 18:09:41,102 - INFO - training batch 1601, loss: 0.648, 51232/60000 datapoints
2025-03-06 18:09:41,297 - INFO - training batch 1651, loss: 0.329, 52832/60000 datapoints
2025-03-06 18:09:41,492 - INFO - training batch 1701, loss: 0.348, 54432/60000 datapoints
2025-03-06 18:09:41,690 - INFO - training batch 1751, loss: 0.203, 56032/60000 datapoints
2025-03-06 18:09:41,903 - INFO - training batch 1801, loss: 0.169, 57632/60000 datapoints
2025-03-06 18:09:42,097 - INFO - training batch 1851, loss: 0.495, 59232/60000 datapoints
2025-03-06 18:09:42,200 - INFO - validation batch 1, loss: 0.304, 32/10016 datapoints
2025-03-06 18:09:42,352 - INFO - validation batch 51, loss: 0.287, 1632/10016 datapoints
2025-03-06 18:09:42,504 - INFO - validation batch 101, loss: 0.284, 3232/10016 datapoints
2025-03-06 18:09:42,660 - INFO - validation batch 151, loss: 0.281, 4832/10016 datapoints
2025-03-06 18:09:42,814 - INFO - validation batch 201, loss: 0.241, 6432/10016 datapoints
2025-03-06 18:09:42,969 - INFO - validation batch 251, loss: 0.276, 8032/10016 datapoints
2025-03-06 18:09:43,121 - INFO - validation batch 301, loss: 0.396, 9632/10016 datapoints
2025-03-06 18:09:43,158 - INFO - Epoch 185/800 done.
2025-03-06 18:09:43,158 - INFO - Final validation performance:
Loss: 0.296, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 18:09:43,158 - INFO - Beginning epoch 186/800
2025-03-06 18:09:43,165 - INFO - training batch 1, loss: 0.304, 32/60000 datapoints
2025-03-06 18:09:43,358 - INFO - training batch 51, loss: 0.265, 1632/60000 datapoints
2025-03-06 18:09:43,549 - INFO - training batch 101, loss: 0.423, 3232/60000 datapoints
2025-03-06 18:09:43,747 - INFO - training batch 151, loss: 0.385, 4832/60000 datapoints
2025-03-06 18:09:43,939 - INFO - training batch 201, loss: 0.324, 6432/60000 datapoints
2025-03-06 18:09:44,137 - INFO - training batch 251, loss: 0.159, 8032/60000 datapoints
2025-03-06 18:09:44,328 - INFO - training batch 301, loss: 0.406, 9632/60000 datapoints
2025-03-06 18:09:44,518 - INFO - training batch 351, loss: 0.431, 11232/60000 datapoints
2025-03-06 18:09:44,711 - INFO - training batch 401, loss: 0.330, 12832/60000 datapoints
2025-03-06 18:09:44,911 - INFO - training batch 451, loss: 0.275, 14432/60000 datapoints
2025-03-06 18:09:45,105 - INFO - training batch 501, loss: 0.246, 16032/60000 datapoints
2025-03-06 18:09:45,297 - INFO - training batch 551, loss: 0.262, 17632/60000 datapoints
2025-03-06 18:09:45,489 - INFO - training batch 601, loss: 0.242, 19232/60000 datapoints
2025-03-06 18:09:45,683 - INFO - training batch 651, loss: 0.313, 20832/60000 datapoints
2025-03-06 18:09:45,874 - INFO - training batch 701, loss: 0.388, 22432/60000 datapoints
2025-03-06 18:09:46,070 - INFO - training batch 751, loss: 0.167, 24032/60000 datapoints
2025-03-06 18:09:46,265 - INFO - training batch 801, loss: 0.329, 25632/60000 datapoints
2025-03-06 18:09:46,457 - INFO - training batch 851, loss: 0.462, 27232/60000 datapoints
2025-03-06 18:09:46,648 - INFO - training batch 901, loss: 0.324, 28832/60000 datapoints
2025-03-06 18:09:46,839 - INFO - training batch 951, loss: 0.204, 30432/60000 datapoints
2025-03-06 18:09:47,031 - INFO - training batch 1001, loss: 0.532, 32032/60000 datapoints
2025-03-06 18:09:47,222 - INFO - training batch 1051, loss: 0.807, 33632/60000 datapoints
2025-03-06 18:09:47,413 - INFO - training batch 1101, loss: 0.234, 35232/60000 datapoints
2025-03-06 18:09:47,605 - INFO - training batch 1151, loss: 0.374, 36832/60000 datapoints
2025-03-06 18:09:47,796 - INFO - training batch 1201, loss: 0.417, 38432/60000 datapoints
2025-03-06 18:09:47,987 - INFO - training batch 1251, loss: 0.431, 40032/60000 datapoints
2025-03-06 18:09:48,179 - INFO - training batch 1301, loss: 0.165, 41632/60000 datapoints
2025-03-06 18:09:48,371 - INFO - training batch 1351, loss: 0.272, 43232/60000 datapoints
2025-03-06 18:09:48,568 - INFO - training batch 1401, loss: 0.234, 44832/60000 datapoints
2025-03-06 18:09:48,762 - INFO - training batch 1451, loss: 0.395, 46432/60000 datapoints
2025-03-06 18:09:48,955 - INFO - training batch 1501, loss: 0.283, 48032/60000 datapoints
2025-03-06 18:09:49,146 - INFO - training batch 1551, loss: 0.238, 49632/60000 datapoints
2025-03-06 18:09:49,338 - INFO - training batch 1601, loss: 0.316, 51232/60000 datapoints
2025-03-06 18:09:49,528 - INFO - training batch 1651, loss: 0.274, 52832/60000 datapoints
2025-03-06 18:09:49,721 - INFO - training batch 1701, loss: 0.341, 54432/60000 datapoints
2025-03-06 18:09:49,922 - INFO - training batch 1751, loss: 0.227, 56032/60000 datapoints
2025-03-06 18:09:50,116 - INFO - training batch 1801, loss: 0.183, 57632/60000 datapoints
2025-03-06 18:09:50,311 - INFO - training batch 1851, loss: 0.473, 59232/60000 datapoints
2025-03-06 18:09:50,409 - INFO - validation batch 1, loss: 0.370, 32/10016 datapoints
2025-03-06 18:09:50,558 - INFO - validation batch 51, loss: 0.321, 1632/10016 datapoints
2025-03-06 18:09:50,710 - INFO - validation batch 101, loss: 0.276, 3232/10016 datapoints
2025-03-06 18:09:50,860 - INFO - validation batch 151, loss: 0.283, 4832/10016 datapoints
2025-03-06 18:09:51,012 - INFO - validation batch 201, loss: 0.441, 6432/10016 datapoints
2025-03-06 18:09:51,162 - INFO - validation batch 251, loss: 0.455, 8032/10016 datapoints
2025-03-06 18:09:51,313 - INFO - validation batch 301, loss: 0.368, 9632/10016 datapoints
2025-03-06 18:09:51,349 - INFO - Epoch 186/800 done.
2025-03-06 18:09:51,349 - INFO - Final validation performance:
Loss: 0.359, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 18:09:51,349 - INFO - Beginning epoch 187/800
2025-03-06 18:09:51,355 - INFO - training batch 1, loss: 0.305, 32/60000 datapoints
2025-03-06 18:09:51,546 - INFO - training batch 51, loss: 0.185, 1632/60000 datapoints
2025-03-06 18:09:51,740 - INFO - training batch 101, loss: 0.656, 3232/60000 datapoints
2025-03-06 18:09:51,952 - INFO - training batch 151, loss: 0.369, 4832/60000 datapoints
2025-03-06 18:09:52,144 - INFO - training batch 201, loss: 0.454, 6432/60000 datapoints
2025-03-06 18:09:52,338 - INFO - training batch 251, loss: 0.422, 8032/60000 datapoints
2025-03-06 18:09:52,528 - INFO - training batch 301, loss: 0.500, 9632/60000 datapoints
2025-03-06 18:09:52,722 - INFO - training batch 351, loss: 0.605, 11232/60000 datapoints
2025-03-06 18:09:52,917 - INFO - training batch 401, loss: 0.181, 12832/60000 datapoints
2025-03-06 18:09:53,108 - INFO - training batch 451, loss: 0.552, 14432/60000 datapoints
2025-03-06 18:09:53,300 - INFO - training batch 501, loss: 0.368, 16032/60000 datapoints
2025-03-06 18:09:53,490 - INFO - training batch 551, loss: 0.315, 17632/60000 datapoints
2025-03-06 18:09:53,683 - INFO - training batch 601, loss: 0.180, 19232/60000 datapoints
2025-03-06 18:09:53,876 - INFO - training batch 651, loss: 0.379, 20832/60000 datapoints
2025-03-06 18:09:54,068 - INFO - training batch 701, loss: 0.377, 22432/60000 datapoints
2025-03-06 18:09:54,259 - INFO - training batch 751, loss: 0.390, 24032/60000 datapoints
2025-03-06 18:09:54,450 - INFO - training batch 801, loss: 0.231, 25632/60000 datapoints
2025-03-06 18:09:54,644 - INFO - training batch 851, loss: 0.250, 27232/60000 datapoints
2025-03-06 18:09:54,837 - INFO - training batch 901, loss: 0.667, 28832/60000 datapoints
2025-03-06 18:09:55,034 - INFO - training batch 951, loss: 0.586, 30432/60000 datapoints
2025-03-06 18:09:55,226 - INFO - training batch 1001, loss: 0.311, 32032/60000 datapoints
2025-03-06 18:09:55,417 - INFO - training batch 1051, loss: 0.496, 33632/60000 datapoints
2025-03-06 18:09:55,611 - INFO - training batch 1101, loss: 0.412, 35232/60000 datapoints
2025-03-06 18:09:55,802 - INFO - training batch 1151, loss: 0.273, 36832/60000 datapoints
2025-03-06 18:09:55,994 - INFO - training batch 1201, loss: 0.330, 38432/60000 datapoints
2025-03-06 18:09:56,189 - INFO - training batch 1251, loss: 0.417, 40032/60000 datapoints
2025-03-06 18:09:56,381 - INFO - training batch 1301, loss: 0.449, 41632/60000 datapoints
2025-03-06 18:09:56,570 - INFO - training batch 1351, loss: 0.206, 43232/60000 datapoints
2025-03-06 18:09:56,762 - INFO - training batch 1401, loss: 0.237, 44832/60000 datapoints
2025-03-06 18:09:56,956 - INFO - training batch 1451, loss: 0.206, 46432/60000 datapoints
2025-03-06 18:09:57,147 - INFO - training batch 1501, loss: 0.226, 48032/60000 datapoints
2025-03-06 18:09:57,340 - INFO - training batch 1551, loss: 0.348, 49632/60000 datapoints
2025-03-06 18:09:57,530 - INFO - training batch 1601, loss: 0.562, 51232/60000 datapoints
2025-03-06 18:09:57,720 - INFO - training batch 1651, loss: 0.277, 52832/60000 datapoints
2025-03-06 18:09:57,913 - INFO - training batch 1701, loss: 0.464, 54432/60000 datapoints
2025-03-06 18:09:58,104 - INFO - training batch 1751, loss: 0.503, 56032/60000 datapoints
2025-03-06 18:09:58,296 - INFO - training batch 1801, loss: 0.254, 57632/60000 datapoints
2025-03-06 18:09:58,493 - INFO - training batch 1851, loss: 0.411, 59232/60000 datapoints
2025-03-06 18:09:58,599 - INFO - validation batch 1, loss: 0.319, 32/10016 datapoints
2025-03-06 18:09:58,754 - INFO - validation batch 51, loss: 0.284, 1632/10016 datapoints
2025-03-06 18:09:58,911 - INFO - validation batch 101, loss: 0.377, 3232/10016 datapoints
2025-03-06 18:09:59,063 - INFO - validation batch 151, loss: 0.574, 4832/10016 datapoints
2025-03-06 18:09:59,215 - INFO - validation batch 201, loss: 0.328, 6432/10016 datapoints
2025-03-06 18:09:59,369 - INFO - validation batch 251, loss: 0.261, 8032/10016 datapoints
2025-03-06 18:09:59,523 - INFO - validation batch 301, loss: 0.699, 9632/10016 datapoints
2025-03-06 18:09:59,559 - INFO - Epoch 187/800 done.
2025-03-06 18:09:59,560 - INFO - Final validation performance:
Loss: 0.406, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 18:09:59,560 - INFO - Beginning epoch 188/800
2025-03-06 18:09:59,567 - INFO - training batch 1, loss: 0.555, 32/60000 datapoints
2025-03-06 18:09:59,760 - INFO - training batch 51, loss: 0.152, 1632/60000 datapoints
2025-03-06 18:09:59,954 - INFO - training batch 101, loss: 0.336, 3232/60000 datapoints
2025-03-06 18:10:00,148 - INFO - training batch 151, loss: 0.504, 4832/60000 datapoints
2025-03-06 18:10:00,341 - INFO - training batch 201, loss: 0.225, 6432/60000 datapoints
2025-03-06 18:10:00,532 - INFO - training batch 251, loss: 0.376, 8032/60000 datapoints
2025-03-06 18:10:00,726 - INFO - training batch 301, loss: 0.356, 9632/60000 datapoints
2025-03-06 18:10:00,922 - INFO - training batch 351, loss: 0.189, 11232/60000 datapoints
2025-03-06 18:10:01,113 - INFO - training batch 401, loss: 0.322, 12832/60000 datapoints
2025-03-06 18:10:01,309 - INFO - training batch 451, loss: 0.420, 14432/60000 datapoints
2025-03-06 18:10:01,501 - INFO - training batch 501, loss: 0.491, 16032/60000 datapoints
2025-03-06 18:10:01,695 - INFO - training batch 551, loss: 0.417, 17632/60000 datapoints
2025-03-06 18:10:01,895 - INFO - training batch 601, loss: 0.664, 19232/60000 datapoints
2025-03-06 18:10:02,097 - INFO - training batch 651, loss: 0.214, 20832/60000 datapoints
2025-03-06 18:10:02,289 - INFO - training batch 701, loss: 0.235, 22432/60000 datapoints
2025-03-06 18:10:02,482 - INFO - training batch 751, loss: 0.333, 24032/60000 datapoints
2025-03-06 18:10:02,679 - INFO - training batch 801, loss: 0.390, 25632/60000 datapoints
2025-03-06 18:10:02,873 - INFO - training batch 851, loss: 0.562, 27232/60000 datapoints
2025-03-06 18:10:03,067 - INFO - training batch 901, loss: 0.564, 28832/60000 datapoints
2025-03-06 18:10:03,260 - INFO - training batch 951, loss: 0.293, 30432/60000 datapoints
2025-03-06 18:10:03,453 - INFO - training batch 1001, loss: 0.135, 32032/60000 datapoints
2025-03-06 18:10:03,646 - INFO - training batch 1051, loss: 0.253, 33632/60000 datapoints
2025-03-06 18:10:03,838 - INFO - training batch 1101, loss: 0.137, 35232/60000 datapoints
2025-03-06 18:10:04,030 - INFO - training batch 1151, loss: 0.326, 36832/60000 datapoints
2025-03-06 18:10:04,221 - INFO - training batch 1201, loss: 0.269, 38432/60000 datapoints
2025-03-06 18:10:04,412 - INFO - training batch 1251, loss: 0.208, 40032/60000 datapoints
2025-03-06 18:10:04,606 - INFO - training batch 1301, loss: 0.181, 41632/60000 datapoints
2025-03-06 18:10:04,795 - INFO - training batch 1351, loss: 0.448, 43232/60000 datapoints
2025-03-06 18:10:04,997 - INFO - training batch 1401, loss: 0.469, 44832/60000 datapoints
2025-03-06 18:10:05,189 - INFO - training batch 1451, loss: 0.516, 46432/60000 datapoints
2025-03-06 18:10:05,383 - INFO - training batch 1501, loss: 0.377, 48032/60000 datapoints
2025-03-06 18:10:05,579 - INFO - training batch 1551, loss: 0.309, 49632/60000 datapoints
2025-03-06 18:10:05,807 - INFO - training batch 1601, loss: 0.418, 51232/60000 datapoints
2025-03-06 18:10:06,006 - INFO - training batch 1651, loss: 0.573, 52832/60000 datapoints
2025-03-06 18:10:06,199 - INFO - training batch 1701, loss: 0.209, 54432/60000 datapoints
2025-03-06 18:10:06,391 - INFO - training batch 1751, loss: 0.491, 56032/60000 datapoints
2025-03-06 18:10:06,581 - INFO - training batch 1801, loss: 0.205, 57632/60000 datapoints
2025-03-06 18:10:06,774 - INFO - training batch 1851, loss: 0.398, 59232/60000 datapoints
2025-03-06 18:10:06,871 - INFO - validation batch 1, loss: 0.330, 32/10016 datapoints
2025-03-06 18:10:07,024 - INFO - validation batch 51, loss: 0.540, 1632/10016 datapoints
2025-03-06 18:10:07,174 - INFO - validation batch 101, loss: 0.229, 3232/10016 datapoints
2025-03-06 18:10:07,324 - INFO - validation batch 151, loss: 0.344, 4832/10016 datapoints
2025-03-06 18:10:07,475 - INFO - validation batch 201, loss: 0.546, 6432/10016 datapoints
2025-03-06 18:10:07,629 - INFO - validation batch 251, loss: 0.186, 8032/10016 datapoints
2025-03-06 18:10:07,779 - INFO - validation batch 301, loss: 0.324, 9632/10016 datapoints
2025-03-06 18:10:07,815 - INFO - Epoch 188/800 done.
2025-03-06 18:10:07,815 - INFO - Final validation performance:
Loss: 0.357, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 18:10:07,815 - INFO - Beginning epoch 189/800
2025-03-06 18:10:07,821 - INFO - training batch 1, loss: 0.202, 32/60000 datapoints
2025-03-06 18:10:08,016 - INFO - training batch 51, loss: 0.443, 1632/60000 datapoints
2025-03-06 18:10:08,213 - INFO - training batch 101, loss: 0.307, 3232/60000 datapoints
2025-03-06 18:10:08,406 - INFO - training batch 151, loss: 0.289, 4832/60000 datapoints
2025-03-06 18:10:08,603 - INFO - training batch 201, loss: 0.443, 6432/60000 datapoints
2025-03-06 18:10:08,793 - INFO - training batch 251, loss: 0.768, 8032/60000 datapoints
2025-03-06 18:10:08,991 - INFO - training batch 301, loss: 0.595, 9632/60000 datapoints
2025-03-06 18:10:09,183 - INFO - training batch 351, loss: 0.256, 11232/60000 datapoints
2025-03-06 18:10:09,377 - INFO - training batch 401, loss: 0.352, 12832/60000 datapoints
2025-03-06 18:10:09,570 - INFO - training batch 451, loss: 0.346, 14432/60000 datapoints
2025-03-06 18:10:09,785 - INFO - training batch 501, loss: 0.327, 16032/60000 datapoints
2025-03-06 18:10:09,977 - INFO - training batch 551, loss: 0.387, 17632/60000 datapoints
2025-03-06 18:10:10,181 - INFO - training batch 601, loss: 0.599, 19232/60000 datapoints
2025-03-06 18:10:10,372 - INFO - training batch 651, loss: 0.246, 20832/60000 datapoints
2025-03-06 18:10:10,562 - INFO - training batch 701, loss: 0.331, 22432/60000 datapoints
2025-03-06 18:10:10,756 - INFO - training batch 751, loss: 0.302, 24032/60000 datapoints
2025-03-06 18:10:10,951 - INFO - training batch 801, loss: 0.359, 25632/60000 datapoints
2025-03-06 18:10:11,142 - INFO - training batch 851, loss: 0.263, 27232/60000 datapoints
2025-03-06 18:10:11,331 - INFO - training batch 901, loss: 0.442, 28832/60000 datapoints
2025-03-06 18:10:11,521 - INFO - training batch 951, loss: 0.348, 30432/60000 datapoints
2025-03-06 18:10:11,713 - INFO - training batch 1001, loss: 0.470, 32032/60000 datapoints
2025-03-06 18:10:11,903 - INFO - training batch 1051, loss: 0.174, 33632/60000 datapoints
2025-03-06 18:10:12,116 - INFO - training batch 1101, loss: 0.247, 35232/60000 datapoints
2025-03-06 18:10:12,308 - INFO - training batch 1151, loss: 0.683, 36832/60000 datapoints
2025-03-06 18:10:12,501 - INFO - training batch 1201, loss: 0.212, 38432/60000 datapoints
2025-03-06 18:10:12,694 - INFO - training batch 1251, loss: 0.385, 40032/60000 datapoints
2025-03-06 18:10:12,884 - INFO - training batch 1301, loss: 0.497, 41632/60000 datapoints
2025-03-06 18:10:13,078 - INFO - training batch 1351, loss: 0.394, 43232/60000 datapoints
2025-03-06 18:10:13,271 - INFO - training batch 1401, loss: 0.580, 44832/60000 datapoints
2025-03-06 18:10:13,462 - INFO - training batch 1451, loss: 0.179, 46432/60000 datapoints
2025-03-06 18:10:13,657 - INFO - training batch 1501, loss: 0.921, 48032/60000 datapoints
2025-03-06 18:10:13,846 - INFO - training batch 1551, loss: 0.176, 49632/60000 datapoints
2025-03-06 18:10:14,041 - INFO - training batch 1601, loss: 0.372, 51232/60000 datapoints
2025-03-06 18:10:14,247 - INFO - training batch 1651, loss: 0.651, 52832/60000 datapoints
2025-03-06 18:10:14,439 - INFO - training batch 1701, loss: 0.372, 54432/60000 datapoints
2025-03-06 18:10:14,634 - INFO - training batch 1751, loss: 0.439, 56032/60000 datapoints
2025-03-06 18:10:14,824 - INFO - training batch 1801, loss: 0.426, 57632/60000 datapoints
2025-03-06 18:10:15,024 - INFO - training batch 1851, loss: 0.272, 59232/60000 datapoints
2025-03-06 18:10:15,123 - INFO - validation batch 1, loss: 0.319, 32/10016 datapoints
2025-03-06 18:10:15,274 - INFO - validation batch 51, loss: 0.635, 1632/10016 datapoints
2025-03-06 18:10:15,425 - INFO - validation batch 101, loss: 0.168, 3232/10016 datapoints
2025-03-06 18:10:15,578 - INFO - validation batch 151, loss: 0.235, 4832/10016 datapoints
2025-03-06 18:10:15,729 - INFO - validation batch 201, loss: 0.276, 6432/10016 datapoints
2025-03-06 18:10:15,882 - INFO - validation batch 251, loss: 0.506, 8032/10016 datapoints
2025-03-06 18:10:16,036 - INFO - validation batch 301, loss: 0.471, 9632/10016 datapoints
2025-03-06 18:10:16,072 - INFO - Epoch 189/800 done.
2025-03-06 18:10:16,072 - INFO - Final validation performance:
Loss: 0.373, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 18:10:16,072 - INFO - Beginning epoch 190/800
2025-03-06 18:10:16,078 - INFO - training batch 1, loss: 0.666, 32/60000 datapoints
2025-03-06 18:10:16,272 - INFO - training batch 51, loss: 0.410, 1632/60000 datapoints
2025-03-06 18:10:16,462 - INFO - training batch 101, loss: 0.166, 3232/60000 datapoints
2025-03-06 18:10:16,663 - INFO - training batch 151, loss: 0.545, 4832/60000 datapoints
2025-03-06 18:10:16,856 - INFO - training batch 201, loss: 0.314, 6432/60000 datapoints
2025-03-06 18:10:17,050 - INFO - training batch 251, loss: 0.382, 8032/60000 datapoints
2025-03-06 18:10:17,243 - INFO - training batch 301, loss: 0.161, 9632/60000 datapoints
2025-03-06 18:10:17,436 - INFO - training batch 351, loss: 0.322, 11232/60000 datapoints
2025-03-06 18:10:17,629 - INFO - training batch 401, loss: 0.312, 12832/60000 datapoints
2025-03-06 18:10:17,822 - INFO - training batch 451, loss: 0.367, 14432/60000 datapoints
2025-03-06 18:10:18,015 - INFO - training batch 501, loss: 0.280, 16032/60000 datapoints
2025-03-06 18:10:18,209 - INFO - training batch 551, loss: 0.528, 17632/60000 datapoints
2025-03-06 18:10:18,400 - INFO - training batch 601, loss: 0.667, 19232/60000 datapoints
2025-03-06 18:10:18,606 - INFO - training batch 651, loss: 0.527, 20832/60000 datapoints
2025-03-06 18:10:18,799 - INFO - training batch 701, loss: 0.313, 22432/60000 datapoints
2025-03-06 18:10:18,997 - INFO - training batch 751, loss: 0.430, 24032/60000 datapoints
2025-03-06 18:10:19,196 - INFO - training batch 801, loss: 0.179, 25632/60000 datapoints
2025-03-06 18:10:19,389 - INFO - training batch 851, loss: 0.198, 27232/60000 datapoints
2025-03-06 18:10:19,586 - INFO - training batch 901, loss: 0.503, 28832/60000 datapoints
2025-03-06 18:10:19,784 - INFO - training batch 951, loss: 0.434, 30432/60000 datapoints
2025-03-06 18:10:19,983 - INFO - training batch 1001, loss: 0.657, 32032/60000 datapoints
2025-03-06 18:10:20,178 - INFO - training batch 1051, loss: 0.602, 33632/60000 datapoints
2025-03-06 18:10:20,371 - INFO - training batch 1101, loss: 0.532, 35232/60000 datapoints
2025-03-06 18:10:20,563 - INFO - training batch 1151, loss: 0.178, 36832/60000 datapoints
2025-03-06 18:10:20,762 - INFO - training batch 1201, loss: 0.297, 38432/60000 datapoints
2025-03-06 18:10:20,955 - INFO - training batch 1251, loss: 0.371, 40032/60000 datapoints
2025-03-06 18:10:21,151 - INFO - training batch 1301, loss: 0.216, 41632/60000 datapoints
2025-03-06 18:10:21,344 - INFO - training batch 1351, loss: 0.727, 43232/60000 datapoints
2025-03-06 18:10:21,539 - INFO - training batch 1401, loss: 0.688, 44832/60000 datapoints
2025-03-06 18:10:21,734 - INFO - training batch 1451, loss: 0.255, 46432/60000 datapoints
2025-03-06 18:10:21,927 - INFO - training batch 1501, loss: 0.370, 48032/60000 datapoints
2025-03-06 18:10:22,147 - INFO - training batch 1551, loss: 0.288, 49632/60000 datapoints
2025-03-06 18:10:22,340 - INFO - training batch 1601, loss: 0.282, 51232/60000 datapoints
2025-03-06 18:10:22,536 - INFO - training batch 1651, loss: 0.296, 52832/60000 datapoints
2025-03-06 18:10:22,731 - INFO - training batch 1701, loss: 0.348, 54432/60000 datapoints
2025-03-06 18:10:22,922 - INFO - training batch 1751, loss: 0.273, 56032/60000 datapoints
2025-03-06 18:10:23,121 - INFO - training batch 1801, loss: 0.325, 57632/60000 datapoints
2025-03-06 18:10:23,314 - INFO - training batch 1851, loss: 0.251, 59232/60000 datapoints
2025-03-06 18:10:23,415 - INFO - validation batch 1, loss: 0.468, 32/10016 datapoints
2025-03-06 18:10:23,566 - INFO - validation batch 51, loss: 0.272, 1632/10016 datapoints
2025-03-06 18:10:23,722 - INFO - validation batch 101, loss: 0.236, 3232/10016 datapoints
2025-03-06 18:10:23,876 - INFO - validation batch 151, loss: 0.347, 4832/10016 datapoints
2025-03-06 18:10:24,031 - INFO - validation batch 201, loss: 0.194, 6432/10016 datapoints
2025-03-06 18:10:24,186 - INFO - validation batch 251, loss: 0.283, 8032/10016 datapoints
2025-03-06 18:10:24,338 - INFO - validation batch 301, loss: 0.290, 9632/10016 datapoints
2025-03-06 18:10:24,375 - INFO - Epoch 190/800 done.
2025-03-06 18:10:24,376 - INFO - Final validation performance:
Loss: 0.299, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 18:10:24,376 - INFO - Beginning epoch 191/800
2025-03-06 18:10:24,382 - INFO - training batch 1, loss: 0.461, 32/60000 datapoints
2025-03-06 18:10:24,576 - INFO - training batch 51, loss: 0.501, 1632/60000 datapoints
2025-03-06 18:10:24,774 - INFO - training batch 101, loss: 0.446, 3232/60000 datapoints
2025-03-06 18:10:24,971 - INFO - training batch 151, loss: 0.449, 4832/60000 datapoints
2025-03-06 18:10:25,176 - INFO - training batch 201, loss: 0.752, 6432/60000 datapoints
2025-03-06 18:10:25,371 - INFO - training batch 251, loss: 0.190, 8032/60000 datapoints
2025-03-06 18:10:25,566 - INFO - training batch 301, loss: 0.214, 9632/60000 datapoints
2025-03-06 18:10:25,765 - INFO - training batch 351, loss: 0.226, 11232/60000 datapoints
2025-03-06 18:10:25,959 - INFO - training batch 401, loss: 0.109, 12832/60000 datapoints
2025-03-06 18:10:26,169 - INFO - training batch 451, loss: 0.327, 14432/60000 datapoints
2025-03-06 18:10:26,387 - INFO - training batch 501, loss: 0.549, 16032/60000 datapoints
2025-03-06 18:10:26,581 - INFO - training batch 551, loss: 0.369, 17632/60000 datapoints
2025-03-06 18:10:26,777 - INFO - training batch 601, loss: 0.209, 19232/60000 datapoints
2025-03-06 18:10:26,971 - INFO - training batch 651, loss: 0.238, 20832/60000 datapoints
2025-03-06 18:10:27,170 - INFO - training batch 701, loss: 0.331, 22432/60000 datapoints
2025-03-06 18:10:27,366 - INFO - training batch 751, loss: 0.569, 24032/60000 datapoints
2025-03-06 18:10:27,562 - INFO - training batch 801, loss: 0.398, 25632/60000 datapoints
2025-03-06 18:10:27,758 - INFO - training batch 851, loss: 0.250, 27232/60000 datapoints
2025-03-06 18:10:27,952 - INFO - training batch 901, loss: 0.235, 28832/60000 datapoints
2025-03-06 18:10:28,143 - INFO - training batch 951, loss: 0.268, 30432/60000 datapoints
2025-03-06 18:10:28,340 - INFO - training batch 1001, loss: 0.378, 32032/60000 datapoints
2025-03-06 18:10:28,536 - INFO - training batch 1051, loss: 0.234, 33632/60000 datapoints
2025-03-06 18:10:28,738 - INFO - training batch 1101, loss: 0.396, 35232/60000 datapoints
2025-03-06 18:10:28,934 - INFO - training batch 1151, loss: 0.285, 36832/60000 datapoints
2025-03-06 18:10:29,130 - INFO - training batch 1201, loss: 0.127, 38432/60000 datapoints
2025-03-06 18:10:29,326 - INFO - training batch 1251, loss: 0.308, 40032/60000 datapoints
2025-03-06 18:10:29,518 - INFO - training batch 1301, loss: 0.210, 41632/60000 datapoints
2025-03-06 18:10:29,716 - INFO - training batch 1351, loss: 0.292, 43232/60000 datapoints
2025-03-06 18:10:29,911 - INFO - training batch 1401, loss: 0.384, 44832/60000 datapoints
2025-03-06 18:10:30,112 - INFO - training batch 1451, loss: 0.577, 46432/60000 datapoints
2025-03-06 18:10:30,308 - INFO - training batch 1501, loss: 0.330, 48032/60000 datapoints
2025-03-06 18:10:30,501 - INFO - training batch 1551, loss: 0.245, 49632/60000 datapoints
2025-03-06 18:10:30,697 - INFO - training batch 1601, loss: 0.223, 51232/60000 datapoints
2025-03-06 18:10:30,890 - INFO - training batch 1651, loss: 0.205, 52832/60000 datapoints
2025-03-06 18:10:31,087 - INFO - training batch 1701, loss: 0.259, 54432/60000 datapoints
2025-03-06 18:10:31,283 - INFO - training batch 1751, loss: 0.190, 56032/60000 datapoints
2025-03-06 18:10:31,477 - INFO - training batch 1801, loss: 0.666, 57632/60000 datapoints
2025-03-06 18:10:31,677 - INFO - training batch 1851, loss: 0.161, 59232/60000 datapoints
2025-03-06 18:10:31,780 - INFO - validation batch 1, loss: 0.232, 32/10016 datapoints
2025-03-06 18:10:31,937 - INFO - validation batch 51, loss: 0.233, 1632/10016 datapoints
2025-03-06 18:10:32,091 - INFO - validation batch 101, loss: 0.192, 3232/10016 datapoints
2025-03-06 18:10:32,280 - INFO - validation batch 151, loss: 0.209, 4832/10016 datapoints
2025-03-06 18:10:32,432 - INFO - validation batch 201, loss: 0.315, 6432/10016 datapoints
2025-03-06 18:10:32,584 - INFO - validation batch 251, loss: 0.361, 8032/10016 datapoints
2025-03-06 18:10:32,738 - INFO - validation batch 301, loss: 0.227, 9632/10016 datapoints
2025-03-06 18:10:32,777 - INFO - Epoch 191/800 done.
2025-03-06 18:10:32,777 - INFO - Final validation performance:
Loss: 0.253, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 18:10:32,777 - INFO - Beginning epoch 192/800
2025-03-06 18:10:32,783 - INFO - training batch 1, loss: 0.211, 32/60000 datapoints
2025-03-06 18:10:32,977 - INFO - training batch 51, loss: 0.384, 1632/60000 datapoints
2025-03-06 18:10:33,175 - INFO - training batch 101, loss: 0.274, 3232/60000 datapoints
2025-03-06 18:10:33,369 - INFO - training batch 151, loss: 0.616, 4832/60000 datapoints
2025-03-06 18:10:33,565 - INFO - training batch 201, loss: 0.216, 6432/60000 datapoints
2025-03-06 18:10:33,765 - INFO - training batch 251, loss: 0.245, 8032/60000 datapoints
2025-03-06 18:10:33,961 - INFO - training batch 301, loss: 0.389, 9632/60000 datapoints
2025-03-06 18:10:34,159 - INFO - training batch 351, loss: 0.196, 11232/60000 datapoints
2025-03-06 18:10:34,357 - INFO - training batch 401, loss: 0.378, 12832/60000 datapoints
2025-03-06 18:10:34,550 - INFO - training batch 451, loss: 0.194, 14432/60000 datapoints
2025-03-06 18:10:34,751 - INFO - training batch 501, loss: 0.637, 16032/60000 datapoints
2025-03-06 18:10:34,953 - INFO - training batch 551, loss: 0.149, 17632/60000 datapoints
2025-03-06 18:10:35,152 - INFO - training batch 601, loss: 0.359, 19232/60000 datapoints
2025-03-06 18:10:35,348 - INFO - training batch 651, loss: 0.388, 20832/60000 datapoints
2025-03-06 18:10:35,542 - INFO - training batch 701, loss: 0.283, 22432/60000 datapoints
2025-03-06 18:10:35,738 - INFO - training batch 751, loss: 0.268, 24032/60000 datapoints
2025-03-06 18:10:35,932 - INFO - training batch 801, loss: 0.267, 25632/60000 datapoints
2025-03-06 18:10:36,128 - INFO - training batch 851, loss: 0.130, 27232/60000 datapoints
2025-03-06 18:10:36,326 - INFO - training batch 901, loss: 0.496, 28832/60000 datapoints
2025-03-06 18:10:36,520 - INFO - training batch 951, loss: 0.390, 30432/60000 datapoints
2025-03-06 18:10:36,716 - INFO - training batch 1001, loss: 0.440, 32032/60000 datapoints
2025-03-06 18:10:36,910 - INFO - training batch 1051, loss: 0.494, 33632/60000 datapoints
2025-03-06 18:10:37,109 - INFO - training batch 1101, loss: 0.457, 35232/60000 datapoints
2025-03-06 18:10:37,306 - INFO - training batch 1151, loss: 0.216, 36832/60000 datapoints
2025-03-06 18:10:37,501 - INFO - training batch 1201, loss: 0.303, 38432/60000 datapoints
2025-03-06 18:10:37,715 - INFO - training batch 1251, loss: 0.294, 40032/60000 datapoints
2025-03-06 18:10:37,911 - INFO - training batch 1301, loss: 0.431, 41632/60000 datapoints
2025-03-06 18:10:38,121 - INFO - training batch 1351, loss: 0.773, 43232/60000 datapoints
2025-03-06 18:10:38,314 - INFO - training batch 1401, loss: 0.377, 44832/60000 datapoints
2025-03-06 18:10:38,509 - INFO - training batch 1451, loss: 0.415, 46432/60000 datapoints
2025-03-06 18:10:38,711 - INFO - training batch 1501, loss: 0.302, 48032/60000 datapoints
2025-03-06 18:10:38,907 - INFO - training batch 1551, loss: 0.352, 49632/60000 datapoints
2025-03-06 18:10:39,105 - INFO - training batch 1601, loss: 0.444, 51232/60000 datapoints
2025-03-06 18:10:39,302 - INFO - training batch 1651, loss: 0.504, 52832/60000 datapoints
2025-03-06 18:10:39,496 - INFO - training batch 1701, loss: 0.268, 54432/60000 datapoints
2025-03-06 18:10:39,695 - INFO - training batch 1751, loss: 0.395, 56032/60000 datapoints
2025-03-06 18:10:39,894 - INFO - training batch 1801, loss: 0.301, 57632/60000 datapoints
2025-03-06 18:10:40,088 - INFO - training batch 1851, loss: 0.332, 59232/60000 datapoints
2025-03-06 18:10:40,190 - INFO - validation batch 1, loss: 0.357, 32/10016 datapoints
2025-03-06 18:10:40,343 - INFO - validation batch 51, loss: 0.377, 1632/10016 datapoints
2025-03-06 18:10:40,498 - INFO - validation batch 101, loss: 0.182, 3232/10016 datapoints
2025-03-06 18:10:40,655 - INFO - validation batch 151, loss: 0.474, 4832/10016 datapoints
2025-03-06 18:10:40,807 - INFO - validation batch 201, loss: 0.148, 6432/10016 datapoints
2025-03-06 18:10:40,960 - INFO - validation batch 251, loss: 0.457, 8032/10016 datapoints
2025-03-06 18:10:41,117 - INFO - validation batch 301, loss: 0.225, 9632/10016 datapoints
2025-03-06 18:10:41,153 - INFO - Epoch 192/800 done.
2025-03-06 18:10:41,153 - INFO - Final validation performance:
Loss: 0.317, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 18:10:41,153 - INFO - Beginning epoch 193/800
2025-03-06 18:10:41,159 - INFO - training batch 1, loss: 0.221, 32/60000 datapoints
2025-03-06 18:10:41,358 - INFO - training batch 51, loss: 0.616, 1632/60000 datapoints
2025-03-06 18:10:41,553 - INFO - training batch 101, loss: 0.421, 3232/60000 datapoints
2025-03-06 18:10:41,749 - INFO - training batch 151, loss: 0.169, 4832/60000 datapoints
2025-03-06 18:10:41,947 - INFO - training batch 201, loss: 0.290, 6432/60000 datapoints
2025-03-06 18:10:42,143 - INFO - training batch 251, loss: 0.380, 8032/60000 datapoints
2025-03-06 18:10:42,359 - INFO - training batch 301, loss: 0.344, 9632/60000 datapoints
2025-03-06 18:10:42,553 - INFO - training batch 351, loss: 0.482, 11232/60000 datapoints
2025-03-06 18:10:42,750 - INFO - training batch 401, loss: 0.503, 12832/60000 datapoints
2025-03-06 18:10:42,947 - INFO - training batch 451, loss: 0.382, 14432/60000 datapoints
2025-03-06 18:10:43,147 - INFO - training batch 501, loss: 0.485, 16032/60000 datapoints
2025-03-06 18:10:43,344 - INFO - training batch 551, loss: 0.493, 17632/60000 datapoints
2025-03-06 18:10:43,539 - INFO - training batch 601, loss: 0.460, 19232/60000 datapoints
2025-03-06 18:10:43,737 - INFO - training batch 651, loss: 0.601, 20832/60000 datapoints
2025-03-06 18:10:43,933 - INFO - training batch 701, loss: 0.312, 22432/60000 datapoints
2025-03-06 18:10:44,129 - INFO - training batch 751, loss: 0.293, 24032/60000 datapoints
2025-03-06 18:10:44,323 - INFO - training batch 801, loss: 0.327, 25632/60000 datapoints
2025-03-06 18:10:44,518 - INFO - training batch 851, loss: 0.474, 27232/60000 datapoints
2025-03-06 18:10:44,715 - INFO - training batch 901, loss: 0.391, 28832/60000 datapoints
2025-03-06 18:10:44,914 - INFO - training batch 951, loss: 0.314, 30432/60000 datapoints
2025-03-06 18:10:45,117 - INFO - training batch 1001, loss: 0.380, 32032/60000 datapoints
2025-03-06 18:10:45,313 - INFO - training batch 1051, loss: 0.435, 33632/60000 datapoints
2025-03-06 18:10:45,508 - INFO - training batch 1101, loss: 0.367, 35232/60000 datapoints
2025-03-06 18:10:45,705 - INFO - training batch 1151, loss: 0.221, 36832/60000 datapoints
2025-03-06 18:10:45,901 - INFO - training batch 1201, loss: 0.248, 38432/60000 datapoints
2025-03-06 18:10:46,095 - INFO - training batch 1251, loss: 0.191, 40032/60000 datapoints
2025-03-06 18:10:46,293 - INFO - training batch 1301, loss: 0.284, 41632/60000 datapoints
2025-03-06 18:10:46,490 - INFO - training batch 1351, loss: 0.636, 43232/60000 datapoints
2025-03-06 18:10:46,687 - INFO - training batch 1401, loss: 0.455, 44832/60000 datapoints
2025-03-06 18:10:46,887 - INFO - training batch 1451, loss: 0.233, 46432/60000 datapoints
2025-03-06 18:10:47,088 - INFO - training batch 1501, loss: 0.256, 48032/60000 datapoints
2025-03-06 18:10:47,284 - INFO - training batch 1551, loss: 0.465, 49632/60000 datapoints
2025-03-06 18:10:47,479 - INFO - training batch 1601, loss: 0.413, 51232/60000 datapoints
2025-03-06 18:10:47,674 - INFO - training batch 1651, loss: 0.432, 52832/60000 datapoints
2025-03-06 18:10:47,867 - INFO - training batch 1701, loss: 0.272, 54432/60000 datapoints
2025-03-06 18:10:48,062 - INFO - training batch 1751, loss: 0.255, 56032/60000 datapoints
2025-03-06 18:10:48,258 - INFO - training batch 1801, loss: 0.243, 57632/60000 datapoints
2025-03-06 18:10:48,457 - INFO - training batch 1851, loss: 0.329, 59232/60000 datapoints
2025-03-06 18:10:48,560 - INFO - validation batch 1, loss: 0.362, 32/10016 datapoints
2025-03-06 18:10:48,718 - INFO - validation batch 51, loss: 0.408, 1632/10016 datapoints
2025-03-06 18:10:48,872 - INFO - validation batch 101, loss: 0.302, 3232/10016 datapoints
2025-03-06 18:10:49,024 - INFO - validation batch 151, loss: 0.194, 4832/10016 datapoints
2025-03-06 18:10:49,182 - INFO - validation batch 201, loss: 0.297, 6432/10016 datapoints
2025-03-06 18:10:49,335 - INFO - validation batch 251, loss: 0.545, 8032/10016 datapoints
2025-03-06 18:10:49,491 - INFO - validation batch 301, loss: 0.507, 9632/10016 datapoints
2025-03-06 18:10:49,530 - INFO - Epoch 193/800 done.
2025-03-06 18:10:49,530 - INFO - Final validation performance:
Loss: 0.374, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 18:10:49,530 - INFO - Beginning epoch 194/800
2025-03-06 18:10:49,536 - INFO - training batch 1, loss: 0.278, 32/60000 datapoints
2025-03-06 18:10:49,735 - INFO - training batch 51, loss: 0.465, 1632/60000 datapoints
2025-03-06 18:10:49,933 - INFO - training batch 101, loss: 0.246, 3232/60000 datapoints
2025-03-06 18:10:50,128 - INFO - training batch 151, loss: 0.440, 4832/60000 datapoints
2025-03-06 18:10:50,323 - INFO - training batch 201, loss: 0.272, 6432/60000 datapoints
2025-03-06 18:10:50,517 - INFO - training batch 251, loss: 0.432, 8032/60000 datapoints
2025-03-06 18:10:50,715 - INFO - training batch 301, loss: 0.669, 9632/60000 datapoints
2025-03-06 18:10:50,909 - INFO - training batch 351, loss: 0.296, 11232/60000 datapoints
2025-03-06 18:10:51,109 - INFO - training batch 401, loss: 0.649, 12832/60000 datapoints
2025-03-06 18:10:51,307 - INFO - training batch 451, loss: 0.421, 14432/60000 datapoints
2025-03-06 18:10:51,503 - INFO - training batch 501, loss: 0.413, 16032/60000 datapoints
2025-03-06 18:10:51,699 - INFO - training batch 551, loss: 0.274, 17632/60000 datapoints
2025-03-06 18:10:51,893 - INFO - training batch 601, loss: 0.243, 19232/60000 datapoints
2025-03-06 18:10:52,085 - INFO - training batch 651, loss: 0.222, 20832/60000 datapoints
2025-03-06 18:10:52,286 - INFO - training batch 701, loss: 0.220, 22432/60000 datapoints
2025-03-06 18:10:52,503 - INFO - training batch 751, loss: 0.218, 24032/60000 datapoints
2025-03-06 18:10:52,699 - INFO - training batch 801, loss: 0.234, 25632/60000 datapoints
2025-03-06 18:10:52,893 - INFO - training batch 851, loss: 0.343, 27232/60000 datapoints
2025-03-06 18:10:53,092 - INFO - training batch 901, loss: 0.261, 28832/60000 datapoints
2025-03-06 18:10:53,287 - INFO - training batch 951, loss: 0.549, 30432/60000 datapoints
2025-03-06 18:10:53,484 - INFO - training batch 1001, loss: 0.227, 32032/60000 datapoints
2025-03-06 18:10:53,681 - INFO - training batch 1051, loss: 0.316, 33632/60000 datapoints
2025-03-06 18:10:53,876 - INFO - training batch 1101, loss: 0.147, 35232/60000 datapoints
2025-03-06 18:10:54,072 - INFO - training batch 1151, loss: 0.400, 36832/60000 datapoints
2025-03-06 18:10:54,268 - INFO - training batch 1201, loss: 0.345, 38432/60000 datapoints
2025-03-06 18:10:54,462 - INFO - training batch 1251, loss: 0.321, 40032/60000 datapoints
2025-03-06 18:10:54,663 - INFO - training batch 1301, loss: 0.257, 41632/60000 datapoints
2025-03-06 18:10:54,859 - INFO - training batch 1351, loss: 0.457, 43232/60000 datapoints
2025-03-06 18:10:55,057 - INFO - training batch 1401, loss: 0.356, 44832/60000 datapoints
2025-03-06 18:10:55,255 - INFO - training batch 1451, loss: 0.351, 46432/60000 datapoints
2025-03-06 18:10:55,450 - INFO - training batch 1501, loss: 0.294, 48032/60000 datapoints
2025-03-06 18:10:55,649 - INFO - training batch 1551, loss: 0.228, 49632/60000 datapoints
2025-03-06 18:10:55,843 - INFO - training batch 1601, loss: 0.359, 51232/60000 datapoints
2025-03-06 18:10:56,038 - INFO - training batch 1651, loss: 0.547, 52832/60000 datapoints
2025-03-06 18:10:56,234 - INFO - training batch 1701, loss: 0.356, 54432/60000 datapoints
2025-03-06 18:10:56,429 - INFO - training batch 1751, loss: 0.195, 56032/60000 datapoints
2025-03-06 18:10:56,624 - INFO - training batch 1801, loss: 0.300, 57632/60000 datapoints
2025-03-06 18:10:56,816 - INFO - training batch 1851, loss: 0.220, 59232/60000 datapoints
2025-03-06 18:10:56,917 - INFO - validation batch 1, loss: 0.375, 32/10016 datapoints
2025-03-06 18:10:57,070 - INFO - validation batch 51, loss: 0.197, 1632/10016 datapoints
2025-03-06 18:10:57,227 - INFO - validation batch 101, loss: 0.282, 3232/10016 datapoints
2025-03-06 18:10:57,380 - INFO - validation batch 151, loss: 0.353, 4832/10016 datapoints
2025-03-06 18:10:57,534 - INFO - validation batch 201, loss: 0.236, 6432/10016 datapoints
2025-03-06 18:10:57,691 - INFO - validation batch 251, loss: 0.217, 8032/10016 datapoints
2025-03-06 18:10:57,843 - INFO - validation batch 301, loss: 0.145, 9632/10016 datapoints
2025-03-06 18:10:57,880 - INFO - Epoch 194/800 done.
2025-03-06 18:10:57,881 - INFO - Final validation performance:
Loss: 0.258, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 18:10:57,881 - INFO - Beginning epoch 195/800
2025-03-06 18:10:57,887 - INFO - training batch 1, loss: 0.304, 32/60000 datapoints
2025-03-06 18:10:58,082 - INFO - training batch 51, loss: 0.335, 1632/60000 datapoints
2025-03-06 18:10:58,278 - INFO - training batch 101, loss: 0.311, 3232/60000 datapoints
2025-03-06 18:10:58,477 - INFO - training batch 151, loss: 0.336, 4832/60000 datapoints
2025-03-06 18:10:58,694 - INFO - training batch 201, loss: 0.348, 6432/60000 datapoints
2025-03-06 18:10:58,894 - INFO - training batch 251, loss: 0.370, 8032/60000 datapoints
2025-03-06 18:10:59,098 - INFO - training batch 301, loss: 0.343, 9632/60000 datapoints
2025-03-06 18:10:59,293 - INFO - training batch 351, loss: 0.444, 11232/60000 datapoints
2025-03-06 18:10:59,489 - INFO - training batch 401, loss: 0.502, 12832/60000 datapoints
2025-03-06 18:10:59,688 - INFO - training batch 451, loss: 0.610, 14432/60000 datapoints
2025-03-06 18:10:59,882 - INFO - training batch 501, loss: 0.351, 16032/60000 datapoints
2025-03-06 18:11:00,081 - INFO - training batch 551, loss: 0.552, 17632/60000 datapoints
2025-03-06 18:11:00,282 - INFO - training batch 601, loss: 0.339, 19232/60000 datapoints
2025-03-06 18:11:00,479 - INFO - training batch 651, loss: 0.306, 20832/60000 datapoints
2025-03-06 18:11:00,677 - INFO - training batch 701, loss: 0.437, 22432/60000 datapoints
2025-03-06 18:11:00,871 - INFO - training batch 751, loss: 0.379, 24032/60000 datapoints
2025-03-06 18:11:01,064 - INFO - training batch 801, loss: 0.504, 25632/60000 datapoints
2025-03-06 18:11:01,260 - INFO - training batch 851, loss: 0.589, 27232/60000 datapoints
2025-03-06 18:11:01,456 - INFO - training batch 901, loss: 0.335, 28832/60000 datapoints
2025-03-06 18:11:01,653 - INFO - training batch 951, loss: 0.465, 30432/60000 datapoints
2025-03-06 18:11:01,848 - INFO - training batch 1001, loss: 0.323, 32032/60000 datapoints
2025-03-06 18:11:02,044 - INFO - training batch 1051, loss: 0.310, 33632/60000 datapoints
2025-03-06 18:11:02,243 - INFO - training batch 1101, loss: 0.548, 35232/60000 datapoints
2025-03-06 18:11:02,461 - INFO - training batch 1151, loss: 0.274, 36832/60000 datapoints
2025-03-06 18:11:02,659 - INFO - training batch 1201, loss: 1.028, 38432/60000 datapoints
2025-03-06 18:11:02,854 - INFO - training batch 1251, loss: 0.687, 40032/60000 datapoints
2025-03-06 18:11:03,051 - INFO - training batch 1301, loss: 0.620, 41632/60000 datapoints
2025-03-06 18:11:03,246 - INFO - training batch 1351, loss: 0.659, 43232/60000 datapoints
2025-03-06 18:11:03,443 - INFO - training batch 1401, loss: 0.615, 44832/60000 datapoints
2025-03-06 18:11:03,643 - INFO - training batch 1451, loss: 0.216, 46432/60000 datapoints
2025-03-06 18:11:03,840 - INFO - training batch 1501, loss: 0.448, 48032/60000 datapoints
2025-03-06 18:11:04,036 - INFO - training batch 1551, loss: 0.368, 49632/60000 datapoints
2025-03-06 18:11:04,236 - INFO - training batch 1601, loss: 0.348, 51232/60000 datapoints
2025-03-06 18:11:04,429 - INFO - training batch 1651, loss: 0.444, 52832/60000 datapoints
2025-03-06 18:11:04,627 - INFO - training batch 1701, loss: 0.125, 54432/60000 datapoints
2025-03-06 18:11:04,819 - INFO - training batch 1751, loss: 0.482, 56032/60000 datapoints
2025-03-06 18:11:05,019 - INFO - training batch 1801, loss: 0.133, 57632/60000 datapoints
2025-03-06 18:11:05,215 - INFO - training batch 1851, loss: 0.610, 59232/60000 datapoints
2025-03-06 18:11:05,317 - INFO - validation batch 1, loss: 0.406, 32/10016 datapoints
2025-03-06 18:11:05,472 - INFO - validation batch 51, loss: 0.186, 1632/10016 datapoints
2025-03-06 18:11:05,629 - INFO - validation batch 101, loss: 0.424, 3232/10016 datapoints
2025-03-06 18:11:05,782 - INFO - validation batch 151, loss: 0.532, 4832/10016 datapoints
2025-03-06 18:11:05,937 - INFO - validation batch 201, loss: 0.458, 6432/10016 datapoints
2025-03-06 18:11:06,089 - INFO - validation batch 251, loss: 0.327, 8032/10016 datapoints
2025-03-06 18:11:06,244 - INFO - validation batch 301, loss: 0.537, 9632/10016 datapoints
2025-03-06 18:11:06,283 - INFO - Epoch 195/800 done.
2025-03-06 18:11:06,283 - INFO - Final validation performance:
Loss: 0.410, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 18:11:06,283 - INFO - Beginning epoch 196/800
2025-03-06 18:11:06,289 - INFO - training batch 1, loss: 0.392, 32/60000 datapoints
2025-03-06 18:11:06,482 - INFO - training batch 51, loss: 0.418, 1632/60000 datapoints
2025-03-06 18:11:06,679 - INFO - training batch 101, loss: 0.250, 3232/60000 datapoints
2025-03-06 18:11:06,875 - INFO - training batch 151, loss: 0.303, 4832/60000 datapoints
2025-03-06 18:11:07,068 - INFO - training batch 201, loss: 0.364, 6432/60000 datapoints
2025-03-06 18:11:07,267 - INFO - training batch 251, loss: 0.228, 8032/60000 datapoints
2025-03-06 18:11:07,464 - INFO - training batch 301, loss: 0.134, 9632/60000 datapoints
2025-03-06 18:11:07,665 - INFO - training batch 351, loss: 0.725, 11232/60000 datapoints
2025-03-06 18:11:07,860 - INFO - training batch 401, loss: 0.395, 12832/60000 datapoints
2025-03-06 18:11:08,090 - INFO - training batch 451, loss: 0.347, 14432/60000 datapoints
2025-03-06 18:11:08,288 - INFO - training batch 501, loss: 0.378, 16032/60000 datapoints
2025-03-06 18:11:08,485 - INFO - training batch 551, loss: 0.407, 17632/60000 datapoints
2025-03-06 18:11:08,680 - INFO - training batch 601, loss: 0.378, 19232/60000 datapoints
2025-03-06 18:11:08,878 - INFO - training batch 651, loss: 0.337, 20832/60000 datapoints
2025-03-06 18:11:09,074 - INFO - training batch 701, loss: 0.370, 22432/60000 datapoints
2025-03-06 18:11:09,272 - INFO - training batch 751, loss: 0.296, 24032/60000 datapoints
2025-03-06 18:11:09,465 - INFO - training batch 801, loss: 0.442, 25632/60000 datapoints
2025-03-06 18:11:09,665 - INFO - training batch 851, loss: 0.212, 27232/60000 datapoints
2025-03-06 18:11:09,860 - INFO - training batch 901, loss: 0.269, 28832/60000 datapoints
2025-03-06 18:11:10,056 - INFO - training batch 951, loss: 0.203, 30432/60000 datapoints
2025-03-06 18:11:10,254 - INFO - training batch 1001, loss: 0.428, 32032/60000 datapoints
2025-03-06 18:11:10,452 - INFO - training batch 1051, loss: 0.311, 33632/60000 datapoints
2025-03-06 18:11:10,660 - INFO - training batch 1101, loss: 0.246, 35232/60000 datapoints
2025-03-06 18:11:10,902 - INFO - training batch 1151, loss: 0.188, 36832/60000 datapoints
2025-03-06 18:11:11,113 - INFO - training batch 1201, loss: 0.421, 38432/60000 datapoints
2025-03-06 18:11:11,326 - INFO - training batch 1251, loss: 0.720, 40032/60000 datapoints
2025-03-06 18:11:11,520 - INFO - training batch 1301, loss: 0.149, 41632/60000 datapoints
2025-03-06 18:11:11,716 - INFO - training batch 1351, loss: 0.171, 43232/60000 datapoints
2025-03-06 18:11:11,914 - INFO - training batch 1401, loss: 0.592, 44832/60000 datapoints
2025-03-06 18:11:12,109 - INFO - training batch 1451, loss: 0.486, 46432/60000 datapoints
2025-03-06 18:11:12,304 - INFO - training batch 1501, loss: 0.377, 48032/60000 datapoints
2025-03-06 18:11:12,514 - INFO - training batch 1551, loss: 0.518, 49632/60000 datapoints
2025-03-06 18:11:12,722 - INFO - training batch 1601, loss: 0.286, 51232/60000 datapoints
2025-03-06 18:11:12,917 - INFO - training batch 1651, loss: 0.303, 52832/60000 datapoints
2025-03-06 18:11:13,112 - INFO - training batch 1701, loss: 0.287, 54432/60000 datapoints
2025-03-06 18:11:13,313 - INFO - training batch 1751, loss: 0.676, 56032/60000 datapoints
2025-03-06 18:11:13,536 - INFO - training batch 1801, loss: 0.226, 57632/60000 datapoints
2025-03-06 18:11:13,735 - INFO - training batch 1851, loss: 0.262, 59232/60000 datapoints
2025-03-06 18:11:13,837 - INFO - validation batch 1, loss: 0.652, 32/10016 datapoints
2025-03-06 18:11:13,990 - INFO - validation batch 51, loss: 0.142, 1632/10016 datapoints
2025-03-06 18:11:14,143 - INFO - validation batch 101, loss: 0.624, 3232/10016 datapoints
2025-03-06 18:11:14,297 - INFO - validation batch 151, loss: 0.564, 4832/10016 datapoints
2025-03-06 18:11:14,451 - INFO - validation batch 201, loss: 0.453, 6432/10016 datapoints
2025-03-06 18:11:14,607 - INFO - validation batch 251, loss: 0.320, 8032/10016 datapoints
2025-03-06 18:11:14,764 - INFO - validation batch 301, loss: 0.153, 9632/10016 datapoints
2025-03-06 18:11:14,799 - INFO - Epoch 196/800 done.
2025-03-06 18:11:14,799 - INFO - Final validation performance:
Loss: 0.415, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 18:11:14,800 - INFO - Beginning epoch 197/800
2025-03-06 18:11:14,806 - INFO - training batch 1, loss: 0.403, 32/60000 datapoints
2025-03-06 18:11:15,008 - INFO - training batch 51, loss: 0.245, 1632/60000 datapoints
2025-03-06 18:11:15,208 - INFO - training batch 101, loss: 0.449, 3232/60000 datapoints
2025-03-06 18:11:15,405 - INFO - training batch 151, loss: 0.394, 4832/60000 datapoints
2025-03-06 18:11:15,601 - INFO - training batch 201, loss: 0.196, 6432/60000 datapoints
2025-03-06 18:11:15,800 - INFO - training batch 251, loss: 0.248, 8032/60000 datapoints
2025-03-06 18:11:15,993 - INFO - training batch 301, loss: 0.446, 9632/60000 datapoints
2025-03-06 18:11:16,190 - INFO - training batch 351, loss: 0.164, 11232/60000 datapoints
2025-03-06 18:11:16,386 - INFO - training batch 401, loss: 0.363, 12832/60000 datapoints
2025-03-06 18:11:16,579 - INFO - training batch 451, loss: 0.274, 14432/60000 datapoints
2025-03-06 18:11:16,776 - INFO - training batch 501, loss: 0.183, 16032/60000 datapoints
2025-03-06 18:11:16,972 - INFO - training batch 551, loss: 0.337, 17632/60000 datapoints
2025-03-06 18:11:17,171 - INFO - training batch 601, loss: 0.263, 19232/60000 datapoints
2025-03-06 18:11:17,376 - INFO - training batch 651, loss: 0.396, 20832/60000 datapoints
2025-03-06 18:11:17,580 - INFO - training batch 701, loss: 0.367, 22432/60000 datapoints
2025-03-06 18:11:17,799 - INFO - training batch 751, loss: 0.147, 24032/60000 datapoints
2025-03-06 18:11:17,995 - INFO - training batch 801, loss: 0.336, 25632/60000 datapoints
2025-03-06 18:11:18,187 - INFO - training batch 851, loss: 0.188, 27232/60000 datapoints
2025-03-06 18:11:18,380 - INFO - training batch 901, loss: 0.553, 28832/60000 datapoints
2025-03-06 18:11:18,574 - INFO - training batch 951, loss: 0.484, 30432/60000 datapoints
2025-03-06 18:11:18,777 - INFO - training batch 1001, loss: 0.303, 32032/60000 datapoints
2025-03-06 18:11:18,973 - INFO - training batch 1051, loss: 0.467, 33632/60000 datapoints
2025-03-06 18:11:19,170 - INFO - training batch 1101, loss: 0.246, 35232/60000 datapoints
2025-03-06 18:11:19,365 - INFO - training batch 1151, loss: 0.744, 36832/60000 datapoints
2025-03-06 18:11:19,560 - INFO - training batch 1201, loss: 0.297, 38432/60000 datapoints
2025-03-06 18:11:19,757 - INFO - training batch 1251, loss: 0.319, 40032/60000 datapoints
2025-03-06 18:11:19,958 - INFO - training batch 1301, loss: 0.193, 41632/60000 datapoints
2025-03-06 18:11:20,165 - INFO - training batch 1351, loss: 0.268, 43232/60000 datapoints
2025-03-06 18:11:20,363 - INFO - training batch 1401, loss: 0.373, 44832/60000 datapoints
2025-03-06 18:11:20,559 - INFO - training batch 1451, loss: 0.744, 46432/60000 datapoints
2025-03-06 18:11:20,758 - INFO - training batch 1501, loss: 0.189, 48032/60000 datapoints
2025-03-06 18:11:20,953 - INFO - training batch 1551, loss: 0.167, 49632/60000 datapoints
2025-03-06 18:11:21,148 - INFO - training batch 1601, loss: 0.475, 51232/60000 datapoints
2025-03-06 18:11:21,342 - INFO - training batch 1651, loss: 0.330, 52832/60000 datapoints
2025-03-06 18:11:21,538 - INFO - training batch 1701, loss: 0.162, 54432/60000 datapoints
2025-03-06 18:11:21,734 - INFO - training batch 1751, loss: 0.228, 56032/60000 datapoints
2025-03-06 18:11:21,929 - INFO - training batch 1801, loss: 0.494, 57632/60000 datapoints
2025-03-06 18:11:22,124 - INFO - training batch 1851, loss: 0.439, 59232/60000 datapoints
2025-03-06 18:11:22,226 - INFO - validation batch 1, loss: 0.383, 32/10016 datapoints
2025-03-06 18:11:22,380 - INFO - validation batch 51, loss: 0.450, 1632/10016 datapoints
2025-03-06 18:11:22,534 - INFO - validation batch 101, loss: 0.369, 3232/10016 datapoints
2025-03-06 18:11:22,711 - INFO - validation batch 151, loss: 0.301, 4832/10016 datapoints
2025-03-06 18:11:22,865 - INFO - validation batch 201, loss: 0.198, 6432/10016 datapoints
2025-03-06 18:11:23,019 - INFO - validation batch 251, loss: 0.338, 8032/10016 datapoints
2025-03-06 18:11:23,174 - INFO - validation batch 301, loss: 0.213, 9632/10016 datapoints
2025-03-06 18:11:23,213 - INFO - Epoch 197/800 done.
2025-03-06 18:11:23,214 - INFO - Final validation performance:
Loss: 0.322, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 18:11:23,214 - INFO - Beginning epoch 198/800
2025-03-06 18:11:23,220 - INFO - training batch 1, loss: 0.260, 32/60000 datapoints
2025-03-06 18:11:23,415 - INFO - training batch 51, loss: 0.337, 1632/60000 datapoints
2025-03-06 18:11:23,614 - INFO - training batch 101, loss: 0.546, 3232/60000 datapoints
2025-03-06 18:11:23,808 - INFO - training batch 151, loss: 0.341, 4832/60000 datapoints
2025-03-06 18:11:24,005 - INFO - training batch 201, loss: 0.431, 6432/60000 datapoints
2025-03-06 18:11:24,202 - INFO - training batch 251, loss: 0.313, 8032/60000 datapoints
2025-03-06 18:11:24,394 - INFO - training batch 301, loss: 0.171, 9632/60000 datapoints
2025-03-06 18:11:24,609 - INFO - training batch 351, loss: 0.313, 11232/60000 datapoints
2025-03-06 18:11:24,815 - INFO - training batch 401, loss: 0.322, 12832/60000 datapoints
2025-03-06 18:11:25,011 - INFO - training batch 451, loss: 0.875, 14432/60000 datapoints
2025-03-06 18:11:25,209 - INFO - training batch 501, loss: 0.696, 16032/60000 datapoints
2025-03-06 18:11:25,403 - INFO - training batch 551, loss: 0.153, 17632/60000 datapoints
2025-03-06 18:11:25,605 - INFO - training batch 601, loss: 0.207, 19232/60000 datapoints
2025-03-06 18:11:25,798 - INFO - training batch 651, loss: 0.668, 20832/60000 datapoints
2025-03-06 18:11:25,990 - INFO - training batch 701, loss: 0.198, 22432/60000 datapoints
2025-03-06 18:11:26,185 - INFO - training batch 751, loss: 0.349, 24032/60000 datapoints
2025-03-06 18:11:26,377 - INFO - training batch 801, loss: 0.159, 25632/60000 datapoints
2025-03-06 18:11:26,569 - INFO - training batch 851, loss: 0.510, 27232/60000 datapoints
2025-03-06 18:11:26,767 - INFO - training batch 901, loss: 0.296, 28832/60000 datapoints
2025-03-06 18:11:26,962 - INFO - training batch 951, loss: 0.313, 30432/60000 datapoints
2025-03-06 18:11:27,154 - INFO - training batch 1001, loss: 0.258, 32032/60000 datapoints
2025-03-06 18:11:27,348 - INFO - training batch 1051, loss: 0.227, 33632/60000 datapoints
2025-03-06 18:11:27,544 - INFO - training batch 1101, loss: 0.374, 35232/60000 datapoints
2025-03-06 18:11:27,740 - INFO - training batch 1151, loss: 0.158, 36832/60000 datapoints
2025-03-06 18:11:27,933 - INFO - training batch 1201, loss: 0.291, 38432/60000 datapoints
2025-03-06 18:11:28,126 - INFO - training batch 1251, loss: 0.102, 40032/60000 datapoints
2025-03-06 18:11:28,332 - INFO - training batch 1301, loss: 0.363, 41632/60000 datapoints
2025-03-06 18:11:28,527 - INFO - training batch 1351, loss: 0.292, 43232/60000 datapoints
2025-03-06 18:11:28,723 - INFO - training batch 1401, loss: 0.470, 44832/60000 datapoints
2025-03-06 18:11:28,927 - INFO - training batch 1451, loss: 0.228, 46432/60000 datapoints
2025-03-06 18:11:29,120 - INFO - training batch 1501, loss: 0.247, 48032/60000 datapoints
2025-03-06 18:11:29,314 - INFO - training batch 1551, loss: 0.394, 49632/60000 datapoints
2025-03-06 18:11:29,506 - INFO - training batch 1601, loss: 0.190, 51232/60000 datapoints
2025-03-06 18:11:29,700 - INFO - training batch 1651, loss: 0.110, 52832/60000 datapoints
2025-03-06 18:11:29,894 - INFO - training batch 1701, loss: 0.388, 54432/60000 datapoints
2025-03-06 18:11:30,087 - INFO - training batch 1751, loss: 0.268, 56032/60000 datapoints
2025-03-06 18:11:30,282 - INFO - training batch 1801, loss: 0.460, 57632/60000 datapoints
2025-03-06 18:11:30,475 - INFO - training batch 1851, loss: 0.221, 59232/60000 datapoints
2025-03-06 18:11:30,574 - INFO - validation batch 1, loss: 0.249, 32/10016 datapoints
2025-03-06 18:11:30,726 - INFO - validation batch 51, loss: 0.247, 1632/10016 datapoints
2025-03-06 18:11:30,876 - INFO - validation batch 101, loss: 0.368, 3232/10016 datapoints
2025-03-06 18:11:31,026 - INFO - validation batch 151, loss: 0.364, 4832/10016 datapoints
2025-03-06 18:11:31,177 - INFO - validation batch 201, loss: 0.381, 6432/10016 datapoints
2025-03-06 18:11:31,330 - INFO - validation batch 251, loss: 0.446, 8032/10016 datapoints
2025-03-06 18:11:31,480 - INFO - validation batch 301, loss: 0.695, 9632/10016 datapoints
2025-03-06 18:11:31,516 - INFO - Epoch 198/800 done.
2025-03-06 18:11:31,516 - INFO - Final validation performance:
Loss: 0.393, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 18:11:31,516 - INFO - Beginning epoch 199/800
2025-03-06 18:11:31,523 - INFO - training batch 1, loss: 0.333, 32/60000 datapoints
2025-03-06 18:11:31,716 - INFO - training batch 51, loss: 0.495, 1632/60000 datapoints
2025-03-06 18:11:31,910 - INFO - training batch 101, loss: 0.488, 3232/60000 datapoints
2025-03-06 18:11:32,104 - INFO - training batch 151, loss: 0.352, 4832/60000 datapoints
2025-03-06 18:11:32,297 - INFO - training batch 201, loss: 0.371, 6432/60000 datapoints
2025-03-06 18:11:32,489 - INFO - training batch 251, loss: 0.182, 8032/60000 datapoints
2025-03-06 18:11:32,693 - INFO - training batch 301, loss: 0.490, 9632/60000 datapoints
2025-03-06 18:11:32,899 - INFO - training batch 351, loss: 0.320, 11232/60000 datapoints
2025-03-06 18:11:33,094 - INFO - training batch 401, loss: 0.441, 12832/60000 datapoints
2025-03-06 18:11:33,288 - INFO - training batch 451, loss: 0.240, 14432/60000 datapoints
2025-03-06 18:11:33,480 - INFO - training batch 501, loss: 0.322, 16032/60000 datapoints
2025-03-06 18:11:33,673 - INFO - training batch 551, loss: 0.205, 17632/60000 datapoints
2025-03-06 18:11:33,865 - INFO - training batch 601, loss: 0.308, 19232/60000 datapoints
2025-03-06 18:11:34,058 - INFO - training batch 651, loss: 0.781, 20832/60000 datapoints
2025-03-06 18:11:34,250 - INFO - training batch 701, loss: 0.222, 22432/60000 datapoints
2025-03-06 18:11:34,443 - INFO - training batch 751, loss: 0.326, 24032/60000 datapoints
2025-03-06 18:11:34,645 - INFO - training batch 801, loss: 0.388, 25632/60000 datapoints
2025-03-06 18:11:34,837 - INFO - training batch 851, loss: 0.427, 27232/60000 datapoints
2025-03-06 18:11:35,037 - INFO - training batch 901, loss: 0.323, 28832/60000 datapoints
2025-03-06 18:11:35,233 - INFO - training batch 951, loss: 0.290, 30432/60000 datapoints
2025-03-06 18:11:35,425 - INFO - training batch 1001, loss: 0.387, 32032/60000 datapoints
2025-03-06 18:11:35,619 - INFO - training batch 1051, loss: 0.285, 33632/60000 datapoints
2025-03-06 18:11:35,809 - INFO - training batch 1101, loss: 0.228, 35232/60000 datapoints
2025-03-06 18:11:36,000 - INFO - training batch 1151, loss: 0.676, 36832/60000 datapoints
2025-03-06 18:11:36,193 - INFO - training batch 1201, loss: 0.323, 38432/60000 datapoints
2025-03-06 18:11:36,387 - INFO - training batch 1251, loss: 0.415, 40032/60000 datapoints
2025-03-06 18:11:36,579 - INFO - training batch 1301, loss: 0.342, 41632/60000 datapoints
2025-03-06 18:11:36,772 - INFO - training batch 1351, loss: 0.412, 43232/60000 datapoints
2025-03-06 18:11:36,962 - INFO - training batch 1401, loss: 0.438, 44832/60000 datapoints
2025-03-06 18:11:37,154 - INFO - training batch 1451, loss: 0.197, 46432/60000 datapoints
2025-03-06 18:11:37,349 - INFO - training batch 1501, loss: 0.196, 48032/60000 datapoints
2025-03-06 18:11:37,543 - INFO - training batch 1551, loss: 0.828, 49632/60000 datapoints
2025-03-06 18:11:37,751 - INFO - training batch 1601, loss: 0.220, 51232/60000 datapoints
2025-03-06 18:11:37,943 - INFO - training batch 1651, loss: 0.251, 52832/60000 datapoints
2025-03-06 18:11:38,136 - INFO - training batch 1701, loss: 0.416, 54432/60000 datapoints
2025-03-06 18:11:38,329 - INFO - training batch 1751, loss: 0.461, 56032/60000 datapoints
2025-03-06 18:11:38,522 - INFO - training batch 1801, loss: 0.327, 57632/60000 datapoints
2025-03-06 18:11:38,721 - INFO - training batch 1851, loss: 0.277, 59232/60000 datapoints
2025-03-06 18:11:38,825 - INFO - validation batch 1, loss: 0.256, 32/10016 datapoints
2025-03-06 18:11:38,975 - INFO - validation batch 51, loss: 0.371, 1632/10016 datapoints
2025-03-06 18:11:39,131 - INFO - validation batch 101, loss: 0.388, 3232/10016 datapoints
2025-03-06 18:11:39,285 - INFO - validation batch 151, loss: 0.250, 4832/10016 datapoints
2025-03-06 18:11:39,440 - INFO - validation batch 201, loss: 0.428, 6432/10016 datapoints
2025-03-06 18:11:39,599 - INFO - validation batch 251, loss: 0.183, 8032/10016 datapoints
2025-03-06 18:11:39,754 - INFO - validation batch 301, loss: 0.503, 9632/10016 datapoints
2025-03-06 18:11:39,791 - INFO - Epoch 199/800 done.
2025-03-06 18:11:39,791 - INFO - Final validation performance:
Loss: 0.340, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 18:11:39,792 - INFO - Beginning epoch 200/800
2025-03-06 18:11:39,797 - INFO - training batch 1, loss: 0.312, 32/60000 datapoints
2025-03-06 18:11:39,996 - INFO - training batch 51, loss: 0.259, 1632/60000 datapoints
2025-03-06 18:11:40,201 - INFO - training batch 101, loss: 0.354, 3232/60000 datapoints
2025-03-06 18:11:40,402 - INFO - training batch 151, loss: 0.168, 4832/60000 datapoints
2025-03-06 18:11:40,604 - INFO - training batch 201, loss: 0.673, 6432/60000 datapoints
2025-03-06 18:11:40,798 - INFO - training batch 251, loss: 0.199, 8032/60000 datapoints
2025-03-06 18:11:40,993 - INFO - training batch 301, loss: 0.307, 9632/60000 datapoints
2025-03-06 18:11:41,188 - INFO - training batch 351, loss: 0.363, 11232/60000 datapoints
2025-03-06 18:11:41,387 - INFO - training batch 401, loss: 0.274, 12832/60000 datapoints
2025-03-06 18:11:41,583 - INFO - training batch 451, loss: 0.693, 14432/60000 datapoints
2025-03-06 18:11:41,781 - INFO - training batch 501, loss: 0.643, 16032/60000 datapoints
2025-03-06 18:11:41,976 - INFO - training batch 551, loss: 0.254, 17632/60000 datapoints
2025-03-06 18:11:42,170 - INFO - training batch 601, loss: 0.778, 19232/60000 datapoints
2025-03-06 18:11:42,363 - INFO - training batch 651, loss: 0.209, 20832/60000 datapoints
2025-03-06 18:11:42,558 - INFO - training batch 701, loss: 0.393, 22432/60000 datapoints
2025-03-06 18:11:42,762 - INFO - training batch 751, loss: 0.273, 24032/60000 datapoints
2025-03-06 18:11:42,971 - INFO - training batch 801, loss: 0.202, 25632/60000 datapoints
2025-03-06 18:11:43,170 - INFO - training batch 851, loss: 0.433, 27232/60000 datapoints
2025-03-06 18:11:43,370 - INFO - training batch 901, loss: 0.266, 28832/60000 datapoints
2025-03-06 18:11:43,566 - INFO - training batch 951, loss: 0.253, 30432/60000 datapoints
2025-03-06 18:11:43,763 - INFO - training batch 1001, loss: 0.173, 32032/60000 datapoints
2025-03-06 18:11:43,959 - INFO - training batch 1051, loss: 0.427, 33632/60000 datapoints
2025-03-06 18:11:44,157 - INFO - training batch 1101, loss: 0.394, 35232/60000 datapoints
2025-03-06 18:11:44,353 - INFO - training batch 1151, loss: 0.324, 36832/60000 datapoints
2025-03-06 18:11:44,547 - INFO - training batch 1201, loss: 0.885, 38432/60000 datapoints
2025-03-06 18:11:44,747 - INFO - training batch 1251, loss: 0.469, 40032/60000 datapoints
2025-03-06 18:11:44,948 - INFO - training batch 1301, loss: 0.203, 41632/60000 datapoints
2025-03-06 18:11:45,144 - INFO - training batch 1351, loss: 0.325, 43232/60000 datapoints
2025-03-06 18:11:45,342 - INFO - training batch 1401, loss: 0.298, 44832/60000 datapoints
2025-03-06 18:11:45,537 - INFO - training batch 1451, loss: 0.360, 46432/60000 datapoints
2025-03-06 18:11:45,736 - INFO - training batch 1501, loss: 0.420, 48032/60000 datapoints
2025-03-06 18:11:45,930 - INFO - training batch 1551, loss: 0.541, 49632/60000 datapoints
2025-03-06 18:11:46,126 - INFO - training batch 1601, loss: 0.243, 51232/60000 datapoints
2025-03-06 18:11:46,331 - INFO - training batch 1651, loss: 0.345, 52832/60000 datapoints
2025-03-06 18:11:46,556 - INFO - training batch 1701, loss: 0.422, 54432/60000 datapoints
2025-03-06 18:11:46,775 - INFO - training batch 1751, loss: 0.783, 56032/60000 datapoints
2025-03-06 18:11:46,973 - INFO - training batch 1801, loss: 0.200, 57632/60000 datapoints
2025-03-06 18:11:47,171 - INFO - training batch 1851, loss: 0.366, 59232/60000 datapoints
2025-03-06 18:11:47,275 - INFO - validation batch 1, loss: 0.217, 32/10016 datapoints
2025-03-06 18:11:47,430 - INFO - validation batch 51, loss: 0.309, 1632/10016 datapoints
2025-03-06 18:11:47,583 - INFO - validation batch 101, loss: 0.400, 3232/10016 datapoints
2025-03-06 18:11:47,744 - INFO - validation batch 151, loss: 0.222, 4832/10016 datapoints
2025-03-06 18:11:47,898 - INFO - validation batch 201, loss: 0.467, 6432/10016 datapoints
2025-03-06 18:11:48,049 - INFO - validation batch 251, loss: 0.782, 8032/10016 datapoints
2025-03-06 18:11:48,209 - INFO - validation batch 301, loss: 0.291, 9632/10016 datapoints
2025-03-06 18:11:48,245 - INFO - Epoch 200/800 done.
2025-03-06 18:11:48,245 - INFO - Final validation performance:
Loss: 0.384, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 18:11:48,246 - INFO - Beginning epoch 201/800
2025-03-06 18:11:48,252 - INFO - training batch 1, loss: 0.437, 32/60000 datapoints
2025-03-06 18:11:48,459 - INFO - training batch 51, loss: 0.291, 1632/60000 datapoints
2025-03-06 18:11:48,660 - INFO - training batch 101, loss: 0.359, 3232/60000 datapoints
2025-03-06 18:11:48,861 - INFO - training batch 151, loss: 0.687, 4832/60000 datapoints
2025-03-06 18:11:49,055 - INFO - training batch 201, loss: 0.176, 6432/60000 datapoints
2025-03-06 18:11:49,257 - INFO - training batch 251, loss: 0.214, 8032/60000 datapoints
2025-03-06 18:11:49,451 - INFO - training batch 301, loss: 0.411, 9632/60000 datapoints
2025-03-06 18:11:49,650 - INFO - training batch 351, loss: 0.308, 11232/60000 datapoints
2025-03-06 18:11:49,844 - INFO - training batch 401, loss: 0.436, 12832/60000 datapoints
2025-03-06 18:11:50,040 - INFO - training batch 451, loss: 0.265, 14432/60000 datapoints
2025-03-06 18:11:50,240 - INFO - training batch 501, loss: 0.347, 16032/60000 datapoints
2025-03-06 18:11:50,436 - INFO - training batch 551, loss: 0.240, 17632/60000 datapoints
2025-03-06 18:11:50,632 - INFO - training batch 601, loss: 0.450, 19232/60000 datapoints
2025-03-06 18:11:50,827 - INFO - training batch 651, loss: 0.450, 20832/60000 datapoints
2025-03-06 18:11:51,020 - INFO - training batch 701, loss: 0.838, 22432/60000 datapoints
2025-03-06 18:11:51,218 - INFO - training batch 751, loss: 0.446, 24032/60000 datapoints
2025-03-06 18:11:51,428 - INFO - training batch 801, loss: 0.362, 25632/60000 datapoints
2025-03-06 18:11:51,624 - INFO - training batch 851, loss: 0.218, 27232/60000 datapoints
2025-03-06 18:11:51,819 - INFO - training batch 901, loss: 0.446, 28832/60000 datapoints
2025-03-06 18:11:52,013 - INFO - training batch 951, loss: 0.271, 30432/60000 datapoints
2025-03-06 18:11:52,214 - INFO - training batch 1001, loss: 0.243, 32032/60000 datapoints
2025-03-06 18:11:52,424 - INFO - training batch 1051, loss: 0.706, 33632/60000 datapoints
2025-03-06 18:11:52,623 - INFO - training batch 1101, loss: 0.448, 35232/60000 datapoints
2025-03-06 18:11:52,818 - INFO - training batch 1151, loss: 0.328, 36832/60000 datapoints
2025-03-06 18:11:53,035 - INFO - training batch 1201, loss: 0.494, 38432/60000 datapoints
2025-03-06 18:11:53,231 - INFO - training batch 1251, loss: 0.305, 40032/60000 datapoints
2025-03-06 18:11:53,430 - INFO - training batch 1301, loss: 0.106, 41632/60000 datapoints
2025-03-06 18:11:53,627 - INFO - training batch 1351, loss: 0.406, 43232/60000 datapoints
2025-03-06 18:11:53,824 - INFO - training batch 1401, loss: 0.442, 44832/60000 datapoints
2025-03-06 18:11:54,020 - INFO - training batch 1451, loss: 0.412, 46432/60000 datapoints
2025-03-06 18:11:54,214 - INFO - training batch 1501, loss: 0.577, 48032/60000 datapoints
2025-03-06 18:11:54,409 - INFO - training batch 1551, loss: 0.225, 49632/60000 datapoints
2025-03-06 18:11:54,606 - INFO - training batch 1601, loss: 0.302, 51232/60000 datapoints
2025-03-06 18:11:54,801 - INFO - training batch 1651, loss: 0.210, 52832/60000 datapoints
2025-03-06 18:11:54,999 - INFO - training batch 1701, loss: 0.258, 54432/60000 datapoints
2025-03-06 18:11:55,193 - INFO - training batch 1751, loss: 0.473, 56032/60000 datapoints
2025-03-06 18:11:55,391 - INFO - training batch 1801, loss: 0.248, 57632/60000 datapoints
2025-03-06 18:11:55,585 - INFO - training batch 1851, loss: 0.382, 59232/60000 datapoints
2025-03-06 18:11:55,689 - INFO - validation batch 1, loss: 0.445, 32/10016 datapoints
2025-03-06 18:11:55,841 - INFO - validation batch 51, loss: 0.198, 1632/10016 datapoints
2025-03-06 18:11:55,994 - INFO - validation batch 101, loss: 0.163, 3232/10016 datapoints
2025-03-06 18:11:56,147 - INFO - validation batch 151, loss: 0.384, 4832/10016 datapoints
2025-03-06 18:11:56,302 - INFO - validation batch 201, loss: 0.364, 6432/10016 datapoints
2025-03-06 18:11:56,454 - INFO - validation batch 251, loss: 0.245, 8032/10016 datapoints
2025-03-06 18:11:56,609 - INFO - validation batch 301, loss: 0.495, 9632/10016 datapoints
2025-03-06 18:11:56,645 - INFO - Epoch 201/800 done.
2025-03-06 18:11:56,645 - INFO - Final validation performance:
Loss: 0.328, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 18:11:56,645 - INFO - Beginning epoch 202/800
2025-03-06 18:11:56,651 - INFO - training batch 1, loss: 0.420, 32/60000 datapoints
2025-03-06 18:11:56,850 - INFO - training batch 51, loss: 0.327, 1632/60000 datapoints
2025-03-06 18:11:57,047 - INFO - training batch 101, loss: 0.465, 3232/60000 datapoints
2025-03-06 18:11:57,243 - INFO - training batch 151, loss: 0.280, 4832/60000 datapoints
2025-03-06 18:11:57,441 - INFO - training batch 201, loss: 0.260, 6432/60000 datapoints
2025-03-06 18:11:57,640 - INFO - training batch 251, loss: 0.358, 8032/60000 datapoints
2025-03-06 18:11:57,837 - INFO - training batch 301, loss: 0.288, 9632/60000 datapoints
2025-03-06 18:11:58,032 - INFO - training batch 351, loss: 0.440, 11232/60000 datapoints
2025-03-06 18:11:58,227 - INFO - training batch 401, loss: 0.376, 12832/60000 datapoints
2025-03-06 18:11:58,425 - INFO - training batch 451, loss: 0.389, 14432/60000 datapoints
2025-03-06 18:11:58,623 - INFO - training batch 501, loss: 0.406, 16032/60000 datapoints
2025-03-06 18:11:58,820 - INFO - training batch 551, loss: 0.480, 17632/60000 datapoints
2025-03-06 18:11:59,016 - INFO - training batch 601, loss: 0.258, 19232/60000 datapoints
2025-03-06 18:11:59,216 - INFO - training batch 651, loss: 0.236, 20832/60000 datapoints
2025-03-06 18:11:59,414 - INFO - training batch 701, loss: 0.188, 22432/60000 datapoints
2025-03-06 18:11:59,609 - INFO - training batch 751, loss: 0.227, 24032/60000 datapoints
2025-03-06 18:11:59,805 - INFO - training batch 801, loss: 0.312, 25632/60000 datapoints
2025-03-06 18:11:59,999 - INFO - training batch 851, loss: 0.264, 27232/60000 datapoints
2025-03-06 18:12:00,196 - INFO - training batch 901, loss: 0.272, 28832/60000 datapoints
2025-03-06 18:12:00,396 - INFO - training batch 951, loss: 0.344, 30432/60000 datapoints
2025-03-06 18:12:00,593 - INFO - training batch 1001, loss: 0.372, 32032/60000 datapoints
2025-03-06 18:12:00,791 - INFO - training batch 1051, loss: 0.557, 33632/60000 datapoints
2025-03-06 18:12:00,985 - INFO - training batch 1101, loss: 0.289, 35232/60000 datapoints
2025-03-06 18:12:01,179 - INFO - training batch 1151, loss: 0.518, 36832/60000 datapoints
2025-03-06 18:12:01,378 - INFO - training batch 1201, loss: 0.265, 38432/60000 datapoints
2025-03-06 18:12:01,573 - INFO - training batch 1251, loss: 0.199, 40032/60000 datapoints
2025-03-06 18:12:01,771 - INFO - training batch 1301, loss: 0.480, 41632/60000 datapoints
2025-03-06 18:12:01,964 - INFO - training batch 1351, loss: 0.440, 43232/60000 datapoints
2025-03-06 18:12:02,159 - INFO - training batch 1401, loss: 0.391, 44832/60000 datapoints
2025-03-06 18:12:02,356 - INFO - training batch 1451, loss: 0.329, 46432/60000 datapoints
2025-03-06 18:12:02,549 - INFO - training batch 1501, loss: 0.207, 48032/60000 datapoints
2025-03-06 18:12:02,746 - INFO - training batch 1551, loss: 0.453, 49632/60000 datapoints
2025-03-06 18:12:02,944 - INFO - training batch 1601, loss: 0.615, 51232/60000 datapoints
2025-03-06 18:12:03,152 - INFO - training batch 1651, loss: 0.498, 52832/60000 datapoints
2025-03-06 18:12:03,352 - INFO - training batch 1701, loss: 0.381, 54432/60000 datapoints
2025-03-06 18:12:03,549 - INFO - training batch 1751, loss: 0.384, 56032/60000 datapoints
2025-03-06 18:12:03,747 - INFO - training batch 1801, loss: 0.359, 57632/60000 datapoints
2025-03-06 18:12:03,942 - INFO - training batch 1851, loss: 0.223, 59232/60000 datapoints
2025-03-06 18:12:04,043 - INFO - validation batch 1, loss: 0.168, 32/10016 datapoints
2025-03-06 18:12:04,202 - INFO - validation batch 51, loss: 0.350, 1632/10016 datapoints
2025-03-06 18:12:04,365 - INFO - validation batch 101, loss: 0.146, 3232/10016 datapoints
2025-03-06 18:12:04,519 - INFO - validation batch 151, loss: 0.233, 4832/10016 datapoints
2025-03-06 18:12:04,678 - INFO - validation batch 201, loss: 0.310, 6432/10016 datapoints
2025-03-06 18:12:04,834 - INFO - validation batch 251, loss: 0.224, 8032/10016 datapoints
2025-03-06 18:12:04,990 - INFO - validation batch 301, loss: 0.440, 9632/10016 datapoints
2025-03-06 18:12:05,027 - INFO - Epoch 202/800 done.
2025-03-06 18:12:05,027 - INFO - Final validation performance:
Loss: 0.267, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 18:12:05,028 - INFO - Beginning epoch 203/800
2025-03-06 18:12:05,034 - INFO - training batch 1, loss: 0.571, 32/60000 datapoints
2025-03-06 18:12:05,227 - INFO - training batch 51, loss: 0.309, 1632/60000 datapoints
2025-03-06 18:12:05,426 - INFO - training batch 101, loss: 0.524, 3232/60000 datapoints
2025-03-06 18:12:05,624 - INFO - training batch 151, loss: 0.286, 4832/60000 datapoints
2025-03-06 18:12:05,823 - INFO - training batch 201, loss: 0.261, 6432/60000 datapoints
2025-03-06 18:12:06,020 - INFO - training batch 251, loss: 0.289, 8032/60000 datapoints
2025-03-06 18:12:06,221 - INFO - training batch 301, loss: 0.309, 9632/60000 datapoints
2025-03-06 18:12:06,419 - INFO - training batch 351, loss: 0.300, 11232/60000 datapoints
2025-03-06 18:12:06,616 - INFO - training batch 401, loss: 0.190, 12832/60000 datapoints
2025-03-06 18:12:06,811 - INFO - training batch 451, loss: 0.218, 14432/60000 datapoints
2025-03-06 18:12:07,007 - INFO - training batch 501, loss: 0.284, 16032/60000 datapoints
2025-03-06 18:12:07,202 - INFO - training batch 551, loss: 0.371, 17632/60000 datapoints
2025-03-06 18:12:07,402 - INFO - training batch 601, loss: 0.160, 19232/60000 datapoints
2025-03-06 18:12:07,600 - INFO - training batch 651, loss: 0.329, 20832/60000 datapoints
2025-03-06 18:12:07,796 - INFO - training batch 701, loss: 0.405, 22432/60000 datapoints
2025-03-06 18:12:07,992 - INFO - training batch 751, loss: 0.394, 24032/60000 datapoints
2025-03-06 18:12:08,194 - INFO - training batch 801, loss: 0.329, 25632/60000 datapoints
2025-03-06 18:12:08,389 - INFO - training batch 851, loss: 0.307, 27232/60000 datapoints
2025-03-06 18:12:08,582 - INFO - training batch 901, loss: 0.446, 28832/60000 datapoints
2025-03-06 18:12:08,778 - INFO - training batch 951, loss: 0.593, 30432/60000 datapoints
2025-03-06 18:12:08,980 - INFO - training batch 1001, loss: 0.437, 32032/60000 datapoints
2025-03-06 18:12:09,179 - INFO - training batch 1051, loss: 0.583, 33632/60000 datapoints
2025-03-06 18:12:09,380 - INFO - training batch 1101, loss: 0.411, 35232/60000 datapoints
2025-03-06 18:12:09,573 - INFO - training batch 1151, loss: 0.479, 36832/60000 datapoints
2025-03-06 18:12:09,776 - INFO - training batch 1201, loss: 0.330, 38432/60000 datapoints
2025-03-06 18:12:09,996 - INFO - training batch 1251, loss: 0.198, 40032/60000 datapoints
2025-03-06 18:12:10,214 - INFO - training batch 1301, loss: 0.261, 41632/60000 datapoints
2025-03-06 18:12:10,438 - INFO - training batch 1351, loss: 0.414, 43232/60000 datapoints
2025-03-06 18:12:10,638 - INFO - training batch 1401, loss: 0.284, 44832/60000 datapoints
2025-03-06 18:12:10,835 - INFO - training batch 1451, loss: 0.220, 46432/60000 datapoints
2025-03-06 18:12:11,031 - INFO - training batch 1501, loss: 0.249, 48032/60000 datapoints
2025-03-06 18:12:11,241 - INFO - training batch 1551, loss: 0.290, 49632/60000 datapoints
2025-03-06 18:12:11,555 - INFO - training batch 1601, loss: 0.296, 51232/60000 datapoints
2025-03-06 18:12:11,939 - INFO - training batch 1651, loss: 0.321, 52832/60000 datapoints
2025-03-06 18:12:12,221 - INFO - training batch 1701, loss: 0.270, 54432/60000 datapoints
2025-03-06 18:12:12,447 - INFO - training batch 1751, loss: 0.218, 56032/60000 datapoints
2025-03-06 18:12:12,671 - INFO - training batch 1801, loss: 0.360, 57632/60000 datapoints
2025-03-06 18:12:12,892 - INFO - training batch 1851, loss: 0.396, 59232/60000 datapoints
2025-03-06 18:12:13,001 - INFO - validation batch 1, loss: 0.189, 32/10016 datapoints
2025-03-06 18:12:13,188 - INFO - validation batch 51, loss: 0.099, 1632/10016 datapoints
2025-03-06 18:12:13,357 - INFO - validation batch 101, loss: 0.253, 3232/10016 datapoints
2025-03-06 18:12:13,523 - INFO - validation batch 151, loss: 0.120, 4832/10016 datapoints
2025-03-06 18:12:13,694 - INFO - validation batch 201, loss: 0.405, 6432/10016 datapoints
2025-03-06 18:12:13,862 - INFO - validation batch 251, loss: 0.256, 8032/10016 datapoints
2025-03-06 18:12:14,027 - INFO - validation batch 301, loss: 0.351, 9632/10016 datapoints
2025-03-06 18:12:14,067 - INFO - Epoch 203/800 done.
2025-03-06 18:12:14,067 - INFO - Final validation performance:
Loss: 0.239, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 18:12:14,067 - INFO - Beginning epoch 204/800
2025-03-06 18:12:14,073 - INFO - training batch 1, loss: 0.335, 32/60000 datapoints
2025-03-06 18:12:14,275 - INFO - training batch 51, loss: 0.259, 1632/60000 datapoints
2025-03-06 18:12:14,478 - INFO - training batch 101, loss: 0.414, 3232/60000 datapoints
2025-03-06 18:12:14,683 - INFO - training batch 151, loss: 0.257, 4832/60000 datapoints
2025-03-06 18:12:14,888 - INFO - training batch 201, loss: 0.429, 6432/60000 datapoints
2025-03-06 18:12:15,090 - INFO - training batch 251, loss: 0.278, 8032/60000 datapoints
2025-03-06 18:12:15,291 - INFO - training batch 301, loss: 0.444, 9632/60000 datapoints
2025-03-06 18:12:15,494 - INFO - training batch 351, loss: 0.451, 11232/60000 datapoints
2025-03-06 18:12:15,692 - INFO - training batch 401, loss: 0.265, 12832/60000 datapoints
2025-03-06 18:12:15,889 - INFO - training batch 451, loss: 0.574, 14432/60000 datapoints
2025-03-06 18:12:16,090 - INFO - training batch 501, loss: 0.276, 16032/60000 datapoints
2025-03-06 18:12:16,290 - INFO - training batch 551, loss: 0.272, 17632/60000 datapoints
2025-03-06 18:12:16,485 - INFO - training batch 601, loss: 0.817, 19232/60000 datapoints
2025-03-06 18:12:16,683 - INFO - training batch 651, loss: 0.428, 20832/60000 datapoints
2025-03-06 18:12:16,879 - INFO - training batch 701, loss: 0.306, 22432/60000 datapoints
2025-03-06 18:12:17,076 - INFO - training batch 751, loss: 0.189, 24032/60000 datapoints
2025-03-06 18:12:17,275 - INFO - training batch 801, loss: 0.411, 25632/60000 datapoints
2025-03-06 18:12:17,475 - INFO - training batch 851, loss: 0.345, 27232/60000 datapoints
2025-03-06 18:12:17,675 - INFO - training batch 901, loss: 0.277, 28832/60000 datapoints
2025-03-06 18:12:17,875 - INFO - training batch 951, loss: 0.245, 30432/60000 datapoints
2025-03-06 18:12:18,074 - INFO - training batch 1001, loss: 0.620, 32032/60000 datapoints
2025-03-06 18:12:18,282 - INFO - training batch 1051, loss: 0.347, 33632/60000 datapoints
2025-03-06 18:12:18,489 - INFO - training batch 1101, loss: 0.239, 35232/60000 datapoints
2025-03-06 18:12:18,698 - INFO - training batch 1151, loss: 0.415, 36832/60000 datapoints
2025-03-06 18:12:18,917 - INFO - training batch 1201, loss: 0.225, 38432/60000 datapoints
2025-03-06 18:12:19,129 - INFO - training batch 1251, loss: 0.216, 40032/60000 datapoints
2025-03-06 18:12:19,336 - INFO - training batch 1301, loss: 0.235, 41632/60000 datapoints
2025-03-06 18:12:19,543 - INFO - training batch 1351, loss: 0.331, 43232/60000 datapoints
2025-03-06 18:12:19,746 - INFO - training batch 1401, loss: 0.626, 44832/60000 datapoints
2025-03-06 18:12:19,943 - INFO - training batch 1451, loss: 0.274, 46432/60000 datapoints
2025-03-06 18:12:20,139 - INFO - training batch 1501, loss: 0.723, 48032/60000 datapoints
2025-03-06 18:12:20,335 - INFO - training batch 1551, loss: 0.217, 49632/60000 datapoints
2025-03-06 18:12:20,532 - INFO - training batch 1601, loss: 0.480, 51232/60000 datapoints
2025-03-06 18:12:20,733 - INFO - training batch 1651, loss: 0.381, 52832/60000 datapoints
2025-03-06 18:12:20,931 - INFO - training batch 1701, loss: 0.470, 54432/60000 datapoints
2025-03-06 18:12:21,128 - INFO - training batch 1751, loss: 0.233, 56032/60000 datapoints
2025-03-06 18:12:21,324 - INFO - training batch 1801, loss: 0.422, 57632/60000 datapoints
2025-03-06 18:12:21,528 - INFO - training batch 1851, loss: 0.220, 59232/60000 datapoints
2025-03-06 18:12:21,633 - INFO - validation batch 1, loss: 0.711, 32/10016 datapoints
2025-03-06 18:12:21,788 - INFO - validation batch 51, loss: 0.359, 1632/10016 datapoints
2025-03-06 18:12:21,942 - INFO - validation batch 101, loss: 0.231, 3232/10016 datapoints
2025-03-06 18:12:22,096 - INFO - validation batch 151, loss: 0.176, 4832/10016 datapoints
2025-03-06 18:12:22,251 - INFO - validation batch 201, loss: 0.257, 6432/10016 datapoints
2025-03-06 18:12:22,406 - INFO - validation batch 251, loss: 0.327, 8032/10016 datapoints
2025-03-06 18:12:22,562 - INFO - validation batch 301, loss: 0.381, 9632/10016 datapoints
2025-03-06 18:12:22,603 - INFO - Epoch 204/800 done.
2025-03-06 18:12:22,603 - INFO - Final validation performance:
Loss: 0.349, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 18:12:22,604 - INFO - Beginning epoch 205/800
2025-03-06 18:12:22,610 - INFO - training batch 1, loss: 0.295, 32/60000 datapoints
2025-03-06 18:12:22,810 - INFO - training batch 51, loss: 0.458, 1632/60000 datapoints
2025-03-06 18:12:23,008 - INFO - training batch 101, loss: 0.332, 3232/60000 datapoints
2025-03-06 18:12:23,226 - INFO - training batch 151, loss: 0.415, 4832/60000 datapoints
2025-03-06 18:12:23,428 - INFO - training batch 201, loss: 0.162, 6432/60000 datapoints
2025-03-06 18:12:23,631 - INFO - training batch 251, loss: 0.575, 8032/60000 datapoints
2025-03-06 18:12:23,830 - INFO - training batch 301, loss: 0.538, 9632/60000 datapoints
2025-03-06 18:12:24,031 - INFO - training batch 351, loss: 0.222, 11232/60000 datapoints
2025-03-06 18:12:24,231 - INFO - training batch 401, loss: 0.390, 12832/60000 datapoints
2025-03-06 18:12:24,429 - INFO - training batch 451, loss: 0.202, 14432/60000 datapoints
2025-03-06 18:12:24,627 - INFO - training batch 501, loss: 0.319, 16032/60000 datapoints
2025-03-06 18:12:24,831 - INFO - training batch 551, loss: 0.192, 17632/60000 datapoints
2025-03-06 18:12:25,031 - INFO - training batch 601, loss: 0.447, 19232/60000 datapoints
2025-03-06 18:12:25,229 - INFO - training batch 651, loss: 0.292, 20832/60000 datapoints
2025-03-06 18:12:25,428 - INFO - training batch 701, loss: 0.406, 22432/60000 datapoints
2025-03-06 18:12:25,630 - INFO - training batch 751, loss: 0.759, 24032/60000 datapoints
2025-03-06 18:12:25,826 - INFO - training batch 801, loss: 0.412, 25632/60000 datapoints
2025-03-06 18:12:26,028 - INFO - training batch 851, loss: 0.259, 27232/60000 datapoints
2025-03-06 18:12:26,227 - INFO - training batch 901, loss: 0.218, 28832/60000 datapoints
2025-03-06 18:12:26,424 - INFO - training batch 951, loss: 0.665, 30432/60000 datapoints
2025-03-06 18:12:26,622 - INFO - training batch 1001, loss: 0.222, 32032/60000 datapoints
2025-03-06 18:12:26,818 - INFO - training batch 1051, loss: 0.373, 33632/60000 datapoints
2025-03-06 18:12:27,037 - INFO - training batch 1101, loss: 0.292, 35232/60000 datapoints
2025-03-06 18:12:27,248 - INFO - training batch 1151, loss: 0.481, 36832/60000 datapoints
2025-03-06 18:12:27,446 - INFO - training batch 1201, loss: 0.206, 38432/60000 datapoints
2025-03-06 18:12:27,645 - INFO - training batch 1251, loss: 0.437, 40032/60000 datapoints
2025-03-06 18:12:27,842 - INFO - training batch 1301, loss: 0.140, 41632/60000 datapoints
2025-03-06 18:12:28,044 - INFO - training batch 1351, loss: 0.421, 43232/60000 datapoints
2025-03-06 18:12:28,244 - INFO - training batch 1401, loss: 0.289, 44832/60000 datapoints
2025-03-06 18:12:28,439 - INFO - training batch 1451, loss: 0.425, 46432/60000 datapoints
2025-03-06 18:12:28,639 - INFO - training batch 1501, loss: 0.123, 48032/60000 datapoints
2025-03-06 18:12:28,838 - INFO - training batch 1551, loss: 0.435, 49632/60000 datapoints
2025-03-06 18:12:29,035 - INFO - training batch 1601, loss: 0.397, 51232/60000 datapoints
2025-03-06 18:12:29,232 - INFO - training batch 1651, loss: 0.440, 52832/60000 datapoints
2025-03-06 18:12:29,432 - INFO - training batch 1701, loss: 0.459, 54432/60000 datapoints
2025-03-06 18:12:29,630 - INFO - training batch 1751, loss: 0.284, 56032/60000 datapoints
2025-03-06 18:12:29,827 - INFO - training batch 1801, loss: 0.248, 57632/60000 datapoints
2025-03-06 18:12:30,028 - INFO - training batch 1851, loss: 0.390, 59232/60000 datapoints
2025-03-06 18:12:30,130 - INFO - validation batch 1, loss: 0.399, 32/10016 datapoints
2025-03-06 18:12:30,285 - INFO - validation batch 51, loss: 0.592, 1632/10016 datapoints
2025-03-06 18:12:30,440 - INFO - validation batch 101, loss: 0.109, 3232/10016 datapoints
2025-03-06 18:12:30,598 - INFO - validation batch 151, loss: 0.201, 4832/10016 datapoints
2025-03-06 18:12:30,753 - INFO - validation batch 201, loss: 0.229, 6432/10016 datapoints
2025-03-06 18:12:30,911 - INFO - validation batch 251, loss: 0.375, 8032/10016 datapoints
2025-03-06 18:12:31,066 - INFO - validation batch 301, loss: 0.323, 9632/10016 datapoints
2025-03-06 18:12:31,104 - INFO - Epoch 205/800 done.
2025-03-06 18:12:31,104 - INFO - Final validation performance:
Loss: 0.318, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 18:12:31,105 - INFO - Beginning epoch 206/800
2025-03-06 18:12:31,110 - INFO - training batch 1, loss: 0.238, 32/60000 datapoints
2025-03-06 18:12:31,315 - INFO - training batch 51, loss: 0.505, 1632/60000 datapoints
2025-03-06 18:12:31,514 - INFO - training batch 101, loss: 0.643, 3232/60000 datapoints
2025-03-06 18:12:31,731 - INFO - training batch 151, loss: 0.270, 4832/60000 datapoints
2025-03-06 18:12:31,961 - INFO - training batch 201, loss: 0.179, 6432/60000 datapoints
2025-03-06 18:12:32,158 - INFO - training batch 251, loss: 0.285, 8032/60000 datapoints
2025-03-06 18:12:32,357 - INFO - training batch 301, loss: 0.134, 9632/60000 datapoints
2025-03-06 18:12:32,575 - INFO - training batch 351, loss: 0.213, 11232/60000 datapoints
2025-03-06 18:12:32,775 - INFO - training batch 401, loss: 0.618, 12832/60000 datapoints
2025-03-06 18:12:32,972 - INFO - training batch 451, loss: 0.350, 14432/60000 datapoints
2025-03-06 18:12:33,179 - INFO - training batch 501, loss: 0.347, 16032/60000 datapoints
2025-03-06 18:12:33,396 - INFO - training batch 551, loss: 0.300, 17632/60000 datapoints
2025-03-06 18:12:33,591 - INFO - training batch 601, loss: 0.293, 19232/60000 datapoints
2025-03-06 18:12:33,791 - INFO - training batch 651, loss: 0.337, 20832/60000 datapoints
2025-03-06 18:12:33,986 - INFO - training batch 701, loss: 0.407, 22432/60000 datapoints
2025-03-06 18:12:34,181 - INFO - training batch 751, loss: 0.434, 24032/60000 datapoints
2025-03-06 18:12:34,382 - INFO - training batch 801, loss: 0.475, 25632/60000 datapoints
2025-03-06 18:12:34,578 - INFO - training batch 851, loss: 0.331, 27232/60000 datapoints
2025-03-06 18:12:34,781 - INFO - training batch 901, loss: 0.377, 28832/60000 datapoints
2025-03-06 18:12:34,986 - INFO - training batch 951, loss: 0.495, 30432/60000 datapoints
2025-03-06 18:12:35,187 - INFO - training batch 1001, loss: 0.260, 32032/60000 datapoints
2025-03-06 18:12:35,386 - INFO - training batch 1051, loss: 0.184, 33632/60000 datapoints
2025-03-06 18:12:35,583 - INFO - training batch 1101, loss: 0.231, 35232/60000 datapoints
2025-03-06 18:12:35,784 - INFO - training batch 1151, loss: 0.233, 36832/60000 datapoints
2025-03-06 18:12:35,979 - INFO - training batch 1201, loss: 0.314, 38432/60000 datapoints
2025-03-06 18:12:36,179 - INFO - training batch 1251, loss: 0.433, 40032/60000 datapoints
2025-03-06 18:12:36,378 - INFO - training batch 1301, loss: 0.133, 41632/60000 datapoints
2025-03-06 18:12:36,576 - INFO - training batch 1351, loss: 0.287, 43232/60000 datapoints
2025-03-06 18:12:36,777 - INFO - training batch 1401, loss: 0.273, 44832/60000 datapoints
2025-03-06 18:12:36,976 - INFO - training batch 1451, loss: 0.464, 46432/60000 datapoints
2025-03-06 18:12:37,172 - INFO - training batch 1501, loss: 0.304, 48032/60000 datapoints
2025-03-06 18:12:37,369 - INFO - training batch 1551, loss: 0.275, 49632/60000 datapoints
2025-03-06 18:12:37,567 - INFO - training batch 1601, loss: 0.244, 51232/60000 datapoints
2025-03-06 18:12:37,782 - INFO - training batch 1651, loss: 0.445, 52832/60000 datapoints
2025-03-06 18:12:37,979 - INFO - training batch 1701, loss: 0.235, 54432/60000 datapoints
2025-03-06 18:12:38,176 - INFO - training batch 1751, loss: 0.501, 56032/60000 datapoints
2025-03-06 18:12:38,380 - INFO - training batch 1801, loss: 0.212, 57632/60000 datapoints
2025-03-06 18:12:38,578 - INFO - training batch 1851, loss: 0.325, 59232/60000 datapoints
2025-03-06 18:12:38,684 - INFO - validation batch 1, loss: 0.204, 32/10016 datapoints
2025-03-06 18:12:38,844 - INFO - validation batch 51, loss: 0.233, 1632/10016 datapoints
2025-03-06 18:12:39,007 - INFO - validation batch 101, loss: 0.667, 3232/10016 datapoints
2025-03-06 18:12:39,165 - INFO - validation batch 151, loss: 0.257, 4832/10016 datapoints
2025-03-06 18:12:39,327 - INFO - validation batch 201, loss: 0.272, 6432/10016 datapoints
2025-03-06 18:12:39,487 - INFO - validation batch 251, loss: 0.383, 8032/10016 datapoints
2025-03-06 18:12:39,646 - INFO - validation batch 301, loss: 0.201, 9632/10016 datapoints
2025-03-06 18:12:39,686 - INFO - Epoch 206/800 done.
2025-03-06 18:12:39,686 - INFO - Final validation performance:
Loss: 0.317, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 18:12:39,686 - INFO - Beginning epoch 207/800
2025-03-06 18:12:39,692 - INFO - training batch 1, loss: 0.234, 32/60000 datapoints
2025-03-06 18:12:39,892 - INFO - training batch 51, loss: 0.373, 1632/60000 datapoints
2025-03-06 18:12:40,090 - INFO - training batch 101, loss: 0.814, 3232/60000 datapoints
2025-03-06 18:12:40,290 - INFO - training batch 151, loss: 0.589, 4832/60000 datapoints
2025-03-06 18:12:40,487 - INFO - training batch 201, loss: 0.482, 6432/60000 datapoints
2025-03-06 18:12:40,686 - INFO - training batch 251, loss: 0.570, 8032/60000 datapoints
2025-03-06 18:12:40,884 - INFO - training batch 301, loss: 0.244, 9632/60000 datapoints
2025-03-06 18:12:41,078 - INFO - training batch 351, loss: 0.287, 11232/60000 datapoints
2025-03-06 18:12:41,274 - INFO - training batch 401, loss: 0.339, 12832/60000 datapoints
2025-03-06 18:12:41,475 - INFO - training batch 451, loss: 0.158, 14432/60000 datapoints
2025-03-06 18:12:41,672 - INFO - training batch 501, loss: 0.281, 16032/60000 datapoints
2025-03-06 18:12:41,875 - INFO - training batch 551, loss: 0.267, 17632/60000 datapoints
2025-03-06 18:12:42,072 - INFO - training batch 601, loss: 0.312, 19232/60000 datapoints
2025-03-06 18:12:42,272 - INFO - training batch 651, loss: 0.327, 20832/60000 datapoints
2025-03-06 18:12:42,481 - INFO - training batch 701, loss: 0.275, 22432/60000 datapoints
2025-03-06 18:12:42,688 - INFO - training batch 751, loss: 0.477, 24032/60000 datapoints
2025-03-06 18:12:42,887 - INFO - training batch 801, loss: 0.346, 25632/60000 datapoints
2025-03-06 18:12:43,083 - INFO - training batch 851, loss: 1.001, 27232/60000 datapoints
2025-03-06 18:12:43,279 - INFO - training batch 901, loss: 0.260, 28832/60000 datapoints
2025-03-06 18:12:43,497 - INFO - training batch 951, loss: 0.546, 30432/60000 datapoints
2025-03-06 18:12:43,699 - INFO - training batch 1001, loss: 0.561, 32032/60000 datapoints
2025-03-06 18:12:43,897 - INFO - training batch 1051, loss: 0.450, 33632/60000 datapoints
2025-03-06 18:12:44,109 - INFO - training batch 1101, loss: 0.394, 35232/60000 datapoints
2025-03-06 18:12:44,305 - INFO - training batch 1151, loss: 0.248, 36832/60000 datapoints
2025-03-06 18:12:44,503 - INFO - training batch 1201, loss: 0.202, 38432/60000 datapoints
2025-03-06 18:12:44,700 - INFO - training batch 1251, loss: 0.454, 40032/60000 datapoints
2025-03-06 18:12:44,905 - INFO - training batch 1301, loss: 0.432, 41632/60000 datapoints
2025-03-06 18:12:45,105 - INFO - training batch 1351, loss: 0.421, 43232/60000 datapoints
2025-03-06 18:12:45,302 - INFO - training batch 1401, loss: 0.522, 44832/60000 datapoints
2025-03-06 18:12:45,503 - INFO - training batch 1451, loss: 0.333, 46432/60000 datapoints
2025-03-06 18:12:45,704 - INFO - training batch 1501, loss: 0.544, 48032/60000 datapoints
2025-03-06 18:12:45,907 - INFO - training batch 1551, loss: 0.428, 49632/60000 datapoints
2025-03-06 18:12:46,103 - INFO - training batch 1601, loss: 0.352, 51232/60000 datapoints
2025-03-06 18:12:46,301 - INFO - training batch 1651, loss: 0.291, 52832/60000 datapoints
2025-03-06 18:12:46,498 - INFO - training batch 1701, loss: 0.307, 54432/60000 datapoints
2025-03-06 18:12:46,696 - INFO - training batch 1751, loss: 0.327, 56032/60000 datapoints
2025-03-06 18:12:46,892 - INFO - training batch 1801, loss: 0.263, 57632/60000 datapoints
2025-03-06 18:12:47,092 - INFO - training batch 1851, loss: 0.424, 59232/60000 datapoints
2025-03-06 18:12:47,196 - INFO - validation batch 1, loss: 0.247, 32/10016 datapoints
2025-03-06 18:12:47,356 - INFO - validation batch 51, loss: 0.278, 1632/10016 datapoints
2025-03-06 18:12:47,513 - INFO - validation batch 101, loss: 0.439, 3232/10016 datapoints
2025-03-06 18:12:47,669 - INFO - validation batch 151, loss: 0.348, 4832/10016 datapoints
2025-03-06 18:12:47,824 - INFO - validation batch 201, loss: 0.302, 6432/10016 datapoints
2025-03-06 18:12:47,979 - INFO - validation batch 251, loss: 0.430, 8032/10016 datapoints
2025-03-06 18:12:48,133 - INFO - validation batch 301, loss: 0.519, 9632/10016 datapoints
2025-03-06 18:12:48,171 - INFO - Epoch 207/800 done.
2025-03-06 18:12:48,171 - INFO - Final validation performance:
Loss: 0.366, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 18:12:48,171 - INFO - Beginning epoch 208/800
2025-03-06 18:12:48,177 - INFO - training batch 1, loss: 0.491, 32/60000 datapoints
2025-03-06 18:12:48,378 - INFO - training batch 51, loss: 0.466, 1632/60000 datapoints
2025-03-06 18:12:48,574 - INFO - training batch 101, loss: 0.669, 3232/60000 datapoints
2025-03-06 18:12:48,770 - INFO - training batch 151, loss: 0.196, 4832/60000 datapoints
2025-03-06 18:12:48,966 - INFO - training batch 201, loss: 0.332, 6432/60000 datapoints
2025-03-06 18:12:49,166 - INFO - training batch 251, loss: 0.474, 8032/60000 datapoints
2025-03-06 18:12:49,362 - INFO - training batch 301, loss: 0.548, 9632/60000 datapoints
2025-03-06 18:12:49,561 - INFO - training batch 351, loss: 0.389, 11232/60000 datapoints
2025-03-06 18:12:49,761 - INFO - training batch 401, loss: 0.784, 12832/60000 datapoints
2025-03-06 18:12:49,958 - INFO - training batch 451, loss: 0.242, 14432/60000 datapoints
2025-03-06 18:12:50,153 - INFO - training batch 501, loss: 0.392, 16032/60000 datapoints
2025-03-06 18:12:50,350 - INFO - training batch 551, loss: 0.486, 17632/60000 datapoints
2025-03-06 18:12:50,548 - INFO - training batch 601, loss: 0.222, 19232/60000 datapoints
2025-03-06 18:12:50,746 - INFO - training batch 651, loss: 0.493, 20832/60000 datapoints
2025-03-06 18:12:50,943 - INFO - training batch 701, loss: 0.704, 22432/60000 datapoints
2025-03-06 18:12:51,137 - INFO - training batch 751, loss: 0.334, 24032/60000 datapoints
2025-03-06 18:12:51,332 - INFO - training batch 801, loss: 0.464, 25632/60000 datapoints
2025-03-06 18:12:51,533 - INFO - training batch 851, loss: 0.296, 27232/60000 datapoints
2025-03-06 18:12:51,733 - INFO - training batch 901, loss: 0.300, 28832/60000 datapoints
2025-03-06 18:12:51,932 - INFO - training batch 951, loss: 0.370, 30432/60000 datapoints
2025-03-06 18:12:52,128 - INFO - training batch 1001, loss: 0.367, 32032/60000 datapoints
2025-03-06 18:12:52,326 - INFO - training batch 1051, loss: 0.542, 33632/60000 datapoints
2025-03-06 18:12:52,523 - INFO - training batch 1101, loss: 0.452, 35232/60000 datapoints
2025-03-06 18:12:52,721 - INFO - training batch 1151, loss: 0.370, 36832/60000 datapoints
2025-03-06 18:12:52,919 - INFO - training batch 1201, loss: 0.413, 38432/60000 datapoints
2025-03-06 18:12:53,112 - INFO - training batch 1251, loss: 0.160, 40032/60000 datapoints
2025-03-06 18:12:53,308 - INFO - training batch 1301, loss: 0.648, 41632/60000 datapoints
2025-03-06 18:12:53,528 - INFO - training batch 1351, loss: 0.215, 43232/60000 datapoints
2025-03-06 18:12:53,725 - INFO - training batch 1401, loss: 0.487, 44832/60000 datapoints
2025-03-06 18:12:53,926 - INFO - training batch 1451, loss: 0.357, 46432/60000 datapoints
2025-03-06 18:12:54,125 - INFO - training batch 1501, loss: 0.175, 48032/60000 datapoints
2025-03-06 18:12:54,346 - INFO - training batch 1551, loss: 0.328, 49632/60000 datapoints
2025-03-06 18:12:54,557 - INFO - training batch 1601, loss: 0.531, 51232/60000 datapoints
2025-03-06 18:12:54,756 - INFO - training batch 1651, loss: 0.332, 52832/60000 datapoints
2025-03-06 18:12:55,023 - INFO - training batch 1701, loss: 0.184, 54432/60000 datapoints
2025-03-06 18:12:55,226 - INFO - training batch 1751, loss: 0.374, 56032/60000 datapoints
2025-03-06 18:12:55,422 - INFO - training batch 1801, loss: 0.250, 57632/60000 datapoints
2025-03-06 18:12:55,626 - INFO - training batch 1851, loss: 0.320, 59232/60000 datapoints
2025-03-06 18:12:55,729 - INFO - validation batch 1, loss: 0.611, 32/10016 datapoints
2025-03-06 18:12:55,885 - INFO - validation batch 51, loss: 0.580, 1632/10016 datapoints
2025-03-06 18:12:56,040 - INFO - validation batch 101, loss: 0.468, 3232/10016 datapoints
2025-03-06 18:12:56,196 - INFO - validation batch 151, loss: 0.244, 4832/10016 datapoints
2025-03-06 18:12:56,353 - INFO - validation batch 201, loss: 0.437, 6432/10016 datapoints
2025-03-06 18:12:56,509 - INFO - validation batch 251, loss: 0.446, 8032/10016 datapoints
2025-03-06 18:12:56,664 - INFO - validation batch 301, loss: 0.463, 9632/10016 datapoints
2025-03-06 18:12:56,700 - INFO - Epoch 208/800 done.
2025-03-06 18:12:56,700 - INFO - Final validation performance:
Loss: 0.464, top-1 acc: 0.904top-5 acc: 0.904
2025-03-06 18:12:56,700 - INFO - Beginning epoch 209/800
2025-03-06 18:12:56,708 - INFO - training batch 1, loss: 0.297, 32/60000 datapoints
2025-03-06 18:12:56,904 - INFO - training batch 51, loss: 0.606, 1632/60000 datapoints
2025-03-06 18:12:57,102 - INFO - training batch 101, loss: 0.392, 3232/60000 datapoints
2025-03-06 18:12:57,312 - INFO - training batch 151, loss: 0.245, 4832/60000 datapoints
2025-03-06 18:12:57,542 - INFO - training batch 201, loss: 0.552, 6432/60000 datapoints
2025-03-06 18:12:57,774 - INFO - training batch 251, loss: 0.393, 8032/60000 datapoints
2025-03-06 18:12:57,985 - INFO - training batch 301, loss: 0.540, 9632/60000 datapoints
2025-03-06 18:12:58,195 - INFO - training batch 351, loss: 0.276, 11232/60000 datapoints
2025-03-06 18:12:58,400 - INFO - training batch 401, loss: 0.306, 12832/60000 datapoints
2025-03-06 18:12:58,606 - INFO - training batch 451, loss: 0.271, 14432/60000 datapoints
2025-03-06 18:12:58,809 - INFO - training batch 501, loss: 0.306, 16032/60000 datapoints
2025-03-06 18:12:59,012 - INFO - training batch 551, loss: 0.261, 17632/60000 datapoints
2025-03-06 18:12:59,219 - INFO - training batch 601, loss: 0.360, 19232/60000 datapoints
2025-03-06 18:12:59,417 - INFO - training batch 651, loss: 0.446, 20832/60000 datapoints
2025-03-06 18:12:59,617 - INFO - training batch 701, loss: 0.285, 22432/60000 datapoints
2025-03-06 18:12:59,814 - INFO - training batch 751, loss: 0.282, 24032/60000 datapoints
2025-03-06 18:13:00,013 - INFO - training batch 801, loss: 0.299, 25632/60000 datapoints
2025-03-06 18:13:00,221 - INFO - training batch 851, loss: 0.280, 27232/60000 datapoints
2025-03-06 18:13:00,420 - INFO - training batch 901, loss: 0.568, 28832/60000 datapoints
2025-03-06 18:13:00,619 - INFO - training batch 951, loss: 0.433, 30432/60000 datapoints
2025-03-06 18:13:00,815 - INFO - training batch 1001, loss: 0.238, 32032/60000 datapoints
2025-03-06 18:13:01,012 - INFO - training batch 1051, loss: 0.224, 33632/60000 datapoints
2025-03-06 18:13:01,209 - INFO - training batch 1101, loss: 0.225, 35232/60000 datapoints
2025-03-06 18:13:01,407 - INFO - training batch 1151, loss: 0.221, 36832/60000 datapoints
2025-03-06 18:13:01,610 - INFO - training batch 1201, loss: 0.259, 38432/60000 datapoints
2025-03-06 18:13:01,807 - INFO - training batch 1251, loss: 0.225, 40032/60000 datapoints
2025-03-06 18:13:02,018 - INFO - training batch 1301, loss: 0.446, 41632/60000 datapoints
2025-03-06 18:13:02,213 - INFO - training batch 1351, loss: 0.351, 43232/60000 datapoints
2025-03-06 18:13:02,413 - INFO - training batch 1401, loss: 0.143, 44832/60000 datapoints
2025-03-06 18:13:02,620 - INFO - training batch 1451, loss: 0.174, 46432/60000 datapoints
2025-03-06 18:13:02,827 - INFO - training batch 1501, loss: 0.568, 48032/60000 datapoints
2025-03-06 18:13:03,041 - INFO - training batch 1551, loss: 0.374, 49632/60000 datapoints
2025-03-06 18:13:03,242 - INFO - training batch 1601, loss: 0.294, 51232/60000 datapoints
2025-03-06 18:13:03,447 - INFO - training batch 1651, loss: 0.419, 52832/60000 datapoints
2025-03-06 18:13:03,674 - INFO - training batch 1701, loss: 0.167, 54432/60000 datapoints
2025-03-06 18:13:03,890 - INFO - training batch 1751, loss: 0.255, 56032/60000 datapoints
2025-03-06 18:13:04,104 - INFO - training batch 1801, loss: 0.478, 57632/60000 datapoints
2025-03-06 18:13:04,313 - INFO - training batch 1851, loss: 0.292, 59232/60000 datapoints
2025-03-06 18:13:04,416 - INFO - validation batch 1, loss: 0.629, 32/10016 datapoints
2025-03-06 18:13:04,570 - INFO - validation batch 51, loss: 0.189, 1632/10016 datapoints
2025-03-06 18:13:04,731 - INFO - validation batch 101, loss: 0.338, 3232/10016 datapoints
2025-03-06 18:13:04,889 - INFO - validation batch 151, loss: 0.457, 4832/10016 datapoints
2025-03-06 18:13:05,043 - INFO - validation batch 201, loss: 0.160, 6432/10016 datapoints
2025-03-06 18:13:05,200 - INFO - validation batch 251, loss: 0.292, 8032/10016 datapoints
2025-03-06 18:13:05,353 - INFO - validation batch 301, loss: 0.524, 9632/10016 datapoints
2025-03-06 18:13:05,391 - INFO - Epoch 209/800 done.
2025-03-06 18:13:05,391 - INFO - Final validation performance:
Loss: 0.370, top-1 acc: 0.904top-5 acc: 0.904
2025-03-06 18:13:05,392 - INFO - Beginning epoch 210/800
2025-03-06 18:13:05,397 - INFO - training batch 1, loss: 0.288, 32/60000 datapoints
2025-03-06 18:13:05,593 - INFO - training batch 51, loss: 0.318, 1632/60000 datapoints
2025-03-06 18:13:05,793 - INFO - training batch 101, loss: 0.392, 3232/60000 datapoints
2025-03-06 18:13:05,988 - INFO - training batch 151, loss: 0.595, 4832/60000 datapoints
2025-03-06 18:13:06,192 - INFO - training batch 201, loss: 0.530, 6432/60000 datapoints
2025-03-06 18:13:06,389 - INFO - training batch 251, loss: 0.262, 8032/60000 datapoints
2025-03-06 18:13:06,585 - INFO - training batch 301, loss: 0.276, 9632/60000 datapoints
2025-03-06 18:13:06,788 - INFO - training batch 351, loss: 0.499, 11232/60000 datapoints
2025-03-06 18:13:06,987 - INFO - training batch 401, loss: 0.311, 12832/60000 datapoints
2025-03-06 18:13:07,186 - INFO - training batch 451, loss: 0.224, 14432/60000 datapoints
2025-03-06 18:13:07,383 - INFO - training batch 501, loss: 0.333, 16032/60000 datapoints
2025-03-06 18:13:07,583 - INFO - training batch 551, loss: 0.240, 17632/60000 datapoints
2025-03-06 18:13:07,786 - INFO - training batch 601, loss: 0.443, 19232/60000 datapoints
2025-03-06 18:13:07,984 - INFO - training batch 651, loss: 0.251, 20832/60000 datapoints
2025-03-06 18:13:08,191 - INFO - training batch 701, loss: 0.253, 22432/60000 datapoints
2025-03-06 18:13:08,388 - INFO - training batch 751, loss: 0.443, 24032/60000 datapoints
2025-03-06 18:13:08,588 - INFO - training batch 801, loss: 0.231, 25632/60000 datapoints
2025-03-06 18:13:08,792 - INFO - training batch 851, loss: 0.527, 27232/60000 datapoints
2025-03-06 18:13:08,992 - INFO - training batch 901, loss: 0.453, 28832/60000 datapoints
2025-03-06 18:13:09,193 - INFO - training batch 951, loss: 0.222, 30432/60000 datapoints
2025-03-06 18:13:09,404 - INFO - training batch 1001, loss: 0.513, 32032/60000 datapoints
2025-03-06 18:13:09,610 - INFO - training batch 1051, loss: 0.315, 33632/60000 datapoints
2025-03-06 18:13:09,813 - INFO - training batch 1101, loss: 0.297, 35232/60000 datapoints
2025-03-06 18:13:10,016 - INFO - training batch 1151, loss: 0.444, 36832/60000 datapoints
2025-03-06 18:13:10,216 - INFO - training batch 1201, loss: 0.410, 38432/60000 datapoints
2025-03-06 18:13:10,426 - INFO - training batch 1251, loss: 0.265, 40032/60000 datapoints
2025-03-06 18:13:10,625 - INFO - training batch 1301, loss: 0.257, 41632/60000 datapoints
2025-03-06 18:13:10,819 - INFO - training batch 1351, loss: 0.424, 43232/60000 datapoints
2025-03-06 18:13:11,015 - INFO - training batch 1401, loss: 0.396, 44832/60000 datapoints
2025-03-06 18:13:11,212 - INFO - training batch 1451, loss: 0.300, 46432/60000 datapoints
2025-03-06 18:13:11,408 - INFO - training batch 1501, loss: 0.299, 48032/60000 datapoints
2025-03-06 18:13:11,611 - INFO - training batch 1551, loss: 0.316, 49632/60000 datapoints
2025-03-06 18:13:11,808 - INFO - training batch 1601, loss: 0.300, 51232/60000 datapoints
2025-03-06 18:13:12,004 - INFO - training batch 1651, loss: 0.451, 52832/60000 datapoints
2025-03-06 18:13:12,200 - INFO - training batch 1701, loss: 0.314, 54432/60000 datapoints
2025-03-06 18:13:12,398 - INFO - training batch 1751, loss: 0.279, 56032/60000 datapoints
2025-03-06 18:13:12,592 - INFO - training batch 1801, loss: 0.466, 57632/60000 datapoints
2025-03-06 18:13:12,794 - INFO - training batch 1851, loss: 0.417, 59232/60000 datapoints
2025-03-06 18:13:12,895 - INFO - validation batch 1, loss: 0.266, 32/10016 datapoints
2025-03-06 18:13:13,049 - INFO - validation batch 51, loss: 0.599, 1632/10016 datapoints
2025-03-06 18:13:13,206 - INFO - validation batch 101, loss: 0.241, 3232/10016 datapoints
2025-03-06 18:13:13,363 - INFO - validation batch 151, loss: 0.348, 4832/10016 datapoints
2025-03-06 18:13:13,521 - INFO - validation batch 201, loss: 0.385, 6432/10016 datapoints
2025-03-06 18:13:13,699 - INFO - validation batch 251, loss: 0.281, 8032/10016 datapoints
2025-03-06 18:13:13,855 - INFO - validation batch 301, loss: 0.318, 9632/10016 datapoints
2025-03-06 18:13:13,896 - INFO - Epoch 210/800 done.
2025-03-06 18:13:13,896 - INFO - Final validation performance:
Loss: 0.348, top-1 acc: 0.904top-5 acc: 0.904
2025-03-06 18:13:13,896 - INFO - Beginning epoch 211/800
2025-03-06 18:13:13,902 - INFO - training batch 1, loss: 0.683, 32/60000 datapoints
2025-03-06 18:13:14,103 - INFO - training batch 51, loss: 0.525, 1632/60000 datapoints
2025-03-06 18:13:14,304 - INFO - training batch 101, loss: 0.337, 3232/60000 datapoints
2025-03-06 18:13:14,507 - INFO - training batch 151, loss: 0.418, 4832/60000 datapoints
2025-03-06 18:13:14,710 - INFO - training batch 201, loss: 0.399, 6432/60000 datapoints
2025-03-06 18:13:14,913 - INFO - training batch 251, loss: 0.192, 8032/60000 datapoints
2025-03-06 18:13:15,114 - INFO - training batch 301, loss: 0.321, 9632/60000 datapoints
2025-03-06 18:13:15,313 - INFO - training batch 351, loss: 0.264, 11232/60000 datapoints
2025-03-06 18:13:15,515 - INFO - training batch 401, loss: 0.289, 12832/60000 datapoints
2025-03-06 18:13:15,719 - INFO - training batch 451, loss: 0.275, 14432/60000 datapoints
2025-03-06 18:13:15,920 - INFO - training batch 501, loss: 0.434, 16032/60000 datapoints
2025-03-06 18:13:16,116 - INFO - training batch 551, loss: 0.263, 17632/60000 datapoints
2025-03-06 18:13:16,325 - INFO - training batch 601, loss: 0.134, 19232/60000 datapoints
2025-03-06 18:13:16,524 - INFO - training batch 651, loss: 0.571, 20832/60000 datapoints
2025-03-06 18:13:16,730 - INFO - training batch 701, loss: 0.507, 22432/60000 datapoints
2025-03-06 18:13:16,929 - INFO - training batch 751, loss: 0.317, 24032/60000 datapoints
2025-03-06 18:13:17,127 - INFO - training batch 801, loss: 0.469, 25632/60000 datapoints
2025-03-06 18:13:17,328 - INFO - training batch 851, loss: 0.478, 27232/60000 datapoints
2025-03-06 18:13:17,527 - INFO - training batch 901, loss: 0.427, 28832/60000 datapoints
2025-03-06 18:13:17,731 - INFO - training batch 951, loss: 0.156, 30432/60000 datapoints
2025-03-06 18:13:17,932 - INFO - training batch 1001, loss: 0.398, 32032/60000 datapoints
2025-03-06 18:13:18,132 - INFO - training batch 1051, loss: 0.337, 33632/60000 datapoints
2025-03-06 18:13:18,332 - INFO - training batch 1101, loss: 0.433, 35232/60000 datapoints
2025-03-06 18:13:18,529 - INFO - training batch 1151, loss: 0.176, 36832/60000 datapoints
2025-03-06 18:13:18,728 - INFO - training batch 1201, loss: 0.372, 38432/60000 datapoints
2025-03-06 18:13:18,925 - INFO - training batch 1251, loss: 0.757, 40032/60000 datapoints
2025-03-06 18:13:19,127 - INFO - training batch 1301, loss: 0.383, 41632/60000 datapoints
2025-03-06 18:13:19,327 - INFO - training batch 1351, loss: 0.348, 43232/60000 datapoints
2025-03-06 18:13:19,527 - INFO - training batch 1401, loss: 0.348, 44832/60000 datapoints
2025-03-06 18:13:19,732 - INFO - training batch 1451, loss: 0.312, 46432/60000 datapoints
2025-03-06 18:13:19,932 - INFO - training batch 1501, loss: 0.133, 48032/60000 datapoints
2025-03-06 18:13:20,135 - INFO - training batch 1551, loss: 0.388, 49632/60000 datapoints
2025-03-06 18:13:20,337 - INFO - training batch 1601, loss: 0.474, 51232/60000 datapoints
2025-03-06 18:13:20,537 - INFO - training batch 1651, loss: 0.429, 52832/60000 datapoints
2025-03-06 18:13:20,738 - INFO - training batch 1701, loss: 0.467, 54432/60000 datapoints
2025-03-06 18:13:20,938 - INFO - training batch 1751, loss: 0.425, 56032/60000 datapoints
2025-03-06 18:13:21,137 - INFO - training batch 1801, loss: 0.209, 57632/60000 datapoints
2025-03-06 18:13:21,343 - INFO - training batch 1851, loss: 0.551, 59232/60000 datapoints
2025-03-06 18:13:21,446 - INFO - validation batch 1, loss: 0.470, 32/10016 datapoints
2025-03-06 18:13:21,606 - INFO - validation batch 51, loss: 0.272, 1632/10016 datapoints
2025-03-06 18:13:21,765 - INFO - validation batch 101, loss: 0.199, 3232/10016 datapoints
2025-03-06 18:13:21,920 - INFO - validation batch 151, loss: 0.321, 4832/10016 datapoints
2025-03-06 18:13:22,073 - INFO - validation batch 201, loss: 0.384, 6432/10016 datapoints
2025-03-06 18:13:22,230 - INFO - validation batch 251, loss: 0.287, 8032/10016 datapoints
2025-03-06 18:13:22,386 - INFO - validation batch 301, loss: 0.256, 9632/10016 datapoints
2025-03-06 18:13:22,425 - INFO - Epoch 211/800 done.
2025-03-06 18:13:22,425 - INFO - Final validation performance:
Loss: 0.313, top-1 acc: 0.904top-5 acc: 0.904
2025-03-06 18:13:22,426 - INFO - Beginning epoch 212/800
2025-03-06 18:13:22,431 - INFO - training batch 1, loss: 0.421, 32/60000 datapoints
2025-03-06 18:13:22,635 - INFO - training batch 51, loss: 0.402, 1632/60000 datapoints
2025-03-06 18:13:22,839 - INFO - training batch 101, loss: 0.207, 3232/60000 datapoints
2025-03-06 18:13:23,041 - INFO - training batch 151, loss: 0.203, 4832/60000 datapoints
2025-03-06 18:13:23,243 - INFO - training batch 201, loss: 0.264, 6432/60000 datapoints
2025-03-06 18:13:23,443 - INFO - training batch 251, loss: 0.445, 8032/60000 datapoints
2025-03-06 18:13:23,652 - INFO - training batch 301, loss: 0.336, 9632/60000 datapoints
2025-03-06 18:13:23,866 - INFO - training batch 351, loss: 0.664, 11232/60000 datapoints
2025-03-06 18:13:24,066 - INFO - training batch 401, loss: 0.393, 12832/60000 datapoints
2025-03-06 18:13:24,266 - INFO - training batch 451, loss: 0.211, 14432/60000 datapoints
2025-03-06 18:13:24,465 - INFO - training batch 501, loss: 0.293, 16032/60000 datapoints
2025-03-06 18:13:24,665 - INFO - training batch 551, loss: 0.210, 17632/60000 datapoints
2025-03-06 18:13:24,869 - INFO - training batch 601, loss: 0.469, 19232/60000 datapoints
2025-03-06 18:13:25,069 - INFO - training batch 651, loss: 0.458, 20832/60000 datapoints
2025-03-06 18:13:25,264 - INFO - training batch 701, loss: 0.340, 22432/60000 datapoints
2025-03-06 18:13:25,460 - INFO - training batch 751, loss: 0.373, 24032/60000 datapoints
2025-03-06 18:13:25,662 - INFO - training batch 801, loss: 0.374, 25632/60000 datapoints
2025-03-06 18:13:25,861 - INFO - training batch 851, loss: 0.344, 27232/60000 datapoints
2025-03-06 18:13:26,058 - INFO - training batch 901, loss: 0.166, 28832/60000 datapoints
2025-03-06 18:13:26,257 - INFO - training batch 951, loss: 0.140, 30432/60000 datapoints
2025-03-06 18:13:26,456 - INFO - training batch 1001, loss: 0.313, 32032/60000 datapoints
2025-03-06 18:13:26,654 - INFO - training batch 1051, loss: 0.259, 33632/60000 datapoints
2025-03-06 18:13:26,853 - INFO - training batch 1101, loss: 0.385, 35232/60000 datapoints
2025-03-06 18:13:27,054 - INFO - training batch 1151, loss: 0.347, 36832/60000 datapoints
2025-03-06 18:13:27,252 - INFO - training batch 1201, loss: 0.280, 38432/60000 datapoints
2025-03-06 18:13:27,470 - INFO - training batch 1251, loss: 0.466, 40032/60000 datapoints
2025-03-06 18:13:27,689 - INFO - training batch 1301, loss: 0.298, 41632/60000 datapoints
2025-03-06 18:13:27,889 - INFO - training batch 1351, loss: 0.166, 43232/60000 datapoints
2025-03-06 18:13:28,088 - INFO - training batch 1401, loss: 0.363, 44832/60000 datapoints
2025-03-06 18:13:28,291 - INFO - training batch 1451, loss: 0.277, 46432/60000 datapoints
2025-03-06 18:13:28,490 - INFO - training batch 1501, loss: 0.507, 48032/60000 datapoints
2025-03-06 18:13:28,693 - INFO - training batch 1551, loss: 0.463, 49632/60000 datapoints
2025-03-06 18:13:28,893 - INFO - training batch 1601, loss: 0.435, 51232/60000 datapoints
2025-03-06 18:13:29,091 - INFO - training batch 1651, loss: 0.336, 52832/60000 datapoints
2025-03-06 18:13:29,292 - INFO - training batch 1701, loss: 0.207, 54432/60000 datapoints
2025-03-06 18:13:29,490 - INFO - training batch 1751, loss: 0.439, 56032/60000 datapoints
2025-03-06 18:13:29,693 - INFO - training batch 1801, loss: 0.396, 57632/60000 datapoints
2025-03-06 18:13:29,894 - INFO - training batch 1851, loss: 0.318, 59232/60000 datapoints
2025-03-06 18:13:29,998 - INFO - validation batch 1, loss: 0.278, 32/10016 datapoints
2025-03-06 18:13:30,154 - INFO - validation batch 51, loss: 0.316, 1632/10016 datapoints
2025-03-06 18:13:30,310 - INFO - validation batch 101, loss: 0.328, 3232/10016 datapoints
2025-03-06 18:13:30,466 - INFO - validation batch 151, loss: 0.371, 4832/10016 datapoints
2025-03-06 18:13:30,623 - INFO - validation batch 201, loss: 0.223, 6432/10016 datapoints
2025-03-06 18:13:30,780 - INFO - validation batch 251, loss: 0.213, 8032/10016 datapoints
2025-03-06 18:13:30,935 - INFO - validation batch 301, loss: 0.267, 9632/10016 datapoints
2025-03-06 18:13:30,975 - INFO - Epoch 212/800 done.
2025-03-06 18:13:30,975 - INFO - Final validation performance:
Loss: 0.285, top-1 acc: 0.904top-5 acc: 0.904
2025-03-06 18:13:30,975 - INFO - Beginning epoch 213/800
2025-03-06 18:13:30,981 - INFO - training batch 1, loss: 0.457, 32/60000 datapoints
2025-03-06 18:13:31,178 - INFO - training batch 51, loss: 0.356, 1632/60000 datapoints
2025-03-06 18:13:31,382 - INFO - training batch 101, loss: 0.232, 3232/60000 datapoints
2025-03-06 18:13:31,581 - INFO - training batch 151, loss: 0.586, 4832/60000 datapoints
2025-03-06 18:13:31,788 - INFO - training batch 201, loss: 0.221, 6432/60000 datapoints
2025-03-06 18:13:31,986 - INFO - training batch 251, loss: 0.258, 8032/60000 datapoints
2025-03-06 18:13:32,182 - INFO - training batch 301, loss: 0.359, 9632/60000 datapoints
2025-03-06 18:13:32,383 - INFO - training batch 351, loss: 0.312, 11232/60000 datapoints
2025-03-06 18:13:32,581 - INFO - training batch 401, loss: 0.365, 12832/60000 datapoints
2025-03-06 18:13:32,782 - INFO - training batch 451, loss: 0.269, 14432/60000 datapoints
2025-03-06 18:13:32,980 - INFO - training batch 501, loss: 0.425, 16032/60000 datapoints
2025-03-06 18:13:33,176 - INFO - training batch 551, loss: 0.304, 17632/60000 datapoints
2025-03-06 18:13:33,372 - INFO - training batch 601, loss: 0.198, 19232/60000 datapoints
2025-03-06 18:13:33,568 - INFO - training batch 651, loss: 0.495, 20832/60000 datapoints
2025-03-06 18:13:33,785 - INFO - training batch 701, loss: 0.335, 22432/60000 datapoints
2025-03-06 18:13:33,983 - INFO - training batch 751, loss: 0.403, 24032/60000 datapoints
2025-03-06 18:13:34,178 - INFO - training batch 801, loss: 0.184, 25632/60000 datapoints
2025-03-06 18:13:34,373 - INFO - training batch 851, loss: 0.288, 27232/60000 datapoints
2025-03-06 18:13:34,568 - INFO - training batch 901, loss: 0.368, 28832/60000 datapoints
2025-03-06 18:13:34,767 - INFO - training batch 951, loss: 0.297, 30432/60000 datapoints
2025-03-06 18:13:34,971 - INFO - training batch 1001, loss: 0.625, 32032/60000 datapoints
2025-03-06 18:13:35,167 - INFO - training batch 1051, loss: 0.272, 33632/60000 datapoints
2025-03-06 18:13:35,363 - INFO - training batch 1101, loss: 0.330, 35232/60000 datapoints
2025-03-06 18:13:35,566 - INFO - training batch 1151, loss: 0.350, 36832/60000 datapoints
2025-03-06 18:13:35,766 - INFO - training batch 1201, loss: 0.276, 38432/60000 datapoints
2025-03-06 18:13:35,959 - INFO - training batch 1251, loss: 0.258, 40032/60000 datapoints
2025-03-06 18:13:36,154 - INFO - training batch 1301, loss: 0.167, 41632/60000 datapoints
2025-03-06 18:13:36,351 - INFO - training batch 1351, loss: 0.309, 43232/60000 datapoints
2025-03-06 18:13:36,544 - INFO - training batch 1401, loss: 0.217, 44832/60000 datapoints
2025-03-06 18:13:36,741 - INFO - training batch 1451, loss: 0.515, 46432/60000 datapoints
2025-03-06 18:13:36,936 - INFO - training batch 1501, loss: 0.469, 48032/60000 datapoints
2025-03-06 18:13:37,128 - INFO - training batch 1551, loss: 0.350, 49632/60000 datapoints
2025-03-06 18:13:37,321 - INFO - training batch 1601, loss: 0.309, 51232/60000 datapoints
2025-03-06 18:13:37,517 - INFO - training batch 1651, loss: 0.388, 52832/60000 datapoints
2025-03-06 18:13:37,772 - INFO - training batch 1701, loss: 0.303, 54432/60000 datapoints
2025-03-06 18:13:37,971 - INFO - training batch 1751, loss: 0.154, 56032/60000 datapoints
2025-03-06 18:13:38,165 - INFO - training batch 1801, loss: 0.448, 57632/60000 datapoints
2025-03-06 18:13:38,360 - INFO - training batch 1851, loss: 0.398, 59232/60000 datapoints
2025-03-06 18:13:38,468 - INFO - validation batch 1, loss: 0.308, 32/10016 datapoints
2025-03-06 18:13:38,625 - INFO - validation batch 51, loss: 0.674, 1632/10016 datapoints
2025-03-06 18:13:38,778 - INFO - validation batch 101, loss: 0.477, 3232/10016 datapoints
2025-03-06 18:13:38,933 - INFO - validation batch 151, loss: 0.171, 4832/10016 datapoints
2025-03-06 18:13:39,091 - INFO - validation batch 201, loss: 0.630, 6432/10016 datapoints
2025-03-06 18:13:39,248 - INFO - validation batch 251, loss: 0.384, 8032/10016 datapoints
2025-03-06 18:13:39,402 - INFO - validation batch 301, loss: 0.382, 9632/10016 datapoints
2025-03-06 18:13:39,440 - INFO - Epoch 213/800 done.
2025-03-06 18:13:39,440 - INFO - Final validation performance:
Loss: 0.432, top-1 acc: 0.904top-5 acc: 0.904
2025-03-06 18:13:39,441 - INFO - Beginning epoch 214/800
2025-03-06 18:13:39,446 - INFO - training batch 1, loss: 0.427, 32/60000 datapoints
2025-03-06 18:13:39,647 - INFO - training batch 51, loss: 0.498, 1632/60000 datapoints
2025-03-06 18:13:39,841 - INFO - training batch 101, loss: 0.406, 3232/60000 datapoints
2025-03-06 18:13:40,040 - INFO - training batch 151, loss: 0.443, 4832/60000 datapoints
2025-03-06 18:13:40,237 - INFO - training batch 201, loss: 0.181, 6432/60000 datapoints
2025-03-06 18:13:40,435 - INFO - training batch 251, loss: 0.282, 8032/60000 datapoints
2025-03-06 18:13:40,636 - INFO - training batch 301, loss: 0.501, 9632/60000 datapoints
2025-03-06 18:13:40,831 - INFO - training batch 351, loss: 0.371, 11232/60000 datapoints
2025-03-06 18:13:41,029 - INFO - training batch 401, loss: 0.190, 12832/60000 datapoints
2025-03-06 18:13:41,222 - INFO - training batch 451, loss: 0.391, 14432/60000 datapoints
2025-03-06 18:13:41,419 - INFO - training batch 501, loss: 0.328, 16032/60000 datapoints
2025-03-06 18:13:41,616 - INFO - training batch 551, loss: 0.267, 17632/60000 datapoints
2025-03-06 18:13:41,813 - INFO - training batch 601, loss: 0.203, 19232/60000 datapoints
2025-03-06 18:13:42,010 - INFO - training batch 651, loss: 0.313, 20832/60000 datapoints
2025-03-06 18:13:42,207 - INFO - training batch 701, loss: 0.409, 22432/60000 datapoints
2025-03-06 18:13:42,403 - INFO - training batch 751, loss: 0.387, 24032/60000 datapoints
2025-03-06 18:13:42,601 - INFO - training batch 801, loss: 0.245, 25632/60000 datapoints
2025-03-06 18:13:42,797 - INFO - training batch 851, loss: 0.244, 27232/60000 datapoints
2025-03-06 18:13:42,996 - INFO - training batch 901, loss: 0.223, 28832/60000 datapoints
2025-03-06 18:13:43,193 - INFO - training batch 951, loss: 0.373, 30432/60000 datapoints
2025-03-06 18:13:43,395 - INFO - training batch 1001, loss: 0.261, 32032/60000 datapoints
2025-03-06 18:13:43,605 - INFO - training batch 1051, loss: 0.249, 33632/60000 datapoints
2025-03-06 18:13:43,832 - INFO - training batch 1101, loss: 0.150, 35232/60000 datapoints
2025-03-06 18:13:44,045 - INFO - training batch 1151, loss: 0.222, 36832/60000 datapoints
2025-03-06 18:13:44,246 - INFO - training batch 1201, loss: 0.317, 38432/60000 datapoints
2025-03-06 18:13:44,445 - INFO - training batch 1251, loss: 0.288, 40032/60000 datapoints
2025-03-06 18:13:44,648 - INFO - training batch 1301, loss: 0.165, 41632/60000 datapoints
2025-03-06 18:13:44,849 - INFO - training batch 1351, loss: 0.507, 43232/60000 datapoints
2025-03-06 18:13:45,056 - INFO - training batch 1401, loss: 0.241, 44832/60000 datapoints
2025-03-06 18:13:45,267 - INFO - training batch 1451, loss: 0.408, 46432/60000 datapoints
2025-03-06 18:13:45,480 - INFO - training batch 1501, loss: 0.316, 48032/60000 datapoints
2025-03-06 18:13:45,692 - INFO - training batch 1551, loss: 0.233, 49632/60000 datapoints
2025-03-06 18:13:45,899 - INFO - training batch 1601, loss: 0.424, 51232/60000 datapoints
2025-03-06 18:13:46,105 - INFO - training batch 1651, loss: 0.346, 52832/60000 datapoints
2025-03-06 18:13:46,312 - INFO - training batch 1701, loss: 0.324, 54432/60000 datapoints
2025-03-06 18:13:46,526 - INFO - training batch 1751, loss: 0.363, 56032/60000 datapoints
2025-03-06 18:13:46,739 - INFO - training batch 1801, loss: 0.259, 57632/60000 datapoints
2025-03-06 18:13:46,934 - INFO - training batch 1851, loss: 0.178, 59232/60000 datapoints
2025-03-06 18:13:47,035 - INFO - validation batch 1, loss: 0.282, 32/10016 datapoints
2025-03-06 18:13:47,186 - INFO - validation batch 51, loss: 0.535, 1632/10016 datapoints
2025-03-06 18:13:47,338 - INFO - validation batch 101, loss: 0.260, 3232/10016 datapoints
2025-03-06 18:13:47,492 - INFO - validation batch 151, loss: 0.233, 4832/10016 datapoints
2025-03-06 18:13:47,647 - INFO - validation batch 201, loss: 0.251, 6432/10016 datapoints
2025-03-06 18:13:47,803 - INFO - validation batch 251, loss: 0.421, 8032/10016 datapoints
2025-03-06 18:13:47,959 - INFO - validation batch 301, loss: 0.553, 9632/10016 datapoints
2025-03-06 18:13:47,996 - INFO - Epoch 214/800 done.
2025-03-06 18:13:47,996 - INFO - Final validation performance:
Loss: 0.362, top-1 acc: 0.904top-5 acc: 0.904
2025-03-06 18:13:47,996 - INFO - Beginning epoch 215/800
2025-03-06 18:13:48,003 - INFO - training batch 1, loss: 0.517, 32/60000 datapoints
2025-03-06 18:13:48,203 - INFO - training batch 51, loss: 0.300, 1632/60000 datapoints
2025-03-06 18:13:48,397 - INFO - training batch 101, loss: 0.171, 3232/60000 datapoints
2025-03-06 18:13:48,593 - INFO - training batch 151, loss: 0.475, 4832/60000 datapoints
2025-03-06 18:13:48,789 - INFO - training batch 201, loss: 0.278, 6432/60000 datapoints
2025-03-06 18:13:48,986 - INFO - training batch 251, loss: 0.344, 8032/60000 datapoints
2025-03-06 18:13:49,183 - INFO - training batch 301, loss: 0.270, 9632/60000 datapoints
2025-03-06 18:13:49,378 - INFO - training batch 351, loss: 0.397, 11232/60000 datapoints
2025-03-06 18:13:49,575 - INFO - training batch 401, loss: 0.354, 12832/60000 datapoints
2025-03-06 18:13:49,775 - INFO - training batch 451, loss: 0.370, 14432/60000 datapoints
2025-03-06 18:13:49,973 - INFO - training batch 501, loss: 0.165, 16032/60000 datapoints
2025-03-06 18:13:50,172 - INFO - training batch 551, loss: 0.307, 17632/60000 datapoints
2025-03-06 18:13:50,367 - INFO - training batch 601, loss: 0.198, 19232/60000 datapoints
2025-03-06 18:13:50,563 - INFO - training batch 651, loss: 0.273, 20832/60000 datapoints
2025-03-06 18:13:50,759 - INFO - training batch 701, loss: 0.469, 22432/60000 datapoints
2025-03-06 18:13:50,955 - INFO - training batch 751, loss: 0.091, 24032/60000 datapoints
2025-03-06 18:13:51,151 - INFO - training batch 801, loss: 0.279, 25632/60000 datapoints
2025-03-06 18:13:51,346 - INFO - training batch 851, loss: 0.458, 27232/60000 datapoints
2025-03-06 18:13:51,541 - INFO - training batch 901, loss: 0.341, 28832/60000 datapoints
2025-03-06 18:13:51,741 - INFO - training batch 951, loss: 0.713, 30432/60000 datapoints
2025-03-06 18:13:51,935 - INFO - training batch 1001, loss: 0.185, 32032/60000 datapoints
2025-03-06 18:13:52,131 - INFO - training batch 1051, loss: 0.363, 33632/60000 datapoints
2025-03-06 18:13:52,329 - INFO - training batch 1101, loss: 0.268, 35232/60000 datapoints
2025-03-06 18:13:52,524 - INFO - training batch 1151, loss: 0.219, 36832/60000 datapoints
2025-03-06 18:13:52,723 - INFO - training batch 1201, loss: 0.429, 38432/60000 datapoints
2025-03-06 18:13:52,917 - INFO - training batch 1251, loss: 0.259, 40032/60000 datapoints
2025-03-06 18:13:53,128 - INFO - training batch 1301, loss: 0.320, 41632/60000 datapoints
2025-03-06 18:13:53,321 - INFO - training batch 1351, loss: 0.727, 43232/60000 datapoints
2025-03-06 18:13:53,518 - INFO - training batch 1401, loss: 0.689, 44832/60000 datapoints
2025-03-06 18:13:53,715 - INFO - training batch 1451, loss: 0.279, 46432/60000 datapoints
2025-03-06 18:13:53,916 - INFO - training batch 1501, loss: 0.440, 48032/60000 datapoints
2025-03-06 18:13:54,124 - INFO - training batch 1551, loss: 0.147, 49632/60000 datapoints
2025-03-06 18:13:54,317 - INFO - training batch 1601, loss: 0.406, 51232/60000 datapoints
2025-03-06 18:13:54,511 - INFO - training batch 1651, loss: 0.257, 52832/60000 datapoints
2025-03-06 18:13:54,708 - INFO - training batch 1701, loss: 0.321, 54432/60000 datapoints
2025-03-06 18:13:54,908 - INFO - training batch 1751, loss: 0.292, 56032/60000 datapoints
2025-03-06 18:13:55,105 - INFO - training batch 1801, loss: 0.212, 57632/60000 datapoints
2025-03-06 18:13:55,298 - INFO - training batch 1851, loss: 0.249, 59232/60000 datapoints
2025-03-06 18:13:55,399 - INFO - validation batch 1, loss: 0.256, 32/10016 datapoints
2025-03-06 18:13:55,553 - INFO - validation batch 51, loss: 0.209, 1632/10016 datapoints
2025-03-06 18:13:55,710 - INFO - validation batch 101, loss: 0.333, 3232/10016 datapoints
2025-03-06 18:13:55,861 - INFO - validation batch 151, loss: 0.367, 4832/10016 datapoints
2025-03-06 18:13:56,014 - INFO - validation batch 201, loss: 0.311, 6432/10016 datapoints
2025-03-06 18:13:56,170 - INFO - validation batch 251, loss: 0.281, 8032/10016 datapoints
2025-03-06 18:13:56,324 - INFO - validation batch 301, loss: 0.394, 9632/10016 datapoints
2025-03-06 18:13:56,360 - INFO - Epoch 215/800 done.
2025-03-06 18:13:56,360 - INFO - Final validation performance:
Loss: 0.307, top-1 acc: 0.905top-5 acc: 0.905
2025-03-06 18:13:56,361 - INFO - Beginning epoch 216/800
2025-03-06 18:13:56,367 - INFO - training batch 1, loss: 0.442, 32/60000 datapoints
2025-03-06 18:13:56,562 - INFO - training batch 51, loss: 0.566, 1632/60000 datapoints
2025-03-06 18:13:56,753 - INFO - training batch 101, loss: 0.326, 3232/60000 datapoints
2025-03-06 18:13:56,945 - INFO - training batch 151, loss: 0.399, 4832/60000 datapoints
2025-03-06 18:13:57,137 - INFO - training batch 201, loss: 0.446, 6432/60000 datapoints
2025-03-06 18:13:57,330 - INFO - training batch 251, loss: 0.372, 8032/60000 datapoints
2025-03-06 18:13:57,525 - INFO - training batch 301, loss: 0.433, 9632/60000 datapoints
2025-03-06 18:13:57,724 - INFO - training batch 351, loss: 0.108, 11232/60000 datapoints
2025-03-06 18:13:57,917 - INFO - training batch 401, loss: 0.196, 12832/60000 datapoints
2025-03-06 18:13:58,111 - INFO - training batch 451, loss: 0.322, 14432/60000 datapoints
2025-03-06 18:13:58,303 - INFO - training batch 501, loss: 0.240, 16032/60000 datapoints
2025-03-06 18:13:58,494 - INFO - training batch 551, loss: 0.417, 17632/60000 datapoints
2025-03-06 18:13:58,691 - INFO - training batch 601, loss: 0.286, 19232/60000 datapoints
2025-03-06 18:13:58,884 - INFO - training batch 651, loss: 0.366, 20832/60000 datapoints
2025-03-06 18:13:59,079 - INFO - training batch 701, loss: 0.303, 22432/60000 datapoints
2025-03-06 18:13:59,272 - INFO - training batch 751, loss: 0.374, 24032/60000 datapoints
2025-03-06 18:13:59,465 - INFO - training batch 801, loss: 0.379, 25632/60000 datapoints
2025-03-06 18:13:59,660 - INFO - training batch 851, loss: 0.283, 27232/60000 datapoints
2025-03-06 18:13:59,858 - INFO - training batch 901, loss: 0.441, 28832/60000 datapoints
2025-03-06 18:14:00,054 - INFO - training batch 951, loss: 0.181, 30432/60000 datapoints
2025-03-06 18:14:00,255 - INFO - training batch 1001, loss: 0.218, 32032/60000 datapoints
2025-03-06 18:14:00,451 - INFO - training batch 1051, loss: 0.451, 33632/60000 datapoints
2025-03-06 18:14:00,647 - INFO - training batch 1101, loss: 0.135, 35232/60000 datapoints
2025-03-06 18:14:00,843 - INFO - training batch 1151, loss: 0.219, 36832/60000 datapoints
2025-03-06 18:14:01,040 - INFO - training batch 1201, loss: 0.465, 38432/60000 datapoints
2025-03-06 18:14:01,240 - INFO - training batch 1251, loss: 0.185, 40032/60000 datapoints
2025-03-06 18:14:01,433 - INFO - training batch 1301, loss: 0.232, 41632/60000 datapoints
2025-03-06 18:14:01,633 - INFO - training batch 1351, loss: 0.422, 43232/60000 datapoints
2025-03-06 18:14:01,831 - INFO - training batch 1401, loss: 0.216, 44832/60000 datapoints
2025-03-06 18:14:02,024 - INFO - training batch 1451, loss: 0.154, 46432/60000 datapoints
2025-03-06 18:14:02,216 - INFO - training batch 1501, loss: 0.377, 48032/60000 datapoints
2025-03-06 18:14:02,410 - INFO - training batch 1551, loss: 0.235, 49632/60000 datapoints
2025-03-06 18:14:02,606 - INFO - training batch 1601, loss: 0.272, 51232/60000 datapoints
2025-03-06 18:14:02,798 - INFO - training batch 1651, loss: 0.409, 52832/60000 datapoints
2025-03-06 18:14:02,989 - INFO - training batch 1701, loss: 0.713, 54432/60000 datapoints
2025-03-06 18:14:03,185 - INFO - training batch 1751, loss: 0.508, 56032/60000 datapoints
2025-03-06 18:14:03,380 - INFO - training batch 1801, loss: 0.368, 57632/60000 datapoints
2025-03-06 18:14:03,575 - INFO - training batch 1851, loss: 0.399, 59232/60000 datapoints
2025-03-06 18:14:03,679 - INFO - validation batch 1, loss: 0.567, 32/10016 datapoints
2025-03-06 18:14:03,835 - INFO - validation batch 51, loss: 0.249, 1632/10016 datapoints
2025-03-06 18:14:03,989 - INFO - validation batch 101, loss: 0.305, 3232/10016 datapoints
2025-03-06 18:14:04,158 - INFO - validation batch 151, loss: 0.177, 4832/10016 datapoints
2025-03-06 18:14:04,311 - INFO - validation batch 201, loss: 0.422, 6432/10016 datapoints
2025-03-06 18:14:04,463 - INFO - validation batch 251, loss: 0.288, 8032/10016 datapoints
2025-03-06 18:14:04,621 - INFO - validation batch 301, loss: 0.828, 9632/10016 datapoints
2025-03-06 18:14:04,659 - INFO - Epoch 216/800 done.
2025-03-06 18:14:04,659 - INFO - Final validation performance:
Loss: 0.405, top-1 acc: 0.905top-5 acc: 0.905
2025-03-06 18:14:04,659 - INFO - Beginning epoch 217/800
2025-03-06 18:14:04,665 - INFO - training batch 1, loss: 0.282, 32/60000 datapoints
2025-03-06 18:14:04,862 - INFO - training batch 51, loss: 0.378, 1632/60000 datapoints
2025-03-06 18:14:05,055 - INFO - training batch 101, loss: 0.207, 3232/60000 datapoints
2025-03-06 18:14:05,246 - INFO - training batch 151, loss: 0.314, 4832/60000 datapoints
2025-03-06 18:14:05,438 - INFO - training batch 201, loss: 0.213, 6432/60000 datapoints
2025-03-06 18:14:05,637 - INFO - training batch 251, loss: 0.274, 8032/60000 datapoints
2025-03-06 18:14:05,840 - INFO - training batch 301, loss: 0.242, 9632/60000 datapoints
2025-03-06 18:14:06,032 - INFO - training batch 351, loss: 0.363, 11232/60000 datapoints
2025-03-06 18:14:06,228 - INFO - training batch 401, loss: 0.207, 12832/60000 datapoints
2025-03-06 18:14:06,419 - INFO - training batch 451, loss: 0.153, 14432/60000 datapoints
2025-03-06 18:14:06,612 - INFO - training batch 501, loss: 0.365, 16032/60000 datapoints
2025-03-06 18:14:06,842 - INFO - training batch 551, loss: 0.267, 17632/60000 datapoints
2025-03-06 18:14:07,033 - INFO - training batch 601, loss: 0.327, 19232/60000 datapoints
2025-03-06 18:14:07,225 - INFO - training batch 651, loss: 0.164, 20832/60000 datapoints
2025-03-06 18:14:07,416 - INFO - training batch 701, loss: 0.443, 22432/60000 datapoints
2025-03-06 18:14:07,612 - INFO - training batch 751, loss: 0.315, 24032/60000 datapoints
2025-03-06 18:14:07,817 - INFO - training batch 801, loss: 0.567, 25632/60000 datapoints
2025-03-06 18:14:08,012 - INFO - training batch 851, loss: 0.406, 27232/60000 datapoints
2025-03-06 18:14:08,222 - INFO - training batch 901, loss: 0.612, 28832/60000 datapoints
2025-03-06 18:14:08,422 - INFO - training batch 951, loss: 0.357, 30432/60000 datapoints
2025-03-06 18:14:08,616 - INFO - training batch 1001, loss: 0.307, 32032/60000 datapoints
2025-03-06 18:14:08,811 - INFO - training batch 1051, loss: 0.227, 33632/60000 datapoints
2025-03-06 18:14:09,014 - INFO - training batch 1101, loss: 0.353, 35232/60000 datapoints
2025-03-06 18:14:09,210 - INFO - training batch 1151, loss: 0.368, 36832/60000 datapoints
2025-03-06 18:14:09,403 - INFO - training batch 1201, loss: 0.400, 38432/60000 datapoints
2025-03-06 18:14:09,600 - INFO - training batch 1251, loss: 0.221, 40032/60000 datapoints
2025-03-06 18:14:09,794 - INFO - training batch 1301, loss: 0.381, 41632/60000 datapoints
2025-03-06 18:14:09,988 - INFO - training batch 1351, loss: 0.379, 43232/60000 datapoints
2025-03-06 18:14:10,181 - INFO - training batch 1401, loss: 0.641, 44832/60000 datapoints
2025-03-06 18:14:10,375 - INFO - training batch 1451, loss: 0.516, 46432/60000 datapoints
2025-03-06 18:14:10,568 - INFO - training batch 1501, loss: 0.371, 48032/60000 datapoints
2025-03-06 18:14:10,769 - INFO - training batch 1551, loss: 0.412, 49632/60000 datapoints
2025-03-06 18:14:10,966 - INFO - training batch 1601, loss: 0.316, 51232/60000 datapoints
2025-03-06 18:14:11,164 - INFO - training batch 1651, loss: 0.379, 52832/60000 datapoints
2025-03-06 18:14:11,363 - INFO - training batch 1701, loss: 0.469, 54432/60000 datapoints
2025-03-06 18:14:11,557 - INFO - training batch 1751, loss: 0.784, 56032/60000 datapoints
2025-03-06 18:14:11,761 - INFO - training batch 1801, loss: 0.190, 57632/60000 datapoints
2025-03-06 18:14:11,959 - INFO - training batch 1851, loss: 0.472, 59232/60000 datapoints
2025-03-06 18:14:12,062 - INFO - validation batch 1, loss: 0.143, 32/10016 datapoints
2025-03-06 18:14:12,217 - INFO - validation batch 51, loss: 0.308, 1632/10016 datapoints
2025-03-06 18:14:12,371 - INFO - validation batch 101, loss: 0.148, 3232/10016 datapoints
2025-03-06 18:14:12,526 - INFO - validation batch 151, loss: 0.286, 4832/10016 datapoints
2025-03-06 18:14:12,683 - INFO - validation batch 201, loss: 0.200, 6432/10016 datapoints
2025-03-06 18:14:12,838 - INFO - validation batch 251, loss: 0.209, 8032/10016 datapoints
2025-03-06 18:14:12,991 - INFO - validation batch 301, loss: 0.403, 9632/10016 datapoints
2025-03-06 18:14:13,029 - INFO - Epoch 217/800 done.
2025-03-06 18:14:13,029 - INFO - Final validation performance:
Loss: 0.242, top-1 acc: 0.905top-5 acc: 0.905
2025-03-06 18:14:13,030 - INFO - Beginning epoch 218/800
2025-03-06 18:14:13,036 - INFO - training batch 1, loss: 0.422, 32/60000 datapoints
2025-03-06 18:14:13,236 - INFO - training batch 51, loss: 0.404, 1632/60000 datapoints
2025-03-06 18:14:13,435 - INFO - training batch 101, loss: 0.265, 3232/60000 datapoints
2025-03-06 18:14:13,638 - INFO - training batch 151, loss: 0.382, 4832/60000 datapoints
2025-03-06 18:14:13,839 - INFO - training batch 201, loss: 0.388, 6432/60000 datapoints
2025-03-06 18:14:14,038 - INFO - training batch 251, loss: 0.212, 8032/60000 datapoints
2025-03-06 18:14:14,258 - INFO - training batch 301, loss: 0.264, 9632/60000 datapoints
2025-03-06 18:14:14,455 - INFO - training batch 351, loss: 0.601, 11232/60000 datapoints
2025-03-06 18:14:14,652 - INFO - training batch 401, loss: 0.234, 12832/60000 datapoints
2025-03-06 18:14:14,863 - INFO - training batch 451, loss: 0.294, 14432/60000 datapoints
2025-03-06 18:14:15,057 - INFO - training batch 501, loss: 0.220, 16032/60000 datapoints
2025-03-06 18:14:15,252 - INFO - training batch 551, loss: 0.344, 17632/60000 datapoints
2025-03-06 18:14:15,445 - INFO - training batch 601, loss: 0.262, 19232/60000 datapoints
2025-03-06 18:14:15,645 - INFO - training batch 651, loss: 0.272, 20832/60000 datapoints
2025-03-06 18:14:15,851 - INFO - training batch 701, loss: 0.450, 22432/60000 datapoints
2025-03-06 18:14:16,048 - INFO - training batch 751, loss: 0.361, 24032/60000 datapoints
2025-03-06 18:14:16,251 - INFO - training batch 801, loss: 0.172, 25632/60000 datapoints
2025-03-06 18:14:16,449 - INFO - training batch 851, loss: 0.645, 27232/60000 datapoints
2025-03-06 18:14:16,651 - INFO - training batch 901, loss: 0.436, 28832/60000 datapoints
2025-03-06 18:14:16,848 - INFO - training batch 951, loss: 0.200, 30432/60000 datapoints
2025-03-06 18:14:17,045 - INFO - training batch 1001, loss: 0.509, 32032/60000 datapoints
2025-03-06 18:14:17,243 - INFO - training batch 1051, loss: 0.184, 33632/60000 datapoints
2025-03-06 18:14:17,442 - INFO - training batch 1101, loss: 0.244, 35232/60000 datapoints
2025-03-06 18:14:17,643 - INFO - training batch 1151, loss: 0.281, 36832/60000 datapoints
2025-03-06 18:14:17,844 - INFO - training batch 1201, loss: 0.678, 38432/60000 datapoints
2025-03-06 18:14:18,041 - INFO - training batch 1251, loss: 0.377, 40032/60000 datapoints
2025-03-06 18:14:18,240 - INFO - training batch 1301, loss: 0.325, 41632/60000 datapoints
2025-03-06 18:14:18,458 - INFO - training batch 1351, loss: 0.194, 43232/60000 datapoints
2025-03-06 18:14:18,657 - INFO - training batch 1401, loss: 0.372, 44832/60000 datapoints
2025-03-06 18:14:18,857 - INFO - training batch 1451, loss: 0.478, 46432/60000 datapoints
2025-03-06 18:14:19,054 - INFO - training batch 1501, loss: 0.162, 48032/60000 datapoints
2025-03-06 18:14:19,249 - INFO - training batch 1551, loss: 0.124, 49632/60000 datapoints
2025-03-06 18:14:19,450 - INFO - training batch 1601, loss: 0.255, 51232/60000 datapoints
2025-03-06 18:14:19,648 - INFO - training batch 1651, loss: 0.385, 52832/60000 datapoints
2025-03-06 18:14:19,848 - INFO - training batch 1701, loss: 0.488, 54432/60000 datapoints
2025-03-06 18:14:20,049 - INFO - training batch 1751, loss: 0.231, 56032/60000 datapoints
2025-03-06 18:14:20,259 - INFO - training batch 1801, loss: 0.347, 57632/60000 datapoints
2025-03-06 18:14:20,462 - INFO - training batch 1851, loss: 0.599, 59232/60000 datapoints
2025-03-06 18:14:20,566 - INFO - validation batch 1, loss: 0.244, 32/10016 datapoints
2025-03-06 18:14:20,726 - INFO - validation batch 51, loss: 0.264, 1632/10016 datapoints
2025-03-06 18:14:20,887 - INFO - validation batch 101, loss: 0.335, 3232/10016 datapoints
2025-03-06 18:14:21,046 - INFO - validation batch 151, loss: 0.321, 4832/10016 datapoints
2025-03-06 18:14:21,204 - INFO - validation batch 201, loss: 0.248, 6432/10016 datapoints
2025-03-06 18:14:21,362 - INFO - validation batch 251, loss: 0.145, 8032/10016 datapoints
2025-03-06 18:14:21,518 - INFO - validation batch 301, loss: 0.476, 9632/10016 datapoints
2025-03-06 18:14:21,555 - INFO - Epoch 218/800 done.
2025-03-06 18:14:21,555 - INFO - Final validation performance:
Loss: 0.290, top-1 acc: 0.905top-5 acc: 0.905
2025-03-06 18:14:21,556 - INFO - Beginning epoch 219/800
2025-03-06 18:14:21,562 - INFO - training batch 1, loss: 0.214, 32/60000 datapoints
2025-03-06 18:14:21,775 - INFO - training batch 51, loss: 0.271, 1632/60000 datapoints
2025-03-06 18:14:21,984 - INFO - training batch 101, loss: 0.617, 3232/60000 datapoints
2025-03-06 18:14:22,192 - INFO - training batch 151, loss: 0.377, 4832/60000 datapoints
2025-03-06 18:14:22,396 - INFO - training batch 201, loss: 0.378, 6432/60000 datapoints
2025-03-06 18:14:22,599 - INFO - training batch 251, loss: 0.990, 8032/60000 datapoints
2025-03-06 18:14:22,799 - INFO - training batch 301, loss: 0.199, 9632/60000 datapoints
2025-03-06 18:14:23,000 - INFO - training batch 351, loss: 0.386, 11232/60000 datapoints
2025-03-06 18:14:23,206 - INFO - training batch 401, loss: 0.378, 12832/60000 datapoints
2025-03-06 18:14:23,408 - INFO - training batch 451, loss: 0.293, 14432/60000 datapoints
2025-03-06 18:14:23,609 - INFO - training batch 501, loss: 0.405, 16032/60000 datapoints
2025-03-06 18:14:23,808 - INFO - training batch 551, loss: 0.405, 17632/60000 datapoints
2025-03-06 18:14:24,010 - INFO - training batch 601, loss: 0.473, 19232/60000 datapoints
2025-03-06 18:14:24,227 - INFO - training batch 651, loss: 0.339, 20832/60000 datapoints
2025-03-06 18:14:24,430 - INFO - training batch 701, loss: 0.364, 22432/60000 datapoints
2025-03-06 18:14:24,632 - INFO - training batch 751, loss: 0.748, 24032/60000 datapoints
2025-03-06 18:14:24,836 - INFO - training batch 801, loss: 0.302, 25632/60000 datapoints
2025-03-06 18:14:25,041 - INFO - training batch 851, loss: 0.252, 27232/60000 datapoints
2025-03-06 18:14:25,242 - INFO - training batch 901, loss: 0.518, 28832/60000 datapoints
2025-03-06 18:14:25,443 - INFO - training batch 951, loss: 0.329, 30432/60000 datapoints
2025-03-06 18:14:25,647 - INFO - training batch 1001, loss: 0.532, 32032/60000 datapoints
2025-03-06 18:14:25,849 - INFO - training batch 1051, loss: 0.200, 33632/60000 datapoints
2025-03-06 18:14:26,051 - INFO - training batch 1101, loss: 0.463, 35232/60000 datapoints
2025-03-06 18:14:26,251 - INFO - training batch 1151, loss: 0.346, 36832/60000 datapoints
2025-03-06 18:14:26,450 - INFO - training batch 1201, loss: 0.137, 38432/60000 datapoints
2025-03-06 18:14:26,650 - INFO - training batch 1251, loss: 0.439, 40032/60000 datapoints
2025-03-06 18:14:26,851 - INFO - training batch 1301, loss: 0.235, 41632/60000 datapoints
2025-03-06 18:14:27,051 - INFO - training batch 1351, loss: 0.177, 43232/60000 datapoints
2025-03-06 18:14:27,253 - INFO - training batch 1401, loss: 0.584, 44832/60000 datapoints
2025-03-06 18:14:27,455 - INFO - training batch 1451, loss: 0.132, 46432/60000 datapoints
2025-03-06 18:14:27,655 - INFO - training batch 1501, loss: 0.235, 48032/60000 datapoints
2025-03-06 18:14:27,858 - INFO - training batch 1551, loss: 0.283, 49632/60000 datapoints
2025-03-06 18:14:28,058 - INFO - training batch 1601, loss: 0.686, 51232/60000 datapoints
2025-03-06 18:14:28,260 - INFO - training batch 1651, loss: 0.364, 52832/60000 datapoints
2025-03-06 18:14:28,457 - INFO - training batch 1701, loss: 0.475, 54432/60000 datapoints
2025-03-06 18:14:28,656 - INFO - training batch 1751, loss: 0.222, 56032/60000 datapoints
2025-03-06 18:14:28,857 - INFO - training batch 1801, loss: 0.171, 57632/60000 datapoints
2025-03-06 18:14:29,057 - INFO - training batch 1851, loss: 0.474, 59232/60000 datapoints
2025-03-06 18:14:29,161 - INFO - validation batch 1, loss: 0.322, 32/10016 datapoints
2025-03-06 18:14:29,321 - INFO - validation batch 51, loss: 0.244, 1632/10016 datapoints
2025-03-06 18:14:29,478 - INFO - validation batch 101, loss: 0.360, 3232/10016 datapoints
2025-03-06 18:14:29,636 - INFO - validation batch 151, loss: 0.360, 4832/10016 datapoints
2025-03-06 18:14:29,794 - INFO - validation batch 201, loss: 0.249, 6432/10016 datapoints
2025-03-06 18:14:29,953 - INFO - validation batch 251, loss: 0.420, 8032/10016 datapoints
2025-03-06 18:14:30,109 - INFO - validation batch 301, loss: 0.379, 9632/10016 datapoints
2025-03-06 18:14:30,147 - INFO - Epoch 219/800 done.
2025-03-06 18:14:30,148 - INFO - Final validation performance:
Loss: 0.334, top-1 acc: 0.905top-5 acc: 0.905
2025-03-06 18:14:30,148 - INFO - Beginning epoch 220/800
2025-03-06 18:14:30,155 - INFO - training batch 1, loss: 0.376, 32/60000 datapoints
2025-03-06 18:14:30,362 - INFO - training batch 51, loss: 0.242, 1632/60000 datapoints
2025-03-06 18:14:30,565 - INFO - training batch 101, loss: 0.308, 3232/60000 datapoints
2025-03-06 18:14:30,773 - INFO - training batch 151, loss: 0.204, 4832/60000 datapoints
2025-03-06 18:14:30,979 - INFO - training batch 201, loss: 0.232, 6432/60000 datapoints
2025-03-06 18:14:31,178 - INFO - training batch 251, loss: 0.378, 8032/60000 datapoints
2025-03-06 18:14:31,379 - INFO - training batch 301, loss: 0.285, 9632/60000 datapoints
2025-03-06 18:14:31,575 - INFO - training batch 351, loss: 0.257, 11232/60000 datapoints
2025-03-06 18:14:31,774 - INFO - training batch 401, loss: 0.294, 12832/60000 datapoints
2025-03-06 18:14:31,978 - INFO - training batch 451, loss: 0.502, 14432/60000 datapoints
2025-03-06 18:14:32,176 - INFO - training batch 501, loss: 0.289, 16032/60000 datapoints
2025-03-06 18:14:32,381 - INFO - training batch 551, loss: 0.366, 17632/60000 datapoints
2025-03-06 18:14:32,583 - INFO - training batch 601, loss: 0.394, 19232/60000 datapoints
2025-03-06 18:14:32,782 - INFO - training batch 651, loss: 0.340, 20832/60000 datapoints
2025-03-06 18:14:32,983 - INFO - training batch 701, loss: 0.350, 22432/60000 datapoints
2025-03-06 18:14:33,182 - INFO - training batch 751, loss: 0.366, 24032/60000 datapoints
2025-03-06 18:14:33,380 - INFO - training batch 801, loss: 0.408, 25632/60000 datapoints
2025-03-06 18:14:33,579 - INFO - training batch 851, loss: 0.303, 27232/60000 datapoints
2025-03-06 18:14:33,777 - INFO - training batch 901, loss: 0.295, 28832/60000 datapoints
2025-03-06 18:14:33,978 - INFO - training batch 951, loss: 0.398, 30432/60000 datapoints
2025-03-06 18:14:34,177 - INFO - training batch 1001, loss: 0.251, 32032/60000 datapoints
2025-03-06 18:14:34,398 - INFO - training batch 1051, loss: 0.131, 33632/60000 datapoints
2025-03-06 18:14:34,598 - INFO - training batch 1101, loss: 0.420, 35232/60000 datapoints
2025-03-06 18:14:34,797 - INFO - training batch 1151, loss: 0.183, 36832/60000 datapoints
2025-03-06 18:14:35,005 - INFO - training batch 1201, loss: 0.318, 38432/60000 datapoints
2025-03-06 18:14:35,209 - INFO - training batch 1251, loss: 0.283, 40032/60000 datapoints
2025-03-06 18:14:35,412 - INFO - training batch 1301, loss: 0.310, 41632/60000 datapoints
2025-03-06 18:14:35,616 - INFO - training batch 1351, loss: 0.321, 43232/60000 datapoints
2025-03-06 18:14:35,823 - INFO - training batch 1401, loss: 0.294, 44832/60000 datapoints
2025-03-06 18:14:36,032 - INFO - training batch 1451, loss: 0.171, 46432/60000 datapoints
2025-03-06 18:14:36,234 - INFO - training batch 1501, loss: 0.381, 48032/60000 datapoints
2025-03-06 18:14:36,439 - INFO - training batch 1551, loss: 0.299, 49632/60000 datapoints
2025-03-06 18:14:36,641 - INFO - training batch 1601, loss: 0.290, 51232/60000 datapoints
2025-03-06 18:14:36,842 - INFO - training batch 1651, loss: 0.332, 52832/60000 datapoints
2025-03-06 18:14:37,072 - INFO - training batch 1701, loss: 0.267, 54432/60000 datapoints
2025-03-06 18:14:37,270 - INFO - training batch 1751, loss: 0.561, 56032/60000 datapoints
2025-03-06 18:14:37,471 - INFO - training batch 1801, loss: 0.204, 57632/60000 datapoints
2025-03-06 18:14:37,685 - INFO - training batch 1851, loss: 0.443, 59232/60000 datapoints
2025-03-06 18:14:37,789 - INFO - validation batch 1, loss: 0.225, 32/10016 datapoints
2025-03-06 18:14:37,946 - INFO - validation batch 51, loss: 0.266, 1632/10016 datapoints
2025-03-06 18:14:38,100 - INFO - validation batch 101, loss: 0.131, 3232/10016 datapoints
2025-03-06 18:14:38,255 - INFO - validation batch 151, loss: 0.419, 4832/10016 datapoints
2025-03-06 18:14:38,416 - INFO - validation batch 201, loss: 0.660, 6432/10016 datapoints
2025-03-06 18:14:38,570 - INFO - validation batch 251, loss: 0.282, 8032/10016 datapoints
2025-03-06 18:14:38,723 - INFO - validation batch 301, loss: 0.322, 9632/10016 datapoints
2025-03-06 18:14:38,760 - INFO - Epoch 220/800 done.
2025-03-06 18:14:38,760 - INFO - Final validation performance:
Loss: 0.329, top-1 acc: 0.905top-5 acc: 0.905
2025-03-06 18:14:38,761 - INFO - Beginning epoch 221/800
2025-03-06 18:14:38,767 - INFO - training batch 1, loss: 0.419, 32/60000 datapoints
2025-03-06 18:14:38,965 - INFO - training batch 51, loss: 0.270, 1632/60000 datapoints
2025-03-06 18:14:39,170 - INFO - training batch 101, loss: 0.548, 3232/60000 datapoints
2025-03-06 18:14:39,374 - INFO - training batch 151, loss: 0.572, 4832/60000 datapoints
2025-03-06 18:14:39,576 - INFO - training batch 201, loss: 0.355, 6432/60000 datapoints
2025-03-06 18:14:39,780 - INFO - training batch 251, loss: 0.261, 8032/60000 datapoints
2025-03-06 18:14:39,989 - INFO - training batch 301, loss: 0.317, 9632/60000 datapoints
2025-03-06 18:14:40,189 - INFO - training batch 351, loss: 0.215, 11232/60000 datapoints
2025-03-06 18:14:40,393 - INFO - training batch 401, loss: 0.382, 12832/60000 datapoints
2025-03-06 18:14:40,595 - INFO - training batch 451, loss: 0.549, 14432/60000 datapoints
2025-03-06 18:14:40,796 - INFO - training batch 501, loss: 0.385, 16032/60000 datapoints
2025-03-06 18:14:40,999 - INFO - training batch 551, loss: 0.307, 17632/60000 datapoints
2025-03-06 18:14:41,198 - INFO - training batch 601, loss: 0.292, 19232/60000 datapoints
2025-03-06 18:14:41,398 - INFO - training batch 651, loss: 0.415, 20832/60000 datapoints
2025-03-06 18:14:41,598 - INFO - training batch 701, loss: 0.186, 22432/60000 datapoints
2025-03-06 18:14:41,799 - INFO - training batch 751, loss: 0.259, 24032/60000 datapoints
2025-03-06 18:14:42,004 - INFO - training batch 801, loss: 0.319, 25632/60000 datapoints
2025-03-06 18:14:42,206 - INFO - training batch 851, loss: 0.361, 27232/60000 datapoints
2025-03-06 18:14:42,408 - INFO - training batch 901, loss: 0.382, 28832/60000 datapoints
2025-03-06 18:14:42,608 - INFO - training batch 951, loss: 0.372, 30432/60000 datapoints
2025-03-06 18:14:42,806 - INFO - training batch 1001, loss: 0.403, 32032/60000 datapoints
2025-03-06 18:14:43,002 - INFO - training batch 1051, loss: 0.327, 33632/60000 datapoints
2025-03-06 18:14:43,199 - INFO - training batch 1101, loss: 0.304, 35232/60000 datapoints
2025-03-06 18:14:43,396 - INFO - training batch 1151, loss: 0.199, 36832/60000 datapoints
2025-03-06 18:14:43,591 - INFO - training batch 1201, loss: 0.521, 38432/60000 datapoints
2025-03-06 18:14:43,789 - INFO - training batch 1251, loss: 0.371, 40032/60000 datapoints
2025-03-06 18:14:43,989 - INFO - training batch 1301, loss: 0.270, 41632/60000 datapoints
2025-03-06 18:14:44,187 - INFO - training batch 1351, loss: 0.373, 43232/60000 datapoints
2025-03-06 18:14:44,400 - INFO - training batch 1401, loss: 0.259, 44832/60000 datapoints
2025-03-06 18:14:44,600 - INFO - training batch 1451, loss: 0.424, 46432/60000 datapoints
2025-03-06 18:14:44,796 - INFO - training batch 1501, loss: 0.253, 48032/60000 datapoints
2025-03-06 18:14:45,002 - INFO - training batch 1551, loss: 0.256, 49632/60000 datapoints
2025-03-06 18:14:45,197 - INFO - training batch 1601, loss: 0.262, 51232/60000 datapoints
2025-03-06 18:14:45,397 - INFO - training batch 1651, loss: 0.330, 52832/60000 datapoints
2025-03-06 18:14:45,598 - INFO - training batch 1701, loss: 0.266, 54432/60000 datapoints
2025-03-06 18:14:45,795 - INFO - training batch 1751, loss: 0.418, 56032/60000 datapoints
2025-03-06 18:14:45,996 - INFO - training batch 1801, loss: 0.397, 57632/60000 datapoints
2025-03-06 18:14:46,202 - INFO - training batch 1851, loss: 0.269, 59232/60000 datapoints
2025-03-06 18:14:46,307 - INFO - validation batch 1, loss: 0.183, 32/10016 datapoints
2025-03-06 18:14:46,474 - INFO - validation batch 51, loss: 0.390, 1632/10016 datapoints
2025-03-06 18:14:46,637 - INFO - validation batch 101, loss: 0.411, 3232/10016 datapoints
2025-03-06 18:14:46,795 - INFO - validation batch 151, loss: 0.149, 4832/10016 datapoints
2025-03-06 18:14:46,952 - INFO - validation batch 201, loss: 0.323, 6432/10016 datapoints
2025-03-06 18:14:47,108 - INFO - validation batch 251, loss: 0.176, 8032/10016 datapoints
2025-03-06 18:14:47,267 - INFO - validation batch 301, loss: 0.260, 9632/10016 datapoints
2025-03-06 18:14:47,305 - INFO - Epoch 221/800 done.
2025-03-06 18:14:47,305 - INFO - Final validation performance:
Loss: 0.270, top-1 acc: 0.905top-5 acc: 0.905
2025-03-06 18:14:47,305 - INFO - Beginning epoch 222/800
2025-03-06 18:14:47,311 - INFO - training batch 1, loss: 0.224, 32/60000 datapoints
2025-03-06 18:14:47,509 - INFO - training batch 51, loss: 0.607, 1632/60000 datapoints
2025-03-06 18:14:47,704 - INFO - training batch 101, loss: 0.406, 3232/60000 datapoints
2025-03-06 18:14:47,902 - INFO - training batch 151, loss: 0.088, 4832/60000 datapoints
2025-03-06 18:14:48,097 - INFO - training batch 201, loss: 0.176, 6432/60000 datapoints
2025-03-06 18:14:48,292 - INFO - training batch 251, loss: 0.386, 8032/60000 datapoints
2025-03-06 18:14:48,486 - INFO - training batch 301, loss: 0.450, 9632/60000 datapoints
2025-03-06 18:14:48,681 - INFO - training batch 351, loss: 0.714, 11232/60000 datapoints
2025-03-06 18:14:48,876 - INFO - training batch 401, loss: 0.240, 12832/60000 datapoints
2025-03-06 18:14:49,071 - INFO - training batch 451, loss: 0.257, 14432/60000 datapoints
2025-03-06 18:14:49,265 - INFO - training batch 501, loss: 0.180, 16032/60000 datapoints
2025-03-06 18:14:49,461 - INFO - training batch 551, loss: 0.240, 17632/60000 datapoints
2025-03-06 18:14:49,660 - INFO - training batch 601, loss: 0.333, 19232/60000 datapoints
2025-03-06 18:14:49,853 - INFO - training batch 651, loss: 0.258, 20832/60000 datapoints
2025-03-06 18:14:50,051 - INFO - training batch 701, loss: 0.183, 22432/60000 datapoints
2025-03-06 18:14:50,249 - INFO - training batch 751, loss: 0.312, 24032/60000 datapoints
2025-03-06 18:14:50,446 - INFO - training batch 801, loss: 0.270, 25632/60000 datapoints
2025-03-06 18:14:50,648 - INFO - training batch 851, loss: 0.370, 27232/60000 datapoints
2025-03-06 18:14:50,848 - INFO - training batch 901, loss: 0.404, 28832/60000 datapoints
2025-03-06 18:14:51,043 - INFO - training batch 951, loss: 0.164, 30432/60000 datapoints
2025-03-06 18:14:51,238 - INFO - training batch 1001, loss: 0.333, 32032/60000 datapoints
2025-03-06 18:14:51,432 - INFO - training batch 1051, loss: 0.419, 33632/60000 datapoints
2025-03-06 18:14:51,627 - INFO - training batch 1101, loss: 0.349, 35232/60000 datapoints
2025-03-06 18:14:51,821 - INFO - training batch 1151, loss: 0.337, 36832/60000 datapoints
2025-03-06 18:14:52,024 - INFO - training batch 1201, loss: 0.231, 38432/60000 datapoints
2025-03-06 18:14:52,221 - INFO - training batch 1251, loss: 0.269, 40032/60000 datapoints
2025-03-06 18:14:52,418 - INFO - training batch 1301, loss: 0.421, 41632/60000 datapoints
2025-03-06 18:14:52,620 - INFO - training batch 1351, loss: 0.465, 43232/60000 datapoints
2025-03-06 18:14:52,817 - INFO - training batch 1401, loss: 0.307, 44832/60000 datapoints
2025-03-06 18:14:53,015 - INFO - training batch 1451, loss: 0.279, 46432/60000 datapoints
2025-03-06 18:14:53,216 - INFO - training batch 1501, loss: 0.178, 48032/60000 datapoints
2025-03-06 18:14:53,415 - INFO - training batch 1551, loss: 0.404, 49632/60000 datapoints
2025-03-06 18:14:53,616 - INFO - training batch 1601, loss: 0.190, 51232/60000 datapoints
2025-03-06 18:14:53,815 - INFO - training batch 1651, loss: 0.346, 52832/60000 datapoints
2025-03-06 18:14:54,024 - INFO - training batch 1701, loss: 0.114, 54432/60000 datapoints
2025-03-06 18:14:54,224 - INFO - training batch 1751, loss: 0.285, 56032/60000 datapoints
2025-03-06 18:14:54,426 - INFO - training batch 1801, loss: 0.376, 57632/60000 datapoints
2025-03-06 18:14:54,638 - INFO - training batch 1851, loss: 0.483, 59232/60000 datapoints
2025-03-06 18:14:54,739 - INFO - validation batch 1, loss: 0.358, 32/10016 datapoints
2025-03-06 18:14:54,896 - INFO - validation batch 51, loss: 0.255, 1632/10016 datapoints
2025-03-06 18:14:55,052 - INFO - validation batch 101, loss: 0.432, 3232/10016 datapoints
2025-03-06 18:14:55,211 - INFO - validation batch 151, loss: 0.208, 4832/10016 datapoints
2025-03-06 18:14:55,367 - INFO - validation batch 201, loss: 0.474, 6432/10016 datapoints
2025-03-06 18:14:55,524 - INFO - validation batch 251, loss: 0.143, 8032/10016 datapoints
2025-03-06 18:14:55,683 - INFO - validation batch 301, loss: 0.322, 9632/10016 datapoints
2025-03-06 18:14:55,720 - INFO - Epoch 222/800 done.
2025-03-06 18:14:55,720 - INFO - Final validation performance:
Loss: 0.313, top-1 acc: 0.905top-5 acc: 0.905
2025-03-06 18:14:55,720 - INFO - Beginning epoch 223/800
2025-03-06 18:14:55,726 - INFO - training batch 1, loss: 0.261, 32/60000 datapoints
2025-03-06 18:14:55,929 - INFO - training batch 51, loss: 0.362, 1632/60000 datapoints
2025-03-06 18:14:56,128 - INFO - training batch 101, loss: 0.299, 3232/60000 datapoints
2025-03-06 18:14:56,329 - INFO - training batch 151, loss: 0.170, 4832/60000 datapoints
2025-03-06 18:14:56,526 - INFO - training batch 201, loss: 0.354, 6432/60000 datapoints
2025-03-06 18:14:56,725 - INFO - training batch 251, loss: 0.401, 8032/60000 datapoints
2025-03-06 18:14:56,925 - INFO - training batch 301, loss: 0.120, 9632/60000 datapoints
2025-03-06 18:14:57,118 - INFO - training batch 351, loss: 0.699, 11232/60000 datapoints
2025-03-06 18:14:57,312 - INFO - training batch 401, loss: 0.413, 12832/60000 datapoints
2025-03-06 18:14:57,511 - INFO - training batch 451, loss: 0.291, 14432/60000 datapoints
2025-03-06 18:14:57,712 - INFO - training batch 501, loss: 0.655, 16032/60000 datapoints
2025-03-06 18:14:57,913 - INFO - training batch 551, loss: 0.090, 17632/60000 datapoints
2025-03-06 18:14:58,112 - INFO - training batch 601, loss: 0.537, 19232/60000 datapoints
2025-03-06 18:14:58,312 - INFO - training batch 651, loss: 0.555, 20832/60000 datapoints
2025-03-06 18:14:58,513 - INFO - training batch 701, loss: 0.428, 22432/60000 datapoints
2025-03-06 18:14:58,713 - INFO - training batch 751, loss: 0.373, 24032/60000 datapoints
2025-03-06 18:14:58,912 - INFO - training batch 801, loss: 0.420, 25632/60000 datapoints
2025-03-06 18:14:59,110 - INFO - training batch 851, loss: 0.344, 27232/60000 datapoints
2025-03-06 18:14:59,306 - INFO - training batch 901, loss: 0.465, 28832/60000 datapoints
2025-03-06 18:14:59,509 - INFO - training batch 951, loss: 0.384, 30432/60000 datapoints
2025-03-06 18:14:59,712 - INFO - training batch 1001, loss: 0.530, 32032/60000 datapoints
2025-03-06 18:14:59,910 - INFO - training batch 1051, loss: 0.649, 33632/60000 datapoints
2025-03-06 18:15:00,110 - INFO - training batch 1101, loss: 0.244, 35232/60000 datapoints
2025-03-06 18:15:00,314 - INFO - training batch 1151, loss: 0.167, 36832/60000 datapoints
2025-03-06 18:15:00,520 - INFO - training batch 1201, loss: 0.369, 38432/60000 datapoints
2025-03-06 18:15:00,726 - INFO - training batch 1251, loss: 0.323, 40032/60000 datapoints
2025-03-06 18:15:00,926 - INFO - training batch 1301, loss: 0.252, 41632/60000 datapoints
2025-03-06 18:15:01,130 - INFO - training batch 1351, loss: 0.451, 43232/60000 datapoints
2025-03-06 18:15:01,334 - INFO - training batch 1401, loss: 0.153, 44832/60000 datapoints
2025-03-06 18:15:01,542 - INFO - training batch 1451, loss: 0.396, 46432/60000 datapoints
2025-03-06 18:15:01,746 - INFO - training batch 1501, loss: 0.446, 48032/60000 datapoints
2025-03-06 18:15:01,952 - INFO - training batch 1551, loss: 0.250, 49632/60000 datapoints
2025-03-06 18:15:02,166 - INFO - training batch 1601, loss: 0.625, 51232/60000 datapoints
2025-03-06 18:15:02,363 - INFO - training batch 1651, loss: 0.309, 52832/60000 datapoints
2025-03-06 18:15:02,563 - INFO - training batch 1701, loss: 0.688, 54432/60000 datapoints
2025-03-06 18:15:02,774 - INFO - training batch 1751, loss: 0.413, 56032/60000 datapoints
2025-03-06 18:15:02,982 - INFO - training batch 1801, loss: 0.299, 57632/60000 datapoints
2025-03-06 18:15:03,184 - INFO - training batch 1851, loss: 0.327, 59232/60000 datapoints
2025-03-06 18:15:03,288 - INFO - validation batch 1, loss: 0.373, 32/10016 datapoints
2025-03-06 18:15:03,447 - INFO - validation batch 51, loss: 0.500, 1632/10016 datapoints
2025-03-06 18:15:03,603 - INFO - validation batch 101, loss: 0.263, 3232/10016 datapoints
2025-03-06 18:15:03,760 - INFO - validation batch 151, loss: 0.454, 4832/10016 datapoints
2025-03-06 18:15:03,915 - INFO - validation batch 201, loss: 0.441, 6432/10016 datapoints
2025-03-06 18:15:04,080 - INFO - validation batch 251, loss: 0.388, 8032/10016 datapoints
2025-03-06 18:15:04,254 - INFO - validation batch 301, loss: 0.290, 9632/10016 datapoints
2025-03-06 18:15:04,305 - INFO - Epoch 223/800 done.
2025-03-06 18:15:04,306 - INFO - Final validation performance:
Loss: 0.387, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 18:15:04,306 - INFO - Beginning epoch 224/800
2025-03-06 18:15:04,314 - INFO - training batch 1, loss: 0.554, 32/60000 datapoints
2025-03-06 18:15:04,621 - INFO - training batch 51, loss: 0.450, 1632/60000 datapoints
2025-03-06 18:15:04,883 - INFO - training batch 101, loss: 0.512, 3232/60000 datapoints
2025-03-06 18:15:05,195 - INFO - training batch 151, loss: 0.557, 4832/60000 datapoints
2025-03-06 18:15:05,439 - INFO - training batch 201, loss: 0.590, 6432/60000 datapoints
2025-03-06 18:15:05,688 - INFO - training batch 251, loss: 0.429, 8032/60000 datapoints
2025-03-06 18:15:05,924 - INFO - training batch 301, loss: 0.229, 9632/60000 datapoints
2025-03-06 18:15:06,158 - INFO - training batch 351, loss: 0.141, 11232/60000 datapoints
2025-03-06 18:15:06,425 - INFO - training batch 401, loss: 0.400, 12832/60000 datapoints
2025-03-06 18:15:06,644 - INFO - training batch 451, loss: 0.383, 14432/60000 datapoints
2025-03-06 18:15:06,857 - INFO - training batch 501, loss: 0.328, 16032/60000 datapoints
2025-03-06 18:15:07,062 - INFO - training batch 551, loss: 0.329, 17632/60000 datapoints
2025-03-06 18:15:07,272 - INFO - training batch 601, loss: 0.336, 19232/60000 datapoints
2025-03-06 18:15:07,478 - INFO - training batch 651, loss: 0.311, 20832/60000 datapoints
2025-03-06 18:15:07,688 - INFO - training batch 701, loss: 0.219, 22432/60000 datapoints
2025-03-06 18:15:07,892 - INFO - training batch 751, loss: 0.333, 24032/60000 datapoints
2025-03-06 18:15:08,104 - INFO - training batch 801, loss: 0.144, 25632/60000 datapoints
2025-03-06 18:15:08,305 - INFO - training batch 851, loss: 0.238, 27232/60000 datapoints
2025-03-06 18:15:08,511 - INFO - training batch 901, loss: 0.140, 28832/60000 datapoints
2025-03-06 18:15:08,718 - INFO - training batch 951, loss: 0.270, 30432/60000 datapoints
2025-03-06 18:15:08,921 - INFO - training batch 1001, loss: 0.195, 32032/60000 datapoints
2025-03-06 18:15:09,126 - INFO - training batch 1051, loss: 0.348, 33632/60000 datapoints
2025-03-06 18:15:09,328 - INFO - training batch 1101, loss: 0.510, 35232/60000 datapoints
2025-03-06 18:15:09,530 - INFO - training batch 1151, loss: 0.367, 36832/60000 datapoints
2025-03-06 18:15:09,730 - INFO - training batch 1201, loss: 0.281, 38432/60000 datapoints
2025-03-06 18:15:09,938 - INFO - training batch 1251, loss: 0.157, 40032/60000 datapoints
2025-03-06 18:15:10,145 - INFO - training batch 1301, loss: 0.317, 41632/60000 datapoints
2025-03-06 18:15:10,351 - INFO - training batch 1351, loss: 0.457, 43232/60000 datapoints
2025-03-06 18:15:10,555 - INFO - training batch 1401, loss: 0.218, 44832/60000 datapoints
2025-03-06 18:15:10,763 - INFO - training batch 1451, loss: 0.285, 46432/60000 datapoints
2025-03-06 18:15:10,963 - INFO - training batch 1501, loss: 0.479, 48032/60000 datapoints
2025-03-06 18:15:11,156 - INFO - training batch 1551, loss: 0.168, 49632/60000 datapoints
2025-03-06 18:15:11,354 - INFO - training batch 1601, loss: 0.468, 51232/60000 datapoints
2025-03-06 18:15:11,548 - INFO - training batch 1651, loss: 0.350, 52832/60000 datapoints
2025-03-06 18:15:11,746 - INFO - training batch 1701, loss: 0.238, 54432/60000 datapoints
2025-03-06 18:15:11,944 - INFO - training batch 1751, loss: 0.236, 56032/60000 datapoints
2025-03-06 18:15:12,142 - INFO - training batch 1801, loss: 0.300, 57632/60000 datapoints
2025-03-06 18:15:12,339 - INFO - training batch 1851, loss: 0.524, 59232/60000 datapoints
2025-03-06 18:15:12,439 - INFO - validation batch 1, loss: 0.320, 32/10016 datapoints
2025-03-06 18:15:12,591 - INFO - validation batch 51, loss: 0.193, 1632/10016 datapoints
2025-03-06 18:15:12,746 - INFO - validation batch 101, loss: 0.226, 3232/10016 datapoints
2025-03-06 18:15:12,898 - INFO - validation batch 151, loss: 0.164, 4832/10016 datapoints
2025-03-06 18:15:13,049 - INFO - validation batch 201, loss: 0.338, 6432/10016 datapoints
2025-03-06 18:15:13,204 - INFO - validation batch 251, loss: 0.264, 8032/10016 datapoints
2025-03-06 18:15:13,361 - INFO - validation batch 301, loss: 0.418, 9632/10016 datapoints
2025-03-06 18:15:13,398 - INFO - Epoch 224/800 done.
2025-03-06 18:15:13,398 - INFO - Final validation performance:
Loss: 0.275, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 18:15:13,398 - INFO - Beginning epoch 225/800
2025-03-06 18:15:13,404 - INFO - training batch 1, loss: 0.228, 32/60000 datapoints
2025-03-06 18:15:13,606 - INFO - training batch 51, loss: 0.238, 1632/60000 datapoints
2025-03-06 18:15:13,808 - INFO - training batch 101, loss: 0.205, 3232/60000 datapoints
2025-03-06 18:15:14,008 - INFO - training batch 151, loss: 0.413, 4832/60000 datapoints
2025-03-06 18:15:14,209 - INFO - training batch 201, loss: 0.757, 6432/60000 datapoints
2025-03-06 18:15:14,406 - INFO - training batch 251, loss: 0.295, 8032/60000 datapoints
2025-03-06 18:15:14,607 - INFO - training batch 301, loss: 0.184, 9632/60000 datapoints
2025-03-06 18:15:14,823 - INFO - training batch 351, loss: 0.525, 11232/60000 datapoints
2025-03-06 18:15:15,019 - INFO - training batch 401, loss: 0.255, 12832/60000 datapoints
2025-03-06 18:15:15,224 - INFO - training batch 451, loss: 0.263, 14432/60000 datapoints
2025-03-06 18:15:15,423 - INFO - training batch 501, loss: 0.466, 16032/60000 datapoints
2025-03-06 18:15:15,618 - INFO - training batch 551, loss: 0.103, 17632/60000 datapoints
2025-03-06 18:15:15,809 - INFO - training batch 601, loss: 0.453, 19232/60000 datapoints
2025-03-06 18:15:16,002 - INFO - training batch 651, loss: 0.291, 20832/60000 datapoints
2025-03-06 18:15:16,196 - INFO - training batch 701, loss: 0.374, 22432/60000 datapoints
2025-03-06 18:15:16,387 - INFO - training batch 751, loss: 0.140, 24032/60000 datapoints
2025-03-06 18:15:16,587 - INFO - training batch 801, loss: 0.554, 25632/60000 datapoints
2025-03-06 18:15:16,781 - INFO - training batch 851, loss: 0.181, 27232/60000 datapoints
2025-03-06 18:15:16,971 - INFO - training batch 901, loss: 0.367, 28832/60000 datapoints
2025-03-06 18:15:17,161 - INFO - training batch 951, loss: 0.322, 30432/60000 datapoints
2025-03-06 18:15:17,353 - INFO - training batch 1001, loss: 0.332, 32032/60000 datapoints
2025-03-06 18:15:17,546 - INFO - training batch 1051, loss: 0.541, 33632/60000 datapoints
2025-03-06 18:15:17,737 - INFO - training batch 1101, loss: 0.518, 35232/60000 datapoints
2025-03-06 18:15:17,929 - INFO - training batch 1151, loss: 0.404, 36832/60000 datapoints
2025-03-06 18:15:18,123 - INFO - training batch 1201, loss: 0.261, 38432/60000 datapoints
2025-03-06 18:15:18,314 - INFO - training batch 1251, loss: 0.153, 40032/60000 datapoints
2025-03-06 18:15:18,506 - INFO - training batch 1301, loss: 0.263, 41632/60000 datapoints
2025-03-06 18:15:18,698 - INFO - training batch 1351, loss: 0.285, 43232/60000 datapoints
2025-03-06 18:15:18,891 - INFO - training batch 1401, loss: 0.352, 44832/60000 datapoints
2025-03-06 18:15:19,083 - INFO - training batch 1451, loss: 0.234, 46432/60000 datapoints
2025-03-06 18:15:19,275 - INFO - training batch 1501, loss: 0.484, 48032/60000 datapoints
2025-03-06 18:15:19,469 - INFO - training batch 1551, loss: 0.232, 49632/60000 datapoints
2025-03-06 18:15:19,670 - INFO - training batch 1601, loss: 0.293, 51232/60000 datapoints
2025-03-06 18:15:19,862 - INFO - training batch 1651, loss: 0.382, 52832/60000 datapoints
2025-03-06 18:15:20,059 - INFO - training batch 1701, loss: 0.232, 54432/60000 datapoints
2025-03-06 18:15:20,253 - INFO - training batch 1751, loss: 0.319, 56032/60000 datapoints
2025-03-06 18:15:20,445 - INFO - training batch 1801, loss: 0.326, 57632/60000 datapoints
2025-03-06 18:15:20,644 - INFO - training batch 1851, loss: 0.373, 59232/60000 datapoints
2025-03-06 18:15:20,746 - INFO - validation batch 1, loss: 0.427, 32/10016 datapoints
2025-03-06 18:15:20,901 - INFO - validation batch 51, loss: 0.358, 1632/10016 datapoints
2025-03-06 18:15:21,053 - INFO - validation batch 101, loss: 0.691, 3232/10016 datapoints
2025-03-06 18:15:21,206 - INFO - validation batch 151, loss: 0.251, 4832/10016 datapoints
2025-03-06 18:15:21,360 - INFO - validation batch 201, loss: 0.127, 6432/10016 datapoints
2025-03-06 18:15:21,518 - INFO - validation batch 251, loss: 0.441, 8032/10016 datapoints
2025-03-06 18:15:21,675 - INFO - validation batch 301, loss: 0.289, 9632/10016 datapoints
2025-03-06 18:15:21,713 - INFO - Epoch 225/800 done.
2025-03-06 18:15:21,713 - INFO - Final validation performance:
Loss: 0.369, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 18:15:21,713 - INFO - Beginning epoch 226/800
2025-03-06 18:15:21,720 - INFO - training batch 1, loss: 0.449, 32/60000 datapoints
2025-03-06 18:15:21,919 - INFO - training batch 51, loss: 0.616, 1632/60000 datapoints
2025-03-06 18:15:22,114 - INFO - training batch 101, loss: 0.825, 3232/60000 datapoints
2025-03-06 18:15:22,309 - INFO - training batch 151, loss: 0.296, 4832/60000 datapoints
2025-03-06 18:15:22,504 - INFO - training batch 201, loss: 0.290, 6432/60000 datapoints
2025-03-06 18:15:22,702 - INFO - training batch 251, loss: 0.134, 8032/60000 datapoints
2025-03-06 18:15:22,895 - INFO - training batch 301, loss: 0.439, 9632/60000 datapoints
2025-03-06 18:15:23,088 - INFO - training batch 351, loss: 0.285, 11232/60000 datapoints
2025-03-06 18:15:23,283 - INFO - training batch 401, loss: 0.504, 12832/60000 datapoints
2025-03-06 18:15:23,476 - INFO - training batch 451, loss: 0.262, 14432/60000 datapoints
2025-03-06 18:15:23,670 - INFO - training batch 501, loss: 0.475, 16032/60000 datapoints
2025-03-06 18:15:23,864 - INFO - training batch 551, loss: 0.339, 17632/60000 datapoints
2025-03-06 18:15:24,063 - INFO - training batch 601, loss: 0.423, 19232/60000 datapoints
2025-03-06 18:15:24,256 - INFO - training batch 651, loss: 0.308, 20832/60000 datapoints
2025-03-06 18:15:24,451 - INFO - training batch 701, loss: 0.381, 22432/60000 datapoints
2025-03-06 18:15:24,646 - INFO - training batch 751, loss: 0.257, 24032/60000 datapoints
2025-03-06 18:15:24,864 - INFO - training batch 801, loss: 0.292, 25632/60000 datapoints
2025-03-06 18:15:25,068 - INFO - training batch 851, loss: 0.526, 27232/60000 datapoints
2025-03-06 18:15:25,267 - INFO - training batch 901, loss: 0.219, 28832/60000 datapoints
2025-03-06 18:15:25,462 - INFO - training batch 951, loss: 0.455, 30432/60000 datapoints
2025-03-06 18:15:25,659 - INFO - training batch 1001, loss: 0.218, 32032/60000 datapoints
2025-03-06 18:15:25,853 - INFO - training batch 1051, loss: 0.300, 33632/60000 datapoints
2025-03-06 18:15:26,052 - INFO - training batch 1101, loss: 0.324, 35232/60000 datapoints
2025-03-06 18:15:26,249 - INFO - training batch 1151, loss: 0.186, 36832/60000 datapoints
2025-03-06 18:15:26,444 - INFO - training batch 1201, loss: 0.178, 38432/60000 datapoints
2025-03-06 18:15:26,638 - INFO - training batch 1251, loss: 0.668, 40032/60000 datapoints
2025-03-06 18:15:26,831 - INFO - training batch 1301, loss: 0.398, 41632/60000 datapoints
2025-03-06 18:15:27,023 - INFO - training batch 1351, loss: 0.244, 43232/60000 datapoints
2025-03-06 18:15:27,215 - INFO - training batch 1401, loss: 0.227, 44832/60000 datapoints
2025-03-06 18:15:27,412 - INFO - training batch 1451, loss: 0.283, 46432/60000 datapoints
2025-03-06 18:15:27,607 - INFO - training batch 1501, loss: 0.351, 48032/60000 datapoints
2025-03-06 18:15:27,800 - INFO - training batch 1551, loss: 0.274, 49632/60000 datapoints
2025-03-06 18:15:27,996 - INFO - training batch 1601, loss: 0.151, 51232/60000 datapoints
2025-03-06 18:15:28,191 - INFO - training batch 1651, loss: 0.542, 52832/60000 datapoints
2025-03-06 18:15:28,385 - INFO - training batch 1701, loss: 0.338, 54432/60000 datapoints
2025-03-06 18:15:28,579 - INFO - training batch 1751, loss: 0.524, 56032/60000 datapoints
2025-03-06 18:15:28,778 - INFO - training batch 1801, loss: 0.283, 57632/60000 datapoints
2025-03-06 18:15:28,972 - INFO - training batch 1851, loss: 0.307, 59232/60000 datapoints
2025-03-06 18:15:29,073 - INFO - validation batch 1, loss: 0.272, 32/10016 datapoints
2025-03-06 18:15:29,226 - INFO - validation batch 51, loss: 0.173, 1632/10016 datapoints
2025-03-06 18:15:29,377 - INFO - validation batch 101, loss: 0.280, 3232/10016 datapoints
2025-03-06 18:15:29,534 - INFO - validation batch 151, loss: 0.274, 4832/10016 datapoints
2025-03-06 18:15:29,690 - INFO - validation batch 201, loss: 0.448, 6432/10016 datapoints
2025-03-06 18:15:29,842 - INFO - validation batch 251, loss: 0.398, 8032/10016 datapoints
2025-03-06 18:15:29,999 - INFO - validation batch 301, loss: 0.407, 9632/10016 datapoints
2025-03-06 18:15:30,036 - INFO - Epoch 226/800 done.
2025-03-06 18:15:30,037 - INFO - Final validation performance:
Loss: 0.322, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 18:15:30,037 - INFO - Beginning epoch 227/800
2025-03-06 18:15:30,043 - INFO - training batch 1, loss: 0.258, 32/60000 datapoints
2025-03-06 18:15:30,236 - INFO - training batch 51, loss: 0.218, 1632/60000 datapoints
2025-03-06 18:15:30,431 - INFO - training batch 101, loss: 0.220, 3232/60000 datapoints
2025-03-06 18:15:30,625 - INFO - training batch 151, loss: 0.453, 4832/60000 datapoints
2025-03-06 18:15:30,819 - INFO - training batch 201, loss: 0.123, 6432/60000 datapoints
2025-03-06 18:15:31,014 - INFO - training batch 251, loss: 0.362, 8032/60000 datapoints
2025-03-06 18:15:31,208 - INFO - training batch 301, loss: 0.305, 9632/60000 datapoints
2025-03-06 18:15:31,402 - INFO - training batch 351, loss: 0.300, 11232/60000 datapoints
2025-03-06 18:15:31,596 - INFO - training batch 401, loss: 0.471, 12832/60000 datapoints
2025-03-06 18:15:31,792 - INFO - training batch 451, loss: 0.348, 14432/60000 datapoints
2025-03-06 18:15:31,988 - INFO - training batch 501, loss: 0.420, 16032/60000 datapoints
2025-03-06 18:15:32,187 - INFO - training batch 551, loss: 0.346, 17632/60000 datapoints
2025-03-06 18:15:32,381 - INFO - training batch 601, loss: 0.479, 19232/60000 datapoints
2025-03-06 18:15:32,580 - INFO - training batch 651, loss: 0.266, 20832/60000 datapoints
2025-03-06 18:15:32,778 - INFO - training batch 701, loss: 0.438, 22432/60000 datapoints
2025-03-06 18:15:32,973 - INFO - training batch 751, loss: 0.346, 24032/60000 datapoints
2025-03-06 18:15:33,165 - INFO - training batch 801, loss: 0.431, 25632/60000 datapoints
2025-03-06 18:15:33,361 - INFO - training batch 851, loss: 0.182, 27232/60000 datapoints
2025-03-06 18:15:33,557 - INFO - training batch 901, loss: 0.286, 28832/60000 datapoints
2025-03-06 18:15:33,750 - INFO - training batch 951, loss: 0.681, 30432/60000 datapoints
2025-03-06 18:15:33,946 - INFO - training batch 1001, loss: 0.156, 32032/60000 datapoints
2025-03-06 18:15:34,143 - INFO - training batch 1051, loss: 0.249, 33632/60000 datapoints
2025-03-06 18:15:34,337 - INFO - training batch 1101, loss: 0.218, 35232/60000 datapoints
2025-03-06 18:15:34,532 - INFO - training batch 1151, loss: 0.368, 36832/60000 datapoints
2025-03-06 18:15:34,747 - INFO - training batch 1201, loss: 0.158, 38432/60000 datapoints
2025-03-06 18:15:34,986 - INFO - training batch 1251, loss: 0.331, 40032/60000 datapoints
2025-03-06 18:15:35,183 - INFO - training batch 1301, loss: 0.254, 41632/60000 datapoints
2025-03-06 18:15:35,378 - INFO - training batch 1351, loss: 0.463, 43232/60000 datapoints
2025-03-06 18:15:35,575 - INFO - training batch 1401, loss: 0.364, 44832/60000 datapoints
2025-03-06 18:15:35,770 - INFO - training batch 1451, loss: 0.468, 46432/60000 datapoints
2025-03-06 18:15:35,964 - INFO - training batch 1501, loss: 0.213, 48032/60000 datapoints
2025-03-06 18:15:36,162 - INFO - training batch 1551, loss: 0.216, 49632/60000 datapoints
2025-03-06 18:15:36,363 - INFO - training batch 1601, loss: 0.324, 51232/60000 datapoints
2025-03-06 18:15:36,556 - INFO - training batch 1651, loss: 0.646, 52832/60000 datapoints
2025-03-06 18:15:36,756 - INFO - training batch 1701, loss: 0.175, 54432/60000 datapoints
2025-03-06 18:15:36,953 - INFO - training batch 1751, loss: 0.439, 56032/60000 datapoints
2025-03-06 18:15:37,150 - INFO - training batch 1801, loss: 0.588, 57632/60000 datapoints
2025-03-06 18:15:37,368 - INFO - training batch 1851, loss: 0.366, 59232/60000 datapoints
2025-03-06 18:15:37,476 - INFO - validation batch 1, loss: 0.205, 32/10016 datapoints
2025-03-06 18:15:37,647 - INFO - validation batch 51, loss: 0.229, 1632/10016 datapoints
2025-03-06 18:15:37,798 - INFO - validation batch 101, loss: 0.576, 3232/10016 datapoints
2025-03-06 18:15:37,950 - INFO - validation batch 151, loss: 0.420, 4832/10016 datapoints
2025-03-06 18:15:38,104 - INFO - validation batch 201, loss: 0.284, 6432/10016 datapoints
2025-03-06 18:15:38,260 - INFO - validation batch 251, loss: 0.407, 8032/10016 datapoints
2025-03-06 18:15:38,413 - INFO - validation batch 301, loss: 0.483, 9632/10016 datapoints
2025-03-06 18:15:38,449 - INFO - Epoch 227/800 done.
2025-03-06 18:15:38,449 - INFO - Final validation performance:
Loss: 0.372, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 18:15:38,449 - INFO - Beginning epoch 228/800
2025-03-06 18:15:38,456 - INFO - training batch 1, loss: 0.325, 32/60000 datapoints
2025-03-06 18:15:38,656 - INFO - training batch 51, loss: 0.751, 1632/60000 datapoints
2025-03-06 18:15:38,851 - INFO - training batch 101, loss: 0.665, 3232/60000 datapoints
2025-03-06 18:15:39,054 - INFO - training batch 151, loss: 0.196, 4832/60000 datapoints
2025-03-06 18:15:39,249 - INFO - training batch 201, loss: 0.134, 6432/60000 datapoints
2025-03-06 18:15:39,446 - INFO - training batch 251, loss: 0.222, 8032/60000 datapoints
2025-03-06 18:15:39,644 - INFO - training batch 301, loss: 0.399, 9632/60000 datapoints
2025-03-06 18:15:39,837 - INFO - training batch 351, loss: 0.830, 11232/60000 datapoints
2025-03-06 18:15:40,032 - INFO - training batch 401, loss: 0.286, 12832/60000 datapoints
2025-03-06 18:15:40,230 - INFO - training batch 451, loss: 0.396, 14432/60000 datapoints
2025-03-06 18:15:40,424 - INFO - training batch 501, loss: 0.335, 16032/60000 datapoints
2025-03-06 18:15:40,622 - INFO - training batch 551, loss: 0.323, 17632/60000 datapoints
2025-03-06 18:15:40,816 - INFO - training batch 601, loss: 0.327, 19232/60000 datapoints
2025-03-06 18:15:41,013 - INFO - training batch 651, loss: 0.494, 20832/60000 datapoints
2025-03-06 18:15:41,205 - INFO - training batch 701, loss: 0.165, 22432/60000 datapoints
2025-03-06 18:15:41,397 - INFO - training batch 751, loss: 0.415, 24032/60000 datapoints
2025-03-06 18:15:41,591 - INFO - training batch 801, loss: 0.532, 25632/60000 datapoints
2025-03-06 18:15:41,787 - INFO - training batch 851, loss: 0.325, 27232/60000 datapoints
2025-03-06 18:15:41,981 - INFO - training batch 901, loss: 0.243, 28832/60000 datapoints
2025-03-06 18:15:42,179 - INFO - training batch 951, loss: 0.613, 30432/60000 datapoints
2025-03-06 18:15:42,374 - INFO - training batch 1001, loss: 0.400, 32032/60000 datapoints
2025-03-06 18:15:42,571 - INFO - training batch 1051, loss: 0.183, 33632/60000 datapoints
2025-03-06 18:15:42,766 - INFO - training batch 1101, loss: 0.522, 35232/60000 datapoints
2025-03-06 18:15:42,961 - INFO - training batch 1151, loss: 0.380, 36832/60000 datapoints
2025-03-06 18:15:43,156 - INFO - training batch 1201, loss: 0.180, 38432/60000 datapoints
2025-03-06 18:15:43,351 - INFO - training batch 1251, loss: 0.544, 40032/60000 datapoints
2025-03-06 18:15:43,546 - INFO - training batch 1301, loss: 0.362, 41632/60000 datapoints
2025-03-06 18:15:43,746 - INFO - training batch 1351, loss: 0.247, 43232/60000 datapoints
2025-03-06 18:15:43,940 - INFO - training batch 1401, loss: 0.276, 44832/60000 datapoints
2025-03-06 18:15:44,138 - INFO - training batch 1451, loss: 0.280, 46432/60000 datapoints
2025-03-06 18:15:44,331 - INFO - training batch 1501, loss: 0.325, 48032/60000 datapoints
2025-03-06 18:15:44,525 - INFO - training batch 1551, loss: 0.383, 49632/60000 datapoints
2025-03-06 18:15:44,720 - INFO - training batch 1601, loss: 0.463, 51232/60000 datapoints
2025-03-06 18:15:44,932 - INFO - training batch 1651, loss: 0.329, 52832/60000 datapoints
2025-03-06 18:15:45,135 - INFO - training batch 1701, loss: 0.209, 54432/60000 datapoints
2025-03-06 18:15:45,330 - INFO - training batch 1751, loss: 0.253, 56032/60000 datapoints
2025-03-06 18:15:45,526 - INFO - training batch 1801, loss: 0.190, 57632/60000 datapoints
2025-03-06 18:15:45,726 - INFO - training batch 1851, loss: 0.446, 59232/60000 datapoints
2025-03-06 18:15:45,827 - INFO - validation batch 1, loss: 0.240, 32/10016 datapoints
2025-03-06 18:15:45,981 - INFO - validation batch 51, loss: 0.416, 1632/10016 datapoints
2025-03-06 18:15:46,136 - INFO - validation batch 101, loss: 0.307, 3232/10016 datapoints
2025-03-06 18:15:46,292 - INFO - validation batch 151, loss: 0.218, 4832/10016 datapoints
2025-03-06 18:15:46,443 - INFO - validation batch 201, loss: 0.152, 6432/10016 datapoints
2025-03-06 18:15:46,597 - INFO - validation batch 251, loss: 0.303, 8032/10016 datapoints
2025-03-06 18:15:46,749 - INFO - validation batch 301, loss: 0.392, 9632/10016 datapoints
2025-03-06 18:15:46,787 - INFO - Epoch 228/800 done.
2025-03-06 18:15:46,787 - INFO - Final validation performance:
Loss: 0.290, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 18:15:46,787 - INFO - Beginning epoch 229/800
2025-03-06 18:15:46,793 - INFO - training batch 1, loss: 0.294, 32/60000 datapoints
2025-03-06 18:15:46,990 - INFO - training batch 51, loss: 0.205, 1632/60000 datapoints
2025-03-06 18:15:47,186 - INFO - training batch 101, loss: 0.342, 3232/60000 datapoints
2025-03-06 18:15:47,382 - INFO - training batch 151, loss: 0.426, 4832/60000 datapoints
2025-03-06 18:15:47,576 - INFO - training batch 201, loss: 0.327, 6432/60000 datapoints
2025-03-06 18:15:47,772 - INFO - training batch 251, loss: 0.412, 8032/60000 datapoints
2025-03-06 18:15:47,964 - INFO - training batch 301, loss: 0.204, 9632/60000 datapoints
2025-03-06 18:15:48,162 - INFO - training batch 351, loss: 0.249, 11232/60000 datapoints
2025-03-06 18:15:48,359 - INFO - training batch 401, loss: 0.136, 12832/60000 datapoints
2025-03-06 18:15:48,554 - INFO - training batch 451, loss: 0.249, 14432/60000 datapoints
2025-03-06 18:15:48,753 - INFO - training batch 501, loss: 0.221, 16032/60000 datapoints
2025-03-06 18:15:48,946 - INFO - training batch 551, loss: 0.320, 17632/60000 datapoints
2025-03-06 18:15:49,144 - INFO - training batch 601, loss: 0.176, 19232/60000 datapoints
2025-03-06 18:15:49,337 - INFO - training batch 651, loss: 0.499, 20832/60000 datapoints
2025-03-06 18:15:49,535 - INFO - training batch 701, loss: 0.366, 22432/60000 datapoints
2025-03-06 18:15:49,734 - INFO - training batch 751, loss: 0.319, 24032/60000 datapoints
2025-03-06 18:15:49,930 - INFO - training batch 801, loss: 0.236, 25632/60000 datapoints
2025-03-06 18:15:50,130 - INFO - training batch 851, loss: 0.262, 27232/60000 datapoints
2025-03-06 18:15:50,325 - INFO - training batch 901, loss: 0.268, 28832/60000 datapoints
2025-03-06 18:15:50,519 - INFO - training batch 951, loss: 0.330, 30432/60000 datapoints
2025-03-06 18:15:50,719 - INFO - training batch 1001, loss: 0.227, 32032/60000 datapoints
2025-03-06 18:15:50,921 - INFO - training batch 1051, loss: 0.415, 33632/60000 datapoints
2025-03-06 18:15:51,114 - INFO - training batch 1101, loss: 0.688, 35232/60000 datapoints
2025-03-06 18:15:51,310 - INFO - training batch 1151, loss: 0.398, 36832/60000 datapoints
2025-03-06 18:15:51,508 - INFO - training batch 1201, loss: 0.479, 38432/60000 datapoints
2025-03-06 18:15:51,705 - INFO - training batch 1251, loss: 0.214, 40032/60000 datapoints
2025-03-06 18:15:51,901 - INFO - training batch 1301, loss: 0.273, 41632/60000 datapoints
2025-03-06 18:15:52,098 - INFO - training batch 1351, loss: 0.459, 43232/60000 datapoints
2025-03-06 18:15:52,294 - INFO - training batch 1401, loss: 0.676, 44832/60000 datapoints
2025-03-06 18:15:52,489 - INFO - training batch 1451, loss: 0.571, 46432/60000 datapoints
2025-03-06 18:15:52,688 - INFO - training batch 1501, loss: 0.349, 48032/60000 datapoints
2025-03-06 18:15:52,880 - INFO - training batch 1551, loss: 0.433, 49632/60000 datapoints
2025-03-06 18:15:53,073 - INFO - training batch 1601, loss: 0.581, 51232/60000 datapoints
2025-03-06 18:15:53,266 - INFO - training batch 1651, loss: 0.453, 52832/60000 datapoints
2025-03-06 18:15:53,461 - INFO - training batch 1701, loss: 0.166, 54432/60000 datapoints
2025-03-06 18:15:53,658 - INFO - training batch 1751, loss: 0.150, 56032/60000 datapoints
2025-03-06 18:15:53,851 - INFO - training batch 1801, loss: 0.496, 57632/60000 datapoints
2025-03-06 18:15:54,046 - INFO - training batch 1851, loss: 0.611, 59232/60000 datapoints
2025-03-06 18:15:54,148 - INFO - validation batch 1, loss: 0.209, 32/10016 datapoints
2025-03-06 18:15:54,302 - INFO - validation batch 51, loss: 0.241, 1632/10016 datapoints
2025-03-06 18:15:54,453 - INFO - validation batch 101, loss: 0.460, 3232/10016 datapoints
2025-03-06 18:15:54,609 - INFO - validation batch 151, loss: 0.228, 4832/10016 datapoints
2025-03-06 18:15:54,762 - INFO - validation batch 201, loss: 0.384, 6432/10016 datapoints
2025-03-06 18:15:54,917 - INFO - validation batch 251, loss: 0.609, 8032/10016 datapoints
2025-03-06 18:15:55,086 - INFO - validation batch 301, loss: 0.264, 9632/10016 datapoints
2025-03-06 18:15:55,124 - INFO - Epoch 229/800 done.
2025-03-06 18:15:55,125 - INFO - Final validation performance:
Loss: 0.342, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 18:15:55,125 - INFO - Beginning epoch 230/800
2025-03-06 18:15:55,130 - INFO - training batch 1, loss: 0.281, 32/60000 datapoints
2025-03-06 18:15:55,327 - INFO - training batch 51, loss: 0.180, 1632/60000 datapoints
2025-03-06 18:15:55,521 - INFO - training batch 101, loss: 0.439, 3232/60000 datapoints
2025-03-06 18:15:55,719 - INFO - training batch 151, loss: 0.341, 4832/60000 datapoints
2025-03-06 18:15:55,915 - INFO - training batch 201, loss: 0.177, 6432/60000 datapoints
2025-03-06 18:15:56,113 - INFO - training batch 251, loss: 0.223, 8032/60000 datapoints
2025-03-06 18:15:56,311 - INFO - training batch 301, loss: 0.754, 9632/60000 datapoints
2025-03-06 18:15:56,505 - INFO - training batch 351, loss: 0.315, 11232/60000 datapoints
2025-03-06 18:15:56,700 - INFO - training batch 401, loss: 0.224, 12832/60000 datapoints
2025-03-06 18:15:56,894 - INFO - training batch 451, loss: 0.188, 14432/60000 datapoints
2025-03-06 18:15:57,086 - INFO - training batch 501, loss: 0.451, 16032/60000 datapoints
2025-03-06 18:15:57,280 - INFO - training batch 551, loss: 0.285, 17632/60000 datapoints
2025-03-06 18:15:57,476 - INFO - training batch 601, loss: 0.264, 19232/60000 datapoints
2025-03-06 18:15:57,673 - INFO - training batch 651, loss: 0.276, 20832/60000 datapoints
2025-03-06 18:15:57,870 - INFO - training batch 701, loss: 0.669, 22432/60000 datapoints
2025-03-06 18:15:58,066 - INFO - training batch 751, loss: 0.482, 24032/60000 datapoints
2025-03-06 18:15:58,263 - INFO - training batch 801, loss: 0.380, 25632/60000 datapoints
2025-03-06 18:15:58,460 - INFO - training batch 851, loss: 0.119, 27232/60000 datapoints
2025-03-06 18:15:58,657 - INFO - training batch 901, loss: 0.164, 28832/60000 datapoints
2025-03-06 18:15:58,852 - INFO - training batch 951, loss: 0.177, 30432/60000 datapoints
2025-03-06 18:15:59,047 - INFO - training batch 1001, loss: 0.566, 32032/60000 datapoints
2025-03-06 18:15:59,240 - INFO - training batch 1051, loss: 0.284, 33632/60000 datapoints
2025-03-06 18:15:59,434 - INFO - training batch 1101, loss: 0.478, 35232/60000 datapoints
2025-03-06 18:15:59,632 - INFO - training batch 1151, loss: 0.189, 36832/60000 datapoints
2025-03-06 18:15:59,827 - INFO - training batch 1201, loss: 0.320, 38432/60000 datapoints
2025-03-06 18:16:00,021 - INFO - training batch 1251, loss: 0.415, 40032/60000 datapoints
2025-03-06 18:16:00,216 - INFO - training batch 1301, loss: 0.441, 41632/60000 datapoints
2025-03-06 18:16:00,412 - INFO - training batch 1351, loss: 0.396, 43232/60000 datapoints
2025-03-06 18:16:00,608 - INFO - training batch 1401, loss: 0.395, 44832/60000 datapoints
2025-03-06 18:16:00,802 - INFO - training batch 1451, loss: 0.445, 46432/60000 datapoints
2025-03-06 18:16:00,993 - INFO - training batch 1501, loss: 0.185, 48032/60000 datapoints
2025-03-06 18:16:01,186 - INFO - training batch 1551, loss: 0.285, 49632/60000 datapoints
2025-03-06 18:16:01,379 - INFO - training batch 1601, loss: 0.252, 51232/60000 datapoints
2025-03-06 18:16:01,574 - INFO - training batch 1651, loss: 0.678, 52832/60000 datapoints
2025-03-06 18:16:01,772 - INFO - training batch 1701, loss: 0.289, 54432/60000 datapoints
2025-03-06 18:16:01,968 - INFO - training batch 1751, loss: 0.542, 56032/60000 datapoints
2025-03-06 18:16:02,165 - INFO - training batch 1801, loss: 0.330, 57632/60000 datapoints
2025-03-06 18:16:02,362 - INFO - training batch 1851, loss: 0.129, 59232/60000 datapoints
2025-03-06 18:16:02,465 - INFO - validation batch 1, loss: 0.280, 32/10016 datapoints
2025-03-06 18:16:02,620 - INFO - validation batch 51, loss: 0.418, 1632/10016 datapoints
2025-03-06 18:16:02,772 - INFO - validation batch 101, loss: 0.202, 3232/10016 datapoints
2025-03-06 18:16:02,924 - INFO - validation batch 151, loss: 0.317, 4832/10016 datapoints
2025-03-06 18:16:03,076 - INFO - validation batch 201, loss: 0.319, 6432/10016 datapoints
2025-03-06 18:16:03,227 - INFO - validation batch 251, loss: 0.202, 8032/10016 datapoints
2025-03-06 18:16:03,383 - INFO - validation batch 301, loss: 0.430, 9632/10016 datapoints
2025-03-06 18:16:03,421 - INFO - Epoch 230/800 done.
2025-03-06 18:16:03,422 - INFO - Final validation performance:
Loss: 0.310, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 18:16:03,422 - INFO - Beginning epoch 231/800
2025-03-06 18:16:03,427 - INFO - training batch 1, loss: 0.305, 32/60000 datapoints
2025-03-06 18:16:03,625 - INFO - training batch 51, loss: 0.447, 1632/60000 datapoints
2025-03-06 18:16:03,824 - INFO - training batch 101, loss: 0.220, 3232/60000 datapoints
2025-03-06 18:16:04,022 - INFO - training batch 151, loss: 0.210, 4832/60000 datapoints
2025-03-06 18:16:04,220 - INFO - training batch 201, loss: 0.251, 6432/60000 datapoints
2025-03-06 18:16:04,417 - INFO - training batch 251, loss: 0.452, 8032/60000 datapoints
2025-03-06 18:16:04,619 - INFO - training batch 301, loss: 0.187, 9632/60000 datapoints
2025-03-06 18:16:04,815 - INFO - training batch 351, loss: 0.298, 11232/60000 datapoints
2025-03-06 18:16:05,018 - INFO - training batch 401, loss: 0.368, 12832/60000 datapoints
2025-03-06 18:16:05,232 - INFO - training batch 451, loss: 0.148, 14432/60000 datapoints
2025-03-06 18:16:05,430 - INFO - training batch 501, loss: 0.310, 16032/60000 datapoints
2025-03-06 18:16:05,631 - INFO - training batch 551, loss: 0.422, 17632/60000 datapoints
2025-03-06 18:16:05,824 - INFO - training batch 601, loss: 0.278, 19232/60000 datapoints
2025-03-06 18:16:06,020 - INFO - training batch 651, loss: 0.269, 20832/60000 datapoints
2025-03-06 18:16:06,220 - INFO - training batch 701, loss: 0.152, 22432/60000 datapoints
2025-03-06 18:16:06,421 - INFO - training batch 751, loss: 0.248, 24032/60000 datapoints
2025-03-06 18:16:06,618 - INFO - training batch 801, loss: 0.629, 25632/60000 datapoints
2025-03-06 18:16:06,812 - INFO - training batch 851, loss: 0.426, 27232/60000 datapoints
2025-03-06 18:16:07,007 - INFO - training batch 901, loss: 0.485, 28832/60000 datapoints
2025-03-06 18:16:07,203 - INFO - training batch 951, loss: 0.475, 30432/60000 datapoints
2025-03-06 18:16:07,404 - INFO - training batch 1001, loss: 0.379, 32032/60000 datapoints
2025-03-06 18:16:07,605 - INFO - training batch 1051, loss: 0.360, 33632/60000 datapoints
2025-03-06 18:16:07,800 - INFO - training batch 1101, loss: 0.871, 35232/60000 datapoints
2025-03-06 18:16:07,996 - INFO - training batch 1151, loss: 0.313, 36832/60000 datapoints
2025-03-06 18:16:08,206 - INFO - training batch 1201, loss: 0.272, 38432/60000 datapoints
2025-03-06 18:16:08,402 - INFO - training batch 1251, loss: 0.276, 40032/60000 datapoints
2025-03-06 18:16:08,598 - INFO - training batch 1301, loss: 0.387, 41632/60000 datapoints
2025-03-06 18:16:08,792 - INFO - training batch 1351, loss: 0.676, 43232/60000 datapoints
2025-03-06 18:16:08,989 - INFO - training batch 1401, loss: 0.411, 44832/60000 datapoints
2025-03-06 18:16:09,182 - INFO - training batch 1451, loss: 0.156, 46432/60000 datapoints
2025-03-06 18:16:09,380 - INFO - training batch 1501, loss: 0.422, 48032/60000 datapoints
2025-03-06 18:16:09,577 - INFO - training batch 1551, loss: 0.353, 49632/60000 datapoints
2025-03-06 18:16:09,774 - INFO - training batch 1601, loss: 0.361, 51232/60000 datapoints
2025-03-06 18:16:09,968 - INFO - training batch 1651, loss: 0.317, 52832/60000 datapoints
2025-03-06 18:16:10,164 - INFO - training batch 1701, loss: 0.294, 54432/60000 datapoints
2025-03-06 18:16:10,360 - INFO - training batch 1751, loss: 0.363, 56032/60000 datapoints
2025-03-06 18:16:10,555 - INFO - training batch 1801, loss: 0.185, 57632/60000 datapoints
2025-03-06 18:16:10,770 - INFO - training batch 1851, loss: 0.219, 59232/60000 datapoints
2025-03-06 18:16:10,881 - INFO - validation batch 1, loss: 0.257, 32/10016 datapoints
2025-03-06 18:16:11,056 - INFO - validation batch 51, loss: 0.349, 1632/10016 datapoints
2025-03-06 18:16:11,210 - INFO - validation batch 101, loss: 0.222, 3232/10016 datapoints
2025-03-06 18:16:11,363 - INFO - validation batch 151, loss: 0.420, 4832/10016 datapoints
2025-03-06 18:16:11,516 - INFO - validation batch 201, loss: 0.405, 6432/10016 datapoints
2025-03-06 18:16:11,674 - INFO - validation batch 251, loss: 0.267, 8032/10016 datapoints
2025-03-06 18:16:11,826 - INFO - validation batch 301, loss: 0.317, 9632/10016 datapoints
2025-03-06 18:16:11,863 - INFO - Epoch 231/800 done.
2025-03-06 18:16:11,863 - INFO - Final validation performance:
Loss: 0.320, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 18:16:11,863 - INFO - Beginning epoch 232/800
2025-03-06 18:16:11,870 - INFO - training batch 1, loss: 0.391, 32/60000 datapoints
2025-03-06 18:16:12,063 - INFO - training batch 51, loss: 0.534, 1632/60000 datapoints
2025-03-06 18:16:12,257 - INFO - training batch 101, loss: 0.640, 3232/60000 datapoints
2025-03-06 18:16:12,447 - INFO - training batch 151, loss: 0.259, 4832/60000 datapoints
2025-03-06 18:16:12,641 - INFO - training batch 201, loss: 0.504, 6432/60000 datapoints
2025-03-06 18:16:12,835 - INFO - training batch 251, loss: 0.569, 8032/60000 datapoints
2025-03-06 18:16:13,026 - INFO - training batch 301, loss: 0.441, 9632/60000 datapoints
2025-03-06 18:16:13,216 - INFO - training batch 351, loss: 0.302, 11232/60000 datapoints
2025-03-06 18:16:13,408 - INFO - training batch 401, loss: 0.270, 12832/60000 datapoints
2025-03-06 18:16:13,603 - INFO - training batch 451, loss: 0.328, 14432/60000 datapoints
2025-03-06 18:16:13,793 - INFO - training batch 501, loss: 0.163, 16032/60000 datapoints
2025-03-06 18:16:13,983 - INFO - training batch 551, loss: 0.362, 17632/60000 datapoints
2025-03-06 18:16:14,176 - INFO - training batch 601, loss: 0.615, 19232/60000 datapoints
2025-03-06 18:16:14,368 - INFO - training batch 651, loss: 0.363, 20832/60000 datapoints
2025-03-06 18:16:14,561 - INFO - training batch 701, loss: 0.173, 22432/60000 datapoints
2025-03-06 18:16:14,757 - INFO - training batch 751, loss: 0.395, 24032/60000 datapoints
2025-03-06 18:16:14,953 - INFO - training batch 801, loss: 0.224, 25632/60000 datapoints
2025-03-06 18:16:15,150 - INFO - training batch 851, loss: 0.231, 27232/60000 datapoints
2025-03-06 18:16:15,358 - INFO - training batch 901, loss: 0.454, 28832/60000 datapoints
2025-03-06 18:16:15,553 - INFO - training batch 951, loss: 0.346, 30432/60000 datapoints
2025-03-06 18:16:15,748 - INFO - training batch 1001, loss: 0.178, 32032/60000 datapoints
2025-03-06 18:16:15,937 - INFO - training batch 1051, loss: 0.341, 33632/60000 datapoints
2025-03-06 18:16:16,130 - INFO - training batch 1101, loss: 0.493, 35232/60000 datapoints
2025-03-06 18:16:16,329 - INFO - training batch 1151, loss: 0.287, 36832/60000 datapoints
2025-03-06 18:16:16,540 - INFO - training batch 1201, loss: 0.314, 38432/60000 datapoints
2025-03-06 18:16:16,737 - INFO - training batch 1251, loss: 0.366, 40032/60000 datapoints
2025-03-06 18:16:16,928 - INFO - training batch 1301, loss: 0.261, 41632/60000 datapoints
2025-03-06 18:16:17,120 - INFO - training batch 1351, loss: 0.214, 43232/60000 datapoints
2025-03-06 18:16:17,309 - INFO - training batch 1401, loss: 0.248, 44832/60000 datapoints
2025-03-06 18:16:17,501 - INFO - training batch 1451, loss: 0.169, 46432/60000 datapoints
2025-03-06 18:16:17,694 - INFO - training batch 1501, loss: 0.360, 48032/60000 datapoints
2025-03-06 18:16:17,886 - INFO - training batch 1551, loss: 0.157, 49632/60000 datapoints
2025-03-06 18:16:18,077 - INFO - training batch 1601, loss: 0.278, 51232/60000 datapoints
2025-03-06 18:16:18,270 - INFO - training batch 1651, loss: 0.382, 52832/60000 datapoints
2025-03-06 18:16:18,461 - INFO - training batch 1701, loss: 0.380, 54432/60000 datapoints
2025-03-06 18:16:18,655 - INFO - training batch 1751, loss: 0.311, 56032/60000 datapoints
2025-03-06 18:16:18,845 - INFO - training batch 1801, loss: 0.301, 57632/60000 datapoints
2025-03-06 18:16:19,036 - INFO - training batch 1851, loss: 0.459, 59232/60000 datapoints
2025-03-06 18:16:19,134 - INFO - validation batch 1, loss: 0.364, 32/10016 datapoints
2025-03-06 18:16:19,283 - INFO - validation batch 51, loss: 0.460, 1632/10016 datapoints
2025-03-06 18:16:19,433 - INFO - validation batch 101, loss: 0.372, 3232/10016 datapoints
2025-03-06 18:16:19,587 - INFO - validation batch 151, loss: 0.429, 4832/10016 datapoints
2025-03-06 18:16:19,741 - INFO - validation batch 201, loss: 0.356, 6432/10016 datapoints
2025-03-06 18:16:19,892 - INFO - validation batch 251, loss: 0.224, 8032/10016 datapoints
2025-03-06 18:16:20,044 - INFO - validation batch 301, loss: 0.328, 9632/10016 datapoints
2025-03-06 18:16:20,080 - INFO - Epoch 232/800 done.
2025-03-06 18:16:20,080 - INFO - Final validation performance:
Loss: 0.362, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 18:16:20,080 - INFO - Beginning epoch 233/800
2025-03-06 18:16:20,086 - INFO - training batch 1, loss: 0.185, 32/60000 datapoints
2025-03-06 18:16:20,280 - INFO - training batch 51, loss: 0.205, 1632/60000 datapoints
2025-03-06 18:16:20,471 - INFO - training batch 101, loss: 0.307, 3232/60000 datapoints
2025-03-06 18:16:20,667 - INFO - training batch 151, loss: 0.231, 4832/60000 datapoints
2025-03-06 18:16:20,858 - INFO - training batch 201, loss: 0.194, 6432/60000 datapoints
2025-03-06 18:16:21,057 - INFO - training batch 251, loss: 0.255, 8032/60000 datapoints
2025-03-06 18:16:21,251 - INFO - training batch 301, loss: 0.408, 9632/60000 datapoints
2025-03-06 18:16:21,445 - INFO - training batch 351, loss: 0.593, 11232/60000 datapoints
2025-03-06 18:16:21,643 - INFO - training batch 401, loss: 0.381, 12832/60000 datapoints
2025-03-06 18:16:21,841 - INFO - training batch 451, loss: 0.262, 14432/60000 datapoints
2025-03-06 18:16:22,035 - INFO - training batch 501, loss: 0.240, 16032/60000 datapoints
2025-03-06 18:16:22,229 - INFO - training batch 551, loss: 0.453, 17632/60000 datapoints
2025-03-06 18:16:22,423 - INFO - training batch 601, loss: 0.364, 19232/60000 datapoints
2025-03-06 18:16:22,623 - INFO - training batch 651, loss: 0.265, 20832/60000 datapoints
2025-03-06 18:16:22,818 - INFO - training batch 701, loss: 0.120, 22432/60000 datapoints
2025-03-06 18:16:23,012 - INFO - training batch 751, loss: 0.416, 24032/60000 datapoints
2025-03-06 18:16:23,208 - INFO - training batch 801, loss: 0.223, 25632/60000 datapoints
2025-03-06 18:16:23,404 - INFO - training batch 851, loss: 0.271, 27232/60000 datapoints
2025-03-06 18:16:23,602 - INFO - training batch 901, loss: 0.146, 28832/60000 datapoints
2025-03-06 18:16:23,795 - INFO - training batch 951, loss: 0.386, 30432/60000 datapoints
2025-03-06 18:16:23,989 - INFO - training batch 1001, loss: 0.346, 32032/60000 datapoints
2025-03-06 18:16:24,182 - INFO - training batch 1051, loss: 0.262, 33632/60000 datapoints
2025-03-06 18:16:24,378 - INFO - training batch 1101, loss: 0.292, 35232/60000 datapoints
2025-03-06 18:16:24,574 - INFO - training batch 1151, loss: 0.374, 36832/60000 datapoints
2025-03-06 18:16:24,772 - INFO - training batch 1201, loss: 0.284, 38432/60000 datapoints
2025-03-06 18:16:24,975 - INFO - training batch 1251, loss: 0.414, 40032/60000 datapoints
2025-03-06 18:16:25,179 - INFO - training batch 1301, loss: 0.115, 41632/60000 datapoints
2025-03-06 18:16:25,390 - INFO - training batch 1351, loss: 0.199, 43232/60000 datapoints
2025-03-06 18:16:25,584 - INFO - training batch 1401, loss: 0.168, 44832/60000 datapoints
2025-03-06 18:16:25,781 - INFO - training batch 1451, loss: 0.259, 46432/60000 datapoints
2025-03-06 18:16:25,975 - INFO - training batch 1501, loss: 0.226, 48032/60000 datapoints
2025-03-06 18:16:26,171 - INFO - training batch 1551, loss: 0.237, 49632/60000 datapoints
2025-03-06 18:16:26,371 - INFO - training batch 1601, loss: 0.322, 51232/60000 datapoints
2025-03-06 18:16:26,567 - INFO - training batch 1651, loss: 0.451, 52832/60000 datapoints
2025-03-06 18:16:26,768 - INFO - training batch 1701, loss: 0.399, 54432/60000 datapoints
2025-03-06 18:16:26,963 - INFO - training batch 1751, loss: 0.243, 56032/60000 datapoints
2025-03-06 18:16:27,159 - INFO - training batch 1801, loss: 0.190, 57632/60000 datapoints
2025-03-06 18:16:27,353 - INFO - training batch 1851, loss: 0.216, 59232/60000 datapoints
2025-03-06 18:16:27,453 - INFO - validation batch 1, loss: 0.293, 32/10016 datapoints
2025-03-06 18:16:27,609 - INFO - validation batch 51, loss: 0.536, 1632/10016 datapoints
2025-03-06 18:16:27,761 - INFO - validation batch 101, loss: 0.427, 3232/10016 datapoints
2025-03-06 18:16:27,914 - INFO - validation batch 151, loss: 0.442, 4832/10016 datapoints
2025-03-06 18:16:28,065 - INFO - validation batch 201, loss: 0.214, 6432/10016 datapoints
2025-03-06 18:16:28,224 - INFO - validation batch 251, loss: 0.123, 8032/10016 datapoints
2025-03-06 18:16:28,377 - INFO - validation batch 301, loss: 0.549, 9632/10016 datapoints
2025-03-06 18:16:28,415 - INFO - Epoch 233/800 done.
2025-03-06 18:16:28,415 - INFO - Final validation performance:
Loss: 0.369, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 18:16:28,415 - INFO - Beginning epoch 234/800
2025-03-06 18:16:28,422 - INFO - training batch 1, loss: 0.453, 32/60000 datapoints
2025-03-06 18:16:28,625 - INFO - training batch 51, loss: 0.158, 1632/60000 datapoints
2025-03-06 18:16:28,820 - INFO - training batch 101, loss: 0.229, 3232/60000 datapoints
2025-03-06 18:16:29,013 - INFO - training batch 151, loss: 0.410, 4832/60000 datapoints
2025-03-06 18:16:29,209 - INFO - training batch 201, loss: 0.280, 6432/60000 datapoints
2025-03-06 18:16:29,404 - INFO - training batch 251, loss: 0.489, 8032/60000 datapoints
2025-03-06 18:16:29,604 - INFO - training batch 301, loss: 0.404, 9632/60000 datapoints
2025-03-06 18:16:29,797 - INFO - training batch 351, loss: 0.128, 11232/60000 datapoints
2025-03-06 18:16:29,993 - INFO - training batch 401, loss: 0.192, 12832/60000 datapoints
2025-03-06 18:16:30,188 - INFO - training batch 451, loss: 0.285, 14432/60000 datapoints
2025-03-06 18:16:30,389 - INFO - training batch 501, loss: 0.262, 16032/60000 datapoints
2025-03-06 18:16:30,582 - INFO - training batch 551, loss: 0.571, 17632/60000 datapoints
2025-03-06 18:16:30,781 - INFO - training batch 601, loss: 0.233, 19232/60000 datapoints
2025-03-06 18:16:30,973 - INFO - training batch 651, loss: 0.165, 20832/60000 datapoints
2025-03-06 18:16:31,167 - INFO - training batch 701, loss: 0.217, 22432/60000 datapoints
2025-03-06 18:16:31,360 - INFO - training batch 751, loss: 0.205, 24032/60000 datapoints
2025-03-06 18:16:31,556 - INFO - training batch 801, loss: 0.179, 25632/60000 datapoints
2025-03-06 18:16:31,757 - INFO - training batch 851, loss: 0.409, 27232/60000 datapoints
2025-03-06 18:16:31,951 - INFO - training batch 901, loss: 0.170, 28832/60000 datapoints
2025-03-06 18:16:32,145 - INFO - training batch 951, loss: 0.221, 30432/60000 datapoints
2025-03-06 18:16:32,342 - INFO - training batch 1001, loss: 0.254, 32032/60000 datapoints
2025-03-06 18:16:32,537 - INFO - training batch 1051, loss: 0.330, 33632/60000 datapoints
2025-03-06 18:16:32,736 - INFO - training batch 1101, loss: 0.273, 35232/60000 datapoints
2025-03-06 18:16:32,931 - INFO - training batch 1151, loss: 0.452, 36832/60000 datapoints
2025-03-06 18:16:33,126 - INFO - training batch 1201, loss: 0.335, 38432/60000 datapoints
2025-03-06 18:16:33,319 - INFO - training batch 1251, loss: 0.300, 40032/60000 datapoints
2025-03-06 18:16:33,513 - INFO - training batch 1301, loss: 0.260, 41632/60000 datapoints
2025-03-06 18:16:33,710 - INFO - training batch 1351, loss: 0.412, 43232/60000 datapoints
2025-03-06 18:16:33,906 - INFO - training batch 1401, loss: 0.231, 44832/60000 datapoints
2025-03-06 18:16:34,098 - INFO - training batch 1451, loss: 0.227, 46432/60000 datapoints
2025-03-06 18:16:34,297 - INFO - training batch 1501, loss: 0.399, 48032/60000 datapoints
2025-03-06 18:16:34,492 - INFO - training batch 1551, loss: 0.403, 49632/60000 datapoints
2025-03-06 18:16:34,689 - INFO - training batch 1601, loss: 0.172, 51232/60000 datapoints
2025-03-06 18:16:34,891 - INFO - training batch 1651, loss: 0.334, 52832/60000 datapoints
2025-03-06 18:16:35,087 - INFO - training batch 1701, loss: 0.247, 54432/60000 datapoints
2025-03-06 18:16:35,280 - INFO - training batch 1751, loss: 0.432, 56032/60000 datapoints
2025-03-06 18:16:35,493 - INFO - training batch 1801, loss: 0.388, 57632/60000 datapoints
2025-03-06 18:16:35,693 - INFO - training batch 1851, loss: 0.662, 59232/60000 datapoints
2025-03-06 18:16:35,794 - INFO - validation batch 1, loss: 0.357, 32/10016 datapoints
2025-03-06 18:16:35,945 - INFO - validation batch 51, loss: 0.377, 1632/10016 datapoints
2025-03-06 18:16:36,097 - INFO - validation batch 101, loss: 0.412, 3232/10016 datapoints
2025-03-06 18:16:36,256 - INFO - validation batch 151, loss: 0.211, 4832/10016 datapoints
2025-03-06 18:16:36,411 - INFO - validation batch 201, loss: 0.404, 6432/10016 datapoints
2025-03-06 18:16:36,564 - INFO - validation batch 251, loss: 0.137, 8032/10016 datapoints
2025-03-06 18:16:36,726 - INFO - validation batch 301, loss: 0.371, 9632/10016 datapoints
2025-03-06 18:16:36,764 - INFO - Epoch 234/800 done.
2025-03-06 18:16:36,764 - INFO - Final validation performance:
Loss: 0.324, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 18:16:36,764 - INFO - Beginning epoch 235/800
2025-03-06 18:16:36,771 - INFO - training batch 1, loss: 0.239, 32/60000 datapoints
2025-03-06 18:16:36,962 - INFO - training batch 51, loss: 0.451, 1632/60000 datapoints
2025-03-06 18:16:37,152 - INFO - training batch 101, loss: 0.739, 3232/60000 datapoints
2025-03-06 18:16:37,348 - INFO - training batch 151, loss: 0.148, 4832/60000 datapoints
2025-03-06 18:16:37,538 - INFO - training batch 201, loss: 0.258, 6432/60000 datapoints
2025-03-06 18:16:37,772 - INFO - training batch 251, loss: 0.248, 8032/60000 datapoints
2025-03-06 18:16:37,987 - INFO - training batch 301, loss: 0.295, 9632/60000 datapoints
2025-03-06 18:16:38,178 - INFO - training batch 351, loss: 0.349, 11232/60000 datapoints
2025-03-06 18:16:38,373 - INFO - training batch 401, loss: 0.326, 12832/60000 datapoints
2025-03-06 18:16:38,566 - INFO - training batch 451, loss: 0.318, 14432/60000 datapoints
2025-03-06 18:16:38,760 - INFO - training batch 501, loss: 0.276, 16032/60000 datapoints
2025-03-06 18:16:38,952 - INFO - training batch 551, loss: 0.253, 17632/60000 datapoints
2025-03-06 18:16:39,167 - INFO - training batch 601, loss: 0.268, 19232/60000 datapoints
2025-03-06 18:16:39,358 - INFO - training batch 651, loss: 0.228, 20832/60000 datapoints
2025-03-06 18:16:39,548 - INFO - training batch 701, loss: 0.374, 22432/60000 datapoints
2025-03-06 18:16:39,745 - INFO - training batch 751, loss: 0.385, 24032/60000 datapoints
2025-03-06 18:16:39,937 - INFO - training batch 801, loss: 0.450, 25632/60000 datapoints
2025-03-06 18:16:40,128 - INFO - training batch 851, loss: 0.174, 27232/60000 datapoints
2025-03-06 18:16:40,325 - INFO - training batch 901, loss: 0.226, 28832/60000 datapoints
2025-03-06 18:16:40,516 - INFO - training batch 951, loss: 0.269, 30432/60000 datapoints
2025-03-06 18:16:40,711 - INFO - training batch 1001, loss: 0.477, 32032/60000 datapoints
2025-03-06 18:16:40,904 - INFO - training batch 1051, loss: 0.405, 33632/60000 datapoints
2025-03-06 18:16:41,102 - INFO - training batch 1101, loss: 0.153, 35232/60000 datapoints
2025-03-06 18:16:41,295 - INFO - training batch 1151, loss: 0.204, 36832/60000 datapoints
2025-03-06 18:16:41,491 - INFO - training batch 1201, loss: 0.255, 38432/60000 datapoints
2025-03-06 18:16:41,686 - INFO - training batch 1251, loss: 0.249, 40032/60000 datapoints
2025-03-06 18:16:41,881 - INFO - training batch 1301, loss: 0.398, 41632/60000 datapoints
2025-03-06 18:16:42,075 - INFO - training batch 1351, loss: 0.324, 43232/60000 datapoints
2025-03-06 18:16:42,272 - INFO - training batch 1401, loss: 0.161, 44832/60000 datapoints
2025-03-06 18:16:42,470 - INFO - training batch 1451, loss: 0.357, 46432/60000 datapoints
2025-03-06 18:16:42,667 - INFO - training batch 1501, loss: 0.125, 48032/60000 datapoints
2025-03-06 18:16:42,862 - INFO - training batch 1551, loss: 0.179, 49632/60000 datapoints
2025-03-06 18:16:43,057 - INFO - training batch 1601, loss: 0.352, 51232/60000 datapoints
2025-03-06 18:16:43,253 - INFO - training batch 1651, loss: 0.322, 52832/60000 datapoints
2025-03-06 18:16:43,450 - INFO - training batch 1701, loss: 0.294, 54432/60000 datapoints
2025-03-06 18:16:43,647 - INFO - training batch 1751, loss: 0.277, 56032/60000 datapoints
2025-03-06 18:16:43,842 - INFO - training batch 1801, loss: 0.304, 57632/60000 datapoints
2025-03-06 18:16:44,038 - INFO - training batch 1851, loss: 0.347, 59232/60000 datapoints
2025-03-06 18:16:44,141 - INFO - validation batch 1, loss: 0.211, 32/10016 datapoints
2025-03-06 18:16:44,296 - INFO - validation batch 51, loss: 0.325, 1632/10016 datapoints
2025-03-06 18:16:44,449 - INFO - validation batch 101, loss: 0.466, 3232/10016 datapoints
2025-03-06 18:16:44,603 - INFO - validation batch 151, loss: 0.356, 4832/10016 datapoints
2025-03-06 18:16:44,756 - INFO - validation batch 201, loss: 0.462, 6432/10016 datapoints
2025-03-06 18:16:44,916 - INFO - validation batch 251, loss: 0.245, 8032/10016 datapoints
2025-03-06 18:16:45,068 - INFO - validation batch 301, loss: 0.266, 9632/10016 datapoints
2025-03-06 18:16:45,105 - INFO - Epoch 235/800 done.
2025-03-06 18:16:45,105 - INFO - Final validation performance:
Loss: 0.333, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 18:16:45,105 - INFO - Beginning epoch 236/800
2025-03-06 18:16:45,111 - INFO - training batch 1, loss: 0.145, 32/60000 datapoints
2025-03-06 18:16:45,303 - INFO - training batch 51, loss: 0.203, 1632/60000 datapoints
2025-03-06 18:16:45,518 - INFO - training batch 101, loss: 0.335, 3232/60000 datapoints
2025-03-06 18:16:45,713 - INFO - training batch 151, loss: 0.404, 4832/60000 datapoints
2025-03-06 18:16:45,906 - INFO - training batch 201, loss: 0.253, 6432/60000 datapoints
2025-03-06 18:16:46,100 - INFO - training batch 251, loss: 0.168, 8032/60000 datapoints
2025-03-06 18:16:46,299 - INFO - training batch 301, loss: 0.106, 9632/60000 datapoints
2025-03-06 18:16:46,497 - INFO - training batch 351, loss: 0.487, 11232/60000 datapoints
2025-03-06 18:16:46,693 - INFO - training batch 401, loss: 0.412, 12832/60000 datapoints
2025-03-06 18:16:46,887 - INFO - training batch 451, loss: 0.326, 14432/60000 datapoints
2025-03-06 18:16:47,080 - INFO - training batch 501, loss: 0.587, 16032/60000 datapoints
2025-03-06 18:16:47,275 - INFO - training batch 551, loss: 0.480, 17632/60000 datapoints
2025-03-06 18:16:47,469 - INFO - training batch 601, loss: 0.300, 19232/60000 datapoints
2025-03-06 18:16:47,665 - INFO - training batch 651, loss: 0.347, 20832/60000 datapoints
2025-03-06 18:16:47,865 - INFO - training batch 701, loss: 0.528, 22432/60000 datapoints
2025-03-06 18:16:48,059 - INFO - training batch 751, loss: 0.424, 24032/60000 datapoints
2025-03-06 18:16:48,251 - INFO - training batch 801, loss: 0.337, 25632/60000 datapoints
2025-03-06 18:16:48,448 - INFO - training batch 851, loss: 0.282, 27232/60000 datapoints
2025-03-06 18:16:48,646 - INFO - training batch 901, loss: 0.404, 28832/60000 datapoints
2025-03-06 18:16:48,839 - INFO - training batch 951, loss: 0.651, 30432/60000 datapoints
2025-03-06 18:16:49,033 - INFO - training batch 1001, loss: 0.266, 32032/60000 datapoints
2025-03-06 18:16:49,228 - INFO - training batch 1051, loss: 0.314, 33632/60000 datapoints
2025-03-06 18:16:49,422 - INFO - training batch 1101, loss: 0.296, 35232/60000 datapoints
2025-03-06 18:16:49,620 - INFO - training batch 1151, loss: 0.206, 36832/60000 datapoints
2025-03-06 18:16:49,818 - INFO - training batch 1201, loss: 0.572, 38432/60000 datapoints
2025-03-06 18:16:50,016 - INFO - training batch 1251, loss: 0.340, 40032/60000 datapoints
2025-03-06 18:16:50,211 - INFO - training batch 1301, loss: 0.578, 41632/60000 datapoints
2025-03-06 18:16:50,411 - INFO - training batch 1351, loss: 0.278, 43232/60000 datapoints
2025-03-06 18:16:50,606 - INFO - training batch 1401, loss: 0.342, 44832/60000 datapoints
2025-03-06 18:16:50,802 - INFO - training batch 1451, loss: 0.113, 46432/60000 datapoints
2025-03-06 18:16:50,997 - INFO - training batch 1501, loss: 0.351, 48032/60000 datapoints
2025-03-06 18:16:51,191 - INFO - training batch 1551, loss: 0.263, 49632/60000 datapoints
2025-03-06 18:16:51,388 - INFO - training batch 1601, loss: 0.174, 51232/60000 datapoints
2025-03-06 18:16:51,581 - INFO - training batch 1651, loss: 0.218, 52832/60000 datapoints
2025-03-06 18:16:51,778 - INFO - training batch 1701, loss: 0.292, 54432/60000 datapoints
2025-03-06 18:16:51,972 - INFO - training batch 1751, loss: 0.337, 56032/60000 datapoints
2025-03-06 18:16:52,166 - INFO - training batch 1801, loss: 0.392, 57632/60000 datapoints
2025-03-06 18:16:52,363 - INFO - training batch 1851, loss: 0.143, 59232/60000 datapoints
2025-03-06 18:16:52,463 - INFO - validation batch 1, loss: 0.210, 32/10016 datapoints
2025-03-06 18:16:52,621 - INFO - validation batch 51, loss: 0.157, 1632/10016 datapoints
2025-03-06 18:16:52,771 - INFO - validation batch 101, loss: 0.193, 3232/10016 datapoints
2025-03-06 18:16:52,924 - INFO - validation batch 151, loss: 0.364, 4832/10016 datapoints
2025-03-06 18:16:53,077 - INFO - validation batch 201, loss: 0.502, 6432/10016 datapoints
2025-03-06 18:16:53,235 - INFO - validation batch 251, loss: 0.485, 8032/10016 datapoints
2025-03-06 18:16:53,391 - INFO - validation batch 301, loss: 0.360, 9632/10016 datapoints
2025-03-06 18:16:53,429 - INFO - Epoch 236/800 done.
2025-03-06 18:16:53,429 - INFO - Final validation performance:
Loss: 0.325, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 18:16:53,429 - INFO - Beginning epoch 237/800
2025-03-06 18:16:53,435 - INFO - training batch 1, loss: 0.270, 32/60000 datapoints
2025-03-06 18:16:53,631 - INFO - training batch 51, loss: 0.169, 1632/60000 datapoints
2025-03-06 18:16:53,826 - INFO - training batch 101, loss: 0.323, 3232/60000 datapoints
2025-03-06 18:16:54,022 - INFO - training batch 151, loss: 0.413, 4832/60000 datapoints
2025-03-06 18:16:54,219 - INFO - training batch 201, loss: 0.262, 6432/60000 datapoints
2025-03-06 18:16:54,415 - INFO - training batch 251, loss: 0.195, 8032/60000 datapoints
2025-03-06 18:16:54,610 - INFO - training batch 301, loss: 0.166, 9632/60000 datapoints
2025-03-06 18:16:54,806 - INFO - training batch 351, loss: 0.244, 11232/60000 datapoints
2025-03-06 18:16:55,004 - INFO - training batch 401, loss: 0.314, 12832/60000 datapoints
2025-03-06 18:16:55,199 - INFO - training batch 451, loss: 0.639, 14432/60000 datapoints
2025-03-06 18:16:55,443 - INFO - training batch 501, loss: 0.431, 16032/60000 datapoints
2025-03-06 18:16:55,659 - INFO - training batch 551, loss: 0.287, 17632/60000 datapoints
2025-03-06 18:16:55,856 - INFO - training batch 601, loss: 0.243, 19232/60000 datapoints
2025-03-06 18:16:56,050 - INFO - training batch 651, loss: 0.702, 20832/60000 datapoints
2025-03-06 18:16:56,249 - INFO - training batch 701, loss: 0.168, 22432/60000 datapoints
2025-03-06 18:16:56,447 - INFO - training batch 751, loss: 0.395, 24032/60000 datapoints
2025-03-06 18:16:56,641 - INFO - training batch 801, loss: 0.133, 25632/60000 datapoints
2025-03-06 18:16:56,836 - INFO - training batch 851, loss: 0.157, 27232/60000 datapoints
2025-03-06 18:16:57,040 - INFO - training batch 901, loss: 0.407, 28832/60000 datapoints
2025-03-06 18:16:57,234 - INFO - training batch 951, loss: 0.270, 30432/60000 datapoints
2025-03-06 18:16:57,429 - INFO - training batch 1001, loss: 0.386, 32032/60000 datapoints
2025-03-06 18:16:57,632 - INFO - training batch 1051, loss: 0.298, 33632/60000 datapoints
2025-03-06 18:16:57,826 - INFO - training batch 1101, loss: 0.334, 35232/60000 datapoints
2025-03-06 18:16:58,022 - INFO - training batch 1151, loss: 0.237, 36832/60000 datapoints
2025-03-06 18:16:58,215 - INFO - training batch 1201, loss: 0.152, 38432/60000 datapoints
2025-03-06 18:16:58,415 - INFO - training batch 1251, loss: 0.433, 40032/60000 datapoints
2025-03-06 18:16:58,614 - INFO - training batch 1301, loss: 0.335, 41632/60000 datapoints
2025-03-06 18:16:58,806 - INFO - training batch 1351, loss: 0.409, 43232/60000 datapoints
2025-03-06 18:16:59,003 - INFO - training batch 1401, loss: 0.358, 44832/60000 datapoints
2025-03-06 18:16:59,196 - INFO - training batch 1451, loss: 0.176, 46432/60000 datapoints
2025-03-06 18:16:59,392 - INFO - training batch 1501, loss: 0.178, 48032/60000 datapoints
2025-03-06 18:16:59,588 - INFO - training batch 1551, loss: 0.419, 49632/60000 datapoints
2025-03-06 18:16:59,785 - INFO - training batch 1601, loss: 0.314, 51232/60000 datapoints
2025-03-06 18:16:59,981 - INFO - training batch 1651, loss: 0.480, 52832/60000 datapoints
2025-03-06 18:17:00,178 - INFO - training batch 1701, loss: 0.224, 54432/60000 datapoints
2025-03-06 18:17:00,379 - INFO - training batch 1751, loss: 0.284, 56032/60000 datapoints
2025-03-06 18:17:00,580 - INFO - training batch 1801, loss: 0.523, 57632/60000 datapoints
2025-03-06 18:17:00,782 - INFO - training batch 1851, loss: 0.442, 59232/60000 datapoints
2025-03-06 18:17:00,884 - INFO - validation batch 1, loss: 0.454, 32/10016 datapoints
2025-03-06 18:17:01,037 - INFO - validation batch 51, loss: 0.271, 1632/10016 datapoints
2025-03-06 18:17:01,188 - INFO - validation batch 101, loss: 0.393, 3232/10016 datapoints
2025-03-06 18:17:01,342 - INFO - validation batch 151, loss: 0.353, 4832/10016 datapoints
2025-03-06 18:17:01,496 - INFO - validation batch 201, loss: 0.289, 6432/10016 datapoints
2025-03-06 18:17:01,651 - INFO - validation batch 251, loss: 0.313, 8032/10016 datapoints
2025-03-06 18:17:01,805 - INFO - validation batch 301, loss: 0.266, 9632/10016 datapoints
2025-03-06 18:17:01,843 - INFO - Epoch 237/800 done.
2025-03-06 18:17:01,843 - INFO - Final validation performance:
Loss: 0.334, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 18:17:01,844 - INFO - Beginning epoch 238/800
2025-03-06 18:17:01,849 - INFO - training batch 1, loss: 0.203, 32/60000 datapoints
2025-03-06 18:17:02,045 - INFO - training batch 51, loss: 0.258, 1632/60000 datapoints
2025-03-06 18:17:02,238 - INFO - training batch 101, loss: 0.277, 3232/60000 datapoints
2025-03-06 18:17:02,436 - INFO - training batch 151, loss: 0.568, 4832/60000 datapoints
2025-03-06 18:17:02,632 - INFO - training batch 201, loss: 0.451, 6432/60000 datapoints
2025-03-06 18:17:02,827 - INFO - training batch 251, loss: 0.160, 8032/60000 datapoints
2025-03-06 18:17:03,022 - INFO - training batch 301, loss: 0.519, 9632/60000 datapoints
2025-03-06 18:17:03,216 - INFO - training batch 351, loss: 0.180, 11232/60000 datapoints
2025-03-06 18:17:03,411 - INFO - training batch 401, loss: 0.504, 12832/60000 datapoints
2025-03-06 18:17:03,608 - INFO - training batch 451, loss: 0.265, 14432/60000 datapoints
2025-03-06 18:17:03,802 - INFO - training batch 501, loss: 0.392, 16032/60000 datapoints
2025-03-06 18:17:03,997 - INFO - training batch 551, loss: 0.219, 17632/60000 datapoints
2025-03-06 18:17:04,189 - INFO - training batch 601, loss: 0.270, 19232/60000 datapoints
2025-03-06 18:17:04,386 - INFO - training batch 651, loss: 0.206, 20832/60000 datapoints
2025-03-06 18:17:04,578 - INFO - training batch 701, loss: 0.294, 22432/60000 datapoints
2025-03-06 18:17:04,776 - INFO - training batch 751, loss: 0.663, 24032/60000 datapoints
2025-03-06 18:17:04,975 - INFO - training batch 801, loss: 0.221, 25632/60000 datapoints
2025-03-06 18:17:05,171 - INFO - training batch 851, loss: 0.392, 27232/60000 datapoints
2025-03-06 18:17:05,364 - INFO - training batch 901, loss: 0.392, 28832/60000 datapoints
2025-03-06 18:17:05,564 - INFO - training batch 951, loss: 0.222, 30432/60000 datapoints
2025-03-06 18:17:05,775 - INFO - training batch 1001, loss: 0.218, 32032/60000 datapoints
2025-03-06 18:17:05,970 - INFO - training batch 1051, loss: 0.414, 33632/60000 datapoints
2025-03-06 18:17:06,170 - INFO - training batch 1101, loss: 0.430, 35232/60000 datapoints
2025-03-06 18:17:06,368 - INFO - training batch 1151, loss: 0.440, 36832/60000 datapoints
2025-03-06 18:17:06,566 - INFO - training batch 1201, loss: 0.381, 38432/60000 datapoints
2025-03-06 18:17:06,765 - INFO - training batch 1251, loss: 0.232, 40032/60000 datapoints
2025-03-06 18:17:06,961 - INFO - training batch 1301, loss: 0.499, 41632/60000 datapoints
2025-03-06 18:17:07,155 - INFO - training batch 1351, loss: 0.501, 43232/60000 datapoints
2025-03-06 18:17:07,353 - INFO - training batch 1401, loss: 0.561, 44832/60000 datapoints
2025-03-06 18:17:07,548 - INFO - training batch 1451, loss: 0.184, 46432/60000 datapoints
2025-03-06 18:17:07,749 - INFO - training batch 1501, loss: 0.170, 48032/60000 datapoints
2025-03-06 18:17:07,948 - INFO - training batch 1551, loss: 0.221, 49632/60000 datapoints
2025-03-06 18:17:08,172 - INFO - training batch 1601, loss: 0.514, 51232/60000 datapoints
2025-03-06 18:17:08,378 - INFO - training batch 1651, loss: 0.326, 52832/60000 datapoints
2025-03-06 18:17:08,574 - INFO - training batch 1701, loss: 0.290, 54432/60000 datapoints
2025-03-06 18:17:08,770 - INFO - training batch 1751, loss: 0.476, 56032/60000 datapoints
2025-03-06 18:17:08,968 - INFO - training batch 1801, loss: 0.430, 57632/60000 datapoints
2025-03-06 18:17:09,166 - INFO - training batch 1851, loss: 0.316, 59232/60000 datapoints
2025-03-06 18:17:09,268 - INFO - validation batch 1, loss: 0.453, 32/10016 datapoints
2025-03-06 18:17:09,422 - INFO - validation batch 51, loss: 0.361, 1632/10016 datapoints
2025-03-06 18:17:09,575 - INFO - validation batch 101, loss: 0.214, 3232/10016 datapoints
2025-03-06 18:17:09,733 - INFO - validation batch 151, loss: 0.430, 4832/10016 datapoints
2025-03-06 18:17:09,886 - INFO - validation batch 201, loss: 0.151, 6432/10016 datapoints
2025-03-06 18:17:10,043 - INFO - validation batch 251, loss: 0.214, 8032/10016 datapoints
2025-03-06 18:17:10,196 - INFO - validation batch 301, loss: 0.180, 9632/10016 datapoints
2025-03-06 18:17:10,234 - INFO - Epoch 238/800 done.
2025-03-06 18:17:10,235 - INFO - Final validation performance:
Loss: 0.286, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 18:17:10,235 - INFO - Beginning epoch 239/800
2025-03-06 18:17:10,241 - INFO - training batch 1, loss: 0.120, 32/60000 datapoints
2025-03-06 18:17:10,437 - INFO - training batch 51, loss: 0.392, 1632/60000 datapoints
2025-03-06 18:17:10,634 - INFO - training batch 101, loss: 0.391, 3232/60000 datapoints
2025-03-06 18:17:10,829 - INFO - training batch 151, loss: 0.307, 4832/60000 datapoints
2025-03-06 18:17:11,023 - INFO - training batch 201, loss: 0.369, 6432/60000 datapoints
2025-03-06 18:17:11,217 - INFO - training batch 251, loss: 0.277, 8032/60000 datapoints
2025-03-06 18:17:11,410 - INFO - training batch 301, loss: 0.234, 9632/60000 datapoints
2025-03-06 18:17:11,605 - INFO - training batch 351, loss: 0.519, 11232/60000 datapoints
2025-03-06 18:17:11,798 - INFO - training batch 401, loss: 0.462, 12832/60000 datapoints
2025-03-06 18:17:11,992 - INFO - training batch 451, loss: 0.268, 14432/60000 datapoints
2025-03-06 18:17:12,201 - INFO - training batch 501, loss: 0.312, 16032/60000 datapoints
2025-03-06 18:17:12,410 - INFO - training batch 551, loss: 0.395, 17632/60000 datapoints
2025-03-06 18:17:12,637 - INFO - training batch 601, loss: 0.247, 19232/60000 datapoints
2025-03-06 18:17:12,853 - INFO - training batch 651, loss: 0.269, 20832/60000 datapoints
2025-03-06 18:17:13,068 - INFO - training batch 701, loss: 0.563, 22432/60000 datapoints
2025-03-06 18:17:13,262 - INFO - training batch 751, loss: 0.412, 24032/60000 datapoints
2025-03-06 18:17:13,459 - INFO - training batch 801, loss: 0.396, 25632/60000 datapoints
2025-03-06 18:17:13,657 - INFO - training batch 851, loss: 0.216, 27232/60000 datapoints
2025-03-06 18:17:13,852 - INFO - training batch 901, loss: 0.232, 28832/60000 datapoints
2025-03-06 18:17:14,045 - INFO - training batch 951, loss: 0.350, 30432/60000 datapoints
2025-03-06 18:17:14,243 - INFO - training batch 1001, loss: 0.310, 32032/60000 datapoints
2025-03-06 18:17:14,441 - INFO - training batch 1051, loss: 0.293, 33632/60000 datapoints
2025-03-06 18:17:14,636 - INFO - training batch 1101, loss: 0.398, 35232/60000 datapoints
2025-03-06 18:17:14,829 - INFO - training batch 1151, loss: 0.297, 36832/60000 datapoints
2025-03-06 18:17:15,033 - INFO - training batch 1201, loss: 0.257, 38432/60000 datapoints
2025-03-06 18:17:15,241 - INFO - training batch 1251, loss: 0.265, 40032/60000 datapoints
2025-03-06 18:17:15,435 - INFO - training batch 1301, loss: 0.227, 41632/60000 datapoints
2025-03-06 18:17:15,634 - INFO - training batch 1351, loss: 0.343, 43232/60000 datapoints
2025-03-06 18:17:15,846 - INFO - training batch 1401, loss: 0.206, 44832/60000 datapoints
2025-03-06 18:17:16,041 - INFO - training batch 1451, loss: 0.278, 46432/60000 datapoints
2025-03-06 18:17:16,238 - INFO - training batch 1501, loss: 0.621, 48032/60000 datapoints
2025-03-06 18:17:16,435 - INFO - training batch 1551, loss: 0.200, 49632/60000 datapoints
2025-03-06 18:17:16,634 - INFO - training batch 1601, loss: 0.304, 51232/60000 datapoints
2025-03-06 18:17:16,831 - INFO - training batch 1651, loss: 0.213, 52832/60000 datapoints
2025-03-06 18:17:17,025 - INFO - training batch 1701, loss: 0.252, 54432/60000 datapoints
2025-03-06 18:17:17,220 - INFO - training batch 1751, loss: 0.681, 56032/60000 datapoints
2025-03-06 18:17:17,412 - INFO - training batch 1801, loss: 0.521, 57632/60000 datapoints
2025-03-06 18:17:17,611 - INFO - training batch 1851, loss: 0.332, 59232/60000 datapoints
2025-03-06 18:17:17,713 - INFO - validation batch 1, loss: 0.330, 32/10016 datapoints
2025-03-06 18:17:17,867 - INFO - validation batch 51, loss: 0.327, 1632/10016 datapoints
2025-03-06 18:17:18,021 - INFO - validation batch 101, loss: 0.100, 3232/10016 datapoints
2025-03-06 18:17:18,172 - INFO - validation batch 151, loss: 0.265, 4832/10016 datapoints
2025-03-06 18:17:18,325 - INFO - validation batch 201, loss: 0.181, 6432/10016 datapoints
2025-03-06 18:17:18,481 - INFO - validation batch 251, loss: 0.103, 8032/10016 datapoints
2025-03-06 18:17:18,635 - INFO - validation batch 301, loss: 0.277, 9632/10016 datapoints
2025-03-06 18:17:18,673 - INFO - Epoch 239/800 done.
2025-03-06 18:17:18,673 - INFO - Final validation performance:
Loss: 0.226, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 18:17:18,673 - INFO - Beginning epoch 240/800
2025-03-06 18:17:18,679 - INFO - training batch 1, loss: 0.291, 32/60000 datapoints
2025-03-06 18:17:18,875 - INFO - training batch 51, loss: 0.319, 1632/60000 datapoints
2025-03-06 18:17:19,068 - INFO - training batch 101, loss: 0.384, 3232/60000 datapoints
2025-03-06 18:17:19,261 - INFO - training batch 151, loss: 0.193, 4832/60000 datapoints
2025-03-06 18:17:19,455 - INFO - training batch 201, loss: 0.163, 6432/60000 datapoints
2025-03-06 18:17:19,652 - INFO - training batch 251, loss: 0.291, 8032/60000 datapoints
2025-03-06 18:17:19,852 - INFO - training batch 301, loss: 0.386, 9632/60000 datapoints
2025-03-06 18:17:20,050 - INFO - training batch 351, loss: 0.227, 11232/60000 datapoints
2025-03-06 18:17:20,245 - INFO - training batch 401, loss: 0.404, 12832/60000 datapoints
2025-03-06 18:17:20,444 - INFO - training batch 451, loss: 0.149, 14432/60000 datapoints
2025-03-06 18:17:20,640 - INFO - training batch 501, loss: 0.287, 16032/60000 datapoints
2025-03-06 18:17:20,835 - INFO - training batch 551, loss: 0.299, 17632/60000 datapoints
2025-03-06 18:17:21,030 - INFO - training batch 601, loss: 0.459, 19232/60000 datapoints
2025-03-06 18:17:21,226 - INFO - training batch 651, loss: 0.421, 20832/60000 datapoints
2025-03-06 18:17:21,420 - INFO - training batch 701, loss: 0.332, 22432/60000 datapoints
2025-03-06 18:17:21,615 - INFO - training batch 751, loss: 0.212, 24032/60000 datapoints
2025-03-06 18:17:21,810 - INFO - training batch 801, loss: 0.359, 25632/60000 datapoints
2025-03-06 18:17:22,004 - INFO - training batch 851, loss: 0.292, 27232/60000 datapoints
2025-03-06 18:17:22,198 - INFO - training batch 901, loss: 0.205, 28832/60000 datapoints
2025-03-06 18:17:22,394 - INFO - training batch 951, loss: 0.145, 30432/60000 datapoints
2025-03-06 18:17:22,589 - INFO - training batch 1001, loss: 0.176, 32032/60000 datapoints
2025-03-06 18:17:22,787 - INFO - training batch 1051, loss: 0.253, 33632/60000 datapoints
2025-03-06 18:17:22,981 - INFO - training batch 1101, loss: 0.253, 35232/60000 datapoints
2025-03-06 18:17:23,174 - INFO - training batch 1151, loss: 0.406, 36832/60000 datapoints
2025-03-06 18:17:23,368 - INFO - training batch 1201, loss: 0.258, 38432/60000 datapoints
2025-03-06 18:17:23,562 - INFO - training batch 1251, loss: 0.744, 40032/60000 datapoints
2025-03-06 18:17:23,759 - INFO - training batch 1301, loss: 0.492, 41632/60000 datapoints
2025-03-06 18:17:23,959 - INFO - training batch 1351, loss: 0.353, 43232/60000 datapoints
2025-03-06 18:17:24,152 - INFO - training batch 1401, loss: 0.211, 44832/60000 datapoints
2025-03-06 18:17:24,348 - INFO - training batch 1451, loss: 0.213, 46432/60000 datapoints
2025-03-06 18:17:24,544 - INFO - training batch 1501, loss: 0.417, 48032/60000 datapoints
2025-03-06 18:17:24,739 - INFO - training batch 1551, loss: 0.323, 49632/60000 datapoints
2025-03-06 18:17:24,933 - INFO - training batch 1601, loss: 0.180, 51232/60000 datapoints
2025-03-06 18:17:25,130 - INFO - training batch 1651, loss: 0.181, 52832/60000 datapoints
2025-03-06 18:17:25,323 - INFO - training batch 1701, loss: 0.502, 54432/60000 datapoints
2025-03-06 18:17:25,517 - INFO - training batch 1751, loss: 0.323, 56032/60000 datapoints
2025-03-06 18:17:25,720 - INFO - training batch 1801, loss: 0.324, 57632/60000 datapoints
2025-03-06 18:17:25,932 - INFO - training batch 1851, loss: 0.222, 59232/60000 datapoints
2025-03-06 18:17:26,033 - INFO - validation batch 1, loss: 0.430, 32/10016 datapoints
2025-03-06 18:17:26,188 - INFO - validation batch 51, loss: 0.218, 1632/10016 datapoints
2025-03-06 18:17:26,341 - INFO - validation batch 101, loss: 0.422, 3232/10016 datapoints
2025-03-06 18:17:26,496 - INFO - validation batch 151, loss: 0.227, 4832/10016 datapoints
2025-03-06 18:17:26,650 - INFO - validation batch 201, loss: 0.240, 6432/10016 datapoints
2025-03-06 18:17:26,803 - INFO - validation batch 251, loss: 0.494, 8032/10016 datapoints
2025-03-06 18:17:26,955 - INFO - validation batch 301, loss: 0.208, 9632/10016 datapoints
2025-03-06 18:17:26,992 - INFO - Epoch 240/800 done.
2025-03-06 18:17:26,992 - INFO - Final validation performance:
Loss: 0.320, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 18:17:26,993 - INFO - Beginning epoch 241/800
2025-03-06 18:17:26,998 - INFO - training batch 1, loss: 0.326, 32/60000 datapoints
2025-03-06 18:17:27,194 - INFO - training batch 51, loss: 0.307, 1632/60000 datapoints
2025-03-06 18:17:27,389 - INFO - training batch 101, loss: 0.290, 3232/60000 datapoints
2025-03-06 18:17:27,584 - INFO - training batch 151, loss: 0.289, 4832/60000 datapoints
2025-03-06 18:17:27,781 - INFO - training batch 201, loss: 0.151, 6432/60000 datapoints
2025-03-06 18:17:27,976 - INFO - training batch 251, loss: 0.332, 8032/60000 datapoints
2025-03-06 18:17:28,169 - INFO - training batch 301, loss: 0.350, 9632/60000 datapoints
2025-03-06 18:17:28,365 - INFO - training batch 351, loss: 0.403, 11232/60000 datapoints
2025-03-06 18:17:28,564 - INFO - training batch 401, loss: 0.304, 12832/60000 datapoints
2025-03-06 18:17:28,799 - INFO - training batch 451, loss: 0.241, 14432/60000 datapoints
2025-03-06 18:17:28,993 - INFO - training batch 501, loss: 0.184, 16032/60000 datapoints
2025-03-06 18:17:29,187 - INFO - training batch 551, loss: 0.490, 17632/60000 datapoints
2025-03-06 18:17:29,382 - INFO - training batch 601, loss: 0.333, 19232/60000 datapoints
2025-03-06 18:17:29,574 - INFO - training batch 651, loss: 0.134, 20832/60000 datapoints
2025-03-06 18:17:29,770 - INFO - training batch 701, loss: 0.494, 22432/60000 datapoints
2025-03-06 18:17:29,963 - INFO - training batch 751, loss: 0.278, 24032/60000 datapoints
2025-03-06 18:17:30,158 - INFO - training batch 801, loss: 0.424, 25632/60000 datapoints
2025-03-06 18:17:30,354 - INFO - training batch 851, loss: 0.277, 27232/60000 datapoints
2025-03-06 18:17:30,550 - INFO - training batch 901, loss: 0.221, 28832/60000 datapoints
2025-03-06 18:17:30,746 - INFO - training batch 951, loss: 0.256, 30432/60000 datapoints
2025-03-06 18:17:30,939 - INFO - training batch 1001, loss: 0.118, 32032/60000 datapoints
2025-03-06 18:17:31,133 - INFO - training batch 1051, loss: 0.223, 33632/60000 datapoints
2025-03-06 18:17:31,327 - INFO - training batch 1101, loss: 0.220, 35232/60000 datapoints
2025-03-06 18:17:31,519 - INFO - training batch 1151, loss: 0.204, 36832/60000 datapoints
2025-03-06 18:17:31,714 - INFO - training batch 1201, loss: 0.190, 38432/60000 datapoints
2025-03-06 18:17:31,909 - INFO - training batch 1251, loss: 0.219, 40032/60000 datapoints
2025-03-06 18:17:32,104 - INFO - training batch 1301, loss: 0.188, 41632/60000 datapoints
2025-03-06 18:17:32,298 - INFO - training batch 1351, loss: 0.637, 43232/60000 datapoints
2025-03-06 18:17:32,495 - INFO - training batch 1401, loss: 0.290, 44832/60000 datapoints
2025-03-06 18:17:32,691 - INFO - training batch 1451, loss: 0.576, 46432/60000 datapoints
2025-03-06 18:17:32,886 - INFO - training batch 1501, loss: 0.447, 48032/60000 datapoints
2025-03-06 18:17:33,079 - INFO - training batch 1551, loss: 0.413, 49632/60000 datapoints
2025-03-06 18:17:33,274 - INFO - training batch 1601, loss: 0.155, 51232/60000 datapoints
2025-03-06 18:17:33,468 - INFO - training batch 1651, loss: 0.203, 52832/60000 datapoints
2025-03-06 18:17:33,664 - INFO - training batch 1701, loss: 0.399, 54432/60000 datapoints
2025-03-06 18:17:33,860 - INFO - training batch 1751, loss: 0.166, 56032/60000 datapoints
2025-03-06 18:17:34,055 - INFO - training batch 1801, loss: 0.358, 57632/60000 datapoints
2025-03-06 18:17:34,250 - INFO - training batch 1851, loss: 0.301, 59232/60000 datapoints
2025-03-06 18:17:34,353 - INFO - validation batch 1, loss: 0.441, 32/10016 datapoints
2025-03-06 18:17:34,507 - INFO - validation batch 51, loss: 0.192, 1632/10016 datapoints
2025-03-06 18:17:34,661 - INFO - validation batch 101, loss: 0.529, 3232/10016 datapoints
2025-03-06 18:17:34,815 - INFO - validation batch 151, loss: 0.212, 4832/10016 datapoints
2025-03-06 18:17:34,970 - INFO - validation batch 201, loss: 0.366, 6432/10016 datapoints
2025-03-06 18:17:35,128 - INFO - validation batch 251, loss: 0.431, 8032/10016 datapoints
2025-03-06 18:17:35,280 - INFO - validation batch 301, loss: 0.305, 9632/10016 datapoints
2025-03-06 18:17:35,318 - INFO - Epoch 241/800 done.
2025-03-06 18:17:35,319 - INFO - Final validation performance:
Loss: 0.354, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 18:17:35,319 - INFO - Beginning epoch 242/800
2025-03-06 18:17:35,325 - INFO - training batch 1, loss: 0.444, 32/60000 datapoints
2025-03-06 18:17:35,519 - INFO - training batch 51, loss: 0.303, 1632/60000 datapoints
2025-03-06 18:17:35,716 - INFO - training batch 101, loss: 0.399, 3232/60000 datapoints
2025-03-06 18:17:35,931 - INFO - training batch 151, loss: 0.216, 4832/60000 datapoints
2025-03-06 18:17:36,127 - INFO - training batch 201, loss: 0.280, 6432/60000 datapoints
2025-03-06 18:17:36,324 - INFO - training batch 251, loss: 0.570, 8032/60000 datapoints
2025-03-06 18:17:36,520 - INFO - training batch 301, loss: 0.435, 9632/60000 datapoints
2025-03-06 18:17:36,717 - INFO - training batch 351, loss: 0.595, 11232/60000 datapoints
2025-03-06 18:17:36,910 - INFO - training batch 401, loss: 0.392, 12832/60000 datapoints
2025-03-06 18:17:37,126 - INFO - training batch 451, loss: 0.319, 14432/60000 datapoints
2025-03-06 18:17:37,329 - INFO - training batch 501, loss: 0.367, 16032/60000 datapoints
2025-03-06 18:17:37,523 - INFO - training batch 551, loss: 0.205, 17632/60000 datapoints
2025-03-06 18:17:37,736 - INFO - training batch 601, loss: 0.417, 19232/60000 datapoints
2025-03-06 18:17:37,931 - INFO - training batch 651, loss: 0.352, 20832/60000 datapoints
2025-03-06 18:17:38,124 - INFO - training batch 701, loss: 0.206, 22432/60000 datapoints
2025-03-06 18:17:38,353 - INFO - training batch 751, loss: 0.345, 24032/60000 datapoints
2025-03-06 18:17:38,550 - INFO - training batch 801, loss: 0.268, 25632/60000 datapoints
2025-03-06 18:17:38,747 - INFO - training batch 851, loss: 0.306, 27232/60000 datapoints
2025-03-06 18:17:38,942 - INFO - training batch 901, loss: 0.437, 28832/60000 datapoints
2025-03-06 18:17:39,148 - INFO - training batch 951, loss: 0.230, 30432/60000 datapoints
2025-03-06 18:17:39,345 - INFO - training batch 1001, loss: 0.414, 32032/60000 datapoints
2025-03-06 18:17:39,540 - INFO - training batch 1051, loss: 0.315, 33632/60000 datapoints
2025-03-06 18:17:39,736 - INFO - training batch 1101, loss: 0.467, 35232/60000 datapoints
2025-03-06 18:17:39,933 - INFO - training batch 1151, loss: 0.229, 36832/60000 datapoints
2025-03-06 18:17:40,126 - INFO - training batch 1201, loss: 0.227, 38432/60000 datapoints
2025-03-06 18:17:40,319 - INFO - training batch 1251, loss: 0.343, 40032/60000 datapoints
2025-03-06 18:17:40,518 - INFO - training batch 1301, loss: 0.245, 41632/60000 datapoints
2025-03-06 18:17:40,715 - INFO - training batch 1351, loss: 0.306, 43232/60000 datapoints
2025-03-06 18:17:40,911 - INFO - training batch 1401, loss: 0.520, 44832/60000 datapoints
2025-03-06 18:17:41,107 - INFO - training batch 1451, loss: 0.269, 46432/60000 datapoints
2025-03-06 18:17:41,301 - INFO - training batch 1501, loss: 0.317, 48032/60000 datapoints
2025-03-06 18:17:41,495 - INFO - training batch 1551, loss: 0.125, 49632/60000 datapoints
2025-03-06 18:17:41,691 - INFO - training batch 1601, loss: 0.511, 51232/60000 datapoints
2025-03-06 18:17:41,886 - INFO - training batch 1651, loss: 0.179, 52832/60000 datapoints
2025-03-06 18:17:42,079 - INFO - training batch 1701, loss: 0.324, 54432/60000 datapoints
2025-03-06 18:17:42,271 - INFO - training batch 1751, loss: 0.729, 56032/60000 datapoints
2025-03-06 18:17:42,466 - INFO - training batch 1801, loss: 0.123, 57632/60000 datapoints
2025-03-06 18:17:42,667 - INFO - training batch 1851, loss: 0.265, 59232/60000 datapoints
2025-03-06 18:17:42,769 - INFO - validation batch 1, loss: 0.301, 32/10016 datapoints
2025-03-06 18:17:42,922 - INFO - validation batch 51, loss: 0.108, 1632/10016 datapoints
2025-03-06 18:17:43,075 - INFO - validation batch 101, loss: 0.448, 3232/10016 datapoints
2025-03-06 18:17:43,226 - INFO - validation batch 151, loss: 0.219, 4832/10016 datapoints
2025-03-06 18:17:43,379 - INFO - validation batch 201, loss: 0.150, 6432/10016 datapoints
2025-03-06 18:17:43,533 - INFO - validation batch 251, loss: 0.438, 8032/10016 datapoints
2025-03-06 18:17:43,689 - INFO - validation batch 301, loss: 0.509, 9632/10016 datapoints
2025-03-06 18:17:43,726 - INFO - Epoch 242/800 done.
2025-03-06 18:17:43,726 - INFO - Final validation performance:
Loss: 0.310, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 18:17:43,727 - INFO - Beginning epoch 243/800
2025-03-06 18:17:43,732 - INFO - training batch 1, loss: 0.204, 32/60000 datapoints
2025-03-06 18:17:43,930 - INFO - training batch 51, loss: 0.175, 1632/60000 datapoints
2025-03-06 18:17:44,123 - INFO - training batch 101, loss: 0.197, 3232/60000 datapoints
2025-03-06 18:17:44,318 - INFO - training batch 151, loss: 0.348, 4832/60000 datapoints
2025-03-06 18:17:44,517 - INFO - training batch 201, loss: 0.216, 6432/60000 datapoints
2025-03-06 18:17:44,717 - INFO - training batch 251, loss: 0.339, 8032/60000 datapoints
2025-03-06 18:17:44,914 - INFO - training batch 301, loss: 0.489, 9632/60000 datapoints
2025-03-06 18:17:45,112 - INFO - training batch 351, loss: 0.391, 11232/60000 datapoints
2025-03-06 18:17:45,333 - INFO - training batch 401, loss: 0.482, 12832/60000 datapoints
2025-03-06 18:17:45,533 - INFO - training batch 451, loss: 0.451, 14432/60000 datapoints
2025-03-06 18:17:45,728 - INFO - training batch 501, loss: 0.279, 16032/60000 datapoints
2025-03-06 18:17:45,935 - INFO - training batch 551, loss: 0.352, 17632/60000 datapoints
2025-03-06 18:17:46,136 - INFO - training batch 601, loss: 0.143, 19232/60000 datapoints
2025-03-06 18:17:46,333 - INFO - training batch 651, loss: 0.309, 20832/60000 datapoints
2025-03-06 18:17:46,529 - INFO - training batch 701, loss: 0.327, 22432/60000 datapoints
2025-03-06 18:17:46,726 - INFO - training batch 751, loss: 0.340, 24032/60000 datapoints
2025-03-06 18:17:46,921 - INFO - training batch 801, loss: 0.390, 25632/60000 datapoints
2025-03-06 18:17:47,115 - INFO - training batch 851, loss: 0.178, 27232/60000 datapoints
2025-03-06 18:17:47,309 - INFO - training batch 901, loss: 0.448, 28832/60000 datapoints
2025-03-06 18:17:47,505 - INFO - training batch 951, loss: 0.521, 30432/60000 datapoints
2025-03-06 18:17:47,702 - INFO - training batch 1001, loss: 0.249, 32032/60000 datapoints
2025-03-06 18:17:47,895 - INFO - training batch 1051, loss: 0.288, 33632/60000 datapoints
2025-03-06 18:17:48,092 - INFO - training batch 1101, loss: 0.367, 35232/60000 datapoints
2025-03-06 18:17:48,284 - INFO - training batch 1151, loss: 0.275, 36832/60000 datapoints
2025-03-06 18:17:48,480 - INFO - training batch 1201, loss: 0.363, 38432/60000 datapoints
2025-03-06 18:17:48,679 - INFO - training batch 1251, loss: 0.263, 40032/60000 datapoints
2025-03-06 18:17:48,875 - INFO - training batch 1301, loss: 0.260, 41632/60000 datapoints
2025-03-06 18:17:49,073 - INFO - training batch 1351, loss: 0.398, 43232/60000 datapoints
2025-03-06 18:17:49,265 - INFO - training batch 1401, loss: 0.135, 44832/60000 datapoints
2025-03-06 18:17:49,460 - INFO - training batch 1451, loss: 0.312, 46432/60000 datapoints
2025-03-06 18:17:49,657 - INFO - training batch 1501, loss: 0.329, 48032/60000 datapoints
2025-03-06 18:17:49,854 - INFO - training batch 1551, loss: 0.462, 49632/60000 datapoints
2025-03-06 18:17:50,048 - INFO - training batch 1601, loss: 0.316, 51232/60000 datapoints
2025-03-06 18:17:50,247 - INFO - training batch 1651, loss: 0.180, 52832/60000 datapoints
2025-03-06 18:17:50,448 - INFO - training batch 1701, loss: 0.515, 54432/60000 datapoints
2025-03-06 18:17:50,650 - INFO - training batch 1751, loss: 0.227, 56032/60000 datapoints
2025-03-06 18:17:50,845 - INFO - training batch 1801, loss: 0.340, 57632/60000 datapoints
2025-03-06 18:17:51,039 - INFO - training batch 1851, loss: 0.269, 59232/60000 datapoints
2025-03-06 18:17:51,140 - INFO - validation batch 1, loss: 0.292, 32/10016 datapoints
2025-03-06 18:17:51,292 - INFO - validation batch 51, loss: 0.244, 1632/10016 datapoints
2025-03-06 18:17:51,445 - INFO - validation batch 101, loss: 0.281, 3232/10016 datapoints
2025-03-06 18:17:51,602 - INFO - validation batch 151, loss: 0.357, 4832/10016 datapoints
2025-03-06 18:17:51,754 - INFO - validation batch 201, loss: 0.165, 6432/10016 datapoints
2025-03-06 18:17:51,906 - INFO - validation batch 251, loss: 0.362, 8032/10016 datapoints
2025-03-06 18:17:52,058 - INFO - validation batch 301, loss: 0.354, 9632/10016 datapoints
2025-03-06 18:17:52,096 - INFO - Epoch 243/800 done.
2025-03-06 18:17:52,096 - INFO - Final validation performance:
Loss: 0.294, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 18:17:52,096 - INFO - Beginning epoch 244/800
2025-03-06 18:17:52,102 - INFO - training batch 1, loss: 0.362, 32/60000 datapoints
2025-03-06 18:17:52,295 - INFO - training batch 51, loss: 0.421, 1632/60000 datapoints
2025-03-06 18:17:52,490 - INFO - training batch 101, loss: 0.324, 3232/60000 datapoints
2025-03-06 18:17:52,693 - INFO - training batch 151, loss: 0.313, 4832/60000 datapoints
2025-03-06 18:17:52,887 - INFO - training batch 201, loss: 0.457, 6432/60000 datapoints
2025-03-06 18:17:53,083 - INFO - training batch 251, loss: 0.242, 8032/60000 datapoints
2025-03-06 18:17:53,281 - INFO - training batch 301, loss: 0.188, 9632/60000 datapoints
2025-03-06 18:17:53,476 - INFO - training batch 351, loss: 0.468, 11232/60000 datapoints
2025-03-06 18:17:53,673 - INFO - training batch 401, loss: 0.545, 12832/60000 datapoints
2025-03-06 18:17:53,867 - INFO - training batch 451, loss: 0.152, 14432/60000 datapoints
2025-03-06 18:17:54,060 - INFO - training batch 501, loss: 0.460, 16032/60000 datapoints
2025-03-06 18:17:54,254 - INFO - training batch 551, loss: 0.310, 17632/60000 datapoints
2025-03-06 18:17:54,456 - INFO - training batch 601, loss: 0.122, 19232/60000 datapoints
2025-03-06 18:17:54,656 - INFO - training batch 651, loss: 0.248, 20832/60000 datapoints
2025-03-06 18:17:54,853 - INFO - training batch 701, loss: 0.406, 22432/60000 datapoints
2025-03-06 18:17:55,048 - INFO - training batch 751, loss: 0.221, 24032/60000 datapoints
2025-03-06 18:17:55,242 - INFO - training batch 801, loss: 0.246, 25632/60000 datapoints
2025-03-06 18:17:55,436 - INFO - training batch 851, loss: 0.307, 27232/60000 datapoints
2025-03-06 18:17:55,637 - INFO - training batch 901, loss: 0.226, 28832/60000 datapoints
2025-03-06 18:17:55,830 - INFO - training batch 951, loss: 0.439, 30432/60000 datapoints
2025-03-06 18:17:56,039 - INFO - training batch 1001, loss: 0.200, 32032/60000 datapoints
2025-03-06 18:17:56,240 - INFO - training batch 1051, loss: 0.282, 33632/60000 datapoints
2025-03-06 18:17:56,434 - INFO - training batch 1101, loss: 0.181, 35232/60000 datapoints
2025-03-06 18:17:56,633 - INFO - training batch 1151, loss: 0.411, 36832/60000 datapoints
2025-03-06 18:17:56,827 - INFO - training batch 1201, loss: 0.516, 38432/60000 datapoints
2025-03-06 18:17:57,018 - INFO - training batch 1251, loss: 0.665, 40032/60000 datapoints
2025-03-06 18:17:57,214 - INFO - training batch 1301, loss: 0.208, 41632/60000 datapoints
2025-03-06 18:17:57,407 - INFO - training batch 1351, loss: 0.144, 43232/60000 datapoints
2025-03-06 18:17:57,602 - INFO - training batch 1401, loss: 0.118, 44832/60000 datapoints
2025-03-06 18:17:57,796 - INFO - training batch 1451, loss: 0.290, 46432/60000 datapoints
2025-03-06 18:17:57,988 - INFO - training batch 1501, loss: 0.336, 48032/60000 datapoints
2025-03-06 18:17:58,183 - INFO - training batch 1551, loss: 0.250, 49632/60000 datapoints
2025-03-06 18:17:58,378 - INFO - training batch 1601, loss: 0.156, 51232/60000 datapoints
2025-03-06 18:17:58,575 - INFO - training batch 1651, loss: 0.416, 52832/60000 datapoints
2025-03-06 18:17:58,772 - INFO - training batch 1701, loss: 0.330, 54432/60000 datapoints
2025-03-06 18:17:58,964 - INFO - training batch 1751, loss: 0.163, 56032/60000 datapoints
2025-03-06 18:17:59,159 - INFO - training batch 1801, loss: 0.315, 57632/60000 datapoints
2025-03-06 18:17:59,355 - INFO - training batch 1851, loss: 0.236, 59232/60000 datapoints
2025-03-06 18:17:59,459 - INFO - validation batch 1, loss: 0.275, 32/10016 datapoints
2025-03-06 18:17:59,615 - INFO - validation batch 51, loss: 0.134, 1632/10016 datapoints
2025-03-06 18:17:59,768 - INFO - validation batch 101, loss: 0.271, 3232/10016 datapoints
2025-03-06 18:17:59,922 - INFO - validation batch 151, loss: 0.370, 4832/10016 datapoints
2025-03-06 18:18:00,076 - INFO - validation batch 201, loss: 0.459, 6432/10016 datapoints
2025-03-06 18:18:00,230 - INFO - validation batch 251, loss: 0.209, 8032/10016 datapoints
2025-03-06 18:18:00,383 - INFO - validation batch 301, loss: 0.171, 9632/10016 datapoints
2025-03-06 18:18:00,420 - INFO - Epoch 244/800 done.
2025-03-06 18:18:00,420 - INFO - Final validation performance:
Loss: 0.270, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 18:18:00,420 - INFO - Beginning epoch 245/800
2025-03-06 18:18:00,426 - INFO - training batch 1, loss: 0.293, 32/60000 datapoints
2025-03-06 18:18:00,624 - INFO - training batch 51, loss: 0.148, 1632/60000 datapoints
2025-03-06 18:18:00,820 - INFO - training batch 101, loss: 0.203, 3232/60000 datapoints
2025-03-06 18:18:01,013 - INFO - training batch 151, loss: 0.235, 4832/60000 datapoints
2025-03-06 18:18:01,208 - INFO - training batch 201, loss: 0.469, 6432/60000 datapoints
2025-03-06 18:18:01,402 - INFO - training batch 251, loss: 0.424, 8032/60000 datapoints
2025-03-06 18:18:01,601 - INFO - training batch 301, loss: 0.648, 9632/60000 datapoints
2025-03-06 18:18:01,797 - INFO - training batch 351, loss: 0.151, 11232/60000 datapoints
2025-03-06 18:18:01,992 - INFO - training batch 401, loss: 0.520, 12832/60000 datapoints
2025-03-06 18:18:02,187 - INFO - training batch 451, loss: 0.526, 14432/60000 datapoints
2025-03-06 18:18:02,380 - INFO - training batch 501, loss: 0.290, 16032/60000 datapoints
2025-03-06 18:18:02,573 - INFO - training batch 551, loss: 0.700, 17632/60000 datapoints
2025-03-06 18:18:02,774 - INFO - training batch 601, loss: 0.204, 19232/60000 datapoints
2025-03-06 18:18:02,968 - INFO - training batch 651, loss: 0.172, 20832/60000 datapoints
2025-03-06 18:18:03,164 - INFO - training batch 701, loss: 0.203, 22432/60000 datapoints
2025-03-06 18:18:03,357 - INFO - training batch 751, loss: 0.485, 24032/60000 datapoints
2025-03-06 18:18:03,551 - INFO - training batch 801, loss: 0.269, 25632/60000 datapoints
2025-03-06 18:18:03,750 - INFO - training batch 851, loss: 0.471, 27232/60000 datapoints
2025-03-06 18:18:03,945 - INFO - training batch 901, loss: 0.291, 28832/60000 datapoints
2025-03-06 18:18:04,143 - INFO - training batch 951, loss: 0.293, 30432/60000 datapoints
2025-03-06 18:18:04,337 - INFO - training batch 1001, loss: 0.198, 32032/60000 datapoints
2025-03-06 18:18:04,532 - INFO - training batch 1051, loss: 0.303, 33632/60000 datapoints
2025-03-06 18:18:04,736 - INFO - training batch 1101, loss: 0.159, 35232/60000 datapoints
2025-03-06 18:18:04,932 - INFO - training batch 1151, loss: 0.220, 36832/60000 datapoints
2025-03-06 18:18:05,128 - INFO - training batch 1201, loss: 0.105, 38432/60000 datapoints
2025-03-06 18:18:05,321 - INFO - training batch 1251, loss: 0.394, 40032/60000 datapoints
2025-03-06 18:18:05,514 - INFO - training batch 1301, loss: 0.162, 41632/60000 datapoints
2025-03-06 18:18:05,710 - INFO - training batch 1351, loss: 0.394, 43232/60000 datapoints
2025-03-06 18:18:05,904 - INFO - training batch 1401, loss: 0.392, 44832/60000 datapoints
2025-03-06 18:18:06,114 - INFO - training batch 1451, loss: 0.264, 46432/60000 datapoints
2025-03-06 18:18:06,314 - INFO - training batch 1501, loss: 0.193, 48032/60000 datapoints
2025-03-06 18:18:06,510 - INFO - training batch 1551, loss: 0.443, 49632/60000 datapoints
2025-03-06 18:18:06,708 - INFO - training batch 1601, loss: 0.303, 51232/60000 datapoints
2025-03-06 18:18:06,901 - INFO - training batch 1651, loss: 0.128, 52832/60000 datapoints
2025-03-06 18:18:07,094 - INFO - training batch 1701, loss: 0.230, 54432/60000 datapoints
2025-03-06 18:18:07,295 - INFO - training batch 1751, loss: 0.158, 56032/60000 datapoints
2025-03-06 18:18:07,488 - INFO - training batch 1801, loss: 0.231, 57632/60000 datapoints
2025-03-06 18:18:07,686 - INFO - training batch 1851, loss: 0.277, 59232/60000 datapoints
2025-03-06 18:18:07,788 - INFO - validation batch 1, loss: 0.242, 32/10016 datapoints
2025-03-06 18:18:07,942 - INFO - validation batch 51, loss: 0.277, 1632/10016 datapoints
2025-03-06 18:18:08,099 - INFO - validation batch 101, loss: 0.238, 3232/10016 datapoints
2025-03-06 18:18:08,251 - INFO - validation batch 151, loss: 0.370, 4832/10016 datapoints
2025-03-06 18:18:08,405 - INFO - validation batch 201, loss: 0.351, 6432/10016 datapoints
2025-03-06 18:18:08,555 - INFO - validation batch 251, loss: 0.214, 8032/10016 datapoints
2025-03-06 18:18:08,714 - INFO - validation batch 301, loss: 0.310, 9632/10016 datapoints
2025-03-06 18:18:08,749 - INFO - Epoch 245/800 done.
2025-03-06 18:18:08,749 - INFO - Final validation performance:
Loss: 0.286, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 18:18:08,750 - INFO - Beginning epoch 246/800
2025-03-06 18:18:08,756 - INFO - training batch 1, loss: 0.220, 32/60000 datapoints
2025-03-06 18:18:08,952 - INFO - training batch 51, loss: 0.208, 1632/60000 datapoints
2025-03-06 18:18:09,148 - INFO - training batch 101, loss: 0.149, 3232/60000 datapoints
2025-03-06 18:18:09,344 - INFO - training batch 151, loss: 0.200, 4832/60000 datapoints
2025-03-06 18:18:09,538 - INFO - training batch 201, loss: 0.161, 6432/60000 datapoints
2025-03-06 18:18:09,739 - INFO - training batch 251, loss: 0.222, 8032/60000 datapoints
2025-03-06 18:18:09,940 - INFO - training batch 301, loss: 0.326, 9632/60000 datapoints
2025-03-06 18:18:10,137 - INFO - training batch 351, loss: 0.343, 11232/60000 datapoints
2025-03-06 18:18:10,334 - INFO - training batch 401, loss: 0.250, 12832/60000 datapoints
2025-03-06 18:18:10,529 - INFO - training batch 451, loss: 0.195, 14432/60000 datapoints
2025-03-06 18:18:10,735 - INFO - training batch 501, loss: 0.286, 16032/60000 datapoints
2025-03-06 18:18:10,942 - INFO - training batch 551, loss: 0.556, 17632/60000 datapoints
2025-03-06 18:18:11,139 - INFO - training batch 601, loss: 0.356, 19232/60000 datapoints
2025-03-06 18:18:11,336 - INFO - training batch 651, loss: 0.216, 20832/60000 datapoints
2025-03-06 18:18:11,529 - INFO - training batch 701, loss: 0.163, 22432/60000 datapoints
2025-03-06 18:18:11,726 - INFO - training batch 751, loss: 0.306, 24032/60000 datapoints
2025-03-06 18:18:11,923 - INFO - training batch 801, loss: 0.473, 25632/60000 datapoints
2025-03-06 18:18:12,115 - INFO - training batch 851, loss: 0.165, 27232/60000 datapoints
2025-03-06 18:18:12,309 - INFO - training batch 901, loss: 0.291, 28832/60000 datapoints
2025-03-06 18:18:12,502 - INFO - training batch 951, loss: 0.481, 30432/60000 datapoints
2025-03-06 18:18:12,702 - INFO - training batch 1001, loss: 0.432, 32032/60000 datapoints
2025-03-06 18:18:12,897 - INFO - training batch 1051, loss: 0.286, 33632/60000 datapoints
2025-03-06 18:18:13,092 - INFO - training batch 1101, loss: 0.226, 35232/60000 datapoints
2025-03-06 18:18:13,285 - INFO - training batch 1151, loss: 0.243, 36832/60000 datapoints
2025-03-06 18:18:13,480 - INFO - training batch 1201, loss: 0.131, 38432/60000 datapoints
2025-03-06 18:18:13,676 - INFO - training batch 1251, loss: 0.437, 40032/60000 datapoints
2025-03-06 18:18:13,871 - INFO - training batch 1301, loss: 0.474, 41632/60000 datapoints
2025-03-06 18:18:14,066 - INFO - training batch 1351, loss: 0.233, 43232/60000 datapoints
2025-03-06 18:18:14,260 - INFO - training batch 1401, loss: 0.389, 44832/60000 datapoints
2025-03-06 18:18:14,453 - INFO - training batch 1451, loss: 0.160, 46432/60000 datapoints
2025-03-06 18:18:14,652 - INFO - training batch 1501, loss: 0.265, 48032/60000 datapoints
2025-03-06 18:18:14,854 - INFO - training batch 1551, loss: 0.351, 49632/60000 datapoints
2025-03-06 18:18:15,050 - INFO - training batch 1601, loss: 0.155, 51232/60000 datapoints
2025-03-06 18:18:15,245 - INFO - training batch 1651, loss: 0.452, 52832/60000 datapoints
2025-03-06 18:18:15,441 - INFO - training batch 1701, loss: 0.255, 54432/60000 datapoints
2025-03-06 18:18:15,639 - INFO - training batch 1751, loss: 0.156, 56032/60000 datapoints
2025-03-06 18:18:15,833 - INFO - training batch 1801, loss: 0.469, 57632/60000 datapoints
2025-03-06 18:18:16,027 - INFO - training batch 1851, loss: 0.327, 59232/60000 datapoints
2025-03-06 18:18:16,130 - INFO - validation batch 1, loss: 0.298, 32/10016 datapoints
2025-03-06 18:18:16,299 - INFO - validation batch 51, loss: 0.371, 1632/10016 datapoints
2025-03-06 18:18:16,453 - INFO - validation batch 101, loss: 0.431, 3232/10016 datapoints
2025-03-06 18:18:16,607 - INFO - validation batch 151, loss: 0.234, 4832/10016 datapoints
2025-03-06 18:18:16,765 - INFO - validation batch 201, loss: 0.304, 6432/10016 datapoints
2025-03-06 18:18:16,919 - INFO - validation batch 251, loss: 0.221, 8032/10016 datapoints
2025-03-06 18:18:17,072 - INFO - validation batch 301, loss: 0.257, 9632/10016 datapoints
2025-03-06 18:18:17,109 - INFO - Epoch 246/800 done.
2025-03-06 18:18:17,109 - INFO - Final validation performance:
Loss: 0.302, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 18:18:17,109 - INFO - Beginning epoch 247/800
2025-03-06 18:18:17,115 - INFO - training batch 1, loss: 0.212, 32/60000 datapoints
2025-03-06 18:18:17,310 - INFO - training batch 51, loss: 0.403, 1632/60000 datapoints
2025-03-06 18:18:17,500 - INFO - training batch 101, loss: 0.192, 3232/60000 datapoints
2025-03-06 18:18:17,693 - INFO - training batch 151, loss: 0.207, 4832/60000 datapoints
2025-03-06 18:18:17,885 - INFO - training batch 201, loss: 0.152, 6432/60000 datapoints
2025-03-06 18:18:18,080 - INFO - training batch 251, loss: 0.529, 8032/60000 datapoints
2025-03-06 18:18:18,271 - INFO - training batch 301, loss: 0.437, 9632/60000 datapoints
2025-03-06 18:18:18,463 - INFO - training batch 351, loss: 0.519, 11232/60000 datapoints
2025-03-06 18:18:18,657 - INFO - training batch 401, loss: 0.286, 12832/60000 datapoints
2025-03-06 18:18:18,849 - INFO - training batch 451, loss: 0.542, 14432/60000 datapoints
2025-03-06 18:18:19,042 - INFO - training batch 501, loss: 0.421, 16032/60000 datapoints
2025-03-06 18:18:19,237 - INFO - training batch 551, loss: 0.628, 17632/60000 datapoints
2025-03-06 18:18:19,429 - INFO - training batch 601, loss: 0.179, 19232/60000 datapoints
2025-03-06 18:18:19,624 - INFO - training batch 651, loss: 0.349, 20832/60000 datapoints
2025-03-06 18:18:19,836 - INFO - training batch 701, loss: 0.485, 22432/60000 datapoints
2025-03-06 18:18:20,031 - INFO - training batch 751, loss: 0.561, 24032/60000 datapoints
2025-03-06 18:18:20,222 - INFO - training batch 801, loss: 0.194, 25632/60000 datapoints
2025-03-06 18:18:20,415 - INFO - training batch 851, loss: 0.413, 27232/60000 datapoints
2025-03-06 18:18:20,609 - INFO - training batch 901, loss: 0.305, 28832/60000 datapoints
2025-03-06 18:18:20,805 - INFO - training batch 951, loss: 0.273, 30432/60000 datapoints
2025-03-06 18:18:20,999 - INFO - training batch 1001, loss: 0.340, 32032/60000 datapoints
2025-03-06 18:18:21,191 - INFO - training batch 1051, loss: 0.349, 33632/60000 datapoints
2025-03-06 18:18:21,385 - INFO - training batch 1101, loss: 0.382, 35232/60000 datapoints
2025-03-06 18:18:21,576 - INFO - training batch 1151, loss: 0.609, 36832/60000 datapoints
2025-03-06 18:18:21,770 - INFO - training batch 1201, loss: 0.561, 38432/60000 datapoints
2025-03-06 18:18:21,964 - INFO - training batch 1251, loss: 0.234, 40032/60000 datapoints
2025-03-06 18:18:22,154 - INFO - training batch 1301, loss: 0.241, 41632/60000 datapoints
2025-03-06 18:18:22,346 - INFO - training batch 1351, loss: 0.202, 43232/60000 datapoints
2025-03-06 18:18:22,540 - INFO - training batch 1401, loss: 0.397, 44832/60000 datapoints
2025-03-06 18:18:22,737 - INFO - training batch 1451, loss: 0.457, 46432/60000 datapoints
2025-03-06 18:18:22,928 - INFO - training batch 1501, loss: 0.548, 48032/60000 datapoints
2025-03-06 18:18:23,119 - INFO - training batch 1551, loss: 0.372, 49632/60000 datapoints
2025-03-06 18:18:23,310 - INFO - training batch 1601, loss: 0.231, 51232/60000 datapoints
2025-03-06 18:18:23,500 - INFO - training batch 1651, loss: 0.377, 52832/60000 datapoints
2025-03-06 18:18:23,693 - INFO - training batch 1701, loss: 0.308, 54432/60000 datapoints
2025-03-06 18:18:23,885 - INFO - training batch 1751, loss: 0.312, 56032/60000 datapoints
2025-03-06 18:18:24,078 - INFO - training batch 1801, loss: 0.263, 57632/60000 datapoints
2025-03-06 18:18:24,269 - INFO - training batch 1851, loss: 0.273, 59232/60000 datapoints
2025-03-06 18:18:24,367 - INFO - validation batch 1, loss: 0.252, 32/10016 datapoints
2025-03-06 18:18:24,520 - INFO - validation batch 51, loss: 0.361, 1632/10016 datapoints
2025-03-06 18:18:24,674 - INFO - validation batch 101, loss: 0.489, 3232/10016 datapoints
2025-03-06 18:18:24,826 - INFO - validation batch 151, loss: 0.216, 4832/10016 datapoints
2025-03-06 18:18:24,979 - INFO - validation batch 201, loss: 0.150, 6432/10016 datapoints
2025-03-06 18:18:25,134 - INFO - validation batch 251, loss: 0.409, 8032/10016 datapoints
2025-03-06 18:18:25,284 - INFO - validation batch 301, loss: 0.296, 9632/10016 datapoints
2025-03-06 18:18:25,322 - INFO - Epoch 247/800 done.
2025-03-06 18:18:25,322 - INFO - Final validation performance:
Loss: 0.310, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 18:18:25,322 - INFO - Beginning epoch 248/800
2025-03-06 18:18:25,328 - INFO - training batch 1, loss: 0.305, 32/60000 datapoints
2025-03-06 18:18:25,521 - INFO - training batch 51, loss: 0.265, 1632/60000 datapoints
2025-03-06 18:18:25,715 - INFO - training batch 101, loss: 0.644, 3232/60000 datapoints
2025-03-06 18:18:25,907 - INFO - training batch 151, loss: 0.385, 4832/60000 datapoints
2025-03-06 18:18:26,096 - INFO - training batch 201, loss: 0.414, 6432/60000 datapoints
2025-03-06 18:18:26,311 - INFO - training batch 251, loss: 0.521, 8032/60000 datapoints
2025-03-06 18:18:26,503 - INFO - training batch 301, loss: 0.427, 9632/60000 datapoints
2025-03-06 18:18:26,699 - INFO - training batch 351, loss: 0.331, 11232/60000 datapoints
2025-03-06 18:18:26,890 - INFO - training batch 401, loss: 0.152, 12832/60000 datapoints
2025-03-06 18:18:27,083 - INFO - training batch 451, loss: 0.419, 14432/60000 datapoints
2025-03-06 18:18:27,283 - INFO - training batch 501, loss: 0.285, 16032/60000 datapoints
2025-03-06 18:18:27,477 - INFO - training batch 551, loss: 0.483, 17632/60000 datapoints
2025-03-06 18:18:27,674 - INFO - training batch 601, loss: 0.348, 19232/60000 datapoints
2025-03-06 18:18:27,867 - INFO - training batch 651, loss: 0.306, 20832/60000 datapoints
2025-03-06 18:18:28,060 - INFO - training batch 701, loss: 0.324, 22432/60000 datapoints
2025-03-06 18:18:28,253 - INFO - training batch 751, loss: 0.236, 24032/60000 datapoints
2025-03-06 18:18:28,447 - INFO - training batch 801, loss: 0.177, 25632/60000 datapoints
2025-03-06 18:18:28,644 - INFO - training batch 851, loss: 0.262, 27232/60000 datapoints
2025-03-06 18:18:28,839 - INFO - training batch 901, loss: 0.252, 28832/60000 datapoints
2025-03-06 18:18:29,032 - INFO - training batch 951, loss: 0.416, 30432/60000 datapoints
2025-03-06 18:18:29,225 - INFO - training batch 1001, loss: 0.482, 32032/60000 datapoints
2025-03-06 18:18:29,424 - INFO - training batch 1051, loss: 0.393, 33632/60000 datapoints
2025-03-06 18:18:29,625 - INFO - training batch 1101, loss: 0.664, 35232/60000 datapoints
2025-03-06 18:18:29,827 - INFO - training batch 1151, loss: 0.458, 36832/60000 datapoints
2025-03-06 18:18:30,028 - INFO - training batch 1201, loss: 0.296, 38432/60000 datapoints
2025-03-06 18:18:30,223 - INFO - training batch 1251, loss: 0.490, 40032/60000 datapoints
2025-03-06 18:18:30,422 - INFO - training batch 1301, loss: 0.237, 41632/60000 datapoints
2025-03-06 18:18:30,621 - INFO - training batch 1351, loss: 0.486, 43232/60000 datapoints
2025-03-06 18:18:30,819 - INFO - training batch 1401, loss: 0.395, 44832/60000 datapoints
2025-03-06 18:18:31,013 - INFO - training batch 1451, loss: 0.322, 46432/60000 datapoints
2025-03-06 18:18:31,207 - INFO - training batch 1501, loss: 0.313, 48032/60000 datapoints
2025-03-06 18:18:31,400 - INFO - training batch 1551, loss: 0.243, 49632/60000 datapoints
2025-03-06 18:18:31,595 - INFO - training batch 1601, loss: 0.367, 51232/60000 datapoints
2025-03-06 18:18:31,794 - INFO - training batch 1651, loss: 0.210, 52832/60000 datapoints
2025-03-06 18:18:31,988 - INFO - training batch 1701, loss: 0.346, 54432/60000 datapoints
2025-03-06 18:18:32,182 - INFO - training batch 1751, loss: 0.810, 56032/60000 datapoints
2025-03-06 18:18:32,376 - INFO - training batch 1801, loss: 0.169, 57632/60000 datapoints
2025-03-06 18:18:32,571 - INFO - training batch 1851, loss: 0.174, 59232/60000 datapoints
2025-03-06 18:18:32,674 - INFO - validation batch 1, loss: 0.262, 32/10016 datapoints
2025-03-06 18:18:32,829 - INFO - validation batch 51, loss: 0.584, 1632/10016 datapoints
2025-03-06 18:18:32,981 - INFO - validation batch 101, loss: 0.198, 3232/10016 datapoints
2025-03-06 18:18:33,132 - INFO - validation batch 151, loss: 0.266, 4832/10016 datapoints
2025-03-06 18:18:33,286 - INFO - validation batch 201, loss: 0.482, 6432/10016 datapoints
2025-03-06 18:18:33,438 - INFO - validation batch 251, loss: 0.250, 8032/10016 datapoints
2025-03-06 18:18:33,592 - INFO - validation batch 301, loss: 0.171, 9632/10016 datapoints
2025-03-06 18:18:33,632 - INFO - Epoch 248/800 done.
2025-03-06 18:18:33,632 - INFO - Final validation performance:
Loss: 0.316, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 18:18:33,633 - INFO - Beginning epoch 249/800
2025-03-06 18:18:33,639 - INFO - training batch 1, loss: 0.397, 32/60000 datapoints
2025-03-06 18:18:33,833 - INFO - training batch 51, loss: 0.284, 1632/60000 datapoints
2025-03-06 18:18:34,029 - INFO - training batch 101, loss: 0.148, 3232/60000 datapoints
2025-03-06 18:18:34,222 - INFO - training batch 151, loss: 0.252, 4832/60000 datapoints
2025-03-06 18:18:34,418 - INFO - training batch 201, loss: 0.154, 6432/60000 datapoints
2025-03-06 18:18:34,617 - INFO - training batch 251, loss: 0.257, 8032/60000 datapoints
2025-03-06 18:18:34,814 - INFO - training batch 301, loss: 0.256, 9632/60000 datapoints
2025-03-06 18:18:35,016 - INFO - training batch 351, loss: 0.163, 11232/60000 datapoints
2025-03-06 18:18:35,212 - INFO - training batch 401, loss: 0.301, 12832/60000 datapoints
2025-03-06 18:18:35,408 - INFO - training batch 451, loss: 0.590, 14432/60000 datapoints
2025-03-06 18:18:35,605 - INFO - training batch 501, loss: 0.258, 16032/60000 datapoints
2025-03-06 18:18:35,798 - INFO - training batch 551, loss: 0.598, 17632/60000 datapoints
2025-03-06 18:18:35,991 - INFO - training batch 601, loss: 0.166, 19232/60000 datapoints
2025-03-06 18:18:36,188 - INFO - training batch 651, loss: 0.168, 20832/60000 datapoints
2025-03-06 18:18:36,402 - INFO - training batch 701, loss: 0.249, 22432/60000 datapoints
2025-03-06 18:18:36,600 - INFO - training batch 751, loss: 0.140, 24032/60000 datapoints
2025-03-06 18:18:36,798 - INFO - training batch 801, loss: 0.532, 25632/60000 datapoints
2025-03-06 18:18:36,990 - INFO - training batch 851, loss: 0.171, 27232/60000 datapoints
2025-03-06 18:18:37,187 - INFO - training batch 901, loss: 0.604, 28832/60000 datapoints
2025-03-06 18:18:37,381 - INFO - training batch 951, loss: 0.325, 30432/60000 datapoints
2025-03-06 18:18:37,584 - INFO - training batch 1001, loss: 0.518, 32032/60000 datapoints
2025-03-06 18:18:37,790 - INFO - training batch 1051, loss: 0.533, 33632/60000 datapoints
2025-03-06 18:18:37,983 - INFO - training batch 1101, loss: 0.219, 35232/60000 datapoints
2025-03-06 18:18:38,180 - INFO - training batch 1151, loss: 0.299, 36832/60000 datapoints
2025-03-06 18:18:38,374 - INFO - training batch 1201, loss: 0.259, 38432/60000 datapoints
2025-03-06 18:18:38,579 - INFO - training batch 1251, loss: 0.413, 40032/60000 datapoints
2025-03-06 18:18:38,801 - INFO - training batch 1301, loss: 0.294, 41632/60000 datapoints
2025-03-06 18:18:38,999 - INFO - training batch 1351, loss: 0.299, 43232/60000 datapoints
2025-03-06 18:18:39,194 - INFO - training batch 1401, loss: 0.280, 44832/60000 datapoints
2025-03-06 18:18:39,386 - INFO - training batch 1451, loss: 0.181, 46432/60000 datapoints
2025-03-06 18:18:39,581 - INFO - training batch 1501, loss: 0.233, 48032/60000 datapoints
2025-03-06 18:18:39,776 - INFO - training batch 1551, loss: 0.336, 49632/60000 datapoints
2025-03-06 18:18:39,974 - INFO - training batch 1601, loss: 0.213, 51232/60000 datapoints
2025-03-06 18:18:40,167 - INFO - training batch 1651, loss: 0.332, 52832/60000 datapoints
2025-03-06 18:18:40,365 - INFO - training batch 1701, loss: 0.821, 54432/60000 datapoints
2025-03-06 18:18:40,558 - INFO - training batch 1751, loss: 0.294, 56032/60000 datapoints
2025-03-06 18:18:40,760 - INFO - training batch 1801, loss: 0.243, 57632/60000 datapoints
2025-03-06 18:18:40,956 - INFO - training batch 1851, loss: 0.409, 59232/60000 datapoints
2025-03-06 18:18:41,058 - INFO - validation batch 1, loss: 0.547, 32/10016 datapoints
2025-03-06 18:18:41,210 - INFO - validation batch 51, loss: 0.211, 1632/10016 datapoints
2025-03-06 18:18:41,363 - INFO - validation batch 101, loss: 0.199, 3232/10016 datapoints
2025-03-06 18:18:41,517 - INFO - validation batch 151, loss: 0.164, 4832/10016 datapoints
2025-03-06 18:18:41,672 - INFO - validation batch 201, loss: 0.279, 6432/10016 datapoints
2025-03-06 18:18:41,824 - INFO - validation batch 251, loss: 0.291, 8032/10016 datapoints
2025-03-06 18:18:41,978 - INFO - validation batch 301, loss: 0.384, 9632/10016 datapoints
2025-03-06 18:18:42,015 - INFO - Epoch 249/800 done.
2025-03-06 18:18:42,016 - INFO - Final validation performance:
Loss: 0.296, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:18:42,016 - INFO - Beginning epoch 250/800
2025-03-06 18:18:42,022 - INFO - training batch 1, loss: 0.120, 32/60000 datapoints
2025-03-06 18:18:42,216 - INFO - training batch 51, loss: 0.404, 1632/60000 datapoints
2025-03-06 18:18:42,407 - INFO - training batch 101, loss: 0.520, 3232/60000 datapoints
2025-03-06 18:18:42,602 - INFO - training batch 151, loss: 0.616, 4832/60000 datapoints
2025-03-06 18:18:42,799 - INFO - training batch 201, loss: 0.286, 6432/60000 datapoints
2025-03-06 18:18:42,994 - INFO - training batch 251, loss: 0.200, 8032/60000 datapoints
2025-03-06 18:18:43,186 - INFO - training batch 301, loss: 0.224, 9632/60000 datapoints
2025-03-06 18:18:43,378 - INFO - training batch 351, loss: 0.207, 11232/60000 datapoints
2025-03-06 18:18:43,573 - INFO - training batch 401, loss: 0.273, 12832/60000 datapoints
2025-03-06 18:18:43,768 - INFO - training batch 451, loss: 0.566, 14432/60000 datapoints
2025-03-06 18:18:43,959 - INFO - training batch 501, loss: 0.145, 16032/60000 datapoints
2025-03-06 18:18:44,151 - INFO - training batch 551, loss: 0.298, 17632/60000 datapoints
2025-03-06 18:18:44,344 - INFO - training batch 601, loss: 0.159, 19232/60000 datapoints
2025-03-06 18:18:44,536 - INFO - training batch 651, loss: 0.215, 20832/60000 datapoints
2025-03-06 18:18:44,738 - INFO - training batch 701, loss: 0.445, 22432/60000 datapoints
2025-03-06 18:18:44,937 - INFO - training batch 751, loss: 0.221, 24032/60000 datapoints
2025-03-06 18:18:45,129 - INFO - training batch 801, loss: 0.392, 25632/60000 datapoints
2025-03-06 18:18:45,321 - INFO - training batch 851, loss: 0.363, 27232/60000 datapoints
2025-03-06 18:18:45,514 - INFO - training batch 901, loss: 0.175, 28832/60000 datapoints
2025-03-06 18:18:45,709 - INFO - training batch 951, loss: 0.390, 30432/60000 datapoints
2025-03-06 18:18:45,903 - INFO - training batch 1001, loss: 0.280, 32032/60000 datapoints
2025-03-06 18:18:46,094 - INFO - training batch 1051, loss: 0.272, 33632/60000 datapoints
2025-03-06 18:18:46,289 - INFO - training batch 1101, loss: 0.604, 35232/60000 datapoints
2025-03-06 18:18:46,501 - INFO - training batch 1151, loss: 0.497, 36832/60000 datapoints
2025-03-06 18:18:46,697 - INFO - training batch 1201, loss: 0.398, 38432/60000 datapoints
2025-03-06 18:18:46,890 - INFO - training batch 1251, loss: 0.426, 40032/60000 datapoints
2025-03-06 18:18:47,080 - INFO - training batch 1301, loss: 0.445, 41632/60000 datapoints
2025-03-06 18:18:47,277 - INFO - training batch 1351, loss: 0.172, 43232/60000 datapoints
2025-03-06 18:18:47,472 - INFO - training batch 1401, loss: 0.551, 44832/60000 datapoints
2025-03-06 18:18:47,670 - INFO - training batch 1451, loss: 0.411, 46432/60000 datapoints
2025-03-06 18:18:47,866 - INFO - training batch 1501, loss: 0.365, 48032/60000 datapoints
2025-03-06 18:18:48,058 - INFO - training batch 1551, loss: 0.219, 49632/60000 datapoints
2025-03-06 18:18:48,252 - INFO - training batch 1601, loss: 0.325, 51232/60000 datapoints
2025-03-06 18:18:48,446 - INFO - training batch 1651, loss: 0.275, 52832/60000 datapoints
2025-03-06 18:18:48,640 - INFO - training batch 1701, loss: 0.185, 54432/60000 datapoints
2025-03-06 18:18:48,837 - INFO - training batch 1751, loss: 0.310, 56032/60000 datapoints
2025-03-06 18:18:49,030 - INFO - training batch 1801, loss: 0.208, 57632/60000 datapoints
2025-03-06 18:18:49,232 - INFO - training batch 1851, loss: 0.316, 59232/60000 datapoints
2025-03-06 18:18:49,334 - INFO - validation batch 1, loss: 0.385, 32/10016 datapoints
2025-03-06 18:18:49,487 - INFO - validation batch 51, loss: 0.448, 1632/10016 datapoints
2025-03-06 18:18:49,644 - INFO - validation batch 101, loss: 0.283, 3232/10016 datapoints
2025-03-06 18:18:49,796 - INFO - validation batch 151, loss: 0.164, 4832/10016 datapoints
2025-03-06 18:18:49,951 - INFO - validation batch 201, loss: 0.172, 6432/10016 datapoints
2025-03-06 18:18:50,108 - INFO - validation batch 251, loss: 0.420, 8032/10016 datapoints
2025-03-06 18:18:50,266 - INFO - validation batch 301, loss: 0.553, 9632/10016 datapoints
2025-03-06 18:18:50,301 - INFO - Epoch 250/800 done.
2025-03-06 18:18:50,301 - INFO - Final validation performance:
Loss: 0.347, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:18:50,301 - INFO - Beginning epoch 251/800
2025-03-06 18:18:50,309 - INFO - training batch 1, loss: 0.323, 32/60000 datapoints
2025-03-06 18:18:50,504 - INFO - training batch 51, loss: 0.137, 1632/60000 datapoints
2025-03-06 18:18:50,703 - INFO - training batch 101, loss: 0.303, 3232/60000 datapoints
2025-03-06 18:18:50,901 - INFO - training batch 151, loss: 0.335, 4832/60000 datapoints
2025-03-06 18:18:51,096 - INFO - training batch 201, loss: 0.800, 6432/60000 datapoints
2025-03-06 18:18:51,290 - INFO - training batch 251, loss: 0.233, 8032/60000 datapoints
2025-03-06 18:18:51,485 - INFO - training batch 301, loss: 0.453, 9632/60000 datapoints
2025-03-06 18:18:51,684 - INFO - training batch 351, loss: 0.376, 11232/60000 datapoints
2025-03-06 18:18:51,880 - INFO - training batch 401, loss: 0.149, 12832/60000 datapoints
2025-03-06 18:18:52,074 - INFO - training batch 451, loss: 0.254, 14432/60000 datapoints
2025-03-06 18:18:52,269 - INFO - training batch 501, loss: 0.201, 16032/60000 datapoints
2025-03-06 18:18:52,463 - INFO - training batch 551, loss: 0.287, 17632/60000 datapoints
2025-03-06 18:18:52,663 - INFO - training batch 601, loss: 0.671, 19232/60000 datapoints
2025-03-06 18:18:52,861 - INFO - training batch 651, loss: 0.240, 20832/60000 datapoints
2025-03-06 18:18:53,054 - INFO - training batch 701, loss: 0.191, 22432/60000 datapoints
2025-03-06 18:18:53,246 - INFO - training batch 751, loss: 0.237, 24032/60000 datapoints
2025-03-06 18:18:53,439 - INFO - training batch 801, loss: 0.526, 25632/60000 datapoints
2025-03-06 18:18:53,634 - INFO - training batch 851, loss: 0.163, 27232/60000 datapoints
2025-03-06 18:18:53,828 - INFO - training batch 901, loss: 0.576, 28832/60000 datapoints
2025-03-06 18:18:54,021 - INFO - training batch 951, loss: 0.376, 30432/60000 datapoints
2025-03-06 18:18:54,217 - INFO - training batch 1001, loss: 0.158, 32032/60000 datapoints
2025-03-06 18:18:54,413 - INFO - training batch 1051, loss: 0.221, 33632/60000 datapoints
2025-03-06 18:18:54,613 - INFO - training batch 1101, loss: 0.251, 35232/60000 datapoints
2025-03-06 18:18:54,811 - INFO - training batch 1151, loss: 0.178, 36832/60000 datapoints
2025-03-06 18:18:55,010 - INFO - training batch 1201, loss: 0.174, 38432/60000 datapoints
2025-03-06 18:18:55,205 - INFO - training batch 1251, loss: 0.299, 40032/60000 datapoints
2025-03-06 18:18:55,402 - INFO - training batch 1301, loss: 0.242, 41632/60000 datapoints
2025-03-06 18:18:55,605 - INFO - training batch 1351, loss: 0.477, 43232/60000 datapoints
2025-03-06 18:18:55,802 - INFO - training batch 1401, loss: 0.396, 44832/60000 datapoints
2025-03-06 18:18:55,997 - INFO - training batch 1451, loss: 0.177, 46432/60000 datapoints
2025-03-06 18:18:56,191 - INFO - training batch 1501, loss: 0.417, 48032/60000 datapoints
2025-03-06 18:18:56,387 - INFO - training batch 1551, loss: 0.269, 49632/60000 datapoints
2025-03-06 18:18:56,601 - INFO - training batch 1601, loss: 0.248, 51232/60000 datapoints
2025-03-06 18:18:56,799 - INFO - training batch 1651, loss: 0.225, 52832/60000 datapoints
2025-03-06 18:18:56,994 - INFO - training batch 1701, loss: 0.308, 54432/60000 datapoints
2025-03-06 18:18:57,189 - INFO - training batch 1751, loss: 0.227, 56032/60000 datapoints
2025-03-06 18:18:57,389 - INFO - training batch 1801, loss: 0.426, 57632/60000 datapoints
2025-03-06 18:18:57,582 - INFO - training batch 1851, loss: 0.659, 59232/60000 datapoints
2025-03-06 18:18:57,685 - INFO - validation batch 1, loss: 0.221, 32/10016 datapoints
2025-03-06 18:18:57,840 - INFO - validation batch 51, loss: 0.584, 1632/10016 datapoints
2025-03-06 18:18:57,992 - INFO - validation batch 101, loss: 0.140, 3232/10016 datapoints
2025-03-06 18:18:58,144 - INFO - validation batch 151, loss: 0.373, 4832/10016 datapoints
2025-03-06 18:18:58,300 - INFO - validation batch 201, loss: 0.382, 6432/10016 datapoints
2025-03-06 18:18:58,451 - INFO - validation batch 251, loss: 0.273, 8032/10016 datapoints
2025-03-06 18:18:58,609 - INFO - validation batch 301, loss: 0.484, 9632/10016 datapoints
2025-03-06 18:18:58,645 - INFO - Epoch 251/800 done.
2025-03-06 18:18:58,645 - INFO - Final validation performance:
Loss: 0.351, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:18:58,645 - INFO - Beginning epoch 252/800
2025-03-06 18:18:58,651 - INFO - training batch 1, loss: 0.232, 32/60000 datapoints
2025-03-06 18:18:58,854 - INFO - training batch 51, loss: 0.550, 1632/60000 datapoints
2025-03-06 18:18:59,048 - INFO - training batch 101, loss: 0.304, 3232/60000 datapoints
2025-03-06 18:18:59,247 - INFO - training batch 151, loss: 0.447, 4832/60000 datapoints
2025-03-06 18:18:59,442 - INFO - training batch 201, loss: 0.338, 6432/60000 datapoints
2025-03-06 18:18:59,639 - INFO - training batch 251, loss: 0.766, 8032/60000 datapoints
2025-03-06 18:18:59,833 - INFO - training batch 301, loss: 0.424, 9632/60000 datapoints
2025-03-06 18:19:00,033 - INFO - training batch 351, loss: 0.310, 11232/60000 datapoints
2025-03-06 18:19:00,226 - INFO - training batch 401, loss: 0.763, 12832/60000 datapoints
2025-03-06 18:19:00,423 - INFO - training batch 451, loss: 0.192, 14432/60000 datapoints
2025-03-06 18:19:00,619 - INFO - training batch 501, loss: 0.342, 16032/60000 datapoints
2025-03-06 18:19:00,818 - INFO - training batch 551, loss: 0.191, 17632/60000 datapoints
2025-03-06 18:19:01,014 - INFO - training batch 601, loss: 0.298, 19232/60000 datapoints
2025-03-06 18:19:01,215 - INFO - training batch 651, loss: 0.319, 20832/60000 datapoints
2025-03-06 18:19:01,410 - INFO - training batch 701, loss: 0.371, 22432/60000 datapoints
2025-03-06 18:19:01,608 - INFO - training batch 751, loss: 0.184, 24032/60000 datapoints
2025-03-06 18:19:01,801 - INFO - training batch 801, loss: 0.464, 25632/60000 datapoints
2025-03-06 18:19:02,000 - INFO - training batch 851, loss: 0.192, 27232/60000 datapoints
2025-03-06 18:19:02,198 - INFO - training batch 901, loss: 0.294, 28832/60000 datapoints
2025-03-06 18:19:02,395 - INFO - training batch 951, loss: 0.501, 30432/60000 datapoints
2025-03-06 18:19:02,590 - INFO - training batch 1001, loss: 0.236, 32032/60000 datapoints
2025-03-06 18:19:02,787 - INFO - training batch 1051, loss: 0.528, 33632/60000 datapoints
2025-03-06 18:19:02,987 - INFO - training batch 1101, loss: 0.540, 35232/60000 datapoints
2025-03-06 18:19:03,181 - INFO - training batch 1151, loss: 0.284, 36832/60000 datapoints
2025-03-06 18:19:03,377 - INFO - training batch 1201, loss: 0.521, 38432/60000 datapoints
2025-03-06 18:19:03,572 - INFO - training batch 1251, loss: 0.350, 40032/60000 datapoints
2025-03-06 18:19:03,767 - INFO - training batch 1301, loss: 0.254, 41632/60000 datapoints
2025-03-06 18:19:03,963 - INFO - training batch 1351, loss: 0.233, 43232/60000 datapoints
2025-03-06 18:19:04,157 - INFO - training batch 1401, loss: 0.178, 44832/60000 datapoints
2025-03-06 18:19:04,351 - INFO - training batch 1451, loss: 0.286, 46432/60000 datapoints
2025-03-06 18:19:04,545 - INFO - training batch 1501, loss: 0.314, 48032/60000 datapoints
2025-03-06 18:19:04,742 - INFO - training batch 1551, loss: 0.246, 49632/60000 datapoints
2025-03-06 18:19:04,944 - INFO - training batch 1601, loss: 0.374, 51232/60000 datapoints
2025-03-06 18:19:05,139 - INFO - training batch 1651, loss: 0.469, 52832/60000 datapoints
2025-03-06 18:19:05,334 - INFO - training batch 1701, loss: 0.603, 54432/60000 datapoints
2025-03-06 18:19:05,530 - INFO - training batch 1751, loss: 0.170, 56032/60000 datapoints
2025-03-06 18:19:05,726 - INFO - training batch 1801, loss: 0.370, 57632/60000 datapoints
2025-03-06 18:19:05,931 - INFO - training batch 1851, loss: 0.322, 59232/60000 datapoints
2025-03-06 18:19:06,032 - INFO - validation batch 1, loss: 0.237, 32/10016 datapoints
2025-03-06 18:19:06,189 - INFO - validation batch 51, loss: 0.238, 1632/10016 datapoints
2025-03-06 18:19:06,341 - INFO - validation batch 101, loss: 0.241, 3232/10016 datapoints
2025-03-06 18:19:06,497 - INFO - validation batch 151, loss: 0.162, 4832/10016 datapoints
2025-03-06 18:19:06,672 - INFO - validation batch 201, loss: 0.380, 6432/10016 datapoints
2025-03-06 18:19:06,827 - INFO - validation batch 251, loss: 0.267, 8032/10016 datapoints
2025-03-06 18:19:06,983 - INFO - validation batch 301, loss: 0.282, 9632/10016 datapoints
2025-03-06 18:19:07,020 - INFO - Epoch 252/800 done.
2025-03-06 18:19:07,020 - INFO - Final validation performance:
Loss: 0.258, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:19:07,020 - INFO - Beginning epoch 253/800
2025-03-06 18:19:07,026 - INFO - training batch 1, loss: 0.236, 32/60000 datapoints
2025-03-06 18:19:07,222 - INFO - training batch 51, loss: 0.267, 1632/60000 datapoints
2025-03-06 18:19:07,420 - INFO - training batch 101, loss: 0.209, 3232/60000 datapoints
2025-03-06 18:19:07,621 - INFO - training batch 151, loss: 0.289, 4832/60000 datapoints
2025-03-06 18:19:07,817 - INFO - training batch 201, loss: 0.195, 6432/60000 datapoints
2025-03-06 18:19:08,015 - INFO - training batch 251, loss: 0.463, 8032/60000 datapoints
2025-03-06 18:19:08,215 - INFO - training batch 301, loss: 0.144, 9632/60000 datapoints
2025-03-06 18:19:08,409 - INFO - training batch 351, loss: 0.373, 11232/60000 datapoints
2025-03-06 18:19:08,605 - INFO - training batch 401, loss: 0.313, 12832/60000 datapoints
2025-03-06 18:19:08,806 - INFO - training batch 451, loss: 0.625, 14432/60000 datapoints
2025-03-06 18:19:09,012 - INFO - training batch 501, loss: 0.139, 16032/60000 datapoints
2025-03-06 18:19:09,208 - INFO - training batch 551, loss: 0.299, 17632/60000 datapoints
2025-03-06 18:19:09,408 - INFO - training batch 601, loss: 0.424, 19232/60000 datapoints
2025-03-06 18:19:09,607 - INFO - training batch 651, loss: 0.250, 20832/60000 datapoints
2025-03-06 18:19:09,803 - INFO - training batch 701, loss: 0.462, 22432/60000 datapoints
2025-03-06 18:19:09,999 - INFO - training batch 751, loss: 0.290, 24032/60000 datapoints
2025-03-06 18:19:10,198 - INFO - training batch 801, loss: 0.234, 25632/60000 datapoints
2025-03-06 18:19:10,394 - INFO - training batch 851, loss: 0.294, 27232/60000 datapoints
2025-03-06 18:19:10,588 - INFO - training batch 901, loss: 0.127, 28832/60000 datapoints
2025-03-06 18:19:10,788 - INFO - training batch 951, loss: 0.559, 30432/60000 datapoints
2025-03-06 18:19:10,989 - INFO - training batch 1001, loss: 0.316, 32032/60000 datapoints
2025-03-06 18:19:11,185 - INFO - training batch 1051, loss: 0.325, 33632/60000 datapoints
2025-03-06 18:19:11,380 - INFO - training batch 1101, loss: 0.128, 35232/60000 datapoints
2025-03-06 18:19:11,575 - INFO - training batch 1151, loss: 0.415, 36832/60000 datapoints
2025-03-06 18:19:11,771 - INFO - training batch 1201, loss: 0.569, 38432/60000 datapoints
2025-03-06 18:19:11,968 - INFO - training batch 1251, loss: 0.273, 40032/60000 datapoints
2025-03-06 18:19:12,162 - INFO - training batch 1301, loss: 0.455, 41632/60000 datapoints
2025-03-06 18:19:12,356 - INFO - training batch 1351, loss: 0.341, 43232/60000 datapoints
2025-03-06 18:19:12,552 - INFO - training batch 1401, loss: 0.233, 44832/60000 datapoints
2025-03-06 18:19:12,747 - INFO - training batch 1451, loss: 0.536, 46432/60000 datapoints
2025-03-06 18:19:12,942 - INFO - training batch 1501, loss: 0.282, 48032/60000 datapoints
2025-03-06 18:19:13,137 - INFO - training batch 1551, loss: 0.501, 49632/60000 datapoints
2025-03-06 18:19:13,330 - INFO - training batch 1601, loss: 0.246, 51232/60000 datapoints
2025-03-06 18:19:13,527 - INFO - training batch 1651, loss: 0.421, 52832/60000 datapoints
2025-03-06 18:19:13,724 - INFO - training batch 1701, loss: 0.231, 54432/60000 datapoints
2025-03-06 18:19:13,918 - INFO - training batch 1751, loss: 0.182, 56032/60000 datapoints
2025-03-06 18:19:14,113 - INFO - training batch 1801, loss: 0.243, 57632/60000 datapoints
2025-03-06 18:19:14,309 - INFO - training batch 1851, loss: 0.375, 59232/60000 datapoints
2025-03-06 18:19:14,410 - INFO - validation batch 1, loss: 0.304, 32/10016 datapoints
2025-03-06 18:19:14,564 - INFO - validation batch 51, loss: 0.118, 1632/10016 datapoints
2025-03-06 18:19:14,717 - INFO - validation batch 101, loss: 0.225, 3232/10016 datapoints
2025-03-06 18:19:14,875 - INFO - validation batch 151, loss: 0.299, 4832/10016 datapoints
2025-03-06 18:19:15,029 - INFO - validation batch 201, loss: 0.381, 6432/10016 datapoints
2025-03-06 18:19:15,181 - INFO - validation batch 251, loss: 0.494, 8032/10016 datapoints
2025-03-06 18:19:15,333 - INFO - validation batch 301, loss: 0.166, 9632/10016 datapoints
2025-03-06 18:19:15,370 - INFO - Epoch 253/800 done.
2025-03-06 18:19:15,370 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:19:15,371 - INFO - Beginning epoch 254/800
2025-03-06 18:19:15,376 - INFO - training batch 1, loss: 0.311, 32/60000 datapoints
2025-03-06 18:19:15,571 - INFO - training batch 51, loss: 0.367, 1632/60000 datapoints
2025-03-06 18:19:15,768 - INFO - training batch 101, loss: 0.337, 3232/60000 datapoints
2025-03-06 18:19:15,961 - INFO - training batch 151, loss: 0.207, 4832/60000 datapoints
2025-03-06 18:19:16,158 - INFO - training batch 201, loss: 0.501, 6432/60000 datapoints
2025-03-06 18:19:16,355 - INFO - training batch 251, loss: 0.168, 8032/60000 datapoints
2025-03-06 18:19:16,550 - INFO - training batch 301, loss: 0.164, 9632/60000 datapoints
2025-03-06 18:19:16,767 - INFO - training batch 351, loss: 0.329, 11232/60000 datapoints
2025-03-06 18:19:16,966 - INFO - training batch 401, loss: 0.209, 12832/60000 datapoints
2025-03-06 18:19:17,160 - INFO - training batch 451, loss: 0.180, 14432/60000 datapoints
2025-03-06 18:19:17,355 - INFO - training batch 501, loss: 0.426, 16032/60000 datapoints
2025-03-06 18:19:17,550 - INFO - training batch 551, loss: 0.177, 17632/60000 datapoints
2025-03-06 18:19:17,745 - INFO - training batch 601, loss: 0.197, 19232/60000 datapoints
2025-03-06 18:19:17,942 - INFO - training batch 651, loss: 0.105, 20832/60000 datapoints
2025-03-06 18:19:18,135 - INFO - training batch 701, loss: 0.215, 22432/60000 datapoints
2025-03-06 18:19:18,331 - INFO - training batch 751, loss: 0.627, 24032/60000 datapoints
2025-03-06 18:19:18,526 - INFO - training batch 801, loss: 0.360, 25632/60000 datapoints
2025-03-06 18:19:18,723 - INFO - training batch 851, loss: 0.304, 27232/60000 datapoints
2025-03-06 18:19:18,918 - INFO - training batch 901, loss: 0.239, 28832/60000 datapoints
2025-03-06 18:19:19,113 - INFO - training batch 951, loss: 0.175, 30432/60000 datapoints
2025-03-06 18:19:19,311 - INFO - training batch 1001, loss: 0.339, 32032/60000 datapoints
2025-03-06 18:19:19,515 - INFO - training batch 1051, loss: 0.432, 33632/60000 datapoints
2025-03-06 18:19:19,712 - INFO - training batch 1101, loss: 0.438, 35232/60000 datapoints
2025-03-06 18:19:19,907 - INFO - training batch 1151, loss: 0.346, 36832/60000 datapoints
2025-03-06 18:19:20,106 - INFO - training batch 1201, loss: 0.521, 38432/60000 datapoints
2025-03-06 18:19:20,300 - INFO - training batch 1251, loss: 0.259, 40032/60000 datapoints
2025-03-06 18:19:20,500 - INFO - training batch 1301, loss: 0.195, 41632/60000 datapoints
2025-03-06 18:19:20,698 - INFO - training batch 1351, loss: 0.277, 43232/60000 datapoints
2025-03-06 18:19:20,896 - INFO - training batch 1401, loss: 0.119, 44832/60000 datapoints
2025-03-06 18:19:21,090 - INFO - training batch 1451, loss: 0.464, 46432/60000 datapoints
2025-03-06 18:19:21,284 - INFO - training batch 1501, loss: 0.456, 48032/60000 datapoints
2025-03-06 18:19:21,479 - INFO - training batch 1551, loss: 0.398, 49632/60000 datapoints
2025-03-06 18:19:21,677 - INFO - training batch 1601, loss: 0.485, 51232/60000 datapoints
2025-03-06 18:19:21,874 - INFO - training batch 1651, loss: 0.275, 52832/60000 datapoints
2025-03-06 18:19:22,068 - INFO - training batch 1701, loss: 0.392, 54432/60000 datapoints
2025-03-06 18:19:22,264 - INFO - training batch 1751, loss: 0.248, 56032/60000 datapoints
2025-03-06 18:19:22,459 - INFO - training batch 1801, loss: 0.292, 57632/60000 datapoints
2025-03-06 18:19:22,657 - INFO - training batch 1851, loss: 0.288, 59232/60000 datapoints
2025-03-06 18:19:22,758 - INFO - validation batch 1, loss: 0.294, 32/10016 datapoints
2025-03-06 18:19:22,916 - INFO - validation batch 51, loss: 0.190, 1632/10016 datapoints
2025-03-06 18:19:23,071 - INFO - validation batch 101, loss: 0.361, 3232/10016 datapoints
2025-03-06 18:19:23,225 - INFO - validation batch 151, loss: 0.235, 4832/10016 datapoints
2025-03-06 18:19:23,378 - INFO - validation batch 201, loss: 0.411, 6432/10016 datapoints
2025-03-06 18:19:23,529 - INFO - validation batch 251, loss: 0.296, 8032/10016 datapoints
2025-03-06 18:19:23,685 - INFO - validation batch 301, loss: 0.503, 9632/10016 datapoints
2025-03-06 18:19:23,722 - INFO - Epoch 254/800 done.
2025-03-06 18:19:23,722 - INFO - Final validation performance:
Loss: 0.327, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:19:23,722 - INFO - Beginning epoch 255/800
2025-03-06 18:19:23,728 - INFO - training batch 1, loss: 0.527, 32/60000 datapoints
2025-03-06 18:19:23,921 - INFO - training batch 51, loss: 0.146, 1632/60000 datapoints
2025-03-06 18:19:24,117 - INFO - training batch 101, loss: 0.333, 3232/60000 datapoints
2025-03-06 18:19:24,308 - INFO - training batch 151, loss: 0.139, 4832/60000 datapoints
2025-03-06 18:19:24,502 - INFO - training batch 201, loss: 0.231, 6432/60000 datapoints
2025-03-06 18:19:24,698 - INFO - training batch 251, loss: 0.148, 8032/60000 datapoints
2025-03-06 18:19:24,896 - INFO - training batch 301, loss: 0.180, 9632/60000 datapoints
2025-03-06 18:19:25,092 - INFO - training batch 351, loss: 0.314, 11232/60000 datapoints
2025-03-06 18:19:25,289 - INFO - training batch 401, loss: 0.157, 12832/60000 datapoints
2025-03-06 18:19:25,482 - INFO - training batch 451, loss: 0.544, 14432/60000 datapoints
2025-03-06 18:19:25,680 - INFO - training batch 501, loss: 0.533, 16032/60000 datapoints
2025-03-06 18:19:25,873 - INFO - training batch 551, loss: 0.380, 17632/60000 datapoints
2025-03-06 18:19:26,069 - INFO - training batch 601, loss: 0.394, 19232/60000 datapoints
2025-03-06 18:19:26,267 - INFO - training batch 651, loss: 0.383, 20832/60000 datapoints
2025-03-06 18:19:26,466 - INFO - training batch 701, loss: 0.193, 22432/60000 datapoints
2025-03-06 18:19:26,663 - INFO - training batch 751, loss: 0.535, 24032/60000 datapoints
2025-03-06 18:19:26,880 - INFO - training batch 801, loss: 0.447, 25632/60000 datapoints
2025-03-06 18:19:27,078 - INFO - training batch 851, loss: 0.242, 27232/60000 datapoints
2025-03-06 18:19:27,272 - INFO - training batch 901, loss: 0.405, 28832/60000 datapoints
2025-03-06 18:19:27,467 - INFO - training batch 951, loss: 0.446, 30432/60000 datapoints
2025-03-06 18:19:27,665 - INFO - training batch 1001, loss: 0.101, 32032/60000 datapoints
2025-03-06 18:19:27,864 - INFO - training batch 1051, loss: 0.178, 33632/60000 datapoints
2025-03-06 18:19:28,059 - INFO - training batch 1101, loss: 0.180, 35232/60000 datapoints
2025-03-06 18:19:28,254 - INFO - training batch 1151, loss: 0.376, 36832/60000 datapoints
2025-03-06 18:19:28,448 - INFO - training batch 1201, loss: 0.095, 38432/60000 datapoints
2025-03-06 18:19:28,647 - INFO - training batch 1251, loss: 0.168, 40032/60000 datapoints
2025-03-06 18:19:28,842 - INFO - training batch 1301, loss: 0.278, 41632/60000 datapoints
2025-03-06 18:19:29,038 - INFO - training batch 1351, loss: 0.346, 43232/60000 datapoints
2025-03-06 18:19:29,232 - INFO - training batch 1401, loss: 0.252, 44832/60000 datapoints
2025-03-06 18:19:29,427 - INFO - training batch 1451, loss: 0.247, 46432/60000 datapoints
2025-03-06 18:19:29,623 - INFO - training batch 1501, loss: 0.536, 48032/60000 datapoints
2025-03-06 18:19:29,817 - INFO - training batch 1551, loss: 0.294, 49632/60000 datapoints
2025-03-06 18:19:30,013 - INFO - training batch 1601, loss: 0.140, 51232/60000 datapoints
2025-03-06 18:19:30,211 - INFO - training batch 1651, loss: 0.213, 52832/60000 datapoints
2025-03-06 18:19:30,405 - INFO - training batch 1701, loss: 0.414, 54432/60000 datapoints
2025-03-06 18:19:30,601 - INFO - training batch 1751, loss: 0.377, 56032/60000 datapoints
2025-03-06 18:19:30,795 - INFO - training batch 1801, loss: 0.185, 57632/60000 datapoints
2025-03-06 18:19:30,990 - INFO - training batch 1851, loss: 0.315, 59232/60000 datapoints
2025-03-06 18:19:31,091 - INFO - validation batch 1, loss: 0.349, 32/10016 datapoints
2025-03-06 18:19:31,245 - INFO - validation batch 51, loss: 0.214, 1632/10016 datapoints
2025-03-06 18:19:31,398 - INFO - validation batch 101, loss: 0.238, 3232/10016 datapoints
2025-03-06 18:19:31,550 - INFO - validation batch 151, loss: 0.426, 4832/10016 datapoints
2025-03-06 18:19:31,723 - INFO - validation batch 201, loss: 0.303, 6432/10016 datapoints
2025-03-06 18:19:31,881 - INFO - validation batch 251, loss: 0.404, 8032/10016 datapoints
2025-03-06 18:19:32,035 - INFO - validation batch 301, loss: 0.433, 9632/10016 datapoints
2025-03-06 18:19:32,072 - INFO - Epoch 255/800 done.
2025-03-06 18:19:32,072 - INFO - Final validation performance:
Loss: 0.338, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:19:32,072 - INFO - Beginning epoch 256/800
2025-03-06 18:19:32,078 - INFO - training batch 1, loss: 0.498, 32/60000 datapoints
2025-03-06 18:19:32,271 - INFO - training batch 51, loss: 0.764, 1632/60000 datapoints
2025-03-06 18:19:32,463 - INFO - training batch 101, loss: 0.290, 3232/60000 datapoints
2025-03-06 18:19:32,660 - INFO - training batch 151, loss: 0.449, 4832/60000 datapoints
2025-03-06 18:19:32,856 - INFO - training batch 201, loss: 0.290, 6432/60000 datapoints
2025-03-06 18:19:33,055 - INFO - training batch 251, loss: 0.385, 8032/60000 datapoints
2025-03-06 18:19:33,252 - INFO - training batch 301, loss: 0.355, 9632/60000 datapoints
2025-03-06 18:19:33,444 - INFO - training batch 351, loss: 0.564, 11232/60000 datapoints
2025-03-06 18:19:33,640 - INFO - training batch 401, loss: 0.273, 12832/60000 datapoints
2025-03-06 18:19:33,836 - INFO - training batch 451, loss: 0.565, 14432/60000 datapoints
2025-03-06 18:19:34,031 - INFO - training batch 501, loss: 0.290, 16032/60000 datapoints
2025-03-06 18:19:34,223 - INFO - training batch 551, loss: 0.209, 17632/60000 datapoints
2025-03-06 18:19:34,417 - INFO - training batch 601, loss: 0.212, 19232/60000 datapoints
2025-03-06 18:19:34,614 - INFO - training batch 651, loss: 0.362, 20832/60000 datapoints
2025-03-06 18:19:34,809 - INFO - training batch 701, loss: 0.334, 22432/60000 datapoints
2025-03-06 18:19:35,011 - INFO - training batch 751, loss: 0.218, 24032/60000 datapoints
2025-03-06 18:19:35,206 - INFO - training batch 801, loss: 0.241, 25632/60000 datapoints
2025-03-06 18:19:35,401 - INFO - training batch 851, loss: 0.233, 27232/60000 datapoints
2025-03-06 18:19:35,598 - INFO - training batch 901, loss: 0.449, 28832/60000 datapoints
2025-03-06 18:19:35,793 - INFO - training batch 951, loss: 0.295, 30432/60000 datapoints
2025-03-06 18:19:35,987 - INFO - training batch 1001, loss: 0.107, 32032/60000 datapoints
2025-03-06 18:19:36,184 - INFO - training batch 1051, loss: 0.270, 33632/60000 datapoints
2025-03-06 18:19:36,379 - INFO - training batch 1101, loss: 0.277, 35232/60000 datapoints
2025-03-06 18:19:36,576 - INFO - training batch 1151, loss: 0.486, 36832/60000 datapoints
2025-03-06 18:19:36,771 - INFO - training batch 1201, loss: 0.243, 38432/60000 datapoints
2025-03-06 18:19:36,985 - INFO - training batch 1251, loss: 0.251, 40032/60000 datapoints
2025-03-06 18:19:37,179 - INFO - training batch 1301, loss: 0.270, 41632/60000 datapoints
2025-03-06 18:19:37,377 - INFO - training batch 1351, loss: 0.174, 43232/60000 datapoints
2025-03-06 18:19:37,570 - INFO - training batch 1401, loss: 0.169, 44832/60000 datapoints
2025-03-06 18:19:37,782 - INFO - training batch 1451, loss: 0.308, 46432/60000 datapoints
2025-03-06 18:19:37,983 - INFO - training batch 1501, loss: 0.361, 48032/60000 datapoints
2025-03-06 18:19:38,177 - INFO - training batch 1551, loss: 0.219, 49632/60000 datapoints
2025-03-06 18:19:38,372 - INFO - training batch 1601, loss: 0.203, 51232/60000 datapoints
2025-03-06 18:19:38,567 - INFO - training batch 1651, loss: 0.248, 52832/60000 datapoints
2025-03-06 18:19:38,765 - INFO - training batch 1701, loss: 0.130, 54432/60000 datapoints
2025-03-06 18:19:38,967 - INFO - training batch 1751, loss: 0.205, 56032/60000 datapoints
2025-03-06 18:19:39,188 - INFO - training batch 1801, loss: 0.652, 57632/60000 datapoints
2025-03-06 18:19:39,382 - INFO - training batch 1851, loss: 0.329, 59232/60000 datapoints
2025-03-06 18:19:39,486 - INFO - validation batch 1, loss: 0.240, 32/10016 datapoints
2025-03-06 18:19:39,643 - INFO - validation batch 51, loss: 0.361, 1632/10016 datapoints
2025-03-06 18:19:39,796 - INFO - validation batch 101, loss: 0.250, 3232/10016 datapoints
2025-03-06 18:19:39,948 - INFO - validation batch 151, loss: 0.438, 4832/10016 datapoints
2025-03-06 18:19:40,101 - INFO - validation batch 201, loss: 0.289, 6432/10016 datapoints
2025-03-06 18:19:40,255 - INFO - validation batch 251, loss: 0.168, 8032/10016 datapoints
2025-03-06 18:19:40,410 - INFO - validation batch 301, loss: 0.269, 9632/10016 datapoints
2025-03-06 18:19:40,447 - INFO - Epoch 256/800 done.
2025-03-06 18:19:40,448 - INFO - Final validation performance:
Loss: 0.288, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:19:40,448 - INFO - Beginning epoch 257/800
2025-03-06 18:19:40,454 - INFO - training batch 1, loss: 0.427, 32/60000 datapoints
2025-03-06 18:19:40,654 - INFO - training batch 51, loss: 0.391, 1632/60000 datapoints
2025-03-06 18:19:40,851 - INFO - training batch 101, loss: 0.129, 3232/60000 datapoints
2025-03-06 18:19:41,054 - INFO - training batch 151, loss: 0.266, 4832/60000 datapoints
2025-03-06 18:19:41,247 - INFO - training batch 201, loss: 0.336, 6432/60000 datapoints
2025-03-06 18:19:41,442 - INFO - training batch 251, loss: 0.261, 8032/60000 datapoints
2025-03-06 18:19:41,646 - INFO - training batch 301, loss: 0.299, 9632/60000 datapoints
2025-03-06 18:19:41,846 - INFO - training batch 351, loss: 0.230, 11232/60000 datapoints
2025-03-06 18:19:42,043 - INFO - training batch 401, loss: 0.353, 12832/60000 datapoints
2025-03-06 18:19:42,237 - INFO - training batch 451, loss: 0.586, 14432/60000 datapoints
2025-03-06 18:19:42,431 - INFO - training batch 501, loss: 0.369, 16032/60000 datapoints
2025-03-06 18:19:42,626 - INFO - training batch 551, loss: 0.228, 17632/60000 datapoints
2025-03-06 18:19:42,821 - INFO - training batch 601, loss: 0.236, 19232/60000 datapoints
2025-03-06 18:19:43,019 - INFO - training batch 651, loss: 0.128, 20832/60000 datapoints
2025-03-06 18:19:43,213 - INFO - training batch 701, loss: 0.153, 22432/60000 datapoints
2025-03-06 18:19:43,421 - INFO - training batch 751, loss: 0.291, 24032/60000 datapoints
2025-03-06 18:19:43,618 - INFO - training batch 801, loss: 0.226, 25632/60000 datapoints
2025-03-06 18:19:43,814 - INFO - training batch 851, loss: 0.195, 27232/60000 datapoints
2025-03-06 18:19:44,010 - INFO - training batch 901, loss: 0.135, 28832/60000 datapoints
2025-03-06 18:19:44,204 - INFO - training batch 951, loss: 0.450, 30432/60000 datapoints
2025-03-06 18:19:44,400 - INFO - training batch 1001, loss: 0.410, 32032/60000 datapoints
2025-03-06 18:19:44,594 - INFO - training batch 1051, loss: 0.326, 33632/60000 datapoints
2025-03-06 18:19:44,791 - INFO - training batch 1101, loss: 0.257, 35232/60000 datapoints
2025-03-06 18:19:44,996 - INFO - training batch 1151, loss: 0.321, 36832/60000 datapoints
2025-03-06 18:19:45,191 - INFO - training batch 1201, loss: 0.535, 38432/60000 datapoints
2025-03-06 18:19:45,386 - INFO - training batch 1251, loss: 0.412, 40032/60000 datapoints
2025-03-06 18:19:45,579 - INFO - training batch 1301, loss: 0.218, 41632/60000 datapoints
2025-03-06 18:19:45,776 - INFO - training batch 1351, loss: 0.360, 43232/60000 datapoints
2025-03-06 18:19:45,974 - INFO - training batch 1401, loss: 0.359, 44832/60000 datapoints
2025-03-06 18:19:46,170 - INFO - training batch 1451, loss: 0.346, 46432/60000 datapoints
2025-03-06 18:19:46,366 - INFO - training batch 1501, loss: 0.328, 48032/60000 datapoints
2025-03-06 18:19:46,560 - INFO - training batch 1551, loss: 0.309, 49632/60000 datapoints
2025-03-06 18:19:46,755 - INFO - training batch 1601, loss: 0.350, 51232/60000 datapoints
2025-03-06 18:19:46,967 - INFO - training batch 1651, loss: 0.251, 52832/60000 datapoints
2025-03-06 18:19:47,164 - INFO - training batch 1701, loss: 0.343, 54432/60000 datapoints
2025-03-06 18:19:47,357 - INFO - training batch 1751, loss: 0.374, 56032/60000 datapoints
2025-03-06 18:19:47,551 - INFO - training batch 1801, loss: 0.225, 57632/60000 datapoints
2025-03-06 18:19:47,747 - INFO - training batch 1851, loss: 0.298, 59232/60000 datapoints
2025-03-06 18:19:47,848 - INFO - validation batch 1, loss: 0.132, 32/10016 datapoints
2025-03-06 18:19:48,001 - INFO - validation batch 51, loss: 0.365, 1632/10016 datapoints
2025-03-06 18:19:48,152 - INFO - validation batch 101, loss: 0.199, 3232/10016 datapoints
2025-03-06 18:19:48,304 - INFO - validation batch 151, loss: 0.273, 4832/10016 datapoints
2025-03-06 18:19:48,457 - INFO - validation batch 201, loss: 0.119, 6432/10016 datapoints
2025-03-06 18:19:48,610 - INFO - validation batch 251, loss: 0.282, 8032/10016 datapoints
2025-03-06 18:19:48,762 - INFO - validation batch 301, loss: 0.183, 9632/10016 datapoints
2025-03-06 18:19:48,798 - INFO - Epoch 257/800 done.
2025-03-06 18:19:48,799 - INFO - Final validation performance:
Loss: 0.222, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:19:48,799 - INFO - Beginning epoch 258/800
2025-03-06 18:19:48,806 - INFO - training batch 1, loss: 0.210, 32/60000 datapoints
2025-03-06 18:19:49,011 - INFO - training batch 51, loss: 0.448, 1632/60000 datapoints
2025-03-06 18:19:49,204 - INFO - training batch 101, loss: 0.213, 3232/60000 datapoints
2025-03-06 18:19:49,399 - INFO - training batch 151, loss: 0.461, 4832/60000 datapoints
2025-03-06 18:19:49,594 - INFO - training batch 201, loss: 0.218, 6432/60000 datapoints
2025-03-06 18:19:49,791 - INFO - training batch 251, loss: 0.124, 8032/60000 datapoints
2025-03-06 18:19:49,987 - INFO - training batch 301, loss: 0.273, 9632/60000 datapoints
2025-03-06 18:19:50,186 - INFO - training batch 351, loss: 0.240, 11232/60000 datapoints
2025-03-06 18:19:50,381 - INFO - training batch 401, loss: 0.104, 12832/60000 datapoints
2025-03-06 18:19:50,576 - INFO - training batch 451, loss: 0.158, 14432/60000 datapoints
2025-03-06 18:19:50,773 - INFO - training batch 501, loss: 0.194, 16032/60000 datapoints
2025-03-06 18:19:50,968 - INFO - training batch 551, loss: 0.365, 17632/60000 datapoints
2025-03-06 18:19:51,165 - INFO - training batch 601, loss: 0.189, 19232/60000 datapoints
2025-03-06 18:19:51,355 - INFO - training batch 651, loss: 0.235, 20832/60000 datapoints
2025-03-06 18:19:51,549 - INFO - training batch 701, loss: 0.305, 22432/60000 datapoints
2025-03-06 18:19:51,744 - INFO - training batch 751, loss: 0.289, 24032/60000 datapoints
2025-03-06 18:19:51,940 - INFO - training batch 801, loss: 0.341, 25632/60000 datapoints
2025-03-06 18:19:52,135 - INFO - training batch 851, loss: 0.243, 27232/60000 datapoints
2025-03-06 18:19:52,328 - INFO - training batch 901, loss: 0.351, 28832/60000 datapoints
2025-03-06 18:19:52,522 - INFO - training batch 951, loss: 0.513, 30432/60000 datapoints
2025-03-06 18:19:52,722 - INFO - training batch 1001, loss: 0.333, 32032/60000 datapoints
2025-03-06 18:19:52,918 - INFO - training batch 1051, loss: 0.188, 33632/60000 datapoints
2025-03-06 18:19:53,114 - INFO - training batch 1101, loss: 0.548, 35232/60000 datapoints
2025-03-06 18:19:53,310 - INFO - training batch 1151, loss: 0.257, 36832/60000 datapoints
2025-03-06 18:19:53,505 - INFO - training batch 1201, loss: 0.374, 38432/60000 datapoints
2025-03-06 18:19:53,700 - INFO - training batch 1251, loss: 0.242, 40032/60000 datapoints
2025-03-06 18:19:53,894 - INFO - training batch 1301, loss: 0.571, 41632/60000 datapoints
2025-03-06 18:19:54,090 - INFO - training batch 1351, loss: 0.389, 43232/60000 datapoints
2025-03-06 18:19:54,285 - INFO - training batch 1401, loss: 0.349, 44832/60000 datapoints
2025-03-06 18:19:54,480 - INFO - training batch 1451, loss: 0.389, 46432/60000 datapoints
2025-03-06 18:19:54,676 - INFO - training batch 1501, loss: 0.343, 48032/60000 datapoints
2025-03-06 18:19:54,876 - INFO - training batch 1551, loss: 0.280, 49632/60000 datapoints
2025-03-06 18:19:55,075 - INFO - training batch 1601, loss: 0.256, 51232/60000 datapoints
2025-03-06 18:19:55,269 - INFO - training batch 1651, loss: 0.280, 52832/60000 datapoints
2025-03-06 18:19:55,465 - INFO - training batch 1701, loss: 0.448, 54432/60000 datapoints
2025-03-06 18:19:55,661 - INFO - training batch 1751, loss: 0.273, 56032/60000 datapoints
2025-03-06 18:19:55,856 - INFO - training batch 1801, loss: 0.356, 57632/60000 datapoints
2025-03-06 18:19:56,052 - INFO - training batch 1851, loss: 0.272, 59232/60000 datapoints
2025-03-06 18:19:56,153 - INFO - validation batch 1, loss: 0.453, 32/10016 datapoints
2025-03-06 18:19:56,307 - INFO - validation batch 51, loss: 0.184, 1632/10016 datapoints
2025-03-06 18:19:56,460 - INFO - validation batch 101, loss: 0.485, 3232/10016 datapoints
2025-03-06 18:19:56,614 - INFO - validation batch 151, loss: 0.215, 4832/10016 datapoints
2025-03-06 18:19:56,767 - INFO - validation batch 201, loss: 0.139, 6432/10016 datapoints
2025-03-06 18:19:56,925 - INFO - validation batch 251, loss: 0.549, 8032/10016 datapoints
2025-03-06 18:19:57,097 - INFO - validation batch 301, loss: 0.205, 9632/10016 datapoints
2025-03-06 18:19:57,135 - INFO - Epoch 258/800 done.
2025-03-06 18:19:57,135 - INFO - Final validation performance:
Loss: 0.319, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:19:57,136 - INFO - Beginning epoch 259/800
2025-03-06 18:19:57,141 - INFO - training batch 1, loss: 0.369, 32/60000 datapoints
2025-03-06 18:19:57,335 - INFO - training batch 51, loss: 0.421, 1632/60000 datapoints
2025-03-06 18:19:57,528 - INFO - training batch 101, loss: 0.455, 3232/60000 datapoints
2025-03-06 18:19:57,725 - INFO - training batch 151, loss: 0.248, 4832/60000 datapoints
2025-03-06 18:19:57,918 - INFO - training batch 201, loss: 0.277, 6432/60000 datapoints
2025-03-06 18:19:58,115 - INFO - training batch 251, loss: 0.438, 8032/60000 datapoints
2025-03-06 18:19:58,310 - INFO - training batch 301, loss: 0.150, 9632/60000 datapoints
2025-03-06 18:19:58,505 - INFO - training batch 351, loss: 0.370, 11232/60000 datapoints
2025-03-06 18:19:58,704 - INFO - training batch 401, loss: 0.512, 12832/60000 datapoints
2025-03-06 18:19:58,898 - INFO - training batch 451, loss: 0.446, 14432/60000 datapoints
2025-03-06 18:19:59,097 - INFO - training batch 501, loss: 0.447, 16032/60000 datapoints
2025-03-06 18:19:59,291 - INFO - training batch 551, loss: 0.273, 17632/60000 datapoints
2025-03-06 18:19:59,487 - INFO - training batch 601, loss: 0.319, 19232/60000 datapoints
2025-03-06 18:19:59,684 - INFO - training batch 651, loss: 0.596, 20832/60000 datapoints
2025-03-06 18:19:59,877 - INFO - training batch 701, loss: 0.196, 22432/60000 datapoints
2025-03-06 18:20:00,074 - INFO - training batch 751, loss: 0.495, 24032/60000 datapoints
2025-03-06 18:20:00,277 - INFO - training batch 801, loss: 0.262, 25632/60000 datapoints
2025-03-06 18:20:00,472 - INFO - training batch 851, loss: 0.357, 27232/60000 datapoints
2025-03-06 18:20:00,668 - INFO - training batch 901, loss: 0.330, 28832/60000 datapoints
2025-03-06 18:20:00,860 - INFO - training batch 951, loss: 0.293, 30432/60000 datapoints
2025-03-06 18:20:01,057 - INFO - training batch 1001, loss: 0.324, 32032/60000 datapoints
2025-03-06 18:20:01,251 - INFO - training batch 1051, loss: 0.150, 33632/60000 datapoints
2025-03-06 18:20:01,444 - INFO - training batch 1101, loss: 0.649, 35232/60000 datapoints
2025-03-06 18:20:01,643 - INFO - training batch 1151, loss: 0.202, 36832/60000 datapoints
2025-03-06 18:20:01,836 - INFO - training batch 1201, loss: 0.482, 38432/60000 datapoints
2025-03-06 18:20:02,029 - INFO - training batch 1251, loss: 0.349, 40032/60000 datapoints
2025-03-06 18:20:02,224 - INFO - training batch 1301, loss: 0.285, 41632/60000 datapoints
2025-03-06 18:20:02,419 - INFO - training batch 1351, loss: 0.158, 43232/60000 datapoints
2025-03-06 18:20:02,616 - INFO - training batch 1401, loss: 0.589, 44832/60000 datapoints
2025-03-06 18:20:02,811 - INFO - training batch 1451, loss: 0.143, 46432/60000 datapoints
2025-03-06 18:20:03,009 - INFO - training batch 1501, loss: 0.323, 48032/60000 datapoints
2025-03-06 18:20:03,208 - INFO - training batch 1551, loss: 0.219, 49632/60000 datapoints
2025-03-06 18:20:03,402 - INFO - training batch 1601, loss: 0.312, 51232/60000 datapoints
2025-03-06 18:20:03,601 - INFO - training batch 1651, loss: 0.422, 52832/60000 datapoints
2025-03-06 18:20:03,796 - INFO - training batch 1701, loss: 0.313, 54432/60000 datapoints
2025-03-06 18:20:03,990 - INFO - training batch 1751, loss: 0.854, 56032/60000 datapoints
2025-03-06 18:20:04,187 - INFO - training batch 1801, loss: 0.293, 57632/60000 datapoints
2025-03-06 18:20:04,381 - INFO - training batch 1851, loss: 0.293, 59232/60000 datapoints
2025-03-06 18:20:04,483 - INFO - validation batch 1, loss: 0.548, 32/10016 datapoints
2025-03-06 18:20:04,639 - INFO - validation batch 51, loss: 0.439, 1632/10016 datapoints
2025-03-06 18:20:04,790 - INFO - validation batch 101, loss: 0.289, 3232/10016 datapoints
2025-03-06 18:20:04,947 - INFO - validation batch 151, loss: 0.199, 4832/10016 datapoints
2025-03-06 18:20:05,103 - INFO - validation batch 201, loss: 0.306, 6432/10016 datapoints
2025-03-06 18:20:05,257 - INFO - validation batch 251, loss: 0.171, 8032/10016 datapoints
2025-03-06 18:20:05,411 - INFO - validation batch 301, loss: 0.224, 9632/10016 datapoints
2025-03-06 18:20:05,448 - INFO - Epoch 259/800 done.
2025-03-06 18:20:05,448 - INFO - Final validation performance:
Loss: 0.311, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:20:05,449 - INFO - Beginning epoch 260/800
2025-03-06 18:20:05,455 - INFO - training batch 1, loss: 0.538, 32/60000 datapoints
2025-03-06 18:20:05,664 - INFO - training batch 51, loss: 0.225, 1632/60000 datapoints
2025-03-06 18:20:05,859 - INFO - training batch 101, loss: 0.355, 3232/60000 datapoints
2025-03-06 18:20:06,056 - INFO - training batch 151, loss: 0.296, 4832/60000 datapoints
2025-03-06 18:20:06,253 - INFO - training batch 201, loss: 0.389, 6432/60000 datapoints
2025-03-06 18:20:06,448 - INFO - training batch 251, loss: 0.362, 8032/60000 datapoints
2025-03-06 18:20:06,646 - INFO - training batch 301, loss: 0.580, 9632/60000 datapoints
2025-03-06 18:20:06,842 - INFO - training batch 351, loss: 0.258, 11232/60000 datapoints
2025-03-06 18:20:07,039 - INFO - training batch 401, loss: 0.218, 12832/60000 datapoints
2025-03-06 18:20:07,258 - INFO - training batch 451, loss: 0.162, 14432/60000 datapoints
2025-03-06 18:20:07,450 - INFO - training batch 501, loss: 0.270, 16032/60000 datapoints
2025-03-06 18:20:07,650 - INFO - training batch 551, loss: 0.460, 17632/60000 datapoints
2025-03-06 18:20:07,847 - INFO - training batch 601, loss: 0.366, 19232/60000 datapoints
2025-03-06 18:20:08,048 - INFO - training batch 651, loss: 0.183, 20832/60000 datapoints
2025-03-06 18:20:08,249 - INFO - training batch 701, loss: 0.736, 22432/60000 datapoints
2025-03-06 18:20:08,444 - INFO - training batch 751, loss: 0.487, 24032/60000 datapoints
2025-03-06 18:20:08,642 - INFO - training batch 801, loss: 0.240, 25632/60000 datapoints
2025-03-06 18:20:08,840 - INFO - training batch 851, loss: 0.175, 27232/60000 datapoints
2025-03-06 18:20:09,036 - INFO - training batch 901, loss: 0.098, 28832/60000 datapoints
2025-03-06 18:20:09,235 - INFO - training batch 951, loss: 0.235, 30432/60000 datapoints
2025-03-06 18:20:09,428 - INFO - training batch 1001, loss: 0.316, 32032/60000 datapoints
2025-03-06 18:20:09,626 - INFO - training batch 1051, loss: 0.366, 33632/60000 datapoints
2025-03-06 18:20:09,823 - INFO - training batch 1101, loss: 0.357, 35232/60000 datapoints
2025-03-06 18:20:10,019 - INFO - training batch 1151, loss: 0.289, 36832/60000 datapoints
2025-03-06 18:20:10,219 - INFO - training batch 1201, loss: 0.328, 38432/60000 datapoints
2025-03-06 18:20:10,415 - INFO - training batch 1251, loss: 0.399, 40032/60000 datapoints
2025-03-06 18:20:10,613 - INFO - training batch 1301, loss: 0.440, 41632/60000 datapoints
2025-03-06 18:20:10,808 - INFO - training batch 1351, loss: 0.241, 43232/60000 datapoints
2025-03-06 18:20:11,002 - INFO - training batch 1401, loss: 0.280, 44832/60000 datapoints
2025-03-06 18:20:11,199 - INFO - training batch 1451, loss: 0.269, 46432/60000 datapoints
2025-03-06 18:20:11,393 - INFO - training batch 1501, loss: 0.125, 48032/60000 datapoints
2025-03-06 18:20:11,585 - INFO - training batch 1551, loss: 0.269, 49632/60000 datapoints
2025-03-06 18:20:11,783 - INFO - training batch 1601, loss: 0.358, 51232/60000 datapoints
2025-03-06 18:20:11,975 - INFO - training batch 1651, loss: 0.585, 52832/60000 datapoints
2025-03-06 18:20:12,175 - INFO - training batch 1701, loss: 0.225, 54432/60000 datapoints
2025-03-06 18:20:12,370 - INFO - training batch 1751, loss: 0.640, 56032/60000 datapoints
2025-03-06 18:20:12,563 - INFO - training batch 1801, loss: 0.334, 57632/60000 datapoints
2025-03-06 18:20:12,760 - INFO - training batch 1851, loss: 0.438, 59232/60000 datapoints
2025-03-06 18:20:12,862 - INFO - validation batch 1, loss: 0.238, 32/10016 datapoints
2025-03-06 18:20:13,015 - INFO - validation batch 51, loss: 0.313, 1632/10016 datapoints
2025-03-06 18:20:13,171 - INFO - validation batch 101, loss: 0.241, 3232/10016 datapoints
2025-03-06 18:20:13,325 - INFO - validation batch 151, loss: 0.323, 4832/10016 datapoints
2025-03-06 18:20:13,478 - INFO - validation batch 201, loss: 0.176, 6432/10016 datapoints
2025-03-06 18:20:13,634 - INFO - validation batch 251, loss: 0.330, 8032/10016 datapoints
2025-03-06 18:20:13,787 - INFO - validation batch 301, loss: 0.364, 9632/10016 datapoints
2025-03-06 18:20:13,825 - INFO - Epoch 260/800 done.
2025-03-06 18:20:13,825 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:20:13,825 - INFO - Beginning epoch 261/800
2025-03-06 18:20:13,831 - INFO - training batch 1, loss: 0.496, 32/60000 datapoints
2025-03-06 18:20:14,024 - INFO - training batch 51, loss: 0.297, 1632/60000 datapoints
2025-03-06 18:20:14,221 - INFO - training batch 101, loss: 0.448, 3232/60000 datapoints
2025-03-06 18:20:14,416 - INFO - training batch 151, loss: 0.315, 4832/60000 datapoints
2025-03-06 18:20:14,613 - INFO - training batch 201, loss: 0.398, 6432/60000 datapoints
2025-03-06 18:20:14,810 - INFO - training batch 251, loss: 0.212, 8032/60000 datapoints
2025-03-06 18:20:15,008 - INFO - training batch 301, loss: 0.215, 9632/60000 datapoints
2025-03-06 18:20:15,206 - INFO - training batch 351, loss: 0.175, 11232/60000 datapoints
2025-03-06 18:20:15,401 - INFO - training batch 401, loss: 0.294, 12832/60000 datapoints
2025-03-06 18:20:15,597 - INFO - training batch 451, loss: 0.455, 14432/60000 datapoints
2025-03-06 18:20:15,799 - INFO - training batch 501, loss: 0.371, 16032/60000 datapoints
2025-03-06 18:20:15,993 - INFO - training batch 551, loss: 0.141, 17632/60000 datapoints
2025-03-06 18:20:16,192 - INFO - training batch 601, loss: 0.337, 19232/60000 datapoints
2025-03-06 18:20:16,386 - INFO - training batch 651, loss: 0.397, 20832/60000 datapoints
2025-03-06 18:20:16,579 - INFO - training batch 701, loss: 0.175, 22432/60000 datapoints
2025-03-06 18:20:16,776 - INFO - training batch 751, loss: 0.412, 24032/60000 datapoints
2025-03-06 18:20:16,968 - INFO - training batch 801, loss: 0.242, 25632/60000 datapoints
2025-03-06 18:20:17,171 - INFO - training batch 851, loss: 0.432, 27232/60000 datapoints
2025-03-06 18:20:17,378 - INFO - training batch 901, loss: 0.424, 28832/60000 datapoints
2025-03-06 18:20:17,572 - INFO - training batch 951, loss: 0.270, 30432/60000 datapoints
2025-03-06 18:20:17,768 - INFO - training batch 1001, loss: 0.376, 32032/60000 datapoints
2025-03-06 18:20:17,963 - INFO - training batch 1051, loss: 0.266, 33632/60000 datapoints
2025-03-06 18:20:18,159 - INFO - training batch 1101, loss: 0.230, 35232/60000 datapoints
2025-03-06 18:20:18,354 - INFO - training batch 1151, loss: 0.137, 36832/60000 datapoints
2025-03-06 18:20:18,549 - INFO - training batch 1201, loss: 0.236, 38432/60000 datapoints
2025-03-06 18:20:18,745 - INFO - training batch 1251, loss: 0.226, 40032/60000 datapoints
2025-03-06 18:20:18,937 - INFO - training batch 1301, loss: 0.106, 41632/60000 datapoints
2025-03-06 18:20:19,133 - INFO - training batch 1351, loss: 0.227, 43232/60000 datapoints
2025-03-06 18:20:19,327 - INFO - training batch 1401, loss: 0.564, 44832/60000 datapoints
2025-03-06 18:20:19,519 - INFO - training batch 1451, loss: 0.476, 46432/60000 datapoints
2025-03-06 18:20:19,717 - INFO - training batch 1501, loss: 0.428, 48032/60000 datapoints
2025-03-06 18:20:19,910 - INFO - training batch 1551, loss: 0.866, 49632/60000 datapoints
2025-03-06 18:20:20,103 - INFO - training batch 1601, loss: 0.252, 51232/60000 datapoints
2025-03-06 18:20:20,301 - INFO - training batch 1651, loss: 0.412, 52832/60000 datapoints
2025-03-06 18:20:20,496 - INFO - training batch 1701, loss: 0.177, 54432/60000 datapoints
2025-03-06 18:20:20,691 - INFO - training batch 1751, loss: 0.381, 56032/60000 datapoints
2025-03-06 18:20:20,885 - INFO - training batch 1801, loss: 0.564, 57632/60000 datapoints
2025-03-06 18:20:21,078 - INFO - training batch 1851, loss: 0.175, 59232/60000 datapoints
2025-03-06 18:20:21,181 - INFO - validation batch 1, loss: 0.219, 32/10016 datapoints
2025-03-06 18:20:21,334 - INFO - validation batch 51, loss: 0.209, 1632/10016 datapoints
2025-03-06 18:20:21,486 - INFO - validation batch 101, loss: 0.503, 3232/10016 datapoints
2025-03-06 18:20:21,644 - INFO - validation batch 151, loss: 0.428, 4832/10016 datapoints
2025-03-06 18:20:21,798 - INFO - validation batch 201, loss: 0.410, 6432/10016 datapoints
2025-03-06 18:20:21,950 - INFO - validation batch 251, loss: 0.271, 8032/10016 datapoints
2025-03-06 18:20:22,102 - INFO - validation batch 301, loss: 0.462, 9632/10016 datapoints
2025-03-06 18:20:22,139 - INFO - Epoch 261/800 done.
2025-03-06 18:20:22,140 - INFO - Final validation performance:
Loss: 0.357, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:20:22,140 - INFO - Beginning epoch 262/800
2025-03-06 18:20:22,146 - INFO - training batch 1, loss: 0.436, 32/60000 datapoints
2025-03-06 18:20:22,344 - INFO - training batch 51, loss: 0.284, 1632/60000 datapoints
2025-03-06 18:20:22,540 - INFO - training batch 101, loss: 0.496, 3232/60000 datapoints
2025-03-06 18:20:22,738 - INFO - training batch 151, loss: 0.345, 4832/60000 datapoints
2025-03-06 18:20:22,931 - INFO - training batch 201, loss: 0.380, 6432/60000 datapoints
2025-03-06 18:20:23,126 - INFO - training batch 251, loss: 0.457, 8032/60000 datapoints
2025-03-06 18:20:23,322 - INFO - training batch 301, loss: 0.461, 9632/60000 datapoints
2025-03-06 18:20:23,516 - INFO - training batch 351, loss: 0.205, 11232/60000 datapoints
2025-03-06 18:20:23,713 - INFO - training batch 401, loss: 0.180, 12832/60000 datapoints
2025-03-06 18:20:23,909 - INFO - training batch 451, loss: 0.658, 14432/60000 datapoints
2025-03-06 18:20:24,103 - INFO - training batch 501, loss: 0.235, 16032/60000 datapoints
2025-03-06 18:20:24,301 - INFO - training batch 551, loss: 0.350, 17632/60000 datapoints
2025-03-06 18:20:24,496 - INFO - training batch 601, loss: 0.357, 19232/60000 datapoints
2025-03-06 18:20:24,695 - INFO - training batch 651, loss: 0.164, 20832/60000 datapoints
2025-03-06 18:20:24,893 - INFO - training batch 701, loss: 0.328, 22432/60000 datapoints
2025-03-06 18:20:25,089 - INFO - training batch 751, loss: 0.359, 24032/60000 datapoints
2025-03-06 18:20:25,286 - INFO - training batch 801, loss: 0.419, 25632/60000 datapoints
2025-03-06 18:20:25,481 - INFO - training batch 851, loss: 0.196, 27232/60000 datapoints
2025-03-06 18:20:25,678 - INFO - training batch 901, loss: 0.273, 28832/60000 datapoints
2025-03-06 18:20:25,872 - INFO - training batch 951, loss: 0.291, 30432/60000 datapoints
2025-03-06 18:20:26,064 - INFO - training batch 1001, loss: 0.572, 32032/60000 datapoints
2025-03-06 18:20:26,261 - INFO - training batch 1051, loss: 0.238, 33632/60000 datapoints
2025-03-06 18:20:26,456 - INFO - training batch 1101, loss: 0.151, 35232/60000 datapoints
2025-03-06 18:20:26,651 - INFO - training batch 1151, loss: 0.220, 36832/60000 datapoints
2025-03-06 18:20:26,845 - INFO - training batch 1201, loss: 0.396, 38432/60000 datapoints
2025-03-06 18:20:27,038 - INFO - training batch 1251, loss: 0.343, 40032/60000 datapoints
2025-03-06 18:20:27,238 - INFO - training batch 1301, loss: 0.358, 41632/60000 datapoints
2025-03-06 18:20:27,453 - INFO - training batch 1351, loss: 0.135, 43232/60000 datapoints
2025-03-06 18:20:27,647 - INFO - training batch 1401, loss: 0.212, 44832/60000 datapoints
2025-03-06 18:20:27,843 - INFO - training batch 1451, loss: 0.283, 46432/60000 datapoints
2025-03-06 18:20:28,035 - INFO - training batch 1501, loss: 0.364, 48032/60000 datapoints
2025-03-06 18:20:28,230 - INFO - training batch 1551, loss: 0.397, 49632/60000 datapoints
2025-03-06 18:20:28,426 - INFO - training batch 1601, loss: 0.363, 51232/60000 datapoints
2025-03-06 18:20:28,622 - INFO - training batch 1651, loss: 0.369, 52832/60000 datapoints
2025-03-06 18:20:28,815 - INFO - training batch 1701, loss: 0.384, 54432/60000 datapoints
2025-03-06 18:20:29,007 - INFO - training batch 1751, loss: 0.295, 56032/60000 datapoints
2025-03-06 18:20:29,202 - INFO - training batch 1801, loss: 0.307, 57632/60000 datapoints
2025-03-06 18:20:29,397 - INFO - training batch 1851, loss: 0.415, 59232/60000 datapoints
2025-03-06 18:20:29,498 - INFO - validation batch 1, loss: 0.118, 32/10016 datapoints
2025-03-06 18:20:29,653 - INFO - validation batch 51, loss: 0.167, 1632/10016 datapoints
2025-03-06 18:20:29,807 - INFO - validation batch 101, loss: 0.341, 3232/10016 datapoints
2025-03-06 18:20:29,961 - INFO - validation batch 151, loss: 0.279, 4832/10016 datapoints
2025-03-06 18:20:30,113 - INFO - validation batch 201, loss: 0.229, 6432/10016 datapoints
2025-03-06 18:20:30,271 - INFO - validation batch 251, loss: 0.155, 8032/10016 datapoints
2025-03-06 18:20:30,427 - INFO - validation batch 301, loss: 0.404, 9632/10016 datapoints
2025-03-06 18:20:30,463 - INFO - Epoch 262/800 done.
2025-03-06 18:20:30,463 - INFO - Final validation performance:
Loss: 0.242, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:20:30,463 - INFO - Beginning epoch 263/800
2025-03-06 18:20:30,470 - INFO - training batch 1, loss: 0.337, 32/60000 datapoints
2025-03-06 18:20:30,663 - INFO - training batch 51, loss: 0.403, 1632/60000 datapoints
2025-03-06 18:20:30,856 - INFO - training batch 101, loss: 0.231, 3232/60000 datapoints
2025-03-06 18:20:31,048 - INFO - training batch 151, loss: 0.447, 4832/60000 datapoints
2025-03-06 18:20:31,242 - INFO - training batch 201, loss: 0.157, 6432/60000 datapoints
2025-03-06 18:20:31,438 - INFO - training batch 251, loss: 0.227, 8032/60000 datapoints
2025-03-06 18:20:31,631 - INFO - training batch 301, loss: 0.178, 9632/60000 datapoints
2025-03-06 18:20:31,823 - INFO - training batch 351, loss: 0.478, 11232/60000 datapoints
2025-03-06 18:20:32,015 - INFO - training batch 401, loss: 0.292, 12832/60000 datapoints
2025-03-06 18:20:32,205 - INFO - training batch 451, loss: 0.424, 14432/60000 datapoints
2025-03-06 18:20:32,398 - INFO - training batch 501, loss: 0.333, 16032/60000 datapoints
2025-03-06 18:20:32,589 - INFO - training batch 551, loss: 0.323, 17632/60000 datapoints
2025-03-06 18:20:32,783 - INFO - training batch 601, loss: 0.386, 19232/60000 datapoints
2025-03-06 18:20:32,975 - INFO - training batch 651, loss: 0.400, 20832/60000 datapoints
2025-03-06 18:20:33,169 - INFO - training batch 701, loss: 0.218, 22432/60000 datapoints
2025-03-06 18:20:33,360 - INFO - training batch 751, loss: 0.224, 24032/60000 datapoints
2025-03-06 18:20:33,552 - INFO - training batch 801, loss: 0.329, 25632/60000 datapoints
2025-03-06 18:20:33,745 - INFO - training batch 851, loss: 0.114, 27232/60000 datapoints
2025-03-06 18:20:33,944 - INFO - training batch 901, loss: 0.511, 28832/60000 datapoints
2025-03-06 18:20:34,136 - INFO - training batch 951, loss: 0.290, 30432/60000 datapoints
2025-03-06 18:20:34,327 - INFO - training batch 1001, loss: 0.392, 32032/60000 datapoints
2025-03-06 18:20:34,521 - INFO - training batch 1051, loss: 0.442, 33632/60000 datapoints
2025-03-06 18:20:34,715 - INFO - training batch 1101, loss: 0.203, 35232/60000 datapoints
2025-03-06 18:20:34,915 - INFO - training batch 1151, loss: 0.251, 36832/60000 datapoints
2025-03-06 18:20:35,105 - INFO - training batch 1201, loss: 0.366, 38432/60000 datapoints
2025-03-06 18:20:35,299 - INFO - training batch 1251, loss: 0.321, 40032/60000 datapoints
2025-03-06 18:20:35,492 - INFO - training batch 1301, loss: 0.553, 41632/60000 datapoints
2025-03-06 18:20:35,686 - INFO - training batch 1351, loss: 0.554, 43232/60000 datapoints
2025-03-06 18:20:35,878 - INFO - training batch 1401, loss: 0.233, 44832/60000 datapoints
2025-03-06 18:20:36,071 - INFO - training batch 1451, loss: 0.318, 46432/60000 datapoints
2025-03-06 18:20:36,264 - INFO - training batch 1501, loss: 0.236, 48032/60000 datapoints
2025-03-06 18:20:36,455 - INFO - training batch 1551, loss: 0.197, 49632/60000 datapoints
2025-03-06 18:20:36,647 - INFO - training batch 1601, loss: 0.252, 51232/60000 datapoints
2025-03-06 18:20:36,839 - INFO - training batch 1651, loss: 0.318, 52832/60000 datapoints
2025-03-06 18:20:37,030 - INFO - training batch 1701, loss: 0.139, 54432/60000 datapoints
2025-03-06 18:20:37,223 - INFO - training batch 1751, loss: 0.296, 56032/60000 datapoints
2025-03-06 18:20:37,435 - INFO - training batch 1801, loss: 0.203, 57632/60000 datapoints
2025-03-06 18:20:37,643 - INFO - training batch 1851, loss: 0.170, 59232/60000 datapoints
2025-03-06 18:20:37,741 - INFO - validation batch 1, loss: 0.232, 32/10016 datapoints
2025-03-06 18:20:37,900 - INFO - validation batch 51, loss: 0.388, 1632/10016 datapoints
2025-03-06 18:20:38,064 - INFO - validation batch 101, loss: 0.175, 3232/10016 datapoints
2025-03-06 18:20:38,213 - INFO - validation batch 151, loss: 0.148, 4832/10016 datapoints
2025-03-06 18:20:38,366 - INFO - validation batch 201, loss: 0.101, 6432/10016 datapoints
2025-03-06 18:20:38,517 - INFO - validation batch 251, loss: 0.457, 8032/10016 datapoints
2025-03-06 18:20:38,668 - INFO - validation batch 301, loss: 0.230, 9632/10016 datapoints
2025-03-06 18:20:38,703 - INFO - Epoch 263/800 done.
2025-03-06 18:20:38,703 - INFO - Final validation performance:
Loss: 0.247, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:20:38,704 - INFO - Beginning epoch 264/800
2025-03-06 18:20:38,709 - INFO - training batch 1, loss: 0.441, 32/60000 datapoints
2025-03-06 18:20:38,901 - INFO - training batch 51, loss: 0.367, 1632/60000 datapoints
2025-03-06 18:20:39,096 - INFO - training batch 101, loss: 0.519, 3232/60000 datapoints
2025-03-06 18:20:39,289 - INFO - training batch 151, loss: 0.125, 4832/60000 datapoints
2025-03-06 18:20:39,506 - INFO - training batch 201, loss: 0.465, 6432/60000 datapoints
2025-03-06 18:20:39,704 - INFO - training batch 251, loss: 0.290, 8032/60000 datapoints
2025-03-06 18:20:39,896 - INFO - training batch 301, loss: 0.244, 9632/60000 datapoints
2025-03-06 18:20:40,086 - INFO - training batch 351, loss: 0.100, 11232/60000 datapoints
2025-03-06 18:20:40,278 - INFO - training batch 401, loss: 0.361, 12832/60000 datapoints
2025-03-06 18:20:40,471 - INFO - training batch 451, loss: 0.573, 14432/60000 datapoints
2025-03-06 18:20:40,665 - INFO - training batch 501, loss: 0.225, 16032/60000 datapoints
2025-03-06 18:20:40,856 - INFO - training batch 551, loss: 0.423, 17632/60000 datapoints
2025-03-06 18:20:41,048 - INFO - training batch 601, loss: 0.211, 19232/60000 datapoints
2025-03-06 18:20:41,243 - INFO - training batch 651, loss: 0.359, 20832/60000 datapoints
2025-03-06 18:20:41,434 - INFO - training batch 701, loss: 0.331, 22432/60000 datapoints
2025-03-06 18:20:41,627 - INFO - training batch 751, loss: 0.198, 24032/60000 datapoints
2025-03-06 18:20:41,820 - INFO - training batch 801, loss: 0.264, 25632/60000 datapoints
2025-03-06 18:20:42,013 - INFO - training batch 851, loss: 0.693, 27232/60000 datapoints
2025-03-06 18:20:42,204 - INFO - training batch 901, loss: 0.333, 28832/60000 datapoints
2025-03-06 18:20:42,396 - INFO - training batch 951, loss: 0.552, 30432/60000 datapoints
2025-03-06 18:20:42,587 - INFO - training batch 1001, loss: 0.722, 32032/60000 datapoints
2025-03-06 18:20:42,782 - INFO - training batch 1051, loss: 0.497, 33632/60000 datapoints
2025-03-06 18:20:42,976 - INFO - training batch 1101, loss: 0.448, 35232/60000 datapoints
2025-03-06 18:20:43,169 - INFO - training batch 1151, loss: 0.168, 36832/60000 datapoints
2025-03-06 18:20:43,362 - INFO - training batch 1201, loss: 0.445, 38432/60000 datapoints
2025-03-06 18:20:43,563 - INFO - training batch 1251, loss: 0.404, 40032/60000 datapoints
2025-03-06 18:20:43,756 - INFO - training batch 1301, loss: 0.336, 41632/60000 datapoints
2025-03-06 18:20:43,949 - INFO - training batch 1351, loss: 0.582, 43232/60000 datapoints
2025-03-06 18:20:44,142 - INFO - training batch 1401, loss: 0.321, 44832/60000 datapoints
2025-03-06 18:20:44,332 - INFO - training batch 1451, loss: 0.273, 46432/60000 datapoints
2025-03-06 18:20:44,524 - INFO - training batch 1501, loss: 0.458, 48032/60000 datapoints
2025-03-06 18:20:44,716 - INFO - training batch 1551, loss: 0.360, 49632/60000 datapoints
2025-03-06 18:20:44,910 - INFO - training batch 1601, loss: 0.320, 51232/60000 datapoints
2025-03-06 18:20:45,102 - INFO - training batch 1651, loss: 0.322, 52832/60000 datapoints
2025-03-06 18:20:45,295 - INFO - training batch 1701, loss: 0.160, 54432/60000 datapoints
2025-03-06 18:20:45,486 - INFO - training batch 1751, loss: 0.415, 56032/60000 datapoints
2025-03-06 18:20:45,681 - INFO - training batch 1801, loss: 0.183, 57632/60000 datapoints
2025-03-06 18:20:45,873 - INFO - training batch 1851, loss: 0.210, 59232/60000 datapoints
2025-03-06 18:20:45,972 - INFO - validation batch 1, loss: 0.212, 32/10016 datapoints
2025-03-06 18:20:46,122 - INFO - validation batch 51, loss: 0.461, 1632/10016 datapoints
2025-03-06 18:20:46,274 - INFO - validation batch 101, loss: 0.547, 3232/10016 datapoints
2025-03-06 18:20:46,422 - INFO - validation batch 151, loss: 0.306, 4832/10016 datapoints
2025-03-06 18:20:46,573 - INFO - validation batch 201, loss: 0.206, 6432/10016 datapoints
2025-03-06 18:20:46,725 - INFO - validation batch 251, loss: 0.421, 8032/10016 datapoints
2025-03-06 18:20:46,874 - INFO - validation batch 301, loss: 0.185, 9632/10016 datapoints
2025-03-06 18:20:46,910 - INFO - Epoch 264/800 done.
2025-03-06 18:20:46,910 - INFO - Final validation performance:
Loss: 0.334, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:20:46,910 - INFO - Beginning epoch 265/800
2025-03-06 18:20:46,916 - INFO - training batch 1, loss: 0.403, 32/60000 datapoints
2025-03-06 18:20:47,108 - INFO - training batch 51, loss: 0.378, 1632/60000 datapoints
2025-03-06 18:20:47,301 - INFO - training batch 101, loss: 0.326, 3232/60000 datapoints
2025-03-06 18:20:47,510 - INFO - training batch 151, loss: 0.405, 4832/60000 datapoints
2025-03-06 18:20:47,704 - INFO - training batch 201, loss: 0.246, 6432/60000 datapoints
2025-03-06 18:20:47,895 - INFO - training batch 251, loss: 0.350, 8032/60000 datapoints
2025-03-06 18:20:48,087 - INFO - training batch 301, loss: 0.400, 9632/60000 datapoints
2025-03-06 18:20:48,278 - INFO - training batch 351, loss: 0.377, 11232/60000 datapoints
2025-03-06 18:20:48,475 - INFO - training batch 401, loss: 0.314, 12832/60000 datapoints
2025-03-06 18:20:48,722 - INFO - training batch 451, loss: 0.325, 14432/60000 datapoints
2025-03-06 18:20:48,917 - INFO - training batch 501, loss: 0.472, 16032/60000 datapoints
2025-03-06 18:20:49,111 - INFO - training batch 551, loss: 0.197, 17632/60000 datapoints
2025-03-06 18:20:49,305 - INFO - training batch 601, loss: 0.442, 19232/60000 datapoints
2025-03-06 18:20:49,498 - INFO - training batch 651, loss: 0.884, 20832/60000 datapoints
2025-03-06 18:20:49,697 - INFO - training batch 701, loss: 0.342, 22432/60000 datapoints
2025-03-06 18:20:49,890 - INFO - training batch 751, loss: 0.250, 24032/60000 datapoints
2025-03-06 18:20:50,081 - INFO - training batch 801, loss: 0.228, 25632/60000 datapoints
2025-03-06 18:20:50,273 - INFO - training batch 851, loss: 0.502, 27232/60000 datapoints
2025-03-06 18:20:50,470 - INFO - training batch 901, loss: 0.771, 28832/60000 datapoints
2025-03-06 18:20:50,665 - INFO - training batch 951, loss: 0.397, 30432/60000 datapoints
2025-03-06 18:20:50,858 - INFO - training batch 1001, loss: 0.468, 32032/60000 datapoints
2025-03-06 18:20:51,052 - INFO - training batch 1051, loss: 0.302, 33632/60000 datapoints
2025-03-06 18:20:51,248 - INFO - training batch 1101, loss: 0.358, 35232/60000 datapoints
2025-03-06 18:20:51,441 - INFO - training batch 1151, loss: 0.364, 36832/60000 datapoints
2025-03-06 18:20:51,637 - INFO - training batch 1201, loss: 0.417, 38432/60000 datapoints
2025-03-06 18:20:51,830 - INFO - training batch 1251, loss: 0.398, 40032/60000 datapoints
2025-03-06 18:20:52,024 - INFO - training batch 1301, loss: 0.205, 41632/60000 datapoints
2025-03-06 18:20:52,220 - INFO - training batch 1351, loss: 0.308, 43232/60000 datapoints
2025-03-06 18:20:52,416 - INFO - training batch 1401, loss: 0.373, 44832/60000 datapoints
2025-03-06 18:20:52,612 - INFO - training batch 1451, loss: 0.060, 46432/60000 datapoints
2025-03-06 18:20:52,816 - INFO - training batch 1501, loss: 0.278, 48032/60000 datapoints
2025-03-06 18:20:53,012 - INFO - training batch 1551, loss: 0.408, 49632/60000 datapoints
2025-03-06 18:20:53,206 - INFO - training batch 1601, loss: 0.424, 51232/60000 datapoints
2025-03-06 18:20:53,403 - INFO - training batch 1651, loss: 0.242, 52832/60000 datapoints
2025-03-06 18:20:53,598 - INFO - training batch 1701, loss: 0.184, 54432/60000 datapoints
2025-03-06 18:20:53,792 - INFO - training batch 1751, loss: 0.153, 56032/60000 datapoints
2025-03-06 18:20:53,986 - INFO - training batch 1801, loss: 0.565, 57632/60000 datapoints
2025-03-06 18:20:54,180 - INFO - training batch 1851, loss: 0.242, 59232/60000 datapoints
2025-03-06 18:20:54,283 - INFO - validation batch 1, loss: 0.388, 32/10016 datapoints
2025-03-06 18:20:54,437 - INFO - validation batch 51, loss: 0.246, 1632/10016 datapoints
2025-03-06 18:20:54,590 - INFO - validation batch 101, loss: 0.588, 3232/10016 datapoints
2025-03-06 18:20:54,748 - INFO - validation batch 151, loss: 0.355, 4832/10016 datapoints
2025-03-06 18:20:54,902 - INFO - validation batch 201, loss: 0.076, 6432/10016 datapoints
2025-03-06 18:20:55,054 - INFO - validation batch 251, loss: 0.367, 8032/10016 datapoints
2025-03-06 18:20:55,206 - INFO - validation batch 301, loss: 0.522, 9632/10016 datapoints
2025-03-06 18:20:55,248 - INFO - Epoch 265/800 done.
2025-03-06 18:20:55,248 - INFO - Final validation performance:
Loss: 0.363, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:20:55,248 - INFO - Beginning epoch 266/800
2025-03-06 18:20:55,255 - INFO - training batch 1, loss: 0.595, 32/60000 datapoints
2025-03-06 18:20:55,451 - INFO - training batch 51, loss: 0.415, 1632/60000 datapoints
2025-03-06 18:20:55,647 - INFO - training batch 101, loss: 0.213, 3232/60000 datapoints
2025-03-06 18:20:55,842 - INFO - training batch 151, loss: 0.355, 4832/60000 datapoints
2025-03-06 18:20:56,037 - INFO - training batch 201, loss: 0.306, 6432/60000 datapoints
2025-03-06 18:20:56,235 - INFO - training batch 251, loss: 0.298, 8032/60000 datapoints
2025-03-06 18:20:56,428 - INFO - training batch 301, loss: 0.157, 9632/60000 datapoints
2025-03-06 18:20:56,623 - INFO - training batch 351, loss: 0.467, 11232/60000 datapoints
2025-03-06 18:20:56,819 - INFO - training batch 401, loss: 0.451, 12832/60000 datapoints
2025-03-06 18:20:57,013 - INFO - training batch 451, loss: 0.115, 14432/60000 datapoints
2025-03-06 18:20:57,207 - INFO - training batch 501, loss: 0.265, 16032/60000 datapoints
2025-03-06 18:20:57,402 - INFO - training batch 551, loss: 0.155, 17632/60000 datapoints
2025-03-06 18:20:57,620 - INFO - training batch 601, loss: 0.331, 19232/60000 datapoints
2025-03-06 18:20:57,815 - INFO - training batch 651, loss: 0.229, 20832/60000 datapoints
2025-03-06 18:20:58,012 - INFO - training batch 701, loss: 0.194, 22432/60000 datapoints
2025-03-06 18:20:58,209 - INFO - training batch 751, loss: 0.385, 24032/60000 datapoints
2025-03-06 18:20:58,402 - INFO - training batch 801, loss: 0.361, 25632/60000 datapoints
2025-03-06 18:20:58,602 - INFO - training batch 851, loss: 0.315, 27232/60000 datapoints
2025-03-06 18:20:58,799 - INFO - training batch 901, loss: 0.472, 28832/60000 datapoints
2025-03-06 18:20:58,998 - INFO - training batch 951, loss: 0.182, 30432/60000 datapoints
2025-03-06 18:20:59,192 - INFO - training batch 1001, loss: 0.172, 32032/60000 datapoints
2025-03-06 18:20:59,390 - INFO - training batch 1051, loss: 0.452, 33632/60000 datapoints
2025-03-06 18:20:59,585 - INFO - training batch 1101, loss: 0.178, 35232/60000 datapoints
2025-03-06 18:20:59,780 - INFO - training batch 1151, loss: 0.334, 36832/60000 datapoints
2025-03-06 18:20:59,974 - INFO - training batch 1201, loss: 0.232, 38432/60000 datapoints
2025-03-06 18:21:00,167 - INFO - training batch 1251, loss: 0.200, 40032/60000 datapoints
2025-03-06 18:21:00,368 - INFO - training batch 1301, loss: 0.312, 41632/60000 datapoints
2025-03-06 18:21:00,563 - INFO - training batch 1351, loss: 0.180, 43232/60000 datapoints
2025-03-06 18:21:00,761 - INFO - training batch 1401, loss: 0.243, 44832/60000 datapoints
2025-03-06 18:21:00,955 - INFO - training batch 1451, loss: 0.439, 46432/60000 datapoints
2025-03-06 18:21:01,151 - INFO - training batch 1501, loss: 0.364, 48032/60000 datapoints
2025-03-06 18:21:01,350 - INFO - training batch 1551, loss: 0.342, 49632/60000 datapoints
2025-03-06 18:21:01,546 - INFO - training batch 1601, loss: 0.214, 51232/60000 datapoints
2025-03-06 18:21:01,741 - INFO - training batch 1651, loss: 0.419, 52832/60000 datapoints
2025-03-06 18:21:01,936 - INFO - training batch 1701, loss: 0.611, 54432/60000 datapoints
2025-03-06 18:21:02,128 - INFO - training batch 1751, loss: 0.109, 56032/60000 datapoints
2025-03-06 18:21:02,322 - INFO - training batch 1801, loss: 0.242, 57632/60000 datapoints
2025-03-06 18:21:02,515 - INFO - training batch 1851, loss: 0.368, 59232/60000 datapoints
2025-03-06 18:21:02,617 - INFO - validation batch 1, loss: 0.671, 32/10016 datapoints
2025-03-06 18:21:02,770 - INFO - validation batch 51, loss: 0.718, 1632/10016 datapoints
2025-03-06 18:21:02,922 - INFO - validation batch 101, loss: 0.509, 3232/10016 datapoints
2025-03-06 18:21:03,074 - INFO - validation batch 151, loss: 0.101, 4832/10016 datapoints
2025-03-06 18:21:03,227 - INFO - validation batch 201, loss: 0.394, 6432/10016 datapoints
2025-03-06 18:21:03,383 - INFO - validation batch 251, loss: 0.102, 8032/10016 datapoints
2025-03-06 18:21:03,536 - INFO - validation batch 301, loss: 0.258, 9632/10016 datapoints
2025-03-06 18:21:03,573 - INFO - Epoch 266/800 done.
2025-03-06 18:21:03,574 - INFO - Final validation performance:
Loss: 0.393, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:21:03,574 - INFO - Beginning epoch 267/800
2025-03-06 18:21:03,580 - INFO - training batch 1, loss: 0.372, 32/60000 datapoints
2025-03-06 18:21:03,779 - INFO - training batch 51, loss: 0.242, 1632/60000 datapoints
2025-03-06 18:21:03,974 - INFO - training batch 101, loss: 0.302, 3232/60000 datapoints
2025-03-06 18:21:04,168 - INFO - training batch 151, loss: 0.162, 4832/60000 datapoints
2025-03-06 18:21:04,363 - INFO - training batch 201, loss: 0.317, 6432/60000 datapoints
2025-03-06 18:21:04,559 - INFO - training batch 251, loss: 0.160, 8032/60000 datapoints
2025-03-06 18:21:04,756 - INFO - training batch 301, loss: 0.284, 9632/60000 datapoints
2025-03-06 18:21:04,960 - INFO - training batch 351, loss: 0.471, 11232/60000 datapoints
2025-03-06 18:21:05,155 - INFO - training batch 401, loss: 0.422, 12832/60000 datapoints
2025-03-06 18:21:05,355 - INFO - training batch 451, loss: 0.385, 14432/60000 datapoints
2025-03-06 18:21:05,550 - INFO - training batch 501, loss: 0.295, 16032/60000 datapoints
2025-03-06 18:21:05,744 - INFO - training batch 551, loss: 0.570, 17632/60000 datapoints
2025-03-06 18:21:05,939 - INFO - training batch 601, loss: 0.290, 19232/60000 datapoints
2025-03-06 18:21:06,133 - INFO - training batch 651, loss: 0.597, 20832/60000 datapoints
2025-03-06 18:21:06,330 - INFO - training batch 701, loss: 0.287, 22432/60000 datapoints
2025-03-06 18:21:06,523 - INFO - training batch 751, loss: 0.310, 24032/60000 datapoints
2025-03-06 18:21:06,718 - INFO - training batch 801, loss: 0.198, 25632/60000 datapoints
2025-03-06 18:21:06,912 - INFO - training batch 851, loss: 0.427, 27232/60000 datapoints
2025-03-06 18:21:07,104 - INFO - training batch 901, loss: 0.309, 28832/60000 datapoints
2025-03-06 18:21:07,299 - INFO - training batch 951, loss: 0.398, 30432/60000 datapoints
2025-03-06 18:21:07,495 - INFO - training batch 1001, loss: 0.500, 32032/60000 datapoints
2025-03-06 18:21:07,714 - INFO - training batch 1051, loss: 0.242, 33632/60000 datapoints
2025-03-06 18:21:07,910 - INFO - training batch 1101, loss: 0.350, 35232/60000 datapoints
2025-03-06 18:21:08,116 - INFO - training batch 1151, loss: 0.275, 36832/60000 datapoints
2025-03-06 18:21:08,311 - INFO - training batch 1201, loss: 0.299, 38432/60000 datapoints
2025-03-06 18:21:08,513 - INFO - training batch 1251, loss: 0.252, 40032/60000 datapoints
2025-03-06 18:21:08,709 - INFO - training batch 1301, loss: 0.593, 41632/60000 datapoints
2025-03-06 18:21:08,903 - INFO - training batch 1351, loss: 0.496, 43232/60000 datapoints
2025-03-06 18:21:09,101 - INFO - training batch 1401, loss: 0.207, 44832/60000 datapoints
2025-03-06 18:21:09,297 - INFO - training batch 1451, loss: 0.352, 46432/60000 datapoints
2025-03-06 18:21:09,493 - INFO - training batch 1501, loss: 0.285, 48032/60000 datapoints
2025-03-06 18:21:09,688 - INFO - training batch 1551, loss: 0.137, 49632/60000 datapoints
2025-03-06 18:21:09,882 - INFO - training batch 1601, loss: 0.453, 51232/60000 datapoints
2025-03-06 18:21:10,076 - INFO - training batch 1651, loss: 0.164, 52832/60000 datapoints
2025-03-06 18:21:10,267 - INFO - training batch 1701, loss: 0.405, 54432/60000 datapoints
2025-03-06 18:21:10,469 - INFO - training batch 1751, loss: 0.659, 56032/60000 datapoints
2025-03-06 18:21:10,664 - INFO - training batch 1801, loss: 0.124, 57632/60000 datapoints
2025-03-06 18:21:10,859 - INFO - training batch 1851, loss: 0.430, 59232/60000 datapoints
2025-03-06 18:21:10,960 - INFO - validation batch 1, loss: 0.487, 32/10016 datapoints
2025-03-06 18:21:11,113 - INFO - validation batch 51, loss: 0.329, 1632/10016 datapoints
2025-03-06 18:21:11,265 - INFO - validation batch 101, loss: 0.393, 3232/10016 datapoints
2025-03-06 18:21:11,421 - INFO - validation batch 151, loss: 0.458, 4832/10016 datapoints
2025-03-06 18:21:11,571 - INFO - validation batch 201, loss: 0.301, 6432/10016 datapoints
2025-03-06 18:21:11,726 - INFO - validation batch 251, loss: 0.327, 8032/10016 datapoints
2025-03-06 18:21:11,878 - INFO - validation batch 301, loss: 0.179, 9632/10016 datapoints
2025-03-06 18:21:11,914 - INFO - Epoch 267/800 done.
2025-03-06 18:21:11,915 - INFO - Final validation performance:
Loss: 0.353, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:21:11,915 - INFO - Beginning epoch 268/800
2025-03-06 18:21:11,922 - INFO - training batch 1, loss: 0.374, 32/60000 datapoints
2025-03-06 18:21:12,117 - INFO - training batch 51, loss: 0.311, 1632/60000 datapoints
2025-03-06 18:21:12,310 - INFO - training batch 101, loss: 0.428, 3232/60000 datapoints
2025-03-06 18:21:12,505 - INFO - training batch 151, loss: 0.535, 4832/60000 datapoints
2025-03-06 18:21:12,700 - INFO - training batch 201, loss: 0.378, 6432/60000 datapoints
2025-03-06 18:21:12,895 - INFO - training batch 251, loss: 0.245, 8032/60000 datapoints
2025-03-06 18:21:13,089 - INFO - training batch 301, loss: 0.389, 9632/60000 datapoints
2025-03-06 18:21:13,284 - INFO - training batch 351, loss: 0.427, 11232/60000 datapoints
2025-03-06 18:21:13,484 - INFO - training batch 401, loss: 0.184, 12832/60000 datapoints
2025-03-06 18:21:13,682 - INFO - training batch 451, loss: 0.420, 14432/60000 datapoints
2025-03-06 18:21:13,874 - INFO - training batch 501, loss: 0.193, 16032/60000 datapoints
2025-03-06 18:21:14,066 - INFO - training batch 551, loss: 0.320, 17632/60000 datapoints
2025-03-06 18:21:14,259 - INFO - training batch 601, loss: 0.315, 19232/60000 datapoints
2025-03-06 18:21:14,454 - INFO - training batch 651, loss: 0.300, 20832/60000 datapoints
2025-03-06 18:21:14,658 - INFO - training batch 701, loss: 0.430, 22432/60000 datapoints
2025-03-06 18:21:14,890 - INFO - training batch 751, loss: 0.186, 24032/60000 datapoints
2025-03-06 18:21:15,088 - INFO - training batch 801, loss: 0.142, 25632/60000 datapoints
2025-03-06 18:21:15,280 - INFO - training batch 851, loss: 0.226, 27232/60000 datapoints
2025-03-06 18:21:15,478 - INFO - training batch 901, loss: 0.094, 28832/60000 datapoints
2025-03-06 18:21:15,676 - INFO - training batch 951, loss: 0.564, 30432/60000 datapoints
2025-03-06 18:21:15,870 - INFO - training batch 1001, loss: 0.472, 32032/60000 datapoints
2025-03-06 18:21:16,065 - INFO - training batch 1051, loss: 0.222, 33632/60000 datapoints
2025-03-06 18:21:16,262 - INFO - training batch 1101, loss: 0.268, 35232/60000 datapoints
2025-03-06 18:21:16,458 - INFO - training batch 1151, loss: 0.121, 36832/60000 datapoints
2025-03-06 18:21:16,656 - INFO - training batch 1201, loss: 0.103, 38432/60000 datapoints
2025-03-06 18:21:16,851 - INFO - training batch 1251, loss: 0.182, 40032/60000 datapoints
2025-03-06 18:21:17,048 - INFO - training batch 1301, loss: 0.099, 41632/60000 datapoints
2025-03-06 18:21:17,240 - INFO - training batch 1351, loss: 0.186, 43232/60000 datapoints
2025-03-06 18:21:17,446 - INFO - training batch 1401, loss: 0.434, 44832/60000 datapoints
2025-03-06 18:21:17,670 - INFO - training batch 1451, loss: 0.475, 46432/60000 datapoints
2025-03-06 18:21:17,899 - INFO - training batch 1501, loss: 0.364, 48032/60000 datapoints
2025-03-06 18:21:18,093 - INFO - training batch 1551, loss: 0.167, 49632/60000 datapoints
2025-03-06 18:21:18,286 - INFO - training batch 1601, loss: 0.322, 51232/60000 datapoints
2025-03-06 18:21:18,480 - INFO - training batch 1651, loss: 0.250, 52832/60000 datapoints
2025-03-06 18:21:18,678 - INFO - training batch 1701, loss: 0.703, 54432/60000 datapoints
2025-03-06 18:21:18,874 - INFO - training batch 1751, loss: 0.196, 56032/60000 datapoints
2025-03-06 18:21:19,068 - INFO - training batch 1801, loss: 0.443, 57632/60000 datapoints
2025-03-06 18:21:19,259 - INFO - training batch 1851, loss: 0.237, 59232/60000 datapoints
2025-03-06 18:21:19,361 - INFO - validation batch 1, loss: 0.214, 32/10016 datapoints
2025-03-06 18:21:19,513 - INFO - validation batch 51, loss: 0.206, 1632/10016 datapoints
2025-03-06 18:21:19,670 - INFO - validation batch 101, loss: 0.445, 3232/10016 datapoints
2025-03-06 18:21:19,822 - INFO - validation batch 151, loss: 0.193, 4832/10016 datapoints
2025-03-06 18:21:19,976 - INFO - validation batch 201, loss: 0.207, 6432/10016 datapoints
2025-03-06 18:21:20,130 - INFO - validation batch 251, loss: 0.339, 8032/10016 datapoints
2025-03-06 18:21:20,284 - INFO - validation batch 301, loss: 0.717, 9632/10016 datapoints
2025-03-06 18:21:20,321 - INFO - Epoch 268/800 done.
2025-03-06 18:21:20,321 - INFO - Final validation performance:
Loss: 0.332, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:21:20,321 - INFO - Beginning epoch 269/800
2025-03-06 18:21:20,327 - INFO - training batch 1, loss: 0.356, 32/60000 datapoints
2025-03-06 18:21:20,528 - INFO - training batch 51, loss: 0.246, 1632/60000 datapoints
2025-03-06 18:21:20,722 - INFO - training batch 101, loss: 0.419, 3232/60000 datapoints
2025-03-06 18:21:20,917 - INFO - training batch 151, loss: 0.172, 4832/60000 datapoints
2025-03-06 18:21:21,111 - INFO - training batch 201, loss: 0.330, 6432/60000 datapoints
2025-03-06 18:21:21,305 - INFO - training batch 251, loss: 0.530, 8032/60000 datapoints
2025-03-06 18:21:21,504 - INFO - training batch 301, loss: 0.238, 9632/60000 datapoints
2025-03-06 18:21:21,701 - INFO - training batch 351, loss: 0.341, 11232/60000 datapoints
2025-03-06 18:21:21,895 - INFO - training batch 401, loss: 0.156, 12832/60000 datapoints
2025-03-06 18:21:22,089 - INFO - training batch 451, loss: 0.742, 14432/60000 datapoints
2025-03-06 18:21:22,281 - INFO - training batch 501, loss: 0.247, 16032/60000 datapoints
2025-03-06 18:21:22,474 - INFO - training batch 551, loss: 0.384, 17632/60000 datapoints
2025-03-06 18:21:22,674 - INFO - training batch 601, loss: 0.249, 19232/60000 datapoints
2025-03-06 18:21:22,868 - INFO - training batch 651, loss: 0.367, 20832/60000 datapoints
2025-03-06 18:21:23,065 - INFO - training batch 701, loss: 0.398, 22432/60000 datapoints
2025-03-06 18:21:23,257 - INFO - training batch 751, loss: 0.356, 24032/60000 datapoints
2025-03-06 18:21:23,453 - INFO - training batch 801, loss: 0.192, 25632/60000 datapoints
2025-03-06 18:21:23,652 - INFO - training batch 851, loss: 0.110, 27232/60000 datapoints
2025-03-06 18:21:23,847 - INFO - training batch 901, loss: 0.139, 28832/60000 datapoints
2025-03-06 18:21:24,041 - INFO - training batch 951, loss: 0.342, 30432/60000 datapoints
2025-03-06 18:21:24,233 - INFO - training batch 1001, loss: 0.305, 32032/60000 datapoints
2025-03-06 18:21:24,426 - INFO - training batch 1051, loss: 0.188, 33632/60000 datapoints
2025-03-06 18:21:24,621 - INFO - training batch 1101, loss: 0.251, 35232/60000 datapoints
2025-03-06 18:21:24,814 - INFO - training batch 1151, loss: 0.193, 36832/60000 datapoints
2025-03-06 18:21:25,014 - INFO - training batch 1201, loss: 0.366, 38432/60000 datapoints
2025-03-06 18:21:25,208 - INFO - training batch 1251, loss: 0.418, 40032/60000 datapoints
2025-03-06 18:21:25,404 - INFO - training batch 1301, loss: 0.213, 41632/60000 datapoints
2025-03-06 18:21:25,603 - INFO - training batch 1351, loss: 0.165, 43232/60000 datapoints
2025-03-06 18:21:25,797 - INFO - training batch 1401, loss: 0.280, 44832/60000 datapoints
2025-03-06 18:21:25,993 - INFO - training batch 1451, loss: 0.378, 46432/60000 datapoints
2025-03-06 18:21:26,193 - INFO - training batch 1501, loss: 0.409, 48032/60000 datapoints
2025-03-06 18:21:26,388 - INFO - training batch 1551, loss: 0.596, 49632/60000 datapoints
2025-03-06 18:21:26,582 - INFO - training batch 1601, loss: 0.242, 51232/60000 datapoints
2025-03-06 18:21:26,775 - INFO - training batch 1651, loss: 0.335, 52832/60000 datapoints
2025-03-06 18:21:26,967 - INFO - training batch 1701, loss: 0.338, 54432/60000 datapoints
2025-03-06 18:21:27,162 - INFO - training batch 1751, loss: 0.209, 56032/60000 datapoints
2025-03-06 18:21:27,357 - INFO - training batch 1801, loss: 0.244, 57632/60000 datapoints
2025-03-06 18:21:27,568 - INFO - training batch 1851, loss: 0.652, 59232/60000 datapoints
2025-03-06 18:21:27,672 - INFO - validation batch 1, loss: 0.434, 32/10016 datapoints
2025-03-06 18:21:27,843 - INFO - validation batch 51, loss: 0.154, 1632/10016 datapoints
2025-03-06 18:21:27,995 - INFO - validation batch 101, loss: 0.553, 3232/10016 datapoints
2025-03-06 18:21:28,148 - INFO - validation batch 151, loss: 0.289, 4832/10016 datapoints
2025-03-06 18:21:28,301 - INFO - validation batch 201, loss: 0.227, 6432/10016 datapoints
2025-03-06 18:21:28,452 - INFO - validation batch 251, loss: 0.219, 8032/10016 datapoints
2025-03-06 18:21:28,608 - INFO - validation batch 301, loss: 0.357, 9632/10016 datapoints
2025-03-06 18:21:28,649 - INFO - Epoch 269/800 done.
2025-03-06 18:21:28,649 - INFO - Final validation performance:
Loss: 0.319, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:21:28,649 - INFO - Beginning epoch 270/800
2025-03-06 18:21:28,657 - INFO - training batch 1, loss: 0.214, 32/60000 datapoints
2025-03-06 18:21:28,854 - INFO - training batch 51, loss: 0.303, 1632/60000 datapoints
2025-03-06 18:21:29,056 - INFO - training batch 101, loss: 0.171, 3232/60000 datapoints
2025-03-06 18:21:29,249 - INFO - training batch 151, loss: 0.240, 4832/60000 datapoints
2025-03-06 18:21:29,444 - INFO - training batch 201, loss: 0.348, 6432/60000 datapoints
2025-03-06 18:21:29,638 - INFO - training batch 251, loss: 0.408, 8032/60000 datapoints
2025-03-06 18:21:29,832 - INFO - training batch 301, loss: 0.272, 9632/60000 datapoints
2025-03-06 18:21:30,027 - INFO - training batch 351, loss: 0.877, 11232/60000 datapoints
2025-03-06 18:21:30,221 - INFO - training batch 401, loss: 0.257, 12832/60000 datapoints
2025-03-06 18:21:30,418 - INFO - training batch 451, loss: 0.407, 14432/60000 datapoints
2025-03-06 18:21:30,615 - INFO - training batch 501, loss: 0.164, 16032/60000 datapoints
2025-03-06 18:21:30,807 - INFO - training batch 551, loss: 0.212, 17632/60000 datapoints
2025-03-06 18:21:30,998 - INFO - training batch 601, loss: 0.626, 19232/60000 datapoints
2025-03-06 18:21:31,191 - INFO - training batch 651, loss: 0.355, 20832/60000 datapoints
2025-03-06 18:21:31,387 - INFO - training batch 701, loss: 0.237, 22432/60000 datapoints
2025-03-06 18:21:31,579 - INFO - training batch 751, loss: 0.281, 24032/60000 datapoints
2025-03-06 18:21:31,777 - INFO - training batch 801, loss: 0.328, 25632/60000 datapoints
2025-03-06 18:21:31,971 - INFO - training batch 851, loss: 0.300, 27232/60000 datapoints
2025-03-06 18:21:32,165 - INFO - training batch 901, loss: 0.175, 28832/60000 datapoints
2025-03-06 18:21:32,357 - INFO - training batch 951, loss: 0.531, 30432/60000 datapoints
2025-03-06 18:21:32,551 - INFO - training batch 1001, loss: 0.339, 32032/60000 datapoints
2025-03-06 18:21:32,751 - INFO - training batch 1051, loss: 0.476, 33632/60000 datapoints
2025-03-06 18:21:32,947 - INFO - training batch 1101, loss: 0.234, 35232/60000 datapoints
2025-03-06 18:21:33,140 - INFO - training batch 1151, loss: 0.480, 36832/60000 datapoints
2025-03-06 18:21:33,333 - INFO - training batch 1201, loss: 0.340, 38432/60000 datapoints
2025-03-06 18:21:33,531 - INFO - training batch 1251, loss: 0.252, 40032/60000 datapoints
2025-03-06 18:21:33,728 - INFO - training batch 1301, loss: 0.138, 41632/60000 datapoints
2025-03-06 18:21:33,923 - INFO - training batch 1351, loss: 0.270, 43232/60000 datapoints
2025-03-06 18:21:34,114 - INFO - training batch 1401, loss: 0.344, 44832/60000 datapoints
2025-03-06 18:21:34,308 - INFO - training batch 1451, loss: 0.380, 46432/60000 datapoints
2025-03-06 18:21:34,503 - INFO - training batch 1501, loss: 0.293, 48032/60000 datapoints
2025-03-06 18:21:34,700 - INFO - training batch 1551, loss: 0.361, 49632/60000 datapoints
2025-03-06 18:21:34,898 - INFO - training batch 1601, loss: 0.161, 51232/60000 datapoints
2025-03-06 18:21:35,093 - INFO - training batch 1651, loss: 0.397, 52832/60000 datapoints
2025-03-06 18:21:35,287 - INFO - training batch 1701, loss: 0.182, 54432/60000 datapoints
2025-03-06 18:21:35,484 - INFO - training batch 1751, loss: 0.427, 56032/60000 datapoints
2025-03-06 18:21:35,682 - INFO - training batch 1801, loss: 0.177, 57632/60000 datapoints
2025-03-06 18:21:35,879 - INFO - training batch 1851, loss: 0.320, 59232/60000 datapoints
2025-03-06 18:21:35,980 - INFO - validation batch 1, loss: 0.167, 32/10016 datapoints
2025-03-06 18:21:36,136 - INFO - validation batch 51, loss: 0.190, 1632/10016 datapoints
2025-03-06 18:21:36,292 - INFO - validation batch 101, loss: 0.163, 3232/10016 datapoints
2025-03-06 18:21:36,444 - INFO - validation batch 151, loss: 0.350, 4832/10016 datapoints
2025-03-06 18:21:36,595 - INFO - validation batch 201, loss: 0.445, 6432/10016 datapoints
2025-03-06 18:21:36,750 - INFO - validation batch 251, loss: 0.292, 8032/10016 datapoints
2025-03-06 18:21:36,902 - INFO - validation batch 301, loss: 0.130, 9632/10016 datapoints
2025-03-06 18:21:36,941 - INFO - Epoch 270/800 done.
2025-03-06 18:21:36,941 - INFO - Final validation performance:
Loss: 0.248, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:21:36,942 - INFO - Beginning epoch 271/800
2025-03-06 18:21:36,947 - INFO - training batch 1, loss: 0.419, 32/60000 datapoints
2025-03-06 18:21:37,139 - INFO - training batch 51, loss: 0.472, 1632/60000 datapoints
2025-03-06 18:21:37,337 - INFO - training batch 101, loss: 0.394, 3232/60000 datapoints
2025-03-06 18:21:37,533 - INFO - training batch 151, loss: 0.347, 4832/60000 datapoints
2025-03-06 18:21:37,739 - INFO - training batch 201, loss: 0.423, 6432/60000 datapoints
2025-03-06 18:21:37,956 - INFO - training batch 251, loss: 0.355, 8032/60000 datapoints
2025-03-06 18:21:38,152 - INFO - training batch 301, loss: 0.159, 9632/60000 datapoints
2025-03-06 18:21:38,348 - INFO - training batch 351, loss: 0.227, 11232/60000 datapoints
2025-03-06 18:21:38,543 - INFO - training batch 401, loss: 0.353, 12832/60000 datapoints
2025-03-06 18:21:38,738 - INFO - training batch 451, loss: 0.277, 14432/60000 datapoints
2025-03-06 18:21:38,940 - INFO - training batch 501, loss: 0.176, 16032/60000 datapoints
2025-03-06 18:21:39,140 - INFO - training batch 551, loss: 0.158, 17632/60000 datapoints
2025-03-06 18:21:39,334 - INFO - training batch 601, loss: 0.154, 19232/60000 datapoints
2025-03-06 18:21:39,533 - INFO - training batch 651, loss: 0.318, 20832/60000 datapoints
2025-03-06 18:21:39,729 - INFO - training batch 701, loss: 0.412, 22432/60000 datapoints
2025-03-06 18:21:39,951 - INFO - training batch 751, loss: 0.278, 24032/60000 datapoints
2025-03-06 18:21:40,145 - INFO - training batch 801, loss: 0.240, 25632/60000 datapoints
2025-03-06 18:21:40,339 - INFO - training batch 851, loss: 0.314, 27232/60000 datapoints
2025-03-06 18:21:40,539 - INFO - training batch 901, loss: 0.304, 28832/60000 datapoints
2025-03-06 18:21:40,737 - INFO - training batch 951, loss: 0.231, 30432/60000 datapoints
2025-03-06 18:21:40,931 - INFO - training batch 1001, loss: 0.220, 32032/60000 datapoints
2025-03-06 18:21:41,124 - INFO - training batch 1051, loss: 0.343, 33632/60000 datapoints
2025-03-06 18:21:41,321 - INFO - training batch 1101, loss: 0.356, 35232/60000 datapoints
2025-03-06 18:21:41,517 - INFO - training batch 1151, loss: 0.381, 36832/60000 datapoints
2025-03-06 18:21:41,713 - INFO - training batch 1201, loss: 0.301, 38432/60000 datapoints
2025-03-06 18:21:41,908 - INFO - training batch 1251, loss: 0.164, 40032/60000 datapoints
2025-03-06 18:21:42,102 - INFO - training batch 1301, loss: 0.227, 41632/60000 datapoints
2025-03-06 18:21:42,297 - INFO - training batch 1351, loss: 0.249, 43232/60000 datapoints
2025-03-06 18:21:42,490 - INFO - training batch 1401, loss: 0.299, 44832/60000 datapoints
2025-03-06 18:21:42,684 - INFO - training batch 1451, loss: 0.418, 46432/60000 datapoints
2025-03-06 18:21:42,878 - INFO - training batch 1501, loss: 0.197, 48032/60000 datapoints
2025-03-06 18:21:43,073 - INFO - training batch 1551, loss: 0.293, 49632/60000 datapoints
2025-03-06 18:21:43,266 - INFO - training batch 1601, loss: 0.207, 51232/60000 datapoints
2025-03-06 18:21:43,462 - INFO - training batch 1651, loss: 0.329, 52832/60000 datapoints
2025-03-06 18:21:43,658 - INFO - training batch 1701, loss: 0.143, 54432/60000 datapoints
2025-03-06 18:21:43,853 - INFO - training batch 1751, loss: 0.199, 56032/60000 datapoints
2025-03-06 18:21:44,051 - INFO - training batch 1801, loss: 0.398, 57632/60000 datapoints
2025-03-06 18:21:44,245 - INFO - training batch 1851, loss: 0.333, 59232/60000 datapoints
2025-03-06 18:21:44,348 - INFO - validation batch 1, loss: 0.203, 32/10016 datapoints
2025-03-06 18:21:44,499 - INFO - validation batch 51, loss: 0.158, 1632/10016 datapoints
2025-03-06 18:21:44,653 - INFO - validation batch 101, loss: 0.222, 3232/10016 datapoints
2025-03-06 18:21:44,807 - INFO - validation batch 151, loss: 0.227, 4832/10016 datapoints
2025-03-06 18:21:44,963 - INFO - validation batch 201, loss: 0.419, 6432/10016 datapoints
2025-03-06 18:21:45,115 - INFO - validation batch 251, loss: 0.251, 8032/10016 datapoints
2025-03-06 18:21:45,267 - INFO - validation batch 301, loss: 0.085, 9632/10016 datapoints
2025-03-06 18:21:45,303 - INFO - Epoch 271/800 done.
2025-03-06 18:21:45,303 - INFO - Final validation performance:
Loss: 0.224, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:21:45,304 - INFO - Beginning epoch 272/800
2025-03-06 18:21:45,310 - INFO - training batch 1, loss: 0.351, 32/60000 datapoints
2025-03-06 18:21:45,521 - INFO - training batch 51, loss: 0.274, 1632/60000 datapoints
2025-03-06 18:21:45,718 - INFO - training batch 101, loss: 0.673, 3232/60000 datapoints
2025-03-06 18:21:45,919 - INFO - training batch 151, loss: 0.202, 4832/60000 datapoints
2025-03-06 18:21:46,112 - INFO - training batch 201, loss: 0.418, 6432/60000 datapoints
2025-03-06 18:21:46,311 - INFO - training batch 251, loss: 0.397, 8032/60000 datapoints
2025-03-06 18:21:46,507 - INFO - training batch 301, loss: 0.341, 9632/60000 datapoints
2025-03-06 18:21:46,705 - INFO - training batch 351, loss: 0.673, 11232/60000 datapoints
2025-03-06 18:21:46,900 - INFO - training batch 401, loss: 0.235, 12832/60000 datapoints
2025-03-06 18:21:47,093 - INFO - training batch 451, loss: 0.428, 14432/60000 datapoints
2025-03-06 18:21:47,286 - INFO - training batch 501, loss: 0.654, 16032/60000 datapoints
2025-03-06 18:21:47,486 - INFO - training batch 551, loss: 0.514, 17632/60000 datapoints
2025-03-06 18:21:47,682 - INFO - training batch 601, loss: 0.158, 19232/60000 datapoints
2025-03-06 18:21:47,876 - INFO - training batch 651, loss: 0.462, 20832/60000 datapoints
2025-03-06 18:21:48,089 - INFO - training batch 701, loss: 0.255, 22432/60000 datapoints
2025-03-06 18:21:48,283 - INFO - training batch 751, loss: 0.469, 24032/60000 datapoints
2025-03-06 18:21:48,478 - INFO - training batch 801, loss: 0.260, 25632/60000 datapoints
2025-03-06 18:21:48,675 - INFO - training batch 851, loss: 0.293, 27232/60000 datapoints
2025-03-06 18:21:48,868 - INFO - training batch 901, loss: 0.379, 28832/60000 datapoints
2025-03-06 18:21:49,063 - INFO - training batch 951, loss: 0.173, 30432/60000 datapoints
2025-03-06 18:21:49,259 - INFO - training batch 1001, loss: 0.404, 32032/60000 datapoints
2025-03-06 18:21:49,458 - INFO - training batch 1051, loss: 0.359, 33632/60000 datapoints
2025-03-06 18:21:49,655 - INFO - training batch 1101, loss: 0.334, 35232/60000 datapoints
2025-03-06 18:21:49,850 - INFO - training batch 1151, loss: 0.281, 36832/60000 datapoints
2025-03-06 18:21:50,043 - INFO - training batch 1201, loss: 0.203, 38432/60000 datapoints
2025-03-06 18:21:50,239 - INFO - training batch 1251, loss: 0.147, 40032/60000 datapoints
2025-03-06 18:21:50,433 - INFO - training batch 1301, loss: 0.301, 41632/60000 datapoints
2025-03-06 18:21:50,634 - INFO - training batch 1351, loss: 0.123, 43232/60000 datapoints
2025-03-06 18:21:50,827 - INFO - training batch 1401, loss: 0.313, 44832/60000 datapoints
2025-03-06 18:21:51,024 - INFO - training batch 1451, loss: 0.208, 46432/60000 datapoints
2025-03-06 18:21:51,219 - INFO - training batch 1501, loss: 0.375, 48032/60000 datapoints
2025-03-06 18:21:51,414 - INFO - training batch 1551, loss: 0.250, 49632/60000 datapoints
2025-03-06 18:21:51,612 - INFO - training batch 1601, loss: 0.347, 51232/60000 datapoints
2025-03-06 18:21:51,807 - INFO - training batch 1651, loss: 0.509, 52832/60000 datapoints
2025-03-06 18:21:52,001 - INFO - training batch 1701, loss: 0.226, 54432/60000 datapoints
2025-03-06 18:21:52,195 - INFO - training batch 1751, loss: 0.439, 56032/60000 datapoints
2025-03-06 18:21:52,391 - INFO - training batch 1801, loss: 0.409, 57632/60000 datapoints
2025-03-06 18:21:52,587 - INFO - training batch 1851, loss: 0.330, 59232/60000 datapoints
2025-03-06 18:21:52,690 - INFO - validation batch 1, loss: 0.502, 32/10016 datapoints
2025-03-06 18:21:52,841 - INFO - validation batch 51, loss: 0.308, 1632/10016 datapoints
2025-03-06 18:21:52,997 - INFO - validation batch 101, loss: 0.192, 3232/10016 datapoints
2025-03-06 18:21:53,154 - INFO - validation batch 151, loss: 0.354, 4832/10016 datapoints
2025-03-06 18:21:53,306 - INFO - validation batch 201, loss: 0.286, 6432/10016 datapoints
2025-03-06 18:21:53,462 - INFO - validation batch 251, loss: 0.337, 8032/10016 datapoints
2025-03-06 18:21:53,617 - INFO - validation batch 301, loss: 0.494, 9632/10016 datapoints
2025-03-06 18:21:53,652 - INFO - Epoch 272/800 done.
2025-03-06 18:21:53,652 - INFO - Final validation performance:
Loss: 0.353, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:21:53,653 - INFO - Beginning epoch 273/800
2025-03-06 18:21:53,659 - INFO - training batch 1, loss: 0.370, 32/60000 datapoints
2025-03-06 18:21:53,858 - INFO - training batch 51, loss: 0.168, 1632/60000 datapoints
2025-03-06 18:21:54,052 - INFO - training batch 101, loss: 0.263, 3232/60000 datapoints
2025-03-06 18:21:54,245 - INFO - training batch 151, loss: 0.274, 4832/60000 datapoints
2025-03-06 18:21:54,443 - INFO - training batch 201, loss: 0.210, 6432/60000 datapoints
2025-03-06 18:21:54,638 - INFO - training batch 251, loss: 0.458, 8032/60000 datapoints
2025-03-06 18:21:54,835 - INFO - training batch 301, loss: 0.163, 9632/60000 datapoints
2025-03-06 18:21:55,032 - INFO - training batch 351, loss: 0.326, 11232/60000 datapoints
2025-03-06 18:21:55,226 - INFO - training batch 401, loss: 0.238, 12832/60000 datapoints
2025-03-06 18:21:55,421 - INFO - training batch 451, loss: 0.339, 14432/60000 datapoints
2025-03-06 18:21:55,623 - INFO - training batch 501, loss: 0.098, 16032/60000 datapoints
2025-03-06 18:21:55,815 - INFO - training batch 551, loss: 0.267, 17632/60000 datapoints
2025-03-06 18:21:56,009 - INFO - training batch 601, loss: 0.571, 19232/60000 datapoints
2025-03-06 18:21:56,205 - INFO - training batch 651, loss: 0.254, 20832/60000 datapoints
2025-03-06 18:21:56,399 - INFO - training batch 701, loss: 0.279, 22432/60000 datapoints
2025-03-06 18:21:56,594 - INFO - training batch 751, loss: 0.475, 24032/60000 datapoints
2025-03-06 18:21:56,790 - INFO - training batch 801, loss: 0.324, 25632/60000 datapoints
2025-03-06 18:21:56,986 - INFO - training batch 851, loss: 0.385, 27232/60000 datapoints
2025-03-06 18:21:57,181 - INFO - training batch 901, loss: 0.178, 28832/60000 datapoints
2025-03-06 18:21:57,376 - INFO - training batch 951, loss: 0.378, 30432/60000 datapoints
2025-03-06 18:21:57,571 - INFO - training batch 1001, loss: 0.165, 32032/60000 datapoints
2025-03-06 18:21:57,767 - INFO - training batch 1051, loss: 0.351, 33632/60000 datapoints
2025-03-06 18:21:57,960 - INFO - training batch 1101, loss: 0.331, 35232/60000 datapoints
2025-03-06 18:21:58,173 - INFO - training batch 1151, loss: 0.189, 36832/60000 datapoints
2025-03-06 18:21:58,366 - INFO - training batch 1201, loss: 0.169, 38432/60000 datapoints
2025-03-06 18:21:58,562 - INFO - training batch 1251, loss: 0.512, 40032/60000 datapoints
2025-03-06 18:21:58,758 - INFO - training batch 1301, loss: 0.301, 41632/60000 datapoints
2025-03-06 18:21:58,955 - INFO - training batch 1351, loss: 0.270, 43232/60000 datapoints
2025-03-06 18:21:59,154 - INFO - training batch 1401, loss: 0.446, 44832/60000 datapoints
2025-03-06 18:21:59,354 - INFO - training batch 1451, loss: 0.498, 46432/60000 datapoints
2025-03-06 18:21:59,552 - INFO - training batch 1501, loss: 0.505, 48032/60000 datapoints
2025-03-06 18:21:59,747 - INFO - training batch 1551, loss: 0.538, 49632/60000 datapoints
2025-03-06 18:21:59,944 - INFO - training batch 1601, loss: 0.299, 51232/60000 datapoints
2025-03-06 18:22:00,139 - INFO - training batch 1651, loss: 0.311, 52832/60000 datapoints
2025-03-06 18:22:00,334 - INFO - training batch 1701, loss: 0.208, 54432/60000 datapoints
2025-03-06 18:22:00,531 - INFO - training batch 1751, loss: 0.203, 56032/60000 datapoints
2025-03-06 18:22:00,726 - INFO - training batch 1801, loss: 0.336, 57632/60000 datapoints
2025-03-06 18:22:00,920 - INFO - training batch 1851, loss: 0.246, 59232/60000 datapoints
2025-03-06 18:22:01,022 - INFO - validation batch 1, loss: 0.298, 32/10016 datapoints
2025-03-06 18:22:01,172 - INFO - validation batch 51, loss: 0.326, 1632/10016 datapoints
2025-03-06 18:22:01,325 - INFO - validation batch 101, loss: 0.370, 3232/10016 datapoints
2025-03-06 18:22:01,476 - INFO - validation batch 151, loss: 0.324, 4832/10016 datapoints
2025-03-06 18:22:01,633 - INFO - validation batch 201, loss: 0.203, 6432/10016 datapoints
2025-03-06 18:22:01,786 - INFO - validation batch 251, loss: 0.340, 8032/10016 datapoints
2025-03-06 18:22:01,939 - INFO - validation batch 301, loss: 0.502, 9632/10016 datapoints
2025-03-06 18:22:01,977 - INFO - Epoch 273/800 done.
2025-03-06 18:22:01,978 - INFO - Final validation performance:
Loss: 0.338, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:22:01,978 - INFO - Beginning epoch 274/800
2025-03-06 18:22:01,984 - INFO - training batch 1, loss: 0.633, 32/60000 datapoints
2025-03-06 18:22:02,179 - INFO - training batch 51, loss: 0.088, 1632/60000 datapoints
2025-03-06 18:22:02,372 - INFO - training batch 101, loss: 0.301, 3232/60000 datapoints
2025-03-06 18:22:02,568 - INFO - training batch 151, loss: 0.170, 4832/60000 datapoints
2025-03-06 18:22:02,764 - INFO - training batch 201, loss: 0.381, 6432/60000 datapoints
2025-03-06 18:22:02,957 - INFO - training batch 251, loss: 0.284, 8032/60000 datapoints
2025-03-06 18:22:03,151 - INFO - training batch 301, loss: 0.332, 9632/60000 datapoints
2025-03-06 18:22:03,343 - INFO - training batch 351, loss: 0.300, 11232/60000 datapoints
2025-03-06 18:22:03,539 - INFO - training batch 401, loss: 0.227, 12832/60000 datapoints
2025-03-06 18:22:03,734 - INFO - training batch 451, loss: 0.228, 14432/60000 datapoints
2025-03-06 18:22:03,928 - INFO - training batch 501, loss: 0.305, 16032/60000 datapoints
2025-03-06 18:22:04,121 - INFO - training batch 551, loss: 0.347, 17632/60000 datapoints
2025-03-06 18:22:04,316 - INFO - training batch 601, loss: 0.188, 19232/60000 datapoints
2025-03-06 18:22:04,508 - INFO - training batch 651, loss: 0.352, 20832/60000 datapoints
2025-03-06 18:22:04,705 - INFO - training batch 701, loss: 0.335, 22432/60000 datapoints
2025-03-06 18:22:04,903 - INFO - training batch 751, loss: 0.394, 24032/60000 datapoints
2025-03-06 18:22:05,099 - INFO - training batch 801, loss: 0.464, 25632/60000 datapoints
2025-03-06 18:22:05,292 - INFO - training batch 851, loss: 0.147, 27232/60000 datapoints
2025-03-06 18:22:05,489 - INFO - training batch 901, loss: 0.407, 28832/60000 datapoints
2025-03-06 18:22:05,687 - INFO - training batch 951, loss: 0.202, 30432/60000 datapoints
2025-03-06 18:22:05,881 - INFO - training batch 1001, loss: 0.381, 32032/60000 datapoints
2025-03-06 18:22:06,073 - INFO - training batch 1051, loss: 0.212, 33632/60000 datapoints
2025-03-06 18:22:06,271 - INFO - training batch 1101, loss: 0.200, 35232/60000 datapoints
2025-03-06 18:22:06,465 - INFO - training batch 1151, loss: 0.127, 36832/60000 datapoints
2025-03-06 18:22:06,666 - INFO - training batch 1201, loss: 0.545, 38432/60000 datapoints
2025-03-06 18:22:06,859 - INFO - training batch 1251, loss: 0.347, 40032/60000 datapoints
2025-03-06 18:22:07,053 - INFO - training batch 1301, loss: 0.473, 41632/60000 datapoints
2025-03-06 18:22:07,253 - INFO - training batch 1351, loss: 0.212, 43232/60000 datapoints
2025-03-06 18:22:07,445 - INFO - training batch 1401, loss: 0.382, 44832/60000 datapoints
2025-03-06 18:22:07,643 - INFO - training batch 1451, loss: 0.201, 46432/60000 datapoints
2025-03-06 18:22:07,837 - INFO - training batch 1501, loss: 0.406, 48032/60000 datapoints
2025-03-06 18:22:08,037 - INFO - training batch 1551, loss: 0.495, 49632/60000 datapoints
2025-03-06 18:22:08,261 - INFO - training batch 1601, loss: 0.299, 51232/60000 datapoints
2025-03-06 18:22:08,454 - INFO - training batch 1651, loss: 0.638, 52832/60000 datapoints
2025-03-06 18:22:08,652 - INFO - training batch 1701, loss: 0.665, 54432/60000 datapoints
2025-03-06 18:22:08,847 - INFO - training batch 1751, loss: 0.324, 56032/60000 datapoints
2025-03-06 18:22:09,044 - INFO - training batch 1801, loss: 0.360, 57632/60000 datapoints
2025-03-06 18:22:09,238 - INFO - training batch 1851, loss: 0.271, 59232/60000 datapoints
2025-03-06 18:22:09,339 - INFO - validation batch 1, loss: 0.719, 32/10016 datapoints
2025-03-06 18:22:09,493 - INFO - validation batch 51, loss: 0.135, 1632/10016 datapoints
2025-03-06 18:22:09,651 - INFO - validation batch 101, loss: 0.318, 3232/10016 datapoints
2025-03-06 18:22:09,805 - INFO - validation batch 151, loss: 0.644, 4832/10016 datapoints
2025-03-06 18:22:09,956 - INFO - validation batch 201, loss: 0.182, 6432/10016 datapoints
2025-03-06 18:22:10,112 - INFO - validation batch 251, loss: 0.311, 8032/10016 datapoints
2025-03-06 18:22:10,266 - INFO - validation batch 301, loss: 0.284, 9632/10016 datapoints
2025-03-06 18:22:10,302 - INFO - Epoch 274/800 done.
2025-03-06 18:22:10,302 - INFO - Final validation performance:
Loss: 0.370, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 18:22:10,302 - INFO - Beginning epoch 275/800
2025-03-06 18:22:10,309 - INFO - training batch 1, loss: 0.198, 32/60000 datapoints
2025-03-06 18:22:10,510 - INFO - training batch 51, loss: 0.209, 1632/60000 datapoints
2025-03-06 18:22:10,710 - INFO - training batch 101, loss: 0.223, 3232/60000 datapoints
2025-03-06 18:22:10,907 - INFO - training batch 151, loss: 0.159, 4832/60000 datapoints
2025-03-06 18:22:11,103 - INFO - training batch 201, loss: 0.183, 6432/60000 datapoints
2025-03-06 18:22:11,299 - INFO - training batch 251, loss: 0.517, 8032/60000 datapoints
2025-03-06 18:22:11,494 - INFO - training batch 301, loss: 0.361, 9632/60000 datapoints
2025-03-06 18:22:11,694 - INFO - training batch 351, loss: 0.144, 11232/60000 datapoints
2025-03-06 18:22:11,889 - INFO - training batch 401, loss: 0.291, 12832/60000 datapoints
2025-03-06 18:22:12,089 - INFO - training batch 451, loss: 0.302, 14432/60000 datapoints
2025-03-06 18:22:12,288 - INFO - training batch 501, loss: 0.267, 16032/60000 datapoints
2025-03-06 18:22:12,487 - INFO - training batch 551, loss: 0.095, 17632/60000 datapoints
2025-03-06 18:22:12,685 - INFO - training batch 601, loss: 0.357, 19232/60000 datapoints
2025-03-06 18:22:12,881 - INFO - training batch 651, loss: 0.237, 20832/60000 datapoints
2025-03-06 18:22:13,077 - INFO - training batch 701, loss: 0.239, 22432/60000 datapoints
2025-03-06 18:22:13,272 - INFO - training batch 751, loss: 0.575, 24032/60000 datapoints
2025-03-06 18:22:13,464 - INFO - training batch 801, loss: 0.339, 25632/60000 datapoints
2025-03-06 18:22:13,663 - INFO - training batch 851, loss: 0.673, 27232/60000 datapoints
2025-03-06 18:22:13,859 - INFO - training batch 901, loss: 0.293, 28832/60000 datapoints
2025-03-06 18:22:14,054 - INFO - training batch 951, loss: 0.439, 30432/60000 datapoints
2025-03-06 18:22:14,269 - INFO - training batch 1001, loss: 0.324, 32032/60000 datapoints
2025-03-06 18:22:14,485 - INFO - training batch 1051, loss: 0.192, 33632/60000 datapoints
2025-03-06 18:22:14,712 - INFO - training batch 1101, loss: 0.257, 35232/60000 datapoints
2025-03-06 18:22:14,919 - INFO - training batch 1151, loss: 0.200, 36832/60000 datapoints
2025-03-06 18:22:15,114 - INFO - training batch 1201, loss: 0.490, 38432/60000 datapoints
2025-03-06 18:22:15,308 - INFO - training batch 1251, loss: 0.207, 40032/60000 datapoints
2025-03-06 18:22:15,502 - INFO - training batch 1301, loss: 0.321, 41632/60000 datapoints
2025-03-06 18:22:15,702 - INFO - training batch 1351, loss: 0.255, 43232/60000 datapoints
2025-03-06 18:22:15,895 - INFO - training batch 1401, loss: 0.432, 44832/60000 datapoints
2025-03-06 18:22:16,092 - INFO - training batch 1451, loss: 0.482, 46432/60000 datapoints
2025-03-06 18:22:16,290 - INFO - training batch 1501, loss: 0.191, 48032/60000 datapoints
2025-03-06 18:22:16,483 - INFO - training batch 1551, loss: 0.363, 49632/60000 datapoints
2025-03-06 18:22:16,678 - INFO - training batch 1601, loss: 0.459, 51232/60000 datapoints
2025-03-06 18:22:16,871 - INFO - training batch 1651, loss: 0.194, 52832/60000 datapoints
2025-03-06 18:22:17,065 - INFO - training batch 1701, loss: 0.296, 54432/60000 datapoints
2025-03-06 18:22:17,260 - INFO - training batch 1751, loss: 0.298, 56032/60000 datapoints
2025-03-06 18:22:17,454 - INFO - training batch 1801, loss: 0.424, 57632/60000 datapoints
2025-03-06 18:22:17,655 - INFO - training batch 1851, loss: 0.232, 59232/60000 datapoints
2025-03-06 18:22:17,757 - INFO - validation batch 1, loss: 0.310, 32/10016 datapoints
2025-03-06 18:22:17,910 - INFO - validation batch 51, loss: 0.139, 1632/10016 datapoints
2025-03-06 18:22:18,062 - INFO - validation batch 101, loss: 0.599, 3232/10016 datapoints
2025-03-06 18:22:18,237 - INFO - validation batch 151, loss: 0.345, 4832/10016 datapoints
2025-03-06 18:22:18,390 - INFO - validation batch 201, loss: 0.282, 6432/10016 datapoints
2025-03-06 18:22:18,542 - INFO - validation batch 251, loss: 0.187, 8032/10016 datapoints
2025-03-06 18:22:18,698 - INFO - validation batch 301, loss: 0.246, 9632/10016 datapoints
2025-03-06 18:22:18,735 - INFO - Epoch 275/800 done.
2025-03-06 18:22:18,736 - INFO - Final validation performance:
Loss: 0.301, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:22:18,736 - INFO - Beginning epoch 276/800
2025-03-06 18:22:18,742 - INFO - training batch 1, loss: 0.129, 32/60000 datapoints
2025-03-06 18:22:18,938 - INFO - training batch 51, loss: 0.367, 1632/60000 datapoints
2025-03-06 18:22:19,130 - INFO - training batch 101, loss: 0.284, 3232/60000 datapoints
2025-03-06 18:22:19,328 - INFO - training batch 151, loss: 0.454, 4832/60000 datapoints
2025-03-06 18:22:19,522 - INFO - training batch 201, loss: 0.444, 6432/60000 datapoints
2025-03-06 18:22:19,723 - INFO - training batch 251, loss: 0.213, 8032/60000 datapoints
2025-03-06 18:22:19,918 - INFO - training batch 301, loss: 0.243, 9632/60000 datapoints
2025-03-06 18:22:20,112 - INFO - training batch 351, loss: 0.164, 11232/60000 datapoints
2025-03-06 18:22:20,310 - INFO - training batch 401, loss: 0.528, 12832/60000 datapoints
2025-03-06 18:22:20,507 - INFO - training batch 451, loss: 0.303, 14432/60000 datapoints
2025-03-06 18:22:20,709 - INFO - training batch 501, loss: 0.193, 16032/60000 datapoints
2025-03-06 18:22:20,903 - INFO - training batch 551, loss: 0.423, 17632/60000 datapoints
2025-03-06 18:22:21,097 - INFO - training batch 601, loss: 0.292, 19232/60000 datapoints
2025-03-06 18:22:21,292 - INFO - training batch 651, loss: 0.423, 20832/60000 datapoints
2025-03-06 18:22:21,486 - INFO - training batch 701, loss: 0.192, 22432/60000 datapoints
2025-03-06 18:22:21,685 - INFO - training batch 751, loss: 0.326, 24032/60000 datapoints
2025-03-06 18:22:21,878 - INFO - training batch 801, loss: 0.566, 25632/60000 datapoints
2025-03-06 18:22:22,072 - INFO - training batch 851, loss: 0.403, 27232/60000 datapoints
2025-03-06 18:22:22,269 - INFO - training batch 901, loss: 0.115, 28832/60000 datapoints
2025-03-06 18:22:22,463 - INFO - training batch 951, loss: 0.246, 30432/60000 datapoints
2025-03-06 18:22:22,658 - INFO - training batch 1001, loss: 0.254, 32032/60000 datapoints
2025-03-06 18:22:22,854 - INFO - training batch 1051, loss: 0.073, 33632/60000 datapoints
2025-03-06 18:22:23,049 - INFO - training batch 1101, loss: 0.525, 35232/60000 datapoints
2025-03-06 18:22:23,245 - INFO - training batch 1151, loss: 0.222, 36832/60000 datapoints
2025-03-06 18:22:23,439 - INFO - training batch 1201, loss: 0.263, 38432/60000 datapoints
2025-03-06 18:22:23,639 - INFO - training batch 1251, loss: 0.310, 40032/60000 datapoints
2025-03-06 18:22:23,840 - INFO - training batch 1301, loss: 0.326, 41632/60000 datapoints
2025-03-06 18:22:24,032 - INFO - training batch 1351, loss: 0.378, 43232/60000 datapoints
2025-03-06 18:22:24,226 - INFO - training batch 1401, loss: 0.152, 44832/60000 datapoints
2025-03-06 18:22:24,421 - INFO - training batch 1451, loss: 0.289, 46432/60000 datapoints
2025-03-06 18:22:24,617 - INFO - training batch 1501, loss: 0.362, 48032/60000 datapoints
2025-03-06 18:22:24,813 - INFO - training batch 1551, loss: 0.671, 49632/60000 datapoints
2025-03-06 18:22:25,013 - INFO - training batch 1601, loss: 0.143, 51232/60000 datapoints
2025-03-06 18:22:25,209 - INFO - training batch 1651, loss: 0.196, 52832/60000 datapoints
2025-03-06 18:22:25,402 - INFO - training batch 1701, loss: 0.155, 54432/60000 datapoints
2025-03-06 18:22:25,599 - INFO - training batch 1751, loss: 0.538, 56032/60000 datapoints
2025-03-06 18:22:25,795 - INFO - training batch 1801, loss: 0.355, 57632/60000 datapoints
2025-03-06 18:22:25,990 - INFO - training batch 1851, loss: 0.564, 59232/60000 datapoints
2025-03-06 18:22:26,090 - INFO - validation batch 1, loss: 0.557, 32/10016 datapoints
2025-03-06 18:22:26,247 - INFO - validation batch 51, loss: 0.446, 1632/10016 datapoints
2025-03-06 18:22:26,400 - INFO - validation batch 101, loss: 0.288, 3232/10016 datapoints
2025-03-06 18:22:26,552 - INFO - validation batch 151, loss: 0.503, 4832/10016 datapoints
2025-03-06 18:22:26,704 - INFO - validation batch 201, loss: 0.191, 6432/10016 datapoints
2025-03-06 18:22:26,858 - INFO - validation batch 251, loss: 0.311, 8032/10016 datapoints
2025-03-06 18:22:27,011 - INFO - validation batch 301, loss: 0.088, 9632/10016 datapoints
2025-03-06 18:22:27,047 - INFO - Epoch 276/800 done.
2025-03-06 18:22:27,048 - INFO - Final validation performance:
Loss: 0.341, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:22:27,048 - INFO - Beginning epoch 277/800
2025-03-06 18:22:27,055 - INFO - training batch 1, loss: 0.226, 32/60000 datapoints
2025-03-06 18:22:27,256 - INFO - training batch 51, loss: 0.391, 1632/60000 datapoints
2025-03-06 18:22:27,448 - INFO - training batch 101, loss: 0.412, 3232/60000 datapoints
2025-03-06 18:22:27,649 - INFO - training batch 151, loss: 0.395, 4832/60000 datapoints
2025-03-06 18:22:27,842 - INFO - training batch 201, loss: 0.281, 6432/60000 datapoints
2025-03-06 18:22:28,036 - INFO - training batch 251, loss: 0.219, 8032/60000 datapoints
2025-03-06 18:22:28,229 - INFO - training batch 301, loss: 0.313, 9632/60000 datapoints
2025-03-06 18:22:28,442 - INFO - training batch 351, loss: 0.225, 11232/60000 datapoints
2025-03-06 18:22:28,639 - INFO - training batch 401, loss: 0.543, 12832/60000 datapoints
2025-03-06 18:22:28,832 - INFO - training batch 451, loss: 0.275, 14432/60000 datapoints
2025-03-06 18:22:29,025 - INFO - training batch 501, loss: 0.707, 16032/60000 datapoints
2025-03-06 18:22:29,220 - INFO - training batch 551, loss: 0.424, 17632/60000 datapoints
2025-03-06 18:22:29,415 - INFO - training batch 601, loss: 0.430, 19232/60000 datapoints
2025-03-06 18:22:29,612 - INFO - training batch 651, loss: 0.362, 20832/60000 datapoints
2025-03-06 18:22:29,813 - INFO - training batch 701, loss: 0.218, 22432/60000 datapoints
2025-03-06 18:22:30,013 - INFO - training batch 751, loss: 0.437, 24032/60000 datapoints
2025-03-06 18:22:30,209 - INFO - training batch 801, loss: 0.238, 25632/60000 datapoints
2025-03-06 18:22:30,405 - INFO - training batch 851, loss: 0.691, 27232/60000 datapoints
2025-03-06 18:22:30,603 - INFO - training batch 901, loss: 0.315, 28832/60000 datapoints
2025-03-06 18:22:30,799 - INFO - training batch 951, loss: 0.434, 30432/60000 datapoints
2025-03-06 18:22:30,992 - INFO - training batch 1001, loss: 0.176, 32032/60000 datapoints
2025-03-06 18:22:31,185 - INFO - training batch 1051, loss: 0.152, 33632/60000 datapoints
2025-03-06 18:22:31,380 - INFO - training batch 1101, loss: 0.338, 35232/60000 datapoints
2025-03-06 18:22:31,573 - INFO - training batch 1151, loss: 0.227, 36832/60000 datapoints
2025-03-06 18:22:31,773 - INFO - training batch 1201, loss: 0.342, 38432/60000 datapoints
2025-03-06 18:22:31,968 - INFO - training batch 1251, loss: 0.322, 40032/60000 datapoints
2025-03-06 18:22:32,161 - INFO - training batch 1301, loss: 0.383, 41632/60000 datapoints
2025-03-06 18:22:32,355 - INFO - training batch 1351, loss: 0.250, 43232/60000 datapoints
2025-03-06 18:22:32,547 - INFO - training batch 1401, loss: 0.447, 44832/60000 datapoints
2025-03-06 18:22:32,743 - INFO - training batch 1451, loss: 0.336, 46432/60000 datapoints
2025-03-06 18:22:32,938 - INFO - training batch 1501, loss: 0.421, 48032/60000 datapoints
2025-03-06 18:22:33,131 - INFO - training batch 1551, loss: 0.424, 49632/60000 datapoints
2025-03-06 18:22:33,324 - INFO - training batch 1601, loss: 0.357, 51232/60000 datapoints
2025-03-06 18:22:33,517 - INFO - training batch 1651, loss: 0.538, 52832/60000 datapoints
2025-03-06 18:22:33,717 - INFO - training batch 1701, loss: 0.272, 54432/60000 datapoints
2025-03-06 18:22:33,912 - INFO - training batch 1751, loss: 0.434, 56032/60000 datapoints
2025-03-06 18:22:34,107 - INFO - training batch 1801, loss: 0.208, 57632/60000 datapoints
2025-03-06 18:22:34,301 - INFO - training batch 1851, loss: 0.452, 59232/60000 datapoints
2025-03-06 18:22:34,403 - INFO - validation batch 1, loss: 0.153, 32/10016 datapoints
2025-03-06 18:22:34,556 - INFO - validation batch 51, loss: 0.162, 1632/10016 datapoints
2025-03-06 18:22:34,711 - INFO - validation batch 101, loss: 0.248, 3232/10016 datapoints
2025-03-06 18:22:34,867 - INFO - validation batch 151, loss: 0.488, 4832/10016 datapoints
2025-03-06 18:22:35,019 - INFO - validation batch 201, loss: 0.159, 6432/10016 datapoints
2025-03-06 18:22:35,175 - INFO - validation batch 251, loss: 0.222, 8032/10016 datapoints
2025-03-06 18:22:35,329 - INFO - validation batch 301, loss: 0.313, 9632/10016 datapoints
2025-03-06 18:22:35,369 - INFO - Epoch 277/800 done.
2025-03-06 18:22:35,369 - INFO - Final validation performance:
Loss: 0.249, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:22:35,369 - INFO - Beginning epoch 278/800
2025-03-06 18:22:35,375 - INFO - training batch 1, loss: 0.238, 32/60000 datapoints
2025-03-06 18:22:35,571 - INFO - training batch 51, loss: 0.155, 1632/60000 datapoints
2025-03-06 18:22:35,771 - INFO - training batch 101, loss: 0.221, 3232/60000 datapoints
2025-03-06 18:22:35,971 - INFO - training batch 151, loss: 0.479, 4832/60000 datapoints
2025-03-06 18:22:36,166 - INFO - training batch 201, loss: 0.492, 6432/60000 datapoints
2025-03-06 18:22:36,363 - INFO - training batch 251, loss: 0.375, 8032/60000 datapoints
2025-03-06 18:22:36,557 - INFO - training batch 301, loss: 0.271, 9632/60000 datapoints
2025-03-06 18:22:36,754 - INFO - training batch 351, loss: 0.367, 11232/60000 datapoints
2025-03-06 18:22:36,948 - INFO - training batch 401, loss: 0.341, 12832/60000 datapoints
2025-03-06 18:22:37,143 - INFO - training batch 451, loss: 0.276, 14432/60000 datapoints
2025-03-06 18:22:37,340 - INFO - training batch 501, loss: 0.115, 16032/60000 datapoints
2025-03-06 18:22:37,540 - INFO - training batch 551, loss: 0.312, 17632/60000 datapoints
2025-03-06 18:22:37,754 - INFO - training batch 601, loss: 0.434, 19232/60000 datapoints
2025-03-06 18:22:37,947 - INFO - training batch 651, loss: 0.538, 20832/60000 datapoints
2025-03-06 18:22:38,141 - INFO - training batch 701, loss: 0.506, 22432/60000 datapoints
2025-03-06 18:22:38,340 - INFO - training batch 751, loss: 0.258, 24032/60000 datapoints
2025-03-06 18:22:38,548 - INFO - training batch 801, loss: 0.309, 25632/60000 datapoints
2025-03-06 18:22:38,743 - INFO - training batch 851, loss: 0.252, 27232/60000 datapoints
2025-03-06 18:22:38,937 - INFO - training batch 901, loss: 0.162, 28832/60000 datapoints
2025-03-06 18:22:39,134 - INFO - training batch 951, loss: 0.269, 30432/60000 datapoints
2025-03-06 18:22:39,340 - INFO - training batch 1001, loss: 0.261, 32032/60000 datapoints
2025-03-06 18:22:39,537 - INFO - training batch 1051, loss: 0.193, 33632/60000 datapoints
2025-03-06 18:22:39,739 - INFO - training batch 1101, loss: 0.218, 35232/60000 datapoints
2025-03-06 18:22:39,935 - INFO - training batch 1151, loss: 0.340, 36832/60000 datapoints
2025-03-06 18:22:40,127 - INFO - training batch 1201, loss: 0.221, 38432/60000 datapoints
2025-03-06 18:22:40,353 - INFO - training batch 1251, loss: 0.176, 40032/60000 datapoints
2025-03-06 18:22:40,547 - INFO - training batch 1301, loss: 0.216, 41632/60000 datapoints
2025-03-06 18:22:40,762 - INFO - training batch 1351, loss: 0.533, 43232/60000 datapoints
2025-03-06 18:22:40,954 - INFO - training batch 1401, loss: 0.184, 44832/60000 datapoints
2025-03-06 18:22:41,147 - INFO - training batch 1451, loss: 0.265, 46432/60000 datapoints
2025-03-06 18:22:41,345 - INFO - training batch 1501, loss: 0.224, 48032/60000 datapoints
2025-03-06 18:22:41,537 - INFO - training batch 1551, loss: 0.203, 49632/60000 datapoints
2025-03-06 18:22:41,736 - INFO - training batch 1601, loss: 0.159, 51232/60000 datapoints
2025-03-06 18:22:41,931 - INFO - training batch 1651, loss: 0.146, 52832/60000 datapoints
2025-03-06 18:22:42,122 - INFO - training batch 1701, loss: 0.200, 54432/60000 datapoints
2025-03-06 18:22:42,317 - INFO - training batch 1751, loss: 0.447, 56032/60000 datapoints
2025-03-06 18:22:42,510 - INFO - training batch 1801, loss: 0.067, 57632/60000 datapoints
2025-03-06 18:22:42,704 - INFO - training batch 1851, loss: 0.153, 59232/60000 datapoints
2025-03-06 18:22:42,803 - INFO - validation batch 1, loss: 0.355, 32/10016 datapoints
2025-03-06 18:22:42,956 - INFO - validation batch 51, loss: 0.238, 1632/10016 datapoints
2025-03-06 18:22:43,109 - INFO - validation batch 101, loss: 0.255, 3232/10016 datapoints
2025-03-06 18:22:43,261 - INFO - validation batch 151, loss: 0.396, 4832/10016 datapoints
2025-03-06 18:22:43,414 - INFO - validation batch 201, loss: 0.218, 6432/10016 datapoints
2025-03-06 18:22:43,567 - INFO - validation batch 251, loss: 0.267, 8032/10016 datapoints
2025-03-06 18:22:43,725 - INFO - validation batch 301, loss: 0.258, 9632/10016 datapoints
2025-03-06 18:22:43,763 - INFO - Epoch 278/800 done.
2025-03-06 18:22:43,763 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:22:43,763 - INFO - Beginning epoch 279/800
2025-03-06 18:22:43,771 - INFO - training batch 1, loss: 0.382, 32/60000 datapoints
2025-03-06 18:22:43,964 - INFO - training batch 51, loss: 0.343, 1632/60000 datapoints
2025-03-06 18:22:44,154 - INFO - training batch 101, loss: 0.511, 3232/60000 datapoints
2025-03-06 18:22:44,346 - INFO - training batch 151, loss: 0.238, 4832/60000 datapoints
2025-03-06 18:22:44,537 - INFO - training batch 201, loss: 0.264, 6432/60000 datapoints
2025-03-06 18:22:44,734 - INFO - training batch 251, loss: 0.397, 8032/60000 datapoints
2025-03-06 18:22:44,932 - INFO - training batch 301, loss: 0.270, 9632/60000 datapoints
2025-03-06 18:22:45,124 - INFO - training batch 351, loss: 0.419, 11232/60000 datapoints
2025-03-06 18:22:45,322 - INFO - training batch 401, loss: 0.632, 12832/60000 datapoints
2025-03-06 18:22:45,513 - INFO - training batch 451, loss: 0.276, 14432/60000 datapoints
2025-03-06 18:22:45,781 - INFO - training batch 501, loss: 0.150, 16032/60000 datapoints
2025-03-06 18:22:45,972 - INFO - training batch 551, loss: 0.260, 17632/60000 datapoints
2025-03-06 18:22:46,167 - INFO - training batch 601, loss: 0.202, 19232/60000 datapoints
2025-03-06 18:22:46,360 - INFO - training batch 651, loss: 0.539, 20832/60000 datapoints
2025-03-06 18:22:46,553 - INFO - training batch 701, loss: 0.080, 22432/60000 datapoints
2025-03-06 18:22:46,747 - INFO - training batch 751, loss: 0.532, 24032/60000 datapoints
2025-03-06 18:22:46,937 - INFO - training batch 801, loss: 0.490, 25632/60000 datapoints
2025-03-06 18:22:47,128 - INFO - training batch 851, loss: 0.499, 27232/60000 datapoints
2025-03-06 18:22:47,344 - INFO - training batch 901, loss: 0.288, 28832/60000 datapoints
2025-03-06 18:22:47,544 - INFO - training batch 951, loss: 0.477, 30432/60000 datapoints
2025-03-06 18:22:47,740 - INFO - training batch 1001, loss: 0.381, 32032/60000 datapoints
2025-03-06 18:22:47,931 - INFO - training batch 1051, loss: 0.241, 33632/60000 datapoints
2025-03-06 18:22:48,124 - INFO - training batch 1101, loss: 0.214, 35232/60000 datapoints
2025-03-06 18:22:48,315 - INFO - training batch 1151, loss: 0.245, 36832/60000 datapoints
2025-03-06 18:22:48,529 - INFO - training batch 1201, loss: 0.210, 38432/60000 datapoints
2025-03-06 18:22:48,724 - INFO - training batch 1251, loss: 0.205, 40032/60000 datapoints
2025-03-06 18:22:48,915 - INFO - training batch 1301, loss: 0.156, 41632/60000 datapoints
2025-03-06 18:22:49,106 - INFO - training batch 1351, loss: 0.430, 43232/60000 datapoints
2025-03-06 18:22:49,308 - INFO - training batch 1401, loss: 0.442, 44832/60000 datapoints
2025-03-06 18:22:49,499 - INFO - training batch 1451, loss: 0.625, 46432/60000 datapoints
2025-03-06 18:22:49,697 - INFO - training batch 1501, loss: 0.330, 48032/60000 datapoints
2025-03-06 18:22:49,892 - INFO - training batch 1551, loss: 0.371, 49632/60000 datapoints
2025-03-06 18:22:50,085 - INFO - training batch 1601, loss: 0.153, 51232/60000 datapoints
2025-03-06 18:22:50,277 - INFO - training batch 1651, loss: 0.430, 52832/60000 datapoints
2025-03-06 18:22:50,469 - INFO - training batch 1701, loss: 0.410, 54432/60000 datapoints
2025-03-06 18:22:50,668 - INFO - training batch 1751, loss: 0.294, 56032/60000 datapoints
2025-03-06 18:22:50,858 - INFO - training batch 1801, loss: 0.489, 57632/60000 datapoints
2025-03-06 18:22:51,050 - INFO - training batch 1851, loss: 0.336, 59232/60000 datapoints
2025-03-06 18:22:51,148 - INFO - validation batch 1, loss: 0.183, 32/10016 datapoints
2025-03-06 18:22:51,303 - INFO - validation batch 51, loss: 0.262, 1632/10016 datapoints
2025-03-06 18:22:51,454 - INFO - validation batch 101, loss: 0.439, 3232/10016 datapoints
2025-03-06 18:22:51,609 - INFO - validation batch 151, loss: 0.323, 4832/10016 datapoints
2025-03-06 18:22:51,764 - INFO - validation batch 201, loss: 0.112, 6432/10016 datapoints
2025-03-06 18:22:51,916 - INFO - validation batch 251, loss: 0.497, 8032/10016 datapoints
2025-03-06 18:22:52,069 - INFO - validation batch 301, loss: 0.309, 9632/10016 datapoints
2025-03-06 18:22:52,104 - INFO - Epoch 279/800 done.
2025-03-06 18:22:52,105 - INFO - Final validation performance:
Loss: 0.304, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:22:52,105 - INFO - Beginning epoch 280/800
2025-03-06 18:22:52,111 - INFO - training batch 1, loss: 0.132, 32/60000 datapoints
2025-03-06 18:22:52,304 - INFO - training batch 51, loss: 0.349, 1632/60000 datapoints
2025-03-06 18:22:52,496 - INFO - training batch 101, loss: 0.295, 3232/60000 datapoints
2025-03-06 18:22:52,692 - INFO - training batch 151, loss: 0.456, 4832/60000 datapoints
2025-03-06 18:22:52,882 - INFO - training batch 201, loss: 0.356, 6432/60000 datapoints
2025-03-06 18:22:53,077 - INFO - training batch 251, loss: 0.149, 8032/60000 datapoints
2025-03-06 18:22:53,268 - INFO - training batch 301, loss: 0.313, 9632/60000 datapoints
2025-03-06 18:22:53,459 - INFO - training batch 351, loss: 0.535, 11232/60000 datapoints
2025-03-06 18:22:53,653 - INFO - training batch 401, loss: 0.412, 12832/60000 datapoints
2025-03-06 18:22:53,846 - INFO - training batch 451, loss: 0.305, 14432/60000 datapoints
2025-03-06 18:22:54,036 - INFO - training batch 501, loss: 0.168, 16032/60000 datapoints
2025-03-06 18:22:54,226 - INFO - training batch 551, loss: 0.217, 17632/60000 datapoints
2025-03-06 18:22:54,419 - INFO - training batch 601, loss: 0.205, 19232/60000 datapoints
2025-03-06 18:22:54,614 - INFO - training batch 651, loss: 0.289, 20832/60000 datapoints
2025-03-06 18:22:54,815 - INFO - training batch 701, loss: 0.407, 22432/60000 datapoints
2025-03-06 18:22:55,008 - INFO - training batch 751, loss: 0.241, 24032/60000 datapoints
2025-03-06 18:22:55,200 - INFO - training batch 801, loss: 0.147, 25632/60000 datapoints
2025-03-06 18:22:55,393 - INFO - training batch 851, loss: 0.298, 27232/60000 datapoints
2025-03-06 18:22:55,586 - INFO - training batch 901, loss: 0.161, 28832/60000 datapoints
2025-03-06 18:22:55,784 - INFO - training batch 951, loss: 0.244, 30432/60000 datapoints
2025-03-06 18:22:55,975 - INFO - training batch 1001, loss: 0.284, 32032/60000 datapoints
2025-03-06 18:22:56,170 - INFO - training batch 1051, loss: 0.168, 33632/60000 datapoints
2025-03-06 18:22:56,361 - INFO - training batch 1101, loss: 0.420, 35232/60000 datapoints
2025-03-06 18:22:56,553 - INFO - training batch 1151, loss: 0.366, 36832/60000 datapoints
2025-03-06 18:22:56,747 - INFO - training batch 1201, loss: 0.232, 38432/60000 datapoints
2025-03-06 18:22:56,937 - INFO - training batch 1251, loss: 0.162, 40032/60000 datapoints
2025-03-06 18:22:57,128 - INFO - training batch 1301, loss: 0.537, 41632/60000 datapoints
2025-03-06 18:22:57,319 - INFO - training batch 1351, loss: 0.465, 43232/60000 datapoints
2025-03-06 18:22:57,509 - INFO - training batch 1401, loss: 0.547, 44832/60000 datapoints
2025-03-06 18:22:57,704 - INFO - training batch 1451, loss: 0.246, 46432/60000 datapoints
2025-03-06 18:22:57,904 - INFO - training batch 1501, loss: 0.156, 48032/60000 datapoints
2025-03-06 18:22:58,099 - INFO - training batch 1551, loss: 0.292, 49632/60000 datapoints
2025-03-06 18:22:58,289 - INFO - training batch 1601, loss: 0.148, 51232/60000 datapoints
2025-03-06 18:22:58,483 - INFO - training batch 1651, loss: 0.384, 52832/60000 datapoints
2025-03-06 18:22:58,695 - INFO - training batch 1701, loss: 0.601, 54432/60000 datapoints
2025-03-06 18:22:58,886 - INFO - training batch 1751, loss: 0.355, 56032/60000 datapoints
2025-03-06 18:22:59,079 - INFO - training batch 1801, loss: 0.148, 57632/60000 datapoints
2025-03-06 18:22:59,273 - INFO - training batch 1851, loss: 0.285, 59232/60000 datapoints
2025-03-06 18:22:59,373 - INFO - validation batch 1, loss: 0.129, 32/10016 datapoints
2025-03-06 18:22:59,522 - INFO - validation batch 51, loss: 0.260, 1632/10016 datapoints
2025-03-06 18:22:59,674 - INFO - validation batch 101, loss: 0.303, 3232/10016 datapoints
2025-03-06 18:22:59,826 - INFO - validation batch 151, loss: 0.409, 4832/10016 datapoints
2025-03-06 18:22:59,976 - INFO - validation batch 201, loss: 0.231, 6432/10016 datapoints
2025-03-06 18:23:00,127 - INFO - validation batch 251, loss: 0.540, 8032/10016 datapoints
2025-03-06 18:23:00,279 - INFO - validation batch 301, loss: 0.404, 9632/10016 datapoints
2025-03-06 18:23:00,316 - INFO - Epoch 280/800 done.
2025-03-06 18:23:00,316 - INFO - Final validation performance:
Loss: 0.325, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:23:00,316 - INFO - Beginning epoch 281/800
2025-03-06 18:23:00,322 - INFO - training batch 1, loss: 0.358, 32/60000 datapoints
2025-03-06 18:23:00,513 - INFO - training batch 51, loss: 0.265, 1632/60000 datapoints
2025-03-06 18:23:00,716 - INFO - training batch 101, loss: 0.363, 3232/60000 datapoints
2025-03-06 18:23:00,911 - INFO - training batch 151, loss: 0.264, 4832/60000 datapoints
2025-03-06 18:23:01,101 - INFO - training batch 201, loss: 0.293, 6432/60000 datapoints
2025-03-06 18:23:01,293 - INFO - training batch 251, loss: 0.405, 8032/60000 datapoints
2025-03-06 18:23:01,485 - INFO - training batch 301, loss: 0.156, 9632/60000 datapoints
2025-03-06 18:23:01,677 - INFO - training batch 351, loss: 0.393, 11232/60000 datapoints
2025-03-06 18:23:01,871 - INFO - training batch 401, loss: 0.269, 12832/60000 datapoints
2025-03-06 18:23:02,061 - INFO - training batch 451, loss: 0.424, 14432/60000 datapoints
2025-03-06 18:23:02,251 - INFO - training batch 501, loss: 0.194, 16032/60000 datapoints
2025-03-06 18:23:02,442 - INFO - training batch 551, loss: 0.173, 17632/60000 datapoints
2025-03-06 18:23:02,634 - INFO - training batch 601, loss: 0.241, 19232/60000 datapoints
2025-03-06 18:23:02,826 - INFO - training batch 651, loss: 0.214, 20832/60000 datapoints
2025-03-06 18:23:03,018 - INFO - training batch 701, loss: 0.238, 22432/60000 datapoints
2025-03-06 18:23:03,209 - INFO - training batch 751, loss: 0.441, 24032/60000 datapoints
2025-03-06 18:23:03,403 - INFO - training batch 801, loss: 0.378, 25632/60000 datapoints
2025-03-06 18:23:03,594 - INFO - training batch 851, loss: 0.287, 27232/60000 datapoints
2025-03-06 18:23:03,792 - INFO - training batch 901, loss: 0.180, 28832/60000 datapoints
2025-03-06 18:23:03,983 - INFO - training batch 951, loss: 0.129, 30432/60000 datapoints
2025-03-06 18:23:04,176 - INFO - training batch 1001, loss: 0.195, 32032/60000 datapoints
2025-03-06 18:23:04,366 - INFO - training batch 1051, loss: 0.554, 33632/60000 datapoints
2025-03-06 18:23:04,558 - INFO - training batch 1101, loss: 0.277, 35232/60000 datapoints
2025-03-06 18:23:04,752 - INFO - training batch 1151, loss: 0.319, 36832/60000 datapoints
2025-03-06 18:23:04,949 - INFO - training batch 1201, loss: 0.148, 38432/60000 datapoints
2025-03-06 18:23:05,139 - INFO - training batch 1251, loss: 0.370, 40032/60000 datapoints
2025-03-06 18:23:05,332 - INFO - training batch 1301, loss: 0.467, 41632/60000 datapoints
2025-03-06 18:23:05,524 - INFO - training batch 1351, loss: 0.415, 43232/60000 datapoints
2025-03-06 18:23:05,718 - INFO - training batch 1401, loss: 0.378, 44832/60000 datapoints
2025-03-06 18:23:05,914 - INFO - training batch 1451, loss: 0.209, 46432/60000 datapoints
2025-03-06 18:23:06,106 - INFO - training batch 1501, loss: 0.120, 48032/60000 datapoints
2025-03-06 18:23:06,302 - INFO - training batch 1551, loss: 0.225, 49632/60000 datapoints
2025-03-06 18:23:06,494 - INFO - training batch 1601, loss: 0.271, 51232/60000 datapoints
2025-03-06 18:23:06,686 - INFO - training batch 1651, loss: 0.243, 52832/60000 datapoints
2025-03-06 18:23:06,879 - INFO - training batch 1701, loss: 0.389, 54432/60000 datapoints
2025-03-06 18:23:07,070 - INFO - training batch 1751, loss: 0.462, 56032/60000 datapoints
2025-03-06 18:23:07,262 - INFO - training batch 1801, loss: 0.216, 57632/60000 datapoints
2025-03-06 18:23:07,455 - INFO - training batch 1851, loss: 0.472, 59232/60000 datapoints
2025-03-06 18:23:07,553 - INFO - validation batch 1, loss: 0.324, 32/10016 datapoints
2025-03-06 18:23:07,708 - INFO - validation batch 51, loss: 0.395, 1632/10016 datapoints
2025-03-06 18:23:07,861 - INFO - validation batch 101, loss: 0.190, 3232/10016 datapoints
2025-03-06 18:23:08,013 - INFO - validation batch 151, loss: 0.291, 4832/10016 datapoints
2025-03-06 18:23:08,167 - INFO - validation batch 201, loss: 0.457, 6432/10016 datapoints
2025-03-06 18:23:08,321 - INFO - validation batch 251, loss: 0.088, 8032/10016 datapoints
2025-03-06 18:23:08,473 - INFO - validation batch 301, loss: 0.280, 9632/10016 datapoints
2025-03-06 18:23:08,508 - INFO - Epoch 281/800 done.
2025-03-06 18:23:08,509 - INFO - Final validation performance:
Loss: 0.289, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:23:08,509 - INFO - Beginning epoch 282/800
2025-03-06 18:23:08,515 - INFO - training batch 1, loss: 0.487, 32/60000 datapoints
2025-03-06 18:23:08,728 - INFO - training batch 51, loss: 0.092, 1632/60000 datapoints
2025-03-06 18:23:08,918 - INFO - training batch 101, loss: 0.366, 3232/60000 datapoints
2025-03-06 18:23:09,110 - INFO - training batch 151, loss: 0.073, 4832/60000 datapoints
2025-03-06 18:23:09,304 - INFO - training batch 201, loss: 0.269, 6432/60000 datapoints
2025-03-06 18:23:09,496 - INFO - training batch 251, loss: 0.589, 8032/60000 datapoints
2025-03-06 18:23:09,689 - INFO - training batch 301, loss: 0.291, 9632/60000 datapoints
2025-03-06 18:23:09,884 - INFO - training batch 351, loss: 0.168, 11232/60000 datapoints
2025-03-06 18:23:10,076 - INFO - training batch 401, loss: 0.177, 12832/60000 datapoints
2025-03-06 18:23:10,268 - INFO - training batch 451, loss: 0.274, 14432/60000 datapoints
2025-03-06 18:23:10,459 - INFO - training batch 501, loss: 0.409, 16032/60000 datapoints
2025-03-06 18:23:10,656 - INFO - training batch 551, loss: 0.272, 17632/60000 datapoints
2025-03-06 18:23:10,849 - INFO - training batch 601, loss: 0.371, 19232/60000 datapoints
2025-03-06 18:23:11,040 - INFO - training batch 651, loss: 0.134, 20832/60000 datapoints
2025-03-06 18:23:11,237 - INFO - training batch 701, loss: 0.399, 22432/60000 datapoints
2025-03-06 18:23:11,435 - INFO - training batch 751, loss: 0.540, 24032/60000 datapoints
2025-03-06 18:23:11,632 - INFO - training batch 801, loss: 0.268, 25632/60000 datapoints
2025-03-06 18:23:11,848 - INFO - training batch 851, loss: 0.367, 27232/60000 datapoints
2025-03-06 18:23:12,040 - INFO - training batch 901, loss: 0.213, 28832/60000 datapoints
2025-03-06 18:23:12,233 - INFO - training batch 951, loss: 0.649, 30432/60000 datapoints
2025-03-06 18:23:12,427 - INFO - training batch 1001, loss: 0.241, 32032/60000 datapoints
2025-03-06 18:23:12,623 - INFO - training batch 1051, loss: 0.335, 33632/60000 datapoints
2025-03-06 18:23:12,818 - INFO - training batch 1101, loss: 0.290, 35232/60000 datapoints
2025-03-06 18:23:13,011 - INFO - training batch 1151, loss: 0.269, 36832/60000 datapoints
2025-03-06 18:23:13,206 - INFO - training batch 1201, loss: 0.641, 38432/60000 datapoints
2025-03-06 18:23:13,401 - INFO - training batch 1251, loss: 0.292, 40032/60000 datapoints
2025-03-06 18:23:13,598 - INFO - training batch 1301, loss: 0.344, 41632/60000 datapoints
2025-03-06 18:23:13,797 - INFO - training batch 1351, loss: 0.201, 43232/60000 datapoints
2025-03-06 18:23:13,991 - INFO - training batch 1401, loss: 0.566, 44832/60000 datapoints
2025-03-06 18:23:14,186 - INFO - training batch 1451, loss: 0.369, 46432/60000 datapoints
2025-03-06 18:23:14,380 - INFO - training batch 1501, loss: 0.289, 48032/60000 datapoints
2025-03-06 18:23:14,572 - INFO - training batch 1551, loss: 0.307, 49632/60000 datapoints
2025-03-06 18:23:14,769 - INFO - training batch 1601, loss: 0.411, 51232/60000 datapoints
2025-03-06 18:23:14,968 - INFO - training batch 1651, loss: 0.577, 52832/60000 datapoints
2025-03-06 18:23:15,160 - INFO - training batch 1701, loss: 0.156, 54432/60000 datapoints
2025-03-06 18:23:15,356 - INFO - training batch 1751, loss: 0.356, 56032/60000 datapoints
2025-03-06 18:23:15,549 - INFO - training batch 1801, loss: 0.289, 57632/60000 datapoints
2025-03-06 18:23:15,748 - INFO - training batch 1851, loss: 0.510, 59232/60000 datapoints
2025-03-06 18:23:15,856 - INFO - validation batch 1, loss: 0.228, 32/10016 datapoints
2025-03-06 18:23:16,007 - INFO - validation batch 51, loss: 0.344, 1632/10016 datapoints
2025-03-06 18:23:16,162 - INFO - validation batch 101, loss: 0.362, 3232/10016 datapoints
2025-03-06 18:23:16,316 - INFO - validation batch 151, loss: 0.217, 4832/10016 datapoints
2025-03-06 18:23:16,470 - INFO - validation batch 201, loss: 0.087, 6432/10016 datapoints
2025-03-06 18:23:16,626 - INFO - validation batch 251, loss: 0.432, 8032/10016 datapoints
2025-03-06 18:23:16,780 - INFO - validation batch 301, loss: 0.101, 9632/10016 datapoints
2025-03-06 18:23:16,817 - INFO - Epoch 282/800 done.
2025-03-06 18:23:16,817 - INFO - Final validation performance:
Loss: 0.253, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:23:16,817 - INFO - Beginning epoch 283/800
2025-03-06 18:23:16,823 - INFO - training batch 1, loss: 0.246, 32/60000 datapoints
2025-03-06 18:23:17,018 - INFO - training batch 51, loss: 0.157, 1632/60000 datapoints
2025-03-06 18:23:17,209 - INFO - training batch 101, loss: 0.133, 3232/60000 datapoints
2025-03-06 18:23:17,402 - INFO - training batch 151, loss: 0.208, 4832/60000 datapoints
2025-03-06 18:23:17,593 - INFO - training batch 201, loss: 0.321, 6432/60000 datapoints
2025-03-06 18:23:17,790 - INFO - training batch 251, loss: 0.132, 8032/60000 datapoints
2025-03-06 18:23:17,982 - INFO - training batch 301, loss: 0.206, 9632/60000 datapoints
2025-03-06 18:23:18,174 - INFO - training batch 351, loss: 0.489, 11232/60000 datapoints
2025-03-06 18:23:18,365 - INFO - training batch 401, loss: 0.401, 12832/60000 datapoints
2025-03-06 18:23:18,556 - INFO - training batch 451, loss: 0.350, 14432/60000 datapoints
2025-03-06 18:23:18,769 - INFO - training batch 501, loss: 0.208, 16032/60000 datapoints
2025-03-06 18:23:18,961 - INFO - training batch 551, loss: 0.348, 17632/60000 datapoints
2025-03-06 18:23:19,151 - INFO - training batch 601, loss: 0.318, 19232/60000 datapoints
2025-03-06 18:23:19,343 - INFO - training batch 651, loss: 0.690, 20832/60000 datapoints
2025-03-06 18:23:19,538 - INFO - training batch 701, loss: 0.254, 22432/60000 datapoints
2025-03-06 18:23:19,733 - INFO - training batch 751, loss: 0.400, 24032/60000 datapoints
2025-03-06 18:23:19,930 - INFO - training batch 801, loss: 0.498, 25632/60000 datapoints
2025-03-06 18:23:20,123 - INFO - training batch 851, loss: 0.179, 27232/60000 datapoints
2025-03-06 18:23:20,312 - INFO - training batch 901, loss: 0.161, 28832/60000 datapoints
2025-03-06 18:23:20,505 - INFO - training batch 951, loss: 0.390, 30432/60000 datapoints
2025-03-06 18:23:20,707 - INFO - training batch 1001, loss: 0.296, 32032/60000 datapoints
2025-03-06 18:23:20,897 - INFO - training batch 1051, loss: 0.854, 33632/60000 datapoints
2025-03-06 18:23:21,091 - INFO - training batch 1101, loss: 0.614, 35232/60000 datapoints
2025-03-06 18:23:21,282 - INFO - training batch 1151, loss: 0.308, 36832/60000 datapoints
2025-03-06 18:23:21,475 - INFO - training batch 1201, loss: 0.459, 38432/60000 datapoints
2025-03-06 18:23:21,668 - INFO - training batch 1251, loss: 0.409, 40032/60000 datapoints
2025-03-06 18:23:21,862 - INFO - training batch 1301, loss: 0.345, 41632/60000 datapoints
2025-03-06 18:23:22,054 - INFO - training batch 1351, loss: 0.192, 43232/60000 datapoints
2025-03-06 18:23:22,243 - INFO - training batch 1401, loss: 0.347, 44832/60000 datapoints
2025-03-06 18:23:22,435 - INFO - training batch 1451, loss: 0.191, 46432/60000 datapoints
2025-03-06 18:23:22,629 - INFO - training batch 1501, loss: 0.433, 48032/60000 datapoints
2025-03-06 18:23:22,819 - INFO - training batch 1551, loss: 0.325, 49632/60000 datapoints
2025-03-06 18:23:23,010 - INFO - training batch 1601, loss: 0.496, 51232/60000 datapoints
2025-03-06 18:23:23,201 - INFO - training batch 1651, loss: 0.148, 52832/60000 datapoints
2025-03-06 18:23:23,392 - INFO - training batch 1701, loss: 0.377, 54432/60000 datapoints
2025-03-06 18:23:23,584 - INFO - training batch 1751, loss: 0.412, 56032/60000 datapoints
2025-03-06 18:23:23,776 - INFO - training batch 1801, loss: 0.319, 57632/60000 datapoints
2025-03-06 18:23:23,978 - INFO - training batch 1851, loss: 0.161, 59232/60000 datapoints
2025-03-06 18:23:24,079 - INFO - validation batch 1, loss: 0.389, 32/10016 datapoints
2025-03-06 18:23:24,227 - INFO - validation batch 51, loss: 0.311, 1632/10016 datapoints
2025-03-06 18:23:24,377 - INFO - validation batch 101, loss: 0.454, 3232/10016 datapoints
2025-03-06 18:23:24,529 - INFO - validation batch 151, loss: 0.534, 4832/10016 datapoints
2025-03-06 18:23:24,680 - INFO - validation batch 201, loss: 0.456, 6432/10016 datapoints
2025-03-06 18:23:24,829 - INFO - validation batch 251, loss: 0.524, 8032/10016 datapoints
2025-03-06 18:23:24,982 - INFO - validation batch 301, loss: 0.352, 9632/10016 datapoints
2025-03-06 18:23:25,017 - INFO - Epoch 283/800 done.
2025-03-06 18:23:25,017 - INFO - Final validation performance:
Loss: 0.431, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:23:25,017 - INFO - Beginning epoch 284/800
2025-03-06 18:23:25,023 - INFO - training batch 1, loss: 0.092, 32/60000 datapoints
2025-03-06 18:23:25,216 - INFO - training batch 51, loss: 0.130, 1632/60000 datapoints
2025-03-06 18:23:25,407 - INFO - training batch 101, loss: 0.207, 3232/60000 datapoints
2025-03-06 18:23:25,606 - INFO - training batch 151, loss: 0.427, 4832/60000 datapoints
2025-03-06 18:23:25,811 - INFO - training batch 201, loss: 0.099, 6432/60000 datapoints
2025-03-06 18:23:26,006 - INFO - training batch 251, loss: 0.183, 8032/60000 datapoints
2025-03-06 18:23:26,199 - INFO - training batch 301, loss: 0.531, 9632/60000 datapoints
2025-03-06 18:23:26,391 - INFO - training batch 351, loss: 0.581, 11232/60000 datapoints
2025-03-06 18:23:26,583 - INFO - training batch 401, loss: 0.139, 12832/60000 datapoints
2025-03-06 18:23:26,776 - INFO - training batch 451, loss: 0.217, 14432/60000 datapoints
2025-03-06 18:23:26,968 - INFO - training batch 501, loss: 0.346, 16032/60000 datapoints
2025-03-06 18:23:27,158 - INFO - training batch 551, loss: 0.134, 17632/60000 datapoints
2025-03-06 18:23:27,349 - INFO - training batch 601, loss: 0.236, 19232/60000 datapoints
2025-03-06 18:23:27,541 - INFO - training batch 651, loss: 0.347, 20832/60000 datapoints
2025-03-06 18:23:27,734 - INFO - training batch 701, loss: 0.470, 22432/60000 datapoints
2025-03-06 18:23:27,926 - INFO - training batch 751, loss: 0.391, 24032/60000 datapoints
2025-03-06 18:23:28,119 - INFO - training batch 801, loss: 0.342, 25632/60000 datapoints
2025-03-06 18:23:28,309 - INFO - training batch 851, loss: 0.252, 27232/60000 datapoints
2025-03-06 18:23:28,501 - INFO - training batch 901, loss: 0.472, 28832/60000 datapoints
2025-03-06 18:23:28,695 - INFO - training batch 951, loss: 0.263, 30432/60000 datapoints
2025-03-06 18:23:28,908 - INFO - training batch 1001, loss: 0.220, 32032/60000 datapoints
2025-03-06 18:23:29,100 - INFO - training batch 1051, loss: 0.387, 33632/60000 datapoints
2025-03-06 18:23:29,288 - INFO - training batch 1101, loss: 0.181, 35232/60000 datapoints
2025-03-06 18:23:29,481 - INFO - training batch 1151, loss: 0.230, 36832/60000 datapoints
2025-03-06 18:23:29,675 - INFO - training batch 1201, loss: 0.291, 38432/60000 datapoints
2025-03-06 18:23:29,868 - INFO - training batch 1251, loss: 0.446, 40032/60000 datapoints
2025-03-06 18:23:30,062 - INFO - training batch 1301, loss: 0.465, 41632/60000 datapoints
2025-03-06 18:23:30,253 - INFO - training batch 1351, loss: 0.291, 43232/60000 datapoints
2025-03-06 18:23:30,445 - INFO - training batch 1401, loss: 0.324, 44832/60000 datapoints
2025-03-06 18:23:30,639 - INFO - training batch 1451, loss: 0.312, 46432/60000 datapoints
2025-03-06 18:23:30,833 - INFO - training batch 1501, loss: 0.431, 48032/60000 datapoints
2025-03-06 18:23:31,024 - INFO - training batch 1551, loss: 0.152, 49632/60000 datapoints
2025-03-06 18:23:31,222 - INFO - training batch 1601, loss: 0.316, 51232/60000 datapoints
2025-03-06 18:23:31,422 - INFO - training batch 1651, loss: 0.131, 52832/60000 datapoints
2025-03-06 18:23:31,619 - INFO - training batch 1701, loss: 0.324, 54432/60000 datapoints
2025-03-06 18:23:31,815 - INFO - training batch 1751, loss: 0.292, 56032/60000 datapoints
2025-03-06 18:23:32,014 - INFO - training batch 1801, loss: 0.155, 57632/60000 datapoints
2025-03-06 18:23:32,207 - INFO - training batch 1851, loss: 0.200, 59232/60000 datapoints
2025-03-06 18:23:32,307 - INFO - validation batch 1, loss: 0.449, 32/10016 datapoints
2025-03-06 18:23:32,460 - INFO - validation batch 51, loss: 0.179, 1632/10016 datapoints
2025-03-06 18:23:32,614 - INFO - validation batch 101, loss: 0.403, 3232/10016 datapoints
2025-03-06 18:23:32,765 - INFO - validation batch 151, loss: 0.121, 4832/10016 datapoints
2025-03-06 18:23:32,917 - INFO - validation batch 201, loss: 0.239, 6432/10016 datapoints
2025-03-06 18:23:33,070 - INFO - validation batch 251, loss: 0.272, 8032/10016 datapoints
2025-03-06 18:23:33,224 - INFO - validation batch 301, loss: 0.250, 9632/10016 datapoints
2025-03-06 18:23:33,261 - INFO - Epoch 284/800 done.
2025-03-06 18:23:33,261 - INFO - Final validation performance:
Loss: 0.273, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:23:33,261 - INFO - Beginning epoch 285/800
2025-03-06 18:23:33,268 - INFO - training batch 1, loss: 0.167, 32/60000 datapoints
2025-03-06 18:23:33,459 - INFO - training batch 51, loss: 0.452, 1632/60000 datapoints
2025-03-06 18:23:33,655 - INFO - training batch 101, loss: 0.291, 3232/60000 datapoints
2025-03-06 18:23:33,849 - INFO - training batch 151, loss: 0.204, 4832/60000 datapoints
2025-03-06 18:23:34,042 - INFO - training batch 201, loss: 0.335, 6432/60000 datapoints
2025-03-06 18:23:34,236 - INFO - training batch 251, loss: 0.215, 8032/60000 datapoints
2025-03-06 18:23:34,428 - INFO - training batch 301, loss: 0.307, 9632/60000 datapoints
2025-03-06 18:23:34,623 - INFO - training batch 351, loss: 0.655, 11232/60000 datapoints
2025-03-06 18:23:34,816 - INFO - training batch 401, loss: 0.491, 12832/60000 datapoints
2025-03-06 18:23:35,009 - INFO - training batch 451, loss: 0.308, 14432/60000 datapoints
2025-03-06 18:23:35,202 - INFO - training batch 501, loss: 0.356, 16032/60000 datapoints
2025-03-06 18:23:35,392 - INFO - training batch 551, loss: 0.349, 17632/60000 datapoints
2025-03-06 18:23:35,584 - INFO - training batch 601, loss: 0.386, 19232/60000 datapoints
2025-03-06 18:23:35,778 - INFO - training batch 651, loss: 0.211, 20832/60000 datapoints
2025-03-06 18:23:35,971 - INFO - training batch 701, loss: 0.401, 22432/60000 datapoints
2025-03-06 18:23:36,167 - INFO - training batch 751, loss: 0.282, 24032/60000 datapoints
2025-03-06 18:23:36,359 - INFO - training batch 801, loss: 0.367, 25632/60000 datapoints
2025-03-06 18:23:36,549 - INFO - training batch 851, loss: 0.197, 27232/60000 datapoints
2025-03-06 18:23:36,743 - INFO - training batch 901, loss: 0.583, 28832/60000 datapoints
2025-03-06 18:23:36,935 - INFO - training batch 951, loss: 0.351, 30432/60000 datapoints
2025-03-06 18:23:37,127 - INFO - training batch 1001, loss: 0.169, 32032/60000 datapoints
2025-03-06 18:23:37,320 - INFO - training batch 1051, loss: 0.458, 33632/60000 datapoints
2025-03-06 18:23:37,511 - INFO - training batch 1101, loss: 0.160, 35232/60000 datapoints
2025-03-06 18:23:37,714 - INFO - training batch 1151, loss: 0.239, 36832/60000 datapoints
2025-03-06 18:23:37,907 - INFO - training batch 1201, loss: 0.347, 38432/60000 datapoints
2025-03-06 18:23:38,104 - INFO - training batch 1251, loss: 0.455, 40032/60000 datapoints
2025-03-06 18:23:38,294 - INFO - training batch 1301, loss: 0.419, 41632/60000 datapoints
2025-03-06 18:23:38,485 - INFO - training batch 1351, loss: 0.258, 43232/60000 datapoints
2025-03-06 18:23:38,680 - INFO - training batch 1401, loss: 0.357, 44832/60000 datapoints
2025-03-06 18:23:38,901 - INFO - training batch 1451, loss: 0.186, 46432/60000 datapoints
2025-03-06 18:23:39,096 - INFO - training batch 1501, loss: 0.207, 48032/60000 datapoints
2025-03-06 18:23:39,289 - INFO - training batch 1551, loss: 0.277, 49632/60000 datapoints
2025-03-06 18:23:39,480 - INFO - training batch 1601, loss: 0.316, 51232/60000 datapoints
2025-03-06 18:23:39,684 - INFO - training batch 1651, loss: 0.153, 52832/60000 datapoints
2025-03-06 18:23:39,878 - INFO - training batch 1701, loss: 0.211, 54432/60000 datapoints
2025-03-06 18:23:40,071 - INFO - training batch 1751, loss: 0.474, 56032/60000 datapoints
2025-03-06 18:23:40,262 - INFO - training batch 1801, loss: 0.251, 57632/60000 datapoints
2025-03-06 18:23:40,459 - INFO - training batch 1851, loss: 0.188, 59232/60000 datapoints
2025-03-06 18:23:40,557 - INFO - validation batch 1, loss: 0.375, 32/10016 datapoints
2025-03-06 18:23:40,723 - INFO - validation batch 51, loss: 0.259, 1632/10016 datapoints
2025-03-06 18:23:40,891 - INFO - validation batch 101, loss: 0.313, 3232/10016 datapoints
2025-03-06 18:23:41,040 - INFO - validation batch 151, loss: 0.554, 4832/10016 datapoints
2025-03-06 18:23:41,190 - INFO - validation batch 201, loss: 0.478, 6432/10016 datapoints
2025-03-06 18:23:41,341 - INFO - validation batch 251, loss: 0.163, 8032/10016 datapoints
2025-03-06 18:23:41,492 - INFO - validation batch 301, loss: 0.165, 9632/10016 datapoints
2025-03-06 18:23:41,529 - INFO - Epoch 285/800 done.
2025-03-06 18:23:41,529 - INFO - Final validation performance:
Loss: 0.329, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:23:41,529 - INFO - Beginning epoch 286/800
2025-03-06 18:23:41,535 - INFO - training batch 1, loss: 0.361, 32/60000 datapoints
2025-03-06 18:23:41,732 - INFO - training batch 51, loss: 0.178, 1632/60000 datapoints
2025-03-06 18:23:41,925 - INFO - training batch 101, loss: 0.368, 3232/60000 datapoints
2025-03-06 18:23:42,116 - INFO - training batch 151, loss: 0.325, 4832/60000 datapoints
2025-03-06 18:23:42,307 - INFO - training batch 201, loss: 0.288, 6432/60000 datapoints
2025-03-06 18:23:42,498 - INFO - training batch 251, loss: 0.403, 8032/60000 datapoints
2025-03-06 18:23:42,692 - INFO - training batch 301, loss: 0.569, 9632/60000 datapoints
2025-03-06 18:23:42,886 - INFO - training batch 351, loss: 0.518, 11232/60000 datapoints
2025-03-06 18:23:43,083 - INFO - training batch 401, loss: 0.237, 12832/60000 datapoints
2025-03-06 18:23:43,276 - INFO - training batch 451, loss: 0.185, 14432/60000 datapoints
2025-03-06 18:23:43,468 - INFO - training batch 501, loss: 0.124, 16032/60000 datapoints
2025-03-06 18:23:43,663 - INFO - training batch 551, loss: 0.176, 17632/60000 datapoints
2025-03-06 18:23:43,853 - INFO - training batch 601, loss: 0.471, 19232/60000 datapoints
2025-03-06 18:23:44,046 - INFO - training batch 651, loss: 0.449, 20832/60000 datapoints
2025-03-06 18:23:44,238 - INFO - training batch 701, loss: 0.429, 22432/60000 datapoints
2025-03-06 18:23:44,429 - INFO - training batch 751, loss: 0.187, 24032/60000 datapoints
2025-03-06 18:23:44,622 - INFO - training batch 801, loss: 0.179, 25632/60000 datapoints
2025-03-06 18:23:44,815 - INFO - training batch 851, loss: 0.371, 27232/60000 datapoints
2025-03-06 18:23:45,011 - INFO - training batch 901, loss: 0.483, 28832/60000 datapoints
2025-03-06 18:23:45,203 - INFO - training batch 951, loss: 0.491, 30432/60000 datapoints
2025-03-06 18:23:45,394 - INFO - training batch 1001, loss: 0.228, 32032/60000 datapoints
2025-03-06 18:23:45,586 - INFO - training batch 1051, loss: 0.318, 33632/60000 datapoints
2025-03-06 18:23:45,781 - INFO - training batch 1101, loss: 0.176, 35232/60000 datapoints
2025-03-06 18:23:45,975 - INFO - training batch 1151, loss: 0.579, 36832/60000 datapoints
2025-03-06 18:23:46,167 - INFO - training batch 1201, loss: 0.308, 38432/60000 datapoints
2025-03-06 18:23:46,367 - INFO - training batch 1251, loss: 0.356, 40032/60000 datapoints
2025-03-06 18:23:46,565 - INFO - training batch 1301, loss: 0.545, 41632/60000 datapoints
2025-03-06 18:23:46,760 - INFO - training batch 1351, loss: 0.282, 43232/60000 datapoints
2025-03-06 18:23:46,951 - INFO - training batch 1401, loss: 0.236, 44832/60000 datapoints
2025-03-06 18:23:47,141 - INFO - training batch 1451, loss: 0.336, 46432/60000 datapoints
2025-03-06 18:23:47,333 - INFO - training batch 1501, loss: 0.288, 48032/60000 datapoints
2025-03-06 18:23:47,524 - INFO - training batch 1551, loss: 0.271, 49632/60000 datapoints
2025-03-06 18:23:47,717 - INFO - training batch 1601, loss: 0.151, 51232/60000 datapoints
2025-03-06 18:23:47,911 - INFO - training batch 1651, loss: 0.358, 52832/60000 datapoints
2025-03-06 18:23:48,103 - INFO - training batch 1701, loss: 0.331, 54432/60000 datapoints
2025-03-06 18:23:48,295 - INFO - training batch 1751, loss: 0.610, 56032/60000 datapoints
2025-03-06 18:23:48,487 - INFO - training batch 1801, loss: 0.497, 57632/60000 datapoints
2025-03-06 18:23:48,681 - INFO - training batch 1851, loss: 0.390, 59232/60000 datapoints
2025-03-06 18:23:48,781 - INFO - validation batch 1, loss: 0.267, 32/10016 datapoints
2025-03-06 18:23:48,946 - INFO - validation batch 51, loss: 0.420, 1632/10016 datapoints
2025-03-06 18:23:49,103 - INFO - validation batch 101, loss: 0.184, 3232/10016 datapoints
2025-03-06 18:23:49,253 - INFO - validation batch 151, loss: 0.379, 4832/10016 datapoints
2025-03-06 18:23:49,403 - INFO - validation batch 201, loss: 0.644, 6432/10016 datapoints
2025-03-06 18:23:49,551 - INFO - validation batch 251, loss: 0.320, 8032/10016 datapoints
2025-03-06 18:23:49,706 - INFO - validation batch 301, loss: 0.270, 9632/10016 datapoints
2025-03-06 18:23:49,743 - INFO - Epoch 286/800 done.
2025-03-06 18:23:49,743 - INFO - Final validation performance:
Loss: 0.355, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:23:49,744 - INFO - Beginning epoch 287/800
2025-03-06 18:23:49,749 - INFO - training batch 1, loss: 0.473, 32/60000 datapoints
2025-03-06 18:23:49,943 - INFO - training batch 51, loss: 0.198, 1632/60000 datapoints
2025-03-06 18:23:50,136 - INFO - training batch 101, loss: 0.369, 3232/60000 datapoints
2025-03-06 18:23:50,329 - INFO - training batch 151, loss: 0.118, 4832/60000 datapoints
2025-03-06 18:23:50,522 - INFO - training batch 201, loss: 0.205, 6432/60000 datapoints
2025-03-06 18:23:50,718 - INFO - training batch 251, loss: 0.694, 8032/60000 datapoints
2025-03-06 18:23:50,916 - INFO - training batch 301, loss: 0.503, 9632/60000 datapoints
2025-03-06 18:23:51,107 - INFO - training batch 351, loss: 0.248, 11232/60000 datapoints
2025-03-06 18:23:51,301 - INFO - training batch 401, loss: 0.265, 12832/60000 datapoints
2025-03-06 18:23:51,499 - INFO - training batch 451, loss: 0.477, 14432/60000 datapoints
2025-03-06 18:23:51,695 - INFO - training batch 501, loss: 0.609, 16032/60000 datapoints
2025-03-06 18:23:51,891 - INFO - training batch 551, loss: 0.340, 17632/60000 datapoints
2025-03-06 18:23:52,094 - INFO - training batch 601, loss: 0.321, 19232/60000 datapoints
2025-03-06 18:23:52,287 - INFO - training batch 651, loss: 0.470, 20832/60000 datapoints
2025-03-06 18:23:52,481 - INFO - training batch 701, loss: 0.234, 22432/60000 datapoints
2025-03-06 18:23:52,680 - INFO - training batch 751, loss: 0.374, 24032/60000 datapoints
2025-03-06 18:23:52,876 - INFO - training batch 801, loss: 0.393, 25632/60000 datapoints
2025-03-06 18:23:53,070 - INFO - training batch 851, loss: 0.302, 27232/60000 datapoints
2025-03-06 18:23:53,265 - INFO - training batch 901, loss: 0.218, 28832/60000 datapoints
2025-03-06 18:23:53,459 - INFO - training batch 951, loss: 0.253, 30432/60000 datapoints
2025-03-06 18:23:53,655 - INFO - training batch 1001, loss: 0.343, 32032/60000 datapoints
2025-03-06 18:23:53,850 - INFO - training batch 1051, loss: 0.308, 33632/60000 datapoints
2025-03-06 18:23:54,046 - INFO - training batch 1101, loss: 0.458, 35232/60000 datapoints
2025-03-06 18:23:54,239 - INFO - training batch 1151, loss: 0.460, 36832/60000 datapoints
2025-03-06 18:23:54,433 - INFO - training batch 1201, loss: 0.148, 38432/60000 datapoints
2025-03-06 18:23:54,631 - INFO - training batch 1251, loss: 0.299, 40032/60000 datapoints
2025-03-06 18:23:54,825 - INFO - training batch 1301, loss: 0.141, 41632/60000 datapoints
2025-03-06 18:23:55,021 - INFO - training batch 1351, loss: 0.202, 43232/60000 datapoints
2025-03-06 18:23:55,216 - INFO - training batch 1401, loss: 0.314, 44832/60000 datapoints
2025-03-06 18:23:55,411 - INFO - training batch 1451, loss: 0.326, 46432/60000 datapoints
2025-03-06 18:23:55,608 - INFO - training batch 1501, loss: 0.255, 48032/60000 datapoints
2025-03-06 18:23:55,806 - INFO - training batch 1551, loss: 0.536, 49632/60000 datapoints
2025-03-06 18:23:56,003 - INFO - training batch 1601, loss: 0.262, 51232/60000 datapoints
2025-03-06 18:23:56,198 - INFO - training batch 1651, loss: 0.416, 52832/60000 datapoints
2025-03-06 18:23:56,395 - INFO - training batch 1701, loss: 0.254, 54432/60000 datapoints
2025-03-06 18:23:56,587 - INFO - training batch 1751, loss: 0.156, 56032/60000 datapoints
2025-03-06 18:23:56,784 - INFO - training batch 1801, loss: 0.246, 57632/60000 datapoints
2025-03-06 18:23:56,980 - INFO - training batch 1851, loss: 0.223, 59232/60000 datapoints
2025-03-06 18:23:57,082 - INFO - validation batch 1, loss: 0.181, 32/10016 datapoints
2025-03-06 18:23:57,234 - INFO - validation batch 51, loss: 0.412, 1632/10016 datapoints
2025-03-06 18:23:57,387 - INFO - validation batch 101, loss: 0.213, 3232/10016 datapoints
2025-03-06 18:23:57,539 - INFO - validation batch 151, loss: 0.684, 4832/10016 datapoints
2025-03-06 18:23:57,696 - INFO - validation batch 201, loss: 0.205, 6432/10016 datapoints
2025-03-06 18:23:57,849 - INFO - validation batch 251, loss: 0.414, 8032/10016 datapoints
2025-03-06 18:23:58,005 - INFO - validation batch 301, loss: 0.541, 9632/10016 datapoints
2025-03-06 18:23:58,041 - INFO - Epoch 287/800 done.
2025-03-06 18:23:58,041 - INFO - Final validation performance:
Loss: 0.379, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 18:23:58,042 - INFO - Beginning epoch 288/800
2025-03-06 18:23:58,047 - INFO - training batch 1, loss: 0.226, 32/60000 datapoints
2025-03-06 18:23:58,241 - INFO - training batch 51, loss: 0.548, 1632/60000 datapoints
2025-03-06 18:23:58,437 - INFO - training batch 101, loss: 0.263, 3232/60000 datapoints
2025-03-06 18:23:58,633 - INFO - training batch 151, loss: 0.414, 4832/60000 datapoints
2025-03-06 18:23:58,830 - INFO - training batch 201, loss: 0.499, 6432/60000 datapoints
2025-03-06 18:23:59,041 - INFO - training batch 251, loss: 0.412, 8032/60000 datapoints
2025-03-06 18:23:59,236 - INFO - training batch 301, loss: 0.334, 9632/60000 datapoints
2025-03-06 18:23:59,430 - INFO - training batch 351, loss: 0.362, 11232/60000 datapoints
2025-03-06 18:23:59,624 - INFO - training batch 401, loss: 0.254, 12832/60000 datapoints
2025-03-06 18:23:59,820 - INFO - training batch 451, loss: 0.172, 14432/60000 datapoints
2025-03-06 18:24:00,015 - INFO - training batch 501, loss: 0.404, 16032/60000 datapoints
2025-03-06 18:24:00,208 - INFO - training batch 551, loss: 0.272, 17632/60000 datapoints
2025-03-06 18:24:00,403 - INFO - training batch 601, loss: 0.377, 19232/60000 datapoints
2025-03-06 18:24:00,600 - INFO - training batch 651, loss: 0.387, 20832/60000 datapoints
2025-03-06 18:24:00,818 - INFO - training batch 701, loss: 0.346, 22432/60000 datapoints
2025-03-06 18:24:01,012 - INFO - training batch 751, loss: 0.235, 24032/60000 datapoints
2025-03-06 18:24:01,208 - INFO - training batch 801, loss: 0.203, 25632/60000 datapoints
2025-03-06 18:24:01,405 - INFO - training batch 851, loss: 0.305, 27232/60000 datapoints
2025-03-06 18:24:01,603 - INFO - training batch 901, loss: 0.244, 28832/60000 datapoints
2025-03-06 18:24:01,804 - INFO - training batch 951, loss: 0.478, 30432/60000 datapoints
2025-03-06 18:24:02,001 - INFO - training batch 1001, loss: 0.343, 32032/60000 datapoints
2025-03-06 18:24:02,197 - INFO - training batch 1051, loss: 0.309, 33632/60000 datapoints
2025-03-06 18:24:02,391 - INFO - training batch 1101, loss: 0.314, 35232/60000 datapoints
2025-03-06 18:24:02,586 - INFO - training batch 1151, loss: 0.413, 36832/60000 datapoints
2025-03-06 18:24:02,781 - INFO - training batch 1201, loss: 0.141, 38432/60000 datapoints
2025-03-06 18:24:02,978 - INFO - training batch 1251, loss: 0.182, 40032/60000 datapoints
2025-03-06 18:24:03,173 - INFO - training batch 1301, loss: 0.224, 41632/60000 datapoints
2025-03-06 18:24:03,367 - INFO - training batch 1351, loss: 0.765, 43232/60000 datapoints
2025-03-06 18:24:03,561 - INFO - training batch 1401, loss: 0.368, 44832/60000 datapoints
2025-03-06 18:24:03,758 - INFO - training batch 1451, loss: 0.274, 46432/60000 datapoints
2025-03-06 18:24:03,954 - INFO - training batch 1501, loss: 0.503, 48032/60000 datapoints
2025-03-06 18:24:04,153 - INFO - training batch 1551, loss: 0.260, 49632/60000 datapoints
2025-03-06 18:24:04,347 - INFO - training batch 1601, loss: 0.376, 51232/60000 datapoints
2025-03-06 18:24:04,539 - INFO - training batch 1651, loss: 0.328, 52832/60000 datapoints
2025-03-06 18:24:04,736 - INFO - training batch 1701, loss: 0.359, 54432/60000 datapoints
2025-03-06 18:24:04,936 - INFO - training batch 1751, loss: 0.348, 56032/60000 datapoints
2025-03-06 18:24:05,132 - INFO - training batch 1801, loss: 0.248, 57632/60000 datapoints
2025-03-06 18:24:05,325 - INFO - training batch 1851, loss: 0.413, 59232/60000 datapoints
2025-03-06 18:24:05,427 - INFO - validation batch 1, loss: 0.332, 32/10016 datapoints
2025-03-06 18:24:05,580 - INFO - validation batch 51, loss: 0.433, 1632/10016 datapoints
2025-03-06 18:24:05,734 - INFO - validation batch 101, loss: 0.252, 3232/10016 datapoints
2025-03-06 18:24:05,888 - INFO - validation batch 151, loss: 0.510, 4832/10016 datapoints
2025-03-06 18:24:06,043 - INFO - validation batch 201, loss: 0.206, 6432/10016 datapoints
2025-03-06 18:24:06,199 - INFO - validation batch 251, loss: 0.370, 8032/10016 datapoints
2025-03-06 18:24:06,351 - INFO - validation batch 301, loss: 0.250, 9632/10016 datapoints
2025-03-06 18:24:06,391 - INFO - Epoch 288/800 done.
2025-03-06 18:24:06,391 - INFO - Final validation performance:
Loss: 0.336, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 18:24:06,392 - INFO - Beginning epoch 289/800
2025-03-06 18:24:06,398 - INFO - training batch 1, loss: 0.284, 32/60000 datapoints
2025-03-06 18:24:06,592 - INFO - training batch 51, loss: 0.382, 1632/60000 datapoints
2025-03-06 18:24:06,787 - INFO - training batch 101, loss: 0.135, 3232/60000 datapoints
2025-03-06 18:24:06,986 - INFO - training batch 151, loss: 0.213, 4832/60000 datapoints
2025-03-06 18:24:07,179 - INFO - training batch 201, loss: 0.275, 6432/60000 datapoints
2025-03-06 18:24:07,372 - INFO - training batch 251, loss: 0.381, 8032/60000 datapoints
2025-03-06 18:24:07,566 - INFO - training batch 301, loss: 0.463, 9632/60000 datapoints
2025-03-06 18:24:07,765 - INFO - training batch 351, loss: 0.334, 11232/60000 datapoints
2025-03-06 18:24:07,959 - INFO - training batch 401, loss: 0.561, 12832/60000 datapoints
2025-03-06 18:24:08,167 - INFO - training batch 451, loss: 0.637, 14432/60000 datapoints
2025-03-06 18:24:08,362 - INFO - training batch 501, loss: 0.316, 16032/60000 datapoints
2025-03-06 18:24:08,555 - INFO - training batch 551, loss: 0.281, 17632/60000 datapoints
2025-03-06 18:24:08,751 - INFO - training batch 601, loss: 0.357, 19232/60000 datapoints
2025-03-06 18:24:08,947 - INFO - training batch 651, loss: 0.310, 20832/60000 datapoints
2025-03-06 18:24:09,162 - INFO - training batch 701, loss: 0.165, 22432/60000 datapoints
2025-03-06 18:24:09,358 - INFO - training batch 751, loss: 0.099, 24032/60000 datapoints
2025-03-06 18:24:09,552 - INFO - training batch 801, loss: 0.219, 25632/60000 datapoints
2025-03-06 18:24:09,758 - INFO - training batch 851, loss: 0.111, 27232/60000 datapoints
2025-03-06 18:24:09,954 - INFO - training batch 901, loss: 0.188, 28832/60000 datapoints
2025-03-06 18:24:10,150 - INFO - training batch 951, loss: 0.098, 30432/60000 datapoints
2025-03-06 18:24:10,351 - INFO - training batch 1001, loss: 0.198, 32032/60000 datapoints
2025-03-06 18:24:10,544 - INFO - training batch 1051, loss: 0.352, 33632/60000 datapoints
2025-03-06 18:24:10,746 - INFO - training batch 1101, loss: 0.078, 35232/60000 datapoints
2025-03-06 18:24:10,945 - INFO - training batch 1151, loss: 0.570, 36832/60000 datapoints
2025-03-06 18:24:11,138 - INFO - training batch 1201, loss: 0.486, 38432/60000 datapoints
2025-03-06 18:24:11,332 - INFO - training batch 1251, loss: 0.375, 40032/60000 datapoints
2025-03-06 18:24:11,528 - INFO - training batch 1301, loss: 0.276, 41632/60000 datapoints
2025-03-06 18:24:11,723 - INFO - training batch 1351, loss: 0.390, 43232/60000 datapoints
2025-03-06 18:24:11,919 - INFO - training batch 1401, loss: 0.619, 44832/60000 datapoints
2025-03-06 18:24:12,118 - INFO - training batch 1451, loss: 0.474, 46432/60000 datapoints
2025-03-06 18:24:12,312 - INFO - training batch 1501, loss: 0.242, 48032/60000 datapoints
2025-03-06 18:24:12,508 - INFO - training batch 1551, loss: 0.300, 49632/60000 datapoints
2025-03-06 18:24:12,705 - INFO - training batch 1601, loss: 0.258, 51232/60000 datapoints
2025-03-06 18:24:12,900 - INFO - training batch 1651, loss: 0.340, 52832/60000 datapoints
2025-03-06 18:24:13,096 - INFO - training batch 1701, loss: 0.265, 54432/60000 datapoints
2025-03-06 18:24:13,292 - INFO - training batch 1751, loss: 0.280, 56032/60000 datapoints
2025-03-06 18:24:13,489 - INFO - training batch 1801, loss: 0.489, 57632/60000 datapoints
2025-03-06 18:24:13,684 - INFO - training batch 1851, loss: 0.186, 59232/60000 datapoints
2025-03-06 18:24:13,786 - INFO - validation batch 1, loss: 0.816, 32/10016 datapoints
2025-03-06 18:24:13,939 - INFO - validation batch 51, loss: 0.161, 1632/10016 datapoints
2025-03-06 18:24:14,095 - INFO - validation batch 101, loss: 0.365, 3232/10016 datapoints
2025-03-06 18:24:14,247 - INFO - validation batch 151, loss: 0.434, 4832/10016 datapoints
2025-03-06 18:24:14,400 - INFO - validation batch 201, loss: 0.391, 6432/10016 datapoints
2025-03-06 18:24:14,553 - INFO - validation batch 251, loss: 0.530, 8032/10016 datapoints
2025-03-06 18:24:14,706 - INFO - validation batch 301, loss: 0.229, 9632/10016 datapoints
2025-03-06 18:24:14,743 - INFO - Epoch 289/800 done.
2025-03-06 18:24:14,743 - INFO - Final validation performance:
Loss: 0.418, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:24:14,744 - INFO - Beginning epoch 290/800
2025-03-06 18:24:14,749 - INFO - training batch 1, loss: 0.293, 32/60000 datapoints
2025-03-06 18:24:14,949 - INFO - training batch 51, loss: 0.595, 1632/60000 datapoints
2025-03-06 18:24:15,145 - INFO - training batch 101, loss: 0.366, 3232/60000 datapoints
2025-03-06 18:24:15,339 - INFO - training batch 151, loss: 0.176, 4832/60000 datapoints
2025-03-06 18:24:15,534 - INFO - training batch 201, loss: 0.521, 6432/60000 datapoints
2025-03-06 18:24:15,733 - INFO - training batch 251, loss: 0.317, 8032/60000 datapoints
2025-03-06 18:24:15,925 - INFO - training batch 301, loss: 0.187, 9632/60000 datapoints
2025-03-06 18:24:16,124 - INFO - training batch 351, loss: 0.412, 11232/60000 datapoints
2025-03-06 18:24:16,320 - INFO - training batch 401, loss: 0.202, 12832/60000 datapoints
2025-03-06 18:24:16,515 - INFO - training batch 451, loss: 0.411, 14432/60000 datapoints
2025-03-06 18:24:16,712 - INFO - training batch 501, loss: 0.250, 16032/60000 datapoints
2025-03-06 18:24:16,909 - INFO - training batch 551, loss: 0.348, 17632/60000 datapoints
2025-03-06 18:24:17,112 - INFO - training batch 601, loss: 0.396, 19232/60000 datapoints
2025-03-06 18:24:17,305 - INFO - training batch 651, loss: 0.213, 20832/60000 datapoints
2025-03-06 18:24:17,501 - INFO - training batch 701, loss: 0.324, 22432/60000 datapoints
2025-03-06 18:24:17,696 - INFO - training batch 751, loss: 0.217, 24032/60000 datapoints
2025-03-06 18:24:17,889 - INFO - training batch 801, loss: 0.098, 25632/60000 datapoints
2025-03-06 18:24:18,085 - INFO - training batch 851, loss: 0.339, 27232/60000 datapoints
2025-03-06 18:24:18,278 - INFO - training batch 901, loss: 0.263, 28832/60000 datapoints
2025-03-06 18:24:18,473 - INFO - training batch 951, loss: 0.296, 30432/60000 datapoints
2025-03-06 18:24:18,668 - INFO - training batch 1001, loss: 0.304, 32032/60000 datapoints
2025-03-06 18:24:18,864 - INFO - training batch 1051, loss: 0.462, 33632/60000 datapoints
2025-03-06 18:24:19,057 - INFO - training batch 1101, loss: 0.366, 35232/60000 datapoints
2025-03-06 18:24:19,273 - INFO - training batch 1151, loss: 0.096, 36832/60000 datapoints
2025-03-06 18:24:19,467 - INFO - training batch 1201, loss: 0.249, 38432/60000 datapoints
2025-03-06 18:24:19,665 - INFO - training batch 1251, loss: 0.130, 40032/60000 datapoints
2025-03-06 18:24:19,860 - INFO - training batch 1301, loss: 0.467, 41632/60000 datapoints
2025-03-06 18:24:20,058 - INFO - training batch 1351, loss: 0.282, 43232/60000 datapoints
2025-03-06 18:24:20,254 - INFO - training batch 1401, loss: 0.262, 44832/60000 datapoints
2025-03-06 18:24:20,458 - INFO - training batch 1451, loss: 0.344, 46432/60000 datapoints
2025-03-06 18:24:20,657 - INFO - training batch 1501, loss: 0.318, 48032/60000 datapoints
2025-03-06 18:24:20,854 - INFO - training batch 1551, loss: 0.250, 49632/60000 datapoints
2025-03-06 18:24:21,049 - INFO - training batch 1601, loss: 0.243, 51232/60000 datapoints
2025-03-06 18:24:21,246 - INFO - training batch 1651, loss: 0.236, 52832/60000 datapoints
2025-03-06 18:24:21,441 - INFO - training batch 1701, loss: 0.114, 54432/60000 datapoints
2025-03-06 18:24:21,639 - INFO - training batch 1751, loss: 0.315, 56032/60000 datapoints
2025-03-06 18:24:21,834 - INFO - training batch 1801, loss: 0.281, 57632/60000 datapoints
2025-03-06 18:24:22,030 - INFO - training batch 1851, loss: 0.228, 59232/60000 datapoints
2025-03-06 18:24:22,131 - INFO - validation batch 1, loss: 0.350, 32/10016 datapoints
2025-03-06 18:24:22,284 - INFO - validation batch 51, loss: 0.555, 1632/10016 datapoints
2025-03-06 18:24:22,437 - INFO - validation batch 101, loss: 0.287, 3232/10016 datapoints
2025-03-06 18:24:22,591 - INFO - validation batch 151, loss: 0.178, 4832/10016 datapoints
2025-03-06 18:24:22,746 - INFO - validation batch 201, loss: 0.542, 6432/10016 datapoints
2025-03-06 18:24:22,898 - INFO - validation batch 251, loss: 0.077, 8032/10016 datapoints
2025-03-06 18:24:23,051 - INFO - validation batch 301, loss: 0.121, 9632/10016 datapoints
2025-03-06 18:24:23,090 - INFO - Epoch 290/800 done.
2025-03-06 18:24:23,090 - INFO - Final validation performance:
Loss: 0.301, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:24:23,091 - INFO - Beginning epoch 291/800
2025-03-06 18:24:23,097 - INFO - training batch 1, loss: 0.163, 32/60000 datapoints
2025-03-06 18:24:23,291 - INFO - training batch 51, loss: 0.330, 1632/60000 datapoints
2025-03-06 18:24:23,484 - INFO - training batch 101, loss: 0.090, 3232/60000 datapoints
2025-03-06 18:24:23,682 - INFO - training batch 151, loss: 0.560, 4832/60000 datapoints
2025-03-06 18:24:23,877 - INFO - training batch 201, loss: 0.256, 6432/60000 datapoints
2025-03-06 18:24:24,075 - INFO - training batch 251, loss: 0.510, 8032/60000 datapoints
2025-03-06 18:24:24,271 - INFO - training batch 301, loss: 0.222, 9632/60000 datapoints
2025-03-06 18:24:24,465 - INFO - training batch 351, loss: 0.378, 11232/60000 datapoints
2025-03-06 18:24:24,661 - INFO - training batch 401, loss: 0.521, 12832/60000 datapoints
2025-03-06 18:24:24,864 - INFO - training batch 451, loss: 0.701, 14432/60000 datapoints
2025-03-06 18:24:25,060 - INFO - training batch 501, loss: 0.193, 16032/60000 datapoints
2025-03-06 18:24:25,255 - INFO - training batch 551, loss: 0.230, 17632/60000 datapoints
2025-03-06 18:24:25,451 - INFO - training batch 601, loss: 0.576, 19232/60000 datapoints
2025-03-06 18:24:25,652 - INFO - training batch 651, loss: 0.210, 20832/60000 datapoints
2025-03-06 18:24:25,845 - INFO - training batch 701, loss: 0.218, 22432/60000 datapoints
2025-03-06 18:24:26,041 - INFO - training batch 751, loss: 0.351, 24032/60000 datapoints
2025-03-06 18:24:26,239 - INFO - training batch 801, loss: 0.266, 25632/60000 datapoints
2025-03-06 18:24:26,435 - INFO - training batch 851, loss: 0.524, 27232/60000 datapoints
2025-03-06 18:24:26,632 - INFO - training batch 901, loss: 0.482, 28832/60000 datapoints
2025-03-06 18:24:26,838 - INFO - training batch 951, loss: 0.193, 30432/60000 datapoints
2025-03-06 18:24:27,030 - INFO - training batch 1001, loss: 0.204, 32032/60000 datapoints
2025-03-06 18:24:27,224 - INFO - training batch 1051, loss: 0.275, 33632/60000 datapoints
2025-03-06 18:24:27,417 - INFO - training batch 1101, loss: 0.291, 35232/60000 datapoints
2025-03-06 18:24:27,613 - INFO - training batch 1151, loss: 0.326, 36832/60000 datapoints
2025-03-06 18:24:27,808 - INFO - training batch 1201, loss: 0.194, 38432/60000 datapoints
2025-03-06 18:24:28,001 - INFO - training batch 1251, loss: 0.501, 40032/60000 datapoints
2025-03-06 18:24:28,198 - INFO - training batch 1301, loss: 0.231, 41632/60000 datapoints
2025-03-06 18:24:28,391 - INFO - training batch 1351, loss: 0.332, 43232/60000 datapoints
2025-03-06 18:24:28,585 - INFO - training batch 1401, loss: 0.425, 44832/60000 datapoints
2025-03-06 18:24:28,784 - INFO - training batch 1451, loss: 0.340, 46432/60000 datapoints
2025-03-06 18:24:28,977 - INFO - training batch 1501, loss: 0.337, 48032/60000 datapoints
2025-03-06 18:24:29,172 - INFO - training batch 1551, loss: 0.370, 49632/60000 datapoints
2025-03-06 18:24:29,381 - INFO - training batch 1601, loss: 0.208, 51232/60000 datapoints
2025-03-06 18:24:29,573 - INFO - training batch 1651, loss: 0.255, 52832/60000 datapoints
2025-03-06 18:24:29,769 - INFO - training batch 1701, loss: 0.215, 54432/60000 datapoints
2025-03-06 18:24:29,964 - INFO - training batch 1751, loss: 0.170, 56032/60000 datapoints
2025-03-06 18:24:30,159 - INFO - training batch 1801, loss: 0.132, 57632/60000 datapoints
2025-03-06 18:24:30,351 - INFO - training batch 1851, loss: 0.135, 59232/60000 datapoints
2025-03-06 18:24:30,452 - INFO - validation batch 1, loss: 0.155, 32/10016 datapoints
2025-03-06 18:24:30,607 - INFO - validation batch 51, loss: 0.481, 1632/10016 datapoints
2025-03-06 18:24:30,761 - INFO - validation batch 101, loss: 0.326, 3232/10016 datapoints
2025-03-06 18:24:30,915 - INFO - validation batch 151, loss: 0.281, 4832/10016 datapoints
2025-03-06 18:24:31,067 - INFO - validation batch 201, loss: 0.225, 6432/10016 datapoints
2025-03-06 18:24:31,221 - INFO - validation batch 251, loss: 0.290, 8032/10016 datapoints
2025-03-06 18:24:31,374 - INFO - validation batch 301, loss: 0.161, 9632/10016 datapoints
2025-03-06 18:24:31,411 - INFO - Epoch 291/800 done.
2025-03-06 18:24:31,411 - INFO - Final validation performance:
Loss: 0.274, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:24:31,412 - INFO - Beginning epoch 292/800
2025-03-06 18:24:31,418 - INFO - training batch 1, loss: 0.671, 32/60000 datapoints
2025-03-06 18:24:31,613 - INFO - training batch 51, loss: 0.600, 1632/60000 datapoints
2025-03-06 18:24:31,807 - INFO - training batch 101, loss: 0.167, 3232/60000 datapoints
2025-03-06 18:24:31,998 - INFO - training batch 151, loss: 0.318, 4832/60000 datapoints
2025-03-06 18:24:32,202 - INFO - training batch 201, loss: 0.270, 6432/60000 datapoints
2025-03-06 18:24:32,401 - INFO - training batch 251, loss: 0.668, 8032/60000 datapoints
2025-03-06 18:24:32,593 - INFO - training batch 301, loss: 0.116, 9632/60000 datapoints
2025-03-06 18:24:32,786 - INFO - training batch 351, loss: 0.252, 11232/60000 datapoints
2025-03-06 18:24:32,978 - INFO - training batch 401, loss: 0.099, 12832/60000 datapoints
2025-03-06 18:24:33,172 - INFO - training batch 451, loss: 0.337, 14432/60000 datapoints
2025-03-06 18:24:33,363 - INFO - training batch 501, loss: 0.181, 16032/60000 datapoints
2025-03-06 18:24:33,555 - INFO - training batch 551, loss: 0.364, 17632/60000 datapoints
2025-03-06 18:24:33,749 - INFO - training batch 601, loss: 0.273, 19232/60000 datapoints
2025-03-06 18:24:33,941 - INFO - training batch 651, loss: 0.436, 20832/60000 datapoints
2025-03-06 18:24:34,136 - INFO - training batch 701, loss: 0.241, 22432/60000 datapoints
2025-03-06 18:24:34,328 - INFO - training batch 751, loss: 0.385, 24032/60000 datapoints
2025-03-06 18:24:34,520 - INFO - training batch 801, loss: 0.390, 25632/60000 datapoints
2025-03-06 18:24:34,716 - INFO - training batch 851, loss: 0.309, 27232/60000 datapoints
2025-03-06 18:24:34,911 - INFO - training batch 901, loss: 0.413, 28832/60000 datapoints
2025-03-06 18:24:35,103 - INFO - training batch 951, loss: 0.320, 30432/60000 datapoints
2025-03-06 18:24:35,294 - INFO - training batch 1001, loss: 0.245, 32032/60000 datapoints
2025-03-06 18:24:35,485 - INFO - training batch 1051, loss: 0.229, 33632/60000 datapoints
2025-03-06 18:24:35,679 - INFO - training batch 1101, loss: 0.297, 35232/60000 datapoints
2025-03-06 18:24:35,870 - INFO - training batch 1151, loss: 0.278, 36832/60000 datapoints
2025-03-06 18:24:36,061 - INFO - training batch 1201, loss: 0.500, 38432/60000 datapoints
2025-03-06 18:24:36,257 - INFO - training batch 1251, loss: 0.397, 40032/60000 datapoints
2025-03-06 18:24:36,449 - INFO - training batch 1301, loss: 0.497, 41632/60000 datapoints
2025-03-06 18:24:36,643 - INFO - training batch 1351, loss: 0.170, 43232/60000 datapoints
2025-03-06 18:24:36,835 - INFO - training batch 1401, loss: 0.348, 44832/60000 datapoints
2025-03-06 18:24:37,026 - INFO - training batch 1451, loss: 0.366, 46432/60000 datapoints
2025-03-06 18:24:37,215 - INFO - training batch 1501, loss: 0.331, 48032/60000 datapoints
2025-03-06 18:24:37,406 - INFO - training batch 1551, loss: 0.230, 49632/60000 datapoints
2025-03-06 18:24:37,608 - INFO - training batch 1601, loss: 0.193, 51232/60000 datapoints
2025-03-06 18:24:37,803 - INFO - training batch 1651, loss: 0.348, 52832/60000 datapoints
2025-03-06 18:24:37,994 - INFO - training batch 1701, loss: 0.252, 54432/60000 datapoints
2025-03-06 18:24:38,190 - INFO - training batch 1751, loss: 0.265, 56032/60000 datapoints
2025-03-06 18:24:38,380 - INFO - training batch 1801, loss: 0.145, 57632/60000 datapoints
2025-03-06 18:24:38,570 - INFO - training batch 1851, loss: 0.177, 59232/60000 datapoints
2025-03-06 18:24:38,670 - INFO - validation batch 1, loss: 0.388, 32/10016 datapoints
2025-03-06 18:24:38,821 - INFO - validation batch 51, loss: 0.168, 1632/10016 datapoints
2025-03-06 18:24:38,972 - INFO - validation batch 101, loss: 0.230, 3232/10016 datapoints
2025-03-06 18:24:39,138 - INFO - validation batch 151, loss: 0.406, 4832/10016 datapoints
2025-03-06 18:24:39,291 - INFO - validation batch 201, loss: 0.325, 6432/10016 datapoints
2025-03-06 18:24:39,457 - INFO - validation batch 251, loss: 0.410, 8032/10016 datapoints
2025-03-06 18:24:39,610 - INFO - validation batch 301, loss: 0.402, 9632/10016 datapoints
2025-03-06 18:24:39,645 - INFO - Epoch 292/800 done.
2025-03-06 18:24:39,646 - INFO - Final validation performance:
Loss: 0.333, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 18:24:39,646 - INFO - Beginning epoch 293/800
2025-03-06 18:24:39,652 - INFO - training batch 1, loss: 0.336, 32/60000 datapoints
2025-03-06 18:24:39,846 - INFO - training batch 51, loss: 0.615, 1632/60000 datapoints
2025-03-06 18:24:40,049 - INFO - training batch 101, loss: 0.239, 3232/60000 datapoints
2025-03-06 18:24:40,244 - INFO - training batch 151, loss: 0.198, 4832/60000 datapoints
2025-03-06 18:24:40,436 - INFO - training batch 201, loss: 0.445, 6432/60000 datapoints
2025-03-06 18:24:40,634 - INFO - training batch 251, loss: 0.481, 8032/60000 datapoints
2025-03-06 18:24:40,828 - INFO - training batch 301, loss: 0.251, 9632/60000 datapoints
2025-03-06 18:24:41,031 - INFO - training batch 351, loss: 0.265, 11232/60000 datapoints
2025-03-06 18:24:41,248 - INFO - training batch 401, loss: 0.145, 12832/60000 datapoints
2025-03-06 18:24:41,441 - INFO - training batch 451, loss: 0.084, 14432/60000 datapoints
2025-03-06 18:24:41,634 - INFO - training batch 501, loss: 0.230, 16032/60000 datapoints
2025-03-06 18:24:41,827 - INFO - training batch 551, loss: 0.112, 17632/60000 datapoints
2025-03-06 18:24:42,019 - INFO - training batch 601, loss: 0.374, 19232/60000 datapoints
2025-03-06 18:24:42,214 - INFO - training batch 651, loss: 0.195, 20832/60000 datapoints
2025-03-06 18:24:42,406 - INFO - training batch 701, loss: 0.392, 22432/60000 datapoints
2025-03-06 18:24:42,600 - INFO - training batch 751, loss: 0.316, 24032/60000 datapoints
2025-03-06 18:24:42,791 - INFO - training batch 801, loss: 0.228, 25632/60000 datapoints
2025-03-06 18:24:42,983 - INFO - training batch 851, loss: 0.471, 27232/60000 datapoints
2025-03-06 18:24:43,174 - INFO - training batch 901, loss: 0.438, 28832/60000 datapoints
2025-03-06 18:24:43,369 - INFO - training batch 951, loss: 0.133, 30432/60000 datapoints
2025-03-06 18:24:43,559 - INFO - training batch 1001, loss: 0.151, 32032/60000 datapoints
2025-03-06 18:24:43,753 - INFO - training batch 1051, loss: 0.305, 33632/60000 datapoints
2025-03-06 18:24:43,946 - INFO - training batch 1101, loss: 0.152, 35232/60000 datapoints
2025-03-06 18:24:44,140 - INFO - training batch 1151, loss: 0.246, 36832/60000 datapoints
2025-03-06 18:24:44,332 - INFO - training batch 1201, loss: 0.154, 38432/60000 datapoints
2025-03-06 18:24:44,523 - INFO - training batch 1251, loss: 0.285, 40032/60000 datapoints
2025-03-06 18:24:44,715 - INFO - training batch 1301, loss: 0.166, 41632/60000 datapoints
2025-03-06 18:24:44,912 - INFO - training batch 1351, loss: 0.375, 43232/60000 datapoints
2025-03-06 18:24:45,105 - INFO - training batch 1401, loss: 0.281, 44832/60000 datapoints
2025-03-06 18:24:45,295 - INFO - training batch 1451, loss: 0.479, 46432/60000 datapoints
2025-03-06 18:24:45,538 - INFO - training batch 1501, loss: 0.348, 48032/60000 datapoints
2025-03-06 18:24:45,732 - INFO - training batch 1551, loss: 0.344, 49632/60000 datapoints
2025-03-06 18:24:45,924 - INFO - training batch 1601, loss: 0.436, 51232/60000 datapoints
2025-03-06 18:24:46,116 - INFO - training batch 1651, loss: 0.460, 52832/60000 datapoints
2025-03-06 18:24:46,310 - INFO - training batch 1701, loss: 0.200, 54432/60000 datapoints
2025-03-06 18:24:46,502 - INFO - training batch 1751, loss: 0.170, 56032/60000 datapoints
2025-03-06 18:24:46,696 - INFO - training batch 1801, loss: 0.240, 57632/60000 datapoints
2025-03-06 18:24:46,888 - INFO - training batch 1851, loss: 0.359, 59232/60000 datapoints
2025-03-06 18:24:46,987 - INFO - validation batch 1, loss: 0.323, 32/10016 datapoints
2025-03-06 18:24:47,137 - INFO - validation batch 51, loss: 0.220, 1632/10016 datapoints
2025-03-06 18:24:47,286 - INFO - validation batch 101, loss: 0.202, 3232/10016 datapoints
2025-03-06 18:24:47,438 - INFO - validation batch 151, loss: 0.375, 4832/10016 datapoints
2025-03-06 18:24:47,591 - INFO - validation batch 201, loss: 0.267, 6432/10016 datapoints
2025-03-06 18:24:47,742 - INFO - validation batch 251, loss: 0.298, 8032/10016 datapoints
2025-03-06 18:24:47,893 - INFO - validation batch 301, loss: 0.172, 9632/10016 datapoints
2025-03-06 18:24:47,930 - INFO - Epoch 293/800 done.
2025-03-06 18:24:47,930 - INFO - Final validation performance:
Loss: 0.265, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 18:24:47,930 - INFO - Beginning epoch 294/800
2025-03-06 18:24:47,936 - INFO - training batch 1, loss: 0.321, 32/60000 datapoints
2025-03-06 18:24:48,128 - INFO - training batch 51, loss: 0.428, 1632/60000 datapoints
2025-03-06 18:24:48,321 - INFO - training batch 101, loss: 0.238, 3232/60000 datapoints
2025-03-06 18:24:48,512 - INFO - training batch 151, loss: 0.175, 4832/60000 datapoints
2025-03-06 18:24:48,704 - INFO - training batch 201, loss: 0.193, 6432/60000 datapoints
2025-03-06 18:24:48,895 - INFO - training batch 251, loss: 0.283, 8032/60000 datapoints
2025-03-06 18:24:49,088 - INFO - training batch 301, loss: 0.212, 9632/60000 datapoints
2025-03-06 18:24:49,279 - INFO - training batch 351, loss: 0.197, 11232/60000 datapoints
2025-03-06 18:24:49,493 - INFO - training batch 401, loss: 0.200, 12832/60000 datapoints
2025-03-06 18:24:49,686 - INFO - training batch 451, loss: 0.154, 14432/60000 datapoints
2025-03-06 18:24:49,880 - INFO - training batch 501, loss: 0.326, 16032/60000 datapoints
2025-03-06 18:24:50,073 - INFO - training batch 551, loss: 0.675, 17632/60000 datapoints
2025-03-06 18:24:50,268 - INFO - training batch 601, loss: 0.570, 19232/60000 datapoints
2025-03-06 18:24:50,462 - INFO - training batch 651, loss: 0.160, 20832/60000 datapoints
2025-03-06 18:24:50,654 - INFO - training batch 701, loss: 0.202, 22432/60000 datapoints
2025-03-06 18:24:50,848 - INFO - training batch 751, loss: 0.230, 24032/60000 datapoints
2025-03-06 18:24:51,041 - INFO - training batch 801, loss: 0.405, 25632/60000 datapoints
2025-03-06 18:24:51,231 - INFO - training batch 851, loss: 0.535, 27232/60000 datapoints
2025-03-06 18:24:51,420 - INFO - training batch 901, loss: 0.364, 28832/60000 datapoints
2025-03-06 18:24:51,618 - INFO - training batch 951, loss: 0.190, 30432/60000 datapoints
2025-03-06 18:24:51,813 - INFO - training batch 1001, loss: 0.335, 32032/60000 datapoints
2025-03-06 18:24:52,007 - INFO - training batch 1051, loss: 0.180, 33632/60000 datapoints
2025-03-06 18:24:52,203 - INFO - training batch 1101, loss: 0.218, 35232/60000 datapoints
2025-03-06 18:24:52,398 - INFO - training batch 1151, loss: 0.628, 36832/60000 datapoints
2025-03-06 18:24:52,592 - INFO - training batch 1201, loss: 0.375, 38432/60000 datapoints
2025-03-06 18:24:52,790 - INFO - training batch 1251, loss: 0.320, 40032/60000 datapoints
2025-03-06 18:24:52,985 - INFO - training batch 1301, loss: 0.377, 41632/60000 datapoints
2025-03-06 18:24:53,180 - INFO - training batch 1351, loss: 0.204, 43232/60000 datapoints
2025-03-06 18:24:53,374 - INFO - training batch 1401, loss: 0.181, 44832/60000 datapoints
2025-03-06 18:24:53,569 - INFO - training batch 1451, loss: 0.290, 46432/60000 datapoints
2025-03-06 18:24:53,764 - INFO - training batch 1501, loss: 0.369, 48032/60000 datapoints
2025-03-06 18:24:53,958 - INFO - training batch 1551, loss: 0.180, 49632/60000 datapoints
2025-03-06 18:24:54,154 - INFO - training batch 1601, loss: 0.074, 51232/60000 datapoints
2025-03-06 18:24:54,347 - INFO - training batch 1651, loss: 0.261, 52832/60000 datapoints
2025-03-06 18:24:54,539 - INFO - training batch 1701, loss: 0.234, 54432/60000 datapoints
2025-03-06 18:24:54,735 - INFO - training batch 1751, loss: 0.223, 56032/60000 datapoints
2025-03-06 18:24:54,934 - INFO - training batch 1801, loss: 0.168, 57632/60000 datapoints
2025-03-06 18:24:55,128 - INFO - training batch 1851, loss: 0.311, 59232/60000 datapoints
2025-03-06 18:24:55,229 - INFO - validation batch 1, loss: 0.167, 32/10016 datapoints
2025-03-06 18:24:55,382 - INFO - validation batch 51, loss: 0.166, 1632/10016 datapoints
2025-03-06 18:24:55,535 - INFO - validation batch 101, loss: 0.276, 3232/10016 datapoints
2025-03-06 18:24:55,691 - INFO - validation batch 151, loss: 0.233, 4832/10016 datapoints
2025-03-06 18:24:55,843 - INFO - validation batch 201, loss: 0.338, 6432/10016 datapoints
2025-03-06 18:24:55,995 - INFO - validation batch 251, loss: 0.603, 8032/10016 datapoints
2025-03-06 18:24:56,151 - INFO - validation batch 301, loss: 0.100, 9632/10016 datapoints
2025-03-06 18:24:56,191 - INFO - Epoch 294/800 done.
2025-03-06 18:24:56,191 - INFO - Final validation performance:
Loss: 0.269, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 18:24:56,191 - INFO - Beginning epoch 295/800
2025-03-06 18:24:56,197 - INFO - training batch 1, loss: 0.489, 32/60000 datapoints
2025-03-06 18:24:56,391 - INFO - training batch 51, loss: 0.496, 1632/60000 datapoints
2025-03-06 18:24:56,586 - INFO - training batch 101, loss: 0.147, 3232/60000 datapoints
2025-03-06 18:24:56,780 - INFO - training batch 151, loss: 0.134, 4832/60000 datapoints
2025-03-06 18:24:56,974 - INFO - training batch 201, loss: 0.168, 6432/60000 datapoints
2025-03-06 18:24:57,168 - INFO - training batch 251, loss: 0.325, 8032/60000 datapoints
2025-03-06 18:24:57,362 - INFO - training batch 301, loss: 0.151, 9632/60000 datapoints
2025-03-06 18:24:57,558 - INFO - training batch 351, loss: 0.225, 11232/60000 datapoints
2025-03-06 18:24:57,754 - INFO - training batch 401, loss: 0.437, 12832/60000 datapoints
2025-03-06 18:24:57,948 - INFO - training batch 451, loss: 0.505, 14432/60000 datapoints
2025-03-06 18:24:58,142 - INFO - training batch 501, loss: 0.311, 16032/60000 datapoints
2025-03-06 18:24:58,339 - INFO - training batch 551, loss: 0.452, 17632/60000 datapoints
2025-03-06 18:24:58,534 - INFO - training batch 601, loss: 0.617, 19232/60000 datapoints
2025-03-06 18:24:58,731 - INFO - training batch 651, loss: 0.218, 20832/60000 datapoints
2025-03-06 18:24:58,926 - INFO - training batch 701, loss: 0.248, 22432/60000 datapoints
2025-03-06 18:24:59,119 - INFO - training batch 751, loss: 0.523, 24032/60000 datapoints
2025-03-06 18:24:59,313 - INFO - training batch 801, loss: 0.157, 25632/60000 datapoints
2025-03-06 18:24:59,519 - INFO - training batch 851, loss: 0.209, 27232/60000 datapoints
2025-03-06 18:24:59,717 - INFO - training batch 901, loss: 0.628, 28832/60000 datapoints
2025-03-06 18:24:59,911 - INFO - training batch 951, loss: 0.315, 30432/60000 datapoints
2025-03-06 18:25:00,105 - INFO - training batch 1001, loss: 0.299, 32032/60000 datapoints
2025-03-06 18:25:00,301 - INFO - training batch 1051, loss: 0.392, 33632/60000 datapoints
2025-03-06 18:25:00,495 - INFO - training batch 1101, loss: 0.730, 35232/60000 datapoints
2025-03-06 18:25:00,690 - INFO - training batch 1151, loss: 0.172, 36832/60000 datapoints
2025-03-06 18:25:00,887 - INFO - training batch 1201, loss: 0.249, 38432/60000 datapoints
2025-03-06 18:25:01,085 - INFO - training batch 1251, loss: 0.221, 40032/60000 datapoints
2025-03-06 18:25:01,276 - INFO - training batch 1301, loss: 0.192, 41632/60000 datapoints
2025-03-06 18:25:01,471 - INFO - training batch 1351, loss: 0.569, 43232/60000 datapoints
2025-03-06 18:25:01,669 - INFO - training batch 1401, loss: 0.373, 44832/60000 datapoints
2025-03-06 18:25:01,865 - INFO - training batch 1451, loss: 0.331, 46432/60000 datapoints
2025-03-06 18:25:02,058 - INFO - training batch 1501, loss: 0.401, 48032/60000 datapoints
2025-03-06 18:25:02,255 - INFO - training batch 1551, loss: 0.426, 49632/60000 datapoints
2025-03-06 18:25:02,448 - INFO - training batch 1601, loss: 0.405, 51232/60000 datapoints
2025-03-06 18:25:02,643 - INFO - training batch 1651, loss: 0.496, 52832/60000 datapoints
2025-03-06 18:25:02,842 - INFO - training batch 1701, loss: 0.354, 54432/60000 datapoints
2025-03-06 18:25:03,035 - INFO - training batch 1751, loss: 0.129, 56032/60000 datapoints
2025-03-06 18:25:03,233 - INFO - training batch 1801, loss: 0.308, 57632/60000 datapoints
2025-03-06 18:25:03,428 - INFO - training batch 1851, loss: 0.315, 59232/60000 datapoints
2025-03-06 18:25:03,529 - INFO - validation batch 1, loss: 0.284, 32/10016 datapoints
2025-03-06 18:25:03,686 - INFO - validation batch 51, loss: 0.330, 1632/10016 datapoints
2025-03-06 18:25:03,839 - INFO - validation batch 101, loss: 0.235, 3232/10016 datapoints
2025-03-06 18:25:03,991 - INFO - validation batch 151, loss: 0.540, 4832/10016 datapoints
2025-03-06 18:25:04,145 - INFO - validation batch 201, loss: 0.287, 6432/10016 datapoints
2025-03-06 18:25:04,302 - INFO - validation batch 251, loss: 0.147, 8032/10016 datapoints
2025-03-06 18:25:04,455 - INFO - validation batch 301, loss: 0.264, 9632/10016 datapoints
2025-03-06 18:25:04,493 - INFO - Epoch 295/800 done.
2025-03-06 18:25:04,493 - INFO - Final validation performance:
Loss: 0.298, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 18:25:04,493 - INFO - Beginning epoch 296/800
2025-03-06 18:25:04,499 - INFO - training batch 1, loss: 0.281, 32/60000 datapoints
2025-03-06 18:25:04,699 - INFO - training batch 51, loss: 0.148, 1632/60000 datapoints
2025-03-06 18:25:04,899 - INFO - training batch 101, loss: 0.215, 3232/60000 datapoints
2025-03-06 18:25:05,095 - INFO - training batch 151, loss: 0.352, 4832/60000 datapoints
2025-03-06 18:25:05,290 - INFO - training batch 201, loss: 0.170, 6432/60000 datapoints
2025-03-06 18:25:05,485 - INFO - training batch 251, loss: 0.350, 8032/60000 datapoints
2025-03-06 18:25:05,682 - INFO - training batch 301, loss: 0.388, 9632/60000 datapoints
2025-03-06 18:25:05,880 - INFO - training batch 351, loss: 0.424, 11232/60000 datapoints
2025-03-06 18:25:06,074 - INFO - training batch 401, loss: 0.160, 12832/60000 datapoints
2025-03-06 18:25:06,274 - INFO - training batch 451, loss: 0.229, 14432/60000 datapoints
2025-03-06 18:25:06,469 - INFO - training batch 501, loss: 0.265, 16032/60000 datapoints
2025-03-06 18:25:06,667 - INFO - training batch 551, loss: 0.503, 17632/60000 datapoints
2025-03-06 18:25:06,863 - INFO - training batch 601, loss: 0.360, 19232/60000 datapoints
2025-03-06 18:25:07,057 - INFO - training batch 651, loss: 0.241, 20832/60000 datapoints
2025-03-06 18:25:07,251 - INFO - training batch 701, loss: 0.128, 22432/60000 datapoints
2025-03-06 18:25:07,444 - INFO - training batch 751, loss: 0.359, 24032/60000 datapoints
2025-03-06 18:25:07,655 - INFO - training batch 801, loss: 0.297, 25632/60000 datapoints
2025-03-06 18:25:07,878 - INFO - training batch 851, loss: 0.159, 27232/60000 datapoints
2025-03-06 18:25:08,084 - INFO - training batch 901, loss: 0.284, 28832/60000 datapoints
2025-03-06 18:25:08,285 - INFO - training batch 951, loss: 0.198, 30432/60000 datapoints
2025-03-06 18:25:08,477 - INFO - training batch 1001, loss: 0.461, 32032/60000 datapoints
2025-03-06 18:25:08,674 - INFO - training batch 1051, loss: 0.204, 33632/60000 datapoints
2025-03-06 18:25:08,868 - INFO - training batch 1101, loss: 0.216, 35232/60000 datapoints
2025-03-06 18:25:09,063 - INFO - training batch 1151, loss: 0.245, 36832/60000 datapoints
2025-03-06 18:25:09,258 - INFO - training batch 1201, loss: 0.555, 38432/60000 datapoints
2025-03-06 18:25:09,452 - INFO - training batch 1251, loss: 0.543, 40032/60000 datapoints
2025-03-06 18:25:09,670 - INFO - training batch 1301, loss: 0.310, 41632/60000 datapoints
2025-03-06 18:25:09,865 - INFO - training batch 1351, loss: 0.249, 43232/60000 datapoints
2025-03-06 18:25:10,062 - INFO - training batch 1401, loss: 0.743, 44832/60000 datapoints
2025-03-06 18:25:10,259 - INFO - training batch 1451, loss: 0.314, 46432/60000 datapoints
2025-03-06 18:25:10,453 - INFO - training batch 1501, loss: 0.293, 48032/60000 datapoints
2025-03-06 18:25:10,651 - INFO - training batch 1551, loss: 0.403, 49632/60000 datapoints
2025-03-06 18:25:10,847 - INFO - training batch 1601, loss: 0.136, 51232/60000 datapoints
2025-03-06 18:25:11,044 - INFO - training batch 1651, loss: 0.442, 52832/60000 datapoints
2025-03-06 18:25:11,238 - INFO - training batch 1701, loss: 0.174, 54432/60000 datapoints
2025-03-06 18:25:11,433 - INFO - training batch 1751, loss: 0.460, 56032/60000 datapoints
2025-03-06 18:25:11,631 - INFO - training batch 1801, loss: 0.605, 57632/60000 datapoints
2025-03-06 18:25:11,829 - INFO - training batch 1851, loss: 0.426, 59232/60000 datapoints
2025-03-06 18:25:11,932 - INFO - validation batch 1, loss: 0.270, 32/10016 datapoints
2025-03-06 18:25:12,087 - INFO - validation batch 51, loss: 0.212, 1632/10016 datapoints
2025-03-06 18:25:12,243 - INFO - validation batch 101, loss: 0.190, 3232/10016 datapoints
2025-03-06 18:25:12,395 - INFO - validation batch 151, loss: 0.337, 4832/10016 datapoints
2025-03-06 18:25:12,551 - INFO - validation batch 201, loss: 0.122, 6432/10016 datapoints
2025-03-06 18:25:12,704 - INFO - validation batch 251, loss: 0.276, 8032/10016 datapoints
2025-03-06 18:25:12,857 - INFO - validation batch 301, loss: 0.421, 9632/10016 datapoints
2025-03-06 18:25:12,895 - INFO - Epoch 296/800 done.
2025-03-06 18:25:12,895 - INFO - Final validation performance:
Loss: 0.261, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 18:25:12,895 - INFO - Beginning epoch 297/800
2025-03-06 18:25:12,901 - INFO - training batch 1, loss: 0.325, 32/60000 datapoints
2025-03-06 18:25:13,097 - INFO - training batch 51, loss: 0.409, 1632/60000 datapoints
2025-03-06 18:25:13,294 - INFO - training batch 101, loss: 0.178, 3232/60000 datapoints
2025-03-06 18:25:13,490 - INFO - training batch 151, loss: 0.391, 4832/60000 datapoints
2025-03-06 18:25:13,689 - INFO - training batch 201, loss: 0.100, 6432/60000 datapoints
2025-03-06 18:25:13,890 - INFO - training batch 251, loss: 0.228, 8032/60000 datapoints
2025-03-06 18:25:14,089 - INFO - training batch 301, loss: 0.581, 9632/60000 datapoints
2025-03-06 18:25:14,286 - INFO - training batch 351, loss: 0.452, 11232/60000 datapoints
2025-03-06 18:25:14,481 - INFO - training batch 401, loss: 0.341, 12832/60000 datapoints
2025-03-06 18:25:14,675 - INFO - training batch 451, loss: 0.222, 14432/60000 datapoints
2025-03-06 18:25:14,870 - INFO - training batch 501, loss: 0.267, 16032/60000 datapoints
2025-03-06 18:25:15,069 - INFO - training batch 551, loss: 0.135, 17632/60000 datapoints
2025-03-06 18:25:15,264 - INFO - training batch 601, loss: 0.598, 19232/60000 datapoints
2025-03-06 18:25:15,458 - INFO - training batch 651, loss: 0.226, 20832/60000 datapoints
2025-03-06 18:25:15,653 - INFO - training batch 701, loss: 0.326, 22432/60000 datapoints
2025-03-06 18:25:15,849 - INFO - training batch 751, loss: 0.270, 24032/60000 datapoints
2025-03-06 18:25:16,043 - INFO - training batch 801, loss: 0.154, 25632/60000 datapoints
2025-03-06 18:25:16,242 - INFO - training batch 851, loss: 0.456, 27232/60000 datapoints
2025-03-06 18:25:16,438 - INFO - training batch 901, loss: 0.184, 28832/60000 datapoints
2025-03-06 18:25:16,636 - INFO - training batch 951, loss: 0.313, 30432/60000 datapoints
2025-03-06 18:25:16,831 - INFO - training batch 1001, loss: 0.270, 32032/60000 datapoints
2025-03-06 18:25:17,025 - INFO - training batch 1051, loss: 0.167, 33632/60000 datapoints
2025-03-06 18:25:17,220 - INFO - training batch 1101, loss: 0.113, 35232/60000 datapoints
2025-03-06 18:25:17,414 - INFO - training batch 1151, loss: 0.123, 36832/60000 datapoints
2025-03-06 18:25:17,612 - INFO - training batch 1201, loss: 0.584, 38432/60000 datapoints
2025-03-06 18:25:17,809 - INFO - training batch 1251, loss: 0.162, 40032/60000 datapoints
2025-03-06 18:25:18,005 - INFO - training batch 1301, loss: 0.690, 41632/60000 datapoints
2025-03-06 18:25:18,207 - INFO - training batch 1351, loss: 0.337, 43232/60000 datapoints
2025-03-06 18:25:18,416 - INFO - training batch 1401, loss: 0.432, 44832/60000 datapoints
2025-03-06 18:25:18,613 - INFO - training batch 1451, loss: 0.189, 46432/60000 datapoints
2025-03-06 18:25:18,807 - INFO - training batch 1501, loss: 0.285, 48032/60000 datapoints
2025-03-06 18:25:19,002 - INFO - training batch 1551, loss: 0.702, 49632/60000 datapoints
2025-03-06 18:25:19,194 - INFO - training batch 1601, loss: 0.273, 51232/60000 datapoints
2025-03-06 18:25:19,389 - INFO - training batch 1651, loss: 0.195, 52832/60000 datapoints
2025-03-06 18:25:19,582 - INFO - training batch 1701, loss: 0.307, 54432/60000 datapoints
2025-03-06 18:25:19,799 - INFO - training batch 1751, loss: 0.146, 56032/60000 datapoints
2025-03-06 18:25:19,993 - INFO - training batch 1801, loss: 0.351, 57632/60000 datapoints
2025-03-06 18:25:20,184 - INFO - training batch 1851, loss: 0.252, 59232/60000 datapoints
2025-03-06 18:25:20,291 - INFO - validation batch 1, loss: 0.237, 32/10016 datapoints
2025-03-06 18:25:20,444 - INFO - validation batch 51, loss: 0.126, 1632/10016 datapoints
2025-03-06 18:25:20,598 - INFO - validation batch 101, loss: 0.259, 3232/10016 datapoints
2025-03-06 18:25:20,753 - INFO - validation batch 151, loss: 0.159, 4832/10016 datapoints
2025-03-06 18:25:20,908 - INFO - validation batch 201, loss: 0.270, 6432/10016 datapoints
2025-03-06 18:25:21,062 - INFO - validation batch 251, loss: 0.321, 8032/10016 datapoints
2025-03-06 18:25:21,216 - INFO - validation batch 301, loss: 0.253, 9632/10016 datapoints
2025-03-06 18:25:21,251 - INFO - Epoch 297/800 done.
2025-03-06 18:25:21,252 - INFO - Final validation performance:
Loss: 0.232, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 18:25:21,252 - INFO - Beginning epoch 298/800
2025-03-06 18:25:21,259 - INFO - training batch 1, loss: 0.356, 32/60000 datapoints
2025-03-06 18:25:21,458 - INFO - training batch 51, loss: 0.209, 1632/60000 datapoints
2025-03-06 18:25:21,655 - INFO - training batch 101, loss: 0.246, 3232/60000 datapoints
2025-03-06 18:25:21,852 - INFO - training batch 151, loss: 0.317, 4832/60000 datapoints
2025-03-06 18:25:22,045 - INFO - training batch 201, loss: 0.267, 6432/60000 datapoints
2025-03-06 18:25:22,239 - INFO - training batch 251, loss: 0.320, 8032/60000 datapoints
2025-03-06 18:25:22,438 - INFO - training batch 301, loss: 0.224, 9632/60000 datapoints
2025-03-06 18:25:22,636 - INFO - training batch 351, loss: 0.254, 11232/60000 datapoints
2025-03-06 18:25:22,830 - INFO - training batch 401, loss: 0.380, 12832/60000 datapoints
2025-03-06 18:25:23,025 - INFO - training batch 451, loss: 0.295, 14432/60000 datapoints
2025-03-06 18:25:23,218 - INFO - training batch 501, loss: 0.356, 16032/60000 datapoints
2025-03-06 18:25:23,418 - INFO - training batch 551, loss: 0.340, 17632/60000 datapoints
2025-03-06 18:25:23,614 - INFO - training batch 601, loss: 0.613, 19232/60000 datapoints
2025-03-06 18:25:23,809 - INFO - training batch 651, loss: 0.101, 20832/60000 datapoints
2025-03-06 18:25:24,004 - INFO - training batch 701, loss: 0.278, 22432/60000 datapoints
2025-03-06 18:25:24,197 - INFO - training batch 751, loss: 0.206, 24032/60000 datapoints
2025-03-06 18:25:24,397 - INFO - training batch 801, loss: 0.588, 25632/60000 datapoints
2025-03-06 18:25:24,589 - INFO - training batch 851, loss: 0.194, 27232/60000 datapoints
2025-03-06 18:25:24,783 - INFO - training batch 901, loss: 0.288, 28832/60000 datapoints
2025-03-06 18:25:24,981 - INFO - training batch 951, loss: 0.221, 30432/60000 datapoints
2025-03-06 18:25:25,174 - INFO - training batch 1001, loss: 0.214, 32032/60000 datapoints
2025-03-06 18:25:25,370 - INFO - training batch 1051, loss: 0.360, 33632/60000 datapoints
2025-03-06 18:25:25,563 - INFO - training batch 1101, loss: 0.575, 35232/60000 datapoints
2025-03-06 18:25:25,761 - INFO - training batch 1151, loss: 0.242, 36832/60000 datapoints
2025-03-06 18:25:25,956 - INFO - training batch 1201, loss: 0.113, 38432/60000 datapoints
2025-03-06 18:25:26,152 - INFO - training batch 1251, loss: 0.399, 40032/60000 datapoints
2025-03-06 18:25:26,352 - INFO - training batch 1301, loss: 0.099, 41632/60000 datapoints
2025-03-06 18:25:26,546 - INFO - training batch 1351, loss: 0.337, 43232/60000 datapoints
2025-03-06 18:25:26,741 - INFO - training batch 1401, loss: 0.255, 44832/60000 datapoints
2025-03-06 18:25:26,935 - INFO - training batch 1451, loss: 0.354, 46432/60000 datapoints
2025-03-06 18:25:27,130 - INFO - training batch 1501, loss: 0.349, 48032/60000 datapoints
2025-03-06 18:25:27,324 - INFO - training batch 1551, loss: 0.516, 49632/60000 datapoints
2025-03-06 18:25:27,520 - INFO - training batch 1601, loss: 0.339, 51232/60000 datapoints
2025-03-06 18:25:27,732 - INFO - training batch 1651, loss: 0.171, 52832/60000 datapoints
2025-03-06 18:25:27,936 - INFO - training batch 1701, loss: 0.213, 54432/60000 datapoints
2025-03-06 18:25:28,134 - INFO - training batch 1751, loss: 0.295, 56032/60000 datapoints
2025-03-06 18:25:28,338 - INFO - training batch 1801, loss: 0.366, 57632/60000 datapoints
2025-03-06 18:25:28,539 - INFO - training batch 1851, loss: 0.408, 59232/60000 datapoints
2025-03-06 18:25:28,645 - INFO - validation batch 1, loss: 0.430, 32/10016 datapoints
2025-03-06 18:25:28,796 - INFO - validation batch 51, loss: 0.416, 1632/10016 datapoints
2025-03-06 18:25:28,949 - INFO - validation batch 101, loss: 0.291, 3232/10016 datapoints
2025-03-06 18:25:29,101 - INFO - validation batch 151, loss: 0.317, 4832/10016 datapoints
2025-03-06 18:25:29,253 - INFO - validation batch 201, loss: 0.392, 6432/10016 datapoints
2025-03-06 18:25:29,409 - INFO - validation batch 251, loss: 0.362, 8032/10016 datapoints
2025-03-06 18:25:29,563 - INFO - validation batch 301, loss: 0.257, 9632/10016 datapoints
2025-03-06 18:25:29,601 - INFO - Epoch 298/800 done.
2025-03-06 18:25:29,601 - INFO - Final validation performance:
Loss: 0.352, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 18:25:29,602 - INFO - Beginning epoch 299/800
2025-03-06 18:25:29,609 - INFO - training batch 1, loss: 0.205, 32/60000 datapoints
2025-03-06 18:25:29,824 - INFO - training batch 51, loss: 0.186, 1632/60000 datapoints
2025-03-06 18:25:30,019 - INFO - training batch 101, loss: 0.331, 3232/60000 datapoints
2025-03-06 18:25:30,211 - INFO - training batch 151, loss: 0.418, 4832/60000 datapoints
2025-03-06 18:25:30,406 - INFO - training batch 201, loss: 0.448, 6432/60000 datapoints
2025-03-06 18:25:30,601 - INFO - training batch 251, loss: 0.446, 8032/60000 datapoints
2025-03-06 18:25:30,829 - INFO - training batch 301, loss: 0.357, 9632/60000 datapoints
2025-03-06 18:25:31,027 - INFO - training batch 351, loss: 0.230, 11232/60000 datapoints
2025-03-06 18:25:31,217 - INFO - training batch 401, loss: 0.385, 12832/60000 datapoints
2025-03-06 18:25:31,410 - INFO - training batch 451, loss: 0.349, 14432/60000 datapoints
2025-03-06 18:25:31,605 - INFO - training batch 501, loss: 0.334, 16032/60000 datapoints
2025-03-06 18:25:31,797 - INFO - training batch 551, loss: 0.153, 17632/60000 datapoints
2025-03-06 18:25:31,989 - INFO - training batch 601, loss: 0.133, 19232/60000 datapoints
2025-03-06 18:25:32,185 - INFO - training batch 651, loss: 0.442, 20832/60000 datapoints
2025-03-06 18:25:32,383 - INFO - training batch 701, loss: 0.112, 22432/60000 datapoints
2025-03-06 18:25:32,579 - INFO - training batch 751, loss: 0.274, 24032/60000 datapoints
2025-03-06 18:25:32,775 - INFO - training batch 801, loss: 0.491, 25632/60000 datapoints
2025-03-06 18:25:32,971 - INFO - training batch 851, loss: 0.452, 27232/60000 datapoints
2025-03-06 18:25:33,165 - INFO - training batch 901, loss: 0.109, 28832/60000 datapoints
2025-03-06 18:25:33,364 - INFO - training batch 951, loss: 0.169, 30432/60000 datapoints
2025-03-06 18:25:33,560 - INFO - training batch 1001, loss: 0.313, 32032/60000 datapoints
2025-03-06 18:25:33,755 - INFO - training batch 1051, loss: 0.177, 33632/60000 datapoints
2025-03-06 18:25:33,949 - INFO - training batch 1101, loss: 0.141, 35232/60000 datapoints
2025-03-06 18:25:34,142 - INFO - training batch 1151, loss: 0.352, 36832/60000 datapoints
2025-03-06 18:25:34,338 - INFO - training batch 1201, loss: 0.396, 38432/60000 datapoints
2025-03-06 18:25:34,534 - INFO - training batch 1251, loss: 0.300, 40032/60000 datapoints
2025-03-06 18:25:34,730 - INFO - training batch 1301, loss: 0.139, 41632/60000 datapoints
2025-03-06 18:25:34,927 - INFO - training batch 1351, loss: 0.494, 43232/60000 datapoints
2025-03-06 18:25:35,122 - INFO - training batch 1401, loss: 0.269, 44832/60000 datapoints
2025-03-06 18:25:35,314 - INFO - training batch 1451, loss: 0.222, 46432/60000 datapoints
2025-03-06 18:25:35,509 - INFO - training batch 1501, loss: 0.432, 48032/60000 datapoints
2025-03-06 18:25:35,705 - INFO - training batch 1551, loss: 0.187, 49632/60000 datapoints
2025-03-06 18:25:35,901 - INFO - training batch 1601, loss: 0.172, 51232/60000 datapoints
2025-03-06 18:25:36,097 - INFO - training batch 1651, loss: 0.254, 52832/60000 datapoints
2025-03-06 18:25:36,293 - INFO - training batch 1701, loss: 0.304, 54432/60000 datapoints
2025-03-06 18:25:36,490 - INFO - training batch 1751, loss: 0.286, 56032/60000 datapoints
2025-03-06 18:25:36,687 - INFO - training batch 1801, loss: 0.245, 57632/60000 datapoints
2025-03-06 18:25:36,885 - INFO - training batch 1851, loss: 0.446, 59232/60000 datapoints
2025-03-06 18:25:36,986 - INFO - validation batch 1, loss: 0.363, 32/10016 datapoints
2025-03-06 18:25:37,138 - INFO - validation batch 51, loss: 0.214, 1632/10016 datapoints
2025-03-06 18:25:37,291 - INFO - validation batch 101, loss: 0.329, 3232/10016 datapoints
2025-03-06 18:25:37,443 - INFO - validation batch 151, loss: 0.134, 4832/10016 datapoints
2025-03-06 18:25:37,611 - INFO - validation batch 201, loss: 0.372, 6432/10016 datapoints
2025-03-06 18:25:37,764 - INFO - validation batch 251, loss: 0.141, 8032/10016 datapoints
2025-03-06 18:25:37,917 - INFO - validation batch 301, loss: 0.105, 9632/10016 datapoints
2025-03-06 18:25:37,952 - INFO - Epoch 299/800 done.
2025-03-06 18:25:37,952 - INFO - Final validation performance:
Loss: 0.237, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 18:25:37,953 - INFO - Beginning epoch 300/800
2025-03-06 18:25:37,960 - INFO - training batch 1, loss: 0.200, 32/60000 datapoints
2025-03-06 18:25:38,158 - INFO - training batch 51, loss: 0.107, 1632/60000 datapoints
2025-03-06 18:25:38,356 - INFO - training batch 101, loss: 0.553, 3232/60000 datapoints
2025-03-06 18:25:38,551 - INFO - training batch 151, loss: 0.341, 4832/60000 datapoints
2025-03-06 18:25:38,748 - INFO - training batch 201, loss: 0.218, 6432/60000 datapoints
2025-03-06 18:25:38,942 - INFO - training batch 251, loss: 0.203, 8032/60000 datapoints
2025-03-06 18:25:39,144 - INFO - training batch 301, loss: 0.189, 9632/60000 datapoints
2025-03-06 18:25:39,338 - INFO - training batch 351, loss: 0.447, 11232/60000 datapoints
2025-03-06 18:25:39,534 - INFO - training batch 401, loss: 0.229, 12832/60000 datapoints
2025-03-06 18:25:39,732 - INFO - training batch 451, loss: 0.126, 14432/60000 datapoints
2025-03-06 18:25:39,945 - INFO - training batch 501, loss: 0.430, 16032/60000 datapoints
2025-03-06 18:25:40,141 - INFO - training batch 551, loss: 0.222, 17632/60000 datapoints
2025-03-06 18:25:40,335 - INFO - training batch 601, loss: 0.245, 19232/60000 datapoints
2025-03-06 18:25:40,536 - INFO - training batch 651, loss: 0.482, 20832/60000 datapoints
2025-03-06 18:25:40,732 - INFO - training batch 701, loss: 0.396, 22432/60000 datapoints
2025-03-06 18:25:40,928 - INFO - training batch 751, loss: 0.480, 24032/60000 datapoints
2025-03-06 18:25:41,126 - INFO - training batch 801, loss: 0.473, 25632/60000 datapoints
2025-03-06 18:25:41,321 - INFO - training batch 851, loss: 0.352, 27232/60000 datapoints
2025-03-06 18:25:41,516 - INFO - training batch 901, loss: 0.265, 28832/60000 datapoints
2025-03-06 18:25:41,744 - INFO - training batch 951, loss: 0.186, 30432/60000 datapoints
2025-03-06 18:25:41,937 - INFO - training batch 1001, loss: 0.243, 32032/60000 datapoints
2025-03-06 18:25:42,131 - INFO - training batch 1051, loss: 0.088, 33632/60000 datapoints
2025-03-06 18:25:42,322 - INFO - training batch 1101, loss: 0.161, 35232/60000 datapoints
2025-03-06 18:25:42,519 - INFO - training batch 1151, loss: 0.169, 36832/60000 datapoints
2025-03-06 18:25:42,715 - INFO - training batch 1201, loss: 0.133, 38432/60000 datapoints
2025-03-06 18:25:42,913 - INFO - training batch 1251, loss: 0.327, 40032/60000 datapoints
2025-03-06 18:25:43,108 - INFO - training batch 1301, loss: 0.726, 41632/60000 datapoints
2025-03-06 18:25:43,303 - INFO - training batch 1351, loss: 0.223, 43232/60000 datapoints
2025-03-06 18:25:43,498 - INFO - training batch 1401, loss: 0.871, 44832/60000 datapoints
2025-03-06 18:25:43,697 - INFO - training batch 1451, loss: 0.237, 46432/60000 datapoints
2025-03-06 18:25:43,892 - INFO - training batch 1501, loss: 0.155, 48032/60000 datapoints
2025-03-06 18:25:44,084 - INFO - training batch 1551, loss: 0.397, 49632/60000 datapoints
2025-03-06 18:25:44,278 - INFO - training batch 1601, loss: 0.251, 51232/60000 datapoints
2025-03-06 18:25:44,474 - INFO - training batch 1651, loss: 0.169, 52832/60000 datapoints
2025-03-06 18:25:44,673 - INFO - training batch 1701, loss: 0.482, 54432/60000 datapoints
2025-03-06 18:25:44,871 - INFO - training batch 1751, loss: 0.230, 56032/60000 datapoints
2025-03-06 18:25:45,067 - INFO - training batch 1801, loss: 0.333, 57632/60000 datapoints
2025-03-06 18:25:45,262 - INFO - training batch 1851, loss: 0.221, 59232/60000 datapoints
2025-03-06 18:25:45,361 - INFO - validation batch 1, loss: 0.222, 32/10016 datapoints
2025-03-06 18:25:45,515 - INFO - validation batch 51, loss: 0.203, 1632/10016 datapoints
2025-03-06 18:25:45,671 - INFO - validation batch 101, loss: 0.491, 3232/10016 datapoints
2025-03-06 18:25:45,825 - INFO - validation batch 151, loss: 0.206, 4832/10016 datapoints
2025-03-06 18:25:45,978 - INFO - validation batch 201, loss: 0.370, 6432/10016 datapoints
2025-03-06 18:25:46,133 - INFO - validation batch 251, loss: 0.258, 8032/10016 datapoints
2025-03-06 18:25:46,288 - INFO - validation batch 301, loss: 0.289, 9632/10016 datapoints
2025-03-06 18:25:46,326 - INFO - Epoch 300/800 done.
2025-03-06 18:25:46,327 - INFO - Final validation performance:
Loss: 0.291, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:25:46,327 - INFO - Beginning epoch 301/800
2025-03-06 18:25:46,333 - INFO - training batch 1, loss: 0.189, 32/60000 datapoints
2025-03-06 18:25:46,530 - INFO - training batch 51, loss: 0.461, 1632/60000 datapoints
2025-03-06 18:25:46,729 - INFO - training batch 101, loss: 0.634, 3232/60000 datapoints
2025-03-06 18:25:46,924 - INFO - training batch 151, loss: 0.476, 4832/60000 datapoints
2025-03-06 18:25:47,118 - INFO - training batch 201, loss: 0.262, 6432/60000 datapoints
2025-03-06 18:25:47,312 - INFO - training batch 251, loss: 0.332, 8032/60000 datapoints
2025-03-06 18:25:47,505 - INFO - training batch 301, loss: 0.352, 9632/60000 datapoints
2025-03-06 18:25:47,702 - INFO - training batch 351, loss: 0.330, 11232/60000 datapoints
2025-03-06 18:25:47,895 - INFO - training batch 401, loss: 0.570, 12832/60000 datapoints
2025-03-06 18:25:48,091 - INFO - training batch 451, loss: 0.467, 14432/60000 datapoints
2025-03-06 18:25:48,284 - INFO - training batch 501, loss: 0.162, 16032/60000 datapoints
2025-03-06 18:25:48,482 - INFO - training batch 551, loss: 0.303, 17632/60000 datapoints
2025-03-06 18:25:48,680 - INFO - training batch 601, loss: 0.313, 19232/60000 datapoints
2025-03-06 18:25:48,877 - INFO - training batch 651, loss: 0.349, 20832/60000 datapoints
2025-03-06 18:25:49,071 - INFO - training batch 701, loss: 0.285, 22432/60000 datapoints
2025-03-06 18:25:49,264 - INFO - training batch 751, loss: 0.278, 24032/60000 datapoints
2025-03-06 18:25:49,459 - INFO - training batch 801, loss: 0.393, 25632/60000 datapoints
2025-03-06 18:25:49,659 - INFO - training batch 851, loss: 0.444, 27232/60000 datapoints
2025-03-06 18:25:49,857 - INFO - training batch 901, loss: 0.611, 28832/60000 datapoints
2025-03-06 18:25:50,071 - INFO - training batch 951, loss: 0.498, 30432/60000 datapoints
2025-03-06 18:25:50,267 - INFO - training batch 1001, loss: 0.203, 32032/60000 datapoints
2025-03-06 18:25:50,463 - INFO - training batch 1051, loss: 0.398, 33632/60000 datapoints
2025-03-06 18:25:50,659 - INFO - training batch 1101, loss: 0.329, 35232/60000 datapoints
2025-03-06 18:25:50,853 - INFO - training batch 1151, loss: 0.167, 36832/60000 datapoints
2025-03-06 18:25:51,048 - INFO - training batch 1201, loss: 0.194, 38432/60000 datapoints
2025-03-06 18:25:51,246 - INFO - training batch 1251, loss: 0.149, 40032/60000 datapoints
2025-03-06 18:25:51,440 - INFO - training batch 1301, loss: 0.253, 41632/60000 datapoints
2025-03-06 18:25:51,639 - INFO - training batch 1351, loss: 0.407, 43232/60000 datapoints
2025-03-06 18:25:51,836 - INFO - training batch 1401, loss: 0.247, 44832/60000 datapoints
2025-03-06 18:25:52,031 - INFO - training batch 1451, loss: 0.170, 46432/60000 datapoints
2025-03-06 18:25:52,226 - INFO - training batch 1501, loss: 0.149, 48032/60000 datapoints
2025-03-06 18:25:52,422 - INFO - training batch 1551, loss: 0.277, 49632/60000 datapoints
2025-03-06 18:25:52,620 - INFO - training batch 1601, loss: 0.246, 51232/60000 datapoints
2025-03-06 18:25:52,815 - INFO - training batch 1651, loss: 0.391, 52832/60000 datapoints
2025-03-06 18:25:53,010 - INFO - training batch 1701, loss: 0.188, 54432/60000 datapoints
2025-03-06 18:25:53,205 - INFO - training batch 1751, loss: 0.336, 56032/60000 datapoints
2025-03-06 18:25:53,400 - INFO - training batch 1801, loss: 0.152, 57632/60000 datapoints
2025-03-06 18:25:53,595 - INFO - training batch 1851, loss: 0.204, 59232/60000 datapoints
2025-03-06 18:25:53,699 - INFO - validation batch 1, loss: 0.219, 32/10016 datapoints
2025-03-06 18:25:53,851 - INFO - validation batch 51, loss: 0.555, 1632/10016 datapoints
2025-03-06 18:25:54,003 - INFO - validation batch 101, loss: 0.301, 3232/10016 datapoints
2025-03-06 18:25:54,155 - INFO - validation batch 151, loss: 0.386, 4832/10016 datapoints
2025-03-06 18:25:54,308 - INFO - validation batch 201, loss: 0.360, 6432/10016 datapoints
2025-03-06 18:25:54,465 - INFO - validation batch 251, loss: 0.560, 8032/10016 datapoints
2025-03-06 18:25:54,617 - INFO - validation batch 301, loss: 0.436, 9632/10016 datapoints
2025-03-06 18:25:54,653 - INFO - Epoch 301/800 done.
2025-03-06 18:25:54,654 - INFO - Final validation performance:
Loss: 0.402, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 18:25:54,654 - INFO - Beginning epoch 302/800
2025-03-06 18:25:54,661 - INFO - training batch 1, loss: 0.527, 32/60000 datapoints
2025-03-06 18:25:54,865 - INFO - training batch 51, loss: 0.397, 1632/60000 datapoints
2025-03-06 18:25:55,061 - INFO - training batch 101, loss: 0.516, 3232/60000 datapoints
2025-03-06 18:25:55,262 - INFO - training batch 151, loss: 0.205, 4832/60000 datapoints
2025-03-06 18:25:55,455 - INFO - training batch 201, loss: 0.397, 6432/60000 datapoints
2025-03-06 18:25:55,654 - INFO - training batch 251, loss: 0.224, 8032/60000 datapoints
2025-03-06 18:25:55,848 - INFO - training batch 301, loss: 0.401, 9632/60000 datapoints
2025-03-06 18:25:56,047 - INFO - training batch 351, loss: 0.221, 11232/60000 datapoints
2025-03-06 18:25:56,246 - INFO - training batch 401, loss: 0.291, 12832/60000 datapoints
2025-03-06 18:25:56,441 - INFO - training batch 451, loss: 0.136, 14432/60000 datapoints
2025-03-06 18:25:56,638 - INFO - training batch 501, loss: 0.518, 16032/60000 datapoints
2025-03-06 18:25:56,834 - INFO - training batch 551, loss: 0.332, 17632/60000 datapoints
2025-03-06 18:25:57,027 - INFO - training batch 601, loss: 0.278, 19232/60000 datapoints
2025-03-06 18:25:57,222 - INFO - training batch 651, loss: 0.143, 20832/60000 datapoints
2025-03-06 18:25:57,419 - INFO - training batch 701, loss: 0.125, 22432/60000 datapoints
2025-03-06 18:25:57,616 - INFO - training batch 751, loss: 0.397, 24032/60000 datapoints
2025-03-06 18:25:57,810 - INFO - training batch 801, loss: 0.181, 25632/60000 datapoints
2025-03-06 18:25:58,003 - INFO - training batch 851, loss: 0.434, 27232/60000 datapoints
2025-03-06 18:25:58,196 - INFO - training batch 901, loss: 0.412, 28832/60000 datapoints
2025-03-06 18:25:58,393 - INFO - training batch 951, loss: 0.441, 30432/60000 datapoints
2025-03-06 18:25:58,591 - INFO - training batch 1001, loss: 0.191, 32032/60000 datapoints
2025-03-06 18:25:58,788 - INFO - training batch 1051, loss: 0.237, 33632/60000 datapoints
2025-03-06 18:25:58,983 - INFO - training batch 1101, loss: 0.284, 35232/60000 datapoints
2025-03-06 18:25:59,177 - INFO - training batch 1151, loss: 0.309, 36832/60000 datapoints
2025-03-06 18:25:59,371 - INFO - training batch 1201, loss: 0.304, 38432/60000 datapoints
2025-03-06 18:25:59,565 - INFO - training batch 1251, loss: 0.204, 40032/60000 datapoints
2025-03-06 18:25:59,762 - INFO - training batch 1301, loss: 0.373, 41632/60000 datapoints
2025-03-06 18:25:59,965 - INFO - training batch 1351, loss: 0.104, 43232/60000 datapoints
2025-03-06 18:26:00,176 - INFO - training batch 1401, loss: 0.504, 44832/60000 datapoints
2025-03-06 18:26:00,371 - INFO - training batch 1451, loss: 0.265, 46432/60000 datapoints
2025-03-06 18:26:00,567 - INFO - training batch 1501, loss: 0.168, 48032/60000 datapoints
2025-03-06 18:26:00,764 - INFO - training batch 1551, loss: 0.092, 49632/60000 datapoints
2025-03-06 18:26:00,960 - INFO - training batch 1601, loss: 0.424, 51232/60000 datapoints
2025-03-06 18:26:01,160 - INFO - training batch 1651, loss: 0.295, 52832/60000 datapoints
2025-03-06 18:26:01,356 - INFO - training batch 1701, loss: 0.642, 54432/60000 datapoints
2025-03-06 18:26:01,550 - INFO - training batch 1751, loss: 0.302, 56032/60000 datapoints
2025-03-06 18:26:01,746 - INFO - training batch 1801, loss: 0.227, 57632/60000 datapoints
2025-03-06 18:26:01,941 - INFO - training batch 1851, loss: 0.193, 59232/60000 datapoints
2025-03-06 18:26:02,045 - INFO - validation batch 1, loss: 0.077, 32/10016 datapoints
2025-03-06 18:26:02,198 - INFO - validation batch 51, loss: 0.296, 1632/10016 datapoints
2025-03-06 18:26:02,350 - INFO - validation batch 101, loss: 0.181, 3232/10016 datapoints
2025-03-06 18:26:02,506 - INFO - validation batch 151, loss: 0.350, 4832/10016 datapoints
2025-03-06 18:26:02,660 - INFO - validation batch 201, loss: 0.152, 6432/10016 datapoints
2025-03-06 18:26:02,813 - INFO - validation batch 251, loss: 0.214, 8032/10016 datapoints
2025-03-06 18:26:02,965 - INFO - validation batch 301, loss: 0.422, 9632/10016 datapoints
2025-03-06 18:26:03,001 - INFO - Epoch 302/800 done.
2025-03-06 18:26:03,001 - INFO - Final validation performance:
Loss: 0.242, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:26:03,002 - INFO - Beginning epoch 303/800
2025-03-06 18:26:03,008 - INFO - training batch 1, loss: 0.328, 32/60000 datapoints
2025-03-06 18:26:03,206 - INFO - training batch 51, loss: 0.314, 1632/60000 datapoints
2025-03-06 18:26:03,403 - INFO - training batch 101, loss: 0.215, 3232/60000 datapoints
2025-03-06 18:26:03,600 - INFO - training batch 151, loss: 0.317, 4832/60000 datapoints
2025-03-06 18:26:03,796 - INFO - training batch 201, loss: 0.322, 6432/60000 datapoints
2025-03-06 18:26:03,990 - INFO - training batch 251, loss: 0.100, 8032/60000 datapoints
2025-03-06 18:26:04,183 - INFO - training batch 301, loss: 0.337, 9632/60000 datapoints
2025-03-06 18:26:04,378 - INFO - training batch 351, loss: 0.264, 11232/60000 datapoints
2025-03-06 18:26:04,577 - INFO - training batch 401, loss: 0.338, 12832/60000 datapoints
2025-03-06 18:26:04,775 - INFO - training batch 451, loss: 0.189, 14432/60000 datapoints
2025-03-06 18:26:04,975 - INFO - training batch 501, loss: 0.335, 16032/60000 datapoints
2025-03-06 18:26:05,170 - INFO - training batch 551, loss: 0.262, 17632/60000 datapoints
2025-03-06 18:26:05,370 - INFO - training batch 601, loss: 0.269, 19232/60000 datapoints
2025-03-06 18:26:05,565 - INFO - training batch 651, loss: 0.397, 20832/60000 datapoints
2025-03-06 18:26:05,759 - INFO - training batch 701, loss: 0.358, 22432/60000 datapoints
2025-03-06 18:26:05,954 - INFO - training batch 751, loss: 0.478, 24032/60000 datapoints
2025-03-06 18:26:06,153 - INFO - training batch 801, loss: 0.140, 25632/60000 datapoints
2025-03-06 18:26:06,347 - INFO - training batch 851, loss: 0.319, 27232/60000 datapoints
2025-03-06 18:26:06,542 - INFO - training batch 901, loss: 0.305, 28832/60000 datapoints
2025-03-06 18:26:06,740 - INFO - training batch 951, loss: 0.220, 30432/60000 datapoints
2025-03-06 18:26:06,936 - INFO - training batch 1001, loss: 0.351, 32032/60000 datapoints
2025-03-06 18:26:07,129 - INFO - training batch 1051, loss: 0.290, 33632/60000 datapoints
2025-03-06 18:26:07,322 - INFO - training batch 1101, loss: 0.202, 35232/60000 datapoints
2025-03-06 18:26:07,516 - INFO - training batch 1151, loss: 0.497, 36832/60000 datapoints
2025-03-06 18:26:07,714 - INFO - training batch 1201, loss: 0.316, 38432/60000 datapoints
2025-03-06 18:26:07,909 - INFO - training batch 1251, loss: 0.200, 40032/60000 datapoints
2025-03-06 18:26:08,107 - INFO - training batch 1301, loss: 0.527, 41632/60000 datapoints
2025-03-06 18:26:08,301 - INFO - training batch 1351, loss: 0.319, 43232/60000 datapoints
2025-03-06 18:26:08,498 - INFO - training batch 1401, loss: 0.258, 44832/60000 datapoints
2025-03-06 18:26:08,693 - INFO - training batch 1451, loss: 0.186, 46432/60000 datapoints
2025-03-06 18:26:08,888 - INFO - training batch 1501, loss: 0.177, 48032/60000 datapoints
2025-03-06 18:26:09,084 - INFO - training batch 1551, loss: 0.220, 49632/60000 datapoints
2025-03-06 18:26:09,276 - INFO - training batch 1601, loss: 0.272, 51232/60000 datapoints
2025-03-06 18:26:09,470 - INFO - training batch 1651, loss: 0.261, 52832/60000 datapoints
2025-03-06 18:26:09,665 - INFO - training batch 1701, loss: 0.376, 54432/60000 datapoints
2025-03-06 18:26:09,858 - INFO - training batch 1751, loss: 0.220, 56032/60000 datapoints
2025-03-06 18:26:10,057 - INFO - training batch 1801, loss: 0.110, 57632/60000 datapoints
2025-03-06 18:26:10,267 - INFO - training batch 1851, loss: 0.170, 59232/60000 datapoints
2025-03-06 18:26:10,369 - INFO - validation batch 1, loss: 0.361, 32/10016 datapoints
2025-03-06 18:26:10,524 - INFO - validation batch 51, loss: 0.368, 1632/10016 datapoints
2025-03-06 18:26:10,689 - INFO - validation batch 101, loss: 0.132, 3232/10016 datapoints
2025-03-06 18:26:10,843 - INFO - validation batch 151, loss: 0.262, 4832/10016 datapoints
2025-03-06 18:26:10,996 - INFO - validation batch 201, loss: 0.171, 6432/10016 datapoints
2025-03-06 18:26:11,152 - INFO - validation batch 251, loss: 0.458, 8032/10016 datapoints
2025-03-06 18:26:11,306 - INFO - validation batch 301, loss: 0.135, 9632/10016 datapoints
2025-03-06 18:26:11,344 - INFO - Epoch 303/800 done.
2025-03-06 18:26:11,345 - INFO - Final validation performance:
Loss: 0.269, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:26:11,345 - INFO - Beginning epoch 304/800
2025-03-06 18:26:11,351 - INFO - training batch 1, loss: 0.353, 32/60000 datapoints
2025-03-06 18:26:11,546 - INFO - training batch 51, loss: 0.188, 1632/60000 datapoints
2025-03-06 18:26:11,741 - INFO - training batch 101, loss: 0.124, 3232/60000 datapoints
2025-03-06 18:26:11,936 - INFO - training batch 151, loss: 0.431, 4832/60000 datapoints
2025-03-06 18:26:12,129 - INFO - training batch 201, loss: 0.355, 6432/60000 datapoints
2025-03-06 18:26:12,324 - INFO - training batch 251, loss: 0.160, 8032/60000 datapoints
2025-03-06 18:26:12,524 - INFO - training batch 301, loss: 0.427, 9632/60000 datapoints
2025-03-06 18:26:12,720 - INFO - training batch 351, loss: 0.240, 11232/60000 datapoints
2025-03-06 18:26:12,914 - INFO - training batch 401, loss: 0.164, 12832/60000 datapoints
2025-03-06 18:26:13,107 - INFO - training batch 451, loss: 0.425, 14432/60000 datapoints
2025-03-06 18:26:13,299 - INFO - training batch 501, loss: 0.224, 16032/60000 datapoints
2025-03-06 18:26:13,520 - INFO - training batch 551, loss: 0.278, 17632/60000 datapoints
2025-03-06 18:26:13,718 - INFO - training batch 601, loss: 0.119, 19232/60000 datapoints
2025-03-06 18:26:13,922 - INFO - training batch 651, loss: 0.179, 20832/60000 datapoints
2025-03-06 18:26:14,116 - INFO - training batch 701, loss: 0.267, 22432/60000 datapoints
2025-03-06 18:26:14,311 - INFO - training batch 751, loss: 0.528, 24032/60000 datapoints
2025-03-06 18:26:14,509 - INFO - training batch 801, loss: 0.356, 25632/60000 datapoints
2025-03-06 18:26:14,705 - INFO - training batch 851, loss: 0.194, 27232/60000 datapoints
2025-03-06 18:26:14,907 - INFO - training batch 901, loss: 0.349, 28832/60000 datapoints
2025-03-06 18:26:15,101 - INFO - training batch 951, loss: 0.248, 30432/60000 datapoints
2025-03-06 18:26:15,295 - INFO - training batch 1001, loss: 0.325, 32032/60000 datapoints
2025-03-06 18:26:15,490 - INFO - training batch 1051, loss: 0.317, 33632/60000 datapoints
2025-03-06 18:26:15,688 - INFO - training batch 1101, loss: 0.183, 35232/60000 datapoints
2025-03-06 18:26:15,882 - INFO - training batch 1151, loss: 0.360, 36832/60000 datapoints
2025-03-06 18:26:16,080 - INFO - training batch 1201, loss: 0.237, 38432/60000 datapoints
2025-03-06 18:26:16,275 - INFO - training batch 1251, loss: 0.679, 40032/60000 datapoints
2025-03-06 18:26:16,467 - INFO - training batch 1301, loss: 0.321, 41632/60000 datapoints
2025-03-06 18:26:16,666 - INFO - training batch 1351, loss: 0.287, 43232/60000 datapoints
2025-03-06 18:26:16,860 - INFO - training batch 1401, loss: 0.294, 44832/60000 datapoints
2025-03-06 18:26:17,054 - INFO - training batch 1451, loss: 0.109, 46432/60000 datapoints
2025-03-06 18:26:17,246 - INFO - training batch 1501, loss: 0.445, 48032/60000 datapoints
2025-03-06 18:26:17,439 - INFO - training batch 1551, loss: 0.283, 49632/60000 datapoints
2025-03-06 18:26:17,634 - INFO - training batch 1601, loss: 0.210, 51232/60000 datapoints
2025-03-06 18:26:17,830 - INFO - training batch 1651, loss: 0.130, 52832/60000 datapoints
2025-03-06 18:26:18,025 - INFO - training batch 1701, loss: 0.295, 54432/60000 datapoints
2025-03-06 18:26:18,218 - INFO - training batch 1751, loss: 0.433, 56032/60000 datapoints
2025-03-06 18:26:18,416 - INFO - training batch 1801, loss: 0.271, 57632/60000 datapoints
2025-03-06 18:26:18,615 - INFO - training batch 1851, loss: 0.264, 59232/60000 datapoints
2025-03-06 18:26:18,717 - INFO - validation batch 1, loss: 0.280, 32/10016 datapoints
2025-03-06 18:26:18,870 - INFO - validation batch 51, loss: 0.468, 1632/10016 datapoints
2025-03-06 18:26:19,023 - INFO - validation batch 101, loss: 0.213, 3232/10016 datapoints
2025-03-06 18:26:19,175 - INFO - validation batch 151, loss: 0.229, 4832/10016 datapoints
2025-03-06 18:26:19,328 - INFO - validation batch 201, loss: 0.141, 6432/10016 datapoints
2025-03-06 18:26:19,483 - INFO - validation batch 251, loss: 0.180, 8032/10016 datapoints
2025-03-06 18:26:19,638 - INFO - validation batch 301, loss: 0.270, 9632/10016 datapoints
2025-03-06 18:26:19,677 - INFO - Epoch 304/800 done.
2025-03-06 18:26:19,677 - INFO - Final validation performance:
Loss: 0.254, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:26:19,678 - INFO - Beginning epoch 305/800
2025-03-06 18:26:19,683 - INFO - training batch 1, loss: 0.384, 32/60000 datapoints
2025-03-06 18:26:19,875 - INFO - training batch 51, loss: 0.240, 1632/60000 datapoints
2025-03-06 18:26:20,072 - INFO - training batch 101, loss: 0.200, 3232/60000 datapoints
2025-03-06 18:26:20,285 - INFO - training batch 151, loss: 0.462, 4832/60000 datapoints
2025-03-06 18:26:20,486 - INFO - training batch 201, loss: 0.368, 6432/60000 datapoints
2025-03-06 18:26:20,685 - INFO - training batch 251, loss: 0.134, 8032/60000 datapoints
2025-03-06 18:26:20,880 - INFO - training batch 301, loss: 0.210, 9632/60000 datapoints
2025-03-06 18:26:21,073 - INFO - training batch 351, loss: 0.396, 11232/60000 datapoints
2025-03-06 18:26:21,271 - INFO - training batch 401, loss: 0.222, 12832/60000 datapoints
2025-03-06 18:26:21,464 - INFO - training batch 451, loss: 0.135, 14432/60000 datapoints
2025-03-06 18:26:21,661 - INFO - training batch 501, loss: 0.701, 16032/60000 datapoints
2025-03-06 18:26:21,857 - INFO - training batch 551, loss: 0.353, 17632/60000 datapoints
2025-03-06 18:26:22,053 - INFO - training batch 601, loss: 0.167, 19232/60000 datapoints
2025-03-06 18:26:22,246 - INFO - training batch 651, loss: 0.596, 20832/60000 datapoints
2025-03-06 18:26:22,438 - INFO - training batch 701, loss: 0.277, 22432/60000 datapoints
2025-03-06 18:26:22,638 - INFO - training batch 751, loss: 0.135, 24032/60000 datapoints
2025-03-06 18:26:22,832 - INFO - training batch 801, loss: 0.296, 25632/60000 datapoints
2025-03-06 18:26:23,025 - INFO - training batch 851, loss: 0.658, 27232/60000 datapoints
2025-03-06 18:26:23,218 - INFO - training batch 901, loss: 0.232, 28832/60000 datapoints
2025-03-06 18:26:23,412 - INFO - training batch 951, loss: 0.263, 30432/60000 datapoints
2025-03-06 18:26:23,609 - INFO - training batch 1001, loss: 0.267, 32032/60000 datapoints
2025-03-06 18:26:23,802 - INFO - training batch 1051, loss: 0.470, 33632/60000 datapoints
2025-03-06 18:26:23,996 - INFO - training batch 1101, loss: 0.323, 35232/60000 datapoints
2025-03-06 18:26:24,196 - INFO - training batch 1151, loss: 0.366, 36832/60000 datapoints
2025-03-06 18:26:24,389 - INFO - training batch 1201, loss: 0.166, 38432/60000 datapoints
2025-03-06 18:26:24,586 - INFO - training batch 1251, loss: 0.398, 40032/60000 datapoints
2025-03-06 18:26:24,780 - INFO - training batch 1301, loss: 0.227, 41632/60000 datapoints
2025-03-06 18:26:24,983 - INFO - training batch 1351, loss: 0.142, 43232/60000 datapoints
2025-03-06 18:26:25,181 - INFO - training batch 1401, loss: 0.315, 44832/60000 datapoints
2025-03-06 18:26:25,380 - INFO - training batch 1451, loss: 0.408, 46432/60000 datapoints
2025-03-06 18:26:25,577 - INFO - training batch 1501, loss: 0.247, 48032/60000 datapoints
2025-03-06 18:26:25,773 - INFO - training batch 1551, loss: 0.583, 49632/60000 datapoints
2025-03-06 18:26:26,003 - INFO - training batch 1601, loss: 0.453, 51232/60000 datapoints
2025-03-06 18:26:26,202 - INFO - training batch 1651, loss: 0.206, 52832/60000 datapoints
2025-03-06 18:26:26,404 - INFO - training batch 1701, loss: 0.302, 54432/60000 datapoints
2025-03-06 18:26:26,604 - INFO - training batch 1751, loss: 0.321, 56032/60000 datapoints
2025-03-06 18:26:26,798 - INFO - training batch 1801, loss: 0.262, 57632/60000 datapoints
2025-03-06 18:26:26,996 - INFO - training batch 1851, loss: 0.223, 59232/60000 datapoints
2025-03-06 18:26:27,098 - INFO - validation batch 1, loss: 0.345, 32/10016 datapoints
2025-03-06 18:26:27,253 - INFO - validation batch 51, loss: 0.161, 1632/10016 datapoints
2025-03-06 18:26:27,411 - INFO - validation batch 101, loss: 0.292, 3232/10016 datapoints
2025-03-06 18:26:27,567 - INFO - validation batch 151, loss: 0.192, 4832/10016 datapoints
2025-03-06 18:26:27,723 - INFO - validation batch 201, loss: 0.617, 6432/10016 datapoints
2025-03-06 18:26:27,877 - INFO - validation batch 251, loss: 0.164, 8032/10016 datapoints
2025-03-06 18:26:28,031 - INFO - validation batch 301, loss: 0.177, 9632/10016 datapoints
2025-03-06 18:26:28,069 - INFO - Epoch 305/800 done.
2025-03-06 18:26:28,069 - INFO - Final validation performance:
Loss: 0.278, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:26:28,069 - INFO - Beginning epoch 306/800
2025-03-06 18:26:28,075 - INFO - training batch 1, loss: 0.402, 32/60000 datapoints
2025-03-06 18:26:28,271 - INFO - training batch 51, loss: 0.405, 1632/60000 datapoints
2025-03-06 18:26:28,468 - INFO - training batch 101, loss: 0.164, 3232/60000 datapoints
2025-03-06 18:26:28,672 - INFO - training batch 151, loss: 0.362, 4832/60000 datapoints
2025-03-06 18:26:28,881 - INFO - training batch 201, loss: 0.445, 6432/60000 datapoints
2025-03-06 18:26:29,076 - INFO - training batch 251, loss: 0.170, 8032/60000 datapoints
2025-03-06 18:26:29,272 - INFO - training batch 301, loss: 0.337, 9632/60000 datapoints
2025-03-06 18:26:29,467 - INFO - training batch 351, loss: 0.095, 11232/60000 datapoints
2025-03-06 18:26:29,667 - INFO - training batch 401, loss: 0.348, 12832/60000 datapoints
2025-03-06 18:26:29,868 - INFO - training batch 451, loss: 0.189, 14432/60000 datapoints
2025-03-06 18:26:30,065 - INFO - training batch 501, loss: 0.222, 16032/60000 datapoints
2025-03-06 18:26:30,276 - INFO - training batch 551, loss: 0.185, 17632/60000 datapoints
2025-03-06 18:26:30,475 - INFO - training batch 601, loss: 0.538, 19232/60000 datapoints
2025-03-06 18:26:30,675 - INFO - training batch 651, loss: 0.399, 20832/60000 datapoints
2025-03-06 18:26:30,870 - INFO - training batch 701, loss: 0.170, 22432/60000 datapoints
2025-03-06 18:26:31,063 - INFO - training batch 751, loss: 0.293, 24032/60000 datapoints
2025-03-06 18:26:31,264 - INFO - training batch 801, loss: 0.361, 25632/60000 datapoints
2025-03-06 18:26:31,459 - INFO - training batch 851, loss: 0.256, 27232/60000 datapoints
2025-03-06 18:26:31,659 - INFO - training batch 901, loss: 0.440, 28832/60000 datapoints
2025-03-06 18:26:31,853 - INFO - training batch 951, loss: 0.404, 30432/60000 datapoints
2025-03-06 18:26:32,048 - INFO - training batch 1001, loss: 0.180, 32032/60000 datapoints
2025-03-06 18:26:32,243 - INFO - training batch 1051, loss: 0.197, 33632/60000 datapoints
2025-03-06 18:26:32,438 - INFO - training batch 1101, loss: 0.205, 35232/60000 datapoints
2025-03-06 18:26:32,639 - INFO - training batch 1151, loss: 0.218, 36832/60000 datapoints
2025-03-06 18:26:32,834 - INFO - training batch 1201, loss: 0.265, 38432/60000 datapoints
2025-03-06 18:26:33,030 - INFO - training batch 1251, loss: 0.602, 40032/60000 datapoints
2025-03-06 18:26:33,227 - INFO - training batch 1301, loss: 0.215, 41632/60000 datapoints
2025-03-06 18:26:33,421 - INFO - training batch 1351, loss: 0.267, 43232/60000 datapoints
2025-03-06 18:26:33,620 - INFO - training batch 1401, loss: 0.282, 44832/60000 datapoints
2025-03-06 18:26:33,813 - INFO - training batch 1451, loss: 0.248, 46432/60000 datapoints
2025-03-06 18:26:34,011 - INFO - training batch 1501, loss: 0.316, 48032/60000 datapoints
2025-03-06 18:26:34,208 - INFO - training batch 1551, loss: 0.572, 49632/60000 datapoints
2025-03-06 18:26:34,404 - INFO - training batch 1601, loss: 0.197, 51232/60000 datapoints
2025-03-06 18:26:34,604 - INFO - training batch 1651, loss: 0.201, 52832/60000 datapoints
2025-03-06 18:26:34,802 - INFO - training batch 1701, loss: 0.141, 54432/60000 datapoints
2025-03-06 18:26:35,004 - INFO - training batch 1751, loss: 0.429, 56032/60000 datapoints
2025-03-06 18:26:35,197 - INFO - training batch 1801, loss: 0.400, 57632/60000 datapoints
2025-03-06 18:26:35,394 - INFO - training batch 1851, loss: 0.636, 59232/60000 datapoints
2025-03-06 18:26:35,496 - INFO - validation batch 1, loss: 0.217, 32/10016 datapoints
2025-03-06 18:26:35,652 - INFO - validation batch 51, loss: 0.181, 1632/10016 datapoints
2025-03-06 18:26:35,806 - INFO - validation batch 101, loss: 0.126, 3232/10016 datapoints
2025-03-06 18:26:35,959 - INFO - validation batch 151, loss: 0.320, 4832/10016 datapoints
2025-03-06 18:26:36,113 - INFO - validation batch 201, loss: 0.286, 6432/10016 datapoints
2025-03-06 18:26:36,269 - INFO - validation batch 251, loss: 0.794, 8032/10016 datapoints
2025-03-06 18:26:36,420 - INFO - validation batch 301, loss: 0.453, 9632/10016 datapoints
2025-03-06 18:26:36,457 - INFO - Epoch 306/800 done.
2025-03-06 18:26:36,457 - INFO - Final validation performance:
Loss: 0.340, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:26:36,458 - INFO - Beginning epoch 307/800
2025-03-06 18:26:36,464 - INFO - training batch 1, loss: 0.239, 32/60000 datapoints
2025-03-06 18:26:36,661 - INFO - training batch 51, loss: 0.238, 1632/60000 datapoints
2025-03-06 18:26:36,851 - INFO - training batch 101, loss: 0.205, 3232/60000 datapoints
2025-03-06 18:26:37,043 - INFO - training batch 151, loss: 0.117, 4832/60000 datapoints
2025-03-06 18:26:37,236 - INFO - training batch 201, loss: 0.260, 6432/60000 datapoints
2025-03-06 18:26:37,429 - INFO - training batch 251, loss: 0.259, 8032/60000 datapoints
2025-03-06 18:26:37,639 - INFO - training batch 301, loss: 0.230, 9632/60000 datapoints
2025-03-06 18:26:37,834 - INFO - training batch 351, loss: 0.242, 11232/60000 datapoints
2025-03-06 18:26:38,030 - INFO - training batch 401, loss: 0.230, 12832/60000 datapoints
2025-03-06 18:26:38,221 - INFO - training batch 451, loss: 0.284, 14432/60000 datapoints
2025-03-06 18:26:38,412 - INFO - training batch 501, loss: 0.181, 16032/60000 datapoints
2025-03-06 18:26:38,606 - INFO - training batch 551, loss: 0.199, 17632/60000 datapoints
2025-03-06 18:26:38,800 - INFO - training batch 601, loss: 0.383, 19232/60000 datapoints
2025-03-06 18:26:38,993 - INFO - training batch 651, loss: 0.600, 20832/60000 datapoints
2025-03-06 18:26:39,191 - INFO - training batch 701, loss: 0.192, 22432/60000 datapoints
2025-03-06 18:26:39,383 - INFO - training batch 751, loss: 0.639, 24032/60000 datapoints
2025-03-06 18:26:39,573 - INFO - training batch 801, loss: 0.227, 25632/60000 datapoints
2025-03-06 18:26:39,767 - INFO - training batch 851, loss: 0.333, 27232/60000 datapoints
2025-03-06 18:26:39,961 - INFO - training batch 901, loss: 0.441, 28832/60000 datapoints
2025-03-06 18:26:40,152 - INFO - training batch 951, loss: 0.522, 30432/60000 datapoints
2025-03-06 18:26:40,360 - INFO - training batch 1001, loss: 0.252, 32032/60000 datapoints
2025-03-06 18:26:40,552 - INFO - training batch 1051, loss: 0.156, 33632/60000 datapoints
2025-03-06 18:26:40,749 - INFO - training batch 1101, loss: 0.196, 35232/60000 datapoints
2025-03-06 18:26:40,949 - INFO - training batch 1151, loss: 0.180, 36832/60000 datapoints
2025-03-06 18:26:41,143 - INFO - training batch 1201, loss: 0.240, 38432/60000 datapoints
2025-03-06 18:26:41,341 - INFO - training batch 1251, loss: 0.324, 40032/60000 datapoints
2025-03-06 18:26:41,533 - INFO - training batch 1301, loss: 0.196, 41632/60000 datapoints
2025-03-06 18:26:41,728 - INFO - training batch 1351, loss: 0.162, 43232/60000 datapoints
2025-03-06 18:26:41,923 - INFO - training batch 1401, loss: 0.389, 44832/60000 datapoints
2025-03-06 18:26:42,113 - INFO - training batch 1451, loss: 0.339, 46432/60000 datapoints
2025-03-06 18:26:42,305 - INFO - training batch 1501, loss: 0.318, 48032/60000 datapoints
2025-03-06 18:26:42,496 - INFO - training batch 1551, loss: 0.237, 49632/60000 datapoints
2025-03-06 18:26:42,694 - INFO - training batch 1601, loss: 0.173, 51232/60000 datapoints
2025-03-06 18:26:42,885 - INFO - training batch 1651, loss: 0.244, 52832/60000 datapoints
2025-03-06 18:26:43,076 - INFO - training batch 1701, loss: 0.238, 54432/60000 datapoints
2025-03-06 18:26:43,269 - INFO - training batch 1751, loss: 0.657, 56032/60000 datapoints
2025-03-06 18:26:43,462 - INFO - training batch 1801, loss: 0.274, 57632/60000 datapoints
2025-03-06 18:26:43,657 - INFO - training batch 1851, loss: 0.292, 59232/60000 datapoints
2025-03-06 18:26:43,756 - INFO - validation batch 1, loss: 0.263, 32/10016 datapoints
2025-03-06 18:26:43,906 - INFO - validation batch 51, loss: 0.184, 1632/10016 datapoints
2025-03-06 18:26:44,054 - INFO - validation batch 101, loss: 0.286, 3232/10016 datapoints
2025-03-06 18:26:44,204 - INFO - validation batch 151, loss: 0.271, 4832/10016 datapoints
2025-03-06 18:26:44,356 - INFO - validation batch 201, loss: 0.311, 6432/10016 datapoints
2025-03-06 18:26:44,506 - INFO - validation batch 251, loss: 0.146, 8032/10016 datapoints
2025-03-06 18:26:44,661 - INFO - validation batch 301, loss: 0.414, 9632/10016 datapoints
2025-03-06 18:26:44,697 - INFO - Epoch 307/800 done.
2025-03-06 18:26:44,697 - INFO - Final validation performance:
Loss: 0.268, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:26:44,697 - INFO - Beginning epoch 308/800
2025-03-06 18:26:44,703 - INFO - training batch 1, loss: 0.496, 32/60000 datapoints
2025-03-06 18:26:44,901 - INFO - training batch 51, loss: 0.176, 1632/60000 datapoints
2025-03-06 18:26:45,093 - INFO - training batch 101, loss: 0.191, 3232/60000 datapoints
2025-03-06 18:26:45,286 - INFO - training batch 151, loss: 0.131, 4832/60000 datapoints
2025-03-06 18:26:45,477 - INFO - training batch 201, loss: 0.250, 6432/60000 datapoints
2025-03-06 18:26:45,670 - INFO - training batch 251, loss: 0.132, 8032/60000 datapoints
2025-03-06 18:26:45,863 - INFO - training batch 301, loss: 0.211, 9632/60000 datapoints
2025-03-06 18:26:46,056 - INFO - training batch 351, loss: 0.176, 11232/60000 datapoints
2025-03-06 18:26:46,250 - INFO - training batch 401, loss: 0.117, 12832/60000 datapoints
2025-03-06 18:26:46,445 - INFO - training batch 451, loss: 0.289, 14432/60000 datapoints
2025-03-06 18:26:46,642 - INFO - training batch 501, loss: 0.342, 16032/60000 datapoints
2025-03-06 18:26:46,834 - INFO - training batch 551, loss: 0.227, 17632/60000 datapoints
2025-03-06 18:26:47,025 - INFO - training batch 601, loss: 0.188, 19232/60000 datapoints
2025-03-06 18:26:47,215 - INFO - training batch 651, loss: 0.154, 20832/60000 datapoints
2025-03-06 18:26:47,406 - INFO - training batch 701, loss: 0.229, 22432/60000 datapoints
2025-03-06 18:26:47,594 - INFO - training batch 751, loss: 0.147, 24032/60000 datapoints
2025-03-06 18:26:47,788 - INFO - training batch 801, loss: 0.616, 25632/60000 datapoints
2025-03-06 18:26:47,981 - INFO - training batch 851, loss: 0.362, 27232/60000 datapoints
2025-03-06 18:26:48,172 - INFO - training batch 901, loss: 0.377, 28832/60000 datapoints
2025-03-06 18:26:48,365 - INFO - training batch 951, loss: 0.307, 30432/60000 datapoints
2025-03-06 18:26:48,554 - INFO - training batch 1001, loss: 0.286, 32032/60000 datapoints
2025-03-06 18:26:48,748 - INFO - training batch 1051, loss: 0.588, 33632/60000 datapoints
2025-03-06 18:26:48,942 - INFO - training batch 1101, loss: 0.159, 35232/60000 datapoints
2025-03-06 18:26:49,133 - INFO - training batch 1151, loss: 0.542, 36832/60000 datapoints
2025-03-06 18:26:49,324 - INFO - training batch 1201, loss: 0.237, 38432/60000 datapoints
2025-03-06 18:26:49,516 - INFO - training batch 1251, loss: 0.456, 40032/60000 datapoints
2025-03-06 18:26:49,709 - INFO - training batch 1301, loss: 0.215, 41632/60000 datapoints
2025-03-06 18:26:49,902 - INFO - training batch 1351, loss: 0.254, 43232/60000 datapoints
2025-03-06 18:26:50,099 - INFO - training batch 1401, loss: 0.260, 44832/60000 datapoints
2025-03-06 18:26:50,290 - INFO - training batch 1451, loss: 0.266, 46432/60000 datapoints
2025-03-06 18:26:50,499 - INFO - training batch 1501, loss: 0.279, 48032/60000 datapoints
2025-03-06 18:26:50,695 - INFO - training batch 1551, loss: 0.095, 49632/60000 datapoints
2025-03-06 18:26:50,888 - INFO - training batch 1601, loss: 0.368, 51232/60000 datapoints
2025-03-06 18:26:51,082 - INFO - training batch 1651, loss: 0.190, 52832/60000 datapoints
2025-03-06 18:26:51,281 - INFO - training batch 1701, loss: 0.229, 54432/60000 datapoints
2025-03-06 18:26:51,474 - INFO - training batch 1751, loss: 0.287, 56032/60000 datapoints
2025-03-06 18:26:51,667 - INFO - training batch 1801, loss: 0.179, 57632/60000 datapoints
2025-03-06 18:26:51,858 - INFO - training batch 1851, loss: 0.291, 59232/60000 datapoints
2025-03-06 18:26:51,956 - INFO - validation batch 1, loss: 0.361, 32/10016 datapoints
2025-03-06 18:26:52,106 - INFO - validation batch 51, loss: 0.196, 1632/10016 datapoints
2025-03-06 18:26:52,257 - INFO - validation batch 101, loss: 0.240, 3232/10016 datapoints
2025-03-06 18:26:52,409 - INFO - validation batch 151, loss: 0.106, 4832/10016 datapoints
2025-03-06 18:26:52,557 - INFO - validation batch 201, loss: 0.336, 6432/10016 datapoints
2025-03-06 18:26:52,717 - INFO - validation batch 251, loss: 0.180, 8032/10016 datapoints
2025-03-06 18:26:52,869 - INFO - validation batch 301, loss: 0.185, 9632/10016 datapoints
2025-03-06 18:26:52,904 - INFO - Epoch 308/800 done.
2025-03-06 18:26:52,904 - INFO - Final validation performance:
Loss: 0.229, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:26:52,905 - INFO - Beginning epoch 309/800
2025-03-06 18:26:52,911 - INFO - training batch 1, loss: 0.507, 32/60000 datapoints
2025-03-06 18:26:53,099 - INFO - training batch 51, loss: 0.073, 1632/60000 datapoints
2025-03-06 18:26:53,290 - INFO - training batch 101, loss: 0.185, 3232/60000 datapoints
2025-03-06 18:26:53,491 - INFO - training batch 151, loss: 0.204, 4832/60000 datapoints
2025-03-06 18:26:53,684 - INFO - training batch 201, loss: 0.242, 6432/60000 datapoints
2025-03-06 18:26:53,878 - INFO - training batch 251, loss: 0.699, 8032/60000 datapoints
2025-03-06 18:26:54,068 - INFO - training batch 301, loss: 0.147, 9632/60000 datapoints
2025-03-06 18:26:54,257 - INFO - training batch 351, loss: 0.340, 11232/60000 datapoints
2025-03-06 18:26:54,453 - INFO - training batch 401, loss: 0.295, 12832/60000 datapoints
2025-03-06 18:26:54,660 - INFO - training batch 451, loss: 0.198, 14432/60000 datapoints
2025-03-06 18:26:54,856 - INFO - training batch 501, loss: 0.247, 16032/60000 datapoints
2025-03-06 18:26:55,049 - INFO - training batch 551, loss: 0.625, 17632/60000 datapoints
2025-03-06 18:26:55,240 - INFO - training batch 601, loss: 0.515, 19232/60000 datapoints
2025-03-06 18:26:55,441 - INFO - training batch 651, loss: 0.420, 20832/60000 datapoints
2025-03-06 18:26:55,647 - INFO - training batch 701, loss: 0.196, 22432/60000 datapoints
2025-03-06 18:26:55,841 - INFO - training batch 751, loss: 0.500, 24032/60000 datapoints
2025-03-06 18:26:56,036 - INFO - training batch 801, loss: 0.254, 25632/60000 datapoints
2025-03-06 18:26:56,232 - INFO - training batch 851, loss: 0.080, 27232/60000 datapoints
2025-03-06 18:26:56,425 - INFO - training batch 901, loss: 0.279, 28832/60000 datapoints
2025-03-06 18:26:56,616 - INFO - training batch 951, loss: 0.247, 30432/60000 datapoints
2025-03-06 18:26:56,811 - INFO - training batch 1001, loss: 0.167, 32032/60000 datapoints
2025-03-06 18:26:57,002 - INFO - training batch 1051, loss: 0.219, 33632/60000 datapoints
2025-03-06 18:26:57,194 - INFO - training batch 1101, loss: 0.260, 35232/60000 datapoints
2025-03-06 18:26:57,384 - INFO - training batch 1151, loss: 0.447, 36832/60000 datapoints
2025-03-06 18:26:57,575 - INFO - training batch 1201, loss: 0.296, 38432/60000 datapoints
2025-03-06 18:26:57,766 - INFO - training batch 1251, loss: 0.170, 40032/60000 datapoints
2025-03-06 18:26:57,958 - INFO - training batch 1301, loss: 0.308, 41632/60000 datapoints
2025-03-06 18:26:58,147 - INFO - training batch 1351, loss: 0.303, 43232/60000 datapoints
2025-03-06 18:26:58,338 - INFO - training batch 1401, loss: 0.122, 44832/60000 datapoints
2025-03-06 18:26:58,532 - INFO - training batch 1451, loss: 0.247, 46432/60000 datapoints
2025-03-06 18:26:58,727 - INFO - training batch 1501, loss: 0.169, 48032/60000 datapoints
2025-03-06 18:26:58,920 - INFO - training batch 1551, loss: 0.477, 49632/60000 datapoints
2025-03-06 18:26:59,110 - INFO - training batch 1601, loss: 0.169, 51232/60000 datapoints
2025-03-06 18:26:59,302 - INFO - training batch 1651, loss: 0.334, 52832/60000 datapoints
2025-03-06 18:26:59,496 - INFO - training batch 1701, loss: 0.368, 54432/60000 datapoints
2025-03-06 18:26:59,691 - INFO - training batch 1751, loss: 0.420, 56032/60000 datapoints
2025-03-06 18:26:59,882 - INFO - training batch 1801, loss: 0.293, 57632/60000 datapoints
2025-03-06 18:27:00,077 - INFO - training batch 1851, loss: 0.249, 59232/60000 datapoints
2025-03-06 18:27:00,175 - INFO - validation batch 1, loss: 0.056, 32/10016 datapoints
2025-03-06 18:27:00,324 - INFO - validation batch 51, loss: 0.202, 1632/10016 datapoints
2025-03-06 18:27:00,483 - INFO - validation batch 101, loss: 0.248, 3232/10016 datapoints
2025-03-06 18:27:00,650 - INFO - validation batch 151, loss: 0.300, 4832/10016 datapoints
2025-03-06 18:27:00,800 - INFO - validation batch 201, loss: 0.335, 6432/10016 datapoints
2025-03-06 18:27:00,952 - INFO - validation batch 251, loss: 0.609, 8032/10016 datapoints
2025-03-06 18:27:01,101 - INFO - validation batch 301, loss: 0.145, 9632/10016 datapoints
2025-03-06 18:27:01,136 - INFO - Epoch 309/800 done.
2025-03-06 18:27:01,137 - INFO - Final validation performance:
Loss: 0.271, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:27:01,137 - INFO - Beginning epoch 310/800
2025-03-06 18:27:01,143 - INFO - training batch 1, loss: 0.150, 32/60000 datapoints
2025-03-06 18:27:01,336 - INFO - training batch 51, loss: 0.200, 1632/60000 datapoints
2025-03-06 18:27:01,530 - INFO - training batch 101, loss: 0.192, 3232/60000 datapoints
2025-03-06 18:27:01,725 - INFO - training batch 151, loss: 0.324, 4832/60000 datapoints
2025-03-06 18:27:01,918 - INFO - training batch 201, loss: 0.398, 6432/60000 datapoints
2025-03-06 18:27:02,107 - INFO - training batch 251, loss: 0.391, 8032/60000 datapoints
2025-03-06 18:27:02,297 - INFO - training batch 301, loss: 0.210, 9632/60000 datapoints
2025-03-06 18:27:02,487 - INFO - training batch 351, loss: 0.379, 11232/60000 datapoints
2025-03-06 18:27:02,683 - INFO - training batch 401, loss: 0.242, 12832/60000 datapoints
2025-03-06 18:27:02,875 - INFO - training batch 451, loss: 0.212, 14432/60000 datapoints
2025-03-06 18:27:03,065 - INFO - training batch 501, loss: 0.205, 16032/60000 datapoints
2025-03-06 18:27:03,254 - INFO - training batch 551, loss: 0.173, 17632/60000 datapoints
2025-03-06 18:27:03,446 - INFO - training batch 601, loss: 0.450, 19232/60000 datapoints
2025-03-06 18:27:03,639 - INFO - training batch 651, loss: 0.296, 20832/60000 datapoints
2025-03-06 18:27:03,830 - INFO - training batch 701, loss: 0.502, 22432/60000 datapoints
2025-03-06 18:27:04,021 - INFO - training batch 751, loss: 0.333, 24032/60000 datapoints
2025-03-06 18:27:04,211 - INFO - training batch 801, loss: 0.194, 25632/60000 datapoints
2025-03-06 18:27:04,402 - INFO - training batch 851, loss: 0.240, 27232/60000 datapoints
2025-03-06 18:27:04,594 - INFO - training batch 901, loss: 0.741, 28832/60000 datapoints
2025-03-06 18:27:04,790 - INFO - training batch 951, loss: 0.405, 30432/60000 datapoints
2025-03-06 18:27:04,986 - INFO - training batch 1001, loss: 0.314, 32032/60000 datapoints
2025-03-06 18:27:05,179 - INFO - training batch 1051, loss: 0.258, 33632/60000 datapoints
2025-03-06 18:27:05,370 - INFO - training batch 1101, loss: 0.172, 35232/60000 datapoints
2025-03-06 18:27:05,562 - INFO - training batch 1151, loss: 0.260, 36832/60000 datapoints
2025-03-06 18:27:05,754 - INFO - training batch 1201, loss: 0.129, 38432/60000 datapoints
2025-03-06 18:27:05,947 - INFO - training batch 1251, loss: 0.442, 40032/60000 datapoints
2025-03-06 18:27:06,139 - INFO - training batch 1301, loss: 0.467, 41632/60000 datapoints
2025-03-06 18:27:06,333 - INFO - training batch 1351, loss: 0.328, 43232/60000 datapoints
2025-03-06 18:27:06,541 - INFO - training batch 1401, loss: 0.228, 44832/60000 datapoints
2025-03-06 18:27:06,739 - INFO - training batch 1451, loss: 0.142, 46432/60000 datapoints
2025-03-06 18:27:06,930 - INFO - training batch 1501, loss: 0.258, 48032/60000 datapoints
2025-03-06 18:27:07,121 - INFO - training batch 1551, loss: 0.491, 49632/60000 datapoints
2025-03-06 18:27:07,310 - INFO - training batch 1601, loss: 0.199, 51232/60000 datapoints
2025-03-06 18:27:07,501 - INFO - training batch 1651, loss: 0.147, 52832/60000 datapoints
2025-03-06 18:27:07,696 - INFO - training batch 1701, loss: 0.247, 54432/60000 datapoints
2025-03-06 18:27:07,888 - INFO - training batch 1751, loss: 0.532, 56032/60000 datapoints
2025-03-06 18:27:08,086 - INFO - training batch 1801, loss: 0.310, 57632/60000 datapoints
2025-03-06 18:27:08,277 - INFO - training batch 1851, loss: 0.052, 59232/60000 datapoints
2025-03-06 18:27:08,376 - INFO - validation batch 1, loss: 0.277, 32/10016 datapoints
2025-03-06 18:27:08,526 - INFO - validation batch 51, loss: 0.250, 1632/10016 datapoints
2025-03-06 18:27:08,676 - INFO - validation batch 101, loss: 0.316, 3232/10016 datapoints
2025-03-06 18:27:08,830 - INFO - validation batch 151, loss: 0.209, 4832/10016 datapoints
2025-03-06 18:27:08,981 - INFO - validation batch 201, loss: 0.544, 6432/10016 datapoints
2025-03-06 18:27:09,132 - INFO - validation batch 251, loss: 0.360, 8032/10016 datapoints
2025-03-06 18:27:09,283 - INFO - validation batch 301, loss: 0.475, 9632/10016 datapoints
2025-03-06 18:27:09,319 - INFO - Epoch 310/800 done.
2025-03-06 18:27:09,319 - INFO - Final validation performance:
Loss: 0.347, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:27:09,319 - INFO - Beginning epoch 311/800
2025-03-06 18:27:09,325 - INFO - training batch 1, loss: 0.293, 32/60000 datapoints
2025-03-06 18:27:09,520 - INFO - training batch 51, loss: 0.171, 1632/60000 datapoints
2025-03-06 18:27:09,712 - INFO - training batch 101, loss: 0.145, 3232/60000 datapoints
2025-03-06 18:27:09,903 - INFO - training batch 151, loss: 0.345, 4832/60000 datapoints
2025-03-06 18:27:10,095 - INFO - training batch 201, loss: 0.317, 6432/60000 datapoints
2025-03-06 18:27:10,286 - INFO - training batch 251, loss: 0.188, 8032/60000 datapoints
2025-03-06 18:27:10,476 - INFO - training batch 301, loss: 0.194, 9632/60000 datapoints
2025-03-06 18:27:10,696 - INFO - training batch 351, loss: 0.392, 11232/60000 datapoints
2025-03-06 18:27:10,886 - INFO - training batch 401, loss: 0.255, 12832/60000 datapoints
2025-03-06 18:27:11,084 - INFO - training batch 451, loss: 0.335, 14432/60000 datapoints
2025-03-06 18:27:11,284 - INFO - training batch 501, loss: 0.512, 16032/60000 datapoints
2025-03-06 18:27:11,476 - INFO - training batch 551, loss: 0.264, 17632/60000 datapoints
2025-03-06 18:27:11,669 - INFO - training batch 601, loss: 0.173, 19232/60000 datapoints
2025-03-06 18:27:11,861 - INFO - training batch 651, loss: 0.250, 20832/60000 datapoints
2025-03-06 18:27:12,053 - INFO - training batch 701, loss: 0.406, 22432/60000 datapoints
2025-03-06 18:27:12,242 - INFO - training batch 751, loss: 0.252, 24032/60000 datapoints
2025-03-06 18:27:12,432 - INFO - training batch 801, loss: 0.202, 25632/60000 datapoints
2025-03-06 18:27:12,627 - INFO - training batch 851, loss: 0.277, 27232/60000 datapoints
2025-03-06 18:27:12,826 - INFO - training batch 901, loss: 0.112, 28832/60000 datapoints
2025-03-06 18:27:13,018 - INFO - training batch 951, loss: 0.283, 30432/60000 datapoints
2025-03-06 18:27:13,213 - INFO - training batch 1001, loss: 0.182, 32032/60000 datapoints
2025-03-06 18:27:13,407 - INFO - training batch 1051, loss: 0.482, 33632/60000 datapoints
2025-03-06 18:27:13,607 - INFO - training batch 1101, loss: 0.168, 35232/60000 datapoints
2025-03-06 18:27:13,802 - INFO - training batch 1151, loss: 0.270, 36832/60000 datapoints
2025-03-06 18:27:13,996 - INFO - training batch 1201, loss: 0.323, 38432/60000 datapoints
2025-03-06 18:27:14,192 - INFO - training batch 1251, loss: 0.408, 40032/60000 datapoints
2025-03-06 18:27:14,386 - INFO - training batch 1301, loss: 0.402, 41632/60000 datapoints
2025-03-06 18:27:14,582 - INFO - training batch 1351, loss: 0.507, 43232/60000 datapoints
2025-03-06 18:27:14,780 - INFO - training batch 1401, loss: 0.364, 44832/60000 datapoints
2025-03-06 18:27:14,980 - INFO - training batch 1451, loss: 0.196, 46432/60000 datapoints
2025-03-06 18:27:15,176 - INFO - training batch 1501, loss: 0.431, 48032/60000 datapoints
2025-03-06 18:27:15,370 - INFO - training batch 1551, loss: 0.211, 49632/60000 datapoints
2025-03-06 18:27:15,567 - INFO - training batch 1601, loss: 0.181, 51232/60000 datapoints
2025-03-06 18:27:15,764 - INFO - training batch 1651, loss: 0.311, 52832/60000 datapoints
2025-03-06 18:27:15,966 - INFO - training batch 1701, loss: 0.226, 54432/60000 datapoints
2025-03-06 18:27:16,190 - INFO - training batch 1751, loss: 0.297, 56032/60000 datapoints
2025-03-06 18:27:16,401 - INFO - training batch 1801, loss: 0.168, 57632/60000 datapoints
2025-03-06 18:27:16,618 - INFO - training batch 1851, loss: 0.231, 59232/60000 datapoints
2025-03-06 18:27:16,723 - INFO - validation batch 1, loss: 0.355, 32/10016 datapoints
2025-03-06 18:27:16,875 - INFO - validation batch 51, loss: 0.307, 1632/10016 datapoints
2025-03-06 18:27:17,028 - INFO - validation batch 101, loss: 0.213, 3232/10016 datapoints
2025-03-06 18:27:17,184 - INFO - validation batch 151, loss: 0.294, 4832/10016 datapoints
2025-03-06 18:27:17,335 - INFO - validation batch 201, loss: 0.251, 6432/10016 datapoints
2025-03-06 18:27:17,487 - INFO - validation batch 251, loss: 0.285, 8032/10016 datapoints
2025-03-06 18:27:17,643 - INFO - validation batch 301, loss: 0.374, 9632/10016 datapoints
2025-03-06 18:27:17,681 - INFO - Epoch 311/800 done.
2025-03-06 18:27:17,682 - INFO - Final validation performance:
Loss: 0.297, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:27:17,682 - INFO - Beginning epoch 312/800
2025-03-06 18:27:17,688 - INFO - training batch 1, loss: 0.179, 32/60000 datapoints
2025-03-06 18:27:17,886 - INFO - training batch 51, loss: 0.213, 1632/60000 datapoints
2025-03-06 18:27:18,078 - INFO - training batch 101, loss: 0.497, 3232/60000 datapoints
2025-03-06 18:27:18,275 - INFO - training batch 151, loss: 0.415, 4832/60000 datapoints
2025-03-06 18:27:18,470 - INFO - training batch 201, loss: 0.308, 6432/60000 datapoints
2025-03-06 18:27:18,667 - INFO - training batch 251, loss: 0.377, 8032/60000 datapoints
2025-03-06 18:27:18,865 - INFO - training batch 301, loss: 0.270, 9632/60000 datapoints
2025-03-06 18:27:19,060 - INFO - training batch 351, loss: 0.189, 11232/60000 datapoints
2025-03-06 18:27:19,258 - INFO - training batch 401, loss: 0.216, 12832/60000 datapoints
2025-03-06 18:27:19,452 - INFO - training batch 451, loss: 0.222, 14432/60000 datapoints
2025-03-06 18:27:19,649 - INFO - training batch 501, loss: 0.467, 16032/60000 datapoints
2025-03-06 18:27:19,840 - INFO - training batch 551, loss: 0.307, 17632/60000 datapoints
2025-03-06 18:27:20,033 - INFO - training batch 601, loss: 0.240, 19232/60000 datapoints
2025-03-06 18:27:20,228 - INFO - training batch 651, loss: 0.391, 20832/60000 datapoints
2025-03-06 18:27:20,422 - INFO - training batch 701, loss: 0.247, 22432/60000 datapoints
2025-03-06 18:27:20,619 - INFO - training batch 751, loss: 0.538, 24032/60000 datapoints
2025-03-06 18:27:20,833 - INFO - training batch 801, loss: 0.143, 25632/60000 datapoints
2025-03-06 18:27:21,027 - INFO - training batch 851, loss: 0.505, 27232/60000 datapoints
2025-03-06 18:27:21,220 - INFO - training batch 901, loss: 0.305, 28832/60000 datapoints
2025-03-06 18:27:21,414 - INFO - training batch 951, loss: 0.105, 30432/60000 datapoints
2025-03-06 18:27:21,611 - INFO - training batch 1001, loss: 0.537, 32032/60000 datapoints
2025-03-06 18:27:21,810 - INFO - training batch 1051, loss: 0.220, 33632/60000 datapoints
2025-03-06 18:27:22,004 - INFO - training batch 1101, loss: 0.424, 35232/60000 datapoints
2025-03-06 18:27:22,201 - INFO - training batch 1151, loss: 0.289, 36832/60000 datapoints
2025-03-06 18:27:22,393 - INFO - training batch 1201, loss: 0.287, 38432/60000 datapoints
2025-03-06 18:27:22,585 - INFO - training batch 1251, loss: 0.365, 40032/60000 datapoints
2025-03-06 18:27:22,783 - INFO - training batch 1301, loss: 0.407, 41632/60000 datapoints
2025-03-06 18:27:22,978 - INFO - training batch 1351, loss: 0.369, 43232/60000 datapoints
2025-03-06 18:27:23,173 - INFO - training batch 1401, loss: 0.204, 44832/60000 datapoints
2025-03-06 18:27:23,366 - INFO - training batch 1451, loss: 0.328, 46432/60000 datapoints
2025-03-06 18:27:23,575 - INFO - training batch 1501, loss: 0.273, 48032/60000 datapoints
2025-03-06 18:27:23,795 - INFO - training batch 1551, loss: 0.391, 49632/60000 datapoints
2025-03-06 18:27:24,008 - INFO - training batch 1601, loss: 0.279, 51232/60000 datapoints
2025-03-06 18:27:24,204 - INFO - training batch 1651, loss: 0.103, 52832/60000 datapoints
2025-03-06 18:27:24,398 - INFO - training batch 1701, loss: 0.397, 54432/60000 datapoints
2025-03-06 18:27:24,595 - INFO - training batch 1751, loss: 0.192, 56032/60000 datapoints
2025-03-06 18:27:24,796 - INFO - training batch 1801, loss: 0.295, 57632/60000 datapoints
2025-03-06 18:27:24,994 - INFO - training batch 1851, loss: 0.250, 59232/60000 datapoints
2025-03-06 18:27:25,095 - INFO - validation batch 1, loss: 0.375, 32/10016 datapoints
2025-03-06 18:27:25,250 - INFO - validation batch 51, loss: 0.134, 1632/10016 datapoints
2025-03-06 18:27:25,401 - INFO - validation batch 101, loss: 0.368, 3232/10016 datapoints
2025-03-06 18:27:25,551 - INFO - validation batch 151, loss: 0.682, 4832/10016 datapoints
2025-03-06 18:27:25,708 - INFO - validation batch 201, loss: 0.572, 6432/10016 datapoints
2025-03-06 18:27:25,862 - INFO - validation batch 251, loss: 0.216, 8032/10016 datapoints
2025-03-06 18:27:26,015 - INFO - validation batch 301, loss: 0.272, 9632/10016 datapoints
2025-03-06 18:27:26,052 - INFO - Epoch 312/800 done.
2025-03-06 18:27:26,052 - INFO - Final validation performance:
Loss: 0.374, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:27:26,052 - INFO - Beginning epoch 313/800
2025-03-06 18:27:26,060 - INFO - training batch 1, loss: 0.298, 32/60000 datapoints
2025-03-06 18:27:26,262 - INFO - training batch 51, loss: 0.323, 1632/60000 datapoints
2025-03-06 18:27:26,459 - INFO - training batch 101, loss: 0.445, 3232/60000 datapoints
2025-03-06 18:27:26,656 - INFO - training batch 151, loss: 0.255, 4832/60000 datapoints
2025-03-06 18:27:26,852 - INFO - training batch 201, loss: 0.281, 6432/60000 datapoints
2025-03-06 18:27:27,044 - INFO - training batch 251, loss: 0.283, 8032/60000 datapoints
2025-03-06 18:27:27,236 - INFO - training batch 301, loss: 0.232, 9632/60000 datapoints
2025-03-06 18:27:27,431 - INFO - training batch 351, loss: 0.342, 11232/60000 datapoints
2025-03-06 18:27:27,622 - INFO - training batch 401, loss: 0.497, 12832/60000 datapoints
2025-03-06 18:27:27,816 - INFO - training batch 451, loss: 0.201, 14432/60000 datapoints
2025-03-06 18:27:28,007 - INFO - training batch 501, loss: 0.182, 16032/60000 datapoints
2025-03-06 18:27:28,198 - INFO - training batch 551, loss: 0.489, 17632/60000 datapoints
2025-03-06 18:27:28,389 - INFO - training batch 601, loss: 0.324, 19232/60000 datapoints
2025-03-06 18:27:28,580 - INFO - training batch 651, loss: 0.185, 20832/60000 datapoints
2025-03-06 18:27:28,779 - INFO - training batch 701, loss: 0.258, 22432/60000 datapoints
2025-03-06 18:27:28,970 - INFO - training batch 751, loss: 0.326, 24032/60000 datapoints
2025-03-06 18:27:29,162 - INFO - training batch 801, loss: 0.360, 25632/60000 datapoints
2025-03-06 18:27:29,356 - INFO - training batch 851, loss: 0.285, 27232/60000 datapoints
2025-03-06 18:27:29,546 - INFO - training batch 901, loss: 0.400, 28832/60000 datapoints
2025-03-06 18:27:29,738 - INFO - training batch 951, loss: 0.194, 30432/60000 datapoints
2025-03-06 18:27:29,929 - INFO - training batch 1001, loss: 0.247, 32032/60000 datapoints
2025-03-06 18:27:30,121 - INFO - training batch 1051, loss: 0.244, 33632/60000 datapoints
2025-03-06 18:27:30,313 - INFO - training batch 1101, loss: 0.155, 35232/60000 datapoints
2025-03-06 18:27:30,507 - INFO - training batch 1151, loss: 0.212, 36832/60000 datapoints
2025-03-06 18:27:30,703 - INFO - training batch 1201, loss: 0.101, 38432/60000 datapoints
2025-03-06 18:27:30,916 - INFO - training batch 1251, loss: 0.133, 40032/60000 datapoints
2025-03-06 18:27:31,110 - INFO - training batch 1301, loss: 0.899, 41632/60000 datapoints
2025-03-06 18:27:31,303 - INFO - training batch 1351, loss: 0.336, 43232/60000 datapoints
2025-03-06 18:27:31,496 - INFO - training batch 1401, loss: 0.502, 44832/60000 datapoints
2025-03-06 18:27:31,691 - INFO - training batch 1451, loss: 0.089, 46432/60000 datapoints
2025-03-06 18:27:31,892 - INFO - training batch 1501, loss: 0.382, 48032/60000 datapoints
2025-03-06 18:27:32,117 - INFO - training batch 1551, loss: 0.240, 49632/60000 datapoints
2025-03-06 18:27:32,322 - INFO - training batch 1601, loss: 0.192, 51232/60000 datapoints
2025-03-06 18:27:32,513 - INFO - training batch 1651, loss: 0.632, 52832/60000 datapoints
2025-03-06 18:27:32,706 - INFO - training batch 1701, loss: 0.168, 54432/60000 datapoints
2025-03-06 18:27:32,900 - INFO - training batch 1751, loss: 0.136, 56032/60000 datapoints
2025-03-06 18:27:33,101 - INFO - training batch 1801, loss: 0.402, 57632/60000 datapoints
2025-03-06 18:27:33,296 - INFO - training batch 1851, loss: 0.178, 59232/60000 datapoints
2025-03-06 18:27:33,398 - INFO - validation batch 1, loss: 0.234, 32/10016 datapoints
2025-03-06 18:27:33,550 - INFO - validation batch 51, loss: 0.231, 1632/10016 datapoints
2025-03-06 18:27:33,706 - INFO - validation batch 101, loss: 0.253, 3232/10016 datapoints
2025-03-06 18:27:33,860 - INFO - validation batch 151, loss: 0.290, 4832/10016 datapoints
2025-03-06 18:27:34,012 - INFO - validation batch 201, loss: 0.353, 6432/10016 datapoints
2025-03-06 18:27:34,166 - INFO - validation batch 251, loss: 0.106, 8032/10016 datapoints
2025-03-06 18:27:34,318 - INFO - validation batch 301, loss: 0.183, 9632/10016 datapoints
2025-03-06 18:27:34,354 - INFO - Epoch 313/800 done.
2025-03-06 18:27:34,355 - INFO - Final validation performance:
Loss: 0.236, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:27:34,355 - INFO - Beginning epoch 314/800
2025-03-06 18:27:34,362 - INFO - training batch 1, loss: 0.222, 32/60000 datapoints
2025-03-06 18:27:34,559 - INFO - training batch 51, loss: 0.287, 1632/60000 datapoints
2025-03-06 18:27:34,755 - INFO - training batch 101, loss: 0.338, 3232/60000 datapoints
2025-03-06 18:27:34,960 - INFO - training batch 151, loss: 0.188, 4832/60000 datapoints
2025-03-06 18:27:35,156 - INFO - training batch 201, loss: 0.251, 6432/60000 datapoints
2025-03-06 18:27:35,356 - INFO - training batch 251, loss: 0.386, 8032/60000 datapoints
2025-03-06 18:27:35,550 - INFO - training batch 301, loss: 0.215, 9632/60000 datapoints
2025-03-06 18:27:35,749 - INFO - training batch 351, loss: 0.174, 11232/60000 datapoints
2025-03-06 18:27:35,947 - INFO - training batch 401, loss: 0.288, 12832/60000 datapoints
2025-03-06 18:27:36,142 - INFO - training batch 451, loss: 0.325, 14432/60000 datapoints
2025-03-06 18:27:36,340 - INFO - training batch 501, loss: 0.218, 16032/60000 datapoints
2025-03-06 18:27:36,536 - INFO - training batch 551, loss: 0.402, 17632/60000 datapoints
2025-03-06 18:27:36,732 - INFO - training batch 601, loss: 0.168, 19232/60000 datapoints
2025-03-06 18:27:36,931 - INFO - training batch 651, loss: 0.359, 20832/60000 datapoints
2025-03-06 18:27:37,128 - INFO - training batch 701, loss: 0.175, 22432/60000 datapoints
2025-03-06 18:27:37,322 - INFO - training batch 751, loss: 0.277, 24032/60000 datapoints
2025-03-06 18:27:37,517 - INFO - training batch 801, loss: 0.507, 25632/60000 datapoints
2025-03-06 18:27:37,730 - INFO - training batch 851, loss: 0.349, 27232/60000 datapoints
2025-03-06 18:27:37,928 - INFO - training batch 901, loss: 0.305, 28832/60000 datapoints
2025-03-06 18:27:38,121 - INFO - training batch 951, loss: 0.323, 30432/60000 datapoints
2025-03-06 18:27:38,314 - INFO - training batch 1001, loss: 0.563, 32032/60000 datapoints
2025-03-06 18:27:38,511 - INFO - training batch 1051, loss: 0.206, 33632/60000 datapoints
2025-03-06 18:27:38,707 - INFO - training batch 1101, loss: 0.287, 35232/60000 datapoints
2025-03-06 18:27:38,906 - INFO - training batch 1151, loss: 0.301, 36832/60000 datapoints
2025-03-06 18:27:39,101 - INFO - training batch 1201, loss: 0.341, 38432/60000 datapoints
2025-03-06 18:27:39,294 - INFO - training batch 1251, loss: 0.170, 40032/60000 datapoints
2025-03-06 18:27:39,490 - INFO - training batch 1301, loss: 0.532, 41632/60000 datapoints
2025-03-06 18:27:39,687 - INFO - training batch 1351, loss: 0.618, 43232/60000 datapoints
2025-03-06 18:27:39,882 - INFO - training batch 1401, loss: 0.129, 44832/60000 datapoints
2025-03-06 18:27:40,079 - INFO - training batch 1451, loss: 0.441, 46432/60000 datapoints
2025-03-06 18:27:40,271 - INFO - training batch 1501, loss: 0.201, 48032/60000 datapoints
2025-03-06 18:27:40,464 - INFO - training batch 1551, loss: 0.638, 49632/60000 datapoints
2025-03-06 18:27:40,662 - INFO - training batch 1601, loss: 0.102, 51232/60000 datapoints
2025-03-06 18:27:40,876 - INFO - training batch 1651, loss: 0.394, 52832/60000 datapoints
2025-03-06 18:27:41,071 - INFO - training batch 1701, loss: 0.203, 54432/60000 datapoints
2025-03-06 18:27:41,264 - INFO - training batch 1751, loss: 0.255, 56032/60000 datapoints
2025-03-06 18:27:41,469 - INFO - training batch 1801, loss: 0.336, 57632/60000 datapoints
2025-03-06 18:27:41,664 - INFO - training batch 1851, loss: 0.169, 59232/60000 datapoints
2025-03-06 18:27:41,765 - INFO - validation batch 1, loss: 0.195, 32/10016 datapoints
2025-03-06 18:27:41,919 - INFO - validation batch 51, loss: 0.150, 1632/10016 datapoints
2025-03-06 18:27:42,077 - INFO - validation batch 101, loss: 0.256, 3232/10016 datapoints
2025-03-06 18:27:42,228 - INFO - validation batch 151, loss: 0.213, 4832/10016 datapoints
2025-03-06 18:27:42,393 - INFO - validation batch 201, loss: 0.211, 6432/10016 datapoints
2025-03-06 18:27:42,573 - INFO - validation batch 251, loss: 0.149, 8032/10016 datapoints
2025-03-06 18:27:42,726 - INFO - validation batch 301, loss: 0.301, 9632/10016 datapoints
2025-03-06 18:27:42,762 - INFO - Epoch 314/800 done.
2025-03-06 18:27:42,763 - INFO - Final validation performance:
Loss: 0.211, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:27:42,763 - INFO - Beginning epoch 315/800
2025-03-06 18:27:42,769 - INFO - training batch 1, loss: 0.077, 32/60000 datapoints
2025-03-06 18:27:42,966 - INFO - training batch 51, loss: 0.183, 1632/60000 datapoints
2025-03-06 18:27:43,159 - INFO - training batch 101, loss: 0.254, 3232/60000 datapoints
2025-03-06 18:27:43,350 - INFO - training batch 151, loss: 0.216, 4832/60000 datapoints
2025-03-06 18:27:43,545 - INFO - training batch 201, loss: 0.377, 6432/60000 datapoints
2025-03-06 18:27:43,759 - INFO - training batch 251, loss: 0.512, 8032/60000 datapoints
2025-03-06 18:27:43,952 - INFO - training batch 301, loss: 0.463, 9632/60000 datapoints
2025-03-06 18:27:44,142 - INFO - training batch 351, loss: 0.540, 11232/60000 datapoints
2025-03-06 18:27:44,333 - INFO - training batch 401, loss: 0.421, 12832/60000 datapoints
2025-03-06 18:27:44,527 - INFO - training batch 451, loss: 0.278, 14432/60000 datapoints
2025-03-06 18:27:44,720 - INFO - training batch 501, loss: 0.155, 16032/60000 datapoints
2025-03-06 18:27:44,917 - INFO - training batch 551, loss: 0.492, 17632/60000 datapoints
2025-03-06 18:27:45,111 - INFO - training batch 601, loss: 0.138, 19232/60000 datapoints
2025-03-06 18:27:45,303 - INFO - training batch 651, loss: 0.216, 20832/60000 datapoints
2025-03-06 18:27:45,496 - INFO - training batch 701, loss: 0.159, 22432/60000 datapoints
2025-03-06 18:27:45,688 - INFO - training batch 751, loss: 0.287, 24032/60000 datapoints
2025-03-06 18:27:45,878 - INFO - training batch 801, loss: 0.189, 25632/60000 datapoints
2025-03-06 18:27:46,071 - INFO - training batch 851, loss: 0.529, 27232/60000 datapoints
2025-03-06 18:27:46,264 - INFO - training batch 901, loss: 0.154, 28832/60000 datapoints
2025-03-06 18:27:46,457 - INFO - training batch 951, loss: 0.234, 30432/60000 datapoints
2025-03-06 18:27:46,656 - INFO - training batch 1001, loss: 0.400, 32032/60000 datapoints
2025-03-06 18:27:46,849 - INFO - training batch 1051, loss: 0.117, 33632/60000 datapoints
2025-03-06 18:27:47,041 - INFO - training batch 1101, loss: 0.231, 35232/60000 datapoints
2025-03-06 18:27:47,232 - INFO - training batch 1151, loss: 0.300, 36832/60000 datapoints
2025-03-06 18:27:47,424 - INFO - training batch 1201, loss: 0.255, 38432/60000 datapoints
2025-03-06 18:27:47,617 - INFO - training batch 1251, loss: 0.202, 40032/60000 datapoints
2025-03-06 18:27:47,809 - INFO - training batch 1301, loss: 0.227, 41632/60000 datapoints
2025-03-06 18:27:48,001 - INFO - training batch 1351, loss: 0.478, 43232/60000 datapoints
2025-03-06 18:27:48,192 - INFO - training batch 1401, loss: 0.205, 44832/60000 datapoints
2025-03-06 18:27:48,403 - INFO - training batch 1451, loss: 0.239, 46432/60000 datapoints
2025-03-06 18:27:48,600 - INFO - training batch 1501, loss: 0.325, 48032/60000 datapoints
2025-03-06 18:27:48,789 - INFO - training batch 1551, loss: 0.295, 49632/60000 datapoints
2025-03-06 18:27:48,986 - INFO - training batch 1601, loss: 0.338, 51232/60000 datapoints
2025-03-06 18:27:49,177 - INFO - training batch 1651, loss: 0.205, 52832/60000 datapoints
2025-03-06 18:27:49,369 - INFO - training batch 1701, loss: 0.248, 54432/60000 datapoints
2025-03-06 18:27:49,560 - INFO - training batch 1751, loss: 0.276, 56032/60000 datapoints
2025-03-06 18:27:49,757 - INFO - training batch 1801, loss: 0.214, 57632/60000 datapoints
2025-03-06 18:27:49,947 - INFO - training batch 1851, loss: 0.161, 59232/60000 datapoints
2025-03-06 18:27:50,048 - INFO - validation batch 1, loss: 0.201, 32/10016 datapoints
2025-03-06 18:27:50,197 - INFO - validation batch 51, loss: 0.389, 1632/10016 datapoints
2025-03-06 18:27:50,348 - INFO - validation batch 101, loss: 0.306, 3232/10016 datapoints
2025-03-06 18:27:50,498 - INFO - validation batch 151, loss: 0.463, 4832/10016 datapoints
2025-03-06 18:27:50,651 - INFO - validation batch 201, loss: 0.455, 6432/10016 datapoints
2025-03-06 18:27:50,801 - INFO - validation batch 251, loss: 0.357, 8032/10016 datapoints
2025-03-06 18:27:50,971 - INFO - validation batch 301, loss: 0.300, 9632/10016 datapoints
2025-03-06 18:27:51,007 - INFO - Epoch 315/800 done.
2025-03-06 18:27:51,007 - INFO - Final validation performance:
Loss: 0.353, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 18:27:51,008 - INFO - Beginning epoch 316/800
2025-03-06 18:27:51,014 - INFO - training batch 1, loss: 0.176, 32/60000 datapoints
2025-03-06 18:27:51,205 - INFO - training batch 51, loss: 0.320, 1632/60000 datapoints
2025-03-06 18:27:51,398 - INFO - training batch 101, loss: 0.449, 3232/60000 datapoints
2025-03-06 18:27:51,592 - INFO - training batch 151, loss: 0.265, 4832/60000 datapoints
2025-03-06 18:27:51,788 - INFO - training batch 201, loss: 0.363, 6432/60000 datapoints
2025-03-06 18:27:51,978 - INFO - training batch 251, loss: 0.256, 8032/60000 datapoints
2025-03-06 18:27:52,169 - INFO - training batch 301, loss: 0.347, 9632/60000 datapoints
2025-03-06 18:27:52,370 - INFO - training batch 351, loss: 0.171, 11232/60000 datapoints
2025-03-06 18:27:52,563 - INFO - training batch 401, loss: 0.190, 12832/60000 datapoints
2025-03-06 18:27:52,757 - INFO - training batch 451, loss: 0.188, 14432/60000 datapoints
2025-03-06 18:27:52,951 - INFO - training batch 501, loss: 0.354, 16032/60000 datapoints
2025-03-06 18:27:53,143 - INFO - training batch 551, loss: 0.333, 17632/60000 datapoints
2025-03-06 18:27:53,342 - INFO - training batch 601, loss: 0.282, 19232/60000 datapoints
2025-03-06 18:27:53,540 - INFO - training batch 651, loss: 0.374, 20832/60000 datapoints
2025-03-06 18:27:53,736 - INFO - training batch 701, loss: 0.119, 22432/60000 datapoints
2025-03-06 18:27:53,931 - INFO - training batch 751, loss: 0.236, 24032/60000 datapoints
2025-03-06 18:27:54,124 - INFO - training batch 801, loss: 0.434, 25632/60000 datapoints
2025-03-06 18:27:54,318 - INFO - training batch 851, loss: 0.210, 27232/60000 datapoints
2025-03-06 18:27:54,514 - INFO - training batch 901, loss: 0.163, 28832/60000 datapoints
2025-03-06 18:27:54,715 - INFO - training batch 951, loss: 0.633, 30432/60000 datapoints
2025-03-06 18:27:54,916 - INFO - training batch 1001, loss: 0.262, 32032/60000 datapoints
2025-03-06 18:27:55,113 - INFO - training batch 1051, loss: 0.242, 33632/60000 datapoints
2025-03-06 18:27:55,307 - INFO - training batch 1101, loss: 0.338, 35232/60000 datapoints
2025-03-06 18:27:55,502 - INFO - training batch 1151, loss: 0.245, 36832/60000 datapoints
2025-03-06 18:27:55,703 - INFO - training batch 1201, loss: 0.263, 38432/60000 datapoints
2025-03-06 18:27:55,899 - INFO - training batch 1251, loss: 0.254, 40032/60000 datapoints
2025-03-06 18:27:56,094 - INFO - training batch 1301, loss: 0.252, 41632/60000 datapoints
2025-03-06 18:27:56,292 - INFO - training batch 1351, loss: 0.497, 43232/60000 datapoints
2025-03-06 18:27:56,487 - INFO - training batch 1401, loss: 0.424, 44832/60000 datapoints
2025-03-06 18:27:56,684 - INFO - training batch 1451, loss: 0.404, 46432/60000 datapoints
2025-03-06 18:27:56,880 - INFO - training batch 1501, loss: 0.529, 48032/60000 datapoints
2025-03-06 18:27:57,073 - INFO - training batch 1551, loss: 0.224, 49632/60000 datapoints
2025-03-06 18:27:57,267 - INFO - training batch 1601, loss: 0.550, 51232/60000 datapoints
2025-03-06 18:27:57,458 - INFO - training batch 1651, loss: 0.164, 52832/60000 datapoints
2025-03-06 18:27:57,656 - INFO - training batch 1701, loss: 0.199, 54432/60000 datapoints
2025-03-06 18:27:57,851 - INFO - training batch 1751, loss: 0.223, 56032/60000 datapoints
2025-03-06 18:27:58,046 - INFO - training batch 1801, loss: 0.489, 57632/60000 datapoints
2025-03-06 18:27:58,239 - INFO - training batch 1851, loss: 0.392, 59232/60000 datapoints
2025-03-06 18:27:58,341 - INFO - validation batch 1, loss: 0.407, 32/10016 datapoints
2025-03-06 18:27:58,494 - INFO - validation batch 51, loss: 0.169, 1632/10016 datapoints
2025-03-06 18:27:58,651 - INFO - validation batch 101, loss: 0.248, 3232/10016 datapoints
2025-03-06 18:27:58,803 - INFO - validation batch 151, loss: 0.126, 4832/10016 datapoints
2025-03-06 18:27:58,959 - INFO - validation batch 201, loss: 0.215, 6432/10016 datapoints
2025-03-06 18:27:59,114 - INFO - validation batch 251, loss: 0.243, 8032/10016 datapoints
2025-03-06 18:27:59,265 - INFO - validation batch 301, loss: 0.298, 9632/10016 datapoints
2025-03-06 18:27:59,301 - INFO - Epoch 316/800 done.
2025-03-06 18:27:59,301 - INFO - Final validation performance:
Loss: 0.244, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:27:59,302 - INFO - Beginning epoch 317/800
2025-03-06 18:27:59,309 - INFO - training batch 1, loss: 0.327, 32/60000 datapoints
2025-03-06 18:27:59,508 - INFO - training batch 51, loss: 0.265, 1632/60000 datapoints
2025-03-06 18:27:59,704 - INFO - training batch 101, loss: 0.380, 3232/60000 datapoints
2025-03-06 18:27:59,896 - INFO - training batch 151, loss: 0.448, 4832/60000 datapoints
2025-03-06 18:28:00,089 - INFO - training batch 201, loss: 0.288, 6432/60000 datapoints
2025-03-06 18:28:00,284 - INFO - training batch 251, loss: 0.349, 8032/60000 datapoints
2025-03-06 18:28:00,480 - INFO - training batch 301, loss: 0.545, 9632/60000 datapoints
2025-03-06 18:28:00,677 - INFO - training batch 351, loss: 0.167, 11232/60000 datapoints
2025-03-06 18:28:00,874 - INFO - training batch 401, loss: 0.459, 12832/60000 datapoints
2025-03-06 18:28:01,093 - INFO - training batch 451, loss: 0.146, 14432/60000 datapoints
2025-03-06 18:28:01,287 - INFO - training batch 501, loss: 0.251, 16032/60000 datapoints
2025-03-06 18:28:01,485 - INFO - training batch 551, loss: 0.213, 17632/60000 datapoints
2025-03-06 18:28:01,680 - INFO - training batch 601, loss: 0.289, 19232/60000 datapoints
2025-03-06 18:28:01,875 - INFO - training batch 651, loss: 0.244, 20832/60000 datapoints
2025-03-06 18:28:02,068 - INFO - training batch 701, loss: 0.399, 22432/60000 datapoints
2025-03-06 18:28:02,265 - INFO - training batch 751, loss: 0.204, 24032/60000 datapoints
2025-03-06 18:28:02,459 - INFO - training batch 801, loss: 0.372, 25632/60000 datapoints
2025-03-06 18:28:02,655 - INFO - training batch 851, loss: 0.171, 27232/60000 datapoints
2025-03-06 18:28:02,850 - INFO - training batch 901, loss: 0.247, 28832/60000 datapoints
2025-03-06 18:28:03,045 - INFO - training batch 951, loss: 0.235, 30432/60000 datapoints
2025-03-06 18:28:03,237 - INFO - training batch 1001, loss: 0.279, 32032/60000 datapoints
2025-03-06 18:28:03,433 - INFO - training batch 1051, loss: 0.295, 33632/60000 datapoints
2025-03-06 18:28:03,633 - INFO - training batch 1101, loss: 0.204, 35232/60000 datapoints
2025-03-06 18:28:03,829 - INFO - training batch 1151, loss: 0.495, 36832/60000 datapoints
2025-03-06 18:28:04,023 - INFO - training batch 1201, loss: 0.543, 38432/60000 datapoints
2025-03-06 18:28:04,218 - INFO - training batch 1251, loss: 0.222, 40032/60000 datapoints
2025-03-06 18:28:04,411 - INFO - training batch 1301, loss: 0.355, 41632/60000 datapoints
2025-03-06 18:28:04,608 - INFO - training batch 1351, loss: 0.148, 43232/60000 datapoints
2025-03-06 18:28:04,803 - INFO - training batch 1401, loss: 0.298, 44832/60000 datapoints
2025-03-06 18:28:05,006 - INFO - training batch 1451, loss: 0.172, 46432/60000 datapoints
2025-03-06 18:28:05,203 - INFO - training batch 1501, loss: 0.188, 48032/60000 datapoints
2025-03-06 18:28:05,396 - INFO - training batch 1551, loss: 0.302, 49632/60000 datapoints
2025-03-06 18:28:05,591 - INFO - training batch 1601, loss: 0.332, 51232/60000 datapoints
2025-03-06 18:28:05,787 - INFO - training batch 1651, loss: 0.160, 52832/60000 datapoints
2025-03-06 18:28:05,981 - INFO - training batch 1701, loss: 0.322, 54432/60000 datapoints
2025-03-06 18:28:06,179 - INFO - training batch 1751, loss: 0.222, 56032/60000 datapoints
2025-03-06 18:28:06,374 - INFO - training batch 1801, loss: 0.378, 57632/60000 datapoints
2025-03-06 18:28:06,574 - INFO - training batch 1851, loss: 0.354, 59232/60000 datapoints
2025-03-06 18:28:06,681 - INFO - validation batch 1, loss: 0.312, 32/10016 datapoints
2025-03-06 18:28:06,835 - INFO - validation batch 51, loss: 0.301, 1632/10016 datapoints
2025-03-06 18:28:06,988 - INFO - validation batch 101, loss: 0.229, 3232/10016 datapoints
2025-03-06 18:28:07,140 - INFO - validation batch 151, loss: 0.295, 4832/10016 datapoints
2025-03-06 18:28:07,292 - INFO - validation batch 201, loss: 0.435, 6432/10016 datapoints
2025-03-06 18:28:07,443 - INFO - validation batch 251, loss: 0.498, 8032/10016 datapoints
2025-03-06 18:28:07,602 - INFO - validation batch 301, loss: 0.555, 9632/10016 datapoints
2025-03-06 18:28:07,642 - INFO - Epoch 317/800 done.
2025-03-06 18:28:07,642 - INFO - Final validation performance:
Loss: 0.375, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:28:07,642 - INFO - Beginning epoch 318/800
2025-03-06 18:28:07,648 - INFO - training batch 1, loss: 0.132, 32/60000 datapoints
2025-03-06 18:28:07,845 - INFO - training batch 51, loss: 0.209, 1632/60000 datapoints
2025-03-06 18:28:08,050 - INFO - training batch 101, loss: 0.220, 3232/60000 datapoints
2025-03-06 18:28:08,248 - INFO - training batch 151, loss: 0.261, 4832/60000 datapoints
2025-03-06 18:28:08,440 - INFO - training batch 201, loss: 0.281, 6432/60000 datapoints
2025-03-06 18:28:08,634 - INFO - training batch 251, loss: 0.358, 8032/60000 datapoints
2025-03-06 18:28:08,827 - INFO - training batch 301, loss: 0.383, 9632/60000 datapoints
2025-03-06 18:28:09,026 - INFO - training batch 351, loss: 0.437, 11232/60000 datapoints
2025-03-06 18:28:09,218 - INFO - training batch 401, loss: 0.410, 12832/60000 datapoints
2025-03-06 18:28:09,414 - INFO - training batch 451, loss: 0.312, 14432/60000 datapoints
2025-03-06 18:28:09,610 - INFO - training batch 501, loss: 0.463, 16032/60000 datapoints
2025-03-06 18:28:09,805 - INFO - training batch 551, loss: 0.119, 17632/60000 datapoints
2025-03-06 18:28:09,999 - INFO - training batch 601, loss: 0.284, 19232/60000 datapoints
2025-03-06 18:28:10,193 - INFO - training batch 651, loss: 0.185, 20832/60000 datapoints
2025-03-06 18:28:10,389 - INFO - training batch 701, loss: 0.354, 22432/60000 datapoints
2025-03-06 18:28:10,583 - INFO - training batch 751, loss: 0.224, 24032/60000 datapoints
2025-03-06 18:28:10,782 - INFO - training batch 801, loss: 0.155, 25632/60000 datapoints
2025-03-06 18:28:10,979 - INFO - training batch 851, loss: 0.242, 27232/60000 datapoints
2025-03-06 18:28:11,194 - INFO - training batch 901, loss: 0.281, 28832/60000 datapoints
2025-03-06 18:28:11,394 - INFO - training batch 951, loss: 0.345, 30432/60000 datapoints
2025-03-06 18:28:11,596 - INFO - training batch 1001, loss: 0.347, 32032/60000 datapoints
2025-03-06 18:28:11,794 - INFO - training batch 1051, loss: 0.236, 33632/60000 datapoints
2025-03-06 18:28:11,988 - INFO - training batch 1101, loss: 0.366, 35232/60000 datapoints
2025-03-06 18:28:12,181 - INFO - training batch 1151, loss: 0.412, 36832/60000 datapoints
2025-03-06 18:28:12,376 - INFO - training batch 1201, loss: 0.422, 38432/60000 datapoints
2025-03-06 18:28:12,571 - INFO - training batch 1251, loss: 0.333, 40032/60000 datapoints
2025-03-06 18:28:12,770 - INFO - training batch 1301, loss: 0.296, 41632/60000 datapoints
2025-03-06 18:28:12,968 - INFO - training batch 1351, loss: 0.237, 43232/60000 datapoints
2025-03-06 18:28:13,165 - INFO - training batch 1401, loss: 0.155, 44832/60000 datapoints
2025-03-06 18:28:13,373 - INFO - training batch 1451, loss: 0.171, 46432/60000 datapoints
2025-03-06 18:28:13,569 - INFO - training batch 1501, loss: 0.439, 48032/60000 datapoints
2025-03-06 18:28:13,765 - INFO - training batch 1551, loss: 0.119, 49632/60000 datapoints
2025-03-06 18:28:13,959 - INFO - training batch 1601, loss: 0.371, 51232/60000 datapoints
2025-03-06 18:28:14,151 - INFO - training batch 1651, loss: 0.136, 52832/60000 datapoints
2025-03-06 18:28:14,345 - INFO - training batch 1701, loss: 0.200, 54432/60000 datapoints
2025-03-06 18:28:14,538 - INFO - training batch 1751, loss: 0.194, 56032/60000 datapoints
2025-03-06 18:28:14,733 - INFO - training batch 1801, loss: 0.212, 57632/60000 datapoints
2025-03-06 18:28:14,935 - INFO - training batch 1851, loss: 0.755, 59232/60000 datapoints
2025-03-06 18:28:15,037 - INFO - validation batch 1, loss: 0.472, 32/10016 datapoints
2025-03-06 18:28:15,190 - INFO - validation batch 51, loss: 0.251, 1632/10016 datapoints
2025-03-06 18:28:15,342 - INFO - validation batch 101, loss: 0.249, 3232/10016 datapoints
2025-03-06 18:28:15,494 - INFO - validation batch 151, loss: 0.758, 4832/10016 datapoints
2025-03-06 18:28:15,647 - INFO - validation batch 201, loss: 0.083, 6432/10016 datapoints
2025-03-06 18:28:15,800 - INFO - validation batch 251, loss: 0.179, 8032/10016 datapoints
2025-03-06 18:28:15,953 - INFO - validation batch 301, loss: 0.232, 9632/10016 datapoints
2025-03-06 18:28:15,992 - INFO - Epoch 318/800 done.
2025-03-06 18:28:15,992 - INFO - Final validation performance:
Loss: 0.318, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:28:15,992 - INFO - Beginning epoch 319/800
2025-03-06 18:28:15,998 - INFO - training batch 1, loss: 0.246, 32/60000 datapoints
2025-03-06 18:28:16,195 - INFO - training batch 51, loss: 0.223, 1632/60000 datapoints
2025-03-06 18:28:16,389 - INFO - training batch 101, loss: 0.501, 3232/60000 datapoints
2025-03-06 18:28:16,584 - INFO - training batch 151, loss: 0.271, 4832/60000 datapoints
2025-03-06 18:28:16,781 - INFO - training batch 201, loss: 0.546, 6432/60000 datapoints
2025-03-06 18:28:16,980 - INFO - training batch 251, loss: 0.139, 8032/60000 datapoints
2025-03-06 18:28:17,173 - INFO - training batch 301, loss: 0.475, 9632/60000 datapoints
2025-03-06 18:28:17,369 - INFO - training batch 351, loss: 0.341, 11232/60000 datapoints
2025-03-06 18:28:17,562 - INFO - training batch 401, loss: 0.223, 12832/60000 datapoints
2025-03-06 18:28:17,759 - INFO - training batch 451, loss: 0.446, 14432/60000 datapoints
2025-03-06 18:28:17,955 - INFO - training batch 501, loss: 0.256, 16032/60000 datapoints
2025-03-06 18:28:18,148 - INFO - training batch 551, loss: 0.328, 17632/60000 datapoints
2025-03-06 18:28:18,343 - INFO - training batch 601, loss: 0.429, 19232/60000 datapoints
2025-03-06 18:28:18,537 - INFO - training batch 651, loss: 0.256, 20832/60000 datapoints
2025-03-06 18:28:18,734 - INFO - training batch 701, loss: 0.399, 22432/60000 datapoints
2025-03-06 18:28:18,928 - INFO - training batch 751, loss: 0.307, 24032/60000 datapoints
2025-03-06 18:28:19,125 - INFO - training batch 801, loss: 0.376, 25632/60000 datapoints
2025-03-06 18:28:19,318 - INFO - training batch 851, loss: 0.384, 27232/60000 datapoints
2025-03-06 18:28:19,512 - INFO - training batch 901, loss: 0.224, 28832/60000 datapoints
2025-03-06 18:28:19,709 - INFO - training batch 951, loss: 0.199, 30432/60000 datapoints
2025-03-06 18:28:19,903 - INFO - training batch 1001, loss: 0.326, 32032/60000 datapoints
2025-03-06 18:28:20,096 - INFO - training batch 1051, loss: 0.265, 33632/60000 datapoints
2025-03-06 18:28:20,288 - INFO - training batch 1101, loss: 0.376, 35232/60000 datapoints
2025-03-06 18:28:20,483 - INFO - training batch 1151, loss: 0.336, 36832/60000 datapoints
2025-03-06 18:28:20,680 - INFO - training batch 1201, loss: 0.234, 38432/60000 datapoints
2025-03-06 18:28:20,874 - INFO - training batch 1251, loss: 0.299, 40032/60000 datapoints
2025-03-06 18:28:21,071 - INFO - training batch 1301, loss: 0.313, 41632/60000 datapoints
2025-03-06 18:28:21,288 - INFO - training batch 1351, loss: 0.212, 43232/60000 datapoints
2025-03-06 18:28:21,485 - INFO - training batch 1401, loss: 0.277, 44832/60000 datapoints
2025-03-06 18:28:21,686 - INFO - training batch 1451, loss: 0.180, 46432/60000 datapoints
2025-03-06 18:28:21,881 - INFO - training batch 1501, loss: 0.884, 48032/60000 datapoints
2025-03-06 18:28:22,074 - INFO - training batch 1551, loss: 0.438, 49632/60000 datapoints
2025-03-06 18:28:22,267 - INFO - training batch 1601, loss: 0.195, 51232/60000 datapoints
2025-03-06 18:28:22,462 - INFO - training batch 1651, loss: 0.094, 52832/60000 datapoints
2025-03-06 18:28:22,657 - INFO - training batch 1701, loss: 0.236, 54432/60000 datapoints
2025-03-06 18:28:22,855 - INFO - training batch 1751, loss: 0.204, 56032/60000 datapoints
2025-03-06 18:28:23,052 - INFO - training batch 1801, loss: 0.274, 57632/60000 datapoints
2025-03-06 18:28:23,246 - INFO - training batch 1851, loss: 0.230, 59232/60000 datapoints
2025-03-06 18:28:23,344 - INFO - validation batch 1, loss: 0.218, 32/10016 datapoints
2025-03-06 18:28:23,498 - INFO - validation batch 51, loss: 0.433, 1632/10016 datapoints
2025-03-06 18:28:23,652 - INFO - validation batch 101, loss: 0.343, 3232/10016 datapoints
2025-03-06 18:28:23,804 - INFO - validation batch 151, loss: 0.261, 4832/10016 datapoints
2025-03-06 18:28:23,956 - INFO - validation batch 201, loss: 0.481, 6432/10016 datapoints
2025-03-06 18:28:24,108 - INFO - validation batch 251, loss: 0.214, 8032/10016 datapoints
2025-03-06 18:28:24,260 - INFO - validation batch 301, loss: 0.181, 9632/10016 datapoints
2025-03-06 18:28:24,297 - INFO - Epoch 319/800 done.
2025-03-06 18:28:24,297 - INFO - Final validation performance:
Loss: 0.305, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:28:24,297 - INFO - Beginning epoch 320/800
2025-03-06 18:28:24,303 - INFO - training batch 1, loss: 0.132, 32/60000 datapoints
2025-03-06 18:28:24,502 - INFO - training batch 51, loss: 0.211, 1632/60000 datapoints
2025-03-06 18:28:24,697 - INFO - training batch 101, loss: 0.446, 3232/60000 datapoints
2025-03-06 18:28:24,895 - INFO - training batch 151, loss: 0.439, 4832/60000 datapoints
2025-03-06 18:28:25,097 - INFO - training batch 201, loss: 0.367, 6432/60000 datapoints
2025-03-06 18:28:25,291 - INFO - training batch 251, loss: 0.286, 8032/60000 datapoints
2025-03-06 18:28:25,486 - INFO - training batch 301, loss: 0.162, 9632/60000 datapoints
2025-03-06 18:28:25,683 - INFO - training batch 351, loss: 0.208, 11232/60000 datapoints
2025-03-06 18:28:25,877 - INFO - training batch 401, loss: 0.582, 12832/60000 datapoints
2025-03-06 18:28:26,073 - INFO - training batch 451, loss: 0.344, 14432/60000 datapoints
2025-03-06 18:28:26,270 - INFO - training batch 501, loss: 0.442, 16032/60000 datapoints
2025-03-06 18:28:26,463 - INFO - training batch 551, loss: 0.430, 17632/60000 datapoints
2025-03-06 18:28:26,666 - INFO - training batch 601, loss: 0.266, 19232/60000 datapoints
2025-03-06 18:28:26,861 - INFO - training batch 651, loss: 0.436, 20832/60000 datapoints
2025-03-06 18:28:27,057 - INFO - training batch 701, loss: 0.319, 22432/60000 datapoints
2025-03-06 18:28:27,250 - INFO - training batch 751, loss: 0.483, 24032/60000 datapoints
2025-03-06 18:28:27,444 - INFO - training batch 801, loss: 0.224, 25632/60000 datapoints
2025-03-06 18:28:27,639 - INFO - training batch 851, loss: 0.388, 27232/60000 datapoints
2025-03-06 18:28:27,832 - INFO - training batch 901, loss: 0.317, 28832/60000 datapoints
2025-03-06 18:28:28,030 - INFO - training batch 951, loss: 0.434, 30432/60000 datapoints
2025-03-06 18:28:28,224 - INFO - training batch 1001, loss: 0.286, 32032/60000 datapoints
2025-03-06 18:28:28,417 - INFO - training batch 1051, loss: 0.199, 33632/60000 datapoints
2025-03-06 18:28:28,616 - INFO - training batch 1101, loss: 0.654, 35232/60000 datapoints
2025-03-06 18:28:28,807 - INFO - training batch 1151, loss: 0.095, 36832/60000 datapoints
2025-03-06 18:28:29,004 - INFO - training batch 1201, loss: 0.399, 38432/60000 datapoints
2025-03-06 18:28:29,199 - INFO - training batch 1251, loss: 0.578, 40032/60000 datapoints
2025-03-06 18:28:29,392 - INFO - training batch 1301, loss: 0.395, 41632/60000 datapoints
2025-03-06 18:28:29,585 - INFO - training batch 1351, loss: 0.227, 43232/60000 datapoints
2025-03-06 18:28:29,781 - INFO - training batch 1401, loss: 0.366, 44832/60000 datapoints
2025-03-06 18:28:29,973 - INFO - training batch 1451, loss: 0.205, 46432/60000 datapoints
2025-03-06 18:28:30,167 - INFO - training batch 1501, loss: 0.374, 48032/60000 datapoints
2025-03-06 18:28:30,362 - INFO - training batch 1551, loss: 0.312, 49632/60000 datapoints
2025-03-06 18:28:30,558 - INFO - training batch 1601, loss: 0.288, 51232/60000 datapoints
2025-03-06 18:28:30,754 - INFO - training batch 1651, loss: 0.362, 52832/60000 datapoints
2025-03-06 18:28:30,947 - INFO - training batch 1701, loss: 0.161, 54432/60000 datapoints
2025-03-06 18:28:31,145 - INFO - training batch 1751, loss: 0.271, 56032/60000 datapoints
2025-03-06 18:28:31,359 - INFO - training batch 1801, loss: 0.429, 57632/60000 datapoints
2025-03-06 18:28:31,556 - INFO - training batch 1851, loss: 0.310, 59232/60000 datapoints
2025-03-06 18:28:31,661 - INFO - validation batch 1, loss: 0.210, 32/10016 datapoints
2025-03-06 18:28:31,813 - INFO - validation batch 51, loss: 0.227, 1632/10016 datapoints
2025-03-06 18:28:31,965 - INFO - validation batch 101, loss: 0.386, 3232/10016 datapoints
2025-03-06 18:28:32,119 - INFO - validation batch 151, loss: 0.334, 4832/10016 datapoints
2025-03-06 18:28:32,273 - INFO - validation batch 201, loss: 0.350, 6432/10016 datapoints
2025-03-06 18:28:32,426 - INFO - validation batch 251, loss: 0.259, 8032/10016 datapoints
2025-03-06 18:28:32,579 - INFO - validation batch 301, loss: 0.166, 9632/10016 datapoints
2025-03-06 18:28:32,617 - INFO - Epoch 320/800 done.
2025-03-06 18:28:32,618 - INFO - Final validation performance:
Loss: 0.276, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:28:32,618 - INFO - Beginning epoch 321/800
2025-03-06 18:28:32,626 - INFO - training batch 1, loss: 0.280, 32/60000 datapoints
2025-03-06 18:28:32,820 - INFO - training batch 51, loss: 0.172, 1632/60000 datapoints
2025-03-06 18:28:33,015 - INFO - training batch 101, loss: 0.597, 3232/60000 datapoints
2025-03-06 18:28:33,205 - INFO - training batch 151, loss: 0.660, 4832/60000 datapoints
2025-03-06 18:28:33,395 - INFO - training batch 201, loss: 0.234, 6432/60000 datapoints
2025-03-06 18:28:33,594 - INFO - training batch 251, loss: 0.127, 8032/60000 datapoints
2025-03-06 18:28:33,789 - INFO - training batch 301, loss: 0.216, 9632/60000 datapoints
2025-03-06 18:28:33,983 - INFO - training batch 351, loss: 0.150, 11232/60000 datapoints
2025-03-06 18:28:34,178 - INFO - training batch 401, loss: 0.106, 12832/60000 datapoints
2025-03-06 18:28:34,372 - INFO - training batch 451, loss: 0.438, 14432/60000 datapoints
2025-03-06 18:28:34,566 - INFO - training batch 501, loss: 0.355, 16032/60000 datapoints
2025-03-06 18:28:34,763 - INFO - training batch 551, loss: 0.199, 17632/60000 datapoints
2025-03-06 18:28:34,961 - INFO - training batch 601, loss: 0.393, 19232/60000 datapoints
2025-03-06 18:28:35,161 - INFO - training batch 651, loss: 0.252, 20832/60000 datapoints
2025-03-06 18:28:35,356 - INFO - training batch 701, loss: 0.530, 22432/60000 datapoints
2025-03-06 18:28:35,550 - INFO - training batch 751, loss: 0.329, 24032/60000 datapoints
2025-03-06 18:28:35,745 - INFO - training batch 801, loss: 0.183, 25632/60000 datapoints
2025-03-06 18:28:35,937 - INFO - training batch 851, loss: 0.183, 27232/60000 datapoints
2025-03-06 18:28:36,132 - INFO - training batch 901, loss: 0.144, 28832/60000 datapoints
2025-03-06 18:28:36,329 - INFO - training batch 951, loss: 0.433, 30432/60000 datapoints
2025-03-06 18:28:36,523 - INFO - training batch 1001, loss: 0.096, 32032/60000 datapoints
2025-03-06 18:28:36,719 - INFO - training batch 1051, loss: 0.318, 33632/60000 datapoints
2025-03-06 18:28:36,911 - INFO - training batch 1101, loss: 0.312, 35232/60000 datapoints
2025-03-06 18:28:37,108 - INFO - training batch 1151, loss: 0.336, 36832/60000 datapoints
2025-03-06 18:28:37,299 - INFO - training batch 1201, loss: 0.374, 38432/60000 datapoints
2025-03-06 18:28:37,490 - INFO - training batch 1251, loss: 0.256, 40032/60000 datapoints
2025-03-06 18:28:37,696 - INFO - training batch 1301, loss: 0.203, 41632/60000 datapoints
2025-03-06 18:28:37,889 - INFO - training batch 1351, loss: 0.164, 43232/60000 datapoints
2025-03-06 18:28:38,090 - INFO - training batch 1401, loss: 0.223, 44832/60000 datapoints
2025-03-06 18:28:38,286 - INFO - training batch 1451, loss: 0.689, 46432/60000 datapoints
2025-03-06 18:28:38,479 - INFO - training batch 1501, loss: 0.224, 48032/60000 datapoints
2025-03-06 18:28:38,675 - INFO - training batch 1551, loss: 0.212, 49632/60000 datapoints
2025-03-06 18:28:38,867 - INFO - training batch 1601, loss: 0.304, 51232/60000 datapoints
2025-03-06 18:28:39,069 - INFO - training batch 1651, loss: 0.524, 52832/60000 datapoints
2025-03-06 18:28:39,264 - INFO - training batch 1701, loss: 0.278, 54432/60000 datapoints
2025-03-06 18:28:39,456 - INFO - training batch 1751, loss: 0.184, 56032/60000 datapoints
2025-03-06 18:28:39,652 - INFO - training batch 1801, loss: 0.248, 57632/60000 datapoints
2025-03-06 18:28:39,844 - INFO - training batch 1851, loss: 0.126, 59232/60000 datapoints
2025-03-06 18:28:39,947 - INFO - validation batch 1, loss: 0.418, 32/10016 datapoints
2025-03-06 18:28:40,101 - INFO - validation batch 51, loss: 0.190, 1632/10016 datapoints
2025-03-06 18:28:40,254 - INFO - validation batch 101, loss: 0.282, 3232/10016 datapoints
2025-03-06 18:28:40,407 - INFO - validation batch 151, loss: 0.383, 4832/10016 datapoints
2025-03-06 18:28:40,560 - INFO - validation batch 201, loss: 0.167, 6432/10016 datapoints
2025-03-06 18:28:40,716 - INFO - validation batch 251, loss: 0.253, 8032/10016 datapoints
2025-03-06 18:28:40,868 - INFO - validation batch 301, loss: 0.229, 9632/10016 datapoints
2025-03-06 18:28:40,903 - INFO - Epoch 321/800 done.
2025-03-06 18:28:40,903 - INFO - Final validation performance:
Loss: 0.275, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:28:40,904 - INFO - Beginning epoch 322/800
2025-03-06 18:28:40,910 - INFO - training batch 1, loss: 0.148, 32/60000 datapoints
2025-03-06 18:28:41,110 - INFO - training batch 51, loss: 0.491, 1632/60000 datapoints
2025-03-06 18:28:41,308 - INFO - training batch 101, loss: 0.466, 3232/60000 datapoints
2025-03-06 18:28:41,515 - INFO - training batch 151, loss: 0.362, 4832/60000 datapoints
2025-03-06 18:28:41,715 - INFO - training batch 201, loss: 0.384, 6432/60000 datapoints
2025-03-06 18:28:41,917 - INFO - training batch 251, loss: 0.694, 8032/60000 datapoints
2025-03-06 18:28:42,115 - INFO - training batch 301, loss: 0.162, 9632/60000 datapoints
2025-03-06 18:28:42,311 - INFO - training batch 351, loss: 0.386, 11232/60000 datapoints
2025-03-06 18:28:42,506 - INFO - training batch 401, loss: 0.214, 12832/60000 datapoints
2025-03-06 18:28:42,705 - INFO - training batch 451, loss: 0.281, 14432/60000 datapoints
2025-03-06 18:28:42,932 - INFO - training batch 501, loss: 0.331, 16032/60000 datapoints
2025-03-06 18:28:43,129 - INFO - training batch 551, loss: 0.433, 17632/60000 datapoints
2025-03-06 18:28:43,322 - INFO - training batch 601, loss: 0.168, 19232/60000 datapoints
2025-03-06 18:28:43,515 - INFO - training batch 651, loss: 0.217, 20832/60000 datapoints
2025-03-06 18:28:43,712 - INFO - training batch 701, loss: 0.547, 22432/60000 datapoints
2025-03-06 18:28:43,905 - INFO - training batch 751, loss: 0.680, 24032/60000 datapoints
2025-03-06 18:28:44,100 - INFO - training batch 801, loss: 0.729, 25632/60000 datapoints
2025-03-06 18:28:44,294 - INFO - training batch 851, loss: 0.164, 27232/60000 datapoints
2025-03-06 18:28:44,488 - INFO - training batch 901, loss: 0.171, 28832/60000 datapoints
2025-03-06 18:28:44,684 - INFO - training batch 951, loss: 0.276, 30432/60000 datapoints
2025-03-06 18:28:44,884 - INFO - training batch 1001, loss: 0.156, 32032/60000 datapoints
2025-03-06 18:28:45,085 - INFO - training batch 1051, loss: 0.432, 33632/60000 datapoints
2025-03-06 18:28:45,278 - INFO - training batch 1101, loss: 0.208, 35232/60000 datapoints
2025-03-06 18:28:45,472 - INFO - training batch 1151, loss: 0.158, 36832/60000 datapoints
2025-03-06 18:28:45,669 - INFO - training batch 1201, loss: 0.331, 38432/60000 datapoints
2025-03-06 18:28:45,863 - INFO - training batch 1251, loss: 0.212, 40032/60000 datapoints
2025-03-06 18:28:46,059 - INFO - training batch 1301, loss: 0.289, 41632/60000 datapoints
2025-03-06 18:28:46,258 - INFO - training batch 1351, loss: 0.144, 43232/60000 datapoints
2025-03-06 18:28:46,453 - INFO - training batch 1401, loss: 0.211, 44832/60000 datapoints
2025-03-06 18:28:46,653 - INFO - training batch 1451, loss: 0.134, 46432/60000 datapoints
2025-03-06 18:28:46,853 - INFO - training batch 1501, loss: 0.345, 48032/60000 datapoints
2025-03-06 18:28:47,051 - INFO - training batch 1551, loss: 0.283, 49632/60000 datapoints
2025-03-06 18:28:47,243 - INFO - training batch 1601, loss: 0.424, 51232/60000 datapoints
2025-03-06 18:28:47,437 - INFO - training batch 1651, loss: 0.654, 52832/60000 datapoints
2025-03-06 18:28:47,635 - INFO - training batch 1701, loss: 0.348, 54432/60000 datapoints
2025-03-06 18:28:47,829 - INFO - training batch 1751, loss: 0.243, 56032/60000 datapoints
2025-03-06 18:28:48,023 - INFO - training batch 1801, loss: 0.556, 57632/60000 datapoints
2025-03-06 18:28:48,273 - INFO - training batch 1851, loss: 0.167, 59232/60000 datapoints
2025-03-06 18:28:48,375 - INFO - validation batch 1, loss: 0.232, 32/10016 datapoints
2025-03-06 18:28:48,526 - INFO - validation batch 51, loss: 0.126, 1632/10016 datapoints
2025-03-06 18:28:48,680 - INFO - validation batch 101, loss: 0.193, 3232/10016 datapoints
2025-03-06 18:28:48,835 - INFO - validation batch 151, loss: 0.078, 4832/10016 datapoints
2025-03-06 18:28:48,987 - INFO - validation batch 201, loss: 0.577, 6432/10016 datapoints
2025-03-06 18:28:49,143 - INFO - validation batch 251, loss: 0.316, 8032/10016 datapoints
2025-03-06 18:28:49,296 - INFO - validation batch 301, loss: 0.370, 9632/10016 datapoints
2025-03-06 18:28:49,333 - INFO - Epoch 322/800 done.
2025-03-06 18:28:49,334 - INFO - Final validation performance:
Loss: 0.270, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:28:49,334 - INFO - Beginning epoch 323/800
2025-03-06 18:28:49,340 - INFO - training batch 1, loss: 0.300, 32/60000 datapoints
2025-03-06 18:28:49,532 - INFO - training batch 51, loss: 0.124, 1632/60000 datapoints
2025-03-06 18:28:49,732 - INFO - training batch 101, loss: 0.451, 3232/60000 datapoints
2025-03-06 18:28:49,924 - INFO - training batch 151, loss: 0.226, 4832/60000 datapoints
2025-03-06 18:28:50,119 - INFO - training batch 201, loss: 0.267, 6432/60000 datapoints
2025-03-06 18:28:50,314 - INFO - training batch 251, loss: 0.122, 8032/60000 datapoints
2025-03-06 18:28:50,508 - INFO - training batch 301, loss: 0.401, 9632/60000 datapoints
2025-03-06 18:28:50,702 - INFO - training batch 351, loss: 0.296, 11232/60000 datapoints
2025-03-06 18:28:50,899 - INFO - training batch 401, loss: 0.433, 12832/60000 datapoints
2025-03-06 18:28:51,098 - INFO - training batch 451, loss: 0.186, 14432/60000 datapoints
2025-03-06 18:28:51,293 - INFO - training batch 501, loss: 0.179, 16032/60000 datapoints
2025-03-06 18:28:51,510 - INFO - training batch 551, loss: 0.465, 17632/60000 datapoints
2025-03-06 18:28:51,709 - INFO - training batch 601, loss: 0.334, 19232/60000 datapoints
2025-03-06 18:28:51,903 - INFO - training batch 651, loss: 0.124, 20832/60000 datapoints
2025-03-06 18:28:52,096 - INFO - training batch 701, loss: 0.211, 22432/60000 datapoints
2025-03-06 18:28:52,288 - INFO - training batch 751, loss: 0.177, 24032/60000 datapoints
2025-03-06 18:28:52,482 - INFO - training batch 801, loss: 0.283, 25632/60000 datapoints
2025-03-06 18:28:52,678 - INFO - training batch 851, loss: 0.517, 27232/60000 datapoints
2025-03-06 18:28:52,874 - INFO - training batch 901, loss: 0.167, 28832/60000 datapoints
2025-03-06 18:28:53,071 - INFO - training batch 951, loss: 0.190, 30432/60000 datapoints
2025-03-06 18:28:53,267 - INFO - training batch 1001, loss: 0.343, 32032/60000 datapoints
2025-03-06 18:28:53,462 - INFO - training batch 1051, loss: 0.282, 33632/60000 datapoints
2025-03-06 18:28:53,659 - INFO - training batch 1101, loss: 0.216, 35232/60000 datapoints
2025-03-06 18:28:53,852 - INFO - training batch 1151, loss: 0.373, 36832/60000 datapoints
2025-03-06 18:28:54,044 - INFO - training batch 1201, loss: 0.208, 38432/60000 datapoints
2025-03-06 18:28:54,236 - INFO - training batch 1251, loss: 0.132, 40032/60000 datapoints
2025-03-06 18:28:54,429 - INFO - training batch 1301, loss: 0.165, 41632/60000 datapoints
2025-03-06 18:28:54,625 - INFO - training batch 1351, loss: 0.368, 43232/60000 datapoints
2025-03-06 18:28:54,820 - INFO - training batch 1401, loss: 0.349, 44832/60000 datapoints
2025-03-06 18:28:55,016 - INFO - training batch 1451, loss: 0.430, 46432/60000 datapoints
2025-03-06 18:28:55,214 - INFO - training batch 1501, loss: 0.134, 48032/60000 datapoints
2025-03-06 18:28:55,409 - INFO - training batch 1551, loss: 0.409, 49632/60000 datapoints
2025-03-06 18:28:55,608 - INFO - training batch 1601, loss: 0.487, 51232/60000 datapoints
2025-03-06 18:28:55,805 - INFO - training batch 1651, loss: 0.372, 52832/60000 datapoints
2025-03-06 18:28:55,997 - INFO - training batch 1701, loss: 0.270, 54432/60000 datapoints
2025-03-06 18:28:56,193 - INFO - training batch 1751, loss: 0.190, 56032/60000 datapoints
2025-03-06 18:28:56,387 - INFO - training batch 1801, loss: 0.190, 57632/60000 datapoints
2025-03-06 18:28:56,581 - INFO - training batch 1851, loss: 0.121, 59232/60000 datapoints
2025-03-06 18:28:56,685 - INFO - validation batch 1, loss: 0.236, 32/10016 datapoints
2025-03-06 18:28:56,839 - INFO - validation batch 51, loss: 0.535, 1632/10016 datapoints
2025-03-06 18:28:56,992 - INFO - validation batch 101, loss: 0.376, 3232/10016 datapoints
2025-03-06 18:28:57,147 - INFO - validation batch 151, loss: 0.176, 4832/10016 datapoints
2025-03-06 18:28:57,299 - INFO - validation batch 201, loss: 0.158, 6432/10016 datapoints
2025-03-06 18:28:57,452 - INFO - validation batch 251, loss: 0.246, 8032/10016 datapoints
2025-03-06 18:28:57,606 - INFO - validation batch 301, loss: 0.385, 9632/10016 datapoints
2025-03-06 18:28:57,643 - INFO - Epoch 323/800 done.
2025-03-06 18:28:57,643 - INFO - Final validation performance:
Loss: 0.302, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:28:57,644 - INFO - Beginning epoch 324/800
2025-03-06 18:28:57,650 - INFO - training batch 1, loss: 0.372, 32/60000 datapoints
2025-03-06 18:28:57,846 - INFO - training batch 51, loss: 0.421, 1632/60000 datapoints
2025-03-06 18:28:58,040 - INFO - training batch 101, loss: 0.481, 3232/60000 datapoints
2025-03-06 18:28:58,235 - INFO - training batch 151, loss: 0.487, 4832/60000 datapoints
2025-03-06 18:28:58,430 - INFO - training batch 201, loss: 0.319, 6432/60000 datapoints
2025-03-06 18:28:58,629 - INFO - training batch 251, loss: 0.230, 8032/60000 datapoints
2025-03-06 18:28:58,822 - INFO - training batch 301, loss: 0.205, 9632/60000 datapoints
2025-03-06 18:28:59,018 - INFO - training batch 351, loss: 0.560, 11232/60000 datapoints
2025-03-06 18:28:59,214 - INFO - training batch 401, loss: 0.120, 12832/60000 datapoints
2025-03-06 18:28:59,408 - INFO - training batch 451, loss: 0.297, 14432/60000 datapoints
2025-03-06 18:28:59,602 - INFO - training batch 501, loss: 0.178, 16032/60000 datapoints
2025-03-06 18:28:59,797 - INFO - training batch 551, loss: 0.614, 17632/60000 datapoints
2025-03-06 18:28:59,993 - INFO - training batch 601, loss: 0.190, 19232/60000 datapoints
2025-03-06 18:29:00,186 - INFO - training batch 651, loss: 0.425, 20832/60000 datapoints
2025-03-06 18:29:00,378 - INFO - training batch 701, loss: 0.188, 22432/60000 datapoints
2025-03-06 18:29:00,572 - INFO - training batch 751, loss: 0.291, 24032/60000 datapoints
2025-03-06 18:29:00,766 - INFO - training batch 801, loss: 0.265, 25632/60000 datapoints
2025-03-06 18:29:00,961 - INFO - training batch 851, loss: 0.142, 27232/60000 datapoints
2025-03-06 18:29:01,159 - INFO - training batch 901, loss: 0.172, 28832/60000 datapoints
2025-03-06 18:29:01,357 - INFO - training batch 951, loss: 0.222, 30432/60000 datapoints
2025-03-06 18:29:01,571 - INFO - training batch 1001, loss: 0.323, 32032/60000 datapoints
2025-03-06 18:29:01,773 - INFO - training batch 1051, loss: 0.189, 33632/60000 datapoints
2025-03-06 18:29:01,968 - INFO - training batch 1101, loss: 0.348, 35232/60000 datapoints
2025-03-06 18:29:02,162 - INFO - training batch 1151, loss: 0.116, 36832/60000 datapoints
2025-03-06 18:29:02,354 - INFO - training batch 1201, loss: 0.223, 38432/60000 datapoints
2025-03-06 18:29:02,552 - INFO - training batch 1251, loss: 0.516, 40032/60000 datapoints
2025-03-06 18:29:02,747 - INFO - training batch 1301, loss: 0.139, 41632/60000 datapoints
2025-03-06 18:29:02,942 - INFO - training batch 1351, loss: 0.502, 43232/60000 datapoints
2025-03-06 18:29:03,138 - INFO - training batch 1401, loss: 0.189, 44832/60000 datapoints
2025-03-06 18:29:03,332 - INFO - training batch 1451, loss: 0.209, 46432/60000 datapoints
2025-03-06 18:29:03,529 - INFO - training batch 1501, loss: 0.397, 48032/60000 datapoints
2025-03-06 18:29:03,726 - INFO - training batch 1551, loss: 0.371, 49632/60000 datapoints
2025-03-06 18:29:03,921 - INFO - training batch 1601, loss: 0.285, 51232/60000 datapoints
2025-03-06 18:29:04,112 - INFO - training batch 1651, loss: 0.206, 52832/60000 datapoints
2025-03-06 18:29:04,306 - INFO - training batch 1701, loss: 0.248, 54432/60000 datapoints
2025-03-06 18:29:04,499 - INFO - training batch 1751, loss: 0.248, 56032/60000 datapoints
2025-03-06 18:29:04,695 - INFO - training batch 1801, loss: 0.242, 57632/60000 datapoints
2025-03-06 18:29:04,893 - INFO - training batch 1851, loss: 0.183, 59232/60000 datapoints
2025-03-06 18:29:04,994 - INFO - validation batch 1, loss: 0.183, 32/10016 datapoints
2025-03-06 18:29:05,150 - INFO - validation batch 51, loss: 0.200, 1632/10016 datapoints
2025-03-06 18:29:05,302 - INFO - validation batch 101, loss: 0.281, 3232/10016 datapoints
2025-03-06 18:29:05,455 - INFO - validation batch 151, loss: 0.367, 4832/10016 datapoints
2025-03-06 18:29:05,609 - INFO - validation batch 201, loss: 0.342, 6432/10016 datapoints
2025-03-06 18:29:05,761 - INFO - validation batch 251, loss: 0.353, 8032/10016 datapoints
2025-03-06 18:29:05,915 - INFO - validation batch 301, loss: 0.188, 9632/10016 datapoints
2025-03-06 18:29:05,952 - INFO - Epoch 324/800 done.
2025-03-06 18:29:05,952 - INFO - Final validation performance:
Loss: 0.273, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:29:05,952 - INFO - Beginning epoch 325/800
2025-03-06 18:29:05,959 - INFO - training batch 1, loss: 0.278, 32/60000 datapoints
2025-03-06 18:29:06,158 - INFO - training batch 51, loss: 0.151, 1632/60000 datapoints
2025-03-06 18:29:06,354 - INFO - training batch 101, loss: 0.434, 3232/60000 datapoints
2025-03-06 18:29:06,549 - INFO - training batch 151, loss: 0.278, 4832/60000 datapoints
2025-03-06 18:29:06,747 - INFO - training batch 201, loss: 0.196, 6432/60000 datapoints
2025-03-06 18:29:06,943 - INFO - training batch 251, loss: 0.433, 8032/60000 datapoints
2025-03-06 18:29:07,141 - INFO - training batch 301, loss: 0.272, 9632/60000 datapoints
2025-03-06 18:29:07,334 - INFO - training batch 351, loss: 0.160, 11232/60000 datapoints
2025-03-06 18:29:07,528 - INFO - training batch 401, loss: 0.197, 12832/60000 datapoints
2025-03-06 18:29:07,725 - INFO - training batch 451, loss: 0.357, 14432/60000 datapoints
2025-03-06 18:29:07,919 - INFO - training batch 501, loss: 0.253, 16032/60000 datapoints
2025-03-06 18:29:08,117 - INFO - training batch 551, loss: 0.306, 17632/60000 datapoints
2025-03-06 18:29:08,309 - INFO - training batch 601, loss: 0.074, 19232/60000 datapoints
2025-03-06 18:29:08,502 - INFO - training batch 651, loss: 0.114, 20832/60000 datapoints
2025-03-06 18:29:08,703 - INFO - training batch 701, loss: 0.356, 22432/60000 datapoints
2025-03-06 18:29:08,900 - INFO - training batch 751, loss: 0.268, 24032/60000 datapoints
2025-03-06 18:29:09,095 - INFO - training batch 801, loss: 0.318, 25632/60000 datapoints
2025-03-06 18:29:09,291 - INFO - training batch 851, loss: 0.240, 27232/60000 datapoints
2025-03-06 18:29:09,487 - INFO - training batch 901, loss: 0.356, 28832/60000 datapoints
2025-03-06 18:29:09,683 - INFO - training batch 951, loss: 0.167, 30432/60000 datapoints
2025-03-06 18:29:09,875 - INFO - training batch 1001, loss: 0.096, 32032/60000 datapoints
2025-03-06 18:29:10,071 - INFO - training batch 1051, loss: 0.235, 33632/60000 datapoints
2025-03-06 18:29:10,265 - INFO - training batch 1101, loss: 0.169, 35232/60000 datapoints
2025-03-06 18:29:10,461 - INFO - training batch 1151, loss: 0.358, 36832/60000 datapoints
2025-03-06 18:29:10,669 - INFO - training batch 1201, loss: 0.442, 38432/60000 datapoints
2025-03-06 18:29:10,863 - INFO - training batch 1251, loss: 0.355, 40032/60000 datapoints
2025-03-06 18:29:11,059 - INFO - training batch 1301, loss: 0.505, 41632/60000 datapoints
2025-03-06 18:29:11,264 - INFO - training batch 1351, loss: 0.355, 43232/60000 datapoints
2025-03-06 18:29:11,457 - INFO - training batch 1401, loss: 0.347, 44832/60000 datapoints
2025-03-06 18:29:11,687 - INFO - training batch 1451, loss: 0.498, 46432/60000 datapoints
2025-03-06 18:29:11,904 - INFO - training batch 1501, loss: 0.173, 48032/60000 datapoints
2025-03-06 18:29:12,099 - INFO - training batch 1551, loss: 0.219, 49632/60000 datapoints
2025-03-06 18:29:12,291 - INFO - training batch 1601, loss: 0.385, 51232/60000 datapoints
2025-03-06 18:29:12,483 - INFO - training batch 1651, loss: 0.227, 52832/60000 datapoints
2025-03-06 18:29:12,677 - INFO - training batch 1701, loss: 0.383, 54432/60000 datapoints
2025-03-06 18:29:12,872 - INFO - training batch 1751, loss: 0.113, 56032/60000 datapoints
2025-03-06 18:29:13,068 - INFO - training batch 1801, loss: 0.281, 57632/60000 datapoints
2025-03-06 18:29:13,265 - INFO - training batch 1851, loss: 0.216, 59232/60000 datapoints
2025-03-06 18:29:13,366 - INFO - validation batch 1, loss: 0.329, 32/10016 datapoints
2025-03-06 18:29:13,520 - INFO - validation batch 51, loss: 0.199, 1632/10016 datapoints
2025-03-06 18:29:13,680 - INFO - validation batch 101, loss: 0.225, 3232/10016 datapoints
2025-03-06 18:29:13,833 - INFO - validation batch 151, loss: 0.443, 4832/10016 datapoints
2025-03-06 18:29:13,986 - INFO - validation batch 201, loss: 0.542, 6432/10016 datapoints
2025-03-06 18:29:14,137 - INFO - validation batch 251, loss: 0.307, 8032/10016 datapoints
2025-03-06 18:29:14,296 - INFO - validation batch 301, loss: 0.639, 9632/10016 datapoints
2025-03-06 18:29:14,334 - INFO - Epoch 325/800 done.
2025-03-06 18:29:14,334 - INFO - Final validation performance:
Loss: 0.383, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:29:14,335 - INFO - Beginning epoch 326/800
2025-03-06 18:29:14,340 - INFO - training batch 1, loss: 0.518, 32/60000 datapoints
2025-03-06 18:29:14,533 - INFO - training batch 51, loss: 0.188, 1632/60000 datapoints
2025-03-06 18:29:14,728 - INFO - training batch 101, loss: 0.189, 3232/60000 datapoints
2025-03-06 18:29:14,926 - INFO - training batch 151, loss: 0.176, 4832/60000 datapoints
2025-03-06 18:29:15,122 - INFO - training batch 201, loss: 0.187, 6432/60000 datapoints
2025-03-06 18:29:15,320 - INFO - training batch 251, loss: 0.194, 8032/60000 datapoints
2025-03-06 18:29:15,514 - INFO - training batch 301, loss: 0.278, 9632/60000 datapoints
2025-03-06 18:29:15,715 - INFO - training batch 351, loss: 0.078, 11232/60000 datapoints
2025-03-06 18:29:15,907 - INFO - training batch 401, loss: 0.305, 12832/60000 datapoints
2025-03-06 18:29:16,103 - INFO - training batch 451, loss: 0.293, 14432/60000 datapoints
2025-03-06 18:29:16,300 - INFO - training batch 501, loss: 0.306, 16032/60000 datapoints
2025-03-06 18:29:16,494 - INFO - training batch 551, loss: 0.375, 17632/60000 datapoints
2025-03-06 18:29:16,690 - INFO - training batch 601, loss: 0.533, 19232/60000 datapoints
2025-03-06 18:29:16,885 - INFO - training batch 651, loss: 0.182, 20832/60000 datapoints
2025-03-06 18:29:17,082 - INFO - training batch 701, loss: 0.515, 22432/60000 datapoints
2025-03-06 18:29:17,277 - INFO - training batch 751, loss: 0.332, 24032/60000 datapoints
2025-03-06 18:29:17,469 - INFO - training batch 801, loss: 0.422, 25632/60000 datapoints
2025-03-06 18:29:17,664 - INFO - training batch 851, loss: 0.231, 27232/60000 datapoints
2025-03-06 18:29:17,858 - INFO - training batch 901, loss: 0.491, 28832/60000 datapoints
2025-03-06 18:29:18,051 - INFO - training batch 951, loss: 0.373, 30432/60000 datapoints
2025-03-06 18:29:18,245 - INFO - training batch 1001, loss: 0.231, 32032/60000 datapoints
2025-03-06 18:29:18,437 - INFO - training batch 1051, loss: 0.166, 33632/60000 datapoints
2025-03-06 18:29:18,635 - INFO - training batch 1101, loss: 0.366, 35232/60000 datapoints
2025-03-06 18:29:18,830 - INFO - training batch 1151, loss: 0.246, 36832/60000 datapoints
2025-03-06 18:29:19,025 - INFO - training batch 1201, loss: 0.107, 38432/60000 datapoints
2025-03-06 18:29:19,227 - INFO - training batch 1251, loss: 0.175, 40032/60000 datapoints
2025-03-06 18:29:19,420 - INFO - training batch 1301, loss: 0.642, 41632/60000 datapoints
2025-03-06 18:29:19,616 - INFO - training batch 1351, loss: 0.369, 43232/60000 datapoints
2025-03-06 18:29:19,809 - INFO - training batch 1401, loss: 0.325, 44832/60000 datapoints
2025-03-06 18:29:20,004 - INFO - training batch 1451, loss: 0.502, 46432/60000 datapoints
2025-03-06 18:29:20,198 - INFO - training batch 1501, loss: 0.224, 48032/60000 datapoints
2025-03-06 18:29:20,391 - INFO - training batch 1551, loss: 0.455, 49632/60000 datapoints
2025-03-06 18:29:20,586 - INFO - training batch 1601, loss: 0.407, 51232/60000 datapoints
2025-03-06 18:29:20,780 - INFO - training batch 1651, loss: 0.579, 52832/60000 datapoints
2025-03-06 18:29:20,974 - INFO - training batch 1701, loss: 0.348, 54432/60000 datapoints
2025-03-06 18:29:21,183 - INFO - training batch 1751, loss: 0.147, 56032/60000 datapoints
2025-03-06 18:29:21,380 - INFO - training batch 1801, loss: 0.258, 57632/60000 datapoints
2025-03-06 18:29:21,574 - INFO - training batch 1851, loss: 0.103, 59232/60000 datapoints
2025-03-06 18:29:21,693 - INFO - validation batch 1, loss: 0.265, 32/10016 datapoints
2025-03-06 18:29:21,850 - INFO - validation batch 51, loss: 0.258, 1632/10016 datapoints
2025-03-06 18:29:22,002 - INFO - validation batch 101, loss: 0.105, 3232/10016 datapoints
2025-03-06 18:29:22,153 - INFO - validation batch 151, loss: 0.559, 4832/10016 datapoints
2025-03-06 18:29:22,306 - INFO - validation batch 201, loss: 0.217, 6432/10016 datapoints
2025-03-06 18:29:22,459 - INFO - validation batch 251, loss: 0.246, 8032/10016 datapoints
2025-03-06 18:29:22,615 - INFO - validation batch 301, loss: 0.147, 9632/10016 datapoints
2025-03-06 18:29:22,650 - INFO - Epoch 326/800 done.
2025-03-06 18:29:22,651 - INFO - Final validation performance:
Loss: 0.257, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:29:22,651 - INFO - Beginning epoch 327/800
2025-03-06 18:29:22,658 - INFO - training batch 1, loss: 0.079, 32/60000 datapoints
2025-03-06 18:29:22,858 - INFO - training batch 51, loss: 0.198, 1632/60000 datapoints
2025-03-06 18:29:23,053 - INFO - training batch 101, loss: 0.260, 3232/60000 datapoints
2025-03-06 18:29:23,250 - INFO - training batch 151, loss: 0.332, 4832/60000 datapoints
2025-03-06 18:29:23,444 - INFO - training batch 201, loss: 0.575, 6432/60000 datapoints
2025-03-06 18:29:23,640 - INFO - training batch 251, loss: 0.424, 8032/60000 datapoints
2025-03-06 18:29:23,838 - INFO - training batch 301, loss: 0.339, 9632/60000 datapoints
2025-03-06 18:29:24,033 - INFO - training batch 351, loss: 0.271, 11232/60000 datapoints
2025-03-06 18:29:24,229 - INFO - training batch 401, loss: 0.572, 12832/60000 datapoints
2025-03-06 18:29:24,423 - INFO - training batch 451, loss: 0.266, 14432/60000 datapoints
2025-03-06 18:29:24,618 - INFO - training batch 501, loss: 0.572, 16032/60000 datapoints
2025-03-06 18:29:24,811 - INFO - training batch 551, loss: 0.243, 17632/60000 datapoints
2025-03-06 18:29:25,011 - INFO - training batch 601, loss: 0.211, 19232/60000 datapoints
2025-03-06 18:29:25,209 - INFO - training batch 651, loss: 0.295, 20832/60000 datapoints
2025-03-06 18:29:25,400 - INFO - training batch 701, loss: 0.350, 22432/60000 datapoints
2025-03-06 18:29:25,595 - INFO - training batch 751, loss: 0.215, 24032/60000 datapoints
2025-03-06 18:29:25,790 - INFO - training batch 801, loss: 0.289, 25632/60000 datapoints
2025-03-06 18:29:25,984 - INFO - training batch 851, loss: 0.294, 27232/60000 datapoints
2025-03-06 18:29:26,182 - INFO - training batch 901, loss: 0.194, 28832/60000 datapoints
2025-03-06 18:29:26,376 - INFO - training batch 951, loss: 0.649, 30432/60000 datapoints
2025-03-06 18:29:26,568 - INFO - training batch 1001, loss: 0.115, 32032/60000 datapoints
2025-03-06 18:29:26,763 - INFO - training batch 1051, loss: 0.348, 33632/60000 datapoints
2025-03-06 18:29:26,959 - INFO - training batch 1101, loss: 0.135, 35232/60000 datapoints
2025-03-06 18:29:27,153 - INFO - training batch 1151, loss: 0.196, 36832/60000 datapoints
2025-03-06 18:29:27,348 - INFO - training batch 1201, loss: 0.599, 38432/60000 datapoints
2025-03-06 18:29:27,541 - INFO - training batch 1251, loss: 0.359, 40032/60000 datapoints
2025-03-06 18:29:27,738 - INFO - training batch 1301, loss: 0.121, 41632/60000 datapoints
2025-03-06 18:29:27,932 - INFO - training batch 1351, loss: 0.400, 43232/60000 datapoints
2025-03-06 18:29:28,128 - INFO - training batch 1401, loss: 0.314, 44832/60000 datapoints
2025-03-06 18:29:28,324 - INFO - training batch 1451, loss: 0.150, 46432/60000 datapoints
2025-03-06 18:29:28,518 - INFO - training batch 1501, loss: 0.355, 48032/60000 datapoints
2025-03-06 18:29:28,716 - INFO - training batch 1551, loss: 0.244, 49632/60000 datapoints
2025-03-06 18:29:28,910 - INFO - training batch 1601, loss: 0.342, 51232/60000 datapoints
2025-03-06 18:29:29,106 - INFO - training batch 1651, loss: 0.226, 52832/60000 datapoints
2025-03-06 18:29:29,302 - INFO - training batch 1701, loss: 0.422, 54432/60000 datapoints
2025-03-06 18:29:29,496 - INFO - training batch 1751, loss: 0.071, 56032/60000 datapoints
2025-03-06 18:29:29,693 - INFO - training batch 1801, loss: 0.684, 57632/60000 datapoints
2025-03-06 18:29:29,887 - INFO - training batch 1851, loss: 0.323, 59232/60000 datapoints
2025-03-06 18:29:29,989 - INFO - validation batch 1, loss: 0.436, 32/10016 datapoints
2025-03-06 18:29:30,143 - INFO - validation batch 51, loss: 0.410, 1632/10016 datapoints
2025-03-06 18:29:30,297 - INFO - validation batch 101, loss: 0.233, 3232/10016 datapoints
2025-03-06 18:29:30,450 - INFO - validation batch 151, loss: 0.244, 4832/10016 datapoints
2025-03-06 18:29:30,605 - INFO - validation batch 201, loss: 0.133, 6432/10016 datapoints
2025-03-06 18:29:30,759 - INFO - validation batch 251, loss: 0.545, 8032/10016 datapoints
2025-03-06 18:29:30,911 - INFO - validation batch 301, loss: 0.139, 9632/10016 datapoints
2025-03-06 18:29:30,949 - INFO - Epoch 327/800 done.
2025-03-06 18:29:30,949 - INFO - Final validation performance:
Loss: 0.306, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:29:30,950 - INFO - Beginning epoch 328/800
2025-03-06 18:29:30,955 - INFO - training batch 1, loss: 0.159, 32/60000 datapoints
2025-03-06 18:29:31,157 - INFO - training batch 51, loss: 0.605, 1632/60000 datapoints
2025-03-06 18:29:31,354 - INFO - training batch 101, loss: 0.282, 3232/60000 datapoints
2025-03-06 18:29:31,546 - INFO - training batch 151, loss: 0.132, 4832/60000 datapoints
2025-03-06 18:29:31,759 - INFO - training batch 201, loss: 0.245, 6432/60000 datapoints
2025-03-06 18:29:31,964 - INFO - training batch 251, loss: 0.094, 8032/60000 datapoints
2025-03-06 18:29:32,160 - INFO - training batch 301, loss: 0.254, 9632/60000 datapoints
2025-03-06 18:29:32,351 - INFO - training batch 351, loss: 0.311, 11232/60000 datapoints
2025-03-06 18:29:32,544 - INFO - training batch 401, loss: 0.202, 12832/60000 datapoints
2025-03-06 18:29:32,742 - INFO - training batch 451, loss: 0.461, 14432/60000 datapoints
2025-03-06 18:29:32,944 - INFO - training batch 501, loss: 0.327, 16032/60000 datapoints
2025-03-06 18:29:33,139 - INFO - training batch 551, loss: 0.184, 17632/60000 datapoints
2025-03-06 18:29:33,333 - INFO - training batch 601, loss: 0.259, 19232/60000 datapoints
2025-03-06 18:29:33,525 - INFO - training batch 651, loss: 0.115, 20832/60000 datapoints
2025-03-06 18:29:33,722 - INFO - training batch 701, loss: 0.887, 22432/60000 datapoints
2025-03-06 18:29:33,918 - INFO - training batch 751, loss: 0.103, 24032/60000 datapoints
2025-03-06 18:29:34,114 - INFO - training batch 801, loss: 0.309, 25632/60000 datapoints
2025-03-06 18:29:34,309 - INFO - training batch 851, loss: 0.263, 27232/60000 datapoints
2025-03-06 18:29:34,503 - INFO - training batch 901, loss: 0.188, 28832/60000 datapoints
2025-03-06 18:29:34,701 - INFO - training batch 951, loss: 0.106, 30432/60000 datapoints
2025-03-06 18:29:34,899 - INFO - training batch 1001, loss: 0.253, 32032/60000 datapoints
2025-03-06 18:29:35,095 - INFO - training batch 1051, loss: 0.177, 33632/60000 datapoints
2025-03-06 18:29:35,294 - INFO - training batch 1101, loss: 0.135, 35232/60000 datapoints
2025-03-06 18:29:35,488 - INFO - training batch 1151, loss: 0.200, 36832/60000 datapoints
2025-03-06 18:29:35,686 - INFO - training batch 1201, loss: 0.258, 38432/60000 datapoints
2025-03-06 18:29:35,882 - INFO - training batch 1251, loss: 0.352, 40032/60000 datapoints
2025-03-06 18:29:36,079 - INFO - training batch 1301, loss: 0.426, 41632/60000 datapoints
2025-03-06 18:29:36,277 - INFO - training batch 1351, loss: 0.146, 43232/60000 datapoints
2025-03-06 18:29:36,473 - INFO - training batch 1401, loss: 0.132, 44832/60000 datapoints
2025-03-06 18:29:36,671 - INFO - training batch 1451, loss: 0.349, 46432/60000 datapoints
2025-03-06 18:29:36,865 - INFO - training batch 1501, loss: 0.373, 48032/60000 datapoints
2025-03-06 18:29:37,059 - INFO - training batch 1551, loss: 0.475, 49632/60000 datapoints
2025-03-06 18:29:37,254 - INFO - training batch 1601, loss: 0.384, 51232/60000 datapoints
2025-03-06 18:29:37,448 - INFO - training batch 1651, loss: 0.272, 52832/60000 datapoints
2025-03-06 18:29:37,657 - INFO - training batch 1701, loss: 0.144, 54432/60000 datapoints
2025-03-06 18:29:37,850 - INFO - training batch 1751, loss: 0.205, 56032/60000 datapoints
2025-03-06 18:29:38,045 - INFO - training batch 1801, loss: 0.253, 57632/60000 datapoints
2025-03-06 18:29:38,238 - INFO - training batch 1851, loss: 0.332, 59232/60000 datapoints
2025-03-06 18:29:38,338 - INFO - validation batch 1, loss: 0.207, 32/10016 datapoints
2025-03-06 18:29:38,491 - INFO - validation batch 51, loss: 0.173, 1632/10016 datapoints
2025-03-06 18:29:38,645 - INFO - validation batch 101, loss: 0.230, 3232/10016 datapoints
2025-03-06 18:29:38,802 - INFO - validation batch 151, loss: 0.421, 4832/10016 datapoints
2025-03-06 18:29:38,954 - INFO - validation batch 201, loss: 0.396, 6432/10016 datapoints
2025-03-06 18:29:39,120 - INFO - validation batch 251, loss: 0.275, 8032/10016 datapoints
2025-03-06 18:29:39,277 - INFO - validation batch 301, loss: 0.154, 9632/10016 datapoints
2025-03-06 18:29:39,314 - INFO - Epoch 328/800 done.
2025-03-06 18:29:39,314 - INFO - Final validation performance:
Loss: 0.265, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:29:39,314 - INFO - Beginning epoch 329/800
2025-03-06 18:29:39,321 - INFO - training batch 1, loss: 0.428, 32/60000 datapoints
2025-03-06 18:29:39,511 - INFO - training batch 51, loss: 0.305, 1632/60000 datapoints
2025-03-06 18:29:39,705 - INFO - training batch 101, loss: 0.242, 3232/60000 datapoints
2025-03-06 18:29:39,896 - INFO - training batch 151, loss: 0.384, 4832/60000 datapoints
2025-03-06 18:29:40,089 - INFO - training batch 201, loss: 0.272, 6432/60000 datapoints
2025-03-06 18:29:40,284 - INFO - training batch 251, loss: 0.347, 8032/60000 datapoints
2025-03-06 18:29:40,479 - INFO - training batch 301, loss: 0.249, 9632/60000 datapoints
2025-03-06 18:29:40,671 - INFO - training batch 351, loss: 0.275, 11232/60000 datapoints
2025-03-06 18:29:40,862 - INFO - training batch 401, loss: 0.188, 12832/60000 datapoints
2025-03-06 18:29:41,054 - INFO - training batch 451, loss: 0.294, 14432/60000 datapoints
2025-03-06 18:29:41,249 - INFO - training batch 501, loss: 0.317, 16032/60000 datapoints
2025-03-06 18:29:41,441 - INFO - training batch 551, loss: 0.125, 17632/60000 datapoints
2025-03-06 18:29:41,631 - INFO - training batch 601, loss: 0.114, 19232/60000 datapoints
2025-03-06 18:29:41,834 - INFO - training batch 651, loss: 0.461, 20832/60000 datapoints
2025-03-06 18:29:42,036 - INFO - training batch 701, loss: 0.350, 22432/60000 datapoints
2025-03-06 18:29:42,237 - INFO - training batch 751, loss: 0.630, 24032/60000 datapoints
2025-03-06 18:29:42,430 - INFO - training batch 801, loss: 0.159, 25632/60000 datapoints
2025-03-06 18:29:42,621 - INFO - training batch 851, loss: 0.716, 27232/60000 datapoints
2025-03-06 18:29:42,814 - INFO - training batch 901, loss: 0.184, 28832/60000 datapoints
2025-03-06 18:29:43,005 - INFO - training batch 951, loss: 0.322, 30432/60000 datapoints
2025-03-06 18:29:43,197 - INFO - training batch 1001, loss: 0.145, 32032/60000 datapoints
2025-03-06 18:29:43,420 - INFO - training batch 1051, loss: 0.056, 33632/60000 datapoints
2025-03-06 18:29:43,616 - INFO - training batch 1101, loss: 0.465, 35232/60000 datapoints
2025-03-06 18:29:43,806 - INFO - training batch 1151, loss: 0.169, 36832/60000 datapoints
2025-03-06 18:29:43,996 - INFO - training batch 1201, loss: 0.144, 38432/60000 datapoints
2025-03-06 18:29:44,187 - INFO - training batch 1251, loss: 0.709, 40032/60000 datapoints
2025-03-06 18:29:44,380 - INFO - training batch 1301, loss: 0.325, 41632/60000 datapoints
2025-03-06 18:29:44,571 - INFO - training batch 1351, loss: 0.855, 43232/60000 datapoints
2025-03-06 18:29:44,762 - INFO - training batch 1401, loss: 0.212, 44832/60000 datapoints
2025-03-06 18:29:44,958 - INFO - training batch 1451, loss: 0.185, 46432/60000 datapoints
2025-03-06 18:29:45,152 - INFO - training batch 1501, loss: 0.284, 48032/60000 datapoints
2025-03-06 18:29:45,347 - INFO - training batch 1551, loss: 0.165, 49632/60000 datapoints
2025-03-06 18:29:45,537 - INFO - training batch 1601, loss: 0.500, 51232/60000 datapoints
2025-03-06 18:29:45,729 - INFO - training batch 1651, loss: 0.163, 52832/60000 datapoints
2025-03-06 18:29:45,921 - INFO - training batch 1701, loss: 0.459, 54432/60000 datapoints
2025-03-06 18:29:46,114 - INFO - training batch 1751, loss: 0.206, 56032/60000 datapoints
2025-03-06 18:29:46,313 - INFO - training batch 1801, loss: 0.148, 57632/60000 datapoints
2025-03-06 18:29:46,505 - INFO - training batch 1851, loss: 0.314, 59232/60000 datapoints
2025-03-06 18:29:46,605 - INFO - validation batch 1, loss: 0.141, 32/10016 datapoints
2025-03-06 18:29:46,754 - INFO - validation batch 51, loss: 0.117, 1632/10016 datapoints
2025-03-06 18:29:46,905 - INFO - validation batch 101, loss: 0.110, 3232/10016 datapoints
2025-03-06 18:29:47,055 - INFO - validation batch 151, loss: 0.345, 4832/10016 datapoints
2025-03-06 18:29:47,205 - INFO - validation batch 201, loss: 0.234, 6432/10016 datapoints
2025-03-06 18:29:47,358 - INFO - validation batch 251, loss: 0.052, 8032/10016 datapoints
2025-03-06 18:29:47,508 - INFO - validation batch 301, loss: 0.190, 9632/10016 datapoints
2025-03-06 18:29:47,543 - INFO - Epoch 329/800 done.
2025-03-06 18:29:47,543 - INFO - Final validation performance:
Loss: 0.170, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:29:47,544 - INFO - Beginning epoch 330/800
2025-03-06 18:29:47,549 - INFO - training batch 1, loss: 0.319, 32/60000 datapoints
2025-03-06 18:29:47,743 - INFO - training batch 51, loss: 0.410, 1632/60000 datapoints
2025-03-06 18:29:47,934 - INFO - training batch 101, loss: 0.275, 3232/60000 datapoints
2025-03-06 18:29:48,124 - INFO - training batch 151, loss: 0.285, 4832/60000 datapoints
2025-03-06 18:29:48,317 - INFO - training batch 201, loss: 0.398, 6432/60000 datapoints
2025-03-06 18:29:48,507 - INFO - training batch 251, loss: 0.311, 8032/60000 datapoints
2025-03-06 18:29:48,701 - INFO - training batch 301, loss: 0.236, 9632/60000 datapoints
2025-03-06 18:29:48,891 - INFO - training batch 351, loss: 0.268, 11232/60000 datapoints
2025-03-06 18:29:49,082 - INFO - training batch 401, loss: 0.145, 12832/60000 datapoints
2025-03-06 18:29:49,277 - INFO - training batch 451, loss: 0.121, 14432/60000 datapoints
2025-03-06 18:29:49,469 - INFO - training batch 501, loss: 0.400, 16032/60000 datapoints
2025-03-06 18:29:49,662 - INFO - training batch 551, loss: 0.451, 17632/60000 datapoints
2025-03-06 18:29:49,853 - INFO - training batch 601, loss: 0.393, 19232/60000 datapoints
2025-03-06 18:29:50,046 - INFO - training batch 651, loss: 0.114, 20832/60000 datapoints
2025-03-06 18:29:50,243 - INFO - training batch 701, loss: 0.277, 22432/60000 datapoints
2025-03-06 18:29:50,435 - INFO - training batch 751, loss: 0.471, 24032/60000 datapoints
2025-03-06 18:29:50,627 - INFO - training batch 801, loss: 0.108, 25632/60000 datapoints
2025-03-06 18:29:50,818 - INFO - training batch 851, loss: 0.296, 27232/60000 datapoints
2025-03-06 18:29:51,009 - INFO - training batch 901, loss: 0.931, 28832/60000 datapoints
2025-03-06 18:29:51,200 - INFO - training batch 951, loss: 0.215, 30432/60000 datapoints
2025-03-06 18:29:51,396 - INFO - training batch 1001, loss: 0.107, 32032/60000 datapoints
2025-03-06 18:29:51,586 - INFO - training batch 1051, loss: 0.553, 33632/60000 datapoints
2025-03-06 18:29:51,784 - INFO - training batch 1101, loss: 0.318, 35232/60000 datapoints
2025-03-06 18:29:51,995 - INFO - training batch 1151, loss: 0.362, 36832/60000 datapoints
2025-03-06 18:29:52,185 - INFO - training batch 1201, loss: 0.263, 38432/60000 datapoints
2025-03-06 18:29:52,380 - INFO - training batch 1251, loss: 0.311, 40032/60000 datapoints
2025-03-06 18:29:52,572 - INFO - training batch 1301, loss: 0.302, 41632/60000 datapoints
2025-03-06 18:29:52,766 - INFO - training batch 1351, loss: 0.144, 43232/60000 datapoints
2025-03-06 18:29:52,964 - INFO - training batch 1401, loss: 0.173, 44832/60000 datapoints
2025-03-06 18:29:53,156 - INFO - training batch 1451, loss: 0.349, 46432/60000 datapoints
2025-03-06 18:29:53,349 - INFO - training batch 1501, loss: 0.186, 48032/60000 datapoints
2025-03-06 18:29:53,540 - INFO - training batch 1551, loss: 0.413, 49632/60000 datapoints
2025-03-06 18:29:53,741 - INFO - training batch 1601, loss: 0.226, 51232/60000 datapoints
2025-03-06 18:29:53,938 - INFO - training batch 1651, loss: 0.522, 52832/60000 datapoints
2025-03-06 18:29:54,133 - INFO - training batch 1701, loss: 0.134, 54432/60000 datapoints
2025-03-06 18:29:54,334 - INFO - training batch 1751, loss: 0.331, 56032/60000 datapoints
2025-03-06 18:29:54,529 - INFO - training batch 1801, loss: 0.467, 57632/60000 datapoints
2025-03-06 18:29:54,724 - INFO - training batch 1851, loss: 0.324, 59232/60000 datapoints
2025-03-06 18:29:54,827 - INFO - validation batch 1, loss: 0.559, 32/10016 datapoints
2025-03-06 18:29:54,985 - INFO - validation batch 51, loss: 0.406, 1632/10016 datapoints
2025-03-06 18:29:55,138 - INFO - validation batch 101, loss: 0.202, 3232/10016 datapoints
2025-03-06 18:29:55,292 - INFO - validation batch 151, loss: 0.222, 4832/10016 datapoints
2025-03-06 18:29:55,445 - INFO - validation batch 201, loss: 0.830, 6432/10016 datapoints
2025-03-06 18:29:55,596 - INFO - validation batch 251, loss: 0.328, 8032/10016 datapoints
2025-03-06 18:29:55,752 - INFO - validation batch 301, loss: 0.231, 9632/10016 datapoints
2025-03-06 18:29:55,789 - INFO - Epoch 330/800 done.
2025-03-06 18:29:55,789 - INFO - Final validation performance:
Loss: 0.397, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:29:55,790 - INFO - Beginning epoch 331/800
2025-03-06 18:29:55,795 - INFO - training batch 1, loss: 0.307, 32/60000 datapoints
2025-03-06 18:29:55,991 - INFO - training batch 51, loss: 0.311, 1632/60000 datapoints
2025-03-06 18:29:56,188 - INFO - training batch 101, loss: 0.161, 3232/60000 datapoints
2025-03-06 18:29:56,382 - INFO - training batch 151, loss: 0.557, 4832/60000 datapoints
2025-03-06 18:29:56,576 - INFO - training batch 201, loss: 0.266, 6432/60000 datapoints
2025-03-06 18:29:56,770 - INFO - training batch 251, loss: 0.566, 8032/60000 datapoints
2025-03-06 18:29:56,965 - INFO - training batch 301, loss: 0.690, 9632/60000 datapoints
2025-03-06 18:29:57,161 - INFO - training batch 351, loss: 0.623, 11232/60000 datapoints
2025-03-06 18:29:57,359 - INFO - training batch 401, loss: 0.295, 12832/60000 datapoints
2025-03-06 18:29:57,552 - INFO - training batch 451, loss: 0.209, 14432/60000 datapoints
2025-03-06 18:29:57,747 - INFO - training batch 501, loss: 0.357, 16032/60000 datapoints
2025-03-06 18:29:57,943 - INFO - training batch 551, loss: 0.314, 17632/60000 datapoints
2025-03-06 18:29:58,138 - INFO - training batch 601, loss: 0.391, 19232/60000 datapoints
2025-03-06 18:29:58,331 - INFO - training batch 651, loss: 0.230, 20832/60000 datapoints
2025-03-06 18:29:58,524 - INFO - training batch 701, loss: 0.279, 22432/60000 datapoints
2025-03-06 18:29:58,722 - INFO - training batch 751, loss: 0.423, 24032/60000 datapoints
2025-03-06 18:29:58,917 - INFO - training batch 801, loss: 0.297, 25632/60000 datapoints
2025-03-06 18:29:59,119 - INFO - training batch 851, loss: 0.654, 27232/60000 datapoints
2025-03-06 18:29:59,336 - INFO - training batch 901, loss: 0.316, 28832/60000 datapoints
2025-03-06 18:29:59,556 - INFO - training batch 951, loss: 0.248, 30432/60000 datapoints
2025-03-06 18:29:59,778 - INFO - training batch 1001, loss: 0.423, 32032/60000 datapoints
2025-03-06 18:30:00,001 - INFO - training batch 1051, loss: 0.154, 33632/60000 datapoints
2025-03-06 18:30:00,195 - INFO - training batch 1101, loss: 0.371, 35232/60000 datapoints
2025-03-06 18:30:00,392 - INFO - training batch 1151, loss: 0.212, 36832/60000 datapoints
2025-03-06 18:30:00,587 - INFO - training batch 1201, loss: 0.279, 38432/60000 datapoints
2025-03-06 18:30:00,783 - INFO - training batch 1251, loss: 0.763, 40032/60000 datapoints
2025-03-06 18:30:00,977 - INFO - training batch 1301, loss: 0.338, 41632/60000 datapoints
2025-03-06 18:30:01,173 - INFO - training batch 1351, loss: 0.399, 43232/60000 datapoints
2025-03-06 18:30:01,374 - INFO - training batch 1401, loss: 0.595, 44832/60000 datapoints
2025-03-06 18:30:01,568 - INFO - training batch 1451, loss: 0.444, 46432/60000 datapoints
2025-03-06 18:30:01,770 - INFO - training batch 1501, loss: 0.367, 48032/60000 datapoints
2025-03-06 18:30:01,967 - INFO - training batch 1551, loss: 0.245, 49632/60000 datapoints
2025-03-06 18:30:02,180 - INFO - training batch 1601, loss: 0.090, 51232/60000 datapoints
2025-03-06 18:30:02,374 - INFO - training batch 1651, loss: 0.429, 52832/60000 datapoints
2025-03-06 18:30:02,567 - INFO - training batch 1701, loss: 0.323, 54432/60000 datapoints
2025-03-06 18:30:02,762 - INFO - training batch 1751, loss: 0.348, 56032/60000 datapoints
2025-03-06 18:30:02,956 - INFO - training batch 1801, loss: 0.353, 57632/60000 datapoints
2025-03-06 18:30:03,151 - INFO - training batch 1851, loss: 0.154, 59232/60000 datapoints
2025-03-06 18:30:03,253 - INFO - validation batch 1, loss: 0.305, 32/10016 datapoints
2025-03-06 18:30:03,408 - INFO - validation batch 51, loss: 0.160, 1632/10016 datapoints
2025-03-06 18:30:03,562 - INFO - validation batch 101, loss: 0.276, 3232/10016 datapoints
2025-03-06 18:30:03,738 - INFO - validation batch 151, loss: 0.260, 4832/10016 datapoints
2025-03-06 18:30:03,892 - INFO - validation batch 201, loss: 0.126, 6432/10016 datapoints
2025-03-06 18:30:04,045 - INFO - validation batch 251, loss: 0.348, 8032/10016 datapoints
2025-03-06 18:30:04,198 - INFO - validation batch 301, loss: 0.151, 9632/10016 datapoints
2025-03-06 18:30:04,235 - INFO - Epoch 331/800 done.
2025-03-06 18:30:04,236 - INFO - Final validation performance:
Loss: 0.232, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 18:30:04,236 - INFO - Beginning epoch 332/800
2025-03-06 18:30:04,243 - INFO - training batch 1, loss: 0.121, 32/60000 datapoints
2025-03-06 18:30:04,437 - INFO - training batch 51, loss: 0.132, 1632/60000 datapoints
2025-03-06 18:30:04,637 - INFO - training batch 101, loss: 0.151, 3232/60000 datapoints
2025-03-06 18:30:04,833 - INFO - training batch 151, loss: 0.353, 4832/60000 datapoints
2025-03-06 18:30:05,034 - INFO - training batch 201, loss: 0.253, 6432/60000 datapoints
2025-03-06 18:30:05,232 - INFO - training batch 251, loss: 0.313, 8032/60000 datapoints
2025-03-06 18:30:05,432 - INFO - training batch 301, loss: 0.173, 9632/60000 datapoints
2025-03-06 18:30:05,628 - INFO - training batch 351, loss: 0.589, 11232/60000 datapoints
2025-03-06 18:30:05,823 - INFO - training batch 401, loss: 0.285, 12832/60000 datapoints
2025-03-06 18:30:06,015 - INFO - training batch 451, loss: 0.304, 14432/60000 datapoints
2025-03-06 18:30:06,213 - INFO - training batch 501, loss: 0.444, 16032/60000 datapoints
2025-03-06 18:30:06,409 - INFO - training batch 551, loss: 0.202, 17632/60000 datapoints
2025-03-06 18:30:06,604 - INFO - training batch 601, loss: 0.219, 19232/60000 datapoints
2025-03-06 18:30:06,798 - INFO - training batch 651, loss: 0.170, 20832/60000 datapoints
2025-03-06 18:30:06,991 - INFO - training batch 701, loss: 0.529, 22432/60000 datapoints
2025-03-06 18:30:07,184 - INFO - training batch 751, loss: 0.111, 24032/60000 datapoints
2025-03-06 18:30:07,382 - INFO - training batch 801, loss: 0.373, 25632/60000 datapoints
2025-03-06 18:30:07,576 - INFO - training batch 851, loss: 0.119, 27232/60000 datapoints
2025-03-06 18:30:07,774 - INFO - training batch 901, loss: 0.605, 28832/60000 datapoints
2025-03-06 18:30:07,968 - INFO - training batch 951, loss: 0.258, 30432/60000 datapoints
2025-03-06 18:30:08,170 - INFO - training batch 1001, loss: 0.497, 32032/60000 datapoints
2025-03-06 18:30:08,365 - INFO - training batch 1051, loss: 0.225, 33632/60000 datapoints
2025-03-06 18:30:08,557 - INFO - training batch 1101, loss: 0.757, 35232/60000 datapoints
2025-03-06 18:30:08,753 - INFO - training batch 1151, loss: 0.228, 36832/60000 datapoints
2025-03-06 18:30:08,948 - INFO - training batch 1201, loss: 0.259, 38432/60000 datapoints
2025-03-06 18:30:09,145 - INFO - training batch 1251, loss: 0.252, 40032/60000 datapoints
2025-03-06 18:30:09,340 - INFO - training batch 1301, loss: 0.130, 41632/60000 datapoints
2025-03-06 18:30:09,535 - INFO - training batch 1351, loss: 0.309, 43232/60000 datapoints
2025-03-06 18:30:09,733 - INFO - training batch 1401, loss: 0.197, 44832/60000 datapoints
2025-03-06 18:30:09,928 - INFO - training batch 1451, loss: 0.535, 46432/60000 datapoints
2025-03-06 18:30:10,123 - INFO - training batch 1501, loss: 0.099, 48032/60000 datapoints
2025-03-06 18:30:10,317 - INFO - training batch 1551, loss: 0.165, 49632/60000 datapoints
2025-03-06 18:30:10,510 - INFO - training batch 1601, loss: 0.448, 51232/60000 datapoints
2025-03-06 18:30:10,708 - INFO - training batch 1651, loss: 0.262, 52832/60000 datapoints
2025-03-06 18:30:10,901 - INFO - training batch 1701, loss: 0.382, 54432/60000 datapoints
2025-03-06 18:30:11,094 - INFO - training batch 1751, loss: 0.232, 56032/60000 datapoints
2025-03-06 18:30:11,288 - INFO - training batch 1801, loss: 0.112, 57632/60000 datapoints
2025-03-06 18:30:11,484 - INFO - training batch 1851, loss: 0.159, 59232/60000 datapoints
2025-03-06 18:30:11,584 - INFO - validation batch 1, loss: 0.549, 32/10016 datapoints
2025-03-06 18:30:11,736 - INFO - validation batch 51, loss: 0.318, 1632/10016 datapoints
2025-03-06 18:30:11,894 - INFO - validation batch 101, loss: 0.146, 3232/10016 datapoints
2025-03-06 18:30:12,049 - INFO - validation batch 151, loss: 0.378, 4832/10016 datapoints
2025-03-06 18:30:12,219 - INFO - validation batch 201, loss: 0.204, 6432/10016 datapoints
2025-03-06 18:30:12,376 - INFO - validation batch 251, loss: 0.122, 8032/10016 datapoints
2025-03-06 18:30:12,528 - INFO - validation batch 301, loss: 0.332, 9632/10016 datapoints
2025-03-06 18:30:12,565 - INFO - Epoch 332/800 done.
2025-03-06 18:30:12,565 - INFO - Final validation performance:
Loss: 0.293, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:30:12,565 - INFO - Beginning epoch 333/800
2025-03-06 18:30:12,572 - INFO - training batch 1, loss: 0.350, 32/60000 datapoints
2025-03-06 18:30:12,771 - INFO - training batch 51, loss: 0.188, 1632/60000 datapoints
2025-03-06 18:30:12,978 - INFO - training batch 101, loss: 0.477, 3232/60000 datapoints
2025-03-06 18:30:13,170 - INFO - training batch 151, loss: 0.249, 4832/60000 datapoints
2025-03-06 18:30:13,367 - INFO - training batch 201, loss: 0.243, 6432/60000 datapoints
2025-03-06 18:30:13,560 - INFO - training batch 251, loss: 0.247, 8032/60000 datapoints
2025-03-06 18:30:13,757 - INFO - training batch 301, loss: 0.303, 9632/60000 datapoints
2025-03-06 18:30:13,952 - INFO - training batch 351, loss: 0.252, 11232/60000 datapoints
2025-03-06 18:30:14,149 - INFO - training batch 401, loss: 0.638, 12832/60000 datapoints
2025-03-06 18:30:14,342 - INFO - training batch 451, loss: 0.214, 14432/60000 datapoints
2025-03-06 18:30:14,535 - INFO - training batch 501, loss: 0.290, 16032/60000 datapoints
2025-03-06 18:30:14,733 - INFO - training batch 551, loss: 0.390, 17632/60000 datapoints
2025-03-06 18:30:14,932 - INFO - training batch 601, loss: 0.491, 19232/60000 datapoints
2025-03-06 18:30:15,126 - INFO - training batch 651, loss: 0.365, 20832/60000 datapoints
2025-03-06 18:30:15,322 - INFO - training batch 701, loss: 0.237, 22432/60000 datapoints
2025-03-06 18:30:15,515 - INFO - training batch 751, loss: 0.440, 24032/60000 datapoints
2025-03-06 18:30:15,711 - INFO - training batch 801, loss: 0.180, 25632/60000 datapoints
2025-03-06 18:30:15,907 - INFO - training batch 851, loss: 0.347, 27232/60000 datapoints
2025-03-06 18:30:16,102 - INFO - training batch 901, loss: 0.279, 28832/60000 datapoints
2025-03-06 18:30:16,298 - INFO - training batch 951, loss: 0.400, 30432/60000 datapoints
2025-03-06 18:30:16,493 - INFO - training batch 1001, loss: 0.405, 32032/60000 datapoints
2025-03-06 18:30:16,690 - INFO - training batch 1051, loss: 0.192, 33632/60000 datapoints
2025-03-06 18:30:16,889 - INFO - training batch 1101, loss: 0.319, 35232/60000 datapoints
2025-03-06 18:30:17,085 - INFO - training batch 1151, loss: 0.209, 36832/60000 datapoints
2025-03-06 18:30:17,279 - INFO - training batch 1201, loss: 0.461, 38432/60000 datapoints
2025-03-06 18:30:17,474 - INFO - training batch 1251, loss: 0.296, 40032/60000 datapoints
2025-03-06 18:30:17,669 - INFO - training batch 1301, loss: 0.647, 41632/60000 datapoints
2025-03-06 18:30:17,864 - INFO - training batch 1351, loss: 0.549, 43232/60000 datapoints
2025-03-06 18:30:18,060 - INFO - training batch 1401, loss: 0.293, 44832/60000 datapoints
2025-03-06 18:30:18,254 - INFO - training batch 1451, loss: 0.267, 46432/60000 datapoints
2025-03-06 18:30:18,447 - INFO - training batch 1501, loss: 0.368, 48032/60000 datapoints
2025-03-06 18:30:18,644 - INFO - training batch 1551, loss: 0.254, 49632/60000 datapoints
2025-03-06 18:30:18,839 - INFO - training batch 1601, loss: 0.493, 51232/60000 datapoints
2025-03-06 18:30:19,033 - INFO - training batch 1651, loss: 0.240, 52832/60000 datapoints
2025-03-06 18:30:19,228 - INFO - training batch 1701, loss: 0.193, 54432/60000 datapoints
2025-03-06 18:30:19,424 - INFO - training batch 1751, loss: 0.645, 56032/60000 datapoints
2025-03-06 18:30:19,621 - INFO - training batch 1801, loss: 0.447, 57632/60000 datapoints
2025-03-06 18:30:19,816 - INFO - training batch 1851, loss: 0.279, 59232/60000 datapoints
2025-03-06 18:30:19,917 - INFO - validation batch 1, loss: 0.416, 32/10016 datapoints
2025-03-06 18:30:20,069 - INFO - validation batch 51, loss: 0.235, 1632/10016 datapoints
2025-03-06 18:30:20,222 - INFO - validation batch 101, loss: 0.234, 3232/10016 datapoints
2025-03-06 18:30:20,373 - INFO - validation batch 151, loss: 0.246, 4832/10016 datapoints
2025-03-06 18:30:20,525 - INFO - validation batch 201, loss: 0.299, 6432/10016 datapoints
2025-03-06 18:30:20,678 - INFO - validation batch 251, loss: 0.390, 8032/10016 datapoints
2025-03-06 18:30:20,831 - INFO - validation batch 301, loss: 0.587, 9632/10016 datapoints
2025-03-06 18:30:20,869 - INFO - Epoch 333/800 done.
2025-03-06 18:30:20,869 - INFO - Final validation performance:
Loss: 0.344, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:30:20,870 - INFO - Beginning epoch 334/800
2025-03-06 18:30:20,875 - INFO - training batch 1, loss: 0.220, 32/60000 datapoints
2025-03-06 18:30:21,069 - INFO - training batch 51, loss: 0.395, 1632/60000 datapoints
2025-03-06 18:30:21,262 - INFO - training batch 101, loss: 0.169, 3232/60000 datapoints
2025-03-06 18:30:21,457 - INFO - training batch 151, loss: 0.157, 4832/60000 datapoints
2025-03-06 18:30:21,649 - INFO - training batch 201, loss: 0.203, 6432/60000 datapoints
2025-03-06 18:30:21,847 - INFO - training batch 251, loss: 0.574, 8032/60000 datapoints
2025-03-06 18:30:22,038 - INFO - training batch 301, loss: 0.230, 9632/60000 datapoints
2025-03-06 18:30:22,248 - INFO - training batch 351, loss: 0.563, 11232/60000 datapoints
2025-03-06 18:30:22,439 - INFO - training batch 401, loss: 0.361, 12832/60000 datapoints
2025-03-06 18:30:22,632 - INFO - training batch 451, loss: 0.525, 14432/60000 datapoints
2025-03-06 18:30:22,823 - INFO - training batch 501, loss: 0.301, 16032/60000 datapoints
2025-03-06 18:30:23,013 - INFO - training batch 551, loss: 0.150, 17632/60000 datapoints
2025-03-06 18:30:23,204 - INFO - training batch 601, loss: 0.412, 19232/60000 datapoints
2025-03-06 18:30:23,398 - INFO - training batch 651, loss: 0.322, 20832/60000 datapoints
2025-03-06 18:30:23,589 - INFO - training batch 701, loss: 0.240, 22432/60000 datapoints
2025-03-06 18:30:23,782 - INFO - training batch 751, loss: 0.629, 24032/60000 datapoints
2025-03-06 18:30:23,973 - INFO - training batch 801, loss: 0.373, 25632/60000 datapoints
2025-03-06 18:30:24,161 - INFO - training batch 851, loss: 0.223, 27232/60000 datapoints
2025-03-06 18:30:24,353 - INFO - training batch 901, loss: 0.206, 28832/60000 datapoints
2025-03-06 18:30:24,543 - INFO - training batch 951, loss: 0.143, 30432/60000 datapoints
2025-03-06 18:30:24,734 - INFO - training batch 1001, loss: 0.284, 32032/60000 datapoints
2025-03-06 18:30:24,938 - INFO - training batch 1051, loss: 0.229, 33632/60000 datapoints
2025-03-06 18:30:25,131 - INFO - training batch 1101, loss: 0.513, 35232/60000 datapoints
2025-03-06 18:30:25,322 - INFO - training batch 1151, loss: 0.402, 36832/60000 datapoints
2025-03-06 18:30:25,515 - INFO - training batch 1201, loss: 0.491, 38432/60000 datapoints
2025-03-06 18:30:25,707 - INFO - training batch 1251, loss: 0.094, 40032/60000 datapoints
2025-03-06 18:30:25,899 - INFO - training batch 1301, loss: 0.462, 41632/60000 datapoints
2025-03-06 18:30:26,090 - INFO - training batch 1351, loss: 0.488, 43232/60000 datapoints
2025-03-06 18:30:26,284 - INFO - training batch 1401, loss: 0.120, 44832/60000 datapoints
2025-03-06 18:30:26,477 - INFO - training batch 1451, loss: 0.278, 46432/60000 datapoints
2025-03-06 18:30:26,669 - INFO - training batch 1501, loss: 0.349, 48032/60000 datapoints
2025-03-06 18:30:26,860 - INFO - training batch 1551, loss: 0.286, 49632/60000 datapoints
2025-03-06 18:30:27,052 - INFO - training batch 1601, loss: 0.294, 51232/60000 datapoints
2025-03-06 18:30:27,242 - INFO - training batch 1651, loss: 0.431, 52832/60000 datapoints
2025-03-06 18:30:27,436 - INFO - training batch 1701, loss: 0.170, 54432/60000 datapoints
2025-03-06 18:30:27,630 - INFO - training batch 1751, loss: 0.300, 56032/60000 datapoints
2025-03-06 18:30:27,821 - INFO - training batch 1801, loss: 0.279, 57632/60000 datapoints
2025-03-06 18:30:28,011 - INFO - training batch 1851, loss: 0.329, 59232/60000 datapoints
2025-03-06 18:30:28,110 - INFO - validation batch 1, loss: 0.234, 32/10016 datapoints
2025-03-06 18:30:28,259 - INFO - validation batch 51, loss: 0.347, 1632/10016 datapoints
2025-03-06 18:30:28,410 - INFO - validation batch 101, loss: 0.398, 3232/10016 datapoints
2025-03-06 18:30:28,558 - INFO - validation batch 151, loss: 0.217, 4832/10016 datapoints
2025-03-06 18:30:28,711 - INFO - validation batch 201, loss: 0.494, 6432/10016 datapoints
2025-03-06 18:30:28,863 - INFO - validation batch 251, loss: 0.280, 8032/10016 datapoints
2025-03-06 18:30:29,012 - INFO - validation batch 301, loss: 0.371, 9632/10016 datapoints
2025-03-06 18:30:29,046 - INFO - Epoch 334/800 done.
2025-03-06 18:30:29,047 - INFO - Final validation performance:
Loss: 0.334, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:30:29,047 - INFO - Beginning epoch 335/800
2025-03-06 18:30:29,054 - INFO - training batch 1, loss: 0.145, 32/60000 datapoints
2025-03-06 18:30:29,247 - INFO - training batch 51, loss: 0.553, 1632/60000 datapoints
2025-03-06 18:30:29,444 - INFO - training batch 101, loss: 0.328, 3232/60000 datapoints
2025-03-06 18:30:29,636 - INFO - training batch 151, loss: 0.127, 4832/60000 datapoints
2025-03-06 18:30:29,827 - INFO - training batch 201, loss: 0.202, 6432/60000 datapoints
2025-03-06 18:30:30,020 - INFO - training batch 251, loss: 0.281, 8032/60000 datapoints
2025-03-06 18:30:30,213 - INFO - training batch 301, loss: 0.374, 9632/60000 datapoints
2025-03-06 18:30:30,411 - INFO - training batch 351, loss: 0.257, 11232/60000 datapoints
2025-03-06 18:30:30,604 - INFO - training batch 401, loss: 0.315, 12832/60000 datapoints
2025-03-06 18:30:30,794 - INFO - training batch 451, loss: 0.437, 14432/60000 datapoints
2025-03-06 18:30:30,985 - INFO - training batch 501, loss: 0.350, 16032/60000 datapoints
2025-03-06 18:30:31,178 - INFO - training batch 551, loss: 0.503, 17632/60000 datapoints
2025-03-06 18:30:31,367 - INFO - training batch 601, loss: 0.606, 19232/60000 datapoints
2025-03-06 18:30:31,562 - INFO - training batch 651, loss: 0.304, 20832/60000 datapoints
2025-03-06 18:30:31,756 - INFO - training batch 701, loss: 0.142, 22432/60000 datapoints
2025-03-06 18:30:31,951 - INFO - training batch 751, loss: 0.258, 24032/60000 datapoints
2025-03-06 18:30:32,143 - INFO - training batch 801, loss: 0.530, 25632/60000 datapoints
2025-03-06 18:30:32,348 - INFO - training batch 851, loss: 0.110, 27232/60000 datapoints
2025-03-06 18:30:32,538 - INFO - training batch 901, loss: 0.181, 28832/60000 datapoints
2025-03-06 18:30:32,729 - INFO - training batch 951, loss: 0.184, 30432/60000 datapoints
2025-03-06 18:30:32,919 - INFO - training batch 1001, loss: 0.169, 32032/60000 datapoints
2025-03-06 18:30:33,109 - INFO - training batch 1051, loss: 0.283, 33632/60000 datapoints
2025-03-06 18:30:33,300 - INFO - training batch 1101, loss: 0.437, 35232/60000 datapoints
2025-03-06 18:30:33,494 - INFO - training batch 1151, loss: 0.229, 36832/60000 datapoints
2025-03-06 18:30:33,689 - INFO - training batch 1201, loss: 0.388, 38432/60000 datapoints
2025-03-06 18:30:33,888 - INFO - training batch 1251, loss: 0.266, 40032/60000 datapoints
2025-03-06 18:30:34,085 - INFO - training batch 1301, loss: 0.446, 41632/60000 datapoints
2025-03-06 18:30:34,279 - INFO - training batch 1351, loss: 0.192, 43232/60000 datapoints
2025-03-06 18:30:34,474 - INFO - training batch 1401, loss: 0.540, 44832/60000 datapoints
2025-03-06 18:30:34,672 - INFO - training batch 1451, loss: 0.199, 46432/60000 datapoints
2025-03-06 18:30:34,871 - INFO - training batch 1501, loss: 0.380, 48032/60000 datapoints
2025-03-06 18:30:35,068 - INFO - training batch 1551, loss: 0.224, 49632/60000 datapoints
2025-03-06 18:30:35,265 - INFO - training batch 1601, loss: 0.521, 51232/60000 datapoints
2025-03-06 18:30:35,465 - INFO - training batch 1651, loss: 0.265, 52832/60000 datapoints
2025-03-06 18:30:35,662 - INFO - training batch 1701, loss: 0.194, 54432/60000 datapoints
2025-03-06 18:30:35,859 - INFO - training batch 1751, loss: 0.158, 56032/60000 datapoints
2025-03-06 18:30:36,052 - INFO - training batch 1801, loss: 0.163, 57632/60000 datapoints
2025-03-06 18:30:36,249 - INFO - training batch 1851, loss: 0.330, 59232/60000 datapoints
2025-03-06 18:30:36,351 - INFO - validation batch 1, loss: 0.330, 32/10016 datapoints
2025-03-06 18:30:36,505 - INFO - validation batch 51, loss: 0.336, 1632/10016 datapoints
2025-03-06 18:30:36,659 - INFO - validation batch 101, loss: 0.127, 3232/10016 datapoints
2025-03-06 18:30:36,812 - INFO - validation batch 151, loss: 0.276, 4832/10016 datapoints
2025-03-06 18:30:36,963 - INFO - validation batch 201, loss: 0.625, 6432/10016 datapoints
2025-03-06 18:30:37,116 - INFO - validation batch 251, loss: 0.210, 8032/10016 datapoints
2025-03-06 18:30:37,268 - INFO - validation batch 301, loss: 0.230, 9632/10016 datapoints
2025-03-06 18:30:37,304 - INFO - Epoch 335/800 done.
2025-03-06 18:30:37,304 - INFO - Final validation performance:
Loss: 0.305, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:30:37,305 - INFO - Beginning epoch 336/800
2025-03-06 18:30:37,311 - INFO - training batch 1, loss: 0.473, 32/60000 datapoints
2025-03-06 18:30:37,515 - INFO - training batch 51, loss: 0.206, 1632/60000 datapoints
2025-03-06 18:30:37,721 - INFO - training batch 101, loss: 0.644, 3232/60000 datapoints
2025-03-06 18:30:37,914 - INFO - training batch 151, loss: 0.317, 4832/60000 datapoints
2025-03-06 18:30:38,113 - INFO - training batch 201, loss: 0.520, 6432/60000 datapoints
2025-03-06 18:30:38,307 - INFO - training batch 251, loss: 0.538, 8032/60000 datapoints
2025-03-06 18:30:38,501 - INFO - training batch 301, loss: 0.315, 9632/60000 datapoints
2025-03-06 18:30:38,699 - INFO - training batch 351, loss: 0.571, 11232/60000 datapoints
2025-03-06 18:30:38,915 - INFO - training batch 401, loss: 0.193, 12832/60000 datapoints
2025-03-06 18:30:39,136 - INFO - training batch 451, loss: 0.433, 14432/60000 datapoints
2025-03-06 18:30:39,345 - INFO - training batch 501, loss: 0.584, 16032/60000 datapoints
2025-03-06 18:30:39,555 - INFO - training batch 551, loss: 0.163, 17632/60000 datapoints
2025-03-06 18:30:39,772 - INFO - training batch 601, loss: 0.195, 19232/60000 datapoints
2025-03-06 18:30:39,973 - INFO - training batch 651, loss: 0.307, 20832/60000 datapoints
2025-03-06 18:30:40,171 - INFO - training batch 701, loss: 0.231, 22432/60000 datapoints
2025-03-06 18:30:40,367 - INFO - training batch 751, loss: 0.179, 24032/60000 datapoints
2025-03-06 18:30:40,561 - INFO - training batch 801, loss: 0.139, 25632/60000 datapoints
2025-03-06 18:30:40,759 - INFO - training batch 851, loss: 0.218, 27232/60000 datapoints
2025-03-06 18:30:40,952 - INFO - training batch 901, loss: 0.429, 28832/60000 datapoints
2025-03-06 18:30:41,148 - INFO - training batch 951, loss: 0.237, 30432/60000 datapoints
2025-03-06 18:30:41,340 - INFO - training batch 1001, loss: 0.455, 32032/60000 datapoints
2025-03-06 18:30:41,536 - INFO - training batch 1051, loss: 0.216, 33632/60000 datapoints
2025-03-06 18:30:41,733 - INFO - training batch 1101, loss: 0.388, 35232/60000 datapoints
2025-03-06 18:30:41,940 - INFO - training batch 1151, loss: 0.126, 36832/60000 datapoints
2025-03-06 18:30:42,136 - INFO - training batch 1201, loss: 0.347, 38432/60000 datapoints
2025-03-06 18:30:42,338 - INFO - training batch 1251, loss: 0.121, 40032/60000 datapoints
2025-03-06 18:30:42,541 - INFO - training batch 1301, loss: 0.250, 41632/60000 datapoints
2025-03-06 18:30:42,743 - INFO - training batch 1351, loss: 0.277, 43232/60000 datapoints
2025-03-06 18:30:42,936 - INFO - training batch 1401, loss: 0.323, 44832/60000 datapoints
2025-03-06 18:30:43,130 - INFO - training batch 1451, loss: 0.084, 46432/60000 datapoints
2025-03-06 18:30:43,325 - INFO - training batch 1501, loss: 0.320, 48032/60000 datapoints
2025-03-06 18:30:43,522 - INFO - training batch 1551, loss: 0.250, 49632/60000 datapoints
2025-03-06 18:30:43,740 - INFO - training batch 1601, loss: 0.305, 51232/60000 datapoints
2025-03-06 18:30:43,940 - INFO - training batch 1651, loss: 0.320, 52832/60000 datapoints
2025-03-06 18:30:44,134 - INFO - training batch 1701, loss: 0.159, 54432/60000 datapoints
2025-03-06 18:30:44,328 - INFO - training batch 1751, loss: 0.302, 56032/60000 datapoints
2025-03-06 18:30:44,522 - INFO - training batch 1801, loss: 0.307, 57632/60000 datapoints
2025-03-06 18:30:44,717 - INFO - training batch 1851, loss: 0.262, 59232/60000 datapoints
2025-03-06 18:30:44,816 - INFO - validation batch 1, loss: 0.143, 32/10016 datapoints
2025-03-06 18:30:44,971 - INFO - validation batch 51, loss: 0.186, 1632/10016 datapoints
2025-03-06 18:30:45,126 - INFO - validation batch 101, loss: 0.131, 3232/10016 datapoints
2025-03-06 18:30:45,277 - INFO - validation batch 151, loss: 0.110, 4832/10016 datapoints
2025-03-06 18:30:45,428 - INFO - validation batch 201, loss: 0.120, 6432/10016 datapoints
2025-03-06 18:30:45,584 - INFO - validation batch 251, loss: 0.301, 8032/10016 datapoints
2025-03-06 18:30:45,738 - INFO - validation batch 301, loss: 0.134, 9632/10016 datapoints
2025-03-06 18:30:45,776 - INFO - Epoch 336/800 done.
2025-03-06 18:30:45,776 - INFO - Final validation performance:
Loss: 0.161, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:30:45,777 - INFO - Beginning epoch 337/800
2025-03-06 18:30:45,782 - INFO - training batch 1, loss: 0.196, 32/60000 datapoints
2025-03-06 18:30:45,975 - INFO - training batch 51, loss: 0.519, 1632/60000 datapoints
2025-03-06 18:30:46,175 - INFO - training batch 101, loss: 0.366, 3232/60000 datapoints
2025-03-06 18:30:46,383 - INFO - training batch 151, loss: 0.130, 4832/60000 datapoints
2025-03-06 18:30:46,576 - INFO - training batch 201, loss: 0.595, 6432/60000 datapoints
2025-03-06 18:30:46,774 - INFO - training batch 251, loss: 0.082, 8032/60000 datapoints
2025-03-06 18:30:46,966 - INFO - training batch 301, loss: 0.108, 9632/60000 datapoints
2025-03-06 18:30:47,163 - INFO - training batch 351, loss: 0.521, 11232/60000 datapoints
2025-03-06 18:30:47,357 - INFO - training batch 401, loss: 0.481, 12832/60000 datapoints
2025-03-06 18:30:47,554 - INFO - training batch 451, loss: 0.207, 14432/60000 datapoints
2025-03-06 18:30:47,748 - INFO - training batch 501, loss: 0.466, 16032/60000 datapoints
2025-03-06 18:30:47,944 - INFO - training batch 551, loss: 0.470, 17632/60000 datapoints
2025-03-06 18:30:48,137 - INFO - training batch 601, loss: 0.238, 19232/60000 datapoints
2025-03-06 18:30:48,331 - INFO - training batch 651, loss: 0.431, 20832/60000 datapoints
2025-03-06 18:30:48,526 - INFO - training batch 701, loss: 0.218, 22432/60000 datapoints
2025-03-06 18:30:48,722 - INFO - training batch 751, loss: 0.433, 24032/60000 datapoints
2025-03-06 18:30:48,917 - INFO - training batch 801, loss: 0.637, 25632/60000 datapoints
2025-03-06 18:30:49,113 - INFO - training batch 851, loss: 0.232, 27232/60000 datapoints
2025-03-06 18:30:49,307 - INFO - training batch 901, loss: 0.350, 28832/60000 datapoints
2025-03-06 18:30:49,504 - INFO - training batch 951, loss: 0.266, 30432/60000 datapoints
2025-03-06 18:30:49,702 - INFO - training batch 1001, loss: 0.682, 32032/60000 datapoints
2025-03-06 18:30:49,895 - INFO - training batch 1051, loss: 0.313, 33632/60000 datapoints
2025-03-06 18:30:50,089 - INFO - training batch 1101, loss: 0.247, 35232/60000 datapoints
2025-03-06 18:30:50,283 - INFO - training batch 1151, loss: 0.312, 36832/60000 datapoints
2025-03-06 18:30:50,476 - INFO - training batch 1201, loss: 0.343, 38432/60000 datapoints
2025-03-06 18:30:50,673 - INFO - training batch 1251, loss: 0.310, 40032/60000 datapoints
2025-03-06 18:30:50,867 - INFO - training batch 1301, loss: 0.320, 41632/60000 datapoints
2025-03-06 18:30:51,059 - INFO - training batch 1351, loss: 0.189, 43232/60000 datapoints
2025-03-06 18:30:51,252 - INFO - training batch 1401, loss: 0.335, 44832/60000 datapoints
2025-03-06 18:30:51,447 - INFO - training batch 1451, loss: 0.650, 46432/60000 datapoints
2025-03-06 18:30:51,648 - INFO - training batch 1501, loss: 0.235, 48032/60000 datapoints
2025-03-06 18:30:51,843 - INFO - training batch 1551, loss: 0.316, 49632/60000 datapoints
2025-03-06 18:30:52,039 - INFO - training batch 1601, loss: 0.153, 51232/60000 datapoints
2025-03-06 18:30:52,235 - INFO - training batch 1651, loss: 0.311, 52832/60000 datapoints
2025-03-06 18:30:52,439 - INFO - training batch 1701, loss: 0.319, 54432/60000 datapoints
2025-03-06 18:30:52,643 - INFO - training batch 1751, loss: 0.238, 56032/60000 datapoints
2025-03-06 18:30:52,835 - INFO - training batch 1801, loss: 0.264, 57632/60000 datapoints
2025-03-06 18:30:53,035 - INFO - training batch 1851, loss: 0.212, 59232/60000 datapoints
2025-03-06 18:30:53,138 - INFO - validation batch 1, loss: 0.466, 32/10016 datapoints
2025-03-06 18:30:53,290 - INFO - validation batch 51, loss: 0.266, 1632/10016 datapoints
2025-03-06 18:30:53,441 - INFO - validation batch 101, loss: 0.531, 3232/10016 datapoints
2025-03-06 18:30:53,595 - INFO - validation batch 151, loss: 0.230, 4832/10016 datapoints
2025-03-06 18:30:53,750 - INFO - validation batch 201, loss: 0.246, 6432/10016 datapoints
2025-03-06 18:30:53,902 - INFO - validation batch 251, loss: 0.283, 8032/10016 datapoints
2025-03-06 18:30:54,054 - INFO - validation batch 301, loss: 0.358, 9632/10016 datapoints
2025-03-06 18:30:54,092 - INFO - Epoch 337/800 done.
2025-03-06 18:30:54,092 - INFO - Final validation performance:
Loss: 0.340, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:30:54,093 - INFO - Beginning epoch 338/800
2025-03-06 18:30:54,098 - INFO - training batch 1, loss: 0.732, 32/60000 datapoints
2025-03-06 18:30:54,295 - INFO - training batch 51, loss: 0.284, 1632/60000 datapoints
2025-03-06 18:30:54,490 - INFO - training batch 101, loss: 0.249, 3232/60000 datapoints
2025-03-06 18:30:54,685 - INFO - training batch 151, loss: 0.164, 4832/60000 datapoints
2025-03-06 18:30:54,884 - INFO - training batch 201, loss: 0.395, 6432/60000 datapoints
2025-03-06 18:30:55,078 - INFO - training batch 251, loss: 0.297, 8032/60000 datapoints
2025-03-06 18:30:55,274 - INFO - training batch 301, loss: 0.371, 9632/60000 datapoints
2025-03-06 18:30:55,467 - INFO - training batch 351, loss: 0.391, 11232/60000 datapoints
2025-03-06 18:30:55,673 - INFO - training batch 401, loss: 0.148, 12832/60000 datapoints
2025-03-06 18:30:55,870 - INFO - training batch 451, loss: 0.163, 14432/60000 datapoints
2025-03-06 18:30:56,067 - INFO - training batch 501, loss: 0.276, 16032/60000 datapoints
2025-03-06 18:30:56,264 - INFO - training batch 551, loss: 0.354, 17632/60000 datapoints
2025-03-06 18:30:56,460 - INFO - training batch 601, loss: 0.239, 19232/60000 datapoints
2025-03-06 18:30:56,655 - INFO - training batch 651, loss: 0.468, 20832/60000 datapoints
2025-03-06 18:30:56,848 - INFO - training batch 701, loss: 0.253, 22432/60000 datapoints
2025-03-06 18:30:57,041 - INFO - training batch 751, loss: 0.306, 24032/60000 datapoints
2025-03-06 18:30:57,236 - INFO - training batch 801, loss: 0.183, 25632/60000 datapoints
2025-03-06 18:30:57,429 - INFO - training batch 851, loss: 0.354, 27232/60000 datapoints
2025-03-06 18:30:57,629 - INFO - training batch 901, loss: 0.260, 28832/60000 datapoints
2025-03-06 18:30:57,824 - INFO - training batch 951, loss: 0.160, 30432/60000 datapoints
2025-03-06 18:30:58,016 - INFO - training batch 1001, loss: 0.222, 32032/60000 datapoints
2025-03-06 18:30:58,211 - INFO - training batch 1051, loss: 0.302, 33632/60000 datapoints
2025-03-06 18:30:58,404 - INFO - training batch 1101, loss: 0.157, 35232/60000 datapoints
2025-03-06 18:30:58,607 - INFO - training batch 1151, loss: 0.323, 36832/60000 datapoints
2025-03-06 18:30:58,809 - INFO - training batch 1201, loss: 0.170, 38432/60000 datapoints
2025-03-06 18:30:59,008 - INFO - training batch 1251, loss: 0.205, 40032/60000 datapoints
2025-03-06 18:30:59,202 - INFO - training batch 1301, loss: 0.369, 41632/60000 datapoints
2025-03-06 18:30:59,395 - INFO - training batch 1351, loss: 0.109, 43232/60000 datapoints
2025-03-06 18:30:59,591 - INFO - training batch 1401, loss: 0.279, 44832/60000 datapoints
2025-03-06 18:30:59,786 - INFO - training batch 1451, loss: 0.398, 46432/60000 datapoints
2025-03-06 18:30:59,978 - INFO - training batch 1501, loss: 0.232, 48032/60000 datapoints
2025-03-06 18:31:00,171 - INFO - training batch 1551, loss: 0.204, 49632/60000 datapoints
2025-03-06 18:31:00,366 - INFO - training batch 1601, loss: 0.133, 51232/60000 datapoints
2025-03-06 18:31:00,559 - INFO - training batch 1651, loss: 0.230, 52832/60000 datapoints
2025-03-06 18:31:00,753 - INFO - training batch 1701, loss: 0.398, 54432/60000 datapoints
2025-03-06 18:31:00,947 - INFO - training batch 1751, loss: 0.302, 56032/60000 datapoints
2025-03-06 18:31:01,142 - INFO - training batch 1801, loss: 0.366, 57632/60000 datapoints
2025-03-06 18:31:01,337 - INFO - training batch 1851, loss: 0.349, 59232/60000 datapoints
2025-03-06 18:31:01,439 - INFO - validation batch 1, loss: 0.396, 32/10016 datapoints
2025-03-06 18:31:01,594 - INFO - validation batch 51, loss: 0.127, 1632/10016 datapoints
2025-03-06 18:31:01,748 - INFO - validation batch 101, loss: 0.294, 3232/10016 datapoints
2025-03-06 18:31:01,901 - INFO - validation batch 151, loss: 0.283, 4832/10016 datapoints
2025-03-06 18:31:02,054 - INFO - validation batch 201, loss: 0.167, 6432/10016 datapoints
2025-03-06 18:31:02,207 - INFO - validation batch 251, loss: 0.108, 8032/10016 datapoints
2025-03-06 18:31:02,361 - INFO - validation batch 301, loss: 0.410, 9632/10016 datapoints
2025-03-06 18:31:02,399 - INFO - Epoch 338/800 done.
2025-03-06 18:31:02,399 - INFO - Final validation performance:
Loss: 0.255, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:31:02,399 - INFO - Beginning epoch 339/800
2025-03-06 18:31:02,406 - INFO - training batch 1, loss: 0.253, 32/60000 datapoints
2025-03-06 18:31:02,624 - INFO - training batch 51, loss: 0.202, 1632/60000 datapoints
2025-03-06 18:31:02,819 - INFO - training batch 101, loss: 0.286, 3232/60000 datapoints
2025-03-06 18:31:03,010 - INFO - training batch 151, loss: 0.293, 4832/60000 datapoints
2025-03-06 18:31:03,204 - INFO - training batch 201, loss: 0.135, 6432/60000 datapoints
2025-03-06 18:31:03,398 - INFO - training batch 251, loss: 0.194, 8032/60000 datapoints
2025-03-06 18:31:03,593 - INFO - training batch 301, loss: 0.314, 9632/60000 datapoints
2025-03-06 18:31:03,789 - INFO - training batch 351, loss: 0.335, 11232/60000 datapoints
2025-03-06 18:31:03,983 - INFO - training batch 401, loss: 0.208, 12832/60000 datapoints
2025-03-06 18:31:04,178 - INFO - training batch 451, loss: 0.454, 14432/60000 datapoints
2025-03-06 18:31:04,372 - INFO - training batch 501, loss: 0.445, 16032/60000 datapoints
2025-03-06 18:31:04,567 - INFO - training batch 551, loss: 0.125, 17632/60000 datapoints
2025-03-06 18:31:04,765 - INFO - training batch 601, loss: 0.386, 19232/60000 datapoints
2025-03-06 18:31:04,964 - INFO - training batch 651, loss: 0.580, 20832/60000 datapoints
2025-03-06 18:31:05,158 - INFO - training batch 701, loss: 0.456, 22432/60000 datapoints
2025-03-06 18:31:05,353 - INFO - training batch 751, loss: 0.365, 24032/60000 datapoints
2025-03-06 18:31:05,550 - INFO - training batch 801, loss: 0.366, 25632/60000 datapoints
2025-03-06 18:31:05,744 - INFO - training batch 851, loss: 0.548, 27232/60000 datapoints
2025-03-06 18:31:05,939 - INFO - training batch 901, loss: 0.221, 28832/60000 datapoints
2025-03-06 18:31:06,131 - INFO - training batch 951, loss: 0.291, 30432/60000 datapoints
2025-03-06 18:31:06,326 - INFO - training batch 1001, loss: 0.420, 32032/60000 datapoints
2025-03-06 18:31:06,522 - INFO - training batch 1051, loss: 0.321, 33632/60000 datapoints
2025-03-06 18:31:06,717 - INFO - training batch 1101, loss: 0.187, 35232/60000 datapoints
2025-03-06 18:31:06,911 - INFO - training batch 1151, loss: 0.273, 36832/60000 datapoints
2025-03-06 18:31:07,103 - INFO - training batch 1201, loss: 0.258, 38432/60000 datapoints
2025-03-06 18:31:07,296 - INFO - training batch 1251, loss: 0.199, 40032/60000 datapoints
2025-03-06 18:31:07,489 - INFO - training batch 1301, loss: 0.643, 41632/60000 datapoints
2025-03-06 18:31:07,689 - INFO - training batch 1351, loss: 0.170, 43232/60000 datapoints
2025-03-06 18:31:07,885 - INFO - training batch 1401, loss: 0.318, 44832/60000 datapoints
2025-03-06 18:31:08,086 - INFO - training batch 1451, loss: 0.429, 46432/60000 datapoints
2025-03-06 18:31:08,280 - INFO - training batch 1501, loss: 0.169, 48032/60000 datapoints
2025-03-06 18:31:08,480 - INFO - training batch 1551, loss: 0.399, 49632/60000 datapoints
2025-03-06 18:31:08,679 - INFO - training batch 1601, loss: 0.273, 51232/60000 datapoints
2025-03-06 18:31:08,874 - INFO - training batch 1651, loss: 0.318, 52832/60000 datapoints
2025-03-06 18:31:09,072 - INFO - training batch 1701, loss: 0.240, 54432/60000 datapoints
2025-03-06 18:31:09,267 - INFO - training batch 1751, loss: 0.386, 56032/60000 datapoints
2025-03-06 18:31:09,461 - INFO - training batch 1801, loss: 0.203, 57632/60000 datapoints
2025-03-06 18:31:09,659 - INFO - training batch 1851, loss: 0.548, 59232/60000 datapoints
2025-03-06 18:31:09,760 - INFO - validation batch 1, loss: 0.372, 32/10016 datapoints
2025-03-06 18:31:09,912 - INFO - validation batch 51, loss: 0.408, 1632/10016 datapoints
2025-03-06 18:31:10,065 - INFO - validation batch 101, loss: 0.204, 3232/10016 datapoints
2025-03-06 18:31:10,217 - INFO - validation batch 151, loss: 0.409, 4832/10016 datapoints
2025-03-06 18:31:10,370 - INFO - validation batch 201, loss: 0.139, 6432/10016 datapoints
2025-03-06 18:31:10,522 - INFO - validation batch 251, loss: 0.213, 8032/10016 datapoints
2025-03-06 18:31:10,677 - INFO - validation batch 301, loss: 0.197, 9632/10016 datapoints
2025-03-06 18:31:10,713 - INFO - Epoch 339/800 done.
2025-03-06 18:31:10,713 - INFO - Final validation performance:
Loss: 0.278, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:31:10,713 - INFO - Beginning epoch 340/800
2025-03-06 18:31:10,720 - INFO - training batch 1, loss: 0.186, 32/60000 datapoints
2025-03-06 18:31:10,918 - INFO - training batch 51, loss: 0.274, 1632/60000 datapoints
2025-03-06 18:31:11,109 - INFO - training batch 101, loss: 0.187, 3232/60000 datapoints
2025-03-06 18:31:11,302 - INFO - training batch 151, loss: 0.269, 4832/60000 datapoints
2025-03-06 18:31:11,494 - INFO - training batch 201, loss: 0.111, 6432/60000 datapoints
2025-03-06 18:31:11,695 - INFO - training batch 251, loss: 0.392, 8032/60000 datapoints
2025-03-06 18:31:11,887 - INFO - training batch 301, loss: 0.183, 9632/60000 datapoints
2025-03-06 18:31:12,082 - INFO - training batch 351, loss: 0.280, 11232/60000 datapoints
2025-03-06 18:31:12,271 - INFO - training batch 401, loss: 0.468, 12832/60000 datapoints
2025-03-06 18:31:12,464 - INFO - training batch 451, loss: 0.455, 14432/60000 datapoints
2025-03-06 18:31:12,680 - INFO - training batch 501, loss: 0.328, 16032/60000 datapoints
2025-03-06 18:31:12,873 - INFO - training batch 551, loss: 0.694, 17632/60000 datapoints
2025-03-06 18:31:13,067 - INFO - training batch 601, loss: 0.471, 19232/60000 datapoints
2025-03-06 18:31:13,258 - INFO - training batch 651, loss: 0.357, 20832/60000 datapoints
2025-03-06 18:31:13,449 - INFO - training batch 701, loss: 0.150, 22432/60000 datapoints
2025-03-06 18:31:13,645 - INFO - training batch 751, loss: 0.200, 24032/60000 datapoints
2025-03-06 18:31:13,838 - INFO - training batch 801, loss: 0.224, 25632/60000 datapoints
2025-03-06 18:31:14,029 - INFO - training batch 851, loss: 0.410, 27232/60000 datapoints
2025-03-06 18:31:14,227 - INFO - training batch 901, loss: 0.422, 28832/60000 datapoints
2025-03-06 18:31:14,422 - INFO - training batch 951, loss: 0.365, 30432/60000 datapoints
2025-03-06 18:31:14,617 - INFO - training batch 1001, loss: 0.157, 32032/60000 datapoints
2025-03-06 18:31:14,810 - INFO - training batch 1051, loss: 0.215, 33632/60000 datapoints
2025-03-06 18:31:15,009 - INFO - training batch 1101, loss: 0.213, 35232/60000 datapoints
2025-03-06 18:31:15,202 - INFO - training batch 1151, loss: 0.399, 36832/60000 datapoints
2025-03-06 18:31:15,396 - INFO - training batch 1201, loss: 0.692, 38432/60000 datapoints
2025-03-06 18:31:15,594 - INFO - training batch 1251, loss: 0.423, 40032/60000 datapoints
2025-03-06 18:31:15,793 - INFO - training batch 1301, loss: 0.319, 41632/60000 datapoints
2025-03-06 18:31:15,987 - INFO - training batch 1351, loss: 0.427, 43232/60000 datapoints
2025-03-06 18:31:16,183 - INFO - training batch 1401, loss: 0.313, 44832/60000 datapoints
2025-03-06 18:31:16,378 - INFO - training batch 1451, loss: 0.375, 46432/60000 datapoints
2025-03-06 18:31:16,572 - INFO - training batch 1501, loss: 0.453, 48032/60000 datapoints
2025-03-06 18:31:16,768 - INFO - training batch 1551, loss: 0.480, 49632/60000 datapoints
2025-03-06 18:31:16,965 - INFO - training batch 1601, loss: 0.180, 51232/60000 datapoints
2025-03-06 18:31:17,160 - INFO - training batch 1651, loss: 0.166, 52832/60000 datapoints
2025-03-06 18:31:17,354 - INFO - training batch 1701, loss: 0.384, 54432/60000 datapoints
2025-03-06 18:31:17,554 - INFO - training batch 1751, loss: 0.347, 56032/60000 datapoints
2025-03-06 18:31:17,781 - INFO - training batch 1801, loss: 0.151, 57632/60000 datapoints
2025-03-06 18:31:18,005 - INFO - training batch 1851, loss: 0.318, 59232/60000 datapoints
2025-03-06 18:31:18,107 - INFO - validation batch 1, loss: 0.175, 32/10016 datapoints
2025-03-06 18:31:18,259 - INFO - validation batch 51, loss: 0.194, 1632/10016 datapoints
2025-03-06 18:31:18,413 - INFO - validation batch 101, loss: 0.358, 3232/10016 datapoints
2025-03-06 18:31:18,579 - INFO - validation batch 151, loss: 0.197, 4832/10016 datapoints
2025-03-06 18:31:18,733 - INFO - validation batch 201, loss: 0.518, 6432/10016 datapoints
2025-03-06 18:31:18,885 - INFO - validation batch 251, loss: 0.233, 8032/10016 datapoints
2025-03-06 18:31:19,039 - INFO - validation batch 301, loss: 0.213, 9632/10016 datapoints
2025-03-06 18:31:19,076 - INFO - Epoch 340/800 done.
2025-03-06 18:31:19,077 - INFO - Final validation performance:
Loss: 0.270, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:31:19,077 - INFO - Beginning epoch 341/800
2025-03-06 18:31:19,083 - INFO - training batch 1, loss: 0.619, 32/60000 datapoints
2025-03-06 18:31:19,276 - INFO - training batch 51, loss: 0.092, 1632/60000 datapoints
2025-03-06 18:31:19,471 - INFO - training batch 101, loss: 0.260, 3232/60000 datapoints
2025-03-06 18:31:19,672 - INFO - training batch 151, loss: 0.381, 4832/60000 datapoints
2025-03-06 18:31:19,871 - INFO - training batch 201, loss: 0.397, 6432/60000 datapoints
2025-03-06 18:31:20,065 - INFO - training batch 251, loss: 0.191, 8032/60000 datapoints
2025-03-06 18:31:20,259 - INFO - training batch 301, loss: 0.355, 9632/60000 datapoints
2025-03-06 18:31:20,452 - INFO - training batch 351, loss: 0.358, 11232/60000 datapoints
2025-03-06 18:31:20,648 - INFO - training batch 401, loss: 0.084, 12832/60000 datapoints
2025-03-06 18:31:20,844 - INFO - training batch 451, loss: 0.161, 14432/60000 datapoints
2025-03-06 18:31:21,038 - INFO - training batch 501, loss: 0.223, 16032/60000 datapoints
2025-03-06 18:31:21,236 - INFO - training batch 551, loss: 0.258, 17632/60000 datapoints
2025-03-06 18:31:21,428 - INFO - training batch 601, loss: 0.457, 19232/60000 datapoints
2025-03-06 18:31:21,627 - INFO - training batch 651, loss: 0.520, 20832/60000 datapoints
2025-03-06 18:31:21,822 - INFO - training batch 701, loss: 0.165, 22432/60000 datapoints
2025-03-06 18:31:22,019 - INFO - training batch 751, loss: 0.353, 24032/60000 datapoints
2025-03-06 18:31:22,213 - INFO - training batch 801, loss: 0.339, 25632/60000 datapoints
2025-03-06 18:31:22,408 - INFO - training batch 851, loss: 0.493, 27232/60000 datapoints
2025-03-06 18:31:22,608 - INFO - training batch 901, loss: 0.443, 28832/60000 datapoints
2025-03-06 18:31:22,828 - INFO - training batch 951, loss: 0.382, 30432/60000 datapoints
2025-03-06 18:31:23,025 - INFO - training batch 1001, loss: 0.127, 32032/60000 datapoints
2025-03-06 18:31:23,221 - INFO - training batch 1051, loss: 0.221, 33632/60000 datapoints
2025-03-06 18:31:23,418 - INFO - training batch 1101, loss: 0.277, 35232/60000 datapoints
2025-03-06 18:31:23,619 - INFO - training batch 1151, loss: 0.219, 36832/60000 datapoints
2025-03-06 18:31:23,820 - INFO - training batch 1201, loss: 0.270, 38432/60000 datapoints
2025-03-06 18:31:24,018 - INFO - training batch 1251, loss: 0.294, 40032/60000 datapoints
2025-03-06 18:31:24,214 - INFO - training batch 1301, loss: 0.185, 41632/60000 datapoints
2025-03-06 18:31:24,409 - INFO - training batch 1351, loss: 0.253, 43232/60000 datapoints
2025-03-06 18:31:24,612 - INFO - training batch 1401, loss: 0.140, 44832/60000 datapoints
2025-03-06 18:31:24,809 - INFO - training batch 1451, loss: 0.421, 46432/60000 datapoints
2025-03-06 18:31:25,009 - INFO - training batch 1501, loss: 0.243, 48032/60000 datapoints
2025-03-06 18:31:25,204 - INFO - training batch 1551, loss: 0.409, 49632/60000 datapoints
2025-03-06 18:31:25,403 - INFO - training batch 1601, loss: 0.303, 51232/60000 datapoints
2025-03-06 18:31:25,600 - INFO - training batch 1651, loss: 0.173, 52832/60000 datapoints
2025-03-06 18:31:25,804 - INFO - training batch 1701, loss: 0.106, 54432/60000 datapoints
2025-03-06 18:31:26,003 - INFO - training batch 1751, loss: 0.272, 56032/60000 datapoints
2025-03-06 18:31:26,204 - INFO - training batch 1801, loss: 0.314, 57632/60000 datapoints
2025-03-06 18:31:26,403 - INFO - training batch 1851, loss: 0.610, 59232/60000 datapoints
2025-03-06 18:31:26,508 - INFO - validation batch 1, loss: 0.222, 32/10016 datapoints
2025-03-06 18:31:26,667 - INFO - validation batch 51, loss: 0.244, 1632/10016 datapoints
2025-03-06 18:31:26,821 - INFO - validation batch 101, loss: 0.738, 3232/10016 datapoints
2025-03-06 18:31:26,976 - INFO - validation batch 151, loss: 0.111, 4832/10016 datapoints
2025-03-06 18:31:27,139 - INFO - validation batch 201, loss: 0.422, 6432/10016 datapoints
2025-03-06 18:31:27,293 - INFO - validation batch 251, loss: 0.462, 8032/10016 datapoints
2025-03-06 18:31:27,447 - INFO - validation batch 301, loss: 0.180, 9632/10016 datapoints
2025-03-06 18:31:27,486 - INFO - Epoch 341/800 done.
2025-03-06 18:31:27,487 - INFO - Final validation performance:
Loss: 0.340, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 18:31:27,487 - INFO - Beginning epoch 342/800
2025-03-06 18:31:27,492 - INFO - training batch 1, loss: 0.193, 32/60000 datapoints
2025-03-06 18:31:27,695 - INFO - training batch 51, loss: 0.306, 1632/60000 datapoints
2025-03-06 18:31:27,895 - INFO - training batch 101, loss: 0.397, 3232/60000 datapoints
2025-03-06 18:31:28,093 - INFO - training batch 151, loss: 0.564, 4832/60000 datapoints
2025-03-06 18:31:28,291 - INFO - training batch 201, loss: 0.222, 6432/60000 datapoints
2025-03-06 18:31:28,490 - INFO - training batch 251, loss: 0.439, 8032/60000 datapoints
2025-03-06 18:31:28,688 - INFO - training batch 301, loss: 0.188, 9632/60000 datapoints
2025-03-06 18:31:28,885 - INFO - training batch 351, loss: 0.131, 11232/60000 datapoints
2025-03-06 18:31:29,083 - INFO - training batch 401, loss: 0.343, 12832/60000 datapoints
2025-03-06 18:31:29,283 - INFO - training batch 451, loss: 0.253, 14432/60000 datapoints
2025-03-06 18:31:29,478 - INFO - training batch 501, loss: 0.450, 16032/60000 datapoints
2025-03-06 18:31:29,681 - INFO - training batch 551, loss: 0.387, 17632/60000 datapoints
2025-03-06 18:31:29,877 - INFO - training batch 601, loss: 0.221, 19232/60000 datapoints
2025-03-06 18:31:30,074 - INFO - training batch 651, loss: 0.383, 20832/60000 datapoints
2025-03-06 18:31:30,268 - INFO - training batch 701, loss: 0.091, 22432/60000 datapoints
2025-03-06 18:31:30,462 - INFO - training batch 751, loss: 0.244, 24032/60000 datapoints
2025-03-06 18:31:30,660 - INFO - training batch 801, loss: 0.350, 25632/60000 datapoints
2025-03-06 18:31:30,854 - INFO - training batch 851, loss: 0.209, 27232/60000 datapoints
2025-03-06 18:31:31,048 - INFO - training batch 901, loss: 0.156, 28832/60000 datapoints
2025-03-06 18:31:31,241 - INFO - training batch 951, loss: 0.200, 30432/60000 datapoints
2025-03-06 18:31:31,437 - INFO - training batch 1001, loss: 0.128, 32032/60000 datapoints
2025-03-06 18:31:31,638 - INFO - training batch 1051, loss: 0.317, 33632/60000 datapoints
2025-03-06 18:31:31,836 - INFO - training batch 1101, loss: 0.214, 35232/60000 datapoints
2025-03-06 18:31:32,036 - INFO - training batch 1151, loss: 0.288, 36832/60000 datapoints
2025-03-06 18:31:32,229 - INFO - training batch 1201, loss: 0.279, 38432/60000 datapoints
2025-03-06 18:31:32,425 - INFO - training batch 1251, loss: 0.311, 40032/60000 datapoints
2025-03-06 18:31:32,622 - INFO - training batch 1301, loss: 0.161, 41632/60000 datapoints
2025-03-06 18:31:32,838 - INFO - training batch 1351, loss: 0.203, 43232/60000 datapoints
2025-03-06 18:31:33,035 - INFO - training batch 1401, loss: 0.326, 44832/60000 datapoints
2025-03-06 18:31:33,232 - INFO - training batch 1451, loss: 0.467, 46432/60000 datapoints
2025-03-06 18:31:33,429 - INFO - training batch 1501, loss: 0.074, 48032/60000 datapoints
2025-03-06 18:31:33,631 - INFO - training batch 1551, loss: 0.077, 49632/60000 datapoints
2025-03-06 18:31:33,832 - INFO - training batch 1601, loss: 0.188, 51232/60000 datapoints
2025-03-06 18:31:34,030 - INFO - training batch 1651, loss: 0.189, 52832/60000 datapoints
2025-03-06 18:31:34,225 - INFO - training batch 1701, loss: 0.153, 54432/60000 datapoints
2025-03-06 18:31:34,420 - INFO - training batch 1751, loss: 0.311, 56032/60000 datapoints
2025-03-06 18:31:34,619 - INFO - training batch 1801, loss: 0.422, 57632/60000 datapoints
2025-03-06 18:31:34,815 - INFO - training batch 1851, loss: 0.119, 59232/60000 datapoints
2025-03-06 18:31:34,921 - INFO - validation batch 1, loss: 0.281, 32/10016 datapoints
2025-03-06 18:31:35,076 - INFO - validation batch 51, loss: 0.237, 1632/10016 datapoints
2025-03-06 18:31:35,230 - INFO - validation batch 101, loss: 0.246, 3232/10016 datapoints
2025-03-06 18:31:35,383 - INFO - validation batch 151, loss: 0.735, 4832/10016 datapoints
2025-03-06 18:31:35,537 - INFO - validation batch 201, loss: 0.516, 6432/10016 datapoints
2025-03-06 18:31:35,698 - INFO - validation batch 251, loss: 0.371, 8032/10016 datapoints
2025-03-06 18:31:35,853 - INFO - validation batch 301, loss: 0.352, 9632/10016 datapoints
2025-03-06 18:31:35,891 - INFO - Epoch 342/800 done.
2025-03-06 18:31:35,892 - INFO - Final validation performance:
Loss: 0.391, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 18:31:35,892 - INFO - Beginning epoch 343/800
2025-03-06 18:31:35,898 - INFO - training batch 1, loss: 0.248, 32/60000 datapoints
2025-03-06 18:31:36,096 - INFO - training batch 51, loss: 0.326, 1632/60000 datapoints
2025-03-06 18:31:36,294 - INFO - training batch 101, loss: 0.568, 3232/60000 datapoints
2025-03-06 18:31:36,490 - INFO - training batch 151, loss: 0.207, 4832/60000 datapoints
2025-03-06 18:31:36,691 - INFO - training batch 201, loss: 0.161, 6432/60000 datapoints
2025-03-06 18:31:36,887 - INFO - training batch 251, loss: 0.428, 8032/60000 datapoints
2025-03-06 18:31:37,083 - INFO - training batch 301, loss: 0.354, 9632/60000 datapoints
2025-03-06 18:31:37,279 - INFO - training batch 351, loss: 0.268, 11232/60000 datapoints
2025-03-06 18:31:37,475 - INFO - training batch 401, loss: 0.239, 12832/60000 datapoints
2025-03-06 18:31:37,694 - INFO - training batch 451, loss: 0.377, 14432/60000 datapoints
2025-03-06 18:31:37,893 - INFO - training batch 501, loss: 0.184, 16032/60000 datapoints
2025-03-06 18:31:38,088 - INFO - training batch 551, loss: 0.265, 17632/60000 datapoints
2025-03-06 18:31:38,289 - INFO - training batch 601, loss: 0.089, 19232/60000 datapoints
2025-03-06 18:31:38,486 - INFO - training batch 651, loss: 0.082, 20832/60000 datapoints
2025-03-06 18:31:38,683 - INFO - training batch 701, loss: 0.289, 22432/60000 datapoints
2025-03-06 18:31:38,878 - INFO - training batch 751, loss: 0.201, 24032/60000 datapoints
2025-03-06 18:31:39,074 - INFO - training batch 801, loss: 0.337, 25632/60000 datapoints
2025-03-06 18:31:39,270 - INFO - training batch 851, loss: 0.222, 27232/60000 datapoints
2025-03-06 18:31:39,466 - INFO - training batch 901, loss: 0.206, 28832/60000 datapoints
2025-03-06 18:31:39,663 - INFO - training batch 951, loss: 0.123, 30432/60000 datapoints
2025-03-06 18:31:39,860 - INFO - training batch 1001, loss: 0.182, 32032/60000 datapoints
2025-03-06 18:31:40,055 - INFO - training batch 1051, loss: 0.569, 33632/60000 datapoints
2025-03-06 18:31:40,250 - INFO - training batch 1101, loss: 0.209, 35232/60000 datapoints
2025-03-06 18:31:40,445 - INFO - training batch 1151, loss: 0.255, 36832/60000 datapoints
2025-03-06 18:31:40,643 - INFO - training batch 1201, loss: 0.190, 38432/60000 datapoints
2025-03-06 18:31:40,838 - INFO - training batch 1251, loss: 0.131, 40032/60000 datapoints
2025-03-06 18:31:41,031 - INFO - training batch 1301, loss: 0.194, 41632/60000 datapoints
2025-03-06 18:31:41,228 - INFO - training batch 1351, loss: 0.246, 43232/60000 datapoints
2025-03-06 18:31:41,422 - INFO - training batch 1401, loss: 0.630, 44832/60000 datapoints
2025-03-06 18:31:41,624 - INFO - training batch 1451, loss: 0.240, 46432/60000 datapoints
2025-03-06 18:31:41,823 - INFO - training batch 1501, loss: 0.311, 48032/60000 datapoints
2025-03-06 18:31:42,018 - INFO - training batch 1551, loss: 0.298, 49632/60000 datapoints
2025-03-06 18:31:42,214 - INFO - training batch 1601, loss: 0.379, 51232/60000 datapoints
2025-03-06 18:31:42,408 - INFO - training batch 1651, loss: 0.287, 52832/60000 datapoints
2025-03-06 18:31:42,603 - INFO - training batch 1701, loss: 0.573, 54432/60000 datapoints
2025-03-06 18:31:42,803 - INFO - training batch 1751, loss: 0.165, 56032/60000 datapoints
2025-03-06 18:31:43,012 - INFO - training batch 1801, loss: 0.301, 57632/60000 datapoints
2025-03-06 18:31:43,216 - INFO - training batch 1851, loss: 0.158, 59232/60000 datapoints
2025-03-06 18:31:43,319 - INFO - validation batch 1, loss: 0.419, 32/10016 datapoints
2025-03-06 18:31:43,473 - INFO - validation batch 51, loss: 0.305, 1632/10016 datapoints
2025-03-06 18:31:43,629 - INFO - validation batch 101, loss: 0.333, 3232/10016 datapoints
2025-03-06 18:31:43,787 - INFO - validation batch 151, loss: 0.161, 4832/10016 datapoints
2025-03-06 18:31:43,939 - INFO - validation batch 201, loss: 0.142, 6432/10016 datapoints
2025-03-06 18:31:44,104 - INFO - validation batch 251, loss: 0.170, 8032/10016 datapoints
2025-03-06 18:31:44,273 - INFO - validation batch 301, loss: 0.206, 9632/10016 datapoints
2025-03-06 18:31:44,310 - INFO - Epoch 343/800 done.
2025-03-06 18:31:44,310 - INFO - Final validation performance:
Loss: 0.248, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 18:31:44,310 - INFO - Beginning epoch 344/800
2025-03-06 18:31:44,317 - INFO - training batch 1, loss: 0.148, 32/60000 datapoints
2025-03-06 18:31:44,516 - INFO - training batch 51, loss: 0.224, 1632/60000 datapoints
2025-03-06 18:31:44,714 - INFO - training batch 101, loss: 0.205, 3232/60000 datapoints
2025-03-06 18:31:44,912 - INFO - training batch 151, loss: 0.316, 4832/60000 datapoints
2025-03-06 18:31:45,105 - INFO - training batch 201, loss: 0.252, 6432/60000 datapoints
2025-03-06 18:31:45,302 - INFO - training batch 251, loss: 0.241, 8032/60000 datapoints
2025-03-06 18:31:45,495 - INFO - training batch 301, loss: 0.252, 9632/60000 datapoints
2025-03-06 18:31:45,693 - INFO - training batch 351, loss: 0.240, 11232/60000 datapoints
2025-03-06 18:31:45,892 - INFO - training batch 401, loss: 0.181, 12832/60000 datapoints
2025-03-06 18:31:46,089 - INFO - training batch 451, loss: 0.236, 14432/60000 datapoints
2025-03-06 18:31:46,289 - INFO - training batch 501, loss: 0.180, 16032/60000 datapoints
2025-03-06 18:31:46,481 - INFO - training batch 551, loss: 0.244, 17632/60000 datapoints
2025-03-06 18:31:46,679 - INFO - training batch 601, loss: 0.278, 19232/60000 datapoints
2025-03-06 18:31:46,876 - INFO - training batch 651, loss: 0.400, 20832/60000 datapoints
2025-03-06 18:31:47,072 - INFO - training batch 701, loss: 0.335, 22432/60000 datapoints
2025-03-06 18:31:47,267 - INFO - training batch 751, loss: 0.117, 24032/60000 datapoints
2025-03-06 18:31:47,460 - INFO - training batch 801, loss: 0.084, 25632/60000 datapoints
2025-03-06 18:31:47,659 - INFO - training batch 851, loss: 0.361, 27232/60000 datapoints
2025-03-06 18:31:47,857 - INFO - training batch 901, loss: 0.177, 28832/60000 datapoints
2025-03-06 18:31:48,054 - INFO - training batch 951, loss: 0.361, 30432/60000 datapoints
2025-03-06 18:31:48,251 - INFO - training batch 1001, loss: 0.143, 32032/60000 datapoints
2025-03-06 18:31:48,445 - INFO - training batch 1051, loss: 0.311, 33632/60000 datapoints
2025-03-06 18:31:48,641 - INFO - training batch 1101, loss: 0.165, 35232/60000 datapoints
2025-03-06 18:31:48,839 - INFO - training batch 1151, loss: 0.216, 36832/60000 datapoints
2025-03-06 18:31:49,034 - INFO - training batch 1201, loss: 0.429, 38432/60000 datapoints
2025-03-06 18:31:49,229 - INFO - training batch 1251, loss: 0.383, 40032/60000 datapoints
2025-03-06 18:31:49,424 - INFO - training batch 1301, loss: 0.158, 41632/60000 datapoints
2025-03-06 18:31:49,621 - INFO - training batch 1351, loss: 0.175, 43232/60000 datapoints
2025-03-06 18:31:49,818 - INFO - training batch 1401, loss: 0.193, 44832/60000 datapoints
2025-03-06 18:31:50,013 - INFO - training batch 1451, loss: 0.548, 46432/60000 datapoints
2025-03-06 18:31:50,207 - INFO - training batch 1501, loss: 0.216, 48032/60000 datapoints
2025-03-06 18:31:50,403 - INFO - training batch 1551, loss: 0.176, 49632/60000 datapoints
2025-03-06 18:31:50,598 - INFO - training batch 1601, loss: 0.586, 51232/60000 datapoints
2025-03-06 18:31:50,794 - INFO - training batch 1651, loss: 0.288, 52832/60000 datapoints
2025-03-06 18:31:50,987 - INFO - training batch 1701, loss: 0.222, 54432/60000 datapoints
2025-03-06 18:31:51,181 - INFO - training batch 1751, loss: 0.278, 56032/60000 datapoints
2025-03-06 18:31:51,376 - INFO - training batch 1801, loss: 0.676, 57632/60000 datapoints
2025-03-06 18:31:51,569 - INFO - training batch 1851, loss: 0.266, 59232/60000 datapoints
2025-03-06 18:31:51,673 - INFO - validation batch 1, loss: 0.538, 32/10016 datapoints
2025-03-06 18:31:51,831 - INFO - validation batch 51, loss: 0.209, 1632/10016 datapoints
2025-03-06 18:31:51,983 - INFO - validation batch 101, loss: 0.267, 3232/10016 datapoints
2025-03-06 18:31:52,137 - INFO - validation batch 151, loss: 0.269, 4832/10016 datapoints
2025-03-06 18:31:52,292 - INFO - validation batch 201, loss: 0.323, 6432/10016 datapoints
2025-03-06 18:31:52,444 - INFO - validation batch 251, loss: 0.171, 8032/10016 datapoints
2025-03-06 18:31:52,595 - INFO - validation batch 301, loss: 0.780, 9632/10016 datapoints
2025-03-06 18:31:52,635 - INFO - Epoch 344/800 done.
2025-03-06 18:31:52,635 - INFO - Final validation performance:
Loss: 0.365, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:31:52,635 - INFO - Beginning epoch 345/800
2025-03-06 18:31:52,641 - INFO - training batch 1, loss: 0.397, 32/60000 datapoints
2025-03-06 18:31:52,836 - INFO - training batch 51, loss: 0.202, 1632/60000 datapoints
2025-03-06 18:31:53,045 - INFO - training batch 101, loss: 0.400, 3232/60000 datapoints
2025-03-06 18:31:53,239 - INFO - training batch 151, loss: 0.380, 4832/60000 datapoints
2025-03-06 18:31:53,432 - INFO - training batch 201, loss: 0.241, 6432/60000 datapoints
2025-03-06 18:31:53,631 - INFO - training batch 251, loss: 0.134, 8032/60000 datapoints
2025-03-06 18:31:53,834 - INFO - training batch 301, loss: 0.325, 9632/60000 datapoints
2025-03-06 18:31:54,027 - INFO - training batch 351, loss: 0.242, 11232/60000 datapoints
2025-03-06 18:31:54,220 - INFO - training batch 401, loss: 0.211, 12832/60000 datapoints
2025-03-06 18:31:54,415 - INFO - training batch 451, loss: 0.196, 14432/60000 datapoints
2025-03-06 18:31:54,612 - INFO - training batch 501, loss: 0.198, 16032/60000 datapoints
2025-03-06 18:31:54,807 - INFO - training batch 551, loss: 0.259, 17632/60000 datapoints
2025-03-06 18:31:55,008 - INFO - training batch 601, loss: 0.183, 19232/60000 datapoints
2025-03-06 18:31:55,201 - INFO - training batch 651, loss: 0.262, 20832/60000 datapoints
2025-03-06 18:31:55,394 - INFO - training batch 701, loss: 0.203, 22432/60000 datapoints
2025-03-06 18:31:55,586 - INFO - training batch 751, loss: 0.261, 24032/60000 datapoints
2025-03-06 18:31:55,786 - INFO - training batch 801, loss: 0.237, 25632/60000 datapoints
2025-03-06 18:31:55,980 - INFO - training batch 851, loss: 0.553, 27232/60000 datapoints
2025-03-06 18:31:56,176 - INFO - training batch 901, loss: 0.580, 28832/60000 datapoints
2025-03-06 18:31:56,372 - INFO - training batch 951, loss: 0.285, 30432/60000 datapoints
2025-03-06 18:31:56,567 - INFO - training batch 1001, loss: 0.207, 32032/60000 datapoints
2025-03-06 18:31:56,763 - INFO - training batch 1051, loss: 0.276, 33632/60000 datapoints
2025-03-06 18:31:56,962 - INFO - training batch 1101, loss: 0.158, 35232/60000 datapoints
2025-03-06 18:31:57,157 - INFO - training batch 1151, loss: 0.178, 36832/60000 datapoints
2025-03-06 18:31:57,354 - INFO - training batch 1201, loss: 0.235, 38432/60000 datapoints
2025-03-06 18:31:57,547 - INFO - training batch 1251, loss: 0.201, 40032/60000 datapoints
2025-03-06 18:31:57,742 - INFO - training batch 1301, loss: 0.294, 41632/60000 datapoints
2025-03-06 18:31:57,941 - INFO - training batch 1351, loss: 0.363, 43232/60000 datapoints
2025-03-06 18:31:58,136 - INFO - training batch 1401, loss: 0.211, 44832/60000 datapoints
2025-03-06 18:31:58,330 - INFO - training batch 1451, loss: 0.382, 46432/60000 datapoints
2025-03-06 18:31:58,529 - INFO - training batch 1501, loss: 0.340, 48032/60000 datapoints
2025-03-06 18:31:58,727 - INFO - training batch 1551, loss: 0.323, 49632/60000 datapoints
2025-03-06 18:31:58,923 - INFO - training batch 1601, loss: 0.279, 51232/60000 datapoints
2025-03-06 18:31:59,116 - INFO - training batch 1651, loss: 0.388, 52832/60000 datapoints
2025-03-06 18:31:59,309 - INFO - training batch 1701, loss: 0.179, 54432/60000 datapoints
2025-03-06 18:31:59,505 - INFO - training batch 1751, loss: 0.223, 56032/60000 datapoints
2025-03-06 18:31:59,702 - INFO - training batch 1801, loss: 0.263, 57632/60000 datapoints
2025-03-06 18:31:59,899 - INFO - training batch 1851, loss: 0.431, 59232/60000 datapoints
2025-03-06 18:32:00,002 - INFO - validation batch 1, loss: 0.235, 32/10016 datapoints
2025-03-06 18:32:00,153 - INFO - validation batch 51, loss: 0.253, 1632/10016 datapoints
2025-03-06 18:32:00,308 - INFO - validation batch 101, loss: 0.333, 3232/10016 datapoints
2025-03-06 18:32:00,463 - INFO - validation batch 151, loss: 0.300, 4832/10016 datapoints
2025-03-06 18:32:00,620 - INFO - validation batch 201, loss: 0.439, 6432/10016 datapoints
2025-03-06 18:32:00,776 - INFO - validation batch 251, loss: 0.139, 8032/10016 datapoints
2025-03-06 18:32:00,928 - INFO - validation batch 301, loss: 0.218, 9632/10016 datapoints
2025-03-06 18:32:00,964 - INFO - Epoch 345/800 done.
2025-03-06 18:32:00,964 - INFO - Final validation performance:
Loss: 0.274, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 18:32:00,964 - INFO - Beginning epoch 346/800
2025-03-06 18:32:00,971 - INFO - training batch 1, loss: 0.284, 32/60000 datapoints
2025-03-06 18:32:01,163 - INFO - training batch 51, loss: 0.564, 1632/60000 datapoints
2025-03-06 18:32:01,355 - INFO - training batch 101, loss: 0.133, 3232/60000 datapoints
2025-03-06 18:32:01,547 - INFO - training batch 151, loss: 0.101, 4832/60000 datapoints
2025-03-06 18:32:01,740 - INFO - training batch 201, loss: 0.241, 6432/60000 datapoints
2025-03-06 18:32:01,940 - INFO - training batch 251, loss: 0.362, 8032/60000 datapoints
2025-03-06 18:32:02,137 - INFO - training batch 301, loss: 0.659, 9632/60000 datapoints
2025-03-06 18:32:02,328 - INFO - training batch 351, loss: 0.201, 11232/60000 datapoints
2025-03-06 18:32:02,518 - INFO - training batch 401, loss: 0.206, 12832/60000 datapoints
2025-03-06 18:32:02,711 - INFO - training batch 451, loss: 0.249, 14432/60000 datapoints
2025-03-06 18:32:02,902 - INFO - training batch 501, loss: 0.180, 16032/60000 datapoints
2025-03-06 18:32:03,112 - INFO - training batch 551, loss: 0.371, 17632/60000 datapoints
2025-03-06 18:32:03,300 - INFO - training batch 601, loss: 0.297, 19232/60000 datapoints
2025-03-06 18:32:03,490 - INFO - training batch 651, loss: 0.222, 20832/60000 datapoints
2025-03-06 18:32:03,682 - INFO - training batch 701, loss: 0.435, 22432/60000 datapoints
2025-03-06 18:32:03,877 - INFO - training batch 751, loss: 0.329, 24032/60000 datapoints
2025-03-06 18:32:04,070 - INFO - training batch 801, loss: 0.512, 25632/60000 datapoints
2025-03-06 18:32:04,261 - INFO - training batch 851, loss: 0.709, 27232/60000 datapoints
2025-03-06 18:32:04,451 - INFO - training batch 901, loss: 0.179, 28832/60000 datapoints
2025-03-06 18:32:04,645 - INFO - training batch 951, loss: 0.579, 30432/60000 datapoints
2025-03-06 18:32:04,841 - INFO - training batch 1001, loss: 0.216, 32032/60000 datapoints
2025-03-06 18:32:05,036 - INFO - training batch 1051, loss: 0.826, 33632/60000 datapoints
2025-03-06 18:32:05,227 - INFO - training batch 1101, loss: 0.440, 35232/60000 datapoints
2025-03-06 18:32:05,419 - INFO - training batch 1151, loss: 0.394, 36832/60000 datapoints
2025-03-06 18:32:05,613 - INFO - training batch 1201, loss: 0.235, 38432/60000 datapoints
2025-03-06 18:32:05,807 - INFO - training batch 1251, loss: 0.418, 40032/60000 datapoints
2025-03-06 18:32:06,000 - INFO - training batch 1301, loss: 0.418, 41632/60000 datapoints
2025-03-06 18:32:06,193 - INFO - training batch 1351, loss: 0.270, 43232/60000 datapoints
2025-03-06 18:32:06,388 - INFO - training batch 1401, loss: 0.272, 44832/60000 datapoints
2025-03-06 18:32:06,582 - INFO - training batch 1451, loss: 0.338, 46432/60000 datapoints
2025-03-06 18:32:06,774 - INFO - training batch 1501, loss: 0.437, 48032/60000 datapoints
2025-03-06 18:32:06,968 - INFO - training batch 1551, loss: 0.784, 49632/60000 datapoints
2025-03-06 18:32:07,161 - INFO - training batch 1601, loss: 0.269, 51232/60000 datapoints
2025-03-06 18:32:07,352 - INFO - training batch 1651, loss: 0.567, 52832/60000 datapoints
2025-03-06 18:32:07,545 - INFO - training batch 1701, loss: 0.249, 54432/60000 datapoints
2025-03-06 18:32:07,742 - INFO - training batch 1751, loss: 0.265, 56032/60000 datapoints
2025-03-06 18:32:07,936 - INFO - training batch 1801, loss: 0.463, 57632/60000 datapoints
2025-03-06 18:32:08,134 - INFO - training batch 1851, loss: 0.194, 59232/60000 datapoints
2025-03-06 18:32:08,233 - INFO - validation batch 1, loss: 0.361, 32/10016 datapoints
2025-03-06 18:32:08,382 - INFO - validation batch 51, loss: 0.167, 1632/10016 datapoints
2025-03-06 18:32:08,531 - INFO - validation batch 101, loss: 0.404, 3232/10016 datapoints
2025-03-06 18:32:08,683 - INFO - validation batch 151, loss: 0.505, 4832/10016 datapoints
2025-03-06 18:32:08,834 - INFO - validation batch 201, loss: 0.435, 6432/10016 datapoints
2025-03-06 18:32:08,985 - INFO - validation batch 251, loss: 0.175, 8032/10016 datapoints
2025-03-06 18:32:09,135 - INFO - validation batch 301, loss: 0.602, 9632/10016 datapoints
2025-03-06 18:32:09,170 - INFO - Epoch 346/800 done.
2025-03-06 18:32:09,171 - INFO - Final validation performance:
Loss: 0.378, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:32:09,171 - INFO - Beginning epoch 347/800
2025-03-06 18:32:09,177 - INFO - training batch 1, loss: 0.170, 32/60000 datapoints
2025-03-06 18:32:09,367 - INFO - training batch 51, loss: 0.332, 1632/60000 datapoints
2025-03-06 18:32:09,560 - INFO - training batch 101, loss: 0.180, 3232/60000 datapoints
2025-03-06 18:32:09,755 - INFO - training batch 151, loss: 0.255, 4832/60000 datapoints
2025-03-06 18:32:09,950 - INFO - training batch 201, loss: 0.408, 6432/60000 datapoints
2025-03-06 18:32:10,144 - INFO - training batch 251, loss: 0.195, 8032/60000 datapoints
2025-03-06 18:32:10,338 - INFO - training batch 301, loss: 0.196, 9632/60000 datapoints
2025-03-06 18:32:10,531 - INFO - training batch 351, loss: 0.457, 11232/60000 datapoints
2025-03-06 18:32:10,724 - INFO - training batch 401, loss: 0.460, 12832/60000 datapoints
2025-03-06 18:32:10,914 - INFO - training batch 451, loss: 0.429, 14432/60000 datapoints
2025-03-06 18:32:11,105 - INFO - training batch 501, loss: 0.221, 16032/60000 datapoints
2025-03-06 18:32:11,296 - INFO - training batch 551, loss: 0.229, 17632/60000 datapoints
2025-03-06 18:32:11,487 - INFO - training batch 601, loss: 0.345, 19232/60000 datapoints
2025-03-06 18:32:11,680 - INFO - training batch 651, loss: 0.215, 20832/60000 datapoints
2025-03-06 18:32:11,874 - INFO - training batch 701, loss: 0.225, 22432/60000 datapoints
2025-03-06 18:32:12,070 - INFO - training batch 751, loss: 0.342, 24032/60000 datapoints
2025-03-06 18:32:12,266 - INFO - training batch 801, loss: 0.149, 25632/60000 datapoints
2025-03-06 18:32:12,459 - INFO - training batch 851, loss: 0.215, 27232/60000 datapoints
2025-03-06 18:32:12,656 - INFO - training batch 901, loss: 0.265, 28832/60000 datapoints
2025-03-06 18:32:12,848 - INFO - training batch 951, loss: 0.152, 30432/60000 datapoints
2025-03-06 18:32:13,041 - INFO - training batch 1001, loss: 0.288, 32032/60000 datapoints
2025-03-06 18:32:13,252 - INFO - training batch 1051, loss: 0.355, 33632/60000 datapoints
2025-03-06 18:32:13,445 - INFO - training batch 1101, loss: 0.268, 35232/60000 datapoints
2025-03-06 18:32:13,641 - INFO - training batch 1151, loss: 0.298, 36832/60000 datapoints
2025-03-06 18:32:13,834 - INFO - training batch 1201, loss: 0.504, 38432/60000 datapoints
2025-03-06 18:32:14,026 - INFO - training batch 1251, loss: 0.278, 40032/60000 datapoints
2025-03-06 18:32:14,219 - INFO - training batch 1301, loss: 0.133, 41632/60000 datapoints
2025-03-06 18:32:14,409 - INFO - training batch 1351, loss: 0.375, 43232/60000 datapoints
2025-03-06 18:32:14,608 - INFO - training batch 1401, loss: 0.249, 44832/60000 datapoints
2025-03-06 18:32:14,801 - INFO - training batch 1451, loss: 0.319, 46432/60000 datapoints
2025-03-06 18:32:15,000 - INFO - training batch 1501, loss: 0.231, 48032/60000 datapoints
2025-03-06 18:32:15,196 - INFO - training batch 1551, loss: 0.411, 49632/60000 datapoints
2025-03-06 18:32:15,390 - INFO - training batch 1601, loss: 0.090, 51232/60000 datapoints
2025-03-06 18:32:15,583 - INFO - training batch 1651, loss: 0.141, 52832/60000 datapoints
2025-03-06 18:32:15,782 - INFO - training batch 1701, loss: 0.165, 54432/60000 datapoints
2025-03-06 18:32:15,977 - INFO - training batch 1751, loss: 0.482, 56032/60000 datapoints
2025-03-06 18:32:16,175 - INFO - training batch 1801, loss: 0.299, 57632/60000 datapoints
2025-03-06 18:32:16,371 - INFO - training batch 1851, loss: 0.452, 59232/60000 datapoints
2025-03-06 18:32:16,472 - INFO - validation batch 1, loss: 0.392, 32/10016 datapoints
2025-03-06 18:32:16,628 - INFO - validation batch 51, loss: 0.162, 1632/10016 datapoints
2025-03-06 18:32:16,779 - INFO - validation batch 101, loss: 0.338, 3232/10016 datapoints
2025-03-06 18:32:16,932 - INFO - validation batch 151, loss: 0.115, 4832/10016 datapoints
2025-03-06 18:32:17,083 - INFO - validation batch 201, loss: 0.535, 6432/10016 datapoints
2025-03-06 18:32:17,238 - INFO - validation batch 251, loss: 0.262, 8032/10016 datapoints
2025-03-06 18:32:17,396 - INFO - validation batch 301, loss: 0.266, 9632/10016 datapoints
2025-03-06 18:32:17,435 - INFO - Epoch 347/800 done.
2025-03-06 18:32:17,435 - INFO - Final validation performance:
Loss: 0.296, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:32:17,435 - INFO - Beginning epoch 348/800
2025-03-06 18:32:17,441 - INFO - training batch 1, loss: 0.467, 32/60000 datapoints
2025-03-06 18:32:17,668 - INFO - training batch 51, loss: 0.219, 1632/60000 datapoints
2025-03-06 18:32:17,885 - INFO - training batch 101, loss: 0.183, 3232/60000 datapoints
2025-03-06 18:32:18,099 - INFO - training batch 151, loss: 0.268, 4832/60000 datapoints
2025-03-06 18:32:18,293 - INFO - training batch 201, loss: 0.474, 6432/60000 datapoints
2025-03-06 18:32:18,488 - INFO - training batch 251, loss: 0.185, 8032/60000 datapoints
2025-03-06 18:32:18,693 - INFO - training batch 301, loss: 0.243, 9632/60000 datapoints
2025-03-06 18:32:18,887 - INFO - training batch 351, loss: 0.118, 11232/60000 datapoints
2025-03-06 18:32:19,081 - INFO - training batch 401, loss: 0.189, 12832/60000 datapoints
2025-03-06 18:32:19,278 - INFO - training batch 451, loss: 0.129, 14432/60000 datapoints
2025-03-06 18:32:19,469 - INFO - training batch 501, loss: 0.127, 16032/60000 datapoints
2025-03-06 18:32:19,665 - INFO - training batch 551, loss: 0.451, 17632/60000 datapoints
2025-03-06 18:32:19,861 - INFO - training batch 601, loss: 0.606, 19232/60000 datapoints
2025-03-06 18:32:20,058 - INFO - training batch 651, loss: 0.465, 20832/60000 datapoints
2025-03-06 18:32:20,254 - INFO - training batch 701, loss: 0.195, 22432/60000 datapoints
2025-03-06 18:32:20,450 - INFO - training batch 751, loss: 0.516, 24032/60000 datapoints
2025-03-06 18:32:20,648 - INFO - training batch 801, loss: 0.573, 25632/60000 datapoints
2025-03-06 18:32:20,838 - INFO - training batch 851, loss: 0.490, 27232/60000 datapoints
2025-03-06 18:32:21,030 - INFO - training batch 901, loss: 0.250, 28832/60000 datapoints
2025-03-06 18:32:21,224 - INFO - training batch 951, loss: 0.390, 30432/60000 datapoints
2025-03-06 18:32:21,418 - INFO - training batch 1001, loss: 0.269, 32032/60000 datapoints
2025-03-06 18:32:21,613 - INFO - training batch 1051, loss: 0.176, 33632/60000 datapoints
2025-03-06 18:32:21,808 - INFO - training batch 1101, loss: 0.173, 35232/60000 datapoints
2025-03-06 18:32:22,006 - INFO - training batch 1151, loss: 0.192, 36832/60000 datapoints
2025-03-06 18:32:22,203 - INFO - training batch 1201, loss: 0.392, 38432/60000 datapoints
2025-03-06 18:32:22,396 - INFO - training batch 1251, loss: 0.166, 40032/60000 datapoints
2025-03-06 18:32:22,589 - INFO - training batch 1301, loss: 0.496, 41632/60000 datapoints
2025-03-06 18:32:22,789 - INFO - training batch 1351, loss: 0.308, 43232/60000 datapoints
2025-03-06 18:32:22,985 - INFO - training batch 1401, loss: 0.396, 44832/60000 datapoints
2025-03-06 18:32:23,196 - INFO - training batch 1451, loss: 0.514, 46432/60000 datapoints
2025-03-06 18:32:23,389 - INFO - training batch 1501, loss: 0.274, 48032/60000 datapoints
2025-03-06 18:32:23,582 - INFO - training batch 1551, loss: 0.452, 49632/60000 datapoints
2025-03-06 18:32:23,777 - INFO - training batch 1601, loss: 0.318, 51232/60000 datapoints
2025-03-06 18:32:23,974 - INFO - training batch 1651, loss: 0.186, 52832/60000 datapoints
2025-03-06 18:32:24,170 - INFO - training batch 1701, loss: 0.285, 54432/60000 datapoints
2025-03-06 18:32:24,365 - INFO - training batch 1751, loss: 0.224, 56032/60000 datapoints
2025-03-06 18:32:24,563 - INFO - training batch 1801, loss: 0.151, 57632/60000 datapoints
2025-03-06 18:32:24,761 - INFO - training batch 1851, loss: 0.460, 59232/60000 datapoints
2025-03-06 18:32:24,868 - INFO - validation batch 1, loss: 0.221, 32/10016 datapoints
2025-03-06 18:32:25,019 - INFO - validation batch 51, loss: 0.408, 1632/10016 datapoints
2025-03-06 18:32:25,172 - INFO - validation batch 101, loss: 0.191, 3232/10016 datapoints
2025-03-06 18:32:25,324 - INFO - validation batch 151, loss: 0.329, 4832/10016 datapoints
2025-03-06 18:32:25,476 - INFO - validation batch 201, loss: 0.234, 6432/10016 datapoints
2025-03-06 18:32:25,632 - INFO - validation batch 251, loss: 0.111, 8032/10016 datapoints
2025-03-06 18:32:25,784 - INFO - validation batch 301, loss: 0.280, 9632/10016 datapoints
2025-03-06 18:32:25,821 - INFO - Epoch 348/800 done.
2025-03-06 18:32:25,821 - INFO - Final validation performance:
Loss: 0.253, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:32:25,821 - INFO - Beginning epoch 349/800
2025-03-06 18:32:25,827 - INFO - training batch 1, loss: 0.454, 32/60000 datapoints
2025-03-06 18:32:26,025 - INFO - training batch 51, loss: 0.193, 1632/60000 datapoints
2025-03-06 18:32:26,224 - INFO - training batch 101, loss: 0.204, 3232/60000 datapoints
2025-03-06 18:32:26,421 - INFO - training batch 151, loss: 0.530, 4832/60000 datapoints
2025-03-06 18:32:26,617 - INFO - training batch 201, loss: 0.299, 6432/60000 datapoints
2025-03-06 18:32:26,814 - INFO - training batch 251, loss: 0.492, 8032/60000 datapoints
2025-03-06 18:32:27,009 - INFO - training batch 301, loss: 0.434, 9632/60000 datapoints
2025-03-06 18:32:27,204 - INFO - training batch 351, loss: 0.242, 11232/60000 datapoints
2025-03-06 18:32:27,404 - INFO - training batch 401, loss: 0.280, 12832/60000 datapoints
2025-03-06 18:32:27,596 - INFO - training batch 451, loss: 0.135, 14432/60000 datapoints
2025-03-06 18:32:27,794 - INFO - training batch 501, loss: 0.178, 16032/60000 datapoints
2025-03-06 18:32:27,990 - INFO - training batch 551, loss: 0.284, 17632/60000 datapoints
2025-03-06 18:32:28,186 - INFO - training batch 601, loss: 0.182, 19232/60000 datapoints
2025-03-06 18:32:28,380 - INFO - training batch 651, loss: 0.173, 20832/60000 datapoints
2025-03-06 18:32:28,573 - INFO - training batch 701, loss: 0.253, 22432/60000 datapoints
2025-03-06 18:32:28,770 - INFO - training batch 751, loss: 0.200, 24032/60000 datapoints
2025-03-06 18:32:28,964 - INFO - training batch 801, loss: 0.186, 25632/60000 datapoints
2025-03-06 18:32:29,158 - INFO - training batch 851, loss: 0.373, 27232/60000 datapoints
2025-03-06 18:32:29,352 - INFO - training batch 901, loss: 0.277, 28832/60000 datapoints
2025-03-06 18:32:29,546 - INFO - training batch 951, loss: 0.190, 30432/60000 datapoints
2025-03-06 18:32:29,745 - INFO - training batch 1001, loss: 0.314, 32032/60000 datapoints
2025-03-06 18:32:29,940 - INFO - training batch 1051, loss: 0.203, 33632/60000 datapoints
2025-03-06 18:32:30,134 - INFO - training batch 1101, loss: 0.252, 35232/60000 datapoints
2025-03-06 18:32:30,332 - INFO - training batch 1151, loss: 0.359, 36832/60000 datapoints
2025-03-06 18:32:30,524 - INFO - training batch 1201, loss: 0.402, 38432/60000 datapoints
2025-03-06 18:32:30,718 - INFO - training batch 1251, loss: 0.220, 40032/60000 datapoints
2025-03-06 18:32:30,914 - INFO - training batch 1301, loss: 0.182, 41632/60000 datapoints
2025-03-06 18:32:31,108 - INFO - training batch 1351, loss: 0.207, 43232/60000 datapoints
2025-03-06 18:32:31,303 - INFO - training batch 1401, loss: 0.486, 44832/60000 datapoints
2025-03-06 18:32:31,494 - INFO - training batch 1451, loss: 0.410, 46432/60000 datapoints
2025-03-06 18:32:31,690 - INFO - training batch 1501, loss: 0.332, 48032/60000 datapoints
2025-03-06 18:32:31,886 - INFO - training batch 1551, loss: 0.179, 49632/60000 datapoints
2025-03-06 18:32:32,082 - INFO - training batch 1601, loss: 0.239, 51232/60000 datapoints
2025-03-06 18:32:32,281 - INFO - training batch 1651, loss: 0.197, 52832/60000 datapoints
2025-03-06 18:32:32,474 - INFO - training batch 1701, loss: 0.456, 54432/60000 datapoints
2025-03-06 18:32:32,671 - INFO - training batch 1751, loss: 0.318, 56032/60000 datapoints
2025-03-06 18:32:32,864 - INFO - training batch 1801, loss: 0.366, 57632/60000 datapoints
2025-03-06 18:32:33,057 - INFO - training batch 1851, loss: 0.271, 59232/60000 datapoints
2025-03-06 18:32:33,158 - INFO - validation batch 1, loss: 0.228, 32/10016 datapoints
2025-03-06 18:32:33,331 - INFO - validation batch 51, loss: 0.248, 1632/10016 datapoints
2025-03-06 18:32:33,484 - INFO - validation batch 101, loss: 0.302, 3232/10016 datapoints
2025-03-06 18:32:33,639 - INFO - validation batch 151, loss: 0.266, 4832/10016 datapoints
2025-03-06 18:32:33,792 - INFO - validation batch 201, loss: 0.136, 6432/10016 datapoints
2025-03-06 18:32:33,947 - INFO - validation batch 251, loss: 0.312, 8032/10016 datapoints
2025-03-06 18:32:34,101 - INFO - validation batch 301, loss: 0.067, 9632/10016 datapoints
2025-03-06 18:32:34,139 - INFO - Epoch 349/800 done.
2025-03-06 18:32:34,139 - INFO - Final validation performance:
Loss: 0.223, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:32:34,139 - INFO - Beginning epoch 350/800
2025-03-06 18:32:34,146 - INFO - training batch 1, loss: 0.216, 32/60000 datapoints
2025-03-06 18:32:34,340 - INFO - training batch 51, loss: 0.334, 1632/60000 datapoints
2025-03-06 18:32:34,535 - INFO - training batch 101, loss: 0.294, 3232/60000 datapoints
2025-03-06 18:32:34,728 - INFO - training batch 151, loss: 0.399, 4832/60000 datapoints
2025-03-06 18:32:34,932 - INFO - training batch 201, loss: 0.221, 6432/60000 datapoints
2025-03-06 18:32:35,125 - INFO - training batch 251, loss: 0.237, 8032/60000 datapoints
2025-03-06 18:32:35,320 - INFO - training batch 301, loss: 0.172, 9632/60000 datapoints
2025-03-06 18:32:35,515 - INFO - training batch 351, loss: 0.563, 11232/60000 datapoints
2025-03-06 18:32:35,710 - INFO - training batch 401, loss: 0.343, 12832/60000 datapoints
2025-03-06 18:32:35,908 - INFO - training batch 451, loss: 0.400, 14432/60000 datapoints
2025-03-06 18:32:36,103 - INFO - training batch 501, loss: 0.648, 16032/60000 datapoints
2025-03-06 18:32:36,301 - INFO - training batch 551, loss: 0.191, 17632/60000 datapoints
2025-03-06 18:32:36,500 - INFO - training batch 601, loss: 0.369, 19232/60000 datapoints
2025-03-06 18:32:36,695 - INFO - training batch 651, loss: 0.291, 20832/60000 datapoints
2025-03-06 18:32:36,889 - INFO - training batch 701, loss: 0.226, 22432/60000 datapoints
2025-03-06 18:32:37,082 - INFO - training batch 751, loss: 0.125, 24032/60000 datapoints
2025-03-06 18:32:37,276 - INFO - training batch 801, loss: 0.082, 25632/60000 datapoints
2025-03-06 18:32:37,474 - INFO - training batch 851, loss: 0.259, 27232/60000 datapoints
2025-03-06 18:32:37,690 - INFO - training batch 901, loss: 0.075, 28832/60000 datapoints
2025-03-06 18:32:37,886 - INFO - training batch 951, loss: 0.316, 30432/60000 datapoints
2025-03-06 18:32:38,083 - INFO - training batch 1001, loss: 0.128, 32032/60000 datapoints
2025-03-06 18:32:38,278 - INFO - training batch 1051, loss: 0.117, 33632/60000 datapoints
2025-03-06 18:32:38,471 - INFO - training batch 1101, loss: 0.259, 35232/60000 datapoints
2025-03-06 18:32:38,668 - INFO - training batch 1151, loss: 0.150, 36832/60000 datapoints
2025-03-06 18:32:38,862 - INFO - training batch 1201, loss: 0.113, 38432/60000 datapoints
2025-03-06 18:32:39,059 - INFO - training batch 1251, loss: 0.218, 40032/60000 datapoints
2025-03-06 18:32:39,251 - INFO - training batch 1301, loss: 0.282, 41632/60000 datapoints
2025-03-06 18:32:39,444 - INFO - training batch 1351, loss: 0.319, 43232/60000 datapoints
2025-03-06 18:32:39,639 - INFO - training batch 1401, loss: 0.284, 44832/60000 datapoints
2025-03-06 18:32:39,831 - INFO - training batch 1451, loss: 0.353, 46432/60000 datapoints
2025-03-06 18:32:40,030 - INFO - training batch 1501, loss: 0.292, 48032/60000 datapoints
2025-03-06 18:32:40,224 - INFO - training batch 1551, loss: 0.248, 49632/60000 datapoints
2025-03-06 18:32:40,419 - INFO - training batch 1601, loss: 0.198, 51232/60000 datapoints
2025-03-06 18:32:40,616 - INFO - training batch 1651, loss: 0.457, 52832/60000 datapoints
2025-03-06 18:32:40,810 - INFO - training batch 1701, loss: 0.328, 54432/60000 datapoints
2025-03-06 18:32:41,004 - INFO - training batch 1751, loss: 0.455, 56032/60000 datapoints
2025-03-06 18:32:41,198 - INFO - training batch 1801, loss: 0.449, 57632/60000 datapoints
2025-03-06 18:32:41,397 - INFO - training batch 1851, loss: 0.390, 59232/60000 datapoints
2025-03-06 18:32:41,498 - INFO - validation batch 1, loss: 0.161, 32/10016 datapoints
2025-03-06 18:32:41,654 - INFO - validation batch 51, loss: 0.310, 1632/10016 datapoints
2025-03-06 18:32:41,805 - INFO - validation batch 101, loss: 0.209, 3232/10016 datapoints
2025-03-06 18:32:41,962 - INFO - validation batch 151, loss: 0.199, 4832/10016 datapoints
2025-03-06 18:32:42,113 - INFO - validation batch 201, loss: 0.082, 6432/10016 datapoints
2025-03-06 18:32:42,270 - INFO - validation batch 251, loss: 0.302, 8032/10016 datapoints
2025-03-06 18:32:42,425 - INFO - validation batch 301, loss: 0.262, 9632/10016 datapoints
2025-03-06 18:32:42,462 - INFO - Epoch 350/800 done.
2025-03-06 18:32:42,462 - INFO - Final validation performance:
Loss: 0.218, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 18:32:42,462 - INFO - Beginning epoch 351/800
2025-03-06 18:32:42,468 - INFO - training batch 1, loss: 0.183, 32/60000 datapoints
2025-03-06 18:32:42,667 - INFO - training batch 51, loss: 0.244, 1632/60000 datapoints
2025-03-06 18:32:42,859 - INFO - training batch 101, loss: 0.187, 3232/60000 datapoints
2025-03-06 18:32:43,105 - INFO - training batch 151, loss: 0.338, 4832/60000 datapoints
2025-03-06 18:32:43,301 - INFO - training batch 201, loss: 0.143, 6432/60000 datapoints
2025-03-06 18:32:43,520 - INFO - training batch 251, loss: 0.317, 8032/60000 datapoints
2025-03-06 18:32:43,716 - INFO - training batch 301, loss: 0.454, 9632/60000 datapoints
2025-03-06 18:32:43,908 - INFO - training batch 351, loss: 0.378, 11232/60000 datapoints
2025-03-06 18:32:44,103 - INFO - training batch 401, loss: 0.095, 12832/60000 datapoints
2025-03-06 18:32:44,294 - INFO - training batch 451, loss: 0.144, 14432/60000 datapoints
2025-03-06 18:32:44,509 - INFO - training batch 501, loss: 0.219, 16032/60000 datapoints
2025-03-06 18:32:44,709 - INFO - training batch 551, loss: 0.341, 17632/60000 datapoints
2025-03-06 18:32:44,904 - INFO - training batch 601, loss: 0.293, 19232/60000 datapoints
2025-03-06 18:32:45,096 - INFO - training batch 651, loss: 0.183, 20832/60000 datapoints
2025-03-06 18:32:45,286 - INFO - training batch 701, loss: 0.244, 22432/60000 datapoints
2025-03-06 18:32:45,478 - INFO - training batch 751, loss: 0.224, 24032/60000 datapoints
2025-03-06 18:32:45,671 - INFO - training batch 801, loss: 0.298, 25632/60000 datapoints
2025-03-06 18:32:45,861 - INFO - training batch 851, loss: 0.463, 27232/60000 datapoints
2025-03-06 18:32:46,057 - INFO - training batch 901, loss: 0.343, 28832/60000 datapoints
2025-03-06 18:32:46,257 - INFO - training batch 951, loss: 0.238, 30432/60000 datapoints
2025-03-06 18:32:46,450 - INFO - training batch 1001, loss: 0.170, 32032/60000 datapoints
2025-03-06 18:32:46,646 - INFO - training batch 1051, loss: 0.237, 33632/60000 datapoints
2025-03-06 18:32:46,838 - INFO - training batch 1101, loss: 0.321, 35232/60000 datapoints
2025-03-06 18:32:47,029 - INFO - training batch 1151, loss: 0.310, 36832/60000 datapoints
2025-03-06 18:32:47,220 - INFO - training batch 1201, loss: 0.444, 38432/60000 datapoints
2025-03-06 18:32:47,413 - INFO - training batch 1251, loss: 0.271, 40032/60000 datapoints
2025-03-06 18:32:47,606 - INFO - training batch 1301, loss: 0.167, 41632/60000 datapoints
2025-03-06 18:32:47,798 - INFO - training batch 1351, loss: 0.216, 43232/60000 datapoints
2025-03-06 18:32:48,000 - INFO - training batch 1401, loss: 0.295, 44832/60000 datapoints
2025-03-06 18:32:48,200 - INFO - training batch 1451, loss: 0.287, 46432/60000 datapoints
2025-03-06 18:32:48,392 - INFO - training batch 1501, loss: 0.450, 48032/60000 datapoints
2025-03-06 18:32:48,583 - INFO - training batch 1551, loss: 0.205, 49632/60000 datapoints
2025-03-06 18:32:48,777 - INFO - training batch 1601, loss: 0.198, 51232/60000 datapoints
2025-03-06 18:32:48,973 - INFO - training batch 1651, loss: 0.225, 52832/60000 datapoints
2025-03-06 18:32:49,166 - INFO - training batch 1701, loss: 0.132, 54432/60000 datapoints
2025-03-06 18:32:49,356 - INFO - training batch 1751, loss: 0.403, 56032/60000 datapoints
2025-03-06 18:32:49,549 - INFO - training batch 1801, loss: 0.207, 57632/60000 datapoints
2025-03-06 18:32:49,755 - INFO - training batch 1851, loss: 0.153, 59232/60000 datapoints
2025-03-06 18:32:49,858 - INFO - validation batch 1, loss: 0.338, 32/10016 datapoints
2025-03-06 18:32:50,010 - INFO - validation batch 51, loss: 0.155, 1632/10016 datapoints
2025-03-06 18:32:50,160 - INFO - validation batch 101, loss: 0.434, 3232/10016 datapoints
2025-03-06 18:32:50,310 - INFO - validation batch 151, loss: 0.147, 4832/10016 datapoints
2025-03-06 18:32:50,461 - INFO - validation batch 201, loss: 0.320, 6432/10016 datapoints
2025-03-06 18:32:50,622 - INFO - validation batch 251, loss: 0.190, 8032/10016 datapoints
2025-03-06 18:32:50,781 - INFO - validation batch 301, loss: 0.123, 9632/10016 datapoints
2025-03-06 18:32:50,816 - INFO - Epoch 351/800 done.
2025-03-06 18:32:50,817 - INFO - Final validation performance:
Loss: 0.244, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 18:32:50,817 - INFO - Beginning epoch 352/800
2025-03-06 18:32:50,823 - INFO - training batch 1, loss: 0.171, 32/60000 datapoints
2025-03-06 18:32:51,017 - INFO - training batch 51, loss: 0.293, 1632/60000 datapoints
2025-03-06 18:32:51,209 - INFO - training batch 101, loss: 0.179, 3232/60000 datapoints
2025-03-06 18:32:51,402 - INFO - training batch 151, loss: 0.382, 4832/60000 datapoints
2025-03-06 18:32:51,595 - INFO - training batch 201, loss: 0.244, 6432/60000 datapoints
2025-03-06 18:32:51,791 - INFO - training batch 251, loss: 0.404, 8032/60000 datapoints
2025-03-06 18:32:51,989 - INFO - training batch 301, loss: 0.231, 9632/60000 datapoints
2025-03-06 18:32:52,183 - INFO - training batch 351, loss: 0.391, 11232/60000 datapoints
2025-03-06 18:32:52,374 - INFO - training batch 401, loss: 0.204, 12832/60000 datapoints
2025-03-06 18:32:52,568 - INFO - training batch 451, loss: 0.263, 14432/60000 datapoints
2025-03-06 18:32:52,761 - INFO - training batch 501, loss: 0.556, 16032/60000 datapoints
2025-03-06 18:32:52,954 - INFO - training batch 551, loss: 0.130, 17632/60000 datapoints
2025-03-06 18:32:53,146 - INFO - training batch 601, loss: 0.384, 19232/60000 datapoints
2025-03-06 18:32:53,336 - INFO - training batch 651, loss: 0.284, 20832/60000 datapoints
2025-03-06 18:32:53,547 - INFO - training batch 701, loss: 0.237, 22432/60000 datapoints
2025-03-06 18:32:53,742 - INFO - training batch 751, loss: 0.164, 24032/60000 datapoints
2025-03-06 18:32:53,933 - INFO - training batch 801, loss: 0.186, 25632/60000 datapoints
2025-03-06 18:32:54,131 - INFO - training batch 851, loss: 0.263, 27232/60000 datapoints
2025-03-06 18:32:54,440 - INFO - training batch 901, loss: 0.193, 28832/60000 datapoints
2025-03-06 18:32:55,008 - INFO - training batch 951, loss: 0.236, 30432/60000 datapoints
2025-03-06 18:32:55,403 - INFO - training batch 1001, loss: 0.358, 32032/60000 datapoints
2025-03-06 18:32:55,824 - INFO - training batch 1051, loss: 0.340, 33632/60000 datapoints
2025-03-06 18:32:56,952 - INFO - training batch 1101, loss: 0.107, 35232/60000 datapoints
2025-03-06 18:32:57,494 - INFO - training batch 1151, loss: 0.208, 36832/60000 datapoints
2025-03-06 18:32:58,098 - INFO - training batch 1201, loss: 0.302, 38432/60000 datapoints
2025-03-06 18:32:58,867 - INFO - training batch 1251, loss: 0.270, 40032/60000 datapoints
2025-03-06 18:32:59,225 - INFO - training batch 1301, loss: 0.231, 41632/60000 datapoints
2025-03-06 18:32:59,569 - INFO - training batch 1351, loss: 0.346, 43232/60000 datapoints
2025-03-06 18:32:59,919 - INFO - training batch 1401, loss: 0.310, 44832/60000 datapoints
2025-03-06 18:33:00,288 - INFO - training batch 1451, loss: 0.303, 46432/60000 datapoints
2025-03-06 18:33:00,635 - INFO - training batch 1501, loss: 0.598, 48032/60000 datapoints
2025-03-06 18:33:00,949 - INFO - training batch 1551, loss: 0.232, 49632/60000 datapoints
2025-03-06 18:33:01,309 - INFO - training batch 1601, loss: 0.317, 51232/60000 datapoints
2025-03-06 18:33:01,841 - INFO - training batch 1651, loss: 0.279, 52832/60000 datapoints
2025-03-06 18:33:02,188 - INFO - training batch 1701, loss: 0.142, 54432/60000 datapoints
2025-03-06 18:33:02,605 - INFO - training batch 1751, loss: 0.174, 56032/60000 datapoints
2025-03-06 18:33:02,926 - INFO - training batch 1801, loss: 0.178, 57632/60000 datapoints
2025-03-06 18:33:03,245 - INFO - training batch 1851, loss: 0.536, 59232/60000 datapoints
2025-03-06 18:33:03,416 - INFO - validation batch 1, loss: 0.423, 32/10016 datapoints
2025-03-06 18:33:03,797 - INFO - validation batch 51, loss: 0.313, 1632/10016 datapoints
2025-03-06 18:33:04,202 - INFO - validation batch 101, loss: 0.196, 3232/10016 datapoints
2025-03-06 18:33:04,617 - INFO - validation batch 151, loss: 0.265, 4832/10016 datapoints
2025-03-06 18:33:05,343 - INFO - validation batch 201, loss: 0.255, 6432/10016 datapoints
2025-03-06 18:33:05,996 - INFO - validation batch 251, loss: 0.416, 8032/10016 datapoints
2025-03-06 18:33:06,332 - INFO - validation batch 301, loss: 0.490, 9632/10016 datapoints
2025-03-06 18:33:06,404 - INFO - Epoch 352/800 done.
2025-03-06 18:33:06,405 - INFO - Final validation performance:
Loss: 0.337, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 18:33:06,406 - INFO - Beginning epoch 353/800
2025-03-06 18:33:06,415 - INFO - training batch 1, loss: 0.081, 32/60000 datapoints
2025-03-06 18:33:06,813 - INFO - training batch 51, loss: 0.337, 1632/60000 datapoints
2025-03-06 18:33:07,125 - INFO - training batch 101, loss: 0.293, 3232/60000 datapoints
2025-03-06 18:33:07,445 - INFO - training batch 151, loss: 0.131, 4832/60000 datapoints
2025-03-06 18:33:07,779 - INFO - training batch 201, loss: 0.242, 6432/60000 datapoints
2025-03-06 18:33:08,105 - INFO - training batch 251, loss: 0.367, 8032/60000 datapoints
2025-03-06 18:33:08,423 - INFO - training batch 301, loss: 0.335, 9632/60000 datapoints
2025-03-06 18:33:08,740 - INFO - training batch 351, loss: 0.555, 11232/60000 datapoints
2025-03-06 18:33:09,057 - INFO - training batch 401, loss: 0.363, 12832/60000 datapoints
2025-03-06 18:33:09,366 - INFO - training batch 451, loss: 0.370, 14432/60000 datapoints
2025-03-06 18:33:09,684 - INFO - training batch 501, loss: 0.104, 16032/60000 datapoints
2025-03-06 18:33:09,997 - INFO - training batch 551, loss: 0.194, 17632/60000 datapoints
2025-03-06 18:33:10,469 - INFO - training batch 601, loss: 0.302, 19232/60000 datapoints
2025-03-06 18:33:10,896 - INFO - training batch 651, loss: 0.319, 20832/60000 datapoints
2025-03-06 18:33:11,289 - INFO - training batch 701, loss: 0.497, 22432/60000 datapoints
2025-03-06 18:33:11,675 - INFO - training batch 751, loss: 0.314, 24032/60000 datapoints
2025-03-06 18:33:12,221 - INFO - training batch 801, loss: 0.258, 25632/60000 datapoints
2025-03-06 18:33:12,615 - INFO - training batch 851, loss: 0.356, 27232/60000 datapoints
2025-03-06 18:33:12,917 - INFO - training batch 901, loss: 0.221, 28832/60000 datapoints
2025-03-06 18:33:13,278 - INFO - training batch 951, loss: 0.214, 30432/60000 datapoints
2025-03-06 18:33:13,627 - INFO - training batch 1001, loss: 0.191, 32032/60000 datapoints
2025-03-06 18:33:13,948 - INFO - training batch 1051, loss: 0.331, 33632/60000 datapoints
2025-03-06 18:33:14,319 - INFO - training batch 1101, loss: 0.185, 35232/60000 datapoints
2025-03-06 18:33:14,687 - INFO - training batch 1151, loss: 0.362, 36832/60000 datapoints
2025-03-06 18:33:15,049 - INFO - training batch 1201, loss: 0.155, 38432/60000 datapoints
2025-03-06 18:33:15,380 - INFO - training batch 1251, loss: 0.332, 40032/60000 datapoints
2025-03-06 18:33:15,730 - INFO - training batch 1301, loss: 0.240, 41632/60000 datapoints
2025-03-06 18:33:16,023 - INFO - training batch 1351, loss: 0.365, 43232/60000 datapoints
2025-03-06 18:33:16,328 - INFO - training batch 1401, loss: 0.169, 44832/60000 datapoints
2025-03-06 18:33:16,684 - INFO - training batch 1451, loss: 0.466, 46432/60000 datapoints
2025-03-06 18:33:16,908 - INFO - training batch 1501, loss: 0.157, 48032/60000 datapoints
2025-03-06 18:33:17,126 - INFO - training batch 1551, loss: 0.462, 49632/60000 datapoints
2025-03-06 18:33:17,359 - INFO - training batch 1601, loss: 0.200, 51232/60000 datapoints
2025-03-06 18:33:17,596 - INFO - training batch 1651, loss: 0.274, 52832/60000 datapoints
2025-03-06 18:33:17,877 - INFO - training batch 1701, loss: 0.217, 54432/60000 datapoints
2025-03-06 18:33:18,115 - INFO - training batch 1751, loss: 0.198, 56032/60000 datapoints
2025-03-06 18:33:18,354 - INFO - training batch 1801, loss: 0.181, 57632/60000 datapoints
2025-03-06 18:33:18,605 - INFO - training batch 1851, loss: 0.327, 59232/60000 datapoints
2025-03-06 18:33:18,722 - INFO - validation batch 1, loss: 0.207, 32/10016 datapoints
2025-03-06 18:33:18,914 - INFO - validation batch 51, loss: 0.309, 1632/10016 datapoints
2025-03-06 18:33:19,154 - INFO - validation batch 101, loss: 0.252, 3232/10016 datapoints
2025-03-06 18:33:19,378 - INFO - validation batch 151, loss: 0.289, 4832/10016 datapoints
2025-03-06 18:33:19,590 - INFO - validation batch 201, loss: 0.300, 6432/10016 datapoints
2025-03-06 18:33:19,982 - INFO - validation batch 251, loss: 0.190, 8032/10016 datapoints
2025-03-06 18:33:20,185 - INFO - validation batch 301, loss: 0.244, 9632/10016 datapoints
2025-03-06 18:33:20,236 - INFO - Epoch 353/800 done.
2025-03-06 18:33:20,236 - INFO - Final validation performance:
Loss: 0.256, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 18:33:20,237 - INFO - Beginning epoch 354/800
2025-03-06 18:33:20,243 - INFO - training batch 1, loss: 0.335, 32/60000 datapoints
2025-03-06 18:33:20,488 - INFO - training batch 51, loss: 0.132, 1632/60000 datapoints
2025-03-06 18:33:20,753 - INFO - training batch 101, loss: 0.155, 3232/60000 datapoints
2025-03-06 18:33:21,017 - INFO - training batch 151, loss: 0.148, 4832/60000 datapoints
2025-03-06 18:33:21,273 - INFO - training batch 201, loss: 0.215, 6432/60000 datapoints
2025-03-06 18:33:21,531 - INFO - training batch 251, loss: 0.426, 8032/60000 datapoints
2025-03-06 18:33:21,783 - INFO - training batch 301, loss: 0.380, 9632/60000 datapoints
2025-03-06 18:33:22,011 - INFO - training batch 351, loss: 0.306, 11232/60000 datapoints
2025-03-06 18:33:22,284 - INFO - training batch 401, loss: 0.166, 12832/60000 datapoints
2025-03-06 18:33:22,498 - INFO - training batch 451, loss: 0.433, 14432/60000 datapoints
2025-03-06 18:33:22,752 - INFO - training batch 501, loss: 0.135, 16032/60000 datapoints
2025-03-06 18:33:23,002 - INFO - training batch 551, loss: 0.406, 17632/60000 datapoints
2025-03-06 18:33:23,212 - INFO - training batch 601, loss: 0.063, 19232/60000 datapoints
2025-03-06 18:33:24,147 - INFO - training batch 651, loss: 0.375, 20832/60000 datapoints
2025-03-06 18:33:24,393 - INFO - training batch 701, loss: 0.251, 22432/60000 datapoints
2025-03-06 18:33:24,619 - INFO - training batch 751, loss: 0.320, 24032/60000 datapoints
2025-03-06 18:33:24,843 - INFO - training batch 801, loss: 0.272, 25632/60000 datapoints
2025-03-06 18:33:25,060 - INFO - training batch 851, loss: 0.187, 27232/60000 datapoints
2025-03-06 18:33:25,282 - INFO - training batch 901, loss: 0.212, 28832/60000 datapoints
2025-03-06 18:33:25,503 - INFO - training batch 951, loss: 0.263, 30432/60000 datapoints
2025-03-06 18:33:25,747 - INFO - training batch 1001, loss: 0.216, 32032/60000 datapoints
2025-03-06 18:33:25,980 - INFO - training batch 1051, loss: 0.582, 33632/60000 datapoints
2025-03-06 18:33:26,230 - INFO - training batch 1101, loss: 0.304, 35232/60000 datapoints
2025-03-06 18:33:26,454 - INFO - training batch 1151, loss: 0.254, 36832/60000 datapoints
2025-03-06 18:33:26,698 - INFO - training batch 1201, loss: 0.314, 38432/60000 datapoints
2025-03-06 18:33:26,967 - INFO - training batch 1251, loss: 0.599, 40032/60000 datapoints
2025-03-06 18:33:27,192 - INFO - training batch 1301, loss: 0.359, 41632/60000 datapoints
2025-03-06 18:33:27,415 - INFO - training batch 1351, loss: 0.201, 43232/60000 datapoints
2025-03-06 18:33:27,657 - INFO - training batch 1401, loss: 0.094, 44832/60000 datapoints
2025-03-06 18:33:27,878 - INFO - training batch 1451, loss: 0.180, 46432/60000 datapoints
2025-03-06 18:33:28,104 - INFO - training batch 1501, loss: 0.238, 48032/60000 datapoints
2025-03-06 18:33:28,325 - INFO - training batch 1551, loss: 0.140, 49632/60000 datapoints
2025-03-06 18:33:28,549 - INFO - training batch 1601, loss: 0.482, 51232/60000 datapoints
2025-03-06 18:33:28,805 - INFO - training batch 1651, loss: 0.231, 52832/60000 datapoints
2025-03-06 18:33:29,028 - INFO - training batch 1701, loss: 0.210, 54432/60000 datapoints
2025-03-06 18:33:29,272 - INFO - training batch 1751, loss: 0.554, 56032/60000 datapoints
2025-03-06 18:33:29,495 - INFO - training batch 1801, loss: 0.387, 57632/60000 datapoints
2025-03-06 18:33:29,715 - INFO - training batch 1851, loss: 0.209, 59232/60000 datapoints
2025-03-06 18:33:29,826 - INFO - validation batch 1, loss: 0.286, 32/10016 datapoints
2025-03-06 18:33:30,001 - INFO - validation batch 51, loss: 0.138, 1632/10016 datapoints
2025-03-06 18:33:30,181 - INFO - validation batch 101, loss: 0.198, 3232/10016 datapoints
2025-03-06 18:33:30,358 - INFO - validation batch 151, loss: 0.142, 4832/10016 datapoints
2025-03-06 18:33:30,528 - INFO - validation batch 201, loss: 0.229, 6432/10016 datapoints
2025-03-06 18:33:30,696 - INFO - validation batch 251, loss: 0.292, 8032/10016 datapoints
2025-03-06 18:33:30,855 - INFO - validation batch 301, loss: 0.847, 9632/10016 datapoints
2025-03-06 18:33:30,894 - INFO - Epoch 354/800 done.
2025-03-06 18:33:30,894 - INFO - Final validation performance:
Loss: 0.304, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 18:33:30,895 - INFO - Beginning epoch 355/800
2025-03-06 18:33:30,901 - INFO - training batch 1, loss: 0.217, 32/60000 datapoints
2025-03-06 18:33:31,123 - INFO - training batch 51, loss: 0.305, 1632/60000 datapoints
2025-03-06 18:33:31,327 - INFO - training batch 101, loss: 0.230, 3232/60000 datapoints
2025-03-06 18:33:31,536 - INFO - training batch 151, loss: 0.115, 4832/60000 datapoints
2025-03-06 18:33:31,749 - INFO - training batch 201, loss: 0.438, 6432/60000 datapoints
2025-03-06 18:33:31,957 - INFO - training batch 251, loss: 0.081, 8032/60000 datapoints
2025-03-06 18:33:32,177 - INFO - training batch 301, loss: 0.351, 9632/60000 datapoints
2025-03-06 18:33:32,382 - INFO - training batch 351, loss: 0.186, 11232/60000 datapoints
2025-03-06 18:33:32,595 - INFO - training batch 401, loss: 0.260, 12832/60000 datapoints
2025-03-06 18:33:32,801 - INFO - training batch 451, loss: 0.244, 14432/60000 datapoints
2025-03-06 18:33:33,005 - INFO - training batch 501, loss: 0.120, 16032/60000 datapoints
2025-03-06 18:33:33,224 - INFO - training batch 551, loss: 0.277, 17632/60000 datapoints
2025-03-06 18:33:33,429 - INFO - training batch 601, loss: 0.148, 19232/60000 datapoints
2025-03-06 18:33:33,647 - INFO - training batch 651, loss: 0.320, 20832/60000 datapoints
2025-03-06 18:33:33,853 - INFO - training batch 701, loss: 0.309, 22432/60000 datapoints
2025-03-06 18:33:34,059 - INFO - training batch 751, loss: 0.617, 24032/60000 datapoints
2025-03-06 18:33:34,311 - INFO - training batch 801, loss: 0.214, 25632/60000 datapoints
2025-03-06 18:33:34,523 - INFO - training batch 851, loss: 0.404, 27232/60000 datapoints
2025-03-06 18:33:34,740 - INFO - training batch 901, loss: 0.194, 28832/60000 datapoints
2025-03-06 18:33:34,952 - INFO - training batch 951, loss: 0.121, 30432/60000 datapoints
2025-03-06 18:33:35,157 - INFO - training batch 1001, loss: 0.402, 32032/60000 datapoints
2025-03-06 18:33:35,366 - INFO - training batch 1051, loss: 0.199, 33632/60000 datapoints
2025-03-06 18:33:35,571 - INFO - training batch 1101, loss: 0.335, 35232/60000 datapoints
2025-03-06 18:33:35,775 - INFO - training batch 1151, loss: 0.362, 36832/60000 datapoints
2025-03-06 18:33:35,985 - INFO - training batch 1201, loss: 0.398, 38432/60000 datapoints
2025-03-06 18:33:36,209 - INFO - training batch 1251, loss: 0.343, 40032/60000 datapoints
2025-03-06 18:33:36,413 - INFO - training batch 1301, loss: 0.086, 41632/60000 datapoints
2025-03-06 18:33:36,620 - INFO - training batch 1351, loss: 0.413, 43232/60000 datapoints
2025-03-06 18:33:36,824 - INFO - training batch 1401, loss: 0.245, 44832/60000 datapoints
2025-03-06 18:33:37,026 - INFO - training batch 1451, loss: 0.275, 46432/60000 datapoints
2025-03-06 18:33:37,234 - INFO - training batch 1501, loss: 0.230, 48032/60000 datapoints
2025-03-06 18:33:37,435 - INFO - training batch 1551, loss: 0.120, 49632/60000 datapoints
2025-03-06 18:33:37,678 - INFO - training batch 1601, loss: 0.241, 51232/60000 datapoints
2025-03-06 18:33:37,895 - INFO - training batch 1651, loss: 0.215, 52832/60000 datapoints
2025-03-06 18:33:38,103 - INFO - training batch 1701, loss: 0.300, 54432/60000 datapoints
2025-03-06 18:33:38,308 - INFO - training batch 1751, loss: 0.493, 56032/60000 datapoints
2025-03-06 18:33:38,511 - INFO - training batch 1801, loss: 0.184, 57632/60000 datapoints
2025-03-06 18:33:38,724 - INFO - training batch 1851, loss: 0.246, 59232/60000 datapoints
2025-03-06 18:33:38,832 - INFO - validation batch 1, loss: 0.176, 32/10016 datapoints
2025-03-06 18:33:38,993 - INFO - validation batch 51, loss: 0.192, 1632/10016 datapoints
2025-03-06 18:33:39,167 - INFO - validation batch 101, loss: 0.121, 3232/10016 datapoints
2025-03-06 18:33:39,333 - INFO - validation batch 151, loss: 0.167, 4832/10016 datapoints
2025-03-06 18:33:39,501 - INFO - validation batch 201, loss: 0.082, 6432/10016 datapoints
2025-03-06 18:33:39,666 - INFO - validation batch 251, loss: 0.364, 8032/10016 datapoints
2025-03-06 18:33:39,824 - INFO - validation batch 301, loss: 0.500, 9632/10016 datapoints
2025-03-06 18:33:39,862 - INFO - Epoch 355/800 done.
2025-03-06 18:33:39,862 - INFO - Final validation performance:
Loss: 0.229, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 18:33:39,863 - INFO - Beginning epoch 356/800
2025-03-06 18:33:39,869 - INFO - training batch 1, loss: 0.182, 32/60000 datapoints
2025-03-06 18:33:40,072 - INFO - training batch 51, loss: 0.115, 1632/60000 datapoints
2025-03-06 18:33:40,295 - INFO - training batch 101, loss: 0.479, 3232/60000 datapoints
2025-03-06 18:33:40,499 - INFO - training batch 151, loss: 0.456, 4832/60000 datapoints
2025-03-06 18:33:40,709 - INFO - training batch 201, loss: 0.661, 6432/60000 datapoints
2025-03-06 18:33:40,913 - INFO - training batch 251, loss: 0.159, 8032/60000 datapoints
2025-03-06 18:33:41,119 - INFO - training batch 301, loss: 0.325, 9632/60000 datapoints
2025-03-06 18:33:41,318 - INFO - training batch 351, loss: 0.410, 11232/60000 datapoints
2025-03-06 18:33:41,522 - INFO - training batch 401, loss: 0.243, 12832/60000 datapoints
2025-03-06 18:33:41,731 - INFO - training batch 451, loss: 0.169, 14432/60000 datapoints
2025-03-06 18:33:41,932 - INFO - training batch 501, loss: 0.263, 16032/60000 datapoints
2025-03-06 18:33:42,141 - INFO - training batch 551, loss: 0.260, 17632/60000 datapoints
2025-03-06 18:33:42,350 - INFO - training batch 601, loss: 0.271, 19232/60000 datapoints
2025-03-06 18:33:42,552 - INFO - training batch 651, loss: 0.268, 20832/60000 datapoints
2025-03-06 18:33:42,763 - INFO - training batch 701, loss: 0.483, 22432/60000 datapoints
2025-03-06 18:33:42,965 - INFO - training batch 751, loss: 0.410, 24032/60000 datapoints
2025-03-06 18:33:43,171 - INFO - training batch 801, loss: 0.260, 25632/60000 datapoints
2025-03-06 18:33:43,374 - INFO - training batch 851, loss: 0.611, 27232/60000 datapoints
2025-03-06 18:33:43,573 - INFO - training batch 901, loss: 0.205, 28832/60000 datapoints
2025-03-06 18:33:43,782 - INFO - training batch 951, loss: 0.233, 30432/60000 datapoints
2025-03-06 18:33:43,983 - INFO - training batch 1001, loss: 0.307, 32032/60000 datapoints
2025-03-06 18:33:44,194 - INFO - training batch 1051, loss: 0.338, 33632/60000 datapoints
2025-03-06 18:33:44,415 - INFO - training batch 1101, loss: 0.281, 35232/60000 datapoints
2025-03-06 18:33:44,618 - INFO - training batch 1151, loss: 0.230, 36832/60000 datapoints
2025-03-06 18:33:44,827 - INFO - training batch 1201, loss: 0.259, 38432/60000 datapoints
2025-03-06 18:33:45,032 - INFO - training batch 1251, loss: 0.273, 40032/60000 datapoints
2025-03-06 18:33:45,238 - INFO - training batch 1301, loss: 0.168, 41632/60000 datapoints
2025-03-06 18:33:45,457 - INFO - training batch 1351, loss: 0.470, 43232/60000 datapoints
2025-03-06 18:33:45,666 - INFO - training batch 1401, loss: 0.542, 44832/60000 datapoints
2025-03-06 18:33:45,873 - INFO - training batch 1451, loss: 0.361, 46432/60000 datapoints
2025-03-06 18:33:46,080 - INFO - training batch 1501, loss: 0.218, 48032/60000 datapoints
2025-03-06 18:33:46,295 - INFO - training batch 1551, loss: 0.310, 49632/60000 datapoints
2025-03-06 18:33:46,498 - INFO - training batch 1601, loss: 0.311, 51232/60000 datapoints
2025-03-06 18:33:46,709 - INFO - training batch 1651, loss: 0.198, 52832/60000 datapoints
2025-03-06 18:33:46,913 - INFO - training batch 1701, loss: 0.241, 54432/60000 datapoints
2025-03-06 18:33:47,113 - INFO - training batch 1751, loss: 0.373, 56032/60000 datapoints
2025-03-06 18:33:47,319 - INFO - training batch 1801, loss: 0.248, 57632/60000 datapoints
2025-03-06 18:33:47,519 - INFO - training batch 1851, loss: 0.371, 59232/60000 datapoints
2025-03-06 18:33:47,627 - INFO - validation batch 1, loss: 0.266, 32/10016 datapoints
2025-03-06 18:33:47,784 - INFO - validation batch 51, loss: 0.503, 1632/10016 datapoints
2025-03-06 18:33:47,946 - INFO - validation batch 101, loss: 0.130, 3232/10016 datapoints
2025-03-06 18:33:48,106 - INFO - validation batch 151, loss: 0.528, 4832/10016 datapoints
2025-03-06 18:33:48,279 - INFO - validation batch 201, loss: 0.295, 6432/10016 datapoints
2025-03-06 18:33:48,444 - INFO - validation batch 251, loss: 0.364, 8032/10016 datapoints
2025-03-06 18:33:48,615 - INFO - validation batch 301, loss: 0.166, 9632/10016 datapoints
2025-03-06 18:33:48,654 - INFO - Epoch 356/800 done.
2025-03-06 18:33:48,654 - INFO - Final validation performance:
Loss: 0.322, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 18:33:48,655 - INFO - Beginning epoch 357/800
2025-03-06 18:33:48,664 - INFO - training batch 1, loss: 0.347, 32/60000 datapoints
2025-03-06 18:33:48,872 - INFO - training batch 51, loss: 0.207, 1632/60000 datapoints
2025-03-06 18:33:49,077 - INFO - training batch 101, loss: 0.404, 3232/60000 datapoints
2025-03-06 18:33:49,282 - INFO - training batch 151, loss: 0.330, 4832/60000 datapoints
2025-03-06 18:33:49,490 - INFO - training batch 201, loss: 0.442, 6432/60000 datapoints
2025-03-06 18:33:49,695 - INFO - training batch 251, loss: 0.235, 8032/60000 datapoints
2025-03-06 18:33:49,904 - INFO - training batch 301, loss: 0.239, 9632/60000 datapoints
2025-03-06 18:33:50,104 - INFO - training batch 351, loss: 0.250, 11232/60000 datapoints
2025-03-06 18:33:50,309 - INFO - training batch 401, loss: 0.295, 12832/60000 datapoints
2025-03-06 18:33:50,512 - INFO - training batch 451, loss: 0.194, 14432/60000 datapoints
2025-03-06 18:33:50,724 - INFO - training batch 501, loss: 0.401, 16032/60000 datapoints
2025-03-06 18:33:50,935 - INFO - training batch 551, loss: 0.292, 17632/60000 datapoints
2025-03-06 18:33:51,144 - INFO - training batch 601, loss: 0.279, 19232/60000 datapoints
2025-03-06 18:33:51,353 - INFO - training batch 651, loss: 0.370, 20832/60000 datapoints
2025-03-06 18:33:51,558 - INFO - training batch 701, loss: 0.222, 22432/60000 datapoints
2025-03-06 18:33:51,771 - INFO - training batch 751, loss: 0.656, 24032/60000 datapoints
2025-03-06 18:33:51,978 - INFO - training batch 801, loss: 0.196, 25632/60000 datapoints
2025-03-06 18:33:52,197 - INFO - training batch 851, loss: 0.211, 27232/60000 datapoints
2025-03-06 18:33:52,401 - INFO - training batch 901, loss: 0.242, 28832/60000 datapoints
2025-03-06 18:33:52,610 - INFO - training batch 951, loss: 0.194, 30432/60000 datapoints
2025-03-06 18:33:52,815 - INFO - training batch 1001, loss: 0.256, 32032/60000 datapoints
2025-03-06 18:33:53,028 - INFO - training batch 1051, loss: 0.195, 33632/60000 datapoints
2025-03-06 18:33:53,229 - INFO - training batch 1101, loss: 0.254, 35232/60000 datapoints
2025-03-06 18:33:53,440 - INFO - training batch 1151, loss: 0.059, 36832/60000 datapoints
2025-03-06 18:33:53,652 - INFO - training batch 1201, loss: 0.449, 38432/60000 datapoints
2025-03-06 18:33:53,868 - INFO - training batch 1251, loss: 0.143, 40032/60000 datapoints
2025-03-06 18:33:54,081 - INFO - training batch 1301, loss: 0.503, 41632/60000 datapoints
2025-03-06 18:33:54,314 - INFO - training batch 1351, loss: 0.482, 43232/60000 datapoints
2025-03-06 18:33:54,554 - INFO - training batch 1401, loss: 0.323, 44832/60000 datapoints
2025-03-06 18:33:54,773 - INFO - training batch 1451, loss: 0.159, 46432/60000 datapoints
2025-03-06 18:33:54,998 - INFO - training batch 1501, loss: 0.191, 48032/60000 datapoints
2025-03-06 18:33:55,215 - INFO - training batch 1551, loss: 0.373, 49632/60000 datapoints
2025-03-06 18:33:55,432 - INFO - training batch 1601, loss: 0.410, 51232/60000 datapoints
2025-03-06 18:33:55,650 - INFO - training batch 1651, loss: 0.229, 52832/60000 datapoints
2025-03-06 18:33:55,868 - INFO - training batch 1701, loss: 0.260, 54432/60000 datapoints
2025-03-06 18:33:56,086 - INFO - training batch 1751, loss: 0.311, 56032/60000 datapoints
2025-03-06 18:33:56,301 - INFO - training batch 1801, loss: 0.390, 57632/60000 datapoints
2025-03-06 18:33:56,522 - INFO - training batch 1851, loss: 0.261, 59232/60000 datapoints
2025-03-06 18:33:56,631 - INFO - validation batch 1, loss: 0.206, 32/10016 datapoints
2025-03-06 18:33:56,798 - INFO - validation batch 51, loss: 0.203, 1632/10016 datapoints
2025-03-06 18:33:57,004 - INFO - validation batch 101, loss: 0.247, 3232/10016 datapoints
2025-03-06 18:33:57,195 - INFO - validation batch 151, loss: 0.291, 4832/10016 datapoints
2025-03-06 18:33:57,525 - INFO - validation batch 201, loss: 0.140, 6432/10016 datapoints
2025-03-06 18:33:57,871 - INFO - validation batch 251, loss: 0.541, 8032/10016 datapoints
2025-03-06 18:33:58,109 - INFO - validation batch 301, loss: 0.503, 9632/10016 datapoints
2025-03-06 18:33:58,171 - INFO - Epoch 357/800 done.
2025-03-06 18:33:58,171 - INFO - Final validation performance:
Loss: 0.304, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 18:33:58,172 - INFO - Beginning epoch 358/800
2025-03-06 18:33:58,178 - INFO - training batch 1, loss: 0.194, 32/60000 datapoints
2025-03-06 18:33:58,415 - INFO - training batch 51, loss: 0.474, 1632/60000 datapoints
2025-03-06 18:33:58,709 - INFO - training batch 101, loss: 0.079, 3232/60000 datapoints
2025-03-06 18:33:58,955 - INFO - training batch 151, loss: 0.183, 4832/60000 datapoints
2025-03-06 18:33:59,198 - INFO - training batch 201, loss: 0.429, 6432/60000 datapoints
2025-03-06 18:33:59,443 - INFO - training batch 251, loss: 0.155, 8032/60000 datapoints
2025-03-06 18:33:59,673 - INFO - training batch 301, loss: 0.178, 9632/60000 datapoints
2025-03-06 18:33:59,899 - INFO - training batch 351, loss: 0.078, 11232/60000 datapoints
2025-03-06 18:34:00,130 - INFO - training batch 401, loss: 0.583, 12832/60000 datapoints
2025-03-06 18:34:00,366 - INFO - training batch 451, loss: 0.394, 14432/60000 datapoints
2025-03-06 18:34:00,594 - INFO - training batch 501, loss: 0.115, 16032/60000 datapoints
2025-03-06 18:34:00,863 - INFO - training batch 551, loss: 0.262, 17632/60000 datapoints
2025-03-06 18:34:01,064 - INFO - training batch 601, loss: 0.245, 19232/60000 datapoints
2025-03-06 18:34:01,285 - INFO - training batch 651, loss: 0.351, 20832/60000 datapoints
2025-03-06 18:34:01,561 - INFO - training batch 701, loss: 0.203, 22432/60000 datapoints
2025-03-06 18:34:01,887 - INFO - training batch 751, loss: 0.481, 24032/60000 datapoints
2025-03-06 18:34:02,142 - INFO - training batch 801, loss: 0.410, 25632/60000 datapoints
2025-03-06 18:34:02,397 - INFO - training batch 851, loss: 0.092, 27232/60000 datapoints
2025-03-06 18:34:03,008 - INFO - training batch 901, loss: 0.267, 28832/60000 datapoints
2025-03-06 18:34:03,548 - INFO - training batch 951, loss: 0.232, 30432/60000 datapoints
2025-03-06 18:34:03,862 - INFO - training batch 1001, loss: 0.082, 32032/60000 datapoints
2025-03-06 18:34:04,192 - INFO - training batch 1051, loss: 0.549, 33632/60000 datapoints
2025-03-06 18:34:04,500 - INFO - training batch 1101, loss: 0.172, 35232/60000 datapoints
2025-03-06 18:34:04,904 - INFO - training batch 1151, loss: 0.257, 36832/60000 datapoints
2025-03-06 18:34:05,151 - INFO - training batch 1201, loss: 0.414, 38432/60000 datapoints
2025-03-06 18:34:05,408 - INFO - training batch 1251, loss: 0.238, 40032/60000 datapoints
2025-03-06 18:34:05,718 - INFO - training batch 1301, loss: 0.347, 41632/60000 datapoints
2025-03-06 18:34:05,992 - INFO - training batch 1351, loss: 0.151, 43232/60000 datapoints
2025-03-06 18:34:06,298 - INFO - training batch 1401, loss: 0.170, 44832/60000 datapoints
2025-03-06 18:34:06,618 - INFO - training batch 1451, loss: 0.092, 46432/60000 datapoints
2025-03-06 18:34:06,894 - INFO - training batch 1501, loss: 0.267, 48032/60000 datapoints
2025-03-06 18:34:07,183 - INFO - training batch 1551, loss: 0.266, 49632/60000 datapoints
2025-03-06 18:34:07,515 - INFO - training batch 1601, loss: 0.230, 51232/60000 datapoints
2025-03-06 18:34:07,850 - INFO - training batch 1651, loss: 0.409, 52832/60000 datapoints
2025-03-06 18:34:08,161 - INFO - training batch 1701, loss: 0.242, 54432/60000 datapoints
2025-03-06 18:34:08,527 - INFO - training batch 1751, loss: 0.143, 56032/60000 datapoints
2025-03-06 18:34:08,802 - INFO - training batch 1801, loss: 0.234, 57632/60000 datapoints
2025-03-06 18:34:09,038 - INFO - training batch 1851, loss: 0.258, 59232/60000 datapoints
2025-03-06 18:34:09,161 - INFO - validation batch 1, loss: 0.498, 32/10016 datapoints
2025-03-06 18:34:09,370 - INFO - validation batch 51, loss: 0.302, 1632/10016 datapoints
2025-03-06 18:34:09,590 - INFO - validation batch 101, loss: 0.341, 3232/10016 datapoints
2025-03-06 18:34:09,818 - INFO - validation batch 151, loss: 0.400, 4832/10016 datapoints
2025-03-06 18:34:10,085 - INFO - validation batch 201, loss: 0.169, 6432/10016 datapoints
2025-03-06 18:34:10,341 - INFO - validation batch 251, loss: 0.105, 8032/10016 datapoints
2025-03-06 18:34:10,551 - INFO - validation batch 301, loss: 0.136, 9632/10016 datapoints
2025-03-06 18:34:10,597 - INFO - Epoch 358/800 done.
2025-03-06 18:34:10,597 - INFO - Final validation performance:
Loss: 0.279, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 18:34:10,598 - INFO - Beginning epoch 359/800
2025-03-06 18:34:10,608 - INFO - training batch 1, loss: 0.354, 32/60000 datapoints
2025-03-06 18:34:10,854 - INFO - training batch 51, loss: 0.343, 1632/60000 datapoints
2025-03-06 18:34:11,083 - INFO - training batch 101, loss: 0.164, 3232/60000 datapoints
2025-03-06 18:34:11,344 - INFO - training batch 151, loss: 0.364, 4832/60000 datapoints
2025-03-06 18:34:11,578 - INFO - training batch 201, loss: 0.752, 6432/60000 datapoints
2025-03-06 18:34:11,799 - INFO - training batch 251, loss: 0.382, 8032/60000 datapoints
2025-03-06 18:34:12,015 - INFO - training batch 301, loss: 0.275, 9632/60000 datapoints
2025-03-06 18:34:12,228 - INFO - training batch 351, loss: 0.187, 11232/60000 datapoints
2025-03-06 18:34:12,471 - INFO - training batch 401, loss: 0.295, 12832/60000 datapoints
2025-03-06 18:34:12,704 - INFO - training batch 451, loss: 0.505, 14432/60000 datapoints
2025-03-06 18:34:12,957 - INFO - training batch 501, loss: 0.453, 16032/60000 datapoints
2025-03-06 18:34:13,169 - INFO - training batch 551, loss: 0.266, 17632/60000 datapoints
2025-03-06 18:34:13,380 - INFO - training batch 601, loss: 0.284, 19232/60000 datapoints
2025-03-06 18:34:13,592 - INFO - training batch 651, loss: 0.278, 20832/60000 datapoints
2025-03-06 18:34:13,806 - INFO - training batch 701, loss: 0.178, 22432/60000 datapoints
2025-03-06 18:34:14,021 - INFO - training batch 751, loss: 0.101, 24032/60000 datapoints
2025-03-06 18:34:14,231 - INFO - training batch 801, loss: 0.305, 25632/60000 datapoints
2025-03-06 18:34:14,436 - INFO - training batch 851, loss: 0.306, 27232/60000 datapoints
2025-03-06 18:34:14,654 - INFO - training batch 901, loss: 0.265, 28832/60000 datapoints
2025-03-06 18:34:14,887 - INFO - training batch 951, loss: 0.571, 30432/60000 datapoints
2025-03-06 18:34:15,094 - INFO - training batch 1001, loss: 0.339, 32032/60000 datapoints
2025-03-06 18:34:15,299 - INFO - training batch 1051, loss: 0.438, 33632/60000 datapoints
2025-03-06 18:34:15,497 - INFO - training batch 1101, loss: 0.148, 35232/60000 datapoints
2025-03-06 18:34:15,702 - INFO - training batch 1151, loss: 0.267, 36832/60000 datapoints
2025-03-06 18:34:15,904 - INFO - training batch 1201, loss: 0.315, 38432/60000 datapoints
2025-03-06 18:34:16,116 - INFO - training batch 1251, loss: 0.580, 40032/60000 datapoints
2025-03-06 18:34:16,340 - INFO - training batch 1301, loss: 0.341, 41632/60000 datapoints
2025-03-06 18:34:16,593 - INFO - training batch 1351, loss: 0.142, 43232/60000 datapoints
2025-03-06 18:34:16,832 - INFO - training batch 1401, loss: 0.557, 44832/60000 datapoints
2025-03-06 18:34:17,078 - INFO - training batch 1451, loss: 0.589, 46432/60000 datapoints
2025-03-06 18:34:17,376 - INFO - training batch 1501, loss: 0.300, 48032/60000 datapoints
2025-03-06 18:34:17,832 - INFO - training batch 1551, loss: 0.252, 49632/60000 datapoints
2025-03-06 18:34:18,186 - INFO - training batch 1601, loss: 0.352, 51232/60000 datapoints
2025-03-06 18:34:18,461 - INFO - training batch 1651, loss: 0.169, 52832/60000 datapoints
2025-03-06 18:34:18,679 - INFO - training batch 1701, loss: 0.134, 54432/60000 datapoints
2025-03-06 18:34:18,893 - INFO - training batch 1751, loss: 0.241, 56032/60000 datapoints
2025-03-06 18:34:19,114 - INFO - training batch 1801, loss: 0.484, 57632/60000 datapoints
2025-03-06 18:34:19,338 - INFO - training batch 1851, loss: 0.364, 59232/60000 datapoints
2025-03-06 18:34:19,454 - INFO - validation batch 1, loss: 0.125, 32/10016 datapoints
2025-03-06 18:34:19,638 - INFO - validation batch 51, loss: 0.114, 1632/10016 datapoints
2025-03-06 18:34:19,859 - INFO - validation batch 101, loss: 0.240, 3232/10016 datapoints
2025-03-06 18:34:20,042 - INFO - validation batch 151, loss: 0.295, 4832/10016 datapoints
2025-03-06 18:34:20,215 - INFO - validation batch 201, loss: 0.245, 6432/10016 datapoints
2025-03-06 18:34:20,503 - INFO - validation batch 251, loss: 0.321, 8032/10016 datapoints
2025-03-06 18:34:20,678 - INFO - validation batch 301, loss: 0.225, 9632/10016 datapoints
2025-03-06 18:34:20,719 - INFO - Epoch 359/800 done.
2025-03-06 18:34:20,719 - INFO - Final validation performance:
Loss: 0.224, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 18:34:20,720 - INFO - Beginning epoch 360/800
2025-03-06 18:34:20,728 - INFO - training batch 1, loss: 0.412, 32/60000 datapoints
2025-03-06 18:34:20,957 - INFO - training batch 51, loss: 0.373, 1632/60000 datapoints
2025-03-06 18:34:21,193 - INFO - training batch 101, loss: 0.477, 3232/60000 datapoints
2025-03-06 18:34:21,407 - INFO - training batch 151, loss: 0.272, 4832/60000 datapoints
2025-03-06 18:34:21,655 - INFO - training batch 201, loss: 0.189, 6432/60000 datapoints
2025-03-06 18:34:22,007 - INFO - training batch 251, loss: 0.516, 8032/60000 datapoints
2025-03-06 18:34:22,249 - INFO - training batch 301, loss: 0.196, 9632/60000 datapoints
2025-03-06 18:34:22,478 - INFO - training batch 351, loss: 0.384, 11232/60000 datapoints
2025-03-06 18:34:22,721 - INFO - training batch 401, loss: 0.295, 12832/60000 datapoints
2025-03-06 18:34:22,966 - INFO - training batch 451, loss: 0.230, 14432/60000 datapoints
2025-03-06 18:34:23,173 - INFO - training batch 501, loss: 0.203, 16032/60000 datapoints
2025-03-06 18:34:23,378 - INFO - training batch 551, loss: 0.278, 17632/60000 datapoints
2025-03-06 18:34:23,756 - INFO - training batch 601, loss: 0.719, 19232/60000 datapoints
2025-03-06 18:34:23,970 - INFO - training batch 651, loss: 0.385, 20832/60000 datapoints
2025-03-06 18:34:24,213 - INFO - training batch 701, loss: 0.643, 22432/60000 datapoints
2025-03-06 18:34:24,442 - INFO - training batch 751, loss: 0.152, 24032/60000 datapoints
2025-03-06 18:34:24,666 - INFO - training batch 801, loss: 0.355, 25632/60000 datapoints
2025-03-06 18:34:24,942 - INFO - training batch 851, loss: 0.401, 27232/60000 datapoints
2025-03-06 18:34:25,293 - INFO - training batch 901, loss: 0.208, 28832/60000 datapoints
2025-03-06 18:34:25,523 - INFO - training batch 951, loss: 0.245, 30432/60000 datapoints
2025-03-06 18:34:25,761 - INFO - training batch 1001, loss: 0.250, 32032/60000 datapoints
2025-03-06 18:34:26,015 - INFO - training batch 1051, loss: 0.451, 33632/60000 datapoints
2025-03-06 18:34:26,243 - INFO - training batch 1101, loss: 0.303, 35232/60000 datapoints
2025-03-06 18:34:26,492 - INFO - training batch 1151, loss: 0.429, 36832/60000 datapoints
2025-03-06 18:34:26,737 - INFO - training batch 1201, loss: 0.256, 38432/60000 datapoints
2025-03-06 18:34:26,981 - INFO - training batch 1251, loss: 0.254, 40032/60000 datapoints
2025-03-06 18:34:27,223 - INFO - training batch 1301, loss: 0.629, 41632/60000 datapoints
2025-03-06 18:34:27,441 - INFO - training batch 1351, loss: 0.298, 43232/60000 datapoints
2025-03-06 18:34:27,671 - INFO - training batch 1401, loss: 0.332, 44832/60000 datapoints
2025-03-06 18:34:27,938 - INFO - training batch 1451, loss: 0.192, 46432/60000 datapoints
2025-03-06 18:34:28,154 - INFO - training batch 1501, loss: 0.137, 48032/60000 datapoints
2025-03-06 18:34:28,374 - INFO - training batch 1551, loss: 0.268, 49632/60000 datapoints
2025-03-06 18:34:28,589 - INFO - training batch 1601, loss: 0.208, 51232/60000 datapoints
2025-03-06 18:34:28,812 - INFO - training batch 1651, loss: 0.247, 52832/60000 datapoints
2025-03-06 18:34:29,034 - INFO - training batch 1701, loss: 0.242, 54432/60000 datapoints
2025-03-06 18:34:29,275 - INFO - training batch 1751, loss: 0.510, 56032/60000 datapoints
2025-03-06 18:34:29,491 - INFO - training batch 1801, loss: 0.138, 57632/60000 datapoints
2025-03-06 18:34:29,701 - INFO - training batch 1851, loss: 0.311, 59232/60000 datapoints
2025-03-06 18:34:29,812 - INFO - validation batch 1, loss: 0.310, 32/10016 datapoints
2025-03-06 18:34:29,975 - INFO - validation batch 51, loss: 0.382, 1632/10016 datapoints
2025-03-06 18:34:30,138 - INFO - validation batch 101, loss: 0.246, 3232/10016 datapoints
2025-03-06 18:34:30,322 - INFO - validation batch 151, loss: 0.152, 4832/10016 datapoints
2025-03-06 18:34:30,505 - INFO - validation batch 201, loss: 0.373, 6432/10016 datapoints
2025-03-06 18:34:30,703 - INFO - validation batch 251, loss: 0.247, 8032/10016 datapoints
2025-03-06 18:34:30,871 - INFO - validation batch 301, loss: 0.324, 9632/10016 datapoints
2025-03-06 18:34:30,912 - INFO - Epoch 360/800 done.
2025-03-06 18:34:30,912 - INFO - Final validation performance:
Loss: 0.291, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 18:34:30,912 - INFO - Beginning epoch 361/800
2025-03-06 18:34:30,920 - INFO - training batch 1, loss: 0.265, 32/60000 datapoints
2025-03-06 18:34:31,142 - INFO - training batch 51, loss: 0.176, 1632/60000 datapoints
2025-03-06 18:34:31,346 - INFO - training batch 101, loss: 0.218, 3232/60000 datapoints
2025-03-06 18:34:31,556 - INFO - training batch 151, loss: 0.079, 4832/60000 datapoints
2025-03-06 18:34:31,774 - INFO - training batch 201, loss: 0.351, 6432/60000 datapoints
2025-03-06 18:34:32,012 - INFO - training batch 251, loss: 0.315, 8032/60000 datapoints
2025-03-06 18:34:32,220 - INFO - training batch 301, loss: 0.433, 9632/60000 datapoints
2025-03-06 18:34:32,443 - INFO - training batch 351, loss: 0.373, 11232/60000 datapoints
2025-03-06 18:34:32,665 - INFO - training batch 401, loss: 0.184, 12832/60000 datapoints
2025-03-06 18:34:32,875 - INFO - training batch 451, loss: 0.182, 14432/60000 datapoints
2025-03-06 18:34:33,164 - INFO - training batch 501, loss: 0.374, 16032/60000 datapoints
2025-03-06 18:34:33,378 - INFO - training batch 551, loss: 0.138, 17632/60000 datapoints
2025-03-06 18:34:33,610 - INFO - training batch 601, loss: 0.338, 19232/60000 datapoints
2025-03-06 18:34:33,819 - INFO - training batch 651, loss: 0.446, 20832/60000 datapoints
2025-03-06 18:34:34,028 - INFO - training batch 701, loss: 0.214, 22432/60000 datapoints
2025-03-06 18:34:34,258 - INFO - training batch 751, loss: 0.158, 24032/60000 datapoints
2025-03-06 18:34:34,502 - INFO - training batch 801, loss: 0.238, 25632/60000 datapoints
2025-03-06 18:34:34,731 - INFO - training batch 851, loss: 0.326, 27232/60000 datapoints
2025-03-06 18:34:34,983 - INFO - training batch 901, loss: 0.549, 28832/60000 datapoints
2025-03-06 18:34:35,358 - INFO - training batch 951, loss: 0.221, 30432/60000 datapoints
2025-03-06 18:34:35,591 - INFO - training batch 1001, loss: 0.328, 32032/60000 datapoints
2025-03-06 18:34:35,812 - INFO - training batch 1051, loss: 0.440, 33632/60000 datapoints
2025-03-06 18:34:36,033 - INFO - training batch 1101, loss: 0.208, 35232/60000 datapoints
2025-03-06 18:34:36,253 - INFO - training batch 1151, loss: 0.297, 36832/60000 datapoints
2025-03-06 18:34:36,475 - INFO - training batch 1201, loss: 0.222, 38432/60000 datapoints
2025-03-06 18:34:36,688 - INFO - training batch 1251, loss: 0.173, 40032/60000 datapoints
2025-03-06 18:34:36,894 - INFO - training batch 1301, loss: 0.213, 41632/60000 datapoints
2025-03-06 18:34:37,102 - INFO - training batch 1351, loss: 0.365, 43232/60000 datapoints
2025-03-06 18:34:37,310 - INFO - training batch 1401, loss: 0.252, 44832/60000 datapoints
2025-03-06 18:34:37,519 - INFO - training batch 1451, loss: 0.162, 46432/60000 datapoints
2025-03-06 18:34:37,779 - INFO - training batch 1501, loss: 0.208, 48032/60000 datapoints
2025-03-06 18:34:38,031 - INFO - training batch 1551, loss: 0.127, 49632/60000 datapoints
2025-03-06 18:34:38,240 - INFO - training batch 1601, loss: 0.137, 51232/60000 datapoints
2025-03-06 18:34:38,452 - INFO - training batch 1651, loss: 0.186, 52832/60000 datapoints
2025-03-06 18:34:38,781 - INFO - training batch 1701, loss: 0.285, 54432/60000 datapoints
2025-03-06 18:34:39,005 - INFO - training batch 1751, loss: 0.395, 56032/60000 datapoints
2025-03-06 18:34:39,208 - INFO - training batch 1801, loss: 0.277, 57632/60000 datapoints
2025-03-06 18:34:39,411 - INFO - training batch 1851, loss: 0.309, 59232/60000 datapoints
2025-03-06 18:34:39,517 - INFO - validation batch 1, loss: 0.384, 32/10016 datapoints
2025-03-06 18:34:39,690 - INFO - validation batch 51, loss: 0.093, 1632/10016 datapoints
2025-03-06 18:34:39,894 - INFO - validation batch 101, loss: 0.412, 3232/10016 datapoints
2025-03-06 18:34:40,076 - INFO - validation batch 151, loss: 0.130, 4832/10016 datapoints
2025-03-06 18:34:40,237 - INFO - validation batch 201, loss: 0.202, 6432/10016 datapoints
2025-03-06 18:34:40,412 - INFO - validation batch 251, loss: 0.301, 8032/10016 datapoints
2025-03-06 18:34:40,571 - INFO - validation batch 301, loss: 0.240, 9632/10016 datapoints
2025-03-06 18:34:40,613 - INFO - Epoch 361/800 done.
2025-03-06 18:34:40,614 - INFO - Final validation performance:
Loss: 0.252, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 18:34:40,614 - INFO - Beginning epoch 362/800
2025-03-06 18:34:40,621 - INFO - training batch 1, loss: 0.194, 32/60000 datapoints
2025-03-06 18:34:40,823 - INFO - training batch 51, loss: 0.275, 1632/60000 datapoints
2025-03-06 18:34:41,024 - INFO - training batch 101, loss: 0.311, 3232/60000 datapoints
2025-03-06 18:34:41,227 - INFO - training batch 151, loss: 0.262, 4832/60000 datapoints
2025-03-06 18:34:41,429 - INFO - training batch 201, loss: 0.231, 6432/60000 datapoints
2025-03-06 18:34:41,637 - INFO - training batch 251, loss: 0.645, 8032/60000 datapoints
2025-03-06 18:34:41,836 - INFO - training batch 301, loss: 0.593, 9632/60000 datapoints
2025-03-06 18:34:42,036 - INFO - training batch 351, loss: 0.228, 11232/60000 datapoints
2025-03-06 18:34:42,233 - INFO - training batch 401, loss: 0.369, 12832/60000 datapoints
2025-03-06 18:34:42,434 - INFO - training batch 451, loss: 0.506, 14432/60000 datapoints
2025-03-06 18:34:42,635 - INFO - training batch 501, loss: 0.205, 16032/60000 datapoints
2025-03-06 18:34:42,836 - INFO - training batch 551, loss: 0.090, 17632/60000 datapoints
2025-03-06 18:34:43,033 - INFO - training batch 601, loss: 0.154, 19232/60000 datapoints
2025-03-06 18:34:43,230 - INFO - training batch 651, loss: 0.211, 20832/60000 datapoints
2025-03-06 18:34:43,430 - INFO - training batch 701, loss: 0.211, 22432/60000 datapoints
2025-03-06 18:34:43,643 - INFO - training batch 751, loss: 0.278, 24032/60000 datapoints
2025-03-06 18:34:43,847 - INFO - training batch 801, loss: 0.178, 25632/60000 datapoints
2025-03-06 18:34:44,045 - INFO - training batch 851, loss: 0.246, 27232/60000 datapoints
2025-03-06 18:34:44,246 - INFO - training batch 901, loss: 0.247, 28832/60000 datapoints
2025-03-06 18:34:44,445 - INFO - training batch 951, loss: 0.242, 30432/60000 datapoints
2025-03-06 18:34:44,647 - INFO - training batch 1001, loss: 0.331, 32032/60000 datapoints
2025-03-06 18:34:44,847 - INFO - training batch 1051, loss: 0.267, 33632/60000 datapoints
2025-03-06 18:34:45,062 - INFO - training batch 1101, loss: 0.276, 35232/60000 datapoints
2025-03-06 18:34:45,260 - INFO - training batch 1151, loss: 0.067, 36832/60000 datapoints
2025-03-06 18:34:45,459 - INFO - training batch 1201, loss: 0.115, 38432/60000 datapoints
2025-03-06 18:34:45,659 - INFO - training batch 1251, loss: 0.651, 40032/60000 datapoints
2025-03-06 18:34:45,853 - INFO - training batch 1301, loss: 0.388, 41632/60000 datapoints
2025-03-06 18:34:46,055 - INFO - training batch 1351, loss: 0.241, 43232/60000 datapoints
2025-03-06 18:34:46,256 - INFO - training batch 1401, loss: 0.342, 44832/60000 datapoints
2025-03-06 18:34:46,459 - INFO - training batch 1451, loss: 0.407, 46432/60000 datapoints
2025-03-06 18:34:46,663 - INFO - training batch 1501, loss: 0.230, 48032/60000 datapoints
2025-03-06 18:34:46,862 - INFO - training batch 1551, loss: 0.288, 49632/60000 datapoints
2025-03-06 18:34:47,059 - INFO - training batch 1601, loss: 0.052, 51232/60000 datapoints
2025-03-06 18:34:47,254 - INFO - training batch 1651, loss: 0.294, 52832/60000 datapoints
2025-03-06 18:34:47,451 - INFO - training batch 1701, loss: 0.433, 54432/60000 datapoints
2025-03-06 18:34:47,657 - INFO - training batch 1751, loss: 0.384, 56032/60000 datapoints
2025-03-06 18:34:47,854 - INFO - training batch 1801, loss: 0.110, 57632/60000 datapoints
2025-03-06 18:34:48,053 - INFO - training batch 1851, loss: 0.254, 59232/60000 datapoints
2025-03-06 18:34:48,157 - INFO - validation batch 1, loss: 0.292, 32/10016 datapoints
2025-03-06 18:34:48,313 - INFO - validation batch 51, loss: 0.528, 1632/10016 datapoints
2025-03-06 18:34:48,471 - INFO - validation batch 101, loss: 0.153, 3232/10016 datapoints
2025-03-06 18:34:48,632 - INFO - validation batch 151, loss: 0.332, 4832/10016 datapoints
2025-03-06 18:34:48,816 - INFO - validation batch 201, loss: 0.325, 6432/10016 datapoints
2025-03-06 18:34:48,994 - INFO - validation batch 251, loss: 0.264, 8032/10016 datapoints
2025-03-06 18:34:49,192 - INFO - validation batch 301, loss: 0.815, 9632/10016 datapoints
2025-03-06 18:34:49,240 - INFO - Epoch 362/800 done.
2025-03-06 18:34:49,240 - INFO - Final validation performance:
Loss: 0.387, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 18:34:49,240 - INFO - Beginning epoch 363/800
2025-03-06 18:34:49,248 - INFO - training batch 1, loss: 0.232, 32/60000 datapoints
2025-03-06 18:34:49,461 - INFO - training batch 51, loss: 0.208, 1632/60000 datapoints
2025-03-06 18:34:49,667 - INFO - training batch 101, loss: 0.124, 3232/60000 datapoints
2025-03-06 18:34:49,905 - INFO - training batch 151, loss: 0.466, 4832/60000 datapoints
2025-03-06 18:34:50,139 - INFO - training batch 201, loss: 0.400, 6432/60000 datapoints
2025-03-06 18:34:50,355 - INFO - training batch 251, loss: 0.168, 8032/60000 datapoints
2025-03-06 18:34:50,575 - INFO - training batch 301, loss: 0.238, 9632/60000 datapoints
2025-03-06 18:34:50,805 - INFO - training batch 351, loss: 0.331, 11232/60000 datapoints
2025-03-06 18:34:51,026 - INFO - training batch 401, loss: 0.474, 12832/60000 datapoints
2025-03-06 18:34:51,228 - INFO - training batch 451, loss: 0.354, 14432/60000 datapoints
2025-03-06 18:34:51,437 - INFO - training batch 501, loss: 0.139, 16032/60000 datapoints
2025-03-06 18:34:51,666 - INFO - training batch 551, loss: 0.254, 17632/60000 datapoints
2025-03-06 18:34:51,896 - INFO - training batch 601, loss: 0.209, 19232/60000 datapoints
2025-03-06 18:34:52,141 - INFO - training batch 651, loss: 0.447, 20832/60000 datapoints
2025-03-06 18:34:52,368 - INFO - training batch 701, loss: 0.253, 22432/60000 datapoints
2025-03-06 18:34:52,582 - INFO - training batch 751, loss: 0.311, 24032/60000 datapoints
2025-03-06 18:34:52,796 - INFO - training batch 801, loss: 0.092, 25632/60000 datapoints
2025-03-06 18:34:53,020 - INFO - training batch 851, loss: 0.165, 27232/60000 datapoints
2025-03-06 18:34:53,248 - INFO - training batch 901, loss: 0.256, 28832/60000 datapoints
2025-03-06 18:34:53,483 - INFO - training batch 951, loss: 0.090, 30432/60000 datapoints
2025-03-06 18:34:53,726 - INFO - training batch 1001, loss: 0.187, 32032/60000 datapoints
2025-03-06 18:34:53,942 - INFO - training batch 1051, loss: 0.135, 33632/60000 datapoints
2025-03-06 18:34:54,152 - INFO - training batch 1101, loss: 0.150, 35232/60000 datapoints
2025-03-06 18:34:54,380 - INFO - training batch 1151, loss: 0.240, 36832/60000 datapoints
2025-03-06 18:34:54,600 - INFO - training batch 1201, loss: 0.393, 38432/60000 datapoints
2025-03-06 18:34:54,833 - INFO - training batch 1251, loss: 0.234, 40032/60000 datapoints
2025-03-06 18:34:55,088 - INFO - training batch 1301, loss: 0.112, 41632/60000 datapoints
2025-03-06 18:34:55,312 - INFO - training batch 1351, loss: 0.284, 43232/60000 datapoints
2025-03-06 18:34:55,547 - INFO - training batch 1401, loss: 0.249, 44832/60000 datapoints
2025-03-06 18:34:55,838 - INFO - training batch 1451, loss: 0.370, 46432/60000 datapoints
2025-03-06 18:34:56,222 - INFO - training batch 1501, loss: 0.428, 48032/60000 datapoints
2025-03-06 18:34:56,550 - INFO - training batch 1551, loss: 0.387, 49632/60000 datapoints
2025-03-06 18:34:56,774 - INFO - training batch 1601, loss: 0.318, 51232/60000 datapoints
2025-03-06 18:34:57,003 - INFO - training batch 1651, loss: 0.345, 52832/60000 datapoints
2025-03-06 18:34:57,221 - INFO - training batch 1701, loss: 0.170, 54432/60000 datapoints
2025-03-06 18:34:57,428 - INFO - training batch 1751, loss: 0.300, 56032/60000 datapoints
2025-03-06 18:34:57,637 - INFO - training batch 1801, loss: 0.172, 57632/60000 datapoints
2025-03-06 18:34:57,841 - INFO - training batch 1851, loss: 0.183, 59232/60000 datapoints
2025-03-06 18:34:57,951 - INFO - validation batch 1, loss: 0.506, 32/10016 datapoints
2025-03-06 18:34:58,130 - INFO - validation batch 51, loss: 0.189, 1632/10016 datapoints
2025-03-06 18:34:58,312 - INFO - validation batch 101, loss: 0.593, 3232/10016 datapoints
2025-03-06 18:34:58,475 - INFO - validation batch 151, loss: 0.448, 4832/10016 datapoints
2025-03-06 18:34:58,657 - INFO - validation batch 201, loss: 0.408, 6432/10016 datapoints
2025-03-06 18:34:58,824 - INFO - validation batch 251, loss: 0.104, 8032/10016 datapoints
2025-03-06 18:34:58,994 - INFO - validation batch 301, loss: 0.249, 9632/10016 datapoints
2025-03-06 18:34:59,033 - INFO - Epoch 363/800 done.
2025-03-06 18:34:59,033 - INFO - Final validation performance:
Loss: 0.357, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 18:34:59,033 - INFO - Beginning epoch 364/800
2025-03-06 18:34:59,041 - INFO - training batch 1, loss: 0.453, 32/60000 datapoints
2025-03-06 18:34:59,262 - INFO - training batch 51, loss: 0.117, 1632/60000 datapoints
2025-03-06 18:34:59,463 - INFO - training batch 101, loss: 0.391, 3232/60000 datapoints
2025-03-06 18:34:59,670 - INFO - training batch 151, loss: 0.443, 4832/60000 datapoints
2025-03-06 18:34:59,925 - INFO - training batch 201, loss: 0.111, 6432/60000 datapoints
2025-03-06 18:35:00,130 - INFO - training batch 251, loss: 0.360, 8032/60000 datapoints
2025-03-06 18:35:00,337 - INFO - training batch 301, loss: 0.464, 9632/60000 datapoints
2025-03-06 18:35:00,594 - INFO - training batch 351, loss: 0.351, 11232/60000 datapoints
2025-03-06 18:35:00,820 - INFO - training batch 401, loss: 0.425, 12832/60000 datapoints
2025-03-06 18:35:01,023 - INFO - training batch 451, loss: 0.171, 14432/60000 datapoints
2025-03-06 18:35:01,231 - INFO - training batch 501, loss: 0.308, 16032/60000 datapoints
2025-03-06 18:35:01,434 - INFO - training batch 551, loss: 0.164, 17632/60000 datapoints
2025-03-06 18:35:01,678 - INFO - training batch 601, loss: 0.408, 19232/60000 datapoints
2025-03-06 18:35:01,898 - INFO - training batch 651, loss: 0.396, 20832/60000 datapoints
2025-03-06 18:35:02,123 - INFO - training batch 701, loss: 0.070, 22432/60000 datapoints
2025-03-06 18:35:02,357 - INFO - training batch 751, loss: 0.348, 24032/60000 datapoints
2025-03-06 18:35:02,579 - INFO - training batch 801, loss: 0.336, 25632/60000 datapoints
2025-03-06 18:35:02,796 - INFO - training batch 851, loss: 0.445, 27232/60000 datapoints
2025-03-06 18:35:03,017 - INFO - training batch 901, loss: 0.150, 28832/60000 datapoints
2025-03-06 18:35:03,285 - INFO - training batch 951, loss: 0.614, 30432/60000 datapoints
2025-03-06 18:35:03,491 - INFO - training batch 1001, loss: 0.159, 32032/60000 datapoints
2025-03-06 18:35:03,727 - INFO - training batch 1051, loss: 0.317, 33632/60000 datapoints
2025-03-06 18:35:03,937 - INFO - training batch 1101, loss: 0.639, 35232/60000 datapoints
2025-03-06 18:35:04,144 - INFO - training batch 1151, loss: 0.403, 36832/60000 datapoints
2025-03-06 18:35:04,359 - INFO - training batch 1201, loss: 0.200, 38432/60000 datapoints
2025-03-06 18:35:04,562 - INFO - training batch 1251, loss: 0.543, 40032/60000 datapoints
2025-03-06 18:35:04,808 - INFO - training batch 1301, loss: 0.274, 41632/60000 datapoints
2025-03-06 18:35:05,054 - INFO - training batch 1351, loss: 0.171, 43232/60000 datapoints
2025-03-06 18:35:05,276 - INFO - training batch 1401, loss: 0.303, 44832/60000 datapoints
2025-03-06 18:35:05,592 - INFO - training batch 1451, loss: 0.377, 46432/60000 datapoints
2025-03-06 18:35:05,802 - INFO - training batch 1501, loss: 0.140, 48032/60000 datapoints
2025-03-06 18:35:06,160 - INFO - training batch 1551, loss: 0.410, 49632/60000 datapoints
2025-03-06 18:35:06,397 - INFO - training batch 1601, loss: 0.379, 51232/60000 datapoints
2025-03-06 18:35:06,603 - INFO - training batch 1651, loss: 0.475, 52832/60000 datapoints
2025-03-06 18:35:06,811 - INFO - training batch 1701, loss: 0.348, 54432/60000 datapoints
2025-03-06 18:35:07,043 - INFO - training batch 1751, loss: 0.645, 56032/60000 datapoints
2025-03-06 18:35:07,273 - INFO - training batch 1801, loss: 0.242, 57632/60000 datapoints
2025-03-06 18:35:07,472 - INFO - training batch 1851, loss: 0.322, 59232/60000 datapoints
2025-03-06 18:35:07,576 - INFO - validation batch 1, loss: 0.314, 32/10016 datapoints
2025-03-06 18:35:07,736 - INFO - validation batch 51, loss: 0.176, 1632/10016 datapoints
2025-03-06 18:35:07,897 - INFO - validation batch 101, loss: 0.453, 3232/10016 datapoints
2025-03-06 18:35:08,074 - INFO - validation batch 151, loss: 0.409, 4832/10016 datapoints
2025-03-06 18:35:08,254 - INFO - validation batch 201, loss: 0.196, 6432/10016 datapoints
2025-03-06 18:35:08,428 - INFO - validation batch 251, loss: 0.110, 8032/10016 datapoints
2025-03-06 18:35:08,590 - INFO - validation batch 301, loss: 0.222, 9632/10016 datapoints
2025-03-06 18:35:08,631 - INFO - Epoch 364/800 done.
2025-03-06 18:35:08,632 - INFO - Final validation performance:
Loss: 0.269, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 18:35:08,632 - INFO - Beginning epoch 365/800
2025-03-06 18:35:08,638 - INFO - training batch 1, loss: 0.111, 32/60000 datapoints
2025-03-06 18:35:08,852 - INFO - training batch 51, loss: 0.279, 1632/60000 datapoints
2025-03-06 18:35:09,054 - INFO - training batch 101, loss: 0.365, 3232/60000 datapoints
2025-03-06 18:35:09,248 - INFO - training batch 151, loss: 0.106, 4832/60000 datapoints
2025-03-06 18:35:09,448 - INFO - training batch 201, loss: 0.695, 6432/60000 datapoints
2025-03-06 18:35:09,645 - INFO - training batch 251, loss: 0.347, 8032/60000 datapoints
2025-03-06 18:35:09,844 - INFO - training batch 301, loss: 0.258, 9632/60000 datapoints
2025-03-06 18:35:10,040 - INFO - training batch 351, loss: 0.297, 11232/60000 datapoints
2025-03-06 18:35:10,236 - INFO - training batch 401, loss: 0.115, 12832/60000 datapoints
2025-03-06 18:35:10,437 - INFO - training batch 451, loss: 0.158, 14432/60000 datapoints
2025-03-06 18:35:10,637 - INFO - training batch 501, loss: 0.287, 16032/60000 datapoints
2025-03-06 18:35:10,839 - INFO - training batch 551, loss: 0.360, 17632/60000 datapoints
2025-03-06 18:35:11,031 - INFO - training batch 601, loss: 0.534, 19232/60000 datapoints
2025-03-06 18:35:11,225 - INFO - training batch 651, loss: 0.370, 20832/60000 datapoints
2025-03-06 18:35:11,419 - INFO - training batch 701, loss: 0.286, 22432/60000 datapoints
2025-03-06 18:35:11,617 - INFO - training batch 751, loss: 0.319, 24032/60000 datapoints
2025-03-06 18:35:11,814 - INFO - training batch 801, loss: 0.277, 25632/60000 datapoints
2025-03-06 18:35:12,009 - INFO - training batch 851, loss: 0.261, 27232/60000 datapoints
2025-03-06 18:35:12,220 - INFO - training batch 901, loss: 0.369, 28832/60000 datapoints
2025-03-06 18:35:12,420 - INFO - training batch 951, loss: 0.153, 30432/60000 datapoints
2025-03-06 18:35:12,637 - INFO - training batch 1001, loss: 0.230, 32032/60000 datapoints
2025-03-06 18:35:12,846 - INFO - training batch 1051, loss: 0.312, 33632/60000 datapoints
2025-03-06 18:35:13,048 - INFO - training batch 1101, loss: 0.318, 35232/60000 datapoints
2025-03-06 18:35:13,250 - INFO - training batch 1151, loss: 0.231, 36832/60000 datapoints
2025-03-06 18:35:13,657 - INFO - training batch 1201, loss: 0.183, 38432/60000 datapoints
2025-03-06 18:35:13,929 - INFO - training batch 1251, loss: 0.598, 40032/60000 datapoints
2025-03-06 18:35:14,202 - INFO - training batch 1301, loss: 0.216, 41632/60000 datapoints
2025-03-06 18:35:14,433 - INFO - training batch 1351, loss: 0.360, 43232/60000 datapoints
2025-03-06 18:35:14,648 - INFO - training batch 1401, loss: 0.359, 44832/60000 datapoints
2025-03-06 18:35:14,892 - INFO - training batch 1451, loss: 0.235, 46432/60000 datapoints
2025-03-06 18:35:15,138 - INFO - training batch 1501, loss: 0.293, 48032/60000 datapoints
2025-03-06 18:35:15,444 - INFO - training batch 1551, loss: 0.081, 49632/60000 datapoints
2025-03-06 18:35:15,676 - INFO - training batch 1601, loss: 0.456, 51232/60000 datapoints
2025-03-06 18:35:15,894 - INFO - training batch 1651, loss: 0.516, 52832/60000 datapoints
2025-03-06 18:35:16,103 - INFO - training batch 1701, loss: 0.123, 54432/60000 datapoints
2025-03-06 18:35:16,340 - INFO - training batch 1751, loss: 0.838, 56032/60000 datapoints
2025-03-06 18:35:16,572 - INFO - training batch 1801, loss: 0.282, 57632/60000 datapoints
2025-03-06 18:35:16,816 - INFO - training batch 1851, loss: 0.471, 59232/60000 datapoints
2025-03-06 18:35:16,930 - INFO - validation batch 1, loss: 0.349, 32/10016 datapoints
2025-03-06 18:35:17,115 - INFO - validation batch 51, loss: 0.420, 1632/10016 datapoints
2025-03-06 18:35:17,293 - INFO - validation batch 101, loss: 0.105, 3232/10016 datapoints
2025-03-06 18:35:17,475 - INFO - validation batch 151, loss: 0.152, 4832/10016 datapoints
2025-03-06 18:35:17,677 - INFO - validation batch 201, loss: 0.260, 6432/10016 datapoints
2025-03-06 18:35:17,885 - INFO - validation batch 251, loss: 0.132, 8032/10016 datapoints
2025-03-06 18:35:18,051 - INFO - validation batch 301, loss: 0.237, 9632/10016 datapoints
2025-03-06 18:35:18,093 - INFO - Epoch 365/800 done.
2025-03-06 18:35:18,093 - INFO - Final validation performance:
Loss: 0.236, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 18:35:18,093 - INFO - Beginning epoch 366/800
2025-03-06 18:35:18,099 - INFO - training batch 1, loss: 0.293, 32/60000 datapoints
2025-03-06 18:35:18,320 - INFO - training batch 51, loss: 0.312, 1632/60000 datapoints
2025-03-06 18:35:18,571 - INFO - training batch 101, loss: 0.215, 3232/60000 datapoints
2025-03-06 18:35:18,899 - INFO - training batch 151, loss: 0.186, 4832/60000 datapoints
2025-03-06 18:35:19,146 - INFO - training batch 201, loss: 0.153, 6432/60000 datapoints
2025-03-06 18:35:19,412 - INFO - training batch 251, loss: 0.186, 8032/60000 datapoints
2025-03-06 18:35:19,668 - INFO - training batch 301, loss: 0.198, 9632/60000 datapoints
2025-03-06 18:35:20,125 - INFO - training batch 351, loss: 0.473, 11232/60000 datapoints
2025-03-06 18:35:20,629 - INFO - training batch 401, loss: 0.310, 12832/60000 datapoints
2025-03-06 18:35:20,911 - INFO - training batch 451, loss: 0.282, 14432/60000 datapoints
2025-03-06 18:35:21,181 - INFO - training batch 501, loss: 0.548, 16032/60000 datapoints
2025-03-06 18:35:21,417 - INFO - training batch 551, loss: 0.208, 17632/60000 datapoints
2025-03-06 18:35:21,686 - INFO - training batch 601, loss: 0.291, 19232/60000 datapoints
2025-03-06 18:35:21,913 - INFO - training batch 651, loss: 0.326, 20832/60000 datapoints
2025-03-06 18:35:22,131 - INFO - training batch 701, loss: 0.228, 22432/60000 datapoints
2025-03-06 18:35:22,359 - INFO - training batch 751, loss: 0.200, 24032/60000 datapoints
2025-03-06 18:35:22,619 - INFO - training batch 801, loss: 0.113, 25632/60000 datapoints
2025-03-06 18:35:22,837 - INFO - training batch 851, loss: 0.114, 27232/60000 datapoints
2025-03-06 18:35:23,061 - INFO - training batch 901, loss: 0.324, 28832/60000 datapoints
2025-03-06 18:35:23,273 - INFO - training batch 951, loss: 0.336, 30432/60000 datapoints
2025-03-06 18:35:23,485 - INFO - training batch 1001, loss: 0.197, 32032/60000 datapoints
2025-03-06 18:35:23,704 - INFO - training batch 1051, loss: 0.232, 33632/60000 datapoints
2025-03-06 18:35:23,915 - INFO - training batch 1101, loss: 0.366, 35232/60000 datapoints
2025-03-06 18:35:24,116 - INFO - training batch 1151, loss: 0.367, 36832/60000 datapoints
2025-03-06 18:35:24,315 - INFO - training batch 1201, loss: 0.259, 38432/60000 datapoints
2025-03-06 18:35:24,517 - INFO - training batch 1251, loss: 0.268, 40032/60000 datapoints
2025-03-06 18:35:24,719 - INFO - training batch 1301, loss: 0.285, 41632/60000 datapoints
2025-03-06 18:35:24,925 - INFO - training batch 1351, loss: 0.214, 43232/60000 datapoints
2025-03-06 18:35:25,127 - INFO - training batch 1401, loss: 0.217, 44832/60000 datapoints
2025-03-06 18:35:25,343 - INFO - training batch 1451, loss: 0.178, 46432/60000 datapoints
2025-03-06 18:35:25,577 - INFO - training batch 1501, loss: 0.473, 48032/60000 datapoints
2025-03-06 18:35:25,825 - INFO - training batch 1551, loss: 0.608, 49632/60000 datapoints
2025-03-06 18:35:26,055 - INFO - training batch 1601, loss: 0.194, 51232/60000 datapoints
2025-03-06 18:35:26,289 - INFO - training batch 1651, loss: 0.460, 52832/60000 datapoints
2025-03-06 18:35:26,501 - INFO - training batch 1701, loss: 0.221, 54432/60000 datapoints
2025-03-06 18:35:26,710 - INFO - training batch 1751, loss: 0.185, 56032/60000 datapoints
2025-03-06 18:35:26,918 - INFO - training batch 1801, loss: 0.151, 57632/60000 datapoints
2025-03-06 18:35:27,121 - INFO - training batch 1851, loss: 0.333, 59232/60000 datapoints
2025-03-06 18:35:27,230 - INFO - validation batch 1, loss: 0.308, 32/10016 datapoints
2025-03-06 18:35:27,401 - INFO - validation batch 51, loss: 0.362, 1632/10016 datapoints
2025-03-06 18:35:27,588 - INFO - validation batch 101, loss: 0.294, 3232/10016 datapoints
2025-03-06 18:35:27,795 - INFO - validation batch 151, loss: 0.092, 4832/10016 datapoints
2025-03-06 18:35:27,986 - INFO - validation batch 201, loss: 0.302, 6432/10016 datapoints
2025-03-06 18:35:28,144 - INFO - validation batch 251, loss: 0.435, 8032/10016 datapoints
2025-03-06 18:35:28,322 - INFO - validation batch 301, loss: 0.298, 9632/10016 datapoints
2025-03-06 18:35:28,372 - INFO - Epoch 366/800 done.
2025-03-06 18:35:28,372 - INFO - Final validation performance:
Loss: 0.299, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 18:35:28,373 - INFO - Beginning epoch 367/800
2025-03-06 18:35:28,381 - INFO - training batch 1, loss: 0.205, 32/60000 datapoints
2025-03-06 18:35:28,617 - INFO - training batch 51, loss: 0.469, 1632/60000 datapoints
2025-03-06 18:35:28,870 - INFO - training batch 101, loss: 0.230, 3232/60000 datapoints
2025-03-06 18:35:29,157 - INFO - training batch 151, loss: 0.257, 4832/60000 datapoints
2025-03-06 18:35:29,397 - INFO - training batch 201, loss: 0.140, 6432/60000 datapoints
2025-03-06 18:35:29,624 - INFO - training batch 251, loss: 0.221, 8032/60000 datapoints
2025-03-06 18:35:29,896 - INFO - training batch 301, loss: 0.469, 9632/60000 datapoints
2025-03-06 18:35:30,144 - INFO - training batch 351, loss: 0.319, 11232/60000 datapoints
2025-03-06 18:35:30,358 - INFO - training batch 401, loss: 0.179, 12832/60000 datapoints
2025-03-06 18:35:30,584 - INFO - training batch 451, loss: 0.157, 14432/60000 datapoints
2025-03-06 18:35:30,803 - INFO - training batch 501, loss: 0.275, 16032/60000 datapoints
2025-03-06 18:35:31,012 - INFO - training batch 551, loss: 0.113, 17632/60000 datapoints
2025-03-06 18:35:31,228 - INFO - training batch 601, loss: 0.226, 19232/60000 datapoints
2025-03-06 18:35:31,444 - INFO - training batch 651, loss: 0.397, 20832/60000 datapoints
2025-03-06 18:35:31,673 - INFO - training batch 701, loss: 0.321, 22432/60000 datapoints
2025-03-06 18:35:31,885 - INFO - training batch 751, loss: 0.204, 24032/60000 datapoints
2025-03-06 18:35:32,098 - INFO - training batch 801, loss: 0.203, 25632/60000 datapoints
2025-03-06 18:35:32,310 - INFO - training batch 851, loss: 0.353, 27232/60000 datapoints
2025-03-06 18:35:32,522 - INFO - training batch 901, loss: 0.288, 28832/60000 datapoints
2025-03-06 18:35:32,737 - INFO - training batch 951, loss: 0.264, 30432/60000 datapoints
2025-03-06 18:35:32,946 - INFO - training batch 1001, loss: 0.100, 32032/60000 datapoints
2025-03-06 18:35:33,146 - INFO - training batch 1051, loss: 0.274, 33632/60000 datapoints
2025-03-06 18:35:33,348 - INFO - training batch 1101, loss: 0.115, 35232/60000 datapoints
2025-03-06 18:35:33,607 - INFO - training batch 1151, loss: 0.415, 36832/60000 datapoints
2025-03-06 18:35:33,873 - INFO - training batch 1201, loss: 0.111, 38432/60000 datapoints
2025-03-06 18:35:34,130 - INFO - training batch 1251, loss: 0.227, 40032/60000 datapoints
2025-03-06 18:35:34,447 - INFO - training batch 1301, loss: 0.186, 41632/60000 datapoints
2025-03-06 18:35:34,779 - INFO - training batch 1351, loss: 0.228, 43232/60000 datapoints
2025-03-06 18:35:35,082 - INFO - training batch 1401, loss: 0.342, 44832/60000 datapoints
2025-03-06 18:35:35,317 - INFO - training batch 1451, loss: 0.419, 46432/60000 datapoints
2025-03-06 18:35:35,573 - INFO - training batch 1501, loss: 0.314, 48032/60000 datapoints
2025-03-06 18:35:35,874 - INFO - training batch 1551, loss: 0.362, 49632/60000 datapoints
2025-03-06 18:35:36,124 - INFO - training batch 1601, loss: 0.184, 51232/60000 datapoints
2025-03-06 18:35:36,379 - INFO - training batch 1651, loss: 0.084, 52832/60000 datapoints
2025-03-06 18:35:36,661 - INFO - training batch 1701, loss: 0.271, 54432/60000 datapoints
2025-03-06 18:35:36,915 - INFO - training batch 1751, loss: 0.289, 56032/60000 datapoints
2025-03-06 18:35:37,145 - INFO - training batch 1801, loss: 0.077, 57632/60000 datapoints
2025-03-06 18:35:37,364 - INFO - training batch 1851, loss: 0.171, 59232/60000 datapoints
2025-03-06 18:35:37,476 - INFO - validation batch 1, loss: 0.183, 32/10016 datapoints
2025-03-06 18:35:37,664 - INFO - validation batch 51, loss: 0.399, 1632/10016 datapoints
2025-03-06 18:35:37,829 - INFO - validation batch 101, loss: 0.606, 3232/10016 datapoints
2025-03-06 18:35:37,999 - INFO - validation batch 151, loss: 0.115, 4832/10016 datapoints
2025-03-06 18:35:38,163 - INFO - validation batch 201, loss: 0.333, 6432/10016 datapoints
2025-03-06 18:35:38,330 - INFO - validation batch 251, loss: 0.248, 8032/10016 datapoints
2025-03-06 18:35:38,500 - INFO - validation batch 301, loss: 0.214, 9632/10016 datapoints
2025-03-06 18:35:38,545 - INFO - Epoch 367/800 done.
2025-03-06 18:35:38,545 - INFO - Final validation performance:
Loss: 0.300, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 18:35:38,546 - INFO - Beginning epoch 368/800
2025-03-06 18:35:38,553 - INFO - training batch 1, loss: 0.195, 32/60000 datapoints
2025-03-06 18:35:38,795 - INFO - training batch 51, loss: 0.186, 1632/60000 datapoints
2025-03-06 18:35:39,039 - INFO - training batch 101, loss: 0.255, 3232/60000 datapoints
2025-03-06 18:35:39,271 - INFO - training batch 151, loss: 0.476, 4832/60000 datapoints
2025-03-06 18:35:39,514 - INFO - training batch 201, loss: 0.147, 6432/60000 datapoints
2025-03-06 18:35:39,747 - INFO - training batch 251, loss: 0.376, 8032/60000 datapoints
2025-03-06 18:35:39,997 - INFO - training batch 301, loss: 0.258, 9632/60000 datapoints
2025-03-06 18:35:40,238 - INFO - training batch 351, loss: 0.287, 11232/60000 datapoints
2025-03-06 18:35:40,453 - INFO - training batch 401, loss: 0.182, 12832/60000 datapoints
2025-03-06 18:35:40,696 - INFO - training batch 451, loss: 0.245, 14432/60000 datapoints
2025-03-06 18:35:40,925 - INFO - training batch 501, loss: 0.260, 16032/60000 datapoints
2025-03-06 18:35:41,157 - INFO - training batch 551, loss: 0.320, 17632/60000 datapoints
2025-03-06 18:35:41,381 - INFO - training batch 601, loss: 0.175, 19232/60000 datapoints
2025-03-06 18:35:41,604 - INFO - training batch 651, loss: 0.256, 20832/60000 datapoints
2025-03-06 18:35:41,831 - INFO - training batch 701, loss: 0.168, 22432/60000 datapoints
2025-03-06 18:35:42,098 - INFO - training batch 751, loss: 0.167, 24032/60000 datapoints
2025-03-06 18:35:42,299 - INFO - training batch 801, loss: 0.377, 25632/60000 datapoints
2025-03-06 18:35:42,504 - INFO - training batch 851, loss: 0.346, 27232/60000 datapoints
2025-03-06 18:35:42,709 - INFO - training batch 901, loss: 0.339, 28832/60000 datapoints
2025-03-06 18:35:42,910 - INFO - training batch 951, loss: 0.639, 30432/60000 datapoints
2025-03-06 18:35:42,984 - ERROR - Traceback (most recent call last):
2025-03-06 18:35:42,984 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/train_scripts/train_feedforward_mnist.py", line 186, in <module>
    train_losses, val_losses, train_topk_accs, val_topk_accs, state_dicts, train_time = train(model=model,
2025-03-06 18:35:42,985 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 184, in train
    train_losses, train_topk_accs, state_dict = train_one_epoch(
2025-03-06 18:35:42,987 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 241, in train_one_epoch
    for batch, (X, y) in enumerate(data_loader):
2025-03-06 18:35:42,988 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 652, in __next__
    data = self._next_data()
2025-03-06 18:35:42,988 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 692, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
2025-03-06 18:35:42,988 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    return self.collate_fn(data)
2025-03-06 18:35:42,988 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 175, in default_collate
    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.
2025-03-06 18:35:42,988 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 175, in <listcomp>
    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.
2025-03-06 18:35:42,988 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 155, in default_collate
    return torch.tensor(batch)
2025-03-06 18:35:42,989 - ERROR - KeyboardInterrupt
2025-03-06 18:35:42,989 - ERROR - Traceback (most recent call last):
  File "/Users/patmccarthy/Documents/ThalamoCortex/train_scripts/train_feedforward_mnist.py", line 186, in <module>
    train_losses, val_losses, train_topk_accs, val_topk_accs, state_dicts, train_time = train(model=model,
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 184, in train
    train_losses, train_topk_accs, state_dict = train_one_epoch(
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 241, in train_one_epoch
    for batch, (X, y) in enumerate(data_loader):
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 652, in __next__
    data = self._next_data()
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 692, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    return self.collate_fn(data)
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 175, in default_collate
    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 175, in <listcomp>
    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 155, in default_collate
    return torch.tensor(batch)
KeyboardInterrupt