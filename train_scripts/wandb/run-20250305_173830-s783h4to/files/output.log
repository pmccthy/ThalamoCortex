2025-03-05 17:38:34,423 - INFO - Running hyperparameter combination 1 of 1
2025-03-05 17:38:34,423 - INFO - 0_CTCNet_finetuning_ThalReadout_2
2025-03-05 17:38:36,708 - INFO - Training...
2025-03-05 17:38:36,710 - INFO - Beginning epoch 1/50
2025-03-05 17:38:36,716 - INFO - training batch 1, loss: 0.366, 32/56000 datapoints
2025-03-05 17:38:36,828 - INFO - training batch 51, loss: 0.497, 1632/56000 datapoints
2025-03-05 17:38:36,941 - INFO - training batch 101, loss: 0.295, 3232/56000 datapoints
2025-03-05 17:38:37,031 - INFO - training batch 151, loss: 0.387, 4832/56000 datapoints
2025-03-05 17:38:37,125 - INFO - training batch 201, loss: 0.254, 6432/56000 datapoints
2025-03-05 17:38:37,228 - INFO - training batch 251, loss: 0.282, 8032/56000 datapoints
2025-03-05 17:38:37,314 - INFO - training batch 301, loss: 0.322, 9632/56000 datapoints
2025-03-05 17:38:37,401 - INFO - training batch 351, loss: 0.338, 11232/56000 datapoints
2025-03-05 17:38:37,490 - INFO - training batch 401, loss: 0.406, 12832/56000 datapoints
2025-03-05 17:38:37,575 - INFO - training batch 451, loss: 0.338, 14432/56000 datapoints
2025-03-05 17:38:37,674 - INFO - training batch 501, loss: 0.381, 16032/56000 datapoints
2025-03-05 17:38:37,766 - INFO - training batch 551, loss: 0.213, 17632/56000 datapoints
2025-03-05 17:38:37,858 - INFO - training batch 601, loss: 0.271, 19232/56000 datapoints
2025-03-05 17:38:37,952 - INFO - training batch 651, loss: 0.256, 20832/56000 datapoints
2025-03-05 17:38:38,036 - INFO - training batch 701, loss: 0.339, 22432/56000 datapoints
2025-03-05 17:38:38,125 - INFO - training batch 751, loss: 0.292, 24032/56000 datapoints
2025-03-05 17:38:38,212 - INFO - training batch 801, loss: 0.310, 25632/56000 datapoints
2025-03-05 17:38:38,299 - INFO - training batch 851, loss: 0.231, 27232/56000 datapoints
2025-03-05 17:38:38,385 - INFO - training batch 901, loss: 0.334, 28832/56000 datapoints
2025-03-05 17:38:38,483 - INFO - training batch 951, loss: 0.289, 30432/56000 datapoints
2025-03-05 17:38:38,574 - INFO - training batch 1001, loss: 0.191, 32032/56000 datapoints
2025-03-05 17:38:38,722 - INFO - training batch 1051, loss: 0.205, 33632/56000 datapoints
2025-03-05 17:38:38,843 - INFO - training batch 1101, loss: 0.276, 35232/56000 datapoints
2025-03-05 17:38:38,961 - INFO - training batch 1151, loss: 0.153, 36832/56000 datapoints
2025-03-05 17:38:39,055 - INFO - training batch 1201, loss: 0.224, 38432/56000 datapoints
2025-03-05 17:38:39,186 - INFO - training batch 1251, loss: 0.289, 40032/56000 datapoints
2025-03-05 17:38:39,317 - INFO - training batch 1301, loss: 0.249, 41632/56000 datapoints
2025-03-05 17:38:39,415 - INFO - training batch 1351, loss: 0.162, 43232/56000 datapoints
2025-03-05 17:38:39,519 - INFO - training batch 1401, loss: 0.173, 44832/56000 datapoints
2025-03-05 17:38:39,619 - INFO - training batch 1451, loss: 0.183, 46432/56000 datapoints
2025-03-05 17:38:39,849 - INFO - training batch 1501, loss: 0.345, 48032/56000 datapoints
2025-03-05 17:38:39,963 - INFO - training batch 1551, loss: 0.223, 49632/56000 datapoints
2025-03-05 17:38:40,068 - INFO - training batch 1601, loss: 0.294, 51232/56000 datapoints
2025-03-05 17:38:40,180 - INFO - training batch 1651, loss: 0.146, 52832/56000 datapoints
2025-03-05 17:38:40,290 - INFO - training batch 1701, loss: 0.190, 54432/56000 datapoints
2025-03-05 17:38:40,447 - INFO - validation batch 1, loss: 0.137, 32/13984 datapoints
2025-03-05 17:38:40,547 - INFO - validation batch 51, loss: 0.085, 1632/13984 datapoints
2025-03-05 17:38:40,610 - INFO - validation batch 101, loss: 0.265, 3232/13984 datapoints
2025-03-05 17:38:40,664 - INFO - validation batch 151, loss: 0.140, 4832/13984 datapoints
2025-03-05 17:38:40,713 - INFO - validation batch 201, loss: 0.223, 6432/13984 datapoints
2025-03-05 17:38:40,771 - INFO - validation batch 251, loss: 0.251, 8032/13984 datapoints
2025-03-05 17:38:40,823 - INFO - validation batch 301, loss: 0.368, 9632/13984 datapoints
2025-03-05 17:38:40,870 - INFO - validation batch 351, loss: 0.139, 11232/13984 datapoints
2025-03-05 17:38:40,919 - INFO - validation batch 401, loss: 0.279, 12832/13984 datapoints
2025-03-05 17:38:40,956 - INFO - Epoch 1/50 done.
2025-03-05 17:38:40,956 - INFO - Final validation performance:
Loss: 0.210, top-1 acc: 0.888top-5 acc: 0.888
2025-03-05 17:38:40,956 - INFO - Epoch 1/50 done
2025-03-05 17:38:40,957 - INFO - Beginning epoch 2/50
2025-03-05 17:38:40,959 - INFO - training batch 1, loss: 0.208, 32/56000 datapoints
2025-03-05 17:38:41,051 - INFO - training batch 51, loss: 0.324, 1632/56000 datapoints
2025-03-05 17:38:41,141 - INFO - training batch 101, loss: 0.134, 3232/56000 datapoints
2025-03-05 17:38:41,239 - INFO - training batch 151, loss: 0.251, 4832/56000 datapoints
2025-03-05 17:38:41,335 - INFO - training batch 201, loss: 0.117, 6432/56000 datapoints
2025-03-05 17:38:41,425 - INFO - training batch 251, loss: 0.176, 8032/56000 datapoints
2025-03-05 17:38:41,538 - INFO - training batch 301, loss: 0.215, 9632/56000 datapoints
2025-03-05 17:38:41,675 - INFO - training batch 351, loss: 0.224, 11232/56000 datapoints
2025-03-05 17:38:41,786 - INFO - training batch 401, loss: 0.294, 12832/56000 datapoints
2025-03-05 17:38:41,885 - INFO - training batch 451, loss: 0.281, 14432/56000 datapoints
2025-03-05 17:38:41,983 - INFO - training batch 501, loss: 0.291, 16032/56000 datapoints
2025-03-05 17:38:42,073 - INFO - training batch 551, loss: 0.156, 17632/56000 datapoints
2025-03-05 17:38:42,167 - INFO - training batch 601, loss: 0.160, 19232/56000 datapoints
2025-03-05 17:38:42,260 - INFO - training batch 651, loss: 0.138, 20832/56000 datapoints
2025-03-05 17:38:42,348 - INFO - training batch 701, loss: 0.255, 22432/56000 datapoints
2025-03-05 17:38:42,439 - INFO - training batch 751, loss: 0.200, 24032/56000 datapoints
2025-03-05 17:38:42,534 - INFO - training batch 801, loss: 0.244, 25632/56000 datapoints
2025-03-05 17:38:42,621 - INFO - training batch 851, loss: 0.153, 27232/56000 datapoints
2025-03-05 17:38:42,711 - INFO - training batch 901, loss: 0.245, 28832/56000 datapoints
2025-03-05 17:38:42,809 - INFO - training batch 951, loss: 0.236, 30432/56000 datapoints
2025-03-05 17:38:42,897 - INFO - training batch 1001, loss: 0.122, 32032/56000 datapoints
2025-03-05 17:38:42,984 - INFO - training batch 1051, loss: 0.136, 33632/56000 datapoints
2025-03-05 17:38:43,072 - INFO - training batch 1101, loss: 0.221, 35232/56000 datapoints
2025-03-05 17:38:43,157 - INFO - training batch 1151, loss: 0.091, 36832/56000 datapoints
2025-03-05 17:38:43,241 - INFO - training batch 1201, loss: 0.174, 38432/56000 datapoints
2025-03-05 17:38:43,329 - INFO - training batch 1251, loss: 0.253, 40032/56000 datapoints
2025-03-05 17:38:43,416 - INFO - training batch 1301, loss: 0.201, 41632/56000 datapoints
2025-03-05 17:38:43,503 - INFO - training batch 1351, loss: 0.112, 43232/56000 datapoints
2025-03-05 17:38:43,594 - INFO - training batch 1401, loss: 0.119, 44832/56000 datapoints
2025-03-05 17:38:43,688 - INFO - training batch 1451, loss: 0.134, 46432/56000 datapoints
2025-03-05 17:38:43,774 - INFO - training batch 1501, loss: 0.286, 48032/56000 datapoints
2025-03-05 17:38:43,861 - INFO - training batch 1551, loss: 0.182, 49632/56000 datapoints
2025-03-05 17:38:43,949 - INFO - training batch 1601, loss: 0.239, 51232/56000 datapoints
2025-03-05 17:38:44,038 - INFO - training batch 1651, loss: 0.111, 52832/56000 datapoints
2025-03-05 17:38:44,124 - INFO - training batch 1701, loss: 0.159, 54432/56000 datapoints
2025-03-05 17:38:44,211 - INFO - validation batch 1, loss: 0.097, 32/13984 datapoints
2025-03-05 17:38:44,258 - INFO - validation batch 51, loss: 0.050, 1632/13984 datapoints
2025-03-05 17:38:44,307 - INFO - validation batch 101, loss: 0.204, 3232/13984 datapoints
2025-03-05 17:38:44,359 - INFO - validation batch 151, loss: 0.089, 4832/13984 datapoints
2025-03-05 17:38:44,417 - INFO - validation batch 201, loss: 0.173, 6432/13984 datapoints
2025-03-05 17:38:44,470 - INFO - validation batch 251, loss: 0.207, 8032/13984 datapoints
2025-03-05 17:38:44,521 - INFO - validation batch 301, loss: 0.321, 9632/13984 datapoints
2025-03-05 17:38:44,569 - INFO - validation batch 351, loss: 0.105, 11232/13984 datapoints
2025-03-05 17:38:44,619 - INFO - validation batch 401, loss: 0.245, 12832/13984 datapoints
2025-03-05 17:38:44,656 - INFO - Epoch 2/50 done.
2025-03-05 17:38:44,656 - INFO - Final validation performance:
Loss: 0.166, top-1 acc: 0.888top-5 acc: 0.888
2025-03-05 17:38:44,657 - INFO - Epoch 2/50 done
2025-03-05 17:38:44,657 - INFO - Beginning epoch 3/50
2025-03-05 17:38:44,660 - INFO - training batch 1, loss: 0.182, 32/56000 datapoints
2025-03-05 17:38:44,751 - INFO - training batch 51, loss: 0.270, 1632/56000 datapoints
2025-03-05 17:38:44,841 - INFO - training batch 101, loss: 0.103, 3232/56000 datapoints
2025-03-05 17:38:44,930 - INFO - training batch 151, loss: 0.204, 4832/56000 datapoints
2025-03-05 17:38:45,020 - INFO - training batch 201, loss: 0.089, 6432/56000 datapoints
2025-03-05 17:38:45,108 - INFO - training batch 251, loss: 0.142, 8032/56000 datapoints
2025-03-05 17:38:45,202 - INFO - training batch 301, loss: 0.178, 9632/56000 datapoints
2025-03-05 17:38:45,297 - INFO - training batch 351, loss: 0.189, 11232/56000 datapoints
2025-03-05 17:38:45,380 - INFO - training batch 401, loss: 0.247, 12832/56000 datapoints
2025-03-05 17:38:45,472 - INFO - training batch 451, loss: 0.239, 14432/56000 datapoints
2025-03-05 17:38:45,568 - INFO - training batch 501, loss: 0.241, 16032/56000 datapoints
2025-03-05 17:38:45,669 - INFO - training batch 551, loss: 0.135, 17632/56000 datapoints
2025-03-05 17:38:45,793 - INFO - training batch 601, loss: 0.118, 19232/56000 datapoints
2025-03-05 17:38:45,949 - INFO - training batch 651, loss: 0.103, 20832/56000 datapoints
2025-03-05 17:38:46,066 - INFO - training batch 701, loss: 0.226, 22432/56000 datapoints
2025-03-05 17:38:46,152 - INFO - training batch 751, loss: 0.154, 24032/56000 datapoints
2025-03-05 17:38:46,237 - INFO - training batch 801, loss: 0.198, 25632/56000 datapoints
2025-03-05 17:38:46,323 - INFO - training batch 851, loss: 0.121, 27232/56000 datapoints
2025-03-05 17:38:46,413 - INFO - training batch 901, loss: 0.191, 28832/56000 datapoints
2025-03-05 17:38:46,504 - INFO - training batch 951, loss: 0.194, 30432/56000 datapoints
2025-03-05 17:38:46,603 - INFO - training batch 1001, loss: 0.092, 32032/56000 datapoints
2025-03-05 17:38:46,749 - INFO - training batch 1051, loss: 0.105, 33632/56000 datapoints
2025-03-05 17:38:46,838 - INFO - training batch 1101, loss: 0.189, 35232/56000 datapoints
2025-03-05 17:38:46,924 - INFO - training batch 1151, loss: 0.064, 36832/56000 datapoints
2025-03-05 17:38:47,018 - INFO - training batch 1201, loss: 0.149, 38432/56000 datapoints
2025-03-05 17:38:47,108 - INFO - training batch 1251, loss: 0.230, 40032/56000 datapoints
2025-03-05 17:38:47,195 - INFO - training batch 1301, loss: 0.159, 41632/56000 datapoints
2025-03-05 17:38:47,307 - INFO - training batch 1351, loss: 0.083, 43232/56000 datapoints
2025-03-05 17:38:47,395 - INFO - training batch 1401, loss: 0.092, 44832/56000 datapoints
2025-03-05 17:38:47,482 - INFO - training batch 1451, loss: 0.108, 46432/56000 datapoints
2025-03-05 17:38:47,573 - INFO - training batch 1501, loss: 0.216, 48032/56000 datapoints
2025-03-05 17:38:47,666 - INFO - training batch 1551, loss: 0.133, 49632/56000 datapoints
2025-03-05 17:38:47,755 - INFO - training batch 1601, loss: 0.171, 51232/56000 datapoints
2025-03-05 17:38:47,843 - INFO - training batch 1651, loss: 0.082, 52832/56000 datapoints
2025-03-05 17:38:47,930 - INFO - training batch 1701, loss: 0.128, 54432/56000 datapoints
2025-03-05 17:38:48,017 - INFO - validation batch 1, loss: 0.070, 32/13984 datapoints
2025-03-05 17:38:48,066 - INFO - validation batch 51, loss: 0.041, 1632/13984 datapoints
2025-03-05 17:38:48,113 - INFO - validation batch 101, loss: 0.142, 3232/13984 datapoints
2025-03-05 17:38:48,160 - INFO - validation batch 151, loss: 0.063, 4832/13984 datapoints
2025-03-05 17:38:48,209 - INFO - validation batch 201, loss: 0.128, 6432/13984 datapoints
2025-03-05 17:38:48,257 - INFO - validation batch 251, loss: 0.160, 8032/13984 datapoints
2025-03-05 17:38:48,304 - INFO - validation batch 301, loss: 0.243, 9632/13984 datapoints
2025-03-05 17:38:48,351 - INFO - validation batch 351, loss: 0.080, 11232/13984 datapoints
2025-03-05 17:38:48,399 - INFO - validation batch 401, loss: 0.201, 12832/13984 datapoints
2025-03-05 17:38:48,434 - INFO - Epoch 3/50 done.
2025-03-05 17:38:48,435 - INFO - Final validation performance:
Loss: 0.125, top-1 acc: 0.924top-5 acc: 0.924
2025-03-05 17:38:48,435 - INFO - Epoch 3/50 done
2025-03-05 17:38:48,436 - INFO - Beginning epoch 4/50
2025-03-05 17:38:48,438 - INFO - training batch 1, loss: 0.170, 32/56000 datapoints
2025-03-05 17:38:48,547 - INFO - training batch 51, loss: 0.203, 1632/56000 datapoints
2025-03-05 17:38:48,641 - INFO - training batch 101, loss: 0.078, 3232/56000 datapoints
2025-03-05 17:38:48,728 - INFO - training batch 151, loss: 0.148, 4832/56000 datapoints
2025-03-05 17:38:48,825 - INFO - training batch 201, loss: 0.074, 6432/56000 datapoints
2025-03-05 17:38:48,924 - INFO - training batch 251, loss: 0.101, 8032/56000 datapoints
2025-03-05 17:38:49,018 - INFO - training batch 301, loss: 0.132, 9632/56000 datapoints
2025-03-05 17:38:49,108 - INFO - training batch 351, loss: 0.161, 11232/56000 datapoints
2025-03-05 17:38:49,199 - INFO - training batch 401, loss: 0.181, 12832/56000 datapoints
2025-03-05 17:38:49,289 - INFO - training batch 451, loss: 0.189, 14432/56000 datapoints
2025-03-05 17:38:49,376 - INFO - training batch 501, loss: 0.183, 16032/56000 datapoints
2025-03-05 17:38:49,464 - INFO - training batch 551, loss: 0.117, 17632/56000 datapoints
2025-03-05 17:38:49,549 - INFO - training batch 601, loss: 0.086, 19232/56000 datapoints
2025-03-05 17:38:49,646 - INFO - training batch 651, loss: 0.071, 20832/56000 datapoints
2025-03-05 17:38:49,748 - INFO - training batch 701, loss: 0.202, 22432/56000 datapoints
2025-03-05 17:38:49,842 - INFO - training batch 751, loss: 0.112, 24032/56000 datapoints
2025-03-05 17:38:49,930 - INFO - training batch 801, loss: 0.141, 25632/56000 datapoints
2025-03-05 17:38:50,016 - INFO - training batch 851, loss: 0.088, 27232/56000 datapoints
2025-03-05 17:38:50,104 - INFO - training batch 901, loss: 0.138, 28832/56000 datapoints
2025-03-05 17:38:50,189 - INFO - training batch 951, loss: 0.149, 30432/56000 datapoints
2025-03-05 17:38:50,277 - INFO - training batch 1001, loss: 0.065, 32032/56000 datapoints
2025-03-05 17:38:50,363 - INFO - training batch 1051, loss: 0.077, 33632/56000 datapoints
2025-03-05 17:38:50,450 - INFO - training batch 1101, loss: 0.161, 35232/56000 datapoints
2025-03-05 17:38:50,542 - INFO - training batch 1151, loss: 0.041, 36832/56000 datapoints
2025-03-05 17:38:50,633 - INFO - training batch 1201, loss: 0.120, 38432/56000 datapoints
2025-03-05 17:38:50,717 - INFO - training batch 1251, loss: 0.210, 40032/56000 datapoints
2025-03-05 17:38:50,804 - INFO - training batch 1301, loss: 0.116, 41632/56000 datapoints
2025-03-05 17:38:50,890 - INFO - training batch 1351, loss: 0.056, 43232/56000 datapoints
2025-03-05 17:38:50,978 - INFO - training batch 1401, loss: 0.071, 44832/56000 datapoints
2025-03-05 17:38:51,065 - INFO - training batch 1451, loss: 0.082, 46432/56000 datapoints
2025-03-05 17:38:51,155 - INFO - training batch 1501, loss: 0.153, 48032/56000 datapoints
2025-03-05 17:38:51,243 - INFO - training batch 1551, loss: 0.088, 49632/56000 datapoints
2025-03-05 17:38:51,328 - INFO - training batch 1601, loss: 0.114, 51232/56000 datapoints
2025-03-05 17:38:51,414 - INFO - training batch 1651, loss: 0.058, 52832/56000 datapoints
2025-03-05 17:38:51,501 - INFO - training batch 1701, loss: 0.100, 54432/56000 datapoints
2025-03-05 17:38:51,589 - INFO - validation batch 1, loss: 0.050, 32/13984 datapoints
2025-03-05 17:38:51,640 - INFO - validation batch 51, loss: 0.038, 1632/13984 datapoints
2025-03-05 17:38:51,688 - INFO - validation batch 101, loss: 0.094, 3232/13984 datapoints
2025-03-05 17:38:51,738 - INFO - validation batch 151, loss: 0.047, 4832/13984 datapoints
2025-03-05 17:38:51,794 - INFO - validation batch 201, loss: 0.090, 6432/13984 datapoints
2025-03-05 17:38:51,849 - INFO - validation batch 251, loss: 0.117, 8032/13984 datapoints
2025-03-05 17:38:51,897 - INFO - validation batch 301, loss: 0.165, 9632/13984 datapoints
2025-03-05 17:38:51,944 - INFO - validation batch 351, loss: 0.061, 11232/13984 datapoints
2025-03-05 17:38:51,994 - INFO - validation batch 401, loss: 0.156, 12832/13984 datapoints
2025-03-05 17:38:52,030 - INFO - Epoch 4/50 done.
2025-03-05 17:38:52,031 - INFO - Final validation performance:
Loss: 0.091, top-1 acc: 0.979top-5 acc: 0.979
2025-03-05 17:38:52,031 - INFO - Epoch 4/50 done
2025-03-05 17:38:52,031 - INFO - Beginning epoch 5/50
2025-03-05 17:38:52,033 - INFO - training batch 1, loss: 0.159, 32/56000 datapoints
2025-03-05 17:38:52,125 - INFO - training batch 51, loss: 0.137, 1632/56000 datapoints
2025-03-05 17:38:52,213 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 17:38:52,310 - INFO - training batch 151, loss: 0.108, 4832/56000 datapoints
2025-03-05 17:38:52,403 - INFO - training batch 201, loss: 0.063, 6432/56000 datapoints
2025-03-05 17:38:52,497 - INFO - training batch 251, loss: 0.069, 8032/56000 datapoints
2025-03-05 17:38:52,585 - INFO - training batch 301, loss: 0.100, 9632/56000 datapoints
2025-03-05 17:38:52,683 - INFO - training batch 351, loss: 0.132, 11232/56000 datapoints
2025-03-05 17:38:52,791 - INFO - training batch 401, loss: 0.130, 12832/56000 datapoints
2025-03-05 17:38:52,881 - INFO - training batch 451, loss: 0.137, 14432/56000 datapoints
2025-03-05 17:38:52,974 - INFO - training batch 501, loss: 0.132, 16032/56000 datapoints
2025-03-05 17:38:53,073 - INFO - training batch 551, loss: 0.103, 17632/56000 datapoints
2025-03-05 17:38:53,234 - INFO - training batch 601, loss: 0.054, 19232/56000 datapoints
2025-03-05 17:38:53,370 - INFO - training batch 651, loss: 0.051, 20832/56000 datapoints
2025-03-05 17:38:53,476 - INFO - training batch 701, loss: 0.195, 22432/56000 datapoints
2025-03-05 17:38:53,575 - INFO - training batch 751, loss: 0.082, 24032/56000 datapoints
2025-03-05 17:38:53,699 - INFO - training batch 801, loss: 0.105, 25632/56000 datapoints
2025-03-05 17:38:53,807 - INFO - training batch 851, loss: 0.064, 27232/56000 datapoints
2025-03-05 17:38:53,907 - INFO - training batch 901, loss: 0.099, 28832/56000 datapoints
2025-03-05 17:38:54,129 - INFO - training batch 951, loss: 0.108, 30432/56000 datapoints
2025-03-05 17:38:54,231 - INFO - training batch 1001, loss: 0.048, 32032/56000 datapoints
2025-03-05 17:38:54,337 - INFO - training batch 1051, loss: 0.058, 33632/56000 datapoints
2025-03-05 17:38:54,453 - INFO - training batch 1101, loss: 0.131, 35232/56000 datapoints
2025-03-05 17:38:54,585 - INFO - training batch 1151, loss: 0.026, 36832/56000 datapoints
2025-03-05 17:38:54,725 - INFO - training batch 1201, loss: 0.098, 38432/56000 datapoints
2025-03-05 17:38:54,911 - INFO - training batch 1251, loss: 0.203, 40032/56000 datapoints
2025-03-05 17:38:55,052 - INFO - training batch 1301, loss: 0.083, 41632/56000 datapoints
2025-03-05 17:38:55,216 - INFO - training batch 1351, loss: 0.039, 43232/56000 datapoints
2025-03-05 17:38:55,391 - INFO - training batch 1401, loss: 0.054, 44832/56000 datapoints
2025-03-05 17:38:55,688 - INFO - training batch 1451, loss: 0.059, 46432/56000 datapoints
2025-03-05 17:38:55,829 - INFO - training batch 1501, loss: 0.113, 48032/56000 datapoints
2025-03-05 17:38:56,103 - INFO - training batch 1551, loss: 0.061, 49632/56000 datapoints
2025-03-05 17:38:56,283 - INFO - training batch 1601, loss: 0.080, 51232/56000 datapoints
2025-03-05 17:38:56,480 - INFO - training batch 1651, loss: 0.044, 52832/56000 datapoints
2025-03-05 17:38:56,666 - INFO - training batch 1701, loss: 0.086, 54432/56000 datapoints
2025-03-05 17:38:56,849 - INFO - validation batch 1, loss: 0.037, 32/13984 datapoints
2025-03-05 17:38:56,938 - INFO - validation batch 51, loss: 0.036, 1632/13984 datapoints
2025-03-05 17:38:57,018 - INFO - validation batch 101, loss: 0.064, 3232/13984 datapoints
2025-03-05 17:38:57,095 - INFO - validation batch 151, loss: 0.035, 4832/13984 datapoints
2025-03-05 17:38:57,162 - INFO - validation batch 201, loss: 0.065, 6432/13984 datapoints
2025-03-05 17:38:57,273 - INFO - validation batch 251, loss: 0.084, 8032/13984 datapoints
2025-03-05 17:38:57,425 - INFO - validation batch 301, loss: 0.116, 9632/13984 datapoints
2025-03-05 17:38:57,656 - INFO - validation batch 351, loss: 0.047, 11232/13984 datapoints
2025-03-05 17:38:57,747 - INFO - validation batch 401, loss: 0.129, 12832/13984 datapoints
2025-03-05 17:38:57,844 - INFO - Epoch 5/50 done.
2025-03-05 17:38:57,845 - INFO - Final validation performance:
Loss: 0.068, top-1 acc: 0.989top-5 acc: 0.989
2025-03-05 17:38:57,847 - INFO - Epoch 5/50 done
2025-03-05 17:38:57,847 - INFO - Beginning epoch 6/50
2025-03-05 17:38:57,853 - INFO - training batch 1, loss: 0.153, 32/56000 datapoints
2025-03-05 17:38:58,024 - INFO - training batch 51, loss: 0.096, 1632/56000 datapoints
2025-03-05 17:38:58,167 - INFO - training batch 101, loss: 0.057, 3232/56000 datapoints
2025-03-05 17:38:58,300 - INFO - training batch 151, loss: 0.085, 4832/56000 datapoints
2025-03-05 17:38:58,445 - INFO - training batch 201, loss: 0.057, 6432/56000 datapoints
2025-03-05 17:38:58,575 - INFO - training batch 251, loss: 0.048, 8032/56000 datapoints
2025-03-05 17:38:58,692 - INFO - training batch 301, loss: 0.077, 9632/56000 datapoints
2025-03-05 17:38:58,814 - INFO - training batch 351, loss: 0.114, 11232/56000 datapoints
2025-03-05 17:38:58,921 - INFO - training batch 401, loss: 0.100, 12832/56000 datapoints
2025-03-05 17:38:59,019 - INFO - training batch 451, loss: 0.104, 14432/56000 datapoints
2025-03-05 17:38:59,114 - INFO - training batch 501, loss: 0.098, 16032/56000 datapoints
2025-03-05 17:38:59,214 - INFO - training batch 551, loss: 0.094, 17632/56000 datapoints
2025-03-05 17:38:59,327 - INFO - training batch 601, loss: 0.035, 19232/56000 datapoints
2025-03-05 17:38:59,433 - INFO - training batch 651, loss: 0.038, 20832/56000 datapoints
2025-03-05 17:38:59,532 - INFO - training batch 701, loss: 0.198, 22432/56000 datapoints
2025-03-05 17:38:59,646 - INFO - training batch 751, loss: 0.062, 24032/56000 datapoints
2025-03-05 17:38:59,740 - INFO - training batch 801, loss: 0.082, 25632/56000 datapoints
2025-03-05 17:38:59,835 - INFO - training batch 851, loss: 0.048, 27232/56000 datapoints
2025-03-05 17:38:59,926 - INFO - training batch 901, loss: 0.074, 28832/56000 datapoints
2025-03-05 17:39:00,016 - INFO - training batch 951, loss: 0.079, 30432/56000 datapoints
2025-03-05 17:39:00,104 - INFO - training batch 1001, loss: 0.037, 32032/56000 datapoints
2025-03-05 17:39:00,222 - INFO - training batch 1051, loss: 0.045, 33632/56000 datapoints
2025-03-05 17:39:00,341 - INFO - training batch 1101, loss: 0.112, 35232/56000 datapoints
2025-03-05 17:39:00,446 - INFO - training batch 1151, loss: 0.017, 36832/56000 datapoints
2025-03-05 17:39:00,562 - INFO - training batch 1201, loss: 0.082, 38432/56000 datapoints
2025-03-05 17:39:00,669 - INFO - training batch 1251, loss: 0.203, 40032/56000 datapoints
2025-03-05 17:39:00,754 - INFO - training batch 1301, loss: 0.062, 41632/56000 datapoints
2025-03-05 17:39:00,856 - INFO - training batch 1351, loss: 0.028, 43232/56000 datapoints
2025-03-05 17:39:00,956 - INFO - training batch 1401, loss: 0.042, 44832/56000 datapoints
2025-03-05 17:39:01,056 - INFO - training batch 1451, loss: 0.043, 46432/56000 datapoints
2025-03-05 17:39:01,156 - INFO - training batch 1501, loss: 0.087, 48032/56000 datapoints
2025-03-05 17:39:01,239 - INFO - training batch 1551, loss: 0.044, 49632/56000 datapoints
2025-03-05 17:39:01,338 - INFO - training batch 1601, loss: 0.058, 51232/56000 datapoints
2025-03-05 17:39:01,442 - INFO - training batch 1651, loss: 0.035, 52832/56000 datapoints
2025-03-05 17:39:01,563 - INFO - training batch 1701, loss: 0.080, 54432/56000 datapoints
2025-03-05 17:39:01,677 - INFO - validation batch 1, loss: 0.029, 32/13984 datapoints
2025-03-05 17:39:01,733 - INFO - validation batch 51, loss: 0.035, 1632/13984 datapoints
2025-03-05 17:39:01,783 - INFO - validation batch 101, loss: 0.045, 3232/13984 datapoints
2025-03-05 17:39:01,833 - INFO - validation batch 151, loss: 0.028, 4832/13984 datapoints
2025-03-05 17:39:01,882 - INFO - validation batch 201, loss: 0.049, 6432/13984 datapoints
2025-03-05 17:39:01,933 - INFO - validation batch 251, loss: 0.062, 8032/13984 datapoints
2025-03-05 17:39:01,985 - INFO - validation batch 301, loss: 0.083, 9632/13984 datapoints
2025-03-05 17:39:02,047 - INFO - validation batch 351, loss: 0.037, 11232/13984 datapoints
2025-03-05 17:39:02,112 - INFO - validation batch 401, loss: 0.112, 12832/13984 datapoints
2025-03-05 17:39:02,285 - INFO - Epoch 6/50 done.
2025-03-05 17:39:02,286 - INFO - Final validation performance:
Loss: 0.053, top-1 acc: 0.991top-5 acc: 0.991
2025-03-05 17:39:02,286 - INFO - Epoch 6/50 done
2025-03-05 17:39:02,287 - INFO - Beginning epoch 7/50
2025-03-05 17:39:02,289 - INFO - training batch 1, loss: 0.149, 32/56000 datapoints
2025-03-05 17:39:02,390 - INFO - training batch 51, loss: 0.070, 1632/56000 datapoints
2025-03-05 17:39:02,475 - INFO - training batch 101, loss: 0.054, 3232/56000 datapoints
2025-03-05 17:39:02,579 - INFO - training batch 151, loss: 0.073, 4832/56000 datapoints
2025-03-05 17:39:02,683 - INFO - training batch 201, loss: 0.055, 6432/56000 datapoints
2025-03-05 17:39:02,781 - INFO - training batch 251, loss: 0.036, 8032/56000 datapoints
2025-03-05 17:39:02,890 - INFO - training batch 301, loss: 0.062, 9632/56000 datapoints
2025-03-05 17:39:02,988 - INFO - training batch 351, loss: 0.101, 11232/56000 datapoints
2025-03-05 17:39:03,088 - INFO - training batch 401, loss: 0.082, 12832/56000 datapoints
2025-03-05 17:39:03,201 - INFO - training batch 451, loss: 0.082, 14432/56000 datapoints
2025-03-05 17:39:03,295 - INFO - training batch 501, loss: 0.074, 16032/56000 datapoints
2025-03-05 17:39:03,392 - INFO - training batch 551, loss: 0.089, 17632/56000 datapoints
2025-03-05 17:39:03,502 - INFO - training batch 601, loss: 0.024, 19232/56000 datapoints
2025-03-05 17:39:03,607 - INFO - training batch 651, loss: 0.030, 20832/56000 datapoints
2025-03-05 17:39:03,834 - INFO - training batch 701, loss: 0.206, 22432/56000 datapoints
2025-03-05 17:39:03,949 - INFO - training batch 751, loss: 0.048, 24032/56000 datapoints
2025-03-05 17:39:04,079 - INFO - training batch 801, loss: 0.066, 25632/56000 datapoints
2025-03-05 17:39:04,189 - INFO - training batch 851, loss: 0.036, 27232/56000 datapoints
2025-03-05 17:39:04,305 - INFO - training batch 901, loss: 0.058, 28832/56000 datapoints
2025-03-05 17:39:04,584 - INFO - training batch 951, loss: 0.060, 30432/56000 datapoints
2025-03-05 17:39:04,780 - INFO - training batch 1001, loss: 0.029, 32032/56000 datapoints
2025-03-05 17:39:04,973 - INFO - training batch 1051, loss: 0.035, 33632/56000 datapoints
2025-03-05 17:39:05,219 - INFO - training batch 1101, loss: 0.100, 35232/56000 datapoints
2025-03-05 17:39:05,507 - INFO - training batch 1151, loss: 0.012, 36832/56000 datapoints
2025-03-05 17:39:05,959 - INFO - training batch 1201, loss: 0.070, 38432/56000 datapoints
2025-03-05 17:39:06,104 - INFO - training batch 1251, loss: 0.207, 40032/56000 datapoints
2025-03-05 17:39:06,229 - INFO - training batch 1301, loss: 0.049, 41632/56000 datapoints
2025-03-05 17:39:06,350 - INFO - training batch 1351, loss: 0.021, 43232/56000 datapoints
2025-03-05 17:39:06,507 - INFO - training batch 1401, loss: 0.032, 44832/56000 datapoints
2025-03-05 17:39:06,637 - INFO - training batch 1451, loss: 0.032, 46432/56000 datapoints
2025-03-05 17:39:06,760 - INFO - training batch 1501, loss: 0.070, 48032/56000 datapoints
2025-03-05 17:39:06,952 - INFO - training batch 1551, loss: 0.033, 49632/56000 datapoints
2025-03-05 17:39:07,203 - INFO - training batch 1601, loss: 0.044, 51232/56000 datapoints
2025-03-05 17:39:07,317 - INFO - training batch 1651, loss: 0.028, 52832/56000 datapoints
2025-03-05 17:39:07,450 - INFO - training batch 1701, loss: 0.077, 54432/56000 datapoints
2025-03-05 17:39:07,622 - INFO - validation batch 1, loss: 0.024, 32/13984 datapoints
2025-03-05 17:39:07,676 - INFO - validation batch 51, loss: 0.033, 1632/13984 datapoints
2025-03-05 17:39:07,732 - INFO - validation batch 101, loss: 0.033, 3232/13984 datapoints
2025-03-05 17:39:07,801 - INFO - validation batch 151, loss: 0.024, 4832/13984 datapoints
2025-03-05 17:39:07,865 - INFO - validation batch 201, loss: 0.038, 6432/13984 datapoints
2025-03-05 17:39:07,965 - INFO - validation batch 251, loss: 0.048, 8032/13984 datapoints
2025-03-05 17:39:08,050 - INFO - validation batch 301, loss: 0.062, 9632/13984 datapoints
2025-03-05 17:39:08,124 - INFO - validation batch 351, loss: 0.030, 11232/13984 datapoints
2025-03-05 17:39:08,200 - INFO - validation batch 401, loss: 0.103, 12832/13984 datapoints
2025-03-05 17:39:08,251 - INFO - Epoch 7/50 done.
2025-03-05 17:39:08,251 - INFO - Final validation performance:
Loss: 0.044, top-1 acc: 0.992top-5 acc: 0.992
2025-03-05 17:39:08,252 - INFO - Epoch 7/50 done
2025-03-05 17:39:08,252 - INFO - Beginning epoch 8/50
2025-03-05 17:39:08,256 - INFO - training batch 1, loss: 0.147, 32/56000 datapoints
2025-03-05 17:39:08,388 - INFO - training batch 51, loss: 0.053, 1632/56000 datapoints
2025-03-05 17:39:08,531 - INFO - training batch 101, loss: 0.053, 3232/56000 datapoints
2025-03-05 17:39:08,676 - INFO - training batch 151, loss: 0.067, 4832/56000 datapoints
2025-03-05 17:39:08,825 - INFO - training batch 201, loss: 0.053, 6432/56000 datapoints
2025-03-05 17:39:08,960 - INFO - training batch 251, loss: 0.028, 8032/56000 datapoints
2025-03-05 17:39:09,086 - INFO - training batch 301, loss: 0.051, 9632/56000 datapoints
2025-03-05 17:39:09,281 - INFO - training batch 351, loss: 0.093, 11232/56000 datapoints
2025-03-05 17:39:09,431 - INFO - training batch 401, loss: 0.071, 12832/56000 datapoints
2025-03-05 17:39:09,562 - INFO - training batch 451, loss: 0.068, 14432/56000 datapoints
2025-03-05 17:39:09,679 - INFO - training batch 501, loss: 0.059, 16032/56000 datapoints
2025-03-05 17:39:09,790 - INFO - training batch 551, loss: 0.086, 17632/56000 datapoints
2025-03-05 17:39:09,902 - INFO - training batch 601, loss: 0.017, 19232/56000 datapoints
2025-03-05 17:39:10,001 - INFO - training batch 651, loss: 0.024, 20832/56000 datapoints
2025-03-05 17:39:10,098 - INFO - training batch 701, loss: 0.214, 22432/56000 datapoints
2025-03-05 17:39:10,203 - INFO - training batch 751, loss: 0.039, 24032/56000 datapoints
2025-03-05 17:39:10,301 - INFO - training batch 801, loss: 0.055, 25632/56000 datapoints
2025-03-05 17:39:10,398 - INFO - training batch 851, loss: 0.028, 27232/56000 datapoints
2025-03-05 17:39:10,490 - INFO - training batch 901, loss: 0.047, 28832/56000 datapoints
2025-03-05 17:39:10,591 - INFO - training batch 951, loss: 0.047, 30432/56000 datapoints
2025-03-05 17:39:10,694 - INFO - training batch 1001, loss: 0.023, 32032/56000 datapoints
2025-03-05 17:39:10,784 - INFO - training batch 1051, loss: 0.029, 33632/56000 datapoints
2025-03-05 17:39:10,871 - INFO - training batch 1101, loss: 0.093, 35232/56000 datapoints
2025-03-05 17:39:10,955 - INFO - training batch 1151, loss: 0.008, 36832/56000 datapoints
2025-03-05 17:39:11,035 - INFO - training batch 1201, loss: 0.062, 38432/56000 datapoints
2025-03-05 17:39:11,132 - INFO - training batch 1251, loss: 0.213, 40032/56000 datapoints
2025-03-05 17:39:11,239 - INFO - training batch 1301, loss: 0.041, 41632/56000 datapoints
2025-03-05 17:39:11,345 - INFO - training batch 1351, loss: 0.017, 43232/56000 datapoints
2025-03-05 17:39:11,429 - INFO - training batch 1401, loss: 0.026, 44832/56000 datapoints
2025-03-05 17:39:11,511 - INFO - training batch 1451, loss: 0.024, 46432/56000 datapoints
2025-03-05 17:39:11,593 - INFO - training batch 1501, loss: 0.058, 48032/56000 datapoints
2025-03-05 17:39:11,688 - INFO - training batch 1551, loss: 0.026, 49632/56000 datapoints
2025-03-05 17:39:11,801 - INFO - training batch 1601, loss: 0.035, 51232/56000 datapoints
2025-03-05 17:39:11,906 - INFO - training batch 1651, loss: 0.024, 52832/56000 datapoints
2025-03-05 17:39:11,989 - INFO - training batch 1701, loss: 0.077, 54432/56000 datapoints
2025-03-05 17:39:12,095 - INFO - validation batch 1, loss: 0.020, 32/13984 datapoints
2025-03-05 17:39:12,169 - INFO - validation batch 51, loss: 0.032, 1632/13984 datapoints
2025-03-05 17:39:12,234 - INFO - validation batch 101, loss: 0.025, 3232/13984 datapoints
2025-03-05 17:39:12,290 - INFO - validation batch 151, loss: 0.021, 4832/13984 datapoints
2025-03-05 17:39:12,347 - INFO - validation batch 201, loss: 0.031, 6432/13984 datapoints
2025-03-05 17:39:12,404 - INFO - validation batch 251, loss: 0.038, 8032/13984 datapoints
2025-03-05 17:39:12,457 - INFO - validation batch 301, loss: 0.048, 9632/13984 datapoints
2025-03-05 17:39:12,502 - INFO - validation batch 351, loss: 0.025, 11232/13984 datapoints
2025-03-05 17:39:12,546 - INFO - validation batch 401, loss: 0.098, 12832/13984 datapoints
2025-03-05 17:39:12,578 - INFO - Epoch 8/50 done.
2025-03-05 17:39:12,579 - INFO - Final validation performance:
Loss: 0.037, top-1 acc: 0.992top-5 acc: 0.992
2025-03-05 17:39:12,579 - INFO - Epoch 8/50 done
2025-03-05 17:39:12,584 - INFO - Beginning epoch 9/50
2025-03-05 17:39:12,586 - INFO - training batch 1, loss: 0.146, 32/56000 datapoints
2025-03-05 17:39:12,697 - INFO - training batch 51, loss: 0.042, 1632/56000 datapoints
2025-03-05 17:39:12,800 - INFO - training batch 101, loss: 0.054, 3232/56000 datapoints
2025-03-05 17:39:12,898 - INFO - training batch 151, loss: 0.064, 4832/56000 datapoints
2025-03-05 17:39:13,009 - INFO - training batch 201, loss: 0.051, 6432/56000 datapoints
2025-03-05 17:39:13,115 - INFO - training batch 251, loss: 0.023, 8032/56000 datapoints
2025-03-05 17:39:13,212 - INFO - training batch 301, loss: 0.044, 9632/56000 datapoints
2025-03-05 17:39:13,298 - INFO - training batch 351, loss: 0.088, 11232/56000 datapoints
2025-03-05 17:39:13,385 - INFO - training batch 401, loss: 0.064, 12832/56000 datapoints
2025-03-05 17:39:13,470 - INFO - training batch 451, loss: 0.059, 14432/56000 datapoints
2025-03-05 17:39:13,567 - INFO - training batch 501, loss: 0.048, 16032/56000 datapoints
2025-03-05 17:39:13,665 - INFO - training batch 551, loss: 0.085, 17632/56000 datapoints
2025-03-05 17:39:13,745 - INFO - training batch 601, loss: 0.013, 19232/56000 datapoints
2025-03-05 17:39:13,856 - INFO - training batch 651, loss: 0.020, 20832/56000 datapoints
2025-03-05 17:39:13,963 - INFO - training batch 701, loss: 0.223, 22432/56000 datapoints
2025-03-05 17:39:14,082 - INFO - training batch 751, loss: 0.032, 24032/56000 datapoints
2025-03-05 17:39:14,281 - INFO - training batch 801, loss: 0.048, 25632/56000 datapoints
2025-03-05 17:39:14,388 - INFO - training batch 851, loss: 0.022, 27232/56000 datapoints
2025-03-05 17:39:14,523 - INFO - training batch 901, loss: 0.039, 28832/56000 datapoints
2025-03-05 17:39:14,655 - INFO - training batch 951, loss: 0.039, 30432/56000 datapoints
2025-03-05 17:39:14,803 - INFO - training batch 1001, loss: 0.019, 32032/56000 datapoints
2025-03-05 17:39:15,034 - INFO - training batch 1051, loss: 0.024, 33632/56000 datapoints
2025-03-05 17:39:15,144 - INFO - training batch 1101, loss: 0.088, 35232/56000 datapoints
2025-03-05 17:39:15,249 - INFO - training batch 1151, loss: 0.006, 36832/56000 datapoints
2025-03-05 17:39:15,348 - INFO - training batch 1201, loss: 0.055, 38432/56000 datapoints
2025-03-05 17:39:15,436 - INFO - training batch 1251, loss: 0.218, 40032/56000 datapoints
2025-03-05 17:39:15,522 - INFO - training batch 1301, loss: 0.036, 41632/56000 datapoints
2025-03-05 17:39:15,608 - INFO - training batch 1351, loss: 0.014, 43232/56000 datapoints
2025-03-05 17:39:15,704 - INFO - training batch 1401, loss: 0.021, 44832/56000 datapoints
2025-03-05 17:39:15,790 - INFO - training batch 1451, loss: 0.019, 46432/56000 datapoints
2025-03-05 17:39:15,885 - INFO - training batch 1501, loss: 0.050, 48032/56000 datapoints
2025-03-05 17:39:15,979 - INFO - training batch 1551, loss: 0.022, 49632/56000 datapoints
2025-03-05 17:39:16,077 - INFO - training batch 1601, loss: 0.028, 51232/56000 datapoints
2025-03-05 17:39:16,169 - INFO - training batch 1651, loss: 0.021, 52832/56000 datapoints
2025-03-05 17:39:16,248 - INFO - training batch 1701, loss: 0.077, 54432/56000 datapoints
2025-03-05 17:39:16,331 - INFO - validation batch 1, loss: 0.017, 32/13984 datapoints
2025-03-05 17:39:16,373 - INFO - validation batch 51, loss: 0.031, 1632/13984 datapoints
2025-03-05 17:39:16,415 - INFO - validation batch 101, loss: 0.020, 3232/13984 datapoints
2025-03-05 17:39:16,455 - INFO - validation batch 151, loss: 0.018, 4832/13984 datapoints
2025-03-05 17:39:16,496 - INFO - validation batch 201, loss: 0.025, 6432/13984 datapoints
2025-03-05 17:39:16,537 - INFO - validation batch 251, loss: 0.031, 8032/13984 datapoints
2025-03-05 17:39:16,579 - INFO - validation batch 301, loss: 0.039, 9632/13984 datapoints
2025-03-05 17:39:16,626 - INFO - validation batch 351, loss: 0.021, 11232/13984 datapoints
2025-03-05 17:39:16,670 - INFO - validation batch 401, loss: 0.095, 12832/13984 datapoints
2025-03-05 17:39:16,699 - INFO - Epoch 9/50 done.
2025-03-05 17:39:16,700 - INFO - Final validation performance:
Loss: 0.033, top-1 acc: 0.992top-5 acc: 0.992
2025-03-05 17:39:16,700 - INFO - Epoch 9/50 done
2025-03-05 17:39:16,700 - INFO - Beginning epoch 10/50
2025-03-05 17:39:16,703 - INFO - training batch 1, loss: 0.146, 32/56000 datapoints
2025-03-05 17:39:16,784 - INFO - training batch 51, loss: 0.034, 1632/56000 datapoints
2025-03-05 17:39:16,872 - INFO - training batch 101, loss: 0.054, 3232/56000 datapoints
2025-03-05 17:39:16,957 - INFO - training batch 151, loss: 0.063, 4832/56000 datapoints
2025-03-05 17:39:17,037 - INFO - training batch 201, loss: 0.050, 6432/56000 datapoints
2025-03-05 17:39:17,116 - INFO - training batch 251, loss: 0.019, 8032/56000 datapoints
2025-03-05 17:39:17,198 - INFO - training batch 301, loss: 0.038, 9632/56000 datapoints
2025-03-05 17:39:17,278 - INFO - training batch 351, loss: 0.085, 11232/56000 datapoints
2025-03-05 17:39:17,357 - INFO - training batch 401, loss: 0.060, 12832/56000 datapoints
2025-03-05 17:39:17,436 - INFO - training batch 451, loss: 0.052, 14432/56000 datapoints
2025-03-05 17:39:17,514 - INFO - training batch 501, loss: 0.040, 16032/56000 datapoints
2025-03-05 17:39:17,597 - INFO - training batch 551, loss: 0.083, 17632/56000 datapoints
2025-03-05 17:39:17,753 - INFO - training batch 601, loss: 0.010, 19232/56000 datapoints
2025-03-05 17:39:17,846 - INFO - training batch 651, loss: 0.018, 20832/56000 datapoints
2025-03-05 17:39:17,956 - INFO - training batch 701, loss: 0.231, 22432/56000 datapoints
2025-03-05 17:39:18,040 - INFO - training batch 751, loss: 0.027, 24032/56000 datapoints
2025-03-05 17:39:18,120 - INFO - training batch 801, loss: 0.042, 25632/56000 datapoints
2025-03-05 17:39:18,200 - INFO - training batch 851, loss: 0.018, 27232/56000 datapoints
2025-03-05 17:39:18,280 - INFO - training batch 901, loss: 0.034, 28832/56000 datapoints
2025-03-05 17:39:18,361 - INFO - training batch 951, loss: 0.032, 30432/56000 datapoints
2025-03-05 17:39:18,442 - INFO - training batch 1001, loss: 0.016, 32032/56000 datapoints
2025-03-05 17:39:18,523 - INFO - training batch 1051, loss: 0.020, 33632/56000 datapoints
2025-03-05 17:39:18,613 - INFO - training batch 1101, loss: 0.086, 35232/56000 datapoints
2025-03-05 17:39:18,699 - INFO - training batch 1151, loss: 0.005, 36832/56000 datapoints
2025-03-05 17:39:18,782 - INFO - training batch 1201, loss: 0.050, 38432/56000 datapoints
2025-03-05 17:39:18,872 - INFO - training batch 1251, loss: 0.223, 40032/56000 datapoints
2025-03-05 17:39:18,963 - INFO - training batch 1301, loss: 0.032, 41632/56000 datapoints
2025-03-05 17:39:19,047 - INFO - training batch 1351, loss: 0.012, 43232/56000 datapoints
2025-03-05 17:39:19,133 - INFO - training batch 1401, loss: 0.018, 44832/56000 datapoints
2025-03-05 17:39:19,219 - INFO - training batch 1451, loss: 0.016, 46432/56000 datapoints
2025-03-05 17:39:19,332 - INFO - training batch 1501, loss: 0.044, 48032/56000 datapoints
2025-03-05 17:39:19,444 - INFO - training batch 1551, loss: 0.018, 49632/56000 datapoints
2025-03-05 17:39:19,541 - INFO - training batch 1601, loss: 0.024, 51232/56000 datapoints
2025-03-05 17:39:19,646 - INFO - training batch 1651, loss: 0.019, 52832/56000 datapoints
2025-03-05 17:39:19,746 - INFO - training batch 1701, loss: 0.077, 54432/56000 datapoints
2025-03-05 17:39:19,859 - INFO - validation batch 1, loss: 0.015, 32/13984 datapoints
2025-03-05 17:39:19,930 - INFO - validation batch 51, loss: 0.029, 1632/13984 datapoints
2025-03-05 17:39:20,000 - INFO - validation batch 101, loss: 0.016, 3232/13984 datapoints
2025-03-05 17:39:20,062 - INFO - validation batch 151, loss: 0.017, 4832/13984 datapoints
2025-03-05 17:39:20,124 - INFO - validation batch 201, loss: 0.022, 6432/13984 datapoints
2025-03-05 17:39:20,193 - INFO - validation batch 251, loss: 0.026, 8032/13984 datapoints
2025-03-05 17:39:20,264 - INFO - validation batch 301, loss: 0.032, 9632/13984 datapoints
2025-03-05 17:39:20,328 - INFO - validation batch 351, loss: 0.019, 11232/13984 datapoints
2025-03-05 17:39:20,386 - INFO - validation batch 401, loss: 0.094, 12832/13984 datapoints
2025-03-05 17:39:20,440 - INFO - Epoch 10/50 done.
2025-03-05 17:39:20,440 - INFO - Final validation performance:
Loss: 0.030, top-1 acc: 0.992top-5 acc: 0.992
2025-03-05 17:39:20,441 - INFO - Epoch 10/50 done
2025-03-05 17:39:20,441 - INFO - Beginning epoch 11/50
2025-03-05 17:39:20,445 - INFO - training batch 1, loss: 0.146, 32/56000 datapoints
2025-03-05 17:39:20,598 - INFO - training batch 51, loss: 0.029, 1632/56000 datapoints
2025-03-05 17:39:20,719 - INFO - training batch 101, loss: 0.055, 3232/56000 datapoints
2025-03-05 17:39:20,835 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-05 17:39:20,948 - INFO - training batch 201, loss: 0.049, 6432/56000 datapoints
2025-03-05 17:39:21,087 - INFO - training batch 251, loss: 0.017, 8032/56000 datapoints
2025-03-05 17:39:21,201 - INFO - training batch 301, loss: 0.034, 9632/56000 datapoints
2025-03-05 17:39:21,321 - INFO - training batch 351, loss: 0.083, 11232/56000 datapoints
2025-03-05 17:39:21,453 - INFO - training batch 401, loss: 0.057, 12832/56000 datapoints
2025-03-05 17:39:21,550 - INFO - training batch 451, loss: 0.047, 14432/56000 datapoints
2025-03-05 17:39:21,658 - INFO - training batch 501, loss: 0.034, 16032/56000 datapoints
2025-03-05 17:39:21,772 - INFO - training batch 551, loss: 0.082, 17632/56000 datapoints
2025-03-05 17:39:21,897 - INFO - training batch 601, loss: 0.008, 19232/56000 datapoints
2025-03-05 17:39:22,016 - INFO - training batch 651, loss: 0.016, 20832/56000 datapoints
2025-03-05 17:39:22,119 - INFO - training batch 701, loss: 0.238, 22432/56000 datapoints
2025-03-05 17:39:22,221 - INFO - training batch 751, loss: 0.024, 24032/56000 datapoints
2025-03-05 17:39:22,323 - INFO - training batch 801, loss: 0.038, 25632/56000 datapoints
2025-03-05 17:39:22,423 - INFO - training batch 851, loss: 0.015, 27232/56000 datapoints
2025-03-05 17:39:22,523 - INFO - training batch 901, loss: 0.030, 28832/56000 datapoints
2025-03-05 17:39:22,640 - INFO - training batch 951, loss: 0.028, 30432/56000 datapoints
2025-03-05 17:39:22,738 - INFO - training batch 1001, loss: 0.014, 32032/56000 datapoints
2025-03-05 17:39:22,836 - INFO - training batch 1051, loss: 0.018, 33632/56000 datapoints
2025-03-05 17:39:22,953 - INFO - training batch 1101, loss: 0.084, 35232/56000 datapoints
2025-03-05 17:39:23,063 - INFO - training batch 1151, loss: 0.004, 36832/56000 datapoints
2025-03-05 17:39:23,168 - INFO - training batch 1201, loss: 0.046, 38432/56000 datapoints
2025-03-05 17:39:23,271 - INFO - training batch 1251, loss: 0.227, 40032/56000 datapoints
2025-03-05 17:39:23,381 - INFO - training batch 1301, loss: 0.029, 41632/56000 datapoints
2025-03-05 17:39:23,501 - INFO - training batch 1351, loss: 0.010, 43232/56000 datapoints
2025-03-05 17:39:23,620 - INFO - training batch 1401, loss: 0.015, 44832/56000 datapoints
2025-03-05 17:39:23,760 - INFO - training batch 1451, loss: 0.013, 46432/56000 datapoints
2025-03-05 17:39:23,885 - INFO - training batch 1501, loss: 0.040, 48032/56000 datapoints
2025-03-05 17:39:24,017 - INFO - training batch 1551, loss: 0.016, 49632/56000 datapoints
2025-03-05 17:39:24,153 - INFO - training batch 1601, loss: 0.021, 51232/56000 datapoints
2025-03-05 17:39:24,314 - INFO - training batch 1651, loss: 0.017, 52832/56000 datapoints
2025-03-05 17:39:24,425 - INFO - training batch 1701, loss: 0.078, 54432/56000 datapoints
2025-03-05 17:39:24,528 - INFO - validation batch 1, loss: 0.013, 32/13984 datapoints
2025-03-05 17:39:24,582 - INFO - validation batch 51, loss: 0.028, 1632/13984 datapoints
2025-03-05 17:39:24,662 - INFO - validation batch 101, loss: 0.014, 3232/13984 datapoints
2025-03-05 17:39:24,713 - INFO - validation batch 151, loss: 0.015, 4832/13984 datapoints
2025-03-05 17:39:24,764 - INFO - validation batch 201, loss: 0.019, 6432/13984 datapoints
2025-03-05 17:39:24,823 - INFO - validation batch 251, loss: 0.023, 8032/13984 datapoints
2025-03-05 17:39:24,878 - INFO - validation batch 301, loss: 0.028, 9632/13984 datapoints
2025-03-05 17:39:24,931 - INFO - validation batch 351, loss: 0.017, 11232/13984 datapoints
2025-03-05 17:39:24,985 - INFO - validation batch 401, loss: 0.094, 12832/13984 datapoints
2025-03-05 17:39:25,023 - INFO - Epoch 11/50 done.
2025-03-05 17:39:25,023 - INFO - Final validation performance:
Loss: 0.028, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:39:25,024 - INFO - Epoch 11/50 done
2025-03-05 17:39:25,024 - INFO - Beginning epoch 12/50
2025-03-05 17:39:25,027 - INFO - training batch 1, loss: 0.147, 32/56000 datapoints
2025-03-05 17:39:25,141 - INFO - training batch 51, loss: 0.025, 1632/56000 datapoints
2025-03-05 17:39:25,243 - INFO - training batch 101, loss: 0.056, 3232/56000 datapoints
2025-03-05 17:39:25,349 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-05 17:39:25,466 - INFO - training batch 201, loss: 0.048, 6432/56000 datapoints
2025-03-05 17:39:25,587 - INFO - training batch 251, loss: 0.015, 8032/56000 datapoints
2025-03-05 17:39:25,738 - INFO - training batch 301, loss: 0.031, 9632/56000 datapoints
2025-03-05 17:39:25,865 - INFO - training batch 351, loss: 0.082, 11232/56000 datapoints
2025-03-05 17:39:26,001 - INFO - training batch 401, loss: 0.055, 12832/56000 datapoints
2025-03-05 17:39:26,102 - INFO - training batch 451, loss: 0.043, 14432/56000 datapoints
2025-03-05 17:39:26,221 - INFO - training batch 501, loss: 0.030, 16032/56000 datapoints
2025-03-05 17:39:26,332 - INFO - training batch 551, loss: 0.081, 17632/56000 datapoints
2025-03-05 17:39:26,433 - INFO - training batch 601, loss: 0.007, 19232/56000 datapoints
2025-03-05 17:39:26,538 - INFO - training batch 651, loss: 0.014, 20832/56000 datapoints
2025-03-05 17:39:26,692 - INFO - training batch 701, loss: 0.244, 22432/56000 datapoints
2025-03-05 17:39:26,814 - INFO - training batch 751, loss: 0.021, 24032/56000 datapoints
2025-03-05 17:39:26,923 - INFO - training batch 801, loss: 0.035, 25632/56000 datapoints
2025-03-05 17:39:27,039 - INFO - training batch 851, loss: 0.013, 27232/56000 datapoints
2025-03-05 17:39:27,177 - INFO - training batch 901, loss: 0.027, 28832/56000 datapoints
2025-03-05 17:39:27,285 - INFO - training batch 951, loss: 0.025, 30432/56000 datapoints
2025-03-05 17:39:27,391 - INFO - training batch 1001, loss: 0.012, 32032/56000 datapoints
2025-03-05 17:39:27,499 - INFO - training batch 1051, loss: 0.016, 33632/56000 datapoints
2025-03-05 17:39:27,600 - INFO - training batch 1101, loss: 0.083, 35232/56000 datapoints
2025-03-05 17:39:27,710 - INFO - training batch 1151, loss: 0.004, 36832/56000 datapoints
2025-03-05 17:39:27,840 - INFO - training batch 1201, loss: 0.043, 38432/56000 datapoints
2025-03-05 17:39:27,949 - INFO - training batch 1251, loss: 0.231, 40032/56000 datapoints
2025-03-05 17:39:28,054 - INFO - training batch 1301, loss: 0.027, 41632/56000 datapoints
2025-03-05 17:39:28,157 - INFO - training batch 1351, loss: 0.009, 43232/56000 datapoints
2025-03-05 17:39:28,261 - INFO - training batch 1401, loss: 0.013, 44832/56000 datapoints
2025-03-05 17:39:28,368 - INFO - training batch 1451, loss: 0.011, 46432/56000 datapoints
2025-03-05 17:39:28,473 - INFO - training batch 1501, loss: 0.037, 48032/56000 datapoints
2025-03-05 17:39:28,602 - INFO - training batch 1551, loss: 0.014, 49632/56000 datapoints
2025-03-05 17:39:28,767 - INFO - training batch 1601, loss: 0.019, 51232/56000 datapoints
2025-03-05 17:39:28,909 - INFO - training batch 1651, loss: 0.016, 52832/56000 datapoints
2025-03-05 17:39:29,333 - INFO - training batch 1701, loss: 0.079, 54432/56000 datapoints
2025-03-05 17:39:29,986 - INFO - validation batch 1, loss: 0.012, 32/13984 datapoints
2025-03-05 17:39:30,145 - INFO - validation batch 51, loss: 0.027, 1632/13984 datapoints
2025-03-05 17:39:30,441 - INFO - validation batch 101, loss: 0.012, 3232/13984 datapoints
2025-03-05 17:39:30,612 - INFO - validation batch 151, loss: 0.014, 4832/13984 datapoints
2025-03-05 17:39:30,911 - INFO - validation batch 201, loss: 0.017, 6432/13984 datapoints
2025-03-05 17:39:31,128 - INFO - validation batch 251, loss: 0.020, 8032/13984 datapoints
2025-03-05 17:39:31,358 - INFO - validation batch 301, loss: 0.024, 9632/13984 datapoints
2025-03-05 17:39:32,668 - INFO - validation batch 351, loss: 0.015, 11232/13984 datapoints
2025-03-05 17:39:35,489 - INFO - validation batch 401, loss: 0.094, 12832/13984 datapoints
2025-03-05 17:39:38,160 - INFO - Epoch 12/50 done.
2025-03-05 17:39:38,170 - INFO - Final validation performance:
Loss: 0.026, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:39:38,200 - INFO - Epoch 12/50 done
2025-03-05 17:39:38,204 - INFO - Beginning epoch 13/50
2025-03-05 17:39:38,358 - INFO - training batch 1, loss: 0.147, 32/56000 datapoints
2025-03-05 17:39:39,053 - INFO - training batch 51, loss: 0.022, 1632/56000 datapoints
2025-03-05 17:39:39,455 - INFO - training batch 101, loss: 0.056, 3232/56000 datapoints
2025-03-05 17:39:39,695 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-05 17:39:39,889 - INFO - training batch 201, loss: 0.047, 6432/56000 datapoints
2025-03-05 17:39:40,048 - INFO - training batch 251, loss: 0.014, 8032/56000 datapoints
2025-03-05 17:39:40,198 - INFO - training batch 301, loss: 0.029, 9632/56000 datapoints
2025-03-05 17:39:40,375 - INFO - training batch 351, loss: 0.081, 11232/56000 datapoints
2025-03-05 17:39:40,581 - INFO - training batch 401, loss: 0.054, 12832/56000 datapoints
2025-03-05 17:39:40,757 - INFO - training batch 451, loss: 0.041, 14432/56000 datapoints
2025-03-05 17:39:40,902 - INFO - training batch 501, loss: 0.026, 16032/56000 datapoints
2025-03-05 17:39:41,037 - INFO - training batch 551, loss: 0.080, 17632/56000 datapoints
2025-03-05 17:39:41,180 - INFO - training batch 601, loss: 0.006, 19232/56000 datapoints
2025-03-05 17:39:41,318 - INFO - training batch 651, loss: 0.013, 20832/56000 datapoints
2025-03-05 17:39:41,469 - INFO - training batch 701, loss: 0.249, 22432/56000 datapoints
2025-03-05 17:39:41,617 - INFO - training batch 751, loss: 0.019, 24032/56000 datapoints
2025-03-05 17:39:41,775 - INFO - training batch 801, loss: 0.033, 25632/56000 datapoints
2025-03-05 17:39:41,945 - INFO - training batch 851, loss: 0.012, 27232/56000 datapoints
2025-03-05 17:39:42,114 - INFO - training batch 901, loss: 0.024, 28832/56000 datapoints
2025-03-05 17:39:42,275 - INFO - training batch 951, loss: 0.022, 30432/56000 datapoints
2025-03-05 17:39:42,405 - INFO - training batch 1001, loss: 0.011, 32032/56000 datapoints
2025-03-05 17:39:42,531 - INFO - training batch 1051, loss: 0.014, 33632/56000 datapoints
2025-03-05 17:39:42,666 - INFO - training batch 1101, loss: 0.083, 35232/56000 datapoints
2025-03-05 17:39:42,798 - INFO - training batch 1151, loss: 0.003, 36832/56000 datapoints
2025-03-05 17:39:42,937 - INFO - training batch 1201, loss: 0.040, 38432/56000 datapoints
2025-03-05 17:39:43,083 - INFO - training batch 1251, loss: 0.234, 40032/56000 datapoints
2025-03-05 17:39:43,225 - INFO - training batch 1301, loss: 0.026, 41632/56000 datapoints
2025-03-05 17:39:43,365 - INFO - training batch 1351, loss: 0.008, 43232/56000 datapoints
2025-03-05 17:39:43,501 - INFO - training batch 1401, loss: 0.012, 44832/56000 datapoints
2025-03-05 17:39:43,641 - INFO - training batch 1451, loss: 0.010, 46432/56000 datapoints
2025-03-05 17:39:43,771 - INFO - training batch 1501, loss: 0.035, 48032/56000 datapoints
2025-03-05 17:39:43,915 - INFO - training batch 1551, loss: 0.013, 49632/56000 datapoints
2025-03-05 17:39:44,049 - INFO - training batch 1601, loss: 0.017, 51232/56000 datapoints
2025-03-05 17:39:44,186 - INFO - training batch 1651, loss: 0.015, 52832/56000 datapoints
2025-03-05 17:39:44,331 - INFO - training batch 1701, loss: 0.079, 54432/56000 datapoints
2025-03-05 17:39:44,468 - INFO - validation batch 1, loss: 0.011, 32/13984 datapoints
2025-03-05 17:39:44,595 - INFO - validation batch 51, loss: 0.027, 1632/13984 datapoints
2025-03-05 17:39:44,695 - INFO - validation batch 101, loss: 0.011, 3232/13984 datapoints
2025-03-05 17:39:44,766 - INFO - validation batch 151, loss: 0.013, 4832/13984 datapoints
2025-03-05 17:39:44,834 - INFO - validation batch 201, loss: 0.016, 6432/13984 datapoints
2025-03-05 17:39:44,905 - INFO - validation batch 251, loss: 0.018, 8032/13984 datapoints
2025-03-05 17:39:44,974 - INFO - validation batch 301, loss: 0.022, 9632/13984 datapoints
2025-03-05 17:39:45,042 - INFO - validation batch 351, loss: 0.014, 11232/13984 datapoints
2025-03-05 17:39:45,110 - INFO - validation batch 401, loss: 0.094, 12832/13984 datapoints
2025-03-05 17:39:45,154 - INFO - Epoch 13/50 done.
2025-03-05 17:39:45,155 - INFO - Final validation performance:
Loss: 0.025, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:39:45,155 - INFO - Epoch 13/50 done
2025-03-05 17:39:45,156 - INFO - Beginning epoch 14/50
2025-03-05 17:39:45,159 - INFO - training batch 1, loss: 0.148, 32/56000 datapoints
2025-03-05 17:39:45,288 - INFO - training batch 51, loss: 0.019, 1632/56000 datapoints
2025-03-05 17:39:45,411 - INFO - training batch 101, loss: 0.057, 3232/56000 datapoints
2025-03-05 17:39:45,520 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-05 17:39:45,628 - INFO - training batch 201, loss: 0.046, 6432/56000 datapoints
2025-03-05 17:39:45,741 - INFO - training batch 251, loss: 0.013, 8032/56000 datapoints
2025-03-05 17:39:45,865 - INFO - training batch 301, loss: 0.028, 9632/56000 datapoints
2025-03-05 17:39:45,977 - INFO - training batch 351, loss: 0.080, 11232/56000 datapoints
2025-03-05 17:39:46,084 - INFO - training batch 401, loss: 0.053, 12832/56000 datapoints
2025-03-05 17:39:46,196 - INFO - training batch 451, loss: 0.038, 14432/56000 datapoints
2025-03-05 17:39:46,314 - INFO - training batch 501, loss: 0.023, 16032/56000 datapoints
2025-03-05 17:39:46,427 - INFO - training batch 551, loss: 0.079, 17632/56000 datapoints
2025-03-05 17:39:46,535 - INFO - training batch 601, loss: 0.005, 19232/56000 datapoints
2025-03-05 17:39:46,650 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-05 17:39:46,760 - INFO - training batch 701, loss: 0.253, 22432/56000 datapoints
2025-03-05 17:39:46,870 - INFO - training batch 751, loss: 0.017, 24032/56000 datapoints
2025-03-05 17:39:46,980 - INFO - training batch 801, loss: 0.031, 25632/56000 datapoints
2025-03-05 17:39:47,087 - INFO - training batch 851, loss: 0.011, 27232/56000 datapoints
2025-03-05 17:39:47,202 - INFO - training batch 901, loss: 0.022, 28832/56000 datapoints
2025-03-05 17:39:47,318 - INFO - training batch 951, loss: 0.020, 30432/56000 datapoints
2025-03-05 17:39:47,429 - INFO - training batch 1001, loss: 0.010, 32032/56000 datapoints
2025-03-05 17:39:47,537 - INFO - training batch 1051, loss: 0.013, 33632/56000 datapoints
2025-03-05 17:39:47,645 - INFO - training batch 1101, loss: 0.083, 35232/56000 datapoints
2025-03-05 17:39:47,758 - INFO - training batch 1151, loss: 0.003, 36832/56000 datapoints
2025-03-05 17:39:47,871 - INFO - training batch 1201, loss: 0.037, 38432/56000 datapoints
2025-03-05 17:39:47,980 - INFO - training batch 1251, loss: 0.236, 40032/56000 datapoints
2025-03-05 17:39:48,093 - INFO - training batch 1301, loss: 0.024, 41632/56000 datapoints
2025-03-05 17:39:48,232 - INFO - training batch 1351, loss: 0.007, 43232/56000 datapoints
2025-03-05 17:39:48,343 - INFO - training batch 1401, loss: 0.011, 44832/56000 datapoints
2025-03-05 17:39:48,454 - INFO - training batch 1451, loss: 0.009, 46432/56000 datapoints
2025-03-05 17:39:48,560 - INFO - training batch 1501, loss: 0.033, 48032/56000 datapoints
2025-03-05 17:39:48,673 - INFO - training batch 1551, loss: 0.012, 49632/56000 datapoints
2025-03-05 17:39:48,783 - INFO - training batch 1601, loss: 0.015, 51232/56000 datapoints
2025-03-05 17:39:48,893 - INFO - training batch 1651, loss: 0.015, 52832/56000 datapoints
2025-03-05 17:39:49,001 - INFO - training batch 1701, loss: 0.080, 54432/56000 datapoints
2025-03-05 17:39:49,110 - INFO - validation batch 1, loss: 0.010, 32/13984 datapoints
2025-03-05 17:39:49,171 - INFO - validation batch 51, loss: 0.026, 1632/13984 datapoints
2025-03-05 17:39:49,225 - INFO - validation batch 101, loss: 0.010, 3232/13984 datapoints
2025-03-05 17:39:49,279 - INFO - validation batch 151, loss: 0.013, 4832/13984 datapoints
2025-03-05 17:39:49,336 - INFO - validation batch 201, loss: 0.015, 6432/13984 datapoints
2025-03-05 17:39:49,392 - INFO - validation batch 251, loss: 0.016, 8032/13984 datapoints
2025-03-05 17:39:49,448 - INFO - validation batch 301, loss: 0.020, 9632/13984 datapoints
2025-03-05 17:39:49,502 - INFO - validation batch 351, loss: 0.013, 11232/13984 datapoints
2025-03-05 17:39:49,557 - INFO - validation batch 401, loss: 0.094, 12832/13984 datapoints
2025-03-05 17:39:49,596 - INFO - Epoch 14/50 done.
2025-03-05 17:39:49,596 - INFO - Final validation performance:
Loss: 0.024, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:39:49,597 - INFO - Epoch 14/50 done
2025-03-05 17:39:49,597 - INFO - Beginning epoch 15/50
2025-03-05 17:39:49,600 - INFO - training batch 1, loss: 0.149, 32/56000 datapoints
2025-03-05 17:39:49,714 - INFO - training batch 51, loss: 0.017, 1632/56000 datapoints
2025-03-05 17:39:49,836 - INFO - training batch 101, loss: 0.057, 3232/56000 datapoints
2025-03-05 17:39:49,954 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-05 17:39:50,062 - INFO - training batch 201, loss: 0.044, 6432/56000 datapoints
2025-03-05 17:39:50,171 - INFO - training batch 251, loss: 0.013, 8032/56000 datapoints
2025-03-05 17:39:50,285 - INFO - training batch 301, loss: 0.026, 9632/56000 datapoints
2025-03-05 17:39:50,410 - INFO - training batch 351, loss: 0.079, 11232/56000 datapoints
2025-03-05 17:39:50,529 - INFO - training batch 401, loss: 0.052, 12832/56000 datapoints
2025-03-05 17:39:50,639 - INFO - training batch 451, loss: 0.036, 14432/56000 datapoints
2025-03-05 17:39:50,750 - INFO - training batch 501, loss: 0.021, 16032/56000 datapoints
2025-03-05 17:39:50,863 - INFO - training batch 551, loss: 0.079, 17632/56000 datapoints
2025-03-05 17:39:50,974 - INFO - training batch 601, loss: 0.005, 19232/56000 datapoints
2025-03-05 17:39:51,081 - INFO - training batch 651, loss: 0.012, 20832/56000 datapoints
2025-03-05 17:39:51,198 - INFO - training batch 701, loss: 0.256, 22432/56000 datapoints
2025-03-05 17:39:51,320 - INFO - training batch 751, loss: 0.016, 24032/56000 datapoints
2025-03-05 17:39:51,439 - INFO - training batch 801, loss: 0.029, 25632/56000 datapoints
2025-03-05 17:39:51,548 - INFO - training batch 851, loss: 0.010, 27232/56000 datapoints
2025-03-05 17:39:51,662 - INFO - training batch 901, loss: 0.020, 28832/56000 datapoints
2025-03-05 17:39:51,789 - INFO - training batch 951, loss: 0.019, 30432/56000 datapoints
2025-03-05 17:39:52,131 - INFO - training batch 1001, loss: 0.009, 32032/56000 datapoints
2025-03-05 17:39:53,711 - INFO - training batch 1051, loss: 0.012, 33632/56000 datapoints
2025-03-05 17:39:54,177 - INFO - training batch 1101, loss: 0.083, 35232/56000 datapoints
2025-03-05 17:39:54,399 - INFO - training batch 1151, loss: 0.003, 36832/56000 datapoints
2025-03-05 17:39:54,583 - INFO - training batch 1201, loss: 0.035, 38432/56000 datapoints
2025-03-05 17:39:54,739 - INFO - training batch 1251, loss: 0.237, 40032/56000 datapoints
2025-03-05 17:39:54,860 - INFO - training batch 1301, loss: 0.023, 41632/56000 datapoints
2025-03-05 17:39:54,988 - INFO - training batch 1351, loss: 0.007, 43232/56000 datapoints
2025-03-05 17:39:55,111 - INFO - training batch 1401, loss: 0.010, 44832/56000 datapoints
2025-03-05 17:39:55,232 - INFO - training batch 1451, loss: 0.008, 46432/56000 datapoints
2025-03-05 17:39:55,342 - INFO - training batch 1501, loss: 0.031, 48032/56000 datapoints
2025-03-05 17:39:55,470 - INFO - training batch 1551, loss: 0.011, 49632/56000 datapoints
2025-03-05 17:39:55,606 - INFO - training batch 1601, loss: 0.014, 51232/56000 datapoints
2025-03-05 17:39:55,880 - INFO - training batch 1651, loss: 0.014, 52832/56000 datapoints
2025-03-05 17:39:55,994 - INFO - training batch 1701, loss: 0.080, 54432/56000 datapoints
2025-03-05 17:39:56,178 - INFO - validation batch 1, loss: 0.009, 32/13984 datapoints
2025-03-05 17:39:56,260 - INFO - validation batch 51, loss: 0.025, 1632/13984 datapoints
2025-03-05 17:39:56,343 - INFO - validation batch 101, loss: 0.009, 3232/13984 datapoints
2025-03-05 17:39:56,420 - INFO - validation batch 151, loss: 0.012, 4832/13984 datapoints
2025-03-05 17:39:56,500 - INFO - validation batch 201, loss: 0.014, 6432/13984 datapoints
2025-03-05 17:39:56,584 - INFO - validation batch 251, loss: 0.015, 8032/13984 datapoints
2025-03-05 17:39:56,661 - INFO - validation batch 301, loss: 0.018, 9632/13984 datapoints
2025-03-05 17:39:56,762 - INFO - validation batch 351, loss: 0.012, 11232/13984 datapoints
2025-03-05 17:39:56,868 - INFO - validation batch 401, loss: 0.094, 12832/13984 datapoints
2025-03-05 17:39:56,943 - INFO - Epoch 15/50 done.
2025-03-05 17:39:56,944 - INFO - Final validation performance:
Loss: 0.023, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:39:56,949 - INFO - Epoch 15/50 done
2025-03-05 17:39:56,950 - INFO - Beginning epoch 16/50
2025-03-05 17:39:56,954 - INFO - training batch 1, loss: 0.150, 32/56000 datapoints
2025-03-05 17:39:57,161 - INFO - training batch 51, loss: 0.016, 1632/56000 datapoints
2025-03-05 17:39:57,403 - INFO - training batch 101, loss: 0.058, 3232/56000 datapoints
2025-03-05 17:39:57,575 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-05 17:39:57,752 - INFO - training batch 201, loss: 0.043, 6432/56000 datapoints
2025-03-05 17:39:57,884 - INFO - training batch 251, loss: 0.012, 8032/56000 datapoints
2025-03-05 17:39:58,056 - INFO - training batch 301, loss: 0.025, 9632/56000 datapoints
2025-03-05 17:39:58,201 - INFO - training batch 351, loss: 0.078, 11232/56000 datapoints
2025-03-05 17:39:58,356 - INFO - training batch 401, loss: 0.052, 12832/56000 datapoints
2025-03-05 17:39:58,496 - INFO - training batch 451, loss: 0.035, 14432/56000 datapoints
2025-03-05 17:39:58,636 - INFO - training batch 501, loss: 0.019, 16032/56000 datapoints
2025-03-05 17:39:58,762 - INFO - training batch 551, loss: 0.078, 17632/56000 datapoints
2025-03-05 17:39:58,889 - INFO - training batch 601, loss: 0.004, 19232/56000 datapoints
2025-03-05 17:39:59,021 - INFO - training batch 651, loss: 0.011, 20832/56000 datapoints
2025-03-05 17:39:59,168 - INFO - training batch 701, loss: 0.259, 22432/56000 datapoints
2025-03-05 17:39:59,331 - INFO - training batch 751, loss: 0.015, 24032/56000 datapoints
2025-03-05 17:39:59,478 - INFO - training batch 801, loss: 0.027, 25632/56000 datapoints
2025-03-05 17:39:59,806 - INFO - training batch 851, loss: 0.009, 27232/56000 datapoints
2025-03-05 17:40:00,344 - INFO - training batch 901, loss: 0.019, 28832/56000 datapoints
2025-03-05 17:40:00,526 - INFO - training batch 951, loss: 0.018, 30432/56000 datapoints
2025-03-05 17:40:00,819 - INFO - training batch 1001, loss: 0.008, 32032/56000 datapoints
2025-03-05 17:40:01,046 - INFO - training batch 1051, loss: 0.011, 33632/56000 datapoints
2025-03-05 17:40:01,252 - INFO - training batch 1101, loss: 0.083, 35232/56000 datapoints
2025-03-05 17:40:01,396 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-05 17:40:01,507 - INFO - training batch 1201, loss: 0.033, 38432/56000 datapoints
2025-03-05 17:40:01,619 - INFO - training batch 1251, loss: 0.238, 40032/56000 datapoints
2025-03-05 17:40:01,740 - INFO - training batch 1301, loss: 0.023, 41632/56000 datapoints
2025-03-05 17:40:01,855 - INFO - training batch 1351, loss: 0.006, 43232/56000 datapoints
2025-03-05 17:40:01,992 - INFO - training batch 1401, loss: 0.009, 44832/56000 datapoints
2025-03-05 17:40:02,100 - INFO - training batch 1451, loss: 0.007, 46432/56000 datapoints
2025-03-05 17:40:02,228 - INFO - training batch 1501, loss: 0.030, 48032/56000 datapoints
2025-03-05 17:40:02,338 - INFO - training batch 1551, loss: 0.010, 49632/56000 datapoints
2025-03-05 17:40:02,473 - INFO - training batch 1601, loss: 0.013, 51232/56000 datapoints
2025-03-05 17:40:02,589 - INFO - training batch 1651, loss: 0.014, 52832/56000 datapoints
2025-03-05 17:40:02,755 - INFO - training batch 1701, loss: 0.081, 54432/56000 datapoints
2025-03-05 17:40:02,952 - INFO - validation batch 1, loss: 0.008, 32/13984 datapoints
2025-03-05 17:40:03,025 - INFO - validation batch 51, loss: 0.024, 1632/13984 datapoints
2025-03-05 17:40:03,102 - INFO - validation batch 101, loss: 0.008, 3232/13984 datapoints
2025-03-05 17:40:03,171 - INFO - validation batch 151, loss: 0.012, 4832/13984 datapoints
2025-03-05 17:40:03,262 - INFO - validation batch 201, loss: 0.013, 6432/13984 datapoints
2025-03-05 17:40:03,362 - INFO - validation batch 251, loss: 0.014, 8032/13984 datapoints
2025-03-05 17:40:03,477 - INFO - validation batch 301, loss: 0.017, 9632/13984 datapoints
2025-03-05 17:40:03,557 - INFO - validation batch 351, loss: 0.011, 11232/13984 datapoints
2025-03-05 17:40:03,640 - INFO - validation batch 401, loss: 0.094, 12832/13984 datapoints
2025-03-05 17:40:03,682 - INFO - Epoch 16/50 done.
2025-03-05 17:40:03,683 - INFO - Final validation performance:
Loss: 0.022, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:40:03,684 - INFO - Epoch 16/50 done
2025-03-05 17:40:03,684 - INFO - Beginning epoch 17/50
2025-03-05 17:40:03,688 - INFO - training batch 1, loss: 0.151, 32/56000 datapoints
2025-03-05 17:40:03,832 - INFO - training batch 51, loss: 0.015, 1632/56000 datapoints
2025-03-05 17:40:04,010 - INFO - training batch 101, loss: 0.058, 3232/56000 datapoints
2025-03-05 17:40:04,157 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-05 17:40:04,304 - INFO - training batch 201, loss: 0.042, 6432/56000 datapoints
2025-03-05 17:40:04,414 - INFO - training batch 251, loss: 0.012, 8032/56000 datapoints
2025-03-05 17:40:04,528 - INFO - training batch 301, loss: 0.025, 9632/56000 datapoints
2025-03-05 17:40:04,652 - INFO - training batch 351, loss: 0.078, 11232/56000 datapoints
2025-03-05 17:40:04,764 - INFO - training batch 401, loss: 0.051, 12832/56000 datapoints
2025-03-05 17:40:04,877 - INFO - training batch 451, loss: 0.034, 14432/56000 datapoints
2025-03-05 17:40:05,009 - INFO - training batch 501, loss: 0.018, 16032/56000 datapoints
2025-03-05 17:40:05,119 - INFO - training batch 551, loss: 0.077, 17632/56000 datapoints
2025-03-05 17:40:05,234 - INFO - training batch 601, loss: 0.004, 19232/56000 datapoints
2025-03-05 17:40:05,345 - INFO - training batch 651, loss: 0.011, 20832/56000 datapoints
2025-03-05 17:40:05,457 - INFO - training batch 701, loss: 0.262, 22432/56000 datapoints
2025-03-05 17:40:05,584 - INFO - training batch 751, loss: 0.014, 24032/56000 datapoints
2025-03-05 17:40:05,712 - INFO - training batch 801, loss: 0.026, 25632/56000 datapoints
2025-03-05 17:40:05,823 - INFO - training batch 851, loss: 0.008, 27232/56000 datapoints
2025-03-05 17:40:05,944 - INFO - training batch 901, loss: 0.018, 28832/56000 datapoints
2025-03-05 17:40:06,055 - INFO - training batch 951, loss: 0.017, 30432/56000 datapoints
2025-03-05 17:40:06,163 - INFO - training batch 1001, loss: 0.008, 32032/56000 datapoints
2025-03-05 17:40:06,271 - INFO - training batch 1051, loss: 0.011, 33632/56000 datapoints
2025-03-05 17:40:06,384 - INFO - training batch 1101, loss: 0.083, 35232/56000 datapoints
2025-03-05 17:40:06,492 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-05 17:40:06,605 - INFO - training batch 1201, loss: 0.031, 38432/56000 datapoints
2025-03-05 17:40:06,728 - INFO - training batch 1251, loss: 0.239, 40032/56000 datapoints
2025-03-05 17:40:06,836 - INFO - training batch 1301, loss: 0.022, 41632/56000 datapoints
2025-03-05 17:40:06,981 - INFO - training batch 1351, loss: 0.006, 43232/56000 datapoints
2025-03-05 17:40:07,125 - INFO - training batch 1401, loss: 0.008, 44832/56000 datapoints
2025-03-05 17:40:07,248 - INFO - training batch 1451, loss: 0.007, 46432/56000 datapoints
2025-03-05 17:40:07,361 - INFO - training batch 1501, loss: 0.029, 48032/56000 datapoints
2025-03-05 17:40:07,468 - INFO - training batch 1551, loss: 0.010, 49632/56000 datapoints
2025-03-05 17:40:07,577 - INFO - training batch 1601, loss: 0.013, 51232/56000 datapoints
2025-03-05 17:40:07,692 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:40:07,802 - INFO - training batch 1701, loss: 0.082, 54432/56000 datapoints
2025-03-05 17:40:07,908 - INFO - validation batch 1, loss: 0.008, 32/13984 datapoints
2025-03-05 17:40:07,963 - INFO - validation batch 51, loss: 0.023, 1632/13984 datapoints
2025-03-05 17:40:08,024 - INFO - validation batch 101, loss: 0.007, 3232/13984 datapoints
2025-03-05 17:40:08,089 - INFO - validation batch 151, loss: 0.011, 4832/13984 datapoints
2025-03-05 17:40:08,155 - INFO - validation batch 201, loss: 0.012, 6432/13984 datapoints
2025-03-05 17:40:08,220 - INFO - validation batch 251, loss: 0.013, 8032/13984 datapoints
2025-03-05 17:40:08,291 - INFO - validation batch 301, loss: 0.016, 9632/13984 datapoints
2025-03-05 17:40:08,379 - INFO - validation batch 351, loss: 0.010, 11232/13984 datapoints
2025-03-05 17:40:08,457 - INFO - validation batch 401, loss: 0.093, 12832/13984 datapoints
2025-03-05 17:40:08,509 - INFO - Epoch 17/50 done.
2025-03-05 17:40:08,509 - INFO - Final validation performance:
Loss: 0.022, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:40:08,509 - INFO - Epoch 17/50 done
2025-03-05 17:40:08,510 - INFO - Beginning epoch 18/50
2025-03-05 17:40:08,514 - INFO - training batch 1, loss: 0.152, 32/56000 datapoints
2025-03-05 17:40:08,647 - INFO - training batch 51, loss: 0.014, 1632/56000 datapoints
2025-03-05 17:40:08,759 - INFO - training batch 101, loss: 0.058, 3232/56000 datapoints
2025-03-05 17:40:08,869 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-05 17:40:08,987 - INFO - training batch 201, loss: 0.041, 6432/56000 datapoints
2025-03-05 17:40:09,100 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 17:40:09,209 - INFO - training batch 301, loss: 0.024, 9632/56000 datapoints
2025-03-05 17:40:09,324 - INFO - training batch 351, loss: 0.077, 11232/56000 datapoints
2025-03-05 17:40:09,432 - INFO - training batch 401, loss: 0.051, 12832/56000 datapoints
2025-03-05 17:40:09,541 - INFO - training batch 451, loss: 0.032, 14432/56000 datapoints
2025-03-05 17:40:09,656 - INFO - training batch 501, loss: 0.016, 16032/56000 datapoints
2025-03-05 17:40:09,766 - INFO - training batch 551, loss: 0.076, 17632/56000 datapoints
2025-03-05 17:40:09,875 - INFO - training batch 601, loss: 0.004, 19232/56000 datapoints
2025-03-05 17:40:09,990 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 17:40:10,166 - INFO - training batch 701, loss: 0.264, 22432/56000 datapoints
2025-03-05 17:40:10,329 - INFO - training batch 751, loss: 0.013, 24032/56000 datapoints
2025-03-05 17:40:10,518 - INFO - training batch 801, loss: 0.025, 25632/56000 datapoints
2025-03-05 17:40:10,778 - INFO - training batch 851, loss: 0.008, 27232/56000 datapoints
2025-03-05 17:40:10,952 - INFO - training batch 901, loss: 0.016, 28832/56000 datapoints
2025-03-05 17:40:11,109 - INFO - training batch 951, loss: 0.016, 30432/56000 datapoints
2025-03-05 17:40:11,264 - INFO - training batch 1001, loss: 0.007, 32032/56000 datapoints
2025-03-05 17:40:11,376 - INFO - training batch 1051, loss: 0.010, 33632/56000 datapoints
2025-03-05 17:40:11,482 - INFO - training batch 1101, loss: 0.083, 35232/56000 datapoints
2025-03-05 17:40:11,589 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-05 17:40:11,707 - INFO - training batch 1201, loss: 0.030, 38432/56000 datapoints
2025-03-05 17:40:11,814 - INFO - training batch 1251, loss: 0.239, 40032/56000 datapoints
2025-03-05 17:40:11,924 - INFO - training batch 1301, loss: 0.021, 41632/56000 datapoints
2025-03-05 17:40:12,034 - INFO - training batch 1351, loss: 0.005, 43232/56000 datapoints
2025-03-05 17:40:27,595 - INFO - training batch 1401, loss: 0.008, 44832/56000 datapoints
2025-03-05 17:40:27,710 - INFO - training batch 1451, loss: 0.006, 46432/56000 datapoints
2025-03-05 17:40:27,817 - INFO - training batch 1501, loss: 0.028, 48032/56000 datapoints
2025-03-05 17:40:27,924 - INFO - training batch 1551, loss: 0.009, 49632/56000 datapoints
2025-03-05 17:40:28,035 - INFO - training batch 1601, loss: 0.012, 51232/56000 datapoints
2025-03-05 17:40:28,143 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:40:29,699 - INFO - training batch 1701, loss: 0.082, 54432/56000 datapoints
2025-03-05 17:40:29,968 - INFO - validation batch 1, loss: 0.007, 32/13984 datapoints
2025-03-05 17:40:30,098 - INFO - validation batch 51, loss: 0.023, 1632/13984 datapoints
2025-03-05 17:40:30,187 - INFO - validation batch 101, loss: 0.007, 3232/13984 datapoints
2025-03-05 17:40:30,297 - INFO - validation batch 151, loss: 0.011, 4832/13984 datapoints
2025-03-05 17:40:30,434 - INFO - validation batch 201, loss: 0.011, 6432/13984 datapoints
2025-03-05 17:40:30,550 - INFO - validation batch 251, loss: 0.012, 8032/13984 datapoints
2025-03-05 17:40:30,735 - INFO - validation batch 301, loss: 0.015, 9632/13984 datapoints
2025-03-05 17:40:30,966 - INFO - validation batch 351, loss: 0.010, 11232/13984 datapoints
2025-03-05 17:40:33,134 - INFO - Epoch 18/50 done.
2025-03-05 17:40:33,306 - INFO - Final validation performance:
Loss: 0.021, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:40:33,335 - INFO - Epoch 18/50 done
2025-03-05 17:40:33,339 - INFO - Beginning epoch 19/50
2025-03-05 17:40:33,442 - INFO - training batch 1, loss: 0.153, 32/56000 datapoints
2025-03-05 17:40:41,102 - INFO - training batch 51, loss: 0.013, 1632/56000 datapoints
2025-03-05 17:40:46,600 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-05 17:40:50,069 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-05 17:40:54,421 - INFO - training batch 201, loss: 0.040, 6432/56000 datapoints
2025-03-05 17:40:56,336 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 17:40:58,028 - INFO - training batch 301, loss: 0.024, 9632/56000 datapoints
2025-03-05 17:41:00,136 - INFO - training batch 351, loss: 0.076, 11232/56000 datapoints
2025-03-05 17:41:01,530 - INFO - training batch 401, loss: 0.050, 12832/56000 datapoints
2025-03-05 17:41:02,990 - INFO - training batch 451, loss: 0.031, 14432/56000 datapoints
2025-03-05 17:41:04,994 - INFO - training batch 501, loss: 0.015, 16032/56000 datapoints
2025-03-05 17:41:54,409 - INFO - training batch 551, loss: 0.075, 17632/56000 datapoints
2025-03-05 17:41:54,691 - INFO - training batch 601, loss: 0.003, 19232/56000 datapoints
2025-03-05 17:41:54,907 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 17:41:55,097 - INFO - training batch 701, loss: 0.266, 22432/56000 datapoints
2025-03-05 17:41:55,261 - INFO - training batch 751, loss: 0.012, 24032/56000 datapoints
2025-03-05 17:41:55,403 - INFO - training batch 801, loss: 0.024, 25632/56000 datapoints
2025-03-05 17:41:55,548 - INFO - training batch 851, loss: 0.007, 27232/56000 datapoints
2025-03-05 17:41:55,724 - INFO - training batch 901, loss: 0.015, 28832/56000 datapoints
2025-03-05 17:41:55,875 - INFO - training batch 951, loss: 0.015, 30432/56000 datapoints
2025-03-05 17:41:56,065 - INFO - training batch 1001, loss: 0.007, 32032/56000 datapoints
2025-03-05 17:41:56,216 - INFO - training batch 1051, loss: 0.010, 33632/56000 datapoints
2025-03-05 17:41:56,357 - INFO - training batch 1101, loss: 0.083, 35232/56000 datapoints
2025-03-05 17:41:56,485 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-05 17:41:56,650 - INFO - training batch 1201, loss: 0.029, 38432/56000 datapoints
2025-03-05 17:41:56,789 - INFO - training batch 1251, loss: 0.239, 40032/56000 datapoints
2025-03-05 17:41:56,959 - INFO - training batch 1301, loss: 0.021, 41632/56000 datapoints
2025-03-05 17:41:57,104 - INFO - training batch 1351, loss: 0.005, 43232/56000 datapoints
2025-03-05 17:41:57,232 - INFO - training batch 1401, loss: 0.007, 44832/56000 datapoints
2025-03-05 17:41:57,363 - INFO - training batch 1451, loss: 0.006, 46432/56000 datapoints
2025-03-05 17:41:57,506 - INFO - training batch 1501, loss: 0.027, 48032/56000 datapoints
2025-03-05 17:41:57,635 - INFO - training batch 1551, loss: 0.009, 49632/56000 datapoints
2025-03-05 17:41:57,776 - INFO - training batch 1601, loss: 0.011, 51232/56000 datapoints
2025-03-05 17:41:57,901 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:41:58,047 - INFO - training batch 1701, loss: 0.082, 54432/56000 datapoints
2025-03-05 17:41:58,196 - INFO - validation batch 1, loss: 0.007, 32/13984 datapoints
2025-03-05 17:41:58,281 - INFO - validation batch 51, loss: 0.022, 1632/13984 datapoints
2025-03-05 17:41:58,379 - INFO - validation batch 101, loss: 0.007, 3232/13984 datapoints
2025-03-05 17:41:58,485 - INFO - validation batch 151, loss: 0.011, 4832/13984 datapoints
2025-03-05 17:41:58,595 - INFO - validation batch 201, loss: 0.011, 6432/13984 datapoints
2025-03-05 17:41:58,698 - INFO - validation batch 251, loss: 0.011, 8032/13984 datapoints
2025-03-05 17:41:58,780 - INFO - validation batch 301, loss: 0.014, 9632/13984 datapoints
2025-03-05 17:41:58,853 - INFO - validation batch 351, loss: 0.009, 11232/13984 datapoints
2025-03-05 17:41:58,928 - INFO - validation batch 401, loss: 0.093, 12832/13984 datapoints
2025-03-05 17:41:59,000 - INFO - Epoch 19/50 done.
2025-03-05 17:41:59,005 - INFO - Final validation performance:
Loss: 0.020, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:41:59,011 - INFO - Epoch 19/50 done
2025-03-05 17:41:59,016 - INFO - Beginning epoch 20/50
2025-03-05 17:41:59,022 - INFO - training batch 1, loss: 0.154, 32/56000 datapoints
2025-03-05 17:41:59,148 - INFO - training batch 51, loss: 0.012, 1632/56000 datapoints
2025-03-05 17:41:59,496 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-05 17:41:59,743 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-05 17:41:59,892 - INFO - training batch 201, loss: 0.039, 6432/56000 datapoints
2025-03-05 17:42:00,038 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 17:42:00,167 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-05 17:42:00,276 - INFO - training batch 351, loss: 0.075, 11232/56000 datapoints
2025-03-05 17:42:00,384 - INFO - training batch 401, loss: 0.050, 12832/56000 datapoints
2025-03-05 17:42:00,500 - INFO - training batch 451, loss: 0.030, 14432/56000 datapoints
2025-03-05 17:42:00,640 - INFO - training batch 501, loss: 0.014, 16032/56000 datapoints
2025-03-05 17:42:00,778 - INFO - training batch 551, loss: 0.074, 17632/56000 datapoints
2025-03-05 17:42:00,893 - INFO - training batch 601, loss: 0.003, 19232/56000 datapoints
2025-03-05 17:42:01,005 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 17:42:01,117 - INFO - training batch 701, loss: 0.267, 22432/56000 datapoints
2025-03-05 17:42:01,255 - INFO - training batch 751, loss: 0.012, 24032/56000 datapoints
2025-03-05 17:42:01,378 - INFO - training batch 801, loss: 0.023, 25632/56000 datapoints
2025-03-05 17:42:01,492 - INFO - training batch 851, loss: 0.007, 27232/56000 datapoints
2025-03-05 17:42:01,605 - INFO - training batch 901, loss: 0.015, 28832/56000 datapoints
2025-03-05 17:42:01,712 - INFO - training batch 951, loss: 0.014, 30432/56000 datapoints
2025-03-05 17:42:01,827 - INFO - training batch 1001, loss: 0.007, 32032/56000 datapoints
2025-03-05 17:42:01,949 - INFO - training batch 1051, loss: 0.009, 33632/56000 datapoints
2025-03-05 17:42:02,098 - INFO - training batch 1101, loss: 0.084, 35232/56000 datapoints
2025-03-05 17:42:02,206 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-05 17:42:02,314 - INFO - training batch 1201, loss: 0.027, 38432/56000 datapoints
2025-03-05 17:42:02,424 - INFO - training batch 1251, loss: 0.238, 40032/56000 datapoints
2025-03-05 17:42:02,566 - INFO - training batch 1301, loss: 0.021, 41632/56000 datapoints
2025-03-05 17:42:02,675 - INFO - training batch 1351, loss: 0.005, 43232/56000 datapoints
2025-03-05 17:42:02,788 - INFO - training batch 1401, loss: 0.007, 44832/56000 datapoints
2025-03-05 17:42:02,898 - INFO - training batch 1451, loss: 0.005, 46432/56000 datapoints
2025-03-05 17:42:03,008 - INFO - training batch 1501, loss: 0.027, 48032/56000 datapoints
2025-03-05 17:42:03,124 - INFO - training batch 1551, loss: 0.009, 49632/56000 datapoints
2025-03-05 17:42:03,255 - INFO - training batch 1601, loss: 0.011, 51232/56000 datapoints
2025-03-05 17:42:03,411 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:42:03,534 - INFO - training batch 1701, loss: 0.083, 54432/56000 datapoints
2025-03-05 17:42:03,867 - INFO - validation batch 1, loss: 0.006, 32/13984 datapoints
2025-03-05 17:42:04,338 - INFO - validation batch 51, loss: 0.022, 1632/13984 datapoints
2025-03-05 17:42:04,985 - INFO - validation batch 101, loss: 0.006, 3232/13984 datapoints
2025-03-05 17:42:05,816 - INFO - validation batch 151, loss: 0.010, 4832/13984 datapoints
2025-03-05 17:42:06,726 - INFO - validation batch 201, loss: 0.010, 6432/13984 datapoints
2025-03-05 17:42:07,461 - INFO - validation batch 251, loss: 0.010, 8032/13984 datapoints
2025-03-05 17:42:08,764 - INFO - validation batch 301, loss: 0.013, 9632/13984 datapoints
2025-03-05 17:42:10,663 - INFO - validation batch 351, loss: 0.009, 11232/13984 datapoints
2025-03-05 17:42:11,690 - INFO - validation batch 401, loss: 0.093, 12832/13984 datapoints
2025-03-05 17:42:12,353 - INFO - Epoch 20/50 done.
2025-03-05 17:42:12,355 - INFO - Final validation performance:
Loss: 0.020, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:42:12,361 - INFO - Epoch 20/50 done
2025-03-05 17:42:12,365 - INFO - Beginning epoch 21/50
2025-03-05 17:42:12,389 - INFO - training batch 1, loss: 0.155, 32/56000 datapoints
2025-03-05 17:42:13,896 - INFO - training batch 51, loss: 0.011, 1632/56000 datapoints
2025-03-05 17:44:32,700 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-05 17:44:32,842 - INFO - training batch 151, loss: 0.062, 4832/56000 datapoints
2025-03-05 17:44:32,954 - INFO - training batch 201, loss: 0.038, 6432/56000 datapoints
2025-03-05 17:44:34,613 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 17:44:34,912 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-05 17:44:35,047 - INFO - training batch 351, loss: 0.074, 11232/56000 datapoints
2025-03-05 17:44:35,163 - INFO - training batch 401, loss: 0.050, 12832/56000 datapoints
2025-03-05 17:44:35,289 - INFO - training batch 451, loss: 0.029, 14432/56000 datapoints
2025-03-05 17:44:35,400 - INFO - training batch 501, loss: 0.013, 16032/56000 datapoints
2025-03-05 17:44:35,516 - INFO - training batch 551, loss: 0.073, 17632/56000 datapoints
2025-03-05 17:44:35,627 - INFO - training batch 601, loss: 0.003, 19232/56000 datapoints
2025-03-05 17:44:35,774 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 17:44:35,905 - INFO - training batch 701, loss: 0.268, 22432/56000 datapoints
2025-03-05 17:44:36,013 - INFO - training batch 751, loss: 0.011, 24032/56000 datapoints
2025-03-05 17:44:36,130 - INFO - training batch 801, loss: 0.022, 25632/56000 datapoints
2025-03-05 17:44:36,241 - INFO - training batch 851, loss: 0.007, 27232/56000 datapoints
2025-03-05 17:44:36,361 - INFO - training batch 901, loss: 0.014, 28832/56000 datapoints
2025-03-05 17:44:36,475 - INFO - training batch 951, loss: 0.014, 30432/56000 datapoints
2025-03-05 17:44:36,583 - INFO - training batch 1001, loss: 0.006, 32032/56000 datapoints
2025-03-05 17:44:36,728 - INFO - training batch 1051, loss: 0.009, 33632/56000 datapoints
2025-03-05 17:44:36,841 - INFO - training batch 1101, loss: 0.084, 35232/56000 datapoints
2025-03-05 17:44:36,950 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-05 17:44:37,060 - INFO - training batch 1201, loss: 0.026, 38432/56000 datapoints
2025-03-05 17:44:37,180 - INFO - training batch 1251, loss: 0.238, 40032/56000 datapoints
2025-03-05 17:44:37,293 - INFO - training batch 1301, loss: 0.020, 41632/56000 datapoints
2025-03-05 17:44:37,404 - INFO - training batch 1351, loss: 0.004, 43232/56000 datapoints
2025-03-05 17:44:37,518 - INFO - training batch 1401, loss: 0.007, 44832/56000 datapoints
2025-03-05 17:44:37,631 - INFO - training batch 1451, loss: 0.005, 46432/56000 datapoints
2025-03-05 17:44:37,748 - INFO - training batch 1501, loss: 0.026, 48032/56000 datapoints
2025-03-05 17:44:37,860 - INFO - training batch 1551, loss: 0.008, 49632/56000 datapoints
2025-03-05 17:44:37,966 - INFO - training batch 1601, loss: 0.011, 51232/56000 datapoints
2025-03-05 17:44:38,075 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:44:38,211 - INFO - training batch 1701, loss: 0.083, 54432/56000 datapoints
2025-03-05 17:44:38,327 - INFO - validation batch 1, loss: 0.006, 32/13984 datapoints
2025-03-05 17:44:38,398 - INFO - validation batch 51, loss: 0.021, 1632/13984 datapoints
2025-03-05 17:44:38,483 - INFO - validation batch 101, loss: 0.006, 3232/13984 datapoints
2025-03-05 17:44:38,554 - INFO - validation batch 151, loss: 0.010, 4832/13984 datapoints
2025-03-05 17:44:38,620 - INFO - validation batch 201, loss: 0.010, 6432/13984 datapoints
2025-03-05 17:44:38,685 - INFO - validation batch 251, loss: 0.010, 8032/13984 datapoints
2025-03-05 17:44:38,780 - INFO - validation batch 301, loss: 0.013, 9632/13984 datapoints
2025-03-05 17:44:38,850 - INFO - validation batch 351, loss: 0.009, 11232/13984 datapoints
2025-03-05 17:44:38,915 - INFO - validation batch 401, loss: 0.093, 12832/13984 datapoints
2025-03-05 17:44:38,954 - INFO - Epoch 21/50 done.
2025-03-05 17:44:38,954 - INFO - Final validation performance:
Loss: 0.020, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:44:38,955 - INFO - Epoch 21/50 done
2025-03-05 17:44:38,955 - INFO - Beginning epoch 22/50
2025-03-05 17:44:38,958 - INFO - training batch 1, loss: 0.155, 32/56000 datapoints
2025-03-05 17:44:39,066 - INFO - training batch 51, loss: 0.011, 1632/56000 datapoints
2025-03-05 17:44:39,178 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-05 17:44:39,307 - INFO - training batch 151, loss: 0.061, 4832/56000 datapoints
2025-03-05 17:44:39,454 - INFO - training batch 201, loss: 0.037, 6432/56000 datapoints
2025-03-05 17:44:39,982 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 17:44:40,174 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-05 17:44:40,336 - INFO - training batch 351, loss: 0.073, 11232/56000 datapoints
2025-03-05 17:44:40,485 - INFO - training batch 401, loss: 0.049, 12832/56000 datapoints
2025-03-05 17:44:40,612 - INFO - training batch 451, loss: 0.029, 14432/56000 datapoints
2025-03-05 17:44:40,744 - INFO - training batch 501, loss: 0.013, 16032/56000 datapoints
2025-03-05 17:44:40,869 - INFO - training batch 551, loss: 0.072, 17632/56000 datapoints
2025-03-05 17:44:40,992 - INFO - training batch 601, loss: 0.003, 19232/56000 datapoints
2025-03-05 17:44:41,102 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 17:44:41,211 - INFO - training batch 701, loss: 0.269, 22432/56000 datapoints
2025-03-05 17:44:41,324 - INFO - training batch 751, loss: 0.011, 24032/56000 datapoints
2025-03-05 17:44:41,432 - INFO - training batch 801, loss: 0.021, 25632/56000 datapoints
2025-03-05 17:44:41,548 - INFO - training batch 851, loss: 0.006, 27232/56000 datapoints
2025-03-05 17:44:41,660 - INFO - training batch 901, loss: 0.013, 28832/56000 datapoints
2025-03-05 17:44:41,767 - INFO - training batch 951, loss: 0.013, 30432/56000 datapoints
2025-03-05 17:44:41,912 - INFO - training batch 1001, loss: 0.006, 32032/56000 datapoints
2025-03-05 17:44:42,022 - INFO - training batch 1051, loss: 0.008, 33632/56000 datapoints
2025-03-05 17:44:42,128 - INFO - training batch 1101, loss: 0.084, 35232/56000 datapoints
2025-03-05 17:44:42,235 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-05 17:44:42,346 - INFO - training batch 1201, loss: 0.025, 38432/56000 datapoints
2025-03-05 17:44:42,454 - INFO - training batch 1251, loss: 0.237, 40032/56000 datapoints
2025-03-05 17:44:42,581 - INFO - training batch 1301, loss: 0.020, 41632/56000 datapoints
2025-03-05 17:44:42,722 - INFO - training batch 1351, loss: 0.004, 43232/56000 datapoints
2025-03-05 17:44:42,885 - INFO - training batch 1401, loss: 0.006, 44832/56000 datapoints
2025-03-05 17:44:43,022 - INFO - training batch 1451, loss: 0.005, 46432/56000 datapoints
2025-03-05 17:44:43,162 - INFO - training batch 1501, loss: 0.026, 48032/56000 datapoints
2025-03-05 17:44:43,293 - INFO - training batch 1551, loss: 0.008, 49632/56000 datapoints
2025-03-05 17:44:43,996 - INFO - training batch 1601, loss: 0.010, 51232/56000 datapoints
2025-03-05 17:44:45,121 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:44:46,200 - INFO - training batch 1701, loss: 0.083, 54432/56000 datapoints
2025-03-05 17:44:47,225 - INFO - validation batch 1, loss: 0.006, 32/13984 datapoints
2025-03-05 17:44:47,717 - INFO - validation batch 51, loss: 0.021, 1632/13984 datapoints
2025-03-05 17:44:48,205 - INFO - validation batch 101, loss: 0.006, 3232/13984 datapoints
2025-03-05 17:44:48,834 - INFO - validation batch 151, loss: 0.010, 4832/13984 datapoints
2025-03-05 17:44:49,559 - INFO - validation batch 201, loss: 0.009, 6432/13984 datapoints
2025-03-05 17:44:50,208 - INFO - validation batch 251, loss: 0.009, 8032/13984 datapoints
2025-03-05 17:44:50,850 - INFO - validation batch 301, loss: 0.012, 9632/13984 datapoints
2025-03-05 17:44:51,477 - INFO - validation batch 351, loss: 0.008, 11232/13984 datapoints
2025-03-05 17:44:52,190 - INFO - validation batch 401, loss: 0.093, 12832/13984 datapoints
2025-03-05 17:44:52,700 - INFO - Epoch 22/50 done.
2025-03-05 17:44:52,702 - INFO - Final validation performance:
Loss: 0.019, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:44:52,706 - INFO - Epoch 22/50 done
2025-03-05 17:44:52,707 - INFO - Beginning epoch 23/50
2025-03-05 17:44:52,732 - INFO - training batch 1, loss: 0.156, 32/56000 datapoints
2025-03-05 17:44:53,938 - INFO - training batch 51, loss: 0.010, 1632/56000 datapoints
2025-03-05 17:45:41,777 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-05 17:45:41,909 - INFO - training batch 151, loss: 0.061, 4832/56000 datapoints
2025-03-05 17:45:42,169 - INFO - training batch 201, loss: 0.036, 6432/56000 datapoints
2025-03-05 17:45:42,732 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 17:45:43,247 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-05 17:45:43,489 - INFO - training batch 351, loss: 0.072, 11232/56000 datapoints
2025-03-05 17:45:43,719 - INFO - training batch 401, loss: 0.049, 12832/56000 datapoints
2025-03-05 17:45:43,963 - INFO - training batch 451, loss: 0.028, 14432/56000 datapoints
2025-03-05 17:45:44,230 - INFO - training batch 501, loss: 0.012, 16032/56000 datapoints
2025-03-05 17:45:44,630 - INFO - training batch 551, loss: 0.071, 17632/56000 datapoints
2025-03-05 17:45:44,816 - INFO - training batch 601, loss: 0.003, 19232/56000 datapoints
2025-03-05 17:45:45,000 - INFO - training batch 651, loss: 0.009, 20832/56000 datapoints
2025-03-05 17:45:45,230 - INFO - training batch 701, loss: 0.270, 22432/56000 datapoints
2025-03-05 17:45:45,452 - INFO - training batch 751, loss: 0.010, 24032/56000 datapoints
2025-03-05 17:45:45,622 - INFO - training batch 801, loss: 0.020, 25632/56000 datapoints
2025-03-05 17:45:45,800 - INFO - training batch 851, loss: 0.006, 27232/56000 datapoints
2025-03-05 17:45:45,962 - INFO - training batch 901, loss: 0.013, 28832/56000 datapoints
2025-03-05 17:45:46,131 - INFO - training batch 951, loss: 0.013, 30432/56000 datapoints
2025-03-05 17:45:46,300 - INFO - training batch 1001, loss: 0.006, 32032/56000 datapoints
2025-03-05 17:45:46,477 - INFO - training batch 1051, loss: 0.008, 33632/56000 datapoints
2025-03-05 17:45:46,660 - INFO - training batch 1101, loss: 0.084, 35232/56000 datapoints
2025-03-05 17:45:46,866 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-05 17:45:47,068 - INFO - training batch 1201, loss: 0.024, 38432/56000 datapoints
2025-03-05 17:45:47,261 - INFO - training batch 1251, loss: 0.236, 40032/56000 datapoints
2025-03-05 17:45:47,440 - INFO - training batch 1301, loss: 0.020, 41632/56000 datapoints
2025-03-05 17:45:47,632 - INFO - training batch 1351, loss: 0.004, 43232/56000 datapoints
2025-03-05 17:45:47,902 - INFO - training batch 1401, loss: 0.006, 44832/56000 datapoints
2025-03-05 17:45:48,123 - INFO - training batch 1451, loss: 0.005, 46432/56000 datapoints
2025-03-05 17:45:48,343 - INFO - training batch 1501, loss: 0.025, 48032/56000 datapoints
2025-03-05 17:45:48,541 - INFO - training batch 1551, loss: 0.008, 49632/56000 datapoints
2025-03-05 17:45:48,712 - INFO - training batch 1601, loss: 0.010, 51232/56000 datapoints
2025-03-05 17:45:48,895 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:45:49,072 - INFO - training batch 1701, loss: 0.084, 54432/56000 datapoints
2025-03-05 17:45:49,258 - INFO - validation batch 1, loss: 0.006, 32/13984 datapoints
2025-03-05 17:45:49,371 - INFO - validation batch 51, loss: 0.021, 1632/13984 datapoints
2025-03-05 17:45:49,471 - INFO - validation batch 101, loss: 0.005, 3232/13984 datapoints
2025-03-05 17:45:49,575 - INFO - validation batch 151, loss: 0.010, 4832/13984 datapoints
2025-03-05 17:45:49,655 - INFO - validation batch 201, loss: 0.009, 6432/13984 datapoints
2025-03-05 17:45:49,746 - INFO - validation batch 251, loss: 0.009, 8032/13984 datapoints
2025-03-05 17:45:49,834 - INFO - validation batch 301, loss: 0.012, 9632/13984 datapoints
2025-03-05 17:45:49,906 - INFO - validation batch 351, loss: 0.008, 11232/13984 datapoints
2025-03-05 17:45:49,991 - INFO - validation batch 401, loss: 0.092, 12832/13984 datapoints
2025-03-05 17:45:50,047 - INFO - Epoch 23/50 done.
2025-03-05 17:45:50,049 - INFO - Final validation performance:
Loss: 0.019, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:45:50,052 - INFO - Epoch 23/50 done
2025-03-05 17:45:50,053 - INFO - Beginning epoch 24/50
2025-03-05 17:45:50,058 - INFO - training batch 1, loss: 0.157, 32/56000 datapoints
2025-03-05 17:45:50,234 - INFO - training batch 51, loss: 0.010, 1632/56000 datapoints
2025-03-05 17:45:50,426 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-05 17:45:50,593 - INFO - training batch 151, loss: 0.061, 4832/56000 datapoints
2025-03-05 17:45:50,759 - INFO - training batch 201, loss: 0.035, 6432/56000 datapoints
2025-03-05 17:45:50,892 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 17:45:51,025 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-05 17:45:51,145 - INFO - training batch 351, loss: 0.071, 11232/56000 datapoints
2025-03-05 17:45:51,286 - INFO - training batch 401, loss: 0.049, 12832/56000 datapoints
2025-03-05 17:45:51,418 - INFO - training batch 451, loss: 0.027, 14432/56000 datapoints
2025-03-05 17:45:51,557 - INFO - training batch 501, loss: 0.011, 16032/56000 datapoints
2025-03-05 17:45:51,670 - INFO - training batch 551, loss: 0.070, 17632/56000 datapoints
2025-03-05 17:45:51,821 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 17:45:51,954 - INFO - training batch 651, loss: 0.009, 20832/56000 datapoints
2025-03-05 17:45:52,084 - INFO - training batch 701, loss: 0.270, 22432/56000 datapoints
2025-03-05 17:45:52,195 - INFO - training batch 751, loss: 0.010, 24032/56000 datapoints
2025-03-05 17:45:52,323 - INFO - training batch 801, loss: 0.019, 25632/56000 datapoints
2025-03-05 17:45:52,938 - INFO - training batch 851, loss: 0.006, 27232/56000 datapoints
2025-03-05 17:45:54,213 - INFO - training batch 901, loss: 0.012, 28832/56000 datapoints
2025-03-05 17:45:55,574 - INFO - training batch 951, loss: 0.013, 30432/56000 datapoints
2025-03-05 17:45:56,792 - INFO - training batch 1001, loss: 0.006, 32032/56000 datapoints
2025-03-05 17:45:57,904 - INFO - training batch 1051, loss: 0.008, 33632/56000 datapoints
2025-03-05 17:45:58,951 - INFO - training batch 1101, loss: 0.085, 35232/56000 datapoints
2025-03-05 17:45:59,987 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-05 17:46:01,127 - INFO - training batch 1201, loss: 0.024, 38432/56000 datapoints
2025-03-05 17:46:49,188 - INFO - training batch 1251, loss: 0.235, 40032/56000 datapoints
2025-03-05 17:46:49,322 - INFO - training batch 1301, loss: 0.019, 41632/56000 datapoints
2025-03-05 17:46:49,783 - INFO - training batch 1351, loss: 0.004, 43232/56000 datapoints
2025-03-05 17:46:50,073 - INFO - training batch 1401, loss: 0.006, 44832/56000 datapoints
2025-03-05 17:46:50,238 - INFO - training batch 1451, loss: 0.004, 46432/56000 datapoints
2025-03-05 17:46:50,400 - INFO - training batch 1501, loss: 0.025, 48032/56000 datapoints
2025-03-05 17:46:50,591 - INFO - training batch 1551, loss: 0.008, 49632/56000 datapoints
2025-03-05 17:46:50,743 - INFO - training batch 1601, loss: 0.010, 51232/56000 datapoints
2025-03-05 17:46:50,925 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:46:51,057 - INFO - training batch 1701, loss: 0.084, 54432/56000 datapoints
2025-03-05 17:46:51,187 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-05 17:46:51,244 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-05 17:46:51,306 - INFO - validation batch 101, loss: 0.005, 3232/13984 datapoints
2025-03-05 17:46:51,364 - INFO - validation batch 151, loss: 0.009, 4832/13984 datapoints
2025-03-05 17:46:51,423 - INFO - validation batch 201, loss: 0.009, 6432/13984 datapoints
2025-03-05 17:46:51,489 - INFO - validation batch 251, loss: 0.008, 8032/13984 datapoints
2025-03-05 17:46:51,560 - INFO - validation batch 301, loss: 0.011, 9632/13984 datapoints
2025-03-05 17:46:51,648 - INFO - validation batch 351, loss: 0.008, 11232/13984 datapoints
2025-03-05 17:46:51,712 - INFO - validation batch 401, loss: 0.092, 12832/13984 datapoints
2025-03-05 17:46:51,762 - INFO - Epoch 24/50 done.
2025-03-05 17:46:51,762 - INFO - Final validation performance:
Loss: 0.019, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:46:51,763 - INFO - Epoch 24/50 done
2025-03-05 17:46:51,763 - INFO - Beginning epoch 25/50
2025-03-05 17:46:51,766 - INFO - training batch 1, loss: 0.157, 32/56000 datapoints
2025-03-05 17:46:51,881 - INFO - training batch 51, loss: 0.009, 1632/56000 datapoints
2025-03-05 17:46:52,003 - INFO - training batch 101, loss: 0.061, 3232/56000 datapoints
2025-03-05 17:46:52,165 - INFO - training batch 151, loss: 0.060, 4832/56000 datapoints
2025-03-05 17:46:52,324 - INFO - training batch 201, loss: 0.035, 6432/56000 datapoints
2025-03-05 17:46:52,447 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 17:46:52,559 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-05 17:46:52,677 - INFO - training batch 351, loss: 0.071, 11232/56000 datapoints
2025-03-05 17:46:52,784 - INFO - training batch 401, loss: 0.048, 12832/56000 datapoints
2025-03-05 17:46:52,926 - INFO - training batch 451, loss: 0.027, 14432/56000 datapoints
2025-03-05 17:46:53,047 - INFO - training batch 501, loss: 0.011, 16032/56000 datapoints
2025-03-05 17:46:53,158 - INFO - training batch 551, loss: 0.069, 17632/56000 datapoints
2025-03-05 17:46:53,278 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 17:46:53,401 - INFO - training batch 651, loss: 0.009, 20832/56000 datapoints
2025-03-05 17:46:53,537 - INFO - training batch 701, loss: 0.270, 22432/56000 datapoints
2025-03-05 17:46:53,652 - INFO - training batch 751, loss: 0.010, 24032/56000 datapoints
2025-03-05 17:46:53,764 - INFO - training batch 801, loss: 0.019, 25632/56000 datapoints
2025-03-05 17:46:53,885 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-05 17:46:53,995 - INFO - training batch 901, loss: 0.012, 28832/56000 datapoints
2025-03-05 17:46:54,107 - INFO - training batch 951, loss: 0.012, 30432/56000 datapoints
2025-03-05 17:46:54,222 - INFO - training batch 1001, loss: 0.006, 32032/56000 datapoints
2025-03-05 17:46:54,346 - INFO - training batch 1051, loss: 0.008, 33632/56000 datapoints
2025-03-05 17:46:54,462 - INFO - training batch 1101, loss: 0.085, 35232/56000 datapoints
2025-03-05 17:46:54,592 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-05 17:46:54,707 - INFO - training batch 1201, loss: 0.023, 38432/56000 datapoints
2025-03-05 17:46:54,835 - INFO - training batch 1251, loss: 0.234, 40032/56000 datapoints
2025-03-05 17:46:54,949 - INFO - training batch 1301, loss: 0.019, 41632/56000 datapoints
2025-03-05 17:46:55,060 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-05 17:46:55,172 - INFO - training batch 1401, loss: 0.006, 44832/56000 datapoints
2025-03-05 17:46:55,302 - INFO - training batch 1451, loss: 0.004, 46432/56000 datapoints
2025-03-05 17:46:55,453 - INFO - training batch 1501, loss: 0.024, 48032/56000 datapoints
2025-03-05 17:46:55,581 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-05 17:46:55,694 - INFO - training batch 1601, loss: 0.010, 51232/56000 datapoints
2025-03-05 17:46:55,808 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:46:55,929 - INFO - training batch 1701, loss: 0.084, 54432/56000 datapoints
2025-03-05 17:46:56,044 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-05 17:46:56,103 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-05 17:46:56,168 - INFO - validation batch 101, loss: 0.005, 3232/13984 datapoints
2025-03-05 17:46:56,240 - INFO - validation batch 151, loss: 0.009, 4832/13984 datapoints
2025-03-05 17:46:56,307 - INFO - validation batch 201, loss: 0.009, 6432/13984 datapoints
2025-03-05 17:46:56,388 - INFO - validation batch 251, loss: 0.008, 8032/13984 datapoints
2025-03-05 17:46:56,459 - INFO - validation batch 301, loss: 0.011, 9632/13984 datapoints
2025-03-05 17:46:56,524 - INFO - validation batch 351, loss: 0.007, 11232/13984 datapoints
2025-03-05 17:46:56,580 - INFO - validation batch 401, loss: 0.092, 12832/13984 datapoints
2025-03-05 17:46:56,624 - INFO - Epoch 25/50 done.
2025-03-05 17:46:56,624 - INFO - Final validation performance:
Loss: 0.018, top-1 acc: 0.993top-5 acc: 0.993
2025-03-05 17:46:56,624 - INFO - Epoch 25/50 done
2025-03-05 17:46:56,625 - INFO - Beginning epoch 26/50
2025-03-05 17:46:56,628 - INFO - training batch 1, loss: 0.158, 32/56000 datapoints
2025-03-05 17:46:56,775 - INFO - training batch 51, loss: 0.009, 1632/56000 datapoints
2025-03-05 17:46:56,917 - INFO - training batch 101, loss: 0.061, 3232/56000 datapoints
2025-03-05 17:46:57,036 - INFO - training batch 151, loss: 0.060, 4832/56000 datapoints
2025-03-05 17:46:57,157 - INFO - training batch 201, loss: 0.034, 6432/56000 datapoints
2025-03-05 17:46:57,276 - INFO - training batch 251, loss: 0.010, 8032/56000 datapoints
2025-03-05 17:46:57,390 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-05 17:46:57,507 - INFO - training batch 351, loss: 0.070, 11232/56000 datapoints
2025-03-05 17:46:57,626 - INFO - training batch 401, loss: 0.048, 12832/56000 datapoints
2025-03-05 17:46:57,770 - INFO - training batch 451, loss: 0.026, 14432/56000 datapoints
2025-03-05 17:46:57,882 - INFO - training batch 501, loss: 0.010, 16032/56000 datapoints
2025-03-05 17:46:57,995 - INFO - training batch 551, loss: 0.068, 17632/56000 datapoints
2025-03-05 17:46:58,143 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 17:46:58,264 - INFO - training batch 651, loss: 0.009, 20832/56000 datapoints
2025-03-05 17:46:58,379 - INFO - training batch 701, loss: 0.270, 22432/56000 datapoints
2025-03-05 17:46:58,978 - INFO - training batch 751, loss: 0.009, 24032/56000 datapoints
2025-03-05 17:46:59,267 - INFO - training batch 801, loss: 0.018, 25632/56000 datapoints
2025-03-05 17:46:59,471 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-05 17:47:34,654 - INFO - training batch 901, loss: 0.011, 28832/56000 datapoints
2025-03-05 17:47:34,809 - INFO - training batch 951, loss: 0.012, 30432/56000 datapoints
2025-03-05 17:47:35,286 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-05 17:47:35,549 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-05 17:47:35,699 - INFO - training batch 1101, loss: 0.085, 35232/56000 datapoints
2025-03-05 17:47:35,862 - INFO - training batch 1151, loss: 0.002, 36832/56000 datapoints
2025-03-05 17:47:35,997 - INFO - training batch 1201, loss: 0.022, 38432/56000 datapoints
2025-03-05 17:47:36,117 - INFO - training batch 1251, loss: 0.233, 40032/56000 datapoints
2025-03-05 17:47:36,234 - INFO - training batch 1301, loss: 0.019, 41632/56000 datapoints
2025-03-05 17:47:36,394 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-05 17:47:36,515 - INFO - training batch 1401, loss: 0.006, 44832/56000 datapoints
2025-03-05 17:47:36,629 - INFO - training batch 1451, loss: 0.004, 46432/56000 datapoints
2025-03-05 17:47:36,752 - INFO - training batch 1501, loss: 0.024, 48032/56000 datapoints
2025-03-05 17:47:36,869 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-05 17:47:36,982 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-05 17:47:37,093 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:47:37,211 - INFO - training batch 1701, loss: 0.084, 54432/56000 datapoints
2025-03-05 17:47:37,321 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-05 17:47:37,387 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-05 17:47:37,458 - INFO - validation batch 101, loss: 0.005, 3232/13984 datapoints
2025-03-05 17:47:37,529 - INFO - validation batch 151, loss: 0.009, 4832/13984 datapoints
2025-03-05 17:47:37,602 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-05 17:47:37,668 - INFO - validation batch 251, loss: 0.007, 8032/13984 datapoints
2025-03-05 17:47:37,743 - INFO - validation batch 301, loss: 0.011, 9632/13984 datapoints
2025-03-05 17:47:37,823 - INFO - validation batch 351, loss: 0.007, 11232/13984 datapoints
2025-03-05 17:47:37,883 - INFO - validation batch 401, loss: 0.092, 12832/13984 datapoints
2025-03-05 17:47:37,932 - INFO - Epoch 26/50 done.
2025-03-05 17:47:37,932 - INFO - Final validation performance:
Loss: 0.018, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 17:47:37,933 - INFO - Epoch 26/50 done
2025-03-05 17:47:37,933 - INFO - Beginning epoch 27/50
2025-03-05 17:47:37,936 - INFO - training batch 1, loss: 0.159, 32/56000 datapoints
2025-03-05 17:47:38,046 - INFO - training batch 51, loss: 0.009, 1632/56000 datapoints
2025-03-05 17:47:38,165 - INFO - training batch 101, loss: 0.061, 3232/56000 datapoints
2025-03-05 17:47:38,357 - INFO - training batch 151, loss: 0.060, 4832/56000 datapoints
2025-03-05 17:47:38,489 - INFO - training batch 201, loss: 0.033, 6432/56000 datapoints
2025-03-05 17:47:38,607 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 17:47:38,724 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-05 17:47:38,848 - INFO - training batch 351, loss: 0.070, 11232/56000 datapoints
2025-03-05 17:47:38,979 - INFO - training batch 401, loss: 0.047, 12832/56000 datapoints
2025-03-05 17:47:39,128 - INFO - training batch 451, loss: 0.026, 14432/56000 datapoints
2025-03-05 17:47:39,253 - INFO - training batch 501, loss: 0.010, 16032/56000 datapoints
2025-03-05 17:47:39,390 - INFO - training batch 551, loss: 0.067, 17632/56000 datapoints
2025-03-05 17:47:39,500 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 17:47:39,618 - INFO - training batch 651, loss: 0.009, 20832/56000 datapoints
2025-03-05 17:47:39,728 - INFO - training batch 701, loss: 0.270, 22432/56000 datapoints
2025-03-05 17:47:39,840 - INFO - training batch 751, loss: 0.009, 24032/56000 datapoints
2025-03-05 17:47:39,963 - INFO - training batch 801, loss: 0.018, 25632/56000 datapoints
2025-03-05 17:47:40,075 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-05 17:47:40,190 - INFO - training batch 901, loss: 0.011, 28832/56000 datapoints
2025-03-05 17:47:40,316 - INFO - training batch 951, loss: 0.012, 30432/56000 datapoints
2025-03-05 17:47:40,440 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-05 17:47:40,548 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-05 17:47:40,677 - INFO - training batch 1101, loss: 0.085, 35232/56000 datapoints
2025-03-05 17:47:40,794 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 17:47:40,950 - INFO - training batch 1201, loss: 0.021, 38432/56000 datapoints
2025-03-05 17:47:41,063 - INFO - training batch 1251, loss: 0.231, 40032/56000 datapoints
2025-03-05 17:47:41,173 - INFO - training batch 1301, loss: 0.019, 41632/56000 datapoints
2025-03-05 17:47:41,289 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-05 17:47:41,398 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-05 17:47:41,522 - INFO - training batch 1451, loss: 0.004, 46432/56000 datapoints
2025-03-05 17:47:41,631 - INFO - training batch 1501, loss: 0.024, 48032/56000 datapoints
2025-03-05 17:47:41,740 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-05 17:47:41,847 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-05 17:47:41,958 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:47:42,371 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 17:47:42,529 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-05 17:47:42,615 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-05 17:47:42,700 - INFO - validation batch 101, loss: 0.005, 3232/13984 datapoints
2025-03-05 17:47:42,778 - INFO - validation batch 151, loss: 0.009, 4832/13984 datapoints
2025-03-05 17:47:42,856 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-05 17:47:42,940 - INFO - validation batch 251, loss: 0.007, 8032/13984 datapoints
2025-03-05 17:47:43,012 - INFO - validation batch 301, loss: 0.010, 9632/13984 datapoints
2025-03-05 17:47:43,077 - INFO - validation batch 351, loss: 0.007, 11232/13984 datapoints
2025-03-05 17:47:43,142 - INFO - validation batch 401, loss: 0.091, 12832/13984 datapoints
2025-03-05 17:47:43,186 - INFO - Epoch 27/50 done.
2025-03-05 17:47:43,187 - INFO - Final validation performance:
Loss: 0.018, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 17:47:43,188 - INFO - Epoch 27/50 done
2025-03-05 17:47:43,188 - INFO - Beginning epoch 28/50
2025-03-05 17:47:43,192 - INFO - training batch 1, loss: 0.159, 32/56000 datapoints
2025-03-05 17:47:43,305 - INFO - training batch 51, loss: 0.008, 1632/56000 datapoints
2025-03-05 17:47:43,426 - INFO - training batch 101, loss: 0.061, 3232/56000 datapoints
2025-03-05 17:47:43,541 - INFO - training batch 151, loss: 0.059, 4832/56000 datapoints
2025-03-05 17:47:43,661 - INFO - training batch 201, loss: 0.033, 6432/56000 datapoints
2025-03-05 17:47:43,774 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 17:47:43,900 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-05 17:47:44,014 - INFO - training batch 351, loss: 0.069, 11232/56000 datapoints
2025-03-05 17:47:44,138 - INFO - training batch 401, loss: 0.047, 12832/56000 datapoints
2025-03-05 17:47:44,274 - INFO - training batch 451, loss: 0.026, 14432/56000 datapoints
2025-03-05 17:47:44,398 - INFO - training batch 501, loss: 0.009, 16032/56000 datapoints
2025-03-05 17:47:44,526 - INFO - training batch 551, loss: 0.066, 17632/56000 datapoints
2025-03-05 17:47:44,669 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 17:47:44,804 - INFO - training batch 651, loss: 0.009, 20832/56000 datapoints
2025-03-05 17:47:44,922 - INFO - training batch 701, loss: 0.270, 22432/56000 datapoints
2025-03-05 17:47:45,057 - INFO - training batch 751, loss: 0.009, 24032/56000 datapoints
2025-03-05 17:47:45,182 - INFO - training batch 801, loss: 0.017, 25632/56000 datapoints
2025-03-05 17:47:46,114 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-05 17:47:47,172 - INFO - training batch 901, loss: 0.011, 28832/56000 datapoints
2025-03-05 17:47:48,344 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-05 17:47:49,339 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-05 17:47:50,201 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-05 17:47:51,137 - INFO - training batch 1101, loss: 0.085, 35232/56000 datapoints
2025-03-05 17:47:51,914 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 17:47:52,698 - INFO - training batch 1201, loss: 0.021, 38432/56000 datapoints
2025-03-05 17:47:53,633 - INFO - training batch 1251, loss: 0.230, 40032/56000 datapoints
2025-03-05 17:47:54,523 - INFO - training batch 1301, loss: 0.019, 41632/56000 datapoints
2025-03-05 17:49:27,149 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-05 17:49:27,674 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-05 17:49:27,830 - INFO - training batch 1451, loss: 0.004, 46432/56000 datapoints
2025-03-05 17:49:27,976 - INFO - training batch 1501, loss: 0.024, 48032/56000 datapoints
2025-03-05 17:49:28,107 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-05 17:49:28,265 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-05 17:49:28,470 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:49:28,596 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 17:49:28,712 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-05 17:49:28,768 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-05 17:49:28,825 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-05 17:49:28,884 - INFO - validation batch 151, loss: 0.009, 4832/13984 datapoints
2025-03-05 17:49:28,940 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-05 17:49:29,002 - INFO - validation batch 251, loss: 0.007, 8032/13984 datapoints
2025-03-05 17:49:29,075 - INFO - validation batch 301, loss: 0.010, 9632/13984 datapoints
2025-03-05 17:49:29,132 - INFO - validation batch 351, loss: 0.007, 11232/13984 datapoints
2025-03-05 17:49:29,189 - INFO - validation batch 401, loss: 0.091, 12832/13984 datapoints
2025-03-05 17:49:29,231 - INFO - Epoch 28/50 done.
2025-03-05 17:49:29,232 - INFO - Final validation performance:
Loss: 0.018, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 17:49:29,233 - INFO - Epoch 28/50 done
2025-03-05 17:49:29,233 - INFO - Beginning epoch 29/50
2025-03-05 17:49:29,237 - INFO - training batch 1, loss: 0.159, 32/56000 datapoints
2025-03-05 17:49:29,353 - INFO - training batch 51, loss: 0.008, 1632/56000 datapoints
2025-03-05 17:49:29,466 - INFO - training batch 101, loss: 0.062, 3232/56000 datapoints
2025-03-05 17:49:29,582 - INFO - training batch 151, loss: 0.059, 4832/56000 datapoints
2025-03-05 17:49:29,699 - INFO - training batch 201, loss: 0.032, 6432/56000 datapoints
2025-03-05 17:49:29,818 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 17:49:29,930 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-05 17:49:30,043 - INFO - training batch 351, loss: 0.068, 11232/56000 datapoints
2025-03-05 17:49:30,169 - INFO - training batch 401, loss: 0.046, 12832/56000 datapoints
2025-03-05 17:49:30,288 - INFO - training batch 451, loss: 0.025, 14432/56000 datapoints
2025-03-05 17:49:30,414 - INFO - training batch 501, loss: 0.009, 16032/56000 datapoints
2025-03-05 17:49:30,543 - INFO - training batch 551, loss: 0.065, 17632/56000 datapoints
2025-03-05 17:49:30,685 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 17:49:30,819 - INFO - training batch 651, loss: 0.009, 20832/56000 datapoints
2025-03-05 17:49:30,938 - INFO - training batch 701, loss: 0.270, 22432/56000 datapoints
2025-03-05 17:49:31,075 - INFO - training batch 751, loss: 0.009, 24032/56000 datapoints
2025-03-05 17:49:31,190 - INFO - training batch 801, loss: 0.017, 25632/56000 datapoints
2025-03-05 17:49:31,308 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-05 17:49:31,422 - INFO - training batch 901, loss: 0.010, 28832/56000 datapoints
2025-03-05 17:49:31,536 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-05 17:49:31,668 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-05 17:49:31,789 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-05 17:49:31,899 - INFO - training batch 1101, loss: 0.085, 35232/56000 datapoints
2025-03-05 17:49:32,028 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 17:49:32,180 - INFO - training batch 1201, loss: 0.020, 38432/56000 datapoints
2025-03-05 17:49:32,316 - INFO - training batch 1251, loss: 0.228, 40032/56000 datapoints
2025-03-05 17:49:32,452 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 17:49:32,596 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-05 17:49:32,724 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-05 17:49:32,836 - INFO - training batch 1451, loss: 0.004, 46432/56000 datapoints
2025-03-05 17:49:32,957 - INFO - training batch 1501, loss: 0.023, 48032/56000 datapoints
2025-03-05 17:49:33,118 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-05 17:49:33,275 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-05 17:49:33,400 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 17:49:33,511 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 17:49:33,638 - INFO - validation batch 1, loss: 0.005, 32/13984 datapoints
2025-03-05 17:49:33,700 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-05 17:49:33,779 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-05 17:49:33,839 - INFO - validation batch 151, loss: 0.009, 4832/13984 datapoints
2025-03-05 17:49:33,903 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-05 17:49:33,974 - INFO - validation batch 251, loss: 0.006, 8032/13984 datapoints
2025-03-05 17:49:34,029 - INFO - validation batch 301, loss: 0.010, 9632/13984 datapoints
2025-03-05 17:49:34,100 - INFO - validation batch 351, loss: 0.007, 11232/13984 datapoints
2025-03-05 17:49:34,160 - INFO - validation batch 401, loss: 0.091, 12832/13984 datapoints
2025-03-05 17:49:34,199 - INFO - Epoch 29/50 done.
2025-03-05 17:49:34,199 - INFO - Final validation performance:
Loss: 0.018, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 17:49:34,201 - INFO - Epoch 29/50 done
2025-03-05 17:49:34,201 - INFO - Beginning epoch 30/50
2025-03-05 17:49:34,204 - INFO - training batch 1, loss: 0.160, 32/56000 datapoints
2025-03-05 17:49:34,314 - INFO - training batch 51, loss: 0.008, 1632/56000 datapoints
2025-03-05 17:49:34,427 - INFO - training batch 101, loss: 0.062, 3232/56000 datapoints
2025-03-05 17:49:34,535 - INFO - training batch 151, loss: 0.059, 4832/56000 datapoints
2025-03-05 17:49:34,681 - INFO - training batch 201, loss: 0.031, 6432/56000 datapoints
2025-03-05 17:49:34,791 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 17:49:34,905 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-05 17:49:35,037 - INFO - training batch 351, loss: 0.068, 11232/56000 datapoints
2025-03-05 17:49:35,165 - INFO - training batch 401, loss: 0.046, 12832/56000 datapoints
2025-03-05 17:49:35,277 - INFO - training batch 451, loss: 0.025, 14432/56000 datapoints
2025-03-05 17:49:35,388 - INFO - training batch 501, loss: 0.009, 16032/56000 datapoints
2025-03-05 17:49:35,499 - INFO - training batch 551, loss: 0.064, 17632/56000 datapoints
2025-03-05 17:49:35,611 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 17:49:35,723 - INFO - training batch 651, loss: 0.009, 20832/56000 datapoints
2025-03-05 17:49:35,833 - INFO - training batch 701, loss: 0.270, 22432/56000 datapoints
2025-03-05 17:49:35,944 - INFO - training batch 751, loss: 0.008, 24032/56000 datapoints
2025-03-05 17:49:36,056 - INFO - training batch 801, loss: 0.016, 25632/56000 datapoints
2025-03-05 17:49:36,181 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-05 17:49:36,291 - INFO - training batch 901, loss: 0.010, 28832/56000 datapoints
2025-03-05 17:49:36,403 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-05 17:49:36,513 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-05 17:49:36,621 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-05 17:49:36,733 - INFO - training batch 1101, loss: 0.085, 35232/56000 datapoints
2025-03-05 17:49:36,845 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 17:49:36,955 - INFO - training batch 1201, loss: 0.020, 38432/56000 datapoints
2025-03-05 17:49:37,090 - INFO - training batch 1251, loss: 0.227, 40032/56000 datapoints
2025-03-05 17:49:37,228 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 17:49:37,338 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-05 17:49:37,518 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-05 17:49:37,649 - INFO - training batch 1451, loss: 0.004, 46432/56000 datapoints
2025-03-05 19:49:35,833 - INFO - training batch 1501, loss: 0.023, 48032/56000 datapoints
2025-03-05 19:49:35,959 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-05 19:49:36,073 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-05 19:49:36,910 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 19:49:37,154 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 19:49:37,362 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-05 19:49:37,447 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-05 19:49:37,548 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-05 19:49:37,641 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-05 19:49:37,724 - INFO - validation batch 201, loss: 0.008, 6432/13984 datapoints
2025-03-05 19:49:37,866 - INFO - validation batch 251, loss: 0.006, 8032/13984 datapoints
2025-03-05 19:49:38,003 - INFO - validation batch 301, loss: 0.010, 9632/13984 datapoints
2025-03-05 19:49:38,106 - INFO - validation batch 351, loss: 0.006, 11232/13984 datapoints
2025-03-05 19:49:38,192 - INFO - validation batch 401, loss: 0.091, 12832/13984 datapoints
2025-03-05 19:49:38,260 - INFO - Epoch 30/50 done.
2025-03-05 19:49:38,260 - INFO - Final validation performance:
Loss: 0.017, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 19:49:38,261 - INFO - Epoch 30/50 done
2025-03-05 19:49:38,261 - INFO - Beginning epoch 31/50
2025-03-05 19:49:38,265 - INFO - training batch 1, loss: 0.160, 32/56000 datapoints
2025-03-05 19:49:38,431 - INFO - training batch 51, loss: 0.008, 1632/56000 datapoints
2025-03-05 19:49:38,626 - INFO - training batch 101, loss: 0.062, 3232/56000 datapoints
2025-03-05 19:49:38,801 - INFO - training batch 151, loss: 0.058, 4832/56000 datapoints
2025-03-05 19:49:39,057 - INFO - training batch 201, loss: 0.031, 6432/56000 datapoints
2025-03-05 19:49:39,240 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 19:49:39,419 - INFO - training batch 301, loss: 0.023, 9632/56000 datapoints
2025-03-05 19:49:39,575 - INFO - training batch 351, loss: 0.067, 11232/56000 datapoints
2025-03-05 19:49:39,699 - INFO - training batch 401, loss: 0.046, 12832/56000 datapoints
2025-03-05 19:49:39,836 - INFO - training batch 451, loss: 0.024, 14432/56000 datapoints
2025-03-05 19:49:39,980 - INFO - training batch 501, loss: 0.008, 16032/56000 datapoints
2025-03-05 19:49:40,088 - INFO - training batch 551, loss: 0.064, 17632/56000 datapoints
2025-03-05 19:49:40,202 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 19:49:40,312 - INFO - training batch 651, loss: 0.009, 20832/56000 datapoints
2025-03-05 19:49:40,437 - INFO - training batch 701, loss: 0.270, 22432/56000 datapoints
2025-03-05 19:49:40,556 - INFO - training batch 751, loss: 0.008, 24032/56000 datapoints
2025-03-05 19:49:40,699 - INFO - training batch 801, loss: 0.016, 25632/56000 datapoints
2025-03-05 19:49:40,822 - INFO - training batch 851, loss: 0.005, 27232/56000 datapoints
2025-03-05 19:49:41,015 - INFO - training batch 901, loss: 0.010, 28832/56000 datapoints
2025-03-05 19:49:41,131 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-05 19:49:41,241 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-05 19:49:41,352 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-05 19:49:41,470 - INFO - training batch 1101, loss: 0.085, 35232/56000 datapoints
2025-03-05 19:49:41,595 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 19:49:41,703 - INFO - training batch 1201, loss: 0.019, 38432/56000 datapoints
2025-03-05 19:49:41,812 - INFO - training batch 1251, loss: 0.225, 40032/56000 datapoints
2025-03-05 19:49:41,948 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 19:49:42,077 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-05 19:49:42,216 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-05 19:49:42,327 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 19:49:42,436 - INFO - training batch 1501, loss: 0.023, 48032/56000 datapoints
2025-03-05 19:49:42,542 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-05 19:49:42,650 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-05 19:49:42,757 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 19:49:42,884 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 19:49:43,006 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-05 19:49:43,062 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-05 19:49:43,116 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-05 19:49:43,171 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-05 19:49:43,226 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-05 19:49:43,281 - INFO - validation batch 251, loss: 0.006, 8032/13984 datapoints
2025-03-05 19:49:43,336 - INFO - validation batch 301, loss: 0.010, 9632/13984 datapoints
2025-03-05 19:49:43,402 - INFO - validation batch 351, loss: 0.006, 11232/13984 datapoints
2025-03-05 19:49:43,480 - INFO - validation batch 401, loss: 0.091, 12832/13984 datapoints
2025-03-05 19:49:43,520 - INFO - Epoch 31/50 done.
2025-03-05 19:49:43,520 - INFO - Final validation performance:
Loss: 0.017, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 19:49:43,520 - INFO - Epoch 31/50 done
2025-03-05 19:49:43,521 - INFO - Beginning epoch 32/50
2025-03-05 19:49:43,524 - INFO - training batch 1, loss: 0.161, 32/56000 datapoints
2025-03-05 19:49:43,635 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-05 19:49:43,743 - INFO - training batch 101, loss: 0.062, 3232/56000 datapoints
2025-03-05 19:49:43,868 - INFO - training batch 151, loss: 0.058, 4832/56000 datapoints
2025-03-05 19:49:44,002 - INFO - training batch 201, loss: 0.030, 6432/56000 datapoints
2025-03-05 19:49:44,111 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 19:49:44,221 - INFO - training batch 301, loss: 0.024, 9632/56000 datapoints
2025-03-05 19:49:44,328 - INFO - training batch 351, loss: 0.067, 11232/56000 datapoints
2025-03-05 19:49:44,436 - INFO - training batch 401, loss: 0.045, 12832/56000 datapoints
2025-03-05 19:49:44,545 - INFO - training batch 451, loss: 0.024, 14432/56000 datapoints
2025-03-05 19:49:44,654 - INFO - training batch 501, loss: 0.008, 16032/56000 datapoints
2025-03-05 19:49:44,761 - INFO - training batch 551, loss: 0.063, 17632/56000 datapoints
2025-03-05 19:49:44,877 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 19:49:44,995 - INFO - training batch 651, loss: 0.009, 20832/56000 datapoints
2025-03-05 19:49:45,111 - INFO - training batch 701, loss: 0.269, 22432/56000 datapoints
2025-03-05 19:49:45,220 - INFO - training batch 751, loss: 0.008, 24032/56000 datapoints
2025-03-05 19:49:45,342 - INFO - training batch 801, loss: 0.016, 25632/56000 datapoints
2025-03-05 19:49:45,465 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 19:49:45,592 - INFO - training batch 901, loss: 0.009, 28832/56000 datapoints
2025-03-05 19:49:45,699 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-05 19:49:45,806 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-05 19:49:45,961 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-05 19:49:46,102 - INFO - training batch 1101, loss: 0.085, 35232/56000 datapoints
2025-03-05 19:49:46,230 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 19:49:46,349 - INFO - training batch 1201, loss: 0.019, 38432/56000 datapoints
2025-03-05 19:49:46,456 - INFO - training batch 1251, loss: 0.223, 40032/56000 datapoints
2025-03-05 19:49:47,005 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 19:49:47,802 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-05 19:49:48,677 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-05 19:49:49,574 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 19:49:50,429 - INFO - training batch 1501, loss: 0.023, 48032/56000 datapoints
2025-03-05 19:49:51,405 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-05 19:49:52,479 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-05 19:49:53,469 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 19:49:55,109 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 20:38:43,144 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-05 20:38:43,406 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-05 20:38:43,530 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-05 20:38:43,640 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-05 20:38:43,718 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-05 20:38:43,803 - INFO - validation batch 251, loss: 0.006, 8032/13984 datapoints
2025-03-05 20:38:43,905 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-05 20:38:43,985 - INFO - validation batch 351, loss: 0.006, 11232/13984 datapoints
2025-03-05 20:38:44,078 - INFO - validation batch 401, loss: 0.090, 12832/13984 datapoints
2025-03-05 20:38:44,158 - INFO - Epoch 32/50 done.
2025-03-05 20:38:44,158 - INFO - Final validation performance:
Loss: 0.017, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 20:38:44,159 - INFO - Epoch 32/50 done
2025-03-05 20:38:44,160 - INFO - Beginning epoch 33/50
2025-03-05 20:38:44,163 - INFO - training batch 1, loss: 0.161, 32/56000 datapoints
2025-03-05 20:38:44,318 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-05 20:38:44,459 - INFO - training batch 101, loss: 0.062, 3232/56000 datapoints
2025-03-05 20:38:44,607 - INFO - training batch 151, loss: 0.058, 4832/56000 datapoints
2025-03-05 20:38:44,736 - INFO - training batch 201, loss: 0.030, 6432/56000 datapoints
2025-03-05 20:38:44,850 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 20:38:44,970 - INFO - training batch 301, loss: 0.024, 9632/56000 datapoints
2025-03-05 20:38:45,094 - INFO - training batch 351, loss: 0.066, 11232/56000 datapoints
2025-03-05 20:38:45,213 - INFO - training batch 401, loss: 0.045, 12832/56000 datapoints
2025-03-05 20:38:45,371 - INFO - training batch 451, loss: 0.024, 14432/56000 datapoints
2025-03-05 20:38:45,521 - INFO - training batch 501, loss: 0.008, 16032/56000 datapoints
2025-03-05 20:38:45,661 - INFO - training batch 551, loss: 0.062, 17632/56000 datapoints
2025-03-05 20:38:45,780 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 20:38:45,888 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 20:38:45,999 - INFO - training batch 701, loss: 0.269, 22432/56000 datapoints
2025-03-05 20:38:46,162 - INFO - training batch 751, loss: 0.008, 24032/56000 datapoints
2025-03-05 20:38:46,337 - INFO - training batch 801, loss: 0.015, 25632/56000 datapoints
2025-03-05 20:38:46,466 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 20:38:46,628 - INFO - training batch 901, loss: 0.009, 28832/56000 datapoints
2025-03-05 20:38:47,259 - INFO - training batch 951, loss: 0.011, 30432/56000 datapoints
2025-03-05 20:38:47,404 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-05 20:38:47,558 - INFO - training batch 1051, loss: 0.007, 33632/56000 datapoints
2025-03-05 20:38:47,701 - INFO - training batch 1101, loss: 0.084, 35232/56000 datapoints
2025-03-05 20:38:47,831 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 20:38:47,955 - INFO - training batch 1201, loss: 0.018, 38432/56000 datapoints
2025-03-05 20:38:48,081 - INFO - training batch 1251, loss: 0.222, 40032/56000 datapoints
2025-03-05 20:38:48,205 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 20:38:48,337 - INFO - training batch 1351, loss: 0.003, 43232/56000 datapoints
2025-03-05 20:38:48,464 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-05 20:38:48,571 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 20:38:48,689 - INFO - training batch 1501, loss: 0.023, 48032/56000 datapoints
2025-03-05 20:38:48,810 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-05 20:38:48,930 - INFO - training batch 1601, loss: 0.009, 51232/56000 datapoints
2025-03-05 20:38:49,038 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 20:38:49,146 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 20:38:49,254 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-05 20:38:49,311 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-05 20:38:49,366 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-05 20:38:49,421 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-05 20:38:49,475 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-05 20:38:49,529 - INFO - validation batch 251, loss: 0.006, 8032/13984 datapoints
2025-03-05 20:38:49,584 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-05 20:38:49,646 - INFO - validation batch 351, loss: 0.006, 11232/13984 datapoints
2025-03-05 20:38:49,702 - INFO - validation batch 401, loss: 0.090, 12832/13984 datapoints
2025-03-05 20:38:49,743 - INFO - Epoch 33/50 done.
2025-03-05 20:38:49,743 - INFO - Final validation performance:
Loss: 0.017, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 20:38:49,744 - INFO - Epoch 33/50 done
2025-03-05 20:38:49,744 - INFO - Beginning epoch 34/50
2025-03-05 20:38:49,747 - INFO - training batch 1, loss: 0.161, 32/56000 datapoints
2025-03-05 20:38:49,860 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-05 20:38:49,968 - INFO - training batch 101, loss: 0.062, 3232/56000 datapoints
2025-03-05 20:38:50,075 - INFO - training batch 151, loss: 0.058, 4832/56000 datapoints
2025-03-05 20:38:50,183 - INFO - training batch 201, loss: 0.029, 6432/56000 datapoints
2025-03-05 20:38:50,329 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 20:38:50,443 - INFO - training batch 301, loss: 0.024, 9632/56000 datapoints
2025-03-05 20:38:50,568 - INFO - training batch 351, loss: 0.066, 11232/56000 datapoints
2025-03-05 20:38:50,817 - INFO - training batch 401, loss: 0.044, 12832/56000 datapoints
2025-03-05 20:38:50,956 - INFO - training batch 451, loss: 0.023, 14432/56000 datapoints
2025-03-05 20:38:51,094 - INFO - training batch 501, loss: 0.008, 16032/56000 datapoints
2025-03-05 20:38:51,239 - INFO - training batch 551, loss: 0.061, 17632/56000 datapoints
2025-03-05 20:38:51,363 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 20:38:51,489 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 20:38:52,007 - INFO - training batch 701, loss: 0.268, 22432/56000 datapoints
2025-03-05 20:38:52,167 - INFO - training batch 751, loss: 0.008, 24032/56000 datapoints
2025-03-05 20:38:52,340 - INFO - training batch 801, loss: 0.015, 25632/56000 datapoints
2025-03-05 20:38:52,532 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 20:38:52,660 - INFO - training batch 901, loss: 0.009, 28832/56000 datapoints
2025-03-05 20:38:53,052 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-05 20:38:53,331 - INFO - training batch 1001, loss: 0.005, 32032/56000 datapoints
2025-03-05 22:38:52,298 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-05 22:38:56,683 - INFO - training batch 1101, loss: 0.084, 35232/56000 datapoints
2025-03-05 22:38:58,341 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:38:59,894 - INFO - training batch 1201, loss: 0.018, 38432/56000 datapoints
2025-03-05 22:39:00,694 - INFO - training batch 1251, loss: 0.220, 40032/56000 datapoints
2025-03-05 22:39:01,888 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:39:02,686 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:39:05,618 - INFO - training batch 1401, loss: 0.005, 44832/56000 datapoints
2025-03-05 22:39:07,256 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:39:08,558 - INFO - training batch 1501, loss: 0.023, 48032/56000 datapoints
2025-03-05 22:39:10,045 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-05 22:39:11,443 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:39:12,631 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:39:13,991 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 22:39:15,997 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-05 22:39:17,109 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-05 22:39:18,111 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-05 22:39:18,959 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-05 22:39:36,933 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-05 22:39:37,445 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-05 22:39:37,631 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-05 22:39:37,950 - INFO - validation batch 351, loss: 0.006, 11232/13984 datapoints
2025-03-05 22:39:38,117 - INFO - validation batch 401, loss: 0.090, 12832/13984 datapoints
2025-03-05 22:39:38,224 - INFO - Epoch 34/50 done.
2025-03-05 22:39:38,226 - INFO - Final validation performance:
Loss: 0.017, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:39:38,229 - INFO - Epoch 34/50 done
2025-03-05 22:39:38,230 - INFO - Beginning epoch 35/50
2025-03-05 22:39:38,238 - INFO - training batch 1, loss: 0.162, 32/56000 datapoints
2025-03-05 22:39:38,437 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-05 22:39:38,625 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:39:38,809 - INFO - training batch 151, loss: 0.057, 4832/56000 datapoints
2025-03-05 22:39:38,984 - INFO - training batch 201, loss: 0.029, 6432/56000 datapoints
2025-03-05 22:39:39,132 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:39:39,316 - INFO - training batch 301, loss: 0.024, 9632/56000 datapoints
2025-03-05 22:39:39,489 - INFO - training batch 351, loss: 0.066, 11232/56000 datapoints
2025-03-05 22:39:39,688 - INFO - training batch 401, loss: 0.043, 12832/56000 datapoints
2025-03-05 22:39:39,870 - INFO - training batch 451, loss: 0.023, 14432/56000 datapoints
2025-03-05 22:39:40,073 - INFO - training batch 501, loss: 0.008, 16032/56000 datapoints
2025-03-05 22:39:40,242 - INFO - training batch 551, loss: 0.060, 17632/56000 datapoints
2025-03-05 22:39:40,410 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 22:39:40,563 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:39:40,712 - INFO - training batch 701, loss: 0.268, 22432/56000 datapoints
2025-03-05 22:39:40,862 - INFO - training batch 751, loss: 0.008, 24032/56000 datapoints
2025-03-05 22:39:41,037 - INFO - training batch 801, loss: 0.015, 25632/56000 datapoints
2025-03-05 22:39:41,206 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 22:39:41,368 - INFO - training batch 901, loss: 0.009, 28832/56000 datapoints
2025-03-05 22:39:41,515 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-05 22:39:41,674 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:39:41,821 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-05 22:39:41,953 - INFO - training batch 1101, loss: 0.084, 35232/56000 datapoints
2025-03-05 22:39:42,079 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:39:42,245 - INFO - training batch 1201, loss: 0.018, 38432/56000 datapoints
2025-03-05 22:39:42,423 - INFO - training batch 1251, loss: 0.219, 40032/56000 datapoints
2025-03-05 22:39:43,011 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:39:44,602 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:39:44,842 - INFO - training batch 1501, loss: 0.022, 48032/56000 datapoints
2025-03-05 22:39:45,028 - INFO - training batch 1551, loss: 0.007, 49632/56000 datapoints
2025-03-05 22:39:45,194 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:39:45,367 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:39:45,509 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 22:39:45,707 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-05 22:39:45,797 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-05 22:39:45,881 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-05 22:39:46,040 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-05 22:39:46,110 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-05 22:39:46,197 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-05 22:39:46,276 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-05 22:39:46,383 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-05 22:39:46,447 - INFO - validation batch 401, loss: 0.090, 12832/13984 datapoints
2025-03-05 22:39:46,498 - INFO - Epoch 35/50 done.
2025-03-05 22:39:46,499 - INFO - Final validation performance:
Loss: 0.017, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:39:46,499 - INFO - Epoch 35/50 done
2025-03-05 22:39:46,499 - INFO - Beginning epoch 36/50
2025-03-05 22:39:46,503 - INFO - training batch 1, loss: 0.162, 32/56000 datapoints
2025-03-05 22:39:46,635 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-05 22:39:47,085 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:39:48,750 - INFO - training batch 151, loss: 0.057, 4832/56000 datapoints
2025-03-05 22:39:50,754 - INFO - training batch 201, loss: 0.028, 6432/56000 datapoints
2025-03-05 22:39:54,851 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:39:57,308 - INFO - training batch 301, loss: 0.024, 9632/56000 datapoints
2025-03-05 22:40:00,615 - INFO - training batch 351, loss: 0.065, 11232/56000 datapoints
2025-03-05 22:40:04,685 - INFO - training batch 401, loss: 0.043, 12832/56000 datapoints
2025-03-05 22:40:07,913 - INFO - training batch 451, loss: 0.023, 14432/56000 datapoints
2025-03-05 22:40:09,814 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-05 22:40:11,103 - INFO - training batch 551, loss: 0.059, 17632/56000 datapoints
2025-03-05 22:40:12,520 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 22:40:14,069 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:40:16,016 - INFO - training batch 701, loss: 0.267, 22432/56000 datapoints
2025-03-05 22:40:17,647 - INFO - training batch 751, loss: 0.008, 24032/56000 datapoints
2025-03-05 22:40:18,921 - INFO - training batch 801, loss: 0.014, 25632/56000 datapoints
2025-03-05 22:40:20,452 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 22:40:22,013 - INFO - training batch 901, loss: 0.008, 28832/56000 datapoints
2025-03-05 22:40:23,653 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-05 22:40:24,801 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:40:25,885 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-05 22:40:27,230 - INFO - training batch 1101, loss: 0.084, 35232/56000 datapoints
2025-03-05 22:40:28,613 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:40:29,853 - INFO - training batch 1201, loss: 0.017, 38432/56000 datapoints
2025-03-05 22:40:31,041 - INFO - training batch 1251, loss: 0.217, 40032/56000 datapoints
2025-03-05 22:40:32,384 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:40:32,798 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:40:33,204 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:40:33,626 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:40:34,250 - INFO - training batch 1501, loss: 0.022, 48032/56000 datapoints
2025-03-05 22:40:35,447 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:40:36,052 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:40:37,016 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 22:40:37,490 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-05 22:40:37,728 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-05 22:40:37,958 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-05 22:40:38,192 - INFO - validation batch 151, loss: 0.008, 4832/13984 datapoints
2025-03-05 22:40:38,422 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-05 22:40:38,639 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-05 22:40:38,841 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-05 22:40:39,007 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-05 22:40:39,172 - INFO - validation batch 401, loss: 0.090, 12832/13984 datapoints
2025-03-05 22:40:39,288 - INFO - Epoch 36/50 done.
2025-03-05 22:40:39,288 - INFO - Final validation performance:
Loss: 0.017, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:40:39,290 - INFO - Epoch 36/50 done
2025-03-05 22:40:39,291 - INFO - Beginning epoch 37/50
2025-03-05 22:40:39,308 - INFO - training batch 1, loss: 0.162, 32/56000 datapoints
2025-03-05 22:40:39,600 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-05 22:40:39,873 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:40:40,112 - INFO - training batch 151, loss: 0.057, 4832/56000 datapoints
2025-03-05 22:40:40,370 - INFO - training batch 201, loss: 0.028, 6432/56000 datapoints
2025-03-05 22:40:40,597 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:40:40,822 - INFO - training batch 301, loss: 0.024, 9632/56000 datapoints
2025-03-05 22:40:41,027 - INFO - training batch 351, loss: 0.065, 11232/56000 datapoints
2025-03-05 22:40:41,247 - INFO - training batch 401, loss: 0.042, 12832/56000 datapoints
2025-03-05 22:40:41,449 - INFO - training batch 451, loss: 0.023, 14432/56000 datapoints
2025-03-05 22:40:41,647 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-05 22:40:41,844 - INFO - training batch 551, loss: 0.059, 17632/56000 datapoints
2025-03-05 22:40:42,045 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 22:40:42,240 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:40:42,421 - INFO - training batch 701, loss: 0.267, 22432/56000 datapoints
2025-03-05 22:40:42,603 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-05 22:40:42,785 - INFO - training batch 801, loss: 0.014, 25632/56000 datapoints
2025-03-05 22:40:42,974 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 22:40:43,163 - INFO - training batch 901, loss: 0.008, 28832/56000 datapoints
2025-03-05 22:40:43,329 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-05 22:40:43,506 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:40:43,682 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-05 22:40:43,846 - INFO - training batch 1101, loss: 0.084, 35232/56000 datapoints
2025-03-05 22:40:44,018 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:40:44,212 - INFO - training batch 1201, loss: 0.017, 38432/56000 datapoints
2025-03-05 22:40:44,371 - INFO - training batch 1251, loss: 0.215, 40032/56000 datapoints
2025-03-05 22:40:44,512 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:40:44,668 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:40:44,864 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:40:45,009 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:40:45,148 - INFO - training batch 1501, loss: 0.022, 48032/56000 datapoints
2025-03-05 22:40:45,297 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:40:45,434 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:40:45,604 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:40:45,751 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 22:40:45,883 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-05 22:40:45,947 - INFO - validation batch 51, loss: 0.019, 1632/13984 datapoints
2025-03-05 22:40:46,008 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-05 22:40:46,070 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-05 22:40:46,133 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-05 22:40:46,205 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-05 22:40:46,269 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-05 22:40:46,331 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-05 22:40:46,397 - INFO - validation batch 401, loss: 0.090, 12832/13984 datapoints
2025-03-05 22:40:46,443 - INFO - Epoch 37/50 done.
2025-03-05 22:40:46,443 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:40:46,444 - INFO - Epoch 37/50 done
2025-03-05 22:40:46,444 - INFO - Beginning epoch 38/50
2025-03-05 22:40:46,448 - INFO - training batch 1, loss: 0.162, 32/56000 datapoints
2025-03-05 22:40:46,573 - INFO - training batch 51, loss: 0.007, 1632/56000 datapoints
2025-03-05 22:40:46,698 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:40:46,831 - INFO - training batch 151, loss: 0.057, 4832/56000 datapoints
2025-03-05 22:40:46,953 - INFO - training batch 201, loss: 0.028, 6432/56000 datapoints
2025-03-05 22:40:47,071 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:40:47,196 - INFO - training batch 301, loss: 0.025, 9632/56000 datapoints
2025-03-05 22:40:47,328 - INFO - training batch 351, loss: 0.064, 11232/56000 datapoints
2025-03-05 22:40:47,449 - INFO - training batch 401, loss: 0.042, 12832/56000 datapoints
2025-03-05 22:40:47,566 - INFO - training batch 451, loss: 0.022, 14432/56000 datapoints
2025-03-05 22:40:47,684 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-05 22:40:47,801 - INFO - training batch 551, loss: 0.058, 17632/56000 datapoints
2025-03-05 22:40:47,917 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 22:40:48,029 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:40:48,146 - INFO - training batch 701, loss: 0.266, 22432/56000 datapoints
2025-03-05 22:40:48,259 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-05 22:40:48,371 - INFO - training batch 801, loss: 0.014, 25632/56000 datapoints
2025-03-05 22:40:48,484 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 22:40:48,594 - INFO - training batch 901, loss: 0.008, 28832/56000 datapoints
2025-03-05 22:40:48,702 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-05 22:40:48,809 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:40:48,923 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-05 22:40:49,030 - INFO - training batch 1101, loss: 0.083, 35232/56000 datapoints
2025-03-05 22:40:49,138 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:40:49,255 - INFO - training batch 1201, loss: 0.017, 38432/56000 datapoints
2025-03-05 22:40:49,366 - INFO - training batch 1251, loss: 0.214, 40032/56000 datapoints
2025-03-05 22:40:49,479 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:40:49,587 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:40:49,694 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:40:49,801 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:40:49,916 - INFO - training batch 1501, loss: 0.022, 48032/56000 datapoints
2025-03-05 22:40:50,030 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:40:50,138 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:40:50,253 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:40:50,363 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 22:40:50,477 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-05 22:40:50,532 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-05 22:40:50,588 - INFO - validation batch 101, loss: 0.004, 3232/13984 datapoints
2025-03-05 22:40:50,645 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-05 22:40:50,700 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-05 22:40:50,756 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-05 22:40:50,810 - INFO - validation batch 301, loss: 0.009, 9632/13984 datapoints
2025-03-05 22:40:50,871 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-05 22:40:50,931 - INFO - validation batch 401, loss: 0.090, 12832/13984 datapoints
2025-03-05 22:40:50,971 - INFO - Epoch 38/50 done.
2025-03-05 22:40:50,971 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:40:50,973 - INFO - Epoch 38/50 done
2025-03-05 22:40:50,973 - INFO - Beginning epoch 39/50
2025-03-05 22:40:50,976 - INFO - training batch 1, loss: 0.162, 32/56000 datapoints
2025-03-05 22:40:51,084 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-05 22:40:51,193 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:40:51,308 - INFO - training batch 151, loss: 0.056, 4832/56000 datapoints
2025-03-05 22:40:51,419 - INFO - training batch 201, loss: 0.027, 6432/56000 datapoints
2025-03-05 22:40:51,553 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:40:51,668 - INFO - training batch 301, loss: 0.025, 9632/56000 datapoints
2025-03-05 22:40:51,776 - INFO - training batch 351, loss: 0.064, 11232/56000 datapoints
2025-03-05 22:40:51,884 - INFO - training batch 401, loss: 0.041, 12832/56000 datapoints
2025-03-05 22:40:52,001 - INFO - training batch 451, loss: 0.022, 14432/56000 datapoints
2025-03-05 22:40:52,109 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-05 22:40:52,221 - INFO - training batch 551, loss: 0.057, 17632/56000 datapoints
2025-03-05 22:40:52,331 - INFO - training batch 601, loss: 0.002, 19232/56000 datapoints
2025-03-05 22:40:52,439 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:40:52,554 - INFO - training batch 701, loss: 0.266, 22432/56000 datapoints
2025-03-05 22:40:52,660 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-05 22:40:52,769 - INFO - training batch 801, loss: 0.013, 25632/56000 datapoints
2025-03-05 22:40:52,883 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 22:40:52,992 - INFO - training batch 901, loss: 0.008, 28832/56000 datapoints
2025-03-05 22:40:53,100 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-05 22:40:53,213 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:40:53,323 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-05 22:40:53,429 - INFO - training batch 1101, loss: 0.083, 35232/56000 datapoints
2025-03-05 22:40:53,541 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:40:53,650 - INFO - training batch 1201, loss: 0.017, 38432/56000 datapoints
2025-03-05 22:40:53,759 - INFO - training batch 1251, loss: 0.212, 40032/56000 datapoints
2025-03-05 22:40:53,867 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:40:53,976 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:40:54,085 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:40:54,194 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:40:54,317 - INFO - training batch 1501, loss: 0.022, 48032/56000 datapoints
2025-03-05 22:40:54,451 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:40:54,562 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:40:54,671 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:40:54,779 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 22:40:54,894 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-05 22:40:54,949 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-05 22:40:55,006 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:40:55,061 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-05 22:40:55,116 - INFO - validation batch 201, loss: 0.007, 6432/13984 datapoints
2025-03-05 22:40:55,177 - INFO - validation batch 251, loss: 0.005, 8032/13984 datapoints
2025-03-05 22:40:55,235 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-05 22:40:55,291 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-05 22:40:55,346 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:40:55,386 - INFO - Epoch 39/50 done.
2025-03-05 22:40:55,386 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:40:55,387 - INFO - Epoch 39/50 done
2025-03-05 22:40:55,387 - INFO - Beginning epoch 40/50
2025-03-05 22:40:55,390 - INFO - training batch 1, loss: 0.163, 32/56000 datapoints
2025-03-05 22:40:55,504 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-05 22:40:55,613 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:40:55,732 - INFO - training batch 151, loss: 0.056, 4832/56000 datapoints
2025-03-05 22:40:55,840 - INFO - training batch 201, loss: 0.027, 6432/56000 datapoints
2025-03-05 22:40:55,949 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:40:56,058 - INFO - training batch 301, loss: 0.025, 9632/56000 datapoints
2025-03-05 22:40:56,175 - INFO - training batch 351, loss: 0.064, 11232/56000 datapoints
2025-03-05 22:40:56,283 - INFO - training batch 401, loss: 0.041, 12832/56000 datapoints
2025-03-05 22:40:56,391 - INFO - training batch 451, loss: 0.022, 14432/56000 datapoints
2025-03-05 22:40:56,499 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-05 22:40:56,609 - INFO - training batch 551, loss: 0.056, 17632/56000 datapoints
2025-03-05 22:40:56,716 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-05 22:40:56,823 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:40:56,937 - INFO - training batch 701, loss: 0.265, 22432/56000 datapoints
2025-03-05 22:40:57,047 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-05 22:40:57,157 - INFO - training batch 801, loss: 0.013, 25632/56000 datapoints
2025-03-05 22:40:57,264 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 22:40:57,371 - INFO - training batch 901, loss: 0.008, 28832/56000 datapoints
2025-03-05 22:40:57,483 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-05 22:40:57,599 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:40:57,706 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-05 22:40:57,815 - INFO - training batch 1101, loss: 0.083, 35232/56000 datapoints
2025-03-05 22:40:57,923 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:40:58,033 - INFO - training batch 1201, loss: 0.016, 38432/56000 datapoints
2025-03-05 22:40:58,141 - INFO - training batch 1251, loss: 0.210, 40032/56000 datapoints
2025-03-05 22:40:58,256 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:40:58,363 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:40:58,471 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:40:58,580 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:40:58,690 - INFO - training batch 1501, loss: 0.022, 48032/56000 datapoints
2025-03-05 22:40:58,799 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:40:58,911 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:40:59,018 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:40:59,127 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 22:40:59,275 - INFO - validation batch 1, loss: 0.004, 32/13984 datapoints
2025-03-05 22:40:59,330 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-05 22:40:59,386 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:40:59,441 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-05 22:40:59,497 - INFO - validation batch 201, loss: 0.006, 6432/13984 datapoints
2025-03-05 22:40:59,557 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-05 22:40:59,612 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-05 22:40:59,672 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-05 22:40:59,728 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:40:59,769 - INFO - Epoch 40/50 done.
2025-03-05 22:40:59,769 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:40:59,769 - INFO - Epoch 40/50 done
2025-03-05 22:40:59,770 - INFO - Beginning epoch 41/50
2025-03-05 22:40:59,773 - INFO - training batch 1, loss: 0.163, 32/56000 datapoints
2025-03-05 22:40:59,881 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-05 22:40:59,989 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:41:00,109 - INFO - training batch 151, loss: 0.056, 4832/56000 datapoints
2025-03-05 22:41:00,240 - INFO - training batch 201, loss: 0.026, 6432/56000 datapoints
2025-03-05 22:41:00,355 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:41:00,463 - INFO - training batch 301, loss: 0.025, 9632/56000 datapoints
2025-03-05 22:41:00,575 - INFO - training batch 351, loss: 0.063, 11232/56000 datapoints
2025-03-05 22:41:00,686 - INFO - training batch 401, loss: 0.040, 12832/56000 datapoints
2025-03-05 22:41:00,794 - INFO - training batch 451, loss: 0.022, 14432/56000 datapoints
2025-03-05 22:41:00,912 - INFO - training batch 501, loss: 0.007, 16032/56000 datapoints
2025-03-05 22:41:01,019 - INFO - training batch 551, loss: 0.055, 17632/56000 datapoints
2025-03-05 22:41:01,128 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-05 22:41:01,240 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:41:01,350 - INFO - training batch 701, loss: 0.265, 22432/56000 datapoints
2025-03-05 22:41:01,472 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-05 22:41:01,600 - INFO - training batch 801, loss: 0.013, 25632/56000 datapoints
2025-03-05 22:41:01,723 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 22:41:01,848 - INFO - training batch 901, loss: 0.008, 28832/56000 datapoints
2025-03-05 22:41:01,964 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-05 22:41:02,077 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:41:02,211 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-05 22:41:02,318 - INFO - training batch 1101, loss: 0.083, 35232/56000 datapoints
2025-03-05 22:41:02,426 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:41:02,533 - INFO - training batch 1201, loss: 0.016, 38432/56000 datapoints
2025-03-05 22:41:02,648 - INFO - training batch 1251, loss: 0.209, 40032/56000 datapoints
2025-03-05 22:41:02,756 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:41:02,864 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:41:02,976 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:41:03,083 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:41:03,204 - INFO - training batch 1501, loss: 0.022, 48032/56000 datapoints
2025-03-05 22:41:03,312 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:41:03,419 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:41:03,527 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:41:03,637 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 22:41:03,747 - INFO - validation batch 1, loss: 0.003, 32/13984 datapoints
2025-03-05 22:41:03,801 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-05 22:41:03,859 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:41:03,914 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-05 22:41:03,969 - INFO - validation batch 201, loss: 0.006, 6432/13984 datapoints
2025-03-05 22:41:04,023 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-05 22:41:04,080 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-05 22:41:04,156 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-05 22:41:04,217 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:41:04,257 - INFO - Epoch 41/50 done.
2025-03-05 22:41:04,257 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:41:04,258 - INFO - Epoch 41/50 done
2025-03-05 22:41:04,258 - INFO - Beginning epoch 42/50
2025-03-05 22:41:04,261 - INFO - training batch 1, loss: 0.163, 32/56000 datapoints
2025-03-05 22:41:04,369 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-05 22:41:04,527 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:41:04,647 - INFO - training batch 151, loss: 0.056, 4832/56000 datapoints
2025-03-05 22:41:04,762 - INFO - training batch 201, loss: 0.026, 6432/56000 datapoints
2025-03-05 22:41:04,875 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:41:05,022 - INFO - training batch 301, loss: 0.026, 9632/56000 datapoints
2025-03-05 22:41:05,170 - INFO - training batch 351, loss: 0.063, 11232/56000 datapoints
2025-03-05 22:41:05,310 - INFO - training batch 401, loss: 0.040, 12832/56000 datapoints
2025-03-05 22:41:05,453 - INFO - training batch 451, loss: 0.021, 14432/56000 datapoints
2025-03-05 22:41:05,589 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-05 22:41:05,734 - INFO - training batch 551, loss: 0.054, 17632/56000 datapoints
2025-03-05 22:41:05,860 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-05 22:41:05,984 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:41:06,093 - INFO - training batch 701, loss: 0.264, 22432/56000 datapoints
2025-03-05 22:41:06,219 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-05 22:41:06,328 - INFO - training batch 801, loss: 0.013, 25632/56000 datapoints
2025-03-05 22:41:06,435 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 22:41:06,547 - INFO - training batch 901, loss: 0.007, 28832/56000 datapoints
2025-03-05 22:41:06,658 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-05 22:41:06,765 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:41:06,877 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-05 22:41:06,998 - INFO - training batch 1101, loss: 0.082, 35232/56000 datapoints
2025-03-05 22:41:07,110 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:41:07,228 - INFO - training batch 1201, loss: 0.016, 38432/56000 datapoints
2025-03-05 22:41:07,336 - INFO - training batch 1251, loss: 0.207, 40032/56000 datapoints
2025-03-05 22:41:07,464 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:41:07,579 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:41:07,702 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:41:07,820 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:41:07,929 - INFO - training batch 1501, loss: 0.022, 48032/56000 datapoints
2025-03-05 22:41:08,040 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:41:08,159 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:41:08,269 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:41:08,385 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 22:41:08,498 - INFO - validation batch 1, loss: 0.003, 32/13984 datapoints
2025-03-05 22:41:08,553 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-05 22:41:08,608 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:41:08,666 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-05 22:41:08,722 - INFO - validation batch 201, loss: 0.006, 6432/13984 datapoints
2025-03-05 22:41:08,778 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-05 22:41:08,836 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-05 22:41:08,894 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-05 22:41:08,954 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:41:08,994 - INFO - Epoch 42/50 done.
2025-03-05 22:41:08,995 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:41:08,995 - INFO - Epoch 42/50 done
2025-03-05 22:41:08,996 - INFO - Beginning epoch 43/50
2025-03-05 22:41:08,999 - INFO - training batch 1, loss: 0.163, 32/56000 datapoints
2025-03-05 22:41:09,110 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-05 22:41:09,228 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:41:09,336 - INFO - training batch 151, loss: 0.055, 4832/56000 datapoints
2025-03-05 22:41:09,447 - INFO - training batch 201, loss: 0.026, 6432/56000 datapoints
2025-03-05 22:41:09,557 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:41:09,673 - INFO - training batch 301, loss: 0.026, 9632/56000 datapoints
2025-03-05 22:41:09,784 - INFO - training batch 351, loss: 0.062, 11232/56000 datapoints
2025-03-05 22:41:09,893 - INFO - training batch 401, loss: 0.039, 12832/56000 datapoints
2025-03-05 22:41:10,003 - INFO - training batch 451, loss: 0.021, 14432/56000 datapoints
2025-03-05 22:41:10,116 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-05 22:41:10,234 - INFO - training batch 551, loss: 0.054, 17632/56000 datapoints
2025-03-05 22:41:10,350 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-05 22:41:10,459 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:41:10,568 - INFO - training batch 701, loss: 0.263, 22432/56000 datapoints
2025-03-05 22:41:10,676 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-05 22:41:10,788 - INFO - training batch 801, loss: 0.013, 25632/56000 datapoints
2025-03-05 22:41:10,895 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 22:41:11,009 - INFO - training batch 901, loss: 0.007, 28832/56000 datapoints
2025-03-05 22:41:11,119 - INFO - training batch 951, loss: 0.010, 30432/56000 datapoints
2025-03-05 22:41:11,238 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:41:11,347 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-05 22:41:11,454 - INFO - training batch 1101, loss: 0.082, 35232/56000 datapoints
2025-03-05 22:41:11,568 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:41:11,682 - INFO - training batch 1201, loss: 0.016, 38432/56000 datapoints
2025-03-05 22:41:11,794 - INFO - training batch 1251, loss: 0.205, 40032/56000 datapoints
2025-03-05 22:41:11,901 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:41:12,009 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:41:12,130 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:41:12,241 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:41:12,348 - INFO - training batch 1501, loss: 0.022, 48032/56000 datapoints
2025-03-05 22:41:12,455 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:41:12,565 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:41:12,672 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:41:12,783 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 22:41:12,892 - INFO - validation batch 1, loss: 0.003, 32/13984 datapoints
2025-03-05 22:41:12,953 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-05 22:41:13,008 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:41:13,063 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-05 22:41:13,117 - INFO - validation batch 201, loss: 0.006, 6432/13984 datapoints
2025-03-05 22:41:13,176 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-05 22:41:13,234 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-05 22:41:13,288 - INFO - validation batch 351, loss: 0.005, 11232/13984 datapoints
2025-03-05 22:41:13,343 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:41:13,382 - INFO - Epoch 43/50 done.
2025-03-05 22:41:13,383 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:41:13,383 - INFO - Epoch 43/50 done
2025-03-05 22:41:13,384 - INFO - Beginning epoch 44/50
2025-03-05 22:41:13,388 - INFO - training batch 1, loss: 0.163, 32/56000 datapoints
2025-03-05 22:41:13,502 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-05 22:41:13,612 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:41:13,721 - INFO - training batch 151, loss: 0.055, 4832/56000 datapoints
2025-03-05 22:41:13,836 - INFO - training batch 201, loss: 0.025, 6432/56000 datapoints
2025-03-05 22:41:13,944 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:41:14,053 - INFO - training batch 301, loss: 0.026, 9632/56000 datapoints
2025-03-05 22:41:14,170 - INFO - training batch 351, loss: 0.062, 11232/56000 datapoints
2025-03-05 22:41:14,282 - INFO - training batch 401, loss: 0.039, 12832/56000 datapoints
2025-03-05 22:41:14,390 - INFO - training batch 451, loss: 0.021, 14432/56000 datapoints
2025-03-05 22:41:14,502 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-05 22:41:14,642 - INFO - training batch 551, loss: 0.053, 17632/56000 datapoints
2025-03-05 22:41:14,763 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-05 22:41:14,871 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:41:14,994 - INFO - training batch 701, loss: 0.263, 22432/56000 datapoints
2025-03-05 22:41:15,110 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-05 22:41:15,228 - INFO - training batch 801, loss: 0.012, 25632/56000 datapoints
2025-03-05 22:41:15,340 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 22:41:15,447 - INFO - training batch 901, loss: 0.007, 28832/56000 datapoints
2025-03-05 22:41:15,554 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-05 22:41:15,663 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:41:15,781 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-05 22:41:15,894 - INFO - training batch 1101, loss: 0.082, 35232/56000 datapoints
2025-03-05 22:41:16,001 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:41:16,111 - INFO - training batch 1201, loss: 0.016, 38432/56000 datapoints
2025-03-05 22:41:16,222 - INFO - training batch 1251, loss: 0.204, 40032/56000 datapoints
2025-03-05 22:41:16,334 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:41:16,444 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:41:16,552 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:41:16,659 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:41:16,771 - INFO - training batch 1501, loss: 0.022, 48032/56000 datapoints
2025-03-05 22:41:16,880 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:41:16,992 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:41:17,099 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:41:17,216 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 22:41:17,326 - INFO - validation batch 1, loss: 0.003, 32/13984 datapoints
2025-03-05 22:41:17,381 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-05 22:41:17,435 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:41:17,489 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-05 22:41:17,545 - INFO - validation batch 201, loss: 0.006, 6432/13984 datapoints
2025-03-05 22:41:17,600 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-05 22:41:17,654 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-05 22:41:17,709 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-05 22:41:17,765 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:41:17,825 - INFO - Epoch 44/50 done.
2025-03-05 22:41:17,829 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:41:17,830 - INFO - Epoch 44/50 done
2025-03-05 22:41:17,830 - INFO - Beginning epoch 45/50
2025-03-05 22:41:17,833 - INFO - training batch 1, loss: 0.163, 32/56000 datapoints
2025-03-05 22:41:17,948 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-05 22:41:18,064 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:41:18,175 - INFO - training batch 151, loss: 0.055, 4832/56000 datapoints
2025-03-05 22:41:18,284 - INFO - training batch 201, loss: 0.025, 6432/56000 datapoints
2025-03-05 22:41:18,396 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:41:18,506 - INFO - training batch 301, loss: 0.026, 9632/56000 datapoints
2025-03-05 22:41:18,615 - INFO - training batch 351, loss: 0.062, 11232/56000 datapoints
2025-03-05 22:41:18,724 - INFO - training batch 401, loss: 0.038, 12832/56000 datapoints
2025-03-05 22:41:18,836 - INFO - training batch 451, loss: 0.021, 14432/56000 datapoints
2025-03-05 22:41:18,943 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-05 22:41:19,058 - INFO - training batch 551, loss: 0.052, 17632/56000 datapoints
2025-03-05 22:41:19,175 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-05 22:41:19,287 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:41:19,398 - INFO - training batch 701, loss: 0.262, 22432/56000 datapoints
2025-03-05 22:41:19,506 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-05 22:41:19,617 - INFO - training batch 801, loss: 0.012, 25632/56000 datapoints
2025-03-05 22:41:19,724 - INFO - training batch 851, loss: 0.004, 27232/56000 datapoints
2025-03-05 22:41:19,834 - INFO - training batch 901, loss: 0.007, 28832/56000 datapoints
2025-03-05 22:41:19,941 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-05 22:41:20,050 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:41:20,172 - INFO - training batch 1051, loss: 0.006, 33632/56000 datapoints
2025-03-05 22:41:20,282 - INFO - training batch 1101, loss: 0.081, 35232/56000 datapoints
2025-03-05 22:41:20,403 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:41:20,525 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-05 22:41:20,643 - INFO - training batch 1251, loss: 0.202, 40032/56000 datapoints
2025-03-05 22:41:20,768 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:41:20,882 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:41:20,995 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:41:21,103 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:41:21,213 - INFO - training batch 1501, loss: 0.021, 48032/56000 datapoints
2025-03-05 22:41:21,321 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:41:21,430 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:41:21,539 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:41:21,650 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-05 22:41:21,773 - INFO - validation batch 1, loss: 0.003, 32/13984 datapoints
2025-03-05 22:41:21,867 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-05 22:41:21,939 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:41:22,001 - INFO - validation batch 151, loss: 0.007, 4832/13984 datapoints
2025-03-05 22:41:22,057 - INFO - validation batch 201, loss: 0.006, 6432/13984 datapoints
2025-03-05 22:41:22,112 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-05 22:41:22,174 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-05 22:41:22,231 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-05 22:41:22,286 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:41:22,326 - INFO - Epoch 45/50 done.
2025-03-05 22:41:22,326 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:41:22,326 - INFO - Epoch 45/50 done
2025-03-05 22:41:22,327 - INFO - Beginning epoch 46/50
2025-03-05 22:41:22,330 - INFO - training batch 1, loss: 0.163, 32/56000 datapoints
2025-03-05 22:41:22,448 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-05 22:41:22,558 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:41:22,668 - INFO - training batch 151, loss: 0.055, 4832/56000 datapoints
2025-03-05 22:41:22,782 - INFO - training batch 201, loss: 0.025, 6432/56000 datapoints
2025-03-05 22:41:22,897 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:41:23,017 - INFO - training batch 301, loss: 0.027, 9632/56000 datapoints
2025-03-05 22:41:23,130 - INFO - training batch 351, loss: 0.062, 11232/56000 datapoints
2025-03-05 22:41:23,254 - INFO - training batch 401, loss: 0.038, 12832/56000 datapoints
2025-03-05 22:41:23,374 - INFO - training batch 451, loss: 0.021, 14432/56000 datapoints
2025-03-05 22:41:23,508 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-05 22:41:23,626 - INFO - training batch 551, loss: 0.051, 17632/56000 datapoints
2025-03-05 22:41:23,742 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-05 22:41:23,853 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:41:23,967 - INFO - training batch 701, loss: 0.261, 22432/56000 datapoints
2025-03-05 22:41:24,092 - INFO - training batch 751, loss: 0.007, 24032/56000 datapoints
2025-03-05 22:41:24,217 - INFO - training batch 801, loss: 0.012, 25632/56000 datapoints
2025-03-05 22:41:24,331 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-05 22:41:24,447 - INFO - training batch 901, loss: 0.007, 28832/56000 datapoints
2025-03-05 22:41:24,560 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-05 22:41:24,680 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:41:24,828 - INFO - training batch 1051, loss: 0.005, 33632/56000 datapoints
2025-03-05 22:41:24,949 - INFO - training batch 1101, loss: 0.081, 35232/56000 datapoints
2025-03-05 22:41:25,073 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:41:25,193 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-05 22:41:25,320 - INFO - training batch 1251, loss: 0.200, 40032/56000 datapoints
2025-03-05 22:41:25,443 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:41:25,559 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:41:25,678 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:41:25,809 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:41:25,935 - INFO - training batch 1501, loss: 0.021, 48032/56000 datapoints
2025-03-05 22:41:26,053 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:41:26,177 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:41:26,293 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:41:26,411 - INFO - training batch 1701, loss: 0.084, 54432/56000 datapoints
2025-03-05 22:41:26,527 - INFO - validation batch 1, loss: 0.003, 32/13984 datapoints
2025-03-05 22:41:26,587 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-05 22:41:26,646 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:41:26,707 - INFO - validation batch 151, loss: 0.006, 4832/13984 datapoints
2025-03-05 22:41:26,767 - INFO - validation batch 201, loss: 0.006, 6432/13984 datapoints
2025-03-05 22:41:26,830 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-05 22:41:26,890 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-05 22:41:26,952 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-05 22:41:27,019 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:41:27,062 - INFO - Epoch 46/50 done.
2025-03-05 22:41:27,062 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:41:27,062 - INFO - Epoch 46/50 done
2025-03-05 22:41:27,063 - INFO - Beginning epoch 47/50
2025-03-05 22:41:27,067 - INFO - training batch 1, loss: 0.163, 32/56000 datapoints
2025-03-05 22:41:27,194 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-05 22:41:27,310 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:41:27,434 - INFO - training batch 151, loss: 0.054, 4832/56000 datapoints
2025-03-05 22:41:27,555 - INFO - training batch 201, loss: 0.024, 6432/56000 datapoints
2025-03-05 22:41:27,673 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:41:27,793 - INFO - training batch 301, loss: 0.027, 9632/56000 datapoints
2025-03-05 22:41:27,911 - INFO - training batch 351, loss: 0.061, 11232/56000 datapoints
2025-03-05 22:41:28,031 - INFO - training batch 401, loss: 0.037, 12832/56000 datapoints
2025-03-05 22:41:28,155 - INFO - training batch 451, loss: 0.020, 14432/56000 datapoints
2025-03-05 22:41:28,272 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-05 22:41:28,391 - INFO - training batch 551, loss: 0.051, 17632/56000 datapoints
2025-03-05 22:41:28,510 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-05 22:41:28,628 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:41:28,747 - INFO - training batch 701, loss: 0.261, 22432/56000 datapoints
2025-03-05 22:41:28,863 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-05 22:41:28,981 - INFO - training batch 801, loss: 0.012, 25632/56000 datapoints
2025-03-05 22:41:29,102 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-05 22:41:29,225 - INFO - training batch 901, loss: 0.007, 28832/56000 datapoints
2025-03-05 22:41:29,343 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-05 22:41:29,461 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:41:29,577 - INFO - training batch 1051, loss: 0.005, 33632/56000 datapoints
2025-03-05 22:41:29,695 - INFO - training batch 1101, loss: 0.081, 35232/56000 datapoints
2025-03-05 22:41:29,814 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:41:29,931 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-05 22:41:30,050 - INFO - training batch 1251, loss: 0.199, 40032/56000 datapoints
2025-03-05 22:41:30,172 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:41:30,287 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:41:30,406 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:41:30,526 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:41:30,637 - INFO - training batch 1501, loss: 0.021, 48032/56000 datapoints
2025-03-05 22:41:30,751 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:41:30,863 - INFO - training batch 1601, loss: 0.008, 51232/56000 datapoints
2025-03-05 22:41:30,980 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:41:31,098 - INFO - training batch 1701, loss: 0.084, 54432/56000 datapoints
2025-03-05 22:41:31,216 - INFO - validation batch 1, loss: 0.003, 32/13984 datapoints
2025-03-05 22:41:31,274 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-05 22:41:31,330 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:41:31,389 - INFO - validation batch 151, loss: 0.006, 4832/13984 datapoints
2025-03-05 22:41:31,446 - INFO - validation batch 201, loss: 0.006, 6432/13984 datapoints
2025-03-05 22:41:31,512 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-05 22:41:31,569 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-05 22:41:31,626 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-05 22:41:31,685 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:41:31,726 - INFO - Epoch 47/50 done.
2025-03-05 22:41:31,727 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:41:31,727 - INFO - Epoch 47/50 done
2025-03-05 22:41:31,728 - INFO - Beginning epoch 48/50
2025-03-05 22:41:31,731 - INFO - training batch 1, loss: 0.163, 32/56000 datapoints
2025-03-05 22:41:31,846 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-05 22:41:31,963 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:41:32,082 - INFO - training batch 151, loss: 0.054, 4832/56000 datapoints
2025-03-05 22:41:32,204 - INFO - training batch 201, loss: 0.024, 6432/56000 datapoints
2025-03-05 22:41:32,323 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:41:32,437 - INFO - training batch 301, loss: 0.027, 9632/56000 datapoints
2025-03-05 22:41:32,551 - INFO - training batch 351, loss: 0.061, 11232/56000 datapoints
2025-03-05 22:41:32,663 - INFO - training batch 401, loss: 0.037, 12832/56000 datapoints
2025-03-05 22:41:32,776 - INFO - training batch 451, loss: 0.020, 14432/56000 datapoints
2025-03-05 22:41:32,890 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-05 22:41:33,003 - INFO - training batch 551, loss: 0.050, 17632/56000 datapoints
2025-03-05 22:41:33,122 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-05 22:41:33,237 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:41:33,349 - INFO - training batch 701, loss: 0.260, 22432/56000 datapoints
2025-03-05 22:41:33,460 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-05 22:41:33,576 - INFO - training batch 801, loss: 0.011, 25632/56000 datapoints
2025-03-05 22:41:33,688 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-05 22:41:33,802 - INFO - training batch 901, loss: 0.007, 28832/56000 datapoints
2025-03-05 22:41:33,913 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-05 22:41:34,026 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:41:34,138 - INFO - training batch 1051, loss: 0.005, 33632/56000 datapoints
2025-03-05 22:41:34,256 - INFO - training batch 1101, loss: 0.080, 35232/56000 datapoints
2025-03-05 22:41:34,368 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:41:34,480 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-05 22:41:34,594 - INFO - training batch 1251, loss: 0.197, 40032/56000 datapoints
2025-03-05 22:41:34,710 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:41:34,836 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:41:34,981 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:41:35,100 - INFO - training batch 1451, loss: 0.003, 46432/56000 datapoints
2025-03-05 22:41:35,218 - INFO - training batch 1501, loss: 0.021, 48032/56000 datapoints
2025-03-05 22:41:35,340 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:41:35,457 - INFO - training batch 1601, loss: 0.007, 51232/56000 datapoints
2025-03-05 22:41:35,577 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:41:35,696 - INFO - training batch 1701, loss: 0.084, 54432/56000 datapoints
2025-03-05 22:41:35,815 - INFO - validation batch 1, loss: 0.003, 32/13984 datapoints
2025-03-05 22:41:35,874 - INFO - validation batch 51, loss: 0.017, 1632/13984 datapoints
2025-03-05 22:41:35,933 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:41:35,992 - INFO - validation batch 151, loss: 0.006, 4832/13984 datapoints
2025-03-05 22:41:36,054 - INFO - validation batch 201, loss: 0.006, 6432/13984 datapoints
2025-03-05 22:41:36,113 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-05 22:41:36,174 - INFO - validation batch 301, loss: 0.008, 9632/13984 datapoints
2025-03-05 22:41:36,233 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-05 22:41:36,293 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:41:36,337 - INFO - Epoch 48/50 done.
2025-03-05 22:41:36,337 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:41:36,338 - INFO - Epoch 48/50 done
2025-03-05 22:41:36,338 - INFO - Beginning epoch 49/50
2025-03-05 22:41:36,342 - INFO - training batch 1, loss: 0.163, 32/56000 datapoints
2025-03-05 22:41:36,474 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-05 22:41:36,599 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:41:36,717 - INFO - training batch 151, loss: 0.054, 4832/56000 datapoints
2025-03-05 22:41:36,835 - INFO - training batch 201, loss: 0.024, 6432/56000 datapoints
2025-03-05 22:41:36,953 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:41:37,103 - INFO - training batch 301, loss: 0.027, 9632/56000 datapoints
2025-03-05 22:41:37,241 - INFO - training batch 351, loss: 0.061, 11232/56000 datapoints
2025-03-05 22:41:37,359 - INFO - training batch 401, loss: 0.036, 12832/56000 datapoints
2025-03-05 22:41:37,481 - INFO - training batch 451, loss: 0.020, 14432/56000 datapoints
2025-03-05 22:41:37,600 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-05 22:41:37,717 - INFO - training batch 551, loss: 0.049, 17632/56000 datapoints
2025-03-05 22:41:37,835 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-05 22:41:37,952 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:41:38,069 - INFO - training batch 701, loss: 0.259, 22432/56000 datapoints
2025-03-05 22:41:38,194 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-05 22:41:38,316 - INFO - training batch 801, loss: 0.011, 25632/56000 datapoints
2025-03-05 22:41:38,439 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-05 22:41:38,564 - INFO - training batch 901, loss: 0.007, 28832/56000 datapoints
2025-03-05 22:41:38,686 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-05 22:41:38,808 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:41:38,930 - INFO - training batch 1051, loss: 0.005, 33632/56000 datapoints
2025-03-05 22:41:39,051 - INFO - training batch 1101, loss: 0.080, 35232/56000 datapoints
2025-03-05 22:41:39,181 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:41:39,311 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-05 22:41:39,432 - INFO - training batch 1251, loss: 0.196, 40032/56000 datapoints
2025-03-05 22:41:39,555 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:41:39,679 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:41:39,803 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:41:39,924 - INFO - training batch 1451, loss: 0.002, 46432/56000 datapoints
2025-03-05 22:41:40,046 - INFO - training batch 1501, loss: 0.021, 48032/56000 datapoints
2025-03-05 22:41:40,178 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:41:40,300 - INFO - training batch 1601, loss: 0.007, 51232/56000 datapoints
2025-03-05 22:41:40,425 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:41:40,547 - INFO - training batch 1701, loss: 0.084, 54432/56000 datapoints
2025-03-05 22:41:40,670 - INFO - validation batch 1, loss: 0.003, 32/13984 datapoints
2025-03-05 22:41:40,732 - INFO - validation batch 51, loss: 0.017, 1632/13984 datapoints
2025-03-05 22:41:40,795 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:41:40,861 - INFO - validation batch 151, loss: 0.006, 4832/13984 datapoints
2025-03-05 22:41:40,924 - INFO - validation batch 201, loss: 0.006, 6432/13984 datapoints
2025-03-05 22:41:40,987 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-05 22:41:41,049 - INFO - validation batch 301, loss: 0.007, 9632/13984 datapoints
2025-03-05 22:41:41,118 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-05 22:41:41,187 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:41:41,231 - INFO - Epoch 49/50 done.
2025-03-05 22:41:41,232 - INFO - Final validation performance:
Loss: 0.016, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:41:41,233 - INFO - Epoch 49/50 done
2025-03-05 22:41:41,233 - INFO - Beginning epoch 50/50
2025-03-05 22:41:41,236 - INFO - training batch 1, loss: 0.163, 32/56000 datapoints
2025-03-05 22:41:41,360 - INFO - training batch 51, loss: 0.006, 1632/56000 datapoints
2025-03-05 22:41:41,482 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-05 22:41:41,610 - INFO - training batch 151, loss: 0.054, 4832/56000 datapoints
2025-03-05 22:41:41,736 - INFO - training batch 201, loss: 0.024, 6432/56000 datapoints
2025-03-05 22:41:41,866 - INFO - training batch 251, loss: 0.011, 8032/56000 datapoints
2025-03-05 22:41:41,988 - INFO - training batch 301, loss: 0.028, 9632/56000 datapoints
2025-03-05 22:41:42,112 - INFO - training batch 351, loss: 0.060, 11232/56000 datapoints
2025-03-05 22:41:42,238 - INFO - training batch 401, loss: 0.036, 12832/56000 datapoints
2025-03-05 22:41:42,366 - INFO - training batch 451, loss: 0.020, 14432/56000 datapoints
2025-03-05 22:41:42,497 - INFO - training batch 501, loss: 0.006, 16032/56000 datapoints
2025-03-05 22:41:42,622 - INFO - training batch 551, loss: 0.049, 17632/56000 datapoints
2025-03-05 22:41:42,746 - INFO - training batch 601, loss: 0.001, 19232/56000 datapoints
2025-03-05 22:41:42,874 - INFO - training batch 651, loss: 0.010, 20832/56000 datapoints
2025-03-05 22:41:43,001 - INFO - training batch 701, loss: 0.259, 22432/56000 datapoints
2025-03-05 22:41:43,136 - INFO - training batch 751, loss: 0.006, 24032/56000 datapoints
2025-03-05 22:41:43,269 - INFO - training batch 801, loss: 0.011, 25632/56000 datapoints
2025-03-05 22:41:43,396 - INFO - training batch 851, loss: 0.003, 27232/56000 datapoints
2025-03-05 22:41:43,523 - INFO - training batch 901, loss: 0.006, 28832/56000 datapoints
2025-03-05 22:41:43,651 - INFO - training batch 951, loss: 0.009, 30432/56000 datapoints
2025-03-05 22:41:43,780 - INFO - training batch 1001, loss: 0.004, 32032/56000 datapoints
2025-03-05 22:41:43,908 - INFO - training batch 1051, loss: 0.005, 33632/56000 datapoints
2025-03-05 22:41:44,034 - INFO - training batch 1101, loss: 0.080, 35232/56000 datapoints
2025-03-05 22:41:44,172 - INFO - training batch 1151, loss: 0.001, 36832/56000 datapoints
2025-03-05 22:41:44,299 - INFO - training batch 1201, loss: 0.015, 38432/56000 datapoints
2025-03-05 22:41:44,428 - INFO - training batch 1251, loss: 0.194, 40032/56000 datapoints
2025-03-05 22:41:44,557 - INFO - training batch 1301, loss: 0.018, 41632/56000 datapoints
2025-03-05 22:41:44,686 - INFO - training batch 1351, loss: 0.002, 43232/56000 datapoints
2025-03-05 22:41:44,816 - INFO - training batch 1401, loss: 0.004, 44832/56000 datapoints
2025-03-05 22:41:44,954 - INFO - training batch 1451, loss: 0.002, 46432/56000 datapoints
2025-03-05 22:41:45,128 - INFO - training batch 1501, loss: 0.021, 48032/56000 datapoints
2025-03-05 22:41:45,271 - INFO - training batch 1551, loss: 0.006, 49632/56000 datapoints
2025-03-05 22:41:45,401 - INFO - training batch 1601, loss: 0.007, 51232/56000 datapoints
2025-03-05 22:41:45,530 - INFO - training batch 1651, loss: 0.013, 52832/56000 datapoints
2025-03-05 22:41:45,659 - INFO - training batch 1701, loss: 0.084, 54432/56000 datapoints
2025-03-05 22:41:45,790 - INFO - validation batch 1, loss: 0.003, 32/13984 datapoints
2025-03-05 22:41:45,861 - INFO - validation batch 51, loss: 0.017, 1632/13984 datapoints
2025-03-05 22:41:45,928 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:41:46,013 - INFO - validation batch 151, loss: 0.006, 4832/13984 datapoints
2025-03-05 22:41:46,106 - INFO - validation batch 201, loss: 0.006, 6432/13984 datapoints
2025-03-05 22:41:46,203 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-05 22:41:46,277 - INFO - validation batch 301, loss: 0.007, 9632/13984 datapoints
2025-03-05 22:41:46,342 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-05 22:41:46,407 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:41:46,453 - INFO - Epoch 50/50 done.
2025-03-05 22:41:46,453 - INFO - Final validation performance:
Loss: 0.015, top-1 acc: 0.994top-5 acc: 0.994
2025-03-05 22:41:46,455 - INFO - Epoch 50/50 done
2025-03-05 22:41:46,456 - INFO - Finished training in 18189.75 seconds.
2025-03-05 22:41:46,457 - INFO - Evaluating model...
2025-03-05 22:41:46,460 - INFO - validation batch 1, loss: 0.003, 32/13984 datapoints
2025-03-05 22:41:46,526 - INFO - validation batch 51, loss: 0.017, 1632/13984 datapoints
2025-03-05 22:41:46,591 - INFO - validation batch 101, loss: 0.003, 3232/13984 datapoints
2025-03-05 22:41:46,656 - INFO - validation batch 151, loss: 0.006, 4832/13984 datapoints
2025-03-05 22:41:46,728 - INFO - validation batch 201, loss: 0.006, 6432/13984 datapoints
2025-03-05 22:41:46,793 - INFO - validation batch 251, loss: 0.004, 8032/13984 datapoints
2025-03-05 22:41:46,859 - INFO - validation batch 301, loss: 0.007, 9632/13984 datapoints
2025-03-05 22:41:46,923 - INFO - validation batch 351, loss: 0.004, 11232/13984 datapoints
2025-03-05 22:41:46,993 - INFO - validation batch 401, loss: 0.089, 12832/13984 datapoints
2025-03-05 22:41:47,039 - INFO - Done evaluating.
2025-03-05 22:41:47,054 - ERROR - /Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/numpy/core/_methods.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr = asanyarray(a)
2025-03-05 22:41:47,056 - INFO - Failed hyperparameter combination 1 of 1 with exception: can only concatenate list (not "dict") to list