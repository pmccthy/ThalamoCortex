2025-03-06 17:39:08,610 - INFO - Running hyperparameter combination 1 of 1
2025-03-06 17:39:08,610 - INFO - 0_CTCNet_TC_none
2025-03-06 17:39:08,610 - INFO - Loading data...
2025-03-06 17:39:08,710 - INFO - Done loading.
2025-03-06 17:39:08,711 - INFO - Building model and optimiser...
2025-03-06 17:39:08,718 - INFO - =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Sequential: 1-1                        --
|    └─Linear: 2-1                       1,040
|    └─ReLU: 2-2                         --
├─Sequential: 1-2                        --
|    └─Linear: 2-3                       25,120
|    └─ReLU: 2-4                         --
├─Sequential: 1-3                        --
|    └─Linear: 2-5                       1,056
|    └─ReLU: 2-6                         --
├─Sequential: 1-4                        --
|    └─Linear: 2-7                       330
=================================================================
Total params: 27,546
Trainable params: 27,546
Non-trainable params: 0
=================================================================
2025-03-06 17:39:08,718 - INFO - Done.
2025-03-06 17:39:08,718 - INFO - Training...
2025-03-06 17:39:08,719 - INFO - Beginning epoch 1/800
2025-03-06 17:39:08,791 - INFO - training batch 1, loss: 2.331, 32/60000 datapoints
2025-03-06 17:39:09,088 - INFO - training batch 51, loss: 2.307, 1632/60000 datapoints
2025-03-06 17:39:09,327 - INFO - training batch 101, loss: 2.323, 3232/60000 datapoints
2025-03-06 17:39:09,553 - INFO - training batch 151, loss: 2.300, 4832/60000 datapoints
2025-03-06 17:39:09,808 - INFO - training batch 201, loss: 2.352, 6432/60000 datapoints
2025-03-06 17:39:10,089 - INFO - training batch 251, loss: 2.353, 8032/60000 datapoints
2025-03-06 17:39:10,315 - INFO - training batch 301, loss: 2.331, 9632/60000 datapoints
2025-03-06 17:39:10,582 - INFO - training batch 351, loss: 2.301, 11232/60000 datapoints
2025-03-06 17:39:10,834 - INFO - training batch 401, loss: 2.317, 12832/60000 datapoints
2025-03-06 17:39:11,098 - INFO - training batch 451, loss: 2.271, 14432/60000 datapoints
2025-03-06 17:39:11,361 - INFO - training batch 501, loss: 2.331, 16032/60000 datapoints
2025-03-06 17:39:11,714 - INFO - training batch 551, loss: 2.346, 17632/60000 datapoints
2025-03-06 17:39:12,031 - INFO - training batch 601, loss: 2.313, 19232/60000 datapoints
2025-03-06 17:39:12,287 - INFO - training batch 651, loss: 2.267, 20832/60000 datapoints
2025-03-06 17:39:12,522 - INFO - training batch 701, loss: 2.297, 22432/60000 datapoints
2025-03-06 17:39:12,774 - INFO - training batch 751, loss: 2.353, 24032/60000 datapoints
2025-03-06 17:39:13,011 - INFO - training batch 801, loss: 2.334, 25632/60000 datapoints
2025-03-06 17:39:13,274 - INFO - training batch 851, loss: 2.303, 27232/60000 datapoints
2025-03-06 17:39:13,511 - INFO - training batch 901, loss: 2.303, 28832/60000 datapoints
2025-03-06 17:39:13,754 - INFO - training batch 951, loss: 2.279, 30432/60000 datapoints
2025-03-06 17:39:13,979 - INFO - training batch 1001, loss: 2.314, 32032/60000 datapoints
2025-03-06 17:39:14,230 - INFO - training batch 1051, loss: 2.300, 33632/60000 datapoints
2025-03-06 17:39:14,481 - INFO - training batch 1101, loss: 2.297, 35232/60000 datapoints
2025-03-06 17:39:14,729 - INFO - training batch 1151, loss: 2.310, 36832/60000 datapoints
2025-03-06 17:39:15,003 - INFO - training batch 1201, loss: 2.357, 38432/60000 datapoints
2025-03-06 17:39:15,244 - INFO - training batch 1251, loss: 2.329, 40032/60000 datapoints
2025-03-06 17:39:15,470 - INFO - training batch 1301, loss: 2.347, 41632/60000 datapoints
2025-03-06 17:39:15,683 - INFO - training batch 1351, loss: 2.286, 43232/60000 datapoints
2025-03-06 17:39:15,909 - INFO - training batch 1401, loss: 2.326, 44832/60000 datapoints
2025-03-06 17:39:16,154 - INFO - training batch 1451, loss: 2.269, 46432/60000 datapoints
2025-03-06 17:39:16,406 - INFO - training batch 1501, loss: 2.311, 48032/60000 datapoints
2025-03-06 17:39:16,639 - INFO - training batch 1551, loss: 2.273, 49632/60000 datapoints
2025-03-06 17:39:16,890 - INFO - training batch 1601, loss: 2.294, 51232/60000 datapoints
2025-03-06 17:39:17,150 - INFO - training batch 1651, loss: 2.284, 52832/60000 datapoints
2025-03-06 17:39:17,395 - INFO - training batch 1701, loss: 2.335, 54432/60000 datapoints
2025-03-06 17:39:17,604 - INFO - training batch 1751, loss: 2.297, 56032/60000 datapoints
2025-03-06 17:39:17,818 - INFO - training batch 1801, loss: 2.300, 57632/60000 datapoints
2025-03-06 17:39:18,030 - INFO - training batch 1851, loss: 2.315, 59232/60000 datapoints
2025-03-06 17:39:18,140 - INFO - validation batch 1, loss: 2.337, 32/10016 datapoints
2025-03-06 17:39:18,311 - INFO - validation batch 51, loss: 2.294, 1632/10016 datapoints
2025-03-06 17:39:18,483 - INFO - validation batch 101, loss: 2.285, 3232/10016 datapoints
2025-03-06 17:39:18,674 - INFO - validation batch 151, loss: 2.284, 4832/10016 datapoints
2025-03-06 17:39:18,875 - INFO - validation batch 201, loss: 2.287, 6432/10016 datapoints
2025-03-06 17:39:19,063 - INFO - validation batch 251, loss: 2.285, 8032/10016 datapoints
2025-03-06 17:39:19,245 - INFO - validation batch 301, loss: 2.322, 9632/10016 datapoints
2025-03-06 17:39:19,293 - INFO - train_topk_accs={1: 0.09745, 5: 0.48188333333333333}
2025-03-06 17:39:19,293 - INFO - Epoch 1/800 done.
2025-03-06 17:39:19,293 - INFO - Final validation performance:
Loss: 2.299, top-1 acc: 0.120top-5 acc: 0.120
2025-03-06 17:39:19,294 - INFO - Beginning epoch 2/800
2025-03-06 17:39:19,301 - INFO - training batch 1, loss: 2.274, 32/60000 datapoints
2025-03-06 17:39:19,541 - INFO - training batch 51, loss: 2.342, 1632/60000 datapoints
2025-03-06 17:39:19,807 - INFO - training batch 101, loss: 2.293, 3232/60000 datapoints
2025-03-06 17:39:20,053 - INFO - training batch 151, loss: 2.323, 4832/60000 datapoints
2025-03-06 17:39:20,309 - INFO - training batch 201, loss: 2.345, 6432/60000 datapoints
2025-03-06 17:39:20,552 - INFO - training batch 251, loss: 2.282, 8032/60000 datapoints
2025-03-06 17:39:20,819 - INFO - training batch 301, loss: 2.300, 9632/60000 datapoints
2025-03-06 17:39:21,046 - INFO - training batch 351, loss: 2.285, 11232/60000 datapoints
2025-03-06 17:39:21,265 - INFO - training batch 401, loss: 2.302, 12832/60000 datapoints
2025-03-06 17:39:21,499 - INFO - training batch 451, loss: 2.273, 14432/60000 datapoints
2025-03-06 17:39:21,712 - INFO - training batch 501, loss: 2.334, 16032/60000 datapoints
2025-03-06 17:39:21,924 - INFO - training batch 551, loss: 2.321, 17632/60000 datapoints
2025-03-06 17:39:22,145 - INFO - training batch 601, loss: 2.310, 19232/60000 datapoints
2025-03-06 17:39:22,398 - INFO - training batch 651, loss: 2.313, 20832/60000 datapoints
2025-03-06 17:39:22,696 - INFO - training batch 701, loss: 2.288, 22432/60000 datapoints
2025-03-06 17:39:22,928 - INFO - training batch 751, loss: 2.324, 24032/60000 datapoints
2025-03-06 17:39:23,150 - INFO - training batch 801, loss: 2.299, 25632/60000 datapoints
2025-03-06 17:39:23,391 - INFO - training batch 851, loss: 2.289, 27232/60000 datapoints
2025-03-06 17:39:23,634 - INFO - training batch 901, loss: 2.281, 28832/60000 datapoints
2025-03-06 17:39:23,862 - INFO - training batch 951, loss: 2.292, 30432/60000 datapoints
2025-03-06 17:39:24,102 - INFO - training batch 1001, loss: 2.279, 32032/60000 datapoints
2025-03-06 17:39:24,368 - INFO - training batch 1051, loss: 2.298, 33632/60000 datapoints
2025-03-06 17:39:24,594 - INFO - training batch 1101, loss: 2.273, 35232/60000 datapoints
2025-03-06 17:39:24,821 - INFO - training batch 1151, loss: 2.302, 36832/60000 datapoints
2025-03-06 17:39:25,040 - INFO - training batch 1201, loss: 2.291, 38432/60000 datapoints
2025-03-06 17:39:25,260 - INFO - training batch 1251, loss: 2.269, 40032/60000 datapoints
2025-03-06 17:39:25,496 - INFO - training batch 1301, loss: 2.284, 41632/60000 datapoints
2025-03-06 17:39:25,739 - INFO - training batch 1351, loss: 2.281, 43232/60000 datapoints
2025-03-06 17:39:26,061 - INFO - training batch 1401, loss: 2.326, 44832/60000 datapoints
2025-03-06 17:39:26,282 - INFO - training batch 1451, loss: 2.278, 46432/60000 datapoints
2025-03-06 17:39:26,504 - INFO - training batch 1501, loss: 2.316, 48032/60000 datapoints
2025-03-06 17:39:26,720 - INFO - training batch 1551, loss: 2.310, 49632/60000 datapoints
2025-03-06 17:39:26,930 - INFO - training batch 1601, loss: 2.320, 51232/60000 datapoints
2025-03-06 17:39:27,129 - INFO - training batch 1651, loss: 2.306, 52832/60000 datapoints
2025-03-06 17:39:27,334 - INFO - training batch 1701, loss: 2.288, 54432/60000 datapoints
2025-03-06 17:39:27,537 - INFO - training batch 1751, loss: 2.301, 56032/60000 datapoints
2025-03-06 17:39:27,754 - INFO - training batch 1801, loss: 2.254, 57632/60000 datapoints
2025-03-06 17:39:27,957 - INFO - training batch 1851, loss: 2.291, 59232/60000 datapoints
2025-03-06 17:39:28,065 - INFO - validation batch 1, loss: 2.306, 32/10016 datapoints
2025-03-06 17:39:28,226 - INFO - validation batch 51, loss: 2.252, 1632/10016 datapoints
2025-03-06 17:39:28,388 - INFO - validation batch 101, loss: 2.282, 3232/10016 datapoints
2025-03-06 17:39:28,547 - INFO - validation batch 151, loss: 2.261, 4832/10016 datapoints
2025-03-06 17:39:28,712 - INFO - validation batch 201, loss: 2.284, 6432/10016 datapoints
2025-03-06 17:39:28,875 - INFO - validation batch 251, loss: 2.271, 8032/10016 datapoints
2025-03-06 17:39:29,036 - INFO - validation batch 301, loss: 2.244, 9632/10016 datapoints
2025-03-06 17:39:29,075 - INFO - train_topk_accs={1: 0.14791666666666667, 5: 0.5389166666666667}
2025-03-06 17:39:29,075 - INFO - Epoch 2/800 done.
2025-03-06 17:39:29,075 - INFO - Final validation performance:
Loss: 2.271, top-1 acc: 0.189top-5 acc: 0.189
2025-03-06 17:39:29,076 - INFO - Beginning epoch 3/800
2025-03-06 17:39:29,083 - INFO - training batch 1, loss: 2.281, 32/60000 datapoints
2025-03-06 17:39:29,294 - INFO - training batch 51, loss: 2.289, 1632/60000 datapoints
2025-03-06 17:39:29,497 - INFO - training batch 101, loss: 2.290, 3232/60000 datapoints
2025-03-06 17:39:29,699 - INFO - training batch 151, loss: 2.311, 4832/60000 datapoints
2025-03-06 17:39:29,906 - INFO - training batch 201, loss: 2.286, 6432/60000 datapoints
2025-03-06 17:39:30,114 - INFO - training batch 251, loss: 2.329, 8032/60000 datapoints
2025-03-06 17:39:30,327 - INFO - training batch 301, loss: 2.301, 9632/60000 datapoints
2025-03-06 17:39:30,530 - INFO - training batch 351, loss: 2.283, 11232/60000 datapoints
2025-03-06 17:39:30,740 - INFO - training batch 401, loss: 2.254, 12832/60000 datapoints
2025-03-06 17:39:30,947 - INFO - training batch 451, loss: 2.302, 14432/60000 datapoints
2025-03-06 17:39:31,151 - INFO - training batch 501, loss: 2.237, 16032/60000 datapoints
2025-03-06 17:39:31,356 - INFO - training batch 551, loss: 2.291, 17632/60000 datapoints
2025-03-06 17:39:31,557 - INFO - training batch 601, loss: 2.256, 19232/60000 datapoints
2025-03-06 17:39:31,768 - INFO - training batch 651, loss: 2.266, 20832/60000 datapoints
2025-03-06 17:39:31,976 - INFO - training batch 701, loss: 2.298, 22432/60000 datapoints
2025-03-06 17:39:32,181 - INFO - training batch 751, loss: 2.292, 24032/60000 datapoints
2025-03-06 17:39:32,391 - INFO - training batch 801, loss: 2.283, 25632/60000 datapoints
2025-03-06 17:39:32,598 - INFO - training batch 851, loss: 2.272, 27232/60000 datapoints
2025-03-06 17:39:32,806 - INFO - training batch 901, loss: 2.260, 28832/60000 datapoints
2025-03-06 17:39:33,057 - INFO - training batch 951, loss: 2.262, 30432/60000 datapoints
2025-03-06 17:39:33,317 - INFO - training batch 1001, loss: 2.282, 32032/60000 datapoints
2025-03-06 17:39:33,534 - INFO - training batch 1051, loss: 2.273, 33632/60000 datapoints
2025-03-06 17:39:33,787 - INFO - training batch 1101, loss: 2.288, 35232/60000 datapoints
2025-03-06 17:39:34,067 - INFO - training batch 1151, loss: 2.235, 36832/60000 datapoints
2025-03-06 17:39:34,289 - INFO - training batch 1201, loss: 2.271, 38432/60000 datapoints
2025-03-06 17:39:34,512 - INFO - training batch 1251, loss: 2.299, 40032/60000 datapoints
2025-03-06 17:39:34,736 - INFO - training batch 1301, loss: 2.242, 41632/60000 datapoints
2025-03-06 17:39:34,992 - INFO - training batch 1351, loss: 2.280, 43232/60000 datapoints
2025-03-06 17:39:35,213 - INFO - training batch 1401, loss: 2.250, 44832/60000 datapoints
2025-03-06 17:39:35,602 - INFO - training batch 1451, loss: 2.262, 46432/60000 datapoints
2025-03-06 17:39:35,952 - INFO - training batch 1501, loss: 2.306, 48032/60000 datapoints
2025-03-06 17:39:36,229 - INFO - training batch 1551, loss: 2.240, 49632/60000 datapoints
2025-03-06 17:39:36,471 - INFO - training batch 1601, loss: 2.242, 51232/60000 datapoints
2025-03-06 17:39:36,700 - INFO - training batch 1651, loss: 2.256, 52832/60000 datapoints
2025-03-06 17:39:36,958 - INFO - training batch 1701, loss: 2.222, 54432/60000 datapoints
2025-03-06 17:39:37,210 - INFO - training batch 1751, loss: 2.255, 56032/60000 datapoints
2025-03-06 17:39:37,428 - INFO - training batch 1801, loss: 2.259, 57632/60000 datapoints
2025-03-06 17:39:37,707 - INFO - training batch 1851, loss: 2.252, 59232/60000 datapoints
2025-03-06 17:39:37,820 - INFO - validation batch 1, loss: 2.252, 32/10016 datapoints
2025-03-06 17:39:38,005 - INFO - validation batch 51, loss: 2.257, 1632/10016 datapoints
2025-03-06 17:39:38,191 - INFO - validation batch 101, loss: 2.253, 3232/10016 datapoints
2025-03-06 17:39:38,373 - INFO - validation batch 151, loss: 2.246, 4832/10016 datapoints
2025-03-06 17:39:38,573 - INFO - validation batch 201, loss: 2.258, 6432/10016 datapoints
2025-03-06 17:39:38,758 - INFO - validation batch 251, loss: 2.260, 8032/10016 datapoints
2025-03-06 17:39:38,949 - INFO - validation batch 301, loss: 2.259, 9632/10016 datapoints
2025-03-06 17:39:38,999 - INFO - train_topk_accs={1: 0.23498333333333332, 5: 0.6143666666666666}
2025-03-06 17:39:39,000 - INFO - Epoch 3/800 done.
2025-03-06 17:39:39,000 - INFO - Final validation performance:
Loss: 2.255, top-1 acc: 0.292top-5 acc: 0.292
2025-03-06 17:39:39,000 - INFO - Beginning epoch 4/800
2025-03-06 17:39:39,007 - INFO - training batch 1, loss: 2.304, 32/60000 datapoints
2025-03-06 17:39:39,242 - INFO - training batch 51, loss: 2.262, 1632/60000 datapoints
2025-03-06 17:39:39,504 - INFO - training batch 101, loss: 2.283, 3232/60000 datapoints
2025-03-06 17:39:39,862 - INFO - training batch 151, loss: 2.298, 4832/60000 datapoints
2025-03-06 17:39:40,116 - INFO - training batch 201, loss: 2.259, 6432/60000 datapoints
2025-03-06 17:39:40,366 - INFO - training batch 251, loss: 2.309, 8032/60000 datapoints
2025-03-06 17:39:40,629 - INFO - training batch 301, loss: 2.237, 9632/60000 datapoints
2025-03-06 17:39:40,891 - INFO - training batch 351, loss: 2.265, 11232/60000 datapoints
2025-03-06 17:39:41,123 - INFO - training batch 401, loss: 2.257, 12832/60000 datapoints
2025-03-06 17:39:41,366 - INFO - training batch 451, loss: 2.242, 14432/60000 datapoints
2025-03-06 17:39:41,623 - INFO - training batch 501, loss: 2.213, 16032/60000 datapoints
2025-03-06 17:39:41,880 - INFO - training batch 551, loss: 2.268, 17632/60000 datapoints
2025-03-06 17:39:42,127 - INFO - training batch 601, loss: 2.273, 19232/60000 datapoints
2025-03-06 17:39:42,438 - INFO - training batch 651, loss: 2.286, 20832/60000 datapoints
2025-03-06 17:39:42,660 - INFO - training batch 701, loss: 2.265, 22432/60000 datapoints
2025-03-06 17:39:42,885 - INFO - training batch 751, loss: 2.265, 24032/60000 datapoints
2025-03-06 17:39:43,133 - INFO - training batch 801, loss: 2.289, 25632/60000 datapoints
2025-03-06 17:39:43,361 - INFO - training batch 851, loss: 2.277, 27232/60000 datapoints
2025-03-06 17:39:43,573 - INFO - training batch 901, loss: 2.257, 28832/60000 datapoints
2025-03-06 17:39:43,790 - INFO - training batch 951, loss: 2.222, 30432/60000 datapoints
2025-03-06 17:39:44,027 - INFO - training batch 1001, loss: 2.283, 32032/60000 datapoints
2025-03-06 17:39:44,271 - INFO - training batch 1051, loss: 2.267, 33632/60000 datapoints
2025-03-06 17:39:44,503 - INFO - training batch 1101, loss: 2.287, 35232/60000 datapoints
2025-03-06 17:39:44,717 - INFO - training batch 1151, loss: 2.212, 36832/60000 datapoints
2025-03-06 17:39:44,962 - INFO - training batch 1201, loss: 2.223, 38432/60000 datapoints
2025-03-06 17:39:45,198 - INFO - training batch 1251, loss: 2.226, 40032/60000 datapoints
2025-03-06 17:39:45,406 - INFO - training batch 1301, loss: 2.237, 41632/60000 datapoints
2025-03-06 17:39:45,638 - INFO - training batch 1351, loss: 2.233, 43232/60000 datapoints
2025-03-06 17:39:45,854 - INFO - training batch 1401, loss: 2.240, 44832/60000 datapoints
2025-03-06 17:39:46,368 - INFO - training batch 1451, loss: 2.249, 46432/60000 datapoints
2025-03-06 17:39:46,621 - INFO - training batch 1501, loss: 2.279, 48032/60000 datapoints
2025-03-06 17:39:46,859 - INFO - training batch 1551, loss: 2.225, 49632/60000 datapoints
2025-03-06 17:39:47,091 - INFO - training batch 1601, loss: 2.250, 51232/60000 datapoints
2025-03-06 17:39:47,312 - INFO - training batch 1651, loss: 2.269, 52832/60000 datapoints
2025-03-06 17:39:47,548 - INFO - training batch 1701, loss: 2.248, 54432/60000 datapoints
2025-03-06 17:39:47,767 - INFO - training batch 1751, loss: 2.218, 56032/60000 datapoints
2025-03-06 17:39:47,985 - INFO - training batch 1801, loss: 2.190, 57632/60000 datapoints
2025-03-06 17:39:48,190 - INFO - training batch 1851, loss: 2.245, 59232/60000 datapoints
2025-03-06 17:39:48,296 - INFO - validation batch 1, loss: 2.243, 32/10016 datapoints
2025-03-06 17:39:48,463 - INFO - validation batch 51, loss: 2.261, 1632/10016 datapoints
2025-03-06 17:39:48,629 - INFO - validation batch 101, loss: 2.270, 3232/10016 datapoints
2025-03-06 17:39:48,797 - INFO - validation batch 151, loss: 2.202, 4832/10016 datapoints
2025-03-06 17:39:48,967 - INFO - validation batch 201, loss: 2.244, 6432/10016 datapoints
2025-03-06 17:39:49,130 - INFO - validation batch 251, loss: 2.249, 8032/10016 datapoints
2025-03-06 17:39:49,293 - INFO - validation batch 301, loss: 2.242, 9632/10016 datapoints
2025-03-06 17:39:49,333 - INFO - train_topk_accs={1: 0.32506666666666667, 5: 0.6688333333333333}
2025-03-06 17:39:49,333 - INFO - Epoch 4/800 done.
2025-03-06 17:39:49,333 - INFO - Final validation performance:
Loss: 2.244, top-1 acc: 0.361top-5 acc: 0.361
2025-03-06 17:39:49,334 - INFO - Beginning epoch 5/800
2025-03-06 17:39:49,339 - INFO - training batch 1, loss: 2.237, 32/60000 datapoints
2025-03-06 17:39:49,563 - INFO - training batch 51, loss: 2.247, 1632/60000 datapoints
2025-03-06 17:39:49,790 - INFO - training batch 101, loss: 2.253, 3232/60000 datapoints
2025-03-06 17:39:50,003 - INFO - training batch 151, loss: 2.249, 4832/60000 datapoints
2025-03-06 17:39:50,205 - INFO - training batch 201, loss: 2.210, 6432/60000 datapoints
2025-03-06 17:39:50,426 - INFO - training batch 251, loss: 2.246, 8032/60000 datapoints
2025-03-06 17:39:50,723 - INFO - training batch 301, loss: 2.261, 9632/60000 datapoints
2025-03-06 17:39:51,221 - INFO - training batch 351, loss: 2.216, 11232/60000 datapoints
2025-03-06 17:39:51,481 - INFO - training batch 401, loss: 2.186, 12832/60000 datapoints
2025-03-06 17:39:51,724 - INFO - training batch 451, loss: 2.266, 14432/60000 datapoints
2025-03-06 17:39:52,240 - INFO - training batch 501, loss: 2.224, 16032/60000 datapoints
2025-03-06 17:39:52,530 - INFO - training batch 551, loss: 2.264, 17632/60000 datapoints
2025-03-06 17:39:52,752 - INFO - training batch 601, loss: 2.230, 19232/60000 datapoints
2025-03-06 17:39:52,999 - INFO - training batch 651, loss: 2.221, 20832/60000 datapoints
2025-03-06 17:39:53,218 - INFO - training batch 701, loss: 2.274, 22432/60000 datapoints
2025-03-06 17:39:53,436 - INFO - training batch 751, loss: 2.199, 24032/60000 datapoints
2025-03-06 17:39:53,687 - INFO - training batch 801, loss: 2.249, 25632/60000 datapoints
2025-03-06 17:39:53,938 - INFO - training batch 851, loss: 2.265, 27232/60000 datapoints
2025-03-06 17:39:54,165 - INFO - training batch 901, loss: 2.224, 28832/60000 datapoints
2025-03-06 17:39:54,375 - INFO - training batch 951, loss: 2.282, 30432/60000 datapoints
2025-03-06 17:39:54,586 - INFO - training batch 1001, loss: 2.244, 32032/60000 datapoints
2025-03-06 17:39:54,810 - INFO - training batch 1051, loss: 2.209, 33632/60000 datapoints
2025-03-06 17:39:55,112 - INFO - training batch 1101, loss: 2.218, 35232/60000 datapoints
2025-03-06 17:39:55,331 - INFO - training batch 1151, loss: 2.253, 36832/60000 datapoints
2025-03-06 17:39:55,548 - INFO - training batch 1201, loss: 2.184, 38432/60000 datapoints
2025-03-06 17:39:55,770 - INFO - training batch 1251, loss: 2.215, 40032/60000 datapoints
2025-03-06 17:39:55,980 - INFO - training batch 1301, loss: 2.227, 41632/60000 datapoints
2025-03-06 17:39:56,192 - INFO - training batch 1351, loss: 2.228, 43232/60000 datapoints
2025-03-06 17:39:56,423 - INFO - training batch 1401, loss: 2.224, 44832/60000 datapoints
2025-03-06 17:39:56,696 - INFO - training batch 1451, loss: 2.189, 46432/60000 datapoints
2025-03-06 17:39:56,918 - INFO - training batch 1501, loss: 2.184, 48032/60000 datapoints
2025-03-06 17:39:57,142 - INFO - training batch 1551, loss: 2.230, 49632/60000 datapoints
2025-03-06 17:39:57,367 - INFO - training batch 1601, loss: 2.294, 51232/60000 datapoints
2025-03-06 17:39:57,578 - INFO - training batch 1651, loss: 2.216, 52832/60000 datapoints
2025-03-06 17:39:57,805 - INFO - training batch 1701, loss: 2.208, 54432/60000 datapoints
2025-03-06 17:39:58,019 - INFO - training batch 1751, loss: 2.238, 56032/60000 datapoints
2025-03-06 17:39:58,237 - INFO - training batch 1801, loss: 2.167, 57632/60000 datapoints
2025-03-06 17:39:58,443 - INFO - training batch 1851, loss: 2.205, 59232/60000 datapoints
2025-03-06 17:39:58,549 - INFO - validation batch 1, loss: 2.199, 32/10016 datapoints
2025-03-06 17:39:58,724 - INFO - validation batch 51, loss: 2.192, 1632/10016 datapoints
2025-03-06 17:39:58,893 - INFO - validation batch 101, loss: 2.173, 3232/10016 datapoints
2025-03-06 17:39:59,068 - INFO - validation batch 151, loss: 2.176, 4832/10016 datapoints
2025-03-06 17:39:59,231 - INFO - validation batch 201, loss: 2.198, 6432/10016 datapoints
2025-03-06 17:39:59,405 - INFO - validation batch 251, loss: 2.193, 8032/10016 datapoints
2025-03-06 17:39:59,664 - INFO - validation batch 301, loss: 2.201, 9632/10016 datapoints
2025-03-06 17:39:59,713 - INFO - train_topk_accs={1: 0.37648333333333334, 5: 0.7260166666666666}
2025-03-06 17:39:59,713 - INFO - Epoch 5/800 done.
2025-03-06 17:39:59,713 - INFO - Final validation performance:
Loss: 2.190, top-1 acc: 0.398top-5 acc: 0.398
2025-03-06 17:39:59,714 - INFO - Beginning epoch 6/800
2025-03-06 17:39:59,722 - INFO - training batch 1, loss: 2.243, 32/60000 datapoints
2025-03-06 17:39:59,965 - INFO - training batch 51, loss: 2.219, 1632/60000 datapoints
2025-03-06 17:40:00,194 - INFO - training batch 101, loss: 2.188, 3232/60000 datapoints
2025-03-06 17:40:00,424 - INFO - training batch 151, loss: 2.192, 4832/60000 datapoints
2025-03-06 17:40:00,649 - INFO - training batch 201, loss: 2.216, 6432/60000 datapoints
2025-03-06 17:40:00,868 - INFO - training batch 251, loss: 2.207, 8032/60000 datapoints
2025-03-06 17:40:01,086 - INFO - training batch 301, loss: 2.243, 9632/60000 datapoints
2025-03-06 17:40:01,467 - INFO - training batch 351, loss: 2.210, 11232/60000 datapoints
2025-03-06 17:40:01,764 - INFO - training batch 401, loss: 2.181, 12832/60000 datapoints
2025-03-06 17:40:02,021 - INFO - training batch 451, loss: 2.202, 14432/60000 datapoints
2025-03-06 17:40:02,311 - INFO - training batch 501, loss: 2.204, 16032/60000 datapoints
2025-03-06 17:40:02,535 - INFO - training batch 551, loss: 2.204, 17632/60000 datapoints
2025-03-06 17:40:02,765 - INFO - training batch 601, loss: 2.209, 19232/60000 datapoints
2025-03-06 17:40:03,010 - INFO - training batch 651, loss: 2.229, 20832/60000 datapoints
2025-03-06 17:40:03,234 - INFO - training batch 701, loss: 2.197, 22432/60000 datapoints
2025-03-06 17:40:03,480 - INFO - training batch 751, loss: 2.242, 24032/60000 datapoints
2025-03-06 17:40:03,734 - INFO - training batch 801, loss: 2.243, 25632/60000 datapoints
2025-03-06 17:40:04,056 - INFO - training batch 851, loss: 2.236, 27232/60000 datapoints
2025-03-06 17:40:04,328 - INFO - training batch 901, loss: 2.216, 28832/60000 datapoints
2025-03-06 17:40:04,568 - INFO - training batch 951, loss: 2.229, 30432/60000 datapoints
2025-03-06 17:40:04,833 - INFO - training batch 1001, loss: 2.235, 32032/60000 datapoints
2025-03-06 17:40:05,086 - INFO - training batch 1051, loss: 2.234, 33632/60000 datapoints
2025-03-06 17:40:05,334 - INFO - training batch 1101, loss: 2.204, 35232/60000 datapoints
2025-03-06 17:40:05,590 - INFO - training batch 1151, loss: 2.217, 36832/60000 datapoints
2025-03-06 17:40:05,839 - INFO - training batch 1201, loss: 2.179, 38432/60000 datapoints
2025-03-06 17:40:06,078 - INFO - training batch 1251, loss: 2.222, 40032/60000 datapoints
2025-03-06 17:40:06,321 - INFO - training batch 1301, loss: 2.219, 41632/60000 datapoints
2025-03-06 17:40:06,606 - INFO - training batch 1351, loss: 2.170, 43232/60000 datapoints
2025-03-06 17:40:06,868 - INFO - training batch 1401, loss: 2.132, 44832/60000 datapoints
2025-03-06 17:40:07,198 - INFO - training batch 1451, loss: 2.204, 46432/60000 datapoints
2025-03-06 17:40:07,633 - INFO - training batch 1501, loss: 2.193, 48032/60000 datapoints
2025-03-06 17:40:07,916 - INFO - training batch 1551, loss: 2.206, 49632/60000 datapoints
2025-03-06 17:40:08,173 - INFO - training batch 1601, loss: 2.194, 51232/60000 datapoints
2025-03-06 17:40:08,462 - INFO - training batch 1651, loss: 2.188, 52832/60000 datapoints
2025-03-06 17:40:08,787 - INFO - training batch 1701, loss: 2.168, 54432/60000 datapoints
2025-03-06 17:40:09,070 - INFO - training batch 1751, loss: 2.207, 56032/60000 datapoints
2025-03-06 17:40:09,354 - INFO - training batch 1801, loss: 2.186, 57632/60000 datapoints
2025-03-06 17:40:09,606 - INFO - training batch 1851, loss: 2.163, 59232/60000 datapoints
2025-03-06 17:40:09,716 - INFO - validation batch 1, loss: 2.167, 32/10016 datapoints
2025-03-06 17:40:09,918 - INFO - validation batch 51, loss: 2.175, 1632/10016 datapoints
2025-03-06 17:40:10,130 - INFO - validation batch 101, loss: 2.214, 3232/10016 datapoints
2025-03-06 17:40:10,325 - INFO - validation batch 151, loss: 2.161, 4832/10016 datapoints
2025-03-06 17:40:10,524 - INFO - validation batch 201, loss: 2.212, 6432/10016 datapoints
2025-03-06 17:40:10,710 - INFO - validation batch 251, loss: 2.157, 8032/10016 datapoints
2025-03-06 17:40:10,893 - INFO - validation batch 301, loss: 2.212, 9632/10016 datapoints
2025-03-06 17:40:10,937 - INFO - train_topk_accs={1: 0.4086666666666667, 5: 0.7778833333333334}
2025-03-06 17:40:10,937 - INFO - Epoch 6/800 done.
2025-03-06 17:40:10,937 - INFO - Final validation performance:
Loss: 2.185, top-1 acc: 0.434top-5 acc: 0.434
2025-03-06 17:40:10,938 - INFO - Beginning epoch 7/800
2025-03-06 17:40:10,945 - INFO - training batch 1, loss: 2.197, 32/60000 datapoints
2025-03-06 17:40:11,168 - INFO - training batch 51, loss: 2.173, 1632/60000 datapoints
2025-03-06 17:40:11,380 - INFO - training batch 101, loss: 2.172, 3232/60000 datapoints
2025-03-06 17:40:11,591 - INFO - training batch 151, loss: 2.180, 4832/60000 datapoints
2025-03-06 17:40:11,811 - INFO - training batch 201, loss: 2.188, 6432/60000 datapoints
2025-03-06 17:40:12,019 - INFO - training batch 251, loss: 2.169, 8032/60000 datapoints
2025-03-06 17:40:12,224 - INFO - training batch 301, loss: 2.198, 9632/60000 datapoints
2025-03-06 17:40:12,437 - INFO - training batch 351, loss: 2.184, 11232/60000 datapoints
2025-03-06 17:40:12,645 - INFO - training batch 401, loss: 2.165, 12832/60000 datapoints
2025-03-06 17:40:12,859 - INFO - training batch 451, loss: 2.201, 14432/60000 datapoints
2025-03-06 17:40:13,065 - INFO - training batch 501, loss: 2.143, 16032/60000 datapoints
2025-03-06 17:40:13,267 - INFO - training batch 551, loss: 2.223, 17632/60000 datapoints
2025-03-06 17:40:13,492 - INFO - training batch 601, loss: 2.224, 19232/60000 datapoints
2025-03-06 17:40:13,707 - INFO - training batch 651, loss: 2.193, 20832/60000 datapoints
2025-03-06 17:40:13,915 - INFO - training batch 701, loss: 2.167, 22432/60000 datapoints
2025-03-06 17:40:14,117 - INFO - training batch 751, loss: 2.187, 24032/60000 datapoints
2025-03-06 17:40:14,328 - INFO - training batch 801, loss: 2.195, 25632/60000 datapoints
2025-03-06 17:40:14,536 - INFO - training batch 851, loss: 2.187, 27232/60000 datapoints
2025-03-06 17:40:14,739 - INFO - training batch 901, loss: 2.194, 28832/60000 datapoints
2025-03-06 17:40:14,965 - INFO - training batch 951, loss: 2.159, 30432/60000 datapoints
2025-03-06 17:40:15,191 - INFO - training batch 1001, loss: 2.216, 32032/60000 datapoints
2025-03-06 17:40:15,395 - INFO - training batch 1051, loss: 2.168, 33632/60000 datapoints
2025-03-06 17:40:15,605 - INFO - training batch 1101, loss: 2.193, 35232/60000 datapoints
2025-03-06 17:40:15,818 - INFO - training batch 1151, loss: 2.209, 36832/60000 datapoints
2025-03-06 17:40:16,021 - INFO - training batch 1201, loss: 2.168, 38432/60000 datapoints
2025-03-06 17:40:16,245 - INFO - training batch 1251, loss: 2.164, 40032/60000 datapoints
2025-03-06 17:40:16,463 - INFO - training batch 1301, loss: 2.117, 41632/60000 datapoints
2025-03-06 17:40:16,730 - INFO - training batch 1351, loss: 2.142, 43232/60000 datapoints
2025-03-06 17:40:16,963 - INFO - training batch 1401, loss: 2.160, 44832/60000 datapoints
2025-03-06 17:40:17,191 - INFO - training batch 1451, loss: 2.143, 46432/60000 datapoints
2025-03-06 17:40:17,515 - INFO - training batch 1501, loss: 2.167, 48032/60000 datapoints
2025-03-06 17:40:17,777 - INFO - training batch 1551, loss: 2.184, 49632/60000 datapoints
2025-03-06 17:40:18,035 - INFO - training batch 1601, loss: 2.159, 51232/60000 datapoints
2025-03-06 17:40:18,329 - INFO - training batch 1651, loss: 2.188, 52832/60000 datapoints
2025-03-06 17:40:18,572 - INFO - training batch 1701, loss: 2.152, 54432/60000 datapoints
2025-03-06 17:40:18,803 - INFO - training batch 1751, loss: 2.167, 56032/60000 datapoints
2025-03-06 17:40:19,035 - INFO - training batch 1801, loss: 2.159, 57632/60000 datapoints
2025-03-06 17:40:19,258 - INFO - training batch 1851, loss: 2.202, 59232/60000 datapoints
2025-03-06 17:40:19,378 - INFO - validation batch 1, loss: 2.148, 32/10016 datapoints
2025-03-06 17:40:19,564 - INFO - validation batch 51, loss: 2.135, 1632/10016 datapoints
2025-03-06 17:40:19,741 - INFO - validation batch 101, loss: 2.183, 3232/10016 datapoints
2025-03-06 17:40:19,914 - INFO - validation batch 151, loss: 2.190, 4832/10016 datapoints
2025-03-06 17:40:20,089 - INFO - validation batch 201, loss: 2.152, 6432/10016 datapoints
2025-03-06 17:40:20,261 - INFO - validation batch 251, loss: 2.166, 8032/10016 datapoints
2025-03-06 17:40:20,427 - INFO - validation batch 301, loss: 2.154, 9632/10016 datapoints
2025-03-06 17:40:20,468 - INFO - train_topk_accs={1: 0.4366833333333333, 5: 0.8152}
2025-03-06 17:40:20,468 - INFO - Epoch 7/800 done.
2025-03-06 17:40:20,468 - INFO - Final validation performance:
Loss: 2.161, top-1 acc: 0.459top-5 acc: 0.459
2025-03-06 17:40:20,468 - INFO - Beginning epoch 8/800
2025-03-06 17:40:20,474 - INFO - training batch 1, loss: 2.147, 32/60000 datapoints
2025-03-06 17:40:20,706 - INFO - training batch 51, loss: 2.136, 1632/60000 datapoints
2025-03-06 17:40:20,936 - INFO - training batch 101, loss: 2.167, 3232/60000 datapoints
2025-03-06 17:40:21,168 - INFO - training batch 151, loss: 2.172, 4832/60000 datapoints
2025-03-06 17:40:21,405 - INFO - training batch 201, loss: 2.172, 6432/60000 datapoints
2025-03-06 17:40:21,628 - INFO - training batch 251, loss: 2.145, 8032/60000 datapoints
2025-03-06 17:40:21,843 - INFO - training batch 301, loss: 2.153, 9632/60000 datapoints
2025-03-06 17:40:22,154 - INFO - training batch 351, loss: 2.156, 11232/60000 datapoints
2025-03-06 17:40:22,439 - INFO - training batch 401, loss: 2.141, 12832/60000 datapoints
2025-03-06 17:40:22,699 - INFO - training batch 451, loss: 2.142, 14432/60000 datapoints
2025-03-06 17:40:22,950 - INFO - training batch 501, loss: 2.151, 16032/60000 datapoints
2025-03-06 17:40:23,205 - INFO - training batch 551, loss: 2.140, 17632/60000 datapoints
2025-03-06 17:40:23,471 - INFO - training batch 601, loss: 2.164, 19232/60000 datapoints
2025-03-06 17:40:23,742 - INFO - training batch 651, loss: 2.160, 20832/60000 datapoints
2025-03-06 17:40:24,027 - INFO - training batch 701, loss: 2.139, 22432/60000 datapoints
2025-03-06 17:40:24,285 - INFO - training batch 751, loss: 2.144, 24032/60000 datapoints
2025-03-06 17:40:24,509 - INFO - training batch 801, loss: 2.137, 25632/60000 datapoints
2025-03-06 17:40:24,901 - INFO - training batch 851, loss: 2.161, 27232/60000 datapoints
2025-03-06 17:40:25,274 - INFO - training batch 901, loss: 2.129, 28832/60000 datapoints
2025-03-06 17:40:25,578 - INFO - training batch 951, loss: 2.096, 30432/60000 datapoints
2025-03-06 17:40:25,858 - INFO - training batch 1001, loss: 2.168, 32032/60000 datapoints
2025-03-06 17:40:26,097 - INFO - training batch 1051, loss: 2.169, 33632/60000 datapoints
2025-03-06 17:40:26,407 - INFO - training batch 1101, loss: 2.121, 35232/60000 datapoints
2025-03-06 17:40:26,796 - INFO - training batch 1151, loss: 2.142, 36832/60000 datapoints
2025-03-06 17:40:27,042 - INFO - training batch 1201, loss: 2.140, 38432/60000 datapoints
2025-03-06 17:40:27,328 - INFO - training batch 1251, loss: 2.160, 40032/60000 datapoints
2025-03-06 17:40:27,613 - INFO - training batch 1301, loss: 2.189, 41632/60000 datapoints
2025-03-06 17:40:27,859 - INFO - training batch 1351, loss: 2.131, 43232/60000 datapoints
2025-03-06 17:40:28,120 - INFO - training batch 1401, loss: 2.162, 44832/60000 datapoints
2025-03-06 17:40:28,352 - INFO - training batch 1451, loss: 2.149, 46432/60000 datapoints
2025-03-06 17:40:28,592 - INFO - training batch 1501, loss: 2.152, 48032/60000 datapoints
2025-03-06 17:40:28,827 - INFO - training batch 1551, loss: 2.126, 49632/60000 datapoints
2025-03-06 17:40:29,058 - INFO - training batch 1601, loss: 2.154, 51232/60000 datapoints
2025-03-06 17:40:29,297 - INFO - training batch 1651, loss: 2.142, 52832/60000 datapoints
2025-03-06 17:40:29,515 - INFO - training batch 1701, loss: 2.117, 54432/60000 datapoints
2025-03-06 17:40:29,728 - INFO - training batch 1751, loss: 2.168, 56032/60000 datapoints
2025-03-06 17:40:29,937 - INFO - training batch 1801, loss: 2.165, 57632/60000 datapoints
2025-03-06 17:40:30,146 - INFO - training batch 1851, loss: 2.097, 59232/60000 datapoints
2025-03-06 17:40:30,254 - INFO - validation batch 1, loss: 2.072, 32/10016 datapoints
2025-03-06 17:40:30,418 - INFO - validation batch 51, loss: 2.137, 1632/10016 datapoints
2025-03-06 17:40:30,589 - INFO - validation batch 101, loss: 2.119, 3232/10016 datapoints
2025-03-06 17:40:30,756 - INFO - validation batch 151, loss: 2.152, 4832/10016 datapoints
2025-03-06 17:40:30,918 - INFO - validation batch 201, loss: 2.150, 6432/10016 datapoints
2025-03-06 17:40:31,073 - INFO - validation batch 251, loss: 2.134, 8032/10016 datapoints
2025-03-06 17:40:31,233 - INFO - validation batch 301, loss: 2.122, 9632/10016 datapoints
2025-03-06 17:40:31,272 - INFO - train_topk_accs={1: 0.4643833333333333, 5: 0.8441833333333333}
2025-03-06 17:40:31,272 - INFO - Epoch 8/800 done.
2025-03-06 17:40:31,272 - INFO - Final validation performance:
Loss: 2.127, top-1 acc: 0.487top-5 acc: 0.487
2025-03-06 17:40:31,272 - INFO - Beginning epoch 9/800
2025-03-06 17:40:31,278 - INFO - training batch 1, loss: 2.115, 32/60000 datapoints
2025-03-06 17:40:31,482 - INFO - training batch 51, loss: 2.149, 1632/60000 datapoints
2025-03-06 17:40:31,691 - INFO - training batch 101, loss: 2.115, 3232/60000 datapoints
2025-03-06 17:40:31,907 - INFO - training batch 151, loss: 2.118, 4832/60000 datapoints
2025-03-06 17:40:32,113 - INFO - training batch 201, loss: 2.135, 6432/60000 datapoints
2025-03-06 17:40:32,317 - INFO - training batch 251, loss: 2.157, 8032/60000 datapoints
2025-03-06 17:40:32,527 - INFO - training batch 301, loss: 2.143, 9632/60000 datapoints
2025-03-06 17:40:32,740 - INFO - training batch 351, loss: 2.104, 11232/60000 datapoints
2025-03-06 17:40:32,970 - INFO - training batch 401, loss: 2.101, 12832/60000 datapoints
2025-03-06 17:40:33,184 - INFO - training batch 451, loss: 2.134, 14432/60000 datapoints
2025-03-06 17:40:33,396 - INFO - training batch 501, loss: 2.139, 16032/60000 datapoints
2025-03-06 17:40:33,609 - INFO - training batch 551, loss: 2.147, 17632/60000 datapoints
2025-03-06 17:40:33,812 - INFO - training batch 601, loss: 2.131, 19232/60000 datapoints
2025-03-06 17:40:34,014 - INFO - training batch 651, loss: 2.124, 20832/60000 datapoints
2025-03-06 17:40:34,246 - INFO - training batch 701, loss: 2.112, 22432/60000 datapoints
2025-03-06 17:40:34,507 - INFO - training batch 751, loss: 2.127, 24032/60000 datapoints
2025-03-06 17:40:34,752 - INFO - training batch 801, loss: 2.144, 25632/60000 datapoints
2025-03-06 17:40:34,990 - INFO - training batch 851, loss: 2.134, 27232/60000 datapoints
2025-03-06 17:40:35,213 - INFO - training batch 901, loss: 2.180, 28832/60000 datapoints
2025-03-06 17:40:35,434 - INFO - training batch 951, loss: 2.105, 30432/60000 datapoints
2025-03-06 17:40:35,651 - INFO - training batch 1001, loss: 2.136, 32032/60000 datapoints
2025-03-06 17:40:35,898 - INFO - training batch 1051, loss: 2.107, 33632/60000 datapoints
2025-03-06 17:40:36,146 - INFO - training batch 1101, loss: 2.134, 35232/60000 datapoints
2025-03-06 17:40:36,362 - INFO - training batch 1151, loss: 2.077, 36832/60000 datapoints
2025-03-06 17:40:36,569 - INFO - training batch 1201, loss: 2.087, 38432/60000 datapoints
2025-03-06 17:40:36,811 - INFO - training batch 1251, loss: 2.142, 40032/60000 datapoints
2025-03-06 17:40:37,065 - INFO - training batch 1301, loss: 2.068, 41632/60000 datapoints
2025-03-06 17:40:37,282 - INFO - training batch 1351, loss: 2.122, 43232/60000 datapoints
2025-03-06 17:40:37,567 - INFO - training batch 1401, loss: 2.155, 44832/60000 datapoints
2025-03-06 17:40:37,875 - INFO - training batch 1451, loss: 2.095, 46432/60000 datapoints
2025-03-06 17:40:38,172 - INFO - training batch 1501, loss: 2.104, 48032/60000 datapoints
2025-03-06 17:40:38,403 - INFO - training batch 1551, loss: 2.137, 49632/60000 datapoints
2025-03-06 17:40:38,673 - INFO - training batch 1601, loss: 2.117, 51232/60000 datapoints
2025-03-06 17:40:38,920 - INFO - training batch 1651, loss: 2.133, 52832/60000 datapoints
2025-03-06 17:40:39,151 - INFO - training batch 1701, loss: 2.098, 54432/60000 datapoints
2025-03-06 17:40:39,441 - INFO - training batch 1751, loss: 2.115, 56032/60000 datapoints
2025-03-06 17:40:39,975 - INFO - training batch 1801, loss: 2.117, 57632/60000 datapoints
2025-03-06 17:40:40,280 - INFO - training batch 1851, loss: 2.064, 59232/60000 datapoints
2025-03-06 17:40:40,410 - INFO - validation batch 1, loss: 2.112, 32/10016 datapoints
2025-03-06 17:40:40,615 - INFO - validation batch 51, loss: 2.055, 1632/10016 datapoints
2025-03-06 17:40:40,823 - INFO - validation batch 101, loss: 2.091, 3232/10016 datapoints
2025-03-06 17:40:41,059 - INFO - validation batch 151, loss: 2.087, 4832/10016 datapoints
2025-03-06 17:40:41,261 - INFO - validation batch 201, loss: 2.123, 6432/10016 datapoints
2025-03-06 17:40:41,442 - INFO - validation batch 251, loss: 2.075, 8032/10016 datapoints
2025-03-06 17:40:41,647 - INFO - validation batch 301, loss: 2.118, 9632/10016 datapoints
2025-03-06 17:40:41,693 - INFO - train_topk_accs={1: 0.49535, 5: 0.8725166666666667}
2025-03-06 17:40:41,693 - INFO - Epoch 9/800 done.
2025-03-06 17:40:41,693 - INFO - Final validation performance:
Loss: 2.094, top-1 acc: 0.519top-5 acc: 0.519
2025-03-06 17:40:41,694 - INFO - Beginning epoch 10/800
2025-03-06 17:40:41,700 - INFO - training batch 1, loss: 2.103, 32/60000 datapoints
2025-03-06 17:40:41,927 - INFO - training batch 51, loss: 2.131, 1632/60000 datapoints
2025-03-06 17:40:42,133 - INFO - training batch 101, loss: 2.077, 3232/60000 datapoints
2025-03-06 17:40:42,342 - INFO - training batch 151, loss: 2.070, 4832/60000 datapoints
2025-03-06 17:40:42,588 - INFO - training batch 201, loss: 2.116, 6432/60000 datapoints
2025-03-06 17:40:42,877 - INFO - training batch 251, loss: 2.118, 8032/60000 datapoints
2025-03-06 17:40:43,112 - INFO - training batch 301, loss: 2.131, 9632/60000 datapoints
2025-03-06 17:40:43,366 - INFO - training batch 351, loss: 2.101, 11232/60000 datapoints
2025-03-06 17:40:43,635 - INFO - training batch 401, loss: 2.094, 12832/60000 datapoints
2025-03-06 17:40:43,867 - INFO - training batch 451, loss: 2.061, 14432/60000 datapoints
2025-03-06 17:40:44,105 - INFO - training batch 501, loss: 2.119, 16032/60000 datapoints
2025-03-06 17:40:44,320 - INFO - training batch 551, loss: 2.107, 17632/60000 datapoints
2025-03-06 17:40:44,569 - INFO - training batch 601, loss: 2.136, 19232/60000 datapoints
2025-03-06 17:40:44,857 - INFO - training batch 651, loss: 2.103, 20832/60000 datapoints
2025-03-06 17:40:45,123 - INFO - training batch 701, loss: 2.095, 22432/60000 datapoints
2025-03-06 17:40:45,402 - INFO - training batch 751, loss: 2.124, 24032/60000 datapoints
2025-03-06 17:40:45,745 - INFO - training batch 801, loss: 2.040, 25632/60000 datapoints
2025-03-06 17:40:46,046 - INFO - training batch 851, loss: 2.096, 27232/60000 datapoints
2025-03-06 17:40:46,340 - INFO - training batch 901, loss: 2.099, 28832/60000 datapoints
2025-03-06 17:40:46,609 - INFO - training batch 951, loss: 2.085, 30432/60000 datapoints
2025-03-06 17:40:46,931 - INFO - training batch 1001, loss: 2.105, 32032/60000 datapoints
2025-03-06 17:40:47,169 - INFO - training batch 1051, loss: 2.086, 33632/60000 datapoints
2025-03-06 17:40:47,445 - INFO - training batch 1101, loss: 2.107, 35232/60000 datapoints
2025-03-06 17:40:47,688 - INFO - training batch 1151, loss: 2.116, 36832/60000 datapoints
2025-03-06 17:40:47,929 - INFO - training batch 1201, loss: 2.082, 38432/60000 datapoints
2025-03-06 17:40:48,142 - INFO - training batch 1251, loss: 2.113, 40032/60000 datapoints
2025-03-06 17:40:48,356 - INFO - training batch 1301, loss: 2.074, 41632/60000 datapoints
2025-03-06 17:40:48,596 - INFO - training batch 1351, loss: 2.084, 43232/60000 datapoints
2025-03-06 17:40:48,823 - INFO - training batch 1401, loss: 2.078, 44832/60000 datapoints
2025-03-06 17:40:49,085 - INFO - training batch 1451, loss: 2.075, 46432/60000 datapoints
2025-03-06 17:40:49,377 - INFO - training batch 1501, loss: 2.132, 48032/60000 datapoints
2025-03-06 17:40:49,625 - INFO - training batch 1551, loss: 2.068, 49632/60000 datapoints
2025-03-06 17:40:49,858 - INFO - training batch 1601, loss: 2.091, 51232/60000 datapoints
2025-03-06 17:40:50,080 - INFO - training batch 1651, loss: 2.072, 52832/60000 datapoints
2025-03-06 17:40:50,293 - INFO - training batch 1701, loss: 2.067, 54432/60000 datapoints
2025-03-06 17:40:50,538 - INFO - training batch 1751, loss: 2.078, 56032/60000 datapoints
2025-03-06 17:40:50,749 - INFO - training batch 1801, loss: 2.098, 57632/60000 datapoints
2025-03-06 17:40:50,976 - INFO - training batch 1851, loss: 2.066, 59232/60000 datapoints
2025-03-06 17:40:51,086 - INFO - validation batch 1, loss: 2.061, 32/10016 datapoints
2025-03-06 17:40:51,251 - INFO - validation batch 51, loss: 2.037, 1632/10016 datapoints
2025-03-06 17:40:51,446 - INFO - validation batch 101, loss: 2.070, 3232/10016 datapoints
2025-03-06 17:40:51,626 - INFO - validation batch 151, loss: 2.061, 4832/10016 datapoints
2025-03-06 17:40:51,796 - INFO - validation batch 201, loss: 2.060, 6432/10016 datapoints
2025-03-06 17:40:51,959 - INFO - validation batch 251, loss: 2.083, 8032/10016 datapoints
2025-03-06 17:40:52,127 - INFO - validation batch 301, loss: 2.081, 9632/10016 datapoints
2025-03-06 17:40:52,166 - INFO - train_topk_accs={1: 0.5278166666666667, 5: 0.9028}
2025-03-06 17:40:52,166 - INFO - Epoch 10/800 done.
2025-03-06 17:40:52,167 - INFO - Final validation performance:
Loss: 2.065, top-1 acc: 0.547top-5 acc: 0.547
2025-03-06 17:40:52,167 - INFO - Beginning epoch 11/800
2025-03-06 17:40:52,173 - INFO - training batch 1, loss: 2.057, 32/60000 datapoints
2025-03-06 17:40:52,378 - INFO - training batch 51, loss: 2.076, 1632/60000 datapoints
2025-03-06 17:40:52,577 - INFO - training batch 101, loss: 2.059, 3232/60000 datapoints
2025-03-06 17:40:52,784 - INFO - training batch 151, loss: 2.089, 4832/60000 datapoints
2025-03-06 17:40:52,989 - INFO - training batch 201, loss: 2.095, 6432/60000 datapoints
2025-03-06 17:40:53,195 - INFO - training batch 251, loss: 2.061, 8032/60000 datapoints
2025-03-06 17:40:53,398 - INFO - training batch 301, loss: 2.112, 9632/60000 datapoints
2025-03-06 17:40:53,605 - INFO - training batch 351, loss: 2.083, 11232/60000 datapoints
2025-03-06 17:40:53,808 - INFO - training batch 401, loss: 2.062, 12832/60000 datapoints
2025-03-06 17:40:54,014 - INFO - training batch 451, loss: 2.082, 14432/60000 datapoints
2025-03-06 17:40:54,216 - INFO - training batch 501, loss: 2.110, 16032/60000 datapoints
2025-03-06 17:40:54,478 - INFO - training batch 551, loss: 2.093, 17632/60000 datapoints
2025-03-06 17:40:54,680 - INFO - training batch 601, loss: 2.088, 19232/60000 datapoints
2025-03-06 17:40:54,885 - INFO - training batch 651, loss: 2.045, 20832/60000 datapoints
2025-03-06 17:40:55,087 - INFO - training batch 701, loss: 2.056, 22432/60000 datapoints
2025-03-06 17:40:55,282 - INFO - training batch 751, loss: 2.015, 24032/60000 datapoints
2025-03-06 17:40:55,488 - INFO - training batch 801, loss: 2.075, 25632/60000 datapoints
2025-03-06 17:40:55,706 - INFO - training batch 851, loss: 2.074, 27232/60000 datapoints
2025-03-06 17:40:55,907 - INFO - training batch 901, loss: 2.080, 28832/60000 datapoints
2025-03-06 17:40:56,107 - INFO - training batch 951, loss: 2.073, 30432/60000 datapoints
2025-03-06 17:40:56,314 - INFO - training batch 1001, loss: 2.069, 32032/60000 datapoints
2025-03-06 17:40:56,512 - INFO - training batch 1051, loss: 2.092, 33632/60000 datapoints
2025-03-06 17:40:56,719 - INFO - training batch 1101, loss: 2.048, 35232/60000 datapoints
2025-03-06 17:40:56,932 - INFO - training batch 1151, loss: 2.044, 36832/60000 datapoints
2025-03-06 17:40:57,145 - INFO - training batch 1201, loss: 2.094, 38432/60000 datapoints
2025-03-06 17:40:57,344 - INFO - training batch 1251, loss: 2.036, 40032/60000 datapoints
2025-03-06 17:40:57,541 - INFO - training batch 1301, loss: 2.028, 41632/60000 datapoints
2025-03-06 17:40:57,739 - INFO - training batch 1351, loss: 2.040, 43232/60000 datapoints
2025-03-06 17:40:57,940 - INFO - training batch 1401, loss: 2.029, 44832/60000 datapoints
2025-03-06 17:40:58,138 - INFO - training batch 1451, loss: 2.032, 46432/60000 datapoints
2025-03-06 17:40:58,333 - INFO - training batch 1501, loss: 2.031, 48032/60000 datapoints
2025-03-06 17:40:58,535 - INFO - training batch 1551, loss: 2.039, 49632/60000 datapoints
2025-03-06 17:40:58,740 - INFO - training batch 1601, loss: 2.010, 51232/60000 datapoints
2025-03-06 17:40:58,946 - INFO - training batch 1651, loss: 2.072, 52832/60000 datapoints
2025-03-06 17:40:59,146 - INFO - training batch 1701, loss: 2.046, 54432/60000 datapoints
2025-03-06 17:40:59,371 - INFO - training batch 1751, loss: 2.012, 56032/60000 datapoints
2025-03-06 17:40:59,612 - INFO - training batch 1801, loss: 2.042, 57632/60000 datapoints
2025-03-06 17:40:59,836 - INFO - training batch 1851, loss: 1.983, 59232/60000 datapoints
2025-03-06 17:40:59,951 - INFO - validation batch 1, loss: 2.024, 32/10016 datapoints
2025-03-06 17:41:00,145 - INFO - validation batch 51, loss: 2.046, 1632/10016 datapoints
2025-03-06 17:41:00,320 - INFO - validation batch 101, loss: 1.979, 3232/10016 datapoints
2025-03-06 17:41:00,504 - INFO - validation batch 151, loss: 2.048, 4832/10016 datapoints
2025-03-06 17:41:00,676 - INFO - validation batch 201, loss: 2.054, 6432/10016 datapoints
2025-03-06 17:41:00,856 - INFO - validation batch 251, loss: 2.039, 8032/10016 datapoints
2025-03-06 17:41:01,030 - INFO - validation batch 301, loss: 2.080, 9632/10016 datapoints
2025-03-06 17:41:01,072 - INFO - train_topk_accs={1: 0.5581166666666667, 5: 0.9286}
2025-03-06 17:41:01,072 - INFO - Epoch 11/800 done.
2025-03-06 17:41:01,072 - INFO - Final validation performance:
Loss: 2.039, top-1 acc: 0.575top-5 acc: 0.575
2025-03-06 17:41:01,072 - INFO - Beginning epoch 12/800
2025-03-06 17:41:01,078 - INFO - training batch 1, loss: 2.083, 32/60000 datapoints
2025-03-06 17:41:01,304 - INFO - training batch 51, loss: 2.059, 1632/60000 datapoints
2025-03-06 17:41:01,553 - INFO - training batch 101, loss: 2.056, 3232/60000 datapoints
2025-03-06 17:41:01,811 - INFO - training batch 151, loss: 2.033, 4832/60000 datapoints
2025-03-06 17:41:02,080 - INFO - training batch 201, loss: 2.088, 6432/60000 datapoints
2025-03-06 17:41:02,346 - INFO - training batch 251, loss: 2.057, 8032/60000 datapoints
2025-03-06 17:41:02,553 - INFO - training batch 301, loss: 2.014, 9632/60000 datapoints
2025-03-06 17:41:02,772 - INFO - training batch 351, loss: 2.004, 11232/60000 datapoints
2025-03-06 17:41:03,012 - INFO - training batch 401, loss: 2.025, 12832/60000 datapoints
2025-03-06 17:41:03,221 - INFO - training batch 451, loss: 2.011, 14432/60000 datapoints
2025-03-06 17:41:03,450 - INFO - training batch 501, loss: 2.034, 16032/60000 datapoints
2025-03-06 17:41:03,683 - INFO - training batch 551, loss: 2.001, 17632/60000 datapoints
2025-03-06 17:41:03,917 - INFO - training batch 601, loss: 2.026, 19232/60000 datapoints
2025-03-06 17:41:04,128 - INFO - training batch 651, loss: 1.976, 20832/60000 datapoints
2025-03-06 17:41:04,365 - INFO - training batch 701, loss: 2.013, 22432/60000 datapoints
2025-03-06 17:41:04,659 - INFO - training batch 751, loss: 2.034, 24032/60000 datapoints
2025-03-06 17:41:04,911 - INFO - training batch 801, loss: 2.049, 25632/60000 datapoints
2025-03-06 17:41:05,164 - INFO - training batch 851, loss: 2.034, 27232/60000 datapoints
2025-03-06 17:41:05,395 - INFO - training batch 901, loss: 1.964, 28832/60000 datapoints
2025-03-06 17:41:05,702 - INFO - training batch 951, loss: 1.992, 30432/60000 datapoints
2025-03-06 17:41:06,133 - INFO - training batch 1001, loss: 2.026, 32032/60000 datapoints
2025-03-06 17:41:06,422 - INFO - training batch 1051, loss: 2.035, 33632/60000 datapoints
2025-03-06 17:41:06,720 - INFO - training batch 1101, loss: 2.062, 35232/60000 datapoints
2025-03-06 17:41:07,009 - INFO - training batch 1151, loss: 2.018, 36832/60000 datapoints
2025-03-06 17:41:07,321 - INFO - training batch 1201, loss: 2.013, 38432/60000 datapoints
2025-03-06 17:41:07,583 - INFO - training batch 1251, loss: 2.041, 40032/60000 datapoints
2025-03-06 17:41:07,849 - INFO - training batch 1301, loss: 2.030, 41632/60000 datapoints
2025-03-06 17:41:08,126 - INFO - training batch 1351, loss: 2.032, 43232/60000 datapoints
2025-03-06 17:41:08,366 - INFO - training batch 1401, loss: 2.039, 44832/60000 datapoints
2025-03-06 17:41:08,598 - INFO - training batch 1451, loss: 2.019, 46432/60000 datapoints
2025-03-06 17:41:08,844 - INFO - training batch 1501, loss: 2.010, 48032/60000 datapoints
2025-03-06 17:41:09,069 - INFO - training batch 1551, loss: 2.034, 49632/60000 datapoints
2025-03-06 17:41:09,288 - INFO - training batch 1601, loss: 2.062, 51232/60000 datapoints
2025-03-06 17:41:09,501 - INFO - training batch 1651, loss: 1.986, 52832/60000 datapoints
2025-03-06 17:41:09,708 - INFO - training batch 1701, loss: 2.029, 54432/60000 datapoints
2025-03-06 17:41:09,914 - INFO - training batch 1751, loss: 2.020, 56032/60000 datapoints
2025-03-06 17:41:10,115 - INFO - training batch 1801, loss: 1.962, 57632/60000 datapoints
2025-03-06 17:41:10,315 - INFO - training batch 1851, loss: 2.023, 59232/60000 datapoints
2025-03-06 17:41:10,417 - INFO - validation batch 1, loss: 1.935, 32/10016 datapoints
2025-03-06 17:41:10,574 - INFO - validation batch 51, loss: 1.984, 1632/10016 datapoints
2025-03-06 17:41:10,734 - INFO - validation batch 101, loss: 1.996, 3232/10016 datapoints
2025-03-06 17:41:10,896 - INFO - validation batch 151, loss: 2.017, 4832/10016 datapoints
2025-03-06 17:41:11,055 - INFO - validation batch 201, loss: 1.999, 6432/10016 datapoints
2025-03-06 17:41:11,211 - INFO - validation batch 251, loss: 1.991, 8032/10016 datapoints
2025-03-06 17:41:11,368 - INFO - validation batch 301, loss: 1.995, 9632/10016 datapoints
2025-03-06 17:41:11,406 - INFO - train_topk_accs={1: 0.5841666666666666, 5: 0.9460833333333334}
2025-03-06 17:41:11,406 - INFO - Epoch 12/800 done.
2025-03-06 17:41:11,406 - INFO - Final validation performance:
Loss: 1.988, top-1 acc: 0.599top-5 acc: 0.599
2025-03-06 17:41:11,407 - INFO - Beginning epoch 13/800
2025-03-06 17:41:11,414 - INFO - training batch 1, loss: 1.939, 32/60000 datapoints
2025-03-06 17:41:11,618 - INFO - training batch 51, loss: 1.988, 1632/60000 datapoints
2025-03-06 17:41:11,829 - INFO - training batch 101, loss: 1.979, 3232/60000 datapoints
2025-03-06 17:41:12,061 - INFO - training batch 151, loss: 1.944, 4832/60000 datapoints
2025-03-06 17:41:12,263 - INFO - training batch 201, loss: 2.011, 6432/60000 datapoints
2025-03-06 17:41:12,463 - INFO - training batch 251, loss: 2.031, 8032/60000 datapoints
2025-03-06 17:41:12,667 - INFO - training batch 301, loss: 1.994, 9632/60000 datapoints
2025-03-06 17:41:12,866 - INFO - training batch 351, loss: 2.032, 11232/60000 datapoints
2025-03-06 17:41:13,083 - INFO - training batch 401, loss: 2.027, 12832/60000 datapoints
2025-03-06 17:41:13,284 - INFO - training batch 451, loss: 2.006, 14432/60000 datapoints
2025-03-06 17:41:13,512 - INFO - training batch 501, loss: 2.008, 16032/60000 datapoints
2025-03-06 17:41:13,721 - INFO - training batch 551, loss: 2.029, 17632/60000 datapoints
2025-03-06 17:41:13,920 - INFO - training batch 601, loss: 2.008, 19232/60000 datapoints
2025-03-06 17:41:14,140 - INFO - training batch 651, loss: 1.933, 20832/60000 datapoints
2025-03-06 17:41:14,341 - INFO - training batch 701, loss: 1.985, 22432/60000 datapoints
2025-03-06 17:41:14,555 - INFO - training batch 751, loss: 1.980, 24032/60000 datapoints
2025-03-06 17:41:14,759 - INFO - training batch 801, loss: 1.962, 25632/60000 datapoints
2025-03-06 17:41:14,974 - INFO - training batch 851, loss: 2.029, 27232/60000 datapoints
2025-03-06 17:41:15,179 - INFO - training batch 901, loss: 1.911, 28832/60000 datapoints
2025-03-06 17:41:15,392 - INFO - training batch 951, loss: 2.038, 30432/60000 datapoints
2025-03-06 17:41:15,599 - INFO - training batch 1001, loss: 1.945, 32032/60000 datapoints
2025-03-06 17:41:15,799 - INFO - training batch 1051, loss: 1.989, 33632/60000 datapoints
2025-03-06 17:41:16,008 - INFO - training batch 1101, loss: 2.025, 35232/60000 datapoints
2025-03-06 17:41:16,218 - INFO - training batch 1151, loss: 2.006, 36832/60000 datapoints
2025-03-06 17:41:16,420 - INFO - training batch 1201, loss: 1.982, 38432/60000 datapoints
2025-03-06 17:41:16,626 - INFO - training batch 1251, loss: 1.939, 40032/60000 datapoints
2025-03-06 17:41:16,830 - INFO - training batch 1301, loss: 1.961, 41632/60000 datapoints
2025-03-06 17:41:17,086 - INFO - training batch 1351, loss: 2.026, 43232/60000 datapoints
2025-03-06 17:41:17,367 - INFO - training batch 1401, loss: 1.974, 44832/60000 datapoints
2025-03-06 17:41:17,584 - INFO - training batch 1451, loss: 1.980, 46432/60000 datapoints
2025-03-06 17:41:17,806 - INFO - training batch 1501, loss: 2.024, 48032/60000 datapoints
2025-03-06 17:41:18,103 - INFO - training batch 1551, loss: 1.974, 49632/60000 datapoints
2025-03-06 17:41:18,345 - INFO - training batch 1601, loss: 2.058, 51232/60000 datapoints
2025-03-06 17:41:18,645 - INFO - training batch 1651, loss: 1.948, 52832/60000 datapoints
2025-03-06 17:41:18,870 - INFO - training batch 1701, loss: 1.992, 54432/60000 datapoints
2025-03-06 17:41:19,082 - INFO - training batch 1751, loss: 2.027, 56032/60000 datapoints
2025-03-06 17:41:19,330 - INFO - training batch 1801, loss: 1.961, 57632/60000 datapoints
2025-03-06 17:41:19,577 - INFO - training batch 1851, loss: 1.984, 59232/60000 datapoints
2025-03-06 17:41:19,704 - INFO - validation batch 1, loss: 1.947, 32/10016 datapoints
2025-03-06 17:41:19,884 - INFO - validation batch 51, loss: 1.989, 1632/10016 datapoints
2025-03-06 17:41:20,059 - INFO - validation batch 101, loss: 2.010, 3232/10016 datapoints
2025-03-06 17:41:20,244 - INFO - validation batch 151, loss: 2.007, 4832/10016 datapoints
2025-03-06 17:41:20,442 - INFO - validation batch 201, loss: 2.027, 6432/10016 datapoints
2025-03-06 17:41:20,657 - INFO - validation batch 251, loss: 1.941, 8032/10016 datapoints
2025-03-06 17:41:20,868 - INFO - validation batch 301, loss: 1.927, 9632/10016 datapoints
2025-03-06 17:41:20,908 - INFO - train_topk_accs={1: 0.6064, 5: 0.9578666666666666}
2025-03-06 17:41:20,909 - INFO - Epoch 13/800 done.
2025-03-06 17:41:20,909 - INFO - Final validation performance:
Loss: 1.978, top-1 acc: 0.617top-5 acc: 0.617
2025-03-06 17:41:20,909 - INFO - Beginning epoch 14/800
2025-03-06 17:41:20,917 - INFO - training batch 1, loss: 1.950, 32/60000 datapoints
2025-03-06 17:41:21,268 - INFO - training batch 51, loss: 1.969, 1632/60000 datapoints
2025-03-06 17:41:21,573 - INFO - training batch 101, loss: 1.958, 3232/60000 datapoints
2025-03-06 17:41:21,954 - INFO - training batch 151, loss: 1.933, 4832/60000 datapoints
2025-03-06 17:41:22,435 - INFO - training batch 201, loss: 1.961, 6432/60000 datapoints
2025-03-06 17:41:23,354 - INFO - training batch 251, loss: 1.953, 8032/60000 datapoints
2025-03-06 17:41:23,691 - INFO - training batch 301, loss: 1.974, 9632/60000 datapoints
2025-03-06 17:41:24,044 - INFO - training batch 351, loss: 1.895, 11232/60000 datapoints
2025-03-06 17:41:24,363 - INFO - training batch 401, loss: 1.900, 12832/60000 datapoints
2025-03-06 17:41:24,706 - INFO - training batch 451, loss: 1.944, 14432/60000 datapoints
2025-03-06 17:41:25,038 - INFO - training batch 501, loss: 1.976, 16032/60000 datapoints
2025-03-06 17:41:25,361 - INFO - training batch 551, loss: 1.977, 17632/60000 datapoints
2025-03-06 17:41:25,634 - INFO - training batch 601, loss: 1.923, 19232/60000 datapoints
2025-03-06 17:41:25,944 - INFO - training batch 651, loss: 1.961, 20832/60000 datapoints
2025-03-06 17:41:26,235 - INFO - training batch 701, loss: 1.946, 22432/60000 datapoints
2025-03-06 17:41:26,466 - INFO - training batch 751, loss: 1.945, 24032/60000 datapoints
2025-03-06 17:41:26,684 - INFO - training batch 801, loss: 1.905, 25632/60000 datapoints
2025-03-06 17:41:26,903 - INFO - training batch 851, loss: 1.958, 27232/60000 datapoints
2025-03-06 17:41:27,114 - INFO - training batch 901, loss: 1.910, 28832/60000 datapoints
2025-03-06 17:41:27,342 - INFO - training batch 951, loss: 1.949, 30432/60000 datapoints
2025-03-06 17:41:27,565 - INFO - training batch 1001, loss: 1.952, 32032/60000 datapoints
2025-03-06 17:41:27,829 - INFO - training batch 1051, loss: 1.917, 33632/60000 datapoints
2025-03-06 17:41:28,050 - INFO - training batch 1101, loss: 1.952, 35232/60000 datapoints
2025-03-06 17:41:28,329 - INFO - training batch 1151, loss: 1.963, 36832/60000 datapoints
2025-03-06 17:41:28,556 - INFO - training batch 1201, loss: 1.989, 38432/60000 datapoints
2025-03-06 17:41:28,808 - INFO - training batch 1251, loss: 1.935, 40032/60000 datapoints
2025-03-06 17:41:29,051 - INFO - training batch 1301, loss: 1.936, 41632/60000 datapoints
2025-03-06 17:41:29,292 - INFO - training batch 1351, loss: 1.885, 43232/60000 datapoints
2025-03-06 17:41:29,531 - INFO - training batch 1401, loss: 1.879, 44832/60000 datapoints
2025-03-06 17:41:29,776 - INFO - training batch 1451, loss: 2.000, 46432/60000 datapoints
2025-03-06 17:41:30,015 - INFO - training batch 1501, loss: 1.977, 48032/60000 datapoints
2025-03-06 17:41:30,246 - INFO - training batch 1551, loss: 1.922, 49632/60000 datapoints
2025-03-06 17:41:30,489 - INFO - training batch 1601, loss: 1.967, 51232/60000 datapoints
2025-03-06 17:41:30,721 - INFO - training batch 1651, loss: 1.910, 52832/60000 datapoints
2025-03-06 17:41:30,967 - INFO - training batch 1701, loss: 1.884, 54432/60000 datapoints
2025-03-06 17:41:31,218 - INFO - training batch 1751, loss: 1.915, 56032/60000 datapoints
2025-03-06 17:41:31,471 - INFO - training batch 1801, loss: 1.904, 57632/60000 datapoints
2025-03-06 17:41:31,726 - INFO - training batch 1851, loss: 1.903, 59232/60000 datapoints
2025-03-06 17:41:31,861 - INFO - validation batch 1, loss: 1.942, 32/10016 datapoints
2025-03-06 17:41:32,084 - INFO - validation batch 51, loss: 1.899, 1632/10016 datapoints
2025-03-06 17:41:32,272 - INFO - validation batch 101, loss: 1.983, 3232/10016 datapoints
2025-03-06 17:41:32,457 - INFO - validation batch 151, loss: 1.918, 4832/10016 datapoints
2025-03-06 17:41:32,664 - INFO - validation batch 201, loss: 1.981, 6432/10016 datapoints
2025-03-06 17:41:32,859 - INFO - validation batch 251, loss: 1.934, 8032/10016 datapoints
2025-03-06 17:41:33,220 - INFO - validation batch 301, loss: 1.941, 9632/10016 datapoints
2025-03-06 17:41:33,320 - INFO - train_topk_accs={1: 0.6240833333333333, 5: 0.9649}
2025-03-06 17:41:33,321 - INFO - Epoch 14/800 done.
2025-03-06 17:41:33,322 - INFO - Final validation performance:
Loss: 1.943, top-1 acc: 0.631top-5 acc: 0.631
2025-03-06 17:41:33,322 - INFO - Beginning epoch 15/800
2025-03-06 17:41:33,338 - INFO - training batch 1, loss: 1.961, 32/60000 datapoints
2025-03-06 17:41:33,827 - INFO - training batch 51, loss: 1.957, 1632/60000 datapoints
2025-03-06 17:41:34,322 - INFO - training batch 101, loss: 1.946, 3232/60000 datapoints
2025-03-06 17:41:35,400 - INFO - training batch 151, loss: 1.894, 4832/60000 datapoints
2025-03-06 17:41:35,891 - INFO - training batch 201, loss: 1.952, 6432/60000 datapoints
2025-03-06 17:41:36,302 - INFO - training batch 251, loss: 1.879, 8032/60000 datapoints
2025-03-06 17:41:36,732 - INFO - training batch 301, loss: 1.951, 9632/60000 datapoints
2025-03-06 17:41:37,227 - INFO - training batch 351, loss: 1.887, 11232/60000 datapoints
2025-03-06 17:41:37,672 - INFO - training batch 401, loss: 1.875, 12832/60000 datapoints
2025-03-06 17:41:38,086 - INFO - training batch 451, loss: 1.921, 14432/60000 datapoints
2025-03-06 17:41:38,426 - INFO - training batch 501, loss: 1.875, 16032/60000 datapoints
2025-03-06 17:41:38,744 - INFO - training batch 551, loss: 1.931, 17632/60000 datapoints
2025-03-06 17:41:39,086 - INFO - training batch 601, loss: 1.915, 19232/60000 datapoints
2025-03-06 17:41:39,388 - INFO - training batch 651, loss: 1.964, 20832/60000 datapoints
2025-03-06 17:41:39,683 - INFO - training batch 701, loss: 1.950, 22432/60000 datapoints
2025-03-06 17:41:39,994 - INFO - training batch 751, loss: 1.938, 24032/60000 datapoints
2025-03-06 17:41:40,271 - INFO - training batch 801, loss: 1.941, 25632/60000 datapoints
2025-03-06 17:41:40,502 - INFO - training batch 851, loss: 1.890, 27232/60000 datapoints
2025-03-06 17:41:40,754 - INFO - training batch 901, loss: 1.944, 28832/60000 datapoints
2025-03-06 17:41:40,999 - INFO - training batch 951, loss: 1.926, 30432/60000 datapoints
2025-03-06 17:41:41,231 - INFO - training batch 1001, loss: 1.933, 32032/60000 datapoints
2025-03-06 17:41:41,458 - INFO - training batch 1051, loss: 1.932, 33632/60000 datapoints
2025-03-06 17:41:41,672 - INFO - training batch 1101, loss: 1.901, 35232/60000 datapoints
2025-03-06 17:41:41,885 - INFO - training batch 1151, loss: 1.908, 36832/60000 datapoints
2025-03-06 17:41:42,150 - INFO - training batch 1201, loss: 1.884, 38432/60000 datapoints
2025-03-06 17:41:42,462 - INFO - training batch 1251, loss: 1.891, 40032/60000 datapoints
2025-03-06 17:41:42,684 - INFO - training batch 1301, loss: 1.877, 41632/60000 datapoints
2025-03-06 17:41:42,925 - INFO - training batch 1351, loss: 1.865, 43232/60000 datapoints
2025-03-06 17:41:43,168 - INFO - training batch 1401, loss: 1.872, 44832/60000 datapoints
2025-03-06 17:41:43,483 - INFO - training batch 1451, loss: 1.951, 46432/60000 datapoints
2025-03-06 17:41:43,749 - INFO - training batch 1501, loss: 1.911, 48032/60000 datapoints
2025-03-06 17:41:44,006 - INFO - training batch 1551, loss: 1.842, 49632/60000 datapoints
2025-03-06 17:41:44,252 - INFO - training batch 1601, loss: 1.906, 51232/60000 datapoints
2025-03-06 17:41:44,574 - INFO - training batch 1651, loss: 1.974, 52832/60000 datapoints
2025-03-06 17:41:44,814 - INFO - training batch 1701, loss: 1.874, 54432/60000 datapoints
2025-03-06 17:41:45,045 - INFO - training batch 1751, loss: 1.992, 56032/60000 datapoints
2025-03-06 17:41:45,273 - INFO - training batch 1801, loss: 1.861, 57632/60000 datapoints
2025-03-06 17:41:45,552 - INFO - training batch 1851, loss: 1.887, 59232/60000 datapoints
2025-03-06 17:41:45,683 - INFO - validation batch 1, loss: 1.907, 32/10016 datapoints
2025-03-06 17:41:45,879 - INFO - validation batch 51, loss: 1.828, 1632/10016 datapoints
2025-03-06 17:41:46,153 - INFO - validation batch 101, loss: 1.826, 3232/10016 datapoints
2025-03-06 17:41:46,353 - INFO - validation batch 151, loss: 1.786, 4832/10016 datapoints
2025-03-06 17:41:46,539 - INFO - validation batch 201, loss: 1.852, 6432/10016 datapoints
2025-03-06 17:41:46,729 - INFO - validation batch 251, loss: 1.955, 8032/10016 datapoints
2025-03-06 17:41:46,916 - INFO - validation batch 301, loss: 1.809, 9632/10016 datapoints
2025-03-06 17:41:46,960 - INFO - train_topk_accs={1: 0.6387333333333334, 5: 0.9694833333333334}
2025-03-06 17:41:46,960 - INFO - Epoch 15/800 done.
2025-03-06 17:41:46,960 - INFO - Final validation performance:
Loss: 1.852, top-1 acc: 0.646top-5 acc: 0.646
2025-03-06 17:41:46,961 - INFO - Beginning epoch 16/800
2025-03-06 17:41:46,968 - INFO - training batch 1, loss: 1.912, 32/60000 datapoints
2025-03-06 17:41:47,201 - INFO - training batch 51, loss: 1.865, 1632/60000 datapoints
2025-03-06 17:41:47,428 - INFO - training batch 101, loss: 1.950, 3232/60000 datapoints
2025-03-06 17:41:47,663 - INFO - training batch 151, loss: 1.905, 4832/60000 datapoints
2025-03-06 17:41:47,875 - INFO - training batch 201, loss: 1.886, 6432/60000 datapoints
2025-03-06 17:41:48,131 - INFO - training batch 251, loss: 1.913, 8032/60000 datapoints
2025-03-06 17:41:48,411 - INFO - training batch 301, loss: 1.885, 9632/60000 datapoints
2025-03-06 17:41:48,635 - INFO - training batch 351, loss: 1.883, 11232/60000 datapoints
2025-03-06 17:41:48,865 - INFO - training batch 401, loss: 1.942, 12832/60000 datapoints
2025-03-06 17:41:49,128 - INFO - training batch 451, loss: 1.851, 14432/60000 datapoints
2025-03-06 17:41:49,410 - INFO - training batch 501, loss: 1.873, 16032/60000 datapoints
2025-03-06 17:41:49,674 - INFO - training batch 551, loss: 1.931, 17632/60000 datapoints
2025-03-06 17:41:49,919 - INFO - training batch 601, loss: 1.896, 19232/60000 datapoints
2025-03-06 17:41:50,173 - INFO - training batch 651, loss: 1.966, 20832/60000 datapoints
2025-03-06 17:41:50,496 - INFO - training batch 701, loss: 1.945, 22432/60000 datapoints
2025-03-06 17:41:50,724 - INFO - training batch 751, loss: 1.927, 24032/60000 datapoints
2025-03-06 17:41:51,165 - INFO - training batch 801, loss: 1.902, 25632/60000 datapoints
2025-03-06 17:41:51,394 - INFO - training batch 851, loss: 1.887, 27232/60000 datapoints
2025-03-06 17:41:51,631 - INFO - training batch 901, loss: 1.868, 28832/60000 datapoints
2025-03-06 17:41:51,863 - INFO - training batch 951, loss: 1.914, 30432/60000 datapoints
2025-03-06 17:41:52,099 - INFO - training batch 1001, loss: 1.883, 32032/60000 datapoints
2025-03-06 17:41:52,324 - INFO - training batch 1051, loss: 1.829, 33632/60000 datapoints
2025-03-06 17:41:52,540 - INFO - training batch 1101, loss: 1.828, 35232/60000 datapoints
2025-03-06 17:41:52,749 - INFO - training batch 1151, loss: 1.943, 36832/60000 datapoints
2025-03-06 17:41:52,960 - INFO - training batch 1201, loss: 1.839, 38432/60000 datapoints
2025-03-06 17:41:53,209 - INFO - training batch 1251, loss: 1.812, 40032/60000 datapoints
2025-03-06 17:41:53,444 - INFO - training batch 1301, loss: 1.913, 41632/60000 datapoints
2025-03-06 17:41:53,667 - INFO - training batch 1351, loss: 1.883, 43232/60000 datapoints
2025-03-06 17:41:53,903 - INFO - training batch 1401, loss: 1.841, 44832/60000 datapoints
2025-03-06 17:41:54,118 - INFO - training batch 1451, loss: 1.870, 46432/60000 datapoints
2025-03-06 17:41:54,348 - INFO - training batch 1501, loss: 1.834, 48032/60000 datapoints
2025-03-06 17:41:54,566 - INFO - training batch 1551, loss: 1.860, 49632/60000 datapoints
2025-03-06 17:41:54,788 - INFO - training batch 1601, loss: 1.876, 51232/60000 datapoints
2025-03-06 17:41:55,011 - INFO - training batch 1651, loss: 1.859, 52832/60000 datapoints
2025-03-06 17:41:55,248 - INFO - training batch 1701, loss: 1.901, 54432/60000 datapoints
2025-03-06 17:41:55,497 - INFO - training batch 1751, loss: 1.917, 56032/60000 datapoints
2025-03-06 17:41:55,708 - INFO - training batch 1801, loss: 1.936, 57632/60000 datapoints
2025-03-06 17:41:55,937 - INFO - training batch 1851, loss: 1.783, 59232/60000 datapoints
2025-03-06 17:41:56,063 - INFO - validation batch 1, loss: 1.880, 32/10016 datapoints
2025-03-06 17:41:56,360 - INFO - validation batch 51, loss: 1.894, 1632/10016 datapoints
2025-03-06 17:41:56,548 - INFO - validation batch 101, loss: 1.867, 3232/10016 datapoints
2025-03-06 17:41:56,715 - INFO - validation batch 151, loss: 1.914, 4832/10016 datapoints
2025-03-06 17:41:56,875 - INFO - validation batch 201, loss: 1.832, 6432/10016 datapoints
2025-03-06 17:41:57,039 - INFO - validation batch 251, loss: 1.858, 8032/10016 datapoints
2025-03-06 17:41:57,212 - INFO - validation batch 301, loss: 1.868, 9632/10016 datapoints
2025-03-06 17:41:57,265 - INFO - train_topk_accs={1: 0.6502333333333333, 5: 0.9720166666666666}
2025-03-06 17:41:57,266 - INFO - Epoch 16/800 done.
2025-03-06 17:41:57,266 - INFO - Final validation performance:
Loss: 1.873, top-1 acc: 0.656top-5 acc: 0.656
2025-03-06 17:41:57,266 - INFO - Beginning epoch 17/800
2025-03-06 17:41:57,274 - INFO - training batch 1, loss: 1.913, 32/60000 datapoints
2025-03-06 17:41:57,577 - INFO - training batch 51, loss: 1.900, 1632/60000 datapoints
2025-03-06 17:41:57,811 - INFO - training batch 101, loss: 1.855, 3232/60000 datapoints
2025-03-06 17:41:58,018 - INFO - training batch 151, loss: 1.805, 4832/60000 datapoints
2025-03-06 17:41:58,253 - INFO - training batch 201, loss: 1.847, 6432/60000 datapoints
2025-03-06 17:41:58,762 - INFO - training batch 251, loss: 1.828, 8032/60000 datapoints
2025-03-06 17:41:59,064 - INFO - training batch 301, loss: 1.897, 9632/60000 datapoints
2025-03-06 17:41:59,375 - INFO - training batch 351, loss: 1.912, 11232/60000 datapoints
2025-03-06 17:41:59,747 - INFO - training batch 401, loss: 1.796, 12832/60000 datapoints
2025-03-06 17:42:00,083 - INFO - training batch 451, loss: 1.787, 14432/60000 datapoints
2025-03-06 17:42:00,398 - INFO - training batch 501, loss: 1.832, 16032/60000 datapoints
2025-03-06 17:42:00,626 - INFO - training batch 551, loss: 1.827, 17632/60000 datapoints
2025-03-06 17:42:00,852 - INFO - training batch 601, loss: 1.820, 19232/60000 datapoints
2025-03-06 17:42:00,913 - ERROR - Traceback (most recent call last):
2025-03-06 17:42:00,914 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/train_scripts/train_feedforward_mnist.py", line 186, in <module>
    train_losses, val_losses, train_topk_accs, val_topk_accs, state_dicts, train_time = train(model=model,
2025-03-06 17:42:00,914 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 184, in train
    train_losses, train_topk_accs, state_dict = train_one_epoch(
2025-03-06 17:42:00,914 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 242, in train_one_epoch
2025-03-06 17:42:00,914 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 652, in __next__
    data = self._next_data()
2025-03-06 17:42:00,914 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 692, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
2025-03-06 17:42:00,914 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
2025-03-06 17:42:00,915 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
2025-03-06 17:42:00,915 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torchvision/datasets/mnist.py", line 145, in __getitem__
    img = self.transform(img)
2025-03-06 17:42:00,915 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torchvision/transforms/transforms.py", line 94, in __call__
    img = t(img)
2025-03-06 17:42:00,915 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torchvision/transforms/transforms.py", line 134, in __call__
    return F.to_tensor(pic)
2025-03-06 17:42:00,915 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torchvision/transforms/functional.py", line 137, in to_tensor
    if not (F_pil._is_pil_image(pic) or _is_numpy(pic)):
2025-03-06 17:42:00,915 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py", line 21, in _is_pil_image
    return isinstance(img, Image.Image)
2025-03-06 17:42:00,915 - ERROR - KeyboardInterrupt
2025-03-06 17:42:00,916 - ERROR - Traceback (most recent call last):
  File "/Users/patmccarthy/Documents/ThalamoCortex/train_scripts/train_feedforward_mnist.py", line 186, in <module>
    train_losses, val_losses, train_topk_accs, val_topk_accs, state_dicts, train_time = train(model=model,
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 184, in train
    train_losses, train_topk_accs, state_dict = train_one_epoch(
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 242, in train_one_epoch
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 652, in __next__
    data = self._next_data()
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 692, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torchvision/datasets/mnist.py", line 145, in __getitem__
    img = self.transform(img)
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torchvision/transforms/transforms.py", line 94, in __call__
    img = t(img)
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torchvision/transforms/transforms.py", line 134, in __call__
    return F.to_tensor(pic)
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torchvision/transforms/functional.py", line 137, in to_tensor
    if not (F_pil._is_pil_image(pic) or _is_numpy(pic)):
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py", line 21, in _is_pil_image
    return isinstance(img, Image.Image)
KeyboardInterrupt