2025-03-06 17:36:11,913 - INFO - Running hyperparameter combination 1 of 1
2025-03-06 17:36:11,913 - INFO - 0_CTCNet_TC_none
2025-03-06 17:36:11,914 - INFO - Loading data...
2025-03-06 17:36:12,025 - INFO - Done loading.
2025-03-06 17:36:12,025 - INFO - Building model and optimiser...
2025-03-06 17:36:12,028 - INFO - =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Sequential: 1-1                        --
|    └─Linear: 2-1                       1,040
|    └─ReLU: 2-2                         --
├─Sequential: 1-2                        --
|    └─Linear: 2-3                       25,120
|    └─ReLU: 2-4                         --
├─Sequential: 1-3                        --
|    └─Linear: 2-5                       1,056
|    └─ReLU: 2-6                         --
├─Sequential: 1-4                        --
|    └─Linear: 2-7                       330
=================================================================
Total params: 27,546
Trainable params: 27,546
Non-trainable params: 0
=================================================================
2025-03-06 17:36:12,028 - INFO - Done.
2025-03-06 17:36:12,028 - INFO - Training...
2025-03-06 17:36:12,028 - INFO - Beginning epoch 1/800
2025-03-06 17:36:12,044 - INFO - training batch 1, loss: 2.303, 32/60000 datapoints
2025-03-06 17:36:12,332 - INFO - training batch 51, loss: 2.296, 1632/60000 datapoints
2025-03-06 17:36:12,610 - INFO - training batch 101, loss: 2.309, 3232/60000 datapoints
2025-03-06 17:36:12,874 - INFO - training batch 151, loss: 2.300, 4832/60000 datapoints
2025-03-06 17:36:13,082 - INFO - training batch 201, loss: 2.279, 6432/60000 datapoints
2025-03-06 17:36:13,277 - INFO - training batch 251, loss: 2.296, 8032/60000 datapoints
2025-03-06 17:36:13,472 - INFO - training batch 301, loss: 2.297, 9632/60000 datapoints
2025-03-06 17:36:13,670 - INFO - training batch 351, loss: 2.277, 11232/60000 datapoints
2025-03-06 17:36:13,863 - INFO - training batch 401, loss: 2.301, 12832/60000 datapoints
2025-03-06 17:36:14,078 - INFO - training batch 451, loss: 2.297, 14432/60000 datapoints
2025-03-06 17:36:14,294 - INFO - training batch 501, loss: 2.275, 16032/60000 datapoints
2025-03-06 17:36:14,492 - INFO - training batch 551, loss: 2.321, 17632/60000 datapoints
2025-03-06 17:36:14,691 - INFO - training batch 601, loss: 2.335, 19232/60000 datapoints
2025-03-06 17:36:14,891 - INFO - training batch 651, loss: 2.319, 20832/60000 datapoints
2025-03-06 17:36:15,093 - INFO - training batch 701, loss: 2.274, 22432/60000 datapoints
2025-03-06 17:36:15,300 - INFO - training batch 751, loss: 2.297, 24032/60000 datapoints
2025-03-06 17:36:15,501 - INFO - training batch 801, loss: 2.265, 25632/60000 datapoints
2025-03-06 17:36:15,735 - INFO - training batch 851, loss: 2.306, 27232/60000 datapoints
2025-03-06 17:36:15,943 - INFO - training batch 901, loss: 2.271, 28832/60000 datapoints
2025-03-06 17:36:16,150 - INFO - training batch 951, loss: 2.306, 30432/60000 datapoints
2025-03-06 17:36:16,484 - INFO - training batch 1001, loss: 2.294, 32032/60000 datapoints
2025-03-06 17:36:16,714 - INFO - training batch 1051, loss: 2.308, 33632/60000 datapoints
2025-03-06 17:36:16,994 - INFO - training batch 1101, loss: 2.304, 35232/60000 datapoints
2025-03-06 17:36:17,207 - INFO - training batch 1151, loss: 2.272, 36832/60000 datapoints
2025-03-06 17:36:17,416 - INFO - training batch 1201, loss: 2.252, 38432/60000 datapoints
2025-03-06 17:36:17,662 - INFO - training batch 1251, loss: 2.293, 40032/60000 datapoints
2025-03-06 17:36:17,890 - INFO - training batch 1301, loss: 2.305, 41632/60000 datapoints
2025-03-06 17:36:18,190 - INFO - training batch 1351, loss: 2.291, 43232/60000 datapoints
2025-03-06 17:36:18,477 - INFO - training batch 1401, loss: 2.288, 44832/60000 datapoints
2025-03-06 17:36:18,721 - INFO - training batch 1451, loss: 2.289, 46432/60000 datapoints
2025-03-06 17:36:18,980 - INFO - training batch 1501, loss: 2.317, 48032/60000 datapoints
2025-03-06 17:36:19,194 - INFO - training batch 1551, loss: 2.321, 49632/60000 datapoints
2025-03-06 17:36:19,486 - INFO - training batch 1601, loss: 2.276, 51232/60000 datapoints
2025-03-06 17:36:19,705 - INFO - training batch 1651, loss: 2.289, 52832/60000 datapoints
2025-03-06 17:36:19,958 - INFO - training batch 1701, loss: 2.296, 54432/60000 datapoints
2025-03-06 17:36:20,206 - INFO - training batch 1751, loss: 2.286, 56032/60000 datapoints
2025-03-06 17:36:20,419 - INFO - training batch 1801, loss: 2.304, 57632/60000 datapoints
2025-03-06 17:36:20,622 - INFO - training batch 1851, loss: 2.287, 59232/60000 datapoints
2025-03-06 17:36:20,758 - INFO - validation batch 1, loss: 2.286, 32/10016 datapoints
2025-03-06 17:36:20,953 - INFO - validation batch 51, loss: 2.278, 1632/10016 datapoints
2025-03-06 17:36:21,155 - INFO - validation batch 101, loss: 2.277, 3232/10016 datapoints
2025-03-06 17:36:21,311 - INFO - validation batch 151, loss: 2.283, 4832/10016 datapoints
2025-03-06 17:36:21,466 - INFO - validation batch 201, loss: 2.310, 6432/10016 datapoints
2025-03-06 17:36:21,637 - INFO - validation batch 251, loss: 2.280, 8032/10016 datapoints
2025-03-06 17:36:21,794 - INFO - validation batch 301, loss: 2.263, 9632/10016 datapoints
2025-03-06 17:36:21,835 - INFO - train_topk_accs={1: 0.12311666666666667, 5: 0.5771166666666666}
2025-03-06 17:36:21,836 - INFO - Epoch 1/800 done.
2025-03-06 17:36:21,836 - INFO - Final validation performance:
Loss: 2.283, top-1 acc: 0.135top-5 acc: 0.135
2025-03-06 17:36:21,836 - INFO - Beginning epoch 2/800
2025-03-06 17:36:21,842 - INFO - training batch 1, loss: 2.332, 32/60000 datapoints
2025-03-06 17:36:22,051 - INFO - training batch 51, loss: 2.268, 1632/60000 datapoints
2025-03-06 17:36:22,262 - INFO - training batch 101, loss: 2.278, 3232/60000 datapoints
2025-03-06 17:36:22,468 - INFO - training batch 151, loss: 2.305, 4832/60000 datapoints
2025-03-06 17:36:22,673 - INFO - training batch 201, loss: 2.281, 6432/60000 datapoints
2025-03-06 17:36:22,886 - INFO - training batch 251, loss: 2.278, 8032/60000 datapoints
2025-03-06 17:36:23,093 - INFO - training batch 301, loss: 2.261, 9632/60000 datapoints
2025-03-06 17:36:23,297 - INFO - training batch 351, loss: 2.270, 11232/60000 datapoints
2025-03-06 17:36:23,503 - INFO - training batch 401, loss: 2.282, 12832/60000 datapoints
2025-03-06 17:36:23,711 - INFO - training batch 451, loss: 2.322, 14432/60000 datapoints
2025-03-06 17:36:23,912 - INFO - training batch 501, loss: 2.250, 16032/60000 datapoints
2025-03-06 17:36:24,116 - INFO - training batch 551, loss: 2.293, 17632/60000 datapoints
2025-03-06 17:36:24,316 - INFO - training batch 601, loss: 2.304, 19232/60000 datapoints
2025-03-06 17:36:24,531 - INFO - training batch 651, loss: 2.293, 20832/60000 datapoints
2025-03-06 17:36:24,732 - INFO - training batch 701, loss: 2.267, 22432/60000 datapoints
2025-03-06 17:36:24,934 - INFO - training batch 751, loss: 2.291, 24032/60000 datapoints
2025-03-06 17:36:25,135 - INFO - training batch 801, loss: 2.259, 25632/60000 datapoints
2025-03-06 17:36:25,334 - INFO - training batch 851, loss: 2.289, 27232/60000 datapoints
2025-03-06 17:36:25,530 - INFO - training batch 901, loss: 2.291, 28832/60000 datapoints
2025-03-06 17:36:25,733 - INFO - training batch 951, loss: 2.254, 30432/60000 datapoints
2025-03-06 17:36:25,929 - INFO - training batch 1001, loss: 2.244, 32032/60000 datapoints
2025-03-06 17:36:26,132 - INFO - training batch 1051, loss: 2.271, 33632/60000 datapoints
2025-03-06 17:36:26,330 - INFO - training batch 1101, loss: 2.292, 35232/60000 datapoints
2025-03-06 17:36:26,528 - INFO - training batch 1151, loss: 2.252, 36832/60000 datapoints
2025-03-06 17:36:26,724 - INFO - training batch 1201, loss: 2.271, 38432/60000 datapoints
2025-03-06 17:36:26,924 - INFO - training batch 1251, loss: 2.278, 40032/60000 datapoints
2025-03-06 17:36:27,123 - INFO - training batch 1301, loss: 2.298, 41632/60000 datapoints
2025-03-06 17:36:27,321 - INFO - training batch 1351, loss: 2.288, 43232/60000 datapoints
2025-03-06 17:36:27,522 - INFO - training batch 1401, loss: 2.312, 44832/60000 datapoints
2025-03-06 17:36:27,722 - INFO - training batch 1451, loss: 2.264, 46432/60000 datapoints
2025-03-06 17:36:27,919 - INFO - training batch 1501, loss: 2.294, 48032/60000 datapoints
2025-03-06 17:36:28,119 - INFO - training batch 1551, loss: 2.252, 49632/60000 datapoints
2025-03-06 17:36:28,316 - INFO - training batch 1601, loss: 2.257, 51232/60000 datapoints
2025-03-06 17:36:28,513 - INFO - training batch 1651, loss: 2.262, 52832/60000 datapoints
2025-03-06 17:36:28,721 - INFO - training batch 1701, loss: 2.310, 54432/60000 datapoints
2025-03-06 17:36:28,918 - INFO - training batch 1751, loss: 2.263, 56032/60000 datapoints
2025-03-06 17:36:29,118 - INFO - training batch 1801, loss: 2.309, 57632/60000 datapoints
2025-03-06 17:36:29,312 - INFO - training batch 1851, loss: 2.248, 59232/60000 datapoints
2025-03-06 17:36:29,415 - INFO - validation batch 1, loss: 2.272, 32/10016 datapoints
2025-03-06 17:36:29,573 - INFO - validation batch 51, loss: 2.286, 1632/10016 datapoints
2025-03-06 17:36:29,729 - INFO - validation batch 101, loss: 2.292, 3232/10016 datapoints
2025-03-06 17:36:29,885 - INFO - validation batch 151, loss: 2.312, 4832/10016 datapoints
2025-03-06 17:36:30,043 - INFO - validation batch 201, loss: 2.276, 6432/10016 datapoints
2025-03-06 17:36:30,207 - INFO - validation batch 251, loss: 2.282, 8032/10016 datapoints
2025-03-06 17:36:30,366 - INFO - validation batch 301, loss: 2.284, 9632/10016 datapoints
2025-03-06 17:36:30,405 - INFO - train_topk_accs={1: 0.15381666666666666, 5: 0.6356166666666667}
2025-03-06 17:36:30,406 - INFO - Epoch 2/800 done.
2025-03-06 17:36:30,406 - INFO - Final validation performance:
Loss: 2.286, top-1 acc: 0.162top-5 acc: 0.162
2025-03-06 17:36:30,406 - INFO - Beginning epoch 3/800
2025-03-06 17:36:30,412 - INFO - training batch 1, loss: 2.278, 32/60000 datapoints
2025-03-06 17:36:30,614 - INFO - training batch 51, loss: 2.290, 1632/60000 datapoints
2025-03-06 17:36:30,817 - INFO - training batch 101, loss: 2.285, 3232/60000 datapoints
2025-03-06 17:36:31,015 - INFO - training batch 151, loss: 2.271, 4832/60000 datapoints
2025-03-06 17:36:31,218 - INFO - training batch 201, loss: 2.258, 6432/60000 datapoints
2025-03-06 17:36:31,415 - INFO - training batch 251, loss: 2.275, 8032/60000 datapoints
2025-03-06 17:36:31,616 - INFO - training batch 301, loss: 2.283, 9632/60000 datapoints
2025-03-06 17:36:31,814 - INFO - training batch 351, loss: 2.290, 11232/60000 datapoints
2025-03-06 17:36:32,023 - INFO - training batch 401, loss: 2.275, 12832/60000 datapoints
2025-03-06 17:36:32,228 - INFO - training batch 451, loss: 2.287, 14432/60000 datapoints
2025-03-06 17:36:32,426 - INFO - training batch 501, loss: 2.300, 16032/60000 datapoints
2025-03-06 17:36:32,625 - INFO - training batch 551, loss: 2.260, 17632/60000 datapoints
2025-03-06 17:36:32,826 - INFO - training batch 601, loss: 2.283, 19232/60000 datapoints
2025-03-06 17:36:33,023 - INFO - training batch 651, loss: 2.264, 20832/60000 datapoints
2025-03-06 17:36:33,223 - INFO - training batch 701, loss: 2.256, 22432/60000 datapoints
2025-03-06 17:36:33,429 - INFO - training batch 751, loss: 2.259, 24032/60000 datapoints
2025-03-06 17:36:33,630 - INFO - training batch 801, loss: 2.293, 25632/60000 datapoints
2025-03-06 17:36:33,834 - INFO - training batch 851, loss: 2.261, 27232/60000 datapoints
2025-03-06 17:36:34,035 - INFO - training batch 901, loss: 2.302, 28832/60000 datapoints
2025-03-06 17:36:34,248 - INFO - training batch 951, loss: 2.308, 30432/60000 datapoints
2025-03-06 17:36:34,465 - INFO - training batch 1001, loss: 2.251, 32032/60000 datapoints
2025-03-06 17:36:34,667 - INFO - training batch 1051, loss: 2.260, 33632/60000 datapoints
2025-03-06 17:36:34,872 - INFO - training batch 1101, loss: 2.283, 35232/60000 datapoints
2025-03-06 17:36:35,070 - INFO - training batch 1151, loss: 2.266, 36832/60000 datapoints
2025-03-06 17:36:35,273 - INFO - training batch 1201, loss: 2.241, 38432/60000 datapoints
2025-03-06 17:36:35,472 - INFO - training batch 1251, loss: 2.242, 40032/60000 datapoints
2025-03-06 17:36:35,671 - INFO - training batch 1301, loss: 2.265, 41632/60000 datapoints
2025-03-06 17:36:35,869 - INFO - training batch 1351, loss: 2.262, 43232/60000 datapoints
2025-03-06 17:36:36,071 - INFO - training batch 1401, loss: 2.265, 44832/60000 datapoints
2025-03-06 17:36:36,270 - INFO - training batch 1451, loss: 2.239, 46432/60000 datapoints
2025-03-06 17:36:36,468 - INFO - training batch 1501, loss: 2.318, 48032/60000 datapoints
2025-03-06 17:36:36,667 - INFO - training batch 1551, loss: 2.263, 49632/60000 datapoints
2025-03-06 17:36:36,861 - INFO - training batch 1601, loss: 2.261, 51232/60000 datapoints
2025-03-06 17:36:37,059 - INFO - training batch 1651, loss: 2.292, 52832/60000 datapoints
2025-03-06 17:36:37,260 - INFO - training batch 1701, loss: 2.262, 54432/60000 datapoints
2025-03-06 17:36:37,461 - INFO - training batch 1751, loss: 2.280, 56032/60000 datapoints
2025-03-06 17:36:37,696 - INFO - training batch 1801, loss: 2.296, 57632/60000 datapoints
2025-03-06 17:36:37,901 - INFO - training batch 1851, loss: 2.262, 59232/60000 datapoints
2025-03-06 17:36:38,006 - INFO - validation batch 1, loss: 2.272, 32/10016 datapoints
2025-03-06 17:36:38,164 - INFO - validation batch 51, loss: 2.258, 1632/10016 datapoints
2025-03-06 17:36:38,326 - INFO - validation batch 101, loss: 2.256, 3232/10016 datapoints
2025-03-06 17:36:38,483 - INFO - validation batch 151, loss: 2.282, 4832/10016 datapoints
2025-03-06 17:36:38,642 - INFO - validation batch 201, loss: 2.256, 6432/10016 datapoints
2025-03-06 17:36:38,809 - INFO - validation batch 251, loss: 2.269, 8032/10016 datapoints
2025-03-06 17:36:38,970 - INFO - validation batch 301, loss: 2.262, 9632/10016 datapoints
2025-03-06 17:36:39,009 - INFO - train_topk_accs={1: 0.18316666666666667, 5: 0.6574666666666666}
2025-03-06 17:36:39,009 - INFO - Epoch 3/800 done.
2025-03-06 17:36:39,010 - INFO - Final validation performance:
Loss: 2.265, top-1 acc: 0.197top-5 acc: 0.197
2025-03-06 17:36:39,010 - INFO - Beginning epoch 4/800
2025-03-06 17:36:39,016 - INFO - training batch 1, loss: 2.270, 32/60000 datapoints
2025-03-06 17:36:39,219 - INFO - training batch 51, loss: 2.284, 1632/60000 datapoints
2025-03-06 17:36:39,424 - INFO - training batch 101, loss: 2.276, 3232/60000 datapoints
2025-03-06 17:36:39,626 - INFO - training batch 151, loss: 2.273, 4832/60000 datapoints
2025-03-06 17:36:39,830 - INFO - training batch 201, loss: 2.247, 6432/60000 datapoints
2025-03-06 17:36:40,036 - INFO - training batch 251, loss: 2.299, 8032/60000 datapoints
2025-03-06 17:36:40,239 - INFO - training batch 301, loss: 2.219, 9632/60000 datapoints
2025-03-06 17:36:40,436 - INFO - training batch 351, loss: 2.264, 11232/60000 datapoints
2025-03-06 17:36:40,650 - INFO - training batch 401, loss: 2.309, 12832/60000 datapoints
2025-03-06 17:36:40,847 - INFO - training batch 451, loss: 2.237, 14432/60000 datapoints
2025-03-06 17:36:41,046 - INFO - training batch 501, loss: 2.256, 16032/60000 datapoints
2025-03-06 17:36:41,250 - INFO - training batch 551, loss: 2.282, 17632/60000 datapoints
2025-03-06 17:36:41,448 - INFO - training batch 601, loss: 2.224, 19232/60000 datapoints
2025-03-06 17:36:41,660 - INFO - training batch 651, loss: 2.284, 20832/60000 datapoints
2025-03-06 17:36:41,859 - INFO - training batch 701, loss: 2.234, 22432/60000 datapoints
2025-03-06 17:36:42,073 - INFO - training batch 751, loss: 2.234, 24032/60000 datapoints
2025-03-06 17:36:42,279 - INFO - training batch 801, loss: 2.247, 25632/60000 datapoints
2025-03-06 17:36:42,502 - INFO - training batch 851, loss: 2.255, 27232/60000 datapoints
2025-03-06 17:36:42,704 - INFO - training batch 901, loss: 2.242, 28832/60000 datapoints
2025-03-06 17:36:42,910 - INFO - training batch 951, loss: 2.216, 30432/60000 datapoints
2025-03-06 17:36:43,107 - INFO - training batch 1001, loss: 2.283, 32032/60000 datapoints
2025-03-06 17:36:43,321 - INFO - training batch 1051, loss: 2.287, 33632/60000 datapoints
2025-03-06 17:36:43,548 - INFO - training batch 1101, loss: 2.263, 35232/60000 datapoints
2025-03-06 17:36:43,780 - INFO - training batch 1151, loss: 2.274, 36832/60000 datapoints
2025-03-06 17:36:43,988 - INFO - training batch 1201, loss: 2.226, 38432/60000 datapoints
2025-03-06 17:36:44,219 - INFO - training batch 1251, loss: 2.265, 40032/60000 datapoints
2025-03-06 17:36:44,442 - INFO - training batch 1301, loss: 2.196, 41632/60000 datapoints
2025-03-06 17:36:44,685 - INFO - training batch 1351, loss: 2.216, 43232/60000 datapoints
2025-03-06 17:36:44,915 - INFO - training batch 1401, loss: 2.266, 44832/60000 datapoints
2025-03-06 17:36:45,137 - INFO - training batch 1451, loss: 2.237, 46432/60000 datapoints
2025-03-06 17:36:45,357 - INFO - training batch 1501, loss: 2.240, 48032/60000 datapoints
2025-03-06 17:36:45,576 - INFO - training batch 1551, loss: 2.235, 49632/60000 datapoints
2025-03-06 17:36:45,799 - INFO - training batch 1601, loss: 2.211, 51232/60000 datapoints
2025-03-06 17:36:46,010 - INFO - training batch 1651, loss: 2.196, 52832/60000 datapoints
2025-03-06 17:36:46,227 - INFO - training batch 1701, loss: 2.275, 54432/60000 datapoints
2025-03-06 17:36:46,465 - INFO - training batch 1751, loss: 2.226, 56032/60000 datapoints
2025-03-06 17:36:46,681 - INFO - training batch 1801, loss: 2.264, 57632/60000 datapoints
2025-03-06 17:36:46,879 - INFO - training batch 1851, loss: 2.238, 59232/60000 datapoints
2025-03-06 17:36:46,982 - INFO - validation batch 1, loss: 2.264, 32/10016 datapoints
2025-03-06 17:36:47,150 - INFO - validation batch 51, loss: 2.225, 1632/10016 datapoints
2025-03-06 17:36:47,321 - INFO - validation batch 101, loss: 2.235, 3232/10016 datapoints
2025-03-06 17:36:47,495 - INFO - validation batch 151, loss: 2.197, 4832/10016 datapoints
2025-03-06 17:36:47,655 - INFO - validation batch 201, loss: 2.269, 6432/10016 datapoints
2025-03-06 17:36:47,820 - INFO - validation batch 251, loss: 2.262, 8032/10016 datapoints
2025-03-06 17:36:47,975 - INFO - validation batch 301, loss: 2.236, 9632/10016 datapoints
2025-03-06 17:36:48,017 - INFO - train_topk_accs={1: 0.21326666666666666, 5: 0.69385}
2025-03-06 17:36:48,017 - INFO - Epoch 4/800 done.
2025-03-06 17:36:48,017 - INFO - Final validation performance:
Loss: 2.241, top-1 acc: 0.229top-5 acc: 0.229
2025-03-06 17:36:48,018 - INFO - Beginning epoch 5/800
2025-03-06 17:36:48,023 - INFO - training batch 1, loss: 2.256, 32/60000 datapoints
2025-03-06 17:36:48,305 - INFO - training batch 51, loss: 2.238, 1632/60000 datapoints
2025-03-06 17:36:48,550 - INFO - training batch 101, loss: 2.267, 3232/60000 datapoints
2025-03-06 17:36:48,819 - INFO - training batch 151, loss: 2.232, 4832/60000 datapoints
2025-03-06 17:36:49,030 - INFO - training batch 201, loss: 2.216, 6432/60000 datapoints
2025-03-06 17:36:49,291 - INFO - training batch 251, loss: 2.201, 8032/60000 datapoints
2025-03-06 17:36:49,513 - INFO - training batch 301, loss: 2.230, 9632/60000 datapoints
2025-03-06 17:36:49,742 - INFO - training batch 351, loss: 2.302, 11232/60000 datapoints
2025-03-06 17:36:49,963 - INFO - training batch 401, loss: 2.258, 12832/60000 datapoints
2025-03-06 17:36:50,187 - INFO - training batch 451, loss: 2.253, 14432/60000 datapoints
2025-03-06 17:36:50,410 - INFO - training batch 501, loss: 2.284, 16032/60000 datapoints
2025-03-06 17:36:50,630 - INFO - training batch 551, loss: 2.216, 17632/60000 datapoints
2025-03-06 17:36:50,849 - INFO - training batch 601, loss: 2.281, 19232/60000 datapoints
2025-03-06 17:36:51,066 - INFO - training batch 651, loss: 2.211, 20832/60000 datapoints
2025-03-06 17:36:51,280 - INFO - training batch 701, loss: 2.248, 22432/60000 datapoints
2025-03-06 17:36:51,506 - INFO - training batch 751, loss: 2.232, 24032/60000 datapoints
2025-03-06 17:36:51,729 - INFO - training batch 801, loss: 2.210, 25632/60000 datapoints
2025-03-06 17:36:51,945 - INFO - training batch 851, loss: 2.248, 27232/60000 datapoints
2025-03-06 17:36:52,169 - INFO - training batch 901, loss: 2.225, 28832/60000 datapoints
2025-03-06 17:36:52,388 - INFO - training batch 951, loss: 2.256, 30432/60000 datapoints
2025-03-06 17:36:52,632 - INFO - training batch 1001, loss: 2.239, 32032/60000 datapoints
2025-03-06 17:36:52,856 - INFO - training batch 1051, loss: 2.217, 33632/60000 datapoints
2025-03-06 17:36:53,076 - INFO - training batch 1101, loss: 2.196, 35232/60000 datapoints
2025-03-06 17:36:53,300 - INFO - training batch 1151, loss: 2.243, 36832/60000 datapoints
2025-03-06 17:36:53,513 - INFO - training batch 1201, loss: 2.224, 38432/60000 datapoints
2025-03-06 17:36:53,724 - INFO - training batch 1251, loss: 2.222, 40032/60000 datapoints
2025-03-06 17:36:53,937 - INFO - training batch 1301, loss: 2.212, 41632/60000 datapoints
2025-03-06 17:36:54,149 - INFO - training batch 1351, loss: 2.226, 43232/60000 datapoints
2025-03-06 17:36:54,360 - INFO - training batch 1401, loss: 2.218, 44832/60000 datapoints
2025-03-06 17:36:54,579 - INFO - training batch 1451, loss: 2.194, 46432/60000 datapoints
2025-03-06 17:36:54,811 - INFO - training batch 1501, loss: 2.285, 48032/60000 datapoints
2025-03-06 17:36:55,026 - INFO - training batch 1551, loss: 2.192, 49632/60000 datapoints
2025-03-06 17:36:55,244 - INFO - training batch 1601, loss: 2.214, 51232/60000 datapoints
2025-03-06 17:36:55,459 - INFO - training batch 1651, loss: 2.243, 52832/60000 datapoints
2025-03-06 17:36:55,702 - INFO - training batch 1701, loss: 2.269, 54432/60000 datapoints
2025-03-06 17:36:55,949 - INFO - training batch 1751, loss: 2.270, 56032/60000 datapoints
2025-03-06 17:36:56,212 - INFO - training batch 1801, loss: 2.232, 57632/60000 datapoints
2025-03-06 17:36:56,424 - INFO - training batch 1851, loss: 2.227, 59232/60000 datapoints
2025-03-06 17:36:56,541 - INFO - validation batch 1, loss: 2.262, 32/10016 datapoints
2025-03-06 17:36:56,738 - INFO - validation batch 51, loss: 2.221, 1632/10016 datapoints
2025-03-06 17:36:56,920 - INFO - validation batch 101, loss: 2.198, 3232/10016 datapoints
2025-03-06 17:36:57,097 - INFO - validation batch 151, loss: 2.216, 4832/10016 datapoints
2025-03-06 17:36:57,280 - INFO - validation batch 201, loss: 2.202, 6432/10016 datapoints
2025-03-06 17:36:57,475 - INFO - validation batch 251, loss: 2.244, 8032/10016 datapoints
2025-03-06 17:36:57,666 - INFO - validation batch 301, loss: 2.251, 9632/10016 datapoints
2025-03-06 17:36:57,725 - INFO - train_topk_accs={1: 0.23791666666666667, 5: 0.7238166666666667}
2025-03-06 17:36:57,725 - INFO - Epoch 5/800 done.
2025-03-06 17:36:57,725 - INFO - Final validation performance:
Loss: 2.228, top-1 acc: 0.253top-5 acc: 0.253
2025-03-06 17:36:57,726 - INFO - Beginning epoch 6/800
2025-03-06 17:36:57,732 - INFO - training batch 1, loss: 2.211, 32/60000 datapoints
2025-03-06 17:36:58,018 - INFO - training batch 51, loss: 2.208, 1632/60000 datapoints
2025-03-06 17:36:58,247 - INFO - training batch 101, loss: 2.254, 3232/60000 datapoints
2025-03-06 17:36:58,452 - INFO - training batch 151, loss: 2.248, 4832/60000 datapoints
2025-03-06 17:36:58,669 - INFO - training batch 201, loss: 2.268, 6432/60000 datapoints
2025-03-06 17:36:58,885 - INFO - training batch 251, loss: 2.239, 8032/60000 datapoints
2025-03-06 17:36:59,087 - INFO - training batch 301, loss: 2.269, 9632/60000 datapoints
2025-03-06 17:36:59,304 - INFO - training batch 351, loss: 2.187, 11232/60000 datapoints
2025-03-06 17:36:59,515 - INFO - training batch 401, loss: 2.229, 12832/60000 datapoints
2025-03-06 17:36:59,722 - INFO - training batch 451, loss: 2.254, 14432/60000 datapoints
2025-03-06 17:36:59,929 - INFO - training batch 501, loss: 2.228, 16032/60000 datapoints
2025-03-06 17:37:00,143 - INFO - training batch 551, loss: 2.272, 17632/60000 datapoints
2025-03-06 17:37:00,166 - ERROR - Traceback (most recent call last):
2025-03-06 17:37:00,166 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/train_scripts/train_feedforward_mnist.py", line 186, in <module>
    train_losses, val_losses, train_topk_accs, val_topk_accs, state_dicts, train_time = train(model=model,
2025-03-06 17:37:00,166 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 184, in train
    train_losses, train_topk_accs, state_dict = train_one_epoch(
2025-03-06 17:37:00,166 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 266, in train_one_epoch
    topk_correct[k] += sum([y[i] in topk_pred[i] for i in range(y.size(0))])
2025-03-06 17:37:00,166 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 266, in <listcomp>
    topk_correct[k] += sum([y[i] in topk_pred[i] for i in range(y.size(0))])
2025-03-06 17:37:00,167 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/_tensor.py", line 782, in __contains__
    return (element == self).any().item()  # type: ignore[union-attr]
2025-03-06 17:37:00,167 - ERROR - KeyboardInterrupt
2025-03-06 17:37:00,167 - ERROR - Traceback (most recent call last):
  File "/Users/patmccarthy/Documents/ThalamoCortex/train_scripts/train_feedforward_mnist.py", line 186, in <module>
    train_losses, val_losses, train_topk_accs, val_topk_accs, state_dicts, train_time = train(model=model,
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 184, in train
    train_losses, train_topk_accs, state_dict = train_one_epoch(
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 266, in train_one_epoch
    topk_correct[k] += sum([y[i] in topk_pred[i] for i in range(y.size(0))])
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 266, in <listcomp>
    topk_correct[k] += sum([y[i] in topk_pred[i] for i in range(y.size(0))])
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/_tensor.py", line 782, in __contains__
    return (element == self).any().item()  # type: ignore[union-attr]
KeyboardInterrupt