2025-03-07 18:41:14,677 - INFO - Running hyperparameter combination 1 of 1
2025-03-07 18:41:14,677 - INFO - 0_CTCNet_TC_none
2025-03-07 18:41:14,678 - INFO - Loading data...
2025-03-07 18:41:14,840 - INFO - Done loading.
2025-03-07 18:41:14,840 - INFO - Building model and optimiser...
2025-03-07 18:41:14,850 - INFO - =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Sequential: 1-1                        --
|    └─Linear: 2-1                       1,040
|    └─ReLU: 2-2                         --
├─Sequential: 1-2                        --
|    └─Linear: 2-3                       25,120
|    └─ReLU: 2-4                         --
├─Sequential: 1-3                        --
|    └─Linear: 2-5                       1,056
|    └─ReLU: 2-6                         --
├─Sequential: 1-4                        --
|    └─Linear: 2-7                       330
=================================================================
Total params: 27,546
Trainable params: 27,546
Non-trainable params: 0
=================================================================
2025-03-07 18:41:14,851 - INFO - Done.
2025-03-07 18:41:14,851 - INFO - Training...
2025-03-07 18:41:14,851 - INFO - Beginning epoch 1/800
2025-03-07 18:41:14,995 - INFO - training batch 1, loss: 2.357, 32/60000 datapoints
2025-03-07 18:41:15,665 - INFO - training batch 51, loss: 2.326, 1632/60000 datapoints
2025-03-07 18:41:16,101 - INFO - training batch 101, loss: 2.309, 3232/60000 datapoints
2025-03-07 18:41:16,581 - INFO - training batch 151, loss: 2.289, 4832/60000 datapoints
2025-03-07 18:41:17,424 - INFO - training batch 201, loss: 2.336, 6432/60000 datapoints
2025-03-07 18:41:17,922 - INFO - training batch 251, loss: 2.312, 8032/60000 datapoints
2025-03-07 18:41:18,340 - INFO - training batch 301, loss: 2.318, 9632/60000 datapoints
2025-03-07 18:41:18,714 - INFO - training batch 351, loss: 2.319, 11232/60000 datapoints
2025-03-07 18:41:19,239 - INFO - training batch 401, loss: 2.300, 12832/60000 datapoints
2025-03-07 18:41:20,227 - INFO - training batch 451, loss: 2.312, 14432/60000 datapoints
2025-03-07 18:41:20,791 - INFO - training batch 501, loss: 2.320, 16032/60000 datapoints
2025-03-07 18:41:21,216 - INFO - training batch 551, loss: 2.304, 17632/60000 datapoints
2025-03-07 18:41:21,573 - INFO - training batch 601, loss: 2.334, 19232/60000 datapoints
2025-03-07 18:41:22,013 - INFO - training batch 651, loss: 2.324, 20832/60000 datapoints
2025-03-07 18:41:22,855 - INFO - training batch 701, loss: 2.326, 22432/60000 datapoints
2025-03-07 18:41:23,954 - INFO - training batch 751, loss: 2.303, 24032/60000 datapoints
2025-03-07 18:41:26,649 - INFO - training batch 801, loss: 2.295, 25632/60000 datapoints
2025-03-07 18:41:28,634 - INFO - training batch 851, loss: 2.290, 27232/60000 datapoints
2025-03-07 18:41:29,816 - INFO - training batch 901, loss: 2.309, 28832/60000 datapoints
2025-03-07 18:41:30,580 - INFO - training batch 951, loss: 2.329, 30432/60000 datapoints
2025-03-07 18:41:31,699 - INFO - training batch 1001, loss: 2.316, 32032/60000 datapoints
2025-03-07 18:41:33,217 - INFO - training batch 1051, loss: 2.316, 33632/60000 datapoints
2025-03-07 18:41:34,572 - INFO - training batch 1101, loss: 2.283, 35232/60000 datapoints
2025-03-07 18:41:35,759 - INFO - training batch 1151, loss: 2.298, 36832/60000 datapoints
2025-03-07 18:41:36,941 - INFO - training batch 1201, loss: 2.294, 38432/60000 datapoints
2025-03-07 18:41:38,195 - INFO - training batch 1251, loss: 2.304, 40032/60000 datapoints
2025-03-07 18:41:39,228 - INFO - training batch 1301, loss: 2.294, 41632/60000 datapoints
2025-03-07 18:41:39,978 - INFO - training batch 1351, loss: 2.313, 43232/60000 datapoints
2025-03-07 18:41:41,005 - INFO - training batch 1401, loss: 2.272, 44832/60000 datapoints
2025-03-07 18:41:41,790 - INFO - training batch 1451, loss: 2.313, 46432/60000 datapoints
2025-03-07 18:41:42,386 - INFO - training batch 1501, loss: 2.321, 48032/60000 datapoints
2025-03-07 18:41:42,891 - INFO - training batch 1551, loss: 2.309, 49632/60000 datapoints
2025-03-07 18:41:43,396 - INFO - training batch 1601, loss: 2.298, 51232/60000 datapoints
2025-03-07 18:41:43,860 - INFO - training batch 1651, loss: 2.332, 52832/60000 datapoints
2025-03-07 18:41:44,310 - INFO - training batch 1701, loss: 2.319, 54432/60000 datapoints
2025-03-07 18:41:44,792 - INFO - training batch 1751, loss: 2.285, 56032/60000 datapoints
2025-03-07 18:41:45,253 - INFO - training batch 1801, loss: 2.322, 57632/60000 datapoints
2025-03-07 18:41:45,823 - INFO - training batch 1851, loss: 2.327, 59232/60000 datapoints
2025-03-07 18:41:46,312 - INFO - validation batch 1, loss: 2.286, 32/10016 datapoints
2025-03-07 18:41:46,708 - INFO - validation batch 51, loss: 2.309, 1632/10016 datapoints
2025-03-07 18:41:47,048 - INFO - validation batch 101, loss: 2.274, 3232/10016 datapoints
2025-03-07 18:41:47,525 - INFO - validation batch 151, loss: 2.270, 4832/10016 datapoints
2025-03-07 18:41:47,937 - INFO - validation batch 201, loss: 2.326, 6432/10016 datapoints
2025-03-07 18:41:48,310 - INFO - validation batch 251, loss: 2.266, 8032/10016 datapoints
2025-03-07 18:41:48,678 - INFO - validation batch 301, loss: 2.313, 9632/10016 datapoints
2025-03-07 18:41:48,750 - INFO - Epoch 1/800 done.
2025-03-07 18:41:48,764 - INFO - Final validation performance:
Loss: 2.292, top-1 acc: 0.098top-5 acc: 0.098
2025-03-07 18:41:48,786 - INFO - Beginning epoch 2/800
2025-03-07 18:41:48,804 - INFO - training batch 1, loss: 2.299, 32/60000 datapoints
2025-03-07 18:41:49,289 - INFO - training batch 51, loss: 2.309, 1632/60000 datapoints
2025-03-07 18:41:49,709 - INFO - training batch 101, loss: 2.320, 3232/60000 datapoints
2025-03-07 18:41:50,133 - INFO - training batch 151, loss: 2.302, 4832/60000 datapoints
2025-03-07 18:41:50,550 - INFO - training batch 201, loss: 2.295, 6432/60000 datapoints
2025-03-07 18:41:50,935 - INFO - training batch 251, loss: 2.275, 8032/60000 datapoints
2025-03-07 18:41:51,314 - INFO - training batch 301, loss: 2.315, 9632/60000 datapoints
2025-03-07 18:41:51,622 - INFO - training batch 351, loss: 2.322, 11232/60000 datapoints
2025-03-07 18:41:51,932 - INFO - training batch 401, loss: 2.262, 12832/60000 datapoints
2025-03-07 18:41:52,366 - INFO - training batch 451, loss: 2.287, 14432/60000 datapoints
2025-03-07 18:41:52,765 - INFO - training batch 501, loss: 2.290, 16032/60000 datapoints
2025-03-07 18:41:53,232 - INFO - training batch 551, loss: 2.277, 17632/60000 datapoints
2025-03-07 18:41:53,912 - INFO - training batch 601, loss: 2.292, 19232/60000 datapoints
2025-03-07 18:41:54,484 - INFO - training batch 651, loss: 2.294, 20832/60000 datapoints
2025-03-07 18:41:55,307 - INFO - training batch 701, loss: 2.322, 22432/60000 datapoints
2025-03-07 18:41:56,374 - INFO - training batch 751, loss: 2.314, 24032/60000 datapoints
2025-03-07 18:41:56,833 - INFO - training batch 801, loss: 2.308, 25632/60000 datapoints
2025-03-07 18:41:59,131 - INFO - training batch 851, loss: 2.326, 27232/60000 datapoints
2025-03-07 18:42:00,278 - INFO - training batch 901, loss: 2.313, 28832/60000 datapoints
2025-03-07 18:42:00,802 - INFO - training batch 951, loss: 2.289, 30432/60000 datapoints
2025-03-07 18:42:01,304 - INFO - training batch 1001, loss: 2.264, 32032/60000 datapoints
2025-03-07 18:42:01,767 - INFO - training batch 1051, loss: 2.277, 33632/60000 datapoints
2025-03-07 18:42:02,167 - INFO - training batch 1101, loss: 2.272, 35232/60000 datapoints
2025-03-07 18:42:02,521 - INFO - training batch 1151, loss: 2.253, 36832/60000 datapoints
2025-03-07 18:42:03,039 - INFO - training batch 1201, loss: 2.272, 38432/60000 datapoints
2025-03-07 18:42:03,529 - INFO - training batch 1251, loss: 2.292, 40032/60000 datapoints
2025-03-07 18:42:04,088 - INFO - training batch 1301, loss: 2.267, 41632/60000 datapoints
2025-03-07 18:42:04,755 - INFO - training batch 1351, loss: 2.310, 43232/60000 datapoints
2025-03-07 18:42:05,341 - INFO - training batch 1401, loss: 2.309, 44832/60000 datapoints
2025-03-07 18:42:05,771 - INFO - training batch 1451, loss: 2.268, 46432/60000 datapoints
2025-03-07 18:42:06,220 - INFO - training batch 1501, loss: 2.281, 48032/60000 datapoints
2025-03-07 18:42:06,745 - INFO - training batch 1551, loss: 2.279, 49632/60000 datapoints
2025-03-07 18:42:07,156 - INFO - training batch 1601, loss: 2.262, 51232/60000 datapoints
2025-03-07 18:42:07,522 - INFO - training batch 1651, loss: 2.284, 52832/60000 datapoints
2025-03-07 18:42:08,017 - INFO - training batch 1701, loss: 2.291, 54432/60000 datapoints
2025-03-07 18:42:08,533 - INFO - training batch 1751, loss: 2.306, 56032/60000 datapoints
2025-03-07 18:42:08,940 - INFO - training batch 1801, loss: 2.318, 57632/60000 datapoints
2025-03-07 18:42:09,351 - INFO - training batch 1851, loss: 2.287, 59232/60000 datapoints
2025-03-07 18:42:09,596 - INFO - validation batch 1, loss: 2.254, 32/10016 datapoints
2025-03-07 18:42:09,943 - INFO - validation batch 51, loss: 2.272, 1632/10016 datapoints
2025-03-07 18:42:10,312 - INFO - validation batch 101, loss: 2.311, 3232/10016 datapoints
2025-03-07 18:42:10,669 - INFO - validation batch 151, loss: 2.305, 4832/10016 datapoints
2025-03-07 18:42:11,006 - INFO - validation batch 201, loss: 2.284, 6432/10016 datapoints
2025-03-07 18:42:11,287 - INFO - validation batch 251, loss: 2.300, 8032/10016 datapoints
2025-03-07 18:42:11,556 - INFO - validation batch 301, loss: 2.259, 9632/10016 datapoints
2025-03-07 18:42:11,625 - INFO - Epoch 2/800 done.
2025-03-07 18:42:11,626 - INFO - Final validation performance:
Loss: 2.283, top-1 acc: 0.110top-5 acc: 0.110
2025-03-07 18:42:11,629 - INFO - Beginning epoch 3/800
2025-03-07 18:42:11,641 - INFO - training batch 1, loss: 2.274, 32/60000 datapoints
2025-03-07 18:42:11,998 - INFO - training batch 51, loss: 2.268, 1632/60000 datapoints
2025-03-07 18:42:12,309 - INFO - training batch 101, loss: 2.262, 3232/60000 datapoints
2025-03-07 18:42:12,619 - INFO - training batch 151, loss: 2.259, 4832/60000 datapoints
2025-03-07 18:42:12,939 - INFO - training batch 201, loss: 2.292, 6432/60000 datapoints
2025-03-07 18:42:13,238 - INFO - training batch 251, loss: 2.281, 8032/60000 datapoints
2025-03-07 18:42:13,555 - INFO - training batch 301, loss: 2.277, 9632/60000 datapoints
2025-03-07 18:42:13,878 - INFO - training batch 351, loss: 2.321, 11232/60000 datapoints
2025-03-07 18:42:14,233 - INFO - training batch 401, loss: 2.280, 12832/60000 datapoints
2025-03-07 18:42:14,585 - INFO - training batch 451, loss: 2.251, 14432/60000 datapoints
2025-03-07 18:42:14,978 - INFO - training batch 501, loss: 2.307, 16032/60000 datapoints
2025-03-07 18:42:15,287 - INFO - training batch 551, loss: 2.295, 17632/60000 datapoints
2025-03-07 18:42:15,576 - INFO - training batch 601, loss: 2.256, 19232/60000 datapoints
2025-03-07 18:42:15,866 - INFO - training batch 651, loss: 2.283, 20832/60000 datapoints
2025-03-07 18:42:16,172 - INFO - training batch 701, loss: 2.301, 22432/60000 datapoints
2025-03-07 18:42:16,525 - INFO - training batch 751, loss: 2.288, 24032/60000 datapoints
2025-03-07 18:42:16,931 - INFO - training batch 801, loss: 2.253, 25632/60000 datapoints
2025-03-07 18:42:17,307 - INFO - training batch 851, loss: 2.279, 27232/60000 datapoints
2025-03-07 18:42:17,649 - INFO - training batch 901, loss: 2.290, 28832/60000 datapoints
2025-03-07 18:42:18,041 - INFO - training batch 951, loss: 2.307, 30432/60000 datapoints
2025-03-07 18:42:18,396 - INFO - training batch 1001, loss: 2.302, 32032/60000 datapoints
2025-03-07 18:42:18,851 - INFO - training batch 1051, loss: 2.256, 33632/60000 datapoints
2025-03-07 18:42:19,204 - INFO - training batch 1101, loss: 2.280, 35232/60000 datapoints
2025-03-07 18:42:19,610 - INFO - training batch 1151, loss: 2.272, 36832/60000 datapoints
2025-03-07 18:42:20,023 - INFO - training batch 1201, loss: 2.281, 38432/60000 datapoints
2025-03-07 18:42:20,476 - INFO - training batch 1251, loss: 2.307, 40032/60000 datapoints
2025-03-07 18:42:21,657 - INFO - training batch 1301, loss: 2.259, 41632/60000 datapoints
2025-03-07 18:42:22,765 - INFO - training batch 1351, loss: 2.257, 43232/60000 datapoints
2025-03-07 18:42:23,732 - INFO - training batch 1401, loss: 2.268, 44832/60000 datapoints
2025-03-07 18:42:29,084 - INFO - training batch 1451, loss: 2.271, 46432/60000 datapoints
2025-03-07 18:42:32,811 - INFO - training batch 1501, loss: 2.295, 48032/60000 datapoints
2025-03-07 18:42:35,891 - INFO - training batch 1551, loss: 2.258, 49632/60000 datapoints
2025-03-07 18:42:36,693 - INFO - training batch 1601, loss: 2.257, 51232/60000 datapoints
2025-03-07 18:42:37,277 - INFO - training batch 1651, loss: 2.281, 52832/60000 datapoints
2025-03-07 18:42:37,969 - INFO - training batch 1701, loss: 2.255, 54432/60000 datapoints
2025-03-07 18:42:38,562 - INFO - training batch 1751, loss: 2.277, 56032/60000 datapoints
2025-03-07 18:42:39,144 - INFO - training batch 1801, loss: 2.253, 57632/60000 datapoints
2025-03-07 18:42:40,341 - INFO - training batch 1851, loss: 2.309, 59232/60000 datapoints
2025-03-07 18:42:41,269 - INFO - validation batch 1, loss: 2.282, 32/10016 datapoints
2025-03-07 18:42:42,467 - INFO - validation batch 51, loss: 2.261, 1632/10016 datapoints
2025-03-07 18:42:43,233 - INFO - validation batch 101, loss: 2.248, 3232/10016 datapoints
2025-03-07 18:42:43,797 - INFO - validation batch 151, loss: 2.259, 4832/10016 datapoints
2025-03-07 18:42:44,400 - INFO - validation batch 201, loss: 2.266, 6432/10016 datapoints
2025-03-07 18:42:44,950 - INFO - validation batch 251, loss: 2.270, 8032/10016 datapoints
2025-03-07 18:42:45,899 - INFO - validation batch 301, loss: 2.266, 9632/10016 datapoints
2025-03-07 18:42:46,180 - INFO - Epoch 3/800 done.
2025-03-07 18:42:46,183 - INFO - Final validation performance:
Loss: 2.264, top-1 acc: 0.169top-5 acc: 0.169
2025-03-07 18:42:46,232 - INFO - Beginning epoch 4/800
2025-03-07 18:42:46,323 - INFO - training batch 1, loss: 2.250, 32/60000 datapoints
2025-03-07 18:42:49,407 - INFO - training batch 51, loss: 2.293, 1632/60000 datapoints
2025-03-07 18:42:51,063 - INFO - training batch 101, loss: 2.277, 3232/60000 datapoints
2025-03-07 18:42:51,667 - INFO - training batch 151, loss: 2.263, 4832/60000 datapoints
2025-03-07 18:42:52,652 - INFO - training batch 201, loss: 2.263, 6432/60000 datapoints
2025-03-07 18:42:53,635 - INFO - training batch 251, loss: 2.301, 8032/60000 datapoints
2025-03-07 18:42:55,169 - INFO - training batch 301, loss: 2.278, 9632/60000 datapoints
2025-03-07 18:42:55,810 - INFO - training batch 351, loss: 2.278, 11232/60000 datapoints
2025-03-07 18:42:56,771 - INFO - training batch 401, loss: 2.280, 12832/60000 datapoints
2025-03-07 18:42:57,484 - INFO - training batch 451, loss: 2.236, 14432/60000 datapoints
2025-03-07 18:42:58,129 - INFO - training batch 501, loss: 2.258, 16032/60000 datapoints
2025-03-07 18:42:58,450 - INFO - training batch 551, loss: 2.248, 17632/60000 datapoints
2025-03-07 18:42:58,747 - INFO - training batch 601, loss: 2.255, 19232/60000 datapoints
2025-03-07 18:42:59,089 - INFO - training batch 651, loss: 2.250, 20832/60000 datapoints
2025-03-07 18:42:59,460 - INFO - training batch 701, loss: 2.245, 22432/60000 datapoints
2025-03-07 18:42:59,790 - INFO - training batch 751, loss: 2.269, 24032/60000 datapoints
2025-03-07 18:43:00,158 - INFO - training batch 801, loss: 2.246, 25632/60000 datapoints
2025-03-07 18:43:00,518 - INFO - training batch 851, loss: 2.253, 27232/60000 datapoints
2025-03-07 18:43:00,854 - INFO - training batch 901, loss: 2.256, 28832/60000 datapoints
2025-03-07 18:43:01,205 - INFO - training batch 951, loss: 2.266, 30432/60000 datapoints
2025-03-07 18:43:01,508 - INFO - training batch 1001, loss: 2.250, 32032/60000 datapoints
2025-03-07 18:43:01,828 - INFO - training batch 1051, loss: 2.270, 33632/60000 datapoints
2025-03-07 18:43:02,160 - INFO - training batch 1101, loss: 2.237, 35232/60000 datapoints
2025-03-07 18:43:02,456 - INFO - training batch 1151, loss: 2.256, 36832/60000 datapoints
2025-03-07 18:43:02,743 - INFO - training batch 1201, loss: 2.280, 38432/60000 datapoints
2025-03-07 18:43:03,037 - INFO - training batch 1251, loss: 2.280, 40032/60000 datapoints
2025-03-07 18:43:03,351 - INFO - training batch 1301, loss: 2.256, 41632/60000 datapoints
2025-03-07 18:43:03,642 - INFO - training batch 1351, loss: 2.276, 43232/60000 datapoints
2025-03-07 18:43:03,952 - INFO - training batch 1401, loss: 2.296, 44832/60000 datapoints
2025-03-07 18:43:04,317 - INFO - training batch 1451, loss: 2.226, 46432/60000 datapoints
2025-03-07 18:43:04,695 - INFO - training batch 1501, loss: 2.285, 48032/60000 datapoints
2025-03-07 18:43:05,044 - INFO - training batch 1551, loss: 2.259, 49632/60000 datapoints
2025-03-07 18:43:05,482 - INFO - training batch 1601, loss: 2.254, 51232/60000 datapoints
2025-03-07 18:43:05,871 - INFO - training batch 1651, loss: 2.250, 52832/60000 datapoints
2025-03-07 18:43:06,253 - INFO - training batch 1701, loss: 2.238, 54432/60000 datapoints
2025-03-07 18:43:06,631 - INFO - training batch 1751, loss: 2.267, 56032/60000 datapoints
2025-03-07 18:43:07,004 - INFO - training batch 1801, loss: 2.255, 57632/60000 datapoints
2025-03-07 18:43:07,415 - INFO - training batch 1851, loss: 2.240, 59232/60000 datapoints
2025-03-07 18:43:07,693 - INFO - validation batch 1, loss: 2.266, 32/10016 datapoints
2025-03-07 18:43:08,150 - INFO - validation batch 51, loss: 2.231, 1632/10016 datapoints
2025-03-07 18:43:08,482 - INFO - validation batch 101, loss: 2.285, 3232/10016 datapoints
2025-03-07 18:43:08,779 - INFO - validation batch 151, loss: 2.242, 4832/10016 datapoints
2025-03-07 18:43:09,102 - INFO - validation batch 201, loss: 2.261, 6432/10016 datapoints
2025-03-07 18:43:09,408 - INFO - validation batch 251, loss: 2.270, 8032/10016 datapoints
2025-03-07 18:43:09,721 - INFO - validation batch 301, loss: 2.281, 9632/10016 datapoints
2025-03-07 18:43:09,798 - INFO - Epoch 4/800 done.
2025-03-07 18:43:09,819 - INFO - Final validation performance:
Loss: 2.262, top-1 acc: 0.211top-5 acc: 0.211
2025-03-07 18:43:09,823 - INFO - Beginning epoch 5/800
2025-03-07 18:43:09,837 - INFO - training batch 1, loss: 2.256, 32/60000 datapoints
2025-03-07 18:43:10,214 - INFO - training batch 51, loss: 2.276, 1632/60000 datapoints
2025-03-07 18:43:10,598 - INFO - training batch 101, loss: 2.279, 3232/60000 datapoints
2025-03-07 18:43:10,963 - INFO - training batch 151, loss: 2.216, 4832/60000 datapoints
2025-03-07 18:43:11,334 - INFO - training batch 201, loss: 2.305, 6432/60000 datapoints
2025-03-07 18:43:11,740 - INFO - training batch 251, loss: 2.238, 8032/60000 datapoints
2025-03-07 18:43:12,139 - INFO - training batch 301, loss: 2.267, 9632/60000 datapoints
2025-03-07 18:43:12,665 - INFO - training batch 351, loss: 2.248, 11232/60000 datapoints
2025-03-07 18:43:13,127 - INFO - training batch 401, loss: 2.262, 12832/60000 datapoints
2025-03-07 18:43:13,517 - INFO - training batch 451, loss: 2.228, 14432/60000 datapoints
2025-03-07 18:43:13,911 - INFO - training batch 501, loss: 2.228, 16032/60000 datapoints
2025-03-07 18:43:14,319 - INFO - training batch 551, loss: 2.236, 17632/60000 datapoints
2025-03-07 18:43:14,697 - INFO - training batch 601, loss: 2.255, 19232/60000 datapoints
2025-03-07 18:43:15,098 - INFO - training batch 651, loss: 2.249, 20832/60000 datapoints
2025-03-07 18:43:15,427 - INFO - training batch 701, loss: 2.223, 22432/60000 datapoints
2025-03-07 18:43:15,774 - INFO - training batch 751, loss: 2.246, 24032/60000 datapoints
2025-03-07 18:43:16,113 - INFO - training batch 801, loss: 2.257, 25632/60000 datapoints
2025-03-07 18:43:16,434 - INFO - training batch 851, loss: 2.257, 27232/60000 datapoints
2025-03-07 18:43:16,745 - INFO - training batch 901, loss: 2.237, 28832/60000 datapoints
2025-03-07 18:43:17,068 - INFO - training batch 951, loss: 2.247, 30432/60000 datapoints
2025-03-07 18:43:17,411 - INFO - training batch 1001, loss: 2.225, 32032/60000 datapoints
2025-03-07 18:43:17,758 - INFO - training batch 1051, loss: 2.277, 33632/60000 datapoints
2025-03-07 18:43:18,089 - INFO - training batch 1101, loss: 2.248, 35232/60000 datapoints
2025-03-07 18:43:18,426 - INFO - training batch 1151, loss: 2.229, 36832/60000 datapoints
2025-03-07 18:43:18,745 - INFO - training batch 1201, loss: 2.240, 38432/60000 datapoints
2025-03-07 18:43:19,060 - INFO - training batch 1251, loss: 2.263, 40032/60000 datapoints
2025-03-07 18:43:19,352 - INFO - training batch 1301, loss: 2.296, 41632/60000 datapoints
2025-03-07 18:43:19,652 - INFO - training batch 1351, loss: 2.280, 43232/60000 datapoints
2025-03-07 18:43:19,935 - INFO - training batch 1401, loss: 2.270, 44832/60000 datapoints
2025-03-07 18:43:20,212 - INFO - training batch 1451, loss: 2.205, 46432/60000 datapoints
2025-03-07 18:43:20,596 - INFO - training batch 1501, loss: 2.254, 48032/60000 datapoints
2025-03-07 18:43:21,070 - INFO - training batch 1551, loss: 2.234, 49632/60000 datapoints
2025-03-07 18:43:21,467 - INFO - training batch 1601, loss: 2.205, 51232/60000 datapoints
2025-03-07 18:43:21,979 - INFO - training batch 1651, loss: 2.222, 52832/60000 datapoints
2025-03-07 18:43:22,278 - INFO - training batch 1701, loss: 2.236, 54432/60000 datapoints
2025-03-07 18:43:22,606 - INFO - training batch 1751, loss: 2.232, 56032/60000 datapoints
2025-03-07 18:43:22,964 - INFO - training batch 1801, loss: 2.245, 57632/60000 datapoints
2025-03-07 18:43:23,246 - INFO - training batch 1851, loss: 2.250, 59232/60000 datapoints
2025-03-07 18:43:23,420 - INFO - validation batch 1, loss: 2.226, 32/10016 datapoints
2025-03-07 18:43:23,655 - INFO - validation batch 51, loss: 2.225, 1632/10016 datapoints
2025-03-07 18:43:23,979 - INFO - validation batch 101, loss: 2.229, 3232/10016 datapoints
2025-03-07 18:43:24,280 - INFO - validation batch 151, loss: 2.238, 4832/10016 datapoints
2025-03-07 18:43:24,551 - INFO - validation batch 201, loss: 2.258, 6432/10016 datapoints
2025-03-07 18:43:24,796 - INFO - validation batch 251, loss: 2.215, 8032/10016 datapoints
2025-03-07 18:43:25,038 - INFO - validation batch 301, loss: 2.225, 9632/10016 datapoints
2025-03-07 18:43:25,091 - INFO - Epoch 5/800 done.
2025-03-07 18:43:25,091 - INFO - Final validation performance:
Loss: 2.231, top-1 acc: 0.245top-5 acc: 0.245
2025-03-07 18:43:25,092 - INFO - Beginning epoch 6/800
2025-03-07 18:43:25,102 - INFO - training batch 1, loss: 2.230, 32/60000 datapoints
2025-03-07 18:43:25,395 - INFO - training batch 51, loss: 2.241, 1632/60000 datapoints
2025-03-07 18:43:25,794 - INFO - training batch 101, loss: 2.209, 3232/60000 datapoints
2025-03-07 18:43:26,185 - INFO - training batch 151, loss: 2.258, 4832/60000 datapoints
2025-03-07 18:43:26,632 - INFO - training batch 201, loss: 2.234, 6432/60000 datapoints
2025-03-07 18:43:27,290 - INFO - training batch 251, loss: 2.218, 8032/60000 datapoints
2025-03-07 18:43:28,162 - INFO - training batch 301, loss: 2.281, 9632/60000 datapoints
2025-03-07 18:43:29,488 - INFO - training batch 351, loss: 2.227, 11232/60000 datapoints
2025-03-07 18:43:30,384 - INFO - training batch 401, loss: 2.249, 12832/60000 datapoints
2025-03-07 18:43:30,798 - INFO - training batch 451, loss: 2.235, 14432/60000 datapoints
2025-03-07 18:43:31,376 - INFO - training batch 501, loss: 2.239, 16032/60000 datapoints
2025-03-07 18:43:32,468 - INFO - training batch 551, loss: 2.214, 17632/60000 datapoints
2025-03-07 18:43:35,188 - INFO - training batch 601, loss: 2.234, 19232/60000 datapoints
2025-03-07 18:43:36,033 - INFO - training batch 651, loss: 2.232, 20832/60000 datapoints
2025-03-07 18:43:36,506 - INFO - training batch 701, loss: 2.209, 22432/60000 datapoints
2025-03-07 18:43:36,946 - INFO - training batch 751, loss: 2.259, 24032/60000 datapoints
2025-03-07 18:43:37,348 - INFO - training batch 801, loss: 2.206, 25632/60000 datapoints
2025-03-07 18:43:37,934 - INFO - training batch 851, loss: 2.228, 27232/60000 datapoints
2025-03-07 18:43:38,392 - INFO - training batch 901, loss: 2.222, 28832/60000 datapoints
2025-03-07 18:43:38,795 - INFO - training batch 951, loss: 2.254, 30432/60000 datapoints
2025-03-07 18:43:39,179 - INFO - training batch 1001, loss: 2.225, 32032/60000 datapoints
2025-03-07 18:43:39,741 - INFO - training batch 1051, loss: 2.208, 33632/60000 datapoints
2025-03-07 18:43:40,229 - INFO - training batch 1101, loss: 2.234, 35232/60000 datapoints
2025-03-07 18:43:40,682 - INFO - training batch 1151, loss: 2.249, 36832/60000 datapoints
2025-03-07 18:43:41,105 - INFO - training batch 1201, loss: 2.222, 38432/60000 datapoints
2025-03-07 18:43:41,557 - INFO - training batch 1251, loss: 2.214, 40032/60000 datapoints
2025-03-07 18:43:41,992 - INFO - training batch 1301, loss: 2.243, 41632/60000 datapoints
2025-03-07 18:43:42,417 - INFO - training batch 1351, loss: 2.236, 43232/60000 datapoints
2025-03-07 18:43:42,924 - INFO - training batch 1401, loss: 2.248, 44832/60000 datapoints
2025-03-07 18:43:43,646 - INFO - training batch 1451, loss: 2.265, 46432/60000 datapoints
2025-03-07 18:43:45,207 - INFO - training batch 1501, loss: 2.241, 48032/60000 datapoints
2025-03-07 18:43:46,263 - INFO - training batch 1551, loss: 2.212, 49632/60000 datapoints
2025-03-07 18:43:47,337 - INFO - training batch 1601, loss: 2.240, 51232/60000 datapoints
2025-03-07 18:43:49,298 - INFO - training batch 1651, loss: 2.235, 52832/60000 datapoints
2025-03-07 18:43:50,947 - INFO - training batch 1701, loss: 2.223, 54432/60000 datapoints
2025-03-07 18:43:51,530 - INFO - training batch 1751, loss: 2.221, 56032/60000 datapoints
2025-03-07 18:43:53,978 - INFO - training batch 1801, loss: 2.221, 57632/60000 datapoints
2025-03-07 18:43:54,861 - INFO - training batch 1851, loss: 2.230, 59232/60000 datapoints
2025-03-07 18:43:55,204 - INFO - validation batch 1, loss: 2.190, 32/10016 datapoints
2025-03-07 18:43:56,256 - INFO - validation batch 51, loss: 2.246, 1632/10016 datapoints
2025-03-07 18:43:56,865 - INFO - validation batch 101, loss: 2.183, 3232/10016 datapoints
2025-03-07 18:43:57,261 - INFO - validation batch 151, loss: 2.190, 4832/10016 datapoints
2025-03-07 18:43:57,732 - INFO - validation batch 201, loss: 2.218, 6432/10016 datapoints
2025-03-07 18:43:58,332 - INFO - validation batch 251, loss: 2.190, 8032/10016 datapoints
2025-03-07 18:43:58,868 - INFO - validation batch 301, loss: 2.208, 9632/10016 datapoints
2025-03-07 18:43:58,963 - INFO - Epoch 6/800 done.
2025-03-07 18:43:58,965 - INFO - Final validation performance:
Loss: 2.204, top-1 acc: 0.278top-5 acc: 0.278
2025-03-07 18:43:58,971 - INFO - Beginning epoch 7/800
2025-03-07 18:43:58,985 - INFO - training batch 1, loss: 2.204, 32/60000 datapoints
2025-03-07 18:43:59,510 - INFO - training batch 51, loss: 2.190, 1632/60000 datapoints
2025-03-07 18:44:00,015 - INFO - training batch 101, loss: 2.230, 3232/60000 datapoints
2025-03-07 18:44:00,777 - INFO - training batch 151, loss: 2.208, 4832/60000 datapoints
2025-03-07 18:44:01,533 - INFO - training batch 201, loss: 2.204, 6432/60000 datapoints
2025-03-07 18:44:02,023 - INFO - training batch 251, loss: 2.182, 8032/60000 datapoints
2025-03-07 18:44:02,461 - INFO - training batch 301, loss: 2.247, 9632/60000 datapoints
2025-03-07 18:44:02,894 - INFO - training batch 351, loss: 2.190, 11232/60000 datapoints
2025-03-07 18:44:03,358 - INFO - training batch 401, loss: 2.219, 12832/60000 datapoints
2025-03-07 18:44:03,804 - INFO - training batch 451, loss: 2.159, 14432/60000 datapoints
2025-03-07 18:44:04,168 - INFO - training batch 501, loss: 2.209, 16032/60000 datapoints
2025-03-07 18:44:04,490 - INFO - training batch 551, loss: 2.212, 17632/60000 datapoints
2025-03-07 18:44:04,827 - INFO - training batch 601, loss: 2.258, 19232/60000 datapoints
2025-03-07 18:44:05,221 - INFO - training batch 651, loss: 2.198, 20832/60000 datapoints
2025-03-07 18:44:05,593 - INFO - training batch 701, loss: 2.209, 22432/60000 datapoints
2025-03-07 18:44:05,967 - INFO - training batch 751, loss: 2.204, 24032/60000 datapoints
2025-03-07 18:44:06,369 - INFO - training batch 801, loss: 2.214, 25632/60000 datapoints
2025-03-07 18:44:06,776 - INFO - training batch 851, loss: 2.200, 27232/60000 datapoints
2025-03-07 18:44:07,140 - INFO - training batch 901, loss: 2.267, 28832/60000 datapoints
2025-03-07 18:44:07,509 - INFO - training batch 951, loss: 2.181, 30432/60000 datapoints
2025-03-07 18:44:07,964 - INFO - training batch 1001, loss: 2.233, 32032/60000 datapoints
2025-03-07 18:44:08,659 - INFO - training batch 1051, loss: 2.138, 33632/60000 datapoints
2025-03-07 18:44:09,287 - INFO - training batch 1101, loss: 2.207, 35232/60000 datapoints
2025-03-07 18:44:09,875 - INFO - training batch 1151, loss: 2.198, 36832/60000 datapoints
2025-03-07 18:44:10,508 - INFO - training batch 1201, loss: 2.190, 38432/60000 datapoints
2025-03-07 18:44:11,020 - INFO - training batch 1251, loss: 2.234, 40032/60000 datapoints
2025-03-07 18:44:11,511 - INFO - training batch 1301, loss: 2.174, 41632/60000 datapoints
2025-03-07 18:44:11,941 - INFO - training batch 1351, loss: 2.210, 43232/60000 datapoints
2025-03-07 18:44:12,390 - INFO - training batch 1401, loss: 2.213, 44832/60000 datapoints
2025-03-07 18:44:12,772 - INFO - training batch 1451, loss: 2.200, 46432/60000 datapoints
2025-03-07 18:44:13,166 - INFO - training batch 1501, loss: 2.173, 48032/60000 datapoints
2025-03-07 18:44:13,573 - INFO - training batch 1551, loss: 2.226, 49632/60000 datapoints
2025-03-07 18:44:13,992 - INFO - training batch 1601, loss: 2.205, 51232/60000 datapoints
2025-03-07 18:44:14,359 - INFO - training batch 1651, loss: 2.199, 52832/60000 datapoints
2025-03-07 18:44:14,683 - INFO - training batch 1701, loss: 2.206, 54432/60000 datapoints
2025-03-07 18:44:15,029 - INFO - training batch 1751, loss: 2.228, 56032/60000 datapoints
2025-03-07 18:44:15,338 - INFO - training batch 1801, loss: 2.223, 57632/60000 datapoints
2025-03-07 18:44:15,716 - INFO - training batch 1851, loss: 2.166, 59232/60000 datapoints
2025-03-07 18:44:15,876 - INFO - validation batch 1, loss: 2.171, 32/10016 datapoints
2025-03-07 18:44:16,130 - INFO - validation batch 51, loss: 2.219, 1632/10016 datapoints
2025-03-07 18:44:16,371 - INFO - validation batch 101, loss: 2.175, 3232/10016 datapoints
2025-03-07 18:44:16,611 - INFO - validation batch 151, loss: 2.197, 4832/10016 datapoints
2025-03-07 18:44:16,895 - INFO - validation batch 201, loss: 2.180, 6432/10016 datapoints
2025-03-07 18:44:17,189 - INFO - validation batch 251, loss: 2.195, 8032/10016 datapoints
2025-03-07 18:44:17,473 - INFO - validation batch 301, loss: 2.222, 9632/10016 datapoints
2025-03-07 18:44:17,548 - INFO - Epoch 7/800 done.
2025-03-07 18:44:17,548 - INFO - Final validation performance:
Loss: 2.194, top-1 acc: 0.315top-5 acc: 0.315
2025-03-07 18:44:17,549 - INFO - Beginning epoch 8/800
2025-03-07 18:44:17,559 - INFO - training batch 1, loss: 2.159, 32/60000 datapoints
2025-03-07 18:44:17,928 - INFO - training batch 51, loss: 2.165, 1632/60000 datapoints
2025-03-07 18:44:18,285 - INFO - training batch 101, loss: 2.209, 3232/60000 datapoints
2025-03-07 18:44:18,667 - INFO - training batch 151, loss: 2.159, 4832/60000 datapoints
2025-03-07 18:44:19,527 - INFO - training batch 201, loss: 2.203, 6432/60000 datapoints
2025-03-07 18:44:20,343 - INFO - training batch 251, loss: 2.130, 8032/60000 datapoints
2025-03-07 18:44:20,767 - INFO - training batch 301, loss: 2.178, 9632/60000 datapoints
2025-03-07 18:44:21,151 - INFO - training batch 351, loss: 2.183, 11232/60000 datapoints
2025-03-07 18:44:21,641 - INFO - training batch 401, loss: 2.219, 12832/60000 datapoints
2025-03-07 18:44:22,230 - INFO - training batch 451, loss: 2.212, 14432/60000 datapoints
2025-03-07 18:44:22,943 - INFO - training batch 501, loss: 2.208, 16032/60000 datapoints
2025-03-07 18:44:23,704 - INFO - training batch 551, loss: 2.180, 17632/60000 datapoints
2025-03-07 18:44:24,108 - INFO - training batch 601, loss: 2.196, 19232/60000 datapoints
2025-03-07 18:44:24,561 - INFO - training batch 651, loss: 2.233, 20832/60000 datapoints
2025-03-07 18:44:25,033 - INFO - training batch 701, loss: 2.175, 22432/60000 datapoints
2025-03-07 18:44:25,567 - INFO - training batch 751, loss: 2.220, 24032/60000 datapoints
2025-03-07 18:44:26,001 - INFO - training batch 801, loss: 2.213, 25632/60000 datapoints
2025-03-07 18:44:26,455 - INFO - training batch 851, loss: 2.205, 27232/60000 datapoints
2025-03-07 18:44:26,862 - INFO - training batch 901, loss: 2.221, 28832/60000 datapoints
2025-03-07 18:44:27,245 - INFO - training batch 951, loss: 2.198, 30432/60000 datapoints
2025-03-07 18:44:27,551 - INFO - training batch 1001, loss: 2.177, 32032/60000 datapoints
2025-03-07 18:44:27,856 - INFO - training batch 1051, loss: 2.207, 33632/60000 datapoints
2025-03-07 18:44:28,224 - INFO - training batch 1101, loss: 2.166, 35232/60000 datapoints
2025-03-07 18:44:28,523 - INFO - training batch 1151, loss: 2.193, 36832/60000 datapoints
2025-03-07 18:44:28,812 - INFO - training batch 1201, loss: 2.152, 38432/60000 datapoints
2025-03-07 18:44:29,200 - INFO - training batch 1251, loss: 2.218, 40032/60000 datapoints
2025-03-07 18:44:29,545 - INFO - training batch 1301, loss: 2.196, 41632/60000 datapoints
2025-03-07 18:44:29,901 - INFO - training batch 1351, loss: 2.162, 43232/60000 datapoints
2025-03-07 18:44:30,258 - INFO - training batch 1401, loss: 2.159, 44832/60000 datapoints
2025-03-07 18:44:30,556 - INFO - training batch 1451, loss: 2.181, 46432/60000 datapoints
2025-03-07 18:44:30,860 - INFO - training batch 1501, loss: 2.212, 48032/60000 datapoints
2025-03-07 18:44:31,281 - INFO - training batch 1551, loss: 2.214, 49632/60000 datapoints
2025-03-07 18:44:31,642 - INFO - training batch 1601, loss: 2.180, 51232/60000 datapoints
2025-03-07 18:44:31,981 - INFO - training batch 1651, loss: 2.193, 52832/60000 datapoints
2025-03-07 18:44:32,337 - INFO - training batch 1701, loss: 2.214, 54432/60000 datapoints
2025-03-07 18:44:32,715 - INFO - training batch 1751, loss: 2.180, 56032/60000 datapoints
2025-03-07 18:44:33,158 - INFO - training batch 1801, loss: 2.176, 57632/60000 datapoints
2025-03-07 18:44:33,690 - INFO - training batch 1851, loss: 2.216, 59232/60000 datapoints
2025-03-07 18:44:33,964 - INFO - validation batch 1, loss: 2.202, 32/10016 datapoints
2025-03-07 18:44:34,311 - INFO - validation batch 51, loss: 2.163, 1632/10016 datapoints
2025-03-07 18:44:34,593 - INFO - validation batch 101, loss: 2.187, 3232/10016 datapoints
2025-03-07 18:44:34,904 - INFO - validation batch 151, loss: 2.193, 4832/10016 datapoints
2025-03-07 18:44:35,251 - INFO - validation batch 201, loss: 2.229, 6432/10016 datapoints
2025-03-07 18:44:35,541 - INFO - validation batch 251, loss: 2.167, 8032/10016 datapoints
2025-03-07 18:44:35,820 - INFO - validation batch 301, loss: 2.169, 9632/10016 datapoints
2025-03-07 18:44:35,873 - INFO - Epoch 8/800 done.
2025-03-07 18:44:35,874 - INFO - Final validation performance:
Loss: 2.187, top-1 acc: 0.360top-5 acc: 0.360
2025-03-07 18:44:35,878 - INFO - Beginning epoch 9/800
2025-03-07 18:44:35,893 - INFO - training batch 1, loss: 2.141, 32/60000 datapoints
2025-03-07 18:44:36,190 - INFO - training batch 51, loss: 2.185, 1632/60000 datapoints
2025-03-07 18:44:36,499 - INFO - training batch 101, loss: 2.190, 3232/60000 datapoints
2025-03-07 18:44:36,815 - INFO - training batch 151, loss: 2.188, 4832/60000 datapoints
2025-03-07 18:44:37,172 - INFO - training batch 201, loss: 2.164, 6432/60000 datapoints
2025-03-07 18:44:37,493 - INFO - training batch 251, loss: 2.141, 8032/60000 datapoints
2025-03-07 18:44:37,957 - INFO - training batch 301, loss: 2.144, 9632/60000 datapoints
2025-03-07 18:44:38,397 - INFO - training batch 351, loss: 2.203, 11232/60000 datapoints
2025-03-07 18:44:38,850 - INFO - training batch 401, loss: 2.183, 12832/60000 datapoints
2025-03-07 18:44:39,326 - INFO - training batch 451, loss: 2.176, 14432/60000 datapoints
2025-03-07 18:44:40,089 - INFO - training batch 501, loss: 2.159, 16032/60000 datapoints
2025-03-07 18:44:40,770 - INFO - training batch 551, loss: 2.213, 17632/60000 datapoints
2025-03-07 18:44:41,499 - INFO - training batch 601, loss: 2.164, 19232/60000 datapoints
2025-03-07 18:44:41,942 - INFO - training batch 651, loss: 2.148, 20832/60000 datapoints
2025-03-07 18:44:42,385 - INFO - training batch 701, loss: 2.187, 22432/60000 datapoints
2025-03-07 18:44:42,755 - INFO - training batch 751, loss: 2.170, 24032/60000 datapoints
2025-03-07 18:44:43,098 - INFO - training batch 801, loss: 2.152, 25632/60000 datapoints
2025-03-07 18:44:43,417 - INFO - training batch 851, loss: 2.163, 27232/60000 datapoints
2025-03-07 18:44:43,742 - INFO - training batch 901, loss: 2.149, 28832/60000 datapoints
2025-03-07 18:44:44,072 - INFO - training batch 951, loss: 2.179, 30432/60000 datapoints
2025-03-07 18:44:44,401 - INFO - training batch 1001, loss: 2.130, 32032/60000 datapoints
2025-03-07 18:44:44,736 - INFO - training batch 1051, loss: 2.179, 33632/60000 datapoints
2025-03-07 18:44:45,071 - INFO - training batch 1101, loss: 2.168, 35232/60000 datapoints
2025-03-07 18:44:45,407 - INFO - training batch 1151, loss: 2.193, 36832/60000 datapoints
2025-03-07 18:44:45,741 - INFO - training batch 1201, loss: 2.153, 38432/60000 datapoints
2025-03-07 18:44:46,062 - INFO - training batch 1251, loss: 2.143, 40032/60000 datapoints
2025-03-07 18:44:46,433 - INFO - training batch 1301, loss: 2.118, 41632/60000 datapoints
2025-03-07 18:44:46,766 - INFO - training batch 1351, loss: 2.141, 43232/60000 datapoints
2025-03-07 18:44:47,087 - INFO - training batch 1401, loss: 2.155, 44832/60000 datapoints
2025-03-07 18:44:47,425 - INFO - training batch 1451, loss: 2.119, 46432/60000 datapoints
2025-03-07 18:44:47,769 - INFO - training batch 1501, loss: 2.159, 48032/60000 datapoints
2025-03-07 18:44:48,116 - INFO - training batch 1551, loss: 2.178, 49632/60000 datapoints
2025-03-07 18:44:48,441 - INFO - training batch 1601, loss: 2.152, 51232/60000 datapoints
2025-03-07 18:44:48,782 - INFO - training batch 1651, loss: 2.194, 52832/60000 datapoints
2025-03-07 18:44:49,144 - INFO - training batch 1701, loss: 2.143, 54432/60000 datapoints
2025-03-07 18:44:49,520 - INFO - training batch 1751, loss: 2.166, 56032/60000 datapoints
2025-03-07 18:44:49,874 - INFO - training batch 1801, loss: 2.181, 57632/60000 datapoints
2025-03-07 18:44:50,231 - INFO - training batch 1851, loss: 2.094, 59232/60000 datapoints
2025-03-07 18:44:50,412 - INFO - validation batch 1, loss: 2.182, 32/10016 datapoints
2025-03-07 18:44:50,697 - INFO - validation batch 51, loss: 2.172, 1632/10016 datapoints
2025-03-07 18:44:50,987 - INFO - validation batch 101, loss: 2.185, 3232/10016 datapoints
2025-03-07 18:44:51,330 - INFO - validation batch 151, loss: 2.172, 4832/10016 datapoints
2025-03-07 18:44:51,692 - INFO - validation batch 201, loss: 2.180, 6432/10016 datapoints
2025-03-07 18:44:51,990 - INFO - validation batch 251, loss: 2.102, 8032/10016 datapoints
2025-03-07 18:44:52,260 - INFO - validation batch 301, loss: 2.152, 9632/10016 datapoints
2025-03-07 18:44:52,323 - INFO - Epoch 9/800 done.
2025-03-07 18:44:52,324 - INFO - Final validation performance:
Loss: 2.164, top-1 acc: 0.411top-5 acc: 0.411
2025-03-07 18:44:52,326 - INFO - Beginning epoch 10/800
2025-03-07 18:44:52,338 - INFO - training batch 1, loss: 2.164, 32/60000 datapoints
2025-03-07 18:44:52,663 - INFO - training batch 51, loss: 2.158, 1632/60000 datapoints
2025-03-07 18:44:52,946 - INFO - training batch 101, loss: 2.160, 3232/60000 datapoints
2025-03-07 18:44:53,252 - INFO - training batch 151, loss: 2.181, 4832/60000 datapoints
2025-03-07 18:44:53,531 - INFO - training batch 201, loss: 2.125, 6432/60000 datapoints
2025-03-07 18:44:53,817 - INFO - training batch 251, loss: 2.152, 8032/60000 datapoints
2025-03-07 18:44:54,100 - INFO - training batch 301, loss: 2.163, 9632/60000 datapoints
2025-03-07 18:44:54,415 - INFO - training batch 351, loss: 2.131, 11232/60000 datapoints
2025-03-07 18:44:54,689 - INFO - training batch 401, loss: 2.172, 12832/60000 datapoints
2025-03-07 18:44:54,970 - INFO - training batch 451, loss: 2.150, 14432/60000 datapoints
2025-03-07 18:44:55,289 - INFO - training batch 501, loss: 2.131, 16032/60000 datapoints
2025-03-07 18:44:55,625 - INFO - training batch 551, loss: 2.127, 17632/60000 datapoints
2025-03-07 18:44:55,946 - INFO - training batch 601, loss: 2.144, 19232/60000 datapoints
2025-03-07 18:44:56,266 - INFO - training batch 651, loss: 2.149, 20832/60000 datapoints
2025-03-07 18:44:56,741 - INFO - training batch 701, loss: 2.132, 22432/60000 datapoints
2025-03-07 18:44:57,090 - INFO - training batch 751, loss: 2.129, 24032/60000 datapoints
2025-03-07 18:44:57,431 - INFO - training batch 801, loss: 2.179, 25632/60000 datapoints
2025-03-07 18:44:57,868 - INFO - training batch 851, loss: 2.196, 27232/60000 datapoints
2025-03-07 18:44:58,278 - INFO - training batch 901, loss: 2.162, 28832/60000 datapoints
2025-03-07 18:44:58,787 - INFO - training batch 951, loss: 2.176, 30432/60000 datapoints
2025-03-07 18:44:59,739 - INFO - training batch 1001, loss: 2.197, 32032/60000 datapoints
2025-03-07 18:45:00,485 - INFO - training batch 1051, loss: 2.156, 33632/60000 datapoints
2025-03-07 18:45:01,102 - INFO - training batch 1101, loss: 2.139, 35232/60000 datapoints
2025-03-07 18:45:01,537 - INFO - training batch 1151, loss: 2.117, 36832/60000 datapoints
2025-03-07 18:45:01,912 - INFO - training batch 1201, loss: 2.144, 38432/60000 datapoints
2025-03-07 18:45:02,304 - INFO - training batch 1251, loss: 2.159, 40032/60000 datapoints
2025-03-07 18:45:02,672 - INFO - training batch 1301, loss: 2.185, 41632/60000 datapoints
2025-03-07 18:45:03,056 - INFO - training batch 1351, loss: 2.117, 43232/60000 datapoints
2025-03-07 18:45:03,419 - INFO - training batch 1401, loss: 2.162, 44832/60000 datapoints
2025-03-07 18:45:03,876 - INFO - training batch 1451, loss: 2.133, 46432/60000 datapoints
2025-03-07 18:45:04,376 - INFO - training batch 1501, loss: 2.132, 48032/60000 datapoints
2025-03-07 18:45:04,719 - INFO - training batch 1551, loss: 2.109, 49632/60000 datapoints
2025-03-07 18:45:05,062 - INFO - training batch 1601, loss: 2.153, 51232/60000 datapoints
2025-03-07 18:45:05,365 - INFO - training batch 1651, loss: 2.113, 52832/60000 datapoints
2025-03-07 18:45:05,653 - INFO - training batch 1701, loss: 2.144, 54432/60000 datapoints
2025-03-07 18:45:05,995 - INFO - training batch 1751, loss: 2.076, 56032/60000 datapoints
2025-03-07 18:45:06,395 - INFO - training batch 1801, loss: 2.139, 57632/60000 datapoints
2025-03-07 18:45:07,977 - INFO - training batch 1851, loss: 2.113, 59232/60000 datapoints
2025-03-07 18:45:08,308 - INFO - validation batch 1, loss: 2.096, 32/10016 datapoints
2025-03-07 18:45:08,689 - INFO - validation batch 51, loss: 2.129, 1632/10016 datapoints
2025-03-07 18:45:09,000 - INFO - validation batch 101, loss: 2.116, 3232/10016 datapoints
2025-03-07 18:45:09,357 - INFO - validation batch 151, loss: 2.109, 4832/10016 datapoints
2025-03-07 18:45:09,766 - INFO - validation batch 201, loss: 2.151, 6432/10016 datapoints
2025-03-07 18:45:10,079 - INFO - validation batch 251, loss: 2.131, 8032/10016 datapoints
2025-03-07 18:45:10,372 - INFO - validation batch 301, loss: 2.149, 9632/10016 datapoints
2025-03-07 18:45:10,427 - INFO - Epoch 10/800 done.
2025-03-07 18:45:10,428 - INFO - Final validation performance:
Loss: 2.126, top-1 acc: 0.456top-5 acc: 0.456
2025-03-07 18:45:10,437 - INFO - Beginning epoch 11/800
2025-03-07 18:45:10,454 - INFO - training batch 1, loss: 2.132, 32/60000 datapoints
2025-03-07 18:45:10,834 - INFO - training batch 51, loss: 2.162, 1632/60000 datapoints
2025-03-07 18:45:11,188 - INFO - training batch 101, loss: 2.163, 3232/60000 datapoints
2025-03-07 18:45:11,492 - INFO - training batch 151, loss: 2.115, 4832/60000 datapoints
2025-03-07 18:45:11,807 - INFO - training batch 201, loss: 2.170, 6432/60000 datapoints
2025-03-07 18:45:12,139 - INFO - training batch 251, loss: 2.137, 8032/60000 datapoints
2025-03-07 18:45:12,439 - INFO - training batch 301, loss: 2.136, 9632/60000 datapoints
2025-03-07 18:45:12,746 - INFO - training batch 351, loss: 2.088, 11232/60000 datapoints
2025-03-07 18:45:13,050 - INFO - training batch 401, loss: 2.108, 12832/60000 datapoints
2025-03-07 18:45:13,384 - INFO - training batch 451, loss: 2.049, 14432/60000 datapoints
2025-03-07 18:45:13,720 - INFO - training batch 501, loss: 2.157, 16032/60000 datapoints
2025-03-07 18:45:14,094 - INFO - training batch 551, loss: 2.135, 17632/60000 datapoints
2025-03-07 18:45:14,505 - INFO - training batch 601, loss: 2.122, 19232/60000 datapoints
2025-03-07 18:45:14,907 - INFO - training batch 651, loss: 2.153, 20832/60000 datapoints
2025-03-07 18:45:15,222 - INFO - training batch 701, loss: 2.170, 22432/60000 datapoints
2025-03-07 18:45:15,684 - INFO - training batch 751, loss: 2.065, 24032/60000 datapoints
2025-03-07 18:45:16,759 - INFO - training batch 801, loss: 2.091, 25632/60000 datapoints
2025-03-07 18:45:17,573 - INFO - training batch 851, loss: 2.161, 27232/60000 datapoints
2025-03-07 18:45:18,003 - INFO - training batch 901, loss: 2.171, 28832/60000 datapoints
2025-03-07 18:45:18,494 - INFO - training batch 951, loss: 2.110, 30432/60000 datapoints
2025-03-07 18:45:19,042 - INFO - training batch 1001, loss: 2.145, 32032/60000 datapoints
2025-03-07 18:45:19,436 - INFO - training batch 1051, loss: 2.105, 33632/60000 datapoints
2025-03-07 18:45:19,758 - INFO - training batch 1101, loss: 2.111, 35232/60000 datapoints
2025-03-07 18:45:20,121 - INFO - training batch 1151, loss: 2.107, 36832/60000 datapoints
2025-03-07 18:45:20,429 - INFO - training batch 1201, loss: 2.111, 38432/60000 datapoints
2025-03-07 18:45:20,754 - INFO - training batch 1251, loss: 2.122, 40032/60000 datapoints
2025-03-07 18:45:21,047 - INFO - training batch 1301, loss: 2.156, 41632/60000 datapoints
2025-03-07 18:45:21,377 - INFO - training batch 1351, loss: 2.128, 43232/60000 datapoints
2025-03-07 18:45:21,700 - INFO - training batch 1401, loss: 2.088, 44832/60000 datapoints
2025-03-07 18:45:22,033 - INFO - training batch 1451, loss: 2.165, 46432/60000 datapoints
2025-03-07 18:45:22,322 - INFO - training batch 1501, loss: 2.102, 48032/60000 datapoints
2025-03-07 18:45:22,614 - INFO - training batch 1551, loss: 2.123, 49632/60000 datapoints
2025-03-07 18:45:22,915 - INFO - training batch 1601, loss: 2.084, 51232/60000 datapoints
2025-03-07 18:45:23,771 - INFO - training batch 1651, loss: 2.108, 52832/60000 datapoints
2025-03-07 18:45:25,548 - INFO - training batch 1701, loss: 2.081, 54432/60000 datapoints
2025-03-07 18:45:26,714 - INFO - training batch 1751, loss: 2.062, 56032/60000 datapoints
2025-03-07 18:45:27,177 - INFO - training batch 1801, loss: 2.134, 57632/60000 datapoints
2025-03-07 18:45:27,621 - INFO - training batch 1851, loss: 2.179, 59232/60000 datapoints
2025-03-07 18:45:27,956 - INFO - validation batch 1, loss: 2.117, 32/10016 datapoints
2025-03-07 18:45:28,382 - INFO - validation batch 51, loss: 2.116, 1632/10016 datapoints
2025-03-07 18:45:28,772 - INFO - validation batch 101, loss: 2.113, 3232/10016 datapoints
2025-03-07 18:45:29,189 - INFO - validation batch 151, loss: 2.162, 4832/10016 datapoints
2025-03-07 18:45:29,572 - INFO - validation batch 201, loss: 2.086, 6432/10016 datapoints
2025-03-07 18:45:30,159 - INFO - validation batch 251, loss: 2.143, 8032/10016 datapoints
2025-03-07 18:45:30,740 - INFO - validation batch 301, loss: 2.086, 9632/10016 datapoints
2025-03-07 18:45:30,878 - INFO - Epoch 11/800 done.
2025-03-07 18:45:30,879 - INFO - Final validation performance:
Loss: 2.117, top-1 acc: 0.490top-5 acc: 0.490
2025-03-07 18:45:30,973 - INFO - Beginning epoch 12/800
2025-03-07 18:45:31,077 - INFO - training batch 1, loss: 2.078, 32/60000 datapoints
2025-03-07 18:45:31,596 - INFO - training batch 51, loss: 2.085, 1632/60000 datapoints
2025-03-07 18:45:31,996 - INFO - training batch 101, loss: 2.103, 3232/60000 datapoints
2025-03-07 18:45:32,425 - INFO - training batch 151, loss: 2.085, 4832/60000 datapoints
2025-03-07 18:45:32,989 - INFO - training batch 201, loss: 2.144, 6432/60000 datapoints
2025-03-07 18:45:33,379 - INFO - training batch 251, loss: 2.105, 8032/60000 datapoints
2025-03-07 18:45:33,741 - INFO - training batch 301, loss: 2.037, 9632/60000 datapoints
2025-03-07 18:45:34,088 - INFO - training batch 351, loss: 2.049, 11232/60000 datapoints
2025-03-07 18:45:34,432 - INFO - training batch 401, loss: 2.065, 12832/60000 datapoints
2025-03-07 18:45:34,736 - INFO - training batch 451, loss: 2.062, 14432/60000 datapoints
2025-03-07 18:45:35,629 - INFO - training batch 501, loss: 2.120, 16032/60000 datapoints
2025-03-07 18:45:36,076 - INFO - training batch 551, loss: 2.115, 17632/60000 datapoints
2025-03-07 18:45:36,543 - INFO - training batch 601, loss: 2.084, 19232/60000 datapoints
2025-03-07 18:45:37,056 - INFO - training batch 651, loss: 2.040, 20832/60000 datapoints
2025-03-07 18:45:37,515 - INFO - training batch 701, loss: 2.139, 22432/60000 datapoints
2025-03-07 18:45:37,935 - INFO - training batch 751, loss: 2.111, 24032/60000 datapoints
2025-03-07 18:45:38,784 - INFO - training batch 801, loss: 2.095, 25632/60000 datapoints
2025-03-07 18:45:41,288 - INFO - training batch 851, loss: 2.074, 27232/60000 datapoints
2025-03-07 18:45:42,574 - INFO - training batch 901, loss: 2.089, 28832/60000 datapoints
2025-03-07 18:45:43,036 - INFO - training batch 951, loss: 2.094, 30432/60000 datapoints
2025-03-07 18:45:43,733 - INFO - training batch 1001, loss: 2.102, 32032/60000 datapoints
2025-03-07 18:45:44,438 - INFO - training batch 1051, loss: 2.081, 33632/60000 datapoints
2025-03-07 18:45:44,852 - INFO - training batch 1101, loss: 2.029, 35232/60000 datapoints
2025-03-07 18:45:45,274 - INFO - training batch 1151, loss: 2.101, 36832/60000 datapoints
2025-03-07 18:45:45,663 - INFO - training batch 1201, loss: 2.085, 38432/60000 datapoints
2025-03-07 18:45:46,012 - INFO - training batch 1251, loss: 2.019, 40032/60000 datapoints
2025-03-07 18:45:46,334 - INFO - training batch 1301, loss: 2.069, 41632/60000 datapoints
2025-03-07 18:45:46,730 - INFO - training batch 1351, loss: 2.121, 43232/60000 datapoints
2025-03-07 18:45:47,182 - INFO - training batch 1401, loss: 2.067, 44832/60000 datapoints
2025-03-07 18:45:47,745 - INFO - training batch 1451, loss: 2.116, 46432/60000 datapoints
2025-03-07 18:45:49,140 - INFO - training batch 1501, loss: 2.118, 48032/60000 datapoints
2025-03-07 18:45:49,865 - INFO - training batch 1551, loss: 2.045, 49632/60000 datapoints
2025-03-07 18:45:50,572 - INFO - training batch 1601, loss: 2.107, 51232/60000 datapoints
2025-03-07 18:45:51,245 - INFO - training batch 1651, loss: 2.066, 52832/60000 datapoints
2025-03-07 18:45:51,808 - INFO - training batch 1701, loss: 2.097, 54432/60000 datapoints
2025-03-07 18:45:52,389 - INFO - training batch 1751, loss: 2.108, 56032/60000 datapoints
2025-03-07 18:45:52,941 - INFO - training batch 1801, loss: 2.092, 57632/60000 datapoints
2025-03-07 18:45:53,502 - INFO - training batch 1851, loss: 2.100, 59232/60000 datapoints
2025-03-07 18:45:53,767 - INFO - validation batch 1, loss: 2.048, 32/10016 datapoints
2025-03-07 18:45:54,067 - INFO - validation batch 51, loss: 2.077, 1632/10016 datapoints
2025-03-07 18:45:54,344 - INFO - validation batch 101, loss: 2.058, 3232/10016 datapoints
2025-03-07 18:45:54,815 - INFO - validation batch 151, loss: 2.072, 4832/10016 datapoints
2025-03-07 18:45:55,178 - INFO - validation batch 201, loss: 2.098, 6432/10016 datapoints
2025-03-07 18:45:55,496 - INFO - validation batch 251, loss: 2.033, 8032/10016 datapoints
2025-03-07 18:45:55,841 - INFO - validation batch 301, loss: 2.071, 9632/10016 datapoints
2025-03-07 18:45:55,957 - INFO - Epoch 12/800 done.
2025-03-07 18:45:55,958 - INFO - Final validation performance:
Loss: 2.065, top-1 acc: 0.520top-5 acc: 0.520
2025-03-07 18:45:55,962 - INFO - Beginning epoch 13/800
2025-03-07 18:45:55,975 - INFO - training batch 1, loss: 2.103, 32/60000 datapoints
2025-03-07 18:45:56,703 - INFO - training batch 51, loss: 2.051, 1632/60000 datapoints
2025-03-07 18:45:57,188 - INFO - training batch 101, loss: 2.053, 3232/60000 datapoints
2025-03-07 18:45:57,634 - INFO - training batch 151, loss: 2.066, 4832/60000 datapoints
2025-03-07 18:45:58,025 - INFO - training batch 201, loss: 2.048, 6432/60000 datapoints
2025-03-07 18:45:58,641 - INFO - training batch 251, loss: 2.056, 8032/60000 datapoints
2025-03-07 18:45:59,141 - INFO - training batch 301, loss: 2.099, 9632/60000 datapoints
2025-03-07 18:45:59,957 - INFO - training batch 351, loss: 2.024, 11232/60000 datapoints
2025-03-07 18:46:00,529 - INFO - training batch 401, loss: 2.056, 12832/60000 datapoints
2025-03-07 18:46:01,278 - INFO - training batch 451, loss: 2.032, 14432/60000 datapoints
2025-03-07 18:46:01,744 - INFO - training batch 501, loss: 2.082, 16032/60000 datapoints
2025-03-07 18:46:02,447 - INFO - training batch 551, loss: 2.109, 17632/60000 datapoints
2025-03-07 18:46:02,917 - INFO - training batch 601, loss: 2.067, 19232/60000 datapoints
2025-03-07 18:46:03,247 - INFO - training batch 651, loss: 2.137, 20832/60000 datapoints
2025-03-07 18:46:03,767 - INFO - training batch 701, loss: 1.975, 22432/60000 datapoints
2025-03-07 18:46:04,272 - INFO - training batch 751, loss: 2.023, 24032/60000 datapoints
2025-03-07 18:46:04,692 - INFO - training batch 801, loss: 2.044, 25632/60000 datapoints
2025-03-07 18:46:05,149 - INFO - training batch 851, loss: 1.975, 27232/60000 datapoints
2025-03-07 18:46:05,568 - INFO - training batch 901, loss: 2.026, 28832/60000 datapoints
2025-03-07 18:46:06,056 - INFO - training batch 951, loss: 2.070, 30432/60000 datapoints
2025-03-07 18:46:06,390 - INFO - training batch 1001, loss: 2.103, 32032/60000 datapoints
2025-03-07 18:46:06,961 - INFO - training batch 1051, loss: 2.036, 33632/60000 datapoints
2025-03-07 18:46:07,624 - INFO - training batch 1101, loss: 2.081, 35232/60000 datapoints
2025-03-07 18:46:08,099 - INFO - training batch 1151, loss: 2.101, 36832/60000 datapoints
2025-03-07 18:46:08,617 - INFO - training batch 1201, loss: 1.940, 38432/60000 datapoints
2025-03-07 18:46:09,580 - INFO - training batch 1251, loss: 2.082, 40032/60000 datapoints
2025-03-07 18:46:10,608 - INFO - training batch 1301, loss: 2.052, 41632/60000 datapoints
2025-03-07 18:46:11,409 - INFO - training batch 1351, loss: 2.114, 43232/60000 datapoints
2025-03-07 18:46:11,959 - INFO - training batch 1401, loss: 2.025, 44832/60000 datapoints
2025-03-07 18:46:12,424 - INFO - training batch 1451, loss: 2.088, 46432/60000 datapoints
2025-03-07 18:46:12,832 - INFO - training batch 1501, loss: 2.010, 48032/60000 datapoints
2025-03-07 18:46:13,638 - INFO - training batch 1551, loss: 2.000, 49632/60000 datapoints
2025-03-07 18:46:14,134 - INFO - training batch 1601, loss: 2.007, 51232/60000 datapoints
2025-03-07 18:46:14,583 - INFO - training batch 1651, loss: 2.074, 52832/60000 datapoints
2025-03-07 18:46:14,971 - INFO - training batch 1701, loss: 2.121, 54432/60000 datapoints
2025-03-07 18:46:15,345 - INFO - training batch 1751, loss: 2.076, 56032/60000 datapoints
2025-03-07 18:46:15,751 - INFO - training batch 1801, loss: 2.118, 57632/60000 datapoints
2025-03-07 18:46:16,144 - INFO - training batch 1851, loss: 2.043, 59232/60000 datapoints
2025-03-07 18:46:16,329 - INFO - validation batch 1, loss: 2.005, 32/10016 datapoints
2025-03-07 18:46:16,654 - INFO - validation batch 51, loss: 2.097, 1632/10016 datapoints
2025-03-07 18:46:16,965 - INFO - validation batch 101, loss: 2.017, 3232/10016 datapoints
2025-03-07 18:46:17,200 - INFO - validation batch 151, loss: 2.092, 4832/10016 datapoints
2025-03-07 18:46:17,435 - INFO - validation batch 201, loss: 2.046, 6432/10016 datapoints
2025-03-07 18:46:17,750 - INFO - validation batch 251, loss: 2.047, 8032/10016 datapoints
2025-03-07 18:46:18,082 - INFO - validation batch 301, loss: 2.028, 9632/10016 datapoints
2025-03-07 18:46:18,168 - INFO - Epoch 13/800 done.
2025-03-07 18:46:18,169 - INFO - Final validation performance:
Loss: 2.047, top-1 acc: 0.541top-5 acc: 0.541
2025-03-07 18:46:18,172 - INFO - Beginning epoch 14/800
2025-03-07 18:46:18,183 - INFO - training batch 1, loss: 2.001, 32/60000 datapoints
2025-03-07 18:46:18,622 - INFO - training batch 51, loss: 2.067, 1632/60000 datapoints
2025-03-07 18:46:19,001 - INFO - training batch 101, loss: 2.047, 3232/60000 datapoints
2025-03-07 18:46:19,422 - INFO - training batch 151, loss: 2.014, 4832/60000 datapoints
2025-03-07 18:46:19,809 - INFO - training batch 201, loss: 1.972, 6432/60000 datapoints
2025-03-07 18:46:20,192 - INFO - training batch 251, loss: 2.038, 8032/60000 datapoints
2025-03-07 18:46:20,541 - INFO - training batch 301, loss: 1.999, 9632/60000 datapoints
2025-03-07 18:46:20,919 - INFO - training batch 351, loss: 2.124, 11232/60000 datapoints
2025-03-07 18:46:21,272 - INFO - training batch 401, loss: 1.982, 12832/60000 datapoints
2025-03-07 18:46:21,584 - INFO - training batch 451, loss: 2.035, 14432/60000 datapoints
2025-03-07 18:46:21,893 - INFO - training batch 501, loss: 2.080, 16032/60000 datapoints
2025-03-07 18:46:22,399 - INFO - training batch 551, loss: 2.041, 17632/60000 datapoints
2025-03-07 18:46:22,827 - INFO - training batch 601, loss: 1.999, 19232/60000 datapoints
2025-03-07 18:46:23,230 - INFO - training batch 651, loss: 2.043, 20832/60000 datapoints
2025-03-07 18:46:23,665 - INFO - training batch 701, loss: 1.948, 22432/60000 datapoints
2025-03-07 18:46:24,125 - INFO - training batch 751, loss: 1.986, 24032/60000 datapoints
2025-03-07 18:46:24,541 - INFO - training batch 801, loss: 2.100, 25632/60000 datapoints
2025-03-07 18:46:25,005 - INFO - training batch 851, loss: 2.101, 27232/60000 datapoints
2025-03-07 18:46:25,394 - INFO - training batch 901, loss: 2.095, 28832/60000 datapoints
2025-03-07 18:46:25,706 - INFO - training batch 951, loss: 2.030, 30432/60000 datapoints
2025-03-07 18:46:26,085 - INFO - training batch 1001, loss: 2.009, 32032/60000 datapoints
2025-03-07 18:46:26,443 - INFO - training batch 1051, loss: 2.110, 33632/60000 datapoints
2025-03-07 18:46:26,752 - INFO - training batch 1101, loss: 2.051, 35232/60000 datapoints
2025-03-07 18:46:27,127 - INFO - training batch 1151, loss: 2.132, 36832/60000 datapoints
2025-03-07 18:46:27,474 - INFO - training batch 1201, loss: 2.073, 38432/60000 datapoints
2025-03-07 18:46:27,807 - INFO - training batch 1251, loss: 2.013, 40032/60000 datapoints
2025-03-07 18:46:28,130 - INFO - training batch 1301, loss: 2.052, 41632/60000 datapoints
2025-03-07 18:46:28,529 - INFO - training batch 1351, loss: 2.042, 43232/60000 datapoints
2025-03-07 18:46:28,854 - INFO - training batch 1401, loss: 2.057, 44832/60000 datapoints
2025-03-07 18:46:29,175 - INFO - training batch 1451, loss: 2.039, 46432/60000 datapoints
2025-03-07 18:46:29,466 - INFO - training batch 1501, loss: 1.939, 48032/60000 datapoints
2025-03-07 18:46:29,764 - INFO - training batch 1551, loss: 2.113, 49632/60000 datapoints
2025-03-07 18:46:30,103 - INFO - training batch 1601, loss: 1.980, 51232/60000 datapoints
2025-03-07 18:46:30,597 - INFO - training batch 1651, loss: 2.083, 52832/60000 datapoints
2025-03-07 18:46:30,956 - INFO - training batch 1701, loss: 2.014, 54432/60000 datapoints
2025-03-07 18:46:31,311 - INFO - training batch 1751, loss: 2.120, 56032/60000 datapoints
2025-03-07 18:46:31,636 - INFO - training batch 1801, loss: 1.918, 57632/60000 datapoints
2025-03-07 18:46:32,044 - INFO - training batch 1851, loss: 2.045, 59232/60000 datapoints
2025-03-07 18:46:32,204 - INFO - validation batch 1, loss: 2.041, 32/10016 datapoints
2025-03-07 18:46:32,489 - INFO - validation batch 51, loss: 1.988, 1632/10016 datapoints
2025-03-07 18:46:32,769 - INFO - validation batch 101, loss: 2.008, 3232/10016 datapoints
2025-03-07 18:46:33,057 - INFO - validation batch 151, loss: 1.975, 4832/10016 datapoints
2025-03-07 18:46:33,345 - INFO - validation batch 201, loss: 1.989, 6432/10016 datapoints
2025-03-07 18:46:33,650 - INFO - validation batch 251, loss: 1.907, 8032/10016 datapoints
2025-03-07 18:46:33,981 - INFO - validation batch 301, loss: 1.975, 9632/10016 datapoints
2025-03-07 18:46:34,058 - INFO - Epoch 14/800 done.
2025-03-07 18:46:34,058 - INFO - Final validation performance:
Loss: 1.983, top-1 acc: 0.561top-5 acc: 0.561
2025-03-07 18:46:34,059 - INFO - Beginning epoch 15/800
2025-03-07 18:46:34,068 - INFO - training batch 1, loss: 2.004, 32/60000 datapoints
2025-03-07 18:46:34,388 - INFO - training batch 51, loss: 2.036, 1632/60000 datapoints
2025-03-07 18:46:34,712 - INFO - training batch 101, loss: 2.039, 3232/60000 datapoints
2025-03-07 18:46:35,070 - INFO - training batch 151, loss: 1.921, 4832/60000 datapoints
2025-03-07 18:46:35,418 - INFO - training batch 201, loss: 2.013, 6432/60000 datapoints
2025-03-07 18:46:35,785 - INFO - training batch 251, loss: 1.974, 8032/60000 datapoints
2025-03-07 18:46:36,115 - INFO - training batch 301, loss: 2.019, 9632/60000 datapoints
2025-03-07 18:46:36,429 - INFO - training batch 351, loss: 1.995, 11232/60000 datapoints
2025-03-07 18:46:36,739 - INFO - training batch 401, loss: 1.974, 12832/60000 datapoints
2025-03-07 18:46:37,053 - INFO - training batch 451, loss: 2.021, 14432/60000 datapoints
2025-03-07 18:46:37,371 - INFO - training batch 501, loss: 2.020, 16032/60000 datapoints
2025-03-07 18:46:37,746 - INFO - training batch 551, loss: 2.069, 17632/60000 datapoints
2025-03-07 18:46:38,101 - INFO - training batch 601, loss: 1.921, 19232/60000 datapoints
2025-03-07 18:46:38,449 - INFO - training batch 651, loss: 1.987, 20832/60000 datapoints
2025-03-07 18:46:38,788 - INFO - training batch 701, loss: 1.916, 22432/60000 datapoints
2025-03-07 18:46:39,129 - INFO - training batch 751, loss: 2.088, 24032/60000 datapoints
2025-03-07 18:46:39,486 - INFO - training batch 801, loss: 2.009, 25632/60000 datapoints
2025-03-07 18:46:39,805 - INFO - training batch 851, loss: 2.026, 27232/60000 datapoints
2025-03-07 18:46:40,141 - INFO - training batch 901, loss: 2.075, 28832/60000 datapoints
2025-03-07 18:46:40,502 - INFO - training batch 951, loss: 1.985, 30432/60000 datapoints
2025-03-07 18:46:40,861 - INFO - training batch 1001, loss: 2.061, 32032/60000 datapoints
2025-03-07 18:46:41,202 - INFO - training batch 1051, loss: 1.993, 33632/60000 datapoints
2025-03-07 18:46:41,575 - INFO - training batch 1101, loss: 1.992, 35232/60000 datapoints
2025-03-07 18:46:41,941 - INFO - training batch 1151, loss: 1.940, 36832/60000 datapoints
2025-03-07 18:46:42,360 - INFO - training batch 1201, loss: 2.013, 38432/60000 datapoints
2025-03-07 18:46:42,766 - INFO - training batch 1251, loss: 2.062, 40032/60000 datapoints
2025-03-07 18:46:43,247 - INFO - training batch 1301, loss: 1.941, 41632/60000 datapoints
2025-03-07 18:46:43,630 - INFO - training batch 1351, loss: 2.097, 43232/60000 datapoints
2025-03-07 18:46:43,974 - INFO - training batch 1401, loss: 1.974, 44832/60000 datapoints
2025-03-07 18:46:44,308 - INFO - training batch 1451, loss: 2.030, 46432/60000 datapoints
2025-03-07 18:46:44,644 - INFO - training batch 1501, loss: 2.035, 48032/60000 datapoints
2025-03-07 18:46:44,930 - INFO - training batch 1551, loss: 2.002, 49632/60000 datapoints
2025-03-07 18:46:45,276 - INFO - training batch 1601, loss: 1.954, 51232/60000 datapoints
2025-03-07 18:46:45,611 - INFO - training batch 1651, loss: 2.033, 52832/60000 datapoints
2025-03-07 18:46:45,928 - INFO - training batch 1701, loss: 1.987, 54432/60000 datapoints
2025-03-07 18:46:46,245 - INFO - training batch 1751, loss: 1.964, 56032/60000 datapoints
2025-03-07 18:46:46,570 - INFO - training batch 1801, loss: 2.055, 57632/60000 datapoints
2025-03-07 18:46:46,893 - INFO - training batch 1851, loss: 1.966, 59232/60000 datapoints
2025-03-07 18:46:47,092 - INFO - validation batch 1, loss: 1.977, 32/10016 datapoints
2025-03-07 18:46:47,359 - INFO - validation batch 51, loss: 2.010, 1632/10016 datapoints
2025-03-07 18:46:47,598 - INFO - validation batch 101, loss: 2.002, 3232/10016 datapoints
2025-03-07 18:46:47,847 - INFO - validation batch 151, loss: 1.938, 4832/10016 datapoints
2025-03-07 18:46:48,383 - INFO - validation batch 201, loss: 2.050, 6432/10016 datapoints
2025-03-07 18:46:48,794 - INFO - validation batch 251, loss: 2.016, 8032/10016 datapoints
2025-03-07 18:46:49,312 - INFO - validation batch 301, loss: 2.024, 9632/10016 datapoints
2025-03-07 18:46:49,389 - INFO - Epoch 15/800 done.
2025-03-07 18:46:49,390 - INFO - Final validation performance:
Loss: 2.002, top-1 acc: 0.577top-5 acc: 0.577
2025-03-07 18:46:49,391 - INFO - Beginning epoch 16/800
2025-03-07 18:46:49,401 - INFO - training batch 1, loss: 2.030, 32/60000 datapoints
2025-03-07 18:46:49,823 - INFO - training batch 51, loss: 1.998, 1632/60000 datapoints
2025-03-07 18:46:50,282 - INFO - training batch 101, loss: 1.899, 3232/60000 datapoints
2025-03-07 18:46:50,742 - INFO - training batch 151, loss: 1.961, 4832/60000 datapoints
2025-03-07 18:46:51,214 - INFO - training batch 201, loss: 1.965, 6432/60000 datapoints
2025-03-07 18:46:52,041 - INFO - training batch 251, loss: 1.977, 8032/60000 datapoints
2025-03-07 18:46:52,447 - INFO - training batch 301, loss: 2.003, 9632/60000 datapoints
2025-03-07 18:46:52,815 - INFO - training batch 351, loss: 1.941, 11232/60000 datapoints
2025-03-07 18:46:53,173 - INFO - training batch 401, loss: 1.987, 12832/60000 datapoints
2025-03-07 18:46:53,499 - INFO - training batch 451, loss: 1.940, 14432/60000 datapoints
2025-03-07 18:46:53,834 - INFO - training batch 501, loss: 2.008, 16032/60000 datapoints
2025-03-07 18:46:54,234 - INFO - training batch 551, loss: 1.981, 17632/60000 datapoints
2025-03-07 18:46:54,577 - INFO - training batch 601, loss: 1.952, 19232/60000 datapoints
2025-03-07 18:46:54,855 - INFO - training batch 651, loss: 1.971, 20832/60000 datapoints
2025-03-07 18:46:55,264 - INFO - training batch 701, loss: 1.977, 22432/60000 datapoints
2025-03-07 18:46:55,584 - INFO - training batch 751, loss: 1.976, 24032/60000 datapoints
2025-03-07 18:46:55,869 - INFO - training batch 801, loss: 1.894, 25632/60000 datapoints
2025-03-07 18:46:56,180 - INFO - training batch 851, loss: 1.983, 27232/60000 datapoints
2025-03-07 18:46:56,478 - INFO - training batch 901, loss: 1.974, 28832/60000 datapoints
2025-03-07 18:46:56,878 - INFO - training batch 951, loss: 1.942, 30432/60000 datapoints
2025-03-07 18:46:57,170 - INFO - training batch 1001, loss: 1.991, 32032/60000 datapoints
2025-03-07 18:46:57,571 - INFO - training batch 1051, loss: 2.001, 33632/60000 datapoints
2025-03-07 18:46:57,941 - INFO - training batch 1101, loss: 1.974, 35232/60000 datapoints
2025-03-07 18:46:58,374 - INFO - training batch 1151, loss: 2.060, 36832/60000 datapoints
2025-03-07 18:46:58,919 - INFO - training batch 1201, loss: 2.031, 38432/60000 datapoints
2025-03-07 18:46:59,447 - INFO - training batch 1251, loss: 1.986, 40032/60000 datapoints
2025-03-07 18:46:59,777 - INFO - training batch 1301, loss: 1.922, 41632/60000 datapoints
2025-03-07 18:47:00,091 - INFO - training batch 1351, loss: 1.948, 43232/60000 datapoints
2025-03-07 18:47:00,384 - INFO - training batch 1401, loss: 2.034, 44832/60000 datapoints
2025-03-07 18:47:00,699 - INFO - training batch 1451, loss: 1.974, 46432/60000 datapoints
2025-03-07 18:47:01,478 - INFO - training batch 1501, loss: 2.017, 48032/60000 datapoints
2025-03-07 18:47:01,817 - INFO - training batch 1551, loss: 2.012, 49632/60000 datapoints
2025-03-07 18:47:02,184 - INFO - training batch 1601, loss: 2.008, 51232/60000 datapoints
2025-03-07 18:47:02,541 - INFO - training batch 1651, loss: 1.962, 52832/60000 datapoints
2025-03-07 18:47:02,951 - INFO - training batch 1701, loss: 1.930, 54432/60000 datapoints
2025-03-07 18:47:03,306 - INFO - training batch 1751, loss: 2.019, 56032/60000 datapoints
2025-03-07 18:47:03,627 - INFO - training batch 1801, loss: 1.979, 57632/60000 datapoints
2025-03-07 18:47:04,000 - INFO - training batch 1851, loss: 1.911, 59232/60000 datapoints
2025-03-07 18:47:04,161 - INFO - validation batch 1, loss: 1.994, 32/10016 datapoints
2025-03-07 18:47:04,402 - INFO - validation batch 51, loss: 1.955, 1632/10016 datapoints
2025-03-07 18:47:04,657 - INFO - validation batch 101, loss: 1.975, 3232/10016 datapoints
2025-03-07 18:47:04,986 - INFO - validation batch 151, loss: 1.980, 4832/10016 datapoints
2025-03-07 18:47:05,488 - INFO - validation batch 201, loss: 2.008, 6432/10016 datapoints
2025-03-07 18:47:05,817 - INFO - validation batch 251, loss: 1.943, 8032/10016 datapoints
2025-03-07 18:47:06,137 - INFO - validation batch 301, loss: 1.952, 9632/10016 datapoints
2025-03-07 18:47:06,217 - INFO - Epoch 16/800 done.
2025-03-07 18:47:06,217 - INFO - Final validation performance:
Loss: 1.972, top-1 acc: 0.593top-5 acc: 0.593
2025-03-07 18:47:06,219 - INFO - Beginning epoch 17/800
2025-03-07 18:47:06,252 - INFO - training batch 1, loss: 2.001, 32/60000 datapoints
2025-03-07 18:47:06,613 - INFO - training batch 51, loss: 1.983, 1632/60000 datapoints
2025-03-07 18:47:07,002 - INFO - training batch 101, loss: 1.920, 3232/60000 datapoints
2025-03-07 18:47:07,330 - INFO - training batch 151, loss: 2.016, 4832/60000 datapoints
2025-03-07 18:47:07,628 - INFO - training batch 201, loss: 1.957, 6432/60000 datapoints
2025-03-07 18:47:07,956 - INFO - training batch 251, loss: 1.885, 8032/60000 datapoints
2025-03-07 18:47:08,270 - INFO - training batch 301, loss: 1.935, 9632/60000 datapoints
2025-03-07 18:47:08,567 - INFO - training batch 351, loss: 1.986, 11232/60000 datapoints
2025-03-07 18:47:08,869 - INFO - training batch 401, loss: 1.911, 12832/60000 datapoints
2025-03-07 18:47:09,162 - INFO - training batch 451, loss: 2.015, 14432/60000 datapoints
2025-03-07 18:47:09,458 - INFO - training batch 501, loss: 1.903, 16032/60000 datapoints
2025-03-07 18:47:09,767 - INFO - training batch 551, loss: 2.010, 17632/60000 datapoints
2025-03-07 18:47:10,045 - INFO - training batch 601, loss: 2.017, 19232/60000 datapoints
2025-03-07 18:47:10,354 - INFO - training batch 651, loss: 2.032, 20832/60000 datapoints
2025-03-07 18:47:10,662 - INFO - training batch 701, loss: 1.895, 22432/60000 datapoints
2025-03-07 18:47:11,080 - INFO - training batch 751, loss: 1.979, 24032/60000 datapoints
2025-03-07 18:47:11,367 - INFO - training batch 801, loss: 1.984, 25632/60000 datapoints
2025-03-07 18:47:11,669 - INFO - training batch 851, loss: 1.873, 27232/60000 datapoints
2025-03-07 18:47:12,058 - INFO - training batch 901, loss: 1.896, 28832/60000 datapoints
2025-03-07 18:47:12,404 - INFO - training batch 951, loss: 1.989, 30432/60000 datapoints
2025-03-07 18:47:12,778 - INFO - training batch 1001, loss: 1.934, 32032/60000 datapoints
2025-03-07 18:47:13,183 - INFO - training batch 1051, loss: 1.924, 33632/60000 datapoints
2025-03-07 18:47:13,553 - INFO - training batch 1101, loss: 2.024, 35232/60000 datapoints
2025-03-07 18:47:13,918 - INFO - training batch 1151, loss: 1.874, 36832/60000 datapoints
2025-03-07 18:47:14,275 - INFO - training batch 1201, loss: 1.940, 38432/60000 datapoints
2025-03-07 18:47:14,604 - INFO - training batch 1251, loss: 1.913, 40032/60000 datapoints
2025-03-07 18:47:14,928 - INFO - training batch 1301, loss: 1.971, 41632/60000 datapoints
2025-03-07 18:47:15,277 - INFO - training batch 1351, loss: 1.951, 43232/60000 datapoints
2025-03-07 18:47:15,607 - INFO - training batch 1401, loss: 1.921, 44832/60000 datapoints
2025-03-07 18:47:15,972 - INFO - training batch 1451, loss: 1.931, 46432/60000 datapoints
2025-03-07 18:47:16,306 - INFO - training batch 1501, loss: 1.909, 48032/60000 datapoints
2025-03-07 18:47:16,709 - INFO - training batch 1551, loss: 1.946, 49632/60000 datapoints
2025-03-07 18:47:17,054 - INFO - training batch 1601, loss: 1.877, 51232/60000 datapoints
2025-03-07 18:47:17,346 - INFO - training batch 1651, loss: 1.954, 52832/60000 datapoints
2025-03-07 18:47:17,676 - INFO - training batch 1701, loss: 1.966, 54432/60000 datapoints
2025-03-07 18:47:18,032 - INFO - training batch 1751, loss: 1.930, 56032/60000 datapoints
2025-03-07 18:47:18,552 - INFO - training batch 1801, loss: 1.887, 57632/60000 datapoints
2025-03-07 18:47:18,971 - INFO - training batch 1851, loss: 1.936, 59232/60000 datapoints
2025-03-07 18:47:19,145 - INFO - validation batch 1, loss: 1.918, 32/10016 datapoints
2025-03-07 18:47:19,434 - INFO - validation batch 51, loss: 1.874, 1632/10016 datapoints
2025-03-07 18:47:19,696 - INFO - validation batch 101, loss: 1.874, 3232/10016 datapoints
2025-03-07 18:47:20,182 - INFO - validation batch 151, loss: 1.890, 4832/10016 datapoints
2025-03-07 18:47:20,525 - INFO - validation batch 201, loss: 1.927, 6432/10016 datapoints
2025-03-07 18:47:20,909 - INFO - validation batch 251, loss: 1.834, 8032/10016 datapoints
2025-03-07 18:47:21,230 - INFO - validation batch 301, loss: 1.899, 9632/10016 datapoints
2025-03-07 18:47:21,307 - INFO - Epoch 17/800 done.
2025-03-07 18:47:21,307 - INFO - Final validation performance:
Loss: 1.888, top-1 acc: 0.607top-5 acc: 0.607
2025-03-07 18:47:21,308 - INFO - Beginning epoch 18/800
2025-03-07 18:47:21,320 - INFO - training batch 1, loss: 1.894, 32/60000 datapoints
2025-03-07 18:47:21,719 - INFO - training batch 51, loss: 1.901, 1632/60000 datapoints
2025-03-07 18:47:22,119 - INFO - training batch 101, loss: 1.899, 3232/60000 datapoints
2025-03-07 18:47:22,517 - INFO - training batch 151, loss: 1.942, 4832/60000 datapoints
2025-03-07 18:47:22,963 - INFO - training batch 201, loss: 1.935, 6432/60000 datapoints
2025-03-07 18:47:23,380 - INFO - training batch 251, loss: 1.824, 8032/60000 datapoints
2025-03-07 18:47:23,775 - INFO - training batch 301, loss: 1.877, 9632/60000 datapoints
2025-03-07 18:47:24,181 - INFO - training batch 351, loss: 1.961, 11232/60000 datapoints
2025-03-07 18:47:24,733 - INFO - training batch 401, loss: 1.824, 12832/60000 datapoints
2025-03-07 18:47:25,050 - INFO - training batch 451, loss: 1.974, 14432/60000 datapoints
2025-03-07 18:47:25,367 - INFO - training batch 501, loss: 1.965, 16032/60000 datapoints
2025-03-07 18:47:25,700 - INFO - training batch 551, loss: 1.893, 17632/60000 datapoints
2025-03-07 18:47:26,019 - INFO - training batch 601, loss: 1.926, 19232/60000 datapoints
2025-03-07 18:47:26,430 - INFO - training batch 651, loss: 1.947, 20832/60000 datapoints
2025-03-07 18:47:26,742 - INFO - training batch 701, loss: 1.889, 22432/60000 datapoints
2025-03-07 18:47:27,132 - INFO - training batch 751, loss: 1.802, 24032/60000 datapoints
2025-03-07 18:47:27,510 - INFO - training batch 801, loss: 1.869, 25632/60000 datapoints
2025-03-07 18:47:27,853 - INFO - training batch 851, loss: 1.840, 27232/60000 datapoints
2025-03-07 18:47:28,195 - INFO - training batch 901, loss: 1.892, 28832/60000 datapoints
2025-03-07 18:47:28,826 - INFO - training batch 951, loss: 1.970, 30432/60000 datapoints
2025-03-07 18:47:29,279 - INFO - training batch 1001, loss: 2.027, 32032/60000 datapoints
2025-03-07 18:47:29,607 - INFO - training batch 1051, loss: 1.907, 33632/60000 datapoints
2025-03-07 18:47:29,882 - INFO - training batch 1101, loss: 1.895, 35232/60000 datapoints
2025-03-07 18:47:30,173 - INFO - training batch 1151, loss: 1.850, 36832/60000 datapoints
2025-03-07 18:47:30,460 - INFO - training batch 1201, loss: 1.947, 38432/60000 datapoints
2025-03-07 18:47:30,747 - INFO - training batch 1251, loss: 1.946, 40032/60000 datapoints
2025-03-07 18:47:31,159 - INFO - training batch 1301, loss: 1.826, 41632/60000 datapoints
2025-03-07 18:47:31,548 - INFO - training batch 1351, loss: 1.871, 43232/60000 datapoints
2025-03-07 18:47:31,878 - INFO - training batch 1401, loss: 1.928, 44832/60000 datapoints
2025-03-07 18:47:32,267 - INFO - training batch 1451, loss: 1.896, 46432/60000 datapoints
2025-03-07 18:47:32,575 - INFO - training batch 1501, loss: 1.849, 48032/60000 datapoints
2025-03-07 18:47:32,919 - INFO - training batch 1551, loss: 1.814, 49632/60000 datapoints
2025-03-07 18:47:33,382 - INFO - training batch 1601, loss: 1.874, 51232/60000 datapoints
2025-03-07 18:47:33,752 - INFO - training batch 1651, loss: 1.902, 52832/60000 datapoints
2025-03-07 18:47:34,096 - INFO - training batch 1701, loss: 1.868, 54432/60000 datapoints
2025-03-07 18:47:34,489 - INFO - training batch 1751, loss: 1.843, 56032/60000 datapoints
2025-03-07 18:47:34,968 - INFO - training batch 1801, loss: 1.977, 57632/60000 datapoints
2025-03-07 18:47:35,349 - INFO - training batch 1851, loss: 1.903, 59232/60000 datapoints
2025-03-07 18:47:35,524 - INFO - validation batch 1, loss: 1.862, 32/10016 datapoints
2025-03-07 18:47:36,026 - INFO - validation batch 51, loss: 1.906, 1632/10016 datapoints
2025-03-07 18:47:36,324 - INFO - validation batch 101, loss: 1.843, 3232/10016 datapoints
2025-03-07 18:47:36,698 - INFO - validation batch 151, loss: 1.888, 4832/10016 datapoints
2025-03-07 18:47:37,044 - INFO - validation batch 201, loss: 1.838, 6432/10016 datapoints
2025-03-07 18:47:37,344 - INFO - validation batch 251, loss: 1.815, 8032/10016 datapoints
2025-03-07 18:47:37,660 - INFO - validation batch 301, loss: 1.890, 9632/10016 datapoints
2025-03-07 18:47:37,724 - INFO - Epoch 18/800 done.
2025-03-07 18:47:37,725 - INFO - Final validation performance:
Loss: 1.863, top-1 acc: 0.619top-5 acc: 0.619
2025-03-07 18:47:37,726 - INFO - Beginning epoch 19/800
2025-03-07 18:47:37,735 - INFO - training batch 1, loss: 1.861, 32/60000 datapoints
2025-03-07 18:47:38,125 - INFO - training batch 51, loss: 1.958, 1632/60000 datapoints
2025-03-07 18:47:38,483 - INFO - training batch 101, loss: 1.842, 3232/60000 datapoints
2025-03-07 18:47:38,815 - INFO - training batch 151, loss: 1.914, 4832/60000 datapoints
2025-03-07 18:47:39,140 - INFO - training batch 201, loss: 1.827, 6432/60000 datapoints
2025-03-07 18:47:39,495 - INFO - training batch 251, loss: 1.862, 8032/60000 datapoints
2025-03-07 18:47:39,879 - INFO - training batch 301, loss: 1.844, 9632/60000 datapoints
2025-03-07 18:47:40,267 - INFO - training batch 351, loss: 1.902, 11232/60000 datapoints
2025-03-07 18:47:40,696 - INFO - training batch 401, loss: 1.841, 12832/60000 datapoints
2025-03-07 18:47:41,761 - INFO - training batch 451, loss: 1.799, 14432/60000 datapoints
2025-03-07 18:47:46,779 - INFO - training batch 501, loss: 1.886, 16032/60000 datapoints
2025-03-07 18:47:54,024 - INFO - training batch 551, loss: 1.904, 17632/60000 datapoints
2025-03-07 18:47:58,369 - INFO - training batch 601, loss: 1.838, 19232/60000 datapoints
2025-03-07 18:48:01,888 - INFO - training batch 651, loss: 1.904, 20832/60000 datapoints
2025-03-07 18:48:02,522 - INFO - training batch 701, loss: 1.928, 22432/60000 datapoints
2025-03-07 18:48:03,080 - INFO - training batch 751, loss: 1.846, 24032/60000 datapoints
2025-03-07 18:48:03,692 - INFO - training batch 801, loss: 1.863, 25632/60000 datapoints
2025-03-07 18:48:04,343 - INFO - training batch 851, loss: 1.879, 27232/60000 datapoints
2025-03-07 18:48:04,917 - INFO - training batch 901, loss: 1.887, 28832/60000 datapoints
2025-03-07 18:48:05,323 - INFO - training batch 951, loss: 1.778, 30432/60000 datapoints
2025-03-07 18:48:05,739 - INFO - training batch 1001, loss: 1.921, 32032/60000 datapoints
2025-03-07 18:48:06,168 - INFO - training batch 1051, loss: 1.888, 33632/60000 datapoints
2025-03-07 18:48:06,642 - INFO - training batch 1101, loss: 1.983, 35232/60000 datapoints
2025-03-07 18:48:07,061 - INFO - training batch 1151, loss: 1.928, 36832/60000 datapoints
2025-03-07 18:48:07,458 - INFO - training batch 1201, loss: 1.883, 38432/60000 datapoints
2025-03-07 18:48:07,833 - INFO - training batch 1251, loss: 1.934, 40032/60000 datapoints
2025-03-07 18:48:08,222 - INFO - training batch 1301, loss: 1.907, 41632/60000 datapoints
2025-03-07 18:48:08,638 - INFO - training batch 1351, loss: 2.047, 43232/60000 datapoints
2025-03-07 18:48:09,074 - INFO - training batch 1401, loss: 1.801, 44832/60000 datapoints
2025-03-07 18:48:09,463 - INFO - training batch 1451, loss: 1.900, 46432/60000 datapoints
2025-03-07 18:48:09,847 - INFO - training batch 1501, loss: 1.873, 48032/60000 datapoints
2025-03-07 18:48:10,211 - INFO - training batch 1551, loss: 1.834, 49632/60000 datapoints
2025-03-07 18:48:10,578 - INFO - training batch 1601, loss: 1.796, 51232/60000 datapoints
2025-03-07 18:48:11,076 - INFO - training batch 1651, loss: 1.910, 52832/60000 datapoints
2025-03-07 18:48:11,508 - INFO - training batch 1701, loss: 1.820, 54432/60000 datapoints
2025-03-07 18:48:12,549 - INFO - training batch 1751, loss: 1.885, 56032/60000 datapoints
2025-03-07 18:48:13,141 - INFO - training batch 1801, loss: 1.868, 57632/60000 datapoints
2025-03-07 18:48:13,752 - INFO - training batch 1851, loss: 1.866, 59232/60000 datapoints
2025-03-07 18:48:14,101 - INFO - validation batch 1, loss: 1.821, 32/10016 datapoints
2025-03-07 18:48:14,522 - INFO - validation batch 51, loss: 1.907, 1632/10016 datapoints
2025-03-07 18:48:14,909 - INFO - validation batch 101, loss: 1.882, 3232/10016 datapoints
2025-03-07 18:48:15,388 - INFO - validation batch 151, loss: 1.906, 4832/10016 datapoints
2025-03-07 18:48:15,998 - INFO - validation batch 201, loss: 1.796, 6432/10016 datapoints
2025-03-07 18:48:16,252 - INFO - validation batch 251, loss: 1.864, 8032/10016 datapoints
2025-03-07 18:48:16,519 - INFO - validation batch 301, loss: 1.849, 9632/10016 datapoints
2025-03-07 18:48:16,580 - INFO - Epoch 19/800 done.
2025-03-07 18:48:16,582 - INFO - Final validation performance:
Loss: 1.861, top-1 acc: 0.631top-5 acc: 0.631
2025-03-07 18:48:16,587 - INFO - Beginning epoch 20/800
2025-03-07 18:48:16,597 - INFO - training batch 1, loss: 1.904, 32/60000 datapoints
2025-03-07 18:48:16,936 - INFO - training batch 51, loss: 1.855, 1632/60000 datapoints
2025-03-07 18:48:17,348 - INFO - training batch 101, loss: 1.829, 3232/60000 datapoints
2025-03-07 18:48:17,662 - INFO - training batch 151, loss: 1.916, 4832/60000 datapoints
2025-03-07 18:48:18,056 - INFO - training batch 201, loss: 1.840, 6432/60000 datapoints
2025-03-07 18:48:18,456 - INFO - training batch 251, loss: 1.748, 8032/60000 datapoints
2025-03-07 18:48:18,769 - INFO - training batch 301, loss: 1.775, 9632/60000 datapoints
2025-03-07 18:48:19,227 - INFO - training batch 351, loss: 1.889, 11232/60000 datapoints
2025-03-07 18:48:19,591 - INFO - training batch 401, loss: 1.794, 12832/60000 datapoints
2025-03-07 18:48:19,978 - INFO - training batch 451, loss: 1.752, 14432/60000 datapoints
2025-03-07 18:48:20,333 - INFO - training batch 501, loss: 1.941, 16032/60000 datapoints
2025-03-07 18:48:20,698 - INFO - training batch 551, loss: 1.845, 17632/60000 datapoints
2025-03-07 18:48:21,031 - INFO - training batch 601, loss: 1.864, 19232/60000 datapoints
2025-03-07 18:48:21,346 - INFO - training batch 651, loss: 1.967, 20832/60000 datapoints
2025-03-07 18:48:21,679 - INFO - training batch 701, loss: 1.859, 22432/60000 datapoints
2025-03-07 18:48:22,001 - INFO - training batch 751, loss: 1.899, 24032/60000 datapoints
2025-03-07 18:48:22,315 - INFO - training batch 801, loss: 1.837, 25632/60000 datapoints
2025-03-07 18:48:22,891 - INFO - training batch 851, loss: 1.894, 27232/60000 datapoints
2025-03-07 18:48:23,418 - INFO - training batch 901, loss: 1.915, 28832/60000 datapoints
2025-03-07 18:48:23,904 - INFO - training batch 951, loss: 1.878, 30432/60000 datapoints
2025-03-07 18:48:24,409 - INFO - training batch 1001, loss: 1.797, 32032/60000 datapoints
2025-03-07 18:48:25,234 - INFO - training batch 1051, loss: 1.799, 33632/60000 datapoints
2025-03-07 18:48:25,583 - INFO - training batch 1101, loss: 1.860, 35232/60000 datapoints
2025-03-07 18:48:25,928 - INFO - training batch 1151, loss: 1.872, 36832/60000 datapoints
2025-03-07 18:48:26,243 - INFO - training batch 1201, loss: 1.811, 38432/60000 datapoints
2025-03-07 18:48:26,702 - INFO - training batch 1251, loss: 1.806, 40032/60000 datapoints
2025-03-07 18:48:27,222 - INFO - training batch 1301, loss: 1.924, 41632/60000 datapoints
2025-03-07 18:48:27,570 - INFO - training batch 1351, loss: 1.808, 43232/60000 datapoints
2025-03-07 18:48:27,855 - INFO - training batch 1401, loss: 1.848, 44832/60000 datapoints
2025-03-07 18:48:28,343 - INFO - training batch 1451, loss: 1.809, 46432/60000 datapoints
2025-03-07 18:48:28,863 - INFO - training batch 1501, loss: 1.837, 48032/60000 datapoints
2025-03-07 18:48:29,287 - INFO - training batch 1551, loss: 1.809, 49632/60000 datapoints
2025-03-07 18:48:29,771 - INFO - training batch 1601, loss: 1.812, 51232/60000 datapoints
2025-03-07 18:48:30,227 - INFO - training batch 1651, loss: 1.802, 52832/60000 datapoints
2025-03-07 18:48:30,610 - INFO - training batch 1701, loss: 1.829, 54432/60000 datapoints
2025-03-07 18:48:31,380 - INFO - training batch 1751, loss: 1.784, 56032/60000 datapoints
2025-03-07 18:48:32,126 - INFO - training batch 1801, loss: 1.816, 57632/60000 datapoints
2025-03-07 18:48:32,640 - INFO - training batch 1851, loss: 1.793, 59232/60000 datapoints
2025-03-07 18:48:32,891 - INFO - validation batch 1, loss: 1.727, 32/10016 datapoints
2025-03-07 18:48:33,269 - INFO - validation batch 51, loss: 1.788, 1632/10016 datapoints
2025-03-07 18:48:33,592 - INFO - validation batch 101, loss: 1.705, 3232/10016 datapoints
2025-03-07 18:48:34,150 - INFO - validation batch 151, loss: 1.886, 4832/10016 datapoints
2025-03-07 18:48:34,594 - INFO - validation batch 201, loss: 1.815, 6432/10016 datapoints
2025-03-07 18:48:35,097 - INFO - validation batch 251, loss: 1.876, 8032/10016 datapoints
2025-03-07 18:48:35,568 - INFO - validation batch 301, loss: 1.799, 9632/10016 datapoints
2025-03-07 18:48:35,670 - INFO - Epoch 20/800 done.
2025-03-07 18:48:35,671 - INFO - Final validation performance:
Loss: 1.799, top-1 acc: 0.641top-5 acc: 0.641
2025-03-07 18:48:35,674 - INFO - Beginning epoch 21/800
2025-03-07 18:48:35,692 - INFO - training batch 1, loss: 1.846, 32/60000 datapoints
2025-03-07 18:48:36,168 - INFO - training batch 51, loss: 1.845, 1632/60000 datapoints
2025-03-07 18:48:36,539 - INFO - training batch 101, loss: 1.846, 3232/60000 datapoints
2025-03-07 18:48:36,965 - INFO - training batch 151, loss: 1.799, 4832/60000 datapoints
2025-03-07 18:48:37,354 - INFO - training batch 201, loss: 1.737, 6432/60000 datapoints
2025-03-07 18:48:37,909 - INFO - training batch 251, loss: 1.886, 8032/60000 datapoints
2025-03-07 18:48:38,277 - INFO - training batch 301, loss: 1.818, 9632/60000 datapoints
2025-03-07 18:48:38,571 - INFO - training batch 351, loss: 1.782, 11232/60000 datapoints
2025-03-07 18:48:39,017 - INFO - training batch 401, loss: 1.779, 12832/60000 datapoints
2025-03-07 18:48:39,468 - INFO - training batch 451, loss: 1.844, 14432/60000 datapoints
2025-03-07 18:48:39,850 - INFO - training batch 501, loss: 1.850, 16032/60000 datapoints
2025-03-07 18:48:40,262 - INFO - training batch 551, loss: 1.831, 17632/60000 datapoints
2025-03-07 18:48:40,633 - INFO - training batch 601, loss: 1.863, 19232/60000 datapoints
2025-03-07 18:48:40,955 - INFO - training batch 651, loss: 1.842, 20832/60000 datapoints
2025-03-07 18:48:41,329 - INFO - training batch 701, loss: 1.840, 22432/60000 datapoints
2025-03-07 18:48:41,708 - INFO - training batch 751, loss: 1.933, 24032/60000 datapoints
2025-03-07 18:48:42,123 - INFO - training batch 801, loss: 1.840, 25632/60000 datapoints
2025-03-07 18:48:42,434 - INFO - training batch 851, loss: 1.868, 27232/60000 datapoints
2025-03-07 18:48:42,790 - INFO - training batch 901, loss: 1.836, 28832/60000 datapoints
2025-03-07 18:48:43,142 - INFO - training batch 951, loss: 1.848, 30432/60000 datapoints
2025-03-07 18:48:43,461 - INFO - training batch 1001, loss: 1.956, 32032/60000 datapoints
2025-03-07 18:48:43,957 - INFO - training batch 1051, loss: 1.807, 33632/60000 datapoints
2025-03-07 18:48:44,260 - INFO - training batch 1101, loss: 1.845, 35232/60000 datapoints
2025-03-07 18:48:44,534 - INFO - training batch 1151, loss: 1.795, 36832/60000 datapoints
2025-03-07 18:48:44,820 - INFO - training batch 1201, loss: 1.806, 38432/60000 datapoints
2025-03-07 18:48:45,130 - INFO - training batch 1251, loss: 1.840, 40032/60000 datapoints
2025-03-07 18:48:45,439 - INFO - training batch 1301, loss: 1.766, 41632/60000 datapoints
2025-03-07 18:48:45,767 - INFO - training batch 1351, loss: 1.815, 43232/60000 datapoints
2025-03-07 18:48:46,101 - INFO - training batch 1401, loss: 1.769, 44832/60000 datapoints
2025-03-07 18:48:46,458 - INFO - training batch 1451, loss: 1.841, 46432/60000 datapoints
2025-03-07 18:48:46,840 - INFO - training batch 1501, loss: 1.795, 48032/60000 datapoints
2025-03-07 18:48:47,206 - INFO - training batch 1551, loss: 1.814, 49632/60000 datapoints
2025-03-07 18:48:47,532 - INFO - training batch 1601, loss: 1.713, 51232/60000 datapoints
2025-03-07 18:48:48,323 - INFO - training batch 1651, loss: 1.765, 52832/60000 datapoints
2025-03-07 18:48:48,753 - INFO - training batch 1701, loss: 1.773, 54432/60000 datapoints
2025-03-07 18:48:49,157 - INFO - training batch 1751, loss: 1.759, 56032/60000 datapoints
2025-03-07 18:48:49,572 - INFO - training batch 1801, loss: 1.816, 57632/60000 datapoints
2025-03-07 18:48:49,994 - INFO - training batch 1851, loss: 1.795, 59232/60000 datapoints
2025-03-07 18:48:50,230 - INFO - validation batch 1, loss: 1.783, 32/10016 datapoints
2025-03-07 18:48:50,541 - INFO - validation batch 51, loss: 1.819, 1632/10016 datapoints
2025-03-07 18:48:50,820 - INFO - validation batch 101, loss: 1.713, 3232/10016 datapoints
2025-03-07 18:48:51,188 - INFO - validation batch 151, loss: 1.723, 4832/10016 datapoints
2025-03-07 18:48:51,484 - INFO - validation batch 201, loss: 1.750, 6432/10016 datapoints
2025-03-07 18:48:51,832 - INFO - validation batch 251, loss: 1.854, 8032/10016 datapoints
2025-03-07 18:48:52,102 - INFO - validation batch 301, loss: 1.874, 9632/10016 datapoints
2025-03-07 18:48:52,304 - INFO - Epoch 21/800 done.
2025-03-07 18:48:52,304 - INFO - Final validation performance:
Loss: 1.788, top-1 acc: 0.651top-5 acc: 0.651
2025-03-07 18:48:52,306 - INFO - Beginning epoch 22/800
2025-03-07 18:48:52,316 - INFO - training batch 1, loss: 1.803, 32/60000 datapoints
2025-03-07 18:48:52,660 - INFO - training batch 51, loss: 1.737, 1632/60000 datapoints
2025-03-07 18:48:53,065 - INFO - training batch 101, loss: 1.663, 3232/60000 datapoints
2025-03-07 18:48:53,396 - INFO - training batch 151, loss: 1.780, 4832/60000 datapoints
2025-03-07 18:48:54,089 - INFO - training batch 201, loss: 1.761, 6432/60000 datapoints
2025-03-07 18:48:54,436 - INFO - training batch 251, loss: 1.799, 8032/60000 datapoints
2025-03-07 18:48:54,739 - INFO - training batch 301, loss: 1.922, 9632/60000 datapoints
2025-03-07 18:48:55,018 - INFO - training batch 351, loss: 1.795, 11232/60000 datapoints
2025-03-07 18:48:55,403 - INFO - training batch 401, loss: 1.744, 12832/60000 datapoints
2025-03-07 18:48:55,846 - INFO - training batch 451, loss: 1.829, 14432/60000 datapoints
2025-03-07 18:48:56,300 - INFO - training batch 501, loss: 1.736, 16032/60000 datapoints
2025-03-07 18:48:56,725 - INFO - training batch 551, loss: 1.820, 17632/60000 datapoints
2025-03-07 18:48:57,285 - INFO - training batch 601, loss: 1.785, 19232/60000 datapoints
2025-03-07 18:48:57,698 - INFO - training batch 651, loss: 1.733, 20832/60000 datapoints
2025-03-07 18:48:58,066 - INFO - training batch 701, loss: 1.864, 22432/60000 datapoints
2025-03-07 18:48:58,447 - INFO - training batch 751, loss: 1.808, 24032/60000 datapoints
2025-03-07 18:48:58,830 - INFO - training batch 801, loss: 1.836, 25632/60000 datapoints
2025-03-07 18:48:59,377 - INFO - training batch 851, loss: 1.855, 27232/60000 datapoints
2025-03-07 18:48:59,695 - INFO - training batch 901, loss: 1.801, 28832/60000 datapoints
2025-03-07 18:49:00,018 - INFO - training batch 951, loss: 1.771, 30432/60000 datapoints
2025-03-07 18:49:00,391 - INFO - training batch 1001, loss: 1.708, 32032/60000 datapoints
2025-03-07 18:49:00,737 - INFO - training batch 1051, loss: 1.817, 33632/60000 datapoints
2025-03-07 18:49:01,087 - INFO - training batch 1101, loss: 1.848, 35232/60000 datapoints
2025-03-07 18:49:01,489 - INFO - training batch 1151, loss: 1.714, 36832/60000 datapoints
2025-03-07 18:49:01,979 - INFO - training batch 1201, loss: 1.775, 38432/60000 datapoints
2025-03-07 18:49:02,354 - INFO - training batch 1251, loss: 1.798, 40032/60000 datapoints
2025-03-07 18:49:02,706 - INFO - training batch 1301, loss: 1.787, 41632/60000 datapoints
2025-03-07 18:49:03,039 - INFO - training batch 1351, loss: 1.782, 43232/60000 datapoints
2025-03-07 18:49:03,529 - INFO - training batch 1401, loss: 1.772, 44832/60000 datapoints
2025-03-07 18:49:03,944 - INFO - training batch 1451, loss: 1.824, 46432/60000 datapoints
2025-03-07 18:49:04,501 - INFO - training batch 1501, loss: 1.682, 48032/60000 datapoints
2025-03-07 18:49:04,999 - INFO - training batch 1551, loss: 1.747, 49632/60000 datapoints
2025-03-07 18:49:05,362 - INFO - training batch 1601, loss: 1.878, 51232/60000 datapoints
2025-03-07 18:49:07,366 - INFO - training batch 1651, loss: 1.710, 52832/60000 datapoints
2025-03-07 18:49:10,066 - INFO - training batch 1701, loss: 1.687, 54432/60000 datapoints
2025-03-07 18:49:10,662 - INFO - training batch 1751, loss: 1.906, 56032/60000 datapoints
2025-03-07 18:49:11,266 - INFO - training batch 1801, loss: 1.658, 57632/60000 datapoints
2025-03-07 18:49:11,907 - INFO - training batch 1851, loss: 1.766, 59232/60000 datapoints
2025-03-07 18:49:12,294 - INFO - validation batch 1, loss: 1.805, 32/10016 datapoints
2025-03-07 18:49:12,718 - INFO - validation batch 51, loss: 1.817, 1632/10016 datapoints
2025-03-07 18:49:13,028 - INFO - validation batch 101, loss: 1.681, 3232/10016 datapoints
2025-03-07 18:49:13,572 - INFO - validation batch 151, loss: 1.740, 4832/10016 datapoints
2025-03-07 18:49:14,078 - INFO - validation batch 201, loss: 1.697, 6432/10016 datapoints
2025-03-07 18:49:14,463 - INFO - validation batch 251, loss: 1.894, 8032/10016 datapoints
2025-03-07 18:49:15,058 - INFO - Beginning epoch 23/800
2025-03-07 18:49:15,088 - INFO - training batch 1, loss: 1.744, 32/60000 datapoints
2025-03-07 18:49:15,764 - INFO - training batch 51, loss: 1.675, 1632/60000 datapoints
2025-03-07 18:49:16,260 - INFO - training batch 101, loss: 1.825, 3232/60000 datapoints
2025-03-07 18:49:16,698 - INFO - training batch 151, loss: 1.751, 4832/60000 datapoints
2025-03-07 18:49:17,617 - INFO - training batch 201, loss: 1.824, 6432/60000 datapoints
2025-03-07 18:49:18,830 - INFO - training batch 251, loss: 1.538, 8032/60000 datapoints
2025-03-07 18:49:19,372 - INFO - training batch 301, loss: 1.818, 9632/60000 datapoints
2025-03-07 18:49:20,375 - INFO - training batch 351, loss: 1.841, 11232/60000 datapoints
2025-03-07 18:49:20,848 - INFO - training batch 401, loss: 1.800, 12832/60000 datapoints
2025-03-07 18:49:21,356 - INFO - training batch 451, loss: 1.627, 14432/60000 datapoints
2025-03-07 18:49:21,770 - INFO - training batch 501, loss: 1.655, 16032/60000 datapoints
2025-03-07 18:49:22,449 - INFO - training batch 551, loss: 1.671, 17632/60000 datapoints
2025-03-07 18:49:23,133 - INFO - training batch 601, loss: 1.588, 19232/60000 datapoints
2025-03-07 18:49:23,645 - INFO - training batch 651, loss: 1.731, 20832/60000 datapoints
2025-03-07 18:49:24,157 - INFO - training batch 701, loss: 1.746, 22432/60000 datapoints
2025-03-07 18:49:24,750 - INFO - training batch 751, loss: 1.846, 24032/60000 datapoints
2025-03-07 18:49:25,269 - INFO - training batch 801, loss: 1.732, 25632/60000 datapoints
2025-03-07 18:49:25,712 - INFO - training batch 851, loss: 1.754, 27232/60000 datapoints
2025-03-07 18:49:26,176 - INFO - training batch 901, loss: 1.803, 28832/60000 datapoints
2025-03-07 18:49:26,555 - INFO - training batch 951, loss: 1.832, 30432/60000 datapoints
2025-03-07 18:49:27,011 - INFO - training batch 1001, loss: 1.731, 32032/60000 datapoints
2025-03-07 18:49:27,522 - INFO - training batch 1051, loss: 1.819, 33632/60000 datapoints
2025-03-07 18:49:27,948 - INFO - training batch 1101, loss: 1.831, 35232/60000 datapoints
2025-03-07 18:49:28,407 - INFO - training batch 1151, loss: 1.835, 36832/60000 datapoints
2025-03-07 18:49:28,790 - INFO - training batch 1201, loss: 1.773, 38432/60000 datapoints
2025-03-07 18:49:29,155 - INFO - training batch 1251, loss: 1.709, 40032/60000 datapoints
2025-03-07 18:49:29,535 - INFO - training batch 1301, loss: 1.757, 41632/60000 datapoints
2025-03-07 18:49:29,855 - INFO - training batch 1351, loss: 1.828, 43232/60000 datapoints
2025-03-07 18:49:32,031 - INFO - training batch 1401, loss: 1.760, 44832/60000 datapoints
2025-03-07 18:49:33,447 - INFO - training batch 1451, loss: 1.698, 46432/60000 datapoints
2025-03-07 18:49:34,315 - INFO - training batch 1501, loss: 1.733, 48032/60000 datapoints
2025-03-07 18:49:34,978 - INFO - training batch 1551, loss: 1.713, 49632/60000 datapoints
2025-03-07 18:49:35,566 - INFO - training batch 1601, loss: 1.726, 51232/60000 datapoints
2025-03-07 18:49:36,250 - INFO - training batch 1651, loss: 1.743, 52832/60000 datapoints
2025-03-07 18:49:36,827 - INFO - training batch 1701, loss: 1.725, 54432/60000 datapoints
2025-03-07 18:49:37,294 - INFO - training batch 1751, loss: 1.744, 56032/60000 datapoints
2025-03-07 18:49:37,810 - INFO - training batch 1801, loss: 1.733, 57632/60000 datapoints
2025-03-07 18:49:38,301 - INFO - training batch 1851, loss: 1.796, 59232/60000 datapoints
2025-03-07 18:49:39,010 - INFO - validation batch 1, loss: 1.677, 32/10016 datapoints
2025-03-07 18:49:39,479 - INFO - validation batch 51, loss: 1.786, 1632/10016 datapoints
2025-03-07 18:49:39,793 - INFO - validation batch 101, loss: 1.701, 3232/10016 datapoints
2025-03-07 18:49:40,108 - INFO - validation batch 151, loss: 1.660, 4832/10016 datapoints
2025-03-07 18:49:40,418 - INFO - validation batch 201, loss: 1.777, 6432/10016 datapoints
2025-03-07 18:49:40,853 - INFO - validation batch 251, loss: 1.752, 8032/10016 datapoints
2025-03-07 18:49:41,239 - INFO - validation batch 301, loss: 1.614, 9632/10016 datapoints
2025-03-07 18:49:41,316 - INFO - Epoch 23/800 done.
2025-03-07 18:49:41,319 - INFO - Final validation performance:
Loss: 1.709, top-1 acc: 0.667top-5 acc: 0.667
2025-03-07 18:49:41,324 - INFO - Beginning epoch 24/800
2025-03-07 18:49:41,338 - INFO - training batch 1, loss: 1.734, 32/60000 datapoints
2025-03-07 18:49:41,774 - INFO - training batch 51, loss: 1.673, 1632/60000 datapoints
2025-03-07 18:49:43,770 - INFO - training batch 101, loss: 1.783, 3232/60000 datapoints
2025-03-07 18:49:44,649 - INFO - training batch 151, loss: 1.755, 4832/60000 datapoints
2025-03-07 18:49:45,129 - INFO - training batch 201, loss: 1.644, 6432/60000 datapoints
2025-03-07 18:49:45,572 - INFO - training batch 251, loss: 1.680, 8032/60000 datapoints
2025-03-07 18:49:46,138 - INFO - training batch 301, loss: 1.690, 9632/60000 datapoints
2025-03-07 18:49:46,694 - INFO - training batch 351, loss: 1.817, 11232/60000 datapoints
2025-03-07 18:49:47,188 - INFO - training batch 401, loss: 1.657, 12832/60000 datapoints
2025-03-07 18:49:47,711 - INFO - training batch 451, loss: 1.798, 14432/60000 datapoints
2025-03-07 18:49:48,266 - INFO - training batch 501, loss: 1.713, 16032/60000 datapoints
2025-03-07 18:49:48,647 - INFO - training batch 551, loss: 1.703, 17632/60000 datapoints
2025-03-07 18:49:49,131 - INFO - training batch 601, loss: 1.813, 19232/60000 datapoints
2025-03-07 18:49:49,581 - INFO - training batch 651, loss: 1.615, 20832/60000 datapoints
2025-03-07 18:49:49,996 - INFO - training batch 701, loss: 1.780, 22432/60000 datapoints
2025-03-07 18:49:50,347 - INFO - training batch 751, loss: 1.683, 24032/60000 datapoints
2025-03-07 18:49:50,714 - INFO - training batch 801, loss: 1.793, 25632/60000 datapoints
2025-03-07 18:49:51,052 - INFO - training batch 851, loss: 1.792, 27232/60000 datapoints
2025-03-07 18:49:51,400 - INFO - training batch 901, loss: 1.774, 28832/60000 datapoints
2025-03-07 18:49:51,840 - INFO - training batch 951, loss: 1.853, 30432/60000 datapoints
2025-03-07 18:49:52,340 - INFO - training batch 1001, loss: 1.690, 32032/60000 datapoints
2025-03-07 18:49:52,809 - INFO - training batch 1051, loss: 1.714, 33632/60000 datapoints
2025-03-07 18:49:53,245 - INFO - training batch 1101, loss: 1.595, 35232/60000 datapoints
2025-03-07 18:49:53,715 - INFO - training batch 1151, loss: 1.549, 36832/60000 datapoints
2025-03-07 18:49:54,127 - INFO - training batch 1201, loss: 1.800, 38432/60000 datapoints
2025-03-07 18:49:54,431 - INFO - training batch 1251, loss: 1.638, 40032/60000 datapoints
2025-03-07 18:49:54,732 - INFO - training batch 1301, loss: 1.654, 41632/60000 datapoints
2025-03-07 18:49:55,038 - INFO - training batch 1351, loss: 1.722, 43232/60000 datapoints
2025-03-07 18:49:55,440 - INFO - training batch 1401, loss: 1.651, 44832/60000 datapoints
2025-03-07 18:49:55,944 - INFO - training batch 1451, loss: 1.763, 46432/60000 datapoints
2025-03-07 18:49:56,544 - INFO - training batch 1501, loss: 1.658, 48032/60000 datapoints
2025-03-07 18:49:56,936 - INFO - training batch 1551, loss: 1.781, 49632/60000 datapoints
2025-03-07 18:49:57,338 - INFO - training batch 1601, loss: 1.761, 51232/60000 datapoints
2025-03-07 18:49:57,644 - INFO - training batch 1651, loss: 1.761, 52832/60000 datapoints
2025-03-07 18:49:57,944 - INFO - training batch 1701, loss: 1.614, 54432/60000 datapoints
2025-03-07 18:49:58,231 - INFO - training batch 1751, loss: 1.624, 56032/60000 datapoints
2025-03-07 18:49:58,678 - INFO - training batch 1801, loss: 1.703, 57632/60000 datapoints
2025-03-07 18:49:59,138 - INFO - training batch 1851, loss: 1.687, 59232/60000 datapoints
2025-03-07 18:49:59,365 - INFO - validation batch 1, loss: 1.776, 32/10016 datapoints
2025-03-07 18:49:59,636 - INFO - validation batch 51, loss: 1.756, 1632/10016 datapoints
2025-03-07 18:49:59,892 - INFO - validation batch 101, loss: 1.698, 3232/10016 datapoints
2025-03-07 18:50:00,138 - INFO - validation batch 151, loss: 1.666, 4832/10016 datapoints
2025-03-07 18:50:00,424 - INFO - validation batch 201, loss: 1.779, 6432/10016 datapoints
2025-03-07 18:50:00,720 - INFO - validation batch 251, loss: 1.748, 8032/10016 datapoints
2025-03-07 18:50:01,150 - INFO - validation batch 301, loss: 1.675, 9632/10016 datapoints
2025-03-07 18:50:01,204 - INFO - Epoch 24/800 done.
2025-03-07 18:50:01,204 - INFO - Final validation performance:
Loss: 1.728, top-1 acc: 0.674top-5 acc: 0.674
2025-03-07 18:50:01,205 - INFO - Beginning epoch 25/800
2025-03-07 18:50:01,215 - INFO - training batch 1, loss: 1.614, 32/60000 datapoints
2025-03-07 18:50:01,595 - INFO - training batch 51, loss: 1.675, 1632/60000 datapoints
2025-03-07 18:50:02,029 - INFO - training batch 101, loss: 1.834, 3232/60000 datapoints
2025-03-07 18:50:02,510 - INFO - training batch 151, loss: 1.685, 4832/60000 datapoints
2025-03-07 18:50:02,971 - INFO - training batch 201, loss: 1.623, 6432/60000 datapoints
2025-03-07 18:50:03,433 - INFO - training batch 251, loss: 1.812, 8032/60000 datapoints
2025-03-07 18:50:04,158 - INFO - training batch 301, loss: 1.637, 9632/60000 datapoints
2025-03-07 18:50:04,677 - INFO - training batch 351, loss: 1.692, 11232/60000 datapoints
2025-03-07 18:50:05,098 - INFO - training batch 401, loss: 1.623, 12832/60000 datapoints
2025-03-07 18:50:05,656 - INFO - training batch 451, loss: 1.716, 14432/60000 datapoints
2025-03-07 18:50:06,154 - INFO - training batch 501, loss: 1.824, 16032/60000 datapoints
2025-03-07 18:50:06,588 - INFO - training batch 551, loss: 1.772, 17632/60000 datapoints
2025-03-07 18:50:07,032 - INFO - training batch 601, loss: 1.761, 19232/60000 datapoints
2025-03-07 18:50:07,445 - INFO - training batch 651, loss: 1.793, 20832/60000 datapoints
2025-03-07 18:50:07,769 - INFO - training batch 701, loss: 1.659, 22432/60000 datapoints
2025-03-07 18:50:08,103 - INFO - training batch 751, loss: 1.723, 24032/60000 datapoints
2025-03-07 18:50:08,451 - INFO - training batch 801, loss: 1.743, 25632/60000 datapoints
2025-03-07 18:50:08,776 - INFO - training batch 851, loss: 1.675, 27232/60000 datapoints
2025-03-07 18:50:09,122 - INFO - training batch 901, loss: 1.631, 28832/60000 datapoints
2025-03-07 18:50:09,470 - INFO - training batch 951, loss: 1.568, 30432/60000 datapoints
2025-03-07 18:50:09,780 - INFO - training batch 1001, loss: 1.720, 32032/60000 datapoints
2025-03-07 18:50:10,206 - INFO - training batch 1051, loss: 1.700, 33632/60000 datapoints
2025-03-07 18:50:10,552 - INFO - training batch 1101, loss: 1.579, 35232/60000 datapoints
2025-03-07 18:50:10,893 - INFO - training batch 1151, loss: 1.612, 36832/60000 datapoints
2025-03-07 18:50:11,351 - INFO - training batch 1201, loss: 1.643, 38432/60000 datapoints
2025-03-07 18:50:11,709 - INFO - training batch 1251, loss: 1.724, 40032/60000 datapoints
2025-03-07 18:50:12,004 - INFO - training batch 1301, loss: 1.711, 41632/60000 datapoints
2025-03-07 18:50:12,478 - INFO - training batch 1351, loss: 1.646, 43232/60000 datapoints
2025-03-07 18:50:12,851 - INFO - training batch 1401, loss: 1.765, 44832/60000 datapoints
2025-03-07 18:50:13,153 - INFO - training batch 1451, loss: 1.776, 46432/60000 datapoints
2025-03-07 18:50:13,481 - INFO - training batch 1501, loss: 1.669, 48032/60000 datapoints
2025-03-07 18:50:13,791 - INFO - training batch 1551, loss: 1.838, 49632/60000 datapoints
2025-03-07 18:50:14,138 - INFO - training batch 1601, loss: 1.831, 51232/60000 datapoints
2025-03-07 18:50:14,500 - INFO - training batch 1651, loss: 1.665, 52832/60000 datapoints
2025-03-07 18:50:14,814 - INFO - training batch 1701, loss: 1.659, 54432/60000 datapoints
2025-03-07 18:50:15,145 - INFO - training batch 1751, loss: 1.704, 56032/60000 datapoints
2025-03-07 18:50:15,598 - INFO - training batch 1801, loss: 1.621, 57632/60000 datapoints
2025-03-07 18:50:15,895 - INFO - training batch 1851, loss: 1.708, 59232/60000 datapoints
2025-03-07 18:50:16,090 - INFO - validation batch 1, loss: 1.678, 32/10016 datapoints
2025-03-07 18:50:16,381 - INFO - validation batch 51, loss: 1.693, 1632/10016 datapoints
2025-03-07 18:50:16,856 - INFO - validation batch 101, loss: 1.726, 3232/10016 datapoints
2025-03-07 18:50:17,173 - INFO - validation batch 151, loss: 1.618, 4832/10016 datapoints
2025-03-07 18:50:17,518 - INFO - validation batch 201, loss: 1.504, 6432/10016 datapoints
2025-03-07 18:50:17,856 - INFO - validation batch 251, loss: 1.730, 8032/10016 datapoints
2025-03-07 18:50:18,137 - INFO - validation batch 301, loss: 1.607, 9632/10016 datapoints
2025-03-07 18:50:18,219 - INFO - Epoch 25/800 done.
2025-03-07 18:50:18,219 - INFO - Final validation performance:
Loss: 1.651, top-1 acc: 0.682top-5 acc: 0.682
2025-03-07 18:50:18,220 - INFO - Beginning epoch 26/800
2025-03-07 18:50:18,230 - INFO - training batch 1, loss: 1.536, 32/60000 datapoints
2025-03-07 18:50:18,594 - INFO - training batch 51, loss: 1.557, 1632/60000 datapoints
2025-03-07 18:50:19,374 - INFO - training batch 101, loss: 1.722, 3232/60000 datapoints
2025-03-07 18:50:19,818 - INFO - training batch 151, loss: 1.587, 4832/60000 datapoints
2025-03-07 18:50:20,563 - INFO - training batch 201, loss: 1.573, 6432/60000 datapoints
2025-03-07 18:50:21,177 - INFO - training batch 251, loss: 1.795, 8032/60000 datapoints
2025-03-07 18:50:21,559 - INFO - training batch 301, loss: 1.673, 9632/60000 datapoints
2025-03-07 18:50:21,946 - INFO - training batch 351, loss: 1.766, 11232/60000 datapoints
2025-03-07 18:50:22,308 - INFO - training batch 401, loss: 1.669, 12832/60000 datapoints
2025-03-07 18:50:22,652 - INFO - training batch 451, loss: 1.713, 14432/60000 datapoints
2025-03-07 18:50:23,020 - INFO - training batch 501, loss: 1.584, 16032/60000 datapoints
2025-03-07 18:50:23,377 - INFO - training batch 551, loss: 1.671, 17632/60000 datapoints
2025-03-07 18:50:23,743 - INFO - training batch 601, loss: 1.732, 19232/60000 datapoints
2025-03-07 18:50:24,146 - INFO - training batch 651, loss: 1.658, 20832/60000 datapoints
2025-03-07 18:50:24,489 - INFO - training batch 701, loss: 1.667, 22432/60000 datapoints
2025-03-07 18:50:24,798 - INFO - training batch 751, loss: 1.704, 24032/60000 datapoints
2025-03-07 18:50:25,087 - INFO - training batch 801, loss: 1.817, 25632/60000 datapoints
2025-03-07 18:50:25,361 - INFO - training batch 851, loss: 1.753, 27232/60000 datapoints
2025-03-07 18:50:25,682 - INFO - training batch 901, loss: 1.532, 28832/60000 datapoints
2025-03-07 18:50:25,969 - INFO - training batch 951, loss: 1.530, 30432/60000 datapoints
2025-03-07 18:50:26,260 - INFO - training batch 1001, loss: 1.706, 32032/60000 datapoints
2025-03-07 18:50:26,535 - INFO - training batch 1051, loss: 1.643, 33632/60000 datapoints
2025-03-07 18:50:26,817 - INFO - training batch 1101, loss: 1.767, 35232/60000 datapoints
2025-03-07 18:50:27,211 - INFO - training batch 1151, loss: 1.657, 36832/60000 datapoints
2025-03-07 18:50:27,673 - INFO - training batch 1201, loss: 1.694, 38432/60000 datapoints
2025-03-07 18:50:27,956 - INFO - training batch 1251, loss: 1.659, 40032/60000 datapoints
2025-03-07 18:50:28,357 - INFO - training batch 1301, loss: 1.604, 41632/60000 datapoints
2025-03-07 18:50:28,653 - INFO - training batch 1351, loss: 1.523, 43232/60000 datapoints
2025-03-07 18:50:28,930 - INFO - training batch 1401, loss: 1.737, 44832/60000 datapoints
2025-03-07 18:50:29,212 - INFO - training batch 1451, loss: 1.705, 46432/60000 datapoints
2025-03-07 18:50:29,492 - INFO - training batch 1501, loss: 1.657, 48032/60000 datapoints
2025-03-07 18:50:29,787 - INFO - training batch 1551, loss: 1.579, 49632/60000 datapoints
2025-03-07 18:50:30,061 - INFO - training batch 1601, loss: 1.592, 51232/60000 datapoints
2025-03-07 18:50:30,352 - INFO - training batch 1651, loss: 1.618, 52832/60000 datapoints
2025-03-07 18:50:30,629 - INFO - training batch 1701, loss: 1.586, 54432/60000 datapoints
2025-03-07 18:50:30,980 - INFO - training batch 1751, loss: 1.616, 56032/60000 datapoints
2025-03-07 18:50:32,031 - INFO - training batch 1801, loss: 1.755, 57632/60000 datapoints
2025-03-07 18:50:33,156 - INFO - training batch 1851, loss: 1.638, 59232/60000 datapoints
2025-03-07 18:50:33,358 - INFO - validation batch 1, loss: 1.685, 32/10016 datapoints
2025-03-07 18:50:33,769 - INFO - validation batch 51, loss: 1.583, 1632/10016 datapoints
2025-03-07 18:50:34,371 - INFO - validation batch 101, loss: 1.697, 3232/10016 datapoints
2025-03-07 18:50:34,884 - INFO - validation batch 151, loss: 1.705, 4832/10016 datapoints
2025-03-07 18:50:35,257 - INFO - validation batch 201, loss: 1.642, 6432/10016 datapoints
2025-03-07 18:50:35,557 - INFO - validation batch 251, loss: 1.642, 8032/10016 datapoints
2025-03-07 18:50:35,877 - INFO - validation batch 301, loss: 1.702, 9632/10016 datapoints
2025-03-07 18:50:35,941 - INFO - Epoch 26/800 done.
2025-03-07 18:50:35,941 - INFO - Final validation performance:
Loss: 1.665, top-1 acc: 0.690top-5 acc: 0.690
2025-03-07 18:50:35,946 - INFO - Beginning epoch 27/800
2025-03-07 18:50:35,958 - INFO - training batch 1, loss: 1.682, 32/60000 datapoints
2025-03-07 18:50:36,300 - INFO - training batch 51, loss: 1.620, 1632/60000 datapoints
2025-03-07 18:50:36,629 - INFO - training batch 101, loss: 1.519, 3232/60000 datapoints
2025-03-07 18:50:37,016 - INFO - training batch 151, loss: 1.745, 4832/60000 datapoints
2025-03-07 18:50:38,165 - INFO - training batch 201, loss: 1.594, 6432/60000 datapoints
2025-03-07 18:50:38,677 - INFO - training batch 251, loss: 1.569, 8032/60000 datapoints
2025-03-07 18:50:39,143 - INFO - training batch 301, loss: 1.658, 9632/60000 datapoints
2025-03-07 18:50:39,659 - INFO - training batch 351, loss: 1.717, 11232/60000 datapoints
2025-03-07 18:50:40,191 - INFO - training batch 401, loss: 1.629, 12832/60000 datapoints
2025-03-07 18:50:40,813 - INFO - training batch 451, loss: 1.839, 14432/60000 datapoints
2025-03-07 18:50:41,249 - INFO - training batch 501, loss: 1.588, 16032/60000 datapoints
2025-03-07 18:50:41,721 - INFO - training batch 551, loss: 1.581, 17632/60000 datapoints
2025-03-07 18:50:42,172 - INFO - training batch 601, loss: 1.700, 19232/60000 datapoints
2025-03-07 18:50:42,573 - INFO - training batch 651, loss: 1.705, 20832/60000 datapoints
2025-03-07 18:50:42,979 - INFO - training batch 701, loss: 1.534, 22432/60000 datapoints
2025-03-07 18:50:43,334 - INFO - training batch 751, loss: 1.713, 24032/60000 datapoints
2025-03-07 18:50:43,793 - INFO - training batch 801, loss: 1.526, 25632/60000 datapoints
2025-03-07 18:50:44,621 - INFO - training batch 851, loss: 1.723, 27232/60000 datapoints
2025-03-07 18:50:44,920 - INFO - training batch 901, loss: 1.687, 28832/60000 datapoints
2025-03-07 18:50:45,331 - INFO - training batch 951, loss: 1.573, 30432/60000 datapoints
2025-03-07 18:50:45,850 - INFO - training batch 1001, loss: 1.652, 32032/60000 datapoints
2025-03-07 18:50:46,711 - INFO - training batch 1051, loss: 1.665, 33632/60000 datapoints
2025-03-07 18:50:47,262 - INFO - training batch 1101, loss: 1.600, 35232/60000 datapoints
2025-03-07 18:50:48,038 - INFO - training batch 1151, loss: 1.628, 36832/60000 datapoints
2025-03-07 18:50:48,565 - INFO - training batch 1201, loss: 1.613, 38432/60000 datapoints
2025-03-07 18:50:49,013 - INFO - training batch 1251, loss: 1.692, 40032/60000 datapoints
2025-03-07 18:50:49,571 - INFO - training batch 1301, loss: 1.405, 41632/60000 datapoints
2025-03-07 18:50:49,877 - INFO - training batch 1351, loss: 1.512, 43232/60000 datapoints
2025-03-07 18:50:50,194 - INFO - training batch 1401, loss: 1.595, 44832/60000 datapoints
2025-03-07 18:50:50,503 - INFO - training batch 1451, loss: 1.513, 46432/60000 datapoints
2025-03-07 18:50:50,815 - INFO - training batch 1501, loss: 1.654, 48032/60000 datapoints
2025-03-07 18:50:51,105 - INFO - training batch 1551, loss: 1.634, 49632/60000 datapoints
2025-03-07 18:50:51,398 - INFO - training batch 1601, loss: 1.569, 51232/60000 datapoints
2025-03-07 18:50:51,687 - INFO - training batch 1651, loss: 1.644, 52832/60000 datapoints
2025-03-07 18:50:51,969 - INFO - training batch 1701, loss: 1.642, 54432/60000 datapoints
2025-03-07 18:50:52,249 - INFO - training batch 1751, loss: 1.612, 56032/60000 datapoints
2025-03-07 18:50:52,550 - INFO - training batch 1801, loss: 1.479, 57632/60000 datapoints
2025-03-07 18:50:52,828 - INFO - training batch 1851, loss: 1.563, 59232/60000 datapoints
2025-03-07 18:50:52,984 - INFO - validation batch 1, loss: 1.553, 32/10016 datapoints
2025-03-07 18:50:53,206 - INFO - validation batch 51, loss: 1.641, 1632/10016 datapoints
2025-03-07 18:50:53,427 - INFO - validation batch 101, loss: 1.596, 3232/10016 datapoints
2025-03-07 18:50:53,654 - INFO - validation batch 151, loss: 1.690, 4832/10016 datapoints
2025-03-07 18:50:53,876 - INFO - validation batch 201, loss: 1.662, 6432/10016 datapoints
2025-03-07 18:50:54,112 - INFO - validation batch 251, loss: 1.631, 8032/10016 datapoints
2025-03-07 18:50:54,332 - INFO - validation batch 301, loss: 1.626, 9632/10016 datapoints
2025-03-07 18:50:54,391 - INFO - Epoch 27/800 done.
2025-03-07 18:50:54,392 - INFO - Final validation performance:
Loss: 1.628, top-1 acc: 0.697top-5 acc: 0.697
2025-03-07 18:50:54,393 - INFO - Beginning epoch 28/800
2025-03-07 18:50:54,404 - INFO - training batch 1, loss: 1.671, 32/60000 datapoints
2025-03-07 18:50:54,819 - INFO - training batch 51, loss: 1.572, 1632/60000 datapoints
2025-03-07 18:50:55,213 - INFO - training batch 101, loss: 1.605, 3232/60000 datapoints
2025-03-07 18:50:55,722 - INFO - training batch 151, loss: 1.512, 4832/60000 datapoints
2025-03-07 18:50:56,799 - INFO - training batch 201, loss: 1.659, 6432/60000 datapoints
2025-03-07 18:50:57,494 - INFO - training batch 251, loss: 1.575, 8032/60000 datapoints
2025-03-07 18:50:57,927 - INFO - training batch 301, loss: 1.608, 9632/60000 datapoints
2025-03-07 18:50:58,225 - INFO - training batch 351, loss: 1.623, 11232/60000 datapoints
2025-03-07 18:50:58,589 - INFO - training batch 401, loss: 1.536, 12832/60000 datapoints
2025-03-07 18:50:58,962 - INFO - training batch 451, loss: 1.690, 14432/60000 datapoints
2025-03-07 18:50:59,340 - INFO - training batch 501, loss: 1.548, 16032/60000 datapoints
2025-03-07 18:50:59,720 - INFO - training batch 551, loss: 1.558, 17632/60000 datapoints
2025-03-07 18:51:00,111 - INFO - training batch 601, loss: 1.642, 19232/60000 datapoints
2025-03-07 18:51:00,582 - INFO - training batch 651, loss: 1.725, 20832/60000 datapoints
2025-03-07 18:51:01,122 - INFO - training batch 701, loss: 1.691, 22432/60000 datapoints
2025-03-07 18:51:01,533 - INFO - training batch 751, loss: 1.637, 24032/60000 datapoints
2025-03-07 18:51:01,992 - INFO - training batch 801, loss: 1.555, 25632/60000 datapoints
2025-03-07 18:51:02,445 - INFO - training batch 851, loss: 1.697, 27232/60000 datapoints
2025-03-07 18:51:02,868 - INFO - training batch 901, loss: 1.640, 28832/60000 datapoints
2025-03-07 18:51:03,307 - INFO - training batch 951, loss: 1.613, 30432/60000 datapoints
2025-03-07 18:51:03,709 - INFO - training batch 1001, loss: 1.569, 32032/60000 datapoints
2025-03-07 18:51:04,111 - INFO - training batch 1051, loss: 1.450, 33632/60000 datapoints
2025-03-07 18:51:04,505 - INFO - training batch 1101, loss: 1.573, 35232/60000 datapoints
2025-03-07 18:51:04,914 - INFO - training batch 1151, loss: 1.576, 36832/60000 datapoints
2025-03-07 18:51:05,262 - INFO - training batch 1201, loss: 1.551, 38432/60000 datapoints
2025-03-07 18:51:05,587 - INFO - training batch 1251, loss: 1.630, 40032/60000 datapoints
2025-03-07 18:51:05,999 - INFO - training batch 1301, loss: 1.508, 41632/60000 datapoints
2025-03-07 18:51:06,438 - INFO - training batch 1351, loss: 1.644, 43232/60000 datapoints
2025-03-07 18:51:06,973 - INFO - training batch 1401, loss: 1.575, 44832/60000 datapoints
2025-03-07 18:51:07,393 - INFO - training batch 1451, loss: 1.508, 46432/60000 datapoints
2025-03-07 18:51:07,864 - INFO - training batch 1501, loss: 1.574, 48032/60000 datapoints
2025-03-07 18:51:08,224 - INFO - training batch 1551, loss: 1.412, 49632/60000 datapoints
2025-03-07 18:51:08,506 - INFO - training batch 1601, loss: 1.641, 51232/60000 datapoints
2025-03-07 18:51:08,794 - INFO - training batch 1651, loss: 1.622, 52832/60000 datapoints
2025-03-07 18:51:09,087 - INFO - training batch 1701, loss: 1.656, 54432/60000 datapoints
2025-03-07 18:51:09,374 - INFO - training batch 1751, loss: 1.665, 56032/60000 datapoints
2025-03-07 18:51:09,682 - INFO - training batch 1801, loss: 1.559, 57632/60000 datapoints
2025-03-07 18:51:10,027 - INFO - training batch 1851, loss: 1.398, 59232/60000 datapoints
2025-03-07 18:51:10,224 - INFO - validation batch 1, loss: 1.656, 32/10016 datapoints
2025-03-07 18:51:10,505 - INFO - validation batch 51, loss: 1.571, 1632/10016 datapoints
2025-03-07 18:51:10,767 - INFO - validation batch 101, loss: 1.602, 3232/10016 datapoints
2025-03-07 18:51:11,048 - INFO - validation batch 151, loss: 1.593, 4832/10016 datapoints
2025-03-07 18:51:11,298 - INFO - validation batch 201, loss: 1.478, 6432/10016 datapoints
2025-03-07 18:51:11,614 - INFO - validation batch 251, loss: 1.608, 8032/10016 datapoints
2025-03-07 18:51:11,954 - INFO - validation batch 301, loss: 1.696, 9632/10016 datapoints
2025-03-07 18:51:12,024 - INFO - Epoch 28/800 done.
2025-03-07 18:51:12,024 - INFO - Final validation performance:
Loss: 1.601, top-1 acc: 0.705top-5 acc: 0.705
2025-03-07 18:51:12,026 - INFO - Beginning epoch 29/800
2025-03-07 18:51:12,038 - INFO - training batch 1, loss: 1.613, 32/60000 datapoints
2025-03-07 18:51:12,462 - INFO - training batch 51, loss: 1.587, 1632/60000 datapoints
2025-03-07 18:51:12,870 - INFO - training batch 101, loss: 1.602, 3232/60000 datapoints
2025-03-07 18:51:13,306 - INFO - training batch 151, loss: 1.633, 4832/60000 datapoints
2025-03-07 18:51:13,731 - INFO - training batch 201, loss: 1.499, 6432/60000 datapoints
2025-03-07 18:51:14,186 - INFO - training batch 251, loss: 1.497, 8032/60000 datapoints
2025-03-07 18:51:14,575 - INFO - training batch 301, loss: 1.509, 9632/60000 datapoints
2025-03-07 18:51:14,916 - INFO - training batch 351, loss: 1.637, 11232/60000 datapoints
2025-03-07 18:51:15,273 - INFO - training batch 401, loss: 1.502, 12832/60000 datapoints
2025-03-07 18:51:15,581 - INFO - training batch 451, loss: 1.734, 14432/60000 datapoints
2025-03-07 18:51:15,879 - INFO - training batch 501, loss: 1.688, 16032/60000 datapoints
2025-03-07 18:51:16,214 - INFO - training batch 551, loss: 1.466, 17632/60000 datapoints
2025-03-07 18:51:16,603 - INFO - training batch 601, loss: 1.676, 19232/60000 datapoints
2025-03-07 18:51:16,985 - INFO - training batch 651, loss: 1.517, 20832/60000 datapoints
2025-03-07 18:51:17,352 - INFO - training batch 701, loss: 1.508, 22432/60000 datapoints
2025-03-07 18:51:17,807 - INFO - training batch 751, loss: 1.585, 24032/60000 datapoints
2025-03-07 18:51:18,230 - INFO - training batch 801, loss: 1.600, 25632/60000 datapoints
2025-03-07 18:51:18,521 - INFO - training batch 851, loss: 1.586, 27232/60000 datapoints
2025-03-07 18:51:19,180 - INFO - training batch 901, loss: 1.669, 28832/60000 datapoints
2025-03-07 18:51:19,714 - INFO - training batch 951, loss: 1.553, 30432/60000 datapoints
2025-03-07 18:51:20,169 - INFO - training batch 1001, loss: 1.414, 32032/60000 datapoints
2025-03-07 18:51:20,597 - INFO - training batch 1051, loss: 1.418, 33632/60000 datapoints
2025-03-07 18:51:20,935 - INFO - training batch 1101, loss: 1.658, 35232/60000 datapoints
2025-03-07 18:51:21,411 - INFO - training batch 1151, loss: 1.538, 36832/60000 datapoints
2025-03-07 18:51:21,716 - INFO - training batch 1201, loss: 1.505, 38432/60000 datapoints
2025-03-07 18:51:22,164 - INFO - training batch 1251, loss: 1.497, 40032/60000 datapoints
2025-03-07 18:51:22,441 - INFO - training batch 1301, loss: 1.399, 41632/60000 datapoints
2025-03-07 18:51:22,718 - INFO - training batch 1351, loss: 1.504, 43232/60000 datapoints
2025-03-07 18:51:23,006 - INFO - training batch 1401, loss: 1.375, 44832/60000 datapoints
2025-03-07 18:51:23,340 - INFO - training batch 1451, loss: 1.589, 46432/60000 datapoints
2025-03-07 18:51:23,623 - INFO - training batch 1501, loss: 1.397, 48032/60000 datapoints
2025-03-07 18:51:23,914 - INFO - training batch 1551, loss: 1.674, 49632/60000 datapoints
2025-03-07 18:51:24,315 - INFO - training batch 1601, loss: 1.497, 51232/60000 datapoints
2025-03-07 18:51:24,688 - INFO - training batch 1651, loss: 1.445, 52832/60000 datapoints
2025-03-07 18:51:25,077 - INFO - training batch 1701, loss: 1.475, 54432/60000 datapoints
2025-03-07 18:51:25,500 - INFO - training batch 1751, loss: 1.586, 56032/60000 datapoints
2025-03-07 18:51:25,875 - INFO - training batch 1801, loss: 1.545, 57632/60000 datapoints
2025-03-07 18:51:26,271 - INFO - training batch 1851, loss: 1.466, 59232/60000 datapoints
2025-03-07 18:51:26,421 - INFO - validation batch 1, loss: 1.418, 32/10016 datapoints
2025-03-07 18:51:26,719 - INFO - validation batch 51, loss: 1.521, 1632/10016 datapoints
2025-03-07 18:51:27,045 - INFO - validation batch 101, loss: 1.522, 3232/10016 datapoints
2025-03-07 18:51:27,347 - INFO - validation batch 151, loss: 1.704, 4832/10016 datapoints
2025-03-07 18:51:27,599 - INFO - validation batch 201, loss: 1.482, 6432/10016 datapoints
2025-03-07 18:51:27,990 - INFO - validation batch 251, loss: 1.610, 8032/10016 datapoints
2025-03-07 18:51:28,322 - INFO - validation batch 301, loss: 1.418, 9632/10016 datapoints
2025-03-07 18:51:28,409 - INFO - Epoch 29/800 done.
2025-03-07 18:51:28,410 - INFO - Final validation performance:
Loss: 1.525, top-1 acc: 0.711top-5 acc: 0.711
2025-03-07 18:51:28,411 - INFO - Beginning epoch 30/800
2025-03-07 18:51:28,424 - INFO - training batch 1, loss: 1.576, 32/60000 datapoints
2025-03-07 18:51:28,852 - INFO - training batch 51, loss: 1.488, 1632/60000 datapoints
2025-03-07 18:51:29,570 - INFO - training batch 101, loss: 1.349, 3232/60000 datapoints
2025-03-07 18:51:30,041 - INFO - training batch 151, loss: 1.486, 4832/60000 datapoints
2025-03-07 18:51:30,432 - INFO - training batch 201, loss: 1.394, 6432/60000 datapoints
2025-03-07 18:51:30,807 - INFO - training batch 251, loss: 1.448, 8032/60000 datapoints
2025-03-07 18:51:31,530 - INFO - training batch 301, loss: 1.582, 9632/60000 datapoints
2025-03-07 18:51:32,080 - INFO - training batch 351, loss: 1.553, 11232/60000 datapoints
2025-03-07 18:51:32,576 - INFO - training batch 401, loss: 1.492, 12832/60000 datapoints
2025-03-07 18:51:33,370 - INFO - training batch 451, loss: 1.521, 14432/60000 datapoints
2025-03-07 18:51:33,818 - INFO - training batch 501, loss: 1.547, 16032/60000 datapoints
2025-03-07 18:51:34,236 - INFO - training batch 551, loss: 1.453, 17632/60000 datapoints
2025-03-07 18:51:34,561 - INFO - training batch 601, loss: 1.503, 19232/60000 datapoints
2025-03-07 18:51:34,929 - INFO - training batch 651, loss: 1.803, 20832/60000 datapoints
2025-03-07 18:51:35,309 - INFO - training batch 701, loss: 1.503, 22432/60000 datapoints
2025-03-07 18:51:35,877 - INFO - training batch 751, loss: 1.509, 24032/60000 datapoints
2025-03-07 18:51:36,280 - INFO - training batch 801, loss: 1.597, 25632/60000 datapoints
2025-03-07 18:51:36,612 - INFO - training batch 851, loss: 1.424, 27232/60000 datapoints
2025-03-07 18:51:37,005 - INFO - training batch 901, loss: 1.430, 28832/60000 datapoints
2025-03-07 18:51:37,521 - INFO - training batch 951, loss: 1.535, 30432/60000 datapoints
2025-03-07 18:51:37,884 - INFO - training batch 1001, loss: 1.520, 32032/60000 datapoints
2025-03-07 18:51:38,702 - INFO - training batch 1051, loss: 1.525, 33632/60000 datapoints
2025-03-07 18:51:39,139 - INFO - training batch 1101, loss: 1.547, 35232/60000 datapoints
2025-03-07 18:51:39,463 - INFO - training batch 1151, loss: 1.499, 36832/60000 datapoints
2025-03-07 18:51:39,785 - INFO - training batch 1201, loss: 1.427, 38432/60000 datapoints
2025-03-07 18:51:40,168 - INFO - training batch 1251, loss: 1.546, 40032/60000 datapoints
2025-03-07 18:51:40,519 - INFO - training batch 1301, loss: 1.592, 41632/60000 datapoints
2025-03-07 18:51:41,067 - INFO - training batch 1351, loss: 1.552, 43232/60000 datapoints
2025-03-07 18:51:41,487 - INFO - training batch 1401, loss: 1.560, 44832/60000 datapoints
2025-03-07 18:51:41,814 - INFO - training batch 1451, loss: 1.514, 46432/60000 datapoints
2025-03-07 18:51:42,124 - INFO - training batch 1501, loss: 1.544, 48032/60000 datapoints
2025-03-07 18:51:42,548 - INFO - training batch 1551, loss: 1.444, 49632/60000 datapoints
2025-03-07 18:51:42,883 - INFO - training batch 1601, loss: 1.330, 51232/60000 datapoints
2025-03-07 18:51:43,188 - INFO - training batch 1651, loss: 1.518, 52832/60000 datapoints
2025-03-07 18:51:43,539 - INFO - training batch 1701, loss: 1.635, 54432/60000 datapoints
2025-03-07 18:51:44,046 - INFO - training batch 1751, loss: 1.384, 56032/60000 datapoints
2025-03-07 18:51:44,457 - INFO - training batch 1801, loss: 1.470, 57632/60000 datapoints
2025-03-07 18:51:44,920 - INFO - training batch 1851, loss: 1.411, 59232/60000 datapoints
2025-03-07 18:51:45,147 - INFO - validation batch 1, loss: 1.334, 32/10016 datapoints
2025-03-07 18:51:45,491 - INFO - validation batch 51, loss: 1.546, 1632/10016 datapoints
2025-03-07 18:51:45,855 - INFO - validation batch 101, loss: 1.542, 3232/10016 datapoints
2025-03-07 18:51:46,229 - INFO - validation batch 151, loss: 1.573, 4832/10016 datapoints
2025-03-07 18:51:46,465 - INFO - validation batch 201, loss: 1.450, 6432/10016 datapoints
2025-03-07 18:51:46,746 - INFO - validation batch 251, loss: 1.586, 8032/10016 datapoints
2025-03-07 18:51:47,042 - INFO - validation batch 301, loss: 1.450, 9632/10016 datapoints
2025-03-07 18:51:47,094 - INFO - Epoch 30/800 done.
2025-03-07 18:51:47,094 - INFO - Final validation performance:
Loss: 1.497, top-1 acc: 0.717top-5 acc: 0.717
2025-03-07 18:51:47,095 - INFO - Beginning epoch 31/800
2025-03-07 18:51:47,104 - INFO - training batch 1, loss: 1.484, 32/60000 datapoints
2025-03-07 18:51:47,404 - INFO - training batch 51, loss: 1.573, 1632/60000 datapoints
2025-03-07 18:51:47,696 - INFO - training batch 101, loss: 1.452, 3232/60000 datapoints
2025-03-07 18:51:48,044 - INFO - training batch 151, loss: 1.482, 4832/60000 datapoints
2025-03-07 18:51:48,345 - INFO - training batch 201, loss: 1.737, 6432/60000 datapoints
2025-03-07 18:51:48,628 - INFO - training batch 251, loss: 1.472, 8032/60000 datapoints
2025-03-07 18:51:48,926 - INFO - training batch 301, loss: 1.322, 9632/60000 datapoints
2025-03-07 18:51:49,221 - INFO - training batch 351, loss: 1.514, 11232/60000 datapoints
2025-03-07 18:51:49,506 - INFO - training batch 401, loss: 1.480, 12832/60000 datapoints
2025-03-07 18:51:49,788 - INFO - training batch 451, loss: 1.504, 14432/60000 datapoints
2025-03-07 18:51:51,300 - INFO - training batch 501, loss: 1.415, 16032/60000 datapoints
2025-03-07 18:51:51,778 - INFO - training batch 551, loss: 1.453, 17632/60000 datapoints
2025-03-07 18:51:52,223 - INFO - training batch 601, loss: 1.558, 19232/60000 datapoints
2025-03-07 18:51:52,665 - INFO - training batch 651, loss: 1.738, 20832/60000 datapoints
2025-03-07 18:51:53,094 - INFO - training batch 701, loss: 1.463, 22432/60000 datapoints
2025-03-07 18:51:53,482 - INFO - training batch 751, loss: 1.512, 24032/60000 datapoints
2025-03-07 18:51:53,804 - INFO - training batch 801, loss: 1.428, 25632/60000 datapoints
2025-03-07 18:51:54,120 - INFO - training batch 851, loss: 1.442, 27232/60000 datapoints
2025-03-07 18:51:54,403 - INFO - training batch 901, loss: 1.448, 28832/60000 datapoints
2025-03-07 18:51:54,677 - INFO - training batch 951, loss: 1.660, 30432/60000 datapoints
2025-03-07 18:51:54,962 - INFO - training batch 1001, loss: 1.578, 32032/60000 datapoints
2025-03-07 18:51:55,241 - INFO - training batch 1051, loss: 1.549, 33632/60000 datapoints
2025-03-07 18:51:55,518 - INFO - training batch 1101, loss: 1.371, 35232/60000 datapoints
2025-03-07 18:51:55,802 - INFO - training batch 1151, loss: 1.520, 36832/60000 datapoints
2025-03-07 18:51:56,095 - INFO - training batch 1201, loss: 1.503, 38432/60000 datapoints
2025-03-07 18:51:56,378 - INFO - training batch 1251, loss: 1.563, 40032/60000 datapoints
2025-03-07 18:51:56,652 - INFO - training batch 1301, loss: 1.419, 41632/60000 datapoints
2025-03-07 18:51:56,938 - INFO - training batch 1351, loss: 1.257, 43232/60000 datapoints
2025-03-07 18:51:57,260 - INFO - training batch 1401, loss: 1.427, 44832/60000 datapoints
2025-03-07 18:51:57,535 - INFO - training batch 1451, loss: 1.455, 46432/60000 datapoints
2025-03-07 18:51:57,814 - INFO - training batch 1501, loss: 1.375, 48032/60000 datapoints
2025-03-07 18:51:58,106 - INFO - training batch 1551, loss: 1.439, 49632/60000 datapoints
2025-03-07 18:51:58,390 - INFO - training batch 1601, loss: 1.638, 51232/60000 datapoints
2025-03-07 18:51:58,662 - INFO - training batch 1651, loss: 1.434, 52832/60000 datapoints
2025-03-07 18:51:58,941 - INFO - training batch 1701, loss: 1.442, 54432/60000 datapoints
2025-03-07 18:51:59,248 - INFO - training batch 1751, loss: 1.435, 56032/60000 datapoints
2025-03-07 18:51:59,522 - INFO - training batch 1801, loss: 1.403, 57632/60000 datapoints
2025-03-07 18:51:59,805 - INFO - training batch 1851, loss: 1.351, 59232/60000 datapoints
2025-03-07 18:51:59,976 - INFO - validation batch 1, loss: 1.624, 32/10016 datapoints
2025-03-07 18:52:00,241 - INFO - validation batch 51, loss: 1.564, 1632/10016 datapoints
2025-03-07 18:52:00,490 - INFO - validation batch 101, loss: 1.387, 3232/10016 datapoints
2025-03-07 18:52:00,749 - INFO - validation batch 151, loss: 1.485, 4832/10016 datapoints
2025-03-07 18:52:01,007 - INFO - validation batch 201, loss: 1.511, 6432/10016 datapoints
2025-03-07 18:52:01,254 - INFO - validation batch 251, loss: 1.383, 8032/10016 datapoints
2025-03-07 18:52:01,489 - INFO - validation batch 301, loss: 1.367, 9632/10016 datapoints
2025-03-07 18:52:01,542 - INFO - Epoch 31/800 done.
2025-03-07 18:52:01,542 - INFO - Final validation performance:
Loss: 1.474, top-1 acc: 0.721top-5 acc: 0.721
2025-03-07 18:52:01,543 - INFO - Beginning epoch 32/800
2025-03-07 18:52:01,551 - INFO - training batch 1, loss: 1.564, 32/60000 datapoints
2025-03-07 18:52:01,856 - INFO - training batch 51, loss: 1.488, 1632/60000 datapoints
2025-03-07 18:52:02,219 - INFO - training batch 101, loss: 1.444, 3232/60000 datapoints
2025-03-07 18:52:02,515 - INFO - training batch 151, loss: 1.453, 4832/60000 datapoints
2025-03-07 18:52:02,833 - INFO - training batch 201, loss: 1.578, 6432/60000 datapoints
2025-03-07 18:52:03,195 - INFO - training batch 251, loss: 1.459, 8032/60000 datapoints
2025-03-07 18:52:03,567 - INFO - training batch 301, loss: 1.381, 9632/60000 datapoints
2025-03-07 18:52:03,921 - INFO - training batch 351, loss: 1.375, 11232/60000 datapoints
2025-03-07 18:52:04,268 - INFO - training batch 401, loss: 1.320, 12832/60000 datapoints
2025-03-07 18:52:04,634 - INFO - training batch 451, loss: 1.440, 14432/60000 datapoints
2025-03-07 18:52:04,964 - INFO - training batch 501, loss: 1.484, 16032/60000 datapoints
2025-03-07 18:52:05,349 - INFO - training batch 551, loss: 1.415, 17632/60000 datapoints
2025-03-07 18:52:05,683 - INFO - training batch 601, loss: 1.445, 19232/60000 datapoints
2025-03-07 18:52:06,124 - INFO - training batch 651, loss: 1.430, 20832/60000 datapoints
2025-03-07 18:52:06,720 - INFO - training batch 701, loss: 1.472, 22432/60000 datapoints
2025-03-07 18:52:07,244 - INFO - training batch 751, loss: 1.517, 24032/60000 datapoints
2025-03-07 18:52:07,709 - INFO - training batch 801, loss: 1.711, 25632/60000 datapoints
2025-03-07 18:52:08,241 - INFO - training batch 851, loss: 1.430, 27232/60000 datapoints
2025-03-07 18:52:08,689 - INFO - training batch 901, loss: 1.512, 28832/60000 datapoints
2025-03-07 18:52:09,089 - INFO - training batch 951, loss: 1.529, 30432/60000 datapoints
2025-03-07 18:52:09,560 - INFO - training batch 1001, loss: 1.417, 32032/60000 datapoints
2025-03-07 18:52:09,998 - INFO - training batch 1051, loss: 1.344, 33632/60000 datapoints
2025-03-07 18:52:10,489 - INFO - training batch 1101, loss: 1.437, 35232/60000 datapoints
2025-03-07 18:52:10,957 - INFO - training batch 1151, loss: 1.375, 36832/60000 datapoints
2025-03-07 18:52:11,344 - INFO - training batch 1201, loss: 1.276, 38432/60000 datapoints
2025-03-07 18:52:11,716 - INFO - training batch 1251, loss: 1.484, 40032/60000 datapoints
2025-03-07 18:52:12,047 - INFO - training batch 1301, loss: 1.303, 41632/60000 datapoints
2025-03-07 18:52:12,410 - INFO - training batch 1351, loss: 1.442, 43232/60000 datapoints
2025-03-07 18:52:12,706 - INFO - training batch 1401, loss: 1.390, 44832/60000 datapoints
2025-03-07 18:52:13,020 - INFO - training batch 1451, loss: 1.365, 46432/60000 datapoints
2025-03-07 18:52:13,295 - INFO - training batch 1501, loss: 1.528, 48032/60000 datapoints
2025-03-07 18:52:13,575 - INFO - training batch 1551, loss: 1.393, 49632/60000 datapoints
2025-03-07 18:52:13,853 - INFO - training batch 1601, loss: 1.371, 51232/60000 datapoints
2025-03-07 18:52:14,152 - INFO - training batch 1651, loss: 1.539, 52832/60000 datapoints
2025-03-07 18:52:14,483 - INFO - training batch 1701, loss: 1.494, 54432/60000 datapoints
2025-03-07 18:52:14,872 - INFO - training batch 1751, loss: 1.559, 56032/60000 datapoints
2025-03-07 18:52:15,248 - INFO - training batch 1801, loss: 1.497, 57632/60000 datapoints
2025-03-07 18:52:15,693 - INFO - training batch 1851, loss: 1.467, 59232/60000 datapoints
2025-03-07 18:52:15,910 - INFO - validation batch 1, loss: 1.463, 32/10016 datapoints
2025-03-07 18:52:16,191 - INFO - validation batch 51, loss: 1.525, 1632/10016 datapoints
2025-03-07 18:52:16,612 - INFO - validation batch 101, loss: 1.461, 3232/10016 datapoints
2025-03-07 18:52:16,990 - INFO - validation batch 151, loss: 1.573, 4832/10016 datapoints
2025-03-07 18:52:17,481 - INFO - validation batch 201, loss: 1.365, 6432/10016 datapoints
2025-03-07 18:52:17,760 - INFO - validation batch 251, loss: 1.503, 8032/10016 datapoints
2025-03-07 18:52:18,034 - INFO - validation batch 301, loss: 1.475, 9632/10016 datapoints
2025-03-07 18:52:18,109 - INFO - Epoch 32/800 done.
2025-03-07 18:52:18,110 - INFO - Final validation performance:
Loss: 1.481, top-1 acc: 0.726top-5 acc: 0.726
2025-03-07 18:52:18,111 - INFO - Beginning epoch 33/800
2025-03-07 18:52:18,126 - INFO - training batch 1, loss: 1.468, 32/60000 datapoints
2025-03-07 18:52:18,509 - INFO - training batch 51, loss: 1.526, 1632/60000 datapoints
2025-03-07 18:52:18,867 - INFO - training batch 101, loss: 1.459, 3232/60000 datapoints
2025-03-07 18:52:19,194 - INFO - training batch 151, loss: 1.400, 4832/60000 datapoints
2025-03-07 18:52:19,524 - INFO - training batch 201, loss: 1.465, 6432/60000 datapoints
2025-03-07 18:52:19,871 - INFO - training batch 251, loss: 1.330, 8032/60000 datapoints
2025-03-07 18:52:20,239 - INFO - training batch 301, loss: 1.484, 9632/60000 datapoints
2025-03-07 18:52:20,776 - INFO - training batch 351, loss: 1.371, 11232/60000 datapoints
2025-03-07 18:52:21,156 - INFO - training batch 401, loss: 1.447, 12832/60000 datapoints
2025-03-07 18:52:21,563 - INFO - training batch 451, loss: 1.437, 14432/60000 datapoints
2025-03-07 18:52:21,964 - INFO - training batch 501, loss: 1.562, 16032/60000 datapoints
2025-03-07 18:52:22,333 - INFO - training batch 551, loss: 1.587, 17632/60000 datapoints
2025-03-07 18:52:22,816 - INFO - training batch 601, loss: 1.345, 19232/60000 datapoints
2025-03-07 18:52:23,343 - INFO - training batch 651, loss: 1.404, 20832/60000 datapoints
2025-03-07 18:52:23,743 - INFO - training batch 701, loss: 1.527, 22432/60000 datapoints
2025-03-07 18:52:24,326 - INFO - training batch 751, loss: 1.527, 24032/60000 datapoints
2025-03-07 18:52:24,719 - INFO - training batch 801, loss: 1.320, 25632/60000 datapoints
2025-03-07 18:52:25,078 - INFO - training batch 851, loss: 1.457, 27232/60000 datapoints
2025-03-07 18:52:25,405 - INFO - training batch 901, loss: 1.457, 28832/60000 datapoints
2025-03-07 18:52:25,823 - INFO - training batch 951, loss: 1.416, 30432/60000 datapoints
2025-03-07 18:52:26,239 - INFO - training batch 1001, loss: 1.458, 32032/60000 datapoints
2025-03-07 18:52:26,639 - INFO - training batch 1051, loss: 1.446, 33632/60000 datapoints
2025-03-07 18:52:27,172 - INFO - training batch 1101, loss: 1.455, 35232/60000 datapoints
2025-03-07 18:52:27,617 - INFO - training batch 1151, loss: 1.315, 36832/60000 datapoints
2025-03-07 18:52:27,993 - INFO - training batch 1201, loss: 1.563, 38432/60000 datapoints
2025-03-07 18:52:28,412 - INFO - training batch 1251, loss: 1.495, 40032/60000 datapoints
2025-03-07 18:52:29,108 - INFO - training batch 1301, loss: 1.352, 41632/60000 datapoints
2025-03-07 18:52:29,509 - INFO - training batch 1351, loss: 1.543, 43232/60000 datapoints
2025-03-07 18:52:29,875 - INFO - training batch 1401, loss: 1.523, 44832/60000 datapoints
2025-03-07 18:52:30,197 - INFO - training batch 1451, loss: 1.409, 46432/60000 datapoints
2025-03-07 18:52:30,536 - INFO - training batch 1501, loss: 1.433, 48032/60000 datapoints
2025-03-07 18:52:30,878 - INFO - training batch 1551, loss: 1.456, 49632/60000 datapoints
2025-03-07 18:52:31,472 - INFO - training batch 1601, loss: 1.466, 51232/60000 datapoints
2025-03-07 18:52:32,156 - INFO - training batch 1651, loss: 1.466, 52832/60000 datapoints
2025-03-07 18:52:32,519 - INFO - training batch 1701, loss: 1.224, 54432/60000 datapoints
2025-03-07 18:52:33,121 - INFO - training batch 1751, loss: 1.406, 56032/60000 datapoints
2025-03-07 18:52:33,596 - INFO - training batch 1801, loss: 1.345, 57632/60000 datapoints
2025-03-07 18:52:33,930 - INFO - training batch 1851, loss: 1.247, 59232/60000 datapoints
2025-03-07 18:52:34,131 - INFO - validation batch 1, loss: 1.333, 32/10016 datapoints
2025-03-07 18:52:34,426 - INFO - validation batch 51, loss: 1.439, 1632/10016 datapoints
2025-03-07 18:52:34,742 - INFO - validation batch 101, loss: 1.453, 3232/10016 datapoints
2025-03-07 18:52:35,030 - INFO - validation batch 151, loss: 1.361, 4832/10016 datapoints
2025-03-07 18:52:35,275 - INFO - validation batch 201, loss: 1.351, 6432/10016 datapoints
2025-03-07 18:52:35,519 - INFO - validation batch 251, loss: 1.364, 8032/10016 datapoints
2025-03-07 18:52:35,788 - INFO - validation batch 301, loss: 1.523, 9632/10016 datapoints
2025-03-07 18:52:36,042 - INFO - Epoch 33/800 done.
2025-03-07 18:52:36,043 - INFO - Final validation performance:
Loss: 1.403, top-1 acc: 0.729top-5 acc: 0.729
2025-03-07 18:52:36,044 - INFO - Beginning epoch 34/800
2025-03-07 18:52:36,062 - INFO - training batch 1, loss: 1.362, 32/60000 datapoints
2025-03-07 18:52:36,464 - INFO - training batch 51, loss: 1.295, 1632/60000 datapoints
2025-03-07 18:52:36,858 - INFO - training batch 101, loss: 1.381, 3232/60000 datapoints
2025-03-07 18:52:37,341 - INFO - training batch 151, loss: 1.517, 4832/60000 datapoints
2025-03-07 18:52:37,864 - INFO - training batch 201, loss: 1.517, 6432/60000 datapoints
2025-03-07 18:52:38,524 - INFO - training batch 251, loss: 1.360, 8032/60000 datapoints
2025-03-07 18:52:39,135 - INFO - training batch 301, loss: 1.442, 9632/60000 datapoints
2025-03-07 18:52:39,577 - INFO - training batch 351, loss: 1.473, 11232/60000 datapoints
2025-03-07 18:52:40,043 - INFO - training batch 401, loss: 1.524, 12832/60000 datapoints
2025-03-07 18:52:40,536 - INFO - training batch 451, loss: 1.446, 14432/60000 datapoints
2025-03-07 18:52:41,133 - INFO - training batch 501, loss: 1.363, 16032/60000 datapoints
2025-03-07 18:52:41,667 - INFO - training batch 551, loss: 1.486, 17632/60000 datapoints
2025-03-07 18:52:42,112 - INFO - training batch 601, loss: 1.461, 19232/60000 datapoints
2025-03-07 18:52:42,532 - INFO - training batch 651, loss: 1.292, 20832/60000 datapoints
2025-03-07 18:52:42,948 - INFO - training batch 701, loss: 1.468, 22432/60000 datapoints
2025-03-07 18:52:43,262 - INFO - training batch 751, loss: 1.354, 24032/60000 datapoints
2025-03-07 18:52:43,634 - INFO - training batch 801, loss: 1.446, 25632/60000 datapoints
2025-03-07 18:52:43,933 - INFO - training batch 851, loss: 1.488, 27232/60000 datapoints
2025-03-07 18:52:44,232 - INFO - training batch 901, loss: 1.392, 28832/60000 datapoints
2025-03-07 18:52:44,512 - INFO - training batch 951, loss: 1.492, 30432/60000 datapoints
2025-03-07 18:52:44,785 - INFO - training batch 1001, loss: 1.419, 32032/60000 datapoints
2025-03-07 18:52:45,062 - INFO - training batch 1051, loss: 1.303, 33632/60000 datapoints
2025-03-07 18:52:45,354 - INFO - training batch 1101, loss: 1.323, 35232/60000 datapoints
2025-03-07 18:52:45,643 - INFO - training batch 1151, loss: 1.440, 36832/60000 datapoints
2025-03-07 18:52:45,923 - INFO - training batch 1201, loss: 1.346, 38432/60000 datapoints
2025-03-07 18:52:46,206 - INFO - training batch 1251, loss: 1.404, 40032/60000 datapoints
2025-03-07 18:52:46,487 - INFO - training batch 1301, loss: 1.318, 41632/60000 datapoints
2025-03-07 18:52:46,757 - INFO - training batch 1351, loss: 1.522, 43232/60000 datapoints
2025-03-07 18:52:47,037 - INFO - training batch 1401, loss: 1.487, 44832/60000 datapoints
2025-03-07 18:52:47,312 - INFO - training batch 1451, loss: 1.442, 46432/60000 datapoints
2025-03-07 18:52:47,590 - INFO - training batch 1501, loss: 1.345, 48032/60000 datapoints
2025-03-07 18:52:47,926 - INFO - training batch 1551, loss: 1.512, 49632/60000 datapoints
2025-03-07 18:52:48,217 - INFO - training batch 1601, loss: 1.448, 51232/60000 datapoints
2025-03-07 18:52:48,511 - INFO - training batch 1651, loss: 1.463, 52832/60000 datapoints
2025-03-07 18:52:48,793 - INFO - training batch 1701, loss: 1.334, 54432/60000 datapoints
2025-03-07 18:52:49,086 - INFO - training batch 1751, loss: 1.352, 56032/60000 datapoints
2025-03-07 18:52:49,536 - INFO - training batch 1801, loss: 1.364, 57632/60000 datapoints
2025-03-07 18:52:49,929 - INFO - training batch 1851, loss: 1.401, 59232/60000 datapoints
2025-03-07 18:52:50,154 - INFO - validation batch 1, loss: 1.454, 32/10016 datapoints
2025-03-07 18:52:50,518 - INFO - validation batch 51, loss: 1.417, 1632/10016 datapoints
2025-03-07 18:52:50,913 - INFO - validation batch 101, loss: 1.353, 3232/10016 datapoints
2025-03-07 18:52:51,357 - INFO - validation batch 151, loss: 1.353, 4832/10016 datapoints
2025-03-07 18:52:51,696 - INFO - validation batch 201, loss: 1.521, 6432/10016 datapoints
2025-03-07 18:52:52,022 - INFO - validation batch 251, loss: 1.287, 8032/10016 datapoints
2025-03-07 18:52:52,359 - INFO - validation batch 301, loss: 1.292, 9632/10016 datapoints
2025-03-07 18:52:52,494 - INFO - Epoch 34/800 done.
2025-03-07 18:52:52,496 - INFO - Final validation performance:
Loss: 1.382, top-1 acc: 0.733top-5 acc: 0.733
2025-03-07 18:52:52,499 - INFO - Beginning epoch 35/800
2025-03-07 18:52:52,515 - INFO - training batch 1, loss: 1.532, 32/60000 datapoints
2025-03-07 18:52:52,960 - INFO - training batch 51, loss: 1.388, 1632/60000 datapoints
2025-03-07 18:52:53,525 - INFO - training batch 101, loss: 1.442, 3232/60000 datapoints
2025-03-07 18:52:53,870 - INFO - training batch 151, loss: 1.359, 4832/60000 datapoints
2025-03-07 18:52:54,261 - INFO - training batch 201, loss: 1.364, 6432/60000 datapoints
2025-03-07 18:52:54,608 - INFO - training batch 251, loss: 1.480, 8032/60000 datapoints
2025-03-07 18:52:54,995 - INFO - training batch 301, loss: 1.279, 9632/60000 datapoints
2025-03-07 18:52:55,359 - INFO - training batch 351, loss: 1.306, 11232/60000 datapoints
2025-03-07 18:52:55,712 - INFO - training batch 401, loss: 1.388, 12832/60000 datapoints
2025-03-07 18:52:56,063 - INFO - training batch 451, loss: 1.275, 14432/60000 datapoints
2025-03-07 18:52:56,783 - INFO - training batch 501, loss: 1.495, 16032/60000 datapoints
2025-03-07 18:52:57,255 - INFO - training batch 551, loss: 1.286, 17632/60000 datapoints
2025-03-07 18:52:57,625 - INFO - training batch 601, loss: 1.471, 19232/60000 datapoints
2025-03-07 18:52:57,995 - INFO - training batch 651, loss: 1.352, 20832/60000 datapoints
2025-03-07 18:52:58,450 - INFO - training batch 701, loss: 1.266, 22432/60000 datapoints
2025-03-07 18:52:58,784 - INFO - training batch 751, loss: 1.219, 24032/60000 datapoints
2025-03-07 18:52:59,142 - INFO - training batch 801, loss: 1.309, 25632/60000 datapoints
2025-03-07 18:52:59,600 - INFO - training batch 851, loss: 1.357, 27232/60000 datapoints
2025-03-07 18:52:59,993 - INFO - training batch 901, loss: 1.328, 28832/60000 datapoints
2025-03-07 18:53:00,446 - INFO - training batch 951, loss: 1.369, 30432/60000 datapoints
2025-03-07 18:53:00,869 - INFO - training batch 1001, loss: 1.402, 32032/60000 datapoints
2025-03-07 18:53:01,352 - INFO - training batch 1051, loss: 1.341, 33632/60000 datapoints
2025-03-07 18:53:01,716 - INFO - training batch 1101, loss: 1.471, 35232/60000 datapoints
2025-03-07 18:53:02,258 - INFO - training batch 1151, loss: 1.227, 36832/60000 datapoints
2025-03-07 18:53:02,675 - INFO - training batch 1201, loss: 1.390, 38432/60000 datapoints
2025-03-07 18:53:03,032 - INFO - training batch 1251, loss: 1.271, 40032/60000 datapoints
2025-03-07 18:53:03,402 - INFO - training batch 1301, loss: 1.282, 41632/60000 datapoints
2025-03-07 18:53:03,831 - INFO - training batch 1351, loss: 1.490, 43232/60000 datapoints
2025-03-07 18:53:04,221 - INFO - training batch 1401, loss: 1.347, 44832/60000 datapoints
2025-03-07 18:53:04,581 - INFO - training batch 1451, loss: 1.350, 46432/60000 datapoints
2025-03-07 18:53:05,096 - INFO - training batch 1501, loss: 1.485, 48032/60000 datapoints
2025-03-07 18:53:05,443 - INFO - training batch 1551, loss: 1.218, 49632/60000 datapoints
2025-03-07 18:53:05,840 - INFO - training batch 1601, loss: 1.349, 51232/60000 datapoints
2025-03-07 18:53:06,204 - INFO - training batch 1651, loss: 1.319, 52832/60000 datapoints
2025-03-07 18:53:06,586 - INFO - training batch 1701, loss: 1.352, 54432/60000 datapoints
2025-03-07 18:53:06,990 - INFO - training batch 1751, loss: 1.402, 56032/60000 datapoints
2025-03-07 18:53:07,357 - INFO - training batch 1801, loss: 1.154, 57632/60000 datapoints
2025-03-07 18:53:07,862 - INFO - training batch 1851, loss: 1.299, 59232/60000 datapoints
2025-03-07 18:53:08,058 - INFO - validation batch 1, loss: 1.370, 32/10016 datapoints
2025-03-07 18:53:08,372 - INFO - validation batch 51, loss: 1.489, 1632/10016 datapoints
2025-03-07 18:53:08,636 - INFO - validation batch 101, loss: 1.082, 3232/10016 datapoints
2025-03-07 18:53:08,921 - INFO - validation batch 151, loss: 1.554, 4832/10016 datapoints
2025-03-07 18:53:09,318 - INFO - validation batch 201, loss: 1.416, 6432/10016 datapoints
2025-03-07 18:53:09,672 - INFO - validation batch 251, loss: 1.392, 8032/10016 datapoints
2025-03-07 18:53:10,052 - INFO - validation batch 301, loss: 1.323, 9632/10016 datapoints
2025-03-07 18:53:10,131 - INFO - Epoch 35/800 done.
2025-03-07 18:53:10,132 - INFO - Final validation performance:
Loss: 1.375, top-1 acc: 0.738top-5 acc: 0.738
2025-03-07 18:53:10,134 - INFO - Beginning epoch 36/800
2025-03-07 18:53:10,145 - INFO - training batch 1, loss: 1.336, 32/60000 datapoints
2025-03-07 18:53:10,460 - INFO - training batch 51, loss: 1.294, 1632/60000 datapoints
2025-03-07 18:53:10,797 - INFO - training batch 101, loss: 1.273, 3232/60000 datapoints
2025-03-07 18:53:11,078 - INFO - training batch 151, loss: 1.321, 4832/60000 datapoints
2025-03-07 18:53:11,376 - INFO - training batch 201, loss: 1.390, 6432/60000 datapoints
2025-03-07 18:53:11,678 - INFO - training batch 251, loss: 1.280, 8032/60000 datapoints
2025-03-07 18:53:11,953 - INFO - training batch 301, loss: 1.368, 9632/60000 datapoints
2025-03-07 18:53:12,228 - INFO - training batch 351, loss: 1.330, 11232/60000 datapoints
2025-03-07 18:53:12,495 - INFO - training batch 401, loss: 1.345, 12832/60000 datapoints
2025-03-07 18:53:12,779 - INFO - training batch 451, loss: 1.324, 14432/60000 datapoints
2025-03-07 18:53:13,054 - INFO - training batch 501, loss: 1.312, 16032/60000 datapoints
2025-03-07 18:53:13,328 - INFO - training batch 551, loss: 1.485, 17632/60000 datapoints
2025-03-07 18:53:13,601 - INFO - training batch 601, loss: 1.384, 19232/60000 datapoints
2025-03-07 18:53:13,879 - INFO - training batch 651, loss: 1.415, 20832/60000 datapoints
2025-03-07 18:53:14,159 - INFO - training batch 701, loss: 1.330, 22432/60000 datapoints
2025-03-07 18:53:14,437 - INFO - training batch 751, loss: 1.339, 24032/60000 datapoints
2025-03-07 18:53:14,711 - INFO - training batch 801, loss: 1.491, 25632/60000 datapoints
2025-03-07 18:53:14,982 - INFO - training batch 851, loss: 1.309, 27232/60000 datapoints
2025-03-07 18:53:15,253 - INFO - training batch 901, loss: 1.412, 28832/60000 datapoints
2025-03-07 18:53:15,546 - INFO - training batch 951, loss: 1.312, 30432/60000 datapoints
2025-03-07 18:53:15,819 - INFO - training batch 1001, loss: 1.402, 32032/60000 datapoints
2025-03-07 18:53:16,217 - INFO - training batch 1051, loss: 1.320, 33632/60000 datapoints
2025-03-07 18:53:16,611 - INFO - training batch 1101, loss: 1.475, 35232/60000 datapoints
2025-03-07 18:53:16,992 - INFO - training batch 1151, loss: 1.322, 36832/60000 datapoints
2025-03-07 18:53:17,437 - INFO - training batch 1201, loss: 1.392, 38432/60000 datapoints
2025-03-07 18:53:17,862 - INFO - training batch 1251, loss: 1.353, 40032/60000 datapoints
2025-03-07 18:53:18,299 - INFO - training batch 1301, loss: 1.282, 41632/60000 datapoints
2025-03-07 18:53:18,665 - INFO - training batch 1351, loss: 1.285, 43232/60000 datapoints
2025-03-07 18:53:19,185 - INFO - training batch 1401, loss: 1.309, 44832/60000 datapoints
2025-03-07 18:53:19,513 - INFO - training batch 1451, loss: 1.402, 46432/60000 datapoints
2025-03-07 18:53:19,850 - INFO - training batch 1501, loss: 1.297, 48032/60000 datapoints
2025-03-07 18:53:20,189 - INFO - training batch 1551, loss: 1.244, 49632/60000 datapoints
2025-03-07 18:53:20,605 - INFO - training batch 1601, loss: 1.331, 51232/60000 datapoints
2025-03-07 18:53:21,153 - INFO - training batch 1651, loss: 1.391, 52832/60000 datapoints
2025-03-07 18:53:21,825 - INFO - training batch 1701, loss: 1.399, 54432/60000 datapoints
2025-03-07 18:53:22,496 - INFO - training batch 1751, loss: 1.462, 56032/60000 datapoints
2025-03-07 18:53:22,954 - INFO - training batch 1801, loss: 1.353, 57632/60000 datapoints
2025-03-07 18:53:23,307 - INFO - training batch 1851, loss: 1.373, 59232/60000 datapoints
2025-03-07 18:53:23,499 - INFO - validation batch 1, loss: 1.300, 32/10016 datapoints
2025-03-07 18:53:23,799 - INFO - validation batch 51, loss: 1.373, 1632/10016 datapoints
2025-03-07 18:53:24,162 - INFO - validation batch 101, loss: 1.283, 3232/10016 datapoints
2025-03-07 18:53:24,491 - INFO - validation batch 151, loss: 1.277, 4832/10016 datapoints
2025-03-07 18:53:24,813 - INFO - validation batch 201, loss: 1.205, 6432/10016 datapoints
2025-03-07 18:53:25,313 - INFO - validation batch 251, loss: 1.185, 8032/10016 datapoints
2025-03-07 18:53:25,623 - INFO - validation batch 301, loss: 1.232, 9632/10016 datapoints
2025-03-07 18:53:25,690 - INFO - Epoch 36/800 done.
2025-03-07 18:53:25,690 - INFO - Final validation performance:
Loss: 1.265, top-1 acc: 0.744top-5 acc: 0.744
2025-03-07 18:53:25,691 - INFO - Beginning epoch 37/800
2025-03-07 18:53:25,708 - INFO - training batch 1, loss: 1.353, 32/60000 datapoints
2025-03-07 18:53:26,118 - INFO - training batch 51, loss: 1.411, 1632/60000 datapoints
2025-03-07 18:53:26,529 - INFO - training batch 101, loss: 1.312, 3232/60000 datapoints
2025-03-07 18:53:26,898 - INFO - training batch 151, loss: 1.421, 4832/60000 datapoints
2025-03-07 18:53:27,445 - INFO - training batch 201, loss: 1.390, 6432/60000 datapoints
2025-03-07 18:53:27,753 - INFO - training batch 251, loss: 1.319, 8032/60000 datapoints
2025-03-07 18:53:28,119 - INFO - training batch 301, loss: 1.296, 9632/60000 datapoints
2025-03-07 18:53:28,816 - INFO - training batch 351, loss: 1.283, 11232/60000 datapoints
2025-03-07 18:53:29,194 - INFO - training batch 401, loss: 1.312, 12832/60000 datapoints
2025-03-07 18:53:29,563 - INFO - training batch 451, loss: 1.359, 14432/60000 datapoints
2025-03-07 18:53:29,977 - INFO - training batch 501, loss: 1.171, 16032/60000 datapoints
2025-03-07 18:53:30,375 - INFO - training batch 551, loss: 1.511, 17632/60000 datapoints
2025-03-07 18:53:30,749 - INFO - training batch 601, loss: 1.325, 19232/60000 datapoints
2025-03-07 18:53:31,271 - INFO - training batch 651, loss: 1.203, 20832/60000 datapoints
2025-03-07 18:53:31,702 - INFO - training batch 701, loss: 1.386, 22432/60000 datapoints
2025-03-07 18:53:32,219 - INFO - training batch 751, loss: 1.416, 24032/60000 datapoints
2025-03-07 18:53:32,579 - INFO - training batch 801, loss: 1.357, 25632/60000 datapoints
2025-03-07 18:53:33,143 - INFO - training batch 851, loss: 1.401, 27232/60000 datapoints
2025-03-07 18:53:33,478 - INFO - training batch 901, loss: 1.314, 28832/60000 datapoints
2025-03-07 18:53:33,786 - INFO - training batch 951, loss: 1.432, 30432/60000 datapoints
2025-03-07 18:53:34,146 - INFO - training batch 1001, loss: 1.184, 32032/60000 datapoints
2025-03-07 18:53:34,436 - INFO - training batch 1051, loss: 1.378, 33632/60000 datapoints
2025-03-07 18:53:34,788 - INFO - training batch 1101, loss: 1.364, 35232/60000 datapoints
2025-03-07 18:53:35,244 - INFO - training batch 1151, loss: 1.227, 36832/60000 datapoints
2025-03-07 18:53:35,729 - INFO - training batch 1201, loss: 1.432, 38432/60000 datapoints
2025-03-07 18:53:36,210 - INFO - training batch 1251, loss: 1.368, 40032/60000 datapoints
2025-03-07 18:53:36,550 - INFO - training batch 1301, loss: 1.492, 41632/60000 datapoints
2025-03-07 18:53:36,896 - INFO - training batch 1351, loss: 1.314, 43232/60000 datapoints
2025-03-07 18:53:37,282 - INFO - training batch 1401, loss: 1.280, 44832/60000 datapoints
2025-03-07 18:53:37,654 - INFO - training batch 1451, loss: 1.411, 46432/60000 datapoints
2025-03-07 18:53:38,090 - INFO - training batch 1501, loss: 1.220, 48032/60000 datapoints
2025-03-07 18:53:38,526 - INFO - training batch 1551, loss: 1.260, 49632/60000 datapoints
2025-03-07 18:53:38,941 - INFO - training batch 1601, loss: 1.456, 51232/60000 datapoints
2025-03-07 18:53:39,356 - INFO - training batch 1651, loss: 1.253, 52832/60000 datapoints
2025-03-07 18:53:39,833 - INFO - training batch 1701, loss: 1.349, 54432/60000 datapoints
2025-03-07 18:53:40,232 - INFO - training batch 1751, loss: 1.175, 56032/60000 datapoints
2025-03-07 18:53:40,615 - INFO - training batch 1801, loss: 1.084, 57632/60000 datapoints
2025-03-07 18:53:41,190 - INFO - training batch 1851, loss: 1.423, 59232/60000 datapoints
2025-03-07 18:53:41,372 - INFO - validation batch 1, loss: 1.252, 32/10016 datapoints
2025-03-07 18:53:41,644 - INFO - validation batch 51, loss: 1.370, 1632/10016 datapoints
2025-03-07 18:53:42,001 - INFO - validation batch 101, loss: 1.293, 3232/10016 datapoints
2025-03-07 18:53:42,366 - INFO - validation batch 151, loss: 1.309, 4832/10016 datapoints
2025-03-07 18:53:42,717 - INFO - validation batch 201, loss: 1.427, 6432/10016 datapoints
2025-03-07 18:53:43,036 - INFO - validation batch 251, loss: 1.413, 8032/10016 datapoints
2025-03-07 18:53:43,278 - INFO - validation batch 301, loss: 1.401, 9632/10016 datapoints
2025-03-07 18:53:43,330 - INFO - Epoch 37/800 done.
2025-03-07 18:53:43,331 - INFO - Final validation performance:
Loss: 1.352, top-1 acc: 0.748top-5 acc: 0.748
2025-03-07 18:53:43,332 - INFO - Beginning epoch 38/800
2025-03-07 18:53:43,340 - INFO - training batch 1, loss: 1.321, 32/60000 datapoints
2025-03-07 18:53:43,643 - INFO - training batch 51, loss: 1.407, 1632/60000 datapoints
2025-03-07 18:53:43,923 - INFO - training batch 101, loss: 1.252, 3232/60000 datapoints
2025-03-07 18:53:44,216 - INFO - training batch 151, loss: 1.205, 4832/60000 datapoints
2025-03-07 18:53:44,553 - INFO - training batch 201, loss: 1.314, 6432/60000 datapoints
2025-03-07 18:53:44,924 - INFO - training batch 251, loss: 1.407, 8032/60000 datapoints
2025-03-07 18:53:45,374 - INFO - training batch 301, loss: 1.319, 9632/60000 datapoints
2025-03-07 18:53:45,770 - INFO - training batch 351, loss: 1.277, 11232/60000 datapoints
2025-03-07 18:53:46,145 - INFO - training batch 401, loss: 1.366, 12832/60000 datapoints
2025-03-07 18:53:46,516 - INFO - training batch 451, loss: 1.277, 14432/60000 datapoints
2025-03-07 18:53:47,143 - INFO - training batch 501, loss: 1.451, 16032/60000 datapoints
2025-03-07 18:53:49,032 - INFO - training batch 551, loss: 1.285, 17632/60000 datapoints
2025-03-07 18:53:50,161 - INFO - training batch 601, loss: 1.349, 19232/60000 datapoints
2025-03-07 18:53:50,759 - INFO - training batch 651, loss: 1.285, 20832/60000 datapoints
2025-03-07 18:53:51,756 - INFO - training batch 701, loss: 1.256, 22432/60000 datapoints
2025-03-07 18:53:52,757 - INFO - training batch 751, loss: 1.461, 24032/60000 datapoints
2025-03-07 18:53:53,161 - INFO - training batch 801, loss: 1.493, 25632/60000 datapoints
2025-03-07 18:53:53,574 - INFO - training batch 851, loss: 1.394, 27232/60000 datapoints
2025-03-07 18:53:54,055 - INFO - training batch 901, loss: 1.373, 28832/60000 datapoints
2025-03-07 18:53:54,549 - INFO - training batch 951, loss: 1.321, 30432/60000 datapoints
2025-03-07 18:53:54,989 - INFO - training batch 1001, loss: 1.237, 32032/60000 datapoints
2025-03-07 18:53:55,436 - INFO - training batch 1051, loss: 1.141, 33632/60000 datapoints
2025-03-07 18:53:55,864 - INFO - training batch 1101, loss: 1.335, 35232/60000 datapoints
2025-03-07 18:53:56,294 - INFO - training batch 1151, loss: 1.418, 36832/60000 datapoints
2025-03-07 18:53:56,698 - INFO - training batch 1201, loss: 1.337, 38432/60000 datapoints
2025-03-07 18:53:57,191 - INFO - training batch 1251, loss: 1.347, 40032/60000 datapoints
2025-03-07 18:53:57,625 - INFO - training batch 1301, loss: 1.310, 41632/60000 datapoints
2025-03-07 18:53:58,038 - INFO - training batch 1351, loss: 1.131, 43232/60000 datapoints
2025-03-07 18:53:58,405 - INFO - training batch 1401, loss: 1.196, 44832/60000 datapoints
2025-03-07 18:53:58,714 - INFO - training batch 1451, loss: 1.278, 46432/60000 datapoints
2025-03-07 18:53:59,104 - INFO - training batch 1501, loss: 1.359, 48032/60000 datapoints
2025-03-07 18:53:59,487 - INFO - training batch 1551, loss: 1.327, 49632/60000 datapoints
2025-03-07 18:53:59,826 - INFO - training batch 1601, loss: 1.116, 51232/60000 datapoints
2025-03-07 18:54:00,158 - INFO - training batch 1651, loss: 1.234, 52832/60000 datapoints
2025-03-07 18:54:00,496 - INFO - training batch 1701, loss: 1.222, 54432/60000 datapoints
2025-03-07 18:54:00,878 - INFO - training batch 1751, loss: 1.384, 56032/60000 datapoints
2025-03-07 18:54:01,243 - INFO - training batch 1801, loss: 1.338, 57632/60000 datapoints
2025-03-07 18:54:01,538 - INFO - training batch 1851, loss: 1.270, 59232/60000 datapoints
2025-03-07 18:54:01,711 - INFO - validation batch 1, loss: 1.338, 32/10016 datapoints
2025-03-07 18:54:01,998 - INFO - validation batch 51, loss: 1.207, 1632/10016 datapoints
2025-03-07 18:54:02,284 - INFO - validation batch 101, loss: 1.357, 3232/10016 datapoints
2025-03-07 18:54:02,572 - INFO - validation batch 151, loss: 1.179, 4832/10016 datapoints
2025-03-07 18:54:02,861 - INFO - validation batch 201, loss: 1.214, 6432/10016 datapoints
2025-03-07 18:54:03,134 - INFO - validation batch 251, loss: 1.399, 8032/10016 datapoints
2025-03-07 18:54:03,436 - INFO - validation batch 301, loss: 1.278, 9632/10016 datapoints
2025-03-07 18:54:03,629 - INFO - Epoch 38/800 done.
2025-03-07 18:54:03,631 - INFO - Final validation performance:
Loss: 1.282, top-1 acc: 0.752top-5 acc: 0.752
2025-03-07 18:54:03,634 - INFO - Beginning epoch 39/800
2025-03-07 18:54:03,663 - INFO - training batch 1, loss: 1.316, 32/60000 datapoints
2025-03-07 18:54:04,037 - INFO - training batch 51, loss: 1.238, 1632/60000 datapoints
2025-03-07 18:54:04,371 - INFO - training batch 101, loss: 1.241, 3232/60000 datapoints
2025-03-07 18:54:04,742 - INFO - training batch 151, loss: 1.233, 4832/60000 datapoints
2025-03-07 18:54:05,191 - INFO - training batch 201, loss: 1.404, 6432/60000 datapoints
2025-03-07 18:54:05,568 - INFO - training batch 251, loss: 1.320, 8032/60000 datapoints
2025-03-07 18:54:06,057 - INFO - training batch 301, loss: 1.235, 9632/60000 datapoints
2025-03-07 18:54:06,506 - INFO - training batch 351, loss: 1.302, 11232/60000 datapoints
2025-03-07 18:54:07,011 - INFO - training batch 401, loss: 1.159, 12832/60000 datapoints
2025-03-07 18:54:07,492 - INFO - training batch 451, loss: 1.355, 14432/60000 datapoints
2025-03-07 18:54:07,815 - INFO - training batch 501, loss: 1.117, 16032/60000 datapoints
2025-03-07 18:54:08,167 - INFO - training batch 551, loss: 1.347, 17632/60000 datapoints
2025-03-07 18:54:08,463 - INFO - training batch 601, loss: 1.348, 19232/60000 datapoints
2025-03-07 18:54:08,901 - INFO - training batch 651, loss: 1.308, 20832/60000 datapoints
2025-03-07 18:54:09,285 - INFO - training batch 701, loss: 1.196, 22432/60000 datapoints
2025-03-07 18:54:09,600 - INFO - training batch 751, loss: 1.446, 24032/60000 datapoints
2025-03-07 18:54:09,943 - INFO - training batch 801, loss: 1.384, 25632/60000 datapoints
2025-03-07 18:54:10,360 - INFO - training batch 851, loss: 1.097, 27232/60000 datapoints
2025-03-07 18:54:10,719 - INFO - training batch 901, loss: 1.182, 28832/60000 datapoints
2025-03-07 18:54:11,064 - INFO - training batch 951, loss: 1.053, 30432/60000 datapoints
2025-03-07 18:54:11,411 - INFO - training batch 1001, loss: 1.243, 32032/60000 datapoints
2025-03-07 18:54:11,724 - INFO - training batch 1051, loss: 1.320, 33632/60000 datapoints
2025-03-07 18:54:12,031 - INFO - training batch 1101, loss: 1.248, 35232/60000 datapoints
2025-03-07 18:54:12,388 - INFO - training batch 1151, loss: 1.228, 36832/60000 datapoints
2025-03-07 18:54:12,710 - INFO - training batch 1201, loss: 1.425, 38432/60000 datapoints
2025-03-07 18:54:13,046 - INFO - training batch 1251, loss: 1.468, 40032/60000 datapoints
2025-03-07 18:54:13,367 - INFO - training batch 1301, loss: 1.227, 41632/60000 datapoints
2025-03-07 18:54:13,696 - INFO - training batch 1351, loss: 1.307, 43232/60000 datapoints
2025-03-07 18:54:14,144 - INFO - training batch 1401, loss: 1.307, 44832/60000 datapoints
2025-03-07 18:54:14,441 - INFO - training batch 1451, loss: 1.234, 46432/60000 datapoints
2025-03-07 18:54:14,732 - INFO - training batch 1501, loss: 1.255, 48032/60000 datapoints
2025-03-07 18:54:15,045 - INFO - training batch 1551, loss: 1.198, 49632/60000 datapoints
2025-03-07 18:54:15,398 - INFO - training batch 1601, loss: 1.202, 51232/60000 datapoints
2025-03-07 18:54:15,751 - INFO - training batch 1651, loss: 1.137, 52832/60000 datapoints
2025-03-07 18:54:16,073 - INFO - training batch 1701, loss: 1.387, 54432/60000 datapoints
2025-03-07 18:54:16,426 - INFO - training batch 1751, loss: 1.085, 56032/60000 datapoints
2025-03-07 18:54:16,783 - INFO - training batch 1801, loss: 1.438, 57632/60000 datapoints
2025-03-07 18:54:17,133 - INFO - training batch 1851, loss: 1.296, 59232/60000 datapoints
2025-03-07 18:54:17,318 - INFO - validation batch 1, loss: 1.215, 32/10016 datapoints
2025-03-07 18:54:17,601 - INFO - validation batch 51, loss: 1.226, 1632/10016 datapoints
2025-03-07 18:54:17,846 - INFO - validation batch 101, loss: 1.156, 3232/10016 datapoints
2025-03-07 18:54:18,127 - INFO - validation batch 151, loss: 1.135, 4832/10016 datapoints
2025-03-07 18:54:18,416 - INFO - validation batch 201, loss: 1.291, 6432/10016 datapoints
2025-03-07 18:54:18,951 - INFO - validation batch 251, loss: 1.365, 8032/10016 datapoints
2025-03-07 18:54:19,231 - INFO - validation batch 301, loss: 1.101, 9632/10016 datapoints
2025-03-07 18:54:19,307 - INFO - Epoch 39/800 done.
2025-03-07 18:54:19,307 - INFO - Final validation performance:
Loss: 1.213, top-1 acc: 0.756top-5 acc: 0.756
2025-03-07 18:54:19,308 - INFO - Beginning epoch 40/800
2025-03-07 18:54:19,318 - INFO - training batch 1, loss: 1.350, 32/60000 datapoints
2025-03-07 18:54:19,684 - INFO - training batch 51, loss: 1.379, 1632/60000 datapoints
2025-03-07 18:54:20,015 - INFO - training batch 101, loss: 1.201, 3232/60000 datapoints
2025-03-07 18:54:20,311 - INFO - training batch 151, loss: 1.304, 4832/60000 datapoints
2025-03-07 18:54:20,646 - INFO - training batch 201, loss: 1.290, 6432/60000 datapoints
2025-03-07 18:54:20,971 - INFO - training batch 251, loss: 1.320, 8032/60000 datapoints
2025-03-07 18:54:21,287 - INFO - training batch 301, loss: 1.233, 9632/60000 datapoints
2025-03-07 18:54:21,662 - INFO - training batch 351, loss: 1.395, 11232/60000 datapoints
2025-03-07 18:54:21,996 - INFO - training batch 401, loss: 1.304, 12832/60000 datapoints
2025-03-07 18:54:22,316 - INFO - training batch 451, loss: 1.392, 14432/60000 datapoints
2025-03-07 18:54:22,645 - INFO - training batch 501, loss: 1.149, 16032/60000 datapoints
2025-03-07 18:54:22,964 - INFO - training batch 551, loss: 1.285, 17632/60000 datapoints
2025-03-07 18:54:23,411 - INFO - training batch 601, loss: 1.208, 19232/60000 datapoints
2025-03-07 18:54:23,778 - INFO - training batch 651, loss: 1.272, 20832/60000 datapoints
2025-03-07 18:54:24,136 - INFO - training batch 701, loss: 0.949, 22432/60000 datapoints
2025-03-07 18:54:24,510 - INFO - training batch 751, loss: 1.300, 24032/60000 datapoints
2025-03-07 18:54:24,940 - INFO - training batch 801, loss: 1.345, 25632/60000 datapoints
2025-03-07 18:54:25,593 - INFO - training batch 851, loss: 1.065, 27232/60000 datapoints
2025-03-07 18:54:26,019 - INFO - training batch 901, loss: 1.333, 28832/60000 datapoints
2025-03-07 18:54:26,454 - INFO - training batch 951, loss: 1.084, 30432/60000 datapoints
2025-03-07 18:54:26,843 - INFO - training batch 1001, loss: 1.302, 32032/60000 datapoints
2025-03-07 18:54:27,328 - INFO - training batch 1051, loss: 1.124, 33632/60000 datapoints
2025-03-07 18:54:27,734 - INFO - training batch 1101, loss: 1.171, 35232/60000 datapoints
2025-03-07 18:54:28,151 - INFO - training batch 1151, loss: 1.093, 36832/60000 datapoints
2025-03-07 18:54:28,568 - INFO - training batch 1201, loss: 1.346, 38432/60000 datapoints
2025-03-07 18:54:29,000 - INFO - training batch 1251, loss: 1.309, 40032/60000 datapoints
2025-03-07 18:54:29,539 - INFO - training batch 1301, loss: 1.276, 41632/60000 datapoints
2025-03-07 18:54:29,987 - INFO - training batch 1351, loss: 1.346, 43232/60000 datapoints
2025-03-07 18:54:30,374 - INFO - training batch 1401, loss: 1.253, 44832/60000 datapoints
2025-03-07 18:54:30,759 - INFO - training batch 1451, loss: 1.315, 46432/60000 datapoints
2025-03-07 18:54:31,307 - INFO - training batch 1501, loss: 1.043, 48032/60000 datapoints
2025-03-07 18:54:31,694 - INFO - training batch 1551, loss: 1.175, 49632/60000 datapoints
2025-03-07 18:54:32,122 - INFO - training batch 1601, loss: 1.143, 51232/60000 datapoints
2025-03-07 18:54:32,444 - INFO - training batch 1651, loss: 1.295, 52832/60000 datapoints
2025-03-07 18:54:32,805 - INFO - training batch 1701, loss: 1.218, 54432/60000 datapoints
2025-03-07 18:54:33,112 - INFO - training batch 1751, loss: 1.248, 56032/60000 datapoints
2025-03-07 18:54:33,487 - INFO - training batch 1801, loss: 1.227, 57632/60000 datapoints
2025-03-07 18:54:33,965 - INFO - training batch 1851, loss: 1.174, 59232/60000 datapoints
2025-03-07 18:54:34,119 - INFO - validation batch 1, loss: 1.214, 32/10016 datapoints
2025-03-07 18:54:34,393 - INFO - validation batch 51, loss: 1.088, 1632/10016 datapoints
2025-03-07 18:54:34,686 - INFO - validation batch 101, loss: 1.267, 3232/10016 datapoints
2025-03-07 18:54:34,960 - INFO - validation batch 151, loss: 1.245, 4832/10016 datapoints
2025-03-07 18:54:35,215 - INFO - validation batch 201, loss: 1.254, 6432/10016 datapoints
2025-03-07 18:54:35,478 - INFO - validation batch 251, loss: 1.308, 8032/10016 datapoints
2025-03-07 18:54:35,751 - INFO - validation batch 301, loss: 1.193, 9632/10016 datapoints
2025-03-07 18:54:35,805 - INFO - Epoch 40/800 done.
2025-03-07 18:54:35,806 - INFO - Final validation performance:
Loss: 1.224, top-1 acc: 0.760top-5 acc: 0.760
2025-03-07 18:54:35,806 - INFO - Beginning epoch 41/800
2025-03-07 18:54:35,817 - INFO - training batch 1, loss: 1.244, 32/60000 datapoints
2025-03-07 18:54:36,153 - INFO - training batch 51, loss: 1.130, 1632/60000 datapoints
2025-03-07 18:54:36,455 - INFO - training batch 101, loss: 1.195, 3232/60000 datapoints
2025-03-07 18:54:36,866 - INFO - training batch 151, loss: 1.179, 4832/60000 datapoints
2025-03-07 18:54:37,254 - INFO - training batch 201, loss: 1.227, 6432/60000 datapoints
2025-03-07 18:54:37,626 - INFO - training batch 251, loss: 1.152, 8032/60000 datapoints
2025-03-07 18:54:38,038 - INFO - training batch 301, loss: 1.325, 9632/60000 datapoints
2025-03-07 18:54:38,441 - INFO - training batch 351, loss: 1.218, 11232/60000 datapoints
2025-03-07 18:54:39,005 - INFO - training batch 401, loss: 1.327, 12832/60000 datapoints
2025-03-07 18:54:39,352 - INFO - training batch 451, loss: 1.233, 14432/60000 datapoints
2025-03-07 18:54:39,691 - INFO - training batch 501, loss: 1.423, 16032/60000 datapoints
2025-03-07 18:54:40,078 - INFO - training batch 551, loss: 1.373, 17632/60000 datapoints
2025-03-07 18:54:40,372 - INFO - training batch 601, loss: 1.144, 19232/60000 datapoints
2025-03-07 18:54:40,807 - INFO - training batch 651, loss: 1.221, 20832/60000 datapoints
2025-03-07 18:54:41,472 - INFO - training batch 701, loss: 1.179, 22432/60000 datapoints
2025-03-07 18:54:41,898 - INFO - training batch 751, loss: 1.146, 24032/60000 datapoints
2025-03-07 18:54:42,220 - INFO - training batch 801, loss: 1.263, 25632/60000 datapoints
2025-03-07 18:54:42,512 - INFO - training batch 851, loss: 1.242, 27232/60000 datapoints
2025-03-07 18:54:42,786 - INFO - training batch 901, loss: 1.309, 28832/60000 datapoints
2025-03-07 18:54:43,058 - INFO - training batch 951, loss: 1.035, 30432/60000 datapoints
2025-03-07 18:54:43,348 - INFO - training batch 1001, loss: 1.228, 32032/60000 datapoints
2025-03-07 18:54:43,618 - INFO - training batch 1051, loss: 1.079, 33632/60000 datapoints
2025-03-07 18:54:43,892 - INFO - training batch 1101, loss: 1.215, 35232/60000 datapoints
2025-03-07 18:54:44,178 - INFO - training batch 1151, loss: 1.162, 36832/60000 datapoints
2025-03-07 18:54:44,603 - INFO - training batch 1201, loss: 1.218, 38432/60000 datapoints
2025-03-07 18:54:44,966 - INFO - training batch 1251, loss: 1.241, 40032/60000 datapoints
2025-03-07 18:54:45,329 - INFO - training batch 1301, loss: 1.299, 41632/60000 datapoints
2025-03-07 18:54:45,637 - INFO - training batch 1351, loss: 1.251, 43232/60000 datapoints
2025-03-07 18:54:45,942 - INFO - training batch 1401, loss: 1.097, 44832/60000 datapoints
2025-03-07 18:54:46,229 - INFO - training batch 1451, loss: 1.237, 46432/60000 datapoints
2025-03-07 18:54:46,502 - INFO - training batch 1501, loss: 1.078, 48032/60000 datapoints
2025-03-07 18:54:46,780 - INFO - training batch 1551, loss: 1.149, 49632/60000 datapoints
2025-03-07 18:54:47,117 - INFO - training batch 1601, loss: 1.152, 51232/60000 datapoints
2025-03-07 18:54:47,451 - INFO - training batch 1651, loss: 1.116, 52832/60000 datapoints
2025-03-07 18:54:47,762 - INFO - training batch 1701, loss: 1.170, 54432/60000 datapoints
2025-03-07 18:54:48,149 - INFO - training batch 1751, loss: 1.027, 56032/60000 datapoints
2025-03-07 18:54:48,557 - INFO - training batch 1801, loss: 1.225, 57632/60000 datapoints
2025-03-07 18:54:49,099 - INFO - training batch 1851, loss: 1.186, 59232/60000 datapoints
2025-03-07 18:54:49,491 - INFO - validation batch 1, loss: 1.246, 32/10016 datapoints
2025-03-07 18:54:50,432 - INFO - validation batch 51, loss: 1.133, 1632/10016 datapoints
2025-03-07 18:54:50,856 - INFO - validation batch 101, loss: 1.238, 3232/10016 datapoints
2025-03-07 18:54:51,205 - INFO - validation batch 151, loss: 1.270, 4832/10016 datapoints
2025-03-07 18:54:51,531 - INFO - validation batch 201, loss: 1.122, 6432/10016 datapoints
2025-03-07 18:54:51,870 - INFO - validation batch 251, loss: 1.134, 8032/10016 datapoints
2025-03-07 18:54:52,200 - INFO - validation batch 301, loss: 1.112, 9632/10016 datapoints
2025-03-07 18:54:52,271 - INFO - Epoch 41/800 done.
2025-03-07 18:54:52,271 - INFO - Final validation performance:
Loss: 1.179, top-1 acc: 0.763top-5 acc: 0.763
2025-03-07 18:54:52,272 - INFO - Beginning epoch 42/800
2025-03-07 18:54:52,284 - INFO - training batch 1, loss: 1.099, 32/60000 datapoints
2025-03-07 18:54:52,612 - INFO - training batch 51, loss: 1.083, 1632/60000 datapoints
2025-03-07 18:54:53,211 - INFO - training batch 101, loss: 1.318, 3232/60000 datapoints
2025-03-07 18:54:53,840 - INFO - training batch 151, loss: 1.413, 4832/60000 datapoints
2025-03-07 18:54:54,331 - INFO - training batch 201, loss: 1.115, 6432/60000 datapoints
2025-03-07 18:54:54,868 - INFO - training batch 251, loss: 1.248, 8032/60000 datapoints
2025-03-07 18:54:55,831 - INFO - training batch 301, loss: 1.243, 9632/60000 datapoints
2025-03-07 18:54:56,270 - INFO - training batch 351, loss: 1.179, 11232/60000 datapoints
2025-03-07 18:54:56,641 - INFO - training batch 401, loss: 1.175, 12832/60000 datapoints
2025-03-07 18:54:56,969 - INFO - training batch 451, loss: 1.029, 14432/60000 datapoints
2025-03-07 18:54:57,278 - INFO - training batch 501, loss: 1.221, 16032/60000 datapoints
2025-03-07 18:54:57,641 - INFO - training batch 551, loss: 1.186, 17632/60000 datapoints
2025-03-07 18:54:57,963 - INFO - training batch 601, loss: 1.330, 19232/60000 datapoints
2025-03-07 18:54:58,330 - INFO - training batch 651, loss: 1.147, 20832/60000 datapoints
2025-03-07 18:54:58,696 - INFO - training batch 701, loss: 1.278, 22432/60000 datapoints
2025-03-07 18:54:59,080 - INFO - training batch 751, loss: 1.099, 24032/60000 datapoints
2025-03-07 18:54:59,412 - INFO - training batch 801, loss: 1.163, 25632/60000 datapoints
2025-03-07 18:54:59,789 - INFO - training batch 851, loss: 1.132, 27232/60000 datapoints
2025-03-07 18:55:00,125 - INFO - training batch 901, loss: 1.128, 28832/60000 datapoints
2025-03-07 18:55:00,466 - INFO - training batch 951, loss: 1.145, 30432/60000 datapoints
2025-03-07 18:55:00,797 - INFO - training batch 1001, loss: 1.182, 32032/60000 datapoints
2025-03-07 18:55:01,180 - INFO - training batch 1051, loss: 1.179, 33632/60000 datapoints
2025-03-07 18:55:01,581 - INFO - training batch 1101, loss: 1.086, 35232/60000 datapoints
2025-03-07 18:55:02,174 - INFO - training batch 1151, loss: 1.181, 36832/60000 datapoints
2025-03-07 18:55:02,506 - INFO - training batch 1201, loss: 1.059, 38432/60000 datapoints
2025-03-07 18:55:02,796 - INFO - training batch 1251, loss: 1.104, 40032/60000 datapoints
2025-03-07 18:55:03,127 - INFO - training batch 1301, loss: 1.351, 41632/60000 datapoints
2025-03-07 18:55:03,439 - INFO - training batch 1351, loss: 1.110, 43232/60000 datapoints
2025-03-07 18:55:03,834 - INFO - training batch 1401, loss: 1.145, 44832/60000 datapoints
2025-03-07 18:55:04,196 - INFO - training batch 1451, loss: 1.300, 46432/60000 datapoints
2025-03-07 18:55:04,566 - INFO - training batch 1501, loss: 1.342, 48032/60000 datapoints
2025-03-07 18:55:04,931 - INFO - training batch 1551, loss: 1.247, 49632/60000 datapoints
2025-03-07 18:55:05,308 - INFO - training batch 1601, loss: 1.139, 51232/60000 datapoints
2025-03-07 18:55:05,779 - INFO - training batch 1651, loss: 1.252, 52832/60000 datapoints
2025-03-07 18:55:06,455 - INFO - training batch 1701, loss: 1.172, 54432/60000 datapoints
2025-03-07 18:55:06,951 - INFO - training batch 1751, loss: 1.120, 56032/60000 datapoints
2025-03-07 18:55:07,317 - INFO - training batch 1801, loss: 1.029, 57632/60000 datapoints
2025-03-07 18:55:07,719 - INFO - training batch 1851, loss: 1.102, 59232/60000 datapoints
2025-03-07 18:55:07,893 - INFO - validation batch 1, loss: 1.132, 32/10016 datapoints
2025-03-07 18:55:08,255 - INFO - validation batch 51, loss: 1.185, 1632/10016 datapoints
2025-03-07 18:55:08,553 - INFO - validation batch 101, loss: 1.269, 3232/10016 datapoints
2025-03-07 18:55:08,821 - INFO - validation batch 151, loss: 1.035, 4832/10016 datapoints
2025-03-07 18:55:09,093 - INFO - validation batch 201, loss: 1.146, 6432/10016 datapoints
2025-03-07 18:55:09,339 - INFO - validation batch 251, loss: 1.137, 8032/10016 datapoints
2025-03-07 18:55:09,575 - INFO - validation batch 301, loss: 1.164, 9632/10016 datapoints
2025-03-07 18:55:09,630 - INFO - Epoch 42/800 done.
2025-03-07 18:55:09,630 - INFO - Final validation performance:
Loss: 1.152, top-1 acc: 0.767top-5 acc: 0.767
2025-03-07 18:55:09,631 - INFO - Beginning epoch 43/800
2025-03-07 18:55:09,640 - INFO - training batch 1, loss: 1.239, 32/60000 datapoints
2025-03-07 18:55:09,961 - INFO - training batch 51, loss: 1.142, 1632/60000 datapoints
2025-03-07 18:55:10,341 - INFO - training batch 101, loss: 1.263, 3232/60000 datapoints
2025-03-07 18:55:10,665 - INFO - training batch 151, loss: 1.100, 4832/60000 datapoints
2025-03-07 18:55:11,031 - INFO - training batch 201, loss: 1.282, 6432/60000 datapoints
2025-03-07 18:55:11,357 - INFO - training batch 251, loss: 1.222, 8032/60000 datapoints
2025-03-07 18:55:11,652 - INFO - training batch 301, loss: 1.048, 9632/60000 datapoints
2025-03-07 18:55:12,025 - INFO - training batch 351, loss: 1.108, 11232/60000 datapoints
2025-03-07 18:55:12,376 - INFO - training batch 401, loss: 1.130, 12832/60000 datapoints
2025-03-07 18:55:12,730 - INFO - training batch 451, loss: 1.057, 14432/60000 datapoints
2025-03-07 18:55:13,074 - INFO - training batch 501, loss: 1.331, 16032/60000 datapoints
2025-03-07 18:55:13,377 - INFO - training batch 551, loss: 0.955, 17632/60000 datapoints
2025-03-07 18:55:13,758 - INFO - training batch 601, loss: 1.131, 19232/60000 datapoints
2025-03-07 18:55:14,275 - INFO - training batch 651, loss: 1.294, 20832/60000 datapoints
2025-03-07 18:55:14,862 - INFO - training batch 701, loss: 1.213, 22432/60000 datapoints
2025-03-07 18:55:15,302 - INFO - training batch 751, loss: 1.085, 24032/60000 datapoints
2025-03-07 18:55:15,612 - INFO - training batch 801, loss: 1.206, 25632/60000 datapoints
2025-03-07 18:55:16,002 - INFO - training batch 851, loss: 1.138, 27232/60000 datapoints
2025-03-07 18:55:16,399 - INFO - training batch 901, loss: 1.204, 28832/60000 datapoints
2025-03-07 18:55:16,723 - INFO - training batch 951, loss: 1.310, 30432/60000 datapoints
2025-03-07 18:55:17,055 - INFO - training batch 1001, loss: 1.401, 32032/60000 datapoints
2025-03-07 18:55:17,393 - INFO - training batch 1051, loss: 1.216, 33632/60000 datapoints
2025-03-07 18:55:17,716 - INFO - training batch 1101, loss: 1.208, 35232/60000 datapoints
2025-03-07 18:55:18,083 - INFO - training batch 1151, loss: 1.053, 36832/60000 datapoints
2025-03-07 18:55:18,410 - INFO - training batch 1201, loss: 1.142, 38432/60000 datapoints
2025-03-07 18:55:18,764 - INFO - training batch 1251, loss: 1.182, 40032/60000 datapoints
2025-03-07 18:55:19,199 - INFO - training batch 1301, loss: 1.202, 41632/60000 datapoints
2025-03-07 18:55:19,790 - INFO - training batch 1351, loss: 1.195, 43232/60000 datapoints
2025-03-07 18:55:20,180 - INFO - training batch 1401, loss: 1.171, 44832/60000 datapoints
2025-03-07 18:55:20,516 - INFO - training batch 1451, loss: 1.493, 46432/60000 datapoints
2025-03-07 18:55:20,830 - INFO - training batch 1501, loss: 1.137, 48032/60000 datapoints
2025-03-07 18:55:21,146 - INFO - training batch 1551, loss: 1.032, 49632/60000 datapoints
2025-03-07 18:55:21,478 - INFO - training batch 1601, loss: 1.080, 51232/60000 datapoints
2025-03-07 18:55:21,795 - INFO - training batch 1651, loss: 1.166, 52832/60000 datapoints
2025-03-07 18:55:22,180 - INFO - training batch 1701, loss: 1.003, 54432/60000 datapoints
2025-03-07 18:55:22,510 - INFO - training batch 1751, loss: 1.168, 56032/60000 datapoints
2025-03-07 18:55:22,860 - INFO - training batch 1801, loss: 1.197, 57632/60000 datapoints
2025-03-07 18:55:23,152 - INFO - training batch 1851, loss: 1.218, 59232/60000 datapoints
2025-03-07 18:55:23,314 - INFO - validation batch 1, loss: 1.305, 32/10016 datapoints
2025-03-07 18:55:23,571 - INFO - validation batch 51, loss: 1.071, 1632/10016 datapoints
2025-03-07 18:55:23,907 - INFO - validation batch 101, loss: 1.202, 3232/10016 datapoints
2025-03-07 18:55:24,239 - INFO - validation batch 151, loss: 1.060, 4832/10016 datapoints
2025-03-07 18:55:24,580 - INFO - validation batch 201, loss: 1.103, 6432/10016 datapoints
2025-03-07 18:55:24,828 - INFO - validation batch 251, loss: 1.274, 8032/10016 datapoints
2025-03-07 18:55:25,128 - INFO - validation batch 301, loss: 1.114, 9632/10016 datapoints
2025-03-07 18:55:25,222 - INFO - Epoch 43/800 done.
2025-03-07 18:55:25,222 - INFO - Final validation performance:
Loss: 1.161, top-1 acc: 0.772top-5 acc: 0.772
2025-03-07 18:55:25,224 - INFO - Beginning epoch 44/800
2025-03-07 18:55:25,234 - INFO - training batch 1, loss: 1.210, 32/60000 datapoints
2025-03-07 18:55:25,663 - INFO - training batch 51, loss: 1.009, 1632/60000 datapoints
2025-03-07 18:55:26,112 - INFO - training batch 101, loss: 1.043, 3232/60000 datapoints
2025-03-07 18:55:26,451 - INFO - training batch 151, loss: 1.266, 4832/60000 datapoints
2025-03-07 18:55:26,770 - INFO - training batch 201, loss: 1.079, 6432/60000 datapoints
2025-03-07 18:55:27,195 - INFO - training batch 251, loss: 1.035, 8032/60000 datapoints
2025-03-07 18:55:27,788 - INFO - training batch 301, loss: 1.323, 9632/60000 datapoints
2025-03-07 18:55:28,210 - INFO - training batch 351, loss: 1.047, 11232/60000 datapoints
2025-03-07 18:55:28,632 - INFO - training batch 401, loss: 1.146, 12832/60000 datapoints
2025-03-07 18:55:29,006 - INFO - training batch 451, loss: 1.265, 14432/60000 datapoints
2025-03-07 18:55:29,354 - INFO - training batch 501, loss: 1.222, 16032/60000 datapoints
2025-03-07 18:55:29,708 - INFO - training batch 551, loss: 1.137, 17632/60000 datapoints
2025-03-07 18:55:30,086 - INFO - training batch 601, loss: 1.047, 19232/60000 datapoints
2025-03-07 18:55:30,774 - INFO - training batch 651, loss: 1.113, 20832/60000 datapoints
2025-03-07 18:55:31,356 - INFO - training batch 701, loss: 1.145, 22432/60000 datapoints
2025-03-07 18:55:31,814 - INFO - training batch 751, loss: 1.247, 24032/60000 datapoints
2025-03-07 18:55:32,184 - INFO - training batch 801, loss: 1.173, 25632/60000 datapoints
2025-03-07 18:55:32,512 - INFO - training batch 851, loss: 1.133, 27232/60000 datapoints
2025-03-07 18:55:32,816 - INFO - training batch 901, loss: 1.209, 28832/60000 datapoints
2025-03-07 18:55:33,139 - INFO - training batch 951, loss: 1.330, 30432/60000 datapoints
2025-03-07 18:55:33,560 - INFO - training batch 1001, loss: 1.261, 32032/60000 datapoints
2025-03-07 18:55:33,935 - INFO - training batch 1051, loss: 0.997, 33632/60000 datapoints
2025-03-07 18:55:34,294 - INFO - training batch 1101, loss: 1.048, 35232/60000 datapoints
2025-03-07 18:55:34,667 - INFO - training batch 1151, loss: 1.196, 36832/60000 datapoints
2025-03-07 18:55:34,989 - INFO - training batch 1201, loss: 1.365, 38432/60000 datapoints
2025-03-07 18:55:35,408 - INFO - training batch 1251, loss: 1.156, 40032/60000 datapoints
2025-03-07 18:55:35,834 - INFO - training batch 1301, loss: 1.194, 41632/60000 datapoints
2025-03-07 18:55:36,272 - INFO - training batch 1351, loss: 1.207, 43232/60000 datapoints
2025-03-07 18:55:36,688 - INFO - training batch 1401, loss: 1.287, 44832/60000 datapoints
2025-03-07 18:55:37,074 - INFO - training batch 1451, loss: 1.413, 46432/60000 datapoints
2025-03-07 18:55:37,391 - INFO - training batch 1501, loss: 1.064, 48032/60000 datapoints
2025-03-07 18:55:37,688 - INFO - training batch 1551, loss: 1.103, 49632/60000 datapoints
2025-03-07 18:55:38,225 - INFO - training batch 1601, loss: 0.904, 51232/60000 datapoints
2025-03-07 18:55:38,660 - INFO - training batch 1651, loss: 1.137, 52832/60000 datapoints
2025-03-07 18:55:39,028 - INFO - training batch 1701, loss: 1.108, 54432/60000 datapoints
2025-03-07 18:55:39,368 - INFO - training batch 1751, loss: 1.177, 56032/60000 datapoints
2025-03-07 18:55:39,695 - INFO - training batch 1801, loss: 1.169, 57632/60000 datapoints
2025-03-07 18:55:40,065 - INFO - training batch 1851, loss: 1.132, 59232/60000 datapoints
2025-03-07 18:55:40,266 - INFO - validation batch 1, loss: 1.256, 32/10016 datapoints
2025-03-07 18:55:40,585 - INFO - validation batch 51, loss: 0.978, 1632/10016 datapoints
2025-03-07 18:55:40,959 - INFO - validation batch 101, loss: 0.990, 3232/10016 datapoints
2025-03-07 18:55:41,274 - INFO - validation batch 151, loss: 1.212, 4832/10016 datapoints
2025-03-07 18:55:41,530 - INFO - validation batch 201, loss: 1.171, 6432/10016 datapoints
2025-03-07 18:55:41,773 - INFO - validation batch 251, loss: 1.174, 8032/10016 datapoints
2025-03-07 18:55:42,014 - INFO - validation batch 301, loss: 1.124, 9632/10016 datapoints
2025-03-07 18:55:42,079 - INFO - Epoch 44/800 done.
2025-03-07 18:55:42,079 - INFO - Final validation performance:
Loss: 1.129, top-1 acc: 0.776top-5 acc: 0.776
2025-03-07 18:55:42,080 - INFO - Beginning epoch 45/800
2025-03-07 18:55:42,088 - INFO - training batch 1, loss: 1.242, 32/60000 datapoints
2025-03-07 18:55:42,399 - INFO - training batch 51, loss: 1.045, 1632/60000 datapoints
2025-03-07 18:55:42,976 - INFO - training batch 101, loss: 1.003, 3232/60000 datapoints
2025-03-07 18:55:43,316 - INFO - training batch 151, loss: 1.236, 4832/60000 datapoints
2025-03-07 18:55:43,612 - INFO - training batch 201, loss: 1.027, 6432/60000 datapoints
2025-03-07 18:55:43,903 - INFO - training batch 251, loss: 1.126, 8032/60000 datapoints
2025-03-07 18:55:44,191 - INFO - training batch 301, loss: 1.135, 9632/60000 datapoints
2025-03-07 18:55:44,577 - INFO - training batch 351, loss: 1.312, 11232/60000 datapoints
2025-03-07 18:55:44,927 - INFO - training batch 401, loss: 1.107, 12832/60000 datapoints
2025-03-07 18:55:45,230 - INFO - training batch 451, loss: 1.093, 14432/60000 datapoints
2025-03-07 18:55:45,531 - INFO - training batch 501, loss: 1.241, 16032/60000 datapoints
2025-03-07 18:55:45,821 - INFO - training batch 551, loss: 1.144, 17632/60000 datapoints
2025-03-07 18:55:46,111 - INFO - training batch 601, loss: 1.148, 19232/60000 datapoints
2025-03-07 18:55:46,395 - INFO - training batch 651, loss: 1.059, 20832/60000 datapoints
2025-03-07 18:55:46,673 - INFO - training batch 701, loss: 1.086, 22432/60000 datapoints
2025-03-07 18:55:46,950 - INFO - training batch 751, loss: 1.047, 24032/60000 datapoints
2025-03-07 18:55:47,227 - INFO - training batch 801, loss: 1.321, 25632/60000 datapoints
2025-03-07 18:55:47,498 - INFO - training batch 851, loss: 1.035, 27232/60000 datapoints
2025-03-07 18:55:47,784 - INFO - training batch 901, loss: 1.234, 28832/60000 datapoints
2025-03-07 18:55:48,063 - INFO - training batch 951, loss: 1.095, 30432/60000 datapoints
2025-03-07 18:55:48,343 - INFO - training batch 1001, loss: 1.257, 32032/60000 datapoints
2025-03-07 18:55:48,621 - INFO - training batch 1051, loss: 1.255, 33632/60000 datapoints
2025-03-07 18:55:48,901 - INFO - training batch 1101, loss: 1.243, 35232/60000 datapoints
2025-03-07 18:55:49,173 - INFO - training batch 1151, loss: 1.212, 36832/60000 datapoints
2025-03-07 18:55:49,448 - INFO - training batch 1201, loss: 1.100, 38432/60000 datapoints
2025-03-07 18:55:49,738 - INFO - training batch 1251, loss: 1.330, 40032/60000 datapoints
2025-03-07 18:55:50,019 - INFO - training batch 1301, loss: 1.026, 41632/60000 datapoints
2025-03-07 18:55:50,316 - INFO - training batch 1351, loss: 1.126, 43232/60000 datapoints
2025-03-07 18:55:50,645 - INFO - training batch 1401, loss: 1.008, 44832/60000 datapoints
2025-03-07 18:55:50,946 - INFO - training batch 1451, loss: 1.421, 46432/60000 datapoints
2025-03-07 18:55:51,566 - INFO - training batch 1501, loss: 1.065, 48032/60000 datapoints
2025-03-07 18:55:52,012 - INFO - training batch 1551, loss: 1.119, 49632/60000 datapoints
2025-03-07 18:55:52,297 - INFO - training batch 1601, loss: 1.155, 51232/60000 datapoints
2025-03-07 18:55:52,576 - INFO - training batch 1651, loss: 1.113, 52832/60000 datapoints
2025-03-07 18:55:52,860 - INFO - training batch 1701, loss: 1.203, 54432/60000 datapoints
2025-03-07 18:55:53,150 - INFO - training batch 1751, loss: 1.105, 56032/60000 datapoints
2025-03-07 18:55:53,438 - INFO - training batch 1801, loss: 1.084, 57632/60000 datapoints
2025-03-07 18:55:53,726 - INFO - training batch 1851, loss: 1.025, 59232/60000 datapoints
2025-03-07 18:55:53,873 - INFO - validation batch 1, loss: 1.133, 32/10016 datapoints
2025-03-07 18:55:54,100 - INFO - validation batch 51, loss: 0.949, 1632/10016 datapoints
2025-03-07 18:55:54,339 - INFO - validation batch 101, loss: 1.259, 3232/10016 datapoints
2025-03-07 18:55:54,665 - INFO - validation batch 151, loss: 1.067, 4832/10016 datapoints
2025-03-07 18:55:54,979 - INFO - validation batch 201, loss: 0.939, 6432/10016 datapoints
2025-03-07 18:55:55,297 - INFO - validation batch 251, loss: 0.889, 8032/10016 datapoints
2025-03-07 18:55:55,607 - INFO - validation batch 301, loss: 0.906, 9632/10016 datapoints
2025-03-07 18:55:55,670 - INFO - Epoch 45/800 done.
2025-03-07 18:55:55,670 - INFO - Final validation performance:
Loss: 1.020, top-1 acc: 0.779top-5 acc: 0.779
2025-03-07 18:55:55,671 - INFO - Beginning epoch 46/800
2025-03-07 18:55:55,682 - INFO - training batch 1, loss: 1.138, 32/60000 datapoints
2025-03-07 18:55:56,099 - INFO - training batch 51, loss: 1.010, 1632/60000 datapoints
2025-03-07 18:55:56,429 - INFO - training batch 101, loss: 1.169, 3232/60000 datapoints
2025-03-07 18:55:56,724 - INFO - training batch 151, loss: 1.112, 4832/60000 datapoints
2025-03-07 18:55:57,012 - INFO - training batch 201, loss: 1.006, 6432/60000 datapoints
2025-03-07 18:55:57,308 - INFO - training batch 251, loss: 1.152, 8032/60000 datapoints
2025-03-07 18:55:57,647 - INFO - training batch 301, loss: 1.119, 9632/60000 datapoints
2025-03-07 18:55:57,952 - INFO - training batch 351, loss: 1.104, 11232/60000 datapoints
2025-03-07 18:55:58,250 - INFO - training batch 401, loss: 1.144, 12832/60000 datapoints
2025-03-07 18:55:58,543 - INFO - training batch 451, loss: 1.178, 14432/60000 datapoints
2025-03-07 18:55:58,836 - INFO - training batch 501, loss: 1.061, 16032/60000 datapoints
2025-03-07 18:55:59,137 - INFO - training batch 551, loss: 1.229, 17632/60000 datapoints
2025-03-07 18:55:59,437 - INFO - training batch 601, loss: 0.972, 19232/60000 datapoints
2025-03-07 18:55:59,727 - INFO - training batch 651, loss: 1.004, 20832/60000 datapoints
2025-03-07 18:56:00,067 - INFO - training batch 701, loss: 1.134, 22432/60000 datapoints
2025-03-07 18:56:00,428 - INFO - training batch 751, loss: 1.188, 24032/60000 datapoints
2025-03-07 18:56:00,777 - INFO - training batch 801, loss: 1.154, 25632/60000 datapoints
2025-03-07 18:56:01,090 - INFO - training batch 851, loss: 1.103, 27232/60000 datapoints
2025-03-07 18:56:01,394 - INFO - training batch 901, loss: 1.083, 28832/60000 datapoints
2025-03-07 18:56:01,704 - INFO - training batch 951, loss: 1.197, 30432/60000 datapoints
2025-03-07 18:56:02,127 - INFO - training batch 1001, loss: 1.082, 32032/60000 datapoints
2025-03-07 18:56:02,445 - INFO - training batch 1051, loss: 1.198, 33632/60000 datapoints
2025-03-07 18:56:02,756 - INFO - training batch 1101, loss: 1.143, 35232/60000 datapoints
2025-03-07 18:56:03,100 - INFO - training batch 1151, loss: 1.143, 36832/60000 datapoints
2025-03-07 18:56:03,374 - INFO - training batch 1201, loss: 1.172, 38432/60000 datapoints
2025-03-07 18:56:03,646 - INFO - training batch 1251, loss: 1.075, 40032/60000 datapoints
2025-03-07 18:56:03,927 - INFO - training batch 1301, loss: 1.047, 41632/60000 datapoints
2025-03-07 18:56:04,205 - INFO - training batch 1351, loss: 1.136, 43232/60000 datapoints
2025-03-07 18:56:04,479 - INFO - training batch 1401, loss: 0.859, 44832/60000 datapoints
2025-03-07 18:56:04,756 - INFO - training batch 1451, loss: 0.893, 46432/60000 datapoints
2025-03-07 18:56:05,029 - INFO - training batch 1501, loss: 0.989, 48032/60000 datapoints
2025-03-07 18:56:05,303 - INFO - training batch 1551, loss: 1.097, 49632/60000 datapoints
2025-03-07 18:56:05,578 - INFO - training batch 1601, loss: 1.124, 51232/60000 datapoints
2025-03-07 18:56:05,853 - INFO - training batch 1651, loss: 1.262, 52832/60000 datapoints
2025-03-07 18:56:06,152 - INFO - training batch 1701, loss: 1.296, 54432/60000 datapoints
2025-03-07 18:56:06,428 - INFO - training batch 1751, loss: 1.026, 56032/60000 datapoints
2025-03-07 18:56:06,701 - INFO - training batch 1801, loss: 1.242, 57632/60000 datapoints
2025-03-07 18:56:06,982 - INFO - training batch 1851, loss: 0.976, 59232/60000 datapoints
2025-03-07 18:56:07,322 - INFO - validation batch 1, loss: 0.838, 32/10016 datapoints
2025-03-07 18:56:07,562 - INFO - validation batch 51, loss: 0.966, 1632/10016 datapoints
2025-03-07 18:56:07,785 - INFO - validation batch 101, loss: 1.166, 3232/10016 datapoints
2025-03-07 18:56:08,181 - INFO - validation batch 151, loss: 1.068, 4832/10016 datapoints
2025-03-07 18:56:08,447 - INFO - validation batch 201, loss: 1.146, 6432/10016 datapoints
2025-03-07 18:56:08,719 - INFO - validation batch 251, loss: 0.976, 8032/10016 datapoints
2025-03-07 18:56:09,024 - INFO - validation batch 301, loss: 1.171, 9632/10016 datapoints
2025-03-07 18:56:09,121 - INFO - Epoch 46/800 done.
2025-03-07 18:56:09,122 - INFO - Final validation performance:
Loss: 1.047, top-1 acc: 0.782top-5 acc: 0.782
2025-03-07 18:56:09,123 - INFO - Beginning epoch 47/800
2025-03-07 18:56:09,135 - INFO - training batch 1, loss: 1.167, 32/60000 datapoints
2025-03-07 18:56:09,505 - INFO - training batch 51, loss: 0.879, 1632/60000 datapoints
2025-03-07 18:56:10,138 - INFO - training batch 101, loss: 1.137, 3232/60000 datapoints
2025-03-07 18:56:10,563 - INFO - training batch 151, loss: 1.161, 4832/60000 datapoints
2025-03-07 18:56:10,895 - INFO - training batch 201, loss: 1.039, 6432/60000 datapoints
2025-03-07 18:56:11,233 - INFO - training batch 251, loss: 1.127, 8032/60000 datapoints
2025-03-07 18:56:11,646 - INFO - training batch 301, loss: 1.040, 9632/60000 datapoints
2025-03-07 18:56:11,980 - INFO - training batch 351, loss: 1.174, 11232/60000 datapoints
2025-03-07 18:56:12,378 - INFO - training batch 401, loss: 0.989, 12832/60000 datapoints
2025-03-07 18:56:12,763 - INFO - training batch 451, loss: 1.057, 14432/60000 datapoints
2025-03-07 18:56:13,080 - INFO - training batch 501, loss: 1.104, 16032/60000 datapoints
2025-03-07 18:56:13,406 - INFO - training batch 551, loss: 1.039, 17632/60000 datapoints
2025-03-07 18:56:13,715 - INFO - training batch 601, loss: 1.072, 19232/60000 datapoints
2025-03-07 18:56:14,065 - INFO - training batch 651, loss: 0.986, 20832/60000 datapoints
2025-03-07 18:56:14,399 - INFO - training batch 701, loss: 1.294, 22432/60000 datapoints
2025-03-07 18:56:14,746 - INFO - training batch 751, loss: 1.313, 24032/60000 datapoints
2025-03-07 18:56:15,295 - INFO - training batch 801, loss: 1.014, 25632/60000 datapoints
2025-03-07 18:56:15,627 - INFO - training batch 851, loss: 1.225, 27232/60000 datapoints
2025-03-07 18:56:15,981 - INFO - training batch 901, loss: 1.299, 28832/60000 datapoints
2025-03-07 18:56:16,275 - INFO - training batch 951, loss: 1.019, 30432/60000 datapoints
2025-03-07 18:56:16,635 - INFO - training batch 1001, loss: 1.007, 32032/60000 datapoints
2025-03-07 18:56:17,138 - INFO - training batch 1051, loss: 1.231, 33632/60000 datapoints
2025-03-07 18:56:17,547 - INFO - training batch 1101, loss: 0.962, 35232/60000 datapoints
2025-03-07 18:56:18,097 - INFO - training batch 1151, loss: 1.088, 36832/60000 datapoints
2025-03-07 18:56:18,707 - INFO - training batch 1201, loss: 1.075, 38432/60000 datapoints
2025-03-07 18:56:19,444 - INFO - training batch 1251, loss: 0.950, 40032/60000 datapoints
2025-03-07 18:56:19,878 - INFO - training batch 1301, loss: 1.129, 41632/60000 datapoints
2025-03-07 18:56:20,256 - INFO - training batch 1351, loss: 0.969, 43232/60000 datapoints
2025-03-07 18:56:20,654 - INFO - training batch 1401, loss: 0.993, 44832/60000 datapoints
2025-03-07 18:56:21,039 - INFO - training batch 1451, loss: 1.025, 46432/60000 datapoints
2025-03-07 18:56:21,395 - INFO - training batch 1501, loss: 1.060, 48032/60000 datapoints
2025-03-07 18:56:21,755 - INFO - training batch 1551, loss: 1.220, 49632/60000 datapoints
2025-03-07 18:56:22,150 - INFO - training batch 1601, loss: 0.876, 51232/60000 datapoints
2025-03-07 18:56:22,658 - INFO - training batch 1651, loss: 1.150, 52832/60000 datapoints
2025-03-07 18:56:23,055 - INFO - training batch 1701, loss: 1.162, 54432/60000 datapoints
2025-03-07 18:56:23,477 - INFO - training batch 1751, loss: 1.154, 56032/60000 datapoints
2025-03-07 18:56:24,095 - INFO - training batch 1801, loss: 1.126, 57632/60000 datapoints
2025-03-07 18:56:24,658 - INFO - training batch 1851, loss: 1.074, 59232/60000 datapoints
2025-03-07 18:56:24,866 - INFO - validation batch 1, loss: 0.905, 32/10016 datapoints
2025-03-07 18:56:25,257 - INFO - validation batch 51, loss: 0.890, 1632/10016 datapoints
2025-03-07 18:56:26,120 - INFO - validation batch 101, loss: 1.082, 3232/10016 datapoints
2025-03-07 18:56:26,918 - INFO - validation batch 151, loss: 1.030, 4832/10016 datapoints
2025-03-07 18:56:27,581 - INFO - validation batch 201, loss: 0.912, 6432/10016 datapoints
2025-03-07 18:56:28,198 - INFO - validation batch 251, loss: 1.019, 8032/10016 datapoints
2025-03-07 18:56:28,676 - INFO - validation batch 301, loss: 0.930, 9632/10016 datapoints
2025-03-07 18:56:28,748 - INFO - Epoch 47/800 done.
2025-03-07 18:56:28,749 - INFO - Final validation performance:
Loss: 0.967, top-1 acc: 0.785top-5 acc: 0.785
2025-03-07 18:56:28,753 - INFO - Beginning epoch 48/800
2025-03-07 18:56:28,768 - INFO - training batch 1, loss: 1.184, 32/60000 datapoints
2025-03-07 18:56:29,447 - INFO - training batch 51, loss: 1.086, 1632/60000 datapoints
2025-03-07 18:56:29,946 - INFO - training batch 101, loss: 1.087, 3232/60000 datapoints
2025-03-07 18:56:30,385 - INFO - training batch 151, loss: 1.115, 4832/60000 datapoints
2025-03-07 18:56:30,687 - INFO - training batch 201, loss: 1.206, 6432/60000 datapoints
2025-03-07 18:56:31,140 - INFO - training batch 251, loss: 0.976, 8032/60000 datapoints
2025-03-07 18:56:31,565 - INFO - training batch 301, loss: 0.962, 9632/60000 datapoints
2025-03-07 18:56:31,946 - INFO - training batch 351, loss: 1.030, 11232/60000 datapoints
2025-03-07 18:56:32,330 - INFO - training batch 401, loss: 1.228, 12832/60000 datapoints
2025-03-07 18:56:32,855 - INFO - training batch 451, loss: 1.079, 14432/60000 datapoints
2025-03-07 18:56:33,168 - INFO - training batch 501, loss: 1.063, 16032/60000 datapoints
2025-03-07 18:56:33,492 - INFO - training batch 551, loss: 1.032, 17632/60000 datapoints
2025-03-07 18:56:33,804 - INFO - training batch 601, loss: 1.097, 19232/60000 datapoints
2025-03-07 18:56:34,111 - INFO - training batch 651, loss: 0.958, 20832/60000 datapoints
2025-03-07 18:56:34,417 - INFO - training batch 701, loss: 1.037, 22432/60000 datapoints
2025-03-07 18:56:34,719 - INFO - training batch 751, loss: 1.187, 24032/60000 datapoints
2025-03-07 18:56:35,066 - INFO - training batch 801, loss: 0.886, 25632/60000 datapoints
2025-03-07 18:56:35,391 - INFO - training batch 851, loss: 1.324, 27232/60000 datapoints
2025-03-07 18:56:35,742 - INFO - training batch 901, loss: 1.057, 28832/60000 datapoints
2025-03-07 18:56:36,093 - INFO - training batch 951, loss: 1.176, 30432/60000 datapoints
2025-03-07 18:56:36,401 - INFO - training batch 1001, loss: 1.020, 32032/60000 datapoints
2025-03-07 18:56:36,699 - INFO - training batch 1051, loss: 1.051, 33632/60000 datapoints
2025-03-07 18:56:37,075 - INFO - training batch 1101, loss: 1.065, 35232/60000 datapoints
2025-03-07 18:56:37,613 - INFO - training batch 1151, loss: 1.078, 36832/60000 datapoints
2025-03-07 18:56:37,977 - INFO - training batch 1201, loss: 1.060, 38432/60000 datapoints
2025-03-07 18:56:38,285 - INFO - training batch 1251, loss: 1.046, 40032/60000 datapoints
2025-03-07 18:56:38,639 - INFO - training batch 1301, loss: 1.271, 41632/60000 datapoints
2025-03-07 18:56:38,958 - INFO - training batch 1351, loss: 1.064, 43232/60000 datapoints
2025-03-07 18:56:39,345 - INFO - training batch 1401, loss: 1.045, 44832/60000 datapoints
2025-03-07 18:56:39,793 - INFO - training batch 1451, loss: 1.005, 46432/60000 datapoints
2025-03-07 18:56:40,256 - INFO - training batch 1501, loss: 0.965, 48032/60000 datapoints
2025-03-07 18:56:40,650 - INFO - training batch 1551, loss: 1.025, 49632/60000 datapoints
2025-03-07 18:56:41,035 - INFO - training batch 1601, loss: 1.328, 51232/60000 datapoints
2025-03-07 18:56:41,593 - INFO - training batch 1651, loss: 1.174, 52832/60000 datapoints
2025-03-07 18:56:42,102 - INFO - training batch 1701, loss: 1.004, 54432/60000 datapoints
2025-03-07 18:56:42,531 - INFO - training batch 1751, loss: 0.900, 56032/60000 datapoints
2025-03-07 18:56:42,930 - INFO - training batch 1801, loss: 0.926, 57632/60000 datapoints
2025-03-07 18:56:43,294 - INFO - training batch 1851, loss: 1.065, 59232/60000 datapoints
2025-03-07 18:56:43,462 - INFO - validation batch 1, loss: 0.980, 32/10016 datapoints
2025-03-07 18:56:43,737 - INFO - validation batch 51, loss: 1.018, 1632/10016 datapoints
2025-03-07 18:56:44,019 - INFO - validation batch 101, loss: 1.118, 3232/10016 datapoints
2025-03-07 18:56:44,246 - INFO - validation batch 151, loss: 1.082, 4832/10016 datapoints
2025-03-07 18:56:44,471 - INFO - validation batch 201, loss: 1.097, 6432/10016 datapoints
2025-03-07 18:56:44,700 - INFO - validation batch 251, loss: 1.048, 8032/10016 datapoints
2025-03-07 18:56:44,945 - INFO - validation batch 301, loss: 1.016, 9632/10016 datapoints
2025-03-07 18:56:44,997 - INFO - Epoch 48/800 done.
2025-03-07 18:56:44,997 - INFO - Final validation performance:
Loss: 1.051, top-1 acc: 0.788top-5 acc: 0.788
2025-03-07 18:56:45,000 - INFO - Beginning epoch 49/800
2025-03-07 18:56:45,008 - INFO - training batch 1, loss: 1.014, 32/60000 datapoints
2025-03-07 18:56:45,312 - INFO - training batch 51, loss: 1.058, 1632/60000 datapoints
2025-03-07 18:56:45,619 - INFO - training batch 101, loss: 0.941, 3232/60000 datapoints
2025-03-07 18:56:45,931 - INFO - training batch 151, loss: 1.123, 4832/60000 datapoints
2025-03-07 18:56:46,243 - INFO - training batch 201, loss: 0.940, 6432/60000 datapoints
2025-03-07 18:56:46,524 - INFO - training batch 251, loss: 1.197, 8032/60000 datapoints
2025-03-07 18:56:46,836 - INFO - training batch 301, loss: 0.812, 9632/60000 datapoints
2025-03-07 18:56:47,161 - INFO - training batch 351, loss: 1.014, 11232/60000 datapoints
2025-03-07 18:56:47,464 - INFO - training batch 401, loss: 0.956, 12832/60000 datapoints
2025-03-07 18:56:47,765 - INFO - training batch 451, loss: 1.039, 14432/60000 datapoints
2025-03-07 18:56:48,238 - INFO - training batch 501, loss: 1.136, 16032/60000 datapoints
2025-03-07 18:56:48,755 - INFO - training batch 551, loss: 0.947, 17632/60000 datapoints
2025-03-07 18:56:49,187 - INFO - training batch 601, loss: 0.962, 19232/60000 datapoints
2025-03-07 18:56:49,596 - INFO - training batch 651, loss: 1.148, 20832/60000 datapoints
2025-03-07 18:56:49,918 - INFO - training batch 701, loss: 1.132, 22432/60000 datapoints
2025-03-07 18:56:50,212 - INFO - training batch 751, loss: 0.918, 24032/60000 datapoints
2025-03-07 18:56:50,490 - INFO - training batch 801, loss: 0.997, 25632/60000 datapoints
2025-03-07 18:56:50,775 - INFO - training batch 851, loss: 0.920, 27232/60000 datapoints
2025-03-07 18:56:51,067 - INFO - training batch 901, loss: 1.139, 28832/60000 datapoints
2025-03-07 18:56:51,378 - INFO - training batch 951, loss: 1.031, 30432/60000 datapoints
2025-03-07 18:56:51,692 - INFO - training batch 1001, loss: 0.915, 32032/60000 datapoints
2025-03-07 18:56:51,981 - INFO - training batch 1051, loss: 1.112, 33632/60000 datapoints
2025-03-07 18:56:52,366 - INFO - training batch 1101, loss: 1.026, 35232/60000 datapoints
2025-03-07 18:56:52,676 - INFO - training batch 1151, loss: 0.916, 36832/60000 datapoints
2025-03-07 18:56:52,966 - INFO - training batch 1201, loss: 0.952, 38432/60000 datapoints
2025-03-07 18:56:53,357 - INFO - training batch 1251, loss: 1.131, 40032/60000 datapoints
2025-03-07 18:56:53,662 - INFO - training batch 1301, loss: 0.950, 41632/60000 datapoints
2025-03-07 18:56:53,946 - INFO - training batch 1351, loss: 0.940, 43232/60000 datapoints
2025-03-07 18:56:54,232 - INFO - training batch 1401, loss: 1.103, 44832/60000 datapoints
2025-03-07 18:56:54,551 - INFO - training batch 1451, loss: 1.013, 46432/60000 datapoints
2025-03-07 18:56:54,871 - INFO - training batch 1501, loss: 1.051, 48032/60000 datapoints
2025-03-07 18:56:55,177 - INFO - training batch 1551, loss: 1.167, 49632/60000 datapoints
2025-03-07 18:56:55,474 - INFO - training batch 1601, loss: 1.152, 51232/60000 datapoints
2025-03-07 18:56:55,769 - INFO - training batch 1651, loss: 1.044, 52832/60000 datapoints
2025-03-07 18:56:56,073 - INFO - training batch 1701, loss: 0.992, 54432/60000 datapoints
2025-03-07 18:56:56,377 - INFO - training batch 1751, loss: 0.962, 56032/60000 datapoints
2025-03-07 18:56:56,667 - INFO - training batch 1801, loss: 1.098, 57632/60000 datapoints
2025-03-07 18:56:56,982 - INFO - training batch 1851, loss: 0.975, 59232/60000 datapoints
2025-03-07 18:56:57,132 - INFO - validation batch 1, loss: 0.989, 32/10016 datapoints
2025-03-07 18:56:57,390 - INFO - validation batch 51, loss: 1.045, 1632/10016 datapoints
2025-03-07 18:56:57,632 - INFO - validation batch 101, loss: 0.903, 3232/10016 datapoints
2025-03-07 18:56:57,881 - INFO - validation batch 151, loss: 0.983, 4832/10016 datapoints
2025-03-07 18:56:58,141 - INFO - validation batch 201, loss: 0.904, 6432/10016 datapoints
2025-03-07 18:56:58,388 - INFO - validation batch 251, loss: 0.955, 8032/10016 datapoints
2025-03-07 18:56:58,636 - INFO - validation batch 301, loss: 0.952, 9632/10016 datapoints
2025-03-07 18:56:58,690 - INFO - Epoch 49/800 done.
2025-03-07 18:56:58,691 - INFO - Final validation performance:
Loss: 0.962, top-1 acc: 0.791top-5 acc: 0.791
2025-03-07 18:56:58,691 - INFO - Beginning epoch 50/800
2025-03-07 18:56:58,700 - INFO - training batch 1, loss: 0.961, 32/60000 datapoints
2025-03-07 18:56:59,019 - INFO - training batch 51, loss: 1.210, 1632/60000 datapoints
2025-03-07 18:56:59,314 - INFO - training batch 101, loss: 0.893, 3232/60000 datapoints
2025-03-07 18:56:59,644 - INFO - training batch 151, loss: 1.047, 4832/60000 datapoints
2025-03-07 18:56:59,941 - INFO - training batch 201, loss: 1.053, 6432/60000 datapoints
2025-03-07 18:57:00,260 - INFO - training batch 251, loss: 1.087, 8032/60000 datapoints
2025-03-07 18:57:00,568 - INFO - training batch 301, loss: 0.991, 9632/60000 datapoints
2025-03-07 18:57:00,853 - INFO - training batch 351, loss: 1.083, 11232/60000 datapoints
2025-03-07 18:57:01,148 - INFO - training batch 401, loss: 0.923, 12832/60000 datapoints
2025-03-07 18:57:01,450 - INFO - training batch 451, loss: 1.082, 14432/60000 datapoints
2025-03-07 18:57:01,775 - INFO - training batch 501, loss: 1.006, 16032/60000 datapoints
2025-03-07 18:57:02,112 - INFO - training batch 551, loss: 0.943, 17632/60000 datapoints
2025-03-07 18:57:02,403 - INFO - training batch 601, loss: 1.133, 19232/60000 datapoints
2025-03-07 18:57:02,697 - INFO - training batch 651, loss: 0.862, 20832/60000 datapoints
2025-03-07 18:57:02,989 - INFO - training batch 701, loss: 0.986, 22432/60000 datapoints
2025-03-07 18:57:03,266 - INFO - training batch 751, loss: 1.120, 24032/60000 datapoints
2025-03-07 18:57:03,547 - INFO - training batch 801, loss: 0.966, 25632/60000 datapoints
2025-03-07 18:57:03,866 - INFO - training batch 851, loss: 0.866, 27232/60000 datapoints
2025-03-07 18:57:04,173 - INFO - training batch 901, loss: 1.031, 28832/60000 datapoints
2025-03-07 18:57:04,475 - INFO - training batch 951, loss: 0.783, 30432/60000 datapoints
2025-03-07 18:57:04,809 - INFO - training batch 1001, loss: 1.188, 32032/60000 datapoints
2025-03-07 18:57:05,124 - INFO - training batch 1051, loss: 0.856, 33632/60000 datapoints
2025-03-07 18:57:05,423 - INFO - training batch 1101, loss: 1.158, 35232/60000 datapoints
2025-03-07 18:57:05,722 - INFO - training batch 1151, loss: 0.807, 36832/60000 datapoints
2025-03-07 18:57:06,055 - INFO - training batch 1201, loss: 1.147, 38432/60000 datapoints
2025-03-07 18:57:06,360 - INFO - training batch 1251, loss: 0.959, 40032/60000 datapoints
2025-03-07 18:57:06,722 - INFO - training batch 1301, loss: 1.196, 41632/60000 datapoints
2025-03-07 18:57:07,042 - INFO - training batch 1351, loss: 1.220, 43232/60000 datapoints
2025-03-07 18:57:07,348 - INFO - training batch 1401, loss: 0.944, 44832/60000 datapoints
2025-03-07 18:57:07,633 - INFO - training batch 1451, loss: 0.985, 46432/60000 datapoints
2025-03-07 18:57:07,919 - INFO - training batch 1501, loss: 0.881, 48032/60000 datapoints
2025-03-07 18:57:08,213 - INFO - training batch 1551, loss: 1.042, 49632/60000 datapoints
2025-03-07 18:57:08,488 - INFO - training batch 1601, loss: 1.248, 51232/60000 datapoints
2025-03-07 18:57:08,766 - INFO - training batch 1651, loss: 0.915, 52832/60000 datapoints
2025-03-07 18:57:09,041 - INFO - training batch 1701, loss: 1.167, 54432/60000 datapoints
2025-03-07 18:57:09,360 - INFO - training batch 1751, loss: 1.003, 56032/60000 datapoints
2025-03-07 18:57:09,682 - INFO - training batch 1801, loss: 1.150, 57632/60000 datapoints
2025-03-07 18:57:09,985 - INFO - training batch 1851, loss: 1.033, 59232/60000 datapoints
2025-03-07 18:57:10,133 - INFO - validation batch 1, loss: 0.893, 32/10016 datapoints
2025-03-07 18:57:10,356 - INFO - validation batch 51, loss: 1.109, 1632/10016 datapoints
2025-03-07 18:57:10,573 - INFO - validation batch 101, loss: 1.030, 3232/10016 datapoints
2025-03-07 18:57:10,849 - INFO - validation batch 151, loss: 0.944, 4832/10016 datapoints
2025-03-07 18:57:11,140 - INFO - validation batch 201, loss: 1.012, 6432/10016 datapoints
2025-03-07 18:57:11,425 - INFO - validation batch 251, loss: 0.937, 8032/10016 datapoints
2025-03-07 18:57:11,730 - INFO - validation batch 301, loss: 0.995, 9632/10016 datapoints
2025-03-07 18:57:11,802 - INFO - Epoch 50/800 done.
2025-03-07 18:57:11,802 - INFO - Final validation performance:
Loss: 0.989, top-1 acc: 0.793top-5 acc: 0.793
2025-03-07 18:57:11,803 - INFO - Beginning epoch 51/800
2025-03-07 18:57:11,814 - INFO - training batch 1, loss: 1.028, 32/60000 datapoints
2025-03-07 18:57:12,170 - INFO - training batch 51, loss: 1.096, 1632/60000 datapoints
2025-03-07 18:57:12,463 - INFO - training batch 101, loss: 0.851, 3232/60000 datapoints
2025-03-07 18:57:12,748 - INFO - training batch 151, loss: 1.113, 4832/60000 datapoints
2025-03-07 18:57:13,037 - INFO - training batch 201, loss: 1.063, 6432/60000 datapoints
2025-03-07 18:57:13,325 - INFO - training batch 251, loss: 1.141, 8032/60000 datapoints
2025-03-07 18:57:13,630 - INFO - training batch 301, loss: 1.134, 9632/60000 datapoints
2025-03-07 18:57:13,922 - INFO - training batch 351, loss: 1.082, 11232/60000 datapoints
2025-03-07 18:57:14,225 - INFO - training batch 401, loss: 1.272, 12832/60000 datapoints
2025-03-07 18:57:14,527 - INFO - training batch 451, loss: 1.331, 14432/60000 datapoints
2025-03-07 18:57:14,832 - INFO - training batch 501, loss: 1.067, 16032/60000 datapoints
2025-03-07 18:57:15,139 - INFO - training batch 551, loss: 0.907, 17632/60000 datapoints
2025-03-07 18:57:15,438 - INFO - training batch 601, loss: 0.850, 19232/60000 datapoints
2025-03-07 18:57:15,732 - INFO - training batch 651, loss: 0.991, 20832/60000 datapoints
2025-03-07 18:57:16,029 - INFO - training batch 701, loss: 1.005, 22432/60000 datapoints
2025-03-07 18:57:16,321 - INFO - training batch 751, loss: 1.187, 24032/60000 datapoints
2025-03-07 18:57:16,602 - INFO - training batch 801, loss: 0.859, 25632/60000 datapoints
2025-03-07 18:57:16,899 - INFO - training batch 851, loss: 0.994, 27232/60000 datapoints
2025-03-07 18:57:17,187 - INFO - training batch 901, loss: 0.954, 28832/60000 datapoints
2025-03-07 18:57:17,466 - INFO - training batch 951, loss: 0.945, 30432/60000 datapoints
2025-03-07 18:57:17,740 - INFO - training batch 1001, loss: 0.996, 32032/60000 datapoints
2025-03-07 18:57:18,088 - INFO - training batch 1051, loss: 0.919, 33632/60000 datapoints
2025-03-07 18:57:18,402 - INFO - training batch 1101, loss: 1.009, 35232/60000 datapoints
2025-03-07 18:57:18,770 - INFO - training batch 1151, loss: 1.071, 36832/60000 datapoints
2025-03-07 18:57:19,150 - INFO - training batch 1201, loss: 0.943, 38432/60000 datapoints
2025-03-07 18:57:19,564 - INFO - training batch 1251, loss: 0.859, 40032/60000 datapoints
2025-03-07 18:57:20,141 - INFO - training batch 1301, loss: 1.020, 41632/60000 datapoints
2025-03-07 18:57:20,519 - INFO - training batch 1351, loss: 0.946, 43232/60000 datapoints
2025-03-07 18:57:20,844 - INFO - training batch 1401, loss: 1.038, 44832/60000 datapoints
2025-03-07 18:57:21,197 - INFO - training batch 1451, loss: 0.913, 46432/60000 datapoints
2025-03-07 18:57:21,529 - INFO - training batch 1501, loss: 0.760, 48032/60000 datapoints
2025-03-07 18:57:21,920 - INFO - training batch 1551, loss: 0.953, 49632/60000 datapoints
2025-03-07 18:57:22,279 - INFO - training batch 1601, loss: 1.276, 51232/60000 datapoints
2025-03-07 18:57:22,561 - INFO - training batch 1651, loss: 0.901, 52832/60000 datapoints
2025-03-07 18:57:22,883 - INFO - training batch 1701, loss: 1.052, 54432/60000 datapoints
2025-03-07 18:57:23,215 - INFO - training batch 1751, loss: 0.886, 56032/60000 datapoints
2025-03-07 18:57:23,594 - INFO - training batch 1801, loss: 1.058, 57632/60000 datapoints
2025-03-07 18:57:24,029 - INFO - training batch 1851, loss: 0.960, 59232/60000 datapoints
2025-03-07 18:57:24,225 - INFO - validation batch 1, loss: 1.128, 32/10016 datapoints
2025-03-07 18:57:24,515 - INFO - validation batch 51, loss: 0.814, 1632/10016 datapoints
2025-03-07 18:57:24,825 - INFO - validation batch 101, loss: 1.049, 3232/10016 datapoints
2025-03-07 18:57:25,115 - INFO - validation batch 151, loss: 1.052, 4832/10016 datapoints
2025-03-07 18:57:25,373 - INFO - validation batch 201, loss: 1.009, 6432/10016 datapoints
2025-03-07 18:57:25,629 - INFO - validation batch 251, loss: 0.892, 8032/10016 datapoints
2025-03-07 18:57:25,889 - INFO - validation batch 301, loss: 0.941, 9632/10016 datapoints
2025-03-07 18:57:25,958 - INFO - Epoch 51/800 done.
2025-03-07 18:57:25,959 - INFO - Final validation performance:
Loss: 0.984, top-1 acc: 0.796top-5 acc: 0.796
2025-03-07 18:57:25,960 - INFO - Beginning epoch 52/800
2025-03-07 18:57:25,970 - INFO - training batch 1, loss: 0.907, 32/60000 datapoints
2025-03-07 18:57:26,291 - INFO - training batch 51, loss: 1.109, 1632/60000 datapoints
2025-03-07 18:57:26,582 - INFO - training batch 101, loss: 0.918, 3232/60000 datapoints
2025-03-07 18:57:26,883 - INFO - training batch 151, loss: 0.900, 4832/60000 datapoints
2025-03-07 18:57:27,206 - INFO - training batch 201, loss: 1.119, 6432/60000 datapoints
2025-03-07 18:57:27,513 - INFO - training batch 251, loss: 1.150, 8032/60000 datapoints
2025-03-07 18:57:27,804 - INFO - training batch 301, loss: 0.902, 9632/60000 datapoints
2025-03-07 18:57:28,101 - INFO - training batch 351, loss: 0.887, 11232/60000 datapoints
2025-03-07 18:57:28,419 - INFO - training batch 401, loss: 0.912, 12832/60000 datapoints
2025-03-07 18:57:28,730 - INFO - training batch 451, loss: 0.932, 14432/60000 datapoints
2025-03-07 18:57:29,046 - INFO - training batch 501, loss: 1.111, 16032/60000 datapoints
2025-03-07 18:57:29,333 - INFO - training batch 551, loss: 1.064, 17632/60000 datapoints
2025-03-07 18:57:29,625 - INFO - training batch 601, loss: 0.873, 19232/60000 datapoints
2025-03-07 18:57:29,925 - INFO - training batch 651, loss: 0.896, 20832/60000 datapoints
2025-03-07 18:57:30,223 - INFO - training batch 701, loss: 0.882, 22432/60000 datapoints
2025-03-07 18:57:30,509 - INFO - training batch 751, loss: 0.962, 24032/60000 datapoints
2025-03-07 18:57:30,801 - INFO - training batch 801, loss: 0.899, 25632/60000 datapoints
2025-03-07 18:57:31,297 - INFO - training batch 851, loss: 0.924, 27232/60000 datapoints
2025-03-07 18:57:31,694 - INFO - training batch 901, loss: 0.983, 28832/60000 datapoints
2025-03-07 18:57:32,046 - INFO - training batch 951, loss: 0.829, 30432/60000 datapoints
2025-03-07 18:57:32,358 - INFO - training batch 1001, loss: 0.958, 32032/60000 datapoints
2025-03-07 18:57:32,654 - INFO - training batch 1051, loss: 1.069, 33632/60000 datapoints
2025-03-07 18:57:32,943 - INFO - training batch 1101, loss: 0.864, 35232/60000 datapoints
2025-03-07 18:57:33,227 - INFO - training batch 1151, loss: 0.945, 36832/60000 datapoints
2025-03-07 18:57:33,513 - INFO - training batch 1201, loss: 0.936, 38432/60000 datapoints
2025-03-07 18:57:33,805 - INFO - training batch 1251, loss: 1.072, 40032/60000 datapoints
2025-03-07 18:57:34,095 - INFO - training batch 1301, loss: 0.954, 41632/60000 datapoints
2025-03-07 18:57:34,390 - INFO - training batch 1351, loss: 1.359, 43232/60000 datapoints
2025-03-07 18:57:34,677 - INFO - training batch 1401, loss: 1.089, 44832/60000 datapoints
2025-03-07 18:57:34,971 - INFO - training batch 1451, loss: 0.975, 46432/60000 datapoints
2025-03-07 18:57:35,277 - INFO - training batch 1501, loss: 1.034, 48032/60000 datapoints
2025-03-07 18:57:35,565 - INFO - training batch 1551, loss: 1.113, 49632/60000 datapoints
2025-03-07 18:57:35,854 - INFO - training batch 1601, loss: 1.053, 51232/60000 datapoints
2025-03-07 18:57:36,185 - INFO - training batch 1651, loss: 0.920, 52832/60000 datapoints
2025-03-07 18:57:36,483 - INFO - training batch 1701, loss: 0.974, 54432/60000 datapoints
2025-03-07 18:57:36,767 - INFO - training batch 1751, loss: 1.036, 56032/60000 datapoints
2025-03-07 18:57:37,063 - INFO - training batch 1801, loss: 1.149, 57632/60000 datapoints
2025-03-07 18:57:37,362 - INFO - training batch 1851, loss: 1.110, 59232/60000 datapoints
2025-03-07 18:57:37,527 - INFO - validation batch 1, loss: 1.096, 32/10016 datapoints
2025-03-07 18:57:37,766 - INFO - validation batch 51, loss: 1.047, 1632/10016 datapoints
2025-03-07 18:57:38,016 - INFO - validation batch 101, loss: 0.912, 3232/10016 datapoints
2025-03-07 18:57:38,257 - INFO - validation batch 151, loss: 1.120, 4832/10016 datapoints
2025-03-07 18:57:38,485 - INFO - validation batch 201, loss: 0.980, 6432/10016 datapoints
2025-03-07 18:57:38,744 - INFO - validation batch 251, loss: 0.845, 8032/10016 datapoints
2025-03-07 18:57:38,984 - INFO - validation batch 301, loss: 0.966, 9632/10016 datapoints
2025-03-07 18:57:39,049 - INFO - Epoch 52/800 done.
2025-03-07 18:57:39,049 - INFO - Final validation performance:
Loss: 0.995, top-1 acc: 0.799top-5 acc: 0.799
2025-03-07 18:57:39,050 - INFO - Beginning epoch 53/800
2025-03-07 18:57:39,061 - INFO - training batch 1, loss: 0.979, 32/60000 datapoints
2025-03-07 18:57:39,382 - INFO - training batch 51, loss: 0.836, 1632/60000 datapoints
2025-03-07 18:57:39,698 - INFO - training batch 101, loss: 0.967, 3232/60000 datapoints
2025-03-07 18:57:40,003 - INFO - training batch 151, loss: 1.081, 4832/60000 datapoints
2025-03-07 18:57:40,305 - INFO - training batch 201, loss: 0.872, 6432/60000 datapoints
2025-03-07 18:57:40,626 - INFO - training batch 251, loss: 0.985, 8032/60000 datapoints
2025-03-07 18:57:40,918 - INFO - training batch 301, loss: 0.843, 9632/60000 datapoints
2025-03-07 18:57:41,205 - INFO - training batch 351, loss: 1.133, 11232/60000 datapoints
2025-03-07 18:57:41,485 - INFO - training batch 401, loss: 0.991, 12832/60000 datapoints
2025-03-07 18:57:41,777 - INFO - training batch 451, loss: 0.908, 14432/60000 datapoints
2025-03-07 18:57:42,069 - INFO - training batch 501, loss: 0.979, 16032/60000 datapoints
2025-03-07 18:57:42,378 - INFO - training batch 551, loss: 1.245, 17632/60000 datapoints
2025-03-07 18:57:42,662 - INFO - training batch 601, loss: 0.993, 19232/60000 datapoints
2025-03-07 18:57:42,961 - INFO - training batch 651, loss: 1.010, 20832/60000 datapoints
2025-03-07 18:57:43,248 - INFO - training batch 701, loss: 0.951, 22432/60000 datapoints
2025-03-07 18:57:43,530 - INFO - training batch 751, loss: 1.027, 24032/60000 datapoints
2025-03-07 18:57:43,853 - INFO - training batch 801, loss: 0.925, 25632/60000 datapoints
2025-03-07 18:57:44,148 - INFO - training batch 851, loss: 0.894, 27232/60000 datapoints
2025-03-07 18:57:44,429 - INFO - training batch 901, loss: 0.995, 28832/60000 datapoints
2025-03-07 18:57:44,717 - INFO - training batch 951, loss: 1.003, 30432/60000 datapoints
2025-03-07 18:57:45,021 - INFO - training batch 1001, loss: 0.907, 32032/60000 datapoints
2025-03-07 18:57:45,300 - INFO - training batch 1051, loss: 0.989, 33632/60000 datapoints
2025-03-07 18:57:45,587 - INFO - training batch 1101, loss: 0.816, 35232/60000 datapoints
2025-03-07 18:57:45,914 - INFO - training batch 1151, loss: 0.922, 36832/60000 datapoints
2025-03-07 18:57:46,237 - INFO - training batch 1201, loss: 1.041, 38432/60000 datapoints
2025-03-07 18:57:46,528 - INFO - training batch 1251, loss: 0.963, 40032/60000 datapoints
2025-03-07 18:57:46,821 - INFO - training batch 1301, loss: 0.970, 41632/60000 datapoints
2025-03-07 18:57:47,153 - INFO - training batch 1351, loss: 0.879, 43232/60000 datapoints
2025-03-07 18:57:47,434 - INFO - training batch 1401, loss: 0.880, 44832/60000 datapoints
2025-03-07 18:57:47,718 - INFO - training batch 1451, loss: 0.807, 46432/60000 datapoints
2025-03-07 18:57:48,013 - INFO - training batch 1501, loss: 1.054, 48032/60000 datapoints
2025-03-07 18:57:48,306 - INFO - training batch 1551, loss: 0.867, 49632/60000 datapoints
2025-03-07 18:57:48,588 - INFO - training batch 1601, loss: 1.007, 51232/60000 datapoints
2025-03-07 18:57:48,883 - INFO - training batch 1651, loss: 0.806, 52832/60000 datapoints
2025-03-07 18:57:51,608 - INFO - training batch 1701, loss: 0.974, 54432/60000 datapoints
2025-03-07 18:57:54,073 - INFO - training batch 1751, loss: 1.059, 56032/60000 datapoints
2025-03-07 18:57:55,155 - INFO - training batch 1801, loss: 0.943, 57632/60000 datapoints
2025-03-07 18:57:56,719 - INFO - training batch 1851, loss: 1.309, 59232/60000 datapoints
2025-03-07 18:57:57,137 - INFO - validation batch 1, loss: 0.846, 32/10016 datapoints
2025-03-07 18:57:57,572 - INFO - validation batch 51, loss: 1.052, 1632/10016 datapoints
2025-03-07 18:57:57,979 - INFO - validation batch 101, loss: 0.751, 3232/10016 datapoints
2025-03-07 18:57:58,405 - INFO - validation batch 151, loss: 1.240, 4832/10016 datapoints
2025-03-07 18:57:58,753 - INFO - validation batch 201, loss: 0.873, 6432/10016 datapoints
2025-03-07 18:57:59,143 - INFO - validation batch 251, loss: 1.117, 8032/10016 datapoints
2025-03-07 18:58:02,358 - INFO - validation batch 301, loss: 0.947, 9632/10016 datapoints
2025-03-07 18:58:03,506 - INFO - Epoch 53/800 done.
2025-03-07 18:58:03,511 - INFO - Final validation performance:
Loss: 0.975, top-1 acc: 0.800top-5 acc: 0.800
2025-03-07 18:58:03,545 - INFO - Beginning epoch 54/800
2025-03-07 18:58:03,721 - INFO - training batch 1, loss: 0.859, 32/60000 datapoints
2025-03-07 18:58:07,633 - INFO - training batch 51, loss: 1.025, 1632/60000 datapoints
2025-03-07 18:58:10,752 - INFO - training batch 101, loss: 1.142, 3232/60000 datapoints
2025-03-07 18:58:11,636 - INFO - training batch 151, loss: 0.809, 4832/60000 datapoints
2025-03-07 18:58:12,647 - INFO - training batch 201, loss: 0.781, 6432/60000 datapoints
2025-03-07 18:58:13,203 - INFO - training batch 251, loss: 0.841, 8032/60000 datapoints
2025-03-07 18:58:13,621 - INFO - training batch 301, loss: 0.953, 9632/60000 datapoints
2025-03-07 18:58:14,080 - INFO - training batch 351, loss: 0.907, 11232/60000 datapoints
2025-03-07 18:58:14,699 - INFO - training batch 401, loss: 0.859, 12832/60000 datapoints
2025-03-07 18:58:15,213 - INFO - training batch 451, loss: 1.075, 14432/60000 datapoints
2025-03-07 18:58:15,745 - INFO - training batch 501, loss: 1.157, 16032/60000 datapoints
2025-03-07 18:58:16,281 - INFO - training batch 551, loss: 1.075, 17632/60000 datapoints
2025-03-07 18:58:16,695 - INFO - training batch 601, loss: 1.015, 19232/60000 datapoints
2025-03-07 18:58:17,083 - INFO - training batch 651, loss: 1.087, 20832/60000 datapoints
2025-03-07 18:58:17,440 - INFO - training batch 701, loss: 1.266, 22432/60000 datapoints
2025-03-07 18:58:17,780 - INFO - training batch 751, loss: 1.098, 24032/60000 datapoints
2025-03-07 18:58:18,264 - INFO - training batch 801, loss: 1.090, 25632/60000 datapoints
2025-03-07 18:58:18,703 - INFO - training batch 851, loss: 0.929, 27232/60000 datapoints
2025-03-07 18:58:19,210 - INFO - training batch 901, loss: 0.900, 28832/60000 datapoints
2025-03-07 18:58:20,049 - INFO - training batch 951, loss: 0.803, 30432/60000 datapoints
2025-03-07 18:58:20,557 - INFO - training batch 1001, loss: 0.811, 32032/60000 datapoints
2025-03-07 18:58:20,886 - INFO - training batch 1051, loss: 1.182, 33632/60000 datapoints
2025-03-07 18:58:21,236 - INFO - training batch 1101, loss: 0.930, 35232/60000 datapoints
2025-03-07 18:58:21,611 - INFO - training batch 1151, loss: 0.883, 36832/60000 datapoints
2025-03-07 18:58:22,025 - INFO - training batch 1201, loss: 1.225, 38432/60000 datapoints
2025-03-07 18:58:22,483 - INFO - training batch 1251, loss: 0.803, 40032/60000 datapoints
2025-03-07 18:58:22,909 - INFO - training batch 1301, loss: 1.010, 41632/60000 datapoints
2025-03-07 18:58:23,302 - INFO - training batch 1351, loss: 1.099, 43232/60000 datapoints
2025-03-07 18:58:23,708 - INFO - training batch 1401, loss: 0.893, 44832/60000 datapoints
2025-03-07 18:58:24,072 - INFO - training batch 1451, loss: 1.017, 46432/60000 datapoints
2025-03-07 18:58:24,445 - INFO - training batch 1501, loss: 0.926, 48032/60000 datapoints
2025-03-07 18:58:24,785 - INFO - training batch 1551, loss: 0.667, 49632/60000 datapoints
2025-03-07 18:58:25,091 - INFO - training batch 1601, loss: 0.722, 51232/60000 datapoints
2025-03-07 18:58:25,395 - INFO - training batch 1651, loss: 0.885, 52832/60000 datapoints
2025-03-07 18:58:25,697 - INFO - training batch 1701, loss: 0.876, 54432/60000 datapoints
2025-03-07 18:58:26,063 - INFO - training batch 1751, loss: 0.971, 56032/60000 datapoints
2025-03-07 18:58:26,359 - INFO - training batch 1801, loss: 0.858, 57632/60000 datapoints
2025-03-07 18:58:26,658 - INFO - training batch 1851, loss: 0.934, 59232/60000 datapoints
2025-03-07 18:58:27,105 - INFO - validation batch 1, loss: 0.817, 32/10016 datapoints
2025-03-07 18:58:27,469 - INFO - validation batch 51, loss: 0.922, 1632/10016 datapoints
2025-03-07 18:58:27,724 - INFO - validation batch 101, loss: 1.006, 3232/10016 datapoints
2025-03-07 18:58:28,003 - INFO - validation batch 151, loss: 0.908, 4832/10016 datapoints
2025-03-07 18:58:28,238 - INFO - validation batch 201, loss: 1.167, 6432/10016 datapoints
2025-03-07 18:58:28,519 - INFO - validation batch 251, loss: 0.818, 8032/10016 datapoints
2025-03-07 18:58:28,940 - INFO - validation batch 301, loss: 0.869, 9632/10016 datapoints
2025-03-07 18:58:29,015 - INFO - Epoch 54/800 done.
2025-03-07 18:58:29,016 - INFO - Final validation performance:
Loss: 0.930, top-1 acc: 0.802top-5 acc: 0.802
2025-03-07 18:58:29,019 - INFO - Beginning epoch 55/800
2025-03-07 18:58:29,032 - INFO - training batch 1, loss: 0.901, 32/60000 datapoints
2025-03-07 18:58:29,395 - INFO - training batch 51, loss: 0.965, 1632/60000 datapoints
2025-03-07 18:58:29,847 - INFO - training batch 101, loss: 1.119, 3232/60000 datapoints
2025-03-07 18:58:30,324 - INFO - training batch 151, loss: 0.755, 4832/60000 datapoints
2025-03-07 18:58:31,350 - INFO - training batch 201, loss: 0.747, 6432/60000 datapoints
2025-03-07 18:58:32,008 - INFO - training batch 251, loss: 0.753, 8032/60000 datapoints
2025-03-07 18:58:32,835 - INFO - training batch 301, loss: 0.766, 9632/60000 datapoints
2025-03-07 18:58:33,531 - INFO - training batch 351, loss: 0.986, 11232/60000 datapoints
2025-03-07 18:58:34,020 - INFO - training batch 401, loss: 1.026, 12832/60000 datapoints
2025-03-07 18:58:34,401 - INFO - training batch 451, loss: 0.873, 14432/60000 datapoints
2025-03-07 18:58:34,770 - INFO - training batch 501, loss: 1.046, 16032/60000 datapoints
2025-03-07 18:58:35,255 - INFO - training batch 551, loss: 0.911, 17632/60000 datapoints
2025-03-07 18:58:36,061 - INFO - training batch 601, loss: 0.829, 19232/60000 datapoints
2025-03-07 18:58:38,020 - INFO - training batch 651, loss: 0.946, 20832/60000 datapoints
2025-03-07 18:58:38,580 - INFO - training batch 701, loss: 0.857, 22432/60000 datapoints
2025-03-07 18:58:39,039 - INFO - training batch 751, loss: 0.939, 24032/60000 datapoints
2025-03-07 18:58:39,421 - INFO - training batch 801, loss: 1.064, 25632/60000 datapoints
2025-03-07 18:58:39,776 - INFO - training batch 851, loss: 0.882, 27232/60000 datapoints
2025-03-07 18:58:40,109 - INFO - training batch 901, loss: 0.934, 28832/60000 datapoints
2025-03-07 18:58:40,413 - INFO - training batch 951, loss: 0.892, 30432/60000 datapoints
2025-03-07 18:58:40,726 - INFO - training batch 1001, loss: 0.795, 32032/60000 datapoints
2025-03-07 18:58:41,105 - INFO - training batch 1051, loss: 1.036, 33632/60000 datapoints
2025-03-07 18:58:41,435 - INFO - training batch 1101, loss: 1.071, 35232/60000 datapoints
2025-03-07 18:58:42,274 - INFO - training batch 1151, loss: 1.156, 36832/60000 datapoints
2025-03-07 18:58:42,686 - INFO - training batch 1201, loss: 0.912, 38432/60000 datapoints
2025-03-07 18:58:43,131 - INFO - training batch 1251, loss: 0.944, 40032/60000 datapoints
2025-03-07 18:58:43,648 - INFO - training batch 1301, loss: 0.921, 41632/60000 datapoints
2025-03-07 18:58:44,144 - INFO - training batch 1351, loss: 0.900, 43232/60000 datapoints
2025-03-07 18:58:45,162 - INFO - training batch 1401, loss: 1.062, 44832/60000 datapoints
2025-03-07 18:58:45,634 - INFO - training batch 1451, loss: 0.779, 46432/60000 datapoints
2025-03-07 18:58:45,988 - INFO - training batch 1501, loss: 1.083, 48032/60000 datapoints
2025-03-07 18:58:46,322 - INFO - training batch 1551, loss: 0.962, 49632/60000 datapoints
2025-03-07 18:58:46,655 - INFO - training batch 1601, loss: 0.869, 51232/60000 datapoints
2025-03-07 18:58:46,947 - INFO - training batch 1651, loss: 0.827, 52832/60000 datapoints
2025-03-07 18:58:47,398 - INFO - training batch 1701, loss: 0.962, 54432/60000 datapoints
2025-03-07 18:58:47,823 - INFO - training batch 1751, loss: 0.808, 56032/60000 datapoints
2025-03-07 18:58:48,185 - INFO - training batch 1801, loss: 0.833, 57632/60000 datapoints
2025-03-07 18:58:48,643 - INFO - training batch 1851, loss: 1.016, 59232/60000 datapoints
2025-03-07 18:58:48,844 - INFO - validation batch 1, loss: 0.732, 32/10016 datapoints
2025-03-07 18:58:49,188 - INFO - validation batch 51, loss: 0.795, 1632/10016 datapoints
2025-03-07 18:58:49,760 - INFO - validation batch 101, loss: 1.019, 3232/10016 datapoints
2025-03-07 18:58:50,337 - INFO - validation batch 151, loss: 1.027, 4832/10016 datapoints
2025-03-07 18:58:50,898 - INFO - validation batch 201, loss: 0.875, 6432/10016 datapoints
2025-03-07 18:58:51,305 - INFO - validation batch 251, loss: 0.979, 8032/10016 datapoints
2025-03-07 18:58:51,701 - INFO - validation batch 301, loss: 0.955, 9632/10016 datapoints
2025-03-07 18:58:51,801 - INFO - Epoch 55/800 done.
2025-03-07 18:58:51,802 - INFO - Final validation performance:
Loss: 0.912, top-1 acc: 0.803top-5 acc: 0.803
2025-03-07 18:58:51,805 - INFO - Beginning epoch 56/800
2025-03-07 18:58:51,822 - INFO - training batch 1, loss: 0.890, 32/60000 datapoints
2025-03-07 18:58:52,836 - INFO - training batch 51, loss: 0.924, 1632/60000 datapoints
2025-03-07 18:58:54,110 - INFO - training batch 101, loss: 0.885, 3232/60000 datapoints
2025-03-07 18:58:54,601 - INFO - training batch 151, loss: 0.928, 4832/60000 datapoints
2025-03-07 18:58:55,017 - INFO - training batch 201, loss: 0.897, 6432/60000 datapoints
2025-03-07 18:58:55,392 - INFO - training batch 251, loss: 0.940, 8032/60000 datapoints
2025-03-07 18:58:55,835 - INFO - training batch 301, loss: 0.778, 9632/60000 datapoints
2025-03-07 18:58:56,303 - INFO - training batch 351, loss: 1.094, 11232/60000 datapoints
2025-03-07 18:58:56,653 - INFO - training batch 401, loss: 0.778, 12832/60000 datapoints
2025-03-07 18:58:57,003 - INFO - training batch 451, loss: 0.878, 14432/60000 datapoints
2025-03-07 18:58:57,346 - INFO - training batch 501, loss: 0.994, 16032/60000 datapoints
2025-03-07 18:58:57,640 - INFO - training batch 551, loss: 0.855, 17632/60000 datapoints
2025-03-07 18:58:57,934 - INFO - training batch 601, loss: 0.979, 19232/60000 datapoints
2025-03-07 18:58:58,245 - INFO - training batch 651, loss: 1.014, 20832/60000 datapoints
2025-03-07 18:58:58,529 - INFO - training batch 701, loss: 0.813, 22432/60000 datapoints
2025-03-07 18:58:58,850 - INFO - training batch 751, loss: 1.029, 24032/60000 datapoints
2025-03-07 18:58:59,173 - INFO - training batch 801, loss: 1.045, 25632/60000 datapoints
2025-03-07 18:58:59,452 - INFO - training batch 851, loss: 1.063, 27232/60000 datapoints
2025-03-07 18:58:59,759 - INFO - training batch 901, loss: 0.866, 28832/60000 datapoints
2025-03-07 18:59:00,054 - INFO - training batch 951, loss: 0.901, 30432/60000 datapoints
2025-03-07 18:59:00,336 - INFO - training batch 1001, loss: 0.793, 32032/60000 datapoints
2025-03-07 18:59:00,650 - INFO - training batch 1051, loss: 0.810, 33632/60000 datapoints
2025-03-07 18:59:00,927 - INFO - training batch 1101, loss: 0.920, 35232/60000 datapoints
2025-03-07 18:59:01,220 - INFO - training batch 1151, loss: 0.764, 36832/60000 datapoints
2025-03-07 18:59:01,583 - INFO - training batch 1201, loss: 0.883, 38432/60000 datapoints
2025-03-07 18:59:02,017 - INFO - training batch 1251, loss: 1.037, 40032/60000 datapoints
2025-03-07 18:59:02,341 - INFO - training batch 1301, loss: 0.972, 41632/60000 datapoints
2025-03-07 18:59:02,717 - INFO - training batch 1351, loss: 1.039, 43232/60000 datapoints
2025-03-07 18:59:03,045 - INFO - training batch 1401, loss: 0.694, 44832/60000 datapoints
2025-03-07 18:59:03,359 - INFO - training batch 1451, loss: 0.927, 46432/60000 datapoints
2025-03-07 18:59:03,680 - INFO - training batch 1501, loss: 0.730, 48032/60000 datapoints
2025-03-07 18:59:03,964 - INFO - training batch 1551, loss: 0.884, 49632/60000 datapoints
2025-03-07 18:59:04,252 - INFO - training batch 1601, loss: 0.917, 51232/60000 datapoints
2025-03-07 18:59:04,548 - INFO - training batch 1651, loss: 0.963, 52832/60000 datapoints
2025-03-07 18:59:04,857 - INFO - training batch 1701, loss: 0.906, 54432/60000 datapoints
2025-03-07 18:59:05,243 - INFO - training batch 1751, loss: 1.025, 56032/60000 datapoints
2025-03-07 18:59:05,551 - INFO - training batch 1801, loss: 0.701, 57632/60000 datapoints
2025-03-07 18:59:05,902 - INFO - training batch 1851, loss: 0.982, 59232/60000 datapoints
2025-03-07 18:59:06,166 - INFO - validation batch 1, loss: 0.966, 32/10016 datapoints
2025-03-07 18:59:06,504 - INFO - validation batch 51, loss: 0.975, 1632/10016 datapoints
2025-03-07 18:59:07,029 - INFO - validation batch 101, loss: 0.813, 3232/10016 datapoints
2025-03-07 18:59:07,663 - INFO - validation batch 151, loss: 1.180, 4832/10016 datapoints
2025-03-07 18:59:08,296 - INFO - validation batch 201, loss: 0.960, 6432/10016 datapoints
2025-03-07 18:59:08,909 - INFO - validation batch 251, loss: 1.030, 8032/10016 datapoints
2025-03-07 18:59:09,692 - INFO - validation batch 301, loss: 1.127, 9632/10016 datapoints
2025-03-07 18:59:09,991 - INFO - Epoch 56/800 done.
2025-03-07 18:59:10,003 - INFO - Final validation performance:
Loss: 1.007, top-1 acc: 0.805top-5 acc: 0.805
2025-03-07 18:59:10,020 - INFO - Beginning epoch 57/800
2025-03-07 18:59:10,066 - INFO - training batch 1, loss: 0.971, 32/60000 datapoints
2025-03-07 18:59:11,104 - INFO - training batch 51, loss: 0.898, 1632/60000 datapoints
2025-03-07 18:59:11,604 - INFO - training batch 101, loss: 0.794, 3232/60000 datapoints
2025-03-07 18:59:12,127 - INFO - training batch 151, loss: 0.887, 4832/60000 datapoints
2025-03-07 18:59:13,061 - INFO - training batch 201, loss: 0.998, 6432/60000 datapoints
2025-03-07 18:59:13,925 - INFO - training batch 251, loss: 0.783, 8032/60000 datapoints
2025-03-07 18:59:14,899 - INFO - training batch 301, loss: 0.773, 9632/60000 datapoints
2025-03-07 18:59:15,384 - INFO - training batch 351, loss: 1.087, 11232/60000 datapoints
2025-03-07 18:59:15,888 - INFO - training batch 401, loss: 0.861, 12832/60000 datapoints
2025-03-07 18:59:16,482 - INFO - training batch 451, loss: 1.065, 14432/60000 datapoints
2025-03-07 18:59:16,951 - INFO - training batch 501, loss: 0.831, 16032/60000 datapoints
2025-03-07 18:59:17,617 - INFO - training batch 551, loss: 0.855, 17632/60000 datapoints
2025-03-07 18:59:17,943 - INFO - training batch 601, loss: 0.754, 19232/60000 datapoints
2025-03-07 18:59:18,252 - INFO - training batch 651, loss: 0.736, 20832/60000 datapoints
2025-03-07 18:59:18,552 - INFO - training batch 701, loss: 1.078, 22432/60000 datapoints
2025-03-07 18:59:18,843 - INFO - training batch 751, loss: 0.757, 24032/60000 datapoints
2025-03-07 18:59:19,145 - INFO - training batch 801, loss: 0.860, 25632/60000 datapoints
2025-03-07 18:59:19,454 - INFO - training batch 851, loss: 0.757, 27232/60000 datapoints
2025-03-07 18:59:19,753 - INFO - training batch 901, loss: 0.820, 28832/60000 datapoints
2025-03-07 18:59:20,048 - INFO - training batch 951, loss: 1.046, 30432/60000 datapoints
2025-03-07 18:59:20,334 - INFO - training batch 1001, loss: 0.916, 32032/60000 datapoints
2025-03-07 18:59:20,627 - INFO - training batch 1051, loss: 0.677, 33632/60000 datapoints
2025-03-07 18:59:20,909 - INFO - training batch 1101, loss: 0.796, 35232/60000 datapoints
2025-03-07 18:59:21,205 - INFO - training batch 1151, loss: 0.573, 36832/60000 datapoints
2025-03-07 18:59:21,527 - INFO - training batch 1201, loss: 0.836, 38432/60000 datapoints
2025-03-07 18:59:21,871 - INFO - training batch 1251, loss: 0.878, 40032/60000 datapoints
2025-03-07 18:59:22,246 - INFO - training batch 1301, loss: 0.851, 41632/60000 datapoints
2025-03-07 18:59:22,525 - INFO - training batch 1351, loss: 1.083, 43232/60000 datapoints
2025-03-07 18:59:22,805 - INFO - training batch 1401, loss: 0.913, 44832/60000 datapoints
2025-03-07 18:59:23,106 - INFO - training batch 1451, loss: 1.058, 46432/60000 datapoints
2025-03-07 18:59:23,387 - INFO - training batch 1501, loss: 1.128, 48032/60000 datapoints
2025-03-07 18:59:23,667 - INFO - training batch 1551, loss: 0.801, 49632/60000 datapoints
2025-03-07 18:59:23,976 - INFO - training batch 1601, loss: 0.871, 51232/60000 datapoints
2025-03-07 18:59:24,273 - INFO - training batch 1651, loss: 0.839, 52832/60000 datapoints
2025-03-07 18:59:24,546 - INFO - training batch 1701, loss: 0.803, 54432/60000 datapoints
2025-03-07 18:59:24,822 - INFO - training batch 1751, loss: 0.711, 56032/60000 datapoints
2025-03-07 18:59:25,106 - INFO - training batch 1801, loss: 0.860, 57632/60000 datapoints
2025-03-07 18:59:25,389 - INFO - training batch 1851, loss: 0.752, 59232/60000 datapoints
2025-03-07 18:59:25,527 - INFO - validation batch 1, loss: 1.065, 32/10016 datapoints
2025-03-07 18:59:25,755 - INFO - validation batch 51, loss: 1.020, 1632/10016 datapoints
2025-03-07 18:59:25,996 - INFO - validation batch 101, loss: 1.008, 3232/10016 datapoints
2025-03-07 18:59:26,230 - INFO - validation batch 151, loss: 0.936, 4832/10016 datapoints
2025-03-07 18:59:26,453 - INFO - validation batch 201, loss: 0.840, 6432/10016 datapoints
2025-03-07 18:59:26,685 - INFO - validation batch 251, loss: 0.766, 8032/10016 datapoints
2025-03-07 18:59:26,908 - INFO - validation batch 301, loss: 0.972, 9632/10016 datapoints
2025-03-07 18:59:26,959 - INFO - Epoch 57/800 done.
2025-03-07 18:59:26,960 - INFO - Final validation performance:
Loss: 0.944, top-1 acc: 0.807top-5 acc: 0.807
2025-03-07 18:59:26,961 - INFO - Beginning epoch 58/800
2025-03-07 18:59:26,970 - INFO - training batch 1, loss: 0.980, 32/60000 datapoints
2025-03-07 18:59:27,360 - INFO - training batch 51, loss: 1.143, 1632/60000 datapoints
2025-03-07 18:59:27,667 - INFO - training batch 101, loss: 0.998, 3232/60000 datapoints
2025-03-07 18:59:27,950 - INFO - training batch 151, loss: 1.070, 4832/60000 datapoints
2025-03-07 18:59:28,242 - INFO - training batch 201, loss: 1.052, 6432/60000 datapoints
2025-03-07 18:59:28,513 - INFO - training batch 251, loss: 0.848, 8032/60000 datapoints
2025-03-07 18:59:28,827 - INFO - training batch 301, loss: 0.764, 9632/60000 datapoints
2025-03-07 18:59:29,145 - INFO - training batch 351, loss: 0.848, 11232/60000 datapoints
2025-03-07 18:59:29,423 - INFO - training batch 401, loss: 0.857, 12832/60000 datapoints
2025-03-07 18:59:29,706 - INFO - training batch 451, loss: 0.941, 14432/60000 datapoints
2025-03-07 18:59:29,996 - INFO - training batch 501, loss: 0.913, 16032/60000 datapoints
2025-03-07 18:59:30,288 - INFO - training batch 551, loss: 0.786, 17632/60000 datapoints
2025-03-07 18:59:30,579 - INFO - training batch 601, loss: 0.612, 19232/60000 datapoints
2025-03-07 18:59:30,861 - INFO - training batch 651, loss: 0.729, 20832/60000 datapoints
2025-03-07 18:59:31,238 - INFO - training batch 701, loss: 1.099, 22432/60000 datapoints
2025-03-07 18:59:31,516 - INFO - training batch 751, loss: 0.864, 24032/60000 datapoints
2025-03-07 18:59:31,804 - INFO - training batch 801, loss: 1.098, 25632/60000 datapoints
2025-03-07 18:59:32,108 - INFO - training batch 851, loss: 0.865, 27232/60000 datapoints
2025-03-07 18:59:32,382 - INFO - training batch 901, loss: 0.839, 28832/60000 datapoints
2025-03-07 18:59:32,665 - INFO - training batch 951, loss: 0.861, 30432/60000 datapoints
2025-03-07 18:59:32,938 - INFO - training batch 1001, loss: 0.801, 32032/60000 datapoints
2025-03-07 18:59:33,221 - INFO - training batch 1051, loss: 0.865, 33632/60000 datapoints
2025-03-07 18:59:33,491 - INFO - training batch 1101, loss: 1.004, 35232/60000 datapoints
2025-03-07 18:59:33,788 - INFO - training batch 1151, loss: 0.978, 36832/60000 datapoints
2025-03-07 18:59:34,100 - INFO - training batch 1201, loss: 0.675, 38432/60000 datapoints
2025-03-07 18:59:34,391 - INFO - training batch 1251, loss: 0.713, 40032/60000 datapoints
2025-03-07 18:59:34,670 - INFO - training batch 1301, loss: 0.823, 41632/60000 datapoints
2025-03-07 18:59:34,953 - INFO - training batch 1351, loss: 0.753, 43232/60000 datapoints
2025-03-07 18:59:35,238 - INFO - training batch 1401, loss: 0.920, 44832/60000 datapoints
2025-03-07 18:59:35,507 - INFO - training batch 1451, loss: 0.877, 46432/60000 datapoints
2025-03-07 18:59:35,785 - INFO - training batch 1501, loss: 0.976, 48032/60000 datapoints
2025-03-07 18:59:36,066 - INFO - training batch 1551, loss: 0.788, 49632/60000 datapoints
2025-03-07 18:59:36,339 - INFO - training batch 1601, loss: 0.880, 51232/60000 datapoints
2025-03-07 18:59:36,620 - INFO - training batch 1651, loss: 0.680, 52832/60000 datapoints
2025-03-07 18:59:36,892 - INFO - training batch 1701, loss: 0.920, 54432/60000 datapoints
2025-03-07 18:59:37,248 - INFO - training batch 1751, loss: 0.937, 56032/60000 datapoints
2025-03-07 18:59:37,529 - INFO - training batch 1801, loss: 0.775, 57632/60000 datapoints
2025-03-07 18:59:37,810 - INFO - training batch 1851, loss: 0.766, 59232/60000 datapoints
2025-03-07 18:59:38,031 - INFO - validation batch 1, loss: 0.847, 32/10016 datapoints
2025-03-07 18:59:38,279 - INFO - validation batch 51, loss: 0.680, 1632/10016 datapoints
2025-03-07 18:59:38,503 - INFO - validation batch 101, loss: 0.741, 3232/10016 datapoints
2025-03-07 18:59:38,734 - INFO - validation batch 151, loss: 0.763, 4832/10016 datapoints
2025-03-07 18:59:38,978 - INFO - validation batch 201, loss: 1.069, 6432/10016 datapoints
2025-03-07 18:59:39,273 - INFO - validation batch 251, loss: 0.653, 8032/10016 datapoints
2025-03-07 18:59:39,567 - INFO - validation batch 301, loss: 0.977, 9632/10016 datapoints
2025-03-07 18:59:39,639 - INFO - Epoch 58/800 done.
2025-03-07 18:59:39,639 - INFO - Final validation performance:
Loss: 0.819, top-1 acc: 0.809top-5 acc: 0.809
2025-03-07 18:59:39,641 - INFO - Beginning epoch 59/800
2025-03-07 18:59:39,651 - INFO - training batch 1, loss: 0.895, 32/60000 datapoints
2025-03-07 18:59:40,022 - INFO - training batch 51, loss: 0.857, 1632/60000 datapoints
2025-03-07 18:59:40,396 - INFO - training batch 101, loss: 0.870, 3232/60000 datapoints
2025-03-07 18:59:40,767 - INFO - training batch 151, loss: 0.836, 4832/60000 datapoints
2025-03-07 18:59:41,128 - INFO - training batch 201, loss: 0.846, 6432/60000 datapoints
2025-03-07 18:59:41,480 - INFO - training batch 251, loss: 0.904, 8032/60000 datapoints
2025-03-07 18:59:41,792 - INFO - training batch 301, loss: 0.776, 9632/60000 datapoints
2025-03-07 18:59:42,115 - INFO - training batch 351, loss: 0.704, 11232/60000 datapoints
2025-03-07 18:59:42,398 - INFO - training batch 401, loss: 0.688, 12832/60000 datapoints
2025-03-07 18:59:42,688 - INFO - training batch 451, loss: 1.014, 14432/60000 datapoints
2025-03-07 18:59:42,974 - INFO - training batch 501, loss: 0.742, 16032/60000 datapoints
2025-03-07 18:59:43,295 - INFO - training batch 551, loss: 0.902, 17632/60000 datapoints
2025-03-07 18:59:43,569 - INFO - training batch 601, loss: 1.024, 19232/60000 datapoints
2025-03-07 18:59:43,952 - INFO - training batch 651, loss: 0.980, 20832/60000 datapoints
2025-03-07 18:59:44,278 - INFO - training batch 701, loss: 1.012, 22432/60000 datapoints
2025-03-07 18:59:44,556 - INFO - training batch 751, loss: 0.721, 24032/60000 datapoints
2025-03-07 18:59:44,837 - INFO - training batch 801, loss: 1.117, 25632/60000 datapoints
2025-03-07 18:59:45,151 - INFO - training batch 851, loss: 0.840, 27232/60000 datapoints
2025-03-07 18:59:45,522 - INFO - training batch 901, loss: 0.912, 28832/60000 datapoints
2025-03-07 18:59:45,830 - INFO - training batch 951, loss: 0.776, 30432/60000 datapoints
2025-03-07 18:59:46,112 - INFO - training batch 1001, loss: 0.998, 32032/60000 datapoints
2025-03-07 18:59:46,383 - INFO - training batch 1051, loss: 0.935, 33632/60000 datapoints
2025-03-07 18:59:46,669 - INFO - training batch 1101, loss: 0.845, 35232/60000 datapoints
2025-03-07 18:59:46,945 - INFO - training batch 1151, loss: 0.722, 36832/60000 datapoints
2025-03-07 18:59:47,221 - INFO - training batch 1201, loss: 0.864, 38432/60000 datapoints
2025-03-07 18:59:47,497 - INFO - training batch 1251, loss: 0.846, 40032/60000 datapoints
2025-03-07 18:59:47,780 - INFO - training batch 1301, loss: 0.915, 41632/60000 datapoints
2025-03-07 18:59:48,059 - INFO - training batch 1351, loss: 0.804, 43232/60000 datapoints
2025-03-07 18:59:48,343 - INFO - training batch 1401, loss: 0.836, 44832/60000 datapoints
2025-03-07 18:59:48,625 - INFO - training batch 1451, loss: 0.770, 46432/60000 datapoints
2025-03-07 18:59:48,922 - INFO - training batch 1501, loss: 0.937, 48032/60000 datapoints
2025-03-07 18:59:49,202 - INFO - training batch 1551, loss: 0.852, 49632/60000 datapoints
2025-03-07 18:59:49,478 - INFO - training batch 1601, loss: 0.907, 51232/60000 datapoints
2025-03-07 18:59:49,764 - INFO - training batch 1651, loss: 0.859, 52832/60000 datapoints
2025-03-07 18:59:50,042 - INFO - training batch 1701, loss: 0.889, 54432/60000 datapoints
2025-03-07 18:59:50,320 - INFO - training batch 1751, loss: 1.036, 56032/60000 datapoints
2025-03-07 18:59:50,592 - INFO - training batch 1801, loss: 0.825, 57632/60000 datapoints
2025-03-07 18:59:50,867 - INFO - training batch 1851, loss: 0.981, 59232/60000 datapoints
2025-03-07 18:59:51,010 - INFO - validation batch 1, loss: 1.142, 32/10016 datapoints
2025-03-07 18:59:51,241 - INFO - validation batch 51, loss: 0.947, 1632/10016 datapoints
2025-03-07 18:59:51,462 - INFO - validation batch 101, loss: 0.785, 3232/10016 datapoints
2025-03-07 18:59:51,692 - INFO - validation batch 151, loss: 0.781, 4832/10016 datapoints
2025-03-07 18:59:51,925 - INFO - validation batch 201, loss: 0.748, 6432/10016 datapoints
2025-03-07 18:59:52,153 - INFO - validation batch 251, loss: 0.659, 8032/10016 datapoints
2025-03-07 18:59:52,379 - INFO - validation batch 301, loss: 1.084, 9632/10016 datapoints
2025-03-07 18:59:52,431 - INFO - Epoch 59/800 done.
2025-03-07 18:59:52,431 - INFO - Final validation performance:
Loss: 0.878, top-1 acc: 0.812top-5 acc: 0.812
2025-03-07 18:59:52,432 - INFO - Beginning epoch 60/800
2025-03-07 18:59:52,444 - INFO - training batch 1, loss: 0.662, 32/60000 datapoints
2025-03-07 18:59:52,730 - INFO - training batch 51, loss: 1.144, 1632/60000 datapoints
2025-03-07 18:59:53,009 - INFO - training batch 101, loss: 0.904, 3232/60000 datapoints
2025-03-07 18:59:53,294 - INFO - training batch 151, loss: 0.884, 4832/60000 datapoints
2025-03-07 18:59:53,569 - INFO - training batch 201, loss: 0.859, 6432/60000 datapoints
2025-03-07 18:59:53,850 - INFO - training batch 251, loss: 0.842, 8032/60000 datapoints
2025-03-07 18:59:54,134 - INFO - training batch 301, loss: 0.799, 9632/60000 datapoints
2025-03-07 18:59:54,434 - INFO - training batch 351, loss: 1.034, 11232/60000 datapoints
2025-03-07 18:59:54,716 - INFO - training batch 401, loss: 0.862, 12832/60000 datapoints
2025-03-07 18:59:54,991 - INFO - training batch 451, loss: 0.626, 14432/60000 datapoints
2025-03-07 18:59:55,270 - INFO - training batch 501, loss: 0.823, 16032/60000 datapoints
2025-03-07 18:59:55,550 - INFO - training batch 551, loss: 0.968, 17632/60000 datapoints
2025-03-07 18:59:55,828 - INFO - training batch 601, loss: 1.223, 19232/60000 datapoints
2025-03-07 18:59:56,127 - INFO - training batch 651, loss: 0.992, 20832/60000 datapoints
2025-03-07 18:59:56,435 - INFO - training batch 701, loss: 0.933, 22432/60000 datapoints
2025-03-07 18:59:56,742 - INFO - training batch 751, loss: 0.853, 24032/60000 datapoints
2025-03-07 18:59:57,019 - INFO - training batch 801, loss: 0.905, 25632/60000 datapoints
2025-03-07 18:59:57,295 - INFO - training batch 851, loss: 0.763, 27232/60000 datapoints
2025-03-07 18:59:57,569 - INFO - training batch 901, loss: 0.694, 28832/60000 datapoints
2025-03-07 18:59:57,845 - INFO - training batch 951, loss: 1.030, 30432/60000 datapoints
2025-03-07 18:59:58,144 - INFO - training batch 1001, loss: 0.697, 32032/60000 datapoints
2025-03-07 18:59:58,414 - INFO - training batch 1051, loss: 0.904, 33632/60000 datapoints
2025-03-07 18:59:58,696 - INFO - training batch 1101, loss: 0.865, 35232/60000 datapoints
2025-03-07 18:59:59,000 - INFO - training batch 1151, loss: 0.561, 36832/60000 datapoints
2025-03-07 18:59:59,283 - INFO - training batch 1201, loss: 0.669, 38432/60000 datapoints
2025-03-07 18:59:59,555 - INFO - training batch 1251, loss: 0.895, 40032/60000 datapoints
2025-03-07 18:59:59,832 - INFO - training batch 1301, loss: 0.858, 41632/60000 datapoints
2025-03-07 19:00:00,128 - INFO - training batch 1351, loss: 0.846, 43232/60000 datapoints
2025-03-07 19:00:00,410 - INFO - training batch 1401, loss: 0.837, 44832/60000 datapoints
2025-03-07 19:00:00,691 - INFO - training batch 1451, loss: 0.752, 46432/60000 datapoints
2025-03-07 19:00:00,962 - INFO - training batch 1501, loss: 0.933, 48032/60000 datapoints
2025-03-07 19:00:01,250 - INFO - training batch 1551, loss: 0.703, 49632/60000 datapoints
2025-03-07 19:00:01,522 - INFO - training batch 1601, loss: 0.928, 51232/60000 datapoints
2025-03-07 19:00:01,807 - INFO - training batch 1651, loss: 0.954, 52832/60000 datapoints
2025-03-07 19:00:02,120 - INFO - training batch 1701, loss: 0.750, 54432/60000 datapoints
2025-03-07 19:00:02,401 - INFO - training batch 1751, loss: 0.704, 56032/60000 datapoints
2025-03-07 19:00:02,680 - INFO - training batch 1801, loss: 0.944, 57632/60000 datapoints
2025-03-07 19:00:02,952 - INFO - training batch 1851, loss: 0.891, 59232/60000 datapoints
2025-03-07 19:00:03,098 - INFO - validation batch 1, loss: 0.786, 32/10016 datapoints
2025-03-07 19:00:03,324 - INFO - validation batch 51, loss: 0.821, 1632/10016 datapoints
2025-03-07 19:00:03,543 - INFO - validation batch 101, loss: 0.851, 3232/10016 datapoints
2025-03-07 19:00:03,770 - INFO - validation batch 151, loss: 0.694, 4832/10016 datapoints
2025-03-07 19:00:03,992 - INFO - validation batch 201, loss: 0.752, 6432/10016 datapoints
2025-03-07 19:00:04,219 - INFO - validation batch 251, loss: 0.762, 8032/10016 datapoints
2025-03-07 19:00:04,466 - INFO - validation batch 301, loss: 0.846, 9632/10016 datapoints
2025-03-07 19:00:04,521 - INFO - Epoch 60/800 done.
2025-03-07 19:00:04,521 - INFO - Final validation performance:
Loss: 0.787, top-1 acc: 0.815top-5 acc: 0.815
2025-03-07 19:00:04,522 - INFO - Beginning epoch 61/800
2025-03-07 19:00:04,531 - INFO - training batch 1, loss: 0.677, 32/60000 datapoints
2025-03-07 19:00:04,812 - INFO - training batch 51, loss: 1.130, 1632/60000 datapoints
2025-03-07 19:00:05,102 - INFO - training batch 101, loss: 0.704, 3232/60000 datapoints
2025-03-07 19:00:05,389 - INFO - training batch 151, loss: 1.012, 4832/60000 datapoints
2025-03-07 19:00:05,703 - INFO - training batch 201, loss: 0.896, 6432/60000 datapoints
2025-03-07 19:00:06,056 - INFO - training batch 251, loss: 0.990, 8032/60000 datapoints
2025-03-07 19:00:06,342 - INFO - training batch 301, loss: 0.762, 9632/60000 datapoints
2025-03-07 19:00:06,631 - INFO - training batch 351, loss: 1.054, 11232/60000 datapoints
2025-03-07 19:00:06,928 - INFO - training batch 401, loss: 0.829, 12832/60000 datapoints
2025-03-07 19:00:07,239 - INFO - training batch 451, loss: 0.949, 14432/60000 datapoints
2025-03-07 19:00:07,555 - INFO - training batch 501, loss: 0.754, 16032/60000 datapoints
2025-03-07 19:00:07,874 - INFO - training batch 551, loss: 0.687, 17632/60000 datapoints
2025-03-07 19:00:08,216 - INFO - training batch 601, loss: 0.964, 19232/60000 datapoints
2025-03-07 19:00:08,592 - INFO - training batch 651, loss: 0.802, 20832/60000 datapoints
2025-03-07 19:00:08,934 - INFO - training batch 701, loss: 0.841, 22432/60000 datapoints
2025-03-07 19:00:09,251 - INFO - training batch 751, loss: 0.946, 24032/60000 datapoints
2025-03-07 19:00:09,571 - INFO - training batch 801, loss: 1.014, 25632/60000 datapoints
2025-03-07 19:00:09,898 - INFO - training batch 851, loss: 0.843, 27232/60000 datapoints
2025-03-07 19:00:10,236 - INFO - training batch 901, loss: 0.962, 28832/60000 datapoints
2025-03-07 19:00:10,525 - INFO - training batch 951, loss: 0.860, 30432/60000 datapoints
2025-03-07 19:00:10,811 - INFO - training batch 1001, loss: 1.057, 32032/60000 datapoints
2025-03-07 19:00:11,097 - INFO - training batch 1051, loss: 0.877, 33632/60000 datapoints
2025-03-07 19:00:11,419 - INFO - training batch 1101, loss: 0.837, 35232/60000 datapoints
2025-03-07 19:00:12,298 - INFO - training batch 1151, loss: 0.954, 36832/60000 datapoints
2025-03-07 19:00:12,667 - INFO - training batch 1201, loss: 0.972, 38432/60000 datapoints
2025-03-07 19:00:13,044 - INFO - training batch 1251, loss: 0.895, 40032/60000 datapoints
2025-03-07 19:00:13,410 - INFO - training batch 1301, loss: 0.873, 41632/60000 datapoints
2025-03-07 19:00:13,688 - INFO - training batch 1351, loss: 0.714, 43232/60000 datapoints
2025-03-07 19:00:13,994 - INFO - training batch 1401, loss: 0.771, 44832/60000 datapoints
2025-03-07 19:00:14,292 - INFO - training batch 1451, loss: 0.852, 46432/60000 datapoints
2025-03-07 19:00:14,605 - INFO - training batch 1501, loss: 1.105, 48032/60000 datapoints
2025-03-07 19:00:14,883 - INFO - training batch 1551, loss: 0.813, 49632/60000 datapoints
2025-03-07 19:00:15,209 - INFO - training batch 1601, loss: 0.927, 51232/60000 datapoints
2025-03-07 19:00:15,503 - INFO - training batch 1651, loss: 0.839, 52832/60000 datapoints
2025-03-07 19:00:15,786 - INFO - training batch 1701, loss: 0.732, 54432/60000 datapoints
2025-03-07 19:00:16,070 - INFO - training batch 1751, loss: 1.073, 56032/60000 datapoints
2025-03-07 19:00:16,347 - INFO - training batch 1801, loss: 0.865, 57632/60000 datapoints
2025-03-07 19:00:16,622 - INFO - training batch 1851, loss: 0.747, 59232/60000 datapoints
2025-03-07 19:00:16,766 - INFO - validation batch 1, loss: 0.828, 32/10016 datapoints
2025-03-07 19:00:17,010 - INFO - validation batch 51, loss: 0.858, 1632/10016 datapoints
2025-03-07 19:00:17,242 - INFO - validation batch 101, loss: 1.100, 3232/10016 datapoints
2025-03-07 19:00:17,486 - INFO - validation batch 151, loss: 0.578, 4832/10016 datapoints
2025-03-07 19:00:17,714 - INFO - validation batch 201, loss: 0.817, 6432/10016 datapoints
2025-03-07 19:00:17,941 - INFO - validation batch 251, loss: 0.847, 8032/10016 datapoints
2025-03-07 19:00:18,171 - INFO - validation batch 301, loss: 0.648, 9632/10016 datapoints
2025-03-07 19:00:18,225 - INFO - Epoch 61/800 done.
2025-03-07 19:00:18,225 - INFO - Final validation performance:
Loss: 0.811, top-1 acc: 0.817top-5 acc: 0.817
2025-03-07 19:00:18,226 - INFO - Beginning epoch 62/800
2025-03-07 19:00:18,235 - INFO - training batch 1, loss: 0.827, 32/60000 datapoints
2025-03-07 19:00:18,526 - INFO - training batch 51, loss: 1.025, 1632/60000 datapoints
2025-03-07 19:00:18,810 - INFO - training batch 101, loss: 0.743, 3232/60000 datapoints
2025-03-07 19:00:19,092 - INFO - training batch 151, loss: 0.934, 4832/60000 datapoints
2025-03-07 19:00:19,366 - INFO - training batch 201, loss: 0.696, 6432/60000 datapoints
2025-03-07 19:00:19,648 - INFO - training batch 251, loss: 0.914, 8032/60000 datapoints
2025-03-07 19:00:19,927 - INFO - training batch 301, loss: 0.849, 9632/60000 datapoints
2025-03-07 19:00:20,204 - INFO - training batch 351, loss: 0.923, 11232/60000 datapoints
2025-03-07 19:00:20,478 - INFO - training batch 401, loss: 0.740, 12832/60000 datapoints
2025-03-07 19:00:20,763 - INFO - training batch 451, loss: 1.060, 14432/60000 datapoints
2025-03-07 19:00:21,040 - INFO - training batch 501, loss: 0.787, 16032/60000 datapoints
2025-03-07 19:00:21,315 - INFO - training batch 551, loss: 0.967, 17632/60000 datapoints
2025-03-07 19:00:21,595 - INFO - training batch 601, loss: 0.806, 19232/60000 datapoints
2025-03-07 19:00:21,870 - INFO - training batch 651, loss: 0.987, 20832/60000 datapoints
2025-03-07 19:00:22,155 - INFO - training batch 701, loss: 0.910, 22432/60000 datapoints
2025-03-07 19:00:22,425 - INFO - training batch 751, loss: 0.723, 24032/60000 datapoints
2025-03-07 19:00:22,702 - INFO - training batch 801, loss: 0.827, 25632/60000 datapoints
2025-03-07 19:00:22,983 - INFO - training batch 851, loss: 0.922, 27232/60000 datapoints
2025-03-07 19:00:23,264 - INFO - training batch 901, loss: 1.018, 28832/60000 datapoints
2025-03-07 19:00:23,532 - INFO - training batch 951, loss: 0.680, 30432/60000 datapoints
2025-03-07 19:00:23,811 - INFO - training batch 1001, loss: 0.838, 32032/60000 datapoints
2025-03-07 19:00:24,091 - INFO - training batch 1051, loss: 0.662, 33632/60000 datapoints
2025-03-07 19:00:24,364 - INFO - training batch 1101, loss: 0.850, 35232/60000 datapoints
2025-03-07 19:00:24,661 - INFO - training batch 1151, loss: 0.721, 36832/60000 datapoints
2025-03-07 19:00:24,954 - INFO - training batch 1201, loss: 0.942, 38432/60000 datapoints
2025-03-07 19:00:25,233 - INFO - training batch 1251, loss: 0.729, 40032/60000 datapoints
2025-03-07 19:00:25,505 - INFO - training batch 1301, loss: 0.696, 41632/60000 datapoints
2025-03-07 19:00:25,783 - INFO - training batch 1351, loss: 0.822, 43232/60000 datapoints
2025-03-07 19:00:26,066 - INFO - training batch 1401, loss: 0.851, 44832/60000 datapoints
2025-03-07 19:00:26,339 - INFO - training batch 1451, loss: 0.780, 46432/60000 datapoints
2025-03-07 19:00:26,613 - INFO - training batch 1501, loss: 0.662, 48032/60000 datapoints
2025-03-07 19:00:26,886 - INFO - training batch 1551, loss: 0.863, 49632/60000 datapoints
2025-03-07 19:00:27,189 - INFO - training batch 1601, loss: 0.782, 51232/60000 datapoints
2025-03-07 19:00:27,458 - INFO - training batch 1651, loss: 0.972, 52832/60000 datapoints
2025-03-07 19:00:27,736 - INFO - training batch 1701, loss: 0.914, 54432/60000 datapoints
2025-03-07 19:00:28,010 - INFO - training batch 1751, loss: 0.818, 56032/60000 datapoints
2025-03-07 19:00:28,300 - INFO - training batch 1801, loss: 0.717, 57632/60000 datapoints
2025-03-07 19:00:28,574 - INFO - training batch 1851, loss: 0.808, 59232/60000 datapoints
2025-03-07 19:00:28,718 - INFO - validation batch 1, loss: 0.712, 32/10016 datapoints
2025-03-07 19:00:28,948 - INFO - validation batch 51, loss: 0.871, 1632/10016 datapoints
2025-03-07 19:00:29,171 - INFO - validation batch 101, loss: 0.970, 3232/10016 datapoints
2025-03-07 19:00:29,389 - INFO - validation batch 151, loss: 0.664, 4832/10016 datapoints
2025-03-07 19:00:29,621 - INFO - validation batch 201, loss: 0.744, 6432/10016 datapoints
2025-03-07 19:00:29,841 - INFO - validation batch 251, loss: 1.028, 8032/10016 datapoints
2025-03-07 19:00:30,067 - INFO - validation batch 301, loss: 0.692, 9632/10016 datapoints
2025-03-07 19:00:30,125 - INFO - Epoch 62/800 done.
2025-03-07 19:00:30,125 - INFO - Final validation performance:
Loss: 0.812, top-1 acc: 0.819top-5 acc: 0.819
2025-03-07 19:00:30,126 - INFO - Beginning epoch 63/800
2025-03-07 19:00:30,135 - INFO - training batch 1, loss: 0.688, 32/60000 datapoints
2025-03-07 19:00:30,411 - INFO - training batch 51, loss: 0.758, 1632/60000 datapoints
2025-03-07 19:00:30,685 - INFO - training batch 101, loss: 0.818, 3232/60000 datapoints
2025-03-07 19:00:30,991 - INFO - training batch 151, loss: 0.822, 4832/60000 datapoints
2025-03-07 19:00:31,271 - INFO - training batch 201, loss: 0.497, 6432/60000 datapoints
2025-03-07 19:00:31,677 - INFO - training batch 251, loss: 0.979, 8032/60000 datapoints
2025-03-07 19:00:31,960 - INFO - training batch 301, loss: 0.582, 9632/60000 datapoints
2025-03-07 19:00:32,259 - INFO - training batch 351, loss: 0.699, 11232/60000 datapoints
2025-03-07 19:00:32,552 - INFO - training batch 401, loss: 0.901, 12832/60000 datapoints
2025-03-07 19:00:32,832 - INFO - training batch 451, loss: 0.756, 14432/60000 datapoints
2025-03-07 19:00:33,113 - INFO - training batch 501, loss: 0.654, 16032/60000 datapoints
2025-03-07 19:00:33,392 - INFO - training batch 551, loss: 0.722, 17632/60000 datapoints
2025-03-07 19:00:33,672 - INFO - training batch 601, loss: 0.839, 19232/60000 datapoints
2025-03-07 19:00:33,953 - INFO - training batch 651, loss: 0.717, 20832/60000 datapoints
2025-03-07 19:00:34,255 - INFO - training batch 701, loss: 0.710, 22432/60000 datapoints
2025-03-07 19:00:34,559 - INFO - training batch 751, loss: 0.761, 24032/60000 datapoints
2025-03-07 19:00:34,956 - INFO - training batch 801, loss: 0.892, 25632/60000 datapoints
2025-03-07 19:00:35,430 - INFO - training batch 851, loss: 0.762, 27232/60000 datapoints
2025-03-07 19:00:35,733 - INFO - training batch 901, loss: 0.777, 28832/60000 datapoints
2025-03-07 19:00:36,104 - INFO - training batch 951, loss: 0.636, 30432/60000 datapoints
2025-03-07 19:00:36,439 - INFO - training batch 1001, loss: 0.676, 32032/60000 datapoints
2025-03-07 19:00:36,736 - INFO - training batch 1051, loss: 0.939, 33632/60000 datapoints
2025-03-07 19:00:37,024 - INFO - training batch 1101, loss: 0.640, 35232/60000 datapoints
2025-03-07 19:00:37,304 - INFO - training batch 1151, loss: 0.718, 36832/60000 datapoints
2025-03-07 19:00:37,590 - INFO - training batch 1201, loss: 0.701, 38432/60000 datapoints
2025-03-07 19:00:37,869 - INFO - training batch 1251, loss: 0.830, 40032/60000 datapoints
2025-03-07 19:00:38,184 - INFO - training batch 1301, loss: 0.716, 41632/60000 datapoints
2025-03-07 19:00:38,460 - INFO - training batch 1351, loss: 0.752, 43232/60000 datapoints
2025-03-07 19:00:38,739 - INFO - training batch 1401, loss: 0.850, 44832/60000 datapoints
2025-03-07 19:00:39,019 - INFO - training batch 1451, loss: 0.828, 46432/60000 datapoints
2025-03-07 19:00:39,297 - INFO - training batch 1501, loss: 0.876, 48032/60000 datapoints
2025-03-07 19:00:39,570 - INFO - training batch 1551, loss: 0.686, 49632/60000 datapoints
2025-03-07 19:00:39,844 - INFO - training batch 1601, loss: 0.762, 51232/60000 datapoints
2025-03-07 19:00:40,140 - INFO - training batch 1651, loss: 0.973, 52832/60000 datapoints
2025-03-07 19:00:40,416 - INFO - training batch 1701, loss: 0.718, 54432/60000 datapoints
2025-03-07 19:00:40,692 - INFO - training batch 1751, loss: 0.784, 56032/60000 datapoints
2025-03-07 19:00:40,994 - INFO - training batch 1801, loss: 0.843, 57632/60000 datapoints
2025-03-07 19:00:41,279 - INFO - training batch 1851, loss: 0.945, 59232/60000 datapoints
2025-03-07 19:00:41,419 - INFO - validation batch 1, loss: 0.858, 32/10016 datapoints
2025-03-07 19:00:41,656 - INFO - validation batch 51, loss: 0.850, 1632/10016 datapoints
2025-03-07 19:00:41,880 - INFO - validation batch 101, loss: 0.760, 3232/10016 datapoints
2025-03-07 19:00:42,111 - INFO - validation batch 151, loss: 0.929, 4832/10016 datapoints
2025-03-07 19:00:42,330 - INFO - validation batch 201, loss: 0.677, 6432/10016 datapoints
2025-03-07 19:00:42,551 - INFO - validation batch 251, loss: 0.809, 8032/10016 datapoints
2025-03-07 19:00:42,779 - INFO - validation batch 301, loss: 0.806, 9632/10016 datapoints
2025-03-07 19:00:42,836 - INFO - Epoch 63/800 done.
2025-03-07 19:00:42,836 - INFO - Final validation performance:
Loss: 0.813, top-1 acc: 0.820top-5 acc: 0.820
2025-03-07 19:00:42,837 - INFO - Beginning epoch 64/800
2025-03-07 19:00:42,847 - INFO - training batch 1, loss: 0.688, 32/60000 datapoints
2025-03-07 19:00:43,127 - INFO - training batch 51, loss: 0.792, 1632/60000 datapoints
2025-03-07 19:00:43,407 - INFO - training batch 101, loss: 0.846, 3232/60000 datapoints
2025-03-07 19:00:43,690 - INFO - training batch 151, loss: 0.792, 4832/60000 datapoints
2025-03-07 19:00:43,968 - INFO - training batch 201, loss: 0.921, 6432/60000 datapoints
2025-03-07 19:00:44,282 - INFO - training batch 251, loss: 1.058, 8032/60000 datapoints
2025-03-07 19:00:44,559 - INFO - training batch 301, loss: 0.896, 9632/60000 datapoints
2025-03-07 19:00:44,856 - INFO - training batch 351, loss: 0.778, 11232/60000 datapoints
2025-03-07 19:00:45,183 - INFO - training batch 401, loss: 0.884, 12832/60000 datapoints
2025-03-07 19:00:45,507 - INFO - training batch 451, loss: 0.573, 14432/60000 datapoints
2025-03-07 19:00:45,788 - INFO - training batch 501, loss: 0.880, 16032/60000 datapoints
2025-03-07 19:00:46,070 - INFO - training batch 551, loss: 0.899, 17632/60000 datapoints
2025-03-07 19:00:46,353 - INFO - training batch 601, loss: 0.917, 19232/60000 datapoints
2025-03-07 19:00:46,640 - INFO - training batch 651, loss: 0.810, 20832/60000 datapoints
2025-03-07 19:00:46,927 - INFO - training batch 701, loss: 0.540, 22432/60000 datapoints
2025-03-07 19:00:47,209 - INFO - training batch 751, loss: 0.781, 24032/60000 datapoints
2025-03-07 19:00:47,481 - INFO - training batch 801, loss: 0.773, 25632/60000 datapoints
2025-03-07 19:00:47,791 - INFO - training batch 851, loss: 0.818, 27232/60000 datapoints
2025-03-07 19:00:48,072 - INFO - training batch 901, loss: 0.782, 28832/60000 datapoints
2025-03-07 19:00:48,359 - INFO - training batch 951, loss: 0.790, 30432/60000 datapoints
2025-03-07 19:00:48,637 - INFO - training batch 1001, loss: 0.793, 32032/60000 datapoints
2025-03-07 19:00:48,914 - INFO - training batch 1051, loss: 0.775, 33632/60000 datapoints
2025-03-07 19:00:49,197 - INFO - training batch 1101, loss: 0.764, 35232/60000 datapoints
2025-03-07 19:00:49,470 - INFO - training batch 1151, loss: 0.688, 36832/60000 datapoints
2025-03-07 19:00:49,752 - INFO - training batch 1201, loss: 0.736, 38432/60000 datapoints
2025-03-07 19:00:50,034 - INFO - training batch 1251, loss: 1.040, 40032/60000 datapoints
2025-03-07 19:00:50,311 - INFO - training batch 1301, loss: 0.974, 41632/60000 datapoints
2025-03-07 19:00:50,594 - INFO - training batch 1351, loss: 0.760, 43232/60000 datapoints
2025-03-07 19:00:50,868 - INFO - training batch 1401, loss: 0.740, 44832/60000 datapoints
2025-03-07 19:00:51,151 - INFO - training batch 1451, loss: 0.712, 46432/60000 datapoints
2025-03-07 19:00:51,422 - INFO - training batch 1501, loss: 0.960, 48032/60000 datapoints
2025-03-07 19:00:51,705 - INFO - training batch 1551, loss: 0.733, 49632/60000 datapoints
2025-03-07 19:00:51,978 - INFO - training batch 1601, loss: 0.782, 51232/60000 datapoints
2025-03-07 19:00:52,256 - INFO - training batch 1651, loss: 0.742, 52832/60000 datapoints
2025-03-07 19:00:52,526 - INFO - training batch 1701, loss: 1.087, 54432/60000 datapoints
2025-03-07 19:00:52,802 - INFO - training batch 1751, loss: 0.827, 56032/60000 datapoints
2025-03-07 19:00:53,090 - INFO - training batch 1801, loss: 1.001, 57632/60000 datapoints
2025-03-07 19:00:53,373 - INFO - training batch 1851, loss: 0.892, 59232/60000 datapoints
2025-03-07 19:00:53,515 - INFO - validation batch 1, loss: 0.643, 32/10016 datapoints
2025-03-07 19:00:53,745 - INFO - validation batch 51, loss: 0.765, 1632/10016 datapoints
2025-03-07 19:00:53,970 - INFO - validation batch 101, loss: 0.896, 3232/10016 datapoints
2025-03-07 19:00:54,198 - INFO - validation batch 151, loss: 0.911, 4832/10016 datapoints
2025-03-07 19:00:54,414 - INFO - validation batch 201, loss: 0.804, 6432/10016 datapoints
2025-03-07 19:00:54,656 - INFO - validation batch 251, loss: 0.841, 8032/10016 datapoints
2025-03-07 19:00:54,879 - INFO - validation batch 301, loss: 0.896, 9632/10016 datapoints
2025-03-07 19:00:54,933 - INFO - Epoch 64/800 done.
2025-03-07 19:00:54,933 - INFO - Final validation performance:
Loss: 0.822, top-1 acc: 0.822top-5 acc: 0.822
2025-03-07 19:00:54,934 - INFO - Beginning epoch 65/800
2025-03-07 19:00:54,943 - INFO - training batch 1, loss: 0.698, 32/60000 datapoints
2025-03-07 19:00:55,260 - INFO - training batch 51, loss: 0.907, 1632/60000 datapoints
2025-03-07 19:00:55,532 - INFO - training batch 101, loss: 0.869, 3232/60000 datapoints
2025-03-07 19:00:55,811 - INFO - training batch 151, loss: 0.720, 4832/60000 datapoints
2025-03-07 19:00:56,096 - INFO - training batch 201, loss: 0.679, 6432/60000 datapoints
2025-03-07 19:00:56,369 - INFO - training batch 251, loss: 0.859, 8032/60000 datapoints
2025-03-07 19:00:56,649 - INFO - training batch 301, loss: 1.044, 9632/60000 datapoints
2025-03-07 19:00:56,942 - INFO - training batch 351, loss: 0.677, 11232/60000 datapoints
2025-03-07 19:00:57,220 - INFO - training batch 401, loss: 0.785, 12832/60000 datapoints
2025-03-07 19:00:57,491 - INFO - training batch 451, loss: 0.894, 14432/60000 datapoints
2025-03-07 19:00:57,781 - INFO - training batch 501, loss: 0.530, 16032/60000 datapoints
2025-03-07 19:00:58,058 - INFO - training batch 551, loss: 0.646, 17632/60000 datapoints
2025-03-07 19:00:58,345 - INFO - training batch 601, loss: 1.007, 19232/60000 datapoints
2025-03-07 19:00:58,629 - INFO - training batch 651, loss: 0.742, 20832/60000 datapoints
2025-03-07 19:00:58,910 - INFO - training batch 701, loss: 0.806, 22432/60000 datapoints
2025-03-07 19:00:59,186 - INFO - training batch 751, loss: 0.590, 24032/60000 datapoints
2025-03-07 19:00:59,459 - INFO - training batch 801, loss: 0.788, 25632/60000 datapoints
2025-03-07 19:00:59,738 - INFO - training batch 851, loss: 0.926, 27232/60000 datapoints
2025-03-07 19:01:00,053 - INFO - training batch 901, loss: 0.659, 28832/60000 datapoints
2025-03-07 19:01:00,330 - INFO - training batch 951, loss: 0.750, 30432/60000 datapoints
2025-03-07 19:01:00,628 - INFO - training batch 1001, loss: 0.834, 32032/60000 datapoints
2025-03-07 19:01:00,907 - INFO - training batch 1051, loss: 0.752, 33632/60000 datapoints
2025-03-07 19:01:01,190 - INFO - training batch 1101, loss: 0.700, 35232/60000 datapoints
2025-03-07 19:01:01,461 - INFO - training batch 1151, loss: 0.780, 36832/60000 datapoints
2025-03-07 19:01:01,746 - INFO - training batch 1201, loss: 0.737, 38432/60000 datapoints
2025-03-07 19:01:02,026 - INFO - training batch 1251, loss: 1.021, 40032/60000 datapoints
2025-03-07 19:01:02,306 - INFO - training batch 1301, loss: 0.728, 41632/60000 datapoints
2025-03-07 19:01:02,581 - INFO - training batch 1351, loss: 1.039, 43232/60000 datapoints
2025-03-07 19:01:02,856 - INFO - training batch 1401, loss: 0.654, 44832/60000 datapoints
2025-03-07 19:01:03,142 - INFO - training batch 1451, loss: 0.733, 46432/60000 datapoints
2025-03-07 19:01:03,416 - INFO - training batch 1501, loss: 0.698, 48032/60000 datapoints
2025-03-07 19:01:03,698 - INFO - training batch 1551, loss: 0.702, 49632/60000 datapoints
2025-03-07 19:01:03,971 - INFO - training batch 1601, loss: 0.720, 51232/60000 datapoints
2025-03-07 19:01:04,247 - INFO - training batch 1651, loss: 0.926, 52832/60000 datapoints
2025-03-07 19:01:04,515 - INFO - training batch 1701, loss: 0.854, 54432/60000 datapoints
2025-03-07 19:01:04,790 - INFO - training batch 1751, loss: 0.670, 56032/60000 datapoints
2025-03-07 19:01:05,075 - INFO - training batch 1801, loss: 0.739, 57632/60000 datapoints
2025-03-07 19:01:05,387 - INFO - training batch 1851, loss: 0.728, 59232/60000 datapoints
2025-03-07 19:01:05,528 - INFO - validation batch 1, loss: 0.718, 32/10016 datapoints
2025-03-07 19:01:05,757 - INFO - validation batch 51, loss: 0.734, 1632/10016 datapoints
2025-03-07 19:01:05,977 - INFO - validation batch 101, loss: 0.593, 3232/10016 datapoints
2025-03-07 19:01:06,207 - INFO - validation batch 151, loss: 0.783, 4832/10016 datapoints
2025-03-07 19:01:06,425 - INFO - validation batch 201, loss: 1.044, 6432/10016 datapoints
2025-03-07 19:01:06,650 - INFO - validation batch 251, loss: 0.851, 8032/10016 datapoints
2025-03-07 19:01:06,871 - INFO - validation batch 301, loss: 0.867, 9632/10016 datapoints
2025-03-07 19:01:06,929 - INFO - Epoch 65/800 done.
2025-03-07 19:01:06,929 - INFO - Final validation performance:
Loss: 0.798, top-1 acc: 0.824top-5 acc: 0.824
2025-03-07 19:01:06,930 - INFO - Beginning epoch 66/800
2025-03-07 19:01:06,939 - INFO - training batch 1, loss: 0.782, 32/60000 datapoints
2025-03-07 19:01:07,235 - INFO - training batch 51, loss: 0.744, 1632/60000 datapoints
2025-03-07 19:01:07,504 - INFO - training batch 101, loss: 0.599, 3232/60000 datapoints
2025-03-07 19:01:07,789 - INFO - training batch 151, loss: 0.978, 4832/60000 datapoints
2025-03-07 19:01:08,068 - INFO - training batch 201, loss: 0.845, 6432/60000 datapoints
2025-03-07 19:01:08,352 - INFO - training batch 251, loss: 0.972, 8032/60000 datapoints
2025-03-07 19:01:08,627 - INFO - training batch 301, loss: 0.803, 9632/60000 datapoints
2025-03-07 19:01:08,900 - INFO - training batch 351, loss: 0.801, 11232/60000 datapoints
2025-03-07 19:01:09,178 - INFO - training batch 401, loss: 0.674, 12832/60000 datapoints
2025-03-07 19:01:09,522 - INFO - training batch 451, loss: 0.689, 14432/60000 datapoints
2025-03-07 19:01:09,884 - INFO - training batch 501, loss: 1.036, 16032/60000 datapoints
2025-03-07 19:01:10,244 - INFO - training batch 551, loss: 0.798, 17632/60000 datapoints
2025-03-07 19:01:10,612 - INFO - training batch 601, loss: 0.854, 19232/60000 datapoints
2025-03-07 19:01:10,983 - INFO - training batch 651, loss: 0.677, 20832/60000 datapoints
2025-03-07 19:01:11,501 - INFO - training batch 701, loss: 0.676, 22432/60000 datapoints
2025-03-07 19:01:11,837 - INFO - training batch 751, loss: 0.819, 24032/60000 datapoints
2025-03-07 19:01:12,118 - INFO - training batch 801, loss: 0.847, 25632/60000 datapoints
2025-03-07 19:01:12,387 - INFO - training batch 851, loss: 0.763, 27232/60000 datapoints
2025-03-07 19:01:12,663 - INFO - training batch 901, loss: 0.961, 28832/60000 datapoints
2025-03-07 19:01:12,939 - INFO - training batch 951, loss: 0.665, 30432/60000 datapoints
2025-03-07 19:01:13,217 - INFO - training batch 1001, loss: 0.927, 32032/60000 datapoints
2025-03-07 19:01:13,491 - INFO - training batch 1051, loss: 0.752, 33632/60000 datapoints
2025-03-07 19:01:13,785 - INFO - training batch 1101, loss: 0.725, 35232/60000 datapoints
2025-03-07 19:01:14,073 - INFO - training batch 1151, loss: 0.663, 36832/60000 datapoints
2025-03-07 19:01:14,355 - INFO - training batch 1201, loss: 0.833, 38432/60000 datapoints
2025-03-07 19:01:14,631 - INFO - training batch 1251, loss: 0.734, 40032/60000 datapoints
2025-03-07 19:01:14,909 - INFO - training batch 1301, loss: 0.651, 41632/60000 datapoints
2025-03-07 19:01:15,191 - INFO - training batch 1351, loss: 0.907, 43232/60000 datapoints
2025-03-07 19:01:15,490 - INFO - training batch 1401, loss: 0.664, 44832/60000 datapoints
2025-03-07 19:01:15,774 - INFO - training batch 1451, loss: 0.824, 46432/60000 datapoints
2025-03-07 19:01:16,052 - INFO - training batch 1501, loss: 0.778, 48032/60000 datapoints
2025-03-07 19:01:16,333 - INFO - training batch 1551, loss: 0.813, 49632/60000 datapoints
2025-03-07 19:01:16,616 - INFO - training batch 1601, loss: 0.819, 51232/60000 datapoints
2025-03-07 19:01:16,891 - INFO - training batch 1651, loss: 0.884, 52832/60000 datapoints
2025-03-07 19:01:17,177 - INFO - training batch 1701, loss: 0.759, 54432/60000 datapoints
2025-03-07 19:01:17,447 - INFO - training batch 1751, loss: 0.550, 56032/60000 datapoints
2025-03-07 19:01:17,727 - INFO - training batch 1801, loss: 0.716, 57632/60000 datapoints
2025-03-07 19:01:18,010 - INFO - training batch 1851, loss: 0.728, 59232/60000 datapoints
2025-03-07 19:01:18,165 - INFO - validation batch 1, loss: 0.974, 32/10016 datapoints
2025-03-07 19:01:18,385 - INFO - validation batch 51, loss: 0.660, 1632/10016 datapoints
2025-03-07 19:01:18,621 - INFO - validation batch 101, loss: 0.797, 3232/10016 datapoints
2025-03-07 19:01:18,845 - INFO - validation batch 151, loss: 0.780, 4832/10016 datapoints
2025-03-07 19:01:19,074 - INFO - validation batch 201, loss: 0.912, 6432/10016 datapoints
2025-03-07 19:01:19,299 - INFO - validation batch 251, loss: 0.760, 8032/10016 datapoints
2025-03-07 19:01:19,517 - INFO - validation batch 301, loss: 0.525, 9632/10016 datapoints
2025-03-07 19:01:19,571 - INFO - Epoch 66/800 done.
2025-03-07 19:01:19,571 - INFO - Final validation performance:
Loss: 0.772, top-1 acc: 0.825top-5 acc: 0.825
2025-03-07 19:01:19,573 - INFO - Beginning epoch 67/800
2025-03-07 19:01:19,582 - INFO - training batch 1, loss: 0.808, 32/60000 datapoints
2025-03-07 19:01:19,860 - INFO - training batch 51, loss: 0.826, 1632/60000 datapoints
2025-03-07 19:01:20,143 - INFO - training batch 101, loss: 0.907, 3232/60000 datapoints
2025-03-07 19:01:20,417 - INFO - training batch 151, loss: 0.630, 4832/60000 datapoints
2025-03-07 19:01:20,704 - INFO - training batch 201, loss: 0.811, 6432/60000 datapoints
2025-03-07 19:01:20,980 - INFO - training batch 251, loss: 0.603, 8032/60000 datapoints
2025-03-07 19:01:21,257 - INFO - training batch 301, loss: 0.562, 9632/60000 datapoints
2025-03-07 19:01:21,526 - INFO - training batch 351, loss: 0.654, 11232/60000 datapoints
2025-03-07 19:01:21,812 - INFO - training batch 401, loss: 0.914, 12832/60000 datapoints
2025-03-07 19:01:22,112 - INFO - training batch 451, loss: 0.831, 14432/60000 datapoints
2025-03-07 19:01:22,387 - INFO - training batch 501, loss: 0.838, 16032/60000 datapoints
2025-03-07 19:01:22,662 - INFO - training batch 551, loss: 0.863, 17632/60000 datapoints
2025-03-07 19:01:22,941 - INFO - training batch 601, loss: 0.657, 19232/60000 datapoints
2025-03-07 19:01:23,221 - INFO - training batch 651, loss: 0.770, 20832/60000 datapoints
2025-03-07 19:01:23,497 - INFO - training batch 701, loss: 0.867, 22432/60000 datapoints
2025-03-07 19:01:23,811 - INFO - training batch 751, loss: 0.632, 24032/60000 datapoints
2025-03-07 19:01:24,099 - INFO - training batch 801, loss: 0.698, 25632/60000 datapoints
2025-03-07 19:01:24,377 - INFO - training batch 851, loss: 0.719, 27232/60000 datapoints
2025-03-07 19:01:24,656 - INFO - training batch 901, loss: 0.772, 28832/60000 datapoints
2025-03-07 19:01:24,954 - INFO - training batch 951, loss: 0.660, 30432/60000 datapoints
2025-03-07 19:01:25,230 - INFO - training batch 1001, loss: 0.844, 32032/60000 datapoints
2025-03-07 19:01:25,528 - INFO - training batch 1051, loss: 0.800, 33632/60000 datapoints
2025-03-07 19:01:25,810 - INFO - training batch 1101, loss: 0.840, 35232/60000 datapoints
2025-03-07 19:01:26,094 - INFO - training batch 1151, loss: 0.797, 36832/60000 datapoints
2025-03-07 19:01:26,368 - INFO - training batch 1201, loss: 1.045, 38432/60000 datapoints
2025-03-07 19:01:26,644 - INFO - training batch 1251, loss: 1.014, 40032/60000 datapoints
2025-03-07 19:01:26,923 - INFO - training batch 1301, loss: 0.818, 41632/60000 datapoints
2025-03-07 19:01:27,216 - INFO - training batch 1351, loss: 0.874, 43232/60000 datapoints
2025-03-07 19:01:27,485 - INFO - training batch 1401, loss: 0.797, 44832/60000 datapoints
2025-03-07 19:01:27,767 - INFO - training batch 1451, loss: 0.828, 46432/60000 datapoints
2025-03-07 19:01:28,060 - INFO - training batch 1501, loss: 0.821, 48032/60000 datapoints
2025-03-07 19:01:28,350 - INFO - training batch 1551, loss: 0.895, 49632/60000 datapoints
2025-03-07 19:01:28,627 - INFO - training batch 1601, loss: 0.756, 51232/60000 datapoints
2025-03-07 19:01:28,918 - INFO - training batch 1651, loss: 0.808, 52832/60000 datapoints
2025-03-07 19:01:29,198 - INFO - training batch 1701, loss: 0.747, 54432/60000 datapoints
2025-03-07 19:01:29,470 - INFO - training batch 1751, loss: 0.886, 56032/60000 datapoints
2025-03-07 19:01:29,749 - INFO - training batch 1801, loss: 0.658, 57632/60000 datapoints
2025-03-07 19:01:30,027 - INFO - training batch 1851, loss: 0.699, 59232/60000 datapoints
2025-03-07 19:01:30,175 - INFO - validation batch 1, loss: 0.743, 32/10016 datapoints
2025-03-07 19:01:30,399 - INFO - validation batch 51, loss: 0.906, 1632/10016 datapoints
2025-03-07 19:01:30,623 - INFO - validation batch 101, loss: 0.674, 3232/10016 datapoints
2025-03-07 19:01:30,852 - INFO - validation batch 151, loss: 0.843, 4832/10016 datapoints
2025-03-07 19:01:31,130 - INFO - validation batch 201, loss: 0.617, 6432/10016 datapoints
2025-03-07 19:01:31,355 - INFO - validation batch 251, loss: 0.856, 8032/10016 datapoints
2025-03-07 19:01:31,574 - INFO - validation batch 301, loss: 0.628, 9632/10016 datapoints
2025-03-07 19:01:31,633 - INFO - Epoch 67/800 done.
2025-03-07 19:01:31,633 - INFO - Final validation performance:
Loss: 0.752, top-1 acc: 0.827top-5 acc: 0.827
2025-03-07 19:01:31,634 - INFO - Beginning epoch 68/800
2025-03-07 19:01:31,643 - INFO - training batch 1, loss: 0.736, 32/60000 datapoints
2025-03-07 19:01:31,924 - INFO - training batch 51, loss: 0.607, 1632/60000 datapoints
2025-03-07 19:01:32,215 - INFO - training batch 101, loss: 0.953, 3232/60000 datapoints
2025-03-07 19:01:32,488 - INFO - training batch 151, loss: 1.010, 4832/60000 datapoints
2025-03-07 19:01:32,769 - INFO - training batch 201, loss: 0.693, 6432/60000 datapoints
2025-03-07 19:01:33,043 - INFO - training batch 251, loss: 0.802, 8032/60000 datapoints
2025-03-07 19:01:33,323 - INFO - training batch 301, loss: 0.659, 9632/60000 datapoints
2025-03-07 19:01:33,604 - INFO - training batch 351, loss: 0.817, 11232/60000 datapoints
2025-03-07 19:01:33,880 - INFO - training batch 401, loss: 0.856, 12832/60000 datapoints
2025-03-07 19:01:34,165 - INFO - training batch 451, loss: 0.772, 14432/60000 datapoints
2025-03-07 19:01:34,435 - INFO - training batch 501, loss: 0.776, 16032/60000 datapoints
2025-03-07 19:01:34,711 - INFO - training batch 551, loss: 0.886, 17632/60000 datapoints
2025-03-07 19:01:34,990 - INFO - training batch 601, loss: 0.874, 19232/60000 datapoints
2025-03-07 19:01:35,270 - INFO - training batch 651, loss: 0.741, 20832/60000 datapoints
2025-03-07 19:01:35,568 - INFO - training batch 701, loss: 0.884, 22432/60000 datapoints
2025-03-07 19:01:35,849 - INFO - training batch 751, loss: 0.693, 24032/60000 datapoints
2025-03-07 19:01:36,130 - INFO - training batch 801, loss: 0.972, 25632/60000 datapoints
2025-03-07 19:01:36,402 - INFO - training batch 851, loss: 0.806, 27232/60000 datapoints
2025-03-07 19:01:36,677 - INFO - training batch 901, loss: 0.951, 28832/60000 datapoints
2025-03-07 19:01:36,949 - INFO - training batch 951, loss: 0.739, 30432/60000 datapoints
2025-03-07 19:01:37,227 - INFO - training batch 1001, loss: 0.790, 32032/60000 datapoints
2025-03-07 19:01:37,497 - INFO - training batch 1051, loss: 0.995, 33632/60000 datapoints
2025-03-07 19:01:37,781 - INFO - training batch 1101, loss: 0.838, 35232/60000 datapoints
2025-03-07 19:01:38,061 - INFO - training batch 1151, loss: 0.898, 36832/60000 datapoints
2025-03-07 19:01:38,346 - INFO - training batch 1201, loss: 0.627, 38432/60000 datapoints
2025-03-07 19:01:38,643 - INFO - training batch 1251, loss: 0.777, 40032/60000 datapoints
2025-03-07 19:01:38,937 - INFO - training batch 1301, loss: 0.765, 41632/60000 datapoints
2025-03-07 19:01:39,278 - INFO - training batch 1351, loss: 0.667, 43232/60000 datapoints
2025-03-07 19:01:39,564 - INFO - training batch 1401, loss: 0.924, 44832/60000 datapoints
2025-03-07 19:01:39,848 - INFO - training batch 1451, loss: 0.823, 46432/60000 datapoints
2025-03-07 19:01:40,169 - INFO - training batch 1501, loss: 0.610, 48032/60000 datapoints
2025-03-07 19:01:40,537 - INFO - training batch 1551, loss: 0.807, 49632/60000 datapoints
2025-03-07 19:01:40,869 - INFO - training batch 1601, loss: 0.707, 51232/60000 datapoints
2025-03-07 19:01:41,153 - INFO - training batch 1651, loss: 1.017, 52832/60000 datapoints
2025-03-07 19:01:41,445 - INFO - training batch 1701, loss: 0.650, 54432/60000 datapoints
2025-03-07 19:01:41,736 - INFO - training batch 1751, loss: 0.784, 56032/60000 datapoints
2025-03-07 19:01:42,027 - INFO - training batch 1801, loss: 0.951, 57632/60000 datapoints
2025-03-07 19:01:42,313 - INFO - training batch 1851, loss: 0.732, 59232/60000 datapoints
2025-03-07 19:01:42,450 - INFO - validation batch 1, loss: 0.741, 32/10016 datapoints
2025-03-07 19:01:42,673 - INFO - validation batch 51, loss: 0.650, 1632/10016 datapoints
2025-03-07 19:01:42,900 - INFO - validation batch 101, loss: 0.725, 3232/10016 datapoints
2025-03-07 19:01:43,131 - INFO - validation batch 151, loss: 0.773, 4832/10016 datapoints
2025-03-07 19:01:43,356 - INFO - validation batch 201, loss: 0.691, 6432/10016 datapoints
2025-03-07 19:01:43,585 - INFO - validation batch 251, loss: 0.674, 8032/10016 datapoints
2025-03-07 19:01:43,814 - INFO - validation batch 301, loss: 0.696, 9632/10016 datapoints
2025-03-07 19:01:43,867 - INFO - Epoch 68/800 done.
2025-03-07 19:01:43,867 - INFO - Final validation performance:
Loss: 0.707, top-1 acc: 0.829top-5 acc: 0.829
2025-03-07 19:01:43,868 - INFO - Beginning epoch 69/800
2025-03-07 19:01:43,876 - INFO - training batch 1, loss: 0.845, 32/60000 datapoints
2025-03-07 19:01:44,163 - INFO - training batch 51, loss: 0.973, 1632/60000 datapoints
2025-03-07 19:01:44,452 - INFO - training batch 101, loss: 0.642, 3232/60000 datapoints
2025-03-07 19:01:44,730 - INFO - training batch 151, loss: 0.826, 4832/60000 datapoints
2025-03-07 19:01:45,009 - INFO - training batch 201, loss: 0.597, 6432/60000 datapoints
2025-03-07 19:01:45,291 - INFO - training batch 251, loss: 0.677, 8032/60000 datapoints
2025-03-07 19:01:45,629 - INFO - training batch 301, loss: 0.938, 9632/60000 datapoints
2025-03-07 19:01:45,930 - INFO - training batch 351, loss: 0.860, 11232/60000 datapoints
2025-03-07 19:01:46,211 - INFO - training batch 401, loss: 0.604, 12832/60000 datapoints
2025-03-07 19:01:46,485 - INFO - training batch 451, loss: 0.811, 14432/60000 datapoints
2025-03-07 19:01:46,766 - INFO - training batch 501, loss: 0.821, 16032/60000 datapoints
2025-03-07 19:01:47,051 - INFO - training batch 551, loss: 0.738, 17632/60000 datapoints
2025-03-07 19:01:47,327 - INFO - training batch 601, loss: 0.708, 19232/60000 datapoints
2025-03-07 19:01:47,652 - INFO - training batch 651, loss: 0.899, 20832/60000 datapoints
2025-03-07 19:01:47,933 - INFO - training batch 701, loss: 0.649, 22432/60000 datapoints
2025-03-07 19:01:48,219 - INFO - training batch 751, loss: 0.802, 24032/60000 datapoints
2025-03-07 19:01:48,493 - INFO - training batch 801, loss: 0.841, 25632/60000 datapoints
2025-03-07 19:01:48,768 - INFO - training batch 851, loss: 0.713, 27232/60000 datapoints
2025-03-07 19:01:49,045 - INFO - training batch 901, loss: 0.893, 28832/60000 datapoints
2025-03-07 19:01:49,323 - INFO - training batch 951, loss: 0.892, 30432/60000 datapoints
2025-03-07 19:01:49,604 - INFO - training batch 1001, loss: 0.654, 32032/60000 datapoints
2025-03-07 19:01:49,873 - INFO - training batch 1051, loss: 0.924, 33632/60000 datapoints
2025-03-07 19:01:50,158 - INFO - training batch 1101, loss: 0.886, 35232/60000 datapoints
2025-03-07 19:01:50,433 - INFO - training batch 1151, loss: 0.767, 36832/60000 datapoints
2025-03-07 19:01:50,712 - INFO - training batch 1201, loss: 0.897, 38432/60000 datapoints
2025-03-07 19:01:50,987 - INFO - training batch 1251, loss: 0.716, 40032/60000 datapoints
2025-03-07 19:01:51,266 - INFO - training batch 1301, loss: 0.591, 41632/60000 datapoints
2025-03-07 19:01:51,536 - INFO - training batch 1351, loss: 0.818, 43232/60000 datapoints
2025-03-07 19:01:51,815 - INFO - training batch 1401, loss: 0.710, 44832/60000 datapoints
2025-03-07 19:01:52,093 - INFO - training batch 1451, loss: 0.944, 46432/60000 datapoints
2025-03-07 19:01:52,368 - INFO - training batch 1501, loss: 0.645, 48032/60000 datapoints
2025-03-07 19:01:52,711 - INFO - training batch 1551, loss: 0.860, 49632/60000 datapoints
2025-03-07 19:01:53,005 - INFO - training batch 1601, loss: 0.815, 51232/60000 datapoints
2025-03-07 19:01:53,354 - INFO - training batch 1651, loss: 0.651, 52832/60000 datapoints
2025-03-07 19:01:53,789 - INFO - training batch 1701, loss: 0.643, 54432/60000 datapoints
2025-03-07 19:01:54,674 - INFO - training batch 1751, loss: 0.699, 56032/60000 datapoints
2025-03-07 19:01:55,146 - INFO - training batch 1801, loss: 0.788, 57632/60000 datapoints
2025-03-07 19:01:55,744 - INFO - training batch 1851, loss: 0.760, 59232/60000 datapoints
2025-03-07 19:01:55,961 - INFO - validation batch 1, loss: 0.845, 32/10016 datapoints
2025-03-07 19:01:56,372 - INFO - validation batch 51, loss: 0.675, 1632/10016 datapoints
2025-03-07 19:01:56,709 - INFO - validation batch 101, loss: 0.665, 3232/10016 datapoints
2025-03-07 19:01:57,009 - INFO - validation batch 151, loss: 0.751, 4832/10016 datapoints
2025-03-07 19:01:57,305 - INFO - validation batch 201, loss: 0.822, 6432/10016 datapoints
2025-03-07 19:01:57,622 - INFO - validation batch 251, loss: 0.706, 8032/10016 datapoints
2025-03-07 19:01:57,902 - INFO - validation batch 301, loss: 0.595, 9632/10016 datapoints
2025-03-07 19:01:57,989 - INFO - Epoch 69/800 done.
2025-03-07 19:01:57,989 - INFO - Final validation performance:
Loss: 0.723, top-1 acc: 0.830top-5 acc: 0.830
2025-03-07 19:01:57,990 - INFO - Beginning epoch 70/800
2025-03-07 19:01:58,001 - INFO - training batch 1, loss: 0.690, 32/60000 datapoints
2025-03-07 19:01:58,387 - INFO - training batch 51, loss: 0.680, 1632/60000 datapoints
2025-03-07 19:01:58,944 - INFO - training batch 101, loss: 0.793, 3232/60000 datapoints
2025-03-07 19:01:59,748 - INFO - training batch 151, loss: 0.821, 4832/60000 datapoints
2025-03-07 19:02:00,962 - INFO - training batch 201, loss: 0.759, 6432/60000 datapoints
2025-03-07 19:02:01,427 - INFO - training batch 251, loss: 0.635, 8032/60000 datapoints
2025-03-07 19:02:01,801 - INFO - training batch 301, loss: 1.112, 9632/60000 datapoints
2025-03-07 19:02:02,220 - INFO - training batch 351, loss: 0.748, 11232/60000 datapoints
2025-03-07 19:02:02,614 - INFO - training batch 401, loss: 0.693, 12832/60000 datapoints
2025-03-07 19:02:03,209 - INFO - training batch 451, loss: 0.938, 14432/60000 datapoints
2025-03-07 19:02:04,115 - INFO - training batch 501, loss: 0.784, 16032/60000 datapoints
2025-03-07 19:02:05,067 - INFO - training batch 551, loss: 0.701, 17632/60000 datapoints
2025-03-07 19:02:06,047 - INFO - training batch 601, loss: 0.715, 19232/60000 datapoints
2025-03-07 19:02:10,035 - INFO - training batch 651, loss: 0.842, 20832/60000 datapoints
2025-03-07 19:02:11,780 - INFO - training batch 701, loss: 0.924, 22432/60000 datapoints
2025-03-07 19:02:12,804 - INFO - training batch 751, loss: 0.921, 24032/60000 datapoints
2025-03-07 19:02:13,414 - INFO - training batch 801, loss: 0.791, 25632/60000 datapoints
2025-03-07 19:02:15,004 - INFO - training batch 851, loss: 0.931, 27232/60000 datapoints
2025-03-07 19:02:15,598 - INFO - training batch 901, loss: 0.802, 28832/60000 datapoints
2025-03-07 19:02:16,160 - INFO - training batch 951, loss: 0.778, 30432/60000 datapoints
2025-03-07 19:02:16,868 - INFO - training batch 1001, loss: 0.867, 32032/60000 datapoints
2025-03-07 19:02:17,383 - INFO - training batch 1051, loss: 0.586, 33632/60000 datapoints
2025-03-07 19:02:17,859 - INFO - training batch 1101, loss: 0.849, 35232/60000 datapoints
2025-03-07 19:02:18,390 - INFO - training batch 1151, loss: 0.747, 36832/60000 datapoints
2025-03-07 19:02:18,823 - INFO - training batch 1201, loss: 0.770, 38432/60000 datapoints
2025-03-07 19:02:19,243 - INFO - training batch 1251, loss: 0.416, 40032/60000 datapoints
2025-03-07 19:02:19,636 - INFO - training batch 1301, loss: 0.966, 41632/60000 datapoints
2025-03-07 19:02:20,023 - INFO - training batch 1351, loss: 0.690, 43232/60000 datapoints
2025-03-07 19:02:20,375 - INFO - training batch 1401, loss: 0.869, 44832/60000 datapoints
2025-03-07 19:02:20,720 - INFO - training batch 1451, loss: 0.792, 46432/60000 datapoints
2025-03-07 19:02:21,064 - INFO - training batch 1501, loss: 0.833, 48032/60000 datapoints
2025-03-07 19:02:21,391 - INFO - training batch 1551, loss: 0.908, 49632/60000 datapoints
2025-03-07 19:02:21,722 - INFO - training batch 1601, loss: 0.798, 51232/60000 datapoints
2025-03-07 19:02:22,048 - INFO - training batch 1651, loss: 0.494, 52832/60000 datapoints
2025-03-07 19:02:22,374 - INFO - training batch 1701, loss: 1.054, 54432/60000 datapoints
2025-03-07 19:02:22,653 - INFO - training batch 1751, loss: 0.655, 56032/60000 datapoints
2025-03-07 19:02:22,954 - INFO - training batch 1801, loss: 0.850, 57632/60000 datapoints
2025-03-07 19:02:23,247 - INFO - training batch 1851, loss: 0.758, 59232/60000 datapoints
2025-03-07 19:02:23,430 - INFO - validation batch 1, loss: 0.716, 32/10016 datapoints
2025-03-07 19:02:23,683 - INFO - validation batch 51, loss: 0.731, 1632/10016 datapoints
2025-03-07 19:02:24,168 - INFO - validation batch 101, loss: 0.884, 3232/10016 datapoints
2025-03-07 19:02:24,575 - INFO - validation batch 151, loss: 0.552, 4832/10016 datapoints
2025-03-07 19:02:24,820 - INFO - validation batch 201, loss: 0.518, 6432/10016 datapoints
2025-03-07 19:02:25,099 - INFO - validation batch 251, loss: 0.977, 8032/10016 datapoints
2025-03-07 19:02:25,375 - INFO - validation batch 301, loss: 0.612, 9632/10016 datapoints
2025-03-07 19:02:25,430 - INFO - Epoch 70/800 done.
2025-03-07 19:02:25,432 - INFO - Final validation performance:
Loss: 0.713, top-1 acc: 0.831top-5 acc: 0.831
2025-03-07 19:02:25,436 - INFO - Beginning epoch 71/800
2025-03-07 19:02:25,449 - INFO - training batch 1, loss: 0.856, 32/60000 datapoints
2025-03-07 19:02:25,806 - INFO - training batch 51, loss: 0.671, 1632/60000 datapoints
2025-03-07 19:02:26,128 - INFO - training batch 101, loss: 0.719, 3232/60000 datapoints
2025-03-07 19:02:26,443 - INFO - training batch 151, loss: 0.740, 4832/60000 datapoints
2025-03-07 19:02:26,746 - INFO - training batch 201, loss: 0.519, 6432/60000 datapoints
2025-03-07 19:02:27,212 - INFO - training batch 251, loss: 0.475, 8032/60000 datapoints
2025-03-07 19:02:27,650 - INFO - training batch 301, loss: 0.808, 9632/60000 datapoints
2025-03-07 19:02:28,057 - INFO - training batch 351, loss: 0.692, 11232/60000 datapoints
2025-03-07 19:02:28,454 - INFO - training batch 401, loss: 0.890, 12832/60000 datapoints
2025-03-07 19:02:28,883 - INFO - training batch 451, loss: 0.688, 14432/60000 datapoints
2025-03-07 19:02:29,333 - INFO - training batch 501, loss: 0.918, 16032/60000 datapoints
2025-03-07 19:02:29,778 - INFO - training batch 551, loss: 0.882, 17632/60000 datapoints
2025-03-07 19:02:30,240 - INFO - training batch 601, loss: 0.655, 19232/60000 datapoints
2025-03-07 19:02:30,731 - INFO - training batch 651, loss: 0.841, 20832/60000 datapoints
2025-03-07 19:02:31,229 - INFO - training batch 701, loss: 0.695, 22432/60000 datapoints
2025-03-07 19:02:31,726 - INFO - training batch 751, loss: 0.731, 24032/60000 datapoints
2025-03-07 19:02:32,199 - INFO - training batch 801, loss: 0.789, 25632/60000 datapoints
2025-03-07 19:02:32,644 - INFO - training batch 851, loss: 0.857, 27232/60000 datapoints
2025-03-07 19:02:33,065 - INFO - training batch 901, loss: 0.844, 28832/60000 datapoints
2025-03-07 19:02:33,405 - INFO - training batch 951, loss: 0.588, 30432/60000 datapoints
2025-03-07 19:02:33,770 - INFO - training batch 1001, loss: 0.673, 32032/60000 datapoints
2025-03-07 19:02:34,147 - INFO - training batch 1051, loss: 0.747, 33632/60000 datapoints
2025-03-07 19:02:34,512 - INFO - training batch 1101, loss: 0.726, 35232/60000 datapoints
2025-03-07 19:02:34,880 - INFO - training batch 1151, loss: 0.647, 36832/60000 datapoints
2025-03-07 19:02:35,251 - INFO - training batch 1201, loss: 0.936, 38432/60000 datapoints
2025-03-07 19:02:35,653 - INFO - training batch 1251, loss: 0.854, 40032/60000 datapoints
2025-03-07 19:02:36,042 - INFO - training batch 1301, loss: 0.716, 41632/60000 datapoints
2025-03-07 19:02:36,391 - INFO - training batch 1351, loss: 0.708, 43232/60000 datapoints
2025-03-07 19:02:36,750 - INFO - training batch 1401, loss: 0.698, 44832/60000 datapoints
2025-03-07 19:02:37,112 - INFO - training batch 1451, loss: 0.738, 46432/60000 datapoints
2025-03-07 19:02:37,426 - INFO - training batch 1501, loss: 0.939, 48032/60000 datapoints
2025-03-07 19:02:37,756 - INFO - training batch 1551, loss: 0.627, 49632/60000 datapoints
2025-03-07 19:02:38,061 - INFO - training batch 1601, loss: 0.767, 51232/60000 datapoints
2025-03-07 19:02:38,366 - INFO - training batch 1651, loss: 0.608, 52832/60000 datapoints
2025-03-07 19:02:38,675 - INFO - training batch 1701, loss: 0.602, 54432/60000 datapoints
2025-03-07 19:02:39,050 - INFO - training batch 1751, loss: 0.676, 56032/60000 datapoints
2025-03-07 19:02:39,459 - INFO - training batch 1801, loss: 0.933, 57632/60000 datapoints
2025-03-07 19:02:40,328 - INFO - training batch 1851, loss: 0.782, 59232/60000 datapoints
2025-03-07 19:02:41,203 - INFO - validation batch 1, loss: 0.749, 32/10016 datapoints
2025-03-07 19:02:41,871 - INFO - validation batch 51, loss: 0.709, 1632/10016 datapoints
