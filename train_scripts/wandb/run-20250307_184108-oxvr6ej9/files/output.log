2025-03-07 18:41:14,677 - INFO - Running hyperparameter combination 1 of 1
2025-03-07 18:41:14,677 - INFO - 0_CTCNet_TC_none
2025-03-07 18:41:14,678 - INFO - Loading data...
2025-03-07 18:41:14,840 - INFO - Done loading.
2025-03-07 18:41:14,840 - INFO - Building model and optimiser...
2025-03-07 18:41:14,850 - INFO - =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Sequential: 1-1                        --
|    └─Linear: 2-1                       1,040
|    └─ReLU: 2-2                         --
├─Sequential: 1-2                        --
|    └─Linear: 2-3                       25,120
|    └─ReLU: 2-4                         --
├─Sequential: 1-3                        --
|    └─Linear: 2-5                       1,056
|    └─ReLU: 2-6                         --
├─Sequential: 1-4                        --
|    └─Linear: 2-7                       330
=================================================================
Total params: 27,546
Trainable params: 27,546
Non-trainable params: 0
=================================================================
2025-03-07 18:41:14,851 - INFO - Done.
2025-03-07 18:41:14,851 - INFO - Training...
2025-03-07 18:41:14,851 - INFO - Beginning epoch 1/800
2025-03-07 18:41:14,995 - INFO - training batch 1, loss: 2.357, 32/60000 datapoints
2025-03-07 18:41:15,665 - INFO - training batch 51, loss: 2.326, 1632/60000 datapoints
2025-03-07 18:41:16,101 - INFO - training batch 101, loss: 2.309, 3232/60000 datapoints
2025-03-07 18:41:16,581 - INFO - training batch 151, loss: 2.289, 4832/60000 datapoints
2025-03-07 18:41:17,424 - INFO - training batch 201, loss: 2.336, 6432/60000 datapoints
2025-03-07 18:41:17,922 - INFO - training batch 251, loss: 2.312, 8032/60000 datapoints
2025-03-07 18:41:18,340 - INFO - training batch 301, loss: 2.318, 9632/60000 datapoints
2025-03-07 18:41:18,714 - INFO - training batch 351, loss: 2.319, 11232/60000 datapoints
2025-03-07 18:41:19,239 - INFO - training batch 401, loss: 2.300, 12832/60000 datapoints
2025-03-07 18:41:20,227 - INFO - training batch 451, loss: 2.312, 14432/60000 datapoints
2025-03-07 18:41:20,791 - INFO - training batch 501, loss: 2.320, 16032/60000 datapoints
2025-03-07 18:41:21,216 - INFO - training batch 551, loss: 2.304, 17632/60000 datapoints
2025-03-07 18:41:21,573 - INFO - training batch 601, loss: 2.334, 19232/60000 datapoints
2025-03-07 18:41:22,013 - INFO - training batch 651, loss: 2.324, 20832/60000 datapoints
2025-03-07 18:41:22,855 - INFO - training batch 701, loss: 2.326, 22432/60000 datapoints
2025-03-07 18:41:23,954 - INFO - training batch 751, loss: 2.303, 24032/60000 datapoints
2025-03-07 18:41:26,649 - INFO - training batch 801, loss: 2.295, 25632/60000 datapoints
2025-03-07 18:41:28,634 - INFO - training batch 851, loss: 2.290, 27232/60000 datapoints
2025-03-07 18:41:29,816 - INFO - training batch 901, loss: 2.309, 28832/60000 datapoints
2025-03-07 18:41:30,580 - INFO - training batch 951, loss: 2.329, 30432/60000 datapoints
2025-03-07 18:41:31,699 - INFO - training batch 1001, loss: 2.316, 32032/60000 datapoints
2025-03-07 18:41:33,217 - INFO - training batch 1051, loss: 2.316, 33632/60000 datapoints
2025-03-07 18:41:34,572 - INFO - training batch 1101, loss: 2.283, 35232/60000 datapoints
2025-03-07 18:41:35,759 - INFO - training batch 1151, loss: 2.298, 36832/60000 datapoints
2025-03-07 18:41:36,941 - INFO - training batch 1201, loss: 2.294, 38432/60000 datapoints
2025-03-07 18:41:38,195 - INFO - training batch 1251, loss: 2.304, 40032/60000 datapoints
2025-03-07 18:41:39,228 - INFO - training batch 1301, loss: 2.294, 41632/60000 datapoints
2025-03-07 18:41:39,978 - INFO - training batch 1351, loss: 2.313, 43232/60000 datapoints
2025-03-07 18:41:41,005 - INFO - training batch 1401, loss: 2.272, 44832/60000 datapoints
2025-03-07 18:41:41,790 - INFO - training batch 1451, loss: 2.313, 46432/60000 datapoints
2025-03-07 18:41:42,386 - INFO - training batch 1501, loss: 2.321, 48032/60000 datapoints
2025-03-07 18:41:42,891 - INFO - training batch 1551, loss: 2.309, 49632/60000 datapoints
2025-03-07 18:41:43,396 - INFO - training batch 1601, loss: 2.298, 51232/60000 datapoints
2025-03-07 18:41:43,860 - INFO - training batch 1651, loss: 2.332, 52832/60000 datapoints
2025-03-07 18:41:44,310 - INFO - training batch 1701, loss: 2.319, 54432/60000 datapoints
2025-03-07 18:41:44,792 - INFO - training batch 1751, loss: 2.285, 56032/60000 datapoints
2025-03-07 18:41:45,253 - INFO - training batch 1801, loss: 2.322, 57632/60000 datapoints
2025-03-07 18:41:45,823 - INFO - training batch 1851, loss: 2.327, 59232/60000 datapoints
2025-03-07 18:41:46,312 - INFO - validation batch 1, loss: 2.286, 32/10016 datapoints
2025-03-07 18:41:46,708 - INFO - validation batch 51, loss: 2.309, 1632/10016 datapoints
2025-03-07 18:41:47,048 - INFO - validation batch 101, loss: 2.274, 3232/10016 datapoints
2025-03-07 18:41:47,525 - INFO - validation batch 151, loss: 2.270, 4832/10016 datapoints
2025-03-07 18:41:47,937 - INFO - validation batch 201, loss: 2.326, 6432/10016 datapoints
2025-03-07 18:41:48,310 - INFO - validation batch 251, loss: 2.266, 8032/10016 datapoints
2025-03-07 18:41:48,678 - INFO - validation batch 301, loss: 2.313, 9632/10016 datapoints
2025-03-07 18:41:48,750 - INFO - Epoch 1/800 done.
2025-03-07 18:41:48,764 - INFO - Final validation performance:
Loss: 2.292, top-1 acc: 0.098top-5 acc: 0.098
2025-03-07 18:41:48,786 - INFO - Beginning epoch 2/800
2025-03-07 18:41:48,804 - INFO - training batch 1, loss: 2.299, 32/60000 datapoints
2025-03-07 18:41:49,289 - INFO - training batch 51, loss: 2.309, 1632/60000 datapoints
2025-03-07 18:41:49,709 - INFO - training batch 101, loss: 2.320, 3232/60000 datapoints
2025-03-07 18:41:50,133 - INFO - training batch 151, loss: 2.302, 4832/60000 datapoints
2025-03-07 18:41:50,550 - INFO - training batch 201, loss: 2.295, 6432/60000 datapoints
2025-03-07 18:41:50,935 - INFO - training batch 251, loss: 2.275, 8032/60000 datapoints
2025-03-07 18:41:51,314 - INFO - training batch 301, loss: 2.315, 9632/60000 datapoints
2025-03-07 18:41:51,622 - INFO - training batch 351, loss: 2.322, 11232/60000 datapoints
2025-03-07 18:41:51,932 - INFO - training batch 401, loss: 2.262, 12832/60000 datapoints
2025-03-07 18:41:52,366 - INFO - training batch 451, loss: 2.287, 14432/60000 datapoints
2025-03-07 18:41:52,765 - INFO - training batch 501, loss: 2.290, 16032/60000 datapoints
2025-03-07 18:41:53,232 - INFO - training batch 551, loss: 2.277, 17632/60000 datapoints
2025-03-07 18:41:53,912 - INFO - training batch 601, loss: 2.292, 19232/60000 datapoints
2025-03-07 18:41:54,484 - INFO - training batch 651, loss: 2.294, 20832/60000 datapoints
2025-03-07 18:41:55,307 - INFO - training batch 701, loss: 2.322, 22432/60000 datapoints
2025-03-07 18:41:56,374 - INFO - training batch 751, loss: 2.314, 24032/60000 datapoints
2025-03-07 18:41:56,833 - INFO - training batch 801, loss: 2.308, 25632/60000 datapoints
2025-03-07 18:41:59,131 - INFO - training batch 851, loss: 2.326, 27232/60000 datapoints
2025-03-07 18:42:00,278 - INFO - training batch 901, loss: 2.313, 28832/60000 datapoints
2025-03-07 18:42:00,802 - INFO - training batch 951, loss: 2.289, 30432/60000 datapoints
2025-03-07 18:42:01,304 - INFO - training batch 1001, loss: 2.264, 32032/60000 datapoints
2025-03-07 18:42:01,767 - INFO - training batch 1051, loss: 2.277, 33632/60000 datapoints
2025-03-07 18:42:02,167 - INFO - training batch 1101, loss: 2.272, 35232/60000 datapoints
2025-03-07 18:42:02,521 - INFO - training batch 1151, loss: 2.253, 36832/60000 datapoints
2025-03-07 18:42:03,039 - INFO - training batch 1201, loss: 2.272, 38432/60000 datapoints
2025-03-07 18:42:03,529 - INFO - training batch 1251, loss: 2.292, 40032/60000 datapoints
2025-03-07 18:42:04,088 - INFO - training batch 1301, loss: 2.267, 41632/60000 datapoints
2025-03-07 18:42:04,755 - INFO - training batch 1351, loss: 2.310, 43232/60000 datapoints
2025-03-07 18:42:05,341 - INFO - training batch 1401, loss: 2.309, 44832/60000 datapoints
2025-03-07 18:42:05,771 - INFO - training batch 1451, loss: 2.268, 46432/60000 datapoints
2025-03-07 18:42:06,220 - INFO - training batch 1501, loss: 2.281, 48032/60000 datapoints
2025-03-07 18:42:06,745 - INFO - training batch 1551, loss: 2.279, 49632/60000 datapoints
2025-03-07 18:42:07,156 - INFO - training batch 1601, loss: 2.262, 51232/60000 datapoints
2025-03-07 18:42:07,522 - INFO - training batch 1651, loss: 2.284, 52832/60000 datapoints
2025-03-07 18:42:08,017 - INFO - training batch 1701, loss: 2.291, 54432/60000 datapoints
2025-03-07 18:42:08,533 - INFO - training batch 1751, loss: 2.306, 56032/60000 datapoints
2025-03-07 18:42:08,940 - INFO - training batch 1801, loss: 2.318, 57632/60000 datapoints
2025-03-07 18:42:09,351 - INFO - training batch 1851, loss: 2.287, 59232/60000 datapoints
2025-03-07 18:42:09,596 - INFO - validation batch 1, loss: 2.254, 32/10016 datapoints
2025-03-07 18:42:09,943 - INFO - validation batch 51, loss: 2.272, 1632/10016 datapoints
2025-03-07 18:42:10,312 - INFO - validation batch 101, loss: 2.311, 3232/10016 datapoints
2025-03-07 18:42:10,669 - INFO - validation batch 151, loss: 2.305, 4832/10016 datapoints
2025-03-07 18:42:11,006 - INFO - validation batch 201, loss: 2.284, 6432/10016 datapoints
2025-03-07 18:42:11,287 - INFO - validation batch 251, loss: 2.300, 8032/10016 datapoints
2025-03-07 18:42:11,556 - INFO - validation batch 301, loss: 2.259, 9632/10016 datapoints
2025-03-07 18:42:11,625 - INFO - Epoch 2/800 done.
2025-03-07 18:42:11,626 - INFO - Final validation performance:
Loss: 2.283, top-1 acc: 0.110top-5 acc: 0.110
2025-03-07 18:42:11,629 - INFO - Beginning epoch 3/800
2025-03-07 18:42:11,641 - INFO - training batch 1, loss: 2.274, 32/60000 datapoints
2025-03-07 18:42:11,998 - INFO - training batch 51, loss: 2.268, 1632/60000 datapoints
2025-03-07 18:42:12,309 - INFO - training batch 101, loss: 2.262, 3232/60000 datapoints
2025-03-07 18:42:12,619 - INFO - training batch 151, loss: 2.259, 4832/60000 datapoints
2025-03-07 18:42:12,939 - INFO - training batch 201, loss: 2.292, 6432/60000 datapoints
2025-03-07 18:42:13,238 - INFO - training batch 251, loss: 2.281, 8032/60000 datapoints
2025-03-07 18:42:13,555 - INFO - training batch 301, loss: 2.277, 9632/60000 datapoints
2025-03-07 18:42:13,878 - INFO - training batch 351, loss: 2.321, 11232/60000 datapoints
2025-03-07 18:42:14,233 - INFO - training batch 401, loss: 2.280, 12832/60000 datapoints
2025-03-07 18:42:14,585 - INFO - training batch 451, loss: 2.251, 14432/60000 datapoints
2025-03-07 18:42:14,978 - INFO - training batch 501, loss: 2.307, 16032/60000 datapoints
2025-03-07 18:42:15,287 - INFO - training batch 551, loss: 2.295, 17632/60000 datapoints
2025-03-07 18:42:15,576 - INFO - training batch 601, loss: 2.256, 19232/60000 datapoints
2025-03-07 18:42:15,866 - INFO - training batch 651, loss: 2.283, 20832/60000 datapoints
2025-03-07 18:42:16,172 - INFO - training batch 701, loss: 2.301, 22432/60000 datapoints
2025-03-07 18:42:16,525 - INFO - training batch 751, loss: 2.288, 24032/60000 datapoints
2025-03-07 18:42:16,931 - INFO - training batch 801, loss: 2.253, 25632/60000 datapoints
2025-03-07 18:42:17,307 - INFO - training batch 851, loss: 2.279, 27232/60000 datapoints
2025-03-07 18:42:17,649 - INFO - training batch 901, loss: 2.290, 28832/60000 datapoints
2025-03-07 18:42:18,041 - INFO - training batch 951, loss: 2.307, 30432/60000 datapoints
2025-03-07 18:42:18,396 - INFO - training batch 1001, loss: 2.302, 32032/60000 datapoints
2025-03-07 18:42:18,851 - INFO - training batch 1051, loss: 2.256, 33632/60000 datapoints
2025-03-07 18:42:19,204 - INFO - training batch 1101, loss: 2.280, 35232/60000 datapoints
2025-03-07 18:42:19,610 - INFO - training batch 1151, loss: 2.272, 36832/60000 datapoints
2025-03-07 18:42:20,023 - INFO - training batch 1201, loss: 2.281, 38432/60000 datapoints
2025-03-07 18:42:20,476 - INFO - training batch 1251, loss: 2.307, 40032/60000 datapoints
2025-03-07 18:42:21,657 - INFO - training batch 1301, loss: 2.259, 41632/60000 datapoints
2025-03-07 18:42:22,765 - INFO - training batch 1351, loss: 2.257, 43232/60000 datapoints
2025-03-07 18:42:23,732 - INFO - training batch 1401, loss: 2.268, 44832/60000 datapoints
2025-03-07 18:42:29,084 - INFO - training batch 1451, loss: 2.271, 46432/60000 datapoints
2025-03-07 18:42:32,811 - INFO - training batch 1501, loss: 2.295, 48032/60000 datapoints
2025-03-07 18:42:35,891 - INFO - training batch 1551, loss: 2.258, 49632/60000 datapoints
2025-03-07 18:42:36,693 - INFO - training batch 1601, loss: 2.257, 51232/60000 datapoints
2025-03-07 18:42:37,277 - INFO - training batch 1651, loss: 2.281, 52832/60000 datapoints
2025-03-07 18:42:37,969 - INFO - training batch 1701, loss: 2.255, 54432/60000 datapoints
2025-03-07 18:42:38,562 - INFO - training batch 1751, loss: 2.277, 56032/60000 datapoints
2025-03-07 18:42:39,144 - INFO - training batch 1801, loss: 2.253, 57632/60000 datapoints
2025-03-07 18:42:40,341 - INFO - training batch 1851, loss: 2.309, 59232/60000 datapoints
2025-03-07 18:42:41,269 - INFO - validation batch 1, loss: 2.282, 32/10016 datapoints
2025-03-07 18:42:42,467 - INFO - validation batch 51, loss: 2.261, 1632/10016 datapoints
2025-03-07 18:42:43,233 - INFO - validation batch 101, loss: 2.248, 3232/10016 datapoints
2025-03-07 18:42:43,797 - INFO - validation batch 151, loss: 2.259, 4832/10016 datapoints
2025-03-07 18:42:44,400 - INFO - validation batch 201, loss: 2.266, 6432/10016 datapoints
2025-03-07 18:42:44,950 - INFO - validation batch 251, loss: 2.270, 8032/10016 datapoints
2025-03-07 18:42:45,899 - INFO - validation batch 301, loss: 2.266, 9632/10016 datapoints
2025-03-07 18:42:46,180 - INFO - Epoch 3/800 done.
2025-03-07 18:42:46,183 - INFO - Final validation performance:
Loss: 2.264, top-1 acc: 0.169top-5 acc: 0.169
2025-03-07 18:42:46,232 - INFO - Beginning epoch 4/800
2025-03-07 18:42:46,323 - INFO - training batch 1, loss: 2.250, 32/60000 datapoints
2025-03-07 18:42:49,407 - INFO - training batch 51, loss: 2.293, 1632/60000 datapoints
2025-03-07 18:42:51,063 - INFO - training batch 101, loss: 2.277, 3232/60000 datapoints
2025-03-07 18:42:51,667 - INFO - training batch 151, loss: 2.263, 4832/60000 datapoints
2025-03-07 18:42:52,652 - INFO - training batch 201, loss: 2.263, 6432/60000 datapoints
2025-03-07 18:42:53,635 - INFO - training batch 251, loss: 2.301, 8032/60000 datapoints
2025-03-07 18:42:55,169 - INFO - training batch 301, loss: 2.278, 9632/60000 datapoints
2025-03-07 18:42:55,810 - INFO - training batch 351, loss: 2.278, 11232/60000 datapoints
2025-03-07 18:42:56,771 - INFO - training batch 401, loss: 2.280, 12832/60000 datapoints
2025-03-07 18:42:57,484 - INFO - training batch 451, loss: 2.236, 14432/60000 datapoints
2025-03-07 18:42:58,129 - INFO - training batch 501, loss: 2.258, 16032/60000 datapoints
2025-03-07 18:42:58,450 - INFO - training batch 551, loss: 2.248, 17632/60000 datapoints
2025-03-07 18:42:58,747 - INFO - training batch 601, loss: 2.255, 19232/60000 datapoints
2025-03-07 18:42:59,089 - INFO - training batch 651, loss: 2.250, 20832/60000 datapoints
2025-03-07 18:42:59,460 - INFO - training batch 701, loss: 2.245, 22432/60000 datapoints
2025-03-07 18:42:59,790 - INFO - training batch 751, loss: 2.269, 24032/60000 datapoints
2025-03-07 18:43:00,158 - INFO - training batch 801, loss: 2.246, 25632/60000 datapoints
2025-03-07 18:43:00,518 - INFO - training batch 851, loss: 2.253, 27232/60000 datapoints
2025-03-07 18:43:00,854 - INFO - training batch 901, loss: 2.256, 28832/60000 datapoints
2025-03-07 18:43:01,205 - INFO - training batch 951, loss: 2.266, 30432/60000 datapoints
2025-03-07 18:43:01,508 - INFO - training batch 1001, loss: 2.250, 32032/60000 datapoints
2025-03-07 18:43:01,828 - INFO - training batch 1051, loss: 2.270, 33632/60000 datapoints
2025-03-07 18:43:02,160 - INFO - training batch 1101, loss: 2.237, 35232/60000 datapoints
2025-03-07 18:43:02,456 - INFO - training batch 1151, loss: 2.256, 36832/60000 datapoints
2025-03-07 18:43:02,743 - INFO - training batch 1201, loss: 2.280, 38432/60000 datapoints
2025-03-07 18:43:03,037 - INFO - training batch 1251, loss: 2.280, 40032/60000 datapoints
2025-03-07 18:43:03,351 - INFO - training batch 1301, loss: 2.256, 41632/60000 datapoints
2025-03-07 18:43:03,642 - INFO - training batch 1351, loss: 2.276, 43232/60000 datapoints
2025-03-07 18:43:03,952 - INFO - training batch 1401, loss: 2.296, 44832/60000 datapoints
2025-03-07 18:43:04,317 - INFO - training batch 1451, loss: 2.226, 46432/60000 datapoints
2025-03-07 18:43:04,695 - INFO - training batch 1501, loss: 2.285, 48032/60000 datapoints
2025-03-07 18:43:05,044 - INFO - training batch 1551, loss: 2.259, 49632/60000 datapoints
2025-03-07 18:43:05,482 - INFO - training batch 1601, loss: 2.254, 51232/60000 datapoints
2025-03-07 18:43:05,871 - INFO - training batch 1651, loss: 2.250, 52832/60000 datapoints
2025-03-07 18:43:06,253 - INFO - training batch 1701, loss: 2.238, 54432/60000 datapoints
2025-03-07 18:43:06,631 - INFO - training batch 1751, loss: 2.267, 56032/60000 datapoints
2025-03-07 18:43:07,004 - INFO - training batch 1801, loss: 2.255, 57632/60000 datapoints
2025-03-07 18:43:07,415 - INFO - training batch 1851, loss: 2.240, 59232/60000 datapoints
2025-03-07 18:43:07,693 - INFO - validation batch 1, loss: 2.266, 32/10016 datapoints
2025-03-07 18:43:08,150 - INFO - validation batch 51, loss: 2.231, 1632/10016 datapoints
2025-03-07 18:43:08,482 - INFO - validation batch 101, loss: 2.285, 3232/10016 datapoints
