2025-03-06 18:37:18,967 - INFO - Running hyperparameter combination 1 of 1
2025-03-06 18:37:18,968 - INFO - 0_CTCNet_TC_none
2025-03-06 18:37:18,969 - INFO - Loading data...
2025-03-06 18:37:19,103 - INFO - Done loading.
2025-03-06 18:37:19,103 - INFO - Building model and optimiser...
2025-03-06 18:37:19,113 - INFO - =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Sequential: 1-1                        --
|    └─Linear: 2-1                       1,040
|    └─ReLU: 2-2                         --
├─Sequential: 1-2                        --
|    └─Linear: 2-3                       25,120
|    └─ReLU: 2-4                         --
├─Sequential: 1-3                        --
|    └─Linear: 2-5                       1,056
|    └─ReLU: 2-6                         --
├─Sequential: 1-4                        --
|    └─Linear: 2-7                       330
=================================================================
Total params: 27,546
Trainable params: 27,546
Non-trainable params: 0
=================================================================
2025-03-06 18:37:19,114 - INFO - Done.
2025-03-06 18:37:19,114 - INFO - Training...
2025-03-06 18:37:19,114 - INFO - Beginning epoch 1/800
2025-03-06 18:37:19,196 - INFO - training batch 1, loss: 2.303, 32/60000 datapoints
2025-03-06 18:37:19,502 - INFO - training batch 51, loss: 2.269, 1632/60000 datapoints
2025-03-06 18:37:19,788 - INFO - training batch 101, loss: 2.344, 3232/60000 datapoints
2025-03-06 18:37:20,091 - INFO - training batch 151, loss: 2.331, 4832/60000 datapoints
2025-03-06 18:37:20,374 - INFO - training batch 201, loss: 2.333, 6432/60000 datapoints
2025-03-06 18:37:20,667 - INFO - training batch 251, loss: 2.335, 8032/60000 datapoints
2025-03-06 18:37:20,896 - INFO - training batch 301, loss: 2.303, 9632/60000 datapoints
2025-03-06 18:37:21,130 - INFO - training batch 351, loss: 2.285, 11232/60000 datapoints
2025-03-06 18:37:21,377 - INFO - training batch 401, loss: 2.310, 12832/60000 datapoints
2025-03-06 18:37:21,943 - INFO - training batch 451, loss: 2.281, 14432/60000 datapoints
2025-03-06 18:37:22,261 - INFO - training batch 501, loss: 2.294, 16032/60000 datapoints
2025-03-06 18:37:22,524 - INFO - training batch 551, loss: 2.305, 17632/60000 datapoints
2025-03-06 18:37:22,814 - INFO - training batch 601, loss: 2.319, 19232/60000 datapoints
2025-03-06 18:37:23,067 - INFO - training batch 651, loss: 2.298, 20832/60000 datapoints
2025-03-06 18:37:23,287 - INFO - training batch 701, loss: 2.304, 22432/60000 datapoints
2025-03-06 18:37:23,496 - INFO - training batch 751, loss: 2.311, 24032/60000 datapoints
2025-03-06 18:37:23,817 - INFO - training batch 801, loss: 2.321, 25632/60000 datapoints
2025-03-06 18:37:24,034 - INFO - training batch 851, loss: 2.292, 27232/60000 datapoints
2025-03-06 18:37:24,247 - INFO - training batch 901, loss: 2.321, 28832/60000 datapoints
2025-03-06 18:37:24,455 - INFO - training batch 951, loss: 2.315, 30432/60000 datapoints
2025-03-06 18:37:24,670 - INFO - training batch 1001, loss: 2.308, 32032/60000 datapoints
2025-03-06 18:37:24,932 - INFO - training batch 1051, loss: 2.309, 33632/60000 datapoints
2025-03-06 18:37:25,206 - INFO - training batch 1101, loss: 2.310, 35232/60000 datapoints
2025-03-06 18:37:25,460 - INFO - training batch 1151, loss: 2.311, 36832/60000 datapoints
2025-03-06 18:37:25,752 - INFO - training batch 1201, loss: 2.304, 38432/60000 datapoints
2025-03-06 18:37:26,008 - INFO - training batch 1251, loss: 2.298, 40032/60000 datapoints
2025-03-06 18:37:26,285 - INFO - training batch 1301, loss: 2.329, 41632/60000 datapoints
2025-03-06 18:37:26,575 - INFO - training batch 1351, loss: 2.318, 43232/60000 datapoints
2025-03-06 18:37:26,967 - INFO - training batch 1401, loss: 2.322, 44832/60000 datapoints
2025-03-06 18:37:27,315 - INFO - training batch 1451, loss: 2.268, 46432/60000 datapoints
2025-03-06 18:37:27,586 - INFO - training batch 1501, loss: 2.277, 48032/60000 datapoints
2025-03-06 18:37:27,841 - INFO - training batch 1551, loss: 2.294, 49632/60000 datapoints
2025-03-06 18:37:28,075 - INFO - training batch 1601, loss: 2.305, 51232/60000 datapoints
2025-03-06 18:37:28,402 - INFO - training batch 1651, loss: 2.305, 52832/60000 datapoints
2025-03-06 18:37:28,710 - INFO - training batch 1701, loss: 2.303, 54432/60000 datapoints
2025-03-06 18:37:29,004 - INFO - training batch 1751, loss: 2.292, 56032/60000 datapoints
2025-03-06 18:37:29,268 - INFO - training batch 1801, loss: 2.298, 57632/60000 datapoints
2025-03-06 18:37:29,503 - INFO - training batch 1851, loss: 2.303, 59232/60000 datapoints
2025-03-06 18:37:29,643 - INFO - validation batch 1, loss: 2.318, 32/10016 datapoints
2025-03-06 18:37:29,831 - INFO - validation batch 51, loss: 2.263, 1632/10016 datapoints
2025-03-06 18:37:30,053 - INFO - validation batch 101, loss: 2.288, 3232/10016 datapoints
2025-03-06 18:37:30,265 - INFO - validation batch 151, loss: 2.311, 4832/10016 datapoints
2025-03-06 18:37:30,454 - INFO - validation batch 201, loss: 2.309, 6432/10016 datapoints
2025-03-06 18:37:30,644 - INFO - validation batch 251, loss: 2.307, 8032/10016 datapoints
2025-03-06 18:37:30,818 - INFO - validation batch 301, loss: 2.264, 9632/10016 datapoints
2025-03-06 18:37:30,860 - INFO - Epoch 1/800 done.
2025-03-06 18:37:30,860 - INFO - Final validation performance:
Loss: 2.294, top-1 acc: 0.098top-5 acc: 0.098
2025-03-06 18:37:30,861 - INFO - Beginning epoch 2/800
2025-03-06 18:37:30,869 - INFO - training batch 1, loss: 2.341, 32/60000 datapoints
2025-03-06 18:37:31,098 - INFO - training batch 51, loss: 2.312, 1632/60000 datapoints
2025-03-06 18:37:31,328 - INFO - training batch 101, loss: 2.310, 3232/60000 datapoints
2025-03-06 18:37:31,548 - INFO - training batch 151, loss: 2.303, 4832/60000 datapoints
2025-03-06 18:37:31,769 - INFO - training batch 201, loss: 2.305, 6432/60000 datapoints
2025-03-06 18:37:31,989 - INFO - training batch 251, loss: 2.323, 8032/60000 datapoints
2025-03-06 18:37:32,217 - INFO - training batch 301, loss: 2.332, 9632/60000 datapoints
2025-03-06 18:37:32,443 - INFO - training batch 351, loss: 2.290, 11232/60000 datapoints
2025-03-06 18:37:32,680 - INFO - training batch 401, loss: 2.298, 12832/60000 datapoints
2025-03-06 18:37:32,913 - INFO - training batch 451, loss: 2.292, 14432/60000 datapoints
2025-03-06 18:37:33,166 - INFO - training batch 501, loss: 2.319, 16032/60000 datapoints
2025-03-06 18:37:33,414 - INFO - training batch 551, loss: 2.268, 17632/60000 datapoints
2025-03-06 18:37:33,665 - INFO - training batch 601, loss: 2.299, 19232/60000 datapoints
2025-03-06 18:37:33,957 - INFO - training batch 651, loss: 2.300, 20832/60000 datapoints
2025-03-06 18:37:34,360 - INFO - training batch 701, loss: 2.299, 22432/60000 datapoints
2025-03-06 18:37:34,652 - INFO - training batch 751, loss: 2.274, 24032/60000 datapoints
2025-03-06 18:37:34,949 - INFO - training batch 801, loss: 2.302, 25632/60000 datapoints
2025-03-06 18:37:35,269 - INFO - training batch 851, loss: 2.288, 27232/60000 datapoints
2025-03-06 18:37:35,703 - INFO - training batch 901, loss: 2.314, 28832/60000 datapoints
2025-03-06 18:37:36,150 - INFO - training batch 951, loss: 2.324, 30432/60000 datapoints
2025-03-06 18:37:36,425 - INFO - training batch 1001, loss: 2.306, 32032/60000 datapoints
2025-03-06 18:37:36,739 - INFO - training batch 1051, loss: 2.299, 33632/60000 datapoints
2025-03-06 18:37:37,042 - INFO - training batch 1101, loss: 2.287, 35232/60000 datapoints
2025-03-06 18:37:37,404 - INFO - training batch 1151, loss: 2.285, 36832/60000 datapoints
2025-03-06 18:37:37,681 - INFO - training batch 1201, loss: 2.281, 38432/60000 datapoints
2025-03-06 18:37:37,948 - INFO - training batch 1251, loss: 2.260, 40032/60000 datapoints
2025-03-06 18:37:38,225 - INFO - training batch 1301, loss: 2.312, 41632/60000 datapoints
2025-03-06 18:37:38,489 - INFO - training batch 1351, loss: 2.287, 43232/60000 datapoints
2025-03-06 18:37:38,765 - INFO - training batch 1401, loss: 2.299, 44832/60000 datapoints
2025-03-06 18:37:39,062 - INFO - training batch 1451, loss: 2.300, 46432/60000 datapoints
2025-03-06 18:37:39,336 - INFO - training batch 1501, loss: 2.292, 48032/60000 datapoints
2025-03-06 18:37:39,643 - INFO - training batch 1551, loss: 2.278, 49632/60000 datapoints
2025-03-06 18:37:39,924 - INFO - training batch 1601, loss: 2.296, 51232/60000 datapoints
2025-03-06 18:37:40,241 - INFO - training batch 1651, loss: 2.288, 52832/60000 datapoints
2025-03-06 18:37:40,508 - INFO - training batch 1701, loss: 2.299, 54432/60000 datapoints
2025-03-06 18:37:40,801 - INFO - training batch 1751, loss: 2.256, 56032/60000 datapoints
2025-03-06 18:37:41,230 - INFO - training batch 1801, loss: 2.289, 57632/60000 datapoints
2025-03-06 18:37:41,477 - INFO - training batch 1851, loss: 2.258, 59232/60000 datapoints
2025-03-06 18:37:41,602 - INFO - validation batch 1, loss: 2.253, 32/10016 datapoints
2025-03-06 18:37:41,812 - INFO - validation batch 51, loss: 2.292, 1632/10016 datapoints
2025-03-06 18:37:42,029 - INFO - validation batch 101, loss: 2.283, 3232/10016 datapoints
2025-03-06 18:37:42,268 - INFO - validation batch 151, loss: 2.296, 4832/10016 datapoints
2025-03-06 18:37:42,483 - INFO - validation batch 201, loss: 2.297, 6432/10016 datapoints
2025-03-06 18:37:42,709 - INFO - validation batch 251, loss: 2.291, 8032/10016 datapoints
2025-03-06 18:37:42,935 - INFO - validation batch 301, loss: 2.301, 9632/10016 datapoints
2025-03-06 18:37:42,994 - INFO - Epoch 2/800 done.
2025-03-06 18:37:42,994 - INFO - Final validation performance:
Loss: 2.288, top-1 acc: 0.115top-5 acc: 0.115
2025-03-06 18:37:42,995 - INFO - Beginning epoch 3/800
2025-03-06 18:37:43,007 - INFO - training batch 1, loss: 2.255, 32/60000 datapoints
2025-03-06 18:37:43,325 - INFO - training batch 51, loss: 2.304, 1632/60000 datapoints
2025-03-06 18:37:43,684 - INFO - training batch 101, loss: 2.308, 3232/60000 datapoints
2025-03-06 18:37:43,961 - INFO - training batch 151, loss: 2.317, 4832/60000 datapoints
2025-03-06 18:37:44,220 - INFO - training batch 201, loss: 2.269, 6432/60000 datapoints
2025-03-06 18:37:44,501 - INFO - training batch 251, loss: 2.276, 8032/60000 datapoints
2025-03-06 18:37:44,804 - INFO - training batch 301, loss: 2.287, 9632/60000 datapoints
2025-03-06 18:37:45,070 - INFO - training batch 351, loss: 2.278, 11232/60000 datapoints
2025-03-06 18:37:45,351 - INFO - training batch 401, loss: 2.309, 12832/60000 datapoints
2025-03-06 18:37:45,625 - INFO - training batch 451, loss: 2.313, 14432/60000 datapoints
2025-03-06 18:37:45,903 - INFO - training batch 501, loss: 2.298, 16032/60000 datapoints
2025-03-06 18:37:46,196 - INFO - training batch 551, loss: 2.279, 17632/60000 datapoints
2025-03-06 18:37:46,434 - INFO - training batch 601, loss: 2.302, 19232/60000 datapoints
2025-03-06 18:37:46,649 - INFO - training batch 651, loss: 2.314, 20832/60000 datapoints
2025-03-06 18:37:46,904 - INFO - training batch 701, loss: 2.277, 22432/60000 datapoints
2025-03-06 18:37:47,118 - INFO - training batch 751, loss: 2.276, 24032/60000 datapoints
2025-03-06 18:37:47,323 - INFO - training batch 801, loss: 2.282, 25632/60000 datapoints
2025-03-06 18:37:47,530 - INFO - training batch 851, loss: 2.294, 27232/60000 datapoints
2025-03-06 18:37:47,744 - INFO - training batch 901, loss: 2.287, 28832/60000 datapoints
2025-03-06 18:37:47,962 - INFO - training batch 951, loss: 2.248, 30432/60000 datapoints
2025-03-06 18:37:48,183 - INFO - training batch 1001, loss: 2.280, 32032/60000 datapoints
2025-03-06 18:37:48,416 - INFO - training batch 1051, loss: 2.255, 33632/60000 datapoints
2025-03-06 18:37:48,652 - INFO - training batch 1101, loss: 2.293, 35232/60000 datapoints
2025-03-06 18:37:48,878 - INFO - training batch 1151, loss: 2.244, 36832/60000 datapoints
2025-03-06 18:37:49,107 - INFO - training batch 1201, loss: 2.257, 38432/60000 datapoints
2025-03-06 18:37:49,331 - INFO - training batch 1251, loss: 2.293, 40032/60000 datapoints
2025-03-06 18:37:49,559 - INFO - training batch 1301, loss: 2.268, 41632/60000 datapoints
2025-03-06 18:37:49,775 - INFO - training batch 1351, loss: 2.262, 43232/60000 datapoints
2025-03-06 18:37:49,975 - INFO - training batch 1401, loss: 2.286, 44832/60000 datapoints
2025-03-06 18:37:50,181 - INFO - training batch 1451, loss: 2.274, 46432/60000 datapoints
2025-03-06 18:37:50,389 - INFO - training batch 1501, loss: 2.263, 48032/60000 datapoints
2025-03-06 18:37:50,623 - INFO - training batch 1551, loss: 2.263, 49632/60000 datapoints
2025-03-06 18:37:50,854 - INFO - training batch 1601, loss: 2.261, 51232/60000 datapoints
2025-03-06 18:37:51,106 - INFO - training batch 1651, loss: 2.267, 52832/60000 datapoints
2025-03-06 18:37:51,453 - INFO - training batch 1701, loss: 2.253, 54432/60000 datapoints
2025-03-06 18:37:51,702 - INFO - training batch 1751, loss: 2.273, 56032/60000 datapoints
2025-03-06 18:37:51,948 - INFO - training batch 1801, loss: 2.291, 57632/60000 datapoints
2025-03-06 18:37:52,170 - INFO - training batch 1851, loss: 2.264, 59232/60000 datapoints
2025-03-06 18:37:52,322 - INFO - validation batch 1, loss: 2.284, 32/10016 datapoints
2025-03-06 18:37:52,619 - INFO - validation batch 51, loss: 2.273, 1632/10016 datapoints
2025-03-06 18:37:52,960 - INFO - validation batch 101, loss: 2.270, 3232/10016 datapoints
2025-03-06 18:37:53,188 - INFO - validation batch 151, loss: 2.288, 4832/10016 datapoints
2025-03-06 18:37:53,418 - INFO - validation batch 201, loss: 2.256, 6432/10016 datapoints
2025-03-06 18:37:53,642 - INFO - validation batch 251, loss: 2.252, 8032/10016 datapoints
2025-03-06 18:37:53,835 - INFO - validation batch 301, loss: 2.307, 9632/10016 datapoints
2025-03-06 18:37:53,953 - INFO - Epoch 3/800 done.
2025-03-06 18:37:53,953 - INFO - Final validation performance:
Loss: 2.276, top-1 acc: 0.147top-5 acc: 0.147
2025-03-06 18:37:53,954 - INFO - Beginning epoch 4/800
2025-03-06 18:37:53,988 - INFO - training batch 1, loss: 2.265, 32/60000 datapoints
2025-03-06 18:37:54,304 - INFO - training batch 51, loss: 2.267, 1632/60000 datapoints
2025-03-06 18:37:54,578 - INFO - training batch 101, loss: 2.269, 3232/60000 datapoints
2025-03-06 18:37:54,820 - INFO - training batch 151, loss: 2.298, 4832/60000 datapoints
2025-03-06 18:37:55,134 - INFO - training batch 201, loss: 2.285, 6432/60000 datapoints
2025-03-06 18:37:55,421 - INFO - training batch 251, loss: 2.265, 8032/60000 datapoints
2025-03-06 18:37:55,720 - INFO - training batch 301, loss: 2.303, 9632/60000 datapoints
2025-03-06 18:37:56,025 - INFO - training batch 351, loss: 2.263, 11232/60000 datapoints
2025-03-06 18:37:56,291 - INFO - training batch 401, loss: 2.296, 12832/60000 datapoints
2025-03-06 18:37:56,567 - INFO - training batch 451, loss: 2.259, 14432/60000 datapoints
2025-03-06 18:37:56,802 - INFO - training batch 501, loss: 2.269, 16032/60000 datapoints
2025-03-06 18:37:57,071 - INFO - training batch 551, loss: 2.288, 17632/60000 datapoints
2025-03-06 18:37:57,350 - INFO - training batch 601, loss: 2.265, 19232/60000 datapoints
2025-03-06 18:37:57,562 - INFO - training batch 651, loss: 2.250, 20832/60000 datapoints
2025-03-06 18:37:57,783 - INFO - training batch 701, loss: 2.292, 22432/60000 datapoints
2025-03-06 18:37:58,022 - INFO - training batch 751, loss: 2.256, 24032/60000 datapoints
2025-03-06 18:37:58,253 - INFO - training batch 801, loss: 2.282, 25632/60000 datapoints
2025-03-06 18:37:58,476 - INFO - training batch 851, loss: 2.282, 27232/60000 datapoints
2025-03-06 18:37:58,716 - INFO - training batch 901, loss: 2.291, 28832/60000 datapoints
2025-03-06 18:37:58,939 - INFO - training batch 951, loss: 2.254, 30432/60000 datapoints
2025-03-06 18:37:59,162 - INFO - training batch 1001, loss: 2.242, 32032/60000 datapoints
2025-03-06 18:37:59,386 - INFO - training batch 1051, loss: 2.285, 33632/60000 datapoints
2025-03-06 18:37:59,635 - INFO - training batch 1101, loss: 2.243, 35232/60000 datapoints
2025-03-06 18:37:59,908 - INFO - training batch 1151, loss: 2.264, 36832/60000 datapoints
2025-03-06 18:38:00,185 - INFO - training batch 1201, loss: 2.256, 38432/60000 datapoints
2025-03-06 18:38:00,412 - INFO - training batch 1251, loss: 2.260, 40032/60000 datapoints
2025-03-06 18:38:00,670 - INFO - training batch 1301, loss: 2.248, 41632/60000 datapoints
2025-03-06 18:38:00,956 - INFO - training batch 1351, loss: 2.238, 43232/60000 datapoints
2025-03-06 18:38:01,231 - INFO - training batch 1401, loss: 2.253, 44832/60000 datapoints
2025-03-06 18:38:01,481 - INFO - training batch 1451, loss: 2.254, 46432/60000 datapoints
2025-03-06 18:38:01,706 - INFO - training batch 1501, loss: 2.250, 48032/60000 datapoints
2025-03-06 18:38:01,919 - INFO - training batch 1551, loss: 2.256, 49632/60000 datapoints
2025-03-06 18:38:02,145 - INFO - training batch 1601, loss: 2.280, 51232/60000 datapoints
2025-03-06 18:38:02,388 - INFO - training batch 1651, loss: 2.253, 52832/60000 datapoints
2025-03-06 18:38:02,623 - INFO - training batch 1701, loss: 2.260, 54432/60000 datapoints
2025-03-06 18:38:02,889 - INFO - training batch 1751, loss: 2.254, 56032/60000 datapoints
2025-03-06 18:38:03,114 - INFO - training batch 1801, loss: 2.258, 57632/60000 datapoints
2025-03-06 18:38:03,337 - INFO - training batch 1851, loss: 2.282, 59232/60000 datapoints
2025-03-06 18:38:03,447 - INFO - validation batch 1, loss: 2.276, 32/10016 datapoints
2025-03-06 18:38:03,626 - INFO - validation batch 51, loss: 2.247, 1632/10016 datapoints
2025-03-06 18:38:03,815 - INFO - validation batch 101, loss: 2.248, 3232/10016 datapoints
2025-03-06 18:38:04,006 - INFO - validation batch 151, loss: 2.259, 4832/10016 datapoints
2025-03-06 18:38:04,183 - INFO - validation batch 201, loss: 2.223, 6432/10016 datapoints
2025-03-06 18:38:04,364 - INFO - validation batch 251, loss: 2.265, 8032/10016 datapoints
2025-03-06 18:38:04,555 - INFO - validation batch 301, loss: 2.255, 9632/10016 datapoints
2025-03-06 18:38:04,603 - INFO - Epoch 4/800 done.
2025-03-06 18:38:04,603 - INFO - Final validation performance:
Loss: 2.253, top-1 acc: 0.172top-5 acc: 0.172
2025-03-06 18:38:04,605 - INFO - Beginning epoch 5/800
2025-03-06 18:38:04,612 - INFO - training batch 1, loss: 2.260, 32/60000 datapoints
2025-03-06 18:38:04,939 - INFO - training batch 51, loss: 2.241, 1632/60000 datapoints
2025-03-06 18:38:05,181 - INFO - training batch 101, loss: 2.242, 3232/60000 datapoints
2025-03-06 18:38:05,412 - INFO - training batch 151, loss: 2.232, 4832/60000 datapoints
2025-03-06 18:38:05,644 - INFO - training batch 201, loss: 2.246, 6432/60000 datapoints
2025-03-06 18:38:05,883 - INFO - training batch 251, loss: 2.260, 8032/60000 datapoints
2025-03-06 18:38:06,112 - INFO - training batch 301, loss: 2.244, 9632/60000 datapoints
2025-03-06 18:38:06,347 - INFO - training batch 351, loss: 2.259, 11232/60000 datapoints
2025-03-06 18:38:06,589 - INFO - training batch 401, loss: 2.276, 12832/60000 datapoints
2025-03-06 18:38:06,830 - INFO - training batch 451, loss: 2.268, 14432/60000 datapoints
2025-03-06 18:38:07,068 - INFO - training batch 501, loss: 2.241, 16032/60000 datapoints
2025-03-06 18:38:07,309 - INFO - training batch 551, loss: 2.213, 17632/60000 datapoints
2025-03-06 18:38:07,521 - INFO - training batch 601, loss: 2.266, 19232/60000 datapoints
2025-03-06 18:38:07,746 - INFO - training batch 651, loss: 2.235, 20832/60000 datapoints
2025-03-06 18:38:07,968 - INFO - training batch 701, loss: 2.234, 22432/60000 datapoints
2025-03-06 18:38:08,175 - INFO - training batch 751, loss: 2.245, 24032/60000 datapoints
2025-03-06 18:38:08,382 - INFO - training batch 801, loss: 2.245, 25632/60000 datapoints
2025-03-06 18:38:08,581 - INFO - training batch 851, loss: 2.225, 27232/60000 datapoints
2025-03-06 18:38:08,785 - INFO - training batch 901, loss: 2.278, 28832/60000 datapoints
2025-03-06 18:38:08,993 - INFO - training batch 951, loss: 2.229, 30432/60000 datapoints
2025-03-06 18:38:09,195 - INFO - training batch 1001, loss: 2.243, 32032/60000 datapoints
2025-03-06 18:38:09,396 - INFO - training batch 1051, loss: 2.248, 33632/60000 datapoints
2025-03-06 18:38:09,590 - INFO - training batch 1101, loss: 2.255, 35232/60000 datapoints
2025-03-06 18:38:09,785 - INFO - training batch 1151, loss: 2.252, 36832/60000 datapoints
2025-03-06 18:38:09,981 - INFO - training batch 1201, loss: 2.225, 38432/60000 datapoints
2025-03-06 18:38:10,177 - INFO - training batch 1251, loss: 2.245, 40032/60000 datapoints
2025-03-06 18:38:10,373 - INFO - training batch 1301, loss: 2.264, 41632/60000 datapoints
2025-03-06 18:38:10,586 - INFO - training batch 1351, loss: 2.218, 43232/60000 datapoints
2025-03-06 18:38:10,839 - INFO - training batch 1401, loss: 2.220, 44832/60000 datapoints
2025-03-06 18:38:11,057 - INFO - training batch 1451, loss: 2.236, 46432/60000 datapoints
2025-03-06 18:38:11,286 - INFO - training batch 1501, loss: 2.234, 48032/60000 datapoints
2025-03-06 18:38:11,508 - INFO - training batch 1551, loss: 2.266, 49632/60000 datapoints
2025-03-06 18:38:11,724 - INFO - training batch 1601, loss: 2.229, 51232/60000 datapoints
2025-03-06 18:38:11,933 - INFO - training batch 1651, loss: 2.249, 52832/60000 datapoints
2025-03-06 18:38:12,162 - INFO - training batch 1701, loss: 2.227, 54432/60000 datapoints
2025-03-06 18:38:12,386 - INFO - training batch 1751, loss: 2.248, 56032/60000 datapoints
2025-03-06 18:38:12,620 - INFO - training batch 1801, loss: 2.247, 57632/60000 datapoints
2025-03-06 18:38:12,853 - INFO - training batch 1851, loss: 2.214, 59232/60000 datapoints
2025-03-06 18:38:12,984 - INFO - validation batch 1, loss: 2.202, 32/10016 datapoints
2025-03-06 18:38:13,197 - INFO - validation batch 51, loss: 2.225, 1632/10016 datapoints
2025-03-06 18:38:13,411 - INFO - validation batch 101, loss: 2.213, 3232/10016 datapoints
2025-03-06 18:38:13,588 - INFO - validation batch 151, loss: 2.251, 4832/10016 datapoints
2025-03-06 18:38:13,791 - INFO - validation batch 201, loss: 2.207, 6432/10016 datapoints
2025-03-06 18:38:13,978 - INFO - validation batch 251, loss: 2.258, 8032/10016 datapoints
2025-03-06 18:38:14,156 - INFO - validation batch 301, loss: 2.262, 9632/10016 datapoints
2025-03-06 18:38:14,203 - INFO - Epoch 5/800 done.
2025-03-06 18:38:14,203 - INFO - Final validation performance:
Loss: 2.231, top-1 acc: 0.212top-5 acc: 0.212
2025-03-06 18:38:14,204 - INFO - Beginning epoch 6/800
2025-03-06 18:38:14,212 - INFO - training batch 1, loss: 2.240, 32/60000 datapoints
2025-03-06 18:38:14,442 - INFO - training batch 51, loss: 2.265, 1632/60000 datapoints
2025-03-06 18:38:14,656 - INFO - training batch 101, loss: 2.230, 3232/60000 datapoints
2025-03-06 18:38:14,920 - INFO - training batch 151, loss: 2.269, 4832/60000 datapoints
2025-03-06 18:38:15,186 - INFO - training batch 201, loss: 2.233, 6432/60000 datapoints
2025-03-06 18:38:15,405 - INFO - training batch 251, loss: 2.241, 8032/60000 datapoints
2025-03-06 18:38:15,640 - INFO - training batch 301, loss: 2.243, 9632/60000 datapoints
2025-03-06 18:38:15,870 - INFO - training batch 351, loss: 2.219, 11232/60000 datapoints
2025-03-06 18:38:16,111 - INFO - training batch 401, loss: 2.243, 12832/60000 datapoints
2025-03-06 18:38:16,341 - INFO - training batch 451, loss: 2.243, 14432/60000 datapoints
2025-03-06 18:38:16,562 - INFO - training batch 501, loss: 2.227, 16032/60000 datapoints
2025-03-06 18:38:16,782 - INFO - training batch 551, loss: 2.226, 17632/60000 datapoints
2025-03-06 18:38:16,992 - INFO - training batch 601, loss: 2.262, 19232/60000 datapoints
2025-03-06 18:38:17,237 - INFO - training batch 651, loss: 2.265, 20832/60000 datapoints
2025-03-06 18:38:17,461 - INFO - training batch 701, loss: 2.234, 22432/60000 datapoints
2025-03-06 18:38:17,691 - INFO - training batch 751, loss: 2.212, 24032/60000 datapoints
2025-03-06 18:38:17,912 - INFO - training batch 801, loss: 2.214, 25632/60000 datapoints
2025-03-06 18:38:18,135 - INFO - training batch 851, loss: 2.222, 27232/60000 datapoints
2025-03-06 18:38:18,350 - INFO - training batch 901, loss: 2.230, 28832/60000 datapoints
2025-03-06 18:38:18,577 - INFO - training batch 951, loss: 2.225, 30432/60000 datapoints
2025-03-06 18:38:18,788 - INFO - training batch 1001, loss: 2.258, 32032/60000 datapoints
2025-03-06 18:38:19,005 - INFO - training batch 1051, loss: 2.242, 33632/60000 datapoints
2025-03-06 18:38:19,208 - INFO - training batch 1101, loss: 2.226, 35232/60000 datapoints
2025-03-06 18:38:19,419 - INFO - training batch 1151, loss: 2.198, 36832/60000 datapoints
2025-03-06 18:38:19,625 - INFO - training batch 1201, loss: 2.206, 38432/60000 datapoints
2025-03-06 18:38:19,829 - INFO - training batch 1251, loss: 2.195, 40032/60000 datapoints
2025-03-06 18:38:20,026 - INFO - training batch 1301, loss: 2.223, 41632/60000 datapoints
2025-03-06 18:38:20,226 - INFO - training batch 1351, loss: 2.231, 43232/60000 datapoints
2025-03-06 18:38:20,432 - INFO - training batch 1401, loss: 2.207, 44832/60000 datapoints
2025-03-06 18:38:20,643 - INFO - training batch 1451, loss: 2.223, 46432/60000 datapoints
2025-03-06 18:38:20,868 - INFO - training batch 1501, loss: 2.257, 48032/60000 datapoints
2025-03-06 18:38:21,089 - INFO - training batch 1551, loss: 2.211, 49632/60000 datapoints
2025-03-06 18:38:21,363 - INFO - training batch 1601, loss: 2.198, 51232/60000 datapoints
2025-03-06 18:38:21,659 - INFO - training batch 1651, loss: 2.214, 52832/60000 datapoints
2025-03-06 18:38:21,890 - INFO - training batch 1701, loss: 2.206, 54432/60000 datapoints
2025-03-06 18:38:22,115 - INFO - training batch 1751, loss: 2.183, 56032/60000 datapoints
2025-03-06 18:38:22,344 - INFO - training batch 1801, loss: 2.223, 57632/60000 datapoints
2025-03-06 18:38:22,565 - INFO - training batch 1851, loss: 2.212, 59232/60000 datapoints
2025-03-06 18:38:22,685 - INFO - validation batch 1, loss: 2.225, 32/10016 datapoints
2025-03-06 18:38:22,862 - INFO - validation batch 51, loss: 2.223, 1632/10016 datapoints
2025-03-06 18:38:23,040 - INFO - validation batch 101, loss: 2.230, 3232/10016 datapoints
2025-03-06 18:38:23,212 - INFO - validation batch 151, loss: 2.224, 4832/10016 datapoints
2025-03-06 18:38:23,383 - INFO - validation batch 201, loss: 2.207, 6432/10016 datapoints
2025-03-06 18:38:23,569 - INFO - validation batch 251, loss: 2.168, 8032/10016 datapoints
2025-03-06 18:38:23,816 - INFO - validation batch 301, loss: 2.203, 9632/10016 datapoints
2025-03-06 18:38:23,867 - INFO - Epoch 6/800 done.
2025-03-06 18:38:23,867 - INFO - Final validation performance:
Loss: 2.212, top-1 acc: 0.239top-5 acc: 0.239
2025-03-06 18:38:23,868 - INFO - Beginning epoch 7/800
2025-03-06 18:38:23,875 - INFO - training batch 1, loss: 2.183, 32/60000 datapoints
2025-03-06 18:38:24,125 - INFO - training batch 51, loss: 2.219, 1632/60000 datapoints
2025-03-06 18:38:24,375 - INFO - training batch 101, loss: 2.276, 3232/60000 datapoints
2025-03-06 18:38:24,598 - INFO - training batch 151, loss: 2.233, 4832/60000 datapoints
2025-03-06 18:38:24,841 - INFO - training batch 201, loss: 2.192, 6432/60000 datapoints
2025-03-06 18:38:25,065 - INFO - training batch 251, loss: 2.204, 8032/60000 datapoints
2025-03-06 18:38:25,304 - INFO - training batch 301, loss: 2.238, 9632/60000 datapoints
2025-03-06 18:38:25,516 - INFO - training batch 351, loss: 2.224, 11232/60000 datapoints
2025-03-06 18:38:25,728 - INFO - training batch 401, loss: 2.227, 12832/60000 datapoints
2025-03-06 18:38:25,936 - INFO - training batch 451, loss: 2.215, 14432/60000 datapoints
2025-03-06 18:38:26,154 - INFO - training batch 501, loss: 2.218, 16032/60000 datapoints
2025-03-06 18:38:26,366 - INFO - training batch 551, loss: 2.221, 17632/60000 datapoints
2025-03-06 18:38:26,572 - INFO - training batch 601, loss: 2.190, 19232/60000 datapoints
2025-03-06 18:38:26,799 - INFO - training batch 651, loss: 2.229, 20832/60000 datapoints
2025-03-06 18:38:27,015 - INFO - training batch 701, loss: 2.223, 22432/60000 datapoints
2025-03-06 18:38:27,241 - INFO - training batch 751, loss: 2.212, 24032/60000 datapoints
2025-03-06 18:38:27,452 - INFO - training batch 801, loss: 2.185, 25632/60000 datapoints
2025-03-06 18:38:27,664 - INFO - training batch 851, loss: 2.202, 27232/60000 datapoints
2025-03-06 18:38:27,871 - INFO - training batch 901, loss: 2.200, 28832/60000 datapoints
2025-03-06 18:38:28,100 - INFO - training batch 951, loss: 2.177, 30432/60000 datapoints
2025-03-06 18:38:28,300 - INFO - training batch 1001, loss: 2.205, 32032/60000 datapoints
2025-03-06 18:38:28,517 - INFO - training batch 1051, loss: 2.226, 33632/60000 datapoints
2025-03-06 18:38:28,736 - INFO - training batch 1101, loss: 2.164, 35232/60000 datapoints
2025-03-06 18:38:28,958 - INFO - training batch 1151, loss: 2.189, 36832/60000 datapoints
2025-03-06 18:38:29,181 - INFO - training batch 1201, loss: 2.173, 38432/60000 datapoints
2025-03-06 18:38:29,418 - INFO - training batch 1251, loss: 2.178, 40032/60000 datapoints
2025-03-06 18:38:29,665 - INFO - training batch 1301, loss: 2.219, 41632/60000 datapoints
2025-03-06 18:38:29,880 - INFO - training batch 1351, loss: 2.181, 43232/60000 datapoints
2025-03-06 18:38:30,086 - INFO - training batch 1401, loss: 2.211, 44832/60000 datapoints
2025-03-06 18:38:30,308 - INFO - training batch 1451, loss: 2.177, 46432/60000 datapoints
2025-03-06 18:38:30,506 - INFO - training batch 1501, loss: 2.208, 48032/60000 datapoints
2025-03-06 18:38:30,709 - INFO - training batch 1551, loss: 2.179, 49632/60000 datapoints
2025-03-06 18:38:30,913 - INFO - training batch 1601, loss: 2.227, 51232/60000 datapoints
2025-03-06 18:38:31,124 - INFO - training batch 1651, loss: 2.209, 52832/60000 datapoints
2025-03-06 18:38:31,335 - INFO - training batch 1701, loss: 2.154, 54432/60000 datapoints
2025-03-06 18:38:31,538 - INFO - training batch 1751, loss: 2.239, 56032/60000 datapoints
2025-03-06 18:38:31,745 - INFO - training batch 1801, loss: 2.157, 57632/60000 datapoints
2025-03-06 18:38:31,955 - INFO - training batch 1851, loss: 2.197, 59232/60000 datapoints
2025-03-06 18:38:32,058 - INFO - validation batch 1, loss: 2.196, 32/10016 datapoints
2025-03-06 18:38:32,214 - INFO - validation batch 51, loss: 2.167, 1632/10016 datapoints
2025-03-06 18:38:32,374 - INFO - validation batch 101, loss: 2.197, 3232/10016 datapoints
2025-03-06 18:38:32,540 - INFO - validation batch 151, loss: 2.173, 4832/10016 datapoints
2025-03-06 18:38:32,887 - INFO - validation batch 201, loss: 2.189, 6432/10016 datapoints
2025-03-06 18:38:33,055 - INFO - validation batch 251, loss: 2.206, 8032/10016 datapoints
2025-03-06 18:38:33,218 - INFO - validation batch 301, loss: 2.185, 9632/10016 datapoints
2025-03-06 18:38:33,257 - INFO - Epoch 7/800 done.
2025-03-06 18:38:33,257 - INFO - Final validation performance:
Loss: 2.188, top-1 acc: 0.258top-5 acc: 0.258
2025-03-06 18:38:33,258 - INFO - Beginning epoch 8/800
2025-03-06 18:38:33,265 - INFO - training batch 1, loss: 2.203, 32/60000 datapoints
2025-03-06 18:38:33,488 - INFO - training batch 51, loss: 2.179, 1632/60000 datapoints
2025-03-06 18:38:33,698 - INFO - training batch 101, loss: 2.263, 3232/60000 datapoints
2025-03-06 18:38:33,911 - INFO - training batch 151, loss: 2.193, 4832/60000 datapoints
2025-03-06 18:38:34,124 - INFO - training batch 201, loss: 2.197, 6432/60000 datapoints
2025-03-06 18:38:34,335 - INFO - training batch 251, loss: 2.201, 8032/60000 datapoints
2025-03-06 18:38:34,543 - INFO - training batch 301, loss: 2.205, 9632/60000 datapoints
2025-03-06 18:38:34,752 - INFO - training batch 351, loss: 2.199, 11232/60000 datapoints
2025-03-06 18:38:34,965 - INFO - training batch 401, loss: 2.174, 12832/60000 datapoints
2025-03-06 18:38:35,174 - INFO - training batch 451, loss: 2.181, 14432/60000 datapoints
2025-03-06 18:38:35,382 - INFO - training batch 501, loss: 2.184, 16032/60000 datapoints
2025-03-06 18:38:35,592 - INFO - training batch 551, loss: 2.203, 17632/60000 datapoints
2025-03-06 18:38:35,798 - INFO - training batch 601, loss: 2.191, 19232/60000 datapoints
2025-03-06 18:38:36,003 - INFO - training batch 651, loss: 2.242, 20832/60000 datapoints
2025-03-06 18:38:36,207 - INFO - training batch 701, loss: 2.178, 22432/60000 datapoints
2025-03-06 18:38:36,414 - INFO - training batch 751, loss: 2.202, 24032/60000 datapoints
2025-03-06 18:38:36,630 - INFO - training batch 801, loss: 2.200, 25632/60000 datapoints
2025-03-06 18:38:36,833 - INFO - training batch 851, loss: 2.202, 27232/60000 datapoints
2025-03-06 18:38:37,038 - INFO - training batch 901, loss: 2.231, 28832/60000 datapoints
2025-03-06 18:38:37,250 - INFO - training batch 951, loss: 2.168, 30432/60000 datapoints
2025-03-06 18:38:37,456 - INFO - training batch 1001, loss: 2.166, 32032/60000 datapoints
2025-03-06 18:38:37,673 - INFO - training batch 1051, loss: 2.178, 33632/60000 datapoints
2025-03-06 18:38:37,870 - INFO - training batch 1101, loss: 2.184, 35232/60000 datapoints
2025-03-06 18:38:38,077 - INFO - training batch 1151, loss: 2.193, 36832/60000 datapoints
2025-03-06 18:38:38,289 - INFO - training batch 1201, loss: 2.183, 38432/60000 datapoints
2025-03-06 18:38:38,491 - INFO - training batch 1251, loss: 2.212, 40032/60000 datapoints
2025-03-06 18:38:38,720 - INFO - training batch 1301, loss: 2.185, 41632/60000 datapoints
2025-03-06 18:38:38,941 - INFO - training batch 1351, loss: 2.166, 43232/60000 datapoints
2025-03-06 18:38:39,166 - INFO - training batch 1401, loss: 2.175, 44832/60000 datapoints
2025-03-06 18:38:39,405 - INFO - training batch 1451, loss: 2.185, 46432/60000 datapoints
2025-03-06 18:38:39,620 - INFO - training batch 1501, loss: 2.158, 48032/60000 datapoints
2025-03-06 18:38:39,845 - INFO - training batch 1551, loss: 2.196, 49632/60000 datapoints
2025-03-06 18:38:40,053 - INFO - training batch 1601, loss: 2.189, 51232/60000 datapoints
2025-03-06 18:38:40,252 - INFO - training batch 1651, loss: 2.160, 52832/60000 datapoints
2025-03-06 18:38:40,456 - INFO - training batch 1701, loss: 2.177, 54432/60000 datapoints
2025-03-06 18:38:40,664 - INFO - training batch 1751, loss: 2.155, 56032/60000 datapoints
2025-03-06 18:38:40,864 - INFO - training batch 1801, loss: 2.159, 57632/60000 datapoints
2025-03-06 18:38:41,068 - INFO - training batch 1851, loss: 2.158, 59232/60000 datapoints
2025-03-06 18:38:41,171 - INFO - validation batch 1, loss: 2.175, 32/10016 datapoints
2025-03-06 18:38:41,329 - INFO - validation batch 51, loss: 2.209, 1632/10016 datapoints
2025-03-06 18:38:41,484 - INFO - validation batch 101, loss: 2.136, 3232/10016 datapoints
2025-03-06 18:38:41,664 - INFO - validation batch 151, loss: 2.201, 4832/10016 datapoints
2025-03-06 18:38:41,829 - INFO - validation batch 201, loss: 2.170, 6432/10016 datapoints
2025-03-06 18:38:41,998 - INFO - validation batch 251, loss: 2.142, 8032/10016 datapoints
2025-03-06 18:38:42,161 - INFO - validation batch 301, loss: 2.153, 9632/10016 datapoints
2025-03-06 18:38:42,205 - INFO - Epoch 8/800 done.
2025-03-06 18:38:42,205 - INFO - Final validation performance:
Loss: 2.170, top-1 acc: 0.274top-5 acc: 0.274
2025-03-06 18:38:42,205 - INFO - Beginning epoch 9/800
2025-03-06 18:38:42,211 - INFO - training batch 1, loss: 2.141, 32/60000 datapoints
2025-03-06 18:38:42,433 - INFO - training batch 51, loss: 2.123, 1632/60000 datapoints
2025-03-06 18:38:42,658 - INFO - training batch 101, loss: 2.163, 3232/60000 datapoints
2025-03-06 18:38:42,900 - INFO - training batch 151, loss: 2.175, 4832/60000 datapoints
2025-03-06 18:38:43,136 - INFO - training batch 201, loss: 2.151, 6432/60000 datapoints
2025-03-06 18:38:43,363 - INFO - training batch 251, loss: 2.155, 8032/60000 datapoints
2025-03-06 18:38:43,567 - INFO - training batch 301, loss: 2.175, 9632/60000 datapoints
2025-03-06 18:38:43,776 - INFO - training batch 351, loss: 2.175, 11232/60000 datapoints
2025-03-06 18:38:43,981 - INFO - training batch 401, loss: 2.202, 12832/60000 datapoints
2025-03-06 18:38:44,197 - INFO - training batch 451, loss: 2.146, 14432/60000 datapoints
2025-03-06 18:38:44,420 - INFO - training batch 501, loss: 2.172, 16032/60000 datapoints
2025-03-06 18:38:44,637 - INFO - training batch 551, loss: 2.163, 17632/60000 datapoints
2025-03-06 18:38:44,865 - INFO - training batch 601, loss: 2.143, 19232/60000 datapoints
2025-03-06 18:38:45,088 - INFO - training batch 651, loss: 2.166, 20832/60000 datapoints
2025-03-06 18:38:45,306 - INFO - training batch 701, loss: 2.169, 22432/60000 datapoints
2025-03-06 18:38:45,533 - INFO - training batch 751, loss: 2.124, 24032/60000 datapoints
2025-03-06 18:38:45,775 - INFO - training batch 801, loss: 2.182, 25632/60000 datapoints
2025-03-06 18:38:45,996 - INFO - training batch 851, loss: 2.136, 27232/60000 datapoints
2025-03-06 18:38:46,209 - INFO - training batch 901, loss: 2.168, 28832/60000 datapoints
2025-03-06 18:38:46,423 - INFO - training batch 951, loss: 2.177, 30432/60000 datapoints
2025-03-06 18:38:46,642 - INFO - training batch 1001, loss: 2.184, 32032/60000 datapoints
2025-03-06 18:38:46,849 - INFO - training batch 1051, loss: 2.152, 33632/60000 datapoints
2025-03-06 18:38:47,097 - INFO - training batch 1101, loss: 2.117, 35232/60000 datapoints
2025-03-06 18:38:47,349 - INFO - training batch 1151, loss: 2.145, 36832/60000 datapoints
2025-03-06 18:38:47,599 - INFO - training batch 1201, loss: 2.131, 38432/60000 datapoints
2025-03-06 18:38:47,849 - INFO - training batch 1251, loss: 2.149, 40032/60000 datapoints
2025-03-06 18:38:48,089 - INFO - training batch 1301, loss: 2.131, 41632/60000 datapoints
2025-03-06 18:38:48,319 - INFO - training batch 1351, loss: 2.131, 43232/60000 datapoints
2025-03-06 18:38:48,536 - INFO - training batch 1401, loss: 2.161, 44832/60000 datapoints
2025-03-06 18:38:48,767 - INFO - training batch 1451, loss: 2.144, 46432/60000 datapoints
2025-03-06 18:38:48,967 - INFO - training batch 1501, loss: 2.122, 48032/60000 datapoints
2025-03-06 18:38:49,173 - INFO - training batch 1551, loss: 2.154, 49632/60000 datapoints
2025-03-06 18:38:49,419 - INFO - training batch 1601, loss: 2.144, 51232/60000 datapoints
2025-03-06 18:38:49,668 - INFO - training batch 1651, loss: 2.165, 52832/60000 datapoints
2025-03-06 18:38:49,905 - INFO - training batch 1701, loss: 2.197, 54432/60000 datapoints
2025-03-06 18:38:50,124 - INFO - training batch 1751, loss: 2.158, 56032/60000 datapoints
2025-03-06 18:38:50,334 - INFO - training batch 1801, loss: 2.134, 57632/60000 datapoints
2025-03-06 18:38:50,580 - INFO - training batch 1851, loss: 2.113, 59232/60000 datapoints
2025-03-06 18:38:50,704 - INFO - validation batch 1, loss: 2.146, 32/10016 datapoints
2025-03-06 18:38:50,870 - INFO - validation batch 51, loss: 2.116, 1632/10016 datapoints
2025-03-06 18:38:51,049 - INFO - validation batch 101, loss: 2.111, 3232/10016 datapoints
2025-03-06 18:38:51,229 - INFO - validation batch 151, loss: 2.123, 4832/10016 datapoints
2025-03-06 18:38:51,404 - INFO - validation batch 201, loss: 2.132, 6432/10016 datapoints
2025-03-06 18:38:51,569 - INFO - validation batch 251, loss: 2.159, 8032/10016 datapoints
2025-03-06 18:38:51,744 - INFO - validation batch 301, loss: 2.162, 9632/10016 datapoints
2025-03-06 18:38:51,787 - INFO - Epoch 9/800 done.
2025-03-06 18:38:51,787 - INFO - Final validation performance:
Loss: 2.135, top-1 acc: 0.293top-5 acc: 0.293
2025-03-06 18:38:51,788 - INFO - Beginning epoch 10/800
2025-03-06 18:38:51,796 - INFO - training batch 1, loss: 2.090, 32/60000 datapoints
2025-03-06 18:38:52,028 - INFO - training batch 51, loss: 2.138, 1632/60000 datapoints
2025-03-06 18:38:52,260 - INFO - training batch 101, loss: 2.170, 3232/60000 datapoints
2025-03-06 18:38:52,476 - INFO - training batch 151, loss: 2.183, 4832/60000 datapoints
2025-03-06 18:38:52,697 - INFO - training batch 201, loss: 2.132, 6432/60000 datapoints
2025-03-06 18:38:52,906 - INFO - training batch 251, loss: 2.152, 8032/60000 datapoints
2025-03-06 18:38:53,115 - INFO - training batch 301, loss: 2.190, 9632/60000 datapoints
2025-03-06 18:38:53,342 - INFO - training batch 351, loss: 2.119, 11232/60000 datapoints
2025-03-06 18:38:53,566 - INFO - training batch 401, loss: 2.149, 12832/60000 datapoints
2025-03-06 18:38:53,833 - INFO - training batch 451, loss: 2.098, 14432/60000 datapoints
2025-03-06 18:38:54,094 - INFO - training batch 501, loss: 2.142, 16032/60000 datapoints
2025-03-06 18:38:54,327 - INFO - training batch 551, loss: 2.133, 17632/60000 datapoints
2025-03-06 18:38:54,592 - INFO - training batch 601, loss: 2.143, 19232/60000 datapoints
2025-03-06 18:38:54,881 - INFO - training batch 651, loss: 2.082, 20832/60000 datapoints
2025-03-06 18:38:55,154 - INFO - training batch 701, loss: 2.134, 22432/60000 datapoints
2025-03-06 18:38:55,459 - INFO - training batch 751, loss: 2.127, 24032/60000 datapoints
2025-03-06 18:38:55,715 - INFO - training batch 801, loss: 2.111, 25632/60000 datapoints
2025-03-06 18:38:55,962 - INFO - training batch 851, loss: 2.159, 27232/60000 datapoints
2025-03-06 18:38:56,222 - INFO - training batch 901, loss: 2.177, 28832/60000 datapoints
2025-03-06 18:38:56,478 - INFO - training batch 951, loss: 2.139, 30432/60000 datapoints
2025-03-06 18:38:56,725 - INFO - training batch 1001, loss: 2.124, 32032/60000 datapoints
2025-03-06 18:38:56,970 - INFO - training batch 1051, loss: 2.030, 33632/60000 datapoints
2025-03-06 18:38:57,256 - INFO - training batch 1101, loss: 2.128, 35232/60000 datapoints
2025-03-06 18:38:57,545 - INFO - training batch 1151, loss: 2.114, 36832/60000 datapoints
2025-03-06 18:38:57,801 - INFO - training batch 1201, loss: 2.097, 38432/60000 datapoints
2025-03-06 18:38:58,025 - INFO - training batch 1251, loss: 2.093, 40032/60000 datapoints
2025-03-06 18:38:58,267 - INFO - training batch 1301, loss: 2.133, 41632/60000 datapoints
2025-03-06 18:38:58,488 - INFO - training batch 1351, loss: 2.039, 43232/60000 datapoints
2025-03-06 18:38:58,706 - INFO - training batch 1401, loss: 2.106, 44832/60000 datapoints
2025-03-06 18:38:58,924 - INFO - training batch 1451, loss: 2.144, 46432/60000 datapoints
2025-03-06 18:38:59,138 - INFO - training batch 1501, loss: 2.076, 48032/60000 datapoints
2025-03-06 18:38:59,352 - INFO - training batch 1551, loss: 2.138, 49632/60000 datapoints
2025-03-06 18:38:59,581 - INFO - training batch 1601, loss: 2.127, 51232/60000 datapoints
2025-03-06 18:38:59,810 - INFO - training batch 1651, loss: 2.109, 52832/60000 datapoints
2025-03-06 18:39:00,038 - INFO - training batch 1701, loss: 2.112, 54432/60000 datapoints
2025-03-06 18:39:00,283 - INFO - training batch 1751, loss: 2.124, 56032/60000 datapoints
2025-03-06 18:39:00,488 - INFO - training batch 1801, loss: 2.074, 57632/60000 datapoints
2025-03-06 18:39:00,692 - INFO - training batch 1851, loss: 2.139, 59232/60000 datapoints
2025-03-06 18:39:00,794 - INFO - validation batch 1, loss: 2.128, 32/10016 datapoints
2025-03-06 18:39:00,952 - INFO - validation batch 51, loss: 2.114, 1632/10016 datapoints
2025-03-06 18:39:01,109 - INFO - validation batch 101, loss: 2.117, 3232/10016 datapoints
2025-03-06 18:39:01,266 - INFO - validation batch 151, loss: 2.089, 4832/10016 datapoints
2025-03-06 18:39:01,422 - INFO - validation batch 201, loss: 2.158, 6432/10016 datapoints
2025-03-06 18:39:01,574 - INFO - validation batch 251, loss: 2.071, 8032/10016 datapoints
2025-03-06 18:39:01,733 - INFO - validation batch 301, loss: 2.112, 9632/10016 datapoints
2025-03-06 18:39:01,772 - INFO - Epoch 10/800 done.
2025-03-06 18:39:01,772 - INFO - Final validation performance:
Loss: 2.113, top-1 acc: 0.323top-5 acc: 0.323
2025-03-06 18:39:01,773 - INFO - Beginning epoch 11/800
2025-03-06 18:39:01,779 - INFO - training batch 1, loss: 2.159, 32/60000 datapoints
2025-03-06 18:39:01,975 - INFO - training batch 51, loss: 2.106, 1632/60000 datapoints
2025-03-06 18:39:02,175 - INFO - training batch 101, loss: 2.104, 3232/60000 datapoints
2025-03-06 18:39:02,380 - INFO - training batch 151, loss: 2.153, 4832/60000 datapoints
2025-03-06 18:39:02,585 - INFO - training batch 201, loss: 2.081, 6432/60000 datapoints
2025-03-06 18:39:02,786 - INFO - training batch 251, loss: 2.014, 8032/60000 datapoints
2025-03-06 18:39:02,989 - INFO - training batch 301, loss: 2.129, 9632/60000 datapoints
2025-03-06 18:39:03,189 - INFO - training batch 351, loss: 2.063, 11232/60000 datapoints
2025-03-06 18:39:03,409 - INFO - training batch 401, loss: 2.078, 12832/60000 datapoints
2025-03-06 18:39:03,609 - INFO - training batch 451, loss: 2.128, 14432/60000 datapoints
2025-03-06 18:39:03,819 - INFO - training batch 501, loss: 2.167, 16032/60000 datapoints
2025-03-06 18:39:04,065 - INFO - training batch 551, loss: 2.099, 17632/60000 datapoints
2025-03-06 18:39:04,309 - INFO - training batch 601, loss: 2.058, 19232/60000 datapoints
2025-03-06 18:39:04,566 - INFO - training batch 651, loss: 2.114, 20832/60000 datapoints
2025-03-06 18:39:04,805 - INFO - training batch 701, loss: 2.144, 22432/60000 datapoints
2025-03-06 18:39:05,029 - INFO - training batch 751, loss: 2.103, 24032/60000 datapoints
2025-03-06 18:39:05,266 - INFO - training batch 801, loss: 2.132, 25632/60000 datapoints
2025-03-06 18:39:05,478 - INFO - training batch 851, loss: 2.116, 27232/60000 datapoints
2025-03-06 18:39:05,695 - INFO - training batch 901, loss: 2.076, 28832/60000 datapoints
2025-03-06 18:39:05,920 - INFO - training batch 951, loss: 2.076, 30432/60000 datapoints
2025-03-06 18:39:06,151 - INFO - training batch 1001, loss: 2.147, 32032/60000 datapoints
2025-03-06 18:39:06,365 - INFO - training batch 1051, loss: 2.068, 33632/60000 datapoints
2025-03-06 18:39:06,599 - INFO - training batch 1101, loss: 2.121, 35232/60000 datapoints
2025-03-06 18:39:06,814 - INFO - training batch 1151, loss: 2.085, 36832/60000 datapoints
2025-03-06 18:39:07,042 - INFO - training batch 1201, loss: 2.123, 38432/60000 datapoints
2025-03-06 18:39:07,268 - INFO - training batch 1251, loss: 2.089, 40032/60000 datapoints
2025-03-06 18:39:07,503 - INFO - training batch 1301, loss: 2.152, 41632/60000 datapoints
2025-03-06 18:39:07,775 - INFO - training batch 1351, loss: 2.086, 43232/60000 datapoints
2025-03-06 18:39:08,053 - INFO - training batch 1401, loss: 2.133, 44832/60000 datapoints
2025-03-06 18:39:08,320 - INFO - training batch 1451, loss: 2.062, 46432/60000 datapoints
2025-03-06 18:39:08,547 - INFO - training batch 1501, loss: 2.054, 48032/60000 datapoints
2025-03-06 18:39:08,826 - INFO - training batch 1551, loss: 2.091, 49632/60000 datapoints
2025-03-06 18:39:09,071 - INFO - training batch 1601, loss: 2.144, 51232/60000 datapoints
2025-03-06 18:39:09,308 - INFO - training batch 1651, loss: 2.126, 52832/60000 datapoints
2025-03-06 18:39:09,531 - INFO - training batch 1701, loss: 2.120, 54432/60000 datapoints
2025-03-06 18:39:09,757 - INFO - training batch 1751, loss: 2.094, 56032/60000 datapoints
2025-03-06 18:39:09,997 - INFO - training batch 1801, loss: 2.055, 57632/60000 datapoints
2025-03-06 18:39:10,213 - INFO - training batch 1851, loss: 2.114, 59232/60000 datapoints
2025-03-06 18:39:10,338 - INFO - validation batch 1, loss: 2.051, 32/10016 datapoints
2025-03-06 18:39:10,505 - INFO - validation batch 51, loss: 2.113, 1632/10016 datapoints
2025-03-06 18:39:10,767 - INFO - validation batch 101, loss: 2.118, 3232/10016 datapoints
2025-03-06 18:39:10,941 - INFO - validation batch 151, loss: 2.111, 4832/10016 datapoints
2025-03-06 18:39:11,109 - INFO - validation batch 201, loss: 2.053, 6432/10016 datapoints
2025-03-06 18:39:11,276 - INFO - validation batch 251, loss: 2.053, 8032/10016 datapoints
2025-03-06 18:39:11,448 - INFO - validation batch 301, loss: 2.076, 9632/10016 datapoints
2025-03-06 18:39:11,491 - INFO - Epoch 11/800 done.
2025-03-06 18:39:11,491 - INFO - Final validation performance:
Loss: 2.082, top-1 acc: 0.358top-5 acc: 0.358
2025-03-06 18:39:11,492 - INFO - Beginning epoch 12/800
2025-03-06 18:39:11,499 - INFO - training batch 1, loss: 2.059, 32/60000 datapoints
2025-03-06 18:39:11,756 - INFO - training batch 51, loss: 2.092, 1632/60000 datapoints
2025-03-06 18:39:12,026 - INFO - training batch 101, loss: 2.103, 3232/60000 datapoints
2025-03-06 18:39:12,229 - INFO - training batch 151, loss: 2.099, 4832/60000 datapoints
2025-03-06 18:39:12,437 - INFO - training batch 201, loss: 2.059, 6432/60000 datapoints
2025-03-06 18:39:12,662 - INFO - training batch 251, loss: 2.036, 8032/60000 datapoints
2025-03-06 18:39:12,885 - INFO - training batch 301, loss: 2.103, 9632/60000 datapoints
2025-03-06 18:39:13,089 - INFO - training batch 351, loss: 2.108, 11232/60000 datapoints
2025-03-06 18:39:13,287 - INFO - training batch 401, loss: 2.125, 12832/60000 datapoints
2025-03-06 18:39:13,492 - INFO - training batch 451, loss: 2.045, 14432/60000 datapoints
2025-03-06 18:39:13,699 - INFO - training batch 501, loss: 2.084, 16032/60000 datapoints
2025-03-06 18:39:13,931 - INFO - training batch 551, loss: 2.071, 17632/60000 datapoints
2025-03-06 18:39:14,146 - INFO - training batch 601, loss: 2.109, 19232/60000 datapoints
2025-03-06 18:39:14,358 - INFO - training batch 651, loss: 2.083, 20832/60000 datapoints
2025-03-06 18:39:14,558 - INFO - training batch 701, loss: 2.081, 22432/60000 datapoints
2025-03-06 18:39:14,770 - INFO - training batch 751, loss: 2.096, 24032/60000 datapoints
2025-03-06 18:39:14,981 - INFO - training batch 801, loss: 2.114, 25632/60000 datapoints
2025-03-06 18:39:15,184 - INFO - training batch 851, loss: 2.122, 27232/60000 datapoints
2025-03-06 18:39:15,383 - INFO - training batch 901, loss: 2.046, 28832/60000 datapoints
2025-03-06 18:39:15,579 - INFO - training batch 951, loss: 2.062, 30432/60000 datapoints
2025-03-06 18:39:15,794 - INFO - training batch 1001, loss: 2.092, 32032/60000 datapoints
2025-03-06 18:39:16,025 - INFO - training batch 1051, loss: 2.037, 33632/60000 datapoints
2025-03-06 18:39:16,260 - INFO - training batch 1101, loss: 2.065, 35232/60000 datapoints
2025-03-06 18:39:16,499 - INFO - training batch 1151, loss: 2.078, 36832/60000 datapoints
2025-03-06 18:39:16,731 - INFO - training batch 1201, loss: 2.108, 38432/60000 datapoints
2025-03-06 18:39:16,955 - INFO - training batch 1251, loss: 2.102, 40032/60000 datapoints
2025-03-06 18:39:17,186 - INFO - training batch 1301, loss: 2.062, 41632/60000 datapoints
2025-03-06 18:39:17,395 - INFO - training batch 1351, loss: 2.112, 43232/60000 datapoints
2025-03-06 18:39:17,661 - INFO - training batch 1401, loss: 2.049, 44832/60000 datapoints
2025-03-06 18:39:17,916 - INFO - training batch 1451, loss: 2.036, 46432/60000 datapoints
2025-03-06 18:39:18,145 - INFO - training batch 1501, loss: 2.084, 48032/60000 datapoints
2025-03-06 18:39:18,567 - INFO - training batch 1551, loss: 2.027, 49632/60000 datapoints
2025-03-06 18:39:18,867 - INFO - training batch 1601, loss: 2.110, 51232/60000 datapoints
2025-03-06 18:39:19,109 - INFO - training batch 1651, loss: 2.110, 52832/60000 datapoints
2025-03-06 18:39:19,431 - INFO - training batch 1701, loss: 2.091, 54432/60000 datapoints
2025-03-06 18:39:19,675 - INFO - training batch 1751, loss: 2.060, 56032/60000 datapoints
2025-03-06 18:39:19,923 - INFO - training batch 1801, loss: 2.023, 57632/60000 datapoints
2025-03-06 18:39:20,198 - INFO - training batch 1851, loss: 2.058, 59232/60000 datapoints
2025-03-06 18:39:20,355 - INFO - validation batch 1, loss: 2.034, 32/10016 datapoints
2025-03-06 18:39:20,728 - INFO - validation batch 51, loss: 2.044, 1632/10016 datapoints
2025-03-06 18:39:20,968 - INFO - validation batch 101, loss: 2.082, 3232/10016 datapoints
2025-03-06 18:39:21,226 - INFO - validation batch 151, loss: 2.056, 4832/10016 datapoints
2025-03-06 18:39:21,532 - INFO - validation batch 201, loss: 2.050, 6432/10016 datapoints
2025-03-06 18:39:21,832 - INFO - validation batch 251, loss: 2.049, 8032/10016 datapoints
2025-03-06 18:39:22,020 - INFO - validation batch 301, loss: 2.096, 9632/10016 datapoints
2025-03-06 18:39:22,068 - INFO - Epoch 12/800 done.
2025-03-06 18:39:22,068 - INFO - Final validation performance:
Loss: 2.059, top-1 acc: 0.392top-5 acc: 0.392
2025-03-06 18:39:22,068 - INFO - Beginning epoch 13/800
2025-03-06 18:39:22,075 - INFO - training batch 1, loss: 2.111, 32/60000 datapoints
2025-03-06 18:39:22,355 - INFO - training batch 51, loss: 1.974, 1632/60000 datapoints
2025-03-06 18:39:22,622 - INFO - training batch 101, loss: 2.111, 3232/60000 datapoints
2025-03-06 18:39:22,883 - INFO - training batch 151, loss: 2.102, 4832/60000 datapoints
2025-03-06 18:39:23,111 - INFO - training batch 201, loss: 2.046, 6432/60000 datapoints
2025-03-06 18:39:23,350 - INFO - training batch 251, loss: 2.081, 8032/60000 datapoints
2025-03-06 18:39:23,622 - INFO - training batch 301, loss: 2.048, 9632/60000 datapoints
2025-03-06 18:39:23,900 - INFO - training batch 351, loss: 2.079, 11232/60000 datapoints
2025-03-06 18:39:24,115 - INFO - training batch 401, loss: 2.123, 12832/60000 datapoints
2025-03-06 18:39:24,354 - INFO - training batch 451, loss: 2.057, 14432/60000 datapoints
2025-03-06 18:39:24,596 - INFO - training batch 501, loss: 2.036, 16032/60000 datapoints
2025-03-06 18:39:24,842 - INFO - training batch 551, loss: 2.052, 17632/60000 datapoints
2025-03-06 18:39:25,063 - INFO - training batch 601, loss: 2.026, 19232/60000 datapoints
2025-03-06 18:39:25,282 - INFO - training batch 651, loss: 2.052, 20832/60000 datapoints
2025-03-06 18:39:25,497 - INFO - training batch 701, loss: 2.105, 22432/60000 datapoints
2025-03-06 18:39:25,718 - INFO - training batch 751, loss: 2.011, 24032/60000 datapoints
2025-03-06 18:39:25,942 - INFO - training batch 801, loss: 2.073, 25632/60000 datapoints
2025-03-06 18:39:26,183 - INFO - training batch 851, loss: 2.035, 27232/60000 datapoints
2025-03-06 18:39:26,404 - INFO - training batch 901, loss: 2.081, 28832/60000 datapoints
2025-03-06 18:39:26,619 - INFO - training batch 951, loss: 2.078, 30432/60000 datapoints
2025-03-06 18:39:26,849 - INFO - training batch 1001, loss: 2.024, 32032/60000 datapoints
2025-03-06 18:39:27,093 - INFO - training batch 1051, loss: 2.048, 33632/60000 datapoints
2025-03-06 18:39:27,294 - INFO - training batch 1101, loss: 2.022, 35232/60000 datapoints
2025-03-06 18:39:27,507 - INFO - training batch 1151, loss: 2.086, 36832/60000 datapoints
2025-03-06 18:39:27,728 - INFO - training batch 1201, loss: 1.963, 38432/60000 datapoints
2025-03-06 18:39:27,971 - INFO - training batch 1251, loss: 2.093, 40032/60000 datapoints
2025-03-06 18:39:28,197 - INFO - training batch 1301, loss: 2.041, 41632/60000 datapoints
2025-03-06 18:39:28,404 - INFO - training batch 1351, loss: 2.046, 43232/60000 datapoints
2025-03-06 18:39:28,635 - INFO - training batch 1401, loss: 2.010, 44832/60000 datapoints
2025-03-06 18:39:28,887 - INFO - training batch 1451, loss: 2.055, 46432/60000 datapoints
2025-03-06 18:39:29,109 - INFO - training batch 1501, loss: 2.004, 48032/60000 datapoints
2025-03-06 18:39:29,323 - INFO - training batch 1551, loss: 2.035, 49632/60000 datapoints
2025-03-06 18:39:29,540 - INFO - training batch 1601, loss: 2.026, 51232/60000 datapoints
2025-03-06 18:39:29,752 - INFO - training batch 1651, loss: 1.966, 52832/60000 datapoints
2025-03-06 18:39:29,955 - INFO - training batch 1701, loss: 2.029, 54432/60000 datapoints
2025-03-06 18:39:30,162 - INFO - training batch 1751, loss: 2.062, 56032/60000 datapoints
2025-03-06 18:39:30,368 - INFO - training batch 1801, loss: 2.007, 57632/60000 datapoints
2025-03-06 18:39:30,563 - INFO - training batch 1851, loss: 2.106, 59232/60000 datapoints
2025-03-06 18:39:30,671 - INFO - validation batch 1, loss: 2.026, 32/10016 datapoints
2025-03-06 18:39:30,824 - INFO - validation batch 51, loss: 2.058, 1632/10016 datapoints
2025-03-06 18:39:30,983 - INFO - validation batch 101, loss: 2.008, 3232/10016 datapoints
2025-03-06 18:39:31,160 - INFO - validation batch 151, loss: 2.070, 4832/10016 datapoints
2025-03-06 18:39:31,370 - INFO - validation batch 201, loss: 2.068, 6432/10016 datapoints
2025-03-06 18:39:31,545 - INFO - validation batch 251, loss: 2.078, 8032/10016 datapoints
2025-03-06 18:39:31,719 - INFO - validation batch 301, loss: 2.027, 9632/10016 datapoints
2025-03-06 18:39:31,759 - INFO - Epoch 13/800 done.
2025-03-06 18:39:31,760 - INFO - Final validation performance:
Loss: 2.048, top-1 acc: 0.419top-5 acc: 0.419
2025-03-06 18:39:31,760 - INFO - Beginning epoch 14/800
2025-03-06 18:39:31,767 - INFO - training batch 1, loss: 2.003, 32/60000 datapoints
2025-03-06 18:39:32,009 - INFO - training batch 51, loss: 2.012, 1632/60000 datapoints
2025-03-06 18:39:32,233 - INFO - training batch 101, loss: 2.051, 3232/60000 datapoints
2025-03-06 18:39:32,464 - INFO - training batch 151, loss: 2.081, 4832/60000 datapoints
2025-03-06 18:39:32,685 - INFO - training batch 201, loss: 1.982, 6432/60000 datapoints
2025-03-06 18:39:32,929 - INFO - training batch 251, loss: 2.001, 8032/60000 datapoints
2025-03-06 18:39:33,160 - INFO - training batch 301, loss: 2.060, 9632/60000 datapoints
2025-03-06 18:39:33,381 - INFO - training batch 351, loss: 2.028, 11232/60000 datapoints
2025-03-06 18:39:33,617 - INFO - training batch 401, loss: 2.011, 12832/60000 datapoints
2025-03-06 18:39:33,852 - INFO - training batch 451, loss: 2.012, 14432/60000 datapoints
2025-03-06 18:39:34,091 - INFO - training batch 501, loss: 2.062, 16032/60000 datapoints
2025-03-06 18:39:34,324 - INFO - training batch 551, loss: 2.020, 17632/60000 datapoints
2025-03-06 18:39:34,577 - INFO - training batch 601, loss: 1.981, 19232/60000 datapoints
2025-03-06 18:39:34,855 - INFO - training batch 651, loss: 2.007, 20832/60000 datapoints
2025-03-06 18:39:35,095 - INFO - training batch 701, loss: 2.071, 22432/60000 datapoints
2025-03-06 18:39:35,326 - INFO - training batch 751, loss: 1.962, 24032/60000 datapoints
2025-03-06 18:39:35,568 - INFO - training batch 801, loss: 2.054, 25632/60000 datapoints
2025-03-06 18:39:35,806 - INFO - training batch 851, loss: 2.053, 27232/60000 datapoints
2025-03-06 18:39:36,029 - INFO - training batch 901, loss: 1.966, 28832/60000 datapoints
2025-03-06 18:39:36,269 - INFO - training batch 951, loss: 2.030, 30432/60000 datapoints
2025-03-06 18:39:36,499 - INFO - training batch 1001, loss: 2.021, 32032/60000 datapoints
2025-03-06 18:39:36,746 - INFO - training batch 1051, loss: 1.997, 33632/60000 datapoints
2025-03-06 18:39:36,964 - INFO - training batch 1101, loss: 2.026, 35232/60000 datapoints
2025-03-06 18:39:37,186 - INFO - training batch 1151, loss: 2.038, 36832/60000 datapoints
2025-03-06 18:39:37,408 - INFO - training batch 1201, loss: 1.960, 38432/60000 datapoints
2025-03-06 18:39:37,636 - INFO - training batch 1251, loss: 1.941, 40032/60000 datapoints
2025-03-06 18:39:37,891 - INFO - training batch 1301, loss: 1.982, 41632/60000 datapoints
2025-03-06 18:39:38,124 - INFO - training batch 1351, loss: 2.030, 43232/60000 datapoints
2025-03-06 18:39:38,362 - INFO - training batch 1401, loss: 1.995, 44832/60000 datapoints
2025-03-06 18:39:38,575 - INFO - training batch 1451, loss: 2.010, 46432/60000 datapoints
2025-03-06 18:39:38,807 - INFO - training batch 1501, loss: 2.029, 48032/60000 datapoints
2025-03-06 18:39:39,039 - INFO - training batch 1551, loss: 1.985, 49632/60000 datapoints
2025-03-06 18:39:39,263 - INFO - training batch 1601, loss: 1.993, 51232/60000 datapoints
2025-03-06 18:39:39,488 - INFO - training batch 1651, loss: 2.016, 52832/60000 datapoints
2025-03-06 18:39:39,725 - INFO - training batch 1701, loss: 1.971, 54432/60000 datapoints
2025-03-06 18:39:39,966 - INFO - training batch 1751, loss: 1.988, 56032/60000 datapoints
2025-03-06 18:39:40,197 - INFO - training batch 1801, loss: 2.019, 57632/60000 datapoints
2025-03-06 18:39:40,409 - INFO - training batch 1851, loss: 1.968, 59232/60000 datapoints
2025-03-06 18:39:40,528 - INFO - validation batch 1, loss: 2.046, 32/10016 datapoints
2025-03-06 18:39:40,706 - INFO - validation batch 51, loss: 2.003, 1632/10016 datapoints
2025-03-06 18:39:40,872 - INFO - validation batch 101, loss: 1.964, 3232/10016 datapoints
2025-03-06 18:39:41,030 - INFO - validation batch 151, loss: 2.002, 4832/10016 datapoints
2025-03-06 18:39:41,199 - INFO - validation batch 201, loss: 2.003, 6432/10016 datapoints
2025-03-06 18:39:41,377 - INFO - validation batch 251, loss: 1.974, 8032/10016 datapoints
2025-03-06 18:39:41,542 - INFO - validation batch 301, loss: 1.981, 9632/10016 datapoints
2025-03-06 18:39:41,582 - INFO - Epoch 14/800 done.
2025-03-06 18:39:41,582 - INFO - Final validation performance:
Loss: 1.996, top-1 acc: 0.442top-5 acc: 0.442
2025-03-06 18:39:41,583 - INFO - Beginning epoch 15/800
2025-03-06 18:39:41,591 - INFO - training batch 1, loss: 1.908, 32/60000 datapoints
2025-03-06 18:39:41,833 - INFO - training batch 51, loss: 1.981, 1632/60000 datapoints
2025-03-06 18:39:42,038 - INFO - training batch 101, loss: 1.964, 3232/60000 datapoints
2025-03-06 18:39:42,247 - INFO - training batch 151, loss: 2.037, 4832/60000 datapoints
2025-03-06 18:39:42,477 - INFO - training batch 201, loss: 1.936, 6432/60000 datapoints
2025-03-06 18:39:42,681 - INFO - training batch 251, loss: 2.005, 8032/60000 datapoints
2025-03-06 18:39:42,885 - INFO - training batch 301, loss: 2.063, 9632/60000 datapoints
2025-03-06 18:39:43,086 - INFO - training batch 351, loss: 1.989, 11232/60000 datapoints
2025-03-06 18:39:43,304 - INFO - training batch 401, loss: 1.989, 12832/60000 datapoints
2025-03-06 18:39:43,513 - INFO - training batch 451, loss: 2.007, 14432/60000 datapoints
2025-03-06 18:39:43,727 - INFO - training batch 501, loss: 1.938, 16032/60000 datapoints
2025-03-06 18:39:43,932 - INFO - training batch 551, loss: 1.935, 17632/60000 datapoints
2025-03-06 18:39:44,145 - INFO - training batch 601, loss: 2.038, 19232/60000 datapoints
2025-03-06 18:39:44,484 - INFO - training batch 651, loss: 1.937, 20832/60000 datapoints
2025-03-06 18:39:44,723 - INFO - training batch 701, loss: 1.910, 22432/60000 datapoints
2025-03-06 18:39:44,952 - INFO - training batch 751, loss: 2.031, 24032/60000 datapoints
2025-03-06 18:39:45,182 - INFO - training batch 801, loss: 2.011, 25632/60000 datapoints
2025-03-06 18:39:45,392 - INFO - training batch 851, loss: 2.066, 27232/60000 datapoints
2025-03-06 18:39:45,643 - INFO - training batch 901, loss: 1.975, 28832/60000 datapoints
2025-03-06 18:39:45,881 - INFO - training batch 951, loss: 1.997, 30432/60000 datapoints
2025-03-06 18:39:46,105 - INFO - training batch 1001, loss: 2.014, 32032/60000 datapoints
2025-03-06 18:39:46,326 - INFO - training batch 1051, loss: 2.026, 33632/60000 datapoints
2025-03-06 18:39:46,567 - INFO - training batch 1101, loss: 1.952, 35232/60000 datapoints
2025-03-06 18:39:46,822 - INFO - training batch 1151, loss: 1.960, 36832/60000 datapoints
2025-03-06 18:39:47,040 - INFO - training batch 1201, loss: 1.955, 38432/60000 datapoints
2025-03-06 18:39:47,268 - INFO - training batch 1251, loss: 1.959, 40032/60000 datapoints
2025-03-06 18:39:47,500 - INFO - training batch 1301, loss: 1.951, 41632/60000 datapoints
2025-03-06 18:39:47,725 - INFO - training batch 1351, loss: 1.985, 43232/60000 datapoints
2025-03-06 18:39:47,974 - INFO - training batch 1401, loss: 1.980, 44832/60000 datapoints
2025-03-06 18:39:48,190 - INFO - training batch 1451, loss: 1.908, 46432/60000 datapoints
2025-03-06 18:39:48,412 - INFO - training batch 1501, loss: 1.967, 48032/60000 datapoints
2025-03-06 18:39:48,646 - INFO - training batch 1551, loss: 1.929, 49632/60000 datapoints
2025-03-06 18:39:48,869 - INFO - training batch 1601, loss: 2.058, 51232/60000 datapoints
2025-03-06 18:39:49,086 - INFO - training batch 1651, loss: 1.998, 52832/60000 datapoints
2025-03-06 18:39:49,310 - INFO - training batch 1701, loss: 1.936, 54432/60000 datapoints
2025-03-06 18:39:49,552 - INFO - training batch 1751, loss: 1.917, 56032/60000 datapoints
2025-03-06 18:39:49,757 - INFO - training batch 1801, loss: 2.010, 57632/60000 datapoints
2025-03-06 18:39:49,978 - INFO - training batch 1851, loss: 1.982, 59232/60000 datapoints
2025-03-06 18:39:50,088 - INFO - validation batch 1, loss: 1.880, 32/10016 datapoints
2025-03-06 18:39:50,252 - INFO - validation batch 51, loss: 1.992, 1632/10016 datapoints
2025-03-06 18:39:50,412 - INFO - validation batch 101, loss: 1.923, 3232/10016 datapoints
2025-03-06 18:39:50,588 - INFO - validation batch 151, loss: 1.999, 4832/10016 datapoints
2025-03-06 18:39:50,764 - INFO - validation batch 201, loss: 1.917, 6432/10016 datapoints
2025-03-06 18:39:50,938 - INFO - validation batch 251, loss: 2.005, 8032/10016 datapoints
2025-03-06 18:39:51,116 - INFO - validation batch 301, loss: 2.000, 9632/10016 datapoints
2025-03-06 18:39:51,157 - INFO - Epoch 15/800 done.
2025-03-06 18:39:51,157 - INFO - Final validation performance:
Loss: 1.959, top-1 acc: 0.460top-5 acc: 0.460
2025-03-06 18:39:51,158 - INFO - Beginning epoch 16/800
2025-03-06 18:39:51,165 - INFO - training batch 1, loss: 1.875, 32/60000 datapoints
2025-03-06 18:39:51,425 - INFO - training batch 51, loss: 2.001, 1632/60000 datapoints
2025-03-06 18:39:51,630 - INFO - training batch 101, loss: 1.955, 3232/60000 datapoints
2025-03-06 18:39:51,827 - INFO - training batch 151, loss: 1.931, 4832/60000 datapoints
2025-03-06 18:39:52,028 - INFO - training batch 201, loss: 1.947, 6432/60000 datapoints
2025-03-06 18:39:52,231 - INFO - training batch 251, loss: 1.932, 8032/60000 datapoints
2025-03-06 18:39:52,429 - INFO - training batch 301, loss: 1.973, 9632/60000 datapoints
2025-03-06 18:39:52,656 - INFO - training batch 351, loss: 2.005, 11232/60000 datapoints
2025-03-06 18:39:52,883 - INFO - training batch 401, loss: 2.017, 12832/60000 datapoints
2025-03-06 18:39:53,101 - INFO - training batch 451, loss: 1.901, 14432/60000 datapoints
2025-03-06 18:39:53,335 - INFO - training batch 501, loss: 1.978, 16032/60000 datapoints
2025-03-06 18:39:53,560 - INFO - training batch 551, loss: 1.998, 17632/60000 datapoints
2025-03-06 18:39:53,779 - INFO - training batch 601, loss: 1.974, 19232/60000 datapoints
2025-03-06 18:39:53,992 - INFO - training batch 651, loss: 1.997, 20832/60000 datapoints
2025-03-06 18:39:54,195 - INFO - training batch 701, loss: 1.960, 22432/60000 datapoints
2025-03-06 18:39:54,406 - INFO - training batch 751, loss: 1.926, 24032/60000 datapoints
2025-03-06 18:39:54,619 - INFO - training batch 801, loss: 1.953, 25632/60000 datapoints
2025-03-06 18:39:54,835 - INFO - training batch 851, loss: 1.950, 27232/60000 datapoints
2025-03-06 18:39:55,047 - INFO - training batch 901, loss: 1.936, 28832/60000 datapoints
2025-03-06 18:39:55,288 - INFO - training batch 951, loss: 1.928, 30432/60000 datapoints
2025-03-06 18:39:55,492 - INFO - training batch 1001, loss: 1.938, 32032/60000 datapoints
2025-03-06 18:39:55,702 - INFO - training batch 1051, loss: 1.902, 33632/60000 datapoints
2025-03-06 18:39:55,912 - INFO - training batch 1101, loss: 1.940, 35232/60000 datapoints
2025-03-06 18:39:56,123 - INFO - training batch 1151, loss: 1.990, 36832/60000 datapoints
2025-03-06 18:39:56,323 - INFO - training batch 1201, loss: 1.949, 38432/60000 datapoints
2025-03-06 18:39:56,532 - INFO - training batch 1251, loss: 2.016, 40032/60000 datapoints
2025-03-06 18:39:56,750 - INFO - training batch 1301, loss: 2.026, 41632/60000 datapoints
2025-03-06 18:39:56,950 - INFO - training batch 1351, loss: 1.955, 43232/60000 datapoints
2025-03-06 18:39:57,170 - INFO - training batch 1401, loss: 1.944, 44832/60000 datapoints
2025-03-06 18:39:57,407 - INFO - training batch 1451, loss: 2.003, 46432/60000 datapoints
2025-03-06 18:39:57,641 - INFO - training batch 1501, loss: 1.992, 48032/60000 datapoints
2025-03-06 18:39:57,884 - INFO - training batch 1551, loss: 2.001, 49632/60000 datapoints
2025-03-06 18:39:58,129 - INFO - training batch 1601, loss: 1.951, 51232/60000 datapoints
2025-03-06 18:39:58,350 - INFO - training batch 1651, loss: 1.985, 52832/60000 datapoints
2025-03-06 18:39:58,589 - INFO - training batch 1701, loss: 2.004, 54432/60000 datapoints
2025-03-06 18:39:58,864 - INFO - training batch 1751, loss: 1.971, 56032/60000 datapoints
2025-03-06 18:39:59,106 - INFO - training batch 1801, loss: 1.910, 57632/60000 datapoints
2025-03-06 18:39:59,361 - INFO - training batch 1851, loss: 1.940, 59232/60000 datapoints
2025-03-06 18:39:59,538 - INFO - validation batch 1, loss: 1.935, 32/10016 datapoints
2025-03-06 18:39:59,860 - INFO - validation batch 51, loss: 1.962, 1632/10016 datapoints
2025-03-06 18:40:00,125 - INFO - validation batch 101, loss: 1.925, 3232/10016 datapoints
2025-03-06 18:40:00,375 - INFO - validation batch 151, loss: 1.857, 4832/10016 datapoints
2025-03-06 18:40:00,625 - INFO - validation batch 201, loss: 1.982, 6432/10016 datapoints
2025-03-06 18:40:00,839 - INFO - validation batch 251, loss: 1.908, 8032/10016 datapoints
2025-03-06 18:40:01,030 - INFO - validation batch 301, loss: 1.841, 9632/10016 datapoints
2025-03-06 18:40:01,072 - INFO - Epoch 16/800 done.
2025-03-06 18:40:01,072 - INFO - Final validation performance:
Loss: 1.916, top-1 acc: 0.475top-5 acc: 0.475
2025-03-06 18:40:01,073 - INFO - Beginning epoch 17/800
2025-03-06 18:40:01,080 - INFO - training batch 1, loss: 1.977, 32/60000 datapoints
2025-03-06 18:40:01,500 - INFO - training batch 51, loss: 1.962, 1632/60000 datapoints
2025-03-06 18:40:01,785 - INFO - training batch 101, loss: 1.918, 3232/60000 datapoints
2025-03-06 18:40:02,059 - INFO - training batch 151, loss: 1.949, 4832/60000 datapoints
2025-03-06 18:40:02,313 - INFO - training batch 201, loss: 2.001, 6432/60000 datapoints
2025-03-06 18:40:02,553 - INFO - training batch 251, loss: 1.931, 8032/60000 datapoints
2025-03-06 18:40:02,814 - INFO - training batch 301, loss: 1.934, 9632/60000 datapoints
2025-03-06 18:40:03,065 - INFO - training batch 351, loss: 1.974, 11232/60000 datapoints
2025-03-06 18:40:03,296 - INFO - training batch 401, loss: 1.962, 12832/60000 datapoints
2025-03-06 18:40:03,536 - INFO - training batch 451, loss: 1.894, 14432/60000 datapoints
2025-03-06 18:40:03,795 - INFO - training batch 501, loss: 1.947, 16032/60000 datapoints
2025-03-06 18:40:04,014 - INFO - training batch 551, loss: 2.012, 17632/60000 datapoints
2025-03-06 18:40:04,230 - INFO - training batch 601, loss: 1.914, 19232/60000 datapoints
2025-03-06 18:40:04,437 - INFO - training batch 651, loss: 1.924, 20832/60000 datapoints
2025-03-06 18:40:04,643 - INFO - training batch 701, loss: 1.847, 22432/60000 datapoints
2025-03-06 18:40:04,855 - INFO - training batch 751, loss: 2.003, 24032/60000 datapoints
2025-03-06 18:40:05,051 - INFO - training batch 801, loss: 1.924, 25632/60000 datapoints
2025-03-06 18:40:05,247 - INFO - training batch 851, loss: 1.984, 27232/60000 datapoints
2025-03-06 18:40:05,448 - INFO - training batch 901, loss: 1.881, 28832/60000 datapoints
2025-03-06 18:40:05,657 - INFO - training batch 951, loss: 1.947, 30432/60000 datapoints
2025-03-06 18:40:05,863 - INFO - training batch 1001, loss: 1.924, 32032/60000 datapoints
2025-03-06 18:40:06,069 - INFO - training batch 1051, loss: 1.845, 33632/60000 datapoints
2025-03-06 18:40:06,301 - INFO - training batch 1101, loss: 1.914, 35232/60000 datapoints
2025-03-06 18:40:06,531 - INFO - training batch 1151, loss: 1.888, 36832/60000 datapoints
2025-03-06 18:40:06,761 - INFO - training batch 1201, loss: 1.944, 38432/60000 datapoints
2025-03-06 18:40:06,983 - INFO - training batch 1251, loss: 1.934, 40032/60000 datapoints
2025-03-06 18:40:07,192 - INFO - training batch 1301, loss: 1.979, 41632/60000 datapoints
2025-03-06 18:40:07,387 - INFO - training batch 1351, loss: 1.944, 43232/60000 datapoints
2025-03-06 18:40:07,588 - INFO - training batch 1401, loss: 1.960, 44832/60000 datapoints
2025-03-06 18:40:07,789 - INFO - training batch 1451, loss: 1.921, 46432/60000 datapoints
2025-03-06 18:40:07,982 - INFO - training batch 1501, loss: 1.890, 48032/60000 datapoints
2025-03-06 18:40:08,198 - INFO - training batch 1551, loss: 1.845, 49632/60000 datapoints
2025-03-06 18:40:08,394 - INFO - training batch 1601, loss: 1.854, 51232/60000 datapoints
2025-03-06 18:40:08,589 - INFO - training batch 1651, loss: 1.834, 52832/60000 datapoints
2025-03-06 18:40:08,794 - INFO - training batch 1701, loss: 1.910, 54432/60000 datapoints
2025-03-06 18:40:08,993 - INFO - training batch 1751, loss: 1.936, 56032/60000 datapoints
2025-03-06 18:40:09,189 - INFO - training batch 1801, loss: 1.821, 57632/60000 datapoints
2025-03-06 18:40:09,385 - INFO - training batch 1851, loss: 1.901, 59232/60000 datapoints
2025-03-06 18:40:09,488 - INFO - validation batch 1, loss: 1.887, 32/10016 datapoints
2025-03-06 18:40:09,645 - INFO - validation batch 51, loss: 1.868, 1632/10016 datapoints
2025-03-06 18:40:09,799 - INFO - validation batch 101, loss: 1.902, 3232/10016 datapoints
2025-03-06 18:40:09,954 - INFO - validation batch 151, loss: 1.937, 4832/10016 datapoints
2025-03-06 18:40:10,106 - INFO - validation batch 201, loss: 1.890, 6432/10016 datapoints
2025-03-06 18:40:10,268 - INFO - validation batch 251, loss: 1.952, 8032/10016 datapoints
2025-03-06 18:40:10,422 - INFO - validation batch 301, loss: 1.882, 9632/10016 datapoints
2025-03-06 18:40:10,464 - INFO - Epoch 17/800 done.
2025-03-06 18:40:10,464 - INFO - Final validation performance:
Loss: 1.903, top-1 acc: 0.490top-5 acc: 0.490
2025-03-06 18:40:10,465 - INFO - Beginning epoch 18/800
2025-03-06 18:40:10,473 - INFO - training batch 1, loss: 1.941, 32/60000 datapoints
2025-03-06 18:40:10,736 - INFO - training batch 51, loss: 1.931, 1632/60000 datapoints
2025-03-06 18:40:10,940 - INFO - training batch 101, loss: 1.956, 3232/60000 datapoints
2025-03-06 18:40:11,142 - INFO - training batch 151, loss: 1.870, 4832/60000 datapoints
2025-03-06 18:40:11,346 - INFO - training batch 201, loss: 1.929, 6432/60000 datapoints
2025-03-06 18:40:11,546 - INFO - training batch 251, loss: 1.883, 8032/60000 datapoints
2025-03-06 18:40:11,747 - INFO - training batch 301, loss: 1.935, 9632/60000 datapoints
2025-03-06 18:40:11,945 - INFO - training batch 351, loss: 1.854, 11232/60000 datapoints
2025-03-06 18:40:12,142 - INFO - training batch 401, loss: 1.756, 12832/60000 datapoints
2025-03-06 18:40:12,340 - INFO - training batch 451, loss: 1.951, 14432/60000 datapoints
2025-03-06 18:40:12,562 - INFO - training batch 501, loss: 1.927, 16032/60000 datapoints
2025-03-06 18:40:12,762 - INFO - training batch 551, loss: 1.931, 17632/60000 datapoints
2025-03-06 18:40:12,959 - INFO - training batch 601, loss: 1.886, 19232/60000 datapoints
2025-03-06 18:40:13,156 - INFO - training batch 651, loss: 1.865, 20832/60000 datapoints
2025-03-06 18:40:13,351 - INFO - training batch 701, loss: 1.875, 22432/60000 datapoints
2025-03-06 18:40:13,550 - INFO - training batch 751, loss: 1.878, 24032/60000 datapoints
2025-03-06 18:40:13,752 - INFO - training batch 801, loss: 1.806, 25632/60000 datapoints
2025-03-06 18:40:13,950 - INFO - training batch 851, loss: 1.863, 27232/60000 datapoints
2025-03-06 18:40:14,156 - INFO - training batch 901, loss: 1.901, 28832/60000 datapoints
2025-03-06 18:40:14,352 - INFO - training batch 951, loss: 1.861, 30432/60000 datapoints
2025-03-06 18:40:14,556 - INFO - training batch 1001, loss: 1.870, 32032/60000 datapoints
2025-03-06 18:40:14,756 - INFO - training batch 1051, loss: 1.842, 33632/60000 datapoints
2025-03-06 18:40:14,975 - INFO - training batch 1101, loss: 1.887, 35232/60000 datapoints
2025-03-06 18:40:15,173 - INFO - training batch 1151, loss: 1.895, 36832/60000 datapoints
2025-03-06 18:40:15,373 - INFO - training batch 1201, loss: 1.886, 38432/60000 datapoints
2025-03-06 18:40:15,571 - INFO - training batch 1251, loss: 1.904, 40032/60000 datapoints
2025-03-06 18:40:15,768 - INFO - training batch 1301, loss: 1.873, 41632/60000 datapoints
2025-03-06 18:40:15,962 - INFO - training batch 1351, loss: 1.866, 43232/60000 datapoints
2025-03-06 18:40:16,158 - INFO - training batch 1401, loss: 1.918, 44832/60000 datapoints
2025-03-06 18:40:16,355 - INFO - training batch 1451, loss: 1.910, 46432/60000 datapoints
2025-03-06 18:40:16,549 - INFO - training batch 1501, loss: 1.925, 48032/60000 datapoints
2025-03-06 18:40:16,749 - INFO - training batch 1551, loss: 1.897, 49632/60000 datapoints
2025-03-06 18:40:16,951 - INFO - training batch 1601, loss: 1.880, 51232/60000 datapoints
2025-03-06 18:40:17,149 - INFO - training batch 1651, loss: 1.878, 52832/60000 datapoints
2025-03-06 18:40:17,347 - INFO - training batch 1701, loss: 1.868, 54432/60000 datapoints
2025-03-06 18:40:17,548 - INFO - training batch 1751, loss: 1.868, 56032/60000 datapoints
2025-03-06 18:40:17,750 - INFO - training batch 1801, loss: 1.867, 57632/60000 datapoints
2025-03-06 18:40:17,955 - INFO - training batch 1851, loss: 1.772, 59232/60000 datapoints
2025-03-06 18:40:18,055 - INFO - validation batch 1, loss: 1.827, 32/10016 datapoints
2025-03-06 18:40:18,225 - INFO - validation batch 51, loss: 1.817, 1632/10016 datapoints
2025-03-06 18:40:18,378 - INFO - validation batch 101, loss: 1.867, 3232/10016 datapoints
2025-03-06 18:40:18,531 - INFO - validation batch 151, loss: 1.815, 4832/10016 datapoints
2025-03-06 18:40:18,687 - INFO - validation batch 201, loss: 1.814, 6432/10016 datapoints
2025-03-06 18:40:18,842 - INFO - validation batch 251, loss: 1.923, 8032/10016 datapoints
2025-03-06 18:40:19,003 - INFO - validation batch 301, loss: 1.852, 9632/10016 datapoints
2025-03-06 18:40:19,043 - INFO - Epoch 18/800 done.
2025-03-06 18:40:19,043 - INFO - Final validation performance:
Loss: 1.845, top-1 acc: 0.503top-5 acc: 0.503
2025-03-06 18:40:19,044 - INFO - Beginning epoch 19/800
2025-03-06 18:40:19,051 - INFO - training batch 1, loss: 1.833, 32/60000 datapoints
2025-03-06 18:40:19,264 - INFO - training batch 51, loss: 1.836, 1632/60000 datapoints
2025-03-06 18:40:19,460 - INFO - training batch 101, loss: 1.887, 3232/60000 datapoints
2025-03-06 18:40:19,663 - INFO - training batch 151, loss: 1.890, 4832/60000 datapoints
2025-03-06 18:40:19,861 - INFO - training batch 201, loss: 1.851, 6432/60000 datapoints
2025-03-06 18:40:20,058 - INFO - training batch 251, loss: 1.947, 8032/60000 datapoints
2025-03-06 18:40:20,253 - INFO - training batch 301, loss: 1.896, 9632/60000 datapoints
2025-03-06 18:40:20,446 - INFO - training batch 351, loss: 1.843, 11232/60000 datapoints
2025-03-06 18:40:20,648 - INFO - training batch 401, loss: 1.857, 12832/60000 datapoints
2025-03-06 18:40:20,839 - INFO - training batch 451, loss: 1.843, 14432/60000 datapoints
2025-03-06 18:40:21,032 - INFO - training batch 501, loss: 1.833, 16032/60000 datapoints
2025-03-06 18:40:21,226 - INFO - training batch 551, loss: 1.857, 17632/60000 datapoints
2025-03-06 18:40:21,423 - INFO - training batch 601, loss: 1.839, 19232/60000 datapoints
2025-03-06 18:40:21,621 - INFO - training batch 651, loss: 1.756, 20832/60000 datapoints
2025-03-06 18:40:21,819 - INFO - training batch 701, loss: 1.911, 22432/60000 datapoints
2025-03-06 18:40:22,017 - INFO - training batch 751, loss: 1.779, 24032/60000 datapoints
2025-03-06 18:40:22,212 - INFO - training batch 801, loss: 1.908, 25632/60000 datapoints
2025-03-06 18:40:22,406 - INFO - training batch 851, loss: 1.921, 27232/60000 datapoints
2025-03-06 18:40:22,607 - INFO - training batch 901, loss: 1.816, 28832/60000 datapoints
2025-03-06 18:40:22,801 - INFO - training batch 951, loss: 1.833, 30432/60000 datapoints
2025-03-06 18:40:22,995 - INFO - training batch 1001, loss: 1.936, 32032/60000 datapoints
2025-03-06 18:40:23,197 - INFO - training batch 1051, loss: 1.843, 33632/60000 datapoints
2025-03-06 18:40:23,399 - INFO - training batch 1101, loss: 1.936, 35232/60000 datapoints
2025-03-06 18:40:23,596 - INFO - training batch 1151, loss: 1.883, 36832/60000 datapoints
2025-03-06 18:40:23,796 - INFO - training batch 1201, loss: 1.811, 38432/60000 datapoints
2025-03-06 18:40:23,990 - INFO - training batch 1251, loss: 1.760, 40032/60000 datapoints
2025-03-06 18:40:24,189 - INFO - training batch 1301, loss: 1.835, 41632/60000 datapoints
2025-03-06 18:40:24,385 - INFO - training batch 1351, loss: 1.847, 43232/60000 datapoints
2025-03-06 18:40:24,577 - INFO - training batch 1401, loss: 1.899, 44832/60000 datapoints
2025-03-06 18:40:24,778 - INFO - training batch 1451, loss: 1.760, 46432/60000 datapoints
2025-03-06 18:40:24,976 - INFO - training batch 1501, loss: 1.833, 48032/60000 datapoints
2025-03-06 18:40:25,170 - INFO - training batch 1551, loss: 1.830, 49632/60000 datapoints
2025-03-06 18:40:25,365 - INFO - training batch 1601, loss: 1.725, 51232/60000 datapoints
2025-03-06 18:40:25,568 - INFO - training batch 1651, loss: 1.892, 52832/60000 datapoints
2025-03-06 18:40:25,768 - INFO - training batch 1701, loss: 1.906, 54432/60000 datapoints
2025-03-06 18:40:25,962 - INFO - training batch 1751, loss: 1.877, 56032/60000 datapoints
2025-03-06 18:40:26,163 - INFO - training batch 1801, loss: 1.852, 57632/60000 datapoints
2025-03-06 18:40:26,360 - INFO - training batch 1851, loss: 1.778, 59232/60000 datapoints
2025-03-06 18:40:26,462 - INFO - validation batch 1, loss: 1.820, 32/10016 datapoints
2025-03-06 18:40:26,619 - INFO - validation batch 51, loss: 1.756, 1632/10016 datapoints
2025-03-06 18:40:26,773 - INFO - validation batch 101, loss: 1.839, 3232/10016 datapoints
2025-03-06 18:40:26,927 - INFO - validation batch 151, loss: 1.826, 4832/10016 datapoints
2025-03-06 18:40:27,078 - INFO - validation batch 201, loss: 1.822, 6432/10016 datapoints
2025-03-06 18:40:27,235 - INFO - validation batch 251, loss: 1.920, 8032/10016 datapoints
2025-03-06 18:40:27,389 - INFO - validation batch 301, loss: 1.781, 9632/10016 datapoints
2025-03-06 18:40:27,426 - INFO - Epoch 19/800 done.
2025-03-06 18:40:27,426 - INFO - Final validation performance:
Loss: 1.823, top-1 acc: 0.515top-5 acc: 0.515
2025-03-06 18:40:27,427 - INFO - Beginning epoch 20/800
2025-03-06 18:40:27,434 - INFO - training batch 1, loss: 1.889, 32/60000 datapoints
2025-03-06 18:40:27,641 - INFO - training batch 51, loss: 1.842, 1632/60000 datapoints
2025-03-06 18:40:27,844 - INFO - training batch 101, loss: 1.777, 3232/60000 datapoints
2025-03-06 18:40:28,049 - INFO - training batch 151, loss: 1.764, 4832/60000 datapoints
2025-03-06 18:40:28,275 - INFO - training batch 201, loss: 1.755, 6432/60000 datapoints
2025-03-06 18:40:28,478 - INFO - training batch 251, loss: 1.772, 8032/60000 datapoints
2025-03-06 18:40:28,677 - INFO - training batch 301, loss: 1.791, 9632/60000 datapoints
2025-03-06 18:40:28,876 - INFO - training batch 351, loss: 1.869, 11232/60000 datapoints
2025-03-06 18:40:29,070 - INFO - training batch 401, loss: 1.813, 12832/60000 datapoints
2025-03-06 18:40:29,264 - INFO - training batch 451, loss: 1.909, 14432/60000 datapoints
2025-03-06 18:40:29,465 - INFO - training batch 501, loss: 1.777, 16032/60000 datapoints
2025-03-06 18:40:29,667 - INFO - training batch 551, loss: 1.883, 17632/60000 datapoints
2025-03-06 18:40:29,860 - INFO - training batch 601, loss: 1.748, 19232/60000 datapoints
2025-03-06 18:40:30,059 - INFO - training batch 651, loss: 1.859, 20832/60000 datapoints
2025-03-06 18:40:30,256 - INFO - training batch 701, loss: 1.815, 22432/60000 datapoints
2025-03-06 18:40:30,449 - INFO - training batch 751, loss: 1.810, 24032/60000 datapoints
2025-03-06 18:40:30,654 - INFO - training batch 801, loss: 1.819, 25632/60000 datapoints
2025-03-06 18:40:30,903 - INFO - training batch 851, loss: 1.818, 27232/60000 datapoints
2025-03-06 18:40:31,114 - INFO - training batch 901, loss: 1.855, 28832/60000 datapoints
2025-03-06 18:40:31,335 - INFO - training batch 951, loss: 1.885, 30432/60000 datapoints
2025-03-06 18:40:31,551 - INFO - training batch 1001, loss: 1.800, 32032/60000 datapoints
2025-03-06 18:40:31,750 - INFO - training batch 1051, loss: 1.869, 33632/60000 datapoints
2025-03-06 18:40:31,948 - INFO - training batch 1101, loss: 1.870, 35232/60000 datapoints
2025-03-06 18:40:32,142 - INFO - training batch 1151, loss: 1.855, 36832/60000 datapoints
2025-03-06 18:40:32,342 - INFO - training batch 1201, loss: 1.760, 38432/60000 datapoints
2025-03-06 18:40:32,541 - INFO - training batch 1251, loss: 1.875, 40032/60000 datapoints
2025-03-06 18:40:32,762 - INFO - training batch 1301, loss: 1.784, 41632/60000 datapoints
2025-03-06 18:40:32,983 - INFO - training batch 1351, loss: 1.768, 43232/60000 datapoints
2025-03-06 18:40:33,243 - INFO - training batch 1401, loss: 1.688, 44832/60000 datapoints
2025-03-06 18:40:33,512 - INFO - training batch 1451, loss: 1.770, 46432/60000 datapoints
2025-03-06 18:40:33,784 - INFO - training batch 1501, loss: 1.747, 48032/60000 datapoints
2025-03-06 18:40:34,011 - INFO - training batch 1551, loss: 1.711, 49632/60000 datapoints
2025-03-06 18:40:34,216 - INFO - training batch 1601, loss: 1.787, 51232/60000 datapoints
2025-03-06 18:40:34,413 - INFO - training batch 1651, loss: 1.784, 52832/60000 datapoints
2025-03-06 18:40:34,612 - INFO - training batch 1701, loss: 1.857, 54432/60000 datapoints
2025-03-06 18:40:34,814 - INFO - training batch 1751, loss: 1.823, 56032/60000 datapoints
2025-03-06 18:40:35,011 - INFO - training batch 1801, loss: 1.860, 57632/60000 datapoints
2025-03-06 18:40:35,211 - INFO - training batch 1851, loss: 1.747, 59232/60000 datapoints
2025-03-06 18:40:35,313 - INFO - validation batch 1, loss: 1.845, 32/10016 datapoints
2025-03-06 18:40:35,467 - INFO - validation batch 51, loss: 1.858, 1632/10016 datapoints
2025-03-06 18:40:35,624 - INFO - validation batch 101, loss: 1.740, 3232/10016 datapoints
2025-03-06 18:40:35,786 - INFO - validation batch 151, loss: 1.791, 4832/10016 datapoints
2025-03-06 18:40:35,949 - INFO - validation batch 201, loss: 1.869, 6432/10016 datapoints
2025-03-06 18:40:36,121 - INFO - validation batch 251, loss: 1.784, 8032/10016 datapoints
2025-03-06 18:40:36,311 - INFO - validation batch 301, loss: 1.838, 9632/10016 datapoints
2025-03-06 18:40:36,356 - INFO - Epoch 20/800 done.
2025-03-06 18:40:36,356 - INFO - Final validation performance:
Loss: 1.818, top-1 acc: 0.527top-5 acc: 0.527
2025-03-06 18:40:36,357 - INFO - Beginning epoch 21/800
2025-03-06 18:40:36,364 - INFO - training batch 1, loss: 1.779, 32/60000 datapoints
2025-03-06 18:40:36,680 - INFO - training batch 51, loss: 1.736, 1632/60000 datapoints
2025-03-06 18:40:36,955 - INFO - training batch 101, loss: 1.879, 3232/60000 datapoints
2025-03-06 18:40:37,215 - INFO - training batch 151, loss: 1.781, 4832/60000 datapoints
2025-03-06 18:40:37,441 - INFO - training batch 201, loss: 1.810, 6432/60000 datapoints
2025-03-06 18:40:37,703 - INFO - training batch 251, loss: 1.784, 8032/60000 datapoints
2025-03-06 18:40:37,938 - INFO - training batch 301, loss: 1.731, 9632/60000 datapoints
2025-03-06 18:40:38,183 - INFO - training batch 351, loss: 1.792, 11232/60000 datapoints
2025-03-06 18:40:38,428 - INFO - training batch 401, loss: 1.757, 12832/60000 datapoints
2025-03-06 18:40:38,785 - INFO - training batch 451, loss: 1.672, 14432/60000 datapoints
2025-03-06 18:40:39,157 - INFO - training batch 501, loss: 1.770, 16032/60000 datapoints
2025-03-06 18:40:39,433 - INFO - training batch 551, loss: 1.847, 17632/60000 datapoints
2025-03-06 18:40:39,775 - INFO - training batch 601, loss: 1.869, 19232/60000 datapoints
2025-03-06 18:40:40,294 - INFO - training batch 651, loss: 1.867, 20832/60000 datapoints
2025-03-06 18:40:40,664 - INFO - training batch 701, loss: 1.777, 22432/60000 datapoints
2025-03-06 18:40:41,063 - INFO - training batch 751, loss: 1.787, 24032/60000 datapoints
2025-03-06 18:40:41,332 - INFO - training batch 801, loss: 1.673, 25632/60000 datapoints
2025-03-06 18:40:41,682 - INFO - training batch 851, loss: 1.804, 27232/60000 datapoints
2025-03-06 18:40:41,959 - INFO - training batch 901, loss: 1.781, 28832/60000 datapoints
2025-03-06 18:40:42,207 - INFO - training batch 951, loss: 1.754, 30432/60000 datapoints
2025-03-06 18:40:42,425 - INFO - training batch 1001, loss: 1.783, 32032/60000 datapoints
2025-03-06 18:40:42,635 - INFO - training batch 1051, loss: 1.699, 33632/60000 datapoints
2025-03-06 18:40:42,845 - INFO - training batch 1101, loss: 1.850, 35232/60000 datapoints
2025-03-06 18:40:43,050 - INFO - training batch 1151, loss: 1.716, 36832/60000 datapoints
2025-03-06 18:40:43,252 - INFO - training batch 1201, loss: 1.835, 38432/60000 datapoints
2025-03-06 18:40:43,457 - INFO - training batch 1251, loss: 1.713, 40032/60000 datapoints
2025-03-06 18:40:43,659 - INFO - training batch 1301, loss: 1.834, 41632/60000 datapoints
2025-03-06 18:40:43,874 - INFO - training batch 1351, loss: 1.837, 43232/60000 datapoints
2025-03-06 18:40:44,084 - INFO - training batch 1401, loss: 1.850, 44832/60000 datapoints
2025-03-06 18:40:44,302 - INFO - training batch 1451, loss: 1.778, 46432/60000 datapoints
2025-03-06 18:40:44,509 - INFO - training batch 1501, loss: 1.826, 48032/60000 datapoints
2025-03-06 18:40:44,733 - INFO - training batch 1551, loss: 1.773, 49632/60000 datapoints
2025-03-06 18:40:44,953 - INFO - training batch 1601, loss: 1.759, 51232/60000 datapoints
2025-03-06 18:40:45,175 - INFO - training batch 1651, loss: 1.821, 52832/60000 datapoints
2025-03-06 18:40:45,388 - INFO - training batch 1701, loss: 1.814, 54432/60000 datapoints
2025-03-06 18:40:45,622 - INFO - training batch 1751, loss: 1.643, 56032/60000 datapoints
2025-03-06 18:40:45,904 - INFO - training batch 1801, loss: 1.819, 57632/60000 datapoints
2025-03-06 18:40:46,427 - INFO - training batch 1851, loss: 1.763, 59232/60000 datapoints
2025-03-06 18:40:46,593 - INFO - validation batch 1, loss: 1.796, 32/10016 datapoints
2025-03-06 18:40:46,808 - INFO - validation batch 51, loss: 1.799, 1632/10016 datapoints
2025-03-06 18:40:47,010 - INFO - validation batch 101, loss: 1.764, 3232/10016 datapoints
2025-03-06 18:40:47,200 - INFO - validation batch 151, loss: 1.740, 4832/10016 datapoints
2025-03-06 18:40:47,429 - INFO - validation batch 201, loss: 1.574, 6432/10016 datapoints
2025-03-06 18:40:47,661 - INFO - validation batch 251, loss: 1.699, 8032/10016 datapoints
2025-03-06 18:40:47,846 - INFO - validation batch 301, loss: 1.764, 9632/10016 datapoints
2025-03-06 18:40:47,892 - INFO - Epoch 21/800 done.
2025-03-06 18:40:47,892 - INFO - Final validation performance:
Loss: 1.734, top-1 acc: 0.537top-5 acc: 0.537
2025-03-06 18:40:47,893 - INFO - Beginning epoch 22/800
2025-03-06 18:40:47,903 - INFO - training batch 1, loss: 1.794, 32/60000 datapoints
2025-03-06 18:40:48,214 - INFO - training batch 51, loss: 1.730, 1632/60000 datapoints
2025-03-06 18:40:48,470 - INFO - training batch 101, loss: 1.838, 3232/60000 datapoints
2025-03-06 18:40:48,729 - INFO - training batch 151, loss: 1.704, 4832/60000 datapoints
2025-03-06 18:40:49,087 - INFO - training batch 201, loss: 1.837, 6432/60000 datapoints
2025-03-06 18:40:49,368 - INFO - training batch 251, loss: 1.767, 8032/60000 datapoints
2025-03-06 18:40:49,635 - INFO - training batch 301, loss: 1.787, 9632/60000 datapoints
2025-03-06 18:40:49,881 - INFO - training batch 351, loss: 1.807, 11232/60000 datapoints
2025-03-06 18:40:50,184 - INFO - training batch 401, loss: 1.722, 12832/60000 datapoints
2025-03-06 18:40:50,435 - INFO - training batch 451, loss: 1.789, 14432/60000 datapoints
2025-03-06 18:40:50,673 - INFO - training batch 501, loss: 1.704, 16032/60000 datapoints
2025-03-06 18:40:50,899 - INFO - training batch 551, loss: 1.784, 17632/60000 datapoints
2025-03-06 18:40:51,124 - INFO - training batch 601, loss: 1.779, 19232/60000 datapoints
2025-03-06 18:40:51,353 - INFO - training batch 651, loss: 1.750, 20832/60000 datapoints
2025-03-06 18:40:51,603 - INFO - training batch 701, loss: 1.801, 22432/60000 datapoints
2025-03-06 18:40:52,064 - INFO - training batch 751, loss: 1.750, 24032/60000 datapoints
2025-03-06 18:40:52,735 - INFO - training batch 801, loss: 1.812, 25632/60000 datapoints
2025-03-06 18:40:53,396 - INFO - training batch 851, loss: 1.767, 27232/60000 datapoints
2025-03-06 18:40:53,646 - INFO - training batch 901, loss: 1.707, 28832/60000 datapoints
2025-03-06 18:40:53,879 - INFO - training batch 951, loss: 1.788, 30432/60000 datapoints
2025-03-06 18:40:54,125 - INFO - training batch 1001, loss: 1.716, 32032/60000 datapoints
2025-03-06 18:40:54,461 - INFO - training batch 1051, loss: 1.719, 33632/60000 datapoints
2025-03-06 18:40:54,696 - INFO - training batch 1101, loss: 1.746, 35232/60000 datapoints
2025-03-06 18:40:54,930 - INFO - training batch 1151, loss: 1.728, 36832/60000 datapoints
2025-03-06 18:40:55,159 - INFO - training batch 1201, loss: 1.816, 38432/60000 datapoints
2025-03-06 18:40:55,386 - INFO - training batch 1251, loss: 1.771, 40032/60000 datapoints
2025-03-06 18:40:55,600 - INFO - training batch 1301, loss: 1.811, 41632/60000 datapoints
2025-03-06 18:40:55,815 - INFO - training batch 1351, loss: 1.711, 43232/60000 datapoints
2025-03-06 18:40:56,034 - INFO - training batch 1401, loss: 1.641, 44832/60000 datapoints
2025-03-06 18:40:56,248 - INFO - training batch 1451, loss: 1.883, 46432/60000 datapoints
2025-03-06 18:40:56,465 - INFO - training batch 1501, loss: 1.600, 48032/60000 datapoints
2025-03-06 18:40:56,680 - INFO - training batch 1551, loss: 1.767, 49632/60000 datapoints
2025-03-06 18:40:56,896 - INFO - training batch 1601, loss: 1.814, 51232/60000 datapoints
2025-03-06 18:40:57,105 - INFO - training batch 1651, loss: 1.735, 52832/60000 datapoints
2025-03-06 18:40:57,320 - INFO - training batch 1701, loss: 1.805, 54432/60000 datapoints
2025-03-06 18:40:57,539 - INFO - training batch 1751, loss: 1.772, 56032/60000 datapoints
2025-03-06 18:40:57,769 - INFO - training batch 1801, loss: 1.802, 57632/60000 datapoints
2025-03-06 18:40:57,976 - INFO - training batch 1851, loss: 1.787, 59232/60000 datapoints
2025-03-06 18:40:58,085 - INFO - validation batch 1, loss: 1.659, 32/10016 datapoints
2025-03-06 18:40:58,239 - INFO - validation batch 51, loss: 1.662, 1632/10016 datapoints
2025-03-06 18:40:58,405 - INFO - validation batch 101, loss: 1.730, 3232/10016 datapoints
2025-03-06 18:40:58,588 - INFO - validation batch 151, loss: 1.597, 4832/10016 datapoints
2025-03-06 18:40:58,763 - INFO - validation batch 201, loss: 1.647, 6432/10016 datapoints
2025-03-06 18:40:58,931 - INFO - validation batch 251, loss: 1.684, 8032/10016 datapoints
2025-03-06 18:40:59,090 - INFO - validation batch 301, loss: 1.802, 9632/10016 datapoints
2025-03-06 18:40:59,129 - INFO - Epoch 22/800 done.
2025-03-06 18:40:59,129 - INFO - Final validation performance:
Loss: 1.683, top-1 acc: 0.549top-5 acc: 0.549
2025-03-06 18:40:59,130 - INFO - Beginning epoch 23/800
2025-03-06 18:40:59,137 - INFO - training batch 1, loss: 1.853, 32/60000 datapoints
2025-03-06 18:40:59,346 - INFO - training batch 51, loss: 1.810, 1632/60000 datapoints
2025-03-06 18:40:59,559 - INFO - training batch 101, loss: 1.761, 3232/60000 datapoints
2025-03-06 18:40:59,774 - INFO - training batch 151, loss: 1.751, 4832/60000 datapoints
2025-03-06 18:40:59,978 - INFO - training batch 201, loss: 1.671, 6432/60000 datapoints
2025-03-06 18:41:00,190 - INFO - training batch 251, loss: 1.790, 8032/60000 datapoints
2025-03-06 18:41:00,410 - INFO - training batch 301, loss: 1.728, 9632/60000 datapoints
2025-03-06 18:41:00,647 - INFO - training batch 351, loss: 1.777, 11232/60000 datapoints
2025-03-06 18:41:00,893 - INFO - training batch 401, loss: 1.587, 12832/60000 datapoints
2025-03-06 18:41:01,111 - INFO - training batch 451, loss: 1.675, 14432/60000 datapoints
2025-03-06 18:41:01,375 - INFO - training batch 501, loss: 1.750, 16032/60000 datapoints
2025-03-06 18:41:01,635 - INFO - training batch 551, loss: 1.653, 17632/60000 datapoints
2025-03-06 18:41:01,861 - INFO - training batch 601, loss: 1.767, 19232/60000 datapoints
2025-03-06 18:41:02,223 - INFO - training batch 651, loss: 1.769, 20832/60000 datapoints
2025-03-06 18:41:02,670 - INFO - training batch 701, loss: 1.665, 22432/60000 datapoints
2025-03-06 18:41:03,022 - INFO - training batch 751, loss: 1.746, 24032/60000 datapoints
2025-03-06 18:41:03,316 - INFO - training batch 801, loss: 1.625, 25632/60000 datapoints
2025-03-06 18:41:03,665 - INFO - training batch 851, loss: 1.628, 27232/60000 datapoints
2025-03-06 18:41:04,085 - INFO - training batch 901, loss: 1.716, 28832/60000 datapoints
2025-03-06 18:41:04,447 - INFO - training batch 951, loss: 1.654, 30432/60000 datapoints
2025-03-06 18:41:04,707 - INFO - training batch 1001, loss: 1.680, 32032/60000 datapoints
2025-03-06 18:41:04,989 - INFO - training batch 1051, loss: 1.689, 33632/60000 datapoints
2025-03-06 18:41:05,259 - INFO - training batch 1101, loss: 1.702, 35232/60000 datapoints
2025-03-06 18:41:05,502 - INFO - training batch 1151, loss: 1.766, 36832/60000 datapoints
2025-03-06 18:41:05,733 - INFO - training batch 1201, loss: 1.784, 38432/60000 datapoints
2025-03-06 18:41:05,947 - INFO - training batch 1251, loss: 1.696, 40032/60000 datapoints
2025-03-06 18:41:06,182 - INFO - training batch 1301, loss: 1.654, 41632/60000 datapoints
2025-03-06 18:41:06,397 - INFO - training batch 1351, loss: 1.683, 43232/60000 datapoints
2025-03-06 18:41:06,616 - INFO - training batch 1401, loss: 1.686, 44832/60000 datapoints
2025-03-06 18:41:06,833 - INFO - training batch 1451, loss: 1.741, 46432/60000 datapoints
2025-03-06 18:41:07,135 - INFO - training batch 1501, loss: 1.809, 48032/60000 datapoints
2025-03-06 18:41:07,376 - INFO - training batch 1551, loss: 1.754, 49632/60000 datapoints
2025-03-06 18:41:07,630 - INFO - training batch 1601, loss: 1.747, 51232/60000 datapoints
2025-03-06 18:41:07,852 - INFO - training batch 1651, loss: 1.506, 52832/60000 datapoints
2025-03-06 18:41:08,076 - INFO - training batch 1701, loss: 1.822, 54432/60000 datapoints
2025-03-06 18:41:08,302 - INFO - training batch 1751, loss: 1.579, 56032/60000 datapoints
2025-03-06 18:41:08,522 - INFO - training batch 1801, loss: 1.731, 57632/60000 datapoints
2025-03-06 18:41:08,778 - INFO - training batch 1851, loss: 1.745, 59232/60000 datapoints
2025-03-06 18:41:08,903 - INFO - validation batch 1, loss: 1.601, 32/10016 datapoints
2025-03-06 18:41:09,072 - INFO - validation batch 51, loss: 1.681, 1632/10016 datapoints
2025-03-06 18:41:09,235 - INFO - validation batch 101, loss: 1.720, 3232/10016 datapoints
2025-03-06 18:41:09,400 - INFO - validation batch 151, loss: 1.733, 4832/10016 datapoints
2025-03-06 18:41:09,567 - INFO - validation batch 201, loss: 1.703, 6432/10016 datapoints
2025-03-06 18:41:09,739 - INFO - validation batch 251, loss: 1.731, 8032/10016 datapoints
2025-03-06 18:41:09,899 - INFO - validation batch 301, loss: 1.584, 9632/10016 datapoints
2025-03-06 18:41:09,941 - INFO - Epoch 23/800 done.
2025-03-06 18:41:09,941 - INFO - Final validation performance:
Loss: 1.679, top-1 acc: 0.561top-5 acc: 0.561
2025-03-06 18:41:09,942 - INFO - Beginning epoch 24/800
2025-03-06 18:41:09,950 - INFO - training batch 1, loss: 1.758, 32/60000 datapoints
2025-03-06 18:41:10,162 - INFO - training batch 51, loss: 1.757, 1632/60000 datapoints
2025-03-06 18:41:10,370 - INFO - training batch 101, loss: 1.674, 3232/60000 datapoints
2025-03-06 18:41:10,727 - INFO - training batch 151, loss: 1.758, 4832/60000 datapoints
2025-03-06 18:41:11,101 - INFO - training batch 201, loss: 1.609, 6432/60000 datapoints
2025-03-06 18:41:11,369 - INFO - training batch 251, loss: 1.792, 8032/60000 datapoints
2025-03-06 18:41:11,644 - INFO - training batch 301, loss: 1.820, 9632/60000 datapoints
2025-03-06 18:41:11,894 - INFO - training batch 351, loss: 1.756, 11232/60000 datapoints
2025-03-06 18:41:12,200 - INFO - training batch 401, loss: 1.670, 12832/60000 datapoints
2025-03-06 18:41:12,433 - INFO - training batch 451, loss: 1.723, 14432/60000 datapoints
2025-03-06 18:41:12,659 - INFO - training batch 501, loss: 1.651, 16032/60000 datapoints
2025-03-06 18:41:12,881 - INFO - training batch 551, loss: 1.608, 17632/60000 datapoints
2025-03-06 18:41:13,100 - INFO - training batch 601, loss: 1.763, 19232/60000 datapoints
2025-03-06 18:41:13,310 - INFO - training batch 651, loss: 1.738, 20832/60000 datapoints
2025-03-06 18:41:13,571 - INFO - training batch 701, loss: 1.653, 22432/60000 datapoints
2025-03-06 18:41:13,837 - INFO - training batch 751, loss: 1.634, 24032/60000 datapoints
2025-03-06 18:41:14,088 - INFO - training batch 801, loss: 1.714, 25632/60000 datapoints
2025-03-06 18:41:14,296 - INFO - training batch 851, loss: 1.725, 27232/60000 datapoints
2025-03-06 18:41:14,506 - INFO - training batch 901, loss: 1.676, 28832/60000 datapoints
2025-03-06 18:41:14,717 - INFO - training batch 951, loss: 1.763, 30432/60000 datapoints
2025-03-06 18:41:14,929 - INFO - training batch 1001, loss: 1.683, 32032/60000 datapoints
2025-03-06 18:41:15,165 - INFO - training batch 1051, loss: 1.607, 33632/60000 datapoints
2025-03-06 18:41:15,381 - INFO - training batch 1101, loss: 1.659, 35232/60000 datapoints
2025-03-06 18:41:15,623 - INFO - training batch 1151, loss: 1.740, 36832/60000 datapoints
2025-03-06 18:41:15,839 - INFO - training batch 1201, loss: 1.744, 38432/60000 datapoints
2025-03-06 18:41:16,066 - INFO - training batch 1251, loss: 1.704, 40032/60000 datapoints
2025-03-06 18:41:16,285 - INFO - training batch 1301, loss: 1.739, 41632/60000 datapoints
2025-03-06 18:41:16,493 - INFO - training batch 1351, loss: 1.714, 43232/60000 datapoints
2025-03-06 18:41:16,758 - INFO - training batch 1401, loss: 1.604, 44832/60000 datapoints
2025-03-06 18:41:17,009 - INFO - training batch 1451, loss: 1.594, 46432/60000 datapoints
2025-03-06 18:41:17,214 - INFO - training batch 1501, loss: 1.727, 48032/60000 datapoints
2025-03-06 18:41:17,425 - INFO - training batch 1551, loss: 1.815, 49632/60000 datapoints
2025-03-06 18:41:17,641 - INFO - training batch 1601, loss: 1.722, 51232/60000 datapoints
2025-03-06 18:41:17,848 - INFO - training batch 1651, loss: 1.677, 52832/60000 datapoints
2025-03-06 18:41:18,055 - INFO - training batch 1701, loss: 1.611, 54432/60000 datapoints
2025-03-06 18:41:18,261 - INFO - training batch 1751, loss: 1.677, 56032/60000 datapoints
2025-03-06 18:41:18,479 - INFO - training batch 1801, loss: 1.607, 57632/60000 datapoints
2025-03-06 18:41:18,693 - INFO - training batch 1851, loss: 1.745, 59232/60000 datapoints
2025-03-06 18:41:18,815 - INFO - validation batch 1, loss: 1.721, 32/10016 datapoints
2025-03-06 18:41:19,009 - INFO - validation batch 51, loss: 1.732, 1632/10016 datapoints
2025-03-06 18:41:19,247 - INFO - validation batch 101, loss: 1.695, 3232/10016 datapoints
2025-03-06 18:41:19,414 - INFO - validation batch 151, loss: 1.655, 4832/10016 datapoints
2025-03-06 18:41:19,594 - INFO - validation batch 201, loss: 1.706, 6432/10016 datapoints
2025-03-06 18:41:19,775 - INFO - validation batch 251, loss: 1.611, 8032/10016 datapoints
2025-03-06 18:41:19,951 - INFO - validation batch 301, loss: 1.576, 9632/10016 datapoints
2025-03-06 18:41:19,992 - INFO - Epoch 24/800 done.
2025-03-06 18:41:19,992 - INFO - Final validation performance:
Loss: 1.671, top-1 acc: 0.575top-5 acc: 0.575
2025-03-06 18:41:19,993 - INFO - Beginning epoch 25/800
2025-03-06 18:41:20,000 - INFO - training batch 1, loss: 1.642, 32/60000 datapoints
2025-03-06 18:41:20,282 - INFO - training batch 51, loss: 1.597, 1632/60000 datapoints
2025-03-06 18:41:20,493 - INFO - training batch 101, loss: 1.674, 3232/60000 datapoints
2025-03-06 18:41:20,714 - INFO - training batch 151, loss: 1.691, 4832/60000 datapoints
2025-03-06 18:41:20,926 - INFO - training batch 201, loss: 1.705, 6432/60000 datapoints
2025-03-06 18:41:21,158 - INFO - training batch 251, loss: 1.760, 8032/60000 datapoints
2025-03-06 18:41:21,382 - INFO - training batch 301, loss: 1.652, 9632/60000 datapoints
2025-03-06 18:41:21,634 - INFO - training batch 351, loss: 1.656, 11232/60000 datapoints
2025-03-06 18:41:21,889 - INFO - training batch 401, loss: 1.694, 12832/60000 datapoints
2025-03-06 18:41:22,105 - INFO - training batch 451, loss: 1.666, 14432/60000 datapoints
2025-03-06 18:41:22,333 - INFO - training batch 501, loss: 1.624, 16032/60000 datapoints
2025-03-06 18:41:22,565 - INFO - training batch 551, loss: 1.693, 17632/60000 datapoints
2025-03-06 18:41:22,802 - INFO - training batch 601, loss: 1.671, 19232/60000 datapoints
2025-03-06 18:41:23,018 - INFO - training batch 651, loss: 1.709, 20832/60000 datapoints
2025-03-06 18:41:23,233 - INFO - training batch 701, loss: 1.713, 22432/60000 datapoints
2025-03-06 18:41:23,457 - INFO - training batch 751, loss: 1.589, 24032/60000 datapoints
2025-03-06 18:41:23,682 - INFO - training batch 801, loss: 1.588, 25632/60000 datapoints
2025-03-06 18:41:23,902 - INFO - training batch 851, loss: 1.566, 27232/60000 datapoints
2025-03-06 18:41:24,123 - INFO - training batch 901, loss: 1.700, 28832/60000 datapoints
2025-03-06 18:41:24,591 - INFO - training batch 951, loss: 1.667, 30432/60000 datapoints
2025-03-06 18:41:24,858 - INFO - training batch 1001, loss: 1.618, 32032/60000 datapoints
2025-03-06 18:41:25,075 - INFO - training batch 1051, loss: 1.622, 33632/60000 datapoints
2025-03-06 18:41:25,296 - INFO - training batch 1101, loss: 1.729, 35232/60000 datapoints
2025-03-06 18:41:25,517 - INFO - training batch 1151, loss: 1.605, 36832/60000 datapoints
2025-03-06 18:41:25,750 - INFO - training batch 1201, loss: 1.631, 38432/60000 datapoints
2025-03-06 18:41:25,958 - INFO - training batch 1251, loss: 1.560, 40032/60000 datapoints
2025-03-06 18:41:26,169 - INFO - training batch 1301, loss: 1.667, 41632/60000 datapoints
2025-03-06 18:41:26,381 - INFO - training batch 1351, loss: 1.633, 43232/60000 datapoints
2025-03-06 18:41:26,607 - INFO - training batch 1401, loss: 1.563, 44832/60000 datapoints
2025-03-06 18:41:26,826 - INFO - training batch 1451, loss: 1.813, 46432/60000 datapoints
2025-03-06 18:41:27,053 - INFO - training batch 1501, loss: 1.700, 48032/60000 datapoints
2025-03-06 18:41:27,283 - INFO - training batch 1551, loss: 1.500, 49632/60000 datapoints
2025-03-06 18:41:27,540 - INFO - training batch 1601, loss: 1.587, 51232/60000 datapoints
2025-03-06 18:41:27,788 - INFO - training batch 1651, loss: 1.516, 52832/60000 datapoints
2025-03-06 18:41:27,999 - INFO - training batch 1701, loss: 1.629, 54432/60000 datapoints
2025-03-06 18:41:28,231 - INFO - training batch 1751, loss: 1.641, 56032/60000 datapoints
2025-03-06 18:41:28,446 - INFO - training batch 1801, loss: 1.712, 57632/60000 datapoints
2025-03-06 18:41:28,661 - INFO - training batch 1851, loss: 1.647, 59232/60000 datapoints
2025-03-06 18:41:28,771 - INFO - validation batch 1, loss: 1.642, 32/10016 datapoints
2025-03-06 18:41:28,955 - INFO - validation batch 51, loss: 1.640, 1632/10016 datapoints
2025-03-06 18:41:29,124 - INFO - validation batch 101, loss: 1.569, 3232/10016 datapoints
2025-03-06 18:41:29,310 - INFO - validation batch 151, loss: 1.739, 4832/10016 datapoints
2025-03-06 18:41:29,612 - INFO - validation batch 201, loss: 1.621, 6432/10016 datapoints
2025-03-06 18:41:29,797 - INFO - validation batch 251, loss: 1.597, 8032/10016 datapoints
2025-03-06 18:41:29,963 - INFO - validation batch 301, loss: 1.678, 9632/10016 datapoints
2025-03-06 18:41:30,005 - INFO - Epoch 25/800 done.
2025-03-06 18:41:30,005 - INFO - Final validation performance:
Loss: 1.641, top-1 acc: 0.592top-5 acc: 0.592
2025-03-06 18:41:30,006 - INFO - Beginning epoch 26/800
2025-03-06 18:41:30,012 - INFO - training batch 1, loss: 1.605, 32/60000 datapoints
2025-03-06 18:41:30,229 - INFO - training batch 51, loss: 1.611, 1632/60000 datapoints
2025-03-06 18:41:30,451 - INFO - training batch 101, loss: 1.632, 3232/60000 datapoints
2025-03-06 18:41:30,678 - INFO - training batch 151, loss: 1.673, 4832/60000 datapoints
2025-03-06 18:41:30,889 - INFO - training batch 201, loss: 1.710, 6432/60000 datapoints
2025-03-06 18:41:31,103 - INFO - training batch 251, loss: 1.616, 8032/60000 datapoints
2025-03-06 18:41:31,309 - INFO - training batch 301, loss: 1.666, 9632/60000 datapoints
2025-03-06 18:41:31,534 - INFO - training batch 351, loss: 1.596, 11232/60000 datapoints
2025-03-06 18:41:31,756 - INFO - training batch 401, loss: 1.594, 12832/60000 datapoints
2025-03-06 18:41:31,958 - INFO - training batch 451, loss: 1.652, 14432/60000 datapoints
2025-03-06 18:41:32,162 - INFO - training batch 501, loss: 1.528, 16032/60000 datapoints
2025-03-06 18:41:32,360 - INFO - training batch 551, loss: 1.707, 17632/60000 datapoints
2025-03-06 18:41:32,571 - INFO - training batch 601, loss: 1.630, 19232/60000 datapoints
2025-03-06 18:41:32,795 - INFO - training batch 651, loss: 1.585, 20832/60000 datapoints
2025-03-06 18:41:32,993 - INFO - training batch 701, loss: 1.619, 22432/60000 datapoints
2025-03-06 18:41:33,195 - INFO - training batch 751, loss: 1.550, 24032/60000 datapoints
2025-03-06 18:41:33,396 - INFO - training batch 801, loss: 1.657, 25632/60000 datapoints
2025-03-06 18:41:33,614 - INFO - training batch 851, loss: 1.651, 27232/60000 datapoints
2025-03-06 18:41:33,830 - INFO - training batch 901, loss: 1.596, 28832/60000 datapoints
2025-03-06 18:41:34,029 - INFO - training batch 951, loss: 1.633, 30432/60000 datapoints
2025-03-06 18:41:34,249 - INFO - training batch 1001, loss: 1.620, 32032/60000 datapoints
2025-03-06 18:41:34,473 - INFO - training batch 1051, loss: 1.479, 33632/60000 datapoints
2025-03-06 18:41:34,686 - INFO - training batch 1101, loss: 1.646, 35232/60000 datapoints
2025-03-06 18:41:34,891 - INFO - training batch 1151, loss: 1.652, 36832/60000 datapoints
2025-03-06 18:41:35,093 - INFO - training batch 1201, loss: 1.472, 38432/60000 datapoints
2025-03-06 18:41:35,297 - INFO - training batch 1251, loss: 1.708, 40032/60000 datapoints
2025-03-06 18:41:35,528 - INFO - training batch 1301, loss: 1.563, 41632/60000 datapoints
2025-03-06 18:41:35,747 - INFO - training batch 1351, loss: 1.563, 43232/60000 datapoints
2025-03-06 18:41:35,960 - INFO - training batch 1401, loss: 1.678, 44832/60000 datapoints
2025-03-06 18:41:36,182 - INFO - training batch 1451, loss: 1.674, 46432/60000 datapoints
2025-03-06 18:41:36,394 - INFO - training batch 1501, loss: 1.591, 48032/60000 datapoints
2025-03-06 18:41:36,594 - INFO - training batch 1551, loss: 1.666, 49632/60000 datapoints
2025-03-06 18:41:36,808 - INFO - training batch 1601, loss: 1.644, 51232/60000 datapoints
2025-03-06 18:41:37,012 - INFO - training batch 1651, loss: 1.693, 52832/60000 datapoints
2025-03-06 18:41:37,223 - INFO - training batch 1701, loss: 1.513, 54432/60000 datapoints
2025-03-06 18:41:37,434 - INFO - training batch 1751, loss: 1.583, 56032/60000 datapoints
2025-03-06 18:41:37,675 - INFO - training batch 1801, loss: 1.765, 57632/60000 datapoints
2025-03-06 18:41:37,879 - INFO - training batch 1851, loss: 1.662, 59232/60000 datapoints
2025-03-06 18:41:37,983 - INFO - validation batch 1, loss: 1.597, 32/10016 datapoints
2025-03-06 18:41:38,143 - INFO - validation batch 51, loss: 1.601, 1632/10016 datapoints
2025-03-06 18:41:38,318 - INFO - validation batch 101, loss: 1.567, 3232/10016 datapoints
2025-03-06 18:41:38,476 - INFO - validation batch 151, loss: 1.547, 4832/10016 datapoints
2025-03-06 18:41:38,651 - INFO - validation batch 201, loss: 1.592, 6432/10016 datapoints
2025-03-06 18:41:38,823 - INFO - validation batch 251, loss: 1.555, 8032/10016 datapoints
2025-03-06 18:41:39,012 - INFO - validation batch 301, loss: 1.576, 9632/10016 datapoints
2025-03-06 18:41:39,056 - INFO - Epoch 26/800 done.
2025-03-06 18:41:39,056 - INFO - Final validation performance:
Loss: 1.577, top-1 acc: 0.613top-5 acc: 0.613
2025-03-06 18:41:39,057 - INFO - Beginning epoch 27/800
2025-03-06 18:41:39,064 - INFO - training batch 1, loss: 1.480, 32/60000 datapoints
2025-03-06 18:41:39,293 - INFO - training batch 51, loss: 1.594, 1632/60000 datapoints
2025-03-06 18:41:39,500 - INFO - training batch 101, loss: 1.622, 3232/60000 datapoints
2025-03-06 18:41:39,711 - INFO - training batch 151, loss: 1.647, 4832/60000 datapoints
2025-03-06 18:41:39,929 - INFO - training batch 201, loss: 1.602, 6432/60000 datapoints
2025-03-06 18:41:40,130 - INFO - training batch 251, loss: 1.524, 8032/60000 datapoints
2025-03-06 18:41:40,334 - INFO - training batch 301, loss: 1.512, 9632/60000 datapoints
2025-03-06 18:41:40,549 - INFO - training batch 351, loss: 1.596, 11232/60000 datapoints
2025-03-06 18:41:40,760 - INFO - training batch 401, loss: 1.690, 12832/60000 datapoints
2025-03-06 18:41:40,960 - INFO - training batch 451, loss: 1.527, 14432/60000 datapoints
2025-03-06 18:41:41,165 - INFO - training batch 501, loss: 1.543, 16032/60000 datapoints
2025-03-06 18:41:41,369 - INFO - training batch 551, loss: 1.535, 17632/60000 datapoints
2025-03-06 18:41:41,572 - INFO - training batch 601, loss: 1.645, 19232/60000 datapoints
2025-03-06 18:41:41,791 - INFO - training batch 651, loss: 1.580, 20832/60000 datapoints
2025-03-06 18:41:42,014 - INFO - training batch 701, loss: 1.504, 22432/60000 datapoints
2025-03-06 18:41:42,237 - INFO - training batch 751, loss: 1.563, 24032/60000 datapoints
2025-03-06 18:41:42,448 - INFO - training batch 801, loss: 1.517, 25632/60000 datapoints
2025-03-06 18:41:42,655 - INFO - training batch 851, loss: 1.562, 27232/60000 datapoints
2025-03-06 18:41:42,866 - INFO - training batch 901, loss: 1.567, 28832/60000 datapoints
2025-03-06 18:41:43,066 - INFO - training batch 951, loss: 1.622, 30432/60000 datapoints
2025-03-06 18:41:43,266 - INFO - training batch 1001, loss: 1.644, 32032/60000 datapoints
2025-03-06 18:41:43,465 - INFO - training batch 1051, loss: 1.563, 33632/60000 datapoints
2025-03-06 18:41:43,670 - INFO - training batch 1101, loss: 1.606, 35232/60000 datapoints
2025-03-06 18:41:43,874 - INFO - training batch 1151, loss: 1.586, 36832/60000 datapoints
2025-03-06 18:41:44,073 - INFO - training batch 1201, loss: 1.620, 38432/60000 datapoints
2025-03-06 18:41:44,282 - INFO - training batch 1251, loss: 1.440, 40032/60000 datapoints
2025-03-06 18:41:44,490 - INFO - training batch 1301, loss: 1.537, 41632/60000 datapoints
2025-03-06 18:41:44,718 - INFO - training batch 1351, loss: 1.548, 43232/60000 datapoints
2025-03-06 18:41:44,931 - INFO - training batch 1401, loss: 1.631, 44832/60000 datapoints
2025-03-06 18:41:45,138 - INFO - training batch 1451, loss: 1.593, 46432/60000 datapoints
2025-03-06 18:41:45,345 - INFO - training batch 1501, loss: 1.457, 48032/60000 datapoints
2025-03-06 18:41:45,552 - INFO - training batch 1551, loss: 1.472, 49632/60000 datapoints
2025-03-06 18:41:45,768 - INFO - training batch 1601, loss: 1.567, 51232/60000 datapoints
2025-03-06 18:41:45,992 - INFO - training batch 1651, loss: 1.579, 52832/60000 datapoints
2025-03-06 18:41:46,198 - INFO - training batch 1701, loss: 1.390, 54432/60000 datapoints
2025-03-06 18:41:46,405 - INFO - training batch 1751, loss: 1.581, 56032/60000 datapoints
2025-03-06 18:41:46,619 - INFO - training batch 1801, loss: 1.647, 57632/60000 datapoints
2025-03-06 18:41:46,863 - INFO - training batch 1851, loss: 1.632, 59232/60000 datapoints
2025-03-06 18:41:46,966 - INFO - validation batch 1, loss: 1.553, 32/10016 datapoints
2025-03-06 18:41:47,126 - INFO - validation batch 51, loss: 1.592, 1632/10016 datapoints
2025-03-06 18:41:47,288 - INFO - validation batch 101, loss: 1.461, 3232/10016 datapoints
2025-03-06 18:41:47,449 - INFO - validation batch 151, loss: 1.486, 4832/10016 datapoints
2025-03-06 18:41:47,631 - INFO - validation batch 201, loss: 1.583, 6432/10016 datapoints
2025-03-06 18:41:47,791 - INFO - validation batch 251, loss: 1.600, 8032/10016 datapoints
2025-03-06 18:41:47,949 - INFO - validation batch 301, loss: 1.415, 9632/10016 datapoints
2025-03-06 18:41:47,988 - INFO - Epoch 27/800 done.
2025-03-06 18:41:47,988 - INFO - Final validation performance:
Loss: 1.527, top-1 acc: 0.632top-5 acc: 0.632
2025-03-06 18:41:47,989 - INFO - Beginning epoch 28/800
2025-03-06 18:41:47,996 - INFO - training batch 1, loss: 1.459, 32/60000 datapoints
2025-03-06 18:41:48,243 - INFO - training batch 51, loss: 1.578, 1632/60000 datapoints
2025-03-06 18:41:48,596 - INFO - training batch 101, loss: 1.586, 3232/60000 datapoints
2025-03-06 18:41:48,926 - INFO - training batch 151, loss: 1.647, 4832/60000 datapoints
2025-03-06 18:41:49,147 - INFO - training batch 201, loss: 1.659, 6432/60000 datapoints
2025-03-06 18:41:49,353 - INFO - training batch 251, loss: 1.590, 8032/60000 datapoints
2025-03-06 18:41:49,663 - INFO - training batch 301, loss: 1.489, 9632/60000 datapoints
2025-03-06 18:41:49,972 - INFO - training batch 351, loss: 1.433, 11232/60000 datapoints
2025-03-06 18:41:50,247 - INFO - training batch 401, loss: 1.505, 12832/60000 datapoints
2025-03-06 18:41:50,540 - INFO - training batch 451, loss: 1.689, 14432/60000 datapoints
2025-03-06 18:41:50,877 - INFO - training batch 501, loss: 1.567, 16032/60000 datapoints
2025-03-06 18:41:51,262 - INFO - training batch 551, loss: 1.574, 17632/60000 datapoints
2025-03-06 18:41:51,609 - INFO - training batch 601, loss: 1.476, 19232/60000 datapoints
2025-03-06 18:41:51,973 - INFO - training batch 651, loss: 1.601, 20832/60000 datapoints
2025-03-06 18:41:52,327 - INFO - training batch 701, loss: 1.454, 22432/60000 datapoints
2025-03-06 18:41:52,670 - INFO - training batch 751, loss: 1.548, 24032/60000 datapoints
2025-03-06 18:41:52,940 - INFO - training batch 801, loss: 1.634, 25632/60000 datapoints
2025-03-06 18:41:53,165 - INFO - training batch 851, loss: 1.483, 27232/60000 datapoints
2025-03-06 18:41:53,392 - INFO - training batch 901, loss: 1.750, 28832/60000 datapoints
2025-03-06 18:41:53,621 - INFO - training batch 951, loss: 1.545, 30432/60000 datapoints
2025-03-06 18:41:53,843 - INFO - training batch 1001, loss: 1.463, 32032/60000 datapoints
2025-03-06 18:41:54,057 - INFO - training batch 1051, loss: 1.374, 33632/60000 datapoints
2025-03-06 18:41:54,269 - INFO - training batch 1101, loss: 1.581, 35232/60000 datapoints
2025-03-06 18:41:54,487 - INFO - training batch 1151, loss: 1.472, 36832/60000 datapoints
2025-03-06 18:41:54,705 - INFO - training batch 1201, loss: 1.577, 38432/60000 datapoints
2025-03-06 18:41:54,925 - INFO - training batch 1251, loss: 1.658, 40032/60000 datapoints
2025-03-06 18:41:55,136 - INFO - training batch 1301, loss: 1.575, 41632/60000 datapoints
2025-03-06 18:41:55,347 - INFO - training batch 1351, loss: 1.448, 43232/60000 datapoints
2025-03-06 18:41:55,552 - INFO - training batch 1401, loss: 1.559, 44832/60000 datapoints
2025-03-06 18:41:55,765 - INFO - training batch 1451, loss: 1.527, 46432/60000 datapoints
2025-03-06 18:41:55,972 - INFO - training batch 1501, loss: 1.496, 48032/60000 datapoints
2025-03-06 18:41:56,187 - INFO - training batch 1551, loss: 1.681, 49632/60000 datapoints
2025-03-06 18:41:56,398 - INFO - training batch 1601, loss: 1.565, 51232/60000 datapoints
2025-03-06 18:41:56,601 - INFO - training batch 1651, loss: 1.515, 52832/60000 datapoints
2025-03-06 18:41:56,805 - INFO - training batch 1701, loss: 1.584, 54432/60000 datapoints
2025-03-06 18:41:57,009 - INFO - training batch 1751, loss: 1.444, 56032/60000 datapoints
2025-03-06 18:41:57,214 - INFO - training batch 1801, loss: 1.470, 57632/60000 datapoints
2025-03-06 18:41:57,434 - INFO - training batch 1851, loss: 1.672, 59232/60000 datapoints
2025-03-06 18:41:57,556 - INFO - validation batch 1, loss: 1.582, 32/10016 datapoints
2025-03-06 18:41:57,723 - INFO - validation batch 51, loss: 1.569, 1632/10016 datapoints
2025-03-06 18:41:57,890 - INFO - validation batch 101, loss: 1.485, 3232/10016 datapoints
2025-03-06 18:41:58,048 - INFO - validation batch 151, loss: 1.492, 4832/10016 datapoints
2025-03-06 18:41:58,209 - INFO - validation batch 201, loss: 1.401, 6432/10016 datapoints
2025-03-06 18:41:58,402 - INFO - validation batch 251, loss: 1.665, 8032/10016 datapoints
2025-03-06 18:41:58,569 - INFO - validation batch 301, loss: 1.554, 9632/10016 datapoints
2025-03-06 18:41:58,616 - INFO - Epoch 28/800 done.
2025-03-06 18:41:58,616 - INFO - Final validation performance:
Loss: 1.535, top-1 acc: 0.647top-5 acc: 0.647
2025-03-06 18:41:58,617 - INFO - Beginning epoch 29/800
2025-03-06 18:41:58,623 - INFO - training batch 1, loss: 1.645, 32/60000 datapoints
2025-03-06 18:41:58,829 - INFO - training batch 51, loss: 1.464, 1632/60000 datapoints
2025-03-06 18:41:59,038 - INFO - training batch 101, loss: 1.514, 3232/60000 datapoints
2025-03-06 18:41:59,293 - INFO - training batch 151, loss: 1.520, 4832/60000 datapoints
2025-03-06 18:41:59,495 - INFO - training batch 201, loss: 1.551, 6432/60000 datapoints
2025-03-06 18:41:59,704 - INFO - training batch 251, loss: 1.570, 8032/60000 datapoints
2025-03-06 18:41:59,923 - INFO - training batch 301, loss: 1.437, 9632/60000 datapoints
2025-03-06 18:42:00,129 - INFO - training batch 351, loss: 1.413, 11232/60000 datapoints
2025-03-06 18:42:00,340 - INFO - training batch 401, loss: 1.249, 12832/60000 datapoints
2025-03-06 18:42:00,539 - INFO - training batch 451, loss: 1.569, 14432/60000 datapoints
2025-03-06 18:42:00,748 - INFO - training batch 501, loss: 1.521, 16032/60000 datapoints
2025-03-06 18:42:00,953 - INFO - training batch 551, loss: 1.468, 17632/60000 datapoints
2025-03-06 18:42:01,155 - INFO - training batch 601, loss: 1.446, 19232/60000 datapoints
2025-03-06 18:42:01,354 - INFO - training batch 651, loss: 1.374, 20832/60000 datapoints
2025-03-06 18:42:01,552 - INFO - training batch 701, loss: 1.468, 22432/60000 datapoints
2025-03-06 18:42:01,760 - INFO - training batch 751, loss: 1.555, 24032/60000 datapoints
2025-03-06 18:42:01,964 - INFO - training batch 801, loss: 1.596, 25632/60000 datapoints
2025-03-06 18:42:02,161 - INFO - training batch 851, loss: 1.529, 27232/60000 datapoints
2025-03-06 18:42:02,367 - INFO - training batch 901, loss: 1.445, 28832/60000 datapoints
2025-03-06 18:42:02,569 - INFO - training batch 951, loss: 1.382, 30432/60000 datapoints
2025-03-06 18:42:02,773 - INFO - training batch 1001, loss: 1.377, 32032/60000 datapoints
2025-03-06 18:42:02,974 - INFO - training batch 1051, loss: 1.447, 33632/60000 datapoints
2025-03-06 18:42:03,178 - INFO - training batch 1101, loss: 1.436, 35232/60000 datapoints
2025-03-06 18:42:03,380 - INFO - training batch 1151, loss: 1.521, 36832/60000 datapoints
2025-03-06 18:42:03,583 - INFO - training batch 1201, loss: 1.474, 38432/60000 datapoints
2025-03-06 18:42:03,791 - INFO - training batch 1251, loss: 1.479, 40032/60000 datapoints
2025-03-06 18:42:03,991 - INFO - training batch 1301, loss: 1.539, 41632/60000 datapoints
2025-03-06 18:42:04,193 - INFO - training batch 1351, loss: 1.367, 43232/60000 datapoints
2025-03-06 18:42:04,390 - INFO - training batch 1401, loss: 1.393, 44832/60000 datapoints
2025-03-06 18:42:04,594 - INFO - training batch 1451, loss: 1.476, 46432/60000 datapoints
2025-03-06 18:42:04,805 - INFO - training batch 1501, loss: 1.391, 48032/60000 datapoints
2025-03-06 18:42:05,047 - INFO - training batch 1551, loss: 1.314, 49632/60000 datapoints
2025-03-06 18:42:05,270 - INFO - training batch 1601, loss: 1.654, 51232/60000 datapoints
2025-03-06 18:42:05,490 - INFO - training batch 1651, loss: 1.442, 52832/60000 datapoints
2025-03-06 18:42:05,702 - INFO - training batch 1701, loss: 1.550, 54432/60000 datapoints
2025-03-06 18:42:05,908 - INFO - training batch 1751, loss: 1.524, 56032/60000 datapoints
2025-03-06 18:42:06,109 - INFO - training batch 1801, loss: 1.449, 57632/60000 datapoints
2025-03-06 18:42:06,311 - INFO - training batch 1851, loss: 1.504, 59232/60000 datapoints
2025-03-06 18:42:06,415 - INFO - validation batch 1, loss: 1.476, 32/10016 datapoints
2025-03-06 18:42:06,575 - INFO - validation batch 51, loss: 1.295, 1632/10016 datapoints
2025-03-06 18:42:06,734 - INFO - validation batch 101, loss: 1.442, 3232/10016 datapoints
2025-03-06 18:42:06,888 - INFO - validation batch 151, loss: 1.392, 4832/10016 datapoints
2025-03-06 18:42:07,046 - INFO - validation batch 201, loss: 1.431, 6432/10016 datapoints
2025-03-06 18:42:07,201 - INFO - validation batch 251, loss: 1.301, 8032/10016 datapoints
2025-03-06 18:42:07,355 - INFO - validation batch 301, loss: 1.365, 9632/10016 datapoints
2025-03-06 18:42:07,391 - INFO - Epoch 29/800 done.
2025-03-06 18:42:07,392 - INFO - Final validation performance:
Loss: 1.386, top-1 acc: 0.664top-5 acc: 0.664
2025-03-06 18:42:07,392 - INFO - Beginning epoch 30/800
2025-03-06 18:42:07,398 - INFO - training batch 1, loss: 1.509, 32/60000 datapoints
2025-03-06 18:42:07,595 - INFO - training batch 51, loss: 1.471, 1632/60000 datapoints
2025-03-06 18:42:07,808 - INFO - training batch 101, loss: 1.526, 3232/60000 datapoints
2025-03-06 18:42:08,011 - INFO - training batch 151, loss: 1.544, 4832/60000 datapoints
2025-03-06 18:42:08,208 - INFO - training batch 201, loss: 1.434, 6432/60000 datapoints
2025-03-06 18:42:08,406 - INFO - training batch 251, loss: 1.340, 8032/60000 datapoints
2025-03-06 18:42:08,603 - INFO - training batch 301, loss: 1.407, 9632/60000 datapoints
2025-03-06 18:42:08,803 - INFO - training batch 351, loss: 1.565, 11232/60000 datapoints
2025-03-06 18:42:09,006 - INFO - training batch 401, loss: 1.323, 12832/60000 datapoints
2025-03-06 18:42:09,235 - INFO - training batch 451, loss: 1.503, 14432/60000 datapoints
2025-03-06 18:42:09,437 - INFO - training batch 501, loss: 1.473, 16032/60000 datapoints
2025-03-06 18:42:09,637 - INFO - training batch 551, loss: 1.421, 17632/60000 datapoints
2025-03-06 18:42:09,842 - INFO - training batch 601, loss: 1.496, 19232/60000 datapoints
2025-03-06 18:42:10,039 - INFO - training batch 651, loss: 1.375, 20832/60000 datapoints
2025-03-06 18:42:10,229 - INFO - training batch 701, loss: 1.476, 22432/60000 datapoints
2025-03-06 18:42:10,426 - INFO - training batch 751, loss: 1.395, 24032/60000 datapoints
2025-03-06 18:42:10,624 - INFO - training batch 801, loss: 1.453, 25632/60000 datapoints
2025-03-06 18:42:10,815 - INFO - training batch 851, loss: 1.461, 27232/60000 datapoints
2025-03-06 18:42:11,018 - INFO - training batch 901, loss: 1.499, 28832/60000 datapoints
2025-03-06 18:42:11,213 - INFO - training batch 951, loss: 1.475, 30432/60000 datapoints
2025-03-06 18:42:11,407 - INFO - training batch 1001, loss: 1.240, 32032/60000 datapoints
2025-03-06 18:42:11,607 - INFO - training batch 1051, loss: 1.464, 33632/60000 datapoints
2025-03-06 18:42:11,805 - INFO - training batch 1101, loss: 1.396, 35232/60000 datapoints
2025-03-06 18:42:12,004 - INFO - training batch 1151, loss: 1.491, 36832/60000 datapoints
2025-03-06 18:42:12,197 - INFO - training batch 1201, loss: 1.480, 38432/60000 datapoints
2025-03-06 18:42:12,393 - INFO - training batch 1251, loss: 1.388, 40032/60000 datapoints
2025-03-06 18:42:12,589 - INFO - training batch 1301, loss: 1.339, 41632/60000 datapoints
2025-03-06 18:42:12,792 - INFO - training batch 1351, loss: 1.432, 43232/60000 datapoints
2025-03-06 18:42:12,989 - INFO - training batch 1401, loss: 1.273, 44832/60000 datapoints
2025-03-06 18:42:13,186 - INFO - training batch 1451, loss: 1.415, 46432/60000 datapoints
2025-03-06 18:42:13,388 - INFO - training batch 1501, loss: 1.569, 48032/60000 datapoints
2025-03-06 18:42:13,584 - INFO - training batch 1551, loss: 1.268, 49632/60000 datapoints
2025-03-06 18:42:13,779 - INFO - training batch 1601, loss: 1.620, 51232/60000 datapoints
2025-03-06 18:42:13,976 - INFO - training batch 1651, loss: 1.538, 52832/60000 datapoints
2025-03-06 18:42:14,175 - INFO - training batch 1701, loss: 1.422, 54432/60000 datapoints
2025-03-06 18:42:14,367 - INFO - training batch 1751, loss: 1.362, 56032/60000 datapoints
2025-03-06 18:42:14,566 - INFO - training batch 1801, loss: 1.483, 57632/60000 datapoints
2025-03-06 18:42:14,763 - INFO - training batch 1851, loss: 1.491, 59232/60000 datapoints
2025-03-06 18:42:14,861 - INFO - validation batch 1, loss: 1.439, 32/10016 datapoints
2025-03-06 18:42:15,018 - INFO - validation batch 51, loss: 1.363, 1632/10016 datapoints
2025-03-06 18:42:15,175 - INFO - validation batch 101, loss: 1.615, 3232/10016 datapoints
2025-03-06 18:42:15,330 - INFO - validation batch 151, loss: 1.370, 4832/10016 datapoints
2025-03-06 18:42:15,479 - INFO - validation batch 201, loss: 1.313, 6432/10016 datapoints
2025-03-06 18:42:15,635 - INFO - validation batch 251, loss: 1.352, 8032/10016 datapoints
2025-03-06 18:42:15,788 - INFO - validation batch 301, loss: 1.363, 9632/10016 datapoints
2025-03-06 18:42:15,825 - INFO - Epoch 30/800 done.
2025-03-06 18:42:15,826 - INFO - Final validation performance:
Loss: 1.402, top-1 acc: 0.679top-5 acc: 0.679
2025-03-06 18:42:15,826 - INFO - Beginning epoch 31/800
2025-03-06 18:42:15,833 - INFO - training batch 1, loss: 1.688, 32/60000 datapoints
2025-03-06 18:42:16,034 - INFO - training batch 51, loss: 1.518, 1632/60000 datapoints
2025-03-06 18:42:16,244 - INFO - training batch 101, loss: 1.414, 3232/60000 datapoints
2025-03-06 18:42:16,441 - INFO - training batch 151, loss: 1.381, 4832/60000 datapoints
2025-03-06 18:42:16,654 - INFO - training batch 201, loss: 1.470, 6432/60000 datapoints
2025-03-06 18:42:16,873 - INFO - training batch 251, loss: 1.462, 8032/60000 datapoints
2025-03-06 18:42:17,074 - INFO - training batch 301, loss: 1.552, 9632/60000 datapoints
2025-03-06 18:42:17,269 - INFO - training batch 351, loss: 1.477, 11232/60000 datapoints
2025-03-06 18:42:17,469 - INFO - training batch 401, loss: 1.494, 12832/60000 datapoints
2025-03-06 18:42:17,669 - INFO - training batch 451, loss: 1.418, 14432/60000 datapoints
2025-03-06 18:42:17,870 - INFO - training batch 501, loss: 1.329, 16032/60000 datapoints
2025-03-06 18:42:18,069 - INFO - training batch 551, loss: 1.441, 17632/60000 datapoints
2025-03-06 18:42:18,265 - INFO - training batch 601, loss: 1.488, 19232/60000 datapoints
2025-03-06 18:42:18,463 - INFO - training batch 651, loss: 1.558, 20832/60000 datapoints
2025-03-06 18:42:18,670 - INFO - training batch 701, loss: 1.452, 22432/60000 datapoints
2025-03-06 18:42:18,870 - INFO - training batch 751, loss: 1.423, 24032/60000 datapoints
2025-03-06 18:42:19,069 - INFO - training batch 801, loss: 1.383, 25632/60000 datapoints
2025-03-06 18:42:19,276 - INFO - training batch 851, loss: 1.345, 27232/60000 datapoints
2025-03-06 18:42:19,478 - INFO - training batch 901, loss: 1.427, 28832/60000 datapoints
2025-03-06 18:42:19,679 - INFO - training batch 951, loss: 1.287, 30432/60000 datapoints
2025-03-06 18:42:19,878 - INFO - training batch 1001, loss: 1.551, 32032/60000 datapoints
2025-03-06 18:42:20,111 - INFO - training batch 1051, loss: 1.380, 33632/60000 datapoints
2025-03-06 18:42:20,337 - INFO - training batch 1101, loss: 1.312, 35232/60000 datapoints
2025-03-06 18:42:20,552 - INFO - training batch 1151, loss: 1.616, 36832/60000 datapoints
2025-03-06 18:42:20,767 - INFO - training batch 1201, loss: 1.406, 38432/60000 datapoints
2025-03-06 18:42:20,970 - INFO - training batch 1251, loss: 1.342, 40032/60000 datapoints
2025-03-06 18:42:21,169 - INFO - training batch 1301, loss: 1.482, 41632/60000 datapoints
2025-03-06 18:42:21,364 - INFO - training batch 1351, loss: 1.434, 43232/60000 datapoints
2025-03-06 18:42:21,566 - INFO - training batch 1401, loss: 1.467, 44832/60000 datapoints
2025-03-06 18:42:21,769 - INFO - training batch 1451, loss: 1.417, 46432/60000 datapoints
2025-03-06 18:42:21,968 - INFO - training batch 1501, loss: 1.485, 48032/60000 datapoints
2025-03-06 18:42:22,164 - INFO - training batch 1551, loss: 1.509, 49632/60000 datapoints
2025-03-06 18:42:22,359 - INFO - training batch 1601, loss: 1.593, 51232/60000 datapoints
2025-03-06 18:42:22,558 - INFO - training batch 1651, loss: 1.423, 52832/60000 datapoints
2025-03-06 18:42:22,820 - INFO - training batch 1701, loss: 1.411, 54432/60000 datapoints
2025-03-06 18:42:23,017 - INFO - training batch 1751, loss: 1.349, 56032/60000 datapoints
2025-03-06 18:42:23,215 - INFO - training batch 1801, loss: 1.526, 57632/60000 datapoints
2025-03-06 18:42:23,414 - INFO - training batch 1851, loss: 1.441, 59232/60000 datapoints
2025-03-06 18:42:23,525 - INFO - validation batch 1, loss: 1.396, 32/10016 datapoints
2025-03-06 18:42:23,686 - INFO - validation batch 51, loss: 1.450, 1632/10016 datapoints
2025-03-06 18:42:23,843 - INFO - validation batch 101, loss: 1.398, 3232/10016 datapoints
2025-03-06 18:42:24,000 - INFO - validation batch 151, loss: 1.346, 4832/10016 datapoints
2025-03-06 18:42:24,161 - INFO - validation batch 201, loss: 1.557, 6432/10016 datapoints
2025-03-06 18:42:24,313 - INFO - validation batch 251, loss: 1.331, 8032/10016 datapoints
2025-03-06 18:42:24,474 - INFO - validation batch 301, loss: 1.363, 9632/10016 datapoints
2025-03-06 18:42:24,512 - INFO - Epoch 31/800 done.
2025-03-06 18:42:24,512 - INFO - Final validation performance:
Loss: 1.406, top-1 acc: 0.692top-5 acc: 0.692
2025-03-06 18:42:24,513 - INFO - Beginning epoch 32/800
2025-03-06 18:42:24,519 - INFO - training batch 1, loss: 1.316, 32/60000 datapoints
2025-03-06 18:42:24,742 - INFO - training batch 51, loss: 1.518, 1632/60000 datapoints
2025-03-06 18:42:24,947 - INFO - training batch 101, loss: 1.497, 3232/60000 datapoints
2025-03-06 18:42:25,163 - INFO - training batch 151, loss: 1.209, 4832/60000 datapoints
2025-03-06 18:42:25,357 - INFO - training batch 201, loss: 1.253, 6432/60000 datapoints
2025-03-06 18:42:25,554 - INFO - training batch 251, loss: 1.437, 8032/60000 datapoints
2025-03-06 18:42:25,755 - INFO - training batch 301, loss: 1.256, 9632/60000 datapoints
2025-03-06 18:42:25,953 - INFO - training batch 351, loss: 1.397, 11232/60000 datapoints
2025-03-06 18:42:26,152 - INFO - training batch 401, loss: 1.382, 12832/60000 datapoints
2025-03-06 18:42:26,353 - INFO - training batch 451, loss: 1.406, 14432/60000 datapoints
2025-03-06 18:42:26,545 - INFO - training batch 501, loss: 1.483, 16032/60000 datapoints
2025-03-06 18:42:26,740 - INFO - training batch 551, loss: 1.409, 17632/60000 datapoints
2025-03-06 18:42:26,935 - INFO - training batch 601, loss: 1.301, 19232/60000 datapoints
2025-03-06 18:42:27,131 - INFO - training batch 651, loss: 1.348, 20832/60000 datapoints
2025-03-06 18:42:27,326 - INFO - training batch 701, loss: 1.372, 22432/60000 datapoints
2025-03-06 18:42:27,520 - INFO - training batch 751, loss: 1.468, 24032/60000 datapoints
2025-03-06 18:42:27,715 - INFO - training batch 801, loss: 1.383, 25632/60000 datapoints
2025-03-06 18:42:27,910 - INFO - training batch 851, loss: 1.403, 27232/60000 datapoints
2025-03-06 18:42:28,126 - INFO - training batch 901, loss: 1.198, 28832/60000 datapoints
2025-03-06 18:42:28,326 - INFO - training batch 951, loss: 1.419, 30432/60000 datapoints
2025-03-06 18:42:28,519 - INFO - training batch 1001, loss: 1.541, 32032/60000 datapoints
2025-03-06 18:42:28,721 - INFO - training batch 1051, loss: 1.281, 33632/60000 datapoints
2025-03-06 18:42:28,912 - INFO - training batch 1101, loss: 1.399, 35232/60000 datapoints
2025-03-06 18:42:29,112 - INFO - training batch 1151, loss: 1.377, 36832/60000 datapoints
2025-03-06 18:42:29,307 - INFO - training batch 1201, loss: 1.401, 38432/60000 datapoints
2025-03-06 18:42:29,521 - INFO - training batch 1251, loss: 1.477, 40032/60000 datapoints
2025-03-06 18:42:29,716 - INFO - training batch 1301, loss: 1.468, 41632/60000 datapoints
2025-03-06 18:42:29,912 - INFO - training batch 1351, loss: 1.420, 43232/60000 datapoints
2025-03-06 18:42:30,111 - INFO - training batch 1401, loss: 1.306, 44832/60000 datapoints
2025-03-06 18:42:30,307 - INFO - training batch 1451, loss: 1.399, 46432/60000 datapoints
2025-03-06 18:42:30,501 - INFO - training batch 1501, loss: 1.336, 48032/60000 datapoints
2025-03-06 18:42:30,699 - INFO - training batch 1551, loss: 1.465, 49632/60000 datapoints
2025-03-06 18:42:30,893 - INFO - training batch 1601, loss: 1.443, 51232/60000 datapoints
2025-03-06 18:42:31,088 - INFO - training batch 1651, loss: 1.477, 52832/60000 datapoints
2025-03-06 18:42:31,283 - INFO - training batch 1701, loss: 1.449, 54432/60000 datapoints
2025-03-06 18:42:31,478 - INFO - training batch 1751, loss: 1.403, 56032/60000 datapoints
2025-03-06 18:42:31,674 - INFO - training batch 1801, loss: 1.280, 57632/60000 datapoints
2025-03-06 18:42:31,872 - INFO - training batch 1851, loss: 1.475, 59232/60000 datapoints
2025-03-06 18:42:31,971 - INFO - validation batch 1, loss: 1.436, 32/10016 datapoints
2025-03-06 18:42:32,126 - INFO - validation batch 51, loss: 1.454, 1632/10016 datapoints
2025-03-06 18:42:32,280 - INFO - validation batch 101, loss: 1.302, 3232/10016 datapoints
2025-03-06 18:42:32,432 - INFO - validation batch 151, loss: 1.532, 4832/10016 datapoints
2025-03-06 18:42:32,584 - INFO - validation batch 201, loss: 1.348, 6432/10016 datapoints
2025-03-06 18:42:32,742 - INFO - validation batch 251, loss: 1.245, 8032/10016 datapoints
2025-03-06 18:42:32,893 - INFO - validation batch 301, loss: 1.305, 9632/10016 datapoints
2025-03-06 18:42:32,931 - INFO - Epoch 32/800 done.
2025-03-06 18:42:32,931 - INFO - Final validation performance:
Loss: 1.375, top-1 acc: 0.704top-5 acc: 0.704
2025-03-06 18:42:32,932 - INFO - Beginning epoch 33/800
2025-03-06 18:42:32,939 - INFO - training batch 1, loss: 1.467, 32/60000 datapoints
2025-03-06 18:42:33,144 - INFO - training batch 51, loss: 1.529, 1632/60000 datapoints
2025-03-06 18:42:33,344 - INFO - training batch 101, loss: 1.356, 3232/60000 datapoints
2025-03-06 18:42:33,540 - INFO - training batch 151, loss: 1.443, 4832/60000 datapoints
2025-03-06 18:42:33,747 - INFO - training batch 201, loss: 1.283, 6432/60000 datapoints
2025-03-06 18:42:33,948 - INFO - training batch 251, loss: 1.396, 8032/60000 datapoints
2025-03-06 18:42:34,146 - INFO - training batch 301, loss: 1.394, 9632/60000 datapoints
2025-03-06 18:42:34,343 - INFO - training batch 351, loss: 1.442, 11232/60000 datapoints
2025-03-06 18:42:34,539 - INFO - training batch 401, loss: 1.358, 12832/60000 datapoints
2025-03-06 18:42:34,740 - INFO - training batch 451, loss: 1.473, 14432/60000 datapoints
2025-03-06 18:42:34,936 - INFO - training batch 501, loss: 1.296, 16032/60000 datapoints
2025-03-06 18:42:35,132 - INFO - training batch 551, loss: 1.529, 17632/60000 datapoints
2025-03-06 18:42:35,328 - INFO - training batch 601, loss: 1.304, 19232/60000 datapoints
2025-03-06 18:42:35,523 - INFO - training batch 651, loss: 1.438, 20832/60000 datapoints
2025-03-06 18:42:35,718 - INFO - training batch 701, loss: 1.245, 22432/60000 datapoints
2025-03-06 18:42:35,916 - INFO - training batch 751, loss: 1.316, 24032/60000 datapoints
2025-03-06 18:42:36,111 - INFO - training batch 801, loss: 1.435, 25632/60000 datapoints
2025-03-06 18:42:36,306 - INFO - training batch 851, loss: 1.316, 27232/60000 datapoints
2025-03-06 18:42:36,498 - INFO - training batch 901, loss: 1.314, 28832/60000 datapoints
2025-03-06 18:42:36,693 - INFO - training batch 951, loss: 1.451, 30432/60000 datapoints
2025-03-06 18:42:36,887 - INFO - training batch 1001, loss: 1.379, 32032/60000 datapoints
2025-03-06 18:42:37,081 - INFO - training batch 1051, loss: 1.304, 33632/60000 datapoints
2025-03-06 18:42:37,276 - INFO - training batch 1101, loss: 1.290, 35232/60000 datapoints
2025-03-06 18:42:37,467 - INFO - training batch 1151, loss: 1.288, 36832/60000 datapoints
2025-03-06 18:42:37,682 - INFO - training batch 1201, loss: 1.227, 38432/60000 datapoints
2025-03-06 18:42:37,878 - INFO - training batch 1251, loss: 1.262, 40032/60000 datapoints
2025-03-06 18:42:38,075 - INFO - training batch 1301, loss: 1.164, 41632/60000 datapoints
2025-03-06 18:42:38,270 - INFO - training batch 1351, loss: 1.333, 43232/60000 datapoints
2025-03-06 18:42:38,464 - INFO - training batch 1401, loss: 1.285, 44832/60000 datapoints
2025-03-06 18:42:38,659 - INFO - training batch 1451, loss: 1.335, 46432/60000 datapoints
2025-03-06 18:42:38,854 - INFO - training batch 1501, loss: 1.427, 48032/60000 datapoints
2025-03-06 18:42:39,051 - INFO - training batch 1551, loss: 1.280, 49632/60000 datapoints
2025-03-06 18:42:39,246 - INFO - training batch 1601, loss: 1.304, 51232/60000 datapoints
2025-03-06 18:42:39,457 - INFO - training batch 1651, loss: 1.282, 52832/60000 datapoints
2025-03-06 18:42:39,656 - INFO - training batch 1701, loss: 1.414, 54432/60000 datapoints
2025-03-06 18:42:39,849 - INFO - training batch 1751, loss: 1.262, 56032/60000 datapoints
2025-03-06 18:42:40,046 - INFO - training batch 1801, loss: 1.207, 57632/60000 datapoints
2025-03-06 18:42:40,238 - INFO - training batch 1851, loss: 1.307, 59232/60000 datapoints
2025-03-06 18:42:40,339 - INFO - validation batch 1, loss: 1.379, 32/10016 datapoints
2025-03-06 18:42:40,492 - INFO - validation batch 51, loss: 1.304, 1632/10016 datapoints
2025-03-06 18:42:40,648 - INFO - validation batch 101, loss: 1.357, 3232/10016 datapoints
2025-03-06 18:42:40,801 - INFO - validation batch 151, loss: 1.348, 4832/10016 datapoints
2025-03-06 18:42:40,954 - INFO - validation batch 201, loss: 1.377, 6432/10016 datapoints
2025-03-06 18:42:41,109 - INFO - validation batch 251, loss: 1.371, 8032/10016 datapoints
2025-03-06 18:42:41,261 - INFO - validation batch 301, loss: 1.307, 9632/10016 datapoints
2025-03-06 18:42:41,299 - INFO - Epoch 33/800 done.
2025-03-06 18:42:41,300 - INFO - Final validation performance:
Loss: 1.349, top-1 acc: 0.714top-5 acc: 0.714
2025-03-06 18:42:41,300 - INFO - Beginning epoch 34/800
2025-03-06 18:42:41,307 - INFO - training batch 1, loss: 1.294, 32/60000 datapoints
2025-03-06 18:42:41,502 - INFO - training batch 51, loss: 1.287, 1632/60000 datapoints
2025-03-06 18:42:41,696 - INFO - training batch 101, loss: 1.323, 3232/60000 datapoints
2025-03-06 18:42:41,908 - INFO - training batch 151, loss: 1.341, 4832/60000 datapoints
2025-03-06 18:42:42,105 - INFO - training batch 201, loss: 1.322, 6432/60000 datapoints
2025-03-06 18:42:42,297 - INFO - training batch 251, loss: 1.502, 8032/60000 datapoints
2025-03-06 18:42:42,497 - INFO - training batch 301, loss: 1.256, 9632/60000 datapoints
2025-03-06 18:42:42,697 - INFO - training batch 351, loss: 1.260, 11232/60000 datapoints
2025-03-06 18:42:42,895 - INFO - training batch 401, loss: 1.564, 12832/60000 datapoints
2025-03-06 18:42:43,087 - INFO - training batch 451, loss: 1.536, 14432/60000 datapoints
2025-03-06 18:42:43,281 - INFO - training batch 501, loss: 1.457, 16032/60000 datapoints
2025-03-06 18:42:43,475 - INFO - training batch 551, loss: 1.308, 17632/60000 datapoints
2025-03-06 18:42:43,673 - INFO - training batch 601, loss: 1.353, 19232/60000 datapoints
2025-03-06 18:42:43,870 - INFO - training batch 651, loss: 1.363, 20832/60000 datapoints
2025-03-06 18:42:44,064 - INFO - training batch 701, loss: 1.344, 22432/60000 datapoints
2025-03-06 18:42:44,258 - INFO - training batch 751, loss: 1.343, 24032/60000 datapoints
2025-03-06 18:42:44,452 - INFO - training batch 801, loss: 1.383, 25632/60000 datapoints
2025-03-06 18:42:44,649 - INFO - training batch 851, loss: 1.214, 27232/60000 datapoints
2025-03-06 18:42:44,852 - INFO - training batch 901, loss: 1.407, 28832/60000 datapoints
2025-03-06 18:42:45,047 - INFO - training batch 951, loss: 1.220, 30432/60000 datapoints
2025-03-06 18:42:45,242 - INFO - training batch 1001, loss: 1.299, 32032/60000 datapoints
2025-03-06 18:42:45,437 - INFO - training batch 1051, loss: 1.260, 33632/60000 datapoints
2025-03-06 18:42:45,632 - INFO - training batch 1101, loss: 1.274, 35232/60000 datapoints
2025-03-06 18:42:45,828 - INFO - training batch 1151, loss: 1.364, 36832/60000 datapoints
2025-03-06 18:42:46,027 - INFO - training batch 1201, loss: 1.211, 38432/60000 datapoints
2025-03-06 18:42:46,224 - INFO - training batch 1251, loss: 1.472, 40032/60000 datapoints
2025-03-06 18:42:46,419 - INFO - training batch 1301, loss: 1.281, 41632/60000 datapoints
2025-03-06 18:42:46,615 - INFO - training batch 1351, loss: 1.327, 43232/60000 datapoints
2025-03-06 18:42:46,810 - INFO - training batch 1401, loss: 1.416, 44832/60000 datapoints
2025-03-06 18:42:47,003 - INFO - training batch 1451, loss: 1.316, 46432/60000 datapoints
2025-03-06 18:42:47,198 - INFO - training batch 1501, loss: 1.198, 48032/60000 datapoints
2025-03-06 18:42:47,393 - INFO - training batch 1551, loss: 1.184, 49632/60000 datapoints
2025-03-06 18:42:47,586 - INFO - training batch 1601, loss: 1.380, 51232/60000 datapoints
2025-03-06 18:42:47,781 - INFO - training batch 1651, loss: 1.456, 52832/60000 datapoints
2025-03-06 18:42:47,981 - INFO - training batch 1701, loss: 1.399, 54432/60000 datapoints
2025-03-06 18:42:48,174 - INFO - training batch 1751, loss: 1.297, 56032/60000 datapoints
2025-03-06 18:42:48,369 - INFO - training batch 1801, loss: 1.209, 57632/60000 datapoints
2025-03-06 18:42:48,562 - INFO - training batch 1851, loss: 1.550, 59232/60000 datapoints
2025-03-06 18:42:48,666 - INFO - validation batch 1, loss: 1.244, 32/10016 datapoints
2025-03-06 18:42:48,821 - INFO - validation batch 51, loss: 1.404, 1632/10016 datapoints
2025-03-06 18:42:48,976 - INFO - validation batch 101, loss: 1.210, 3232/10016 datapoints
2025-03-06 18:42:49,130 - INFO - validation batch 151, loss: 1.336, 4832/10016 datapoints
2025-03-06 18:42:49,284 - INFO - validation batch 201, loss: 1.349, 6432/10016 datapoints
2025-03-06 18:42:49,437 - INFO - validation batch 251, loss: 1.283, 8032/10016 datapoints
2025-03-06 18:42:49,609 - INFO - validation batch 301, loss: 1.196, 9632/10016 datapoints
2025-03-06 18:42:49,649 - INFO - Epoch 34/800 done.
2025-03-06 18:42:49,649 - INFO - Final validation performance:
Loss: 1.289, top-1 acc: 0.723top-5 acc: 0.723
2025-03-06 18:42:49,650 - INFO - Beginning epoch 35/800
2025-03-06 18:42:49,656 - INFO - training batch 1, loss: 1.405, 32/60000 datapoints
2025-03-06 18:42:49,851 - INFO - training batch 51, loss: 1.325, 1632/60000 datapoints
2025-03-06 18:42:50,050 - INFO - training batch 101, loss: 1.286, 3232/60000 datapoints
2025-03-06 18:42:50,259 - INFO - training batch 151, loss: 1.334, 4832/60000 datapoints
2025-03-06 18:42:50,454 - INFO - training batch 201, loss: 1.298, 6432/60000 datapoints
2025-03-06 18:42:50,657 - INFO - training batch 251, loss: 1.257, 8032/60000 datapoints
2025-03-06 18:42:50,856 - INFO - training batch 301, loss: 1.169, 9632/60000 datapoints
2025-03-06 18:42:51,056 - INFO - training batch 351, loss: 1.211, 11232/60000 datapoints
2025-03-06 18:42:51,250 - INFO - training batch 401, loss: 1.254, 12832/60000 datapoints
2025-03-06 18:42:51,445 - INFO - training batch 451, loss: 1.270, 14432/60000 datapoints
2025-03-06 18:42:51,644 - INFO - training batch 501, loss: 1.396, 16032/60000 datapoints
2025-03-06 18:42:51,839 - INFO - training batch 551, loss: 1.189, 17632/60000 datapoints
2025-03-06 18:42:52,037 - INFO - training batch 601, loss: 1.232, 19232/60000 datapoints
2025-03-06 18:42:52,234 - INFO - training batch 651, loss: 1.373, 20832/60000 datapoints
2025-03-06 18:42:52,427 - INFO - training batch 701, loss: 1.327, 22432/60000 datapoints
2025-03-06 18:42:52,627 - INFO - training batch 751, loss: 1.406, 24032/60000 datapoints
2025-03-06 18:42:52,821 - INFO - training batch 801, loss: 1.226, 25632/60000 datapoints
2025-03-06 18:42:53,031 - INFO - training batch 851, loss: 1.193, 27232/60000 datapoints
2025-03-06 18:42:53,238 - INFO - training batch 901, loss: 1.334, 28832/60000 datapoints
2025-03-06 18:42:53,433 - INFO - training batch 951, loss: 1.286, 30432/60000 datapoints
2025-03-06 18:42:53,629 - INFO - training batch 1001, loss: 1.387, 32032/60000 datapoints
2025-03-06 18:42:53,824 - INFO - training batch 1051, loss: 1.385, 33632/60000 datapoints
2025-03-06 18:42:54,022 - INFO - training batch 1101, loss: 1.362, 35232/60000 datapoints
2025-03-06 18:42:54,216 - INFO - training batch 1151, loss: 1.310, 36832/60000 datapoints
2025-03-06 18:42:54,412 - INFO - training batch 1201, loss: 1.385, 38432/60000 datapoints
2025-03-06 18:42:54,610 - INFO - training batch 1251, loss: 1.273, 40032/60000 datapoints
2025-03-06 18:42:54,805 - INFO - training batch 1301, loss: 1.376, 41632/60000 datapoints
2025-03-06 18:42:55,005 - INFO - training batch 1351, loss: 1.169, 43232/60000 datapoints
2025-03-06 18:42:55,200 - INFO - training batch 1401, loss: 1.340, 44832/60000 datapoints
2025-03-06 18:42:55,394 - INFO - training batch 1451, loss: 1.368, 46432/60000 datapoints
2025-03-06 18:42:55,589 - INFO - training batch 1501, loss: 1.289, 48032/60000 datapoints
2025-03-06 18:42:55,785 - INFO - training batch 1551, loss: 1.228, 49632/60000 datapoints
2025-03-06 18:42:55,984 - INFO - training batch 1601, loss: 1.193, 51232/60000 datapoints
2025-03-06 18:42:56,181 - INFO - training batch 1651, loss: 1.263, 52832/60000 datapoints
2025-03-06 18:42:56,375 - INFO - training batch 1701, loss: 1.365, 54432/60000 datapoints
2025-03-06 18:42:56,570 - INFO - training batch 1751, loss: 1.194, 56032/60000 datapoints
2025-03-06 18:42:56,765 - INFO - training batch 1801, loss: 1.135, 57632/60000 datapoints
2025-03-06 18:42:56,960 - INFO - training batch 1851, loss: 1.330, 59232/60000 datapoints
2025-03-06 18:42:57,061 - INFO - validation batch 1, loss: 1.345, 32/10016 datapoints
2025-03-06 18:42:57,218 - INFO - validation batch 51, loss: 1.492, 1632/10016 datapoints
2025-03-06 18:42:57,369 - INFO - validation batch 101, loss: 1.279, 3232/10016 datapoints
2025-03-06 18:42:57,522 - INFO - validation batch 151, loss: 1.279, 4832/10016 datapoints
2025-03-06 18:42:57,675 - INFO - validation batch 201, loss: 1.236, 6432/10016 datapoints
2025-03-06 18:42:57,827 - INFO - validation batch 251, loss: 1.145, 8032/10016 datapoints
2025-03-06 18:42:57,982 - INFO - validation batch 301, loss: 1.400, 9632/10016 datapoints
2025-03-06 18:42:58,021 - INFO - Epoch 35/800 done.
2025-03-06 18:42:58,021 - INFO - Final validation performance:
Loss: 1.311, top-1 acc: 0.732top-5 acc: 0.732
2025-03-06 18:42:58,022 - INFO - Beginning epoch 36/800
2025-03-06 18:42:58,027 - INFO - training batch 1, loss: 1.248, 32/60000 datapoints
2025-03-06 18:42:58,239 - INFO - training batch 51, loss: 1.219, 1632/60000 datapoints
2025-03-06 18:42:58,435 - INFO - training batch 101, loss: 1.334, 3232/60000 datapoints
2025-03-06 18:42:58,641 - INFO - training batch 151, loss: 1.046, 4832/60000 datapoints
2025-03-06 18:42:58,836 - INFO - training batch 201, loss: 1.272, 6432/60000 datapoints
2025-03-06 18:42:59,031 - INFO - training batch 251, loss: 1.129, 8032/60000 datapoints
2025-03-06 18:42:59,228 - INFO - training batch 301, loss: 1.455, 9632/60000 datapoints
2025-03-06 18:42:59,421 - INFO - training batch 351, loss: 1.286, 11232/60000 datapoints
2025-03-06 18:42:59,635 - INFO - training batch 401, loss: 1.243, 12832/60000 datapoints
2025-03-06 18:42:59,828 - INFO - training batch 451, loss: 1.463, 14432/60000 datapoints
2025-03-06 18:43:00,028 - INFO - training batch 501, loss: 1.315, 16032/60000 datapoints
2025-03-06 18:43:00,221 - INFO - training batch 551, loss: 1.252, 17632/60000 datapoints
2025-03-06 18:43:00,415 - INFO - training batch 601, loss: 1.271, 19232/60000 datapoints
2025-03-06 18:43:00,614 - INFO - training batch 651, loss: 1.314, 20832/60000 datapoints
2025-03-06 18:43:00,807 - INFO - training batch 701, loss: 1.300, 22432/60000 datapoints
2025-03-06 18:43:01,000 - INFO - training batch 751, loss: 1.290, 24032/60000 datapoints
2025-03-06 18:43:01,196 - INFO - training batch 801, loss: 1.312, 25632/60000 datapoints
2025-03-06 18:43:01,388 - INFO - training batch 851, loss: 1.339, 27232/60000 datapoints
2025-03-06 18:43:01,582 - INFO - training batch 901, loss: 1.340, 28832/60000 datapoints
2025-03-06 18:43:01,781 - INFO - training batch 951, loss: 1.305, 30432/60000 datapoints
2025-03-06 18:43:01,978 - INFO - training batch 1001, loss: 1.255, 32032/60000 datapoints
2025-03-06 18:43:02,173 - INFO - training batch 1051, loss: 1.464, 33632/60000 datapoints
2025-03-06 18:43:02,367 - INFO - training batch 1101, loss: 1.314, 35232/60000 datapoints
2025-03-06 18:43:02,560 - INFO - training batch 1151, loss: 1.240, 36832/60000 datapoints
2025-03-06 18:43:02,756 - INFO - training batch 1201, loss: 1.231, 38432/60000 datapoints
2025-03-06 18:43:02,950 - INFO - training batch 1251, loss: 1.149, 40032/60000 datapoints
2025-03-06 18:43:03,146 - INFO - training batch 1301, loss: 1.355, 41632/60000 datapoints
2025-03-06 18:43:03,340 - INFO - training batch 1351, loss: 1.229, 43232/60000 datapoints
2025-03-06 18:43:03,535 - INFO - training batch 1401, loss: 1.314, 44832/60000 datapoints
2025-03-06 18:43:03,734 - INFO - training batch 1451, loss: 1.235, 46432/60000 datapoints
2025-03-06 18:43:03,931 - INFO - training batch 1501, loss: 1.279, 48032/60000 datapoints
2025-03-06 18:43:04,128 - INFO - training batch 1551, loss: 1.138, 49632/60000 datapoints
2025-03-06 18:43:04,322 - INFO - training batch 1601, loss: 1.297, 51232/60000 datapoints
2025-03-06 18:43:04,521 - INFO - training batch 1651, loss: 1.339, 52832/60000 datapoints
2025-03-06 18:43:04,720 - INFO - training batch 1701, loss: 1.229, 54432/60000 datapoints
2025-03-06 18:43:04,936 - INFO - training batch 1751, loss: 1.450, 56032/60000 datapoints
2025-03-06 18:43:05,129 - INFO - training batch 1801, loss: 1.331, 57632/60000 datapoints
2025-03-06 18:43:05,328 - INFO - training batch 1851, loss: 1.347, 59232/60000 datapoints
2025-03-06 18:43:05,427 - INFO - validation batch 1, loss: 1.231, 32/10016 datapoints
2025-03-06 18:43:05,579 - INFO - validation batch 51, loss: 1.432, 1632/10016 datapoints
2025-03-06 18:43:05,843 - INFO - validation batch 101, loss: 1.313, 3232/10016 datapoints
2025-03-06 18:43:05,999 - INFO - validation batch 151, loss: 1.369, 4832/10016 datapoints
2025-03-06 18:43:06,153 - INFO - validation batch 201, loss: 1.339, 6432/10016 datapoints
2025-03-06 18:43:06,320 - INFO - validation batch 251, loss: 1.434, 8032/10016 datapoints
2025-03-06 18:43:06,471 - INFO - validation batch 301, loss: 1.243, 9632/10016 datapoints
2025-03-06 18:43:06,511 - INFO - Epoch 36/800 done.
2025-03-06 18:43:06,511 - INFO - Final validation performance:
Loss: 1.337, top-1 acc: 0.741top-5 acc: 0.741
2025-03-06 18:43:06,512 - INFO - Beginning epoch 37/800
2025-03-06 18:43:06,518 - INFO - training batch 1, loss: 1.213, 32/60000 datapoints
2025-03-06 18:43:06,730 - INFO - training batch 51, loss: 1.417, 1632/60000 datapoints
2025-03-06 18:43:06,923 - INFO - training batch 101, loss: 1.108, 3232/60000 datapoints
2025-03-06 18:43:07,126 - INFO - training batch 151, loss: 1.314, 4832/60000 datapoints
2025-03-06 18:43:07,332 - INFO - training batch 201, loss: 1.267, 6432/60000 datapoints
2025-03-06 18:43:07,530 - INFO - training batch 251, loss: 1.294, 8032/60000 datapoints
2025-03-06 18:43:07,729 - INFO - training batch 301, loss: 1.331, 9632/60000 datapoints
2025-03-06 18:43:07,923 - INFO - training batch 351, loss: 1.322, 11232/60000 datapoints
2025-03-06 18:43:08,121 - INFO - training batch 401, loss: 1.209, 12832/60000 datapoints
2025-03-06 18:43:08,317 - INFO - training batch 451, loss: 1.236, 14432/60000 datapoints
2025-03-06 18:43:08,513 - INFO - training batch 501, loss: 1.323, 16032/60000 datapoints
2025-03-06 18:43:08,711 - INFO - training batch 551, loss: 1.249, 17632/60000 datapoints
2025-03-06 18:43:08,906 - INFO - training batch 601, loss: 1.164, 19232/60000 datapoints
2025-03-06 18:43:09,105 - INFO - training batch 651, loss: 1.362, 20832/60000 datapoints
2025-03-06 18:43:09,303 - INFO - training batch 701, loss: 1.413, 22432/60000 datapoints
2025-03-06 18:43:09,498 - INFO - training batch 751, loss: 1.157, 24032/60000 datapoints
2025-03-06 18:43:09,714 - INFO - training batch 801, loss: 1.264, 25632/60000 datapoints
2025-03-06 18:43:09,908 - INFO - training batch 851, loss: 1.325, 27232/60000 datapoints
2025-03-06 18:43:10,105 - INFO - training batch 901, loss: 1.230, 28832/60000 datapoints
2025-03-06 18:43:10,306 - INFO - training batch 951, loss: 1.230, 30432/60000 datapoints
2025-03-06 18:43:10,499 - INFO - training batch 1001, loss: 1.381, 32032/60000 datapoints
2025-03-06 18:43:10,696 - INFO - training batch 1051, loss: 1.112, 33632/60000 datapoints
2025-03-06 18:43:10,892 - INFO - training batch 1101, loss: 1.181, 35232/60000 datapoints
2025-03-06 18:43:11,086 - INFO - training batch 1151, loss: 1.237, 36832/60000 datapoints
2025-03-06 18:43:11,282 - INFO - training batch 1201, loss: 1.434, 38432/60000 datapoints
2025-03-06 18:43:11,488 - INFO - training batch 1251, loss: 1.196, 40032/60000 datapoints
2025-03-06 18:43:11,691 - INFO - training batch 1301, loss: 1.112, 41632/60000 datapoints
2025-03-06 18:43:11,888 - INFO - training batch 1351, loss: 1.113, 43232/60000 datapoints
2025-03-06 18:43:12,084 - INFO - training batch 1401, loss: 1.244, 44832/60000 datapoints
2025-03-06 18:43:12,279 - INFO - training batch 1451, loss: 1.113, 46432/60000 datapoints
2025-03-06 18:43:12,504 - INFO - training batch 1501, loss: 1.376, 48032/60000 datapoints
2025-03-06 18:43:12,702 - INFO - training batch 1551, loss: 1.428, 49632/60000 datapoints
2025-03-06 18:43:12,895 - INFO - training batch 1601, loss: 1.204, 51232/60000 datapoints
2025-03-06 18:43:13,090 - INFO - training batch 1651, loss: 1.321, 52832/60000 datapoints
2025-03-06 18:43:13,285 - INFO - training batch 1701, loss: 1.077, 54432/60000 datapoints
2025-03-06 18:43:13,488 - INFO - training batch 1751, loss: 1.213, 56032/60000 datapoints
2025-03-06 18:43:13,686 - INFO - training batch 1801, loss: 1.396, 57632/60000 datapoints
2025-03-06 18:43:13,881 - INFO - training batch 1851, loss: 1.201, 59232/60000 datapoints
2025-03-06 18:43:13,981 - INFO - validation batch 1, loss: 1.236, 32/10016 datapoints
2025-03-06 18:43:14,135 - INFO - validation batch 51, loss: 1.244, 1632/10016 datapoints
2025-03-06 18:43:14,287 - INFO - validation batch 101, loss: 1.202, 3232/10016 datapoints
2025-03-06 18:43:14,441 - INFO - validation batch 151, loss: 1.018, 4832/10016 datapoints
2025-03-06 18:43:14,593 - INFO - validation batch 201, loss: 1.105, 6432/10016 datapoints
2025-03-06 18:43:14,748 - INFO - validation batch 251, loss: 1.120, 8032/10016 datapoints
2025-03-06 18:43:14,907 - INFO - validation batch 301, loss: 1.195, 9632/10016 datapoints
2025-03-06 18:43:14,943 - INFO - Epoch 37/800 done.
2025-03-06 18:43:14,943 - INFO - Final validation performance:
Loss: 1.160, top-1 acc: 0.747top-5 acc: 0.747
2025-03-06 18:43:14,944 - INFO - Beginning epoch 38/800
2025-03-06 18:43:14,951 - INFO - training batch 1, loss: 1.316, 32/60000 datapoints
2025-03-06 18:43:15,149 - INFO - training batch 51, loss: 1.259, 1632/60000 datapoints
2025-03-06 18:43:15,348 - INFO - training batch 101, loss: 1.081, 3232/60000 datapoints
2025-03-06 18:43:15,554 - INFO - training batch 151, loss: 1.162, 4832/60000 datapoints
2025-03-06 18:43:15,754 - INFO - training batch 201, loss: 1.250, 6432/60000 datapoints
2025-03-06 18:43:15,955 - INFO - training batch 251, loss: 1.068, 8032/60000 datapoints
2025-03-06 18:43:16,158 - INFO - training batch 301, loss: 1.246, 9632/60000 datapoints
2025-03-06 18:43:16,355 - INFO - training batch 351, loss: 1.084, 11232/60000 datapoints
2025-03-06 18:43:16,553 - INFO - training batch 401, loss: 1.228, 12832/60000 datapoints
2025-03-06 18:43:16,752 - INFO - training batch 451, loss: 1.139, 14432/60000 datapoints
2025-03-06 18:43:16,946 - INFO - training batch 501, loss: 1.218, 16032/60000 datapoints
2025-03-06 18:43:17,141 - INFO - training batch 551, loss: 1.254, 17632/60000 datapoints
2025-03-06 18:43:17,349 - INFO - training batch 601, loss: 1.184, 19232/60000 datapoints
2025-03-06 18:43:17,545 - INFO - training batch 651, loss: 1.345, 20832/60000 datapoints
2025-03-06 18:43:17,740 - INFO - training batch 701, loss: 1.236, 22432/60000 datapoints
2025-03-06 18:43:17,934 - INFO - training batch 751, loss: 1.131, 24032/60000 datapoints
2025-03-06 18:43:18,131 - INFO - training batch 801, loss: 1.151, 25632/60000 datapoints
2025-03-06 18:43:18,326 - INFO - training batch 851, loss: 1.134, 27232/60000 datapoints
2025-03-06 18:43:18,520 - INFO - training batch 901, loss: 1.194, 28832/60000 datapoints
2025-03-06 18:43:18,713 - INFO - training batch 951, loss: 1.184, 30432/60000 datapoints
2025-03-06 18:43:18,908 - INFO - training batch 1001, loss: 1.093, 32032/60000 datapoints
2025-03-06 18:43:19,100 - INFO - training batch 1051, loss: 1.261, 33632/60000 datapoints
2025-03-06 18:43:19,295 - INFO - training batch 1101, loss: 1.122, 35232/60000 datapoints
2025-03-06 18:43:19,488 - INFO - training batch 1151, loss: 1.136, 36832/60000 datapoints
2025-03-06 18:43:19,687 - INFO - training batch 1201, loss: 1.244, 38432/60000 datapoints
2025-03-06 18:43:19,901 - INFO - training batch 1251, loss: 1.140, 40032/60000 datapoints
2025-03-06 18:43:20,098 - INFO - training batch 1301, loss: 1.154, 41632/60000 datapoints
2025-03-06 18:43:20,296 - INFO - training batch 1351, loss: 1.362, 43232/60000 datapoints
2025-03-06 18:43:20,492 - INFO - training batch 1401, loss: 1.136, 44832/60000 datapoints
2025-03-06 18:43:20,691 - INFO - training batch 1451, loss: 1.192, 46432/60000 datapoints
2025-03-06 18:43:20,887 - INFO - training batch 1501, loss: 1.183, 48032/60000 datapoints
2025-03-06 18:43:21,078 - INFO - training batch 1551, loss: 1.261, 49632/60000 datapoints
2025-03-06 18:43:21,273 - INFO - training batch 1601, loss: 1.351, 51232/60000 datapoints
2025-03-06 18:43:21,466 - INFO - training batch 1651, loss: 1.229, 52832/60000 datapoints
2025-03-06 18:43:21,662 - INFO - training batch 1701, loss: 1.185, 54432/60000 datapoints
2025-03-06 18:43:21,859 - INFO - training batch 1751, loss: 1.148, 56032/60000 datapoints
2025-03-06 18:43:22,056 - INFO - training batch 1801, loss: 1.270, 57632/60000 datapoints
2025-03-06 18:43:22,251 - INFO - training batch 1851, loss: 1.347, 59232/60000 datapoints
2025-03-06 18:43:22,354 - INFO - validation batch 1, loss: 1.312, 32/10016 datapoints
2025-03-06 18:43:22,506 - INFO - validation batch 51, loss: 1.093, 1632/10016 datapoints
2025-03-06 18:43:22,666 - INFO - validation batch 101, loss: 1.056, 3232/10016 datapoints
2025-03-06 18:43:22,823 - INFO - validation batch 151, loss: 1.074, 4832/10016 datapoints
2025-03-06 18:43:22,995 - INFO - validation batch 201, loss: 1.163, 6432/10016 datapoints
2025-03-06 18:43:23,149 - INFO - validation batch 251, loss: 1.228, 8032/10016 datapoints
2025-03-06 18:43:23,302 - INFO - validation batch 301, loss: 1.220, 9632/10016 datapoints
2025-03-06 18:43:23,338 - INFO - Epoch 38/800 done.
2025-03-06 18:43:23,338 - INFO - Final validation performance:
Loss: 1.164, top-1 acc: 0.754top-5 acc: 0.754
2025-03-06 18:43:23,339 - INFO - Beginning epoch 39/800
2025-03-06 18:43:23,345 - INFO - training batch 1, loss: 1.318, 32/60000 datapoints
2025-03-06 18:43:23,551 - INFO - training batch 51, loss: 1.203, 1632/60000 datapoints
2025-03-06 18:43:23,753 - INFO - training batch 101, loss: 1.027, 3232/60000 datapoints
2025-03-06 18:43:23,948 - INFO - training batch 151, loss: 1.262, 4832/60000 datapoints
2025-03-06 18:43:24,150 - INFO - training batch 201, loss: 1.100, 6432/60000 datapoints
2025-03-06 18:43:24,350 - INFO - training batch 251, loss: 1.116, 8032/60000 datapoints
2025-03-06 18:43:24,548 - INFO - training batch 301, loss: 1.131, 9632/60000 datapoints
2025-03-06 18:43:24,748 - INFO - training batch 351, loss: 1.174, 11232/60000 datapoints
2025-03-06 18:43:24,955 - INFO - training batch 401, loss: 1.141, 12832/60000 datapoints
2025-03-06 18:43:25,150 - INFO - training batch 451, loss: 1.119, 14432/60000 datapoints
2025-03-06 18:43:25,501 - INFO - training batch 501, loss: 1.308, 16032/60000 datapoints
2025-03-06 18:43:25,708 - INFO - training batch 551, loss: 1.054, 17632/60000 datapoints
2025-03-06 18:43:25,904 - INFO - training batch 601, loss: 1.349, 19232/60000 datapoints
2025-03-06 18:43:26,100 - INFO - training batch 651, loss: 1.285, 20832/60000 datapoints
2025-03-06 18:43:26,297 - INFO - training batch 701, loss: 1.113, 22432/60000 datapoints
2025-03-06 18:43:26,491 - INFO - training batch 751, loss: 1.107, 24032/60000 datapoints
2025-03-06 18:43:26,685 - INFO - training batch 801, loss: 1.223, 25632/60000 datapoints
2025-03-06 18:43:26,881 - INFO - training batch 851, loss: 1.116, 27232/60000 datapoints
2025-03-06 18:43:27,078 - INFO - training batch 901, loss: 1.080, 28832/60000 datapoints
2025-03-06 18:43:27,272 - INFO - training batch 951, loss: 1.255, 30432/60000 datapoints
2025-03-06 18:43:27,473 - INFO - training batch 1001, loss: 1.298, 32032/60000 datapoints
2025-03-06 18:43:27,673 - INFO - training batch 1051, loss: 1.179, 33632/60000 datapoints
2025-03-06 18:43:27,866 - INFO - training batch 1101, loss: 1.161, 35232/60000 datapoints
2025-03-06 18:43:28,081 - INFO - training batch 1151, loss: 1.258, 36832/60000 datapoints
2025-03-06 18:43:28,277 - INFO - training batch 1201, loss: 1.248, 38432/60000 datapoints
2025-03-06 18:43:28,471 - INFO - training batch 1251, loss: 1.170, 40032/60000 datapoints
2025-03-06 18:43:28,668 - INFO - training batch 1301, loss: 1.193, 41632/60000 datapoints
2025-03-06 18:43:28,860 - INFO - training batch 1351, loss: 1.222, 43232/60000 datapoints
2025-03-06 18:43:29,053 - INFO - training batch 1401, loss: 1.235, 44832/60000 datapoints
2025-03-06 18:43:29,247 - INFO - training batch 1451, loss: 1.140, 46432/60000 datapoints
2025-03-06 18:43:29,441 - INFO - training batch 1501, loss: 1.068, 48032/60000 datapoints
2025-03-06 18:43:29,636 - INFO - training batch 1551, loss: 1.098, 49632/60000 datapoints
2025-03-06 18:43:29,838 - INFO - training batch 1601, loss: 1.169, 51232/60000 datapoints
2025-03-06 18:43:30,039 - INFO - training batch 1651, loss: 1.231, 52832/60000 datapoints
2025-03-06 18:43:30,236 - INFO - training batch 1701, loss: 1.232, 54432/60000 datapoints
2025-03-06 18:43:30,432 - INFO - training batch 1751, loss: 1.141, 56032/60000 datapoints
2025-03-06 18:43:30,627 - INFO - training batch 1801, loss: 1.210, 57632/60000 datapoints
2025-03-06 18:43:30,821 - INFO - training batch 1851, loss: 1.181, 59232/60000 datapoints
2025-03-06 18:43:30,925 - INFO - validation batch 1, loss: 1.231, 32/10016 datapoints
2025-03-06 18:43:31,078 - INFO - validation batch 51, loss: 1.107, 1632/10016 datapoints
2025-03-06 18:43:31,229 - INFO - validation batch 101, loss: 1.357, 3232/10016 datapoints
2025-03-06 18:43:31,381 - INFO - validation batch 151, loss: 1.121, 4832/10016 datapoints
2025-03-06 18:43:31,532 - INFO - validation batch 201, loss: 1.193, 6432/10016 datapoints
2025-03-06 18:43:31,687 - INFO - validation batch 251, loss: 1.206, 8032/10016 datapoints
2025-03-06 18:43:31,840 - INFO - validation batch 301, loss: 1.109, 9632/10016 datapoints
2025-03-06 18:43:31,878 - INFO - Epoch 39/800 done.
2025-03-06 18:43:31,878 - INFO - Final validation performance:
Loss: 1.189, top-1 acc: 0.759top-5 acc: 0.759
2025-03-06 18:43:31,879 - INFO - Beginning epoch 40/800
2025-03-06 18:43:31,885 - INFO - training batch 1, loss: 1.168, 32/60000 datapoints
2025-03-06 18:43:32,085 - INFO - training batch 51, loss: 1.143, 1632/60000 datapoints
2025-03-06 18:43:32,291 - INFO - training batch 101, loss: 1.277, 3232/60000 datapoints
2025-03-06 18:43:32,489 - INFO - training batch 151, loss: 1.034, 4832/60000 datapoints
2025-03-06 18:43:32,694 - INFO - training batch 201, loss: 1.197, 6432/60000 datapoints
2025-03-06 18:43:32,891 - INFO - training batch 251, loss: 1.157, 8032/60000 datapoints
2025-03-06 18:43:33,088 - INFO - training batch 301, loss: 1.121, 9632/60000 datapoints
2025-03-06 18:43:33,281 - INFO - training batch 351, loss: 1.277, 11232/60000 datapoints
2025-03-06 18:43:33,475 - INFO - training batch 401, loss: 1.165, 12832/60000 datapoints
2025-03-06 18:43:33,670 - INFO - training batch 451, loss: 1.151, 14432/60000 datapoints
2025-03-06 18:43:33,866 - INFO - training batch 501, loss: 1.038, 16032/60000 datapoints
2025-03-06 18:43:34,061 - INFO - training batch 551, loss: 1.358, 17632/60000 datapoints
2025-03-06 18:43:34,258 - INFO - training batch 601, loss: 1.351, 19232/60000 datapoints
2025-03-06 18:43:34,451 - INFO - training batch 651, loss: 1.217, 20832/60000 datapoints
2025-03-06 18:43:34,651 - INFO - training batch 701, loss: 1.259, 22432/60000 datapoints
2025-03-06 18:43:34,936 - INFO - training batch 751, loss: 1.159, 24032/60000 datapoints
2025-03-06 18:43:35,186 - INFO - training batch 801, loss: 1.181, 25632/60000 datapoints
2025-03-06 18:43:35,387 - INFO - training batch 851, loss: 1.146, 27232/60000 datapoints
2025-03-06 18:43:35,580 - INFO - training batch 901, loss: 1.161, 28832/60000 datapoints
2025-03-06 18:43:35,779 - INFO - training batch 951, loss: 1.088, 30432/60000 datapoints
2025-03-06 18:43:35,975 - INFO - training batch 1001, loss: 1.231, 32032/60000 datapoints
2025-03-06 18:43:36,173 - INFO - training batch 1051, loss: 1.122, 33632/60000 datapoints
2025-03-06 18:43:36,370 - INFO - training batch 1101, loss: 1.212, 35232/60000 datapoints
2025-03-06 18:43:36,564 - INFO - training batch 1151, loss: 1.208, 36832/60000 datapoints
2025-03-06 18:43:36,758 - INFO - training batch 1201, loss: 1.183, 38432/60000 datapoints
2025-03-06 18:43:36,953 - INFO - training batch 1251, loss: 1.053, 40032/60000 datapoints
2025-03-06 18:43:37,146 - INFO - training batch 1301, loss: 1.171, 41632/60000 datapoints
2025-03-06 18:43:37,339 - INFO - training batch 1351, loss: 1.237, 43232/60000 datapoints
2025-03-06 18:43:37,534 - INFO - training batch 1401, loss: 1.330, 44832/60000 datapoints
2025-03-06 18:43:37,741 - INFO - training batch 1451, loss: 1.099, 46432/60000 datapoints
2025-03-06 18:43:37,934 - INFO - training batch 1501, loss: 1.337, 48032/60000 datapoints
2025-03-06 18:43:38,131 - INFO - training batch 1551, loss: 1.191, 49632/60000 datapoints
2025-03-06 18:43:38,325 - INFO - training batch 1601, loss: 1.097, 51232/60000 datapoints
2025-03-06 18:43:38,518 - INFO - training batch 1651, loss: 1.236, 52832/60000 datapoints
2025-03-06 18:43:38,713 - INFO - training batch 1701, loss: 0.969, 54432/60000 datapoints
2025-03-06 18:43:38,908 - INFO - training batch 1751, loss: 1.140, 56032/60000 datapoints
2025-03-06 18:43:39,107 - INFO - training batch 1801, loss: 1.216, 57632/60000 datapoints
2025-03-06 18:43:39,302 - INFO - training batch 1851, loss: 1.262, 59232/60000 datapoints
2025-03-06 18:43:39,401 - INFO - validation batch 1, loss: 1.044, 32/10016 datapoints
2025-03-06 18:43:39,556 - INFO - validation batch 51, loss: 0.908, 1632/10016 datapoints
2025-03-06 18:43:39,710 - INFO - validation batch 101, loss: 1.291, 3232/10016 datapoints
2025-03-06 18:43:39,861 - INFO - validation batch 151, loss: 1.123, 4832/10016 datapoints
2025-03-06 18:43:40,034 - INFO - validation batch 201, loss: 1.061, 6432/10016 datapoints
2025-03-06 18:43:40,189 - INFO - validation batch 251, loss: 1.216, 8032/10016 datapoints
2025-03-06 18:43:40,341 - INFO - validation batch 301, loss: 1.005, 9632/10016 datapoints
2025-03-06 18:43:40,379 - INFO - Epoch 40/800 done.
2025-03-06 18:43:40,379 - INFO - Final validation performance:
Loss: 1.093, top-1 acc: 0.765top-5 acc: 0.765
2025-03-06 18:43:40,379 - INFO - Beginning epoch 41/800
2025-03-06 18:43:40,386 - INFO - training batch 1, loss: 1.169, 32/60000 datapoints
2025-03-06 18:43:40,582 - INFO - training batch 51, loss: 1.089, 1632/60000 datapoints
2025-03-06 18:43:40,777 - INFO - training batch 101, loss: 1.095, 3232/60000 datapoints
2025-03-06 18:43:40,981 - INFO - training batch 151, loss: 1.077, 4832/60000 datapoints
2025-03-06 18:43:41,181 - INFO - training batch 201, loss: 1.034, 6432/60000 datapoints
2025-03-06 18:43:41,374 - INFO - training batch 251, loss: 1.045, 8032/60000 datapoints
2025-03-06 18:43:41,572 - INFO - training batch 301, loss: 0.997, 9632/60000 datapoints
2025-03-06 18:43:41,769 - INFO - training batch 351, loss: 0.997, 11232/60000 datapoints
2025-03-06 18:43:41,968 - INFO - training batch 401, loss: 1.077, 12832/60000 datapoints
2025-03-06 18:43:42,170 - INFO - training batch 451, loss: 1.120, 14432/60000 datapoints
2025-03-06 18:43:42,364 - INFO - training batch 501, loss: 1.238, 16032/60000 datapoints
2025-03-06 18:43:42,557 - INFO - training batch 551, loss: 1.250, 17632/60000 datapoints
2025-03-06 18:43:42,752 - INFO - training batch 601, loss: 1.068, 19232/60000 datapoints
2025-03-06 18:43:42,944 - INFO - training batch 651, loss: 1.083, 20832/60000 datapoints
2025-03-06 18:43:43,139 - INFO - training batch 701, loss: 1.236, 22432/60000 datapoints
2025-03-06 18:43:43,331 - INFO - training batch 751, loss: 1.110, 24032/60000 datapoints
2025-03-06 18:43:43,525 - INFO - training batch 801, loss: 1.099, 25632/60000 datapoints
2025-03-06 18:43:43,722 - INFO - training batch 851, loss: 1.112, 27232/60000 datapoints
2025-03-06 18:43:43,916 - INFO - training batch 901, loss: 1.195, 28832/60000 datapoints
2025-03-06 18:43:44,115 - INFO - training batch 951, loss: 0.983, 30432/60000 datapoints
2025-03-06 18:43:44,309 - INFO - training batch 1001, loss: 1.182, 32032/60000 datapoints
2025-03-06 18:43:44,500 - INFO - training batch 1051, loss: 1.105, 33632/60000 datapoints
2025-03-06 18:43:44,695 - INFO - training batch 1101, loss: 1.052, 35232/60000 datapoints
2025-03-06 18:43:44,897 - INFO - training batch 1151, loss: 1.234, 36832/60000 datapoints
2025-03-06 18:43:45,099 - INFO - training batch 1201, loss: 1.068, 38432/60000 datapoints
2025-03-06 18:43:45,291 - INFO - training batch 1251, loss: 1.224, 40032/60000 datapoints
2025-03-06 18:43:45,487 - INFO - training batch 1301, loss: 1.126, 41632/60000 datapoints
2025-03-06 18:43:45,686 - INFO - training batch 1351, loss: 1.155, 43232/60000 datapoints
2025-03-06 18:43:45,878 - INFO - training batch 1401, loss: 0.950, 44832/60000 datapoints
2025-03-06 18:43:46,071 - INFO - training batch 1451, loss: 1.114, 46432/60000 datapoints
2025-03-06 18:43:46,267 - INFO - training batch 1501, loss: 0.963, 48032/60000 datapoints
2025-03-06 18:43:46,460 - INFO - training batch 1551, loss: 1.256, 49632/60000 datapoints
2025-03-06 18:43:46,659 - INFO - training batch 1601, loss: 1.158, 51232/60000 datapoints
2025-03-06 18:43:46,854 - INFO - training batch 1651, loss: 1.085, 52832/60000 datapoints
2025-03-06 18:43:47,049 - INFO - training batch 1701, loss: 1.176, 54432/60000 datapoints
2025-03-06 18:43:47,246 - INFO - training batch 1751, loss: 1.192, 56032/60000 datapoints
2025-03-06 18:43:47,441 - INFO - training batch 1801, loss: 1.034, 57632/60000 datapoints
2025-03-06 18:43:47,637 - INFO - training batch 1851, loss: 1.178, 59232/60000 datapoints
2025-03-06 18:43:47,738 - INFO - validation batch 1, loss: 1.084, 32/10016 datapoints
2025-03-06 18:43:47,888 - INFO - validation batch 51, loss: 1.166, 1632/10016 datapoints
2025-03-06 18:43:48,040 - INFO - validation batch 101, loss: 1.098, 3232/10016 datapoints
2025-03-06 18:43:48,198 - INFO - validation batch 151, loss: 1.186, 4832/10016 datapoints
2025-03-06 18:43:48,349 - INFO - validation batch 201, loss: 1.190, 6432/10016 datapoints
2025-03-06 18:43:48,499 - INFO - validation batch 251, loss: 1.029, 8032/10016 datapoints
2025-03-06 18:43:48,653 - INFO - validation batch 301, loss: 1.064, 9632/10016 datapoints
2025-03-06 18:43:48,689 - INFO - Epoch 41/800 done.
2025-03-06 18:43:48,690 - INFO - Final validation performance:
Loss: 1.117, top-1 acc: 0.770top-5 acc: 0.770
2025-03-06 18:43:48,690 - INFO - Beginning epoch 42/800
2025-03-06 18:43:48,697 - INFO - training batch 1, loss: 1.101, 32/60000 datapoints
2025-03-06 18:43:48,894 - INFO - training batch 51, loss: 1.193, 1632/60000 datapoints
2025-03-06 18:43:49,089 - INFO - training batch 101, loss: 1.121, 3232/60000 datapoints
2025-03-06 18:43:49,287 - INFO - training batch 151, loss: 1.169, 4832/60000 datapoints
2025-03-06 18:43:49,482 - INFO - training batch 201, loss: 1.112, 6432/60000 datapoints
2025-03-06 18:43:49,681 - INFO - training batch 251, loss: 1.091, 8032/60000 datapoints
2025-03-06 18:43:49,880 - INFO - training batch 301, loss: 1.036, 9632/60000 datapoints
2025-03-06 18:43:50,093 - INFO - training batch 351, loss: 1.206, 11232/60000 datapoints
2025-03-06 18:43:50,292 - INFO - training batch 401, loss: 1.028, 12832/60000 datapoints
2025-03-06 18:43:50,490 - INFO - training batch 451, loss: 1.184, 14432/60000 datapoints
2025-03-06 18:43:50,687 - INFO - training batch 501, loss: 1.109, 16032/60000 datapoints
2025-03-06 18:43:50,882 - INFO - training batch 551, loss: 1.139, 17632/60000 datapoints
2025-03-06 18:43:51,082 - INFO - training batch 601, loss: 1.215, 19232/60000 datapoints
2025-03-06 18:43:51,279 - INFO - training batch 651, loss: 1.105, 20832/60000 datapoints
2025-03-06 18:43:51,475 - INFO - training batch 701, loss: 1.144, 22432/60000 datapoints
2025-03-06 18:43:51,671 - INFO - training batch 751, loss: 0.962, 24032/60000 datapoints
2025-03-06 18:43:51,867 - INFO - training batch 801, loss: 1.100, 25632/60000 datapoints
2025-03-06 18:43:52,060 - INFO - training batch 851, loss: 1.109, 27232/60000 datapoints
2025-03-06 18:43:52,259 - INFO - training batch 901, loss: 1.139, 28832/60000 datapoints
2025-03-06 18:43:52,453 - INFO - training batch 951, loss: 1.042, 30432/60000 datapoints
2025-03-06 18:43:52,652 - INFO - training batch 1001, loss: 1.085, 32032/60000 datapoints
2025-03-06 18:43:52,845 - INFO - training batch 1051, loss: 1.181, 33632/60000 datapoints
2025-03-06 18:43:53,038 - INFO - training batch 1101, loss: 1.079, 35232/60000 datapoints
2025-03-06 18:43:53,234 - INFO - training batch 1151, loss: 1.073, 36832/60000 datapoints
2025-03-06 18:43:53,427 - INFO - training batch 1201, loss: 1.032, 38432/60000 datapoints
2025-03-06 18:43:53,624 - INFO - training batch 1251, loss: 1.127, 40032/60000 datapoints
2025-03-06 18:43:53,818 - INFO - training batch 1301, loss: 1.158, 41632/60000 datapoints
2025-03-06 18:43:54,015 - INFO - training batch 1351, loss: 0.921, 43232/60000 datapoints
2025-03-06 18:43:54,210 - INFO - training batch 1401, loss: 1.003, 44832/60000 datapoints
2025-03-06 18:43:54,403 - INFO - training batch 1451, loss: 1.227, 46432/60000 datapoints
2025-03-06 18:43:54,597 - INFO - training batch 1501, loss: 0.987, 48032/60000 datapoints
2025-03-06 18:43:54,794 - INFO - training batch 1551, loss: 1.082, 49632/60000 datapoints
2025-03-06 18:43:54,992 - INFO - training batch 1601, loss: 1.253, 51232/60000 datapoints
2025-03-06 18:43:55,190 - INFO - training batch 1651, loss: 1.045, 52832/60000 datapoints
2025-03-06 18:43:55,384 - INFO - training batch 1701, loss: 1.104, 54432/60000 datapoints
2025-03-06 18:43:55,579 - INFO - training batch 1751, loss: 1.107, 56032/60000 datapoints
2025-03-06 18:43:55,776 - INFO - training batch 1801, loss: 1.273, 57632/60000 datapoints
2025-03-06 18:43:55,968 - INFO - training batch 1851, loss: 1.151, 59232/60000 datapoints
2025-03-06 18:43:56,069 - INFO - validation batch 1, loss: 1.154, 32/10016 datapoints
2025-03-06 18:43:56,226 - INFO - validation batch 51, loss: 1.199, 1632/10016 datapoints
2025-03-06 18:43:56,379 - INFO - validation batch 101, loss: 1.042, 3232/10016 datapoints
2025-03-06 18:43:56,532 - INFO - validation batch 151, loss: 1.071, 4832/10016 datapoints
2025-03-06 18:43:56,685 - INFO - validation batch 201, loss: 1.053, 6432/10016 datapoints
2025-03-06 18:43:56,840 - INFO - validation batch 251, loss: 1.070, 8032/10016 datapoints
2025-03-06 18:43:56,994 - INFO - validation batch 301, loss: 1.137, 9632/10016 datapoints
2025-03-06 18:43:57,031 - INFO - Epoch 42/800 done.
2025-03-06 18:43:57,031 - INFO - Final validation performance:
Loss: 1.104, top-1 acc: 0.773top-5 acc: 0.773
2025-03-06 18:43:57,031 - INFO - Beginning epoch 43/800
2025-03-06 18:43:57,039 - INFO - training batch 1, loss: 1.041, 32/60000 datapoints
2025-03-06 18:43:57,239 - INFO - training batch 51, loss: 0.978, 1632/60000 datapoints
2025-03-06 18:43:57,439 - INFO - training batch 101, loss: 1.142, 3232/60000 datapoints
2025-03-06 18:43:57,644 - INFO - training batch 151, loss: 1.051, 4832/60000 datapoints
2025-03-06 18:43:57,837 - INFO - training batch 201, loss: 1.206, 6432/60000 datapoints
2025-03-06 18:43:58,040 - INFO - training batch 251, loss: 0.948, 8032/60000 datapoints
2025-03-06 18:43:58,242 - INFO - training batch 301, loss: 1.035, 9632/60000 datapoints
2025-03-06 18:43:58,438 - INFO - training batch 351, loss: 0.999, 11232/60000 datapoints
2025-03-06 18:43:58,638 - INFO - training batch 401, loss: 1.100, 12832/60000 datapoints
2025-03-06 18:43:58,834 - INFO - training batch 451, loss: 1.175, 14432/60000 datapoints
2025-03-06 18:43:59,029 - INFO - training batch 501, loss: 1.036, 16032/60000 datapoints
2025-03-06 18:43:59,227 - INFO - training batch 551, loss: 1.186, 17632/60000 datapoints
2025-03-06 18:43:59,421 - INFO - training batch 601, loss: 1.020, 19232/60000 datapoints
2025-03-06 18:43:59,616 - INFO - training batch 651, loss: 0.979, 20832/60000 datapoints
2025-03-06 18:43:59,812 - INFO - training batch 701, loss: 1.063, 22432/60000 datapoints
2025-03-06 18:44:00,007 - INFO - training batch 751, loss: 1.193, 24032/60000 datapoints
2025-03-06 18:44:00,223 - INFO - training batch 801, loss: 1.018, 25632/60000 datapoints
2025-03-06 18:44:00,416 - INFO - training batch 851, loss: 1.125, 27232/60000 datapoints
2025-03-06 18:44:00,611 - INFO - training batch 901, loss: 0.995, 28832/60000 datapoints
2025-03-06 18:44:00,805 - INFO - training batch 951, loss: 1.308, 30432/60000 datapoints
2025-03-06 18:44:00,998 - INFO - training batch 1001, loss: 1.139, 32032/60000 datapoints
2025-03-06 18:44:01,193 - INFO - training batch 1051, loss: 1.168, 33632/60000 datapoints
2025-03-06 18:44:01,389 - INFO - training batch 1101, loss: 1.197, 35232/60000 datapoints
2025-03-06 18:44:01,586 - INFO - training batch 1151, loss: 1.115, 36832/60000 datapoints
2025-03-06 18:44:01,784 - INFO - training batch 1201, loss: 1.142, 38432/60000 datapoints
2025-03-06 18:44:01,976 - INFO - training batch 1251, loss: 1.019, 40032/60000 datapoints
2025-03-06 18:44:02,169 - INFO - training batch 1301, loss: 1.023, 41632/60000 datapoints
2025-03-06 18:44:02,368 - INFO - training batch 1351, loss: 0.984, 43232/60000 datapoints
2025-03-06 18:44:02,562 - INFO - training batch 1401, loss: 1.126, 44832/60000 datapoints
2025-03-06 18:44:02,757 - INFO - training batch 1451, loss: 1.076, 46432/60000 datapoints
2025-03-06 18:44:02,953 - INFO - training batch 1501, loss: 1.143, 48032/60000 datapoints
2025-03-06 18:44:03,148 - INFO - training batch 1551, loss: 1.141, 49632/60000 datapoints
2025-03-06 18:44:03,342 - INFO - training batch 1601, loss: 1.049, 51232/60000 datapoints
2025-03-06 18:44:03,536 - INFO - training batch 1651, loss: 1.034, 52832/60000 datapoints
2025-03-06 18:44:03,739 - INFO - training batch 1701, loss: 0.933, 54432/60000 datapoints
2025-03-06 18:44:03,934 - INFO - training batch 1751, loss: 1.037, 56032/60000 datapoints
2025-03-06 18:44:04,127 - INFO - training batch 1801, loss: 0.987, 57632/60000 datapoints
2025-03-06 18:44:04,326 - INFO - training batch 1851, loss: 1.078, 59232/60000 datapoints
2025-03-06 18:44:04,426 - INFO - validation batch 1, loss: 0.978, 32/10016 datapoints
2025-03-06 18:44:04,578 - INFO - validation batch 51, loss: 0.984, 1632/10016 datapoints
2025-03-06 18:44:04,735 - INFO - validation batch 101, loss: 1.064, 3232/10016 datapoints
2025-03-06 18:44:04,892 - INFO - validation batch 151, loss: 0.918, 4832/10016 datapoints
2025-03-06 18:44:05,049 - INFO - validation batch 201, loss: 1.012, 6432/10016 datapoints
2025-03-06 18:44:05,201 - INFO - validation batch 251, loss: 0.994, 8032/10016 datapoints
2025-03-06 18:44:05,356 - INFO - validation batch 301, loss: 1.247, 9632/10016 datapoints
2025-03-06 18:44:05,394 - INFO - Epoch 43/800 done.
2025-03-06 18:44:05,394 - INFO - Final validation performance:
Loss: 1.028, top-1 acc: 0.777top-5 acc: 0.777
2025-03-06 18:44:05,395 - INFO - Beginning epoch 44/800
2025-03-06 18:44:05,401 - INFO - training batch 1, loss: 1.184, 32/60000 datapoints
2025-03-06 18:44:05,602 - INFO - training batch 51, loss: 0.976, 1632/60000 datapoints
2025-03-06 18:44:05,802 - INFO - training batch 101, loss: 1.235, 3232/60000 datapoints
2025-03-06 18:44:05,993 - INFO - training batch 151, loss: 1.095, 4832/60000 datapoints
2025-03-06 18:44:06,188 - INFO - training batch 201, loss: 1.088, 6432/60000 datapoints
2025-03-06 18:44:06,396 - INFO - training batch 251, loss: 0.945, 8032/60000 datapoints
2025-03-06 18:44:06,594 - INFO - training batch 301, loss: 1.094, 9632/60000 datapoints
2025-03-06 18:44:06,792 - INFO - training batch 351, loss: 1.271, 11232/60000 datapoints
2025-03-06 18:44:06,984 - INFO - training batch 401, loss: 0.949, 12832/60000 datapoints
2025-03-06 18:44:07,176 - INFO - training batch 451, loss: 1.091, 14432/60000 datapoints
2025-03-06 18:44:07,368 - INFO - training batch 501, loss: 1.009, 16032/60000 datapoints
2025-03-06 18:44:07,567 - INFO - training batch 551, loss: 1.028, 17632/60000 datapoints
2025-03-06 18:44:07,773 - INFO - training batch 601, loss: 0.866, 19232/60000 datapoints
2025-03-06 18:44:07,965 - INFO - training batch 651, loss: 1.110, 20832/60000 datapoints
2025-03-06 18:44:08,157 - INFO - training batch 701, loss: 1.096, 22432/60000 datapoints
2025-03-06 18:44:08,351 - INFO - training batch 751, loss: 1.025, 24032/60000 datapoints
2025-03-06 18:44:08,544 - INFO - training batch 801, loss: 1.207, 25632/60000 datapoints
2025-03-06 18:44:08,738 - INFO - training batch 851, loss: 1.010, 27232/60000 datapoints
2025-03-06 18:44:08,931 - INFO - training batch 901, loss: 1.054, 28832/60000 datapoints
2025-03-06 18:44:09,123 - INFO - training batch 951, loss: 1.307, 30432/60000 datapoints
2025-03-06 18:44:09,316 - INFO - training batch 1001, loss: 1.019, 32032/60000 datapoints
2025-03-06 18:44:09,507 - INFO - training batch 1051, loss: 0.970, 33632/60000 datapoints
2025-03-06 18:44:09,703 - INFO - training batch 1101, loss: 0.941, 35232/60000 datapoints
2025-03-06 18:44:09,894 - INFO - training batch 1151, loss: 1.026, 36832/60000 datapoints
2025-03-06 18:44:10,084 - INFO - training batch 1201, loss: 0.983, 38432/60000 datapoints
2025-03-06 18:44:10,295 - INFO - training batch 1251, loss: 1.051, 40032/60000 datapoints
2025-03-06 18:44:10,487 - INFO - training batch 1301, loss: 1.159, 41632/60000 datapoints
2025-03-06 18:44:10,682 - INFO - training batch 1351, loss: 1.235, 43232/60000 datapoints
2025-03-06 18:44:10,873 - INFO - training batch 1401, loss: 1.145, 44832/60000 datapoints
2025-03-06 18:44:11,063 - INFO - training batch 1451, loss: 1.110, 46432/60000 datapoints
2025-03-06 18:44:11,255 - INFO - training batch 1501, loss: 1.062, 48032/60000 datapoints
2025-03-06 18:44:11,447 - INFO - training batch 1551, loss: 1.082, 49632/60000 datapoints
2025-03-06 18:44:11,641 - INFO - training batch 1601, loss: 0.957, 51232/60000 datapoints
2025-03-06 18:44:11,841 - INFO - training batch 1651, loss: 1.120, 52832/60000 datapoints
2025-03-06 18:44:12,033 - INFO - training batch 1701, loss: 1.064, 54432/60000 datapoints
2025-03-06 18:44:12,226 - INFO - training batch 1751, loss: 1.113, 56032/60000 datapoints
2025-03-06 18:44:12,419 - INFO - training batch 1801, loss: 1.173, 57632/60000 datapoints
2025-03-06 18:44:12,612 - INFO - training batch 1851, loss: 1.065, 59232/60000 datapoints
2025-03-06 18:44:12,711 - INFO - validation batch 1, loss: 1.097, 32/10016 datapoints
2025-03-06 18:44:12,889 - INFO - validation batch 51, loss: 1.054, 1632/10016 datapoints
2025-03-06 18:44:13,038 - INFO - validation batch 101, loss: 1.103, 3232/10016 datapoints
2025-03-06 18:44:13,188 - INFO - validation batch 151, loss: 1.108, 4832/10016 datapoints
2025-03-06 18:44:13,337 - INFO - validation batch 201, loss: 1.065, 6432/10016 datapoints
2025-03-06 18:44:13,488 - INFO - validation batch 251, loss: 1.043, 8032/10016 datapoints
2025-03-06 18:44:13,641 - INFO - validation batch 301, loss: 0.910, 9632/10016 datapoints
2025-03-06 18:44:13,676 - INFO - Epoch 44/800 done.
2025-03-06 18:44:13,676 - INFO - Final validation performance:
Loss: 1.054, top-1 acc: 0.779top-5 acc: 0.779
2025-03-06 18:44:13,677 - INFO - Beginning epoch 45/800
2025-03-06 18:44:13,683 - INFO - training batch 1, loss: 1.035, 32/60000 datapoints
2025-03-06 18:44:13,893 - INFO - training batch 51, loss: 1.049, 1632/60000 datapoints
2025-03-06 18:44:14,085 - INFO - training batch 101, loss: 1.442, 3232/60000 datapoints
2025-03-06 18:44:14,279 - INFO - training batch 151, loss: 0.956, 4832/60000 datapoints
2025-03-06 18:44:14,477 - INFO - training batch 201, loss: 1.014, 6432/60000 datapoints
2025-03-06 18:44:14,679 - INFO - training batch 251, loss: 0.983, 8032/60000 datapoints
2025-03-06 18:44:14,881 - INFO - training batch 301, loss: 0.928, 9632/60000 datapoints
2025-03-06 18:44:15,076 - INFO - training batch 351, loss: 1.052, 11232/60000 datapoints
2025-03-06 18:44:15,266 - INFO - training batch 401, loss: 0.902, 12832/60000 datapoints
2025-03-06 18:44:15,458 - INFO - training batch 451, loss: 0.968, 14432/60000 datapoints
2025-03-06 18:44:15,653 - INFO - training batch 501, loss: 1.120, 16032/60000 datapoints
2025-03-06 18:44:15,843 - INFO - training batch 551, loss: 0.931, 17632/60000 datapoints
2025-03-06 18:44:16,035 - INFO - training batch 601, loss: 1.136, 19232/60000 datapoints
2025-03-06 18:44:16,231 - INFO - training batch 651, loss: 1.155, 20832/60000 datapoints
2025-03-06 18:44:16,423 - INFO - training batch 701, loss: 0.976, 22432/60000 datapoints
2025-03-06 18:44:16,619 - INFO - training batch 751, loss: 1.125, 24032/60000 datapoints
2025-03-06 18:44:16,811 - INFO - training batch 801, loss: 1.136, 25632/60000 datapoints
2025-03-06 18:44:17,002 - INFO - training batch 851, loss: 1.110, 27232/60000 datapoints
2025-03-06 18:44:17,191 - INFO - training batch 901, loss: 0.932, 28832/60000 datapoints
2025-03-06 18:44:17,380 - INFO - training batch 951, loss: 0.882, 30432/60000 datapoints
2025-03-06 18:44:17,573 - INFO - training batch 1001, loss: 0.827, 32032/60000 datapoints
2025-03-06 18:44:17,768 - INFO - training batch 1051, loss: 1.071, 33632/60000 datapoints
2025-03-06 18:44:17,960 - INFO - training batch 1101, loss: 0.964, 35232/60000 datapoints
2025-03-06 18:44:18,151 - INFO - training batch 1151, loss: 1.016, 36832/60000 datapoints
2025-03-06 18:44:18,346 - INFO - training batch 1201, loss: 0.997, 38432/60000 datapoints
2025-03-06 18:44:18,537 - INFO - training batch 1251, loss: 1.314, 40032/60000 datapoints
2025-03-06 18:44:18,730 - INFO - training batch 1301, loss: 1.014, 41632/60000 datapoints
2025-03-06 18:44:18,922 - INFO - training batch 1351, loss: 1.136, 43232/60000 datapoints
2025-03-06 18:44:19,112 - INFO - training batch 1401, loss: 1.086, 44832/60000 datapoints
2025-03-06 18:44:19,310 - INFO - training batch 1451, loss: 0.978, 46432/60000 datapoints
2025-03-06 18:44:19,502 - INFO - training batch 1501, loss: 1.194, 48032/60000 datapoints
2025-03-06 18:44:19,697 - INFO - training batch 1551, loss: 1.247, 49632/60000 datapoints
2025-03-06 18:44:19,889 - INFO - training batch 1601, loss: 0.812, 51232/60000 datapoints
2025-03-06 18:44:20,082 - INFO - training batch 1651, loss: 0.995, 52832/60000 datapoints
2025-03-06 18:44:20,292 - INFO - training batch 1701, loss: 1.062, 54432/60000 datapoints
2025-03-06 18:44:20,484 - INFO - training batch 1751, loss: 0.945, 56032/60000 datapoints
2025-03-06 18:44:20,679 - INFO - training batch 1801, loss: 1.093, 57632/60000 datapoints
2025-03-06 18:44:20,867 - INFO - training batch 1851, loss: 1.228, 59232/60000 datapoints
2025-03-06 18:44:20,966 - INFO - validation batch 1, loss: 0.956, 32/10016 datapoints
2025-03-06 18:44:21,115 - INFO - validation batch 51, loss: 0.805, 1632/10016 datapoints
2025-03-06 18:44:21,264 - INFO - validation batch 101, loss: 1.062, 3232/10016 datapoints
2025-03-06 18:44:21,419 - INFO - validation batch 151, loss: 0.998, 4832/10016 datapoints
2025-03-06 18:44:21,578 - INFO - validation batch 201, loss: 1.065, 6432/10016 datapoints
2025-03-06 18:44:21,734 - INFO - validation batch 251, loss: 0.945, 8032/10016 datapoints
2025-03-06 18:44:21,887 - INFO - validation batch 301, loss: 0.873, 9632/10016 datapoints
2025-03-06 18:44:21,923 - INFO - Epoch 45/800 done.
2025-03-06 18:44:21,923 - INFO - Final validation performance:
Loss: 0.958, top-1 acc: 0.782top-5 acc: 0.782
2025-03-06 18:44:21,923 - INFO - Beginning epoch 46/800
2025-03-06 18:44:21,931 - INFO - training batch 1, loss: 1.204, 32/60000 datapoints
2025-03-06 18:44:22,128 - INFO - training batch 51, loss: 1.044, 1632/60000 datapoints
2025-03-06 18:44:22,324 - INFO - training batch 101, loss: 1.110, 3232/60000 datapoints
2025-03-06 18:44:22,525 - INFO - training batch 151, loss: 0.979, 4832/60000 datapoints
2025-03-06 18:44:22,727 - INFO - training batch 201, loss: 1.049, 6432/60000 datapoints
2025-03-06 18:44:22,920 - INFO - training batch 251, loss: 0.900, 8032/60000 datapoints
2025-03-06 18:44:23,117 - INFO - training batch 301, loss: 0.966, 9632/60000 datapoints
2025-03-06 18:44:23,319 - INFO - training batch 351, loss: 0.859, 11232/60000 datapoints
2025-03-06 18:44:23,517 - INFO - training batch 401, loss: 1.095, 12832/60000 datapoints
2025-03-06 18:44:23,717 - INFO - training batch 451, loss: 1.052, 14432/60000 datapoints
2025-03-06 18:44:23,913 - INFO - training batch 501, loss: 0.766, 16032/60000 datapoints
2025-03-06 18:44:24,108 - INFO - training batch 551, loss: 0.938, 17632/60000 datapoints
2025-03-06 18:44:24,304 - INFO - training batch 601, loss: 1.140, 19232/60000 datapoints
2025-03-06 18:44:24,498 - INFO - training batch 651, loss: 1.230, 20832/60000 datapoints
2025-03-06 18:44:24,696 - INFO - training batch 701, loss: 1.095, 22432/60000 datapoints
2025-03-06 18:44:24,897 - INFO - training batch 751, loss: 1.075, 24032/60000 datapoints
2025-03-06 18:44:25,095 - INFO - training batch 801, loss: 1.031, 25632/60000 datapoints
2025-03-06 18:44:25,288 - INFO - training batch 851, loss: 1.080, 27232/60000 datapoints
2025-03-06 18:44:25,483 - INFO - training batch 901, loss: 1.086, 28832/60000 datapoints
2025-03-06 18:44:25,680 - INFO - training batch 951, loss: 0.927, 30432/60000 datapoints
2025-03-06 18:44:25,872 - INFO - training batch 1001, loss: 1.035, 32032/60000 datapoints
2025-03-06 18:44:26,065 - INFO - training batch 1051, loss: 1.183, 33632/60000 datapoints
2025-03-06 18:44:26,266 - INFO - training batch 1101, loss: 1.113, 35232/60000 datapoints
2025-03-06 18:44:26,460 - INFO - training batch 1151, loss: 0.961, 36832/60000 datapoints
2025-03-06 18:44:26,657 - INFO - training batch 1201, loss: 1.135, 38432/60000 datapoints
2025-03-06 18:44:26,854 - INFO - training batch 1251, loss: 0.875, 40032/60000 datapoints
2025-03-06 18:44:27,047 - INFO - training batch 1301, loss: 1.300, 41632/60000 datapoints
2025-03-06 18:44:27,241 - INFO - training batch 1351, loss: 1.148, 43232/60000 datapoints
2025-03-06 18:44:27,435 - INFO - training batch 1401, loss: 1.213, 44832/60000 datapoints
2025-03-06 18:44:27,633 - INFO - training batch 1451, loss: 1.084, 46432/60000 datapoints
2025-03-06 18:44:27,830 - INFO - training batch 1501, loss: 0.922, 48032/60000 datapoints
2025-03-06 18:44:28,029 - INFO - training batch 1551, loss: 1.003, 49632/60000 datapoints
2025-03-06 18:44:28,247 - INFO - training batch 1601, loss: 0.992, 51232/60000 datapoints
2025-03-06 18:44:28,446 - INFO - training batch 1651, loss: 1.044, 52832/60000 datapoints
2025-03-06 18:44:28,644 - INFO - training batch 1701, loss: 1.077, 54432/60000 datapoints
2025-03-06 18:44:28,838 - INFO - training batch 1751, loss: 0.805, 56032/60000 datapoints
2025-03-06 18:44:29,039 - INFO - training batch 1801, loss: 1.212, 57632/60000 datapoints
2025-03-06 18:44:29,231 - INFO - training batch 1851, loss: 1.085, 59232/60000 datapoints
2025-03-06 18:44:29,334 - INFO - validation batch 1, loss: 0.912, 32/10016 datapoints
2025-03-06 18:44:29,486 - INFO - validation batch 51, loss: 0.976, 1632/10016 datapoints
2025-03-06 18:44:29,640 - INFO - validation batch 101, loss: 1.039, 3232/10016 datapoints
2025-03-06 18:44:29,794 - INFO - validation batch 151, loss: 0.960, 4832/10016 datapoints
2025-03-06 18:44:29,946 - INFO - validation batch 201, loss: 0.731, 6432/10016 datapoints
2025-03-06 18:44:30,099 - INFO - validation batch 251, loss: 0.853, 8032/10016 datapoints
2025-03-06 18:44:30,252 - INFO - validation batch 301, loss: 1.132, 9632/10016 datapoints
2025-03-06 18:44:30,293 - INFO - Epoch 46/800 done.
2025-03-06 18:44:30,293 - INFO - Final validation performance:
Loss: 0.943, top-1 acc: 0.785top-5 acc: 0.785
2025-03-06 18:44:30,294 - INFO - Beginning epoch 47/800
2025-03-06 18:44:30,301 - INFO - training batch 1, loss: 0.772, 32/60000 datapoints
2025-03-06 18:44:30,508 - INFO - training batch 51, loss: 1.037, 1632/60000 datapoints
2025-03-06 18:44:30,714 - INFO - training batch 101, loss: 1.108, 3232/60000 datapoints
2025-03-06 18:44:30,906 - INFO - training batch 151, loss: 0.885, 4832/60000 datapoints
2025-03-06 18:44:31,097 - INFO - training batch 201, loss: 1.002, 6432/60000 datapoints
2025-03-06 18:44:31,295 - INFO - training batch 251, loss: 0.831, 8032/60000 datapoints
2025-03-06 18:44:31,489 - INFO - training batch 301, loss: 1.002, 9632/60000 datapoints
2025-03-06 18:44:31,688 - INFO - training batch 351, loss: 0.885, 11232/60000 datapoints
2025-03-06 18:44:31,883 - INFO - training batch 401, loss: 1.011, 12832/60000 datapoints
2025-03-06 18:44:32,073 - INFO - training batch 451, loss: 1.179, 14432/60000 datapoints
2025-03-06 18:44:32,265 - INFO - training batch 501, loss: 0.987, 16032/60000 datapoints
2025-03-06 18:44:32,459 - INFO - training batch 551, loss: 1.051, 17632/60000 datapoints
2025-03-06 18:44:32,652 - INFO - training batch 601, loss: 0.883, 19232/60000 datapoints
2025-03-06 18:44:32,849 - INFO - training batch 651, loss: 1.135, 20832/60000 datapoints
2025-03-06 18:44:33,042 - INFO - training batch 701, loss: 0.884, 22432/60000 datapoints
2025-03-06 18:44:33,234 - INFO - training batch 751, loss: 0.990, 24032/60000 datapoints
2025-03-06 18:44:33,426 - INFO - training batch 801, loss: 1.127, 25632/60000 datapoints
2025-03-06 18:44:33,620 - INFO - training batch 851, loss: 0.808, 27232/60000 datapoints
2025-03-06 18:44:33,812 - INFO - training batch 901, loss: 0.965, 28832/60000 datapoints
2025-03-06 18:44:34,108 - INFO - training batch 951, loss: 0.914, 30432/60000 datapoints
2025-03-06 18:44:34,303 - INFO - training batch 1001, loss: 0.955, 32032/60000 datapoints
2025-03-06 18:44:34,495 - INFO - training batch 1051, loss: 1.214, 33632/60000 datapoints
2025-03-06 18:44:34,691 - INFO - training batch 1101, loss: 0.954, 35232/60000 datapoints
2025-03-06 18:44:34,889 - INFO - training batch 1151, loss: 1.283, 36832/60000 datapoints
2025-03-06 18:44:35,082 - INFO - training batch 1201, loss: 1.171, 38432/60000 datapoints
2025-03-06 18:44:35,278 - INFO - training batch 1251, loss: 1.003, 40032/60000 datapoints
2025-03-06 18:44:35,468 - INFO - training batch 1301, loss: 1.163, 41632/60000 datapoints
2025-03-06 18:44:35,663 - INFO - training batch 1351, loss: 1.031, 43232/60000 datapoints
2025-03-06 18:44:35,853 - INFO - training batch 1401, loss: 0.790, 44832/60000 datapoints
2025-03-06 18:44:36,044 - INFO - training batch 1451, loss: 0.978, 46432/60000 datapoints
2025-03-06 18:44:36,240 - INFO - training batch 1501, loss: 0.999, 48032/60000 datapoints
2025-03-06 18:44:36,436 - INFO - training batch 1551, loss: 1.031, 49632/60000 datapoints
2025-03-06 18:44:36,629 - INFO - training batch 1601, loss: 1.032, 51232/60000 datapoints
2025-03-06 18:44:36,825 - INFO - training batch 1651, loss: 0.977, 52832/60000 datapoints
2025-03-06 18:44:37,017 - INFO - training batch 1701, loss: 0.980, 54432/60000 datapoints
2025-03-06 18:44:37,215 - INFO - training batch 1751, loss: 0.964, 56032/60000 datapoints
2025-03-06 18:44:37,408 - INFO - training batch 1801, loss: 0.839, 57632/60000 datapoints
2025-03-06 18:44:37,598 - INFO - training batch 1851, loss: 0.914, 59232/60000 datapoints
2025-03-06 18:44:37,721 - INFO - validation batch 1, loss: 0.860, 32/10016 datapoints
2025-03-06 18:44:37,871 - INFO - validation batch 51, loss: 0.948, 1632/10016 datapoints
2025-03-06 18:44:38,020 - INFO - validation batch 101, loss: 0.856, 3232/10016 datapoints
2025-03-06 18:44:38,172 - INFO - validation batch 151, loss: 1.048, 4832/10016 datapoints
2025-03-06 18:44:38,326 - INFO - validation batch 201, loss: 0.913, 6432/10016 datapoints
2025-03-06 18:44:38,479 - INFO - validation batch 251, loss: 1.041, 8032/10016 datapoints
2025-03-06 18:44:38,633 - INFO - validation batch 301, loss: 1.041, 9632/10016 datapoints
2025-03-06 18:44:38,668 - INFO - Epoch 47/800 done.
2025-03-06 18:44:38,668 - INFO - Final validation performance:
Loss: 0.958, top-1 acc: 0.789top-5 acc: 0.789
2025-03-06 18:44:38,669 - INFO - Beginning epoch 48/800
2025-03-06 18:44:38,675 - INFO - training batch 1, loss: 1.035, 32/60000 datapoints
2025-03-06 18:44:38,868 - INFO - training batch 51, loss: 1.055, 1632/60000 datapoints
2025-03-06 18:44:39,071 - INFO - training batch 101, loss: 1.057, 3232/60000 datapoints
2025-03-06 18:44:39,272 - INFO - training batch 151, loss: 0.971, 4832/60000 datapoints
2025-03-06 18:44:39,467 - INFO - training batch 201, loss: 1.052, 6432/60000 datapoints
2025-03-06 18:44:39,661 - INFO - training batch 251, loss: 1.037, 8032/60000 datapoints
2025-03-06 18:44:39,855 - INFO - training batch 301, loss: 0.965, 9632/60000 datapoints
2025-03-06 18:44:40,054 - INFO - training batch 351, loss: 0.833, 11232/60000 datapoints
2025-03-06 18:44:40,252 - INFO - training batch 401, loss: 0.948, 12832/60000 datapoints
2025-03-06 18:44:40,466 - INFO - training batch 451, loss: 1.033, 14432/60000 datapoints
2025-03-06 18:44:40,665 - INFO - training batch 501, loss: 0.979, 16032/60000 datapoints
2025-03-06 18:44:40,857 - INFO - training batch 551, loss: 0.888, 17632/60000 datapoints
2025-03-06 18:44:41,050 - INFO - training batch 601, loss: 1.015, 19232/60000 datapoints
2025-03-06 18:44:41,242 - INFO - training batch 651, loss: 1.026, 20832/60000 datapoints
2025-03-06 18:44:41,432 - INFO - training batch 701, loss: 0.996, 22432/60000 datapoints
2025-03-06 18:44:41,642 - INFO - training batch 751, loss: 1.017, 24032/60000 datapoints
2025-03-06 18:44:41,838 - INFO - training batch 801, loss: 0.881, 25632/60000 datapoints
2025-03-06 18:44:42,030 - INFO - training batch 851, loss: 0.954, 27232/60000 datapoints
2025-03-06 18:44:42,225 - INFO - training batch 901, loss: 0.949, 28832/60000 datapoints
2025-03-06 18:44:42,422 - INFO - training batch 951, loss: 0.943, 30432/60000 datapoints
2025-03-06 18:44:42,619 - INFO - training batch 1001, loss: 0.891, 32032/60000 datapoints
2025-03-06 18:44:42,813 - INFO - training batch 1051, loss: 1.135, 33632/60000 datapoints
2025-03-06 18:44:43,010 - INFO - training batch 1101, loss: 0.828, 35232/60000 datapoints
2025-03-06 18:44:43,205 - INFO - training batch 1151, loss: 0.900, 36832/60000 datapoints
2025-03-06 18:44:43,400 - INFO - training batch 1201, loss: 0.918, 38432/60000 datapoints
2025-03-06 18:44:43,594 - INFO - training batch 1251, loss: 1.135, 40032/60000 datapoints
2025-03-06 18:44:43,791 - INFO - training batch 1301, loss: 0.971, 41632/60000 datapoints
2025-03-06 18:44:43,985 - INFO - training batch 1351, loss: 0.830, 43232/60000 datapoints
2025-03-06 18:44:44,180 - INFO - training batch 1401, loss: 1.109, 44832/60000 datapoints
2025-03-06 18:44:44,374 - INFO - training batch 1451, loss: 0.795, 46432/60000 datapoints
2025-03-06 18:44:44,568 - INFO - training batch 1501, loss: 0.943, 48032/60000 datapoints
2025-03-06 18:44:44,766 - INFO - training batch 1551, loss: 1.039, 49632/60000 datapoints
2025-03-06 18:44:44,966 - INFO - training batch 1601, loss: 1.049, 51232/60000 datapoints
2025-03-06 18:44:45,162 - INFO - training batch 1651, loss: 0.905, 52832/60000 datapoints
2025-03-06 18:44:45,355 - INFO - training batch 1701, loss: 1.193, 54432/60000 datapoints
2025-03-06 18:44:45,548 - INFO - training batch 1751, loss: 1.078, 56032/60000 datapoints
2025-03-06 18:44:45,744 - INFO - training batch 1801, loss: 0.884, 57632/60000 datapoints
2025-03-06 18:44:45,939 - INFO - training batch 1851, loss: 1.023, 59232/60000 datapoints
2025-03-06 18:44:46,039 - INFO - validation batch 1, loss: 1.102, 32/10016 datapoints
2025-03-06 18:44:46,196 - INFO - validation batch 51, loss: 1.112, 1632/10016 datapoints
2025-03-06 18:44:46,351 - INFO - validation batch 101, loss: 0.982, 3232/10016 datapoints
2025-03-06 18:44:46,504 - INFO - validation batch 151, loss: 0.959, 4832/10016 datapoints
2025-03-06 18:44:46,659 - INFO - validation batch 201, loss: 0.909, 6432/10016 datapoints
2025-03-06 18:44:46,814 - INFO - validation batch 251, loss: 0.910, 8032/10016 datapoints
2025-03-06 18:44:46,968 - INFO - validation batch 301, loss: 1.035, 9632/10016 datapoints
2025-03-06 18:44:47,006 - INFO - Epoch 48/800 done.
2025-03-06 18:44:47,006 - INFO - Final validation performance:
Loss: 1.001, top-1 acc: 0.793top-5 acc: 0.793
2025-03-06 18:44:47,007 - INFO - Beginning epoch 49/800
2025-03-06 18:44:47,013 - INFO - training batch 1, loss: 0.931, 32/60000 datapoints
2025-03-06 18:44:47,223 - INFO - training batch 51, loss: 1.151, 1632/60000 datapoints
2025-03-06 18:44:47,417 - INFO - training batch 101, loss: 0.977, 3232/60000 datapoints
2025-03-06 18:44:47,621 - INFO - training batch 151, loss: 0.928, 4832/60000 datapoints
2025-03-06 18:44:47,819 - INFO - training batch 201, loss: 0.837, 6432/60000 datapoints
2025-03-06 18:44:48,016 - INFO - training batch 251, loss: 1.091, 8032/60000 datapoints
2025-03-06 18:44:48,210 - INFO - training batch 301, loss: 1.141, 9632/60000 datapoints
2025-03-06 18:44:48,408 - INFO - training batch 351, loss: 0.846, 11232/60000 datapoints
2025-03-06 18:44:48,601 - INFO - training batch 401, loss: 0.945, 12832/60000 datapoints
2025-03-06 18:44:48,799 - INFO - training batch 451, loss: 0.929, 14432/60000 datapoints
2025-03-06 18:44:48,994 - INFO - training batch 501, loss: 0.983, 16032/60000 datapoints
2025-03-06 18:44:49,188 - INFO - training batch 551, loss: 0.853, 17632/60000 datapoints
2025-03-06 18:44:49,383 - INFO - training batch 601, loss: 0.822, 19232/60000 datapoints
2025-03-06 18:44:49,575 - INFO - training batch 651, loss: 0.933, 20832/60000 datapoints
2025-03-06 18:44:49,775 - INFO - training batch 701, loss: 1.004, 22432/60000 datapoints
2025-03-06 18:44:49,971 - INFO - training batch 751, loss: 0.908, 24032/60000 datapoints
2025-03-06 18:44:50,165 - INFO - training batch 801, loss: 0.934, 25632/60000 datapoints
2025-03-06 18:44:50,360 - INFO - training batch 851, loss: 0.952, 27232/60000 datapoints
2025-03-06 18:44:50,573 - INFO - training batch 901, loss: 0.993, 28832/60000 datapoints
2025-03-06 18:44:50,768 - INFO - training batch 951, loss: 0.697, 30432/60000 datapoints
2025-03-06 18:44:50,961 - INFO - training batch 1001, loss: 0.887, 32032/60000 datapoints
2025-03-06 18:44:51,154 - INFO - training batch 1051, loss: 0.777, 33632/60000 datapoints
2025-03-06 18:44:51,348 - INFO - training batch 1101, loss: 0.951, 35232/60000 datapoints
2025-03-06 18:44:51,542 - INFO - training batch 1151, loss: 0.971, 36832/60000 datapoints
2025-03-06 18:44:51,741 - INFO - training batch 1201, loss: 1.215, 38432/60000 datapoints
2025-03-06 18:44:51,938 - INFO - training batch 1251, loss: 1.075, 40032/60000 datapoints
2025-03-06 18:44:52,135 - INFO - training batch 1301, loss: 1.081, 41632/60000 datapoints
2025-03-06 18:44:52,336 - INFO - training batch 1351, loss: 1.045, 43232/60000 datapoints
2025-03-06 18:44:52,531 - INFO - training batch 1401, loss: 1.046, 44832/60000 datapoints
2025-03-06 18:44:52,728 - INFO - training batch 1451, loss: 1.097, 46432/60000 datapoints
2025-03-06 18:44:52,922 - INFO - training batch 1501, loss: 1.037, 48032/60000 datapoints
2025-03-06 18:44:53,117 - INFO - training batch 1551, loss: 0.885, 49632/60000 datapoints
2025-03-06 18:44:53,311 - INFO - training batch 1601, loss: 1.142, 51232/60000 datapoints
2025-03-06 18:44:53,504 - INFO - training batch 1651, loss: 0.956, 52832/60000 datapoints
2025-03-06 18:44:53,697 - INFO - training batch 1701, loss: 0.847, 54432/60000 datapoints
2025-03-06 18:44:53,891 - INFO - training batch 1751, loss: 1.019, 56032/60000 datapoints
2025-03-06 18:44:54,085 - INFO - training batch 1801, loss: 0.909, 57632/60000 datapoints
2025-03-06 18:44:54,280 - INFO - training batch 1851, loss: 1.045, 59232/60000 datapoints
2025-03-06 18:44:54,383 - INFO - validation batch 1, loss: 0.880, 32/10016 datapoints
2025-03-06 18:44:54,536 - INFO - validation batch 51, loss: 0.837, 1632/10016 datapoints
2025-03-06 18:44:54,689 - INFO - validation batch 101, loss: 0.638, 3232/10016 datapoints
2025-03-06 18:44:54,842 - INFO - validation batch 151, loss: 1.044, 4832/10016 datapoints
2025-03-06 18:44:54,999 - INFO - validation batch 201, loss: 0.977, 6432/10016 datapoints
2025-03-06 18:44:55,150 - INFO - validation batch 251, loss: 0.959, 8032/10016 datapoints
2025-03-06 18:44:55,308 - INFO - validation batch 301, loss: 0.920, 9632/10016 datapoints
2025-03-06 18:44:55,344 - INFO - Epoch 49/800 done.
2025-03-06 18:44:55,345 - INFO - Final validation performance:
Loss: 0.894, top-1 acc: 0.796top-5 acc: 0.796
2025-03-06 18:44:55,345 - INFO - Beginning epoch 50/800
2025-03-06 18:44:55,352 - INFO - training batch 1, loss: 0.850, 32/60000 datapoints
2025-03-06 18:44:55,554 - INFO - training batch 51, loss: 0.827, 1632/60000 datapoints
2025-03-06 18:44:55,790 - INFO - training batch 101, loss: 1.240, 3232/60000 datapoints
2025-03-06 18:44:56,007 - INFO - training batch 151, loss: 0.937, 4832/60000 datapoints
2025-03-06 18:44:56,231 - INFO - training batch 201, loss: 0.817, 6432/60000 datapoints
2025-03-06 18:44:56,431 - INFO - training batch 251, loss: 0.885, 8032/60000 datapoints
2025-03-06 18:44:56,630 - INFO - training batch 301, loss: 0.786, 9632/60000 datapoints
2025-03-06 18:44:56,825 - INFO - training batch 351, loss: 1.019, 11232/60000 datapoints
2025-03-06 18:44:57,018 - INFO - training batch 401, loss: 0.934, 12832/60000 datapoints
2025-03-06 18:44:57,214 - INFO - training batch 451, loss: 1.134, 14432/60000 datapoints
2025-03-06 18:44:57,411 - INFO - training batch 501, loss: 1.152, 16032/60000 datapoints
2025-03-06 18:44:57,607 - INFO - training batch 551, loss: 0.987, 17632/60000 datapoints
2025-03-06 18:44:57,803 - INFO - training batch 601, loss: 0.970, 19232/60000 datapoints
2025-03-06 18:44:58,000 - INFO - training batch 651, loss: 0.901, 20832/60000 datapoints
2025-03-06 18:44:58,199 - INFO - training batch 701, loss: 0.829, 22432/60000 datapoints
2025-03-06 18:44:58,397 - INFO - training batch 751, loss: 1.131, 24032/60000 datapoints
2025-03-06 18:44:58,590 - INFO - training batch 801, loss: 0.866, 25632/60000 datapoints
2025-03-06 18:44:58,788 - INFO - training batch 851, loss: 0.884, 27232/60000 datapoints
2025-03-06 18:44:58,984 - INFO - training batch 901, loss: 0.849, 28832/60000 datapoints
2025-03-06 18:44:59,179 - INFO - training batch 951, loss: 0.930, 30432/60000 datapoints
2025-03-06 18:44:59,374 - INFO - training batch 1001, loss: 0.954, 32032/60000 datapoints
2025-03-06 18:44:59,565 - INFO - training batch 1051, loss: 0.765, 33632/60000 datapoints
2025-03-06 18:44:59,761 - INFO - training batch 1101, loss: 0.733, 35232/60000 datapoints
2025-03-06 18:44:59,955 - INFO - training batch 1151, loss: 0.966, 36832/60000 datapoints
2025-03-06 18:45:00,152 - INFO - training batch 1201, loss: 0.882, 38432/60000 datapoints
2025-03-06 18:45:00,345 - INFO - training batch 1251, loss: 0.963, 40032/60000 datapoints
2025-03-06 18:45:00,541 - INFO - training batch 1301, loss: 0.997, 41632/60000 datapoints
2025-03-06 18:45:00,758 - INFO - training batch 1351, loss: 1.010, 43232/60000 datapoints
2025-03-06 18:45:00,951 - INFO - training batch 1401, loss: 0.938, 44832/60000 datapoints
2025-03-06 18:45:01,145 - INFO - training batch 1451, loss: 0.976, 46432/60000 datapoints
2025-03-06 18:45:01,338 - INFO - training batch 1501, loss: 1.008, 48032/60000 datapoints
2025-03-06 18:45:01,533 - INFO - training batch 1551, loss: 0.923, 49632/60000 datapoints
2025-03-06 18:45:01,731 - INFO - training batch 1601, loss: 0.940, 51232/60000 datapoints
2025-03-06 18:45:01,926 - INFO - training batch 1651, loss: 0.950, 52832/60000 datapoints
2025-03-06 18:45:02,119 - INFO - training batch 1701, loss: 0.795, 54432/60000 datapoints
2025-03-06 18:45:02,316 - INFO - training batch 1751, loss: 0.923, 56032/60000 datapoints
2025-03-06 18:45:02,513 - INFO - training batch 1801, loss: 0.935, 57632/60000 datapoints
2025-03-06 18:45:02,707 - INFO - training batch 1851, loss: 0.951, 59232/60000 datapoints
2025-03-06 18:45:02,808 - INFO - validation batch 1, loss: 0.790, 32/10016 datapoints
2025-03-06 18:45:02,962 - INFO - validation batch 51, loss: 0.938, 1632/10016 datapoints
2025-03-06 18:45:03,114 - INFO - validation batch 101, loss: 0.871, 3232/10016 datapoints
2025-03-06 18:45:03,267 - INFO - validation batch 151, loss: 1.199, 4832/10016 datapoints
2025-03-06 18:45:03,419 - INFO - validation batch 201, loss: 0.850, 6432/10016 datapoints
2025-03-06 18:45:03,569 - INFO - validation batch 251, loss: 0.735, 8032/10016 datapoints
2025-03-06 18:45:03,724 - INFO - validation batch 301, loss: 0.951, 9632/10016 datapoints
2025-03-06 18:45:03,762 - INFO - Epoch 50/800 done.
2025-03-06 18:45:03,762 - INFO - Final validation performance:
Loss: 0.905, top-1 acc: 0.799top-5 acc: 0.799
2025-03-06 18:45:03,762 - INFO - Beginning epoch 51/800
2025-03-06 18:45:03,769 - INFO - training batch 1, loss: 0.860, 32/60000 datapoints
2025-03-06 18:45:03,964 - INFO - training batch 51, loss: 0.948, 1632/60000 datapoints
2025-03-06 18:45:04,160 - INFO - training batch 101, loss: 0.841, 3232/60000 datapoints
2025-03-06 18:45:04,361 - INFO - training batch 151, loss: 1.005, 4832/60000 datapoints
2025-03-06 18:45:04,561 - INFO - training batch 201, loss: 0.878, 6432/60000 datapoints
2025-03-06 18:45:04,759 - INFO - training batch 251, loss: 1.045, 8032/60000 datapoints
2025-03-06 18:45:04,964 - INFO - training batch 301, loss: 0.973, 9632/60000 datapoints
2025-03-06 18:45:05,162 - INFO - training batch 351, loss: 1.005, 11232/60000 datapoints
2025-03-06 18:45:05,363 - INFO - training batch 401, loss: 1.010, 12832/60000 datapoints
2025-03-06 18:45:05,558 - INFO - training batch 451, loss: 0.934, 14432/60000 datapoints
2025-03-06 18:45:05,754 - INFO - training batch 501, loss: 0.970, 16032/60000 datapoints
2025-03-06 18:45:05,948 - INFO - training batch 551, loss: 0.814, 17632/60000 datapoints
2025-03-06 18:45:06,140 - INFO - training batch 601, loss: 1.038, 19232/60000 datapoints
2025-03-06 18:45:06,337 - INFO - training batch 651, loss: 1.002, 20832/60000 datapoints
2025-03-06 18:45:06,534 - INFO - training batch 701, loss: 1.224, 22432/60000 datapoints
2025-03-06 18:45:06,733 - INFO - training batch 751, loss: 1.029, 24032/60000 datapoints
2025-03-06 18:45:06,929 - INFO - training batch 801, loss: 0.846, 25632/60000 datapoints
2025-03-06 18:45:07,121 - INFO - training batch 851, loss: 0.993, 27232/60000 datapoints
2025-03-06 18:45:07,320 - INFO - training batch 901, loss: 0.855, 28832/60000 datapoints
2025-03-06 18:45:07,513 - INFO - training batch 951, loss: 0.978, 30432/60000 datapoints
2025-03-06 18:45:07,713 - INFO - training batch 1001, loss: 0.950, 32032/60000 datapoints
2025-03-06 18:45:07,912 - INFO - training batch 1051, loss: 1.063, 33632/60000 datapoints
2025-03-06 18:45:08,109 - INFO - training batch 1101, loss: 1.046, 35232/60000 datapoints
2025-03-06 18:45:08,301 - INFO - training batch 1151, loss: 0.913, 36832/60000 datapoints
2025-03-06 18:45:08,498 - INFO - training batch 1201, loss: 1.020, 38432/60000 datapoints
2025-03-06 18:45:08,694 - INFO - training batch 1251, loss: 0.754, 40032/60000 datapoints
2025-03-06 18:45:08,888 - INFO - training batch 1301, loss: 0.793, 41632/60000 datapoints
2025-03-06 18:45:09,085 - INFO - training batch 1351, loss: 0.944, 43232/60000 datapoints
2025-03-06 18:45:09,279 - INFO - training batch 1401, loss: 0.873, 44832/60000 datapoints
2025-03-06 18:45:09,471 - INFO - training batch 1451, loss: 0.772, 46432/60000 datapoints
2025-03-06 18:45:09,667 - INFO - training batch 1501, loss: 0.963, 48032/60000 datapoints
2025-03-06 18:45:09,862 - INFO - training batch 1551, loss: 0.874, 49632/60000 datapoints
2025-03-06 18:45:10,058 - INFO - training batch 1601, loss: 0.977, 51232/60000 datapoints
2025-03-06 18:45:10,249 - INFO - training batch 1651, loss: 0.926, 52832/60000 datapoints
2025-03-06 18:45:10,449 - INFO - training batch 1701, loss: 1.145, 54432/60000 datapoints
2025-03-06 18:45:10,658 - INFO - training batch 1751, loss: 0.790, 56032/60000 datapoints
2025-03-06 18:45:10,872 - INFO - training batch 1801, loss: 0.934, 57632/60000 datapoints
2025-03-06 18:45:11,064 - INFO - training batch 1851, loss: 0.778, 59232/60000 datapoints
2025-03-06 18:45:11,164 - INFO - validation batch 1, loss: 0.736, 32/10016 datapoints
2025-03-06 18:45:11,316 - INFO - validation batch 51, loss: 0.770, 1632/10016 datapoints
2025-03-06 18:45:11,467 - INFO - validation batch 101, loss: 1.093, 3232/10016 datapoints
2025-03-06 18:45:11,622 - INFO - validation batch 151, loss: 0.991, 4832/10016 datapoints
2025-03-06 18:45:11,775 - INFO - validation batch 201, loss: 0.950, 6432/10016 datapoints
2025-03-06 18:45:11,927 - INFO - validation batch 251, loss: 0.820, 8032/10016 datapoints
2025-03-06 18:45:12,088 - INFO - validation batch 301, loss: 0.868, 9632/10016 datapoints
2025-03-06 18:45:12,124 - INFO - Epoch 51/800 done.
2025-03-06 18:45:12,125 - INFO - Final validation performance:
Loss: 0.890, top-1 acc: 0.801top-5 acc: 0.801
2025-03-06 18:45:12,125 - INFO - Beginning epoch 52/800
2025-03-06 18:45:12,131 - INFO - training batch 1, loss: 0.764, 32/60000 datapoints
2025-03-06 18:45:12,339 - INFO - training batch 51, loss: 0.914, 1632/60000 datapoints
2025-03-06 18:45:12,539 - INFO - training batch 101, loss: 0.911, 3232/60000 datapoints
2025-03-06 18:45:12,743 - INFO - training batch 151, loss: 0.963, 4832/60000 datapoints
2025-03-06 18:45:12,940 - INFO - training batch 201, loss: 1.063, 6432/60000 datapoints
2025-03-06 18:45:13,173 - INFO - training batch 251, loss: 1.087, 8032/60000 datapoints
2025-03-06 18:45:13,373 - INFO - training batch 301, loss: 0.879, 9632/60000 datapoints
2025-03-06 18:45:13,570 - INFO - training batch 351, loss: 0.918, 11232/60000 datapoints
2025-03-06 18:45:13,772 - INFO - training batch 401, loss: 0.801, 12832/60000 datapoints
2025-03-06 18:45:13,969 - INFO - training batch 451, loss: 0.913, 14432/60000 datapoints
2025-03-06 18:45:14,164 - INFO - training batch 501, loss: 0.954, 16032/60000 datapoints
2025-03-06 18:45:14,362 - INFO - training batch 551, loss: 0.763, 17632/60000 datapoints
2025-03-06 18:45:14,562 - INFO - training batch 601, loss: 0.996, 19232/60000 datapoints
2025-03-06 18:45:14,760 - INFO - training batch 651, loss: 0.855, 20832/60000 datapoints
2025-03-06 18:45:14,963 - INFO - training batch 701, loss: 0.813, 22432/60000 datapoints
2025-03-06 18:45:15,158 - INFO - training batch 751, loss: 0.805, 24032/60000 datapoints
2025-03-06 18:45:15,357 - INFO - training batch 801, loss: 0.773, 25632/60000 datapoints
2025-03-06 18:45:15,551 - INFO - training batch 851, loss: 0.908, 27232/60000 datapoints
2025-03-06 18:45:15,751 - INFO - training batch 901, loss: 0.918, 28832/60000 datapoints
2025-03-06 18:45:15,945 - INFO - training batch 951, loss: 0.763, 30432/60000 datapoints
2025-03-06 18:45:16,137 - INFO - training batch 1001, loss: 0.925, 32032/60000 datapoints
2025-03-06 18:45:16,336 - INFO - training batch 1051, loss: 0.753, 33632/60000 datapoints
2025-03-06 18:45:16,533 - INFO - training batch 1101, loss: 0.886, 35232/60000 datapoints
2025-03-06 18:45:16,730 - INFO - training batch 1151, loss: 0.770, 36832/60000 datapoints
2025-03-06 18:45:16,924 - INFO - training batch 1201, loss: 0.787, 38432/60000 datapoints
2025-03-06 18:45:17,119 - INFO - training batch 1251, loss: 1.035, 40032/60000 datapoints
2025-03-06 18:45:17,312 - INFO - training batch 1301, loss: 1.089, 41632/60000 datapoints
2025-03-06 18:45:17,505 - INFO - training batch 1351, loss: 0.906, 43232/60000 datapoints
2025-03-06 18:45:17,702 - INFO - training batch 1401, loss: 0.683, 44832/60000 datapoints
2025-03-06 18:45:17,895 - INFO - training batch 1451, loss: 0.898, 46432/60000 datapoints
2025-03-06 18:45:18,090 - INFO - training batch 1501, loss: 0.811, 48032/60000 datapoints
2025-03-06 18:45:18,283 - INFO - training batch 1551, loss: 1.028, 49632/60000 datapoints
2025-03-06 18:45:18,478 - INFO - training batch 1601, loss: 0.972, 51232/60000 datapoints
2025-03-06 18:45:18,675 - INFO - training batch 1651, loss: 1.150, 52832/60000 datapoints
2025-03-06 18:45:18,869 - INFO - training batch 1701, loss: 0.854, 54432/60000 datapoints
2025-03-06 18:45:19,061 - INFO - training batch 1751, loss: 1.031, 56032/60000 datapoints
2025-03-06 18:45:19,255 - INFO - training batch 1801, loss: 0.912, 57632/60000 datapoints
2025-03-06 18:45:19,450 - INFO - training batch 1851, loss: 0.990, 59232/60000 datapoints
2025-03-06 18:45:19,551 - INFO - validation batch 1, loss: 0.899, 32/10016 datapoints
2025-03-06 18:45:19,708 - INFO - validation batch 51, loss: 0.996, 1632/10016 datapoints
2025-03-06 18:45:19,859 - INFO - validation batch 101, loss: 0.872, 3232/10016 datapoints
2025-03-06 18:45:20,013 - INFO - validation batch 151, loss: 0.803, 4832/10016 datapoints
2025-03-06 18:45:20,166 - INFO - validation batch 201, loss: 0.921, 6432/10016 datapoints
2025-03-06 18:45:20,319 - INFO - validation batch 251, loss: 0.819, 8032/10016 datapoints
2025-03-06 18:45:20,473 - INFO - validation batch 301, loss: 0.881, 9632/10016 datapoints
2025-03-06 18:45:20,511 - INFO - Epoch 52/800 done.
2025-03-06 18:45:20,511 - INFO - Final validation performance:
Loss: 0.884, top-1 acc: 0.803top-5 acc: 0.803
2025-03-06 18:45:20,512 - INFO - Beginning epoch 53/800
2025-03-06 18:45:20,518 - INFO - training batch 1, loss: 0.931, 32/60000 datapoints
2025-03-06 18:45:20,717 - INFO - training batch 51, loss: 1.038, 1632/60000 datapoints
2025-03-06 18:45:20,932 - INFO - training batch 101, loss: 0.960, 3232/60000 datapoints
2025-03-06 18:45:21,142 - INFO - training batch 151, loss: 0.839, 4832/60000 datapoints
2025-03-06 18:45:21,337 - INFO - training batch 201, loss: 1.011, 6432/60000 datapoints
2025-03-06 18:45:21,531 - INFO - training batch 251, loss: 0.998, 8032/60000 datapoints
2025-03-06 18:45:21,727 - INFO - training batch 301, loss: 0.902, 9632/60000 datapoints
2025-03-06 18:45:21,931 - INFO - training batch 351, loss: 0.842, 11232/60000 datapoints
2025-03-06 18:45:22,129 - INFO - training batch 401, loss: 0.722, 12832/60000 datapoints
2025-03-06 18:45:22,325 - INFO - training batch 451, loss: 1.207, 14432/60000 datapoints
2025-03-06 18:45:22,522 - INFO - training batch 501, loss: 1.080, 16032/60000 datapoints
2025-03-06 18:45:22,722 - INFO - training batch 551, loss: 0.786, 17632/60000 datapoints
2025-03-06 18:45:22,915 - INFO - training batch 601, loss: 0.926, 19232/60000 datapoints
2025-03-06 18:45:23,108 - INFO - training batch 651, loss: 0.973, 20832/60000 datapoints
2025-03-06 18:45:23,301 - INFO - training batch 701, loss: 1.003, 22432/60000 datapoints
2025-03-06 18:45:23,496 - INFO - training batch 751, loss: 0.886, 24032/60000 datapoints
2025-03-06 18:45:23,690 - INFO - training batch 801, loss: 0.907, 25632/60000 datapoints
2025-03-06 18:45:23,886 - INFO - training batch 851, loss: 0.996, 27232/60000 datapoints
2025-03-06 18:45:24,077 - INFO - training batch 901, loss: 0.987, 28832/60000 datapoints
2025-03-06 18:45:24,278 - INFO - training batch 951, loss: 0.905, 30432/60000 datapoints
2025-03-06 18:45:24,474 - INFO - training batch 1001, loss: 0.889, 32032/60000 datapoints
2025-03-06 18:45:24,670 - INFO - training batch 1051, loss: 0.843, 33632/60000 datapoints
2025-03-06 18:45:24,864 - INFO - training batch 1101, loss: 0.791, 35232/60000 datapoints
2025-03-06 18:45:25,060 - INFO - training batch 1151, loss: 0.747, 36832/60000 datapoints
2025-03-06 18:45:25,252 - INFO - training batch 1201, loss: 0.841, 38432/60000 datapoints
2025-03-06 18:45:25,452 - INFO - training batch 1251, loss: 0.992, 40032/60000 datapoints
2025-03-06 18:45:25,649 - INFO - training batch 1301, loss: 0.931, 41632/60000 datapoints
2025-03-06 18:45:25,848 - INFO - training batch 1351, loss: 1.041, 43232/60000 datapoints
2025-03-06 18:45:26,041 - INFO - training batch 1401, loss: 0.877, 44832/60000 datapoints
2025-03-06 18:45:26,236 - INFO - training batch 1451, loss: 0.891, 46432/60000 datapoints
2025-03-06 18:45:26,429 - INFO - training batch 1501, loss: 0.682, 48032/60000 datapoints
2025-03-06 18:45:26,627 - INFO - training batch 1551, loss: 0.806, 49632/60000 datapoints
2025-03-06 18:45:26,822 - INFO - training batch 1601, loss: 0.973, 51232/60000 datapoints
2025-03-06 18:45:27,016 - INFO - training batch 1651, loss: 0.799, 52832/60000 datapoints
2025-03-06 18:45:27,209 - INFO - training batch 1701, loss: 0.814, 54432/60000 datapoints
2025-03-06 18:45:27,402 - INFO - training batch 1751, loss: 0.750, 56032/60000 datapoints
2025-03-06 18:45:27,595 - INFO - training batch 1801, loss: 0.695, 57632/60000 datapoints
2025-03-06 18:45:27,791 - INFO - training batch 1851, loss: 0.800, 59232/60000 datapoints
2025-03-06 18:45:27,891 - INFO - validation batch 1, loss: 0.877, 32/10016 datapoints
2025-03-06 18:45:28,049 - INFO - validation batch 51, loss: 0.753, 1632/10016 datapoints
2025-03-06 18:45:28,207 - INFO - validation batch 101, loss: 0.795, 3232/10016 datapoints
2025-03-06 18:45:28,360 - INFO - validation batch 151, loss: 0.871, 4832/10016 datapoints
2025-03-06 18:45:28,513 - INFO - validation batch 201, loss: 0.767, 6432/10016 datapoints
2025-03-06 18:45:28,668 - INFO - validation batch 251, loss: 0.825, 8032/10016 datapoints
2025-03-06 18:45:28,820 - INFO - validation batch 301, loss: 1.101, 9632/10016 datapoints
2025-03-06 18:45:28,858 - INFO - Epoch 53/800 done.
2025-03-06 18:45:28,858 - INFO - Final validation performance:
Loss: 0.856, top-1 acc: 0.807top-5 acc: 0.807
2025-03-06 18:45:28,859 - INFO - Beginning epoch 54/800
2025-03-06 18:45:28,865 - INFO - training batch 1, loss: 0.877, 32/60000 datapoints
2025-03-06 18:45:29,070 - INFO - training batch 51, loss: 0.968, 1632/60000 datapoints
2025-03-06 18:45:29,265 - INFO - training batch 101, loss: 0.932, 3232/60000 datapoints
2025-03-06 18:45:29,459 - INFO - training batch 151, loss: 0.937, 4832/60000 datapoints
2025-03-06 18:45:29,661 - INFO - training batch 201, loss: 0.978, 6432/60000 datapoints
2025-03-06 18:45:29,858 - INFO - training batch 251, loss: 0.749, 8032/60000 datapoints
2025-03-06 18:45:30,057 - INFO - training batch 301, loss: 1.041, 9632/60000 datapoints
2025-03-06 18:45:30,249 - INFO - training batch 351, loss: 0.949, 11232/60000 datapoints
2025-03-06 18:45:30,441 - INFO - training batch 401, loss: 0.849, 12832/60000 datapoints
2025-03-06 18:45:30,640 - INFO - training batch 451, loss: 0.805, 14432/60000 datapoints
2025-03-06 18:45:30,838 - INFO - training batch 501, loss: 1.027, 16032/60000 datapoints
2025-03-06 18:45:31,050 - INFO - training batch 551, loss: 0.855, 17632/60000 datapoints
2025-03-06 18:45:31,244 - INFO - training batch 601, loss: 0.993, 19232/60000 datapoints
2025-03-06 18:45:31,438 - INFO - training batch 651, loss: 0.858, 20832/60000 datapoints
2025-03-06 18:45:31,636 - INFO - training batch 701, loss: 1.001, 22432/60000 datapoints
2025-03-06 18:45:31,832 - INFO - training batch 751, loss: 0.839, 24032/60000 datapoints
2025-03-06 18:45:32,025 - INFO - training batch 801, loss: 0.864, 25632/60000 datapoints
2025-03-06 18:45:32,217 - INFO - training batch 851, loss: 0.912, 27232/60000 datapoints
2025-03-06 18:45:32,411 - INFO - training batch 901, loss: 0.612, 28832/60000 datapoints
2025-03-06 18:45:32,610 - INFO - training batch 951, loss: 0.782, 30432/60000 datapoints
2025-03-06 18:45:32,802 - INFO - training batch 1001, loss: 0.768, 32032/60000 datapoints
2025-03-06 18:45:32,995 - INFO - training batch 1051, loss: 0.615, 33632/60000 datapoints
2025-03-06 18:45:33,193 - INFO - training batch 1101, loss: 0.836, 35232/60000 datapoints
2025-03-06 18:45:33,387 - INFO - training batch 1151, loss: 0.975, 36832/60000 datapoints
2025-03-06 18:45:33,580 - INFO - training batch 1201, loss: 0.919, 38432/60000 datapoints
2025-03-06 18:45:33,774 - INFO - training batch 1251, loss: 0.701, 40032/60000 datapoints
2025-03-06 18:45:33,967 - INFO - training batch 1301, loss: 0.848, 41632/60000 datapoints
2025-03-06 18:45:34,161 - INFO - training batch 1351, loss: 0.853, 43232/60000 datapoints
2025-03-06 18:45:34,353 - INFO - training batch 1401, loss: 0.971, 44832/60000 datapoints
2025-03-06 18:45:34,548 - INFO - training batch 1451, loss: 0.802, 46432/60000 datapoints
2025-03-06 18:45:34,746 - INFO - training batch 1501, loss: 0.901, 48032/60000 datapoints
2025-03-06 18:45:34,943 - INFO - training batch 1551, loss: 0.760, 49632/60000 datapoints
2025-03-06 18:45:35,136 - INFO - training batch 1601, loss: 0.867, 51232/60000 datapoints
2025-03-06 18:45:35,336 - INFO - training batch 1651, loss: 0.763, 52832/60000 datapoints
2025-03-06 18:45:35,530 - INFO - training batch 1701, loss: 0.851, 54432/60000 datapoints
2025-03-06 18:45:35,725 - INFO - training batch 1751, loss: 0.826, 56032/60000 datapoints
2025-03-06 18:45:35,921 - INFO - training batch 1801, loss: 0.877, 57632/60000 datapoints
2025-03-06 18:45:36,113 - INFO - training batch 1851, loss: 1.039, 59232/60000 datapoints
2025-03-06 18:45:36,217 - INFO - validation batch 1, loss: 0.865, 32/10016 datapoints
2025-03-06 18:45:36,369 - INFO - validation batch 51, loss: 0.961, 1632/10016 datapoints
2025-03-06 18:45:36,524 - INFO - validation batch 101, loss: 0.743, 3232/10016 datapoints
2025-03-06 18:45:36,678 - INFO - validation batch 151, loss: 0.922, 4832/10016 datapoints
2025-03-06 18:45:36,831 - INFO - validation batch 201, loss: 0.854, 6432/10016 datapoints
2025-03-06 18:45:36,984 - INFO - validation batch 251, loss: 0.934, 8032/10016 datapoints
2025-03-06 18:45:37,136 - INFO - validation batch 301, loss: 0.675, 9632/10016 datapoints
2025-03-06 18:45:37,173 - INFO - Epoch 54/800 done.
2025-03-06 18:45:37,173 - INFO - Final validation performance:
Loss: 0.851, top-1 acc: 0.809top-5 acc: 0.809
2025-03-06 18:45:37,173 - INFO - Beginning epoch 55/800
2025-03-06 18:45:37,179 - INFO - training batch 1, loss: 0.806, 32/60000 datapoints
2025-03-06 18:45:37,376 - INFO - training batch 51, loss: 0.815, 1632/60000 datapoints
2025-03-06 18:45:37,576 - INFO - training batch 101, loss: 1.040, 3232/60000 datapoints
2025-03-06 18:45:37,796 - INFO - training batch 151, loss: 0.864, 4832/60000 datapoints
2025-03-06 18:45:37,990 - INFO - training batch 201, loss: 0.958, 6432/60000 datapoints
2025-03-06 18:45:38,194 - INFO - training batch 251, loss: 0.744, 8032/60000 datapoints
2025-03-06 18:45:38,391 - INFO - training batch 301, loss: 0.831, 9632/60000 datapoints
2025-03-06 18:45:38,591 - INFO - training batch 351, loss: 0.759, 11232/60000 datapoints
2025-03-06 18:45:38,788 - INFO - training batch 401, loss: 0.865, 12832/60000 datapoints
2025-03-06 18:45:38,982 - INFO - training batch 451, loss: 0.646, 14432/60000 datapoints
2025-03-06 18:45:39,177 - INFO - training batch 501, loss: 0.817, 16032/60000 datapoints
2025-03-06 18:45:39,371 - INFO - training batch 551, loss: 0.796, 17632/60000 datapoints
2025-03-06 18:45:39,563 - INFO - training batch 601, loss: 0.779, 19232/60000 datapoints
2025-03-06 18:45:39,761 - INFO - training batch 651, loss: 0.904, 20832/60000 datapoints
2025-03-06 18:45:39,963 - INFO - training batch 701, loss: 1.061, 22432/60000 datapoints
2025-03-06 18:45:40,187 - INFO - training batch 751, loss: 0.744, 24032/60000 datapoints
2025-03-06 18:45:40,380 - INFO - training batch 801, loss: 0.935, 25632/60000 datapoints
2025-03-06 18:45:40,576 - INFO - training batch 851, loss: 1.118, 27232/60000 datapoints
2025-03-06 18:45:40,773 - INFO - training batch 901, loss: 0.745, 28832/60000 datapoints
2025-03-06 18:45:40,983 - INFO - training batch 951, loss: 0.806, 30432/60000 datapoints
2025-03-06 18:45:41,182 - INFO - training batch 1001, loss: 1.024, 32032/60000 datapoints
2025-03-06 18:45:41,375 - INFO - training batch 1051, loss: 0.721, 33632/60000 datapoints
2025-03-06 18:45:41,568 - INFO - training batch 1101, loss: 1.012, 35232/60000 datapoints
2025-03-06 18:45:41,761 - INFO - training batch 1151, loss: 0.864, 36832/60000 datapoints
2025-03-06 18:45:41,956 - INFO - training batch 1201, loss: 0.785, 38432/60000 datapoints
2025-03-06 18:45:42,151 - INFO - training batch 1251, loss: 0.854, 40032/60000 datapoints
2025-03-06 18:45:42,347 - INFO - training batch 1301, loss: 0.804, 41632/60000 datapoints
2025-03-06 18:45:42,544 - INFO - training batch 1351, loss: 1.033, 43232/60000 datapoints
2025-03-06 18:45:42,739 - INFO - training batch 1401, loss: 0.941, 44832/60000 datapoints
2025-03-06 18:45:42,933 - INFO - training batch 1451, loss: 0.760, 46432/60000 datapoints
2025-03-06 18:45:43,124 - INFO - training batch 1501, loss: 0.957, 48032/60000 datapoints
2025-03-06 18:45:43,319 - INFO - training batch 1551, loss: 0.584, 49632/60000 datapoints
2025-03-06 18:45:43,513 - INFO - training batch 1601, loss: 1.135, 51232/60000 datapoints
2025-03-06 18:45:43,709 - INFO - training batch 1651, loss: 0.941, 52832/60000 datapoints
2025-03-06 18:45:43,902 - INFO - training batch 1701, loss: 0.993, 54432/60000 datapoints
2025-03-06 18:45:44,096 - INFO - training batch 1751, loss: 1.048, 56032/60000 datapoints
2025-03-06 18:45:44,289 - INFO - training batch 1801, loss: 0.662, 57632/60000 datapoints
2025-03-06 18:45:44,483 - INFO - training batch 1851, loss: 0.714, 59232/60000 datapoints
2025-03-06 18:45:44,585 - INFO - validation batch 1, loss: 1.026, 32/10016 datapoints
2025-03-06 18:45:44,739 - INFO - validation batch 51, loss: 0.825, 1632/10016 datapoints
2025-03-06 18:45:44,902 - INFO - validation batch 101, loss: 0.796, 3232/10016 datapoints
2025-03-06 18:45:45,053 - INFO - validation batch 151, loss: 1.029, 4832/10016 datapoints
2025-03-06 18:45:45,205 - INFO - validation batch 201, loss: 0.822, 6432/10016 datapoints
2025-03-06 18:45:45,359 - INFO - validation batch 251, loss: 0.932, 8032/10016 datapoints
2025-03-06 18:45:45,510 - INFO - validation batch 301, loss: 0.758, 9632/10016 datapoints
2025-03-06 18:45:45,546 - INFO - Epoch 55/800 done.
2025-03-06 18:45:45,546 - INFO - Final validation performance:
Loss: 0.884, top-1 acc: 0.812top-5 acc: 0.812
2025-03-06 18:45:45,547 - INFO - Beginning epoch 56/800
2025-03-06 18:45:45,554 - INFO - training batch 1, loss: 0.986, 32/60000 datapoints
2025-03-06 18:45:45,748 - INFO - training batch 51, loss: 0.916, 1632/60000 datapoints
2025-03-06 18:45:45,947 - INFO - training batch 101, loss: 1.008, 3232/60000 datapoints
2025-03-06 18:45:46,142 - INFO - training batch 151, loss: 0.922, 4832/60000 datapoints
2025-03-06 18:45:46,335 - INFO - training batch 201, loss: 0.789, 6432/60000 datapoints
2025-03-06 18:45:46,537 - INFO - training batch 251, loss: 0.839, 8032/60000 datapoints
2025-03-06 18:45:46,730 - INFO - training batch 301, loss: 0.875, 9632/60000 datapoints
2025-03-06 18:45:46,923 - INFO - training batch 351, loss: 0.863, 11232/60000 datapoints
2025-03-06 18:45:47,113 - INFO - training batch 401, loss: 0.687, 12832/60000 datapoints
2025-03-06 18:45:47,307 - INFO - training batch 451, loss: 0.804, 14432/60000 datapoints
2025-03-06 18:45:47,498 - INFO - training batch 501, loss: 0.882, 16032/60000 datapoints
2025-03-06 18:45:47,689 - INFO - training batch 551, loss: 0.881, 17632/60000 datapoints
2025-03-06 18:45:47,878 - INFO - training batch 601, loss: 0.826, 19232/60000 datapoints
2025-03-06 18:45:48,071 - INFO - training batch 651, loss: 0.935, 20832/60000 datapoints
2025-03-06 18:45:48,265 - INFO - training batch 701, loss: 0.950, 22432/60000 datapoints
2025-03-06 18:45:48,458 - INFO - training batch 751, loss: 0.758, 24032/60000 datapoints
2025-03-06 18:45:48,655 - INFO - training batch 801, loss: 0.875, 25632/60000 datapoints
2025-03-06 18:45:48,846 - INFO - training batch 851, loss: 0.886, 27232/60000 datapoints
2025-03-06 18:45:49,038 - INFO - training batch 901, loss: 0.720, 28832/60000 datapoints
2025-03-06 18:45:49,229 - INFO - training batch 951, loss: 0.856, 30432/60000 datapoints
2025-03-06 18:45:49,421 - INFO - training batch 1001, loss: 0.770, 32032/60000 datapoints
2025-03-06 18:45:49,615 - INFO - training batch 1051, loss: 0.854, 33632/60000 datapoints
2025-03-06 18:45:49,808 - INFO - training batch 1101, loss: 0.877, 35232/60000 datapoints
2025-03-06 18:45:50,015 - INFO - training batch 1151, loss: 0.703, 36832/60000 datapoints
2025-03-06 18:45:50,204 - INFO - training batch 1201, loss: 0.933, 38432/60000 datapoints
2025-03-06 18:45:50,397 - INFO - training batch 1251, loss: 0.836, 40032/60000 datapoints
2025-03-06 18:45:50,590 - INFO - training batch 1301, loss: 0.810, 41632/60000 datapoints
2025-03-06 18:45:50,781 - INFO - training batch 1351, loss: 0.847, 43232/60000 datapoints
2025-03-06 18:45:50,975 - INFO - training batch 1401, loss: 0.921, 44832/60000 datapoints
2025-03-06 18:45:51,183 - INFO - training batch 1451, loss: 0.824, 46432/60000 datapoints
2025-03-06 18:45:51,373 - INFO - training batch 1501, loss: 0.851, 48032/60000 datapoints
2025-03-06 18:45:51,564 - INFO - training batch 1551, loss: 0.792, 49632/60000 datapoints
2025-03-06 18:45:51,755 - INFO - training batch 1601, loss: 0.681, 51232/60000 datapoints
2025-03-06 18:45:51,945 - INFO - training batch 1651, loss: 0.822, 52832/60000 datapoints
2025-03-06 18:45:52,138 - INFO - training batch 1701, loss: 0.719, 54432/60000 datapoints
2025-03-06 18:45:52,327 - INFO - training batch 1751, loss: 0.647, 56032/60000 datapoints
2025-03-06 18:45:52,519 - INFO - training batch 1801, loss: 0.872, 57632/60000 datapoints
2025-03-06 18:45:52,715 - INFO - training batch 1851, loss: 0.994, 59232/60000 datapoints
2025-03-06 18:45:52,813 - INFO - validation batch 1, loss: 0.948, 32/10016 datapoints
2025-03-06 18:45:52,963 - INFO - validation batch 51, loss: 0.832, 1632/10016 datapoints
2025-03-06 18:45:53,116 - INFO - validation batch 101, loss: 0.781, 3232/10016 datapoints
2025-03-06 18:45:53,265 - INFO - validation batch 151, loss: 0.807, 4832/10016 datapoints
2025-03-06 18:45:53,416 - INFO - validation batch 201, loss: 0.956, 6432/10016 datapoints
2025-03-06 18:45:53,566 - INFO - validation batch 251, loss: 0.757, 8032/10016 datapoints
2025-03-06 18:45:53,717 - INFO - validation batch 301, loss: 0.835, 9632/10016 datapoints
2025-03-06 18:45:53,754 - INFO - Epoch 56/800 done.
2025-03-06 18:45:53,754 - INFO - Final validation performance:
Loss: 0.845, top-1 acc: 0.814top-5 acc: 0.814
2025-03-06 18:45:53,754 - INFO - Beginning epoch 57/800
2025-03-06 18:45:53,760 - INFO - training batch 1, loss: 0.726, 32/60000 datapoints
2025-03-06 18:45:53,964 - INFO - training batch 51, loss: 0.770, 1632/60000 datapoints
2025-03-06 18:45:54,158 - INFO - training batch 101, loss: 0.899, 3232/60000 datapoints
2025-03-06 18:45:54,353 - INFO - training batch 151, loss: 0.968, 4832/60000 datapoints
2025-03-06 18:45:54,545 - INFO - training batch 201, loss: 0.831, 6432/60000 datapoints
2025-03-06 18:45:54,742 - INFO - training batch 251, loss: 0.744, 8032/60000 datapoints
2025-03-06 18:45:54,942 - INFO - training batch 301, loss: 0.725, 9632/60000 datapoints
2025-03-06 18:45:55,138 - INFO - training batch 351, loss: 0.986, 11232/60000 datapoints
2025-03-06 18:45:55,329 - INFO - training batch 401, loss: 0.884, 12832/60000 datapoints
2025-03-06 18:45:55,524 - INFO - training batch 451, loss: 0.827, 14432/60000 datapoints
2025-03-06 18:45:55,718 - INFO - training batch 501, loss: 0.891, 16032/60000 datapoints
2025-03-06 18:45:55,910 - INFO - training batch 551, loss: 0.867, 17632/60000 datapoints
2025-03-06 18:45:56,106 - INFO - training batch 601, loss: 0.755, 19232/60000 datapoints
2025-03-06 18:45:56,298 - INFO - training batch 651, loss: 1.006, 20832/60000 datapoints
2025-03-06 18:45:56,489 - INFO - training batch 701, loss: 0.790, 22432/60000 datapoints
2025-03-06 18:45:56,685 - INFO - training batch 751, loss: 0.907, 24032/60000 datapoints
2025-03-06 18:45:56,876 - INFO - training batch 801, loss: 0.704, 25632/60000 datapoints
2025-03-06 18:45:57,067 - INFO - training batch 851, loss: 1.007, 27232/60000 datapoints
2025-03-06 18:45:57,257 - INFO - training batch 901, loss: 1.055, 28832/60000 datapoints
2025-03-06 18:45:57,449 - INFO - training batch 951, loss: 0.809, 30432/60000 datapoints
2025-03-06 18:45:57,643 - INFO - training batch 1001, loss: 0.870, 32032/60000 datapoints
2025-03-06 18:45:57,832 - INFO - training batch 1051, loss: 0.690, 33632/60000 datapoints
2025-03-06 18:45:58,025 - INFO - training batch 1101, loss: 0.871, 35232/60000 datapoints
2025-03-06 18:45:58,220 - INFO - training batch 1151, loss: 0.877, 36832/60000 datapoints
2025-03-06 18:45:58,412 - INFO - training batch 1201, loss: 0.878, 38432/60000 datapoints
2025-03-06 18:45:58,614 - INFO - training batch 1251, loss: 0.912, 40032/60000 datapoints
2025-03-06 18:45:58,802 - INFO - training batch 1301, loss: 0.853, 41632/60000 datapoints
2025-03-06 18:45:58,993 - INFO - training batch 1351, loss: 0.825, 43232/60000 datapoints
2025-03-06 18:45:59,184 - INFO - training batch 1401, loss: 0.931, 44832/60000 datapoints
2025-03-06 18:45:59,376 - INFO - training batch 1451, loss: 0.961, 46432/60000 datapoints
2025-03-06 18:45:59,566 - INFO - training batch 1501, loss: 0.879, 48032/60000 datapoints
2025-03-06 18:45:59,756 - INFO - training batch 1551, loss: 0.526, 49632/60000 datapoints
2025-03-06 18:45:59,947 - INFO - training batch 1601, loss: 0.650, 51232/60000 datapoints
2025-03-06 18:46:00,139 - INFO - training batch 1651, loss: 0.936, 52832/60000 datapoints
2025-03-06 18:46:00,332 - INFO - training batch 1701, loss: 0.908, 54432/60000 datapoints
2025-03-06 18:46:00,521 - INFO - training batch 1751, loss: 0.890, 56032/60000 datapoints
2025-03-06 18:46:00,718 - INFO - training batch 1801, loss: 0.776, 57632/60000 datapoints
2025-03-06 18:46:00,910 - INFO - training batch 1851, loss: 0.855, 59232/60000 datapoints
2025-03-06 18:46:01,008 - INFO - validation batch 1, loss: 0.848, 32/10016 datapoints
2025-03-06 18:46:01,176 - INFO - validation batch 51, loss: 0.606, 1632/10016 datapoints
2025-03-06 18:46:01,327 - INFO - validation batch 101, loss: 0.701, 3232/10016 datapoints
2025-03-06 18:46:01,476 - INFO - validation batch 151, loss: 0.927, 4832/10016 datapoints
2025-03-06 18:46:01,630 - INFO - validation batch 201, loss: 0.599, 6432/10016 datapoints
2025-03-06 18:46:01,780 - INFO - validation batch 251, loss: 0.658, 8032/10016 datapoints
2025-03-06 18:46:01,931 - INFO - validation batch 301, loss: 0.863, 9632/10016 datapoints
2025-03-06 18:46:01,969 - INFO - Epoch 57/800 done.
2025-03-06 18:46:01,970 - INFO - Final validation performance:
Loss: 0.743, top-1 acc: 0.815top-5 acc: 0.815
2025-03-06 18:46:01,970 - INFO - Beginning epoch 58/800
2025-03-06 18:46:01,976 - INFO - training batch 1, loss: 1.036, 32/60000 datapoints
2025-03-06 18:46:02,175 - INFO - training batch 51, loss: 0.866, 1632/60000 datapoints
2025-03-06 18:46:02,370 - INFO - training batch 101, loss: 0.879, 3232/60000 datapoints
2025-03-06 18:46:02,577 - INFO - training batch 151, loss: 0.906, 4832/60000 datapoints
2025-03-06 18:46:02,777 - INFO - training batch 201, loss: 0.888, 6432/60000 datapoints
2025-03-06 18:46:02,970 - INFO - training batch 251, loss: 0.822, 8032/60000 datapoints
2025-03-06 18:46:03,173 - INFO - training batch 301, loss: 0.627, 9632/60000 datapoints
2025-03-06 18:46:03,372 - INFO - training batch 351, loss: 1.033, 11232/60000 datapoints
2025-03-06 18:46:03,570 - INFO - training batch 401, loss: 0.860, 12832/60000 datapoints
2025-03-06 18:46:03,768 - INFO - training batch 451, loss: 0.719, 14432/60000 datapoints
2025-03-06 18:46:03,960 - INFO - training batch 501, loss: 0.825, 16032/60000 datapoints
2025-03-06 18:46:04,153 - INFO - training batch 551, loss: 0.686, 17632/60000 datapoints
2025-03-06 18:46:04,352 - INFO - training batch 601, loss: 0.948, 19232/60000 datapoints
2025-03-06 18:46:04,549 - INFO - training batch 651, loss: 0.987, 20832/60000 datapoints
2025-03-06 18:46:04,749 - INFO - training batch 701, loss: 0.777, 22432/60000 datapoints
2025-03-06 18:46:04,949 - INFO - training batch 751, loss: 0.788, 24032/60000 datapoints
2025-03-06 18:46:05,143 - INFO - training batch 801, loss: 0.869, 25632/60000 datapoints
2025-03-06 18:46:05,336 - INFO - training batch 851, loss: 0.654, 27232/60000 datapoints
2025-03-06 18:46:05,535 - INFO - training batch 901, loss: 0.676, 28832/60000 datapoints
2025-03-06 18:46:05,730 - INFO - training batch 951, loss: 0.829, 30432/60000 datapoints
2025-03-06 18:46:05,925 - INFO - training batch 1001, loss: 1.018, 32032/60000 datapoints
2025-03-06 18:46:06,119 - INFO - training batch 1051, loss: 0.765, 33632/60000 datapoints
2025-03-06 18:46:06,315 - INFO - training batch 1101, loss: 0.838, 35232/60000 datapoints
2025-03-06 18:46:06,508 - INFO - training batch 1151, loss: 0.918, 36832/60000 datapoints
2025-03-06 18:46:06,708 - INFO - training batch 1201, loss: 0.927, 38432/60000 datapoints
2025-03-06 18:46:06,901 - INFO - training batch 1251, loss: 0.718, 40032/60000 datapoints
2025-03-06 18:46:07,095 - INFO - training batch 1301, loss: 0.867, 41632/60000 datapoints
2025-03-06 18:46:07,290 - INFO - training batch 1351, loss: 0.920, 43232/60000 datapoints
2025-03-06 18:46:07,485 - INFO - training batch 1401, loss: 0.841, 44832/60000 datapoints
2025-03-06 18:46:07,683 - INFO - training batch 1451, loss: 0.745, 46432/60000 datapoints
2025-03-06 18:46:07,875 - INFO - training batch 1501, loss: 0.899, 48032/60000 datapoints
2025-03-06 18:46:08,069 - INFO - training batch 1551, loss: 0.858, 49632/60000 datapoints
2025-03-06 18:46:08,265 - INFO - training batch 1601, loss: 0.862, 51232/60000 datapoints
2025-03-06 18:46:08,466 - INFO - training batch 1651, loss: 1.067, 52832/60000 datapoints
2025-03-06 18:46:08,664 - INFO - training batch 1701, loss: 0.957, 54432/60000 datapoints
2025-03-06 18:46:08,857 - INFO - training batch 1751, loss: 0.775, 56032/60000 datapoints
2025-03-06 18:46:09,053 - INFO - training batch 1801, loss: 0.761, 57632/60000 datapoints
2025-03-06 18:46:09,249 - INFO - training batch 1851, loss: 0.899, 59232/60000 datapoints
2025-03-06 18:46:09,350 - INFO - validation batch 1, loss: 0.703, 32/10016 datapoints
2025-03-06 18:46:09,502 - INFO - validation batch 51, loss: 0.798, 1632/10016 datapoints
2025-03-06 18:46:09,658 - INFO - validation batch 101, loss: 1.039, 3232/10016 datapoints
2025-03-06 18:46:09,814 - INFO - validation batch 151, loss: 0.920, 4832/10016 datapoints
2025-03-06 18:46:09,968 - INFO - validation batch 201, loss: 0.705, 6432/10016 datapoints
2025-03-06 18:46:10,122 - INFO - validation batch 251, loss: 0.867, 8032/10016 datapoints
2025-03-06 18:46:10,278 - INFO - validation batch 301, loss: 0.734, 9632/10016 datapoints
2025-03-06 18:46:10,315 - INFO - Epoch 58/800 done.
2025-03-06 18:46:10,315 - INFO - Final validation performance:
Loss: 0.824, top-1 acc: 0.818top-5 acc: 0.818
2025-03-06 18:46:10,316 - INFO - Beginning epoch 59/800
2025-03-06 18:46:10,322 - INFO - training batch 1, loss: 0.764, 32/60000 datapoints
2025-03-06 18:46:10,516 - INFO - training batch 51, loss: 0.703, 1632/60000 datapoints
2025-03-06 18:46:10,715 - INFO - training batch 101, loss: 0.723, 3232/60000 datapoints
2025-03-06 18:46:10,918 - INFO - training batch 151, loss: 0.917, 4832/60000 datapoints
2025-03-06 18:46:11,114 - INFO - training batch 201, loss: 0.880, 6432/60000 datapoints
2025-03-06 18:46:11,328 - INFO - training batch 251, loss: 0.833, 8032/60000 datapoints
2025-03-06 18:46:11,524 - INFO - training batch 301, loss: 0.561, 9632/60000 datapoints
2025-03-06 18:46:11,723 - INFO - training batch 351, loss: 0.709, 11232/60000 datapoints
2025-03-06 18:46:11,920 - INFO - training batch 401, loss: 0.732, 12832/60000 datapoints
2025-03-06 18:46:12,123 - INFO - training batch 451, loss: 0.657, 14432/60000 datapoints
2025-03-06 18:46:12,317 - INFO - training batch 501, loss: 0.678, 16032/60000 datapoints
2025-03-06 18:46:12,520 - INFO - training batch 551, loss: 1.002, 17632/60000 datapoints
2025-03-06 18:46:12,722 - INFO - training batch 601, loss: 0.894, 19232/60000 datapoints
2025-03-06 18:46:12,914 - INFO - training batch 651, loss: 0.696, 20832/60000 datapoints
2025-03-06 18:46:13,110 - INFO - training batch 701, loss: 0.963, 22432/60000 datapoints
2025-03-06 18:46:13,306 - INFO - training batch 751, loss: 0.721, 24032/60000 datapoints
2025-03-06 18:46:13,499 - INFO - training batch 801, loss: 0.943, 25632/60000 datapoints
2025-03-06 18:46:13,700 - INFO - training batch 851, loss: 0.833, 27232/60000 datapoints
2025-03-06 18:46:13,897 - INFO - training batch 901, loss: 0.707, 28832/60000 datapoints
2025-03-06 18:46:14,090 - INFO - training batch 951, loss: 0.960, 30432/60000 datapoints
2025-03-06 18:46:14,283 - INFO - training batch 1001, loss: 0.792, 32032/60000 datapoints
2025-03-06 18:46:14,477 - INFO - training batch 1051, loss: 0.827, 33632/60000 datapoints
2025-03-06 18:46:14,673 - INFO - training batch 1101, loss: 0.724, 35232/60000 datapoints
2025-03-06 18:46:14,866 - INFO - training batch 1151, loss: 0.680, 36832/60000 datapoints
2025-03-06 18:46:15,064 - INFO - training batch 1201, loss: 0.835, 38432/60000 datapoints
2025-03-06 18:46:15,259 - INFO - training batch 1251, loss: 0.862, 40032/60000 datapoints
2025-03-06 18:46:15,457 - INFO - training batch 1301, loss: 0.855, 41632/60000 datapoints
2025-03-06 18:46:15,654 - INFO - training batch 1351, loss: 0.787, 43232/60000 datapoints
2025-03-06 18:46:15,850 - INFO - training batch 1401, loss: 0.784, 44832/60000 datapoints
2025-03-06 18:46:16,045 - INFO - training batch 1451, loss: 0.902, 46432/60000 datapoints
2025-03-06 18:46:16,242 - INFO - training batch 1501, loss: 0.789, 48032/60000 datapoints
2025-03-06 18:46:16,437 - INFO - training batch 1551, loss: 0.890, 49632/60000 datapoints
2025-03-06 18:46:16,633 - INFO - training batch 1601, loss: 0.685, 51232/60000 datapoints
2025-03-06 18:46:16,831 - INFO - training batch 1651, loss: 0.754, 52832/60000 datapoints
2025-03-06 18:46:17,024 - INFO - training batch 1701, loss: 0.684, 54432/60000 datapoints
2025-03-06 18:46:17,218 - INFO - training batch 1751, loss: 0.693, 56032/60000 datapoints
2025-03-06 18:46:17,412 - INFO - training batch 1801, loss: 0.858, 57632/60000 datapoints
2025-03-06 18:46:17,608 - INFO - training batch 1851, loss: 0.778, 59232/60000 datapoints
2025-03-06 18:46:17,709 - INFO - validation batch 1, loss: 0.851, 32/10016 datapoints
2025-03-06 18:46:17,864 - INFO - validation batch 51, loss: 0.734, 1632/10016 datapoints
2025-03-06 18:46:18,016 - INFO - validation batch 101, loss: 0.728, 3232/10016 datapoints
2025-03-06 18:46:18,171 - INFO - validation batch 151, loss: 0.715, 4832/10016 datapoints
2025-03-06 18:46:18,324 - INFO - validation batch 201, loss: 0.634, 6432/10016 datapoints
2025-03-06 18:46:18,477 - INFO - validation batch 251, loss: 0.725, 8032/10016 datapoints
2025-03-06 18:46:18,631 - INFO - validation batch 301, loss: 1.012, 9632/10016 datapoints
2025-03-06 18:46:18,671 - INFO - Epoch 59/800 done.
2025-03-06 18:46:18,671 - INFO - Final validation performance:
Loss: 0.771, top-1 acc: 0.821top-5 acc: 0.821
2025-03-06 18:46:18,671 - INFO - Beginning epoch 60/800
2025-03-06 18:46:18,678 - INFO - training batch 1, loss: 1.028, 32/60000 datapoints
2025-03-06 18:46:18,873 - INFO - training batch 51, loss: 0.675, 1632/60000 datapoints
2025-03-06 18:46:19,066 - INFO - training batch 101, loss: 0.781, 3232/60000 datapoints
2025-03-06 18:46:19,269 - INFO - training batch 151, loss: 0.744, 4832/60000 datapoints
2025-03-06 18:46:19,465 - INFO - training batch 201, loss: 0.655, 6432/60000 datapoints
2025-03-06 18:46:19,668 - INFO - training batch 251, loss: 0.601, 8032/60000 datapoints
2025-03-06 18:46:19,866 - INFO - training batch 301, loss: 0.647, 9632/60000 datapoints
2025-03-06 18:46:20,063 - INFO - training batch 351, loss: 0.785, 11232/60000 datapoints
2025-03-06 18:46:20,261 - INFO - training batch 401, loss: 0.870, 12832/60000 datapoints
2025-03-06 18:46:20,454 - INFO - training batch 451, loss: 0.715, 14432/60000 datapoints
2025-03-06 18:46:20,649 - INFO - training batch 501, loss: 0.928, 16032/60000 datapoints
2025-03-06 18:46:20,844 - INFO - training batch 551, loss: 0.809, 17632/60000 datapoints
2025-03-06 18:46:21,037 - INFO - training batch 601, loss: 0.751, 19232/60000 datapoints
2025-03-06 18:46:21,234 - INFO - training batch 651, loss: 0.781, 20832/60000 datapoints
2025-03-06 18:46:21,454 - INFO - training batch 701, loss: 0.974, 22432/60000 datapoints
2025-03-06 18:46:21,656 - INFO - training batch 751, loss: 0.760, 24032/60000 datapoints
2025-03-06 18:46:21,855 - INFO - training batch 801, loss: 0.913, 25632/60000 datapoints
2025-03-06 18:46:22,054 - INFO - training batch 851, loss: 0.873, 27232/60000 datapoints
2025-03-06 18:46:22,247 - INFO - training batch 901, loss: 0.906, 28832/60000 datapoints
2025-03-06 18:46:22,439 - INFO - training batch 951, loss: 0.835, 30432/60000 datapoints
2025-03-06 18:46:22,641 - INFO - training batch 1001, loss: 0.840, 32032/60000 datapoints
2025-03-06 18:46:22,836 - INFO - training batch 1051, loss: 0.681, 33632/60000 datapoints
2025-03-06 18:46:23,027 - INFO - training batch 1101, loss: 0.685, 35232/60000 datapoints
2025-03-06 18:46:23,223 - INFO - training batch 1151, loss: 0.993, 36832/60000 datapoints
2025-03-06 18:46:23,417 - INFO - training batch 1201, loss: 0.649, 38432/60000 datapoints
2025-03-06 18:46:23,614 - INFO - training batch 1251, loss: 0.778, 40032/60000 datapoints
2025-03-06 18:46:23,809 - INFO - training batch 1301, loss: 0.879, 41632/60000 datapoints
2025-03-06 18:46:24,004 - INFO - training batch 1351, loss: 0.731, 43232/60000 datapoints
2025-03-06 18:46:24,198 - INFO - training batch 1401, loss: 0.766, 44832/60000 datapoints
2025-03-06 18:46:24,393 - INFO - training batch 1451, loss: 0.912, 46432/60000 datapoints
2025-03-06 18:46:24,587 - INFO - training batch 1501, loss: 0.645, 48032/60000 datapoints
2025-03-06 18:46:24,784 - INFO - training batch 1551, loss: 0.834, 49632/60000 datapoints
2025-03-06 18:46:24,986 - INFO - training batch 1601, loss: 0.713, 51232/60000 datapoints
2025-03-06 18:46:25,178 - INFO - training batch 1651, loss: 0.772, 52832/60000 datapoints
2025-03-06 18:46:25,374 - INFO - training batch 1701, loss: 0.638, 54432/60000 datapoints
2025-03-06 18:46:25,572 - INFO - training batch 1751, loss: 0.819, 56032/60000 datapoints
2025-03-06 18:46:25,769 - INFO - training batch 1801, loss: 0.823, 57632/60000 datapoints
2025-03-06 18:46:25,966 - INFO - training batch 1851, loss: 0.957, 59232/60000 datapoints
2025-03-06 18:46:26,068 - INFO - validation batch 1, loss: 0.749, 32/10016 datapoints
2025-03-06 18:46:26,223 - INFO - validation batch 51, loss: 0.873, 1632/10016 datapoints
2025-03-06 18:46:26,378 - INFO - validation batch 101, loss: 0.839, 3232/10016 datapoints
2025-03-06 18:46:26,531 - INFO - validation batch 151, loss: 0.885, 4832/10016 datapoints
2025-03-06 18:46:26,687 - INFO - validation batch 201, loss: 0.679, 6432/10016 datapoints
2025-03-06 18:46:26,839 - INFO - validation batch 251, loss: 0.824, 8032/10016 datapoints
2025-03-06 18:46:26,992 - INFO - validation batch 301, loss: 0.607, 9632/10016 datapoints
2025-03-06 18:46:27,029 - INFO - Epoch 60/800 done.
2025-03-06 18:46:27,029 - INFO - Final validation performance:
Loss: 0.779, top-1 acc: 0.823top-5 acc: 0.823
2025-03-06 18:46:27,030 - INFO - Beginning epoch 61/800
2025-03-06 18:46:27,036 - INFO - training batch 1, loss: 0.693, 32/60000 datapoints
2025-03-06 18:46:27,240 - INFO - training batch 51, loss: 0.683, 1632/60000 datapoints
2025-03-06 18:46:27,437 - INFO - training batch 101, loss: 0.890, 3232/60000 datapoints
2025-03-06 18:46:27,641 - INFO - training batch 151, loss: 0.692, 4832/60000 datapoints
2025-03-06 18:46:27,835 - INFO - training batch 201, loss: 0.562, 6432/60000 datapoints
2025-03-06 18:46:28,035 - INFO - training batch 251, loss: 0.668, 8032/60000 datapoints
2025-03-06 18:46:28,241 - INFO - training batch 301, loss: 0.666, 9632/60000 datapoints
2025-03-06 18:46:28,440 - INFO - training batch 351, loss: 0.780, 11232/60000 datapoints
2025-03-06 18:46:28,644 - INFO - training batch 401, loss: 0.795, 12832/60000 datapoints
2025-03-06 18:46:28,843 - INFO - training batch 451, loss: 0.901, 14432/60000 datapoints
2025-03-06 18:46:29,034 - INFO - training batch 501, loss: 0.825, 16032/60000 datapoints
2025-03-06 18:46:29,228 - INFO - training batch 551, loss: 0.813, 17632/60000 datapoints
2025-03-06 18:46:29,423 - INFO - training batch 601, loss: 0.666, 19232/60000 datapoints
2025-03-06 18:46:29,616 - INFO - training batch 651, loss: 0.558, 20832/60000 datapoints
2025-03-06 18:46:29,811 - INFO - training batch 701, loss: 0.775, 22432/60000 datapoints
2025-03-06 18:46:30,008 - INFO - training batch 751, loss: 0.763, 24032/60000 datapoints
2025-03-06 18:46:30,202 - INFO - training batch 801, loss: 0.756, 25632/60000 datapoints
2025-03-06 18:46:30,397 - INFO - training batch 851, loss: 0.872, 27232/60000 datapoints
2025-03-06 18:46:30,591 - INFO - training batch 901, loss: 0.845, 28832/60000 datapoints
2025-03-06 18:46:30,808 - INFO - training batch 951, loss: 0.959, 30432/60000 datapoints
2025-03-06 18:46:31,003 - INFO - training batch 1001, loss: 0.834, 32032/60000 datapoints
2025-03-06 18:46:31,196 - INFO - training batch 1051, loss: 0.747, 33632/60000 datapoints
2025-03-06 18:46:31,406 - INFO - training batch 1101, loss: 0.890, 35232/60000 datapoints
2025-03-06 18:46:31,606 - INFO - training batch 1151, loss: 0.736, 36832/60000 datapoints
2025-03-06 18:46:31,800 - INFO - training batch 1201, loss: 0.534, 38432/60000 datapoints
2025-03-06 18:46:31,993 - INFO - training batch 1251, loss: 0.759, 40032/60000 datapoints
2025-03-06 18:46:32,187 - INFO - training batch 1301, loss: 1.004, 41632/60000 datapoints
2025-03-06 18:46:32,381 - INFO - training batch 1351, loss: 0.830, 43232/60000 datapoints
2025-03-06 18:46:32,570 - INFO - training batch 1401, loss: 0.638, 44832/60000 datapoints
2025-03-06 18:46:32,768 - INFO - training batch 1451, loss: 0.662, 46432/60000 datapoints
2025-03-06 18:46:32,961 - INFO - training batch 1501, loss: 0.834, 48032/60000 datapoints
2025-03-06 18:46:33,155 - INFO - training batch 1551, loss: 0.716, 49632/60000 datapoints
2025-03-06 18:46:33,348 - INFO - training batch 1601, loss: 1.036, 51232/60000 datapoints
2025-03-06 18:46:33,543 - INFO - training batch 1651, loss: 0.982, 52832/60000 datapoints
2025-03-06 18:46:33,748 - INFO - training batch 1701, loss: 0.642, 54432/60000 datapoints
2025-03-06 18:46:33,944 - INFO - training batch 1751, loss: 0.702, 56032/60000 datapoints
2025-03-06 18:46:34,138 - INFO - training batch 1801, loss: 0.704, 57632/60000 datapoints
2025-03-06 18:46:34,329 - INFO - training batch 1851, loss: 0.679, 59232/60000 datapoints
2025-03-06 18:46:34,430 - INFO - validation batch 1, loss: 0.697, 32/10016 datapoints
2025-03-06 18:46:34,581 - INFO - validation batch 51, loss: 0.934, 1632/10016 datapoints
2025-03-06 18:46:34,741 - INFO - validation batch 101, loss: 0.550, 3232/10016 datapoints
2025-03-06 18:46:34,907 - INFO - validation batch 151, loss: 0.846, 4832/10016 datapoints
2025-03-06 18:46:35,061 - INFO - validation batch 201, loss: 0.635, 6432/10016 datapoints
2025-03-06 18:46:35,213 - INFO - validation batch 251, loss: 0.786, 8032/10016 datapoints
2025-03-06 18:46:35,366 - INFO - validation batch 301, loss: 1.053, 9632/10016 datapoints
2025-03-06 18:46:35,404 - INFO - Epoch 61/800 done.
2025-03-06 18:46:35,404 - INFO - Final validation performance:
Loss: 0.786, top-1 acc: 0.825top-5 acc: 0.825
2025-03-06 18:46:35,404 - INFO - Beginning epoch 62/800
2025-03-06 18:46:35,411 - INFO - training batch 1, loss: 0.957, 32/60000 datapoints
2025-03-06 18:46:35,626 - INFO - training batch 51, loss: 0.758, 1632/60000 datapoints
2025-03-06 18:46:35,820 - INFO - training batch 101, loss: 1.011, 3232/60000 datapoints
2025-03-06 18:46:36,025 - INFO - training batch 151, loss: 0.717, 4832/60000 datapoints
2025-03-06 18:46:36,224 - INFO - training batch 201, loss: 0.667, 6432/60000 datapoints
2025-03-06 18:46:36,419 - INFO - training batch 251, loss: 0.794, 8032/60000 datapoints
2025-03-06 18:46:36,616 - INFO - training batch 301, loss: 1.048, 9632/60000 datapoints
2025-03-06 18:46:36,812 - INFO - training batch 351, loss: 0.970, 11232/60000 datapoints
2025-03-06 18:46:37,008 - INFO - training batch 401, loss: 0.710, 12832/60000 datapoints
2025-03-06 18:46:37,204 - INFO - training batch 451, loss: 0.804, 14432/60000 datapoints
2025-03-06 18:46:37,398 - INFO - training batch 501, loss: 0.961, 16032/60000 datapoints
2025-03-06 18:46:37,591 - INFO - training batch 551, loss: 0.633, 17632/60000 datapoints
2025-03-06 18:46:37,798 - INFO - training batch 601, loss: 0.701, 19232/60000 datapoints
2025-03-06 18:46:37,993 - INFO - training batch 651, loss: 0.786, 20832/60000 datapoints
2025-03-06 18:46:38,187 - INFO - training batch 701, loss: 0.668, 22432/60000 datapoints
2025-03-06 18:46:38,379 - INFO - training batch 751, loss: 0.702, 24032/60000 datapoints
2025-03-06 18:46:38,571 - INFO - training batch 801, loss: 0.834, 25632/60000 datapoints
2025-03-06 18:46:38,773 - INFO - training batch 851, loss: 0.812, 27232/60000 datapoints
2025-03-06 18:46:38,970 - INFO - training batch 901, loss: 0.693, 28832/60000 datapoints
2025-03-06 18:46:39,174 - INFO - training batch 951, loss: 0.694, 30432/60000 datapoints
2025-03-06 18:46:39,384 - INFO - training batch 1001, loss: 0.779, 32032/60000 datapoints
2025-03-06 18:46:39,580 - INFO - training batch 1051, loss: 0.900, 33632/60000 datapoints
2025-03-06 18:46:39,777 - INFO - training batch 1101, loss: 0.846, 35232/60000 datapoints
2025-03-06 18:46:39,972 - INFO - training batch 1151, loss: 0.633, 36832/60000 datapoints
2025-03-06 18:46:40,166 - INFO - training batch 1201, loss: 0.773, 38432/60000 datapoints
2025-03-06 18:46:40,359 - INFO - training batch 1251, loss: 0.956, 40032/60000 datapoints
2025-03-06 18:46:40,555 - INFO - training batch 1301, loss: 0.768, 41632/60000 datapoints
2025-03-06 18:46:40,758 - INFO - training batch 1351, loss: 0.650, 43232/60000 datapoints
2025-03-06 18:46:40,951 - INFO - training batch 1401, loss: 0.666, 44832/60000 datapoints
2025-03-06 18:46:41,146 - INFO - training batch 1451, loss: 0.895, 46432/60000 datapoints
2025-03-06 18:46:41,340 - INFO - training batch 1501, loss: 0.636, 48032/60000 datapoints
2025-03-06 18:46:41,555 - INFO - training batch 1551, loss: 0.558, 49632/60000 datapoints
2025-03-06 18:46:41,750 - INFO - training batch 1601, loss: 0.832, 51232/60000 datapoints
2025-03-06 18:46:41,943 - INFO - training batch 1651, loss: 0.975, 52832/60000 datapoints
2025-03-06 18:46:42,141 - INFO - training batch 1701, loss: 0.792, 54432/60000 datapoints
2025-03-06 18:46:42,337 - INFO - training batch 1751, loss: 0.874, 56032/60000 datapoints
2025-03-06 18:46:42,532 - INFO - training batch 1801, loss: 0.984, 57632/60000 datapoints
2025-03-06 18:46:42,726 - INFO - training batch 1851, loss: 0.736, 59232/60000 datapoints
2025-03-06 18:46:42,829 - INFO - validation batch 1, loss: 0.888, 32/10016 datapoints
2025-03-06 18:46:42,980 - INFO - validation batch 51, loss: 0.826, 1632/10016 datapoints
2025-03-06 18:46:43,133 - INFO - validation batch 101, loss: 0.646, 3232/10016 datapoints
2025-03-06 18:46:43,287 - INFO - validation batch 151, loss: 0.756, 4832/10016 datapoints
2025-03-06 18:46:43,442 - INFO - validation batch 201, loss: 0.646, 6432/10016 datapoints
2025-03-06 18:46:43,598 - INFO - validation batch 251, loss: 0.796, 8032/10016 datapoints
2025-03-06 18:46:43,752 - INFO - validation batch 301, loss: 0.831, 9632/10016 datapoints
2025-03-06 18:46:43,789 - INFO - Epoch 62/800 done.
2025-03-06 18:46:43,789 - INFO - Final validation performance:
Loss: 0.770, top-1 acc: 0.827top-5 acc: 0.827
2025-03-06 18:46:43,790 - INFO - Beginning epoch 63/800
2025-03-06 18:46:43,797 - INFO - training batch 1, loss: 0.756, 32/60000 datapoints
2025-03-06 18:46:43,995 - INFO - training batch 51, loss: 0.919, 1632/60000 datapoints
2025-03-06 18:46:44,203 - INFO - training batch 101, loss: 0.710, 3232/60000 datapoints
2025-03-06 18:46:44,397 - INFO - training batch 151, loss: 0.948, 4832/60000 datapoints
2025-03-06 18:46:44,593 - INFO - training batch 201, loss: 0.664, 6432/60000 datapoints
2025-03-06 18:46:44,797 - INFO - training batch 251, loss: 0.697, 8032/60000 datapoints
2025-03-06 18:46:44,997 - INFO - training batch 301, loss: 0.866, 9632/60000 datapoints
2025-03-06 18:46:45,190 - INFO - training batch 351, loss: 0.916, 11232/60000 datapoints
2025-03-06 18:46:45,386 - INFO - training batch 401, loss: 0.720, 12832/60000 datapoints
2025-03-06 18:46:45,580 - INFO - training batch 451, loss: 0.797, 14432/60000 datapoints
2025-03-06 18:46:45,777 - INFO - training batch 501, loss: 0.738, 16032/60000 datapoints
2025-03-06 18:46:45,968 - INFO - training batch 551, loss: 0.896, 17632/60000 datapoints
2025-03-06 18:46:46,163 - INFO - training batch 601, loss: 0.689, 19232/60000 datapoints
2025-03-06 18:46:46,359 - INFO - training batch 651, loss: 0.755, 20832/60000 datapoints
2025-03-06 18:46:46,554 - INFO - training batch 701, loss: 0.818, 22432/60000 datapoints
2025-03-06 18:46:46,763 - INFO - training batch 751, loss: 0.746, 24032/60000 datapoints
2025-03-06 18:46:46,966 - INFO - training batch 801, loss: 0.769, 25632/60000 datapoints
2025-03-06 18:46:47,162 - INFO - training batch 851, loss: 0.697, 27232/60000 datapoints
2025-03-06 18:46:47,355 - INFO - training batch 901, loss: 0.751, 28832/60000 datapoints
2025-03-06 18:46:47,548 - INFO - training batch 951, loss: 1.019, 30432/60000 datapoints
2025-03-06 18:46:47,743 - INFO - training batch 1001, loss: 0.838, 32032/60000 datapoints
2025-03-06 18:46:47,938 - INFO - training batch 1051, loss: 0.819, 33632/60000 datapoints
2025-03-06 18:46:48,134 - INFO - training batch 1101, loss: 0.809, 35232/60000 datapoints
2025-03-06 18:46:48,330 - INFO - training batch 1151, loss: 0.790, 36832/60000 datapoints
2025-03-06 18:46:48,524 - INFO - training batch 1201, loss: 0.756, 38432/60000 datapoints
2025-03-06 18:46:48,720 - INFO - training batch 1251, loss: 0.846, 40032/60000 datapoints
2025-03-06 18:46:48,916 - INFO - training batch 1301, loss: 0.762, 41632/60000 datapoints
2025-03-06 18:46:49,110 - INFO - training batch 1351, loss: 1.039, 43232/60000 datapoints
2025-03-06 18:46:49,306 - INFO - training batch 1401, loss: 0.727, 44832/60000 datapoints
2025-03-06 18:46:49,501 - INFO - training batch 1451, loss: 0.822, 46432/60000 datapoints
2025-03-06 18:46:49,699 - INFO - training batch 1501, loss: 0.845, 48032/60000 datapoints
2025-03-06 18:46:49,893 - INFO - training batch 1551, loss: 0.763, 49632/60000 datapoints
2025-03-06 18:46:50,090 - INFO - training batch 1601, loss: 0.801, 51232/60000 datapoints
2025-03-06 18:46:50,289 - INFO - training batch 1651, loss: 0.926, 52832/60000 datapoints
2025-03-06 18:46:50,482 - INFO - training batch 1701, loss: 0.755, 54432/60000 datapoints
2025-03-06 18:46:50,680 - INFO - training batch 1751, loss: 0.822, 56032/60000 datapoints
2025-03-06 18:46:50,891 - INFO - training batch 1801, loss: 0.855, 57632/60000 datapoints
2025-03-06 18:46:51,085 - INFO - training batch 1851, loss: 0.617, 59232/60000 datapoints
2025-03-06 18:46:51,185 - INFO - validation batch 1, loss: 0.724, 32/10016 datapoints
2025-03-06 18:46:51,339 - INFO - validation batch 51, loss: 0.654, 1632/10016 datapoints
2025-03-06 18:46:51,491 - INFO - validation batch 101, loss: 0.760, 3232/10016 datapoints
2025-03-06 18:46:51,668 - INFO - validation batch 151, loss: 0.816, 4832/10016 datapoints
2025-03-06 18:46:51,824 - INFO - validation batch 201, loss: 0.616, 6432/10016 datapoints
2025-03-06 18:46:51,975 - INFO - validation batch 251, loss: 0.759, 8032/10016 datapoints
2025-03-06 18:46:52,131 - INFO - validation batch 301, loss: 0.500, 9632/10016 datapoints
2025-03-06 18:46:52,169 - INFO - Epoch 63/800 done.
2025-03-06 18:46:52,169 - INFO - Final validation performance:
Loss: 0.690, top-1 acc: 0.829top-5 acc: 0.829
2025-03-06 18:46:52,170 - INFO - Beginning epoch 64/800
2025-03-06 18:46:52,176 - INFO - training batch 1, loss: 0.927, 32/60000 datapoints
2025-03-06 18:46:52,376 - INFO - training batch 51, loss: 0.984, 1632/60000 datapoints
2025-03-06 18:46:52,570 - INFO - training batch 101, loss: 0.753, 3232/60000 datapoints
2025-03-06 18:46:52,783 - INFO - training batch 151, loss: 0.685, 4832/60000 datapoints
2025-03-06 18:46:52,978 - INFO - training batch 201, loss: 0.936, 6432/60000 datapoints
2025-03-06 18:46:53,173 - INFO - training batch 251, loss: 1.028, 8032/60000 datapoints
2025-03-06 18:46:53,366 - INFO - training batch 301, loss: 0.720, 9632/60000 datapoints
2025-03-06 18:46:53,568 - INFO - training batch 351, loss: 0.807, 11232/60000 datapoints
2025-03-06 18:46:53,773 - INFO - training batch 401, loss: 0.832, 12832/60000 datapoints
2025-03-06 18:46:53,968 - INFO - training batch 451, loss: 0.681, 14432/60000 datapoints
2025-03-06 18:46:54,164 - INFO - training batch 501, loss: 0.777, 16032/60000 datapoints
2025-03-06 18:46:54,359 - INFO - training batch 551, loss: 0.768, 17632/60000 datapoints
2025-03-06 18:46:54,554 - INFO - training batch 601, loss: 0.487, 19232/60000 datapoints
2025-03-06 18:46:54,750 - INFO - training batch 651, loss: 0.763, 20832/60000 datapoints
2025-03-06 18:46:54,951 - INFO - training batch 701, loss: 0.758, 22432/60000 datapoints
2025-03-06 18:46:55,147 - INFO - training batch 751, loss: 1.046, 24032/60000 datapoints
2025-03-06 18:46:55,340 - INFO - training batch 801, loss: 1.150, 25632/60000 datapoints
2025-03-06 18:46:55,541 - INFO - training batch 851, loss: 0.651, 27232/60000 datapoints
2025-03-06 18:46:55,737 - INFO - training batch 901, loss: 0.806, 28832/60000 datapoints
2025-03-06 18:46:55,931 - INFO - training batch 951, loss: 0.853, 30432/60000 datapoints
2025-03-06 18:46:56,126 - INFO - training batch 1001, loss: 0.599, 32032/60000 datapoints
2025-03-06 18:46:56,324 - INFO - training batch 1051, loss: 0.835, 33632/60000 datapoints
2025-03-06 18:46:56,526 - INFO - training batch 1101, loss: 0.681, 35232/60000 datapoints
2025-03-06 18:46:56,723 - INFO - training batch 1151, loss: 0.680, 36832/60000 datapoints
2025-03-06 18:46:56,918 - INFO - training batch 1201, loss: 0.755, 38432/60000 datapoints
2025-03-06 18:46:57,112 - INFO - training batch 1251, loss: 0.682, 40032/60000 datapoints
2025-03-06 18:46:57,307 - INFO - training batch 1301, loss: 0.779, 41632/60000 datapoints
2025-03-06 18:46:57,500 - INFO - training batch 1351, loss: 0.760, 43232/60000 datapoints
2025-03-06 18:46:57,696 - INFO - training batch 1401, loss: 0.845, 44832/60000 datapoints
2025-03-06 18:46:57,894 - INFO - training batch 1451, loss: 0.569, 46432/60000 datapoints
2025-03-06 18:46:58,089 - INFO - training batch 1501, loss: 0.727, 48032/60000 datapoints
2025-03-06 18:46:58,285 - INFO - training batch 1551, loss: 0.659, 49632/60000 datapoints
2025-03-06 18:46:58,480 - INFO - training batch 1601, loss: 0.712, 51232/60000 datapoints
2025-03-06 18:46:58,683 - INFO - training batch 1651, loss: 0.927, 52832/60000 datapoints
2025-03-06 18:46:58,880 - INFO - training batch 1701, loss: 0.677, 54432/60000 datapoints
2025-03-06 18:46:59,073 - INFO - training batch 1751, loss: 0.859, 56032/60000 datapoints
2025-03-06 18:46:59,271 - INFO - training batch 1801, loss: 0.612, 57632/60000 datapoints
2025-03-06 18:46:59,469 - INFO - training batch 1851, loss: 0.742, 59232/60000 datapoints
2025-03-06 18:46:59,568 - INFO - validation batch 1, loss: 0.734, 32/10016 datapoints
2025-03-06 18:46:59,722 - INFO - validation batch 51, loss: 0.746, 1632/10016 datapoints
2025-03-06 18:46:59,872 - INFO - validation batch 101, loss: 0.664, 3232/10016 datapoints
2025-03-06 18:47:00,026 - INFO - validation batch 151, loss: 0.861, 4832/10016 datapoints
2025-03-06 18:47:00,177 - INFO - validation batch 201, loss: 0.684, 6432/10016 datapoints
2025-03-06 18:47:00,334 - INFO - validation batch 251, loss: 0.671, 8032/10016 datapoints
2025-03-06 18:47:00,486 - INFO - validation batch 301, loss: 0.696, 9632/10016 datapoints
2025-03-06 18:47:00,523 - INFO - Epoch 64/800 done.
2025-03-06 18:47:00,523 - INFO - Final validation performance:
Loss: 0.722, top-1 acc: 0.831top-5 acc: 0.831
2025-03-06 18:47:00,523 - INFO - Beginning epoch 65/800
2025-03-06 18:47:00,530 - INFO - training batch 1, loss: 0.642, 32/60000 datapoints
2025-03-06 18:47:00,733 - INFO - training batch 51, loss: 0.782, 1632/60000 datapoints
2025-03-06 18:47:00,940 - INFO - training batch 101, loss: 0.781, 3232/60000 datapoints
2025-03-06 18:47:01,136 - INFO - training batch 151, loss: 0.601, 4832/60000 datapoints
2025-03-06 18:47:01,332 - INFO - training batch 201, loss: 0.591, 6432/60000 datapoints
2025-03-06 18:47:01,534 - INFO - training batch 251, loss: 0.777, 8032/60000 datapoints
2025-03-06 18:47:01,754 - INFO - training batch 301, loss: 0.841, 9632/60000 datapoints
2025-03-06 18:47:01,951 - INFO - training batch 351, loss: 0.741, 11232/60000 datapoints
2025-03-06 18:47:02,146 - INFO - training batch 401, loss: 0.807, 12832/60000 datapoints
2025-03-06 18:47:02,351 - INFO - training batch 451, loss: 0.680, 14432/60000 datapoints
2025-03-06 18:47:02,559 - INFO - training batch 501, loss: 0.812, 16032/60000 datapoints
2025-03-06 18:47:02,755 - INFO - training batch 551, loss: 0.786, 17632/60000 datapoints
2025-03-06 18:47:02,948 - INFO - training batch 601, loss: 0.844, 19232/60000 datapoints
2025-03-06 18:47:03,141 - INFO - training batch 651, loss: 0.620, 20832/60000 datapoints
2025-03-06 18:47:03,339 - INFO - training batch 701, loss: 0.695, 22432/60000 datapoints
2025-03-06 18:47:03,531 - INFO - training batch 751, loss: 0.823, 24032/60000 datapoints
2025-03-06 18:47:03,730 - INFO - training batch 801, loss: 0.616, 25632/60000 datapoints
2025-03-06 18:47:03,925 - INFO - training batch 851, loss: 0.718, 27232/60000 datapoints
2025-03-06 18:47:04,120 - INFO - training batch 901, loss: 0.596, 28832/60000 datapoints
2025-03-06 18:47:04,315 - INFO - training batch 951, loss: 0.747, 30432/60000 datapoints
2025-03-06 18:47:04,509 - INFO - training batch 1001, loss: 0.680, 32032/60000 datapoints
2025-03-06 18:47:04,706 - INFO - training batch 1051, loss: 0.906, 33632/60000 datapoints
2025-03-06 18:47:04,907 - INFO - training batch 1101, loss: 0.748, 35232/60000 datapoints
2025-03-06 18:47:05,104 - INFO - training batch 1151, loss: 0.719, 36832/60000 datapoints
2025-03-06 18:47:05,300 - INFO - training batch 1201, loss: 0.738, 38432/60000 datapoints
2025-03-06 18:47:05,495 - INFO - training batch 1251, loss: 0.781, 40032/60000 datapoints
2025-03-06 18:47:05,693 - INFO - training batch 1301, loss: 0.708, 41632/60000 datapoints
2025-03-06 18:47:05,890 - INFO - training batch 1351, loss: 0.603, 43232/60000 datapoints
2025-03-06 18:47:06,082 - INFO - training batch 1401, loss: 0.723, 44832/60000 datapoints
2025-03-06 18:47:06,281 - INFO - training batch 1451, loss: 0.885, 46432/60000 datapoints
2025-03-06 18:47:06,476 - INFO - training batch 1501, loss: 0.802, 48032/60000 datapoints
2025-03-06 18:47:06,672 - INFO - training batch 1551, loss: 0.658, 49632/60000 datapoints
2025-03-06 18:47:06,869 - INFO - training batch 1601, loss: 0.654, 51232/60000 datapoints
2025-03-06 18:47:07,063 - INFO - training batch 1651, loss: 0.645, 52832/60000 datapoints
2025-03-06 18:47:07,257 - INFO - training batch 1701, loss: 0.556, 54432/60000 datapoints
2025-03-06 18:47:07,452 - INFO - training batch 1751, loss: 0.846, 56032/60000 datapoints
2025-03-06 18:47:07,647 - INFO - training batch 1801, loss: 0.755, 57632/60000 datapoints
2025-03-06 18:47:07,841 - INFO - training batch 1851, loss: 0.836, 59232/60000 datapoints
2025-03-06 18:47:07,940 - INFO - validation batch 1, loss: 0.761, 32/10016 datapoints
2025-03-06 18:47:08,091 - INFO - validation batch 51, loss: 0.534, 1632/10016 datapoints
2025-03-06 18:47:08,241 - INFO - validation batch 101, loss: 0.762, 3232/10016 datapoints
2025-03-06 18:47:08,394 - INFO - validation batch 151, loss: 0.728, 4832/10016 datapoints
2025-03-06 18:47:08,547 - INFO - validation batch 201, loss: 0.635, 6432/10016 datapoints
2025-03-06 18:47:08,701 - INFO - validation batch 251, loss: 0.800, 8032/10016 datapoints
2025-03-06 18:47:08,857 - INFO - validation batch 301, loss: 0.829, 9632/10016 datapoints
2025-03-06 18:47:08,895 - INFO - Epoch 65/800 done.
2025-03-06 18:47:08,895 - INFO - Final validation performance:
Loss: 0.721, top-1 acc: 0.832top-5 acc: 0.832
2025-03-06 18:47:08,896 - INFO - Beginning epoch 66/800
2025-03-06 18:47:08,903 - INFO - training batch 1, loss: 0.788, 32/60000 datapoints
2025-03-06 18:47:09,100 - INFO - training batch 51, loss: 0.717, 1632/60000 datapoints
2025-03-06 18:47:09,290 - INFO - training batch 101, loss: 0.615, 3232/60000 datapoints
2025-03-06 18:47:09,502 - INFO - training batch 151, loss: 0.748, 4832/60000 datapoints
2025-03-06 18:47:09,698 - INFO - training batch 201, loss: 0.870, 6432/60000 datapoints
2025-03-06 18:47:09,891 - INFO - training batch 251, loss: 0.861, 8032/60000 datapoints
2025-03-06 18:47:10,090 - INFO - training batch 301, loss: 0.628, 9632/60000 datapoints
2025-03-06 18:47:10,285 - INFO - training batch 351, loss: 0.505, 11232/60000 datapoints
2025-03-06 18:47:10,481 - INFO - training batch 401, loss: 0.559, 12832/60000 datapoints
2025-03-06 18:47:10,678 - INFO - training batch 451, loss: 1.002, 14432/60000 datapoints
2025-03-06 18:47:10,872 - INFO - training batch 501, loss: 0.709, 16032/60000 datapoints
2025-03-06 18:47:11,062 - INFO - training batch 551, loss: 0.616, 17632/60000 datapoints
2025-03-06 18:47:11,254 - INFO - training batch 601, loss: 0.616, 19232/60000 datapoints
2025-03-06 18:47:11,446 - INFO - training batch 651, loss: 0.935, 20832/60000 datapoints
2025-03-06 18:47:11,641 - INFO - training batch 701, loss: 0.810, 22432/60000 datapoints
2025-03-06 18:47:11,854 - INFO - training batch 751, loss: 0.754, 24032/60000 datapoints
2025-03-06 18:47:12,045 - INFO - training batch 801, loss: 0.755, 25632/60000 datapoints
2025-03-06 18:47:12,236 - INFO - training batch 851, loss: 0.767, 27232/60000 datapoints
2025-03-06 18:47:12,428 - INFO - training batch 901, loss: 0.750, 28832/60000 datapoints
2025-03-06 18:47:12,619 - INFO - training batch 951, loss: 0.483, 30432/60000 datapoints
2025-03-06 18:47:12,811 - INFO - training batch 1001, loss: 0.844, 32032/60000 datapoints
2025-03-06 18:47:13,017 - INFO - training batch 1051, loss: 0.650, 33632/60000 datapoints
2025-03-06 18:47:13,206 - INFO - training batch 1101, loss: 0.643, 35232/60000 datapoints
2025-03-06 18:47:13,399 - INFO - training batch 1151, loss: 0.629, 36832/60000 datapoints
2025-03-06 18:47:13,592 - INFO - training batch 1201, loss: 0.574, 38432/60000 datapoints
2025-03-06 18:47:13,786 - INFO - training batch 1251, loss: 0.698, 40032/60000 datapoints
2025-03-06 18:47:14,003 - INFO - training batch 1301, loss: 0.607, 41632/60000 datapoints
2025-03-06 18:47:14,198 - INFO - training batch 1351, loss: 0.918, 43232/60000 datapoints
2025-03-06 18:47:14,389 - INFO - training batch 1401, loss: 0.697, 44832/60000 datapoints
2025-03-06 18:47:14,579 - INFO - training batch 1451, loss: 0.748, 46432/60000 datapoints
2025-03-06 18:47:14,772 - INFO - training batch 1501, loss: 0.947, 48032/60000 datapoints
2025-03-06 18:47:14,973 - INFO - training batch 1551, loss: 0.608, 49632/60000 datapoints
2025-03-06 18:47:15,166 - INFO - training batch 1601, loss: 0.713, 51232/60000 datapoints
2025-03-06 18:47:15,358 - INFO - training batch 1651, loss: 0.902, 52832/60000 datapoints
2025-03-06 18:47:15,552 - INFO - training batch 1701, loss: 0.660, 54432/60000 datapoints
2025-03-06 18:47:15,748 - INFO - training batch 1751, loss: 0.680, 56032/60000 datapoints
2025-03-06 18:47:15,942 - INFO - training batch 1801, loss: 0.928, 57632/60000 datapoints
2025-03-06 18:47:16,134 - INFO - training batch 1851, loss: 0.694, 59232/60000 datapoints
2025-03-06 18:47:16,235 - INFO - validation batch 1, loss: 0.834, 32/10016 datapoints
2025-03-06 18:47:16,386 - INFO - validation batch 51, loss: 0.700, 1632/10016 datapoints
2025-03-06 18:47:16,537 - INFO - validation batch 101, loss: 0.691, 3232/10016 datapoints
2025-03-06 18:47:16,691 - INFO - validation batch 151, loss: 0.622, 4832/10016 datapoints
2025-03-06 18:47:16,841 - INFO - validation batch 201, loss: 0.580, 6432/10016 datapoints
2025-03-06 18:47:16,996 - INFO - validation batch 251, loss: 0.720, 8032/10016 datapoints
2025-03-06 18:47:17,145 - INFO - validation batch 301, loss: 0.830, 9632/10016 datapoints
2025-03-06 18:47:17,181 - INFO - Epoch 66/800 done.
2025-03-06 18:47:17,181 - INFO - Final validation performance:
Loss: 0.711, top-1 acc: 0.834top-5 acc: 0.834
2025-03-06 18:47:17,182 - INFO - Beginning epoch 67/800
2025-03-06 18:47:17,188 - INFO - training batch 1, loss: 0.635, 32/60000 datapoints
2025-03-06 18:47:17,380 - INFO - training batch 51, loss: 0.638, 1632/60000 datapoints
2025-03-06 18:47:17,574 - INFO - training batch 101, loss: 1.018, 3232/60000 datapoints
2025-03-06 18:47:17,776 - INFO - training batch 151, loss: 0.719, 4832/60000 datapoints
2025-03-06 18:47:17,972 - INFO - training batch 201, loss: 0.660, 6432/60000 datapoints
2025-03-06 18:47:18,163 - INFO - training batch 251, loss: 0.774, 8032/60000 datapoints
2025-03-06 18:47:18,362 - INFO - training batch 301, loss: 0.695, 9632/60000 datapoints
2025-03-06 18:47:18,555 - INFO - training batch 351, loss: 0.802, 11232/60000 datapoints
2025-03-06 18:47:18,753 - INFO - training batch 401, loss: 0.823, 12832/60000 datapoints
2025-03-06 18:47:18,947 - INFO - training batch 451, loss: 0.801, 14432/60000 datapoints
2025-03-06 18:47:19,139 - INFO - training batch 501, loss: 0.639, 16032/60000 datapoints
2025-03-06 18:47:19,331 - INFO - training batch 551, loss: 0.603, 17632/60000 datapoints
2025-03-06 18:47:19,523 - INFO - training batch 601, loss: 0.851, 19232/60000 datapoints
2025-03-06 18:47:19,720 - INFO - training batch 651, loss: 0.741, 20832/60000 datapoints
2025-03-06 18:47:19,913 - INFO - training batch 701, loss: 0.524, 22432/60000 datapoints
2025-03-06 18:47:20,106 - INFO - training batch 751, loss: 1.032, 24032/60000 datapoints
2025-03-06 18:47:20,299 - INFO - training batch 801, loss: 0.623, 25632/60000 datapoints
2025-03-06 18:47:20,492 - INFO - training batch 851, loss: 0.687, 27232/60000 datapoints
2025-03-06 18:47:20,687 - INFO - training batch 901, loss: 0.913, 28832/60000 datapoints
2025-03-06 18:47:20,878 - INFO - training batch 951, loss: 0.569, 30432/60000 datapoints
2025-03-06 18:47:21,069 - INFO - training batch 1001, loss: 0.700, 32032/60000 datapoints
2025-03-06 18:47:21,261 - INFO - training batch 1051, loss: 0.552, 33632/60000 datapoints
2025-03-06 18:47:21,452 - INFO - training batch 1101, loss: 0.621, 35232/60000 datapoints
2025-03-06 18:47:21,647 - INFO - training batch 1151, loss: 0.718, 36832/60000 datapoints
2025-03-06 18:47:21,864 - INFO - training batch 1201, loss: 0.879, 38432/60000 datapoints
2025-03-06 18:47:22,056 - INFO - training batch 1251, loss: 0.754, 40032/60000 datapoints
2025-03-06 18:47:22,258 - INFO - training batch 1301, loss: 0.633, 41632/60000 datapoints
2025-03-06 18:47:22,461 - INFO - training batch 1351, loss: 0.601, 43232/60000 datapoints
2025-03-06 18:47:22,687 - INFO - training batch 1401, loss: 0.831, 44832/60000 datapoints
2025-03-06 18:47:22,902 - INFO - training batch 1451, loss: 0.698, 46432/60000 datapoints
2025-03-06 18:47:23,120 - INFO - training batch 1501, loss: 0.623, 48032/60000 datapoints
2025-03-06 18:47:23,320 - INFO - training batch 1551, loss: 0.658, 49632/60000 datapoints
2025-03-06 18:47:23,518 - INFO - training batch 1601, loss: 0.740, 51232/60000 datapoints
2025-03-06 18:47:23,714 - INFO - training batch 1651, loss: 0.713, 52832/60000 datapoints
2025-03-06 18:47:23,907 - INFO - training batch 1701, loss: 0.931, 54432/60000 datapoints
2025-03-06 18:47:24,105 - INFO - training batch 1751, loss: 0.768, 56032/60000 datapoints
2025-03-06 18:47:24,298 - INFO - training batch 1801, loss: 0.748, 57632/60000 datapoints
2025-03-06 18:47:24,491 - INFO - training batch 1851, loss: 0.733, 59232/60000 datapoints
2025-03-06 18:47:24,591 - INFO - validation batch 1, loss: 0.564, 32/10016 datapoints
2025-03-06 18:47:24,750 - INFO - validation batch 51, loss: 0.755, 1632/10016 datapoints
2025-03-06 18:47:24,911 - INFO - validation batch 101, loss: 0.846, 3232/10016 datapoints
2025-03-06 18:47:25,065 - INFO - validation batch 151, loss: 0.647, 4832/10016 datapoints
2025-03-06 18:47:25,218 - INFO - validation batch 201, loss: 0.676, 6432/10016 datapoints
2025-03-06 18:47:25,370 - INFO - validation batch 251, loss: 0.703, 8032/10016 datapoints
2025-03-06 18:47:25,525 - INFO - validation batch 301, loss: 0.819, 9632/10016 datapoints
2025-03-06 18:47:25,563 - INFO - Epoch 67/800 done.
2025-03-06 18:47:25,564 - INFO - Final validation performance:
Loss: 0.716, top-1 acc: 0.837top-5 acc: 0.837
2025-03-06 18:47:25,564 - INFO - Beginning epoch 68/800
2025-03-06 18:47:25,571 - INFO - training batch 1, loss: 0.765, 32/60000 datapoints
2025-03-06 18:47:25,811 - INFO - training batch 51, loss: 0.708, 1632/60000 datapoints
2025-03-06 18:47:26,042 - INFO - training batch 101, loss: 0.682, 3232/60000 datapoints
2025-03-06 18:47:26,245 - INFO - training batch 151, loss: 0.630, 4832/60000 datapoints
2025-03-06 18:47:26,443 - INFO - training batch 201, loss: 0.848, 6432/60000 datapoints
2025-03-06 18:47:26,691 - INFO - training batch 251, loss: 0.740, 8032/60000 datapoints
2025-03-06 18:47:26,889 - INFO - training batch 301, loss: 0.716, 9632/60000 datapoints
2025-03-06 18:47:27,086 - INFO - training batch 351, loss: 0.843, 11232/60000 datapoints
2025-03-06 18:47:27,282 - INFO - training batch 401, loss: 0.891, 12832/60000 datapoints
2025-03-06 18:47:27,478 - INFO - training batch 451, loss: 0.513, 14432/60000 datapoints
2025-03-06 18:47:27,677 - INFO - training batch 501, loss: 0.761, 16032/60000 datapoints
2025-03-06 18:47:27,871 - INFO - training batch 551, loss: 0.727, 17632/60000 datapoints
2025-03-06 18:47:28,074 - INFO - training batch 601, loss: 0.753, 19232/60000 datapoints
2025-03-06 18:47:28,269 - INFO - training batch 651, loss: 0.756, 20832/60000 datapoints
2025-03-06 18:47:28,462 - INFO - training batch 701, loss: 0.752, 22432/60000 datapoints
2025-03-06 18:47:28,660 - INFO - training batch 751, loss: 0.667, 24032/60000 datapoints
2025-03-06 18:47:28,854 - INFO - training batch 801, loss: 0.870, 25632/60000 datapoints
2025-03-06 18:47:29,053 - INFO - training batch 851, loss: 0.888, 27232/60000 datapoints
2025-03-06 18:47:29,249 - INFO - training batch 901, loss: 0.449, 28832/60000 datapoints
2025-03-06 18:47:29,451 - INFO - training batch 951, loss: 0.714, 30432/60000 datapoints
2025-03-06 18:47:29,654 - INFO - training batch 1001, loss: 0.526, 32032/60000 datapoints
2025-03-06 18:47:29,845 - INFO - training batch 1051, loss: 0.604, 33632/60000 datapoints
2025-03-06 18:47:30,039 - INFO - training batch 1101, loss: 0.691, 35232/60000 datapoints
2025-03-06 18:47:30,233 - INFO - training batch 1151, loss: 0.442, 36832/60000 datapoints
2025-03-06 18:47:30,430 - INFO - training batch 1201, loss: 0.732, 38432/60000 datapoints
2025-03-06 18:47:30,625 - INFO - training batch 1251, loss: 0.711, 40032/60000 datapoints
2025-03-06 18:47:30,818 - INFO - training batch 1301, loss: 0.575, 41632/60000 datapoints
2025-03-06 18:47:31,026 - INFO - training batch 1351, loss: 0.866, 43232/60000 datapoints
2025-03-06 18:47:31,226 - INFO - training batch 1401, loss: 0.655, 44832/60000 datapoints
2025-03-06 18:47:31,421 - INFO - training batch 1451, loss: 0.843, 46432/60000 datapoints
2025-03-06 18:47:31,617 - INFO - training batch 1501, loss: 0.674, 48032/60000 datapoints
2025-03-06 18:47:31,813 - INFO - training batch 1551, loss: 0.670, 49632/60000 datapoints
2025-03-06 18:47:32,030 - INFO - training batch 1601, loss: 0.627, 51232/60000 datapoints
2025-03-06 18:47:32,225 - INFO - training batch 1651, loss: 0.687, 52832/60000 datapoints
2025-03-06 18:47:32,419 - INFO - training batch 1701, loss: 0.522, 54432/60000 datapoints
2025-03-06 18:47:32,616 - INFO - training batch 1751, loss: 0.758, 56032/60000 datapoints
2025-03-06 18:47:32,808 - INFO - training batch 1801, loss: 0.711, 57632/60000 datapoints
2025-03-06 18:47:33,004 - INFO - training batch 1851, loss: 0.800, 59232/60000 datapoints
2025-03-06 18:47:33,106 - INFO - validation batch 1, loss: 0.765, 32/10016 datapoints
2025-03-06 18:47:33,259 - INFO - validation batch 51, loss: 0.592, 1632/10016 datapoints
2025-03-06 18:47:33,413 - INFO - validation batch 101, loss: 0.744, 3232/10016 datapoints
2025-03-06 18:47:33,565 - INFO - validation batch 151, loss: 0.721, 4832/10016 datapoints
2025-03-06 18:47:33,721 - INFO - validation batch 201, loss: 0.817, 6432/10016 datapoints
2025-03-06 18:47:33,881 - INFO - validation batch 251, loss: 0.926, 8032/10016 datapoints
2025-03-06 18:47:34,036 - INFO - validation batch 301, loss: 0.684, 9632/10016 datapoints
2025-03-06 18:47:34,075 - INFO - Epoch 68/800 done.
2025-03-06 18:47:34,075 - INFO - Final validation performance:
Loss: 0.750, top-1 acc: 0.838top-5 acc: 0.838
2025-03-06 18:47:34,076 - INFO - Beginning epoch 69/800
2025-03-06 18:47:34,082 - INFO - training batch 1, loss: 0.643, 32/60000 datapoints
2025-03-06 18:47:34,281 - INFO - training batch 51, loss: 0.868, 1632/60000 datapoints
2025-03-06 18:47:34,477 - INFO - training batch 101, loss: 0.652, 3232/60000 datapoints
2025-03-06 18:47:34,686 - INFO - training batch 151, loss: 0.696, 4832/60000 datapoints
2025-03-06 18:47:34,884 - INFO - training batch 201, loss: 0.741, 6432/60000 datapoints
2025-03-06 18:47:35,084 - INFO - training batch 251, loss: 0.708, 8032/60000 datapoints
2025-03-06 18:47:35,288 - INFO - training batch 301, loss: 0.733, 9632/60000 datapoints
2025-03-06 18:47:35,483 - INFO - training batch 351, loss: 0.813, 11232/60000 datapoints
2025-03-06 18:47:35,687 - INFO - training batch 401, loss: 0.648, 12832/60000 datapoints
2025-03-06 18:47:35,884 - INFO - training batch 451, loss: 0.675, 14432/60000 datapoints
2025-03-06 18:47:36,080 - INFO - training batch 501, loss: 1.073, 16032/60000 datapoints
2025-03-06 18:47:36,276 - INFO - training batch 551, loss: 0.827, 17632/60000 datapoints
2025-03-06 18:47:36,470 - INFO - training batch 601, loss: 0.618, 19232/60000 datapoints
2025-03-06 18:47:36,666 - INFO - training batch 651, loss: 0.798, 20832/60000 datapoints
2025-03-06 18:47:36,863 - INFO - training batch 701, loss: 0.561, 22432/60000 datapoints
2025-03-06 18:47:37,059 - INFO - training batch 751, loss: 0.689, 24032/60000 datapoints
2025-03-06 18:47:37,253 - INFO - training batch 801, loss: 0.597, 25632/60000 datapoints
2025-03-06 18:47:37,450 - INFO - training batch 851, loss: 0.840, 27232/60000 datapoints
2025-03-06 18:47:37,670 - INFO - training batch 901, loss: 0.787, 28832/60000 datapoints
2025-03-06 18:47:37,867 - INFO - training batch 951, loss: 0.823, 30432/60000 datapoints
2025-03-06 18:47:38,061 - INFO - training batch 1001, loss: 0.944, 32032/60000 datapoints
2025-03-06 18:47:38,255 - INFO - training batch 1051, loss: 0.825, 33632/60000 datapoints
2025-03-06 18:47:38,448 - INFO - training batch 1101, loss: 0.642, 35232/60000 datapoints
2025-03-06 18:47:38,642 - INFO - training batch 1151, loss: 0.561, 36832/60000 datapoints
2025-03-06 18:47:38,835 - INFO - training batch 1201, loss: 0.651, 38432/60000 datapoints
2025-03-06 18:47:39,041 - INFO - training batch 1251, loss: 0.660, 40032/60000 datapoints
2025-03-06 18:47:39,237 - INFO - training batch 1301, loss: 0.523, 41632/60000 datapoints
2025-03-06 18:47:39,432 - INFO - training batch 1351, loss: 0.609, 43232/60000 datapoints
2025-03-06 18:47:39,627 - INFO - training batch 1401, loss: 0.783, 44832/60000 datapoints
2025-03-06 18:47:39,820 - INFO - training batch 1451, loss: 0.858, 46432/60000 datapoints
2025-03-06 18:47:40,020 - INFO - training batch 1501, loss: 0.904, 48032/60000 datapoints
2025-03-06 18:47:40,216 - INFO - training batch 1551, loss: 0.784, 49632/60000 datapoints
2025-03-06 18:47:40,410 - INFO - training batch 1601, loss: 0.801, 51232/60000 datapoints
2025-03-06 18:47:40,608 - INFO - training batch 1651, loss: 0.740, 52832/60000 datapoints
2025-03-06 18:47:40,802 - INFO - training batch 1701, loss: 0.623, 54432/60000 datapoints
2025-03-06 18:47:40,996 - INFO - training batch 1751, loss: 0.794, 56032/60000 datapoints
2025-03-06 18:47:41,191 - INFO - training batch 1801, loss: 0.757, 57632/60000 datapoints
2025-03-06 18:47:41,387 - INFO - training batch 1851, loss: 0.701, 59232/60000 datapoints
2025-03-06 18:47:41,487 - INFO - validation batch 1, loss: 1.080, 32/10016 datapoints
2025-03-06 18:47:41,642 - INFO - validation batch 51, loss: 0.740, 1632/10016 datapoints
2025-03-06 18:47:41,795 - INFO - validation batch 101, loss: 0.599, 3232/10016 datapoints
2025-03-06 18:47:41,955 - INFO - validation batch 151, loss: 0.532, 4832/10016 datapoints
2025-03-06 18:47:42,124 - INFO - validation batch 201, loss: 0.804, 6432/10016 datapoints
2025-03-06 18:47:42,277 - INFO - validation batch 251, loss: 0.733, 8032/10016 datapoints
2025-03-06 18:47:42,430 - INFO - validation batch 301, loss: 0.568, 9632/10016 datapoints
2025-03-06 18:47:42,470 - INFO - Epoch 69/800 done.
2025-03-06 18:47:42,470 - INFO - Final validation performance:
Loss: 0.722, top-1 acc: 0.839top-5 acc: 0.839
2025-03-06 18:47:42,471 - INFO - Beginning epoch 70/800
2025-03-06 18:47:42,477 - INFO - training batch 1, loss: 0.718, 32/60000 datapoints
2025-03-06 18:47:42,678 - INFO - training batch 51, loss: 0.698, 1632/60000 datapoints
2025-03-06 18:47:42,875 - INFO - training batch 101, loss: 0.666, 3232/60000 datapoints
2025-03-06 18:47:43,078 - INFO - training batch 151, loss: 0.619, 4832/60000 datapoints
2025-03-06 18:47:43,276 - INFO - training batch 201, loss: 0.614, 6432/60000 datapoints
2025-03-06 18:47:43,472 - INFO - training batch 251, loss: 0.658, 8032/60000 datapoints
2025-03-06 18:47:43,676 - INFO - training batch 301, loss: 0.550, 9632/60000 datapoints
2025-03-06 18:47:43,875 - INFO - training batch 351, loss: 0.812, 11232/60000 datapoints
2025-03-06 18:47:44,075 - INFO - training batch 401, loss: 0.719, 12832/60000 datapoints
2025-03-06 18:47:44,270 - INFO - training batch 451, loss: 0.867, 14432/60000 datapoints
2025-03-06 18:47:44,467 - INFO - training batch 501, loss: 0.811, 16032/60000 datapoints
2025-03-06 18:47:44,665 - INFO - training batch 551, loss: 0.927, 17632/60000 datapoints
2025-03-06 18:47:44,860 - INFO - training batch 601, loss: 0.654, 19232/60000 datapoints
2025-03-06 18:47:45,060 - INFO - training batch 651, loss: 0.851, 20832/60000 datapoints
2025-03-06 18:47:45,255 - INFO - training batch 701, loss: 0.766, 22432/60000 datapoints
2025-03-06 18:47:45,452 - INFO - training batch 751, loss: 0.687, 24032/60000 datapoints
2025-03-06 18:47:45,650 - INFO - training batch 801, loss: 0.725, 25632/60000 datapoints
2025-03-06 18:47:45,846 - INFO - training batch 851, loss: 0.901, 27232/60000 datapoints
2025-03-06 18:47:46,041 - INFO - training batch 901, loss: 0.745, 28832/60000 datapoints
2025-03-06 18:47:46,239 - INFO - training batch 951, loss: 0.615, 30432/60000 datapoints
2025-03-06 18:47:46,435 - INFO - training batch 1001, loss: 0.713, 32032/60000 datapoints
2025-03-06 18:47:46,633 - INFO - training batch 1051, loss: 0.596, 33632/60000 datapoints
2025-03-06 18:47:46,826 - INFO - training batch 1101, loss: 1.018, 35232/60000 datapoints
2025-03-06 18:47:47,022 - INFO - training batch 1151, loss: 0.740, 36832/60000 datapoints
2025-03-06 18:47:47,218 - INFO - training batch 1201, loss: 0.729, 38432/60000 datapoints
2025-03-06 18:47:47,414 - INFO - training batch 1251, loss: 0.995, 40032/60000 datapoints
2025-03-06 18:47:47,610 - INFO - training batch 1301, loss: 0.832, 41632/60000 datapoints
2025-03-06 18:47:47,805 - INFO - training batch 1351, loss: 0.699, 43232/60000 datapoints
2025-03-06 18:47:48,000 - INFO - training batch 1401, loss: 0.594, 44832/60000 datapoints
2025-03-06 18:47:48,196 - INFO - training batch 1451, loss: 0.743, 46432/60000 datapoints
2025-03-06 18:47:48,389 - INFO - training batch 1501, loss: 0.790, 48032/60000 datapoints
2025-03-06 18:47:48,582 - INFO - training batch 1551, loss: 0.802, 49632/60000 datapoints
2025-03-06 18:47:48,779 - INFO - training batch 1601, loss: 0.681, 51232/60000 datapoints
2025-03-06 18:47:48,977 - INFO - training batch 1651, loss: 0.624, 52832/60000 datapoints
2025-03-06 18:47:49,172 - INFO - training batch 1701, loss: 0.679, 54432/60000 datapoints
2025-03-06 18:47:49,367 - INFO - training batch 1751, loss: 0.534, 56032/60000 datapoints
2025-03-06 18:47:49,562 - INFO - training batch 1801, loss: 0.638, 57632/60000 datapoints
2025-03-06 18:47:49,761 - INFO - training batch 1851, loss: 0.747, 59232/60000 datapoints
2025-03-06 18:47:49,863 - INFO - validation batch 1, loss: 0.571, 32/10016 datapoints
2025-03-06 18:47:50,016 - INFO - validation batch 51, loss: 0.490, 1632/10016 datapoints
2025-03-06 18:47:50,168 - INFO - validation batch 101, loss: 0.611, 3232/10016 datapoints
2025-03-06 18:47:50,321 - INFO - validation batch 151, loss: 0.560, 4832/10016 datapoints
2025-03-06 18:47:50,475 - INFO - validation batch 201, loss: 0.764, 6432/10016 datapoints
2025-03-06 18:47:50,631 - INFO - validation batch 251, loss: 0.512, 8032/10016 datapoints
2025-03-06 18:47:50,781 - INFO - validation batch 301, loss: 0.810, 9632/10016 datapoints
2025-03-06 18:47:50,817 - INFO - Epoch 70/800 done.
2025-03-06 18:47:50,817 - INFO - Final validation performance:
Loss: 0.617, top-1 acc: 0.840top-5 acc: 0.840
2025-03-06 18:47:50,818 - INFO - Beginning epoch 71/800
2025-03-06 18:47:50,825 - INFO - training batch 1, loss: 0.506, 32/60000 datapoints
2025-03-06 18:47:51,035 - INFO - training batch 51, loss: 0.682, 1632/60000 datapoints
2025-03-06 18:47:51,228 - INFO - training batch 101, loss: 0.539, 3232/60000 datapoints
2025-03-06 18:47:51,430 - INFO - training batch 151, loss: 0.619, 4832/60000 datapoints
2025-03-06 18:47:51,632 - INFO - training batch 201, loss: 0.720, 6432/60000 datapoints
2025-03-06 18:47:51,829 - INFO - training batch 251, loss: 0.674, 8032/60000 datapoints
2025-03-06 18:47:52,027 - INFO - training batch 301, loss: 0.913, 9632/60000 datapoints
2025-03-06 18:47:52,237 - INFO - training batch 351, loss: 0.643, 11232/60000 datapoints
2025-03-06 18:47:52,433 - INFO - training batch 401, loss: 0.593, 12832/60000 datapoints
2025-03-06 18:47:52,640 - INFO - training batch 451, loss: 0.836, 14432/60000 datapoints
2025-03-06 18:47:52,834 - INFO - training batch 501, loss: 0.637, 16032/60000 datapoints
2025-03-06 18:47:53,031 - INFO - training batch 551, loss: 0.608, 17632/60000 datapoints
2025-03-06 18:47:53,222 - INFO - training batch 601, loss: 0.454, 19232/60000 datapoints
2025-03-06 18:47:53,417 - INFO - training batch 651, loss: 0.626, 20832/60000 datapoints
2025-03-06 18:47:53,612 - INFO - training batch 701, loss: 0.803, 22432/60000 datapoints
2025-03-06 18:47:53,805 - INFO - training batch 751, loss: 0.490, 24032/60000 datapoints
2025-03-06 18:47:54,000 - INFO - training batch 801, loss: 0.628, 25632/60000 datapoints
2025-03-06 18:47:54,195 - INFO - training batch 851, loss: 0.944, 27232/60000 datapoints
2025-03-06 18:47:54,390 - INFO - training batch 901, loss: 0.855, 28832/60000 datapoints
2025-03-06 18:47:54,592 - INFO - training batch 951, loss: 0.705, 30432/60000 datapoints
2025-03-06 18:47:54,800 - INFO - training batch 1001, loss: 0.578, 32032/60000 datapoints
2025-03-06 18:47:55,002 - INFO - training batch 1051, loss: 0.462, 33632/60000 datapoints
2025-03-06 18:47:55,202 - INFO - training batch 1101, loss: 0.754, 35232/60000 datapoints
2025-03-06 18:47:55,396 - INFO - training batch 1151, loss: 0.687, 36832/60000 datapoints
2025-03-06 18:47:55,618 - INFO - training batch 1201, loss: 0.623, 38432/60000 datapoints
2025-03-06 18:47:55,832 - INFO - training batch 1251, loss: 0.725, 40032/60000 datapoints
2025-03-06 18:47:56,029 - INFO - training batch 1301, loss: 0.662, 41632/60000 datapoints
2025-03-06 18:47:56,228 - INFO - training batch 1351, loss: 0.592, 43232/60000 datapoints
2025-03-06 18:47:56,426 - INFO - training batch 1401, loss: 0.642, 44832/60000 datapoints
2025-03-06 18:47:56,623 - INFO - training batch 1451, loss: 0.734, 46432/60000 datapoints
2025-03-06 18:47:56,825 - INFO - training batch 1501, loss: 0.582, 48032/60000 datapoints
2025-03-06 18:47:57,021 - INFO - training batch 1551, loss: 0.668, 49632/60000 datapoints
2025-03-06 18:47:57,215 - INFO - training batch 1601, loss: 0.527, 51232/60000 datapoints
2025-03-06 18:47:57,410 - INFO - training batch 1651, loss: 0.764, 52832/60000 datapoints
2025-03-06 18:47:57,612 - INFO - training batch 1701, loss: 0.556, 54432/60000 datapoints
2025-03-06 18:47:57,806 - INFO - training batch 1751, loss: 0.689, 56032/60000 datapoints
2025-03-06 18:47:58,002 - INFO - training batch 1801, loss: 0.550, 57632/60000 datapoints
2025-03-06 18:47:58,196 - INFO - training batch 1851, loss: 0.554, 59232/60000 datapoints
2025-03-06 18:47:58,297 - INFO - validation batch 1, loss: 0.537, 32/10016 datapoints
2025-03-06 18:47:58,451 - INFO - validation batch 51, loss: 0.592, 1632/10016 datapoints
2025-03-06 18:47:58,609 - INFO - validation batch 101, loss: 0.570, 3232/10016 datapoints
2025-03-06 18:47:58,762 - INFO - validation batch 151, loss: 0.702, 4832/10016 datapoints
2025-03-06 18:47:58,914 - INFO - validation batch 201, loss: 0.611, 6432/10016 datapoints
2025-03-06 18:47:59,073 - INFO - validation batch 251, loss: 0.426, 8032/10016 datapoints
2025-03-06 18:47:59,225 - INFO - validation batch 301, loss: 0.654, 9632/10016 datapoints
2025-03-06 18:47:59,262 - INFO - Epoch 71/800 done.
2025-03-06 18:47:59,262 - INFO - Final validation performance:
Loss: 0.585, top-1 acc: 0.842top-5 acc: 0.842
2025-03-06 18:47:59,263 - INFO - Beginning epoch 72/800
2025-03-06 18:47:59,269 - INFO - training batch 1, loss: 0.725, 32/60000 datapoints
2025-03-06 18:47:59,464 - INFO - training batch 51, loss: 0.769, 1632/60000 datapoints
2025-03-06 18:47:59,668 - INFO - training batch 101, loss: 0.655, 3232/60000 datapoints
2025-03-06 18:47:59,866 - INFO - training batch 151, loss: 0.782, 4832/60000 datapoints
2025-03-06 18:48:00,063 - INFO - training batch 201, loss: 0.680, 6432/60000 datapoints
2025-03-06 18:48:00,261 - INFO - training batch 251, loss: 0.553, 8032/60000 datapoints
2025-03-06 18:48:00,459 - INFO - training batch 301, loss: 0.712, 9632/60000 datapoints
2025-03-06 18:48:00,660 - INFO - training batch 351, loss: 0.839, 11232/60000 datapoints
2025-03-06 18:48:00,855 - INFO - training batch 401, loss: 0.513, 12832/60000 datapoints
2025-03-06 18:48:01,052 - INFO - training batch 451, loss: 0.723, 14432/60000 datapoints
2025-03-06 18:48:01,245 - INFO - training batch 501, loss: 0.702, 16032/60000 datapoints
2025-03-06 18:48:01,440 - INFO - training batch 551, loss: 0.647, 17632/60000 datapoints
2025-03-06 18:48:01,639 - INFO - training batch 601, loss: 0.525, 19232/60000 datapoints
2025-03-06 18:48:01,835 - INFO - training batch 651, loss: 0.872, 20832/60000 datapoints
2025-03-06 18:48:02,029 - INFO - training batch 701, loss: 0.620, 22432/60000 datapoints
2025-03-06 18:48:02,244 - INFO - training batch 751, loss: 0.800, 24032/60000 datapoints
2025-03-06 18:48:02,437 - INFO - training batch 801, loss: 0.525, 25632/60000 datapoints
2025-03-06 18:48:02,651 - INFO - training batch 851, loss: 0.624, 27232/60000 datapoints
2025-03-06 18:48:02,866 - INFO - training batch 901, loss: 0.646, 28832/60000 datapoints
2025-03-06 18:48:03,060 - INFO - training batch 951, loss: 0.572, 30432/60000 datapoints
2025-03-06 18:48:03,255 - INFO - training batch 1001, loss: 0.790, 32032/60000 datapoints
2025-03-06 18:48:03,450 - INFO - training batch 1051, loss: 0.618, 33632/60000 datapoints
2025-03-06 18:48:03,647 - INFO - training batch 1101, loss: 0.767, 35232/60000 datapoints
2025-03-06 18:48:03,839 - INFO - training batch 1151, loss: 0.984, 36832/60000 datapoints
2025-03-06 18:48:04,033 - INFO - training batch 1201, loss: 0.467, 38432/60000 datapoints
2025-03-06 18:48:04,226 - INFO - training batch 1251, loss: 0.624, 40032/60000 datapoints
2025-03-06 18:48:04,420 - INFO - training batch 1301, loss: 0.814, 41632/60000 datapoints
2025-03-06 18:48:04,617 - INFO - training batch 1351, loss: 0.692, 43232/60000 datapoints
2025-03-06 18:48:04,809 - INFO - training batch 1401, loss: 0.703, 44832/60000 datapoints
2025-03-06 18:48:05,008 - INFO - training batch 1451, loss: 0.545, 46432/60000 datapoints
2025-03-06 18:48:05,206 - INFO - training batch 1501, loss: 0.662, 48032/60000 datapoints
2025-03-06 18:48:05,398 - INFO - training batch 1551, loss: 0.656, 49632/60000 datapoints
2025-03-06 18:48:05,592 - INFO - training batch 1601, loss: 0.493, 51232/60000 datapoints
2025-03-06 18:48:05,790 - INFO - training batch 1651, loss: 0.685, 52832/60000 datapoints
2025-03-06 18:48:05,984 - INFO - training batch 1701, loss: 0.746, 54432/60000 datapoints
2025-03-06 18:48:06,179 - INFO - training batch 1751, loss: 0.639, 56032/60000 datapoints
2025-03-06 18:48:06,376 - INFO - training batch 1801, loss: 0.650, 57632/60000 datapoints
2025-03-06 18:48:06,571 - INFO - training batch 1851, loss: 0.696, 59232/60000 datapoints
2025-03-06 18:48:06,672 - INFO - validation batch 1, loss: 0.591, 32/10016 datapoints
2025-03-06 18:48:06,827 - INFO - validation batch 51, loss: 0.826, 1632/10016 datapoints
2025-03-06 18:48:06,980 - INFO - validation batch 101, loss: 0.560, 3232/10016 datapoints
2025-03-06 18:48:07,134 - INFO - validation batch 151, loss: 0.608, 4832/10016 datapoints
2025-03-06 18:48:07,287 - INFO - validation batch 201, loss: 0.613, 6432/10016 datapoints
2025-03-06 18:48:07,440 - INFO - validation batch 251, loss: 0.685, 8032/10016 datapoints
2025-03-06 18:48:07,595 - INFO - validation batch 301, loss: 0.770, 9632/10016 datapoints
2025-03-06 18:48:07,634 - INFO - Epoch 72/800 done.
2025-03-06 18:48:07,635 - INFO - Final validation performance:
Loss: 0.665, top-1 acc: 0.844top-5 acc: 0.844
2025-03-06 18:48:07,635 - INFO - Beginning epoch 73/800
2025-03-06 18:48:07,642 - INFO - training batch 1, loss: 0.589, 32/60000 datapoints
2025-03-06 18:48:07,839 - INFO - training batch 51, loss: 0.630, 1632/60000 datapoints
2025-03-06 18:48:08,032 - INFO - training batch 101, loss: 0.842, 3232/60000 datapoints
2025-03-06 18:48:08,238 - INFO - training batch 151, loss: 0.619, 4832/60000 datapoints
2025-03-06 18:48:08,436 - INFO - training batch 201, loss: 0.507, 6432/60000 datapoints
2025-03-06 18:48:08,639 - INFO - training batch 251, loss: 0.669, 8032/60000 datapoints
2025-03-06 18:48:08,835 - INFO - training batch 301, loss: 0.587, 9632/60000 datapoints
2025-03-06 18:48:09,037 - INFO - training batch 351, loss: 0.708, 11232/60000 datapoints
2025-03-06 18:48:09,235 - INFO - training batch 401, loss: 0.550, 12832/60000 datapoints
2025-03-06 18:48:09,428 - INFO - training batch 451, loss: 0.671, 14432/60000 datapoints
2025-03-06 18:48:09,623 - INFO - training batch 501, loss: 0.551, 16032/60000 datapoints
2025-03-06 18:48:09,817 - INFO - training batch 551, loss: 0.785, 17632/60000 datapoints
2025-03-06 18:48:10,013 - INFO - training batch 601, loss: 0.720, 19232/60000 datapoints
2025-03-06 18:48:10,211 - INFO - training batch 651, loss: 0.724, 20832/60000 datapoints
2025-03-06 18:48:10,406 - INFO - training batch 701, loss: 0.601, 22432/60000 datapoints
2025-03-06 18:48:10,610 - INFO - training batch 751, loss: 0.592, 24032/60000 datapoints
2025-03-06 18:48:10,807 - INFO - training batch 801, loss: 0.749, 25632/60000 datapoints
2025-03-06 18:48:11,003 - INFO - training batch 851, loss: 0.764, 27232/60000 datapoints
2025-03-06 18:48:11,199 - INFO - training batch 901, loss: 0.558, 28832/60000 datapoints
2025-03-06 18:48:11,396 - INFO - training batch 951, loss: 0.739, 30432/60000 datapoints
2025-03-06 18:48:11,590 - INFO - training batch 1001, loss: 0.627, 32032/60000 datapoints
2025-03-06 18:48:11,785 - INFO - training batch 1051, loss: 0.766, 33632/60000 datapoints
2025-03-06 18:48:11,979 - INFO - training batch 1101, loss: 0.774, 35232/60000 datapoints
2025-03-06 18:48:12,172 - INFO - training batch 1151, loss: 0.877, 36832/60000 datapoints
2025-03-06 18:48:12,385 - INFO - training batch 1201, loss: 0.673, 38432/60000 datapoints
2025-03-06 18:48:12,589 - INFO - training batch 1251, loss: 0.733, 40032/60000 datapoints
2025-03-06 18:48:12,787 - INFO - training batch 1301, loss: 0.468, 41632/60000 datapoints
2025-03-06 18:48:12,980 - INFO - training batch 1351, loss: 0.656, 43232/60000 datapoints
2025-03-06 18:48:13,187 - INFO - training batch 1401, loss: 0.742, 44832/60000 datapoints
2025-03-06 18:48:13,383 - INFO - training batch 1451, loss: 0.602, 46432/60000 datapoints
2025-03-06 18:48:13,577 - INFO - training batch 1501, loss: 0.564, 48032/60000 datapoints
2025-03-06 18:48:13,796 - INFO - training batch 1551, loss: 0.750, 49632/60000 datapoints
2025-03-06 18:48:14,022 - INFO - training batch 1601, loss: 0.885, 51232/60000 datapoints
2025-03-06 18:48:14,242 - INFO - training batch 1651, loss: 0.677, 52832/60000 datapoints
2025-03-06 18:48:14,458 - INFO - training batch 1701, loss: 0.842, 54432/60000 datapoints
2025-03-06 18:48:14,655 - INFO - training batch 1751, loss: 0.626, 56032/60000 datapoints
2025-03-06 18:48:14,849 - INFO - training batch 1801, loss: 0.528, 57632/60000 datapoints
2025-03-06 18:48:15,047 - INFO - training batch 1851, loss: 0.817, 59232/60000 datapoints
2025-03-06 18:48:15,151 - INFO - validation batch 1, loss: 0.526, 32/10016 datapoints
2025-03-06 18:48:15,303 - INFO - validation batch 51, loss: 0.773, 1632/10016 datapoints
2025-03-06 18:48:15,456 - INFO - validation batch 101, loss: 0.612, 3232/10016 datapoints
2025-03-06 18:48:15,611 - INFO - validation batch 151, loss: 0.545, 4832/10016 datapoints
2025-03-06 18:48:15,766 - INFO - validation batch 201, loss: 0.709, 6432/10016 datapoints
2025-03-06 18:48:15,920 - INFO - validation batch 251, loss: 0.565, 8032/10016 datapoints
2025-03-06 18:48:16,075 - INFO - validation batch 301, loss: 0.600, 9632/10016 datapoints
2025-03-06 18:48:16,112 - INFO - Epoch 73/800 done.
2025-03-06 18:48:16,112 - INFO - Final validation performance:
Loss: 0.618, top-1 acc: 0.845top-5 acc: 0.845
2025-03-06 18:48:16,113 - INFO - Beginning epoch 74/800
2025-03-06 18:48:16,119 - INFO - training batch 1, loss: 0.645, 32/60000 datapoints
2025-03-06 18:48:16,341 - INFO - training batch 51, loss: 0.637, 1632/60000 datapoints
2025-03-06 18:48:16,534 - INFO - training batch 101, loss: 0.606, 3232/60000 datapoints
2025-03-06 18:48:16,733 - INFO - training batch 151, loss: 0.599, 4832/60000 datapoints
2025-03-06 18:48:16,931 - INFO - training batch 201, loss: 0.754, 6432/60000 datapoints
2025-03-06 18:48:17,133 - INFO - training batch 251, loss: 0.684, 8032/60000 datapoints
2025-03-06 18:48:17,329 - INFO - training batch 301, loss: 0.669, 9632/60000 datapoints
2025-03-06 18:48:17,522 - INFO - training batch 351, loss: 0.634, 11232/60000 datapoints
2025-03-06 18:48:17,717 - INFO - training batch 401, loss: 0.604, 12832/60000 datapoints
2025-03-06 18:48:17,910 - INFO - training batch 451, loss: 0.912, 14432/60000 datapoints
2025-03-06 18:48:18,104 - INFO - training batch 501, loss: 0.862, 16032/60000 datapoints
2025-03-06 18:48:18,298 - INFO - training batch 551, loss: 0.697, 17632/60000 datapoints
2025-03-06 18:48:18,494 - INFO - training batch 601, loss: 0.576, 19232/60000 datapoints
2025-03-06 18:48:18,692 - INFO - training batch 651, loss: 0.754, 20832/60000 datapoints
2025-03-06 18:48:18,884 - INFO - training batch 701, loss: 0.787, 22432/60000 datapoints
2025-03-06 18:48:19,081 - INFO - training batch 751, loss: 0.509, 24032/60000 datapoints
2025-03-06 18:48:19,276 - INFO - training batch 801, loss: 0.671, 25632/60000 datapoints
2025-03-06 18:48:19,470 - INFO - training batch 851, loss: 0.832, 27232/60000 datapoints
2025-03-06 18:48:19,668 - INFO - training batch 901, loss: 0.723, 28832/60000 datapoints
2025-03-06 18:48:19,861 - INFO - training batch 951, loss: 0.724, 30432/60000 datapoints
2025-03-06 18:48:20,057 - INFO - training batch 1001, loss: 0.878, 32032/60000 datapoints
2025-03-06 18:48:20,254 - INFO - training batch 1051, loss: 0.636, 33632/60000 datapoints
2025-03-06 18:48:20,450 - INFO - training batch 1101, loss: 0.646, 35232/60000 datapoints
2025-03-06 18:48:20,647 - INFO - training batch 1151, loss: 0.587, 36832/60000 datapoints
2025-03-06 18:48:20,858 - INFO - training batch 1201, loss: 0.832, 38432/60000 datapoints
2025-03-06 18:48:21,063 - INFO - training batch 1251, loss: 0.737, 40032/60000 datapoints
2025-03-06 18:48:21,264 - INFO - training batch 1301, loss: 0.784, 41632/60000 datapoints
2025-03-06 18:48:21,463 - INFO - training batch 1351, loss: 0.636, 43232/60000 datapoints
2025-03-06 18:48:21,660 - INFO - training batch 1401, loss: 0.667, 44832/60000 datapoints
2025-03-06 18:48:21,878 - INFO - training batch 1451, loss: 0.699, 46432/60000 datapoints
2025-03-06 18:48:22,083 - INFO - training batch 1501, loss: 0.572, 48032/60000 datapoints
2025-03-06 18:48:22,279 - INFO - training batch 1551, loss: 0.740, 49632/60000 datapoints
2025-03-06 18:48:22,498 - INFO - training batch 1601, loss: 0.887, 51232/60000 datapoints
2025-03-06 18:48:22,702 - INFO - training batch 1651, loss: 0.551, 52832/60000 datapoints
2025-03-06 18:48:22,903 - INFO - training batch 1701, loss: 0.740, 54432/60000 datapoints
2025-03-06 18:48:23,114 - INFO - training batch 1751, loss: 0.792, 56032/60000 datapoints
2025-03-06 18:48:23,310 - INFO - training batch 1801, loss: 0.653, 57632/60000 datapoints
2025-03-06 18:48:23,503 - INFO - training batch 1851, loss: 0.575, 59232/60000 datapoints
2025-03-06 18:48:23,607 - INFO - validation batch 1, loss: 0.893, 32/10016 datapoints
2025-03-06 18:48:23,763 - INFO - validation batch 51, loss: 0.389, 1632/10016 datapoints
2025-03-06 18:48:23,918 - INFO - validation batch 101, loss: 0.713, 3232/10016 datapoints
2025-03-06 18:48:24,088 - INFO - validation batch 151, loss: 0.442, 4832/10016 datapoints
2025-03-06 18:48:24,251 - INFO - validation batch 201, loss: 0.660, 6432/10016 datapoints
2025-03-06 18:48:24,450 - INFO - validation batch 251, loss: 0.558, 8032/10016 datapoints
2025-03-06 18:48:24,613 - INFO - validation batch 301, loss: 0.675, 9632/10016 datapoints
2025-03-06 18:48:24,653 - INFO - Epoch 74/800 done.
2025-03-06 18:48:24,653 - INFO - Final validation performance:
Loss: 0.619, top-1 acc: 0.846top-5 acc: 0.846
2025-03-06 18:48:24,654 - INFO - Beginning epoch 75/800
2025-03-06 18:48:24,660 - INFO - training batch 1, loss: 0.762, 32/60000 datapoints
2025-03-06 18:48:24,872 - INFO - training batch 51, loss: 0.519, 1632/60000 datapoints
2025-03-06 18:48:25,075 - INFO - training batch 101, loss: 0.697, 3232/60000 datapoints
2025-03-06 18:48:25,273 - INFO - training batch 151, loss: 0.482, 4832/60000 datapoints
2025-03-06 18:48:25,476 - INFO - training batch 201, loss: 0.660, 6432/60000 datapoints
2025-03-06 18:48:25,681 - INFO - training batch 251, loss: 0.655, 8032/60000 datapoints
2025-03-06 18:48:25,883 - INFO - training batch 301, loss: 0.503, 9632/60000 datapoints
2025-03-06 18:48:26,083 - INFO - training batch 351, loss: 0.644, 11232/60000 datapoints
2025-03-06 18:48:26,282 - INFO - training batch 401, loss: 0.704, 12832/60000 datapoints
2025-03-06 18:48:26,479 - INFO - training batch 451, loss: 0.703, 14432/60000 datapoints
2025-03-06 18:48:26,676 - INFO - training batch 501, loss: 0.726, 16032/60000 datapoints
2025-03-06 18:48:26,870 - INFO - training batch 551, loss: 0.715, 17632/60000 datapoints
2025-03-06 18:48:27,066 - INFO - training batch 601, loss: 0.461, 19232/60000 datapoints
2025-03-06 18:48:27,262 - INFO - training batch 651, loss: 0.621, 20832/60000 datapoints
2025-03-06 18:48:27,459 - INFO - training batch 701, loss: 0.654, 22432/60000 datapoints
2025-03-06 18:48:27,658 - INFO - training batch 751, loss: 0.540, 24032/60000 datapoints
2025-03-06 18:48:27,853 - INFO - training batch 801, loss: 0.730, 25632/60000 datapoints
2025-03-06 18:48:28,058 - INFO - training batch 851, loss: 0.565, 27232/60000 datapoints
2025-03-06 18:48:28,259 - INFO - training batch 901, loss: 0.668, 28832/60000 datapoints
2025-03-06 18:48:28,456 - INFO - training batch 951, loss: 0.549, 30432/60000 datapoints
2025-03-06 18:48:28,654 - INFO - training batch 1001, loss: 0.747, 32032/60000 datapoints
2025-03-06 18:48:28,851 - INFO - training batch 1051, loss: 0.654, 33632/60000 datapoints
2025-03-06 18:48:29,045 - INFO - training batch 1101, loss: 0.555, 35232/60000 datapoints
2025-03-06 18:48:29,240 - INFO - training batch 1151, loss: 0.739, 36832/60000 datapoints
2025-03-06 18:48:29,442 - INFO - training batch 1201, loss: 0.652, 38432/60000 datapoints
2025-03-06 18:48:29,638 - INFO - training batch 1251, loss: 0.390, 40032/60000 datapoints
2025-03-06 18:48:29,832 - INFO - training batch 1301, loss: 0.670, 41632/60000 datapoints
2025-03-06 18:48:30,026 - INFO - training batch 1351, loss: 0.568, 43232/60000 datapoints
2025-03-06 18:48:30,219 - INFO - training batch 1401, loss: 0.707, 44832/60000 datapoints
2025-03-06 18:48:30,414 - INFO - training batch 1451, loss: 0.594, 46432/60000 datapoints
2025-03-06 18:48:30,610 - INFO - training batch 1501, loss: 1.016, 48032/60000 datapoints
2025-03-06 18:48:30,805 - INFO - training batch 1551, loss: 0.676, 49632/60000 datapoints
2025-03-06 18:48:30,999 - INFO - training batch 1601, loss: 0.810, 51232/60000 datapoints
2025-03-06 18:48:31,197 - INFO - training batch 1651, loss: 0.635, 52832/60000 datapoints
2025-03-06 18:48:31,394 - INFO - training batch 1701, loss: 0.499, 54432/60000 datapoints
2025-03-06 18:48:31,587 - INFO - training batch 1751, loss: 0.761, 56032/60000 datapoints
2025-03-06 18:48:31,790 - INFO - training batch 1801, loss: 0.692, 57632/60000 datapoints
2025-03-06 18:48:31,984 - INFO - training batch 1851, loss: 0.746, 59232/60000 datapoints
2025-03-06 18:48:32,086 - INFO - validation batch 1, loss: 0.595, 32/10016 datapoints
2025-03-06 18:48:32,242 - INFO - validation batch 51, loss: 0.557, 1632/10016 datapoints
2025-03-06 18:48:32,399 - INFO - validation batch 101, loss: 0.665, 3232/10016 datapoints
2025-03-06 18:48:32,568 - INFO - validation batch 151, loss: 0.569, 4832/10016 datapoints
2025-03-06 18:48:32,722 - INFO - validation batch 201, loss: 0.434, 6432/10016 datapoints
2025-03-06 18:48:32,877 - INFO - validation batch 251, loss: 0.742, 8032/10016 datapoints
2025-03-06 18:48:33,028 - INFO - validation batch 301, loss: 0.601, 9632/10016 datapoints
2025-03-06 18:48:33,066 - INFO - Epoch 75/800 done.
2025-03-06 18:48:33,067 - INFO - Final validation performance:
Loss: 0.595, top-1 acc: 0.848top-5 acc: 0.848
2025-03-06 18:48:33,067 - INFO - Beginning epoch 76/800
2025-03-06 18:48:33,073 - INFO - training batch 1, loss: 0.617, 32/60000 datapoints
2025-03-06 18:48:33,285 - INFO - training batch 51, loss: 0.665, 1632/60000 datapoints
2025-03-06 18:48:33,483 - INFO - training batch 101, loss: 0.767, 3232/60000 datapoints
2025-03-06 18:48:33,686 - INFO - training batch 151, loss: 0.670, 4832/60000 datapoints
2025-03-06 18:48:33,886 - INFO - training batch 201, loss: 0.562, 6432/60000 datapoints
2025-03-06 18:48:34,082 - INFO - training batch 251, loss: 0.900, 8032/60000 datapoints
2025-03-06 18:48:34,280 - INFO - training batch 301, loss: 0.617, 9632/60000 datapoints
2025-03-06 18:48:34,477 - INFO - training batch 351, loss: 0.656, 11232/60000 datapoints
2025-03-06 18:48:34,675 - INFO - training batch 401, loss: 0.708, 12832/60000 datapoints
2025-03-06 18:48:34,874 - INFO - training batch 451, loss: 0.903, 14432/60000 datapoints
2025-03-06 18:48:35,071 - INFO - training batch 501, loss: 0.757, 16032/60000 datapoints
2025-03-06 18:48:35,269 - INFO - training batch 551, loss: 0.617, 17632/60000 datapoints
2025-03-06 18:48:35,465 - INFO - training batch 601, loss: 0.814, 19232/60000 datapoints
2025-03-06 18:48:35,659 - INFO - training batch 651, loss: 0.619, 20832/60000 datapoints
2025-03-06 18:48:35,854 - INFO - training batch 701, loss: 0.701, 22432/60000 datapoints
2025-03-06 18:48:36,050 - INFO - training batch 751, loss: 0.647, 24032/60000 datapoints
2025-03-06 18:48:36,251 - INFO - training batch 801, loss: 0.711, 25632/60000 datapoints
2025-03-06 18:48:36,450 - INFO - training batch 851, loss: 0.625, 27232/60000 datapoints
2025-03-06 18:48:36,646 - INFO - training batch 901, loss: 0.663, 28832/60000 datapoints
2025-03-06 18:48:36,840 - INFO - training batch 951, loss: 0.544, 30432/60000 datapoints
2025-03-06 18:48:37,036 - INFO - training batch 1001, loss: 0.659, 32032/60000 datapoints
2025-03-06 18:48:37,230 - INFO - training batch 1051, loss: 0.792, 33632/60000 datapoints
2025-03-06 18:48:37,429 - INFO - training batch 1101, loss: 0.625, 35232/60000 datapoints
2025-03-06 18:48:37,642 - INFO - training batch 1151, loss: 0.746, 36832/60000 datapoints
2025-03-06 18:48:37,836 - INFO - training batch 1201, loss: 0.612, 38432/60000 datapoints
2025-03-06 18:48:38,032 - INFO - training batch 1251, loss: 0.433, 40032/60000 datapoints
2025-03-06 18:48:38,224 - INFO - training batch 1301, loss: 0.685, 41632/60000 datapoints
2025-03-06 18:48:38,418 - INFO - training batch 1351, loss: 0.521, 43232/60000 datapoints
2025-03-06 18:48:38,613 - INFO - training batch 1401, loss: 0.945, 44832/60000 datapoints
2025-03-06 18:48:38,807 - INFO - training batch 1451, loss: 0.577, 46432/60000 datapoints
2025-03-06 18:48:39,004 - INFO - training batch 1501, loss: 0.838, 48032/60000 datapoints
2025-03-06 18:48:39,202 - INFO - training batch 1551, loss: 0.551, 49632/60000 datapoints
2025-03-06 18:48:39,398 - INFO - training batch 1601, loss: 0.719, 51232/60000 datapoints
2025-03-06 18:48:39,596 - INFO - training batch 1651, loss: 0.660, 52832/60000 datapoints
2025-03-06 18:48:39,794 - INFO - training batch 1701, loss: 0.523, 54432/60000 datapoints
2025-03-06 18:48:39,987 - INFO - training batch 1751, loss: 0.758, 56032/60000 datapoints
2025-03-06 18:48:40,182 - INFO - training batch 1801, loss: 0.595, 57632/60000 datapoints
2025-03-06 18:48:40,375 - INFO - training batch 1851, loss: 0.604, 59232/60000 datapoints
2025-03-06 18:48:40,476 - INFO - validation batch 1, loss: 0.627, 32/10016 datapoints
2025-03-06 18:48:40,631 - INFO - validation batch 51, loss: 0.614, 1632/10016 datapoints
2025-03-06 18:48:40,783 - INFO - validation batch 101, loss: 0.597, 3232/10016 datapoints
2025-03-06 18:48:40,937 - INFO - validation batch 151, loss: 0.566, 4832/10016 datapoints
2025-03-06 18:48:41,093 - INFO - validation batch 201, loss: 0.850, 6432/10016 datapoints
2025-03-06 18:48:41,247 - INFO - validation batch 251, loss: 0.571, 8032/10016 datapoints
2025-03-06 18:48:41,401 - INFO - validation batch 301, loss: 0.460, 9632/10016 datapoints
2025-03-06 18:48:41,438 - INFO - Epoch 76/800 done.
2025-03-06 18:48:41,438 - INFO - Final validation performance:
Loss: 0.612, top-1 acc: 0.849top-5 acc: 0.849
2025-03-06 18:48:41,439 - INFO - Beginning epoch 77/800
2025-03-06 18:48:41,446 - INFO - training batch 1, loss: 0.457, 32/60000 datapoints
2025-03-06 18:48:41,660 - INFO - training batch 51, loss: 0.588, 1632/60000 datapoints
2025-03-06 18:48:41,855 - INFO - training batch 101, loss: 0.623, 3232/60000 datapoints
2025-03-06 18:48:42,050 - INFO - training batch 151, loss: 0.760, 4832/60000 datapoints
2025-03-06 18:48:42,252 - INFO - training batch 201, loss: 0.701, 6432/60000 datapoints
2025-03-06 18:48:42,452 - INFO - training batch 251, loss: 0.543, 8032/60000 datapoints
2025-03-06 18:48:42,672 - INFO - training batch 301, loss: 0.590, 9632/60000 datapoints
2025-03-06 18:48:42,864 - INFO - training batch 351, loss: 0.794, 11232/60000 datapoints
2025-03-06 18:48:43,059 - INFO - training batch 401, loss: 0.539, 12832/60000 datapoints
2025-03-06 18:48:43,257 - INFO - training batch 451, loss: 0.686, 14432/60000 datapoints
2025-03-06 18:48:43,452 - INFO - training batch 501, loss: 0.669, 16032/60000 datapoints
2025-03-06 18:48:43,651 - INFO - training batch 551, loss: 0.619, 17632/60000 datapoints
2025-03-06 18:48:43,849 - INFO - training batch 601, loss: 0.476, 19232/60000 datapoints
2025-03-06 18:48:44,044 - INFO - training batch 651, loss: 0.504, 20832/60000 datapoints
2025-03-06 18:48:44,238 - INFO - training batch 701, loss: 0.621, 22432/60000 datapoints
2025-03-06 18:48:44,436 - INFO - training batch 751, loss: 0.642, 24032/60000 datapoints
2025-03-06 18:48:44,635 - INFO - training batch 801, loss: 0.537, 25632/60000 datapoints
2025-03-06 18:48:44,836 - INFO - training batch 851, loss: 0.722, 27232/60000 datapoints
2025-03-06 18:48:45,035 - INFO - training batch 901, loss: 0.663, 28832/60000 datapoints
2025-03-06 18:48:45,230 - INFO - training batch 951, loss: 0.616, 30432/60000 datapoints
2025-03-06 18:48:45,425 - INFO - training batch 1001, loss: 0.649, 32032/60000 datapoints
2025-03-06 18:48:45,621 - INFO - training batch 1051, loss: 0.800, 33632/60000 datapoints
2025-03-06 18:48:45,815 - INFO - training batch 1101, loss: 0.642, 35232/60000 datapoints
2025-03-06 18:48:46,015 - INFO - training batch 1151, loss: 0.647, 36832/60000 datapoints
2025-03-06 18:48:46,212 - INFO - training batch 1201, loss: 0.398, 38432/60000 datapoints
2025-03-06 18:48:46,408 - INFO - training batch 1251, loss: 0.880, 40032/60000 datapoints
2025-03-06 18:48:46,606 - INFO - training batch 1301, loss: 0.653, 41632/60000 datapoints
2025-03-06 18:48:46,802 - INFO - training batch 1351, loss: 0.602, 43232/60000 datapoints
2025-03-06 18:48:46,996 - INFO - training batch 1401, loss: 0.745, 44832/60000 datapoints
2025-03-06 18:48:47,192 - INFO - training batch 1451, loss: 0.508, 46432/60000 datapoints
2025-03-06 18:48:47,383 - INFO - training batch 1501, loss: 0.776, 48032/60000 datapoints
2025-03-06 18:48:47,577 - INFO - training batch 1551, loss: 0.920, 49632/60000 datapoints
2025-03-06 18:48:47,773 - INFO - training batch 1601, loss: 0.713, 51232/60000 datapoints
2025-03-06 18:48:47,970 - INFO - training batch 1651, loss: 0.680, 52832/60000 datapoints
2025-03-06 18:48:48,165 - INFO - training batch 1701, loss: 0.347, 54432/60000 datapoints
2025-03-06 18:48:48,359 - INFO - training batch 1751, loss: 0.481, 56032/60000 datapoints
2025-03-06 18:48:48,554 - INFO - training batch 1801, loss: 0.670, 57632/60000 datapoints
2025-03-06 18:48:48,750 - INFO - training batch 1851, loss: 0.837, 59232/60000 datapoints
2025-03-06 18:48:48,851 - INFO - validation batch 1, loss: 0.583, 32/10016 datapoints
2025-03-06 18:48:49,002 - INFO - validation batch 51, loss: 0.813, 1632/10016 datapoints
2025-03-06 18:48:49,154 - INFO - validation batch 101, loss: 0.930, 3232/10016 datapoints
2025-03-06 18:48:49,308 - INFO - validation batch 151, loss: 0.633, 4832/10016 datapoints
2025-03-06 18:48:49,463 - INFO - validation batch 201, loss: 0.639, 6432/10016 datapoints
2025-03-06 18:48:49,620 - INFO - validation batch 251, loss: 0.723, 8032/10016 datapoints
2025-03-06 18:48:49,771 - INFO - validation batch 301, loss: 0.579, 9632/10016 datapoints
2025-03-06 18:48:49,808 - INFO - Epoch 77/800 done.
2025-03-06 18:48:49,809 - INFO - Final validation performance:
Loss: 0.700, top-1 acc: 0.850top-5 acc: 0.850
2025-03-06 18:48:49,809 - INFO - Beginning epoch 78/800
2025-03-06 18:48:49,815 - INFO - training batch 1, loss: 0.555, 32/60000 datapoints
2025-03-06 18:48:50,022 - INFO - training batch 51, loss: 0.779, 1632/60000 datapoints
2025-03-06 18:48:50,222 - INFO - training batch 101, loss: 0.686, 3232/60000 datapoints
2025-03-06 18:48:50,417 - INFO - training batch 151, loss: 0.519, 4832/60000 datapoints
2025-03-06 18:48:50,621 - INFO - training batch 201, loss: 0.590, 6432/60000 datapoints
2025-03-06 18:48:50,818 - INFO - training batch 251, loss: 0.478, 8032/60000 datapoints
2025-03-06 18:48:51,012 - INFO - training batch 301, loss: 0.633, 9632/60000 datapoints
2025-03-06 18:48:51,212 - INFO - training batch 351, loss: 0.608, 11232/60000 datapoints
2025-03-06 18:48:51,408 - INFO - training batch 401, loss: 0.539, 12832/60000 datapoints
2025-03-06 18:48:51,603 - INFO - training batch 451, loss: 0.490, 14432/60000 datapoints
2025-03-06 18:48:51,799 - INFO - training batch 501, loss: 0.641, 16032/60000 datapoints
2025-03-06 18:48:51,995 - INFO - training batch 551, loss: 0.536, 17632/60000 datapoints
2025-03-06 18:48:52,192 - INFO - training batch 601, loss: 0.645, 19232/60000 datapoints
2025-03-06 18:48:52,385 - INFO - training batch 651, loss: 0.769, 20832/60000 datapoints
2025-03-06 18:48:52,583 - INFO - training batch 701, loss: 0.674, 22432/60000 datapoints
2025-03-06 18:48:52,798 - INFO - training batch 751, loss: 0.525, 24032/60000 datapoints
2025-03-06 18:48:52,991 - INFO - training batch 801, loss: 0.486, 25632/60000 datapoints
2025-03-06 18:48:53,184 - INFO - training batch 851, loss: 0.679, 27232/60000 datapoints
2025-03-06 18:48:53,384 - INFO - training batch 901, loss: 0.576, 28832/60000 datapoints
2025-03-06 18:48:53,580 - INFO - training batch 951, loss: 0.687, 30432/60000 datapoints
2025-03-06 18:48:53,775 - INFO - training batch 1001, loss: 0.642, 32032/60000 datapoints
2025-03-06 18:48:53,970 - INFO - training batch 1051, loss: 0.700, 33632/60000 datapoints
2025-03-06 18:48:54,166 - INFO - training batch 1101, loss: 0.641, 35232/60000 datapoints
2025-03-06 18:48:54,360 - INFO - training batch 1151, loss: 0.714, 36832/60000 datapoints
2025-03-06 18:48:54,553 - INFO - training batch 1201, loss: 0.557, 38432/60000 datapoints
2025-03-06 18:48:54,750 - INFO - training batch 1251, loss: 0.542, 40032/60000 datapoints
2025-03-06 18:48:54,948 - INFO - training batch 1301, loss: 0.573, 41632/60000 datapoints
2025-03-06 18:48:55,144 - INFO - training batch 1351, loss: 0.677, 43232/60000 datapoints
2025-03-06 18:48:55,342 - INFO - training batch 1401, loss: 0.664, 44832/60000 datapoints
2025-03-06 18:48:55,536 - INFO - training batch 1451, loss: 0.529, 46432/60000 datapoints
2025-03-06 18:48:55,732 - INFO - training batch 1501, loss: 0.719, 48032/60000 datapoints
2025-03-06 18:48:55,928 - INFO - training batch 1551, loss: 0.606, 49632/60000 datapoints
2025-03-06 18:48:56,124 - INFO - training batch 1601, loss: 0.593, 51232/60000 datapoints
2025-03-06 18:48:56,328 - INFO - training batch 1651, loss: 0.645, 52832/60000 datapoints
2025-03-06 18:48:56,525 - INFO - training batch 1701, loss: 0.594, 54432/60000 datapoints
2025-03-06 18:48:56,721 - INFO - training batch 1751, loss: 0.689, 56032/60000 datapoints
2025-03-06 18:48:56,915 - INFO - training batch 1801, loss: 0.514, 57632/60000 datapoints
2025-03-06 18:48:57,110 - INFO - training batch 1851, loss: 0.791, 59232/60000 datapoints
2025-03-06 18:48:57,210 - INFO - validation batch 1, loss: 0.467, 32/10016 datapoints
2025-03-06 18:48:57,366 - INFO - validation batch 51, loss: 0.724, 1632/10016 datapoints
2025-03-06 18:48:57,519 - INFO - validation batch 101, loss: 0.736, 3232/10016 datapoints
2025-03-06 18:48:57,675 - INFO - validation batch 151, loss: 0.628, 4832/10016 datapoints
2025-03-06 18:48:57,829 - INFO - validation batch 201, loss: 0.589, 6432/10016 datapoints
2025-03-06 18:48:57,980 - INFO - validation batch 251, loss: 0.683, 8032/10016 datapoints
2025-03-06 18:48:58,132 - INFO - validation batch 301, loss: 0.641, 9632/10016 datapoints
2025-03-06 18:48:58,169 - INFO - Epoch 78/800 done.
2025-03-06 18:48:58,169 - INFO - Final validation performance:
Loss: 0.638, top-1 acc: 0.851top-5 acc: 0.851
2025-03-06 18:48:58,170 - INFO - Beginning epoch 79/800
2025-03-06 18:48:58,176 - INFO - training batch 1, loss: 0.505, 32/60000 datapoints
2025-03-06 18:48:58,371 - INFO - training batch 51, loss: 0.495, 1632/60000 datapoints
2025-03-06 18:48:58,574 - INFO - training batch 101, loss: 0.880, 3232/60000 datapoints
2025-03-06 18:48:58,777 - INFO - training batch 151, loss: 0.667, 4832/60000 datapoints
2025-03-06 18:48:58,973 - INFO - training batch 201, loss: 0.649, 6432/60000 datapoints
2025-03-06 18:48:59,175 - INFO - training batch 251, loss: 0.382, 8032/60000 datapoints
2025-03-06 18:48:59,379 - INFO - training batch 301, loss: 0.427, 9632/60000 datapoints
2025-03-06 18:48:59,578 - INFO - training batch 351, loss: 0.565, 11232/60000 datapoints
2025-03-06 18:48:59,778 - INFO - training batch 401, loss: 0.593, 12832/60000 datapoints
2025-03-06 18:48:59,973 - INFO - training batch 451, loss: 0.460, 14432/60000 datapoints
2025-03-06 18:49:00,168 - INFO - training batch 501, loss: 0.825, 16032/60000 datapoints
2025-03-06 18:49:00,362 - INFO - training batch 551, loss: 0.604, 17632/60000 datapoints
2025-03-06 18:49:00,555 - INFO - training batch 601, loss: 0.486, 19232/60000 datapoints
2025-03-06 18:49:00,751 - INFO - training batch 651, loss: 0.493, 20832/60000 datapoints
2025-03-06 18:49:00,945 - INFO - training batch 701, loss: 0.607, 22432/60000 datapoints
2025-03-06 18:49:01,136 - INFO - training batch 751, loss: 0.553, 24032/60000 datapoints
2025-03-06 18:49:01,330 - INFO - training batch 801, loss: 0.492, 25632/60000 datapoints
2025-03-06 18:49:01,526 - INFO - training batch 851, loss: 0.540, 27232/60000 datapoints
2025-03-06 18:49:01,724 - INFO - training batch 901, loss: 0.453, 28832/60000 datapoints
2025-03-06 18:49:01,917 - INFO - training batch 951, loss: 0.594, 30432/60000 datapoints
2025-03-06 18:49:02,111 - INFO - training batch 1001, loss: 0.629, 32032/60000 datapoints
2025-03-06 18:49:02,305 - INFO - training batch 1051, loss: 0.617, 33632/60000 datapoints
2025-03-06 18:49:02,497 - INFO - training batch 1101, loss: 0.438, 35232/60000 datapoints
2025-03-06 18:49:02,701 - INFO - training batch 1151, loss: 0.783, 36832/60000 datapoints
2025-03-06 18:49:02,907 - INFO - training batch 1201, loss: 0.495, 38432/60000 datapoints
2025-03-06 18:49:03,099 - INFO - training batch 1251, loss: 0.700, 40032/60000 datapoints
2025-03-06 18:49:03,296 - INFO - training batch 1301, loss: 0.525, 41632/60000 datapoints
2025-03-06 18:49:03,492 - INFO - training batch 1351, loss: 0.518, 43232/60000 datapoints
2025-03-06 18:49:03,686 - INFO - training batch 1401, loss: 0.668, 44832/60000 datapoints
2025-03-06 18:49:03,881 - INFO - training batch 1451, loss: 0.406, 46432/60000 datapoints
2025-03-06 18:49:04,073 - INFO - training batch 1501, loss: 0.696, 48032/60000 datapoints
2025-03-06 18:49:04,267 - INFO - training batch 1551, loss: 0.440, 49632/60000 datapoints
2025-03-06 18:49:04,459 - INFO - training batch 1601, loss: 0.581, 51232/60000 datapoints
2025-03-06 18:49:04,661 - INFO - training batch 1651, loss: 0.868, 52832/60000 datapoints
2025-03-06 18:49:04,856 - INFO - training batch 1701, loss: 0.525, 54432/60000 datapoints
2025-03-06 18:49:05,056 - INFO - training batch 1751, loss: 0.551, 56032/60000 datapoints
2025-03-06 18:49:05,253 - INFO - training batch 1801, loss: 0.896, 57632/60000 datapoints
2025-03-06 18:49:05,448 - INFO - training batch 1851, loss: 0.576, 59232/60000 datapoints
2025-03-06 18:49:05,548 - INFO - validation batch 1, loss: 0.536, 32/10016 datapoints
2025-03-06 18:49:05,701 - INFO - validation batch 51, loss: 0.628, 1632/10016 datapoints
2025-03-06 18:49:05,854 - INFO - validation batch 101, loss: 0.506, 3232/10016 datapoints
2025-03-06 18:49:06,008 - INFO - validation batch 151, loss: 0.711, 4832/10016 datapoints
2025-03-06 18:49:06,159 - INFO - validation batch 201, loss: 0.884, 6432/10016 datapoints
2025-03-06 18:49:06,314 - INFO - validation batch 251, loss: 0.721, 8032/10016 datapoints
2025-03-06 18:49:06,467 - INFO - validation batch 301, loss: 0.510, 9632/10016 datapoints
2025-03-06 18:49:06,506 - INFO - Epoch 79/800 done.
2025-03-06 18:49:06,506 - INFO - Final validation performance:
Loss: 0.642, top-1 acc: 0.853top-5 acc: 0.853
2025-03-06 18:49:06,507 - INFO - Beginning epoch 80/800
2025-03-06 18:49:06,513 - INFO - training batch 1, loss: 0.712, 32/60000 datapoints
2025-03-06 18:49:06,735 - INFO - training batch 51, loss: 0.812, 1632/60000 datapoints
2025-03-06 18:49:06,930 - INFO - training batch 101, loss: 0.651, 3232/60000 datapoints
2025-03-06 18:49:07,128 - INFO - training batch 151, loss: 0.502, 4832/60000 datapoints
2025-03-06 18:49:07,328 - INFO - training batch 201, loss: 0.602, 6432/60000 datapoints
2025-03-06 18:49:07,526 - INFO - training batch 251, loss: 0.672, 8032/60000 datapoints
2025-03-06 18:49:07,724 - INFO - training batch 301, loss: 0.659, 9632/60000 datapoints
2025-03-06 18:49:07,920 - INFO - training batch 351, loss: 0.551, 11232/60000 datapoints
2025-03-06 18:49:08,114 - INFO - training batch 401, loss: 0.552, 12832/60000 datapoints
2025-03-06 18:49:08,307 - INFO - training batch 451, loss: 0.640, 14432/60000 datapoints
2025-03-06 18:49:08,502 - INFO - training batch 501, loss: 0.382, 16032/60000 datapoints
2025-03-06 18:49:08,699 - INFO - training batch 551, loss: 0.443, 17632/60000 datapoints
2025-03-06 18:49:08,893 - INFO - training batch 601, loss: 0.672, 19232/60000 datapoints
2025-03-06 18:49:09,092 - INFO - training batch 651, loss: 0.589, 20832/60000 datapoints
2025-03-06 18:49:09,287 - INFO - training batch 701, loss: 0.498, 22432/60000 datapoints
2025-03-06 18:49:09,480 - INFO - training batch 751, loss: 0.659, 24032/60000 datapoints
2025-03-06 18:49:09,675 - INFO - training batch 801, loss: 0.930, 25632/60000 datapoints
2025-03-06 18:49:09,868 - INFO - training batch 851, loss: 0.713, 27232/60000 datapoints
2025-03-06 18:49:10,062 - INFO - training batch 901, loss: 0.471, 28832/60000 datapoints
2025-03-06 18:49:10,253 - INFO - training batch 951, loss: 0.628, 30432/60000 datapoints
2025-03-06 18:49:10,448 - INFO - training batch 1001, loss: 0.748, 32032/60000 datapoints
2025-03-06 18:49:10,645 - INFO - training batch 1051, loss: 0.462, 33632/60000 datapoints
2025-03-06 18:49:10,839 - INFO - training batch 1101, loss: 0.552, 35232/60000 datapoints
2025-03-06 18:49:11,033 - INFO - training batch 1151, loss: 0.422, 36832/60000 datapoints
2025-03-06 18:49:11,226 - INFO - training batch 1201, loss: 0.524, 38432/60000 datapoints
2025-03-06 18:49:11,421 - INFO - training batch 1251, loss: 0.382, 40032/60000 datapoints
2025-03-06 18:49:11,617 - INFO - training batch 1301, loss: 0.565, 41632/60000 datapoints
2025-03-06 18:49:11,819 - INFO - training batch 1351, loss: 0.743, 43232/60000 datapoints
2025-03-06 18:49:12,013 - INFO - training batch 1401, loss: 0.742, 44832/60000 datapoints
2025-03-06 18:49:12,207 - INFO - training batch 1451, loss: 0.714, 46432/60000 datapoints
2025-03-06 18:49:12,400 - INFO - training batch 1501, loss: 0.697, 48032/60000 datapoints
2025-03-06 18:49:12,595 - INFO - training batch 1551, loss: 0.677, 49632/60000 datapoints
2025-03-06 18:49:12,799 - INFO - training batch 1601, loss: 0.800, 51232/60000 datapoints
2025-03-06 18:49:12,999 - INFO - training batch 1651, loss: 0.577, 52832/60000 datapoints
2025-03-06 18:49:13,192 - INFO - training batch 1701, loss: 0.525, 54432/60000 datapoints
2025-03-06 18:49:13,392 - INFO - training batch 1751, loss: 0.596, 56032/60000 datapoints
2025-03-06 18:49:13,594 - INFO - training batch 1801, loss: 0.468, 57632/60000 datapoints
2025-03-06 18:49:13,797 - INFO - training batch 1851, loss: 0.552, 59232/60000 datapoints
2025-03-06 18:49:13,901 - INFO - validation batch 1, loss: 0.422, 32/10016 datapoints
2025-03-06 18:49:14,055 - INFO - validation batch 51, loss: 0.411, 1632/10016 datapoints
2025-03-06 18:49:14,206 - INFO - validation batch 101, loss: 0.385, 3232/10016 datapoints
2025-03-06 18:49:14,360 - INFO - validation batch 151, loss: 0.518, 4832/10016 datapoints
2025-03-06 18:49:14,512 - INFO - validation batch 201, loss: 0.422, 6432/10016 datapoints
2025-03-06 18:49:14,667 - INFO - validation batch 251, loss: 0.613, 8032/10016 datapoints
2025-03-06 18:49:14,845 - INFO - validation batch 301, loss: 0.480, 9632/10016 datapoints
2025-03-06 18:49:14,885 - INFO - Epoch 80/800 done.
2025-03-06 18:49:14,885 - INFO - Final validation performance:
Loss: 0.465, top-1 acc: 0.854top-5 acc: 0.854
2025-03-06 18:49:14,886 - INFO - Beginning epoch 81/800
2025-03-06 18:49:14,894 - INFO - training batch 1, loss: 0.549, 32/60000 datapoints
2025-03-06 18:49:15,152 - INFO - training batch 51, loss: 0.508, 1632/60000 datapoints
2025-03-06 18:49:15,356 - INFO - training batch 101, loss: 0.662, 3232/60000 datapoints
2025-03-06 18:49:15,547 - INFO - training batch 151, loss: 0.673, 4832/60000 datapoints
2025-03-06 18:49:15,749 - INFO - training batch 201, loss: 0.485, 6432/60000 datapoints
2025-03-06 18:49:15,946 - INFO - training batch 251, loss: 0.689, 8032/60000 datapoints
2025-03-06 18:49:16,142 - INFO - training batch 301, loss: 0.702, 9632/60000 datapoints
2025-03-06 18:49:16,343 - INFO - training batch 351, loss: 0.552, 11232/60000 datapoints
2025-03-06 18:49:16,536 - INFO - training batch 401, loss: 0.740, 12832/60000 datapoints
2025-03-06 18:49:16,733 - INFO - training batch 451, loss: 0.711, 14432/60000 datapoints
2025-03-06 18:49:16,928 - INFO - training batch 501, loss: 0.632, 16032/60000 datapoints
2025-03-06 18:49:17,123 - INFO - training batch 551, loss: 0.539, 17632/60000 datapoints
2025-03-06 18:49:17,320 - INFO - training batch 601, loss: 0.720, 19232/60000 datapoints
2025-03-06 18:49:17,520 - INFO - training batch 651, loss: 0.519, 20832/60000 datapoints
2025-03-06 18:49:17,715 - INFO - training batch 701, loss: 0.544, 22432/60000 datapoints
2025-03-06 18:49:17,909 - INFO - training batch 751, loss: 0.632, 24032/60000 datapoints
2025-03-06 18:49:18,102 - INFO - training batch 801, loss: 0.543, 25632/60000 datapoints
2025-03-06 18:49:18,294 - INFO - training batch 851, loss: 0.651, 27232/60000 datapoints
2025-03-06 18:49:18,491 - INFO - training batch 901, loss: 0.556, 28832/60000 datapoints
2025-03-06 18:49:18,685 - INFO - training batch 951, loss: 0.530, 30432/60000 datapoints
2025-03-06 18:49:18,878 - INFO - training batch 1001, loss: 0.402, 32032/60000 datapoints
2025-03-06 18:49:19,071 - INFO - training batch 1051, loss: 0.497, 33632/60000 datapoints
2025-03-06 18:49:19,264 - INFO - training batch 1101, loss: 0.664, 35232/60000 datapoints
2025-03-06 18:49:19,461 - INFO - training batch 1151, loss: 0.554, 36832/60000 datapoints
2025-03-06 18:49:19,660 - INFO - training batch 1201, loss: 0.711, 38432/60000 datapoints
2025-03-06 18:49:19,859 - INFO - training batch 1251, loss: 0.677, 40032/60000 datapoints
2025-03-06 18:49:20,062 - INFO - training batch 1301, loss: 0.482, 41632/60000 datapoints
2025-03-06 18:49:20,270 - INFO - training batch 1351, loss: 0.754, 43232/60000 datapoints
2025-03-06 18:49:20,473 - INFO - training batch 1401, loss: 0.563, 44832/60000 datapoints
2025-03-06 18:49:20,671 - INFO - training batch 1451, loss: 0.552, 46432/60000 datapoints
2025-03-06 18:49:20,864 - INFO - training batch 1501, loss: 0.478, 48032/60000 datapoints
2025-03-06 18:49:21,057 - INFO - training batch 1551, loss: 0.675, 49632/60000 datapoints
2025-03-06 18:49:21,248 - INFO - training batch 1601, loss: 0.699, 51232/60000 datapoints
2025-03-06 18:49:21,444 - INFO - training batch 1651, loss: 0.574, 52832/60000 datapoints
2025-03-06 18:49:21,638 - INFO - training batch 1701, loss: 0.493, 54432/60000 datapoints
2025-03-06 18:49:21,831 - INFO - training batch 1751, loss: 0.475, 56032/60000 datapoints
2025-03-06 18:49:22,025 - INFO - training batch 1801, loss: 0.549, 57632/60000 datapoints
2025-03-06 18:49:22,219 - INFO - training batch 1851, loss: 0.497, 59232/60000 datapoints
2025-03-06 18:49:22,321 - INFO - validation batch 1, loss: 0.619, 32/10016 datapoints
2025-03-06 18:49:22,477 - INFO - validation batch 51, loss: 0.724, 1632/10016 datapoints
2025-03-06 18:49:22,632 - INFO - validation batch 101, loss: 0.746, 3232/10016 datapoints
2025-03-06 18:49:22,783 - INFO - validation batch 151, loss: 0.529, 4832/10016 datapoints
2025-03-06 18:49:22,956 - INFO - validation batch 201, loss: 0.566, 6432/10016 datapoints
2025-03-06 18:49:23,109 - INFO - validation batch 251, loss: 0.430, 8032/10016 datapoints
2025-03-06 18:49:23,262 - INFO - validation batch 301, loss: 0.875, 9632/10016 datapoints
2025-03-06 18:49:23,301 - INFO - Epoch 81/800 done.
2025-03-06 18:49:23,302 - INFO - Final validation performance:
Loss: 0.641, top-1 acc: 0.855top-5 acc: 0.855
2025-03-06 18:49:23,302 - INFO - Beginning epoch 82/800
2025-03-06 18:49:23,311 - INFO - training batch 1, loss: 0.490, 32/60000 datapoints
2025-03-06 18:49:23,508 - INFO - training batch 51, loss: 0.839, 1632/60000 datapoints
2025-03-06 18:49:23,718 - INFO - training batch 101, loss: 0.618, 3232/60000 datapoints
2025-03-06 18:49:23,913 - INFO - training batch 151, loss: 0.624, 4832/60000 datapoints
2025-03-06 18:49:24,108 - INFO - training batch 201, loss: 0.701, 6432/60000 datapoints
2025-03-06 18:49:24,306 - INFO - training batch 251, loss: 0.507, 8032/60000 datapoints
2025-03-06 18:49:24,502 - INFO - training batch 301, loss: 0.562, 9632/60000 datapoints
2025-03-06 18:49:24,701 - INFO - training batch 351, loss: 0.592, 11232/60000 datapoints
2025-03-06 18:49:24,897 - INFO - training batch 401, loss: 0.655, 12832/60000 datapoints
2025-03-06 18:49:25,092 - INFO - training batch 451, loss: 0.666, 14432/60000 datapoints
2025-03-06 18:49:25,284 - INFO - training batch 501, loss: 0.424, 16032/60000 datapoints
2025-03-06 18:49:25,480 - INFO - training batch 551, loss: 0.826, 17632/60000 datapoints
2025-03-06 18:49:25,674 - INFO - training batch 601, loss: 0.738, 19232/60000 datapoints
2025-03-06 18:49:25,868 - INFO - training batch 651, loss: 0.624, 20832/60000 datapoints
2025-03-06 18:49:26,067 - INFO - training batch 701, loss: 0.527, 22432/60000 datapoints
2025-03-06 18:49:26,261 - INFO - training batch 751, loss: 0.614, 24032/60000 datapoints
2025-03-06 18:49:26,455 - INFO - training batch 801, loss: 0.490, 25632/60000 datapoints
2025-03-06 18:49:26,650 - INFO - training batch 851, loss: 0.705, 27232/60000 datapoints
2025-03-06 18:49:26,842 - INFO - training batch 901, loss: 0.571, 28832/60000 datapoints
2025-03-06 18:49:27,041 - INFO - training batch 951, loss: 0.539, 30432/60000 datapoints
2025-03-06 18:49:27,233 - INFO - training batch 1001, loss: 0.599, 32032/60000 datapoints
2025-03-06 18:49:27,431 - INFO - training batch 1051, loss: 0.615, 33632/60000 datapoints
2025-03-06 18:49:27,626 - INFO - training batch 1101, loss: 0.349, 35232/60000 datapoints
2025-03-06 18:49:27,818 - INFO - training batch 1151, loss: 0.867, 36832/60000 datapoints
2025-03-06 18:49:28,023 - INFO - training batch 1201, loss: 0.756, 38432/60000 datapoints
2025-03-06 18:49:28,263 - INFO - training batch 1251, loss: 0.836, 40032/60000 datapoints
2025-03-06 18:49:28,460 - INFO - training batch 1301, loss: 0.902, 41632/60000 datapoints
2025-03-06 18:49:28,656 - INFO - training batch 1351, loss: 0.589, 43232/60000 datapoints
2025-03-06 18:49:28,849 - INFO - training batch 1401, loss: 0.780, 44832/60000 datapoints
2025-03-06 18:49:29,043 - INFO - training batch 1451, loss: 0.760, 46432/60000 datapoints
2025-03-06 18:49:29,237 - INFO - training batch 1501, loss: 0.583, 48032/60000 datapoints
2025-03-06 18:49:29,432 - INFO - training batch 1551, loss: 0.755, 49632/60000 datapoints
2025-03-06 18:49:29,627 - INFO - training batch 1601, loss: 0.650, 51232/60000 datapoints
2025-03-06 18:49:29,825 - INFO - training batch 1651, loss: 0.477, 52832/60000 datapoints
2025-03-06 18:49:30,020 - INFO - training batch 1701, loss: 0.457, 54432/60000 datapoints
2025-03-06 18:49:30,214 - INFO - training batch 1751, loss: 0.736, 56032/60000 datapoints
2025-03-06 18:49:30,407 - INFO - training batch 1801, loss: 0.458, 57632/60000 datapoints
2025-03-06 18:49:30,600 - INFO - training batch 1851, loss: 0.632, 59232/60000 datapoints
2025-03-06 18:49:30,705 - INFO - validation batch 1, loss: 0.643, 32/10016 datapoints
2025-03-06 18:49:30,857 - INFO - validation batch 51, loss: 0.585, 1632/10016 datapoints
2025-03-06 18:49:31,007 - INFO - validation batch 101, loss: 0.622, 3232/10016 datapoints
2025-03-06 18:49:31,163 - INFO - validation batch 151, loss: 0.562, 4832/10016 datapoints
2025-03-06 18:49:31,315 - INFO - validation batch 201, loss: 0.451, 6432/10016 datapoints
2025-03-06 18:49:31,470 - INFO - validation batch 251, loss: 0.572, 8032/10016 datapoints
2025-03-06 18:49:31,624 - INFO - validation batch 301, loss: 0.654, 9632/10016 datapoints
2025-03-06 18:49:31,663 - INFO - Epoch 82/800 done.
2025-03-06 18:49:31,663 - INFO - Final validation performance:
Loss: 0.584, top-1 acc: 0.856top-5 acc: 0.856
2025-03-06 18:49:31,663 - INFO - Beginning epoch 83/800
2025-03-06 18:49:31,670 - INFO - training batch 1, loss: 0.502, 32/60000 datapoints
2025-03-06 18:49:31,863 - INFO - training batch 51, loss: 0.590, 1632/60000 datapoints
2025-03-06 18:49:32,076 - INFO - training batch 101, loss: 0.431, 3232/60000 datapoints
2025-03-06 18:49:32,270 - INFO - training batch 151, loss: 0.765, 4832/60000 datapoints
2025-03-06 18:49:32,475 - INFO - training batch 201, loss: 0.515, 6432/60000 datapoints
2025-03-06 18:49:32,678 - INFO - training batch 251, loss: 0.445, 8032/60000 datapoints
2025-03-06 18:49:32,875 - INFO - training batch 301, loss: 0.504, 9632/60000 datapoints
2025-03-06 18:49:33,089 - INFO - training batch 351, loss: 0.403, 11232/60000 datapoints
2025-03-06 18:49:33,282 - INFO - training batch 401, loss: 0.552, 12832/60000 datapoints
2025-03-06 18:49:33,481 - INFO - training batch 451, loss: 0.641, 14432/60000 datapoints
2025-03-06 18:49:33,676 - INFO - training batch 501, loss: 0.839, 16032/60000 datapoints
2025-03-06 18:49:33,871 - INFO - training batch 551, loss: 0.346, 17632/60000 datapoints
2025-03-06 18:49:34,064 - INFO - training batch 601, loss: 0.598, 19232/60000 datapoints
2025-03-06 18:49:34,258 - INFO - training batch 651, loss: 0.652, 20832/60000 datapoints
2025-03-06 18:49:34,452 - INFO - training batch 701, loss: 0.605, 22432/60000 datapoints
2025-03-06 18:49:34,651 - INFO - training batch 751, loss: 0.559, 24032/60000 datapoints
2025-03-06 18:49:34,848 - INFO - training batch 801, loss: 0.607, 25632/60000 datapoints
2025-03-06 18:49:35,047 - INFO - training batch 851, loss: 0.667, 27232/60000 datapoints
2025-03-06 18:49:35,240 - INFO - training batch 901, loss: 0.587, 28832/60000 datapoints
2025-03-06 18:49:35,436 - INFO - training batch 951, loss: 0.743, 30432/60000 datapoints
2025-03-06 18:49:35,631 - INFO - training batch 1001, loss: 0.627, 32032/60000 datapoints
2025-03-06 18:49:35,825 - INFO - training batch 1051, loss: 0.577, 33632/60000 datapoints
2025-03-06 18:49:36,023 - INFO - training batch 1101, loss: 0.459, 35232/60000 datapoints
2025-03-06 18:49:36,219 - INFO - training batch 1151, loss: 0.643, 36832/60000 datapoints
2025-03-06 18:49:36,412 - INFO - training batch 1201, loss: 0.559, 38432/60000 datapoints
2025-03-06 18:49:36,608 - INFO - training batch 1251, loss: 0.417, 40032/60000 datapoints
2025-03-06 18:49:36,804 - INFO - training batch 1301, loss: 0.968, 41632/60000 datapoints
2025-03-06 18:49:36,998 - INFO - training batch 1351, loss: 0.670, 43232/60000 datapoints
2025-03-06 18:49:37,191 - INFO - training batch 1401, loss: 0.576, 44832/60000 datapoints
2025-03-06 18:49:37,387 - INFO - training batch 1451, loss: 0.580, 46432/60000 datapoints
2025-03-06 18:49:37,581 - INFO - training batch 1501, loss: 0.691, 48032/60000 datapoints
2025-03-06 18:49:37,787 - INFO - training batch 1551, loss: 0.457, 49632/60000 datapoints
2025-03-06 18:49:37,978 - INFO - training batch 1601, loss: 0.472, 51232/60000 datapoints
2025-03-06 18:49:38,173 - INFO - training batch 1651, loss: 0.600, 52832/60000 datapoints
2025-03-06 18:49:38,365 - INFO - training batch 1701, loss: 0.654, 54432/60000 datapoints
2025-03-06 18:49:38,560 - INFO - training batch 1751, loss: 0.346, 56032/60000 datapoints
2025-03-06 18:49:38,755 - INFO - training batch 1801, loss: 0.551, 57632/60000 datapoints
2025-03-06 18:49:38,947 - INFO - training batch 1851, loss: 0.428, 59232/60000 datapoints
2025-03-06 18:49:39,056 - INFO - validation batch 1, loss: 0.477, 32/10016 datapoints
2025-03-06 18:49:39,209 - INFO - validation batch 51, loss: 0.562, 1632/10016 datapoints
2025-03-06 18:49:39,362 - INFO - validation batch 101, loss: 0.568, 3232/10016 datapoints
2025-03-06 18:49:39,518 - INFO - validation batch 151, loss: 0.546, 4832/10016 datapoints
2025-03-06 18:49:39,672 - INFO - validation batch 201, loss: 0.583, 6432/10016 datapoints
2025-03-06 18:49:39,825 - INFO - validation batch 251, loss: 0.513, 8032/10016 datapoints
2025-03-06 18:49:39,977 - INFO - validation batch 301, loss: 0.646, 9632/10016 datapoints
2025-03-06 18:49:40,014 - INFO - Epoch 83/800 done.
2025-03-06 18:49:40,014 - INFO - Final validation performance:
Loss: 0.557, top-1 acc: 0.856top-5 acc: 0.856
2025-03-06 18:49:40,014 - INFO - Beginning epoch 84/800
2025-03-06 18:49:40,020 - INFO - training batch 1, loss: 0.552, 32/60000 datapoints
2025-03-06 18:49:40,217 - INFO - training batch 51, loss: 0.549, 1632/60000 datapoints
2025-03-06 18:49:40,416 - INFO - training batch 101, loss: 0.626, 3232/60000 datapoints
2025-03-06 18:49:40,622 - INFO - training batch 151, loss: 0.505, 4832/60000 datapoints
2025-03-06 18:49:40,817 - INFO - training batch 201, loss: 0.587, 6432/60000 datapoints
2025-03-06 18:49:41,011 - INFO - training batch 251, loss: 0.461, 8032/60000 datapoints
2025-03-06 18:49:41,207 - INFO - training batch 301, loss: 0.658, 9632/60000 datapoints
2025-03-06 18:49:41,404 - INFO - training batch 351, loss: 0.604, 11232/60000 datapoints
2025-03-06 18:49:41,602 - INFO - training batch 401, loss: 0.510, 12832/60000 datapoints
2025-03-06 18:49:41,802 - INFO - training batch 451, loss: 0.611, 14432/60000 datapoints
2025-03-06 18:49:41,997 - INFO - training batch 501, loss: 0.679, 16032/60000 datapoints
2025-03-06 18:49:42,195 - INFO - training batch 551, loss: 0.443, 17632/60000 datapoints
2025-03-06 18:49:42,393 - INFO - training batch 601, loss: 0.716, 19232/60000 datapoints
2025-03-06 18:49:42,597 - INFO - training batch 651, loss: 0.441, 20832/60000 datapoints
2025-03-06 18:49:42,793 - INFO - training batch 701, loss: 0.521, 22432/60000 datapoints
2025-03-06 18:49:42,995 - INFO - training batch 751, loss: 0.779, 24032/60000 datapoints
2025-03-06 18:49:43,209 - INFO - training batch 801, loss: 0.623, 25632/60000 datapoints
2025-03-06 18:49:43,405 - INFO - training batch 851, loss: 0.828, 27232/60000 datapoints
2025-03-06 18:49:43,602 - INFO - training batch 901, loss: 0.535, 28832/60000 datapoints
2025-03-06 18:49:43,803 - INFO - training batch 951, loss: 0.673, 30432/60000 datapoints
2025-03-06 18:49:43,998 - INFO - training batch 1001, loss: 0.561, 32032/60000 datapoints
2025-03-06 18:49:44,191 - INFO - training batch 1051, loss: 0.727, 33632/60000 datapoints
2025-03-06 18:49:44,385 - INFO - training batch 1101, loss: 0.540, 35232/60000 datapoints
2025-03-06 18:49:44,580 - INFO - training batch 1151, loss: 0.539, 36832/60000 datapoints
2025-03-06 18:49:44,776 - INFO - training batch 1201, loss: 0.695, 38432/60000 datapoints
2025-03-06 18:49:44,976 - INFO - training batch 1251, loss: 0.573, 40032/60000 datapoints
2025-03-06 18:49:45,171 - INFO - training batch 1301, loss: 0.513, 41632/60000 datapoints
2025-03-06 18:49:45,367 - INFO - training batch 1351, loss: 0.459, 43232/60000 datapoints
2025-03-06 18:49:45,561 - INFO - training batch 1401, loss: 0.816, 44832/60000 datapoints
2025-03-06 18:49:45,756 - INFO - training batch 1451, loss: 0.616, 46432/60000 datapoints
2025-03-06 18:49:45,951 - INFO - training batch 1501, loss: 0.673, 48032/60000 datapoints
2025-03-06 18:49:46,152 - INFO - training batch 1551, loss: 0.954, 49632/60000 datapoints
2025-03-06 18:49:46,346 - INFO - training batch 1601, loss: 0.543, 51232/60000 datapoints
2025-03-06 18:49:46,538 - INFO - training batch 1651, loss: 0.989, 52832/60000 datapoints
2025-03-06 18:49:46,741 - INFO - training batch 1701, loss: 0.376, 54432/60000 datapoints
2025-03-06 18:49:46,932 - INFO - training batch 1751, loss: 0.674, 56032/60000 datapoints
2025-03-06 18:49:47,125 - INFO - training batch 1801, loss: 0.945, 57632/60000 datapoints
2025-03-06 18:49:47,319 - INFO - training batch 1851, loss: 0.546, 59232/60000 datapoints
2025-03-06 18:49:47,420 - INFO - validation batch 1, loss: 0.563, 32/10016 datapoints
2025-03-06 18:49:47,571 - INFO - validation batch 51, loss: 0.446, 1632/10016 datapoints
2025-03-06 18:49:47,727 - INFO - validation batch 101, loss: 0.534, 3232/10016 datapoints
2025-03-06 18:49:47,878 - INFO - validation batch 151, loss: 0.551, 4832/10016 datapoints
2025-03-06 18:49:48,032 - INFO - validation batch 201, loss: 0.700, 6432/10016 datapoints
2025-03-06 18:49:48,184 - INFO - validation batch 251, loss: 0.700, 8032/10016 datapoints
2025-03-06 18:49:48,335 - INFO - validation batch 301, loss: 0.471, 9632/10016 datapoints
2025-03-06 18:49:48,374 - INFO - Epoch 84/800 done.
2025-03-06 18:49:48,374 - INFO - Final validation performance:
Loss: 0.567, top-1 acc: 0.858top-5 acc: 0.858
2025-03-06 18:49:48,374 - INFO - Beginning epoch 85/800
2025-03-06 18:49:48,381 - INFO - training batch 1, loss: 0.810, 32/60000 datapoints
2025-03-06 18:49:48,575 - INFO - training batch 51, loss: 0.481, 1632/60000 datapoints
2025-03-06 18:49:48,772 - INFO - training batch 101, loss: 0.542, 3232/60000 datapoints
2025-03-06 18:49:48,975 - INFO - training batch 151, loss: 0.772, 4832/60000 datapoints
2025-03-06 18:49:49,172 - INFO - training batch 201, loss: 1.132, 6432/60000 datapoints
2025-03-06 18:49:49,368 - INFO - training batch 251, loss: 0.729, 8032/60000 datapoints
2025-03-06 18:49:49,564 - INFO - training batch 301, loss: 0.765, 9632/60000 datapoints
2025-03-06 18:49:49,769 - INFO - training batch 351, loss: 0.624, 11232/60000 datapoints
2025-03-06 18:49:49,967 - INFO - training batch 401, loss: 0.577, 12832/60000 datapoints
2025-03-06 18:49:50,164 - INFO - training batch 451, loss: 0.922, 14432/60000 datapoints
2025-03-06 18:49:50,363 - INFO - training batch 501, loss: 0.528, 16032/60000 datapoints
2025-03-06 18:49:50,559 - INFO - training batch 551, loss: 0.396, 17632/60000 datapoints
2025-03-06 18:49:50,754 - INFO - training batch 601, loss: 0.526, 19232/60000 datapoints
2025-03-06 18:49:50,947 - INFO - training batch 651, loss: 0.988, 20832/60000 datapoints
2025-03-06 18:49:51,140 - INFO - training batch 701, loss: 0.461, 22432/60000 datapoints
2025-03-06 18:49:51,336 - INFO - training batch 751, loss: 0.744, 24032/60000 datapoints
2025-03-06 18:49:51,533 - INFO - training batch 801, loss: 0.788, 25632/60000 datapoints
2025-03-06 18:49:51,730 - INFO - training batch 851, loss: 0.621, 27232/60000 datapoints
2025-03-06 18:49:51,924 - INFO - training batch 901, loss: 0.453, 28832/60000 datapoints
2025-03-06 18:49:52,118 - INFO - training batch 951, loss: 0.484, 30432/60000 datapoints
2025-03-06 18:49:52,312 - INFO - training batch 1001, loss: 0.494, 32032/60000 datapoints
2025-03-06 18:49:52,505 - INFO - training batch 1051, loss: 0.522, 33632/60000 datapoints
2025-03-06 18:49:52,702 - INFO - training batch 1101, loss: 0.489, 35232/60000 datapoints
2025-03-06 18:49:52,893 - INFO - training batch 1151, loss: 0.633, 36832/60000 datapoints
2025-03-06 18:49:53,086 - INFO - training batch 1201, loss: 0.779, 38432/60000 datapoints
2025-03-06 18:49:53,296 - INFO - training batch 1251, loss: 0.552, 40032/60000 datapoints
2025-03-06 18:49:53,493 - INFO - training batch 1301, loss: 0.817, 41632/60000 datapoints
2025-03-06 18:49:53,687 - INFO - training batch 1351, loss: 0.615, 43232/60000 datapoints
2025-03-06 18:49:53,881 - INFO - training batch 1401, loss: 0.574, 44832/60000 datapoints
2025-03-06 18:49:54,073 - INFO - training batch 1451, loss: 0.617, 46432/60000 datapoints
2025-03-06 18:49:54,266 - INFO - training batch 1501, loss: 0.610, 48032/60000 datapoints
2025-03-06 18:49:54,458 - INFO - training batch 1551, loss: 0.449, 49632/60000 datapoints
2025-03-06 18:49:54,652 - INFO - training batch 1601, loss: 0.581, 51232/60000 datapoints
2025-03-06 18:49:54,847 - INFO - training batch 1651, loss: 0.371, 52832/60000 datapoints
2025-03-06 18:49:55,044 - INFO - training batch 1701, loss: 0.532, 54432/60000 datapoints
2025-03-06 18:49:55,240 - INFO - training batch 1751, loss: 0.444, 56032/60000 datapoints
2025-03-06 18:49:55,434 - INFO - training batch 1801, loss: 0.531, 57632/60000 datapoints
2025-03-06 18:49:55,630 - INFO - training batch 1851, loss: 0.640, 59232/60000 datapoints
2025-03-06 18:49:55,732 - INFO - validation batch 1, loss: 0.497, 32/10016 datapoints
2025-03-06 18:49:55,882 - INFO - validation batch 51, loss: 0.508, 1632/10016 datapoints
2025-03-06 18:49:56,033 - INFO - validation batch 101, loss: 0.499, 3232/10016 datapoints
2025-03-06 18:49:56,190 - INFO - validation batch 151, loss: 0.764, 4832/10016 datapoints
2025-03-06 18:49:56,345 - INFO - validation batch 201, loss: 0.531, 6432/10016 datapoints
2025-03-06 18:49:56,499 - INFO - validation batch 251, loss: 0.491, 8032/10016 datapoints
2025-03-06 18:49:56,653 - INFO - validation batch 301, loss: 0.553, 9632/10016 datapoints
2025-03-06 18:49:56,690 - INFO - Epoch 85/800 done.
2025-03-06 18:49:56,690 - INFO - Final validation performance:
Loss: 0.549, top-1 acc: 0.858top-5 acc: 0.858
2025-03-06 18:49:56,690 - INFO - Beginning epoch 86/800
2025-03-06 18:49:56,697 - INFO - training batch 1, loss: 0.541, 32/60000 datapoints
2025-03-06 18:49:56,910 - INFO - training batch 51, loss: 0.569, 1632/60000 datapoints
2025-03-06 18:49:57,105 - INFO - training batch 101, loss: 0.794, 3232/60000 datapoints
2025-03-06 18:49:57,298 - INFO - training batch 151, loss: 0.493, 4832/60000 datapoints
2025-03-06 18:49:57,500 - INFO - training batch 201, loss: 0.433, 6432/60000 datapoints
2025-03-06 18:49:57,700 - INFO - training batch 251, loss: 0.534, 8032/60000 datapoints
2025-03-06 18:49:57,897 - INFO - training batch 301, loss: 0.662, 9632/60000 datapoints
2025-03-06 18:49:58,095 - INFO - training batch 351, loss: 0.738, 11232/60000 datapoints
2025-03-06 18:49:58,287 - INFO - training batch 401, loss: 0.734, 12832/60000 datapoints
2025-03-06 18:49:58,477 - INFO - training batch 451, loss: 0.639, 14432/60000 datapoints
2025-03-06 18:49:58,677 - INFO - training batch 501, loss: 0.600, 16032/60000 datapoints
2025-03-06 18:49:58,872 - INFO - training batch 551, loss: 0.567, 17632/60000 datapoints
2025-03-06 18:49:59,065 - INFO - training batch 601, loss: 0.455, 19232/60000 datapoints
2025-03-06 18:49:59,259 - INFO - training batch 651, loss: 0.817, 20832/60000 datapoints
2025-03-06 18:49:59,451 - INFO - training batch 701, loss: 0.653, 22432/60000 datapoints
2025-03-06 18:49:59,647 - INFO - training batch 751, loss: 0.676, 24032/60000 datapoints
2025-03-06 18:49:59,846 - INFO - training batch 801, loss: 0.311, 25632/60000 datapoints
2025-03-06 18:50:00,046 - INFO - training batch 851, loss: 0.454, 27232/60000 datapoints
2025-03-06 18:50:00,241 - INFO - training batch 901, loss: 0.644, 28832/60000 datapoints
2025-03-06 18:50:00,437 - INFO - training batch 951, loss: 0.474, 30432/60000 datapoints
2025-03-06 18:50:00,633 - INFO - training batch 1001, loss: 0.464, 32032/60000 datapoints
2025-03-06 18:50:00,827 - INFO - training batch 1051, loss: 0.669, 33632/60000 datapoints
2025-03-06 18:50:01,049 - INFO - training batch 1101, loss: 0.608, 35232/60000 datapoints
2025-03-06 18:50:01,243 - INFO - training batch 1151, loss: 0.564, 36832/60000 datapoints
2025-03-06 18:50:01,437 - INFO - training batch 1201, loss: 0.489, 38432/60000 datapoints
2025-03-06 18:50:01,636 - INFO - training batch 1251, loss: 0.554, 40032/60000 datapoints
2025-03-06 18:50:01,837 - INFO - training batch 1301, loss: 0.455, 41632/60000 datapoints
2025-03-06 18:50:02,067 - INFO - training batch 1351, loss: 0.574, 43232/60000 datapoints
2025-03-06 18:50:02,260 - INFO - training batch 1401, loss: 0.621, 44832/60000 datapoints
2025-03-06 18:50:02,453 - INFO - training batch 1451, loss: 0.565, 46432/60000 datapoints
2025-03-06 18:50:02,655 - INFO - training batch 1501, loss: 0.521, 48032/60000 datapoints
2025-03-06 18:50:02,852 - INFO - training batch 1551, loss: 0.584, 49632/60000 datapoints
2025-03-06 18:50:03,047 - INFO - training batch 1601, loss: 0.586, 51232/60000 datapoints
2025-03-06 18:50:03,260 - INFO - training batch 1651, loss: 0.348, 52832/60000 datapoints
2025-03-06 18:50:03,455 - INFO - training batch 1701, loss: 0.879, 54432/60000 datapoints
2025-03-06 18:50:03,653 - INFO - training batch 1751, loss: 0.534, 56032/60000 datapoints
2025-03-06 18:50:03,848 - INFO - training batch 1801, loss: 0.540, 57632/60000 datapoints
2025-03-06 18:50:04,043 - INFO - training batch 1851, loss: 0.670, 59232/60000 datapoints
2025-03-06 18:50:04,142 - INFO - validation batch 1, loss: 0.566, 32/10016 datapoints
2025-03-06 18:50:04,297 - INFO - validation batch 51, loss: 0.381, 1632/10016 datapoints
2025-03-06 18:50:04,451 - INFO - validation batch 101, loss: 0.741, 3232/10016 datapoints
2025-03-06 18:50:04,607 - INFO - validation batch 151, loss: 0.620, 4832/10016 datapoints
2025-03-06 18:50:04,759 - INFO - validation batch 201, loss: 0.702, 6432/10016 datapoints
2025-03-06 18:50:04,917 - INFO - validation batch 251, loss: 0.503, 8032/10016 datapoints
2025-03-06 18:50:05,069 - INFO - validation batch 301, loss: 0.796, 9632/10016 datapoints
2025-03-06 18:50:05,107 - INFO - Epoch 86/800 done.
2025-03-06 18:50:05,107 - INFO - Final validation performance:
Loss: 0.615, top-1 acc: 0.859top-5 acc: 0.859
2025-03-06 18:50:05,107 - INFO - Beginning epoch 87/800
2025-03-06 18:50:05,113 - INFO - training batch 1, loss: 0.464, 32/60000 datapoints
2025-03-06 18:50:05,316 - INFO - training batch 51, loss: 0.592, 1632/60000 datapoints
2025-03-06 18:50:05,520 - INFO - training batch 101, loss: 0.549, 3232/60000 datapoints
2025-03-06 18:50:05,714 - INFO - training batch 151, loss: 0.437, 4832/60000 datapoints
2025-03-06 18:50:05,914 - INFO - training batch 201, loss: 0.575, 6432/60000 datapoints
2025-03-06 18:50:06,110 - INFO - training batch 251, loss: 0.510, 8032/60000 datapoints
2025-03-06 18:50:06,317 - INFO - training batch 301, loss: 0.609, 9632/60000 datapoints
2025-03-06 18:50:06,514 - INFO - training batch 351, loss: 0.512, 11232/60000 datapoints
2025-03-06 18:50:06,710 - INFO - training batch 401, loss: 0.819, 12832/60000 datapoints
2025-03-06 18:50:06,903 - INFO - training batch 451, loss: 0.523, 14432/60000 datapoints
2025-03-06 18:50:07,097 - INFO - training batch 501, loss: 0.756, 16032/60000 datapoints
2025-03-06 18:50:07,291 - INFO - training batch 551, loss: 0.390, 17632/60000 datapoints
2025-03-06 18:50:07,488 - INFO - training batch 601, loss: 0.302, 19232/60000 datapoints
2025-03-06 18:50:07,687 - INFO - training batch 651, loss: 0.572, 20832/60000 datapoints
2025-03-06 18:50:07,880 - INFO - training batch 701, loss: 0.492, 22432/60000 datapoints
2025-03-06 18:50:08,074 - INFO - training batch 751, loss: 0.853, 24032/60000 datapoints
2025-03-06 18:50:08,265 - INFO - training batch 801, loss: 0.516, 25632/60000 datapoints
2025-03-06 18:50:08,458 - INFO - training batch 851, loss: 0.468, 27232/60000 datapoints
2025-03-06 18:50:08,652 - INFO - training batch 901, loss: 0.590, 28832/60000 datapoints
2025-03-06 18:50:08,843 - INFO - training batch 951, loss: 0.491, 30432/60000 datapoints
2025-03-06 18:50:09,042 - INFO - training batch 1001, loss: 0.556, 32032/60000 datapoints
2025-03-06 18:50:09,235 - INFO - training batch 1051, loss: 0.700, 33632/60000 datapoints
2025-03-06 18:50:09,427 - INFO - training batch 1101, loss: 0.830, 35232/60000 datapoints
2025-03-06 18:50:09,625 - INFO - training batch 1151, loss: 0.466, 36832/60000 datapoints
2025-03-06 18:50:09,819 - INFO - training batch 1201, loss: 0.536, 38432/60000 datapoints
2025-03-06 18:50:10,013 - INFO - training batch 1251, loss: 0.622, 40032/60000 datapoints
2025-03-06 18:50:10,207 - INFO - training batch 1301, loss: 0.642, 41632/60000 datapoints
2025-03-06 18:50:10,400 - INFO - training batch 1351, loss: 0.459, 43232/60000 datapoints
2025-03-06 18:50:10,595 - INFO - training batch 1401, loss: 0.595, 44832/60000 datapoints
2025-03-06 18:50:10,791 - INFO - training batch 1451, loss: 0.356, 46432/60000 datapoints
2025-03-06 18:50:10,986 - INFO - training batch 1501, loss: 0.553, 48032/60000 datapoints
2025-03-06 18:50:11,179 - INFO - training batch 1551, loss: 0.708, 49632/60000 datapoints
2025-03-06 18:50:11,373 - INFO - training batch 1601, loss: 0.668, 51232/60000 datapoints
2025-03-06 18:50:11,569 - INFO - training batch 1651, loss: 0.435, 52832/60000 datapoints
2025-03-06 18:50:11,763 - INFO - training batch 1701, loss: 0.686, 54432/60000 datapoints
2025-03-06 18:50:11,959 - INFO - training batch 1751, loss: 0.556, 56032/60000 datapoints
2025-03-06 18:50:12,152 - INFO - training batch 1801, loss: 0.424, 57632/60000 datapoints
2025-03-06 18:50:12,346 - INFO - training batch 1851, loss: 0.772, 59232/60000 datapoints
2025-03-06 18:50:12,447 - INFO - validation batch 1, loss: 0.761, 32/10016 datapoints
2025-03-06 18:50:12,601 - INFO - validation batch 51, loss: 0.645, 1632/10016 datapoints
2025-03-06 18:50:12,755 - INFO - validation batch 101, loss: 0.572, 3232/10016 datapoints
2025-03-06 18:50:12,907 - INFO - validation batch 151, loss: 0.589, 4832/10016 datapoints
2025-03-06 18:50:13,064 - INFO - validation batch 201, loss: 0.586, 6432/10016 datapoints
2025-03-06 18:50:13,216 - INFO - validation batch 251, loss: 0.716, 8032/10016 datapoints
2025-03-06 18:50:13,387 - INFO - validation batch 301, loss: 0.540, 9632/10016 datapoints
2025-03-06 18:50:13,425 - INFO - Epoch 87/800 done.
2025-03-06 18:50:13,425 - INFO - Final validation performance:
Loss: 0.630, top-1 acc: 0.861top-5 acc: 0.861
2025-03-06 18:50:13,426 - INFO - Beginning epoch 88/800
2025-03-06 18:50:13,432 - INFO - training batch 1, loss: 0.635, 32/60000 datapoints
2025-03-06 18:50:13,634 - INFO - training batch 51, loss: 0.471, 1632/60000 datapoints
2025-03-06 18:50:13,826 - INFO - training batch 101, loss: 0.445, 3232/60000 datapoints
2025-03-06 18:50:14,037 - INFO - training batch 151, loss: 0.537, 4832/60000 datapoints
2025-03-06 18:50:14,233 - INFO - training batch 201, loss: 0.367, 6432/60000 datapoints
2025-03-06 18:50:14,429 - INFO - training batch 251, loss: 0.574, 8032/60000 datapoints
2025-03-06 18:50:14,631 - INFO - training batch 301, loss: 0.468, 9632/60000 datapoints
2025-03-06 18:50:14,828 - INFO - training batch 351, loss: 0.523, 11232/60000 datapoints
2025-03-06 18:50:15,033 - INFO - training batch 401, loss: 0.608, 12832/60000 datapoints
2025-03-06 18:50:15,228 - INFO - training batch 451, loss: 0.505, 14432/60000 datapoints
2025-03-06 18:50:15,421 - INFO - training batch 501, loss: 0.335, 16032/60000 datapoints
2025-03-06 18:50:15,618 - INFO - training batch 551, loss: 0.649, 17632/60000 datapoints
2025-03-06 18:50:15,810 - INFO - training batch 601, loss: 0.431, 19232/60000 datapoints
2025-03-06 18:50:16,005 - INFO - training batch 651, loss: 0.425, 20832/60000 datapoints
2025-03-06 18:50:16,204 - INFO - training batch 701, loss: 0.641, 22432/60000 datapoints
2025-03-06 18:50:16,401 - INFO - training batch 751, loss: 0.430, 24032/60000 datapoints
2025-03-06 18:50:16,597 - INFO - training batch 801, loss: 0.580, 25632/60000 datapoints
2025-03-06 18:50:16,794 - INFO - training batch 851, loss: 0.665, 27232/60000 datapoints
2025-03-06 18:50:16,993 - INFO - training batch 901, loss: 0.691, 28832/60000 datapoints
2025-03-06 18:50:17,189 - INFO - training batch 951, loss: 0.467, 30432/60000 datapoints
2025-03-06 18:50:17,383 - INFO - training batch 1001, loss: 0.599, 32032/60000 datapoints
2025-03-06 18:50:17,577 - INFO - training batch 1051, loss: 0.692, 33632/60000 datapoints
2025-03-06 18:50:17,773 - INFO - training batch 1101, loss: 0.611, 35232/60000 datapoints
2025-03-06 18:50:17,976 - INFO - training batch 1151, loss: 0.397, 36832/60000 datapoints
2025-03-06 18:50:18,172 - INFO - training batch 1201, loss: 0.431, 38432/60000 datapoints
2025-03-06 18:50:18,366 - INFO - training batch 1251, loss: 0.468, 40032/60000 datapoints
2025-03-06 18:50:18,560 - INFO - training batch 1301, loss: 0.525, 41632/60000 datapoints
2025-03-06 18:50:18,754 - INFO - training batch 1351, loss: 0.645, 43232/60000 datapoints
2025-03-06 18:50:18,948 - INFO - training batch 1401, loss: 0.598, 44832/60000 datapoints
2025-03-06 18:50:19,141 - INFO - training batch 1451, loss: 0.406, 46432/60000 datapoints
2025-03-06 18:50:19,332 - INFO - training batch 1501, loss: 0.455, 48032/60000 datapoints
2025-03-06 18:50:19,534 - INFO - training batch 1551, loss: 0.634, 49632/60000 datapoints
2025-03-06 18:50:19,731 - INFO - training batch 1601, loss: 0.552, 51232/60000 datapoints
2025-03-06 18:50:19,925 - INFO - training batch 1651, loss: 0.625, 52832/60000 datapoints
2025-03-06 18:50:20,120 - INFO - training batch 1701, loss: 0.482, 54432/60000 datapoints
2025-03-06 18:50:20,315 - INFO - training batch 1751, loss: 0.524, 56032/60000 datapoints
2025-03-06 18:50:20,509 - INFO - training batch 1801, loss: 0.541, 57632/60000 datapoints
2025-03-06 18:50:20,703 - INFO - training batch 1851, loss: 0.417, 59232/60000 datapoints
2025-03-06 18:50:20,804 - INFO - validation batch 1, loss: 0.339, 32/10016 datapoints
2025-03-06 18:50:20,956 - INFO - validation batch 51, loss: 0.652, 1632/10016 datapoints
2025-03-06 18:50:21,110 - INFO - validation batch 101, loss: 0.605, 3232/10016 datapoints
2025-03-06 18:50:21,259 - INFO - validation batch 151, loss: 0.583, 4832/10016 datapoints
2025-03-06 18:50:21,410 - INFO - validation batch 201, loss: 0.644, 6432/10016 datapoints
2025-03-06 18:50:21,566 - INFO - validation batch 251, loss: 0.723, 8032/10016 datapoints
2025-03-06 18:50:21,720 - INFO - validation batch 301, loss: 0.607, 9632/10016 datapoints
2025-03-06 18:50:21,756 - INFO - Epoch 88/800 done.
2025-03-06 18:50:21,756 - INFO - Final validation performance:
Loss: 0.593, top-1 acc: 0.862top-5 acc: 0.862
2025-03-06 18:50:21,757 - INFO - Beginning epoch 89/800
2025-03-06 18:50:21,763 - INFO - training batch 1, loss: 0.509, 32/60000 datapoints
2025-03-06 18:50:21,972 - INFO - training batch 51, loss: 0.536, 1632/60000 datapoints
2025-03-06 18:50:22,166 - INFO - training batch 101, loss: 0.289, 3232/60000 datapoints
2025-03-06 18:50:22,358 - INFO - training batch 151, loss: 0.482, 4832/60000 datapoints
2025-03-06 18:50:22,559 - INFO - training batch 201, loss: 0.486, 6432/60000 datapoints
2025-03-06 18:50:22,764 - INFO - training batch 251, loss: 0.387, 8032/60000 datapoints
2025-03-06 18:50:22,960 - INFO - training batch 301, loss: 0.704, 9632/60000 datapoints
2025-03-06 18:50:23,155 - INFO - training batch 351, loss: 0.788, 11232/60000 datapoints
2025-03-06 18:50:23,352 - INFO - training batch 401, loss: 0.430, 12832/60000 datapoints
2025-03-06 18:50:23,570 - INFO - training batch 451, loss: 0.656, 14432/60000 datapoints
2025-03-06 18:50:23,764 - INFO - training batch 501, loss: 0.577, 16032/60000 datapoints
2025-03-06 18:50:23,961 - INFO - training batch 551, loss: 0.424, 17632/60000 datapoints
2025-03-06 18:50:24,158 - INFO - training batch 601, loss: 0.540, 19232/60000 datapoints
2025-03-06 18:50:24,352 - INFO - training batch 651, loss: 0.466, 20832/60000 datapoints
2025-03-06 18:50:24,549 - INFO - training batch 701, loss: 0.431, 22432/60000 datapoints
2025-03-06 18:50:24,743 - INFO - training batch 751, loss: 0.731, 24032/60000 datapoints
2025-03-06 18:50:24,940 - INFO - training batch 801, loss: 0.471, 25632/60000 datapoints
2025-03-06 18:50:25,135 - INFO - training batch 851, loss: 0.545, 27232/60000 datapoints
2025-03-06 18:50:25,332 - INFO - training batch 901, loss: 0.734, 28832/60000 datapoints
2025-03-06 18:50:25,524 - INFO - training batch 951, loss: 0.489, 30432/60000 datapoints
2025-03-06 18:50:25,725 - INFO - training batch 1001, loss: 0.708, 32032/60000 datapoints
2025-03-06 18:50:25,917 - INFO - training batch 1051, loss: 0.566, 33632/60000 datapoints
2025-03-06 18:50:26,109 - INFO - training batch 1101, loss: 0.709, 35232/60000 datapoints
2025-03-06 18:50:26,308 - INFO - training batch 1151, loss: 0.837, 36832/60000 datapoints
2025-03-06 18:50:26,501 - INFO - training batch 1201, loss: 0.453, 38432/60000 datapoints
2025-03-06 18:50:26,694 - INFO - training batch 1251, loss: 0.656, 40032/60000 datapoints
2025-03-06 18:50:26,887 - INFO - training batch 1301, loss: 0.634, 41632/60000 datapoints
2025-03-06 18:50:27,081 - INFO - training batch 1351, loss: 0.566, 43232/60000 datapoints
2025-03-06 18:50:27,274 - INFO - training batch 1401, loss: 0.733, 44832/60000 datapoints
2025-03-06 18:50:27,467 - INFO - training batch 1451, loss: 0.697, 46432/60000 datapoints
2025-03-06 18:50:27,665 - INFO - training batch 1501, loss: 0.592, 48032/60000 datapoints
2025-03-06 18:50:27,857 - INFO - training batch 1551, loss: 0.692, 49632/60000 datapoints
2025-03-06 18:50:28,056 - INFO - training batch 1601, loss: 0.567, 51232/60000 datapoints
2025-03-06 18:50:28,255 - INFO - training batch 1651, loss: 0.628, 52832/60000 datapoints
2025-03-06 18:50:28,454 - INFO - training batch 1701, loss: 0.603, 54432/60000 datapoints
2025-03-06 18:50:28,654 - INFO - training batch 1751, loss: 0.797, 56032/60000 datapoints
2025-03-06 18:50:28,846 - INFO - training batch 1801, loss: 0.473, 57632/60000 datapoints
2025-03-06 18:50:29,039 - INFO - training batch 1851, loss: 0.533, 59232/60000 datapoints
2025-03-06 18:50:29,140 - INFO - validation batch 1, loss: 0.719, 32/10016 datapoints
2025-03-06 18:50:29,294 - INFO - validation batch 51, loss: 0.506, 1632/10016 datapoints
2025-03-06 18:50:29,445 - INFO - validation batch 101, loss: 0.578, 3232/10016 datapoints
2025-03-06 18:50:29,599 - INFO - validation batch 151, loss: 0.545, 4832/10016 datapoints
2025-03-06 18:50:29,753 - INFO - validation batch 201, loss: 0.459, 6432/10016 datapoints
2025-03-06 18:50:29,904 - INFO - validation batch 251, loss: 0.510, 8032/10016 datapoints
2025-03-06 18:50:30,062 - INFO - validation batch 301, loss: 0.621, 9632/10016 datapoints
2025-03-06 18:50:30,102 - INFO - Epoch 89/800 done.
2025-03-06 18:50:30,102 - INFO - Final validation performance:
Loss: 0.563, top-1 acc: 0.863top-5 acc: 0.863
2025-03-06 18:50:30,103 - INFO - Beginning epoch 90/800
2025-03-06 18:50:30,110 - INFO - training batch 1, loss: 0.600, 32/60000 datapoints
2025-03-06 18:50:30,307 - INFO - training batch 51, loss: 0.511, 1632/60000 datapoints
2025-03-06 18:50:30,501 - INFO - training batch 101, loss: 0.680, 3232/60000 datapoints
2025-03-06 18:50:30,704 - INFO - training batch 151, loss: 0.771, 4832/60000 datapoints
2025-03-06 18:50:30,901 - INFO - training batch 201, loss: 0.523, 6432/60000 datapoints
2025-03-06 18:50:31,104 - INFO - training batch 251, loss: 0.466, 8032/60000 datapoints
2025-03-06 18:50:31,298 - INFO - training batch 301, loss: 0.464, 9632/60000 datapoints
2025-03-06 18:50:31,497 - INFO - training batch 351, loss: 0.504, 11232/60000 datapoints
2025-03-06 18:50:31,697 - INFO - training batch 401, loss: 0.444, 12832/60000 datapoints
2025-03-06 18:50:31,894 - INFO - training batch 451, loss: 0.450, 14432/60000 datapoints
2025-03-06 18:50:32,088 - INFO - training batch 501, loss: 0.562, 16032/60000 datapoints
2025-03-06 18:50:32,285 - INFO - training batch 551, loss: 0.449, 17632/60000 datapoints
2025-03-06 18:50:32,480 - INFO - training batch 601, loss: 0.628, 19232/60000 datapoints
2025-03-06 18:50:32,676 - INFO - training batch 651, loss: 0.500, 20832/60000 datapoints
2025-03-06 18:50:32,869 - INFO - training batch 701, loss: 0.665, 22432/60000 datapoints
2025-03-06 18:50:33,063 - INFO - training batch 751, loss: 0.660, 24032/60000 datapoints
2025-03-06 18:50:33,257 - INFO - training batch 801, loss: 0.441, 25632/60000 datapoints
2025-03-06 18:50:33,460 - INFO - training batch 851, loss: 0.866, 27232/60000 datapoints
2025-03-06 18:50:33,678 - INFO - training batch 901, loss: 0.701, 28832/60000 datapoints
2025-03-06 18:50:33,872 - INFO - training batch 951, loss: 0.690, 30432/60000 datapoints
2025-03-06 18:50:34,066 - INFO - training batch 1001, loss: 0.672, 32032/60000 datapoints
2025-03-06 18:50:34,259 - INFO - training batch 1051, loss: 0.628, 33632/60000 datapoints
2025-03-06 18:50:34,454 - INFO - training batch 1101, loss: 0.400, 35232/60000 datapoints
2025-03-06 18:50:34,651 - INFO - training batch 1151, loss: 0.589, 36832/60000 datapoints
2025-03-06 18:50:34,845 - INFO - training batch 1201, loss: 0.505, 38432/60000 datapoints
2025-03-06 18:50:35,040 - INFO - training batch 1251, loss: 0.599, 40032/60000 datapoints
2025-03-06 18:50:35,237 - INFO - training batch 1301, loss: 0.552, 41632/60000 datapoints
2025-03-06 18:50:35,429 - INFO - training batch 1351, loss: 0.476, 43232/60000 datapoints
2025-03-06 18:50:35,625 - INFO - training batch 1401, loss: 0.799, 44832/60000 datapoints
2025-03-06 18:50:35,820 - INFO - training batch 1451, loss: 0.705, 46432/60000 datapoints
2025-03-06 18:50:36,012 - INFO - training batch 1501, loss: 0.497, 48032/60000 datapoints
2025-03-06 18:50:36,211 - INFO - training batch 1551, loss: 0.524, 49632/60000 datapoints
2025-03-06 18:50:36,402 - INFO - training batch 1601, loss: 0.578, 51232/60000 datapoints
2025-03-06 18:50:36,595 - INFO - training batch 1651, loss: 0.292, 52832/60000 datapoints
2025-03-06 18:50:36,792 - INFO - training batch 1701, loss: 0.696, 54432/60000 datapoints
2025-03-06 18:50:36,993 - INFO - training batch 1751, loss: 0.437, 56032/60000 datapoints
2025-03-06 18:50:37,193 - INFO - training batch 1801, loss: 0.565, 57632/60000 datapoints
2025-03-06 18:50:37,388 - INFO - training batch 1851, loss: 0.332, 59232/60000 datapoints
2025-03-06 18:50:37,488 - INFO - validation batch 1, loss: 0.381, 32/10016 datapoints
2025-03-06 18:50:37,656 - INFO - validation batch 51, loss: 0.532, 1632/10016 datapoints
2025-03-06 18:50:37,809 - INFO - validation batch 101, loss: 0.560, 3232/10016 datapoints
2025-03-06 18:50:37,961 - INFO - validation batch 151, loss: 0.505, 4832/10016 datapoints
2025-03-06 18:50:38,112 - INFO - validation batch 201, loss: 0.419, 6432/10016 datapoints
2025-03-06 18:50:38,266 - INFO - validation batch 251, loss: 0.477, 8032/10016 datapoints
2025-03-06 18:50:38,416 - INFO - validation batch 301, loss: 0.479, 9632/10016 datapoints
2025-03-06 18:50:38,455 - INFO - Epoch 90/800 done.
2025-03-06 18:50:38,455 - INFO - Final validation performance:
Loss: 0.479, top-1 acc: 0.864top-5 acc: 0.864
2025-03-06 18:50:38,456 - INFO - Beginning epoch 91/800
2025-03-06 18:50:38,462 - INFO - training batch 1, loss: 0.518, 32/60000 datapoints
2025-03-06 18:50:38,672 - INFO - training batch 51, loss: 0.317, 1632/60000 datapoints
2025-03-06 18:50:38,866 - INFO - training batch 101, loss: 0.635, 3232/60000 datapoints
2025-03-06 18:50:39,060 - INFO - training batch 151, loss: 0.504, 4832/60000 datapoints
2025-03-06 18:50:39,264 - INFO - training batch 201, loss: 0.791, 6432/60000 datapoints
2025-03-06 18:50:39,462 - INFO - training batch 251, loss: 0.637, 8032/60000 datapoints
2025-03-06 18:50:39,662 - INFO - training batch 301, loss: 0.522, 9632/60000 datapoints
2025-03-06 18:50:39,859 - INFO - training batch 351, loss: 0.499, 11232/60000 datapoints
2025-03-06 18:50:40,055 - INFO - training batch 401, loss: 0.459, 12832/60000 datapoints
2025-03-06 18:50:40,250 - INFO - training batch 451, loss: 0.405, 14432/60000 datapoints
2025-03-06 18:50:40,444 - INFO - training batch 501, loss: 0.516, 16032/60000 datapoints
2025-03-06 18:50:40,641 - INFO - training batch 551, loss: 0.715, 17632/60000 datapoints
2025-03-06 18:50:40,835 - INFO - training batch 601, loss: 0.531, 19232/60000 datapoints
2025-03-06 18:50:41,028 - INFO - training batch 651, loss: 0.512, 20832/60000 datapoints
2025-03-06 18:50:41,221 - INFO - training batch 701, loss: 0.427, 22432/60000 datapoints
2025-03-06 18:50:41,413 - INFO - training batch 751, loss: 0.565, 24032/60000 datapoints
2025-03-06 18:50:41,610 - INFO - training batch 801, loss: 0.618, 25632/60000 datapoints
2025-03-06 18:50:41,805 - INFO - training batch 851, loss: 0.464, 27232/60000 datapoints
2025-03-06 18:50:41,998 - INFO - training batch 901, loss: 0.484, 28832/60000 datapoints
2025-03-06 18:50:42,192 - INFO - training batch 951, loss: 0.657, 30432/60000 datapoints
2025-03-06 18:50:42,387 - INFO - training batch 1001, loss: 0.400, 32032/60000 datapoints
2025-03-06 18:50:42,582 - INFO - training batch 1051, loss: 0.455, 33632/60000 datapoints
2025-03-06 18:50:42,778 - INFO - training batch 1101, loss: 0.571, 35232/60000 datapoints
2025-03-06 18:50:42,979 - INFO - training batch 1151, loss: 0.732, 36832/60000 datapoints
2025-03-06 18:50:43,174 - INFO - training batch 1201, loss: 0.412, 38432/60000 datapoints
2025-03-06 18:50:43,368 - INFO - training batch 1251, loss: 0.904, 40032/60000 datapoints
2025-03-06 18:50:43,574 - INFO - training batch 1301, loss: 0.585, 41632/60000 datapoints
2025-03-06 18:50:43,789 - INFO - training batch 1351, loss: 0.697, 43232/60000 datapoints
2025-03-06 18:50:43,984 - INFO - training batch 1401, loss: 0.521, 44832/60000 datapoints
2025-03-06 18:50:44,181 - INFO - training batch 1451, loss: 1.011, 46432/60000 datapoints
2025-03-06 18:50:44,375 - INFO - training batch 1501, loss: 0.628, 48032/60000 datapoints
2025-03-06 18:50:44,568 - INFO - training batch 1551, loss: 0.658, 49632/60000 datapoints
2025-03-06 18:50:44,763 - INFO - training batch 1601, loss: 0.497, 51232/60000 datapoints
2025-03-06 18:50:44,962 - INFO - training batch 1651, loss: 0.613, 52832/60000 datapoints
2025-03-06 18:50:45,155 - INFO - training batch 1701, loss: 0.630, 54432/60000 datapoints
2025-03-06 18:50:45,349 - INFO - training batch 1751, loss: 0.333, 56032/60000 datapoints
2025-03-06 18:50:45,544 - INFO - training batch 1801, loss: 0.440, 57632/60000 datapoints
2025-03-06 18:50:45,742 - INFO - training batch 1851, loss: 0.656, 59232/60000 datapoints
2025-03-06 18:50:45,844 - INFO - validation batch 1, loss: 0.720, 32/10016 datapoints
2025-03-06 18:50:45,995 - INFO - validation batch 51, loss: 0.670, 1632/10016 datapoints
2025-03-06 18:50:46,149 - INFO - validation batch 101, loss: 0.494, 3232/10016 datapoints
2025-03-06 18:50:46,308 - INFO - validation batch 151, loss: 0.626, 4832/10016 datapoints
2025-03-06 18:50:46,459 - INFO - validation batch 201, loss: 0.655, 6432/10016 datapoints
2025-03-06 18:50:46,614 - INFO - validation batch 251, loss: 0.551, 8032/10016 datapoints
2025-03-06 18:50:46,767 - INFO - validation batch 301, loss: 0.701, 9632/10016 datapoints
2025-03-06 18:50:46,804 - INFO - Epoch 91/800 done.
2025-03-06 18:50:46,805 - INFO - Final validation performance:
Loss: 0.631, top-1 acc: 0.864top-5 acc: 0.864
2025-03-06 18:50:46,805 - INFO - Beginning epoch 92/800
2025-03-06 18:50:46,812 - INFO - training batch 1, loss: 0.687, 32/60000 datapoints
2025-03-06 18:50:47,017 - INFO - training batch 51, loss: 0.483, 1632/60000 datapoints
2025-03-06 18:50:47,213 - INFO - training batch 101, loss: 0.575, 3232/60000 datapoints
2025-03-06 18:50:47,412 - INFO - training batch 151, loss: 0.501, 4832/60000 datapoints
2025-03-06 18:50:47,608 - INFO - training batch 201, loss: 0.422, 6432/60000 datapoints
2025-03-06 18:50:47,808 - INFO - training batch 251, loss: 0.610, 8032/60000 datapoints
2025-03-06 18:50:48,007 - INFO - training batch 301, loss: 0.531, 9632/60000 datapoints
2025-03-06 18:50:48,202 - INFO - training batch 351, loss: 0.333, 11232/60000 datapoints
2025-03-06 18:50:48,396 - INFO - training batch 401, loss: 0.593, 12832/60000 datapoints
2025-03-06 18:50:48,591 - INFO - training batch 451, loss: 0.808, 14432/60000 datapoints
2025-03-06 18:50:48,803 - INFO - training batch 501, loss: 0.856, 16032/60000 datapoints
2025-03-06 18:50:48,995 - INFO - training batch 551, loss: 0.600, 17632/60000 datapoints
2025-03-06 18:50:49,187 - INFO - training batch 601, loss: 0.617, 19232/60000 datapoints
2025-03-06 18:50:49,385 - INFO - training batch 651, loss: 0.416, 20832/60000 datapoints
2025-03-06 18:50:49,579 - INFO - training batch 701, loss: 0.526, 22432/60000 datapoints
2025-03-06 18:50:49,779 - INFO - training batch 751, loss: 0.520, 24032/60000 datapoints
2025-03-06 18:50:49,975 - INFO - training batch 801, loss: 0.520, 25632/60000 datapoints
2025-03-06 18:50:50,168 - INFO - training batch 851, loss: 0.491, 27232/60000 datapoints
2025-03-06 18:50:50,378 - INFO - training batch 901, loss: 0.563, 28832/60000 datapoints
2025-03-06 18:50:50,570 - INFO - training batch 951, loss: 0.452, 30432/60000 datapoints
2025-03-06 18:50:50,764 - INFO - training batch 1001, loss: 0.762, 32032/60000 datapoints
2025-03-06 18:50:50,959 - INFO - training batch 1051, loss: 0.731, 33632/60000 datapoints
2025-03-06 18:50:51,152 - INFO - training batch 1101, loss: 0.513, 35232/60000 datapoints
2025-03-06 18:50:51,348 - INFO - training batch 1151, loss: 0.436, 36832/60000 datapoints
2025-03-06 18:50:51,540 - INFO - training batch 1201, loss: 0.535, 38432/60000 datapoints
2025-03-06 18:50:51,737 - INFO - training batch 1251, loss: 0.566, 40032/60000 datapoints
2025-03-06 18:50:51,931 - INFO - training batch 1301, loss: 0.511, 41632/60000 datapoints
2025-03-06 18:50:52,124 - INFO - training batch 1351, loss: 0.821, 43232/60000 datapoints
2025-03-06 18:50:52,317 - INFO - training batch 1401, loss: 0.657, 44832/60000 datapoints
2025-03-06 18:50:52,509 - INFO - training batch 1451, loss: 0.494, 46432/60000 datapoints
2025-03-06 18:50:52,709 - INFO - training batch 1501, loss: 0.664, 48032/60000 datapoints
2025-03-06 18:50:52,903 - INFO - training batch 1551, loss: 0.480, 49632/60000 datapoints
2025-03-06 18:50:53,096 - INFO - training batch 1601, loss: 0.797, 51232/60000 datapoints
2025-03-06 18:50:53,288 - INFO - training batch 1651, loss: 0.491, 52832/60000 datapoints
2025-03-06 18:50:53,483 - INFO - training batch 1701, loss: 0.359, 54432/60000 datapoints
2025-03-06 18:50:53,694 - INFO - training batch 1751, loss: 0.483, 56032/60000 datapoints
2025-03-06 18:50:53,892 - INFO - training batch 1801, loss: 0.939, 57632/60000 datapoints
2025-03-06 18:50:54,085 - INFO - training batch 1851, loss: 0.638, 59232/60000 datapoints
2025-03-06 18:50:54,184 - INFO - validation batch 1, loss: 0.722, 32/10016 datapoints
2025-03-06 18:50:54,338 - INFO - validation batch 51, loss: 0.540, 1632/10016 datapoints
2025-03-06 18:50:54,493 - INFO - validation batch 101, loss: 0.786, 3232/10016 datapoints
2025-03-06 18:50:54,646 - INFO - validation batch 151, loss: 0.553, 4832/10016 datapoints
2025-03-06 18:50:54,796 - INFO - validation batch 201, loss: 0.456, 6432/10016 datapoints
2025-03-06 18:50:54,951 - INFO - validation batch 251, loss: 0.515, 8032/10016 datapoints
2025-03-06 18:50:55,102 - INFO - validation batch 301, loss: 0.840, 9632/10016 datapoints
2025-03-06 18:50:55,138 - INFO - Epoch 92/800 done.
2025-03-06 18:50:55,138 - INFO - Final validation performance:
Loss: 0.630, top-1 acc: 0.865top-5 acc: 0.865
2025-03-06 18:50:55,139 - INFO - Beginning epoch 93/800
2025-03-06 18:50:55,147 - INFO - training batch 1, loss: 0.462, 32/60000 datapoints
2025-03-06 18:50:55,348 - INFO - training batch 51, loss: 0.607, 1632/60000 datapoints
2025-03-06 18:50:55,549 - INFO - training batch 101, loss: 0.342, 3232/60000 datapoints
2025-03-06 18:50:55,757 - INFO - training batch 151, loss: 0.592, 4832/60000 datapoints
2025-03-06 18:50:55,956 - INFO - training batch 201, loss: 0.614, 6432/60000 datapoints
2025-03-06 18:50:56,149 - INFO - training batch 251, loss: 0.563, 8032/60000 datapoints
2025-03-06 18:50:56,349 - INFO - training batch 301, loss: 0.849, 9632/60000 datapoints
2025-03-06 18:50:56,547 - INFO - training batch 351, loss: 0.531, 11232/60000 datapoints
2025-03-06 18:50:56,744 - INFO - training batch 401, loss: 0.713, 12832/60000 datapoints
2025-03-06 18:50:56,940 - INFO - training batch 451, loss: 0.691, 14432/60000 datapoints
2025-03-06 18:50:57,132 - INFO - training batch 501, loss: 0.668, 16032/60000 datapoints
2025-03-06 18:50:57,328 - INFO - training batch 551, loss: 0.543, 17632/60000 datapoints
2025-03-06 18:50:57,522 - INFO - training batch 601, loss: 0.561, 19232/60000 datapoints
2025-03-06 18:50:57,719 - INFO - training batch 651, loss: 0.687, 20832/60000 datapoints
2025-03-06 18:50:57,912 - INFO - training batch 701, loss: 0.494, 22432/60000 datapoints
2025-03-06 18:50:58,103 - INFO - training batch 751, loss: 0.652, 24032/60000 datapoints
2025-03-06 18:50:58,298 - INFO - training batch 801, loss: 0.610, 25632/60000 datapoints
2025-03-06 18:50:58,494 - INFO - training batch 851, loss: 0.579, 27232/60000 datapoints
2025-03-06 18:50:58,695 - INFO - training batch 901, loss: 0.540, 28832/60000 datapoints
2025-03-06 18:50:58,899 - INFO - training batch 951, loss: 0.698, 30432/60000 datapoints
2025-03-06 18:50:59,097 - INFO - training batch 1001, loss: 0.560, 32032/60000 datapoints
2025-03-06 18:50:59,292 - INFO - training batch 1051, loss: 0.660, 33632/60000 datapoints
2025-03-06 18:50:59,485 - INFO - training batch 1101, loss: 0.413, 35232/60000 datapoints
2025-03-06 18:50:59,684 - INFO - training batch 1151, loss: 0.606, 36832/60000 datapoints
2025-03-06 18:50:59,878 - INFO - training batch 1201, loss: 0.847, 38432/60000 datapoints
2025-03-06 18:51:00,078 - INFO - training batch 1251, loss: 0.665, 40032/60000 datapoints
2025-03-06 18:51:00,274 - INFO - training batch 1301, loss: 0.453, 41632/60000 datapoints
2025-03-06 18:51:00,470 - INFO - training batch 1351, loss: 0.508, 43232/60000 datapoints
2025-03-06 18:51:00,665 - INFO - training batch 1401, loss: 0.541, 44832/60000 datapoints
2025-03-06 18:51:00,858 - INFO - training batch 1451, loss: 0.697, 46432/60000 datapoints
2025-03-06 18:51:01,053 - INFO - training batch 1501, loss: 0.636, 48032/60000 datapoints
2025-03-06 18:51:01,245 - INFO - training batch 1551, loss: 0.658, 49632/60000 datapoints
2025-03-06 18:51:01,440 - INFO - training batch 1601, loss: 0.667, 51232/60000 datapoints
2025-03-06 18:51:01,634 - INFO - training batch 1651, loss: 0.445, 52832/60000 datapoints
2025-03-06 18:51:01,829 - INFO - training batch 1701, loss: 0.362, 54432/60000 datapoints
2025-03-06 18:51:02,025 - INFO - training batch 1751, loss: 0.550, 56032/60000 datapoints
2025-03-06 18:51:02,220 - INFO - training batch 1801, loss: 0.626, 57632/60000 datapoints
2025-03-06 18:51:02,414 - INFO - training batch 1851, loss: 0.459, 59232/60000 datapoints
2025-03-06 18:51:02,514 - INFO - validation batch 1, loss: 0.558, 32/10016 datapoints
2025-03-06 18:51:02,667 - INFO - validation batch 51, loss: 0.449, 1632/10016 datapoints
2025-03-06 18:51:02,820 - INFO - validation batch 101, loss: 0.526, 3232/10016 datapoints
2025-03-06 18:51:02,973 - INFO - validation batch 151, loss: 0.391, 4832/10016 datapoints
2025-03-06 18:51:03,126 - INFO - validation batch 201, loss: 0.652, 6432/10016 datapoints
2025-03-06 18:51:03,276 - INFO - validation batch 251, loss: 0.646, 8032/10016 datapoints
2025-03-06 18:51:03,433 - INFO - validation batch 301, loss: 0.565, 9632/10016 datapoints
2025-03-06 18:51:03,471 - INFO - Epoch 93/800 done.
2025-03-06 18:51:03,471 - INFO - Final validation performance:
Loss: 0.541, top-1 acc: 0.866top-5 acc: 0.866
2025-03-06 18:51:03,472 - INFO - Beginning epoch 94/800
2025-03-06 18:51:03,478 - INFO - training batch 1, loss: 0.638, 32/60000 datapoints
2025-03-06 18:51:03,689 - INFO - training batch 51, loss: 0.555, 1632/60000 datapoints
2025-03-06 18:51:03,904 - INFO - training batch 101, loss: 0.608, 3232/60000 datapoints
2025-03-06 18:51:04,105 - INFO - training batch 151, loss: 0.449, 4832/60000 datapoints
2025-03-06 18:51:04,303 - INFO - training batch 201, loss: 0.900, 6432/60000 datapoints
2025-03-06 18:51:04,497 - INFO - training batch 251, loss: 0.695, 8032/60000 datapoints
2025-03-06 18:51:04,696 - INFO - training batch 301, loss: 0.542, 9632/60000 datapoints
2025-03-06 18:51:04,892 - INFO - training batch 351, loss: 0.530, 11232/60000 datapoints
2025-03-06 18:51:05,088 - INFO - training batch 401, loss: 0.681, 12832/60000 datapoints
2025-03-06 18:51:05,285 - INFO - training batch 451, loss: 0.576, 14432/60000 datapoints
2025-03-06 18:51:05,481 - INFO - training batch 501, loss: 0.461, 16032/60000 datapoints
2025-03-06 18:51:05,675 - INFO - training batch 551, loss: 0.320, 17632/60000 datapoints
2025-03-06 18:51:05,871 - INFO - training batch 601, loss: 0.394, 19232/60000 datapoints
2025-03-06 18:51:06,062 - INFO - training batch 651, loss: 0.367, 20832/60000 datapoints
2025-03-06 18:51:06,259 - INFO - training batch 701, loss: 0.552, 22432/60000 datapoints
2025-03-06 18:51:06,453 - INFO - training batch 751, loss: 0.440, 24032/60000 datapoints
2025-03-06 18:51:06,649 - INFO - training batch 801, loss: 0.461, 25632/60000 datapoints
2025-03-06 18:51:06,845 - INFO - training batch 851, loss: 0.573, 27232/60000 datapoints
2025-03-06 18:51:07,039 - INFO - training batch 901, loss: 0.497, 28832/60000 datapoints
2025-03-06 18:51:07,232 - INFO - training batch 951, loss: 0.391, 30432/60000 datapoints
2025-03-06 18:51:07,427 - INFO - training batch 1001, loss: 0.720, 32032/60000 datapoints
2025-03-06 18:51:07,625 - INFO - training batch 1051, loss: 0.659, 33632/60000 datapoints
2025-03-06 18:51:07,822 - INFO - training batch 1101, loss: 0.523, 35232/60000 datapoints
2025-03-06 18:51:08,017 - INFO - training batch 1151, loss: 0.411, 36832/60000 datapoints
2025-03-06 18:51:08,210 - INFO - training batch 1201, loss: 0.446, 38432/60000 datapoints
2025-03-06 18:51:08,404 - INFO - training batch 1251, loss: 0.676, 40032/60000 datapoints
2025-03-06 18:51:08,597 - INFO - training batch 1301, loss: 0.429, 41632/60000 datapoints
2025-03-06 18:51:08,792 - INFO - training batch 1351, loss: 0.509, 43232/60000 datapoints
2025-03-06 18:51:08,984 - INFO - training batch 1401, loss: 0.391, 44832/60000 datapoints
2025-03-06 18:51:09,181 - INFO - training batch 1451, loss: 0.486, 46432/60000 datapoints
2025-03-06 18:51:09,376 - INFO - training batch 1501, loss: 0.605, 48032/60000 datapoints
2025-03-06 18:51:09,568 - INFO - training batch 1551, loss: 0.527, 49632/60000 datapoints
2025-03-06 18:51:09,766 - INFO - training batch 1601, loss: 0.473, 51232/60000 datapoints
2025-03-06 18:51:09,960 - INFO - training batch 1651, loss: 0.562, 52832/60000 datapoints
2025-03-06 18:51:10,154 - INFO - training batch 1701, loss: 0.678, 54432/60000 datapoints
2025-03-06 18:51:10,349 - INFO - training batch 1751, loss: 0.539, 56032/60000 datapoints
2025-03-06 18:51:10,542 - INFO - training batch 1801, loss: 0.528, 57632/60000 datapoints
2025-03-06 18:51:10,738 - INFO - training batch 1851, loss: 0.550, 59232/60000 datapoints
2025-03-06 18:51:10,839 - INFO - validation batch 1, loss: 0.409, 32/10016 datapoints
2025-03-06 18:51:10,990 - INFO - validation batch 51, loss: 0.415, 1632/10016 datapoints
2025-03-06 18:51:11,144 - INFO - validation batch 101, loss: 0.308, 3232/10016 datapoints
2025-03-06 18:51:11,293 - INFO - validation batch 151, loss: 0.762, 4832/10016 datapoints
2025-03-06 18:51:11,446 - INFO - validation batch 201, loss: 0.573, 6432/10016 datapoints
2025-03-06 18:51:11,599 - INFO - validation batch 251, loss: 0.498, 8032/10016 datapoints
2025-03-06 18:51:11,757 - INFO - validation batch 301, loss: 0.399, 9632/10016 datapoints
2025-03-06 18:51:11,793 - INFO - Epoch 94/800 done.
2025-03-06 18:51:11,793 - INFO - Final validation performance:
Loss: 0.480, top-1 acc: 0.867top-5 acc: 0.867
2025-03-06 18:51:11,794 - INFO - Beginning epoch 95/800
2025-03-06 18:51:11,801 - INFO - training batch 1, loss: 0.498, 32/60000 datapoints
2025-03-06 18:51:11,993 - INFO - training batch 51, loss: 0.624, 1632/60000 datapoints
2025-03-06 18:51:12,186 - INFO - training batch 101, loss: 0.562, 3232/60000 datapoints
2025-03-06 18:51:12,386 - INFO - training batch 151, loss: 0.406, 4832/60000 datapoints
2025-03-06 18:51:12,582 - INFO - training batch 201, loss: 0.499, 6432/60000 datapoints
2025-03-06 18:51:12,777 - INFO - training batch 251, loss: 1.048, 8032/60000 datapoints
2025-03-06 18:51:12,977 - INFO - training batch 301, loss: 0.481, 9632/60000 datapoints
2025-03-06 18:51:13,171 - INFO - training batch 351, loss: 0.523, 11232/60000 datapoints
2025-03-06 18:51:13,366 - INFO - training batch 401, loss: 0.541, 12832/60000 datapoints
2025-03-06 18:51:13,558 - INFO - training batch 451, loss: 0.589, 14432/60000 datapoints
2025-03-06 18:51:13,755 - INFO - training batch 501, loss: 0.514, 16032/60000 datapoints
2025-03-06 18:51:13,966 - INFO - training batch 551, loss: 0.501, 17632/60000 datapoints
2025-03-06 18:51:14,161 - INFO - training batch 601, loss: 0.583, 19232/60000 datapoints
2025-03-06 18:51:14,360 - INFO - training batch 651, loss: 0.520, 20832/60000 datapoints
2025-03-06 18:51:14,550 - INFO - training batch 701, loss: 0.417, 22432/60000 datapoints
2025-03-06 18:51:14,743 - INFO - training batch 751, loss: 0.698, 24032/60000 datapoints
2025-03-06 18:51:14,938 - INFO - training batch 801, loss: 0.730, 25632/60000 datapoints
2025-03-06 18:51:15,131 - INFO - training batch 851, loss: 0.530, 27232/60000 datapoints
2025-03-06 18:51:15,325 - INFO - training batch 901, loss: 0.623, 28832/60000 datapoints
2025-03-06 18:51:15,544 - INFO - training batch 951, loss: 0.542, 30432/60000 datapoints
2025-03-06 18:51:15,740 - INFO - training batch 1001, loss: 0.572, 32032/60000 datapoints
2025-03-06 18:51:15,929 - INFO - training batch 1051, loss: 0.541, 33632/60000 datapoints
2025-03-06 18:51:16,121 - INFO - training batch 1101, loss: 0.473, 35232/60000 datapoints
2025-03-06 18:51:16,321 - INFO - training batch 1151, loss: 0.606, 36832/60000 datapoints
2025-03-06 18:51:16,512 - INFO - training batch 1201, loss: 0.623, 38432/60000 datapoints
2025-03-06 18:51:16,707 - INFO - training batch 1251, loss: 0.328, 40032/60000 datapoints
2025-03-06 18:51:16,897 - INFO - training batch 1301, loss: 0.434, 41632/60000 datapoints
2025-03-06 18:51:17,087 - INFO - training batch 1351, loss: 0.522, 43232/60000 datapoints
2025-03-06 18:51:17,279 - INFO - training batch 1401, loss: 0.399, 44832/60000 datapoints
2025-03-06 18:51:17,483 - INFO - training batch 1451, loss: 0.572, 46432/60000 datapoints
2025-03-06 18:51:17,693 - INFO - training batch 1501, loss: 0.541, 48032/60000 datapoints
2025-03-06 18:51:17,886 - INFO - training batch 1551, loss: 0.631, 49632/60000 datapoints
2025-03-06 18:51:18,075 - INFO - training batch 1601, loss: 0.471, 51232/60000 datapoints
2025-03-06 18:51:18,266 - INFO - training batch 1651, loss: 0.757, 52832/60000 datapoints
2025-03-06 18:51:18,456 - INFO - training batch 1701, loss: 0.343, 54432/60000 datapoints
2025-03-06 18:51:18,652 - INFO - training batch 1751, loss: 0.383, 56032/60000 datapoints
2025-03-06 18:51:18,843 - INFO - training batch 1801, loss: 0.558, 57632/60000 datapoints
2025-03-06 18:51:19,034 - INFO - training batch 1851, loss: 0.479, 59232/60000 datapoints
2025-03-06 18:51:19,134 - INFO - validation batch 1, loss: 0.625, 32/10016 datapoints
2025-03-06 18:51:19,283 - INFO - validation batch 51, loss: 0.493, 1632/10016 datapoints
2025-03-06 18:51:19,436 - INFO - validation batch 101, loss: 0.610, 3232/10016 datapoints
2025-03-06 18:51:19,586 - INFO - validation batch 151, loss: 0.280, 4832/10016 datapoints
2025-03-06 18:51:19,739 - INFO - validation batch 201, loss: 0.522, 6432/10016 datapoints
2025-03-06 18:51:19,892 - INFO - validation batch 251, loss: 0.509, 8032/10016 datapoints
2025-03-06 18:51:20,043 - INFO - validation batch 301, loss: 0.524, 9632/10016 datapoints
2025-03-06 18:51:20,078 - INFO - Epoch 95/800 done.
2025-03-06 18:51:20,078 - INFO - Final validation performance:
Loss: 0.509, top-1 acc: 0.868top-5 acc: 0.868
2025-03-06 18:51:20,079 - INFO - Beginning epoch 96/800
2025-03-06 18:51:20,085 - INFO - training batch 1, loss: 0.587, 32/60000 datapoints
2025-03-06 18:51:20,291 - INFO - training batch 51, loss: 0.423, 1632/60000 datapoints
2025-03-06 18:51:20,483 - INFO - training batch 101, loss: 0.682, 3232/60000 datapoints
2025-03-06 18:51:20,728 - INFO - training batch 151, loss: 0.531, 4832/60000 datapoints
2025-03-06 18:51:20,922 - INFO - training batch 201, loss: 0.658, 6432/60000 datapoints
2025-03-06 18:51:21,115 - INFO - training batch 251, loss: 0.631, 8032/60000 datapoints
2025-03-06 18:51:21,307 - INFO - training batch 301, loss: 0.558, 9632/60000 datapoints
2025-03-06 18:51:21,498 - INFO - training batch 351, loss: 0.587, 11232/60000 datapoints
2025-03-06 18:51:21,702 - INFO - training batch 401, loss: 0.464, 12832/60000 datapoints
2025-03-06 18:51:21,919 - INFO - training batch 451, loss: 0.613, 14432/60000 datapoints
2025-03-06 18:51:22,109 - INFO - training batch 501, loss: 0.486, 16032/60000 datapoints
2025-03-06 18:51:22,301 - INFO - training batch 551, loss: 0.440, 17632/60000 datapoints
2025-03-06 18:51:22,494 - INFO - training batch 601, loss: 0.524, 19232/60000 datapoints
2025-03-06 18:51:22,688 - INFO - training batch 651, loss: 0.636, 20832/60000 datapoints
2025-03-06 18:51:22,880 - INFO - training batch 701, loss: 0.656, 22432/60000 datapoints
2025-03-06 18:51:23,079 - INFO - training batch 751, loss: 0.538, 24032/60000 datapoints
2025-03-06 18:51:23,271 - INFO - training batch 801, loss: 0.758, 25632/60000 datapoints
2025-03-06 18:51:23,463 - INFO - training batch 851, loss: 0.505, 27232/60000 datapoints
2025-03-06 18:51:23,657 - INFO - training batch 901, loss: 0.428, 28832/60000 datapoints
2025-03-06 18:51:23,852 - INFO - training batch 951, loss: 0.788, 30432/60000 datapoints
2025-03-06 18:51:24,063 - INFO - training batch 1001, loss: 0.342, 32032/60000 datapoints
2025-03-06 18:51:24,255 - INFO - training batch 1051, loss: 0.662, 33632/60000 datapoints
2025-03-06 18:51:24,453 - INFO - training batch 1101, loss: 0.497, 35232/60000 datapoints
2025-03-06 18:51:24,648 - INFO - training batch 1151, loss: 0.527, 36832/60000 datapoints
2025-03-06 18:51:24,844 - INFO - training batch 1201, loss: 0.454, 38432/60000 datapoints
2025-03-06 18:51:25,041 - INFO - training batch 1251, loss: 0.491, 40032/60000 datapoints
2025-03-06 18:51:25,238 - INFO - training batch 1301, loss: 0.570, 41632/60000 datapoints
2025-03-06 18:51:25,433 - INFO - training batch 1351, loss: 0.443, 43232/60000 datapoints
2025-03-06 18:51:25,631 - INFO - training batch 1401, loss: 0.562, 44832/60000 datapoints
2025-03-06 18:51:25,831 - INFO - training batch 1451, loss: 0.628, 46432/60000 datapoints
2025-03-06 18:51:26,025 - INFO - training batch 1501, loss: 0.650, 48032/60000 datapoints
2025-03-06 18:51:26,218 - INFO - training batch 1551, loss: 0.707, 49632/60000 datapoints
2025-03-06 18:51:26,417 - INFO - training batch 1601, loss: 0.681, 51232/60000 datapoints
2025-03-06 18:51:26,613 - INFO - training batch 1651, loss: 0.370, 52832/60000 datapoints
2025-03-06 18:51:26,810 - INFO - training batch 1701, loss: 0.593, 54432/60000 datapoints
2025-03-06 18:51:27,003 - INFO - training batch 1751, loss: 0.418, 56032/60000 datapoints
2025-03-06 18:51:27,195 - INFO - training batch 1801, loss: 0.372, 57632/60000 datapoints
2025-03-06 18:51:27,392 - INFO - training batch 1851, loss: 0.666, 59232/60000 datapoints
2025-03-06 18:51:27,494 - INFO - validation batch 1, loss: 0.672, 32/10016 datapoints
2025-03-06 18:51:27,647 - INFO - validation batch 51, loss: 0.786, 1632/10016 datapoints
2025-03-06 18:51:27,804 - INFO - validation batch 101, loss: 0.445, 3232/10016 datapoints
2025-03-06 18:51:27,955 - INFO - validation batch 151, loss: 0.607, 4832/10016 datapoints
2025-03-06 18:51:28,112 - INFO - validation batch 201, loss: 0.475, 6432/10016 datapoints
2025-03-06 18:51:28,266 - INFO - validation batch 251, loss: 0.507, 8032/10016 datapoints
2025-03-06 18:51:28,418 - INFO - validation batch 301, loss: 0.493, 9632/10016 datapoints
2025-03-06 18:51:28,455 - INFO - Epoch 96/800 done.
2025-03-06 18:51:28,455 - INFO - Final validation performance:
Loss: 0.569, top-1 acc: 0.869top-5 acc: 0.869
2025-03-06 18:51:28,456 - INFO - Beginning epoch 97/800
2025-03-06 18:51:28,462 - INFO - training batch 1, loss: 0.324, 32/60000 datapoints
2025-03-06 18:51:28,661 - INFO - training batch 51, loss: 0.425, 1632/60000 datapoints
2025-03-06 18:51:28,864 - INFO - training batch 101, loss: 0.526, 3232/60000 datapoints
2025-03-06 18:51:29,070 - INFO - training batch 151, loss: 0.388, 4832/60000 datapoints
2025-03-06 18:51:29,270 - INFO - training batch 201, loss: 0.528, 6432/60000 datapoints
2025-03-06 18:51:29,476 - INFO - training batch 251, loss: 0.435, 8032/60000 datapoints
2025-03-06 18:51:29,674 - INFO - training batch 301, loss: 0.523, 9632/60000 datapoints
2025-03-06 18:51:29,871 - INFO - training batch 351, loss: 0.411, 11232/60000 datapoints
2025-03-06 18:51:30,067 - INFO - training batch 401, loss: 0.649, 12832/60000 datapoints
2025-03-06 18:51:30,262 - INFO - training batch 451, loss: 0.537, 14432/60000 datapoints
2025-03-06 18:51:30,463 - INFO - training batch 501, loss: 0.441, 16032/60000 datapoints
2025-03-06 18:51:30,661 - INFO - training batch 551, loss: 0.431, 17632/60000 datapoints
2025-03-06 18:51:30,858 - INFO - training batch 601, loss: 0.444, 19232/60000 datapoints
2025-03-06 18:51:31,050 - INFO - training batch 651, loss: 0.476, 20832/60000 datapoints
2025-03-06 18:51:31,244 - INFO - training batch 701, loss: 0.488, 22432/60000 datapoints
2025-03-06 18:51:31,440 - INFO - training batch 751, loss: 0.512, 24032/60000 datapoints
2025-03-06 18:51:31,638 - INFO - training batch 801, loss: 0.328, 25632/60000 datapoints
2025-03-06 18:51:31,836 - INFO - training batch 851, loss: 0.654, 27232/60000 datapoints
2025-03-06 18:51:32,029 - INFO - training batch 901, loss: 0.551, 28832/60000 datapoints
2025-03-06 18:51:32,223 - INFO - training batch 951, loss: 0.631, 30432/60000 datapoints
2025-03-06 18:51:32,420 - INFO - training batch 1001, loss: 0.748, 32032/60000 datapoints
2025-03-06 18:51:32,614 - INFO - training batch 1051, loss: 0.431, 33632/60000 datapoints
2025-03-06 18:51:32,809 - INFO - training batch 1101, loss: 0.493, 35232/60000 datapoints
2025-03-06 18:51:33,005 - INFO - training batch 1151, loss: 0.553, 36832/60000 datapoints
2025-03-06 18:51:33,198 - INFO - training batch 1201, loss: 0.542, 38432/60000 datapoints
2025-03-06 18:51:33,392 - INFO - training batch 1251, loss: 0.486, 40032/60000 datapoints
2025-03-06 18:51:33,585 - INFO - training batch 1301, loss: 0.340, 41632/60000 datapoints
2025-03-06 18:51:33,781 - INFO - training batch 1351, loss: 0.496, 43232/60000 datapoints
2025-03-06 18:51:33,980 - INFO - training batch 1401, loss: 0.588, 44832/60000 datapoints
2025-03-06 18:51:34,193 - INFO - training batch 1451, loss: 0.365, 46432/60000 datapoints
2025-03-06 18:51:34,389 - INFO - training batch 1501, loss: 0.525, 48032/60000 datapoints
2025-03-06 18:51:34,587 - INFO - training batch 1551, loss: 0.453, 49632/60000 datapoints
2025-03-06 18:51:34,785 - INFO - training batch 1601, loss: 0.570, 51232/60000 datapoints
2025-03-06 18:51:34,984 - INFO - training batch 1651, loss: 0.596, 52832/60000 datapoints
2025-03-06 18:51:35,177 - INFO - training batch 1701, loss: 0.530, 54432/60000 datapoints
2025-03-06 18:51:35,372 - INFO - training batch 1751, loss: 0.515, 56032/60000 datapoints
2025-03-06 18:51:35,572 - INFO - training batch 1801, loss: 0.559, 57632/60000 datapoints
2025-03-06 18:51:35,767 - INFO - training batch 1851, loss: 0.415, 59232/60000 datapoints
2025-03-06 18:51:35,871 - INFO - validation batch 1, loss: 0.673, 32/10016 datapoints
2025-03-06 18:51:36,022 - INFO - validation batch 51, loss: 0.633, 1632/10016 datapoints
2025-03-06 18:51:36,179 - INFO - validation batch 101, loss: 0.585, 3232/10016 datapoints
2025-03-06 18:51:36,335 - INFO - validation batch 151, loss: 0.538, 4832/10016 datapoints
2025-03-06 18:51:36,491 - INFO - validation batch 201, loss: 0.401, 6432/10016 datapoints
2025-03-06 18:51:36,645 - INFO - validation batch 251, loss: 0.342, 8032/10016 datapoints
2025-03-06 18:51:36,797 - INFO - validation batch 301, loss: 0.445, 9632/10016 datapoints
2025-03-06 18:51:36,833 - INFO - Epoch 97/800 done.
2025-03-06 18:51:36,833 - INFO - Final validation performance:
Loss: 0.517, top-1 acc: 0.869top-5 acc: 0.869
2025-03-06 18:51:36,834 - INFO - Beginning epoch 98/800
2025-03-06 18:51:36,841 - INFO - training batch 1, loss: 0.484, 32/60000 datapoints
2025-03-06 18:51:37,055 - INFO - training batch 51, loss: 0.512, 1632/60000 datapoints
2025-03-06 18:51:37,248 - INFO - training batch 101, loss: 0.674, 3232/60000 datapoints
2025-03-06 18:51:37,445 - INFO - training batch 151, loss: 0.616, 4832/60000 datapoints
2025-03-06 18:51:37,662 - INFO - training batch 201, loss: 0.710, 6432/60000 datapoints
2025-03-06 18:51:37,863 - INFO - training batch 251, loss: 0.703, 8032/60000 datapoints
2025-03-06 18:51:38,063 - INFO - training batch 301, loss: 0.500, 9632/60000 datapoints
2025-03-06 18:51:38,256 - INFO - training batch 351, loss: 0.562, 11232/60000 datapoints
2025-03-06 18:51:38,453 - INFO - training batch 401, loss: 0.515, 12832/60000 datapoints
2025-03-06 18:51:38,650 - INFO - training batch 451, loss: 0.655, 14432/60000 datapoints
2025-03-06 18:51:38,843 - INFO - training batch 501, loss: 0.375, 16032/60000 datapoints
2025-03-06 18:51:39,038 - INFO - training batch 551, loss: 0.550, 17632/60000 datapoints
2025-03-06 18:51:39,232 - INFO - training batch 601, loss: 0.658, 19232/60000 datapoints
2025-03-06 18:51:39,427 - INFO - training batch 651, loss: 0.492, 20832/60000 datapoints
2025-03-06 18:51:39,624 - INFO - training batch 701, loss: 0.609, 22432/60000 datapoints
2025-03-06 18:51:39,821 - INFO - training batch 751, loss: 0.893, 24032/60000 datapoints
2025-03-06 18:51:40,020 - INFO - training batch 801, loss: 0.556, 25632/60000 datapoints
2025-03-06 18:51:40,214 - INFO - training batch 851, loss: 0.369, 27232/60000 datapoints
2025-03-06 18:51:40,411 - INFO - training batch 901, loss: 0.527, 28832/60000 datapoints
2025-03-06 18:51:40,609 - INFO - training batch 951, loss: 0.432, 30432/60000 datapoints
2025-03-06 18:51:40,803 - INFO - training batch 1001, loss: 0.578, 32032/60000 datapoints
2025-03-06 18:51:41,000 - INFO - training batch 1051, loss: 0.403, 33632/60000 datapoints
2025-03-06 18:51:41,193 - INFO - training batch 1101, loss: 0.489, 35232/60000 datapoints
2025-03-06 18:51:41,387 - INFO - training batch 1151, loss: 0.550, 36832/60000 datapoints
2025-03-06 18:51:41,582 - INFO - training batch 1201, loss: 0.530, 38432/60000 datapoints
2025-03-06 18:51:41,777 - INFO - training batch 1251, loss: 0.752, 40032/60000 datapoints
2025-03-06 18:51:41,972 - INFO - training batch 1301, loss: 0.587, 41632/60000 datapoints
2025-03-06 18:51:42,165 - INFO - training batch 1351, loss: 0.684, 43232/60000 datapoints
2025-03-06 18:51:42,359 - INFO - training batch 1401, loss: 0.614, 44832/60000 datapoints
2025-03-06 18:51:42,558 - INFO - training batch 1451, loss: 0.454, 46432/60000 datapoints
2025-03-06 18:51:42,754 - INFO - training batch 1501, loss: 0.437, 48032/60000 datapoints
2025-03-06 18:51:42,950 - INFO - training batch 1551, loss: 0.491, 49632/60000 datapoints
2025-03-06 18:51:43,144 - INFO - training batch 1601, loss: 0.564, 51232/60000 datapoints
2025-03-06 18:51:43,335 - INFO - training batch 1651, loss: 0.439, 52832/60000 datapoints
2025-03-06 18:51:43,531 - INFO - training batch 1701, loss: 0.563, 54432/60000 datapoints
2025-03-06 18:51:43,728 - INFO - training batch 1751, loss: 0.465, 56032/60000 datapoints
2025-03-06 18:51:43,924 - INFO - training batch 1801, loss: 0.388, 57632/60000 datapoints
2025-03-06 18:51:44,127 - INFO - training batch 1851, loss: 0.667, 59232/60000 datapoints
2025-03-06 18:51:44,238 - INFO - validation batch 1, loss: 0.578, 32/10016 datapoints
2025-03-06 18:51:44,389 - INFO - validation batch 51, loss: 0.474, 1632/10016 datapoints
2025-03-06 18:51:44,544 - INFO - validation batch 101, loss: 0.379, 3232/10016 datapoints
2025-03-06 18:51:44,703 - INFO - validation batch 151, loss: 0.449, 4832/10016 datapoints
2025-03-06 18:51:44,856 - INFO - validation batch 201, loss: 0.426, 6432/10016 datapoints
2025-03-06 18:51:45,016 - INFO - validation batch 251, loss: 0.368, 8032/10016 datapoints
2025-03-06 18:51:45,173 - INFO - validation batch 301, loss: 0.512, 9632/10016 datapoints
2025-03-06 18:51:45,212 - INFO - Epoch 98/800 done.
2025-03-06 18:51:45,212 - INFO - Final validation performance:
Loss: 0.455, top-1 acc: 0.870top-5 acc: 0.870
2025-03-06 18:51:45,213 - INFO - Beginning epoch 99/800
2025-03-06 18:51:45,219 - INFO - training batch 1, loss: 0.422, 32/60000 datapoints
2025-03-06 18:51:45,413 - INFO - training batch 51, loss: 0.544, 1632/60000 datapoints
2025-03-06 18:51:45,622 - INFO - training batch 101, loss: 0.508, 3232/60000 datapoints
2025-03-06 18:51:45,817 - INFO - training batch 151, loss: 0.476, 4832/60000 datapoints
2025-03-06 18:51:46,015 - INFO - training batch 201, loss: 0.714, 6432/60000 datapoints
2025-03-06 18:51:46,216 - INFO - training batch 251, loss: 0.610, 8032/60000 datapoints
2025-03-06 18:51:46,414 - INFO - training batch 301, loss: 0.453, 9632/60000 datapoints
2025-03-06 18:51:46,616 - INFO - training batch 351, loss: 0.360, 11232/60000 datapoints
2025-03-06 18:51:46,808 - INFO - training batch 401, loss: 0.407, 12832/60000 datapoints
2025-03-06 18:51:47,005 - INFO - training batch 451, loss: 0.589, 14432/60000 datapoints
2025-03-06 18:51:47,199 - INFO - training batch 501, loss: 0.641, 16032/60000 datapoints
2025-03-06 18:51:47,392 - INFO - training batch 551, loss: 0.599, 17632/60000 datapoints
2025-03-06 18:51:47,589 - INFO - training batch 601, loss: 0.454, 19232/60000 datapoints
2025-03-06 18:51:47,787 - INFO - training batch 651, loss: 0.437, 20832/60000 datapoints
2025-03-06 18:51:47,988 - INFO - training batch 701, loss: 0.396, 22432/60000 datapoints
2025-03-06 18:51:48,185 - INFO - training batch 751, loss: 0.506, 24032/60000 datapoints
2025-03-06 18:51:48,379 - INFO - training batch 801, loss: 0.565, 25632/60000 datapoints
2025-03-06 18:51:48,574 - INFO - training batch 851, loss: 0.347, 27232/60000 datapoints
2025-03-06 18:51:48,770 - INFO - training batch 901, loss: 0.531, 28832/60000 datapoints
2025-03-06 18:51:48,964 - INFO - training batch 951, loss: 0.600, 30432/60000 datapoints
2025-03-06 18:51:49,165 - INFO - training batch 1001, loss: 0.415, 32032/60000 datapoints
2025-03-06 18:51:49,357 - INFO - training batch 1051, loss: 0.494, 33632/60000 datapoints
2025-03-06 18:51:49,554 - INFO - training batch 1101, loss: 0.441, 35232/60000 datapoints
2025-03-06 18:51:49,756 - INFO - training batch 1151, loss: 0.775, 36832/60000 datapoints
2025-03-06 18:51:49,954 - INFO - training batch 1201, loss: 0.429, 38432/60000 datapoints
2025-03-06 18:51:50,147 - INFO - training batch 1251, loss: 0.685, 40032/60000 datapoints
2025-03-06 18:51:50,343 - INFO - training batch 1301, loss: 0.556, 41632/60000 datapoints
2025-03-06 18:51:50,539 - INFO - training batch 1351, loss: 0.636, 43232/60000 datapoints
2025-03-06 18:51:50,734 - INFO - training batch 1401, loss: 0.436, 44832/60000 datapoints
2025-03-06 18:51:50,927 - INFO - training batch 1451, loss: 0.406, 46432/60000 datapoints
2025-03-06 18:51:51,121 - INFO - training batch 1501, loss: 0.473, 48032/60000 datapoints
2025-03-06 18:51:51,313 - INFO - training batch 1551, loss: 0.782, 49632/60000 datapoints
2025-03-06 18:51:51,509 - INFO - training batch 1601, loss: 0.366, 51232/60000 datapoints
2025-03-06 18:51:51,708 - INFO - training batch 1651, loss: 0.516, 52832/60000 datapoints
2025-03-06 18:51:51,906 - INFO - training batch 1701, loss: 0.373, 54432/60000 datapoints
2025-03-06 18:51:52,099 - INFO - training batch 1751, loss: 0.707, 56032/60000 datapoints
2025-03-06 18:51:52,292 - INFO - training batch 1801, loss: 0.650, 57632/60000 datapoints
2025-03-06 18:51:52,487 - INFO - training batch 1851, loss: 0.543, 59232/60000 datapoints
2025-03-06 18:51:52,590 - INFO - validation batch 1, loss: 0.460, 32/10016 datapoints
2025-03-06 18:51:52,753 - INFO - validation batch 51, loss: 0.759, 1632/10016 datapoints
2025-03-06 18:51:52,907 - INFO - validation batch 101, loss: 0.545, 3232/10016 datapoints
2025-03-06 18:51:53,060 - INFO - validation batch 151, loss: 0.467, 4832/10016 datapoints
2025-03-06 18:51:53,212 - INFO - validation batch 201, loss: 0.483, 6432/10016 datapoints
2025-03-06 18:51:53,369 - INFO - validation batch 251, loss: 0.505, 8032/10016 datapoints
2025-03-06 18:51:53,521 - INFO - validation batch 301, loss: 0.483, 9632/10016 datapoints
2025-03-06 18:51:53,560 - INFO - Epoch 99/800 done.
2025-03-06 18:51:53,561 - INFO - Final validation performance:
Loss: 0.529, top-1 acc: 0.870top-5 acc: 0.870
2025-03-06 18:51:53,561 - INFO - Beginning epoch 100/800
2025-03-06 18:51:53,568 - INFO - training batch 1, loss: 0.473, 32/60000 datapoints
2025-03-06 18:51:53,776 - INFO - training batch 51, loss: 0.441, 1632/60000 datapoints
2025-03-06 18:51:53,974 - INFO - training batch 101, loss: 0.640, 3232/60000 datapoints
2025-03-06 18:51:54,175 - INFO - training batch 151, loss: 0.427, 4832/60000 datapoints
2025-03-06 18:51:54,386 - INFO - training batch 201, loss: 0.579, 6432/60000 datapoints
2025-03-06 18:51:54,585 - INFO - training batch 251, loss: 0.433, 8032/60000 datapoints
2025-03-06 18:51:54,784 - INFO - training batch 301, loss: 0.844, 9632/60000 datapoints
2025-03-06 18:51:54,983 - INFO - training batch 351, loss: 0.630, 11232/60000 datapoints
2025-03-06 18:51:55,177 - INFO - training batch 401, loss: 0.596, 12832/60000 datapoints
2025-03-06 18:51:55,374 - INFO - training batch 451, loss: 0.607, 14432/60000 datapoints
2025-03-06 18:51:55,570 - INFO - training batch 501, loss: 0.579, 16032/60000 datapoints
2025-03-06 18:51:55,767 - INFO - training batch 551, loss: 0.568, 17632/60000 datapoints
2025-03-06 18:51:55,984 - INFO - training batch 601, loss: 0.444, 19232/60000 datapoints
2025-03-06 18:51:56,181 - INFO - training batch 651, loss: 0.507, 20832/60000 datapoints
2025-03-06 18:51:56,376 - INFO - training batch 701, loss: 0.349, 22432/60000 datapoints
2025-03-06 18:51:56,570 - INFO - training batch 751, loss: 0.458, 24032/60000 datapoints
2025-03-06 18:51:56,795 - INFO - training batch 801, loss: 0.450, 25632/60000 datapoints
2025-03-06 18:51:56,988 - INFO - training batch 851, loss: 0.564, 27232/60000 datapoints
2025-03-06 18:51:57,181 - INFO - training batch 901, loss: 0.700, 28832/60000 datapoints
2025-03-06 18:51:57,372 - INFO - training batch 951, loss: 0.615, 30432/60000 datapoints
2025-03-06 18:51:57,570 - INFO - training batch 1001, loss: 0.371, 32032/60000 datapoints
2025-03-06 18:51:57,765 - INFO - training batch 1051, loss: 0.941, 33632/60000 datapoints
2025-03-06 18:51:57,962 - INFO - training batch 1101, loss: 0.515, 35232/60000 datapoints
2025-03-06 18:51:58,154 - INFO - training batch 1151, loss: 0.488, 36832/60000 datapoints
2025-03-06 18:51:58,348 - INFO - training batch 1201, loss: 0.461, 38432/60000 datapoints
2025-03-06 18:51:58,545 - INFO - training batch 1251, loss: 0.440, 40032/60000 datapoints
2025-03-06 18:51:58,743 - INFO - training batch 1301, loss: 0.359, 41632/60000 datapoints
2025-03-06 18:51:58,937 - INFO - training batch 1351, loss: 0.466, 43232/60000 datapoints
2025-03-06 18:51:59,132 - INFO - training batch 1401, loss: 0.646, 44832/60000 datapoints
2025-03-06 18:51:59,324 - INFO - training batch 1451, loss: 0.562, 46432/60000 datapoints
2025-03-06 18:51:59,518 - INFO - training batch 1501, loss: 0.625, 48032/60000 datapoints
2025-03-06 18:51:59,712 - INFO - training batch 1551, loss: 0.301, 49632/60000 datapoints
2025-03-06 18:51:59,912 - INFO - training batch 1601, loss: 0.538, 51232/60000 datapoints
2025-03-06 18:52:00,107 - INFO - training batch 1651, loss: 0.340, 52832/60000 datapoints
2025-03-06 18:52:00,301 - INFO - training batch 1701, loss: 0.412, 54432/60000 datapoints
2025-03-06 18:52:00,497 - INFO - training batch 1751, loss: 0.358, 56032/60000 datapoints
2025-03-06 18:52:00,693 - INFO - training batch 1801, loss: 0.350, 57632/60000 datapoints
2025-03-06 18:52:00,886 - INFO - training batch 1851, loss: 0.522, 59232/60000 datapoints
2025-03-06 18:52:00,987 - INFO - validation batch 1, loss: 0.397, 32/10016 datapoints
2025-03-06 18:52:01,141 - INFO - validation batch 51, loss: 0.453, 1632/10016 datapoints
2025-03-06 18:52:01,294 - INFO - validation batch 101, loss: 0.361, 3232/10016 datapoints
2025-03-06 18:52:01,446 - INFO - validation batch 151, loss: 0.454, 4832/10016 datapoints
2025-03-06 18:52:01,596 - INFO - validation batch 201, loss: 0.450, 6432/10016 datapoints
2025-03-06 18:52:01,754 - INFO - validation batch 251, loss: 0.556, 8032/10016 datapoints
2025-03-06 18:52:01,909 - INFO - validation batch 301, loss: 0.335, 9632/10016 datapoints
2025-03-06 18:52:01,946 - INFO - Epoch 100/800 done.
2025-03-06 18:52:01,946 - INFO - Final validation performance:
Loss: 0.429, top-1 acc: 0.872top-5 acc: 0.872
2025-03-06 18:52:01,947 - INFO - Beginning epoch 101/800
2025-03-06 18:52:01,954 - INFO - training batch 1, loss: 0.503, 32/60000 datapoints
2025-03-06 18:52:02,144 - INFO - training batch 51, loss: 0.614, 1632/60000 datapoints
2025-03-06 18:52:02,334 - INFO - training batch 101, loss: 0.326, 3232/60000 datapoints
2025-03-06 18:52:02,536 - INFO - training batch 151, loss: 0.363, 4832/60000 datapoints
2025-03-06 18:52:02,736 - INFO - training batch 201, loss: 0.425, 6432/60000 datapoints
2025-03-06 18:52:02,929 - INFO - training batch 251, loss: 0.888, 8032/60000 datapoints
2025-03-06 18:52:03,125 - INFO - training batch 301, loss: 0.613, 9632/60000 datapoints
2025-03-06 18:52:03,337 - INFO - training batch 351, loss: 0.391, 11232/60000 datapoints
2025-03-06 18:52:03,535 - INFO - training batch 401, loss: 0.432, 12832/60000 datapoints
2025-03-06 18:52:03,730 - INFO - training batch 451, loss: 0.384, 14432/60000 datapoints
2025-03-06 18:52:03,924 - INFO - training batch 501, loss: 0.574, 16032/60000 datapoints
2025-03-06 18:52:04,114 - INFO - training batch 551, loss: 0.567, 17632/60000 datapoints
2025-03-06 18:52:04,321 - INFO - training batch 601, loss: 0.575, 19232/60000 datapoints
2025-03-06 18:52:04,513 - INFO - training batch 651, loss: 0.578, 20832/60000 datapoints
2025-03-06 18:52:04,712 - INFO - training batch 701, loss: 0.255, 22432/60000 datapoints
2025-03-06 18:52:04,910 - INFO - training batch 751, loss: 0.597, 24032/60000 datapoints
2025-03-06 18:52:05,109 - INFO - training batch 801, loss: 0.408, 25632/60000 datapoints
2025-03-06 18:52:05,305 - INFO - training batch 851, loss: 0.489, 27232/60000 datapoints
2025-03-06 18:52:05,500 - INFO - training batch 901, loss: 0.469, 28832/60000 datapoints
2025-03-06 18:52:05,699 - INFO - training batch 951, loss: 0.543, 30432/60000 datapoints
2025-03-06 18:52:05,893 - INFO - training batch 1001, loss: 0.580, 32032/60000 datapoints
2025-03-06 18:52:06,091 - INFO - training batch 1051, loss: 0.426, 33632/60000 datapoints
2025-03-06 18:52:06,287 - INFO - training batch 1101, loss: 0.620, 35232/60000 datapoints
2025-03-06 18:52:06,487 - INFO - training batch 1151, loss: 0.442, 36832/60000 datapoints
2025-03-06 18:52:06,684 - INFO - training batch 1201, loss: 0.535, 38432/60000 datapoints
2025-03-06 18:52:06,883 - INFO - training batch 1251, loss: 0.558, 40032/60000 datapoints
2025-03-06 18:52:07,077 - INFO - training batch 1301, loss: 0.601, 41632/60000 datapoints
2025-03-06 18:52:07,270 - INFO - training batch 1351, loss: 0.452, 43232/60000 datapoints
2025-03-06 18:52:07,464 - INFO - training batch 1401, loss: 0.350, 44832/60000 datapoints
2025-03-06 18:52:07,663 - INFO - training batch 1451, loss: 0.489, 46432/60000 datapoints
2025-03-06 18:52:07,859 - INFO - training batch 1501, loss: 0.646, 48032/60000 datapoints
2025-03-06 18:52:08,057 - INFO - training batch 1551, loss: 0.413, 49632/60000 datapoints
2025-03-06 18:52:08,250 - INFO - training batch 1601, loss: 0.383, 51232/60000 datapoints
2025-03-06 18:52:08,444 - INFO - training batch 1651, loss: 0.452, 52832/60000 datapoints
2025-03-06 18:52:08,639 - INFO - training batch 1701, loss: 0.483, 54432/60000 datapoints
2025-03-06 18:52:08,834 - INFO - training batch 1751, loss: 0.586, 56032/60000 datapoints
2025-03-06 18:52:09,031 - INFO - training batch 1801, loss: 0.513, 57632/60000 datapoints
2025-03-06 18:52:09,233 - INFO - training batch 1851, loss: 0.712, 59232/60000 datapoints
2025-03-06 18:52:09,334 - INFO - validation batch 1, loss: 0.545, 32/10016 datapoints
2025-03-06 18:52:09,486 - INFO - validation batch 51, loss: 0.608, 1632/10016 datapoints
2025-03-06 18:52:09,640 - INFO - validation batch 101, loss: 0.510, 3232/10016 datapoints
2025-03-06 18:52:09,794 - INFO - validation batch 151, loss: 0.782, 4832/10016 datapoints
2025-03-06 18:52:09,950 - INFO - validation batch 201, loss: 0.457, 6432/10016 datapoints
2025-03-06 18:52:10,104 - INFO - validation batch 251, loss: 0.576, 8032/10016 datapoints
2025-03-06 18:52:10,258 - INFO - validation batch 301, loss: 0.658, 9632/10016 datapoints
2025-03-06 18:52:10,296 - INFO - Epoch 101/800 done.
2025-03-06 18:52:10,296 - INFO - Final validation performance:
Loss: 0.591, top-1 acc: 0.872top-5 acc: 0.872
2025-03-06 18:52:10,297 - INFO - Beginning epoch 102/800
2025-03-06 18:52:10,304 - INFO - training batch 1, loss: 0.504, 32/60000 datapoints
2025-03-06 18:52:10,506 - INFO - training batch 51, loss: 0.458, 1632/60000 datapoints
2025-03-06 18:52:10,700 - INFO - training batch 101, loss: 0.461, 3232/60000 datapoints
2025-03-06 18:52:10,894 - INFO - training batch 151, loss: 0.451, 4832/60000 datapoints
2025-03-06 18:52:11,085 - INFO - training batch 201, loss: 0.576, 6432/60000 datapoints
2025-03-06 18:52:11,278 - INFO - training batch 251, loss: 0.490, 8032/60000 datapoints
2025-03-06 18:52:11,474 - INFO - training batch 301, loss: 0.421, 9632/60000 datapoints
2025-03-06 18:52:11,671 - INFO - training batch 351, loss: 0.321, 11232/60000 datapoints
2025-03-06 18:52:11,863 - INFO - training batch 401, loss: 0.509, 12832/60000 datapoints
2025-03-06 18:52:12,062 - INFO - training batch 451, loss: 0.271, 14432/60000 datapoints
2025-03-06 18:52:12,256 - INFO - training batch 501, loss: 0.550, 16032/60000 datapoints
2025-03-06 18:52:12,450 - INFO - training batch 551, loss: 0.451, 17632/60000 datapoints
2025-03-06 18:52:12,646 - INFO - training batch 601, loss: 0.426, 19232/60000 datapoints
2025-03-06 18:52:12,836 - INFO - training batch 651, loss: 0.501, 20832/60000 datapoints
2025-03-06 18:52:13,027 - INFO - training batch 701, loss: 0.457, 22432/60000 datapoints
2025-03-06 18:52:13,218 - INFO - training batch 751, loss: 0.528, 24032/60000 datapoints
2025-03-06 18:52:13,412 - INFO - training batch 801, loss: 0.578, 25632/60000 datapoints
2025-03-06 18:52:13,607 - INFO - training batch 851, loss: 0.574, 27232/60000 datapoints
2025-03-06 18:52:13,804 - INFO - training batch 901, loss: 0.621, 28832/60000 datapoints
2025-03-06 18:52:14,000 - INFO - training batch 951, loss: 0.651, 30432/60000 datapoints
2025-03-06 18:52:14,194 - INFO - training batch 1001, loss: 0.550, 32032/60000 datapoints
2025-03-06 18:52:14,405 - INFO - training batch 1051, loss: 0.437, 33632/60000 datapoints
2025-03-06 18:52:14,597 - INFO - training batch 1101, loss: 0.573, 35232/60000 datapoints
2025-03-06 18:52:14,797 - INFO - training batch 1151, loss: 0.608, 36832/60000 datapoints
2025-03-06 18:52:14,993 - INFO - training batch 1201, loss: 0.453, 38432/60000 datapoints
2025-03-06 18:52:15,186 - INFO - training batch 1251, loss: 0.646, 40032/60000 datapoints
2025-03-06 18:52:15,379 - INFO - training batch 1301, loss: 0.857, 41632/60000 datapoints
2025-03-06 18:52:15,572 - INFO - training batch 1351, loss: 0.449, 43232/60000 datapoints
2025-03-06 18:52:15,768 - INFO - training batch 1401, loss: 0.644, 44832/60000 datapoints
2025-03-06 18:52:15,988 - INFO - training batch 1451, loss: 0.451, 46432/60000 datapoints
2025-03-06 18:52:16,181 - INFO - training batch 1501, loss: 0.525, 48032/60000 datapoints
2025-03-06 18:52:16,374 - INFO - training batch 1551, loss: 0.884, 49632/60000 datapoints
2025-03-06 18:52:16,567 - INFO - training batch 1601, loss: 0.464, 51232/60000 datapoints
2025-03-06 18:52:16,761 - INFO - training batch 1651, loss: 0.669, 52832/60000 datapoints
2025-03-06 18:52:16,954 - INFO - training batch 1701, loss: 0.758, 54432/60000 datapoints
2025-03-06 18:52:17,144 - INFO - training batch 1751, loss: 0.447, 56032/60000 datapoints
2025-03-06 18:52:17,334 - INFO - training batch 1801, loss: 0.285, 57632/60000 datapoints
2025-03-06 18:52:17,526 - INFO - training batch 1851, loss: 0.595, 59232/60000 datapoints
2025-03-06 18:52:17,626 - INFO - validation batch 1, loss: 0.474, 32/10016 datapoints
2025-03-06 18:52:17,776 - INFO - validation batch 51, loss: 0.528, 1632/10016 datapoints
2025-03-06 18:52:17,928 - INFO - validation batch 101, loss: 0.646, 3232/10016 datapoints
2025-03-06 18:52:18,080 - INFO - validation batch 151, loss: 0.647, 4832/10016 datapoints
2025-03-06 18:52:18,229 - INFO - validation batch 201, loss: 0.473, 6432/10016 datapoints
2025-03-06 18:52:18,380 - INFO - validation batch 251, loss: 0.560, 8032/10016 datapoints
2025-03-06 18:52:18,529 - INFO - validation batch 301, loss: 0.629, 9632/10016 datapoints
2025-03-06 18:52:18,564 - INFO - Epoch 102/800 done.
2025-03-06 18:52:18,564 - INFO - Final validation performance:
Loss: 0.565, top-1 acc: 0.873top-5 acc: 0.873
2025-03-06 18:52:18,565 - INFO - Beginning epoch 103/800
2025-03-06 18:52:18,571 - INFO - training batch 1, loss: 0.759, 32/60000 datapoints
2025-03-06 18:52:18,773 - INFO - training batch 51, loss: 0.535, 1632/60000 datapoints
2025-03-06 18:52:18,967 - INFO - training batch 101, loss: 0.632, 3232/60000 datapoints
2025-03-06 18:52:19,161 - INFO - training batch 151, loss: 0.554, 4832/60000 datapoints
2025-03-06 18:52:19,357 - INFO - training batch 201, loss: 0.535, 6432/60000 datapoints
2025-03-06 18:52:19,552 - INFO - training batch 251, loss: 0.615, 8032/60000 datapoints
2025-03-06 18:52:19,749 - INFO - training batch 301, loss: 0.495, 9632/60000 datapoints
2025-03-06 18:52:19,941 - INFO - training batch 351, loss: 0.529, 11232/60000 datapoints
2025-03-06 18:52:20,137 - INFO - training batch 401, loss: 0.444, 12832/60000 datapoints
2025-03-06 18:52:20,329 - INFO - training batch 451, loss: 0.694, 14432/60000 datapoints
2025-03-06 18:52:20,521 - INFO - training batch 501, loss: 0.460, 16032/60000 datapoints
2025-03-06 18:52:20,718 - INFO - training batch 551, loss: 0.475, 17632/60000 datapoints
2025-03-06 18:52:20,910 - INFO - training batch 601, loss: 0.663, 19232/60000 datapoints
2025-03-06 18:52:21,102 - INFO - training batch 651, loss: 0.435, 20832/60000 datapoints
2025-03-06 18:52:21,294 - INFO - training batch 701, loss: 0.545, 22432/60000 datapoints
2025-03-06 18:52:21,488 - INFO - training batch 751, loss: 0.302, 24032/60000 datapoints
2025-03-06 18:52:21,680 - INFO - training batch 801, loss: 0.371, 25632/60000 datapoints
2025-03-06 18:52:21,875 - INFO - training batch 851, loss: 0.417, 27232/60000 datapoints
2025-03-06 18:52:22,069 - INFO - training batch 901, loss: 0.590, 28832/60000 datapoints
2025-03-06 18:52:22,258 - INFO - training batch 951, loss: 0.699, 30432/60000 datapoints
2025-03-06 18:52:22,451 - INFO - training batch 1001, loss: 0.420, 32032/60000 datapoints
2025-03-06 18:52:22,649 - INFO - training batch 1051, loss: 0.539, 33632/60000 datapoints
2025-03-06 18:52:22,842 - INFO - training batch 1101, loss: 0.688, 35232/60000 datapoints
2025-03-06 18:52:23,034 - INFO - training batch 1151, loss: 0.575, 36832/60000 datapoints
2025-03-06 18:52:23,224 - INFO - training batch 1201, loss: 0.491, 38432/60000 datapoints
2025-03-06 18:52:23,415 - INFO - training batch 1251, loss: 0.301, 40032/60000 datapoints
2025-03-06 18:52:23,611 - INFO - training batch 1301, loss: 0.567, 41632/60000 datapoints
2025-03-06 18:52:23,801 - INFO - training batch 1351, loss: 0.346, 43232/60000 datapoints
2025-03-06 18:52:23,995 - INFO - training batch 1401, loss: 0.520, 44832/60000 datapoints
2025-03-06 18:52:24,207 - INFO - training batch 1451, loss: 0.655, 46432/60000 datapoints
2025-03-06 18:52:24,409 - INFO - training batch 1501, loss: 0.715, 48032/60000 datapoints
2025-03-06 18:52:24,623 - INFO - training batch 1551, loss: 0.657, 49632/60000 datapoints
2025-03-06 18:52:24,828 - INFO - training batch 1601, loss: 0.434, 51232/60000 datapoints
2025-03-06 18:52:25,045 - INFO - training batch 1651, loss: 0.444, 52832/60000 datapoints
2025-03-06 18:52:25,273 - INFO - training batch 1701, loss: 0.670, 54432/60000 datapoints
2025-03-06 18:52:25,491 - INFO - training batch 1751, loss: 0.463, 56032/60000 datapoints
2025-03-06 18:52:25,700 - INFO - training batch 1801, loss: 0.521, 57632/60000 datapoints
2025-03-06 18:52:25,894 - INFO - training batch 1851, loss: 0.372, 59232/60000 datapoints
2025-03-06 18:52:25,998 - INFO - validation batch 1, loss: 0.517, 32/10016 datapoints
2025-03-06 18:52:26,164 - INFO - validation batch 51, loss: 0.377, 1632/10016 datapoints
2025-03-06 18:52:26,317 - INFO - validation batch 101, loss: 0.370, 3232/10016 datapoints
2025-03-06 18:52:26,476 - INFO - validation batch 151, loss: 0.512, 4832/10016 datapoints
2025-03-06 18:52:26,635 - INFO - validation batch 201, loss: 0.456, 6432/10016 datapoints
2025-03-06 18:52:26,809 - INFO - validation batch 251, loss: 0.383, 8032/10016 datapoints
2025-03-06 18:52:26,962 - INFO - validation batch 301, loss: 0.465, 9632/10016 datapoints
2025-03-06 18:52:27,000 - INFO - Epoch 103/800 done.
2025-03-06 18:52:27,000 - INFO - Final validation performance:
Loss: 0.440, top-1 acc: 0.873top-5 acc: 0.873
2025-03-06 18:52:27,001 - INFO - Beginning epoch 104/800
2025-03-06 18:52:27,007 - INFO - training batch 1, loss: 0.600, 32/60000 datapoints
2025-03-06 18:52:27,215 - INFO - training batch 51, loss: 0.546, 1632/60000 datapoints
2025-03-06 18:52:27,409 - INFO - training batch 101, loss: 0.811, 3232/60000 datapoints
2025-03-06 18:52:27,612 - INFO - training batch 151, loss: 0.463, 4832/60000 datapoints
2025-03-06 18:52:27,813 - INFO - training batch 201, loss: 0.478, 6432/60000 datapoints
2025-03-06 18:52:28,024 - INFO - training batch 251, loss: 0.504, 8032/60000 datapoints
2025-03-06 18:52:28,230 - INFO - training batch 301, loss: 0.299, 9632/60000 datapoints
2025-03-06 18:52:28,425 - INFO - training batch 351, loss: 0.563, 11232/60000 datapoints
2025-03-06 18:52:28,622 - INFO - training batch 401, loss: 0.624, 12832/60000 datapoints
2025-03-06 18:52:28,817 - INFO - training batch 451, loss: 0.534, 14432/60000 datapoints
2025-03-06 18:52:29,010 - INFO - training batch 501, loss: 0.526, 16032/60000 datapoints
2025-03-06 18:52:29,204 - INFO - training batch 551, loss: 0.477, 17632/60000 datapoints
2025-03-06 18:52:29,396 - INFO - training batch 601, loss: 0.597, 19232/60000 datapoints
2025-03-06 18:52:29,591 - INFO - training batch 651, loss: 0.390, 20832/60000 datapoints
2025-03-06 18:52:29,786 - INFO - training batch 701, loss: 0.391, 22432/60000 datapoints
2025-03-06 18:52:29,979 - INFO - training batch 751, loss: 0.423, 24032/60000 datapoints
2025-03-06 18:52:30,174 - INFO - training batch 801, loss: 0.521, 25632/60000 datapoints
2025-03-06 18:52:30,367 - INFO - training batch 851, loss: 0.327, 27232/60000 datapoints
2025-03-06 18:52:30,561 - INFO - training batch 901, loss: 0.452, 28832/60000 datapoints
2025-03-06 18:52:30,760 - INFO - training batch 951, loss: 0.392, 30432/60000 datapoints
2025-03-06 18:52:30,958 - INFO - training batch 1001, loss: 0.481, 32032/60000 datapoints
2025-03-06 18:52:31,151 - INFO - training batch 1051, loss: 0.549, 33632/60000 datapoints
2025-03-06 18:52:31,347 - INFO - training batch 1101, loss: 0.569, 35232/60000 datapoints
2025-03-06 18:52:31,540 - INFO - training batch 1151, loss: 0.416, 36832/60000 datapoints
2025-03-06 18:52:31,738 - INFO - training batch 1201, loss: 0.363, 38432/60000 datapoints
2025-03-06 18:52:31,933 - INFO - training batch 1251, loss: 0.402, 40032/60000 datapoints
2025-03-06 18:52:32,130 - INFO - training batch 1301, loss: 0.425, 41632/60000 datapoints
2025-03-06 18:52:32,321 - INFO - training batch 1351, loss: 0.477, 43232/60000 datapoints
2025-03-06 18:52:32,515 - INFO - training batch 1401, loss: 0.532, 44832/60000 datapoints
2025-03-06 18:52:32,713 - INFO - training batch 1451, loss: 0.555, 46432/60000 datapoints
2025-03-06 18:52:32,906 - INFO - training batch 1501, loss: 0.527, 48032/60000 datapoints
2025-03-06 18:52:33,101 - INFO - training batch 1551, loss: 0.352, 49632/60000 datapoints
2025-03-06 18:52:33,296 - INFO - training batch 1601, loss: 0.477, 51232/60000 datapoints
2025-03-06 18:52:33,489 - INFO - training batch 1651, loss: 0.433, 52832/60000 datapoints
2025-03-06 18:52:33,685 - INFO - training batch 1701, loss: 0.494, 54432/60000 datapoints
2025-03-06 18:52:33,878 - INFO - training batch 1751, loss: 0.516, 56032/60000 datapoints
2025-03-06 18:52:34,077 - INFO - training batch 1801, loss: 0.552, 57632/60000 datapoints
2025-03-06 18:52:34,270 - INFO - training batch 1851, loss: 0.497, 59232/60000 datapoints
2025-03-06 18:52:34,372 - INFO - validation batch 1, loss: 0.343, 32/10016 datapoints
2025-03-06 18:52:34,531 - INFO - validation batch 51, loss: 0.520, 1632/10016 datapoints
2025-03-06 18:52:34,697 - INFO - validation batch 101, loss: 0.371, 3232/10016 datapoints
2025-03-06 18:52:34,851 - INFO - validation batch 151, loss: 0.518, 4832/10016 datapoints
2025-03-06 18:52:35,006 - INFO - validation batch 201, loss: 0.410, 6432/10016 datapoints
2025-03-06 18:52:35,159 - INFO - validation batch 251, loss: 0.355, 8032/10016 datapoints
2025-03-06 18:52:35,314 - INFO - validation batch 301, loss: 0.658, 9632/10016 datapoints
2025-03-06 18:52:35,353 - INFO - Epoch 104/800 done.
2025-03-06 18:52:35,354 - INFO - Final validation performance:
Loss: 0.454, top-1 acc: 0.874top-5 acc: 0.874
2025-03-06 18:52:35,354 - INFO - Beginning epoch 105/800
2025-03-06 18:52:35,360 - INFO - training batch 1, loss: 0.543, 32/60000 datapoints
2025-03-06 18:52:35,558 - INFO - training batch 51, loss: 0.452, 1632/60000 datapoints
2025-03-06 18:52:35,754 - INFO - training batch 101, loss: 0.491, 3232/60000 datapoints
2025-03-06 18:52:35,965 - INFO - training batch 151, loss: 0.397, 4832/60000 datapoints
2025-03-06 18:52:36,165 - INFO - training batch 201, loss: 0.417, 6432/60000 datapoints
2025-03-06 18:52:36,366 - INFO - training batch 251, loss: 0.388, 8032/60000 datapoints
2025-03-06 18:52:36,568 - INFO - training batch 301, loss: 0.349, 9632/60000 datapoints
2025-03-06 18:52:36,766 - INFO - training batch 351, loss: 0.536, 11232/60000 datapoints
2025-03-06 18:52:36,959 - INFO - training batch 401, loss: 0.452, 12832/60000 datapoints
2025-03-06 18:52:37,155 - INFO - training batch 451, loss: 0.379, 14432/60000 datapoints
2025-03-06 18:52:37,348 - INFO - training batch 501, loss: 0.627, 16032/60000 datapoints
2025-03-06 18:52:37,542 - INFO - training batch 551, loss: 0.444, 17632/60000 datapoints
2025-03-06 18:52:37,755 - INFO - training batch 601, loss: 0.767, 19232/60000 datapoints
2025-03-06 18:52:37,950 - INFO - training batch 651, loss: 0.432, 20832/60000 datapoints
2025-03-06 18:52:38,147 - INFO - training batch 701, loss: 0.449, 22432/60000 datapoints
2025-03-06 18:52:38,342 - INFO - training batch 751, loss: 0.516, 24032/60000 datapoints
2025-03-06 18:52:38,537 - INFO - training batch 801, loss: 0.704, 25632/60000 datapoints
2025-03-06 18:52:38,733 - INFO - training batch 851, loss: 0.515, 27232/60000 datapoints
2025-03-06 18:52:38,927 - INFO - training batch 901, loss: 0.390, 28832/60000 datapoints
2025-03-06 18:52:39,124 - INFO - training batch 951, loss: 0.318, 30432/60000 datapoints
2025-03-06 18:52:39,316 - INFO - training batch 1001, loss: 0.460, 32032/60000 datapoints
2025-03-06 18:52:39,511 - INFO - training batch 1051, loss: 0.362, 33632/60000 datapoints
2025-03-06 18:52:39,707 - INFO - training batch 1101, loss: 0.332, 35232/60000 datapoints
2025-03-06 18:52:39,902 - INFO - training batch 1151, loss: 0.451, 36832/60000 datapoints
2025-03-06 18:52:40,102 - INFO - training batch 1201, loss: 0.419, 38432/60000 datapoints
2025-03-06 18:52:40,297 - INFO - training batch 1251, loss: 0.483, 40032/60000 datapoints
2025-03-06 18:52:40,492 - INFO - training batch 1301, loss: 0.668, 41632/60000 datapoints
2025-03-06 18:52:40,690 - INFO - training batch 1351, loss: 0.548, 43232/60000 datapoints
2025-03-06 18:52:40,883 - INFO - training batch 1401, loss: 0.469, 44832/60000 datapoints
2025-03-06 18:52:41,079 - INFO - training batch 1451, loss: 0.525, 46432/60000 datapoints
2025-03-06 18:52:41,273 - INFO - training batch 1501, loss: 0.569, 48032/60000 datapoints
2025-03-06 18:52:41,464 - INFO - training batch 1551, loss: 0.354, 49632/60000 datapoints
2025-03-06 18:52:41,661 - INFO - training batch 1601, loss: 0.556, 51232/60000 datapoints
2025-03-06 18:52:41,854 - INFO - training batch 1651, loss: 0.346, 52832/60000 datapoints
2025-03-06 18:52:42,045 - INFO - training batch 1701, loss: 0.474, 54432/60000 datapoints
2025-03-06 18:52:42,240 - INFO - training batch 1751, loss: 0.299, 56032/60000 datapoints
2025-03-06 18:52:42,433 - INFO - training batch 1801, loss: 0.556, 57632/60000 datapoints
2025-03-06 18:52:42,631 - INFO - training batch 1851, loss: 0.447, 59232/60000 datapoints
2025-03-06 18:52:42,732 - INFO - validation batch 1, loss: 0.540, 32/10016 datapoints
2025-03-06 18:52:42,884 - INFO - validation batch 51, loss: 0.428, 1632/10016 datapoints
2025-03-06 18:52:43,035 - INFO - validation batch 101, loss: 0.336, 3232/10016 datapoints
2025-03-06 18:52:43,188 - INFO - validation batch 151, loss: 0.534, 4832/10016 datapoints
2025-03-06 18:52:43,337 - INFO - validation batch 201, loss: 0.418, 6432/10016 datapoints
2025-03-06 18:52:43,488 - INFO - validation batch 251, loss: 0.418, 8032/10016 datapoints
2025-03-06 18:52:43,642 - INFO - validation batch 301, loss: 0.602, 9632/10016 datapoints
2025-03-06 18:52:43,679 - INFO - Epoch 105/800 done.
2025-03-06 18:52:43,679 - INFO - Final validation performance:
Loss: 0.468, top-1 acc: 0.875top-5 acc: 0.875
2025-03-06 18:52:43,680 - INFO - Beginning epoch 106/800
2025-03-06 18:52:43,687 - INFO - training batch 1, loss: 0.266, 32/60000 datapoints
2025-03-06 18:52:43,893 - INFO - training batch 51, loss: 0.495, 1632/60000 datapoints
2025-03-06 18:52:44,096 - INFO - training batch 101, loss: 0.439, 3232/60000 datapoints
2025-03-06 18:52:44,290 - INFO - training batch 151, loss: 0.425, 4832/60000 datapoints
2025-03-06 18:52:44,489 - INFO - training batch 201, loss: 0.442, 6432/60000 datapoints
2025-03-06 18:52:44,715 - INFO - training batch 251, loss: 0.544, 8032/60000 datapoints
2025-03-06 18:52:44,922 - INFO - training batch 301, loss: 0.594, 9632/60000 datapoints
2025-03-06 18:52:45,122 - INFO - training batch 351, loss: 0.481, 11232/60000 datapoints
2025-03-06 18:52:45,319 - INFO - training batch 401, loss: 0.615, 12832/60000 datapoints
2025-03-06 18:52:45,515 - INFO - training batch 451, loss: 0.434, 14432/60000 datapoints
2025-03-06 18:52:45,713 - INFO - training batch 501, loss: 0.367, 16032/60000 datapoints
2025-03-06 18:52:45,911 - INFO - training batch 551, loss: 0.397, 17632/60000 datapoints
2025-03-06 18:52:46,109 - INFO - training batch 601, loss: 0.735, 19232/60000 datapoints
2025-03-06 18:52:46,307 - INFO - training batch 651, loss: 0.456, 20832/60000 datapoints
2025-03-06 18:52:46,502 - INFO - training batch 701, loss: 0.428, 22432/60000 datapoints
2025-03-06 18:52:46,702 - INFO - training batch 751, loss: 0.549, 24032/60000 datapoints
2025-03-06 18:52:46,895 - INFO - training batch 801, loss: 0.602, 25632/60000 datapoints
2025-03-06 18:52:47,092 - INFO - training batch 851, loss: 0.568, 27232/60000 datapoints
2025-03-06 18:52:47,286 - INFO - training batch 901, loss: 0.707, 28832/60000 datapoints
2025-03-06 18:52:47,481 - INFO - training batch 951, loss: 0.508, 30432/60000 datapoints
2025-03-06 18:52:47,678 - INFO - training batch 1001, loss: 0.379, 32032/60000 datapoints
2025-03-06 18:52:47,873 - INFO - training batch 1051, loss: 0.488, 33632/60000 datapoints
2025-03-06 18:52:48,070 - INFO - training batch 1101, loss: 0.512, 35232/60000 datapoints
2025-03-06 18:52:48,265 - INFO - training batch 1151, loss: 0.375, 36832/60000 datapoints
2025-03-06 18:52:48,456 - INFO - training batch 1201, loss: 0.519, 38432/60000 datapoints
2025-03-06 18:52:48,652 - INFO - training batch 1251, loss: 0.750, 40032/60000 datapoints
2025-03-06 18:52:48,849 - INFO - training batch 1301, loss: 0.461, 41632/60000 datapoints
2025-03-06 18:52:49,043 - INFO - training batch 1351, loss: 0.338, 43232/60000 datapoints
2025-03-06 18:52:49,240 - INFO - training batch 1401, loss: 0.449, 44832/60000 datapoints
2025-03-06 18:52:49,432 - INFO - training batch 1451, loss: 0.800, 46432/60000 datapoints
2025-03-06 18:52:49,627 - INFO - training batch 1501, loss: 0.457, 48032/60000 datapoints
2025-03-06 18:52:49,831 - INFO - training batch 1551, loss: 0.603, 49632/60000 datapoints
2025-03-06 18:52:50,026 - INFO - training batch 1601, loss: 0.666, 51232/60000 datapoints
2025-03-06 18:52:50,224 - INFO - training batch 1651, loss: 0.618, 52832/60000 datapoints
2025-03-06 18:52:50,418 - INFO - training batch 1701, loss: 0.315, 54432/60000 datapoints
2025-03-06 18:52:50,614 - INFO - training batch 1751, loss: 0.596, 56032/60000 datapoints
2025-03-06 18:52:50,807 - INFO - training batch 1801, loss: 0.425, 57632/60000 datapoints
2025-03-06 18:52:51,001 - INFO - training batch 1851, loss: 0.499, 59232/60000 datapoints
2025-03-06 18:52:51,101 - INFO - validation batch 1, loss: 0.500, 32/10016 datapoints
2025-03-06 18:52:51,256 - INFO - validation batch 51, loss: 0.654, 1632/10016 datapoints
2025-03-06 18:52:51,410 - INFO - validation batch 101, loss: 0.509, 3232/10016 datapoints
2025-03-06 18:52:51,564 - INFO - validation batch 151, loss: 0.386, 4832/10016 datapoints
2025-03-06 18:52:51,720 - INFO - validation batch 201, loss: 0.306, 6432/10016 datapoints
2025-03-06 18:52:51,875 - INFO - validation batch 251, loss: 0.463, 8032/10016 datapoints
2025-03-06 18:52:52,026 - INFO - validation batch 301, loss: 0.398, 9632/10016 datapoints
2025-03-06 18:52:52,063 - INFO - Epoch 106/800 done.
2025-03-06 18:52:52,063 - INFO - Final validation performance:
Loss: 0.459, top-1 acc: 0.876top-5 acc: 0.876
2025-03-06 18:52:52,064 - INFO - Beginning epoch 107/800
2025-03-06 18:52:52,070 - INFO - training batch 1, loss: 0.504, 32/60000 datapoints
2025-03-06 18:52:52,280 - INFO - training batch 51, loss: 0.361, 1632/60000 datapoints
2025-03-06 18:52:52,474 - INFO - training batch 101, loss: 0.404, 3232/60000 datapoints
2025-03-06 18:52:52,671 - INFO - training batch 151, loss: 0.640, 4832/60000 datapoints
2025-03-06 18:52:52,874 - INFO - training batch 201, loss: 0.408, 6432/60000 datapoints
2025-03-06 18:52:53,074 - INFO - training batch 251, loss: 0.735, 8032/60000 datapoints
2025-03-06 18:52:53,268 - INFO - training batch 301, loss: 0.519, 9632/60000 datapoints
2025-03-06 18:52:53,467 - INFO - training batch 351, loss: 0.590, 11232/60000 datapoints
2025-03-06 18:52:53,664 - INFO - training batch 401, loss: 0.417, 12832/60000 datapoints
2025-03-06 18:52:53,861 - INFO - training batch 451, loss: 0.544, 14432/60000 datapoints
2025-03-06 18:52:54,054 - INFO - training batch 501, loss: 0.353, 16032/60000 datapoints
2025-03-06 18:52:54,251 - INFO - training batch 551, loss: 0.540, 17632/60000 datapoints
2025-03-06 18:52:54,446 - INFO - training batch 601, loss: 0.600, 19232/60000 datapoints
2025-03-06 18:52:54,644 - INFO - training batch 651, loss: 0.544, 20832/60000 datapoints
2025-03-06 18:52:54,856 - INFO - training batch 701, loss: 0.424, 22432/60000 datapoints
2025-03-06 18:52:55,055 - INFO - training batch 751, loss: 0.376, 24032/60000 datapoints
2025-03-06 18:52:55,247 - INFO - training batch 801, loss: 0.574, 25632/60000 datapoints
2025-03-06 18:52:55,441 - INFO - training batch 851, loss: 0.534, 27232/60000 datapoints
2025-03-06 18:52:55,638 - INFO - training batch 901, loss: 0.485, 28832/60000 datapoints
2025-03-06 18:52:55,833 - INFO - training batch 951, loss: 0.518, 30432/60000 datapoints
2025-03-06 18:52:56,034 - INFO - training batch 1001, loss: 0.635, 32032/60000 datapoints
2025-03-06 18:52:56,252 - INFO - training batch 1051, loss: 0.455, 33632/60000 datapoints
2025-03-06 18:52:56,451 - INFO - training batch 1101, loss: 0.300, 35232/60000 datapoints
2025-03-06 18:52:56,651 - INFO - training batch 1151, loss: 0.242, 36832/60000 datapoints
2025-03-06 18:52:56,846 - INFO - training batch 1201, loss: 0.499, 38432/60000 datapoints
2025-03-06 18:52:57,040 - INFO - training batch 1251, loss: 0.280, 40032/60000 datapoints
2025-03-06 18:52:57,234 - INFO - training batch 1301, loss: 0.307, 41632/60000 datapoints
2025-03-06 18:52:57,429 - INFO - training batch 1351, loss: 0.304, 43232/60000 datapoints
2025-03-06 18:52:57,625 - INFO - training batch 1401, loss: 0.485, 44832/60000 datapoints
2025-03-06 18:52:57,827 - INFO - training batch 1451, loss: 0.580, 46432/60000 datapoints
2025-03-06 18:52:58,023 - INFO - training batch 1501, loss: 0.249, 48032/60000 datapoints
2025-03-06 18:52:58,220 - INFO - training batch 1551, loss: 0.370, 49632/60000 datapoints
2025-03-06 18:52:58,413 - INFO - training batch 1601, loss: 0.464, 51232/60000 datapoints
2025-03-06 18:52:58,611 - INFO - training batch 1651, loss: 0.452, 52832/60000 datapoints
2025-03-06 18:52:58,803 - INFO - training batch 1701, loss: 0.793, 54432/60000 datapoints
2025-03-06 18:52:58,999 - INFO - training batch 1751, loss: 0.589, 56032/60000 datapoints
2025-03-06 18:52:59,194 - INFO - training batch 1801, loss: 0.546, 57632/60000 datapoints
2025-03-06 18:52:59,388 - INFO - training batch 1851, loss: 0.508, 59232/60000 datapoints
2025-03-06 18:52:59,488 - INFO - validation batch 1, loss: 0.523, 32/10016 datapoints
2025-03-06 18:52:59,642 - INFO - validation batch 51, loss: 0.461, 1632/10016 datapoints
2025-03-06 18:52:59,794 - INFO - validation batch 101, loss: 0.521, 3232/10016 datapoints
2025-03-06 18:52:59,946 - INFO - validation batch 151, loss: 0.369, 4832/10016 datapoints
2025-03-06 18:53:00,098 - INFO - validation batch 201, loss: 0.614, 6432/10016 datapoints
2025-03-06 18:53:00,253 - INFO - validation batch 251, loss: 0.299, 8032/10016 datapoints
2025-03-06 18:53:00,405 - INFO - validation batch 301, loss: 0.719, 9632/10016 datapoints
2025-03-06 18:53:00,442 - INFO - Epoch 107/800 done.
2025-03-06 18:53:00,442 - INFO - Final validation performance:
Loss: 0.501, top-1 acc: 0.876top-5 acc: 0.876
2025-03-06 18:53:00,443 - INFO - Beginning epoch 108/800
2025-03-06 18:53:00,449 - INFO - training batch 1, loss: 0.578, 32/60000 datapoints
2025-03-06 18:53:00,661 - INFO - training batch 51, loss: 0.462, 1632/60000 datapoints
2025-03-06 18:53:00,860 - INFO - training batch 101, loss: 0.605, 3232/60000 datapoints
2025-03-06 18:53:01,053 - INFO - training batch 151, loss: 0.654, 4832/60000 datapoints
2025-03-06 18:53:01,257 - INFO - training batch 201, loss: 0.642, 6432/60000 datapoints
2025-03-06 18:53:01,457 - INFO - training batch 251, loss: 0.431, 8032/60000 datapoints
2025-03-06 18:53:01,657 - INFO - training batch 301, loss: 0.589, 9632/60000 datapoints
2025-03-06 18:53:01,853 - INFO - training batch 351, loss: 0.646, 11232/60000 datapoints
2025-03-06 18:53:02,046 - INFO - training batch 401, loss: 0.502, 12832/60000 datapoints
2025-03-06 18:53:02,243 - INFO - training batch 451, loss: 0.524, 14432/60000 datapoints
2025-03-06 18:53:02,438 - INFO - training batch 501, loss: 0.620, 16032/60000 datapoints
2025-03-06 18:53:02,633 - INFO - training batch 551, loss: 0.467, 17632/60000 datapoints
2025-03-06 18:53:02,829 - INFO - training batch 601, loss: 0.649, 19232/60000 datapoints
2025-03-06 18:53:03,025 - INFO - training batch 651, loss: 0.306, 20832/60000 datapoints
2025-03-06 18:53:03,219 - INFO - training batch 701, loss: 0.710, 22432/60000 datapoints
2025-03-06 18:53:03,410 - INFO - training batch 751, loss: 0.621, 24032/60000 datapoints
2025-03-06 18:53:03,601 - INFO - training batch 801, loss: 0.533, 25632/60000 datapoints
2025-03-06 18:53:03,797 - INFO - training batch 851, loss: 0.606, 27232/60000 datapoints
2025-03-06 18:53:03,994 - INFO - training batch 901, loss: 0.549, 28832/60000 datapoints
2025-03-06 18:53:04,189 - INFO - training batch 951, loss: 0.359, 30432/60000 datapoints
2025-03-06 18:53:04,383 - INFO - training batch 1001, loss: 0.430, 32032/60000 datapoints
2025-03-06 18:53:04,576 - INFO - training batch 1051, loss: 0.667, 33632/60000 datapoints
2025-03-06 18:53:04,778 - INFO - training batch 1101, loss: 0.428, 35232/60000 datapoints
2025-03-06 18:53:04,992 - INFO - training batch 1151, loss: 0.490, 36832/60000 datapoints
2025-03-06 18:53:05,187 - INFO - training batch 1201, loss: 0.473, 38432/60000 datapoints
2025-03-06 18:53:05,382 - INFO - training batch 1251, loss: 0.355, 40032/60000 datapoints
2025-03-06 18:53:05,575 - INFO - training batch 1301, loss: 0.358, 41632/60000 datapoints
2025-03-06 18:53:05,772 - INFO - training batch 1351, loss: 0.450, 43232/60000 datapoints
2025-03-06 18:53:05,967 - INFO - training batch 1401, loss: 0.301, 44832/60000 datapoints
2025-03-06 18:53:06,160 - INFO - training batch 1451, loss: 0.527, 46432/60000 datapoints
2025-03-06 18:53:06,357 - INFO - training batch 1501, loss: 0.351, 48032/60000 datapoints
2025-03-06 18:53:06,555 - INFO - training batch 1551, loss: 0.571, 49632/60000 datapoints
2025-03-06 18:53:06,751 - INFO - training batch 1601, loss: 0.460, 51232/60000 datapoints
2025-03-06 18:53:06,947 - INFO - training batch 1651, loss: 0.499, 52832/60000 datapoints
2025-03-06 18:53:07,141 - INFO - training batch 1701, loss: 0.855, 54432/60000 datapoints
2025-03-06 18:53:07,334 - INFO - training batch 1751, loss: 0.451, 56032/60000 datapoints
2025-03-06 18:53:07,528 - INFO - training batch 1801, loss: 0.381, 57632/60000 datapoints
2025-03-06 18:53:07,727 - INFO - training batch 1851, loss: 0.442, 59232/60000 datapoints
2025-03-06 18:53:07,828 - INFO - validation batch 1, loss: 0.379, 32/10016 datapoints
2025-03-06 18:53:07,982 - INFO - validation batch 51, loss: 0.509, 1632/10016 datapoints
2025-03-06 18:53:08,138 - INFO - validation batch 101, loss: 0.391, 3232/10016 datapoints
2025-03-06 18:53:08,290 - INFO - validation batch 151, loss: 0.567, 4832/10016 datapoints
2025-03-06 18:53:08,445 - INFO - validation batch 201, loss: 0.695, 6432/10016 datapoints
2025-03-06 18:53:08,597 - INFO - validation batch 251, loss: 0.544, 8032/10016 datapoints
2025-03-06 18:53:08,749 - INFO - validation batch 301, loss: 0.352, 9632/10016 datapoints
2025-03-06 18:53:08,784 - INFO - Epoch 108/800 done.
2025-03-06 18:53:08,785 - INFO - Final validation performance:
Loss: 0.491, top-1 acc: 0.876top-5 acc: 0.876
2025-03-06 18:53:08,785 - INFO - Beginning epoch 109/800
2025-03-06 18:53:08,794 - INFO - training batch 1, loss: 0.360, 32/60000 datapoints
2025-03-06 18:53:08,997 - INFO - training batch 51, loss: 0.394, 1632/60000 datapoints
2025-03-06 18:53:09,189 - INFO - training batch 101, loss: 0.322, 3232/60000 datapoints
2025-03-06 18:53:09,391 - INFO - training batch 151, loss: 0.549, 4832/60000 datapoints
2025-03-06 18:53:09,589 - INFO - training batch 201, loss: 0.510, 6432/60000 datapoints
2025-03-06 18:53:09,783 - INFO - training batch 251, loss: 0.388, 8032/60000 datapoints
2025-03-06 18:53:09,985 - INFO - training batch 301, loss: 0.457, 9632/60000 datapoints
2025-03-06 18:53:10,181 - INFO - training batch 351, loss: 0.454, 11232/60000 datapoints
2025-03-06 18:53:10,380 - INFO - training batch 401, loss: 0.442, 12832/60000 datapoints
2025-03-06 18:53:10,573 - INFO - training batch 451, loss: 0.497, 14432/60000 datapoints
2025-03-06 18:53:10,770 - INFO - training batch 501, loss: 0.399, 16032/60000 datapoints
2025-03-06 18:53:10,964 - INFO - training batch 551, loss: 0.587, 17632/60000 datapoints
2025-03-06 18:53:11,155 - INFO - training batch 601, loss: 0.610, 19232/60000 datapoints
2025-03-06 18:53:11,348 - INFO - training batch 651, loss: 0.478, 20832/60000 datapoints
2025-03-06 18:53:11,543 - INFO - training batch 701, loss: 0.561, 22432/60000 datapoints
2025-03-06 18:53:11,740 - INFO - training batch 751, loss: 0.628, 24032/60000 datapoints
2025-03-06 18:53:11,937 - INFO - training batch 801, loss: 0.726, 25632/60000 datapoints
2025-03-06 18:53:12,130 - INFO - training batch 851, loss: 0.562, 27232/60000 datapoints
2025-03-06 18:53:12,326 - INFO - training batch 901, loss: 0.454, 28832/60000 datapoints
2025-03-06 18:53:12,519 - INFO - training batch 951, loss: 0.587, 30432/60000 datapoints
2025-03-06 18:53:12,714 - INFO - training batch 1001, loss: 0.729, 32032/60000 datapoints
2025-03-06 18:53:12,909 - INFO - training batch 1051, loss: 0.562, 33632/60000 datapoints
2025-03-06 18:53:13,105 - INFO - training batch 1101, loss: 0.443, 35232/60000 datapoints
2025-03-06 18:53:13,298 - INFO - training batch 1151, loss: 0.644, 36832/60000 datapoints
2025-03-06 18:53:13,495 - INFO - training batch 1201, loss: 0.312, 38432/60000 datapoints
2025-03-06 18:53:13,692 - INFO - training batch 1251, loss: 0.229, 40032/60000 datapoints
2025-03-06 18:53:13,887 - INFO - training batch 1301, loss: 0.483, 41632/60000 datapoints
2025-03-06 18:53:14,081 - INFO - training batch 1351, loss: 0.305, 43232/60000 datapoints
2025-03-06 18:53:14,278 - INFO - training batch 1401, loss: 0.461, 44832/60000 datapoints
2025-03-06 18:53:14,470 - INFO - training batch 1451, loss: 0.868, 46432/60000 datapoints
2025-03-06 18:53:14,667 - INFO - training batch 1501, loss: 0.427, 48032/60000 datapoints
2025-03-06 18:53:14,869 - INFO - training batch 1551, loss: 0.516, 49632/60000 datapoints
2025-03-06 18:53:15,076 - INFO - training batch 1601, loss: 0.360, 51232/60000 datapoints
2025-03-06 18:53:15,277 - INFO - training batch 1651, loss: 0.365, 52832/60000 datapoints
2025-03-06 18:53:15,472 - INFO - training batch 1701, loss: 0.466, 54432/60000 datapoints
2025-03-06 18:53:15,668 - INFO - training batch 1751, loss: 0.505, 56032/60000 datapoints
2025-03-06 18:53:15,863 - INFO - training batch 1801, loss: 0.326, 57632/60000 datapoints
2025-03-06 18:53:16,057 - INFO - training batch 1851, loss: 0.347, 59232/60000 datapoints
2025-03-06 18:53:16,167 - INFO - validation batch 1, loss: 0.533, 32/10016 datapoints
2025-03-06 18:53:16,343 - INFO - validation batch 51, loss: 0.441, 1632/10016 datapoints
2025-03-06 18:53:16,496 - INFO - validation batch 101, loss: 0.450, 3232/10016 datapoints
2025-03-06 18:53:16,658 - INFO - validation batch 151, loss: 0.360, 4832/10016 datapoints
2025-03-06 18:53:16,811 - INFO - validation batch 201, loss: 0.457, 6432/10016 datapoints
2025-03-06 18:53:16,964 - INFO - validation batch 251, loss: 0.367, 8032/10016 datapoints
2025-03-06 18:53:17,117 - INFO - validation batch 301, loss: 0.401, 9632/10016 datapoints
2025-03-06 18:53:17,155 - INFO - Epoch 109/800 done.
2025-03-06 18:53:17,155 - INFO - Final validation performance:
Loss: 0.430, top-1 acc: 0.877top-5 acc: 0.877
2025-03-06 18:53:17,156 - INFO - Beginning epoch 110/800
2025-03-06 18:53:17,162 - INFO - training batch 1, loss: 0.555, 32/60000 datapoints
2025-03-06 18:53:17,356 - INFO - training batch 51, loss: 0.323, 1632/60000 datapoints
2025-03-06 18:53:17,550 - INFO - training batch 101, loss: 0.366, 3232/60000 datapoints
2025-03-06 18:53:17,756 - INFO - training batch 151, loss: 0.505, 4832/60000 datapoints
2025-03-06 18:53:17,952 - INFO - training batch 201, loss: 0.734, 6432/60000 datapoints
2025-03-06 18:53:18,151 - INFO - training batch 251, loss: 0.454, 8032/60000 datapoints
2025-03-06 18:53:18,351 - INFO - training batch 301, loss: 0.381, 9632/60000 datapoints
2025-03-06 18:53:18,548 - INFO - training batch 351, loss: 0.368, 11232/60000 datapoints
2025-03-06 18:53:18,743 - INFO - training batch 401, loss: 0.511, 12832/60000 datapoints
2025-03-06 18:53:18,938 - INFO - training batch 451, loss: 0.409, 14432/60000 datapoints
2025-03-06 18:53:19,130 - INFO - training batch 501, loss: 0.294, 16032/60000 datapoints
2025-03-06 18:53:19,322 - INFO - training batch 551, loss: 0.437, 17632/60000 datapoints
2025-03-06 18:53:19,513 - INFO - training batch 601, loss: 0.644, 19232/60000 datapoints
2025-03-06 18:53:19,712 - INFO - training batch 651, loss: 0.476, 20832/60000 datapoints
2025-03-06 18:53:19,906 - INFO - training batch 701, loss: 0.412, 22432/60000 datapoints
2025-03-06 18:53:20,103 - INFO - training batch 751, loss: 0.325, 24032/60000 datapoints
2025-03-06 18:53:20,301 - INFO - training batch 801, loss: 0.334, 25632/60000 datapoints
2025-03-06 18:53:20,495 - INFO - training batch 851, loss: 0.515, 27232/60000 datapoints
2025-03-06 18:53:20,689 - INFO - training batch 901, loss: 0.500, 28832/60000 datapoints
2025-03-06 18:53:20,883 - INFO - training batch 951, loss: 0.370, 30432/60000 datapoints
2025-03-06 18:53:21,077 - INFO - training batch 1001, loss: 0.409, 32032/60000 datapoints
2025-03-06 18:53:21,271 - INFO - training batch 1051, loss: 0.527, 33632/60000 datapoints
2025-03-06 18:53:21,473 - INFO - training batch 1101, loss: 0.463, 35232/60000 datapoints
2025-03-06 18:53:21,672 - INFO - training batch 1151, loss: 0.433, 36832/60000 datapoints
2025-03-06 18:53:21,867 - INFO - training batch 1201, loss: 0.428, 38432/60000 datapoints
2025-03-06 18:53:22,059 - INFO - training batch 1251, loss: 0.449, 40032/60000 datapoints
2025-03-06 18:53:22,257 - INFO - training batch 1301, loss: 0.761, 41632/60000 datapoints
2025-03-06 18:53:22,453 - INFO - training batch 1351, loss: 0.439, 43232/60000 datapoints
2025-03-06 18:53:22,650 - INFO - training batch 1401, loss: 0.407, 44832/60000 datapoints
2025-03-06 18:53:22,844 - INFO - training batch 1451, loss: 0.469, 46432/60000 datapoints
2025-03-06 18:53:23,038 - INFO - training batch 1501, loss: 0.495, 48032/60000 datapoints
2025-03-06 18:53:23,232 - INFO - training batch 1551, loss: 0.633, 49632/60000 datapoints
2025-03-06 18:53:23,427 - INFO - training batch 1601, loss: 0.639, 51232/60000 datapoints
2025-03-06 18:53:23,624 - INFO - training batch 1651, loss: 0.580, 52832/60000 datapoints
2025-03-06 18:53:23,820 - INFO - training batch 1701, loss: 0.472, 54432/60000 datapoints
2025-03-06 18:53:24,014 - INFO - training batch 1751, loss: 0.519, 56032/60000 datapoints
2025-03-06 18:53:24,211 - INFO - training batch 1801, loss: 0.431, 57632/60000 datapoints
2025-03-06 18:53:24,405 - INFO - training batch 1851, loss: 0.430, 59232/60000 datapoints
2025-03-06 18:53:24,508 - INFO - validation batch 1, loss: 0.507, 32/10016 datapoints
2025-03-06 18:53:24,666 - INFO - validation batch 51, loss: 0.431, 1632/10016 datapoints
2025-03-06 18:53:24,817 - INFO - validation batch 101, loss: 0.416, 3232/10016 datapoints
2025-03-06 18:53:24,993 - INFO - validation batch 151, loss: 0.830, 4832/10016 datapoints
2025-03-06 18:53:25,161 - INFO - validation batch 201, loss: 0.476, 6432/10016 datapoints
2025-03-06 18:53:25,312 - INFO - validation batch 251, loss: 0.354, 8032/10016 datapoints
2025-03-06 18:53:25,467 - INFO - validation batch 301, loss: 0.338, 9632/10016 datapoints
2025-03-06 18:53:25,505 - INFO - Epoch 110/800 done.
2025-03-06 18:53:25,505 - INFO - Final validation performance:
Loss: 0.479, top-1 acc: 0.878top-5 acc: 0.878
2025-03-06 18:53:25,506 - INFO - Beginning epoch 111/800
2025-03-06 18:53:25,512 - INFO - training batch 1, loss: 0.377, 32/60000 datapoints
2025-03-06 18:53:25,724 - INFO - training batch 51, loss: 0.746, 1632/60000 datapoints
2025-03-06 18:53:25,921 - INFO - training batch 101, loss: 0.450, 3232/60000 datapoints
2025-03-06 18:53:26,117 - INFO - training batch 151, loss: 0.534, 4832/60000 datapoints
2025-03-06 18:53:26,319 - INFO - training batch 201, loss: 0.460, 6432/60000 datapoints
2025-03-06 18:53:26,516 - INFO - training batch 251, loss: 0.332, 8032/60000 datapoints
2025-03-06 18:53:26,721 - INFO - training batch 301, loss: 0.601, 9632/60000 datapoints
2025-03-06 18:53:26,914 - INFO - training batch 351, loss: 0.460, 11232/60000 datapoints
2025-03-06 18:53:27,104 - INFO - training batch 401, loss: 0.240, 12832/60000 datapoints
2025-03-06 18:53:27,300 - INFO - training batch 451, loss: 0.568, 14432/60000 datapoints
2025-03-06 18:53:27,494 - INFO - training batch 501, loss: 1.117, 16032/60000 datapoints
2025-03-06 18:53:27,690 - INFO - training batch 551, loss: 0.491, 17632/60000 datapoints
2025-03-06 18:53:27,884 - INFO - training batch 601, loss: 0.371, 19232/60000 datapoints
2025-03-06 18:53:28,111 - INFO - training batch 651, loss: 0.709, 20832/60000 datapoints
2025-03-06 18:53:28,309 - INFO - training batch 701, loss: 0.444, 22432/60000 datapoints
2025-03-06 18:53:28,503 - INFO - training batch 751, loss: 0.430, 24032/60000 datapoints
2025-03-06 18:53:28,700 - INFO - training batch 801, loss: 0.504, 25632/60000 datapoints
2025-03-06 18:53:28,892 - INFO - training batch 851, loss: 0.565, 27232/60000 datapoints
2025-03-06 18:53:29,085 - INFO - training batch 901, loss: 0.436, 28832/60000 datapoints
2025-03-06 18:53:29,279 - INFO - training batch 951, loss: 0.595, 30432/60000 datapoints
2025-03-06 18:53:29,471 - INFO - training batch 1001, loss: 0.462, 32032/60000 datapoints
2025-03-06 18:53:29,666 - INFO - training batch 1051, loss: 0.480, 33632/60000 datapoints
2025-03-06 18:53:29,859 - INFO - training batch 1101, loss: 0.282, 35232/60000 datapoints
2025-03-06 18:53:30,056 - INFO - training batch 1151, loss: 0.407, 36832/60000 datapoints
2025-03-06 18:53:30,253 - INFO - training batch 1201, loss: 0.404, 38432/60000 datapoints
2025-03-06 18:53:30,447 - INFO - training batch 1251, loss: 0.342, 40032/60000 datapoints
2025-03-06 18:53:30,643 - INFO - training batch 1301, loss: 0.327, 41632/60000 datapoints
2025-03-06 18:53:30,839 - INFO - training batch 1351, loss: 0.798, 43232/60000 datapoints
2025-03-06 18:53:31,032 - INFO - training batch 1401, loss: 0.706, 44832/60000 datapoints
2025-03-06 18:53:31,231 - INFO - training batch 1451, loss: 0.357, 46432/60000 datapoints
2025-03-06 18:53:31,427 - INFO - training batch 1501, loss: 0.594, 48032/60000 datapoints
2025-03-06 18:53:31,628 - INFO - training batch 1551, loss: 0.435, 49632/60000 datapoints
2025-03-06 18:53:31,827 - INFO - training batch 1601, loss: 0.328, 51232/60000 datapoints
2025-03-06 18:53:32,023 - INFO - training batch 1651, loss: 0.349, 52832/60000 datapoints
2025-03-06 18:53:32,215 - INFO - training batch 1701, loss: 0.394, 54432/60000 datapoints
2025-03-06 18:53:32,411 - INFO - training batch 1751, loss: 0.430, 56032/60000 datapoints
2025-03-06 18:53:32,606 - INFO - training batch 1801, loss: 0.377, 57632/60000 datapoints
2025-03-06 18:53:32,799 - INFO - training batch 1851, loss: 0.318, 59232/60000 datapoints
2025-03-06 18:53:32,900 - INFO - validation batch 1, loss: 0.458, 32/10016 datapoints
2025-03-06 18:53:33,052 - INFO - validation batch 51, loss: 0.630, 1632/10016 datapoints
2025-03-06 18:53:33,205 - INFO - validation batch 101, loss: 0.490, 3232/10016 datapoints
2025-03-06 18:53:33,354 - INFO - validation batch 151, loss: 0.478, 4832/10016 datapoints
2025-03-06 18:53:33,507 - INFO - validation batch 201, loss: 0.402, 6432/10016 datapoints
2025-03-06 18:53:33,660 - INFO - validation batch 251, loss: 0.440, 8032/10016 datapoints
2025-03-06 18:53:33,815 - INFO - validation batch 301, loss: 0.328, 9632/10016 datapoints
2025-03-06 18:53:33,853 - INFO - Epoch 111/800 done.
2025-03-06 18:53:33,853 - INFO - Final validation performance:
Loss: 0.461, top-1 acc: 0.879top-5 acc: 0.879
2025-03-06 18:53:33,853 - INFO - Beginning epoch 112/800
2025-03-06 18:53:33,860 - INFO - training batch 1, loss: 0.669, 32/60000 datapoints
2025-03-06 18:53:34,053 - INFO - training batch 51, loss: 0.430, 1632/60000 datapoints
2025-03-06 18:53:34,249 - INFO - training batch 101, loss: 0.627, 3232/60000 datapoints
2025-03-06 18:53:34,452 - INFO - training batch 151, loss: 0.442, 4832/60000 datapoints
2025-03-06 18:53:34,651 - INFO - training batch 201, loss: 0.460, 6432/60000 datapoints
2025-03-06 18:53:34,850 - INFO - training batch 251, loss: 0.356, 8032/60000 datapoints
2025-03-06 18:53:35,064 - INFO - training batch 301, loss: 0.514, 9632/60000 datapoints
2025-03-06 18:53:35,267 - INFO - training batch 351, loss: 0.409, 11232/60000 datapoints
2025-03-06 18:53:35,459 - INFO - training batch 401, loss: 0.380, 12832/60000 datapoints
2025-03-06 18:53:35,655 - INFO - training batch 451, loss: 0.633, 14432/60000 datapoints
2025-03-06 18:53:35,846 - INFO - training batch 501, loss: 0.317, 16032/60000 datapoints
2025-03-06 18:53:36,041 - INFO - training batch 551, loss: 0.675, 17632/60000 datapoints
2025-03-06 18:53:36,239 - INFO - training batch 601, loss: 0.275, 19232/60000 datapoints
2025-03-06 18:53:36,439 - INFO - training batch 651, loss: 0.477, 20832/60000 datapoints
2025-03-06 18:53:36,638 - INFO - training batch 701, loss: 0.403, 22432/60000 datapoints
2025-03-06 18:53:36,829 - INFO - training batch 751, loss: 0.785, 24032/60000 datapoints
2025-03-06 18:53:37,024 - INFO - training batch 801, loss: 0.611, 25632/60000 datapoints
2025-03-06 18:53:37,219 - INFO - training batch 851, loss: 0.466, 27232/60000 datapoints
2025-03-06 18:53:37,413 - INFO - training batch 901, loss: 0.428, 28832/60000 datapoints
2025-03-06 18:53:37,621 - INFO - training batch 951, loss: 0.404, 30432/60000 datapoints
2025-03-06 18:53:37,817 - INFO - training batch 1001, loss: 0.606, 32032/60000 datapoints
2025-03-06 18:53:38,012 - INFO - training batch 1051, loss: 0.456, 33632/60000 datapoints
2025-03-06 18:53:38,207 - INFO - training batch 1101, loss: 0.352, 35232/60000 datapoints
2025-03-06 18:53:38,405 - INFO - training batch 1151, loss: 0.420, 36832/60000 datapoints
2025-03-06 18:53:38,600 - INFO - training batch 1201, loss: 0.445, 38432/60000 datapoints
2025-03-06 18:53:38,796 - INFO - training batch 1251, loss: 0.571, 40032/60000 datapoints
2025-03-06 18:53:38,992 - INFO - training batch 1301, loss: 0.228, 41632/60000 datapoints
2025-03-06 18:53:39,187 - INFO - training batch 1351, loss: 0.480, 43232/60000 datapoints
2025-03-06 18:53:39,382 - INFO - training batch 1401, loss: 0.482, 44832/60000 datapoints
2025-03-06 18:53:39,573 - INFO - training batch 1451, loss: 0.419, 46432/60000 datapoints
2025-03-06 18:53:39,768 - INFO - training batch 1501, loss: 0.451, 48032/60000 datapoints
2025-03-06 18:53:39,965 - INFO - training batch 1551, loss: 0.377, 49632/60000 datapoints
2025-03-06 18:53:40,160 - INFO - training batch 1601, loss: 0.677, 51232/60000 datapoints
2025-03-06 18:53:40,357 - INFO - training batch 1651, loss: 0.425, 52832/60000 datapoints
2025-03-06 18:53:40,550 - INFO - training batch 1701, loss: 0.309, 54432/60000 datapoints
2025-03-06 18:53:40,747 - INFO - training batch 1751, loss: 0.323, 56032/60000 datapoints
2025-03-06 18:53:40,942 - INFO - training batch 1801, loss: 0.548, 57632/60000 datapoints
2025-03-06 18:53:41,137 - INFO - training batch 1851, loss: 0.614, 59232/60000 datapoints
2025-03-06 18:53:41,237 - INFO - validation batch 1, loss: 0.330, 32/10016 datapoints
2025-03-06 18:53:41,394 - INFO - validation batch 51, loss: 0.533, 1632/10016 datapoints
2025-03-06 18:53:41,548 - INFO - validation batch 101, loss: 0.554, 3232/10016 datapoints
2025-03-06 18:53:41,701 - INFO - validation batch 151, loss: 0.436, 4832/10016 datapoints
2025-03-06 18:53:41,853 - INFO - validation batch 201, loss: 0.643, 6432/10016 datapoints
2025-03-06 18:53:42,005 - INFO - validation batch 251, loss: 0.492, 8032/10016 datapoints
2025-03-06 18:53:42,160 - INFO - validation batch 301, loss: 0.428, 9632/10016 datapoints
2025-03-06 18:53:42,198 - INFO - Epoch 112/800 done.
2025-03-06 18:53:42,198 - INFO - Final validation performance:
Loss: 0.488, top-1 acc: 0.880top-5 acc: 0.880
2025-03-06 18:53:42,199 - INFO - Beginning epoch 113/800
2025-03-06 18:53:42,206 - INFO - training batch 1, loss: 0.424, 32/60000 datapoints
2025-03-06 18:53:42,414 - INFO - training batch 51, loss: 0.496, 1632/60000 datapoints
2025-03-06 18:53:42,607 - INFO - training batch 101, loss: 0.684, 3232/60000 datapoints
2025-03-06 18:53:42,798 - INFO - training batch 151, loss: 0.435, 4832/60000 datapoints
2025-03-06 18:53:42,992 - INFO - training batch 201, loss: 0.504, 6432/60000 datapoints
2025-03-06 18:53:43,187 - INFO - training batch 251, loss: 0.449, 8032/60000 datapoints
2025-03-06 18:53:43,380 - INFO - training batch 301, loss: 0.790, 9632/60000 datapoints
2025-03-06 18:53:43,574 - INFO - training batch 351, loss: 0.600, 11232/60000 datapoints
2025-03-06 18:53:43,770 - INFO - training batch 401, loss: 0.427, 12832/60000 datapoints
2025-03-06 18:53:43,968 - INFO - training batch 451, loss: 0.446, 14432/60000 datapoints
2025-03-06 18:53:44,160 - INFO - training batch 501, loss: 0.616, 16032/60000 datapoints
2025-03-06 18:53:44,354 - INFO - training batch 551, loss: 0.518, 17632/60000 datapoints
2025-03-06 18:53:44,546 - INFO - training batch 601, loss: 0.349, 19232/60000 datapoints
2025-03-06 18:53:44,739 - INFO - training batch 651, loss: 0.536, 20832/60000 datapoints
2025-03-06 18:53:44,932 - INFO - training batch 701, loss: 0.430, 22432/60000 datapoints
2025-03-06 18:53:45,131 - INFO - training batch 751, loss: 0.342, 24032/60000 datapoints
2025-03-06 18:53:45,337 - INFO - training batch 801, loss: 0.756, 25632/60000 datapoints
2025-03-06 18:53:45,535 - INFO - training batch 851, loss: 0.455, 27232/60000 datapoints
2025-03-06 18:53:45,739 - INFO - training batch 901, loss: 0.616, 28832/60000 datapoints
2025-03-06 18:53:45,934 - INFO - training batch 951, loss: 0.613, 30432/60000 datapoints
2025-03-06 18:53:46,128 - INFO - training batch 1001, loss: 0.465, 32032/60000 datapoints
2025-03-06 18:53:46,325 - INFO - training batch 1051, loss: 0.733, 33632/60000 datapoints
2025-03-06 18:53:46,517 - INFO - training batch 1101, loss: 0.389, 35232/60000 datapoints
2025-03-06 18:53:46,715 - INFO - training batch 1151, loss: 0.409, 36832/60000 datapoints
2025-03-06 18:53:46,909 - INFO - training batch 1201, loss: 0.831, 38432/60000 datapoints
2025-03-06 18:53:47,105 - INFO - training batch 1251, loss: 0.569, 40032/60000 datapoints
2025-03-06 18:53:47,297 - INFO - training batch 1301, loss: 0.701, 41632/60000 datapoints
2025-03-06 18:53:47,492 - INFO - training batch 1351, loss: 0.574, 43232/60000 datapoints
2025-03-06 18:53:47,686 - INFO - training batch 1401, loss: 0.370, 44832/60000 datapoints
2025-03-06 18:53:47,880 - INFO - training batch 1451, loss: 0.586, 46432/60000 datapoints
2025-03-06 18:53:48,075 - INFO - training batch 1501, loss: 0.545, 48032/60000 datapoints
2025-03-06 18:53:48,268 - INFO - training batch 1551, loss: 0.283, 49632/60000 datapoints
2025-03-06 18:53:48,465 - INFO - training batch 1601, loss: 0.358, 51232/60000 datapoints
2025-03-06 18:53:48,661 - INFO - training batch 1651, loss: 0.671, 52832/60000 datapoints
2025-03-06 18:53:48,854 - INFO - training batch 1701, loss: 0.844, 54432/60000 datapoints
2025-03-06 18:53:49,050 - INFO - training batch 1751, loss: 0.292, 56032/60000 datapoints
2025-03-06 18:53:49,241 - INFO - training batch 1801, loss: 0.575, 57632/60000 datapoints
2025-03-06 18:53:49,436 - INFO - training batch 1851, loss: 0.341, 59232/60000 datapoints
2025-03-06 18:53:49,536 - INFO - validation batch 1, loss: 0.437, 32/10016 datapoints
2025-03-06 18:53:49,690 - INFO - validation batch 51, loss: 0.420, 1632/10016 datapoints
2025-03-06 18:53:49,842 - INFO - validation batch 101, loss: 0.670, 3232/10016 datapoints
2025-03-06 18:53:49,996 - INFO - validation batch 151, loss: 0.270, 4832/10016 datapoints
2025-03-06 18:53:50,148 - INFO - validation batch 201, loss: 0.455, 6432/10016 datapoints
2025-03-06 18:53:50,303 - INFO - validation batch 251, loss: 0.419, 8032/10016 datapoints
2025-03-06 18:53:50,456 - INFO - validation batch 301, loss: 0.318, 9632/10016 datapoints
2025-03-06 18:53:50,492 - INFO - Epoch 113/800 done.
2025-03-06 18:53:50,493 - INFO - Final validation performance:
Loss: 0.427, top-1 acc: 0.880top-5 acc: 0.880
2025-03-06 18:53:50,493 - INFO - Beginning epoch 114/800
2025-03-06 18:53:50,500 - INFO - training batch 1, loss: 0.621, 32/60000 datapoints
2025-03-06 18:53:50,703 - INFO - training batch 51, loss: 0.429, 1632/60000 datapoints
2025-03-06 18:53:50,893 - INFO - training batch 101, loss: 0.317, 3232/60000 datapoints
2025-03-06 18:53:51,089 - INFO - training batch 151, loss: 0.475, 4832/60000 datapoints
2025-03-06 18:53:51,284 - INFO - training batch 201, loss: 0.402, 6432/60000 datapoints
2025-03-06 18:53:51,479 - INFO - training batch 251, loss: 0.434, 8032/60000 datapoints
2025-03-06 18:53:51,675 - INFO - training batch 301, loss: 0.391, 9632/60000 datapoints
2025-03-06 18:53:51,867 - INFO - training batch 351, loss: 0.313, 11232/60000 datapoints
2025-03-06 18:53:52,059 - INFO - training batch 401, loss: 0.590, 12832/60000 datapoints
2025-03-06 18:53:52,249 - INFO - training batch 451, loss: 0.260, 14432/60000 datapoints
2025-03-06 18:53:52,447 - INFO - training batch 501, loss: 0.638, 16032/60000 datapoints
2025-03-06 18:53:52,640 - INFO - training batch 551, loss: 0.411, 17632/60000 datapoints
2025-03-06 18:53:52,831 - INFO - training batch 601, loss: 0.434, 19232/60000 datapoints
2025-03-06 18:53:53,025 - INFO - training batch 651, loss: 0.509, 20832/60000 datapoints
2025-03-06 18:53:53,216 - INFO - training batch 701, loss: 0.322, 22432/60000 datapoints
2025-03-06 18:53:53,412 - INFO - training batch 751, loss: 0.572, 24032/60000 datapoints
2025-03-06 18:53:53,606 - INFO - training batch 801, loss: 0.424, 25632/60000 datapoints
2025-03-06 18:53:53,796 - INFO - training batch 851, loss: 0.312, 27232/60000 datapoints
2025-03-06 18:53:53,988 - INFO - training batch 901, loss: 0.459, 28832/60000 datapoints
2025-03-06 18:53:54,178 - INFO - training batch 951, loss: 0.411, 30432/60000 datapoints
2025-03-06 18:53:54,372 - INFO - training batch 1001, loss: 0.441, 32032/60000 datapoints
2025-03-06 18:53:54,564 - INFO - training batch 1051, loss: 0.535, 33632/60000 datapoints
2025-03-06 18:53:54,758 - INFO - training batch 1101, loss: 0.600, 35232/60000 datapoints
2025-03-06 18:53:54,954 - INFO - training batch 1151, loss: 0.518, 36832/60000 datapoints
2025-03-06 18:53:55,145 - INFO - training batch 1201, loss: 0.428, 38432/60000 datapoints
2025-03-06 18:53:55,357 - INFO - training batch 1251, loss: 0.318, 40032/60000 datapoints
2025-03-06 18:53:55,559 - INFO - training batch 1301, loss: 0.438, 41632/60000 datapoints
2025-03-06 18:53:55,752 - INFO - training batch 1351, loss: 0.429, 43232/60000 datapoints
2025-03-06 18:53:55,942 - INFO - training batch 1401, loss: 0.240, 44832/60000 datapoints
2025-03-06 18:53:56,133 - INFO - training batch 1451, loss: 0.329, 46432/60000 datapoints
2025-03-06 18:53:56,327 - INFO - training batch 1501, loss: 0.449, 48032/60000 datapoints
2025-03-06 18:53:56,518 - INFO - training batch 1551, loss: 0.433, 49632/60000 datapoints
2025-03-06 18:53:56,713 - INFO - training batch 1601, loss: 0.627, 51232/60000 datapoints
2025-03-06 18:53:56,903 - INFO - training batch 1651, loss: 0.682, 52832/60000 datapoints
2025-03-06 18:53:57,094 - INFO - training batch 1701, loss: 0.454, 54432/60000 datapoints
2025-03-06 18:53:57,283 - INFO - training batch 1751, loss: 0.412, 56032/60000 datapoints
2025-03-06 18:53:57,474 - INFO - training batch 1801, loss: 0.553, 57632/60000 datapoints
2025-03-06 18:53:57,668 - INFO - training batch 1851, loss: 0.615, 59232/60000 datapoints
2025-03-06 18:53:57,764 - INFO - validation batch 1, loss: 0.357, 32/10016 datapoints
2025-03-06 18:53:57,915 - INFO - validation batch 51, loss: 0.476, 1632/10016 datapoints
2025-03-06 18:53:58,068 - INFO - validation batch 101, loss: 0.417, 3232/10016 datapoints
2025-03-06 18:53:58,217 - INFO - validation batch 151, loss: 0.456, 4832/10016 datapoints
2025-03-06 18:53:58,369 - INFO - validation batch 201, loss: 0.305, 6432/10016 datapoints
2025-03-06 18:53:58,520 - INFO - validation batch 251, loss: 0.464, 8032/10016 datapoints
2025-03-06 18:53:58,674 - INFO - validation batch 301, loss: 0.389, 9632/10016 datapoints
2025-03-06 18:53:58,709 - INFO - Epoch 114/800 done.
2025-03-06 18:53:58,709 - INFO - Final validation performance:
Loss: 0.409, top-1 acc: 0.880top-5 acc: 0.880
2025-03-06 18:53:58,710 - INFO - Beginning epoch 115/800
2025-03-06 18:53:58,716 - INFO - training batch 1, loss: 0.549, 32/60000 datapoints
2025-03-06 18:53:58,918 - INFO - training batch 51, loss: 0.464, 1632/60000 datapoints
2025-03-06 18:53:59,113 - INFO - training batch 101, loss: 0.501, 3232/60000 datapoints
2025-03-06 18:53:59,310 - INFO - training batch 151, loss: 0.449, 4832/60000 datapoints
2025-03-06 18:53:59,506 - INFO - training batch 201, loss: 0.625, 6432/60000 datapoints
2025-03-06 18:53:59,703 - INFO - training batch 251, loss: 0.350, 8032/60000 datapoints
2025-03-06 18:53:59,894 - INFO - training batch 301, loss: 0.471, 9632/60000 datapoints
2025-03-06 18:54:00,091 - INFO - training batch 351, loss: 0.477, 11232/60000 datapoints
2025-03-06 18:54:00,284 - INFO - training batch 401, loss: 0.492, 12832/60000 datapoints
2025-03-06 18:54:00,480 - INFO - training batch 451, loss: 0.547, 14432/60000 datapoints
2025-03-06 18:54:00,674 - INFO - training batch 501, loss: 0.374, 16032/60000 datapoints
2025-03-06 18:54:00,866 - INFO - training batch 551, loss: 0.313, 17632/60000 datapoints
2025-03-06 18:54:01,056 - INFO - training batch 601, loss: 0.498, 19232/60000 datapoints
2025-03-06 18:54:01,251 - INFO - training batch 651, loss: 0.348, 20832/60000 datapoints
2025-03-06 18:54:01,445 - INFO - training batch 701, loss: 0.388, 22432/60000 datapoints
2025-03-06 18:54:01,640 - INFO - training batch 751, loss: 0.505, 24032/60000 datapoints
2025-03-06 18:54:01,833 - INFO - training batch 801, loss: 0.351, 25632/60000 datapoints
2025-03-06 18:54:02,025 - INFO - training batch 851, loss: 0.483, 27232/60000 datapoints
2025-03-06 18:54:02,217 - INFO - training batch 901, loss: 0.417, 28832/60000 datapoints
2025-03-06 18:54:02,412 - INFO - training batch 951, loss: 0.460, 30432/60000 datapoints
2025-03-06 18:54:02,605 - INFO - training batch 1001, loss: 0.399, 32032/60000 datapoints
2025-03-06 18:54:02,798 - INFO - training batch 1051, loss: 0.633, 33632/60000 datapoints
2025-03-06 18:54:02,992 - INFO - training batch 1101, loss: 0.595, 35232/60000 datapoints
2025-03-06 18:54:03,186 - INFO - training batch 1151, loss: 0.473, 36832/60000 datapoints
2025-03-06 18:54:03,378 - INFO - training batch 1201, loss: 0.579, 38432/60000 datapoints
2025-03-06 18:54:03,569 - INFO - training batch 1251, loss: 0.454, 40032/60000 datapoints
2025-03-06 18:54:03,763 - INFO - training batch 1301, loss: 0.328, 41632/60000 datapoints
2025-03-06 18:54:03,958 - INFO - training batch 1351, loss: 0.441, 43232/60000 datapoints
2025-03-06 18:54:04,151 - INFO - training batch 1401, loss: 0.588, 44832/60000 datapoints
2025-03-06 18:54:04,343 - INFO - training batch 1451, loss: 0.355, 46432/60000 datapoints
2025-03-06 18:54:04,536 - INFO - training batch 1501, loss: 0.425, 48032/60000 datapoints
2025-03-06 18:54:04,731 - INFO - training batch 1551, loss: 0.383, 49632/60000 datapoints
2025-03-06 18:54:04,929 - INFO - training batch 1601, loss: 0.576, 51232/60000 datapoints
2025-03-06 18:54:05,120 - INFO - training batch 1651, loss: 0.757, 52832/60000 datapoints
2025-03-06 18:54:05,322 - INFO - training batch 1701, loss: 0.644, 54432/60000 datapoints
2025-03-06 18:54:05,519 - INFO - training batch 1751, loss: 0.505, 56032/60000 datapoints
2025-03-06 18:54:05,715 - INFO - training batch 1801, loss: 0.562, 57632/60000 datapoints
2025-03-06 18:54:05,907 - INFO - training batch 1851, loss: 0.522, 59232/60000 datapoints
2025-03-06 18:54:06,007 - INFO - validation batch 1, loss: 0.405, 32/10016 datapoints
2025-03-06 18:54:06,162 - INFO - validation batch 51, loss: 0.320, 1632/10016 datapoints
2025-03-06 18:54:06,316 - INFO - validation batch 101, loss: 0.665, 3232/10016 datapoints
2025-03-06 18:54:06,471 - INFO - validation batch 151, loss: 0.512, 4832/10016 datapoints
2025-03-06 18:54:06,625 - INFO - validation batch 201, loss: 0.232, 6432/10016 datapoints
2025-03-06 18:54:06,782 - INFO - validation batch 251, loss: 0.266, 8032/10016 datapoints
2025-03-06 18:54:06,936 - INFO - validation batch 301, loss: 0.338, 9632/10016 datapoints
2025-03-06 18:54:06,974 - INFO - Epoch 115/800 done.
2025-03-06 18:54:06,974 - INFO - Final validation performance:
Loss: 0.391, top-1 acc: 0.881top-5 acc: 0.881
2025-03-06 18:54:06,975 - INFO - Beginning epoch 116/800
2025-03-06 18:54:06,981 - INFO - training batch 1, loss: 0.430, 32/60000 datapoints
2025-03-06 18:54:07,189 - INFO - training batch 51, loss: 0.524, 1632/60000 datapoints
2025-03-06 18:54:07,380 - INFO - training batch 101, loss: 0.359, 3232/60000 datapoints
2025-03-06 18:54:07,578 - INFO - training batch 151, loss: 0.562, 4832/60000 datapoints
2025-03-06 18:54:07,779 - INFO - training batch 201, loss: 0.468, 6432/60000 datapoints
2025-03-06 18:54:07,975 - INFO - training batch 251, loss: 0.431, 8032/60000 datapoints
2025-03-06 18:54:08,170 - INFO - training batch 301, loss: 0.317, 9632/60000 datapoints
2025-03-06 18:54:08,365 - INFO - training batch 351, loss: 0.459, 11232/60000 datapoints
2025-03-06 18:54:08,560 - INFO - training batch 401, loss: 0.634, 12832/60000 datapoints
2025-03-06 18:54:08,758 - INFO - training batch 451, loss: 0.368, 14432/60000 datapoints
2025-03-06 18:54:08,953 - INFO - training batch 501, loss: 0.343, 16032/60000 datapoints
2025-03-06 18:54:09,150 - INFO - training batch 551, loss: 0.442, 17632/60000 datapoints
2025-03-06 18:54:09,346 - INFO - training batch 601, loss: 0.450, 19232/60000 datapoints
2025-03-06 18:54:09,537 - INFO - training batch 651, loss: 0.572, 20832/60000 datapoints
2025-03-06 18:54:09,735 - INFO - training batch 701, loss: 0.573, 22432/60000 datapoints
2025-03-06 18:54:09,927 - INFO - training batch 751, loss: 0.428, 24032/60000 datapoints
2025-03-06 18:54:10,123 - INFO - training batch 801, loss: 0.671, 25632/60000 datapoints
2025-03-06 18:54:10,314 - INFO - training batch 851, loss: 0.379, 27232/60000 datapoints
2025-03-06 18:54:10,511 - INFO - training batch 901, loss: 0.456, 28832/60000 datapoints
2025-03-06 18:54:10,709 - INFO - training batch 951, loss: 0.446, 30432/60000 datapoints
2025-03-06 18:54:10,903 - INFO - training batch 1001, loss: 0.403, 32032/60000 datapoints
2025-03-06 18:54:11,101 - INFO - training batch 1051, loss: 0.352, 33632/60000 datapoints
2025-03-06 18:54:11,303 - INFO - training batch 1101, loss: 0.567, 35232/60000 datapoints
2025-03-06 18:54:11,497 - INFO - training batch 1151, loss: 0.708, 36832/60000 datapoints
2025-03-06 18:54:11,694 - INFO - training batch 1201, loss: 0.624, 38432/60000 datapoints
2025-03-06 18:54:11,888 - INFO - training batch 1251, loss: 0.436, 40032/60000 datapoints
2025-03-06 18:54:12,080 - INFO - training batch 1301, loss: 0.711, 41632/60000 datapoints
2025-03-06 18:54:12,274 - INFO - training batch 1351, loss: 0.455, 43232/60000 datapoints
2025-03-06 18:54:12,467 - INFO - training batch 1401, loss: 0.411, 44832/60000 datapoints
2025-03-06 18:54:12,664 - INFO - training batch 1451, loss: 0.465, 46432/60000 datapoints
2025-03-06 18:54:12,859 - INFO - training batch 1501, loss: 0.407, 48032/60000 datapoints
2025-03-06 18:54:13,054 - INFO - training batch 1551, loss: 0.285, 49632/60000 datapoints
2025-03-06 18:54:13,250 - INFO - training batch 1601, loss: 0.557, 51232/60000 datapoints
2025-03-06 18:54:13,442 - INFO - training batch 1651, loss: 0.646, 52832/60000 datapoints
2025-03-06 18:54:13,638 - INFO - training batch 1701, loss: 0.557, 54432/60000 datapoints
2025-03-06 18:54:13,830 - INFO - training batch 1751, loss: 0.346, 56032/60000 datapoints
2025-03-06 18:54:14,027 - INFO - training batch 1801, loss: 0.548, 57632/60000 datapoints
2025-03-06 18:54:14,222 - INFO - training batch 1851, loss: 0.344, 59232/60000 datapoints
2025-03-06 18:54:14,322 - INFO - validation batch 1, loss: 0.564, 32/10016 datapoints
2025-03-06 18:54:14,476 - INFO - validation batch 51, loss: 0.505, 1632/10016 datapoints
2025-03-06 18:54:14,630 - INFO - validation batch 101, loss: 0.494, 3232/10016 datapoints
2025-03-06 18:54:14,781 - INFO - validation batch 151, loss: 0.503, 4832/10016 datapoints
2025-03-06 18:54:14,935 - INFO - validation batch 201, loss: 0.551, 6432/10016 datapoints
2025-03-06 18:54:15,088 - INFO - validation batch 251, loss: 0.549, 8032/10016 datapoints
2025-03-06 18:54:15,242 - INFO - validation batch 301, loss: 0.478, 9632/10016 datapoints
2025-03-06 18:54:15,280 - INFO - Epoch 116/800 done.
2025-03-06 18:54:15,280 - INFO - Final validation performance:
Loss: 0.521, top-1 acc: 0.881top-5 acc: 0.881
2025-03-06 18:54:15,281 - INFO - Beginning epoch 117/800
2025-03-06 18:54:15,287 - INFO - training batch 1, loss: 0.434, 32/60000 datapoints
2025-03-06 18:54:15,510 - INFO - training batch 51, loss: 0.665, 1632/60000 datapoints
2025-03-06 18:54:15,721 - INFO - training batch 101, loss: 0.259, 3232/60000 datapoints
2025-03-06 18:54:15,915 - INFO - training batch 151, loss: 0.318, 4832/60000 datapoints
2025-03-06 18:54:16,107 - INFO - training batch 201, loss: 0.671, 6432/60000 datapoints
2025-03-06 18:54:16,312 - INFO - training batch 251, loss: 0.508, 8032/60000 datapoints
2025-03-06 18:54:16,512 - INFO - training batch 301, loss: 0.390, 9632/60000 datapoints
2025-03-06 18:54:16,741 - INFO - training batch 351, loss: 0.479, 11232/60000 datapoints
2025-03-06 18:54:16,936 - INFO - training batch 401, loss: 0.755, 12832/60000 datapoints
2025-03-06 18:54:17,130 - INFO - training batch 451, loss: 0.470, 14432/60000 datapoints
2025-03-06 18:54:17,323 - INFO - training batch 501, loss: 0.440, 16032/60000 datapoints
2025-03-06 18:54:17,518 - INFO - training batch 551, loss: 0.495, 17632/60000 datapoints
2025-03-06 18:54:17,717 - INFO - training batch 601, loss: 0.348, 19232/60000 datapoints
2025-03-06 18:54:17,915 - INFO - training batch 651, loss: 0.781, 20832/60000 datapoints
2025-03-06 18:54:18,110 - INFO - training batch 701, loss: 0.702, 22432/60000 datapoints
2025-03-06 18:54:18,304 - INFO - training batch 751, loss: 0.362, 24032/60000 datapoints
2025-03-06 18:54:18,503 - INFO - training batch 801, loss: 0.483, 25632/60000 datapoints
2025-03-06 18:54:18,700 - INFO - training batch 851, loss: 0.240, 27232/60000 datapoints
2025-03-06 18:54:18,893 - INFO - training batch 901, loss: 0.491, 28832/60000 datapoints
2025-03-06 18:54:19,088 - INFO - training batch 951, loss: 0.300, 30432/60000 datapoints
2025-03-06 18:54:19,282 - INFO - training batch 1001, loss: 0.434, 32032/60000 datapoints
2025-03-06 18:54:19,476 - INFO - training batch 1051, loss: 0.656, 33632/60000 datapoints
2025-03-06 18:54:19,674 - INFO - training batch 1101, loss: 0.578, 35232/60000 datapoints
2025-03-06 18:54:19,871 - INFO - training batch 1151, loss: 0.503, 36832/60000 datapoints
2025-03-06 18:54:20,066 - INFO - training batch 1201, loss: 0.502, 38432/60000 datapoints
2025-03-06 18:54:20,258 - INFO - training batch 1251, loss: 0.428, 40032/60000 datapoints
2025-03-06 18:54:20,454 - INFO - training batch 1301, loss: 0.512, 41632/60000 datapoints
2025-03-06 18:54:20,649 - INFO - training batch 1351, loss: 0.555, 43232/60000 datapoints
2025-03-06 18:54:20,843 - INFO - training batch 1401, loss: 0.607, 44832/60000 datapoints
2025-03-06 18:54:21,035 - INFO - training batch 1451, loss: 0.406, 46432/60000 datapoints
2025-03-06 18:54:21,230 - INFO - training batch 1501, loss: 0.549, 48032/60000 datapoints
2025-03-06 18:54:21,423 - INFO - training batch 1551, loss: 0.344, 49632/60000 datapoints
2025-03-06 18:54:21,618 - INFO - training batch 1601, loss: 0.420, 51232/60000 datapoints
2025-03-06 18:54:21,813 - INFO - training batch 1651, loss: 0.564, 52832/60000 datapoints
2025-03-06 18:54:22,006 - INFO - training batch 1701, loss: 0.440, 54432/60000 datapoints
2025-03-06 18:54:22,200 - INFO - training batch 1751, loss: 0.547, 56032/60000 datapoints
2025-03-06 18:54:22,394 - INFO - training batch 1801, loss: 0.517, 57632/60000 datapoints
2025-03-06 18:54:22,590 - INFO - training batch 1851, loss: 0.369, 59232/60000 datapoints
2025-03-06 18:54:22,693 - INFO - validation batch 1, loss: 0.662, 32/10016 datapoints
2025-03-06 18:54:22,847 - INFO - validation batch 51, loss: 0.350, 1632/10016 datapoints
2025-03-06 18:54:22,999 - INFO - validation batch 101, loss: 0.488, 3232/10016 datapoints
2025-03-06 18:54:23,151 - INFO - validation batch 151, loss: 0.338, 4832/10016 datapoints
2025-03-06 18:54:23,305 - INFO - validation batch 201, loss: 0.382, 6432/10016 datapoints
2025-03-06 18:54:23,455 - INFO - validation batch 251, loss: 0.258, 8032/10016 datapoints
2025-03-06 18:54:23,612 - INFO - validation batch 301, loss: 0.486, 9632/10016 datapoints
2025-03-06 18:54:23,648 - INFO - Epoch 117/800 done.
2025-03-06 18:54:23,649 - INFO - Final validation performance:
Loss: 0.423, top-1 acc: 0.881top-5 acc: 0.881
2025-03-06 18:54:23,649 - INFO - Beginning epoch 118/800
2025-03-06 18:54:23,656 - INFO - training batch 1, loss: 0.454, 32/60000 datapoints
2025-03-06 18:54:23,862 - INFO - training batch 51, loss: 0.338, 1632/60000 datapoints
2025-03-06 18:54:24,054 - INFO - training batch 101, loss: 0.321, 3232/60000 datapoints
2025-03-06 18:54:24,260 - INFO - training batch 151, loss: 0.447, 4832/60000 datapoints
2025-03-06 18:54:24,461 - INFO - training batch 201, loss: 0.551, 6432/60000 datapoints
2025-03-06 18:54:24,658 - INFO - training batch 251, loss: 0.208, 8032/60000 datapoints
2025-03-06 18:54:24,850 - INFO - training batch 301, loss: 0.546, 9632/60000 datapoints
2025-03-06 18:54:25,043 - INFO - training batch 351, loss: 0.363, 11232/60000 datapoints
2025-03-06 18:54:25,235 - INFO - training batch 401, loss: 0.528, 12832/60000 datapoints
2025-03-06 18:54:25,426 - INFO - training batch 451, loss: 0.301, 14432/60000 datapoints
2025-03-06 18:54:25,640 - INFO - training batch 501, loss: 0.313, 16032/60000 datapoints
2025-03-06 18:54:25,839 - INFO - training batch 551, loss: 0.432, 17632/60000 datapoints
2025-03-06 18:54:26,034 - INFO - training batch 601, loss: 0.359, 19232/60000 datapoints
2025-03-06 18:54:26,229 - INFO - training batch 651, loss: 0.371, 20832/60000 datapoints
2025-03-06 18:54:26,426 - INFO - training batch 701, loss: 0.320, 22432/60000 datapoints
2025-03-06 18:54:26,625 - INFO - training batch 751, loss: 0.447, 24032/60000 datapoints
2025-03-06 18:54:26,820 - INFO - training batch 801, loss: 0.482, 25632/60000 datapoints
2025-03-06 18:54:27,013 - INFO - training batch 851, loss: 0.635, 27232/60000 datapoints
2025-03-06 18:54:27,207 - INFO - training batch 901, loss: 0.525, 28832/60000 datapoints
2025-03-06 18:54:27,400 - INFO - training batch 951, loss: 0.345, 30432/60000 datapoints
2025-03-06 18:54:27,591 - INFO - training batch 1001, loss: 0.359, 32032/60000 datapoints
2025-03-06 18:54:27,787 - INFO - training batch 1051, loss: 0.432, 33632/60000 datapoints
2025-03-06 18:54:27,983 - INFO - training batch 1101, loss: 0.741, 35232/60000 datapoints
2025-03-06 18:54:28,200 - INFO - training batch 1151, loss: 0.431, 36832/60000 datapoints
2025-03-06 18:54:28,393 - INFO - training batch 1201, loss: 0.544, 38432/60000 datapoints
2025-03-06 18:54:28,589 - INFO - training batch 1251, loss: 0.427, 40032/60000 datapoints
2025-03-06 18:54:28,786 - INFO - training batch 1301, loss: 0.442, 41632/60000 datapoints
2025-03-06 18:54:28,979 - INFO - training batch 1351, loss: 0.417, 43232/60000 datapoints
2025-03-06 18:54:29,172 - INFO - training batch 1401, loss: 0.484, 44832/60000 datapoints
2025-03-06 18:54:29,366 - INFO - training batch 1451, loss: 0.574, 46432/60000 datapoints
2025-03-06 18:54:29,559 - INFO - training batch 1501, loss: 0.433, 48032/60000 datapoints
2025-03-06 18:54:29,754 - INFO - training batch 1551, loss: 0.460, 49632/60000 datapoints
2025-03-06 18:54:29,948 - INFO - training batch 1601, loss: 0.561, 51232/60000 datapoints
2025-03-06 18:54:30,144 - INFO - training batch 1651, loss: 0.513, 52832/60000 datapoints
2025-03-06 18:54:30,339 - INFO - training batch 1701, loss: 0.499, 54432/60000 datapoints
2025-03-06 18:54:30,536 - INFO - training batch 1751, loss: 0.620, 56032/60000 datapoints
2025-03-06 18:54:30,733 - INFO - training batch 1801, loss: 0.252, 57632/60000 datapoints
2025-03-06 18:54:30,928 - INFO - training batch 1851, loss: 0.315, 59232/60000 datapoints
2025-03-06 18:54:31,029 - INFO - validation batch 1, loss: 0.213, 32/10016 datapoints
2025-03-06 18:54:31,181 - INFO - validation batch 51, loss: 0.537, 1632/10016 datapoints
2025-03-06 18:54:31,333 - INFO - validation batch 101, loss: 0.597, 3232/10016 datapoints
2025-03-06 18:54:31,488 - INFO - validation batch 151, loss: 0.373, 4832/10016 datapoints
2025-03-06 18:54:31,647 - INFO - validation batch 201, loss: 0.650, 6432/10016 datapoints
2025-03-06 18:54:31,801 - INFO - validation batch 251, loss: 0.613, 8032/10016 datapoints
2025-03-06 18:54:31,954 - INFO - validation batch 301, loss: 0.426, 9632/10016 datapoints
2025-03-06 18:54:31,990 - INFO - Epoch 118/800 done.
2025-03-06 18:54:31,990 - INFO - Final validation performance:
Loss: 0.487, top-1 acc: 0.882top-5 acc: 0.882
2025-03-06 18:54:31,991 - INFO - Beginning epoch 119/800
2025-03-06 18:54:31,997 - INFO - training batch 1, loss: 0.409, 32/60000 datapoints
2025-03-06 18:54:32,192 - INFO - training batch 51, loss: 0.495, 1632/60000 datapoints
2025-03-06 18:54:32,384 - INFO - training batch 101, loss: 0.617, 3232/60000 datapoints
2025-03-06 18:54:32,590 - INFO - training batch 151, loss: 0.393, 4832/60000 datapoints
2025-03-06 18:54:32,786 - INFO - training batch 201, loss: 0.417, 6432/60000 datapoints
2025-03-06 18:54:32,982 - INFO - training batch 251, loss: 0.316, 8032/60000 datapoints
2025-03-06 18:54:33,178 - INFO - training batch 301, loss: 0.347, 9632/60000 datapoints
2025-03-06 18:54:33,371 - INFO - training batch 351, loss: 0.470, 11232/60000 datapoints
2025-03-06 18:54:33,563 - INFO - training batch 401, loss: 0.417, 12832/60000 datapoints
2025-03-06 18:54:33,758 - INFO - training batch 451, loss: 0.581, 14432/60000 datapoints
2025-03-06 18:54:33,949 - INFO - training batch 501, loss: 0.675, 16032/60000 datapoints
2025-03-06 18:54:34,141 - INFO - training batch 551, loss: 0.454, 17632/60000 datapoints
2025-03-06 18:54:34,331 - INFO - training batch 601, loss: 0.544, 19232/60000 datapoints
2025-03-06 18:54:34,526 - INFO - training batch 651, loss: 0.743, 20832/60000 datapoints
2025-03-06 18:54:34,722 - INFO - training batch 701, loss: 0.567, 22432/60000 datapoints
2025-03-06 18:54:34,915 - INFO - training batch 751, loss: 0.527, 24032/60000 datapoints
2025-03-06 18:54:35,107 - INFO - training batch 801, loss: 0.350, 25632/60000 datapoints
2025-03-06 18:54:35,297 - INFO - training batch 851, loss: 0.497, 27232/60000 datapoints
2025-03-06 18:54:35,490 - INFO - training batch 901, loss: 0.653, 28832/60000 datapoints
2025-03-06 18:54:35,699 - INFO - training batch 951, loss: 0.693, 30432/60000 datapoints
2025-03-06 18:54:35,890 - INFO - training batch 1001, loss: 0.597, 32032/60000 datapoints
2025-03-06 18:54:36,081 - INFO - training batch 1051, loss: 0.595, 33632/60000 datapoints
2025-03-06 18:54:36,274 - INFO - training batch 1101, loss: 0.514, 35232/60000 datapoints
2025-03-06 18:54:36,468 - INFO - training batch 1151, loss: 0.615, 36832/60000 datapoints
2025-03-06 18:54:36,662 - INFO - training batch 1201, loss: 0.777, 38432/60000 datapoints
2025-03-06 18:54:36,857 - INFO - training batch 1251, loss: 0.460, 40032/60000 datapoints
2025-03-06 18:54:37,050 - INFO - training batch 1301, loss: 0.323, 41632/60000 datapoints
2025-03-06 18:54:37,240 - INFO - training batch 1351, loss: 0.379, 43232/60000 datapoints
2025-03-06 18:54:37,430 - INFO - training batch 1401, loss: 0.449, 44832/60000 datapoints
2025-03-06 18:54:37,634 - INFO - training batch 1451, loss: 0.341, 46432/60000 datapoints
2025-03-06 18:54:37,824 - INFO - training batch 1501, loss: 0.369, 48032/60000 datapoints
2025-03-06 18:54:38,018 - INFO - training batch 1551, loss: 0.513, 49632/60000 datapoints
2025-03-06 18:54:38,210 - INFO - training batch 1601, loss: 0.534, 51232/60000 datapoints
2025-03-06 18:54:38,399 - INFO - training batch 1651, loss: 0.422, 52832/60000 datapoints
2025-03-06 18:54:38,593 - INFO - training batch 1701, loss: 0.375, 54432/60000 datapoints
2025-03-06 18:54:38,785 - INFO - training batch 1751, loss: 0.752, 56032/60000 datapoints
2025-03-06 18:54:38,976 - INFO - training batch 1801, loss: 0.523, 57632/60000 datapoints
2025-03-06 18:54:39,165 - INFO - training batch 1851, loss: 0.321, 59232/60000 datapoints
2025-03-06 18:54:39,263 - INFO - validation batch 1, loss: 0.675, 32/10016 datapoints
2025-03-06 18:54:39,415 - INFO - validation batch 51, loss: 0.545, 1632/10016 datapoints
2025-03-06 18:54:39,567 - INFO - validation batch 101, loss: 0.318, 3232/10016 datapoints
2025-03-06 18:54:39,722 - INFO - validation batch 151, loss: 0.368, 4832/10016 datapoints
2025-03-06 18:54:39,870 - INFO - validation batch 201, loss: 0.450, 6432/10016 datapoints
2025-03-06 18:54:40,021 - INFO - validation batch 251, loss: 0.561, 8032/10016 datapoints
2025-03-06 18:54:40,171 - INFO - validation batch 301, loss: 0.445, 9632/10016 datapoints
2025-03-06 18:54:40,207 - INFO - Epoch 119/800 done.
2025-03-06 18:54:40,208 - INFO - Final validation performance:
Loss: 0.480, top-1 acc: 0.882top-5 acc: 0.882
2025-03-06 18:54:40,208 - INFO - Beginning epoch 120/800
2025-03-06 18:54:40,215 - INFO - training batch 1, loss: 0.317, 32/60000 datapoints
2025-03-06 18:54:40,415 - INFO - training batch 51, loss: 0.655, 1632/60000 datapoints
2025-03-06 18:54:40,613 - INFO - training batch 101, loss: 0.903, 3232/60000 datapoints
2025-03-06 18:54:40,811 - INFO - training batch 151, loss: 0.385, 4832/60000 datapoints
2025-03-06 18:54:41,004 - INFO - training batch 201, loss: 0.480, 6432/60000 datapoints
2025-03-06 18:54:41,200 - INFO - training batch 251, loss: 0.630, 8032/60000 datapoints
2025-03-06 18:54:41,394 - INFO - training batch 301, loss: 0.835, 9632/60000 datapoints
2025-03-06 18:54:41,585 - INFO - training batch 351, loss: 0.384, 11232/60000 datapoints
2025-03-06 18:54:41,777 - INFO - training batch 401, loss: 0.364, 12832/60000 datapoints
2025-03-06 18:54:41,969 - INFO - training batch 451, loss: 0.444, 14432/60000 datapoints
2025-03-06 18:54:42,160 - INFO - training batch 501, loss: 0.478, 16032/60000 datapoints
2025-03-06 18:54:42,350 - INFO - training batch 551, loss: 0.527, 17632/60000 datapoints
2025-03-06 18:54:42,544 - INFO - training batch 601, loss: 0.434, 19232/60000 datapoints
2025-03-06 18:54:42,737 - INFO - training batch 651, loss: 0.294, 20832/60000 datapoints
2025-03-06 18:54:42,926 - INFO - training batch 701, loss: 0.253, 22432/60000 datapoints
2025-03-06 18:54:43,119 - INFO - training batch 751, loss: 0.556, 24032/60000 datapoints
2025-03-06 18:54:43,310 - INFO - training batch 801, loss: 0.454, 25632/60000 datapoints
2025-03-06 18:54:43,499 - INFO - training batch 851, loss: 0.471, 27232/60000 datapoints
2025-03-06 18:54:43,694 - INFO - training batch 901, loss: 0.595, 28832/60000 datapoints
2025-03-06 18:54:43,886 - INFO - training batch 951, loss: 0.560, 30432/60000 datapoints
2025-03-06 18:54:44,077 - INFO - training batch 1001, loss: 0.423, 32032/60000 datapoints
2025-03-06 18:54:44,270 - INFO - training batch 1051, loss: 0.406, 33632/60000 datapoints
2025-03-06 18:54:44,459 - INFO - training batch 1101, loss: 0.421, 35232/60000 datapoints
2025-03-06 18:54:44,656 - INFO - training batch 1151, loss: 0.355, 36832/60000 datapoints
2025-03-06 18:54:44,846 - INFO - training batch 1201, loss: 0.328, 38432/60000 datapoints
2025-03-06 18:54:45,041 - INFO - training batch 1251, loss: 0.437, 40032/60000 datapoints
2025-03-06 18:54:45,235 - INFO - training batch 1301, loss: 0.745, 41632/60000 datapoints
2025-03-06 18:54:45,425 - INFO - training batch 1351, loss: 0.529, 43232/60000 datapoints
2025-03-06 18:54:45,628 - INFO - training batch 1401, loss: 0.394, 44832/60000 datapoints
2025-03-06 18:54:45,837 - INFO - training batch 1451, loss: 0.517, 46432/60000 datapoints
2025-03-06 18:54:46,035 - INFO - training batch 1501, loss: 0.677, 48032/60000 datapoints
2025-03-06 18:54:46,231 - INFO - training batch 1551, loss: 0.530, 49632/60000 datapoints
2025-03-06 18:54:46,425 - INFO - training batch 1601, loss: 0.399, 51232/60000 datapoints
2025-03-06 18:54:46,626 - INFO - training batch 1651, loss: 0.450, 52832/60000 datapoints
2025-03-06 18:54:46,819 - INFO - training batch 1701, loss: 0.389, 54432/60000 datapoints
2025-03-06 18:54:47,013 - INFO - training batch 1751, loss: 0.675, 56032/60000 datapoints
2025-03-06 18:54:47,205 - INFO - training batch 1801, loss: 0.583, 57632/60000 datapoints
2025-03-06 18:54:47,400 - INFO - training batch 1851, loss: 0.307, 59232/60000 datapoints
2025-03-06 18:54:47,502 - INFO - validation batch 1, loss: 0.505, 32/10016 datapoints
2025-03-06 18:54:47,656 - INFO - validation batch 51, loss: 0.371, 1632/10016 datapoints
2025-03-06 18:54:47,807 - INFO - validation batch 101, loss: 0.456, 3232/10016 datapoints
2025-03-06 18:54:47,959 - INFO - validation batch 151, loss: 0.251, 4832/10016 datapoints
2025-03-06 18:54:48,118 - INFO - validation batch 201, loss: 0.365, 6432/10016 datapoints
2025-03-06 18:54:48,270 - INFO - validation batch 251, loss: 0.252, 8032/10016 datapoints
2025-03-06 18:54:48,421 - INFO - validation batch 301, loss: 0.388, 9632/10016 datapoints
2025-03-06 18:54:48,459 - INFO - Epoch 120/800 done.
2025-03-06 18:54:48,459 - INFO - Final validation performance:
Loss: 0.370, top-1 acc: 0.883top-5 acc: 0.883
2025-03-06 18:54:48,459 - INFO - Beginning epoch 121/800
2025-03-06 18:54:48,465 - INFO - training batch 1, loss: 0.428, 32/60000 datapoints
2025-03-06 18:54:48,669 - INFO - training batch 51, loss: 0.390, 1632/60000 datapoints
2025-03-06 18:54:48,873 - INFO - training batch 101, loss: 0.811, 3232/60000 datapoints
2025-03-06 18:54:49,067 - INFO - training batch 151, loss: 0.612, 4832/60000 datapoints
2025-03-06 18:54:49,262 - INFO - training batch 201, loss: 0.377, 6432/60000 datapoints
2025-03-06 18:54:49,455 - INFO - training batch 251, loss: 0.297, 8032/60000 datapoints
2025-03-06 18:54:49,660 - INFO - training batch 301, loss: 0.528, 9632/60000 datapoints
2025-03-06 18:54:49,855 - INFO - training batch 351, loss: 0.484, 11232/60000 datapoints
2025-03-06 18:54:50,053 - INFO - training batch 401, loss: 0.610, 12832/60000 datapoints
2025-03-06 18:54:50,248 - INFO - training batch 451, loss: 0.593, 14432/60000 datapoints
2025-03-06 18:54:50,444 - INFO - training batch 501, loss: 0.326, 16032/60000 datapoints
2025-03-06 18:54:50,644 - INFO - training batch 551, loss: 0.491, 17632/60000 datapoints
2025-03-06 18:54:50,839 - INFO - training batch 601, loss: 0.380, 19232/60000 datapoints
2025-03-06 18:54:51,031 - INFO - training batch 651, loss: 0.351, 20832/60000 datapoints
2025-03-06 18:54:51,228 - INFO - training batch 701, loss: 0.407, 22432/60000 datapoints
2025-03-06 18:54:51,422 - INFO - training batch 751, loss: 0.502, 24032/60000 datapoints
2025-03-06 18:54:51,620 - INFO - training batch 801, loss: 0.436, 25632/60000 datapoints
2025-03-06 18:54:51,813 - INFO - training batch 851, loss: 0.438, 27232/60000 datapoints
2025-03-06 18:54:52,009 - INFO - training batch 901, loss: 0.415, 28832/60000 datapoints
2025-03-06 18:54:52,203 - INFO - training batch 951, loss: 0.217, 30432/60000 datapoints
2025-03-06 18:54:52,399 - INFO - training batch 1001, loss: 0.582, 32032/60000 datapoints
2025-03-06 18:54:52,594 - INFO - training batch 1051, loss: 0.496, 33632/60000 datapoints
2025-03-06 18:54:52,793 - INFO - training batch 1101, loss: 0.349, 35232/60000 datapoints
2025-03-06 18:54:52,986 - INFO - training batch 1151, loss: 0.505, 36832/60000 datapoints
2025-03-06 18:54:53,181 - INFO - training batch 1201, loss: 0.458, 38432/60000 datapoints
2025-03-06 18:54:53,374 - INFO - training batch 1251, loss: 0.451, 40032/60000 datapoints
2025-03-06 18:54:53,569 - INFO - training batch 1301, loss: 0.660, 41632/60000 datapoints
2025-03-06 18:54:53,766 - INFO - training batch 1351, loss: 0.384, 43232/60000 datapoints
2025-03-06 18:54:53,958 - INFO - training batch 1401, loss: 0.575, 44832/60000 datapoints
2025-03-06 18:54:54,150 - INFO - training batch 1451, loss: 0.423, 46432/60000 datapoints
2025-03-06 18:54:54,346 - INFO - training batch 1501, loss: 0.453, 48032/60000 datapoints
2025-03-06 18:54:54,540 - INFO - training batch 1551, loss: 0.476, 49632/60000 datapoints
2025-03-06 18:54:54,737 - INFO - training batch 1601, loss: 0.539, 51232/60000 datapoints
2025-03-06 18:54:54,936 - INFO - training batch 1651, loss: 0.322, 52832/60000 datapoints
2025-03-06 18:54:55,132 - INFO - training batch 1701, loss: 0.305, 54432/60000 datapoints
2025-03-06 18:54:55,329 - INFO - training batch 1751, loss: 0.596, 56032/60000 datapoints
2025-03-06 18:54:55,524 - INFO - training batch 1801, loss: 0.401, 57632/60000 datapoints
2025-03-06 18:54:55,734 - INFO - training batch 1851, loss: 0.431, 59232/60000 datapoints
2025-03-06 18:54:55,839 - INFO - validation batch 1, loss: 0.284, 32/10016 datapoints
2025-03-06 18:54:55,992 - INFO - validation batch 51, loss: 0.640, 1632/10016 datapoints
2025-03-06 18:54:56,143 - INFO - validation batch 101, loss: 0.491, 3232/10016 datapoints
2025-03-06 18:54:56,302 - INFO - validation batch 151, loss: 0.610, 4832/10016 datapoints
2025-03-06 18:54:56,454 - INFO - validation batch 201, loss: 0.615, 6432/10016 datapoints
2025-03-06 18:54:56,610 - INFO - validation batch 251, loss: 0.475, 8032/10016 datapoints
2025-03-06 18:54:56,762 - INFO - validation batch 301, loss: 0.374, 9632/10016 datapoints
2025-03-06 18:54:56,801 - INFO - Epoch 121/800 done.
2025-03-06 18:54:56,801 - INFO - Final validation performance:
Loss: 0.498, top-1 acc: 0.883top-5 acc: 0.883
2025-03-06 18:54:56,802 - INFO - Beginning epoch 122/800
2025-03-06 18:54:56,808 - INFO - training batch 1, loss: 0.529, 32/60000 datapoints
2025-03-06 18:54:57,003 - INFO - training batch 51, loss: 0.567, 1632/60000 datapoints
2025-03-06 18:54:57,193 - INFO - training batch 101, loss: 0.399, 3232/60000 datapoints
2025-03-06 18:54:57,395 - INFO - training batch 151, loss: 0.433, 4832/60000 datapoints
2025-03-06 18:54:57,589 - INFO - training batch 201, loss: 0.333, 6432/60000 datapoints
2025-03-06 18:54:57,782 - INFO - training batch 251, loss: 0.373, 8032/60000 datapoints
2025-03-06 18:54:57,973 - INFO - training batch 301, loss: 0.331, 9632/60000 datapoints
2025-03-06 18:54:58,169 - INFO - training batch 351, loss: 0.560, 11232/60000 datapoints
2025-03-06 18:54:58,364 - INFO - training batch 401, loss: 0.280, 12832/60000 datapoints
2025-03-06 18:54:58,563 - INFO - training batch 451, loss: 0.296, 14432/60000 datapoints
2025-03-06 18:54:58,760 - INFO - training batch 501, loss: 0.777, 16032/60000 datapoints
2025-03-06 18:54:58,954 - INFO - training batch 551, loss: 0.331, 17632/60000 datapoints
2025-03-06 18:54:59,147 - INFO - training batch 601, loss: 0.473, 19232/60000 datapoints
2025-03-06 18:54:59,339 - INFO - training batch 651, loss: 0.381, 20832/60000 datapoints
2025-03-06 18:54:59,529 - INFO - training batch 701, loss: 0.752, 22432/60000 datapoints
2025-03-06 18:54:59,721 - INFO - training batch 751, loss: 0.368, 24032/60000 datapoints
2025-03-06 18:54:59,916 - INFO - training batch 801, loss: 0.631, 25632/60000 datapoints
2025-03-06 18:55:00,108 - INFO - training batch 851, loss: 0.385, 27232/60000 datapoints
2025-03-06 18:55:00,301 - INFO - training batch 901, loss: 0.386, 28832/60000 datapoints
2025-03-06 18:55:00,492 - INFO - training batch 951, loss: 0.560, 30432/60000 datapoints
2025-03-06 18:55:00,688 - INFO - training batch 1001, loss: 0.466, 32032/60000 datapoints
2025-03-06 18:55:00,879 - INFO - training batch 1051, loss: 0.384, 33632/60000 datapoints
2025-03-06 18:55:01,070 - INFO - training batch 1101, loss: 0.482, 35232/60000 datapoints
2025-03-06 18:55:01,263 - INFO - training batch 1151, loss: 0.562, 36832/60000 datapoints
2025-03-06 18:55:01,454 - INFO - training batch 1201, loss: 0.624, 38432/60000 datapoints
2025-03-06 18:55:01,653 - INFO - training batch 1251, loss: 0.457, 40032/60000 datapoints
2025-03-06 18:55:01,845 - INFO - training batch 1301, loss: 0.520, 41632/60000 datapoints
2025-03-06 18:55:02,038 - INFO - training batch 1351, loss: 0.445, 43232/60000 datapoints
2025-03-06 18:55:02,226 - INFO - training batch 1401, loss: 0.381, 44832/60000 datapoints
2025-03-06 18:55:02,417 - INFO - training batch 1451, loss: 0.326, 46432/60000 datapoints
2025-03-06 18:55:02,615 - INFO - training batch 1501, loss: 0.572, 48032/60000 datapoints
2025-03-06 18:55:02,808 - INFO - training batch 1551, loss: 0.298, 49632/60000 datapoints
2025-03-06 18:55:03,000 - INFO - training batch 1601, loss: 0.397, 51232/60000 datapoints
2025-03-06 18:55:03,192 - INFO - training batch 1651, loss: 0.502, 52832/60000 datapoints
2025-03-06 18:55:03,384 - INFO - training batch 1701, loss: 0.384, 54432/60000 datapoints
2025-03-06 18:55:03,580 - INFO - training batch 1751, loss: 0.396, 56032/60000 datapoints
2025-03-06 18:55:03,775 - INFO - training batch 1801, loss: 0.391, 57632/60000 datapoints
2025-03-06 18:55:03,970 - INFO - training batch 1851, loss: 0.373, 59232/60000 datapoints
2025-03-06 18:55:04,068 - INFO - validation batch 1, loss: 0.435, 32/10016 datapoints
2025-03-06 18:55:04,218 - INFO - validation batch 51, loss: 0.152, 1632/10016 datapoints
2025-03-06 18:55:04,370 - INFO - validation batch 101, loss: 0.316, 3232/10016 datapoints
2025-03-06 18:55:04,521 - INFO - validation batch 151, loss: 0.740, 4832/10016 datapoints
2025-03-06 18:55:04,683 - INFO - validation batch 201, loss: 0.404, 6432/10016 datapoints
2025-03-06 18:55:04,835 - INFO - validation batch 251, loss: 0.472, 8032/10016 datapoints
2025-03-06 18:55:04,994 - INFO - validation batch 301, loss: 0.544, 9632/10016 datapoints
2025-03-06 18:55:05,030 - INFO - Epoch 122/800 done.
2025-03-06 18:55:05,030 - INFO - Final validation performance:
Loss: 0.438, top-1 acc: 0.885top-5 acc: 0.885
2025-03-06 18:55:05,031 - INFO - Beginning epoch 123/800
2025-03-06 18:55:05,037 - INFO - training batch 1, loss: 0.546, 32/60000 datapoints
2025-03-06 18:55:05,229 - INFO - training batch 51, loss: 0.425, 1632/60000 datapoints
2025-03-06 18:55:05,432 - INFO - training batch 101, loss: 0.379, 3232/60000 datapoints
2025-03-06 18:55:05,628 - INFO - training batch 151, loss: 0.386, 4832/60000 datapoints
2025-03-06 18:55:05,835 - INFO - training batch 201, loss: 0.588, 6432/60000 datapoints
2025-03-06 18:55:06,027 - INFO - training batch 251, loss: 0.647, 8032/60000 datapoints
2025-03-06 18:55:06,232 - INFO - training batch 301, loss: 0.341, 9632/60000 datapoints
2025-03-06 18:55:06,432 - INFO - training batch 351, loss: 0.558, 11232/60000 datapoints
2025-03-06 18:55:06,638 - INFO - training batch 401, loss: 0.733, 12832/60000 datapoints
2025-03-06 18:55:06,860 - INFO - training batch 451, loss: 0.363, 14432/60000 datapoints
2025-03-06 18:55:07,060 - INFO - training batch 501, loss: 0.719, 16032/60000 datapoints
2025-03-06 18:55:07,255 - INFO - training batch 551, loss: 0.420, 17632/60000 datapoints
2025-03-06 18:55:07,451 - INFO - training batch 601, loss: 0.509, 19232/60000 datapoints
2025-03-06 18:55:07,651 - INFO - training batch 651, loss: 0.445, 20832/60000 datapoints
2025-03-06 18:55:07,848 - INFO - training batch 701, loss: 0.453, 22432/60000 datapoints
2025-03-06 18:55:08,046 - INFO - training batch 751, loss: 0.621, 24032/60000 datapoints
2025-03-06 18:55:08,242 - INFO - training batch 801, loss: 0.451, 25632/60000 datapoints
2025-03-06 18:55:08,437 - INFO - training batch 851, loss: 0.387, 27232/60000 datapoints
2025-03-06 18:55:08,642 - INFO - training batch 901, loss: 0.331, 28832/60000 datapoints
2025-03-06 18:55:08,837 - INFO - training batch 951, loss: 0.480, 30432/60000 datapoints
2025-03-06 18:55:09,032 - INFO - training batch 1001, loss: 0.337, 32032/60000 datapoints
2025-03-06 18:55:09,224 - INFO - training batch 1051, loss: 0.394, 33632/60000 datapoints
2025-03-06 18:55:09,423 - INFO - training batch 1101, loss: 0.510, 35232/60000 datapoints
2025-03-06 18:55:09,624 - INFO - training batch 1151, loss: 0.670, 36832/60000 datapoints
2025-03-06 18:55:09,824 - INFO - training batch 1201, loss: 0.488, 38432/60000 datapoints
2025-03-06 18:55:10,021 - INFO - training batch 1251, loss: 0.745, 40032/60000 datapoints
2025-03-06 18:55:10,211 - INFO - training batch 1301, loss: 0.427, 41632/60000 datapoints
2025-03-06 18:55:10,405 - INFO - training batch 1351, loss: 0.390, 43232/60000 datapoints
2025-03-06 18:55:10,599 - INFO - training batch 1401, loss: 0.353, 44832/60000 datapoints
2025-03-06 18:55:10,800 - INFO - training batch 1451, loss: 0.386, 46432/60000 datapoints
2025-03-06 18:55:10,996 - INFO - training batch 1501, loss: 0.486, 48032/60000 datapoints
2025-03-06 18:55:11,190 - INFO - training batch 1551, loss: 0.715, 49632/60000 datapoints
2025-03-06 18:55:11,388 - INFO - training batch 1601, loss: 0.334, 51232/60000 datapoints
2025-03-06 18:55:11,585 - INFO - training batch 1651, loss: 0.408, 52832/60000 datapoints
2025-03-06 18:55:11,779 - INFO - training batch 1701, loss: 0.275, 54432/60000 datapoints
2025-03-06 18:55:11,974 - INFO - training batch 1751, loss: 0.456, 56032/60000 datapoints
2025-03-06 18:55:12,170 - INFO - training batch 1801, loss: 0.442, 57632/60000 datapoints
2025-03-06 18:55:12,365 - INFO - training batch 1851, loss: 0.243, 59232/60000 datapoints
2025-03-06 18:55:12,466 - INFO - validation batch 1, loss: 0.426, 32/10016 datapoints
2025-03-06 18:55:12,623 - INFO - validation batch 51, loss: 0.544, 1632/10016 datapoints
2025-03-06 18:55:12,776 - INFO - validation batch 101, loss: 0.493, 3232/10016 datapoints
2025-03-06 18:55:12,931 - INFO - validation batch 151, loss: 0.356, 4832/10016 datapoints
2025-03-06 18:55:13,086 - INFO - validation batch 201, loss: 0.374, 6432/10016 datapoints
2025-03-06 18:55:13,239 - INFO - validation batch 251, loss: 0.722, 8032/10016 datapoints
2025-03-06 18:55:13,393 - INFO - validation batch 301, loss: 0.540, 9632/10016 datapoints
2025-03-06 18:55:13,431 - INFO - Epoch 123/800 done.
2025-03-06 18:55:13,431 - INFO - Final validation performance:
Loss: 0.494, top-1 acc: 0.885top-5 acc: 0.885
2025-03-06 18:55:13,431 - INFO - Beginning epoch 124/800
2025-03-06 18:55:13,438 - INFO - training batch 1, loss: 0.679, 32/60000 datapoints
2025-03-06 18:55:13,638 - INFO - training batch 51, loss: 0.603, 1632/60000 datapoints
2025-03-06 18:55:13,832 - INFO - training batch 101, loss: 0.726, 3232/60000 datapoints
2025-03-06 18:55:14,037 - INFO - training batch 151, loss: 0.387, 4832/60000 datapoints
2025-03-06 18:55:14,236 - INFO - training batch 201, loss: 0.526, 6432/60000 datapoints
2025-03-06 18:55:14,433 - INFO - training batch 251, loss: 0.518, 8032/60000 datapoints
2025-03-06 18:55:14,632 - INFO - training batch 301, loss: 0.508, 9632/60000 datapoints
2025-03-06 18:55:14,832 - INFO - training batch 351, loss: 0.442, 11232/60000 datapoints
2025-03-06 18:55:15,034 - INFO - training batch 401, loss: 0.654, 12832/60000 datapoints
2025-03-06 18:55:15,229 - INFO - training batch 451, loss: 0.223, 14432/60000 datapoints
2025-03-06 18:55:15,425 - INFO - training batch 501, loss: 0.436, 16032/60000 datapoints
2025-03-06 18:55:15,623 - INFO - training batch 551, loss: 0.537, 17632/60000 datapoints
2025-03-06 18:55:15,829 - INFO - training batch 601, loss: 0.471, 19232/60000 datapoints
2025-03-06 18:55:16,053 - INFO - training batch 651, loss: 0.406, 20832/60000 datapoints
2025-03-06 18:55:16,249 - INFO - training batch 701, loss: 0.691, 22432/60000 datapoints
2025-03-06 18:55:16,443 - INFO - training batch 751, loss: 0.269, 24032/60000 datapoints
2025-03-06 18:55:16,643 - INFO - training batch 801, loss: 0.641, 25632/60000 datapoints
2025-03-06 18:55:16,837 - INFO - training batch 851, loss: 0.398, 27232/60000 datapoints
2025-03-06 18:55:17,054 - INFO - training batch 901, loss: 0.538, 28832/60000 datapoints
2025-03-06 18:55:17,255 - INFO - training batch 951, loss: 0.343, 30432/60000 datapoints
2025-03-06 18:55:17,448 - INFO - training batch 1001, loss: 0.368, 32032/60000 datapoints
2025-03-06 18:55:17,644 - INFO - training batch 1051, loss: 0.340, 33632/60000 datapoints
2025-03-06 18:55:17,837 - INFO - training batch 1101, loss: 0.443, 35232/60000 datapoints
2025-03-06 18:55:18,032 - INFO - training batch 1151, loss: 0.612, 36832/60000 datapoints
2025-03-06 18:55:18,224 - INFO - training batch 1201, loss: 0.398, 38432/60000 datapoints
2025-03-06 18:55:18,419 - INFO - training batch 1251, loss: 0.527, 40032/60000 datapoints
2025-03-06 18:55:18,614 - INFO - training batch 1301, loss: 0.448, 41632/60000 datapoints
2025-03-06 18:55:18,816 - INFO - training batch 1351, loss: 0.508, 43232/60000 datapoints
2025-03-06 18:55:19,011 - INFO - training batch 1401, loss: 0.386, 44832/60000 datapoints
2025-03-06 18:55:19,209 - INFO - training batch 1451, loss: 0.333, 46432/60000 datapoints
2025-03-06 18:55:19,404 - INFO - training batch 1501, loss: 0.379, 48032/60000 datapoints
2025-03-06 18:55:19,600 - INFO - training batch 1551, loss: 0.355, 49632/60000 datapoints
2025-03-06 18:55:19,794 - INFO - training batch 1601, loss: 0.408, 51232/60000 datapoints
2025-03-06 18:55:19,988 - INFO - training batch 1651, loss: 0.578, 52832/60000 datapoints
2025-03-06 18:55:20,181 - INFO - training batch 1701, loss: 0.497, 54432/60000 datapoints
2025-03-06 18:55:20,374 - INFO - training batch 1751, loss: 0.968, 56032/60000 datapoints
2025-03-06 18:55:20,569 - INFO - training batch 1801, loss: 0.632, 57632/60000 datapoints
2025-03-06 18:55:20,765 - INFO - training batch 1851, loss: 0.304, 59232/60000 datapoints
2025-03-06 18:55:20,866 - INFO - validation batch 1, loss: 0.484, 32/10016 datapoints
2025-03-06 18:55:21,020 - INFO - validation batch 51, loss: 0.576, 1632/10016 datapoints
2025-03-06 18:55:21,171 - INFO - validation batch 101, loss: 0.342, 3232/10016 datapoints
2025-03-06 18:55:21,323 - INFO - validation batch 151, loss: 0.550, 4832/10016 datapoints
2025-03-06 18:55:21,478 - INFO - validation batch 201, loss: 0.557, 6432/10016 datapoints
2025-03-06 18:55:21,631 - INFO - validation batch 251, loss: 0.452, 8032/10016 datapoints
2025-03-06 18:55:21,783 - INFO - validation batch 301, loss: 0.364, 9632/10016 datapoints
2025-03-06 18:55:21,820 - INFO - Epoch 124/800 done.
2025-03-06 18:55:21,820 - INFO - Final validation performance:
Loss: 0.475, top-1 acc: 0.886top-5 acc: 0.886
2025-03-06 18:55:21,821 - INFO - Beginning epoch 125/800
2025-03-06 18:55:21,828 - INFO - training batch 1, loss: 0.470, 32/60000 datapoints
2025-03-06 18:55:22,035 - INFO - training batch 51, loss: 0.449, 1632/60000 datapoints
2025-03-06 18:55:22,229 - INFO - training batch 101, loss: 0.584, 3232/60000 datapoints
2025-03-06 18:55:22,425 - INFO - training batch 151, loss: 0.524, 4832/60000 datapoints
2025-03-06 18:55:22,624 - INFO - training batch 201, loss: 0.271, 6432/60000 datapoints
2025-03-06 18:55:22,819 - INFO - training batch 251, loss: 0.345, 8032/60000 datapoints
2025-03-06 18:55:23,012 - INFO - training batch 301, loss: 0.338, 9632/60000 datapoints
2025-03-06 18:55:23,209 - INFO - training batch 351, loss: 0.681, 11232/60000 datapoints
2025-03-06 18:55:23,404 - INFO - training batch 401, loss: 0.592, 12832/60000 datapoints
2025-03-06 18:55:23,603 - INFO - training batch 451, loss: 0.518, 14432/60000 datapoints
2025-03-06 18:55:23,799 - INFO - training batch 501, loss: 0.356, 16032/60000 datapoints
2025-03-06 18:55:23,996 - INFO - training batch 551, loss: 0.497, 17632/60000 datapoints
2025-03-06 18:55:24,193 - INFO - training batch 601, loss: 0.550, 19232/60000 datapoints
2025-03-06 18:55:24,392 - INFO - training batch 651, loss: 0.550, 20832/60000 datapoints
2025-03-06 18:55:24,586 - INFO - training batch 701, loss: 0.500, 22432/60000 datapoints
2025-03-06 18:55:24,784 - INFO - training batch 751, loss: 0.510, 24032/60000 datapoints
2025-03-06 18:55:24,983 - INFO - training batch 801, loss: 0.491, 25632/60000 datapoints
2025-03-06 18:55:25,177 - INFO - training batch 851, loss: 0.526, 27232/60000 datapoints
2025-03-06 18:55:25,371 - INFO - training batch 901, loss: 0.708, 28832/60000 datapoints
2025-03-06 18:55:25,566 - INFO - training batch 951, loss: 0.566, 30432/60000 datapoints
2025-03-06 18:55:25,763 - INFO - training batch 1001, loss: 0.553, 32032/60000 datapoints
2025-03-06 18:55:25,967 - INFO - training batch 1051, loss: 0.560, 33632/60000 datapoints
2025-03-06 18:55:26,173 - INFO - training batch 1101, loss: 0.584, 35232/60000 datapoints
2025-03-06 18:55:26,369 - INFO - training batch 1151, loss: 0.573, 36832/60000 datapoints
2025-03-06 18:55:26,562 - INFO - training batch 1201, loss: 0.374, 38432/60000 datapoints
2025-03-06 18:55:26,759 - INFO - training batch 1251, loss: 0.442, 40032/60000 datapoints
2025-03-06 18:55:26,953 - INFO - training batch 1301, loss: 0.306, 41632/60000 datapoints
2025-03-06 18:55:27,147 - INFO - training batch 1351, loss: 0.350, 43232/60000 datapoints
2025-03-06 18:55:27,340 - INFO - training batch 1401, loss: 0.784, 44832/60000 datapoints
2025-03-06 18:55:27,533 - INFO - training batch 1451, loss: 0.642, 46432/60000 datapoints
2025-03-06 18:55:27,730 - INFO - training batch 1501, loss: 0.484, 48032/60000 datapoints
2025-03-06 18:55:27,924 - INFO - training batch 1551, loss: 0.375, 49632/60000 datapoints
2025-03-06 18:55:28,183 - INFO - training batch 1601, loss: 0.264, 51232/60000 datapoints
2025-03-06 18:55:28,376 - INFO - training batch 1651, loss: 0.418, 52832/60000 datapoints
2025-03-06 18:55:28,568 - INFO - training batch 1701, loss: 0.678, 54432/60000 datapoints
2025-03-06 18:55:28,768 - INFO - training batch 1751, loss: 0.369, 56032/60000 datapoints
2025-03-06 18:55:28,962 - INFO - training batch 1801, loss: 0.340, 57632/60000 datapoints
2025-03-06 18:55:29,157 - INFO - training batch 1851, loss: 0.793, 59232/60000 datapoints
2025-03-06 18:55:29,258 - INFO - validation batch 1, loss: 0.411, 32/10016 datapoints
2025-03-06 18:55:29,412 - INFO - validation batch 51, loss: 0.534, 1632/10016 datapoints
2025-03-06 18:55:29,562 - INFO - validation batch 101, loss: 0.274, 3232/10016 datapoints
2025-03-06 18:55:29,717 - INFO - validation batch 151, loss: 0.297, 4832/10016 datapoints
2025-03-06 18:55:29,869 - INFO - validation batch 201, loss: 0.562, 6432/10016 datapoints
2025-03-06 18:55:30,021 - INFO - validation batch 251, loss: 0.581, 8032/10016 datapoints
2025-03-06 18:55:30,174 - INFO - validation batch 301, loss: 0.436, 9632/10016 datapoints
2025-03-06 18:55:30,211 - INFO - Epoch 125/800 done.
2025-03-06 18:55:30,212 - INFO - Final validation performance:
Loss: 0.442, top-1 acc: 0.886top-5 acc: 0.886
2025-03-06 18:55:30,212 - INFO - Beginning epoch 126/800
2025-03-06 18:55:30,218 - INFO - training batch 1, loss: 0.399, 32/60000 datapoints
2025-03-06 18:55:30,422 - INFO - training batch 51, loss: 0.463, 1632/60000 datapoints
2025-03-06 18:55:30,628 - INFO - training batch 101, loss: 0.659, 3232/60000 datapoints
2025-03-06 18:55:30,825 - INFO - training batch 151, loss: 0.568, 4832/60000 datapoints
2025-03-06 18:55:31,018 - INFO - training batch 201, loss: 0.406, 6432/60000 datapoints
2025-03-06 18:55:31,216 - INFO - training batch 251, loss: 0.381, 8032/60000 datapoints
2025-03-06 18:55:31,413 - INFO - training batch 301, loss: 0.416, 9632/60000 datapoints
2025-03-06 18:55:31,612 - INFO - training batch 351, loss: 0.431, 11232/60000 datapoints
2025-03-06 18:55:31,809 - INFO - training batch 401, loss: 0.354, 12832/60000 datapoints
2025-03-06 18:55:32,007 - INFO - training batch 451, loss: 0.429, 14432/60000 datapoints
2025-03-06 18:55:32,208 - INFO - training batch 501, loss: 0.571, 16032/60000 datapoints
2025-03-06 18:55:32,403 - INFO - training batch 551, loss: 0.520, 17632/60000 datapoints
2025-03-06 18:55:32,596 - INFO - training batch 601, loss: 0.366, 19232/60000 datapoints
2025-03-06 18:55:32,797 - INFO - training batch 651, loss: 0.695, 20832/60000 datapoints
2025-03-06 18:55:32,991 - INFO - training batch 701, loss: 0.370, 22432/60000 datapoints
2025-03-06 18:55:33,187 - INFO - training batch 751, loss: 0.300, 24032/60000 datapoints
2025-03-06 18:55:33,382 - INFO - training batch 801, loss: 0.400, 25632/60000 datapoints
2025-03-06 18:55:33,575 - INFO - training batch 851, loss: 0.511, 27232/60000 datapoints
2025-03-06 18:55:33,772 - INFO - training batch 901, loss: 0.394, 28832/60000 datapoints
2025-03-06 18:55:33,967 - INFO - training batch 951, loss: 0.515, 30432/60000 datapoints
2025-03-06 18:55:34,161 - INFO - training batch 1001, loss: 0.493, 32032/60000 datapoints
2025-03-06 18:55:34,352 - INFO - training batch 1051, loss: 0.312, 33632/60000 datapoints
2025-03-06 18:55:34,549 - INFO - training batch 1101, loss: 0.401, 35232/60000 datapoints
2025-03-06 18:55:34,749 - INFO - training batch 1151, loss: 0.523, 36832/60000 datapoints
2025-03-06 18:55:34,948 - INFO - training batch 1201, loss: 0.601, 38432/60000 datapoints
2025-03-06 18:55:35,142 - INFO - training batch 1251, loss: 0.340, 40032/60000 datapoints
2025-03-06 18:55:35,337 - INFO - training batch 1301, loss: 0.466, 41632/60000 datapoints
2025-03-06 18:55:35,530 - INFO - training batch 1351, loss: 0.767, 43232/60000 datapoints
2025-03-06 18:55:35,727 - INFO - training batch 1401, loss: 0.468, 44832/60000 datapoints
2025-03-06 18:55:35,921 - INFO - training batch 1451, loss: 0.351, 46432/60000 datapoints
2025-03-06 18:55:36,133 - INFO - training batch 1501, loss: 0.386, 48032/60000 datapoints
2025-03-06 18:55:36,330 - INFO - training batch 1551, loss: 0.498, 49632/60000 datapoints
2025-03-06 18:55:36,525 - INFO - training batch 1601, loss: 0.490, 51232/60000 datapoints
2025-03-06 18:55:36,724 - INFO - training batch 1651, loss: 0.434, 52832/60000 datapoints
2025-03-06 18:55:36,917 - INFO - training batch 1701, loss: 0.768, 54432/60000 datapoints
2025-03-06 18:55:37,119 - INFO - training batch 1751, loss: 0.443, 56032/60000 datapoints
2025-03-06 18:55:37,314 - INFO - training batch 1801, loss: 0.616, 57632/60000 datapoints
2025-03-06 18:55:37,509 - INFO - training batch 1851, loss: 0.371, 59232/60000 datapoints
2025-03-06 18:55:37,625 - INFO - validation batch 1, loss: 0.538, 32/10016 datapoints
2025-03-06 18:55:37,781 - INFO - validation batch 51, loss: 0.338, 1632/10016 datapoints
2025-03-06 18:55:37,933 - INFO - validation batch 101, loss: 0.591, 3232/10016 datapoints
2025-03-06 18:55:38,087 - INFO - validation batch 151, loss: 0.262, 4832/10016 datapoints
2025-03-06 18:55:38,241 - INFO - validation batch 201, loss: 0.384, 6432/10016 datapoints
2025-03-06 18:55:38,393 - INFO - validation batch 251, loss: 0.319, 8032/10016 datapoints
2025-03-06 18:55:38,546 - INFO - validation batch 301, loss: 0.352, 9632/10016 datapoints
2025-03-06 18:55:38,582 - INFO - Epoch 126/800 done.
2025-03-06 18:55:38,582 - INFO - Final validation performance:
Loss: 0.398, top-1 acc: 0.886top-5 acc: 0.886
2025-03-06 18:55:38,583 - INFO - Beginning epoch 127/800
2025-03-06 18:55:38,591 - INFO - training batch 1, loss: 0.370, 32/60000 datapoints
2025-03-06 18:55:38,808 - INFO - training batch 51, loss: 0.425, 1632/60000 datapoints
2025-03-06 18:55:39,002 - INFO - training batch 101, loss: 0.480, 3232/60000 datapoints
2025-03-06 18:55:39,214 - INFO - training batch 151, loss: 0.544, 4832/60000 datapoints
2025-03-06 18:55:39,408 - INFO - training batch 201, loss: 0.409, 6432/60000 datapoints
2025-03-06 18:55:39,603 - INFO - training batch 251, loss: 0.372, 8032/60000 datapoints
2025-03-06 18:55:39,803 - INFO - training batch 301, loss: 0.528, 9632/60000 datapoints
2025-03-06 18:55:40,001 - INFO - training batch 351, loss: 0.393, 11232/60000 datapoints
2025-03-06 18:55:40,195 - INFO - training batch 401, loss: 0.480, 12832/60000 datapoints
2025-03-06 18:55:40,391 - INFO - training batch 451, loss: 0.614, 14432/60000 datapoints
2025-03-06 18:55:40,586 - INFO - training batch 501, loss: 0.612, 16032/60000 datapoints
2025-03-06 18:55:40,786 - INFO - training batch 551, loss: 0.427, 17632/60000 datapoints
2025-03-06 18:55:40,980 - INFO - training batch 601, loss: 0.428, 19232/60000 datapoints
2025-03-06 18:55:41,175 - INFO - training batch 651, loss: 0.480, 20832/60000 datapoints
2025-03-06 18:55:41,371 - INFO - training batch 701, loss: 0.290, 22432/60000 datapoints
2025-03-06 18:55:41,568 - INFO - training batch 751, loss: 0.340, 24032/60000 datapoints
2025-03-06 18:55:41,764 - INFO - training batch 801, loss: 0.571, 25632/60000 datapoints
2025-03-06 18:55:41,959 - INFO - training batch 851, loss: 0.428, 27232/60000 datapoints
2025-03-06 18:55:42,155 - INFO - training batch 901, loss: 0.463, 28832/60000 datapoints
2025-03-06 18:55:42,349 - INFO - training batch 951, loss: 0.506, 30432/60000 datapoints
2025-03-06 18:55:42,544 - INFO - training batch 1001, loss: 0.404, 32032/60000 datapoints
2025-03-06 18:55:42,744 - INFO - training batch 1051, loss: 0.466, 33632/60000 datapoints
2025-03-06 18:55:42,937 - INFO - training batch 1101, loss: 0.405, 35232/60000 datapoints
2025-03-06 18:55:43,131 - INFO - training batch 1151, loss: 0.663, 36832/60000 datapoints
2025-03-06 18:55:43,326 - INFO - training batch 1201, loss: 0.612, 38432/60000 datapoints
2025-03-06 18:55:43,516 - INFO - training batch 1251, loss: 0.458, 40032/60000 datapoints
2025-03-06 18:55:43,711 - INFO - training batch 1301, loss: 0.192, 41632/60000 datapoints
2025-03-06 18:55:43,906 - INFO - training batch 1351, loss: 0.752, 43232/60000 datapoints
2025-03-06 18:55:44,102 - INFO - training batch 1401, loss: 0.575, 44832/60000 datapoints
2025-03-06 18:55:44,296 - INFO - training batch 1451, loss: 0.534, 46432/60000 datapoints
2025-03-06 18:55:44,489 - INFO - training batch 1501, loss: 0.484, 48032/60000 datapoints
2025-03-06 18:55:44,685 - INFO - training batch 1551, loss: 0.357, 49632/60000 datapoints
2025-03-06 18:55:44,884 - INFO - training batch 1601, loss: 0.201, 51232/60000 datapoints
2025-03-06 18:55:45,080 - INFO - training batch 1651, loss: 0.450, 52832/60000 datapoints
2025-03-06 18:55:45,274 - INFO - training batch 1701, loss: 0.484, 54432/60000 datapoints
2025-03-06 18:55:45,467 - INFO - training batch 1751, loss: 0.237, 56032/60000 datapoints
2025-03-06 18:55:45,662 - INFO - training batch 1801, loss: 0.383, 57632/60000 datapoints
2025-03-06 18:55:45,857 - INFO - training batch 1851, loss: 0.547, 59232/60000 datapoints
2025-03-06 18:55:45,958 - INFO - validation batch 1, loss: 0.398, 32/10016 datapoints
2025-03-06 18:55:46,127 - INFO - validation batch 51, loss: 0.378, 1632/10016 datapoints
2025-03-06 18:55:46,287 - INFO - validation batch 101, loss: 0.433, 3232/10016 datapoints
2025-03-06 18:55:46,440 - INFO - validation batch 151, loss: 0.491, 4832/10016 datapoints
2025-03-06 18:55:46,593 - INFO - validation batch 201, loss: 0.383, 6432/10016 datapoints
2025-03-06 18:55:46,752 - INFO - validation batch 251, loss: 0.581, 8032/10016 datapoints
2025-03-06 18:55:46,909 - INFO - validation batch 301, loss: 0.310, 9632/10016 datapoints
2025-03-06 18:55:46,947 - INFO - Epoch 127/800 done.
2025-03-06 18:55:46,947 - INFO - Final validation performance:
Loss: 0.425, top-1 acc: 0.887top-5 acc: 0.887
2025-03-06 18:55:46,948 - INFO - Beginning epoch 128/800
2025-03-06 18:55:46,955 - INFO - training batch 1, loss: 0.541, 32/60000 datapoints
2025-03-06 18:55:47,158 - INFO - training batch 51, loss: 0.476, 1632/60000 datapoints
2025-03-06 18:55:47,354 - INFO - training batch 101, loss: 0.587, 3232/60000 datapoints
2025-03-06 18:55:47,547 - INFO - training batch 151, loss: 0.735, 4832/60000 datapoints
2025-03-06 18:55:47,740 - INFO - training batch 201, loss: 0.425, 6432/60000 datapoints
2025-03-06 18:55:47,938 - INFO - training batch 251, loss: 0.405, 8032/60000 datapoints
2025-03-06 18:55:48,132 - INFO - training batch 301, loss: 0.497, 9632/60000 datapoints
2025-03-06 18:55:48,328 - INFO - training batch 351, loss: 0.423, 11232/60000 datapoints
2025-03-06 18:55:48,521 - INFO - training batch 401, loss: 0.367, 12832/60000 datapoints
2025-03-06 18:55:48,717 - INFO - training batch 451, loss: 0.717, 14432/60000 datapoints
2025-03-06 18:55:48,913 - INFO - training batch 501, loss: 0.441, 16032/60000 datapoints
2025-03-06 18:55:49,104 - INFO - training batch 551, loss: 0.371, 17632/60000 datapoints
2025-03-06 18:55:49,294 - INFO - training batch 601, loss: 0.326, 19232/60000 datapoints
2025-03-06 18:55:49,487 - INFO - training batch 651, loss: 0.423, 20832/60000 datapoints
2025-03-06 18:55:49,681 - INFO - training batch 701, loss: 0.614, 22432/60000 datapoints
2025-03-06 18:55:49,877 - INFO - training batch 751, loss: 0.492, 24032/60000 datapoints
2025-03-06 18:55:50,070 - INFO - training batch 801, loss: 0.375, 25632/60000 datapoints
2025-03-06 18:55:50,260 - INFO - training batch 851, loss: 0.561, 27232/60000 datapoints
2025-03-06 18:55:50,451 - INFO - training batch 901, loss: 0.546, 28832/60000 datapoints
2025-03-06 18:55:50,645 - INFO - training batch 951, loss: 0.709, 30432/60000 datapoints
2025-03-06 18:55:50,838 - INFO - training batch 1001, loss: 0.419, 32032/60000 datapoints
2025-03-06 18:55:51,028 - INFO - training batch 1051, loss: 0.444, 33632/60000 datapoints
2025-03-06 18:55:51,220 - INFO - training batch 1101, loss: 0.389, 35232/60000 datapoints
2025-03-06 18:55:51,412 - INFO - training batch 1151, loss: 0.498, 36832/60000 datapoints
2025-03-06 18:55:51,605 - INFO - training batch 1201, loss: 0.354, 38432/60000 datapoints
2025-03-06 18:55:51,804 - INFO - training batch 1251, loss: 0.663, 40032/60000 datapoints
2025-03-06 18:55:51,996 - INFO - training batch 1301, loss: 0.598, 41632/60000 datapoints
2025-03-06 18:55:52,188 - INFO - training batch 1351, loss: 0.402, 43232/60000 datapoints
2025-03-06 18:55:52,379 - INFO - training batch 1401, loss: 0.454, 44832/60000 datapoints
2025-03-06 18:55:52,569 - INFO - training batch 1451, loss: 0.319, 46432/60000 datapoints
2025-03-06 18:55:52,764 - INFO - training batch 1501, loss: 0.621, 48032/60000 datapoints
2025-03-06 18:55:52,955 - INFO - training batch 1551, loss: 0.198, 49632/60000 datapoints
2025-03-06 18:55:53,147 - INFO - training batch 1601, loss: 0.376, 51232/60000 datapoints
2025-03-06 18:55:53,337 - INFO - training batch 1651, loss: 0.721, 52832/60000 datapoints
2025-03-06 18:55:53,529 - INFO - training batch 1701, loss: 0.440, 54432/60000 datapoints
2025-03-06 18:55:53,720 - INFO - training batch 1751, loss: 0.272, 56032/60000 datapoints
2025-03-06 18:55:53,911 - INFO - training batch 1801, loss: 0.293, 57632/60000 datapoints
2025-03-06 18:55:54,103 - INFO - training batch 1851, loss: 0.456, 59232/60000 datapoints
2025-03-06 18:55:54,201 - INFO - validation batch 1, loss: 0.357, 32/10016 datapoints
2025-03-06 18:55:54,352 - INFO - validation batch 51, loss: 0.506, 1632/10016 datapoints
2025-03-06 18:55:54,503 - INFO - validation batch 101, loss: 0.231, 3232/10016 datapoints
2025-03-06 18:55:54,654 - INFO - validation batch 151, loss: 0.603, 4832/10016 datapoints
2025-03-06 18:55:54,807 - INFO - validation batch 201, loss: 0.367, 6432/10016 datapoints
2025-03-06 18:55:54,961 - INFO - validation batch 251, loss: 0.487, 8032/10016 datapoints
2025-03-06 18:55:55,112 - INFO - validation batch 301, loss: 0.424, 9632/10016 datapoints
2025-03-06 18:55:55,147 - INFO - Epoch 128/800 done.
2025-03-06 18:55:55,147 - INFO - Final validation performance:
Loss: 0.425, top-1 acc: 0.888top-5 acc: 0.888
2025-03-06 18:55:55,148 - INFO - Beginning epoch 129/800
2025-03-06 18:55:55,154 - INFO - training batch 1, loss: 0.289, 32/60000 datapoints
2025-03-06 18:55:55,344 - INFO - training batch 51, loss: 0.445, 1632/60000 datapoints
2025-03-06 18:55:55,541 - INFO - training batch 101, loss: 0.689, 3232/60000 datapoints
2025-03-06 18:55:55,747 - INFO - training batch 151, loss: 0.456, 4832/60000 datapoints
2025-03-06 18:55:55,938 - INFO - training batch 201, loss: 0.317, 6432/60000 datapoints
2025-03-06 18:55:56,132 - INFO - training batch 251, loss: 0.406, 8032/60000 datapoints
2025-03-06 18:55:56,348 - INFO - training batch 301, loss: 0.653, 9632/60000 datapoints
2025-03-06 18:55:56,541 - INFO - training batch 351, loss: 0.273, 11232/60000 datapoints
2025-03-06 18:55:56,738 - INFO - training batch 401, loss: 0.336, 12832/60000 datapoints
2025-03-06 18:55:56,937 - INFO - training batch 451, loss: 0.420, 14432/60000 datapoints
2025-03-06 18:55:57,132 - INFO - training batch 501, loss: 0.574, 16032/60000 datapoints
2025-03-06 18:55:57,323 - INFO - training batch 551, loss: 0.502, 17632/60000 datapoints
2025-03-06 18:55:57,515 - INFO - training batch 601, loss: 0.439, 19232/60000 datapoints
2025-03-06 18:55:57,707 - INFO - training batch 651, loss: 0.482, 20832/60000 datapoints
2025-03-06 18:55:57,901 - INFO - training batch 701, loss: 0.669, 22432/60000 datapoints
2025-03-06 18:55:58,094 - INFO - training batch 751, loss: 0.450, 24032/60000 datapoints
2025-03-06 18:55:58,286 - INFO - training batch 801, loss: 0.347, 25632/60000 datapoints
2025-03-06 18:55:58,478 - INFO - training batch 851, loss: 0.285, 27232/60000 datapoints
2025-03-06 18:55:58,675 - INFO - training batch 901, loss: 0.437, 28832/60000 datapoints
2025-03-06 18:55:58,867 - INFO - training batch 951, loss: 0.430, 30432/60000 datapoints
2025-03-06 18:55:59,059 - INFO - training batch 1001, loss: 0.846, 32032/60000 datapoints
2025-03-06 18:55:59,250 - INFO - training batch 1051, loss: 0.453, 33632/60000 datapoints
2025-03-06 18:55:59,442 - INFO - training batch 1101, loss: 0.337, 35232/60000 datapoints
2025-03-06 18:55:59,640 - INFO - training batch 1151, loss: 0.614, 36832/60000 datapoints
2025-03-06 18:55:59,870 - INFO - training batch 1201, loss: 0.396, 38432/60000 datapoints
2025-03-06 18:56:00,064 - INFO - training batch 1251, loss: 0.381, 40032/60000 datapoints
2025-03-06 18:56:00,254 - INFO - training batch 1301, loss: 0.444, 41632/60000 datapoints
2025-03-06 18:56:00,445 - INFO - training batch 1351, loss: 0.644, 43232/60000 datapoints
2025-03-06 18:56:00,639 - INFO - training batch 1401, loss: 0.414, 44832/60000 datapoints
2025-03-06 18:56:00,832 - INFO - training batch 1451, loss: 0.443, 46432/60000 datapoints
2025-03-06 18:56:01,024 - INFO - training batch 1501, loss: 0.501, 48032/60000 datapoints
2025-03-06 18:56:01,215 - INFO - training batch 1551, loss: 0.398, 49632/60000 datapoints
2025-03-06 18:56:01,406 - INFO - training batch 1601, loss: 0.587, 51232/60000 datapoints
2025-03-06 18:56:01,598 - INFO - training batch 1651, loss: 0.433, 52832/60000 datapoints
2025-03-06 18:56:01,794 - INFO - training batch 1701, loss: 0.771, 54432/60000 datapoints
2025-03-06 18:56:01,986 - INFO - training batch 1751, loss: 0.352, 56032/60000 datapoints
2025-03-06 18:56:02,176 - INFO - training batch 1801, loss: 0.272, 57632/60000 datapoints
2025-03-06 18:56:02,366 - INFO - training batch 1851, loss: 0.248, 59232/60000 datapoints
2025-03-06 18:56:02,465 - INFO - validation batch 1, loss: 0.433, 32/10016 datapoints
2025-03-06 18:56:02,617 - INFO - validation batch 51, loss: 0.411, 1632/10016 datapoints
2025-03-06 18:56:02,767 - INFO - validation batch 101, loss: 0.358, 3232/10016 datapoints
2025-03-06 18:56:02,920 - INFO - validation batch 151, loss: 0.338, 4832/10016 datapoints
2025-03-06 18:56:03,069 - INFO - validation batch 201, loss: 0.258, 6432/10016 datapoints
2025-03-06 18:56:03,220 - INFO - validation batch 251, loss: 0.617, 8032/10016 datapoints
2025-03-06 18:56:03,370 - INFO - validation batch 301, loss: 0.424, 9632/10016 datapoints
2025-03-06 18:56:03,406 - INFO - Epoch 129/800 done.
2025-03-06 18:56:03,406 - INFO - Final validation performance:
Loss: 0.406, top-1 acc: 0.888top-5 acc: 0.888
2025-03-06 18:56:03,407 - INFO - Beginning epoch 130/800
2025-03-06 18:56:03,413 - INFO - training batch 1, loss: 0.234, 32/60000 datapoints
2025-03-06 18:56:03,610 - INFO - training batch 51, loss: 0.512, 1632/60000 datapoints
2025-03-06 18:56:03,812 - INFO - training batch 101, loss: 0.346, 3232/60000 datapoints
2025-03-06 18:56:04,006 - INFO - training batch 151, loss: 0.521, 4832/60000 datapoints
2025-03-06 18:56:04,202 - INFO - training batch 201, loss: 0.355, 6432/60000 datapoints
2025-03-06 18:56:04,394 - INFO - training batch 251, loss: 0.558, 8032/60000 datapoints
2025-03-06 18:56:04,588 - INFO - training batch 301, loss: 0.540, 9632/60000 datapoints
2025-03-06 18:56:04,787 - INFO - training batch 351, loss: 0.463, 11232/60000 datapoints
2025-03-06 18:56:04,991 - INFO - training batch 401, loss: 0.391, 12832/60000 datapoints
2025-03-06 18:56:05,189 - INFO - training batch 451, loss: 0.719, 14432/60000 datapoints
2025-03-06 18:56:05,380 - INFO - training batch 501, loss: 0.621, 16032/60000 datapoints
2025-03-06 18:56:05,579 - INFO - training batch 551, loss: 0.604, 17632/60000 datapoints
2025-03-06 18:56:05,774 - INFO - training batch 601, loss: 0.449, 19232/60000 datapoints
2025-03-06 18:56:05,966 - INFO - training batch 651, loss: 0.435, 20832/60000 datapoints
2025-03-06 18:56:06,158 - INFO - training batch 701, loss: 0.326, 22432/60000 datapoints
2025-03-06 18:56:06,380 - INFO - training batch 751, loss: 0.482, 24032/60000 datapoints
2025-03-06 18:56:06,581 - INFO - training batch 801, loss: 0.399, 25632/60000 datapoints
2025-03-06 18:56:06,777 - INFO - training batch 851, loss: 0.525, 27232/60000 datapoints
2025-03-06 18:56:06,978 - INFO - training batch 901, loss: 0.524, 28832/60000 datapoints
2025-03-06 18:56:07,176 - INFO - training batch 951, loss: 0.928, 30432/60000 datapoints
2025-03-06 18:56:07,369 - INFO - training batch 1001, loss: 0.316, 32032/60000 datapoints
2025-03-06 18:56:07,565 - INFO - training batch 1051, loss: 0.471, 33632/60000 datapoints
2025-03-06 18:56:07,765 - INFO - training batch 1101, loss: 0.634, 35232/60000 datapoints
2025-03-06 18:56:07,958 - INFO - training batch 1151, loss: 0.447, 36832/60000 datapoints
2025-03-06 18:56:08,154 - INFO - training batch 1201, loss: 0.642, 38432/60000 datapoints
2025-03-06 18:56:08,354 - INFO - training batch 1251, loss: 0.325, 40032/60000 datapoints
2025-03-06 18:56:08,548 - INFO - training batch 1301, loss: 0.455, 41632/60000 datapoints
2025-03-06 18:56:08,744 - INFO - training batch 1351, loss: 0.527, 43232/60000 datapoints
2025-03-06 18:56:08,938 - INFO - training batch 1401, loss: 0.556, 44832/60000 datapoints
2025-03-06 18:56:09,135 - INFO - training batch 1451, loss: 0.599, 46432/60000 datapoints
2025-03-06 18:56:09,355 - INFO - training batch 1501, loss: 0.378, 48032/60000 datapoints
2025-03-06 18:56:09,551 - INFO - training batch 1551, loss: 0.797, 49632/60000 datapoints
2025-03-06 18:56:09,747 - INFO - training batch 1601, loss: 0.542, 51232/60000 datapoints
2025-03-06 18:56:09,941 - INFO - training batch 1651, loss: 0.376, 52832/60000 datapoints
2025-03-06 18:56:10,136 - INFO - training batch 1701, loss: 0.343, 54432/60000 datapoints
2025-03-06 18:56:10,334 - INFO - training batch 1751, loss: 0.675, 56032/60000 datapoints
2025-03-06 18:56:10,530 - INFO - training batch 1801, loss: 0.393, 57632/60000 datapoints
2025-03-06 18:56:10,729 - INFO - training batch 1851, loss: 0.634, 59232/60000 datapoints
2025-03-06 18:56:10,832 - INFO - validation batch 1, loss: 0.475, 32/10016 datapoints
2025-03-06 18:56:10,985 - INFO - validation batch 51, loss: 0.664, 1632/10016 datapoints
2025-03-06 18:56:11,140 - INFO - validation batch 101, loss: 0.463, 3232/10016 datapoints
2025-03-06 18:56:11,294 - INFO - validation batch 151, loss: 0.393, 4832/10016 datapoints
2025-03-06 18:56:11,445 - INFO - validation batch 201, loss: 0.268, 6432/10016 datapoints
2025-03-06 18:56:11,600 - INFO - validation batch 251, loss: 0.468, 8032/10016 datapoints
2025-03-06 18:56:11,754 - INFO - validation batch 301, loss: 0.382, 9632/10016 datapoints
2025-03-06 18:56:11,792 - INFO - Epoch 130/800 done.
2025-03-06 18:56:11,792 - INFO - Final validation performance:
Loss: 0.445, top-1 acc: 0.888top-5 acc: 0.888
2025-03-06 18:56:11,793 - INFO - Beginning epoch 131/800
2025-03-06 18:56:11,800 - INFO - training batch 1, loss: 0.555, 32/60000 datapoints
2025-03-06 18:56:12,000 - INFO - training batch 51, loss: 0.330, 1632/60000 datapoints
2025-03-06 18:56:12,194 - INFO - training batch 101, loss: 0.415, 3232/60000 datapoints
2025-03-06 18:56:12,386 - INFO - training batch 151, loss: 0.374, 4832/60000 datapoints
2025-03-06 18:56:12,581 - INFO - training batch 201, loss: 0.480, 6432/60000 datapoints
2025-03-06 18:56:12,778 - INFO - training batch 251, loss: 0.402, 8032/60000 datapoints
2025-03-06 18:56:12,974 - INFO - training batch 301, loss: 0.619, 9632/60000 datapoints
2025-03-06 18:56:13,167 - INFO - training batch 351, loss: 0.381, 11232/60000 datapoints
2025-03-06 18:56:13,368 - INFO - training batch 401, loss: 0.390, 12832/60000 datapoints
2025-03-06 18:56:13,580 - INFO - training batch 451, loss: 0.586, 14432/60000 datapoints
2025-03-06 18:56:13,777 - INFO - training batch 501, loss: 0.353, 16032/60000 datapoints
2025-03-06 18:56:13,970 - INFO - training batch 551, loss: 0.483, 17632/60000 datapoints
2025-03-06 18:56:14,167 - INFO - training batch 601, loss: 0.338, 19232/60000 datapoints
2025-03-06 18:56:14,357 - INFO - training batch 651, loss: 0.362, 20832/60000 datapoints
2025-03-06 18:56:14,548 - INFO - training batch 701, loss: 0.614, 22432/60000 datapoints
2025-03-06 18:56:14,743 - INFO - training batch 751, loss: 0.479, 24032/60000 datapoints
2025-03-06 18:56:14,940 - INFO - training batch 801, loss: 0.577, 25632/60000 datapoints
2025-03-06 18:56:15,137 - INFO - training batch 851, loss: 0.477, 27232/60000 datapoints
2025-03-06 18:56:15,330 - INFO - training batch 901, loss: 0.629, 28832/60000 datapoints
2025-03-06 18:56:15,524 - INFO - training batch 951, loss: 0.488, 30432/60000 datapoints
2025-03-06 18:56:15,723 - INFO - training batch 1001, loss: 0.429, 32032/60000 datapoints
2025-03-06 18:56:15,926 - INFO - training batch 1051, loss: 0.455, 33632/60000 datapoints
2025-03-06 18:56:16,122 - INFO - training batch 1101, loss: 0.572, 35232/60000 datapoints
2025-03-06 18:56:16,328 - INFO - training batch 1151, loss: 0.456, 36832/60000 datapoints
2025-03-06 18:56:16,539 - INFO - training batch 1201, loss: 0.331, 38432/60000 datapoints
2025-03-06 18:56:16,739 - INFO - training batch 1251, loss: 0.662, 40032/60000 datapoints
2025-03-06 18:56:16,935 - INFO - training batch 1301, loss: 0.376, 41632/60000 datapoints
2025-03-06 18:56:17,131 - INFO - training batch 1351, loss: 0.301, 43232/60000 datapoints
2025-03-06 18:56:17,325 - INFO - training batch 1401, loss: 0.522, 44832/60000 datapoints
2025-03-06 18:56:17,518 - INFO - training batch 1451, loss: 0.339, 46432/60000 datapoints
2025-03-06 18:56:17,715 - INFO - training batch 1501, loss: 0.463, 48032/60000 datapoints
2025-03-06 18:56:17,910 - INFO - training batch 1551, loss: 0.409, 49632/60000 datapoints
2025-03-06 18:56:18,104 - INFO - training batch 1601, loss: 0.259, 51232/60000 datapoints
2025-03-06 18:56:18,298 - INFO - training batch 1651, loss: 0.234, 52832/60000 datapoints
2025-03-06 18:56:18,494 - INFO - training batch 1701, loss: 0.420, 54432/60000 datapoints
2025-03-06 18:56:18,692 - INFO - training batch 1751, loss: 0.520, 56032/60000 datapoints
2025-03-06 18:56:18,887 - INFO - training batch 1801, loss: 0.530, 57632/60000 datapoints
2025-03-06 18:56:19,081 - INFO - training batch 1851, loss: 0.559, 59232/60000 datapoints
2025-03-06 18:56:19,184 - INFO - validation batch 1, loss: 0.571, 32/10016 datapoints
2025-03-06 18:56:19,336 - INFO - validation batch 51, loss: 0.367, 1632/10016 datapoints
2025-03-06 18:56:19,489 - INFO - validation batch 101, loss: 0.344, 3232/10016 datapoints
2025-03-06 18:56:19,645 - INFO - validation batch 151, loss: 0.605, 4832/10016 datapoints
2025-03-06 18:56:19,798 - INFO - validation batch 201, loss: 0.541, 6432/10016 datapoints
2025-03-06 18:56:19,951 - INFO - validation batch 251, loss: 0.269, 8032/10016 datapoints
2025-03-06 18:56:20,104 - INFO - validation batch 301, loss: 0.494, 9632/10016 datapoints
2025-03-06 18:56:20,142 - INFO - Epoch 131/800 done.
2025-03-06 18:56:20,142 - INFO - Final validation performance:
Loss: 0.456, top-1 acc: 0.889top-5 acc: 0.889
2025-03-06 18:56:20,143 - INFO - Beginning epoch 132/800
2025-03-06 18:56:20,150 - INFO - training batch 1, loss: 0.456, 32/60000 datapoints
2025-03-06 18:56:20,343 - INFO - training batch 51, loss: 0.267, 1632/60000 datapoints
2025-03-06 18:56:20,546 - INFO - training batch 101, loss: 0.506, 3232/60000 datapoints
2025-03-06 18:56:20,749 - INFO - training batch 151, loss: 0.393, 4832/60000 datapoints
2025-03-06 18:56:20,945 - INFO - training batch 201, loss: 0.341, 6432/60000 datapoints
2025-03-06 18:56:21,135 - INFO - training batch 251, loss: 0.307, 8032/60000 datapoints
2025-03-06 18:56:21,334 - INFO - training batch 301, loss: 0.443, 9632/60000 datapoints
2025-03-06 18:56:21,526 - INFO - training batch 351, loss: 0.494, 11232/60000 datapoints
2025-03-06 18:56:21,723 - INFO - training batch 401, loss: 0.397, 12832/60000 datapoints
2025-03-06 18:56:21,915 - INFO - training batch 451, loss: 0.438, 14432/60000 datapoints
2025-03-06 18:56:22,108 - INFO - training batch 501, loss: 0.458, 16032/60000 datapoints
2025-03-06 18:56:22,301 - INFO - training batch 551, loss: 0.257, 17632/60000 datapoints
2025-03-06 18:56:22,493 - INFO - training batch 601, loss: 0.272, 19232/60000 datapoints
2025-03-06 18:56:22,689 - INFO - training batch 651, loss: 0.346, 20832/60000 datapoints
2025-03-06 18:56:22,884 - INFO - training batch 701, loss: 0.475, 22432/60000 datapoints
2025-03-06 18:56:23,077 - INFO - training batch 751, loss: 0.306, 24032/60000 datapoints
2025-03-06 18:56:23,273 - INFO - training batch 801, loss: 0.618, 25632/60000 datapoints
2025-03-06 18:56:23,465 - INFO - training batch 851, loss: 0.718, 27232/60000 datapoints
2025-03-06 18:56:23,658 - INFO - training batch 901, loss: 0.684, 28832/60000 datapoints
2025-03-06 18:56:23,851 - INFO - training batch 951, loss: 0.429, 30432/60000 datapoints
2025-03-06 18:56:24,043 - INFO - training batch 1001, loss: 0.609, 32032/60000 datapoints
2025-03-06 18:56:24,237 - INFO - training batch 1051, loss: 0.583, 33632/60000 datapoints
2025-03-06 18:56:24,430 - INFO - training batch 1101, loss: 0.276, 35232/60000 datapoints
2025-03-06 18:56:24,623 - INFO - training batch 1151, loss: 0.405, 36832/60000 datapoints
2025-03-06 18:56:24,815 - INFO - training batch 1201, loss: 0.461, 38432/60000 datapoints
2025-03-06 18:56:25,014 - INFO - training batch 1251, loss: 0.291, 40032/60000 datapoints
2025-03-06 18:56:25,208 - INFO - training batch 1301, loss: 0.613, 41632/60000 datapoints
2025-03-06 18:56:25,399 - INFO - training batch 1351, loss: 0.605, 43232/60000 datapoints
2025-03-06 18:56:25,589 - INFO - training batch 1401, loss: 0.593, 44832/60000 datapoints
2025-03-06 18:56:25,785 - INFO - training batch 1451, loss: 0.428, 46432/60000 datapoints
2025-03-06 18:56:25,977 - INFO - training batch 1501, loss: 0.500, 48032/60000 datapoints
2025-03-06 18:56:26,170 - INFO - training batch 1551, loss: 0.429, 49632/60000 datapoints
2025-03-06 18:56:26,365 - INFO - training batch 1601, loss: 0.562, 51232/60000 datapoints
2025-03-06 18:56:26,573 - INFO - training batch 1651, loss: 0.461, 52832/60000 datapoints
2025-03-06 18:56:26,766 - INFO - training batch 1701, loss: 0.334, 54432/60000 datapoints
2025-03-06 18:56:26,958 - INFO - training batch 1751, loss: 0.541, 56032/60000 datapoints
2025-03-06 18:56:27,152 - INFO - training batch 1801, loss: 0.564, 57632/60000 datapoints
2025-03-06 18:56:27,345 - INFO - training batch 1851, loss: 0.587, 59232/60000 datapoints
2025-03-06 18:56:27,443 - INFO - validation batch 1, loss: 0.308, 32/10016 datapoints
2025-03-06 18:56:27,592 - INFO - validation batch 51, loss: 0.405, 1632/10016 datapoints
2025-03-06 18:56:27,744 - INFO - validation batch 101, loss: 0.351, 3232/10016 datapoints
2025-03-06 18:56:27,895 - INFO - validation batch 151, loss: 0.520, 4832/10016 datapoints
2025-03-06 18:56:28,050 - INFO - validation batch 201, loss: 0.440, 6432/10016 datapoints
2025-03-06 18:56:28,206 - INFO - validation batch 251, loss: 0.774, 8032/10016 datapoints
2025-03-06 18:56:28,359 - INFO - validation batch 301, loss: 0.621, 9632/10016 datapoints
2025-03-06 18:56:28,395 - INFO - Epoch 132/800 done.
2025-03-06 18:56:28,395 - INFO - Final validation performance:
Loss: 0.489, top-1 acc: 0.889top-5 acc: 0.889
2025-03-06 18:56:28,395 - INFO - Beginning epoch 133/800
2025-03-06 18:56:28,401 - INFO - training batch 1, loss: 0.418, 32/60000 datapoints
2025-03-06 18:56:28,601 - INFO - training batch 51, loss: 0.294, 1632/60000 datapoints
2025-03-06 18:56:28,803 - INFO - training batch 101, loss: 0.589, 3232/60000 datapoints
2025-03-06 18:56:28,997 - INFO - training batch 151, loss: 0.446, 4832/60000 datapoints
2025-03-06 18:56:29,195 - INFO - training batch 201, loss: 0.363, 6432/60000 datapoints
2025-03-06 18:56:29,391 - INFO - training batch 251, loss: 0.425, 8032/60000 datapoints
2025-03-06 18:56:29,587 - INFO - training batch 301, loss: 0.412, 9632/60000 datapoints
2025-03-06 18:56:29,782 - INFO - training batch 351, loss: 0.611, 11232/60000 datapoints
2025-03-06 18:56:29,974 - INFO - training batch 401, loss: 0.258, 12832/60000 datapoints
2025-03-06 18:56:30,165 - INFO - training batch 451, loss: 0.328, 14432/60000 datapoints
2025-03-06 18:56:30,358 - INFO - training batch 501, loss: 0.246, 16032/60000 datapoints
2025-03-06 18:56:30,552 - INFO - training batch 551, loss: 0.318, 17632/60000 datapoints
2025-03-06 18:56:30,747 - INFO - training batch 601, loss: 0.572, 19232/60000 datapoints
2025-03-06 18:56:30,948 - INFO - training batch 651, loss: 0.337, 20832/60000 datapoints
2025-03-06 18:56:31,138 - INFO - training batch 701, loss: 0.601, 22432/60000 datapoints
2025-03-06 18:56:31,332 - INFO - training batch 751, loss: 0.315, 24032/60000 datapoints
2025-03-06 18:56:31,523 - INFO - training batch 801, loss: 0.548, 25632/60000 datapoints
2025-03-06 18:56:31,717 - INFO - training batch 851, loss: 0.508, 27232/60000 datapoints
2025-03-06 18:56:31,909 - INFO - training batch 901, loss: 0.407, 28832/60000 datapoints
2025-03-06 18:56:32,100 - INFO - training batch 951, loss: 0.542, 30432/60000 datapoints
2025-03-06 18:56:32,292 - INFO - training batch 1001, loss: 0.375, 32032/60000 datapoints
2025-03-06 18:56:32,487 - INFO - training batch 1051, loss: 0.318, 33632/60000 datapoints
2025-03-06 18:56:32,679 - INFO - training batch 1101, loss: 0.286, 35232/60000 datapoints
2025-03-06 18:56:32,870 - INFO - training batch 1151, loss: 0.225, 36832/60000 datapoints
2025-03-06 18:56:33,062 - INFO - training batch 1201, loss: 0.861, 38432/60000 datapoints
2025-03-06 18:56:33,253 - INFO - training batch 1251, loss: 0.285, 40032/60000 datapoints
2025-03-06 18:56:33,444 - INFO - training batch 1301, loss: 0.391, 41632/60000 datapoints
2025-03-06 18:56:33,636 - INFO - training batch 1351, loss: 0.490, 43232/60000 datapoints
2025-03-06 18:56:33,828 - INFO - training batch 1401, loss: 0.442, 44832/60000 datapoints
2025-03-06 18:56:34,019 - INFO - training batch 1451, loss: 0.291, 46432/60000 datapoints
2025-03-06 18:56:34,210 - INFO - training batch 1501, loss: 0.356, 48032/60000 datapoints
2025-03-06 18:56:34,403 - INFO - training batch 1551, loss: 0.397, 49632/60000 datapoints
2025-03-06 18:56:34,591 - INFO - training batch 1601, loss: 0.658, 51232/60000 datapoints
2025-03-06 18:56:34,786 - INFO - training batch 1651, loss: 0.437, 52832/60000 datapoints
2025-03-06 18:56:34,983 - INFO - training batch 1701, loss: 0.383, 54432/60000 datapoints
2025-03-06 18:56:35,182 - INFO - training batch 1751, loss: 0.381, 56032/60000 datapoints
2025-03-06 18:56:35,378 - INFO - training batch 1801, loss: 0.312, 57632/60000 datapoints
2025-03-06 18:56:35,572 - INFO - training batch 1851, loss: 0.456, 59232/60000 datapoints
2025-03-06 18:56:35,677 - INFO - validation batch 1, loss: 0.588, 32/10016 datapoints
2025-03-06 18:56:35,829 - INFO - validation batch 51, loss: 0.402, 1632/10016 datapoints
2025-03-06 18:56:35,980 - INFO - validation batch 101, loss: 0.449, 3232/10016 datapoints
2025-03-06 18:56:36,132 - INFO - validation batch 151, loss: 0.368, 4832/10016 datapoints
2025-03-06 18:56:36,289 - INFO - validation batch 201, loss: 0.308, 6432/10016 datapoints
2025-03-06 18:56:36,443 - INFO - validation batch 251, loss: 0.536, 8032/10016 datapoints
2025-03-06 18:56:36,615 - INFO - validation batch 301, loss: 0.302, 9632/10016 datapoints
2025-03-06 18:56:36,653 - INFO - Epoch 133/800 done.
2025-03-06 18:56:36,653 - INFO - Final validation performance:
Loss: 0.422, top-1 acc: 0.890top-5 acc: 0.890
2025-03-06 18:56:36,654 - INFO - Beginning epoch 134/800
2025-03-06 18:56:36,660 - INFO - training batch 1, loss: 0.649, 32/60000 datapoints
2025-03-06 18:56:36,856 - INFO - training batch 51, loss: 0.614, 1632/60000 datapoints
2025-03-06 18:56:37,060 - INFO - training batch 101, loss: 0.500, 3232/60000 datapoints
2025-03-06 18:56:37,268 - INFO - training batch 151, loss: 0.440, 4832/60000 datapoints
2025-03-06 18:56:37,463 - INFO - training batch 201, loss: 0.289, 6432/60000 datapoints
2025-03-06 18:56:37,678 - INFO - training batch 251, loss: 0.600, 8032/60000 datapoints
2025-03-06 18:56:37,884 - INFO - training batch 301, loss: 0.409, 9632/60000 datapoints
2025-03-06 18:56:38,081 - INFO - training batch 351, loss: 0.501, 11232/60000 datapoints
2025-03-06 18:56:38,276 - INFO - training batch 401, loss: 0.512, 12832/60000 datapoints
2025-03-06 18:56:38,475 - INFO - training batch 451, loss: 0.921, 14432/60000 datapoints
2025-03-06 18:56:38,675 - INFO - training batch 501, loss: 0.655, 16032/60000 datapoints
2025-03-06 18:56:38,872 - INFO - training batch 551, loss: 0.407, 17632/60000 datapoints
2025-03-06 18:56:39,072 - INFO - training batch 601, loss: 0.678, 19232/60000 datapoints
2025-03-06 18:56:39,267 - INFO - training batch 651, loss: 0.599, 20832/60000 datapoints
2025-03-06 18:56:39,464 - INFO - training batch 701, loss: 0.206, 22432/60000 datapoints
2025-03-06 18:56:39,660 - INFO - training batch 751, loss: 0.230, 24032/60000 datapoints
2025-03-06 18:56:39,855 - INFO - training batch 801, loss: 0.547, 25632/60000 datapoints
2025-03-06 18:56:40,052 - INFO - training batch 851, loss: 0.558, 27232/60000 datapoints
2025-03-06 18:56:40,247 - INFO - training batch 901, loss: 0.512, 28832/60000 datapoints
2025-03-06 18:56:40,445 - INFO - training batch 951, loss: 0.208, 30432/60000 datapoints
2025-03-06 18:56:40,643 - INFO - training batch 1001, loss: 0.439, 32032/60000 datapoints
2025-03-06 18:56:40,838 - INFO - training batch 1051, loss: 0.426, 33632/60000 datapoints
2025-03-06 18:56:41,036 - INFO - training batch 1101, loss: 0.484, 35232/60000 datapoints
2025-03-06 18:56:41,231 - INFO - training batch 1151, loss: 0.626, 36832/60000 datapoints
2025-03-06 18:56:41,428 - INFO - training batch 1201, loss: 0.410, 38432/60000 datapoints
2025-03-06 18:56:41,627 - INFO - training batch 1251, loss: 0.184, 40032/60000 datapoints
2025-03-06 18:56:41,824 - INFO - training batch 1301, loss: 0.322, 41632/60000 datapoints
2025-03-06 18:56:42,021 - INFO - training batch 1351, loss: 0.388, 43232/60000 datapoints
2025-03-06 18:56:42,216 - INFO - training batch 1401, loss: 0.459, 44832/60000 datapoints
2025-03-06 18:56:42,411 - INFO - training batch 1451, loss: 0.537, 46432/60000 datapoints
2025-03-06 18:56:42,607 - INFO - training batch 1501, loss: 0.271, 48032/60000 datapoints
2025-03-06 18:56:42,800 - INFO - training batch 1551, loss: 0.253, 49632/60000 datapoints
2025-03-06 18:56:42,997 - INFO - training batch 1601, loss: 0.398, 51232/60000 datapoints
2025-03-06 18:56:43,191 - INFO - training batch 1651, loss: 0.393, 52832/60000 datapoints
2025-03-06 18:56:43,387 - INFO - training batch 1701, loss: 0.511, 54432/60000 datapoints
2025-03-06 18:56:43,583 - INFO - training batch 1751, loss: 0.509, 56032/60000 datapoints
2025-03-06 18:56:43,781 - INFO - training batch 1801, loss: 0.391, 57632/60000 datapoints
2025-03-06 18:56:43,977 - INFO - training batch 1851, loss: 0.381, 59232/60000 datapoints
2025-03-06 18:56:44,080 - INFO - validation batch 1, loss: 0.311, 32/10016 datapoints
2025-03-06 18:56:44,233 - INFO - validation batch 51, loss: 0.478, 1632/10016 datapoints
2025-03-06 18:56:44,388 - INFO - validation batch 101, loss: 0.310, 3232/10016 datapoints
2025-03-06 18:56:44,541 - INFO - validation batch 151, loss: 0.179, 4832/10016 datapoints
2025-03-06 18:56:44,694 - INFO - validation batch 201, loss: 0.445, 6432/10016 datapoints
2025-03-06 18:56:44,845 - INFO - validation batch 251, loss: 0.407, 8032/10016 datapoints
2025-03-06 18:56:45,005 - INFO - validation batch 301, loss: 0.493, 9632/10016 datapoints
2025-03-06 18:56:45,042 - INFO - Epoch 134/800 done.
2025-03-06 18:56:45,043 - INFO - Final validation performance:
Loss: 0.375, top-1 acc: 0.890top-5 acc: 0.890
2025-03-06 18:56:45,043 - INFO - Beginning epoch 135/800
2025-03-06 18:56:45,050 - INFO - training batch 1, loss: 0.423, 32/60000 datapoints
2025-03-06 18:56:45,242 - INFO - training batch 51, loss: 0.551, 1632/60000 datapoints
2025-03-06 18:56:45,436 - INFO - training batch 101, loss: 0.528, 3232/60000 datapoints
2025-03-06 18:56:45,642 - INFO - training batch 151, loss: 0.935, 4832/60000 datapoints
2025-03-06 18:56:45,836 - INFO - training batch 201, loss: 0.373, 6432/60000 datapoints
2025-03-06 18:56:46,031 - INFO - training batch 251, loss: 0.299, 8032/60000 datapoints
2025-03-06 18:56:46,225 - INFO - training batch 301, loss: 0.283, 9632/60000 datapoints
2025-03-06 18:56:46,422 - INFO - training batch 351, loss: 0.425, 11232/60000 datapoints
2025-03-06 18:56:46,639 - INFO - training batch 401, loss: 0.553, 12832/60000 datapoints
2025-03-06 18:56:46,833 - INFO - training batch 451, loss: 0.771, 14432/60000 datapoints
2025-03-06 18:56:47,030 - INFO - training batch 501, loss: 0.574, 16032/60000 datapoints
2025-03-06 18:56:47,228 - INFO - training batch 551, loss: 0.464, 17632/60000 datapoints
2025-03-06 18:56:47,420 - INFO - training batch 601, loss: 0.459, 19232/60000 datapoints
2025-03-06 18:56:47,614 - INFO - training batch 651, loss: 0.573, 20832/60000 datapoints
2025-03-06 18:56:47,803 - INFO - training batch 701, loss: 0.406, 22432/60000 datapoints
2025-03-06 18:56:47,995 - INFO - training batch 751, loss: 0.260, 24032/60000 datapoints
2025-03-06 18:56:48,186 - INFO - training batch 801, loss: 0.343, 25632/60000 datapoints
2025-03-06 18:56:48,379 - INFO - training batch 851, loss: 0.398, 27232/60000 datapoints
2025-03-06 18:56:48,574 - INFO - training batch 901, loss: 0.276, 28832/60000 datapoints
2025-03-06 18:56:48,767 - INFO - training batch 951, loss: 0.616, 30432/60000 datapoints
2025-03-06 18:56:48,960 - INFO - training batch 1001, loss: 0.518, 32032/60000 datapoints
2025-03-06 18:56:49,153 - INFO - training batch 1051, loss: 0.639, 33632/60000 datapoints
2025-03-06 18:56:49,343 - INFO - training batch 1101, loss: 0.286, 35232/60000 datapoints
2025-03-06 18:56:49,536 - INFO - training batch 1151, loss: 0.266, 36832/60000 datapoints
2025-03-06 18:56:49,731 - INFO - training batch 1201, loss: 0.403, 38432/60000 datapoints
2025-03-06 18:56:49,923 - INFO - training batch 1251, loss: 0.287, 40032/60000 datapoints
2025-03-06 18:56:50,116 - INFO - training batch 1301, loss: 0.419, 41632/60000 datapoints
2025-03-06 18:56:50,308 - INFO - training batch 1351, loss: 0.253, 43232/60000 datapoints
2025-03-06 18:56:50,499 - INFO - training batch 1401, loss: 0.899, 44832/60000 datapoints
2025-03-06 18:56:50,693 - INFO - training batch 1451, loss: 0.625, 46432/60000 datapoints
2025-03-06 18:56:50,884 - INFO - training batch 1501, loss: 0.410, 48032/60000 datapoints
2025-03-06 18:56:51,078 - INFO - training batch 1551, loss: 0.460, 49632/60000 datapoints
2025-03-06 18:56:51,270 - INFO - training batch 1601, loss: 0.445, 51232/60000 datapoints
2025-03-06 18:56:51,460 - INFO - training batch 1651, loss: 0.227, 52832/60000 datapoints
2025-03-06 18:56:51,657 - INFO - training batch 1701, loss: 0.458, 54432/60000 datapoints
2025-03-06 18:56:51,849 - INFO - training batch 1751, loss: 0.542, 56032/60000 datapoints
2025-03-06 18:56:52,042 - INFO - training batch 1801, loss: 0.420, 57632/60000 datapoints
2025-03-06 18:56:52,235 - INFO - training batch 1851, loss: 0.416, 59232/60000 datapoints
2025-03-06 18:56:52,332 - INFO - validation batch 1, loss: 0.626, 32/10016 datapoints
2025-03-06 18:56:52,481 - INFO - validation batch 51, loss: 0.222, 1632/10016 datapoints
2025-03-06 18:56:52,634 - INFO - validation batch 101, loss: 0.510, 3232/10016 datapoints
2025-03-06 18:56:52,783 - INFO - validation batch 151, loss: 0.351, 4832/10016 datapoints
2025-03-06 18:56:52,933 - INFO - validation batch 201, loss: 0.315, 6432/10016 datapoints
2025-03-06 18:56:53,087 - INFO - validation batch 251, loss: 0.539, 8032/10016 datapoints
2025-03-06 18:56:53,237 - INFO - validation batch 301, loss: 0.509, 9632/10016 datapoints
2025-03-06 18:56:53,272 - INFO - Epoch 135/800 done.
2025-03-06 18:56:53,273 - INFO - Final validation performance:
Loss: 0.439, top-1 acc: 0.890top-5 acc: 0.890
2025-03-06 18:56:53,273 - INFO - Beginning epoch 136/800
2025-03-06 18:56:53,280 - INFO - training batch 1, loss: 0.346, 32/60000 datapoints
2025-03-06 18:56:53,473 - INFO - training batch 51, loss: 0.445, 1632/60000 datapoints
2025-03-06 18:56:53,682 - INFO - training batch 101, loss: 0.637, 3232/60000 datapoints
2025-03-06 18:56:53,876 - INFO - training batch 151, loss: 0.715, 4832/60000 datapoints
2025-03-06 18:56:54,071 - INFO - training batch 201, loss: 0.625, 6432/60000 datapoints
2025-03-06 18:56:54,268 - INFO - training batch 251, loss: 0.366, 8032/60000 datapoints
2025-03-06 18:56:54,465 - INFO - training batch 301, loss: 0.491, 9632/60000 datapoints
2025-03-06 18:56:54,662 - INFO - training batch 351, loss: 0.406, 11232/60000 datapoints
2025-03-06 18:56:54,858 - INFO - training batch 401, loss: 0.262, 12832/60000 datapoints
2025-03-06 18:56:55,061 - INFO - training batch 451, loss: 0.219, 14432/60000 datapoints
2025-03-06 18:56:55,259 - INFO - training batch 501, loss: 0.328, 16032/60000 datapoints
2025-03-06 18:56:55,454 - INFO - training batch 551, loss: 0.660, 17632/60000 datapoints
2025-03-06 18:56:55,651 - INFO - training batch 601, loss: 0.413, 19232/60000 datapoints
2025-03-06 18:56:55,846 - INFO - training batch 651, loss: 0.381, 20832/60000 datapoints
2025-03-06 18:56:56,040 - INFO - training batch 701, loss: 0.390, 22432/60000 datapoints
2025-03-06 18:56:56,236 - INFO - training batch 751, loss: 0.351, 24032/60000 datapoints
2025-03-06 18:56:56,431 - INFO - training batch 801, loss: 0.415, 25632/60000 datapoints
2025-03-06 18:56:56,628 - INFO - training batch 851, loss: 0.385, 27232/60000 datapoints
2025-03-06 18:56:56,843 - INFO - training batch 901, loss: 0.448, 28832/60000 datapoints
2025-03-06 18:56:57,038 - INFO - training batch 951, loss: 0.406, 30432/60000 datapoints
2025-03-06 18:56:57,239 - INFO - training batch 1001, loss: 0.516, 32032/60000 datapoints
2025-03-06 18:56:57,432 - INFO - training batch 1051, loss: 0.515, 33632/60000 datapoints
2025-03-06 18:56:57,629 - INFO - training batch 1101, loss: 0.719, 35232/60000 datapoints
2025-03-06 18:56:57,825 - INFO - training batch 1151, loss: 0.458, 36832/60000 datapoints
2025-03-06 18:56:58,020 - INFO - training batch 1201, loss: 0.328, 38432/60000 datapoints
2025-03-06 18:56:58,215 - INFO - training batch 1251, loss: 0.378, 40032/60000 datapoints
2025-03-06 18:56:58,410 - INFO - training batch 1301, loss: 0.367, 41632/60000 datapoints
2025-03-06 18:56:58,608 - INFO - training batch 1351, loss: 0.605, 43232/60000 datapoints
2025-03-06 18:56:58,806 - INFO - training batch 1401, loss: 0.650, 44832/60000 datapoints
2025-03-06 18:56:59,002 - INFO - training batch 1451, loss: 0.541, 46432/60000 datapoints
2025-03-06 18:56:59,197 - INFO - training batch 1501, loss: 0.262, 48032/60000 datapoints
2025-03-06 18:56:59,391 - INFO - training batch 1551, loss: 0.389, 49632/60000 datapoints
2025-03-06 18:56:59,587 - INFO - training batch 1601, loss: 0.573, 51232/60000 datapoints
2025-03-06 18:56:59,782 - INFO - training batch 1651, loss: 0.356, 52832/60000 datapoints
2025-03-06 18:56:59,981 - INFO - training batch 1701, loss: 0.565, 54432/60000 datapoints
2025-03-06 18:57:00,177 - INFO - training batch 1751, loss: 0.405, 56032/60000 datapoints
2025-03-06 18:57:00,370 - INFO - training batch 1801, loss: 0.338, 57632/60000 datapoints
2025-03-06 18:57:00,563 - INFO - training batch 1851, loss: 0.441, 59232/60000 datapoints
2025-03-06 18:57:00,668 - INFO - validation batch 1, loss: 0.356, 32/10016 datapoints
2025-03-06 18:57:00,822 - INFO - validation batch 51, loss: 0.308, 1632/10016 datapoints
2025-03-06 18:57:00,983 - INFO - validation batch 101, loss: 0.406, 3232/10016 datapoints
2025-03-06 18:57:01,141 - INFO - validation batch 151, loss: 0.516, 4832/10016 datapoints
2025-03-06 18:57:01,292 - INFO - validation batch 201, loss: 0.312, 6432/10016 datapoints
2025-03-06 18:57:01,444 - INFO - validation batch 251, loss: 0.313, 8032/10016 datapoints
2025-03-06 18:57:01,597 - INFO - validation batch 301, loss: 0.518, 9632/10016 datapoints
2025-03-06 18:57:01,638 - INFO - Epoch 136/800 done.
2025-03-06 18:57:01,656 - INFO - Final validation performance:
Loss: 0.390, top-1 acc: 0.891top-5 acc: 0.891
2025-03-06 18:57:01,657 - INFO - Beginning epoch 137/800
2025-03-06 18:57:01,687 - INFO - training batch 1, loss: 0.494, 32/60000 datapoints
2025-03-06 18:57:01,920 - INFO - training batch 51, loss: 0.550, 1632/60000 datapoints
2025-03-06 18:57:02,138 - INFO - training batch 101, loss: 0.336, 3232/60000 datapoints
2025-03-06 18:57:02,332 - INFO - training batch 151, loss: 0.418, 4832/60000 datapoints
2025-03-06 18:57:02,533 - INFO - training batch 201, loss: 0.498, 6432/60000 datapoints
2025-03-06 18:57:02,730 - INFO - training batch 251, loss: 0.541, 8032/60000 datapoints
2025-03-06 18:57:02,921 - INFO - training batch 301, loss: 0.554, 9632/60000 datapoints
2025-03-06 18:57:03,112 - INFO - training batch 351, loss: 0.244, 11232/60000 datapoints
2025-03-06 18:57:03,302 - INFO - training batch 401, loss: 0.372, 12832/60000 datapoints
2025-03-06 18:57:03,491 - INFO - training batch 451, loss: 0.410, 14432/60000 datapoints
2025-03-06 18:57:03,684 - INFO - training batch 501, loss: 0.653, 16032/60000 datapoints
2025-03-06 18:57:03,873 - INFO - training batch 551, loss: 0.265, 17632/60000 datapoints
2025-03-06 18:57:04,071 - INFO - training batch 601, loss: 0.588, 19232/60000 datapoints
2025-03-06 18:57:04,273 - INFO - training batch 651, loss: 0.614, 20832/60000 datapoints
2025-03-06 18:57:04,460 - INFO - training batch 701, loss: 0.426, 22432/60000 datapoints
2025-03-06 18:57:04,651 - INFO - training batch 751, loss: 0.492, 24032/60000 datapoints
2025-03-06 18:57:04,837 - INFO - training batch 801, loss: 0.457, 25632/60000 datapoints
2025-03-06 18:57:05,034 - INFO - training batch 851, loss: 0.432, 27232/60000 datapoints
2025-03-06 18:57:05,223 - INFO - training batch 901, loss: 0.368, 28832/60000 datapoints
2025-03-06 18:57:05,410 - INFO - training batch 951, loss: 0.447, 30432/60000 datapoints
2025-03-06 18:57:05,598 - INFO - training batch 1001, loss: 0.258, 32032/60000 datapoints
2025-03-06 18:57:05,789 - INFO - training batch 1051, loss: 0.697, 33632/60000 datapoints
2025-03-06 18:57:05,980 - INFO - training batch 1101, loss: 0.423, 35232/60000 datapoints
2025-03-06 18:57:06,170 - INFO - training batch 1151, loss: 0.281, 36832/60000 datapoints
2025-03-06 18:57:06,362 - INFO - training batch 1201, loss: 0.564, 38432/60000 datapoints
2025-03-06 18:57:06,549 - INFO - training batch 1251, loss: 0.644, 40032/60000 datapoints
2025-03-06 18:57:06,751 - INFO - training batch 1301, loss: 0.300, 41632/60000 datapoints
2025-03-06 18:57:06,957 - INFO - training batch 1351, loss: 0.461, 43232/60000 datapoints
2025-03-06 18:57:07,153 - INFO - training batch 1401, loss: 0.629, 44832/60000 datapoints
2025-03-06 18:57:07,349 - INFO - training batch 1451, loss: 0.382, 46432/60000 datapoints
2025-03-06 18:57:07,539 - INFO - training batch 1501, loss: 0.527, 48032/60000 datapoints
2025-03-06 18:57:07,737 - INFO - training batch 1551, loss: 0.331, 49632/60000 datapoints
2025-03-06 18:57:07,937 - INFO - training batch 1601, loss: 0.578, 51232/60000 datapoints
2025-03-06 18:57:08,125 - INFO - training batch 1651, loss: 0.332, 52832/60000 datapoints
2025-03-06 18:57:08,317 - INFO - training batch 1701, loss: 0.293, 54432/60000 datapoints
2025-03-06 18:57:08,507 - INFO - training batch 1751, loss: 0.433, 56032/60000 datapoints
2025-03-06 18:57:08,698 - INFO - training batch 1801, loss: 0.317, 57632/60000 datapoints
2025-03-06 18:57:08,890 - INFO - training batch 1851, loss: 0.436, 59232/60000 datapoints
2025-03-06 18:57:08,986 - INFO - validation batch 1, loss: 0.497, 32/10016 datapoints
2025-03-06 18:57:09,141 - INFO - validation batch 51, loss: 0.522, 1632/10016 datapoints
2025-03-06 18:57:09,292 - INFO - validation batch 101, loss: 0.319, 3232/10016 datapoints
2025-03-06 18:57:09,441 - INFO - validation batch 151, loss: 0.379, 4832/10016 datapoints
2025-03-06 18:57:09,591 - INFO - validation batch 201, loss: 0.282, 6432/10016 datapoints
2025-03-06 18:57:09,745 - INFO - validation batch 251, loss: 0.382, 8032/10016 datapoints
2025-03-06 18:57:09,894 - INFO - validation batch 301, loss: 0.384, 9632/10016 datapoints
2025-03-06 18:57:09,930 - INFO - Epoch 137/800 done.
2025-03-06 18:57:09,930 - INFO - Final validation performance:
Loss: 0.395, top-1 acc: 0.891top-5 acc: 0.891
2025-03-06 18:57:09,931 - INFO - Beginning epoch 138/800
2025-03-06 18:57:09,937 - INFO - training batch 1, loss: 0.449, 32/60000 datapoints
2025-03-06 18:57:10,127 - INFO - training batch 51, loss: 0.322, 1632/60000 datapoints
2025-03-06 18:57:10,336 - INFO - training batch 101, loss: 0.324, 3232/60000 datapoints
2025-03-06 18:57:10,526 - INFO - training batch 151, loss: 0.367, 4832/60000 datapoints
2025-03-06 18:57:10,726 - INFO - training batch 201, loss: 0.669, 6432/60000 datapoints
2025-03-06 18:57:10,918 - INFO - training batch 251, loss: 0.599, 8032/60000 datapoints
2025-03-06 18:57:11,114 - INFO - training batch 301, loss: 0.453, 9632/60000 datapoints
2025-03-06 18:57:11,308 - INFO - training batch 351, loss: 0.283, 11232/60000 datapoints
2025-03-06 18:57:11,497 - INFO - training batch 401, loss: 0.703, 12832/60000 datapoints
2025-03-06 18:57:11,688 - INFO - training batch 451, loss: 0.351, 14432/60000 datapoints
2025-03-06 18:57:11,878 - INFO - training batch 501, loss: 0.353, 16032/60000 datapoints
2025-03-06 18:57:12,066 - INFO - training batch 551, loss: 0.455, 17632/60000 datapoints
2025-03-06 18:57:12,254 - INFO - training batch 601, loss: 0.476, 19232/60000 datapoints
2025-03-06 18:57:12,444 - INFO - training batch 651, loss: 0.294, 20832/60000 datapoints
2025-03-06 18:57:12,635 - INFO - training batch 701, loss: 0.400, 22432/60000 datapoints
2025-03-06 18:57:12,829 - INFO - training batch 751, loss: 0.744, 24032/60000 datapoints
2025-03-06 18:57:13,018 - INFO - training batch 801, loss: 0.456, 25632/60000 datapoints
2025-03-06 18:57:13,209 - INFO - training batch 851, loss: 0.315, 27232/60000 datapoints
2025-03-06 18:57:13,399 - INFO - training batch 901, loss: 0.447, 28832/60000 datapoints
2025-03-06 18:57:13,590 - INFO - training batch 951, loss: 0.604, 30432/60000 datapoints
2025-03-06 18:57:13,782 - INFO - training batch 1001, loss: 0.287, 32032/60000 datapoints
2025-03-06 18:57:13,973 - INFO - training batch 1051, loss: 0.208, 33632/60000 datapoints
2025-03-06 18:57:14,161 - INFO - training batch 1101, loss: 0.311, 35232/60000 datapoints
2025-03-06 18:57:14,350 - INFO - training batch 1151, loss: 0.263, 36832/60000 datapoints
2025-03-06 18:57:14,538 - INFO - training batch 1201, loss: 0.647, 38432/60000 datapoints
2025-03-06 18:57:14,729 - INFO - training batch 1251, loss: 0.360, 40032/60000 datapoints
2025-03-06 18:57:14,926 - INFO - training batch 1301, loss: 0.354, 41632/60000 datapoints
2025-03-06 18:57:15,118 - INFO - training batch 1351, loss: 0.410, 43232/60000 datapoints
2025-03-06 18:57:15,326 - INFO - training batch 1401, loss: 0.294, 44832/60000 datapoints
2025-03-06 18:57:15,527 - INFO - training batch 1451, loss: 0.295, 46432/60000 datapoints
2025-03-06 18:57:15,726 - INFO - training batch 1501, loss: 0.488, 48032/60000 datapoints
2025-03-06 18:57:15,921 - INFO - training batch 1551, loss: 0.300, 49632/60000 datapoints
2025-03-06 18:57:16,117 - INFO - training batch 1601, loss: 0.372, 51232/60000 datapoints
2025-03-06 18:57:16,320 - INFO - training batch 1651, loss: 0.478, 52832/60000 datapoints
2025-03-06 18:57:16,514 - INFO - training batch 1701, loss: 0.363, 54432/60000 datapoints
2025-03-06 18:57:16,721 - INFO - training batch 1751, loss: 0.531, 56032/60000 datapoints
2025-03-06 18:57:16,946 - INFO - training batch 1801, loss: 0.462, 57632/60000 datapoints
2025-03-06 18:57:17,146 - INFO - training batch 1851, loss: 0.446, 59232/60000 datapoints
2025-03-06 18:57:17,254 - INFO - validation batch 1, loss: 0.503, 32/10016 datapoints
2025-03-06 18:57:17,415 - INFO - validation batch 51, loss: 0.442, 1632/10016 datapoints
2025-03-06 18:57:17,572 - INFO - validation batch 101, loss: 0.179, 3232/10016 datapoints
2025-03-06 18:57:17,730 - INFO - validation batch 151, loss: 0.255, 4832/10016 datapoints
2025-03-06 18:57:17,919 - INFO - validation batch 201, loss: 0.338, 6432/10016 datapoints
2025-03-06 18:57:18,075 - INFO - validation batch 251, loss: 0.233, 8032/10016 datapoints
2025-03-06 18:57:18,230 - INFO - validation batch 301, loss: 0.383, 9632/10016 datapoints
2025-03-06 18:57:18,271 - INFO - Epoch 138/800 done.
2025-03-06 18:57:18,271 - INFO - Final validation performance:
Loss: 0.333, top-1 acc: 0.891top-5 acc: 0.891
2025-03-06 18:57:18,272 - INFO - Beginning epoch 139/800
2025-03-06 18:57:18,278 - INFO - training batch 1, loss: 0.378, 32/60000 datapoints
2025-03-06 18:57:18,491 - INFO - training batch 51, loss: 0.717, 1632/60000 datapoints
2025-03-06 18:57:18,708 - INFO - training batch 101, loss: 0.479, 3232/60000 datapoints
2025-03-06 18:57:18,944 - INFO - training batch 151, loss: 0.863, 4832/60000 datapoints
2025-03-06 18:57:19,143 - INFO - training batch 201, loss: 0.324, 6432/60000 datapoints
2025-03-06 18:57:19,343 - INFO - training batch 251, loss: 0.331, 8032/60000 datapoints
2025-03-06 18:57:19,543 - INFO - training batch 301, loss: 0.655, 9632/60000 datapoints
2025-03-06 18:57:19,744 - INFO - training batch 351, loss: 0.387, 11232/60000 datapoints
2025-03-06 18:57:19,939 - INFO - training batch 401, loss: 0.537, 12832/60000 datapoints
2025-03-06 18:57:20,134 - INFO - training batch 451, loss: 0.425, 14432/60000 datapoints
2025-03-06 18:57:20,327 - INFO - training batch 501, loss: 0.334, 16032/60000 datapoints
2025-03-06 18:57:20,521 - INFO - training batch 551, loss: 0.537, 17632/60000 datapoints
2025-03-06 18:57:20,717 - INFO - training batch 601, loss: 0.253, 19232/60000 datapoints
2025-03-06 18:57:20,911 - INFO - training batch 651, loss: 0.317, 20832/60000 datapoints
2025-03-06 18:57:21,109 - INFO - training batch 701, loss: 0.481, 22432/60000 datapoints
2025-03-06 18:57:21,303 - INFO - training batch 751, loss: 0.361, 24032/60000 datapoints
2025-03-06 18:57:21,499 - INFO - training batch 801, loss: 0.429, 25632/60000 datapoints
2025-03-06 18:57:21,694 - INFO - training batch 851, loss: 0.270, 27232/60000 datapoints
2025-03-06 18:57:21,890 - INFO - training batch 901, loss: 0.457, 28832/60000 datapoints
2025-03-06 18:57:22,083 - INFO - training batch 951, loss: 0.241, 30432/60000 datapoints
2025-03-06 18:57:22,276 - INFO - training batch 1001, loss: 0.450, 32032/60000 datapoints
2025-03-06 18:57:22,477 - INFO - training batch 1051, loss: 0.409, 33632/60000 datapoints
2025-03-06 18:57:22,674 - INFO - training batch 1101, loss: 0.672, 35232/60000 datapoints
2025-03-06 18:57:22,869 - INFO - training batch 1151, loss: 0.296, 36832/60000 datapoints
2025-03-06 18:57:23,063 - INFO - training batch 1201, loss: 0.438, 38432/60000 datapoints
2025-03-06 18:57:23,259 - INFO - training batch 1251, loss: 0.494, 40032/60000 datapoints
2025-03-06 18:57:23,454 - INFO - training batch 1301, loss: 0.527, 41632/60000 datapoints
2025-03-06 18:57:23,649 - INFO - training batch 1351, loss: 0.389, 43232/60000 datapoints
2025-03-06 18:57:23,846 - INFO - training batch 1401, loss: 0.372, 44832/60000 datapoints
2025-03-06 18:57:24,040 - INFO - training batch 1451, loss: 0.719, 46432/60000 datapoints
2025-03-06 18:57:24,235 - INFO - training batch 1501, loss: 0.470, 48032/60000 datapoints
2025-03-06 18:57:24,431 - INFO - training batch 1551, loss: 0.560, 49632/60000 datapoints
2025-03-06 18:57:24,625 - INFO - training batch 1601, loss: 0.371, 51232/60000 datapoints
2025-03-06 18:57:24,819 - INFO - training batch 1651, loss: 0.306, 52832/60000 datapoints
2025-03-06 18:57:25,019 - INFO - training batch 1701, loss: 0.284, 54432/60000 datapoints
2025-03-06 18:57:25,215 - INFO - training batch 1751, loss: 0.545, 56032/60000 datapoints
2025-03-06 18:57:25,409 - INFO - training batch 1801, loss: 0.489, 57632/60000 datapoints
2025-03-06 18:57:25,602 - INFO - training batch 1851, loss: 0.277, 59232/60000 datapoints
2025-03-06 18:57:25,707 - INFO - validation batch 1, loss: 0.418, 32/10016 datapoints
2025-03-06 18:57:25,860 - INFO - validation batch 51, loss: 0.303, 1632/10016 datapoints
2025-03-06 18:57:26,014 - INFO - validation batch 101, loss: 0.652, 3232/10016 datapoints
2025-03-06 18:57:26,170 - INFO - validation batch 151, loss: 0.532, 4832/10016 datapoints
2025-03-06 18:57:26,323 - INFO - validation batch 201, loss: 0.256, 6432/10016 datapoints
2025-03-06 18:57:26,477 - INFO - validation batch 251, loss: 0.291, 8032/10016 datapoints
2025-03-06 18:57:26,633 - INFO - validation batch 301, loss: 0.385, 9632/10016 datapoints
2025-03-06 18:57:26,671 - INFO - Epoch 139/800 done.
2025-03-06 18:57:26,671 - INFO - Final validation performance:
Loss: 0.405, top-1 acc: 0.891top-5 acc: 0.891
2025-03-06 18:57:26,671 - INFO - Beginning epoch 140/800
2025-03-06 18:57:26,678 - INFO - training batch 1, loss: 0.424, 32/60000 datapoints
2025-03-06 18:57:26,892 - INFO - training batch 51, loss: 0.392, 1632/60000 datapoints
2025-03-06 18:57:27,146 - INFO - training batch 101, loss: 0.414, 3232/60000 datapoints
2025-03-06 18:57:27,371 - INFO - training batch 151, loss: 0.405, 4832/60000 datapoints
2025-03-06 18:57:27,567 - INFO - training batch 201, loss: 0.285, 6432/60000 datapoints
2025-03-06 18:57:27,767 - INFO - training batch 251, loss: 0.384, 8032/60000 datapoints
2025-03-06 18:57:27,971 - INFO - training batch 301, loss: 0.484, 9632/60000 datapoints
2025-03-06 18:57:28,195 - INFO - training batch 351, loss: 0.638, 11232/60000 datapoints
2025-03-06 18:57:28,403 - INFO - training batch 401, loss: 0.630, 12832/60000 datapoints
2025-03-06 18:57:28,596 - INFO - training batch 451, loss: 0.360, 14432/60000 datapoints
2025-03-06 18:57:28,797 - INFO - training batch 501, loss: 0.305, 16032/60000 datapoints
2025-03-06 18:57:28,992 - INFO - training batch 551, loss: 0.458, 17632/60000 datapoints
2025-03-06 18:57:29,205 - INFO - training batch 601, loss: 0.552, 19232/60000 datapoints
2025-03-06 18:57:29,399 - INFO - training batch 651, loss: 0.584, 20832/60000 datapoints
2025-03-06 18:57:29,598 - INFO - training batch 701, loss: 0.556, 22432/60000 datapoints
2025-03-06 18:57:29,798 - INFO - training batch 751, loss: 0.504, 24032/60000 datapoints
2025-03-06 18:57:29,992 - INFO - training batch 801, loss: 0.319, 25632/60000 datapoints
2025-03-06 18:57:30,186 - INFO - training batch 851, loss: 0.158, 27232/60000 datapoints
2025-03-06 18:57:30,380 - INFO - training batch 901, loss: 0.402, 28832/60000 datapoints
2025-03-06 18:57:30,573 - INFO - training batch 951, loss: 0.331, 30432/60000 datapoints
2025-03-06 18:57:30,768 - INFO - training batch 1001, loss: 0.389, 32032/60000 datapoints
2025-03-06 18:57:30,963 - INFO - training batch 1051, loss: 0.343, 33632/60000 datapoints
2025-03-06 18:57:31,160 - INFO - training batch 1101, loss: 0.275, 35232/60000 datapoints
2025-03-06 18:57:31,355 - INFO - training batch 1151, loss: 0.287, 36832/60000 datapoints
2025-03-06 18:57:31,557 - INFO - training batch 1201, loss: 0.444, 38432/60000 datapoints
2025-03-06 18:57:31,765 - INFO - training batch 1251, loss: 0.442, 40032/60000 datapoints
2025-03-06 18:57:31,959 - INFO - training batch 1301, loss: 0.374, 41632/60000 datapoints
2025-03-06 18:57:32,156 - INFO - training batch 1351, loss: 0.376, 43232/60000 datapoints
2025-03-06 18:57:32,349 - INFO - training batch 1401, loss: 0.258, 44832/60000 datapoints
2025-03-06 18:57:32,545 - INFO - training batch 1451, loss: 0.569, 46432/60000 datapoints
2025-03-06 18:57:32,745 - INFO - training batch 1501, loss: 0.360, 48032/60000 datapoints
2025-03-06 18:57:32,945 - INFO - training batch 1551, loss: 0.500, 49632/60000 datapoints
2025-03-06 18:57:33,143 - INFO - training batch 1601, loss: 0.480, 51232/60000 datapoints
2025-03-06 18:57:33,339 - INFO - training batch 1651, loss: 0.630, 52832/60000 datapoints
2025-03-06 18:57:33,534 - INFO - training batch 1701, loss: 0.307, 54432/60000 datapoints
2025-03-06 18:57:33,729 - INFO - training batch 1751, loss: 0.557, 56032/60000 datapoints
2025-03-06 18:57:33,924 - INFO - training batch 1801, loss: 0.687, 57632/60000 datapoints
2025-03-06 18:57:34,119 - INFO - training batch 1851, loss: 0.347, 59232/60000 datapoints
2025-03-06 18:57:34,223 - INFO - validation batch 1, loss: 0.382, 32/10016 datapoints
2025-03-06 18:57:34,374 - INFO - validation batch 51, loss: 0.317, 1632/10016 datapoints
2025-03-06 18:57:34,526 - INFO - validation batch 101, loss: 0.588, 3232/10016 datapoints
2025-03-06 18:57:34,681 - INFO - validation batch 151, loss: 0.550, 4832/10016 datapoints
2025-03-06 18:57:34,833 - INFO - validation batch 201, loss: 0.505, 6432/10016 datapoints
2025-03-06 18:57:34,994 - INFO - validation batch 251, loss: 0.375, 8032/10016 datapoints
2025-03-06 18:57:35,150 - INFO - validation batch 301, loss: 0.479, 9632/10016 datapoints
2025-03-06 18:57:35,185 - INFO - Epoch 140/800 done.
2025-03-06 18:57:35,185 - INFO - Final validation performance:
Loss: 0.457, top-1 acc: 0.891top-5 acc: 0.891
2025-03-06 18:57:35,186 - INFO - Beginning epoch 141/800
2025-03-06 18:57:35,193 - INFO - training batch 1, loss: 0.248, 32/60000 datapoints
2025-03-06 18:57:35,406 - INFO - training batch 51, loss: 0.319, 1632/60000 datapoints
2025-03-06 18:57:35,600 - INFO - training batch 101, loss: 0.545, 3232/60000 datapoints
2025-03-06 18:57:35,802 - INFO - training batch 151, loss: 0.274, 4832/60000 datapoints
2025-03-06 18:57:36,003 - INFO - training batch 201, loss: 0.547, 6432/60000 datapoints
2025-03-06 18:57:36,199 - INFO - training batch 251, loss: 0.541, 8032/60000 datapoints
2025-03-06 18:57:36,394 - INFO - training batch 301, loss: 0.357, 9632/60000 datapoints
2025-03-06 18:57:36,593 - INFO - training batch 351, loss: 0.571, 11232/60000 datapoints
2025-03-06 18:57:36,790 - INFO - training batch 401, loss: 0.370, 12832/60000 datapoints
2025-03-06 18:57:36,984 - INFO - training batch 451, loss: 0.521, 14432/60000 datapoints
2025-03-06 18:57:37,203 - INFO - training batch 501, loss: 0.390, 16032/60000 datapoints
2025-03-06 18:57:37,403 - INFO - training batch 551, loss: 0.621, 17632/60000 datapoints
2025-03-06 18:57:37,600 - INFO - training batch 601, loss: 0.305, 19232/60000 datapoints
2025-03-06 18:57:37,817 - INFO - training batch 651, loss: 0.590, 20832/60000 datapoints
2025-03-06 18:57:38,013 - INFO - training batch 701, loss: 0.242, 22432/60000 datapoints
2025-03-06 18:57:38,208 - INFO - training batch 751, loss: 0.516, 24032/60000 datapoints
2025-03-06 18:57:38,401 - INFO - training batch 801, loss: 0.480, 25632/60000 datapoints
2025-03-06 18:57:38,593 - INFO - training batch 851, loss: 0.456, 27232/60000 datapoints
2025-03-06 18:57:38,791 - INFO - training batch 901, loss: 0.548, 28832/60000 datapoints
2025-03-06 18:57:38,984 - INFO - training batch 951, loss: 0.415, 30432/60000 datapoints
2025-03-06 18:57:39,183 - INFO - training batch 1001, loss: 0.556, 32032/60000 datapoints
2025-03-06 18:57:39,380 - INFO - training batch 1051, loss: 0.435, 33632/60000 datapoints
2025-03-06 18:57:39,574 - INFO - training batch 1101, loss: 0.363, 35232/60000 datapoints
2025-03-06 18:57:39,769 - INFO - training batch 1151, loss: 0.381, 36832/60000 datapoints
2025-03-06 18:57:39,965 - INFO - training batch 1201, loss: 0.567, 38432/60000 datapoints
2025-03-06 18:57:40,161 - INFO - training batch 1251, loss: 0.357, 40032/60000 datapoints
2025-03-06 18:57:40,354 - INFO - training batch 1301, loss: 0.600, 41632/60000 datapoints
2025-03-06 18:57:40,552 - INFO - training batch 1351, loss: 0.309, 43232/60000 datapoints
2025-03-06 18:57:40,749 - INFO - training batch 1401, loss: 0.508, 44832/60000 datapoints
2025-03-06 18:57:40,942 - INFO - training batch 1451, loss: 0.321, 46432/60000 datapoints
2025-03-06 18:57:41,137 - INFO - training batch 1501, loss: 0.643, 48032/60000 datapoints
2025-03-06 18:57:41,333 - INFO - training batch 1551, loss: 0.389, 49632/60000 datapoints
2025-03-06 18:57:41,525 - INFO - training batch 1601, loss: 0.269, 51232/60000 datapoints
2025-03-06 18:57:41,718 - INFO - training batch 1651, loss: 0.623, 52832/60000 datapoints
2025-03-06 18:57:41,910 - INFO - training batch 1701, loss: 0.634, 54432/60000 datapoints
2025-03-06 18:57:42,104 - INFO - training batch 1751, loss: 0.570, 56032/60000 datapoints
2025-03-06 18:57:42,299 - INFO - training batch 1801, loss: 0.329, 57632/60000 datapoints
2025-03-06 18:57:42,494 - INFO - training batch 1851, loss: 0.490, 59232/60000 datapoints
2025-03-06 18:57:42,596 - INFO - validation batch 1, loss: 0.525, 32/10016 datapoints
2025-03-06 18:57:42,751 - INFO - validation batch 51, loss: 0.304, 1632/10016 datapoints
2025-03-06 18:57:42,904 - INFO - validation batch 101, loss: 0.519, 3232/10016 datapoints
2025-03-06 18:57:43,058 - INFO - validation batch 151, loss: 0.645, 4832/10016 datapoints
2025-03-06 18:57:43,212 - INFO - validation batch 201, loss: 0.241, 6432/10016 datapoints
2025-03-06 18:57:43,364 - INFO - validation batch 251, loss: 0.622, 8032/10016 datapoints
2025-03-06 18:57:43,516 - INFO - validation batch 301, loss: 0.443, 9632/10016 datapoints
2025-03-06 18:57:43,554 - INFO - Epoch 141/800 done.
2025-03-06 18:57:43,554 - INFO - Final validation performance:
Loss: 0.471, top-1 acc: 0.892top-5 acc: 0.892
2025-03-06 18:57:43,555 - INFO - Beginning epoch 142/800
2025-03-06 18:57:43,561 - INFO - training batch 1, loss: 0.414, 32/60000 datapoints
2025-03-06 18:57:43,771 - INFO - training batch 51, loss: 0.357, 1632/60000 datapoints
2025-03-06 18:57:43,967 - INFO - training batch 101, loss: 0.454, 3232/60000 datapoints
2025-03-06 18:57:44,167 - INFO - training batch 151, loss: 0.420, 4832/60000 datapoints
2025-03-06 18:57:44,366 - INFO - training batch 201, loss: 0.784, 6432/60000 datapoints
2025-03-06 18:57:44,561 - INFO - training batch 251, loss: 0.579, 8032/60000 datapoints
2025-03-06 18:57:44,758 - INFO - training batch 301, loss: 0.529, 9632/60000 datapoints
2025-03-06 18:57:44,957 - INFO - training batch 351, loss: 0.361, 11232/60000 datapoints
2025-03-06 18:57:45,153 - INFO - training batch 401, loss: 0.485, 12832/60000 datapoints
2025-03-06 18:57:45,349 - INFO - training batch 451, loss: 0.366, 14432/60000 datapoints
2025-03-06 18:57:45,541 - INFO - training batch 501, loss: 0.400, 16032/60000 datapoints
2025-03-06 18:57:45,737 - INFO - training batch 551, loss: 0.465, 17632/60000 datapoints
2025-03-06 18:57:45,930 - INFO - training batch 601, loss: 0.407, 19232/60000 datapoints
2025-03-06 18:57:46,123 - INFO - training batch 651, loss: 0.213, 20832/60000 datapoints
2025-03-06 18:57:46,322 - INFO - training batch 701, loss: 0.412, 22432/60000 datapoints
2025-03-06 18:57:46,515 - INFO - training batch 751, loss: 0.339, 24032/60000 datapoints
2025-03-06 18:57:46,714 - INFO - training batch 801, loss: 0.545, 25632/60000 datapoints
2025-03-06 18:57:46,905 - INFO - training batch 851, loss: 0.530, 27232/60000 datapoints
2025-03-06 18:57:47,102 - INFO - training batch 901, loss: 0.448, 28832/60000 datapoints
2025-03-06 18:57:47,312 - INFO - training batch 951, loss: 0.586, 30432/60000 datapoints
2025-03-06 18:57:47,508 - INFO - training batch 1001, loss: 0.749, 32032/60000 datapoints
2025-03-06 18:57:47,703 - INFO - training batch 1051, loss: 0.603, 33632/60000 datapoints
2025-03-06 18:57:47,899 - INFO - training batch 1101, loss: 0.398, 35232/60000 datapoints
2025-03-06 18:57:48,091 - INFO - training batch 1151, loss: 0.394, 36832/60000 datapoints
2025-03-06 18:57:48,286 - INFO - training batch 1201, loss: 0.445, 38432/60000 datapoints
2025-03-06 18:57:48,479 - INFO - training batch 1251, loss: 0.331, 40032/60000 datapoints
2025-03-06 18:57:48,673 - INFO - training batch 1301, loss: 0.429, 41632/60000 datapoints
2025-03-06 18:57:48,868 - INFO - training batch 1351, loss: 0.426, 43232/60000 datapoints
2025-03-06 18:57:49,063 - INFO - training batch 1401, loss: 0.448, 44832/60000 datapoints
2025-03-06 18:57:49,260 - INFO - training batch 1451, loss: 0.327, 46432/60000 datapoints
2025-03-06 18:57:49,450 - INFO - training batch 1501, loss: 0.503, 48032/60000 datapoints
2025-03-06 18:57:49,648 - INFO - training batch 1551, loss: 0.324, 49632/60000 datapoints
2025-03-06 18:57:49,844 - INFO - training batch 1601, loss: 0.384, 51232/60000 datapoints
2025-03-06 18:57:50,039 - INFO - training batch 1651, loss: 0.508, 52832/60000 datapoints
2025-03-06 18:57:50,234 - INFO - training batch 1701, loss: 0.509, 54432/60000 datapoints
2025-03-06 18:57:50,428 - INFO - training batch 1751, loss: 0.451, 56032/60000 datapoints
2025-03-06 18:57:50,625 - INFO - training batch 1801, loss: 0.497, 57632/60000 datapoints
2025-03-06 18:57:50,821 - INFO - training batch 1851, loss: 0.455, 59232/60000 datapoints
2025-03-06 18:57:50,922 - INFO - validation batch 1, loss: 0.240, 32/10016 datapoints
2025-03-06 18:57:51,074 - INFO - validation batch 51, loss: 0.788, 1632/10016 datapoints
2025-03-06 18:57:51,229 - INFO - validation batch 101, loss: 0.264, 3232/10016 datapoints
2025-03-06 18:57:51,382 - INFO - validation batch 151, loss: 0.500, 4832/10016 datapoints
2025-03-06 18:57:51,534 - INFO - validation batch 201, loss: 0.492, 6432/10016 datapoints
2025-03-06 18:57:51,687 - INFO - validation batch 251, loss: 0.400, 8032/10016 datapoints
2025-03-06 18:57:51,840 - INFO - validation batch 301, loss: 0.381, 9632/10016 datapoints
2025-03-06 18:57:51,877 - INFO - Epoch 142/800 done.
2025-03-06 18:57:51,877 - INFO - Final validation performance:
Loss: 0.438, top-1 acc: 0.893top-5 acc: 0.893
2025-03-06 18:57:51,878 - INFO - Beginning epoch 143/800
2025-03-06 18:57:51,884 - INFO - training batch 1, loss: 0.525, 32/60000 datapoints
2025-03-06 18:57:52,083 - INFO - training batch 51, loss: 0.314, 1632/60000 datapoints
2025-03-06 18:57:52,290 - INFO - training batch 101, loss: 0.259, 3232/60000 datapoints
2025-03-06 18:57:52,484 - INFO - training batch 151, loss: 0.305, 4832/60000 datapoints
2025-03-06 18:57:52,681 - INFO - training batch 201, loss: 0.272, 6432/60000 datapoints
2025-03-06 18:57:52,875 - INFO - training batch 251, loss: 0.340, 8032/60000 datapoints
2025-03-06 18:57:53,079 - INFO - training batch 301, loss: 0.372, 9632/60000 datapoints
2025-03-06 18:57:53,280 - INFO - training batch 351, loss: 0.354, 11232/60000 datapoints
2025-03-06 18:57:53,475 - INFO - training batch 401, loss: 0.283, 12832/60000 datapoints
2025-03-06 18:57:53,675 - INFO - training batch 451, loss: 0.483, 14432/60000 datapoints
2025-03-06 18:57:53,867 - INFO - training batch 501, loss: 0.434, 16032/60000 datapoints
2025-03-06 18:57:54,061 - INFO - training batch 551, loss: 0.298, 17632/60000 datapoints
2025-03-06 18:57:54,255 - INFO - training batch 601, loss: 0.455, 19232/60000 datapoints
2025-03-06 18:57:54,447 - INFO - training batch 651, loss: 0.558, 20832/60000 datapoints
2025-03-06 18:57:54,643 - INFO - training batch 701, loss: 0.906, 22432/60000 datapoints
2025-03-06 18:57:54,837 - INFO - training batch 751, loss: 0.499, 24032/60000 datapoints
2025-03-06 18:57:55,042 - INFO - training batch 801, loss: 0.377, 25632/60000 datapoints
2025-03-06 18:57:55,236 - INFO - training batch 851, loss: 0.419, 27232/60000 datapoints
2025-03-06 18:57:55,430 - INFO - training batch 901, loss: 0.355, 28832/60000 datapoints
2025-03-06 18:57:55,628 - INFO - training batch 951, loss: 0.805, 30432/60000 datapoints
2025-03-06 18:57:55,821 - INFO - training batch 1001, loss: 0.648, 32032/60000 datapoints
2025-03-06 18:57:56,015 - INFO - training batch 1051, loss: 0.708, 33632/60000 datapoints
2025-03-06 18:57:56,211 - INFO - training batch 1101, loss: 0.475, 35232/60000 datapoints
2025-03-06 18:57:56,408 - INFO - training batch 1151, loss: 0.465, 36832/60000 datapoints
2025-03-06 18:57:56,600 - INFO - training batch 1201, loss: 0.680, 38432/60000 datapoints
2025-03-06 18:57:56,798 - INFO - training batch 1251, loss: 0.251, 40032/60000 datapoints
2025-03-06 18:57:56,990 - INFO - training batch 1301, loss: 0.737, 41632/60000 datapoints
2025-03-06 18:57:57,184 - INFO - training batch 1351, loss: 0.304, 43232/60000 datapoints
2025-03-06 18:57:57,401 - INFO - training batch 1401, loss: 0.389, 44832/60000 datapoints
2025-03-06 18:57:57,594 - INFO - training batch 1451, loss: 0.255, 46432/60000 datapoints
2025-03-06 18:57:57,791 - INFO - training batch 1501, loss: 0.292, 48032/60000 datapoints
2025-03-06 18:57:57,987 - INFO - training batch 1551, loss: 0.229, 49632/60000 datapoints
2025-03-06 18:57:58,187 - INFO - training batch 1601, loss: 0.341, 51232/60000 datapoints
2025-03-06 18:57:58,391 - INFO - training batch 1651, loss: 0.318, 52832/60000 datapoints
2025-03-06 18:57:58,598 - INFO - training batch 1701, loss: 0.542, 54432/60000 datapoints
2025-03-06 18:57:58,793 - INFO - training batch 1751, loss: 0.281, 56032/60000 datapoints
2025-03-06 18:57:58,987 - INFO - training batch 1801, loss: 0.481, 57632/60000 datapoints
2025-03-06 18:57:59,181 - INFO - training batch 1851, loss: 0.401, 59232/60000 datapoints
2025-03-06 18:57:59,284 - INFO - validation batch 1, loss: 0.303, 32/10016 datapoints
2025-03-06 18:57:59,438 - INFO - validation batch 51, loss: 0.488, 1632/10016 datapoints
2025-03-06 18:57:59,592 - INFO - validation batch 101, loss: 0.595, 3232/10016 datapoints
2025-03-06 18:57:59,746 - INFO - validation batch 151, loss: 0.424, 4832/10016 datapoints
2025-03-06 18:57:59,900 - INFO - validation batch 201, loss: 0.342, 6432/10016 datapoints
2025-03-06 18:58:00,055 - INFO - validation batch 251, loss: 0.607, 8032/10016 datapoints
2025-03-06 18:58:00,208 - INFO - validation batch 301, loss: 0.472, 9632/10016 datapoints
2025-03-06 18:58:00,245 - INFO - Epoch 143/800 done.
2025-03-06 18:58:00,245 - INFO - Final validation performance:
Loss: 0.461, top-1 acc: 0.892top-5 acc: 0.892
2025-03-06 18:58:00,246 - INFO - Beginning epoch 144/800
2025-03-06 18:58:00,252 - INFO - training batch 1, loss: 0.242, 32/60000 datapoints
2025-03-06 18:58:00,455 - INFO - training batch 51, loss: 0.572, 1632/60000 datapoints
2025-03-06 18:58:00,655 - INFO - training batch 101, loss: 0.324, 3232/60000 datapoints
2025-03-06 18:58:00,856 - INFO - training batch 151, loss: 0.623, 4832/60000 datapoints
2025-03-06 18:58:01,051 - INFO - training batch 201, loss: 0.295, 6432/60000 datapoints
2025-03-06 18:58:01,250 - INFO - training batch 251, loss: 0.284, 8032/60000 datapoints
2025-03-06 18:58:01,444 - INFO - training batch 301, loss: 0.353, 9632/60000 datapoints
2025-03-06 18:58:01,639 - INFO - training batch 351, loss: 0.640, 11232/60000 datapoints
2025-03-06 18:58:01,831 - INFO - training batch 401, loss: 0.440, 12832/60000 datapoints
2025-03-06 18:58:02,024 - INFO - training batch 451, loss: 0.268, 14432/60000 datapoints
2025-03-06 18:58:02,215 - INFO - training batch 501, loss: 0.401, 16032/60000 datapoints
2025-03-06 18:58:02,407 - INFO - training batch 551, loss: 0.405, 17632/60000 datapoints
2025-03-06 18:58:02,599 - INFO - training batch 601, loss: 0.724, 19232/60000 datapoints
2025-03-06 18:58:02,793 - INFO - training batch 651, loss: 0.298, 20832/60000 datapoints
2025-03-06 18:58:02,986 - INFO - training batch 701, loss: 0.373, 22432/60000 datapoints
2025-03-06 18:58:03,179 - INFO - training batch 751, loss: 0.511, 24032/60000 datapoints
2025-03-06 18:58:03,375 - INFO - training batch 801, loss: 0.386, 25632/60000 datapoints
2025-03-06 18:58:03,566 - INFO - training batch 851, loss: 0.531, 27232/60000 datapoints
2025-03-06 18:58:03,759 - INFO - training batch 901, loss: 0.662, 28832/60000 datapoints
2025-03-06 18:58:03,950 - INFO - training batch 951, loss: 0.632, 30432/60000 datapoints
2025-03-06 18:58:04,142 - INFO - training batch 1001, loss: 0.316, 32032/60000 datapoints
2025-03-06 18:58:04,332 - INFO - training batch 1051, loss: 0.172, 33632/60000 datapoints
2025-03-06 18:58:04,522 - INFO - training batch 1101, loss: 0.395, 35232/60000 datapoints
2025-03-06 18:58:04,714 - INFO - training batch 1151, loss: 0.511, 36832/60000 datapoints
2025-03-06 18:58:04,910 - INFO - training batch 1201, loss: 0.327, 38432/60000 datapoints
2025-03-06 18:58:05,099 - INFO - training batch 1251, loss: 0.295, 40032/60000 datapoints
2025-03-06 18:58:05,292 - INFO - training batch 1301, loss: 0.219, 41632/60000 datapoints
2025-03-06 18:58:05,482 - INFO - training batch 1351, loss: 0.436, 43232/60000 datapoints
2025-03-06 18:58:05,677 - INFO - training batch 1401, loss: 0.560, 44832/60000 datapoints
2025-03-06 18:58:05,868 - INFO - training batch 1451, loss: 0.393, 46432/60000 datapoints
2025-03-06 18:58:06,059 - INFO - training batch 1501, loss: 0.569, 48032/60000 datapoints
2025-03-06 18:58:06,252 - INFO - training batch 1551, loss: 0.318, 49632/60000 datapoints
2025-03-06 18:58:06,443 - INFO - training batch 1601, loss: 0.542, 51232/60000 datapoints
2025-03-06 18:58:06,634 - INFO - training batch 1651, loss: 0.512, 52832/60000 datapoints
2025-03-06 18:58:06,824 - INFO - training batch 1701, loss: 0.420, 54432/60000 datapoints
2025-03-06 18:58:07,016 - INFO - training batch 1751, loss: 0.456, 56032/60000 datapoints
2025-03-06 18:58:07,206 - INFO - training batch 1801, loss: 0.539, 57632/60000 datapoints
2025-03-06 18:58:07,423 - INFO - training batch 1851, loss: 0.392, 59232/60000 datapoints
2025-03-06 18:58:07,521 - INFO - validation batch 1, loss: 0.265, 32/10016 datapoints
2025-03-06 18:58:07,676 - INFO - validation batch 51, loss: 0.797, 1632/10016 datapoints
2025-03-06 18:58:07,827 - INFO - validation batch 101, loss: 0.469, 3232/10016 datapoints
2025-03-06 18:58:07,976 - INFO - validation batch 151, loss: 0.328, 4832/10016 datapoints
2025-03-06 18:58:08,135 - INFO - validation batch 201, loss: 0.676, 6432/10016 datapoints
2025-03-06 18:58:08,289 - INFO - validation batch 251, loss: 0.354, 8032/10016 datapoints
2025-03-06 18:58:08,441 - INFO - validation batch 301, loss: 0.290, 9632/10016 datapoints
2025-03-06 18:58:08,476 - INFO - Epoch 144/800 done.
2025-03-06 18:58:08,476 - INFO - Final validation performance:
Loss: 0.454, top-1 acc: 0.893top-5 acc: 0.893
2025-03-06 18:58:08,477 - INFO - Beginning epoch 145/800
2025-03-06 18:58:08,483 - INFO - training batch 1, loss: 0.378, 32/60000 datapoints
2025-03-06 18:58:08,687 - INFO - training batch 51, loss: 0.436, 1632/60000 datapoints
2025-03-06 18:58:08,881 - INFO - training batch 101, loss: 0.545, 3232/60000 datapoints
2025-03-06 18:58:09,083 - INFO - training batch 151, loss: 0.222, 4832/60000 datapoints
2025-03-06 18:58:09,276 - INFO - training batch 201, loss: 0.656, 6432/60000 datapoints
2025-03-06 18:58:09,473 - INFO - training batch 251, loss: 0.651, 8032/60000 datapoints
2025-03-06 18:58:09,670 - INFO - training batch 301, loss: 0.231, 9632/60000 datapoints
2025-03-06 18:58:09,863 - INFO - training batch 351, loss: 0.411, 11232/60000 datapoints
2025-03-06 18:58:10,055 - INFO - training batch 401, loss: 0.336, 12832/60000 datapoints
2025-03-06 18:58:10,248 - INFO - training batch 451, loss: 0.301, 14432/60000 datapoints
2025-03-06 18:58:10,443 - INFO - training batch 501, loss: 0.670, 16032/60000 datapoints
2025-03-06 18:58:10,639 - INFO - training batch 551, loss: 0.349, 17632/60000 datapoints
2025-03-06 18:58:10,830 - INFO - training batch 601, loss: 0.501, 19232/60000 datapoints
2025-03-06 18:58:11,022 - INFO - training batch 651, loss: 0.285, 20832/60000 datapoints
2025-03-06 18:58:11,214 - INFO - training batch 701, loss: 0.442, 22432/60000 datapoints
2025-03-06 18:58:11,408 - INFO - training batch 751, loss: 0.271, 24032/60000 datapoints
2025-03-06 18:58:11,600 - INFO - training batch 801, loss: 0.748, 25632/60000 datapoints
2025-03-06 18:58:11,792 - INFO - training batch 851, loss: 0.355, 27232/60000 datapoints
2025-03-06 18:58:11,984 - INFO - training batch 901, loss: 0.443, 28832/60000 datapoints
2025-03-06 18:58:12,175 - INFO - training batch 951, loss: 0.269, 30432/60000 datapoints
2025-03-06 18:58:12,368 - INFO - training batch 1001, loss: 0.697, 32032/60000 datapoints
2025-03-06 18:58:12,559 - INFO - training batch 1051, loss: 0.578, 33632/60000 datapoints
2025-03-06 18:58:12,752 - INFO - training batch 1101, loss: 0.374, 35232/60000 datapoints
2025-03-06 18:58:12,944 - INFO - training batch 1151, loss: 0.290, 36832/60000 datapoints
2025-03-06 18:58:13,137 - INFO - training batch 1201, loss: 0.276, 38432/60000 datapoints
2025-03-06 18:58:13,330 - INFO - training batch 1251, loss: 0.461, 40032/60000 datapoints
2025-03-06 18:58:13,523 - INFO - training batch 1301, loss: 0.295, 41632/60000 datapoints
2025-03-06 18:58:13,716 - INFO - training batch 1351, loss: 0.410, 43232/60000 datapoints
2025-03-06 18:58:13,908 - INFO - training batch 1401, loss: 0.456, 44832/60000 datapoints
2025-03-06 18:58:14,102 - INFO - training batch 1451, loss: 0.324, 46432/60000 datapoints
2025-03-06 18:58:14,295 - INFO - training batch 1501, loss: 0.598, 48032/60000 datapoints
2025-03-06 18:58:14,487 - INFO - training batch 1551, loss: 0.436, 49632/60000 datapoints
2025-03-06 18:58:14,679 - INFO - training batch 1601, loss: 0.359, 51232/60000 datapoints
2025-03-06 18:58:14,873 - INFO - training batch 1651, loss: 0.286, 52832/60000 datapoints
2025-03-06 18:58:15,069 - INFO - training batch 1701, loss: 0.289, 54432/60000 datapoints
2025-03-06 18:58:15,261 - INFO - training batch 1751, loss: 0.399, 56032/60000 datapoints
2025-03-06 18:58:15,455 - INFO - training batch 1801, loss: 0.502, 57632/60000 datapoints
2025-03-06 18:58:15,661 - INFO - training batch 1851, loss: 0.685, 59232/60000 datapoints
2025-03-06 18:58:15,762 - INFO - validation batch 1, loss: 0.421, 32/10016 datapoints
2025-03-06 18:58:15,917 - INFO - validation batch 51, loss: 0.454, 1632/10016 datapoints
2025-03-06 18:58:16,071 - INFO - validation batch 101, loss: 0.433, 3232/10016 datapoints
2025-03-06 18:58:16,227 - INFO - validation batch 151, loss: 0.721, 4832/10016 datapoints
2025-03-06 18:58:16,381 - INFO - validation batch 201, loss: 0.166, 6432/10016 datapoints
2025-03-06 18:58:16,533 - INFO - validation batch 251, loss: 0.272, 8032/10016 datapoints
2025-03-06 18:58:16,690 - INFO - validation batch 301, loss: 0.536, 9632/10016 datapoints
2025-03-06 18:58:16,727 - INFO - Epoch 145/800 done.
2025-03-06 18:58:16,727 - INFO - Final validation performance:
Loss: 0.429, top-1 acc: 0.893top-5 acc: 0.893
2025-03-06 18:58:16,728 - INFO - Beginning epoch 146/800
2025-03-06 18:58:16,734 - INFO - training batch 1, loss: 0.249, 32/60000 datapoints
2025-03-06 18:58:16,932 - INFO - training batch 51, loss: 0.272, 1632/60000 datapoints
2025-03-06 18:58:17,133 - INFO - training batch 101, loss: 0.263, 3232/60000 datapoints
2025-03-06 18:58:17,336 - INFO - training batch 151, loss: 0.401, 4832/60000 datapoints
2025-03-06 18:58:17,555 - INFO - training batch 201, loss: 0.288, 6432/60000 datapoints
2025-03-06 18:58:17,749 - INFO - training batch 251, loss: 0.647, 8032/60000 datapoints
2025-03-06 18:58:17,943 - INFO - training batch 301, loss: 0.451, 9632/60000 datapoints
2025-03-06 18:58:18,171 - INFO - training batch 351, loss: 0.356, 11232/60000 datapoints
2025-03-06 18:58:18,366 - INFO - training batch 401, loss: 0.511, 12832/60000 datapoints
2025-03-06 18:58:18,559 - INFO - training batch 451, loss: 0.673, 14432/60000 datapoints
2025-03-06 18:58:18,756 - INFO - training batch 501, loss: 0.370, 16032/60000 datapoints
2025-03-06 18:58:18,953 - INFO - training batch 551, loss: 0.399, 17632/60000 datapoints
2025-03-06 18:58:19,148 - INFO - training batch 601, loss: 0.491, 19232/60000 datapoints
2025-03-06 18:58:19,343 - INFO - training batch 651, loss: 0.323, 20832/60000 datapoints
2025-03-06 18:58:19,536 - INFO - training batch 701, loss: 0.366, 22432/60000 datapoints
2025-03-06 18:58:19,732 - INFO - training batch 751, loss: 0.353, 24032/60000 datapoints
2025-03-06 18:58:19,925 - INFO - training batch 801, loss: 0.536, 25632/60000 datapoints
2025-03-06 18:58:20,123 - INFO - training batch 851, loss: 0.250, 27232/60000 datapoints
2025-03-06 18:58:20,316 - INFO - training batch 901, loss: 0.316, 28832/60000 datapoints
2025-03-06 18:58:20,514 - INFO - training batch 951, loss: 0.433, 30432/60000 datapoints
2025-03-06 18:58:20,713 - INFO - training batch 1001, loss: 0.476, 32032/60000 datapoints
2025-03-06 18:58:20,909 - INFO - training batch 1051, loss: 0.215, 33632/60000 datapoints
2025-03-06 18:58:21,107 - INFO - training batch 1101, loss: 0.650, 35232/60000 datapoints
2025-03-06 18:58:21,300 - INFO - training batch 1151, loss: 0.381, 36832/60000 datapoints
2025-03-06 18:58:21,498 - INFO - training batch 1201, loss: 0.305, 38432/60000 datapoints
2025-03-06 18:58:21,694 - INFO - training batch 1251, loss: 0.216, 40032/60000 datapoints
2025-03-06 18:58:21,889 - INFO - training batch 1301, loss: 0.363, 41632/60000 datapoints
2025-03-06 18:58:22,084 - INFO - training batch 1351, loss: 0.369, 43232/60000 datapoints
2025-03-06 18:58:22,277 - INFO - training batch 1401, loss: 0.210, 44832/60000 datapoints
2025-03-06 18:58:22,472 - INFO - training batch 1451, loss: 0.555, 46432/60000 datapoints
2025-03-06 18:58:22,670 - INFO - training batch 1501, loss: 0.300, 48032/60000 datapoints
2025-03-06 18:58:22,864 - INFO - training batch 1551, loss: 0.585, 49632/60000 datapoints
2025-03-06 18:58:23,058 - INFO - training batch 1601, loss: 0.480, 51232/60000 datapoints
2025-03-06 18:58:23,250 - INFO - training batch 1651, loss: 0.426, 52832/60000 datapoints
2025-03-06 18:58:23,446 - INFO - training batch 1701, loss: 0.282, 54432/60000 datapoints
2025-03-06 18:58:23,643 - INFO - training batch 1751, loss: 0.353, 56032/60000 datapoints
2025-03-06 18:58:23,836 - INFO - training batch 1801, loss: 0.459, 57632/60000 datapoints
2025-03-06 18:58:24,030 - INFO - training batch 1851, loss: 0.400, 59232/60000 datapoints
2025-03-06 18:58:24,132 - INFO - validation batch 1, loss: 0.334, 32/10016 datapoints
2025-03-06 18:58:24,284 - INFO - validation batch 51, loss: 0.290, 1632/10016 datapoints
2025-03-06 18:58:24,436 - INFO - validation batch 101, loss: 0.511, 3232/10016 datapoints
2025-03-06 18:58:24,588 - INFO - validation batch 151, loss: 0.435, 4832/10016 datapoints
2025-03-06 18:58:24,742 - INFO - validation batch 201, loss: 0.646, 6432/10016 datapoints
2025-03-06 18:58:24,900 - INFO - validation batch 251, loss: 0.471, 8032/10016 datapoints
2025-03-06 18:58:25,055 - INFO - validation batch 301, loss: 0.275, 9632/10016 datapoints
2025-03-06 18:58:25,091 - INFO - Epoch 146/800 done.
2025-03-06 18:58:25,091 - INFO - Final validation performance:
Loss: 0.423, top-1 acc: 0.893top-5 acc: 0.893
2025-03-06 18:58:25,092 - INFO - Beginning epoch 147/800
2025-03-06 18:58:25,098 - INFO - training batch 1, loss: 0.339, 32/60000 datapoints
2025-03-06 18:58:25,295 - INFO - training batch 51, loss: 0.564, 1632/60000 datapoints
2025-03-06 18:58:25,501 - INFO - training batch 101, loss: 0.426, 3232/60000 datapoints
2025-03-06 18:58:25,703 - INFO - training batch 151, loss: 0.403, 4832/60000 datapoints
2025-03-06 18:58:25,900 - INFO - training batch 201, loss: 0.463, 6432/60000 datapoints
2025-03-06 18:58:26,175 - INFO - training batch 251, loss: 0.474, 8032/60000 datapoints
2025-03-06 18:58:26,371 - INFO - training batch 301, loss: 0.377, 9632/60000 datapoints
2025-03-06 18:58:26,568 - INFO - training batch 351, loss: 0.427, 11232/60000 datapoints
2025-03-06 18:58:26,764 - INFO - training batch 401, loss: 0.515, 12832/60000 datapoints
2025-03-06 18:58:26,962 - INFO - training batch 451, loss: 0.267, 14432/60000 datapoints
2025-03-06 18:58:27,156 - INFO - training batch 501, loss: 0.339, 16032/60000 datapoints
2025-03-06 18:58:27,352 - INFO - training batch 551, loss: 0.167, 17632/60000 datapoints
2025-03-06 18:58:27,569 - INFO - training batch 601, loss: 0.491, 19232/60000 datapoints
2025-03-06 18:58:27,766 - INFO - training batch 651, loss: 0.476, 20832/60000 datapoints
2025-03-06 18:58:27,959 - INFO - training batch 701, loss: 0.474, 22432/60000 datapoints
2025-03-06 18:58:28,179 - INFO - training batch 751, loss: 0.200, 24032/60000 datapoints
2025-03-06 18:58:28,377 - INFO - training batch 801, loss: 0.457, 25632/60000 datapoints
2025-03-06 18:58:28,569 - INFO - training batch 851, loss: 0.631, 27232/60000 datapoints
2025-03-06 18:58:28,766 - INFO - training batch 901, loss: 0.415, 28832/60000 datapoints
2025-03-06 18:58:28,961 - INFO - training batch 951, loss: 0.395, 30432/60000 datapoints
2025-03-06 18:58:29,156 - INFO - training batch 1001, loss: 0.402, 32032/60000 datapoints
2025-03-06 18:58:29,350 - INFO - training batch 1051, loss: 0.610, 33632/60000 datapoints
2025-03-06 18:58:29,546 - INFO - training batch 1101, loss: 0.514, 35232/60000 datapoints
2025-03-06 18:58:29,743 - INFO - training batch 1151, loss: 0.218, 36832/60000 datapoints
2025-03-06 18:58:29,938 - INFO - training batch 1201, loss: 0.384, 38432/60000 datapoints
2025-03-06 18:58:30,134 - INFO - training batch 1251, loss: 0.606, 40032/60000 datapoints
2025-03-06 18:58:30,328 - INFO - training batch 1301, loss: 0.384, 41632/60000 datapoints
2025-03-06 18:58:30,525 - INFO - training batch 1351, loss: 0.451, 43232/60000 datapoints
2025-03-06 18:58:30,722 - INFO - training batch 1401, loss: 0.478, 44832/60000 datapoints
2025-03-06 18:58:30,919 - INFO - training batch 1451, loss: 0.340, 46432/60000 datapoints
2025-03-06 18:58:31,116 - INFO - training batch 1501, loss: 0.263, 48032/60000 datapoints
2025-03-06 18:58:31,323 - INFO - training batch 1551, loss: 0.481, 49632/60000 datapoints
2025-03-06 18:58:31,527 - INFO - training batch 1601, loss: 0.321, 51232/60000 datapoints
2025-03-06 18:58:31,724 - INFO - training batch 1651, loss: 0.272, 52832/60000 datapoints
2025-03-06 18:58:31,919 - INFO - training batch 1701, loss: 0.316, 54432/60000 datapoints
2025-03-06 18:58:32,114 - INFO - training batch 1751, loss: 0.201, 56032/60000 datapoints
2025-03-06 18:58:32,306 - INFO - training batch 1801, loss: 0.327, 57632/60000 datapoints
2025-03-06 18:58:32,498 - INFO - training batch 1851, loss: 0.446, 59232/60000 datapoints
2025-03-06 18:58:32,601 - INFO - validation batch 1, loss: 0.576, 32/10016 datapoints
2025-03-06 18:58:32,755 - INFO - validation batch 51, loss: 0.626, 1632/10016 datapoints
2025-03-06 18:58:32,908 - INFO - validation batch 101, loss: 0.288, 3232/10016 datapoints
2025-03-06 18:58:33,063 - INFO - validation batch 151, loss: 0.631, 4832/10016 datapoints
2025-03-06 18:58:33,220 - INFO - validation batch 201, loss: 0.585, 6432/10016 datapoints
2025-03-06 18:58:33,381 - INFO - validation batch 251, loss: 0.608, 8032/10016 datapoints
2025-03-06 18:58:33,535 - INFO - validation batch 301, loss: 0.488, 9632/10016 datapoints
2025-03-06 18:58:33,572 - INFO - Epoch 147/800 done.
2025-03-06 18:58:33,573 - INFO - Final validation performance:
Loss: 0.543, top-1 acc: 0.893top-5 acc: 0.893
2025-03-06 18:58:33,573 - INFO - Beginning epoch 148/800
2025-03-06 18:58:33,580 - INFO - training batch 1, loss: 0.485, 32/60000 datapoints
2025-03-06 18:58:33,794 - INFO - training batch 51, loss: 0.374, 1632/60000 datapoints
2025-03-06 18:58:33,987 - INFO - training batch 101, loss: 0.457, 3232/60000 datapoints
2025-03-06 18:58:34,181 - INFO - training batch 151, loss: 0.420, 4832/60000 datapoints
2025-03-06 18:58:34,380 - INFO - training batch 201, loss: 0.746, 6432/60000 datapoints
2025-03-06 18:58:34,580 - INFO - training batch 251, loss: 0.714, 8032/60000 datapoints
2025-03-06 18:58:34,782 - INFO - training batch 301, loss: 0.841, 9632/60000 datapoints
2025-03-06 18:58:34,982 - INFO - training batch 351, loss: 0.349, 11232/60000 datapoints
2025-03-06 18:58:35,179 - INFO - training batch 401, loss: 0.433, 12832/60000 datapoints
2025-03-06 18:58:35,377 - INFO - training batch 451, loss: 0.513, 14432/60000 datapoints
2025-03-06 18:58:35,571 - INFO - training batch 501, loss: 0.493, 16032/60000 datapoints
2025-03-06 18:58:35,767 - INFO - training batch 551, loss: 0.423, 17632/60000 datapoints
2025-03-06 18:58:35,961 - INFO - training batch 601, loss: 0.492, 19232/60000 datapoints
2025-03-06 18:58:36,157 - INFO - training batch 651, loss: 0.408, 20832/60000 datapoints
2025-03-06 18:58:36,351 - INFO - training batch 701, loss: 0.362, 22432/60000 datapoints
2025-03-06 18:58:36,544 - INFO - training batch 751, loss: 0.350, 24032/60000 datapoints
2025-03-06 18:58:36,740 - INFO - training batch 801, loss: 0.255, 25632/60000 datapoints
2025-03-06 18:58:36,934 - INFO - training batch 851, loss: 0.423, 27232/60000 datapoints
2025-03-06 18:58:37,128 - INFO - training batch 901, loss: 0.520, 28832/60000 datapoints
2025-03-06 18:58:37,323 - INFO - training batch 951, loss: 0.389, 30432/60000 datapoints
2025-03-06 18:58:37,525 - INFO - training batch 1001, loss: 0.345, 32032/60000 datapoints
2025-03-06 18:58:37,745 - INFO - training batch 1051, loss: 0.488, 33632/60000 datapoints
2025-03-06 18:58:37,938 - INFO - training batch 1101, loss: 0.574, 35232/60000 datapoints
2025-03-06 18:58:38,132 - INFO - training batch 1151, loss: 0.267, 36832/60000 datapoints
2025-03-06 18:58:38,326 - INFO - training batch 1201, loss: 0.308, 38432/60000 datapoints
2025-03-06 18:58:38,518 - INFO - training batch 1251, loss: 0.586, 40032/60000 datapoints
2025-03-06 18:58:38,716 - INFO - training batch 1301, loss: 0.349, 41632/60000 datapoints
2025-03-06 18:58:38,909 - INFO - training batch 1351, loss: 0.379, 43232/60000 datapoints
2025-03-06 18:58:39,107 - INFO - training batch 1401, loss: 0.278, 44832/60000 datapoints
2025-03-06 18:58:39,300 - INFO - training batch 1451, loss: 0.325, 46432/60000 datapoints
2025-03-06 18:58:39,495 - INFO - training batch 1501, loss: 0.264, 48032/60000 datapoints
2025-03-06 18:58:39,691 - INFO - training batch 1551, loss: 0.711, 49632/60000 datapoints
2025-03-06 18:58:39,884 - INFO - training batch 1601, loss: 0.378, 51232/60000 datapoints
2025-03-06 18:58:40,079 - INFO - training batch 1651, loss: 0.384, 52832/60000 datapoints
2025-03-06 18:58:40,273 - INFO - training batch 1701, loss: 0.287, 54432/60000 datapoints
2025-03-06 18:58:40,466 - INFO - training batch 1751, loss: 0.620, 56032/60000 datapoints
2025-03-06 18:58:40,664 - INFO - training batch 1801, loss: 0.379, 57632/60000 datapoints
2025-03-06 18:58:40,860 - INFO - training batch 1851, loss: 0.212, 59232/60000 datapoints
2025-03-06 18:58:40,959 - INFO - validation batch 1, loss: 0.295, 32/10016 datapoints
2025-03-06 18:58:41,114 - INFO - validation batch 51, loss: 0.248, 1632/10016 datapoints
2025-03-06 18:58:41,269 - INFO - validation batch 101, loss: 0.644, 3232/10016 datapoints
2025-03-06 18:58:41,425 - INFO - validation batch 151, loss: 0.402, 4832/10016 datapoints
2025-03-06 18:58:41,578 - INFO - validation batch 201, loss: 0.367, 6432/10016 datapoints
2025-03-06 18:58:41,732 - INFO - validation batch 251, loss: 0.303, 8032/10016 datapoints
2025-03-06 18:58:41,887 - INFO - validation batch 301, loss: 0.475, 9632/10016 datapoints
2025-03-06 18:58:41,923 - INFO - Epoch 148/800 done.
2025-03-06 18:58:41,923 - INFO - Final validation performance:
Loss: 0.391, top-1 acc: 0.893top-5 acc: 0.893
2025-03-06 18:58:41,924 - INFO - Beginning epoch 149/800
2025-03-06 18:58:41,930 - INFO - training batch 1, loss: 0.772, 32/60000 datapoints
2025-03-06 18:58:42,126 - INFO - training batch 51, loss: 0.375, 1632/60000 datapoints
2025-03-06 18:58:42,322 - INFO - training batch 101, loss: 0.415, 3232/60000 datapoints
2025-03-06 18:58:42,524 - INFO - training batch 151, loss: 0.391, 4832/60000 datapoints
2025-03-06 18:58:42,724 - INFO - training batch 201, loss: 0.199, 6432/60000 datapoints
2025-03-06 18:58:42,919 - INFO - training batch 251, loss: 0.295, 8032/60000 datapoints
2025-03-06 18:58:43,112 - INFO - training batch 301, loss: 0.279, 9632/60000 datapoints
2025-03-06 18:58:43,315 - INFO - training batch 351, loss: 0.521, 11232/60000 datapoints
2025-03-06 18:58:43,515 - INFO - training batch 401, loss: 0.333, 12832/60000 datapoints
2025-03-06 18:58:43,717 - INFO - training batch 451, loss: 0.493, 14432/60000 datapoints
2025-03-06 18:58:43,914 - INFO - training batch 501, loss: 0.571, 16032/60000 datapoints
2025-03-06 18:58:44,111 - INFO - training batch 551, loss: 0.328, 17632/60000 datapoints
2025-03-06 18:58:44,307 - INFO - training batch 601, loss: 0.470, 19232/60000 datapoints
2025-03-06 18:58:44,498 - INFO - training batch 651, loss: 0.276, 20832/60000 datapoints
2025-03-06 18:58:44,693 - INFO - training batch 701, loss: 0.528, 22432/60000 datapoints
2025-03-06 18:58:44,892 - INFO - training batch 751, loss: 0.402, 24032/60000 datapoints
2025-03-06 18:58:45,083 - INFO - training batch 801, loss: 0.362, 25632/60000 datapoints
2025-03-06 18:58:45,279 - INFO - training batch 851, loss: 0.675, 27232/60000 datapoints
2025-03-06 18:58:45,474 - INFO - training batch 901, loss: 0.467, 28832/60000 datapoints
2025-03-06 18:58:45,668 - INFO - training batch 951, loss: 0.432, 30432/60000 datapoints
2025-03-06 18:58:45,863 - INFO - training batch 1001, loss: 0.334, 32032/60000 datapoints
2025-03-06 18:58:46,058 - INFO - training batch 1051, loss: 0.376, 33632/60000 datapoints
2025-03-06 18:58:46,253 - INFO - training batch 1101, loss: 0.459, 35232/60000 datapoints
2025-03-06 18:58:46,445 - INFO - training batch 1151, loss: 0.304, 36832/60000 datapoints
2025-03-06 18:58:46,642 - INFO - training batch 1201, loss: 0.337, 38432/60000 datapoints
2025-03-06 18:58:46,838 - INFO - training batch 1251, loss: 0.383, 40032/60000 datapoints
2025-03-06 18:58:47,029 - INFO - training batch 1301, loss: 0.308, 41632/60000 datapoints
2025-03-06 18:58:47,223 - INFO - training batch 1351, loss: 0.417, 43232/60000 datapoints
2025-03-06 18:58:47,419 - INFO - training batch 1401, loss: 0.218, 44832/60000 datapoints
2025-03-06 18:58:47,622 - INFO - training batch 1451, loss: 0.374, 46432/60000 datapoints
2025-03-06 18:58:47,830 - INFO - training batch 1501, loss: 0.486, 48032/60000 datapoints
2025-03-06 18:58:48,023 - INFO - training batch 1551, loss: 0.306, 49632/60000 datapoints
2025-03-06 18:58:48,217 - INFO - training batch 1601, loss: 0.664, 51232/60000 datapoints
2025-03-06 18:58:48,413 - INFO - training batch 1651, loss: 0.306, 52832/60000 datapoints
2025-03-06 18:58:48,608 - INFO - training batch 1701, loss: 0.389, 54432/60000 datapoints
2025-03-06 18:58:48,804 - INFO - training batch 1751, loss: 0.358, 56032/60000 datapoints
2025-03-06 18:58:48,997 - INFO - training batch 1801, loss: 0.536, 57632/60000 datapoints
2025-03-06 18:58:49,191 - INFO - training batch 1851, loss: 0.419, 59232/60000 datapoints
2025-03-06 18:58:49,293 - INFO - validation batch 1, loss: 0.398, 32/10016 datapoints
2025-03-06 18:58:49,447 - INFO - validation batch 51, loss: 0.324, 1632/10016 datapoints
2025-03-06 18:58:49,599 - INFO - validation batch 101, loss: 0.515, 3232/10016 datapoints
2025-03-06 18:58:49,753 - INFO - validation batch 151, loss: 0.370, 4832/10016 datapoints
2025-03-06 18:58:49,908 - INFO - validation batch 201, loss: 0.364, 6432/10016 datapoints
2025-03-06 18:58:50,059 - INFO - validation batch 251, loss: 0.247, 8032/10016 datapoints
2025-03-06 18:58:50,212 - INFO - validation batch 301, loss: 0.282, 9632/10016 datapoints
2025-03-06 18:58:50,249 - INFO - Epoch 149/800 done.
2025-03-06 18:58:50,249 - INFO - Final validation performance:
Loss: 0.357, top-1 acc: 0.893top-5 acc: 0.893
2025-03-06 18:58:50,250 - INFO - Beginning epoch 150/800
2025-03-06 18:58:50,256 - INFO - training batch 1, loss: 0.223, 32/60000 datapoints
2025-03-06 18:58:50,457 - INFO - training batch 51, loss: 0.309, 1632/60000 datapoints
2025-03-06 18:58:50,654 - INFO - training batch 101, loss: 0.345, 3232/60000 datapoints
2025-03-06 18:58:50,846 - INFO - training batch 151, loss: 0.337, 4832/60000 datapoints
2025-03-06 18:58:51,042 - INFO - training batch 201, loss: 0.437, 6432/60000 datapoints
2025-03-06 18:58:51,235 - INFO - training batch 251, loss: 0.530, 8032/60000 datapoints
2025-03-06 18:58:51,434 - INFO - training batch 301, loss: 0.325, 9632/60000 datapoints
2025-03-06 18:58:51,628 - INFO - training batch 351, loss: 0.621, 11232/60000 datapoints
2025-03-06 18:58:51,821 - INFO - training batch 401, loss: 0.626, 12832/60000 datapoints
2025-03-06 18:58:52,012 - INFO - training batch 451, loss: 0.264, 14432/60000 datapoints
2025-03-06 18:58:52,203 - INFO - training batch 501, loss: 0.409, 16032/60000 datapoints
2025-03-06 18:58:52,396 - INFO - training batch 551, loss: 0.402, 17632/60000 datapoints
2025-03-06 18:58:52,585 - INFO - training batch 601, loss: 0.280, 19232/60000 datapoints
2025-03-06 18:58:52,778 - INFO - training batch 651, loss: 0.524, 20832/60000 datapoints
2025-03-06 18:58:52,969 - INFO - training batch 701, loss: 0.260, 22432/60000 datapoints
2025-03-06 18:58:53,164 - INFO - training batch 751, loss: 0.610, 24032/60000 datapoints
2025-03-06 18:58:53,356 - INFO - training batch 801, loss: 0.380, 25632/60000 datapoints
2025-03-06 18:58:53,547 - INFO - training batch 851, loss: 0.582, 27232/60000 datapoints
2025-03-06 18:58:53,740 - INFO - training batch 901, loss: 0.382, 28832/60000 datapoints
2025-03-06 18:58:53,932 - INFO - training batch 951, loss: 0.487, 30432/60000 datapoints
2025-03-06 18:58:54,128 - INFO - training batch 1001, loss: 0.490, 32032/60000 datapoints
2025-03-06 18:58:54,322 - INFO - training batch 1051, loss: 0.444, 33632/60000 datapoints
2025-03-06 18:58:54,512 - INFO - training batch 1101, loss: 0.398, 35232/60000 datapoints
2025-03-06 18:58:54,705 - INFO - training batch 1151, loss: 0.358, 36832/60000 datapoints
2025-03-06 18:58:54,904 - INFO - training batch 1201, loss: 0.231, 38432/60000 datapoints
2025-03-06 18:58:55,097 - INFO - training batch 1251, loss: 0.389, 40032/60000 datapoints
2025-03-06 18:58:55,287 - INFO - training batch 1301, loss: 0.446, 41632/60000 datapoints
2025-03-06 18:58:55,483 - INFO - training batch 1351, loss: 0.163, 43232/60000 datapoints
2025-03-06 18:58:55,680 - INFO - training batch 1401, loss: 0.499, 44832/60000 datapoints
2025-03-06 18:58:55,875 - INFO - training batch 1451, loss: 0.353, 46432/60000 datapoints
2025-03-06 18:58:56,068 - INFO - training batch 1501, loss: 0.342, 48032/60000 datapoints
2025-03-06 18:58:56,262 - INFO - training batch 1551, loss: 0.707, 49632/60000 datapoints
2025-03-06 18:58:56,457 - INFO - training batch 1601, loss: 0.406, 51232/60000 datapoints
2025-03-06 18:58:56,652 - INFO - training batch 1651, loss: 0.281, 52832/60000 datapoints
2025-03-06 18:58:56,846 - INFO - training batch 1701, loss: 0.547, 54432/60000 datapoints
2025-03-06 18:58:57,042 - INFO - training batch 1751, loss: 0.530, 56032/60000 datapoints
2025-03-06 18:58:57,234 - INFO - training batch 1801, loss: 0.483, 57632/60000 datapoints
2025-03-06 18:58:57,428 - INFO - training batch 1851, loss: 0.517, 59232/60000 datapoints
2025-03-06 18:58:57,531 - INFO - validation batch 1, loss: 0.448, 32/10016 datapoints
2025-03-06 18:58:57,686 - INFO - validation batch 51, loss: 0.327, 1632/10016 datapoints
2025-03-06 18:58:57,853 - INFO - validation batch 101, loss: 0.381, 3232/10016 datapoints
2025-03-06 18:58:58,005 - INFO - validation batch 151, loss: 0.460, 4832/10016 datapoints
2025-03-06 18:58:58,159 - INFO - validation batch 201, loss: 0.368, 6432/10016 datapoints
2025-03-06 18:58:58,310 - INFO - validation batch 251, loss: 0.282, 8032/10016 datapoints
2025-03-06 18:58:58,463 - INFO - validation batch 301, loss: 0.507, 9632/10016 datapoints
2025-03-06 18:58:58,502 - INFO - Epoch 150/800 done.
2025-03-06 18:58:58,502 - INFO - Final validation performance:
Loss: 0.396, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:58:58,503 - INFO - Beginning epoch 151/800
2025-03-06 18:58:58,509 - INFO - training batch 1, loss: 0.334, 32/60000 datapoints
2025-03-06 18:58:58,705 - INFO - training batch 51, loss: 0.362, 1632/60000 datapoints
2025-03-06 18:58:58,903 - INFO - training batch 101, loss: 0.606, 3232/60000 datapoints
2025-03-06 18:58:59,106 - INFO - training batch 151, loss: 0.291, 4832/60000 datapoints
2025-03-06 18:58:59,304 - INFO - training batch 201, loss: 0.233, 6432/60000 datapoints
2025-03-06 18:58:59,501 - INFO - training batch 251, loss: 0.399, 8032/60000 datapoints
2025-03-06 18:58:59,701 - INFO - training batch 301, loss: 0.296, 9632/60000 datapoints
2025-03-06 18:58:59,893 - INFO - training batch 351, loss: 0.286, 11232/60000 datapoints
2025-03-06 18:59:00,090 - INFO - training batch 401, loss: 0.432, 12832/60000 datapoints
2025-03-06 18:59:00,286 - INFO - training batch 451, loss: 0.481, 14432/60000 datapoints
2025-03-06 18:59:00,479 - INFO - training batch 501, loss: 0.625, 16032/60000 datapoints
2025-03-06 18:59:00,679 - INFO - training batch 551, loss: 0.330, 17632/60000 datapoints
2025-03-06 18:59:00,872 - INFO - training batch 601, loss: 0.318, 19232/60000 datapoints
2025-03-06 18:59:01,066 - INFO - training batch 651, loss: 0.289, 20832/60000 datapoints
2025-03-06 18:59:01,259 - INFO - training batch 701, loss: 0.540, 22432/60000 datapoints
2025-03-06 18:59:01,452 - INFO - training batch 751, loss: 0.407, 24032/60000 datapoints
2025-03-06 18:59:01,651 - INFO - training batch 801, loss: 0.361, 25632/60000 datapoints
2025-03-06 18:59:01,846 - INFO - training batch 851, loss: 0.292, 27232/60000 datapoints
2025-03-06 18:59:02,041 - INFO - training batch 901, loss: 0.586, 28832/60000 datapoints
2025-03-06 18:59:02,234 - INFO - training batch 951, loss: 0.258, 30432/60000 datapoints
2025-03-06 18:59:02,434 - INFO - training batch 1001, loss: 0.225, 32032/60000 datapoints
2025-03-06 18:59:02,629 - INFO - training batch 1051, loss: 0.336, 33632/60000 datapoints
2025-03-06 18:59:02,822 - INFO - training batch 1101, loss: 0.223, 35232/60000 datapoints
2025-03-06 18:59:03,017 - INFO - training batch 1151, loss: 0.593, 36832/60000 datapoints
2025-03-06 18:59:03,215 - INFO - training batch 1201, loss: 0.546, 38432/60000 datapoints
2025-03-06 18:59:03,409 - INFO - training batch 1251, loss: 0.331, 40032/60000 datapoints
2025-03-06 18:59:03,605 - INFO - training batch 1301, loss: 0.309, 41632/60000 datapoints
2025-03-06 18:59:03,801 - INFO - training batch 1351, loss: 0.533, 43232/60000 datapoints
2025-03-06 18:59:03,994 - INFO - training batch 1401, loss: 0.303, 44832/60000 datapoints
2025-03-06 18:59:04,188 - INFO - training batch 1451, loss: 0.377, 46432/60000 datapoints
2025-03-06 18:59:04,382 - INFO - training batch 1501, loss: 0.437, 48032/60000 datapoints
2025-03-06 18:59:04,575 - INFO - training batch 1551, loss: 0.354, 49632/60000 datapoints
2025-03-06 18:59:04,772 - INFO - training batch 1601, loss: 0.422, 51232/60000 datapoints
2025-03-06 18:59:04,970 - INFO - training batch 1651, loss: 0.427, 52832/60000 datapoints
2025-03-06 18:59:05,163 - INFO - training batch 1701, loss: 0.239, 54432/60000 datapoints
2025-03-06 18:59:05,357 - INFO - training batch 1751, loss: 0.834, 56032/60000 datapoints
2025-03-06 18:59:05,553 - INFO - training batch 1801, loss: 0.495, 57632/60000 datapoints
2025-03-06 18:59:05,751 - INFO - training batch 1851, loss: 0.533, 59232/60000 datapoints
2025-03-06 18:59:05,852 - INFO - validation batch 1, loss: 0.295, 32/10016 datapoints
2025-03-06 18:59:06,003 - INFO - validation batch 51, loss: 0.396, 1632/10016 datapoints
2025-03-06 18:59:06,155 - INFO - validation batch 101, loss: 0.313, 3232/10016 datapoints
2025-03-06 18:59:06,310 - INFO - validation batch 151, loss: 0.312, 4832/10016 datapoints
2025-03-06 18:59:06,461 - INFO - validation batch 201, loss: 0.373, 6432/10016 datapoints
2025-03-06 18:59:06,616 - INFO - validation batch 251, loss: 0.418, 8032/10016 datapoints
2025-03-06 18:59:06,770 - INFO - validation batch 301, loss: 0.568, 9632/10016 datapoints
2025-03-06 18:59:06,808 - INFO - Epoch 151/800 done.
2025-03-06 18:59:06,808 - INFO - Final validation performance:
Loss: 0.382, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:59:06,809 - INFO - Beginning epoch 152/800
2025-03-06 18:59:06,815 - INFO - training batch 1, loss: 0.375, 32/60000 datapoints
2025-03-06 18:59:07,009 - INFO - training batch 51, loss: 0.481, 1632/60000 datapoints
2025-03-06 18:59:07,203 - INFO - training batch 101, loss: 0.342, 3232/60000 datapoints
2025-03-06 18:59:07,407 - INFO - training batch 151, loss: 0.288, 4832/60000 datapoints
2025-03-06 18:59:07,617 - INFO - training batch 201, loss: 0.413, 6432/60000 datapoints
2025-03-06 18:59:07,827 - INFO - training batch 251, loss: 0.389, 8032/60000 datapoints
2025-03-06 18:59:08,022 - INFO - training batch 301, loss: 0.830, 9632/60000 datapoints
2025-03-06 18:59:08,214 - INFO - training batch 351, loss: 0.469, 11232/60000 datapoints
2025-03-06 18:59:08,409 - INFO - training batch 401, loss: 0.471, 12832/60000 datapoints
2025-03-06 18:59:08,608 - INFO - training batch 451, loss: 0.151, 14432/60000 datapoints
2025-03-06 18:59:08,801 - INFO - training batch 501, loss: 0.415, 16032/60000 datapoints
2025-03-06 18:59:08,998 - INFO - training batch 551, loss: 0.843, 17632/60000 datapoints
2025-03-06 18:59:09,202 - INFO - training batch 601, loss: 0.671, 19232/60000 datapoints
2025-03-06 18:59:09,394 - INFO - training batch 651, loss: 0.331, 20832/60000 datapoints
2025-03-06 18:59:09,589 - INFO - training batch 701, loss: 0.608, 22432/60000 datapoints
2025-03-06 18:59:09,785 - INFO - training batch 751, loss: 0.549, 24032/60000 datapoints
2025-03-06 18:59:09,978 - INFO - training batch 801, loss: 0.338, 25632/60000 datapoints
2025-03-06 18:59:10,172 - INFO - training batch 851, loss: 0.432, 27232/60000 datapoints
2025-03-06 18:59:10,366 - INFO - training batch 901, loss: 0.366, 28832/60000 datapoints
2025-03-06 18:59:10,562 - INFO - training batch 951, loss: 0.289, 30432/60000 datapoints
2025-03-06 18:59:10,760 - INFO - training batch 1001, loss: 0.420, 32032/60000 datapoints
2025-03-06 18:59:10,954 - INFO - training batch 1051, loss: 0.580, 33632/60000 datapoints
2025-03-06 18:59:11,147 - INFO - training batch 1101, loss: 0.381, 35232/60000 datapoints
2025-03-06 18:59:11,340 - INFO - training batch 1151, loss: 0.369, 36832/60000 datapoints
2025-03-06 18:59:11,538 - INFO - training batch 1201, loss: 0.459, 38432/60000 datapoints
2025-03-06 18:59:11,734 - INFO - training batch 1251, loss: 0.464, 40032/60000 datapoints
2025-03-06 18:59:11,928 - INFO - training batch 1301, loss: 0.533, 41632/60000 datapoints
2025-03-06 18:59:12,123 - INFO - training batch 1351, loss: 0.337, 43232/60000 datapoints
2025-03-06 18:59:12,317 - INFO - training batch 1401, loss: 0.512, 44832/60000 datapoints
2025-03-06 18:59:12,511 - INFO - training batch 1451, loss: 0.530, 46432/60000 datapoints
2025-03-06 18:59:12,709 - INFO - training batch 1501, loss: 0.380, 48032/60000 datapoints
2025-03-06 18:59:12,903 - INFO - training batch 1551, loss: 0.359, 49632/60000 datapoints
2025-03-06 18:59:13,098 - INFO - training batch 1601, loss: 0.243, 51232/60000 datapoints
2025-03-06 18:59:13,292 - INFO - training batch 1651, loss: 0.308, 52832/60000 datapoints
2025-03-06 18:59:13,485 - INFO - training batch 1701, loss: 0.316, 54432/60000 datapoints
2025-03-06 18:59:13,683 - INFO - training batch 1751, loss: 0.562, 56032/60000 datapoints
2025-03-06 18:59:13,877 - INFO - training batch 1801, loss: 0.474, 57632/60000 datapoints
2025-03-06 18:59:14,075 - INFO - training batch 1851, loss: 0.299, 59232/60000 datapoints
2025-03-06 18:59:14,183 - INFO - validation batch 1, loss: 0.447, 32/10016 datapoints
2025-03-06 18:59:14,337 - INFO - validation batch 51, loss: 0.346, 1632/10016 datapoints
2025-03-06 18:59:14,491 - INFO - validation batch 101, loss: 0.477, 3232/10016 datapoints
2025-03-06 18:59:14,646 - INFO - validation batch 151, loss: 0.330, 4832/10016 datapoints
2025-03-06 18:59:14,796 - INFO - validation batch 201, loss: 0.315, 6432/10016 datapoints
2025-03-06 18:59:14,950 - INFO - validation batch 251, loss: 0.476, 8032/10016 datapoints
2025-03-06 18:59:15,103 - INFO - validation batch 301, loss: 0.321, 9632/10016 datapoints
2025-03-06 18:59:15,138 - INFO - Epoch 152/800 done.
2025-03-06 18:59:15,138 - INFO - Final validation performance:
Loss: 0.387, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:59:15,139 - INFO - Beginning epoch 153/800
2025-03-06 18:59:15,146 - INFO - training batch 1, loss: 0.303, 32/60000 datapoints
2025-03-06 18:59:15,345 - INFO - training batch 51, loss: 0.225, 1632/60000 datapoints
2025-03-06 18:59:15,542 - INFO - training batch 101, loss: 0.374, 3232/60000 datapoints
2025-03-06 18:59:15,750 - INFO - training batch 151, loss: 0.342, 4832/60000 datapoints
2025-03-06 18:59:15,943 - INFO - training batch 201, loss: 0.365, 6432/60000 datapoints
2025-03-06 18:59:16,190 - INFO - training batch 251, loss: 0.503, 8032/60000 datapoints
2025-03-06 18:59:16,391 - INFO - training batch 301, loss: 0.188, 9632/60000 datapoints
2025-03-06 18:59:16,590 - INFO - training batch 351, loss: 0.661, 11232/60000 datapoints
2025-03-06 18:59:16,817 - INFO - training batch 401, loss: 0.549, 12832/60000 datapoints
2025-03-06 18:59:17,012 - INFO - training batch 451, loss: 0.543, 14432/60000 datapoints
2025-03-06 18:59:17,206 - INFO - training batch 501, loss: 0.472, 16032/60000 datapoints
2025-03-06 18:59:17,408 - INFO - training batch 551, loss: 0.195, 17632/60000 datapoints
2025-03-06 18:59:17,605 - INFO - training batch 601, loss: 0.384, 19232/60000 datapoints
2025-03-06 18:59:17,801 - INFO - training batch 651, loss: 0.704, 20832/60000 datapoints
2025-03-06 18:59:18,015 - INFO - training batch 701, loss: 0.521, 22432/60000 datapoints
2025-03-06 18:59:18,208 - INFO - training batch 751, loss: 0.565, 24032/60000 datapoints
2025-03-06 18:59:18,423 - INFO - training batch 801, loss: 0.284, 25632/60000 datapoints
2025-03-06 18:59:18,625 - INFO - training batch 851, loss: 0.452, 27232/60000 datapoints
2025-03-06 18:59:18,818 - INFO - training batch 901, loss: 0.276, 28832/60000 datapoints
2025-03-06 18:59:19,011 - INFO - training batch 951, loss: 0.177, 30432/60000 datapoints
2025-03-06 18:59:19,211 - INFO - training batch 1001, loss: 0.299, 32032/60000 datapoints
2025-03-06 18:59:19,407 - INFO - training batch 1051, loss: 0.223, 33632/60000 datapoints
2025-03-06 18:59:19,604 - INFO - training batch 1101, loss: 0.496, 35232/60000 datapoints
2025-03-06 18:59:19,801 - INFO - training batch 1151, loss: 0.505, 36832/60000 datapoints
2025-03-06 18:59:19,995 - INFO - training batch 1201, loss: 0.326, 38432/60000 datapoints
2025-03-06 18:59:20,188 - INFO - training batch 1251, loss: 0.469, 40032/60000 datapoints
2025-03-06 18:59:20,382 - INFO - training batch 1301, loss: 0.365, 41632/60000 datapoints
2025-03-06 18:59:20,576 - INFO - training batch 1351, loss: 0.513, 43232/60000 datapoints
2025-03-06 18:59:20,771 - INFO - training batch 1401, loss: 0.681, 44832/60000 datapoints
2025-03-06 18:59:20,965 - INFO - training batch 1451, loss: 0.460, 46432/60000 datapoints
2025-03-06 18:59:21,157 - INFO - training batch 1501, loss: 0.371, 48032/60000 datapoints
2025-03-06 18:59:21,351 - INFO - training batch 1551, loss: 0.409, 49632/60000 datapoints
2025-03-06 18:59:21,545 - INFO - training batch 1601, loss: 0.399, 51232/60000 datapoints
2025-03-06 18:59:21,740 - INFO - training batch 1651, loss: 0.885, 52832/60000 datapoints
2025-03-06 18:59:21,933 - INFO - training batch 1701, loss: 0.545, 54432/60000 datapoints
2025-03-06 18:59:22,126 - INFO - training batch 1751, loss: 0.267, 56032/60000 datapoints
2025-03-06 18:59:22,318 - INFO - training batch 1801, loss: 0.470, 57632/60000 datapoints
2025-03-06 18:59:22,510 - INFO - training batch 1851, loss: 0.382, 59232/60000 datapoints
2025-03-06 18:59:22,613 - INFO - validation batch 1, loss: 0.335, 32/10016 datapoints
2025-03-06 18:59:22,766 - INFO - validation batch 51, loss: 0.383, 1632/10016 datapoints
2025-03-06 18:59:22,918 - INFO - validation batch 101, loss: 0.347, 3232/10016 datapoints
2025-03-06 18:59:23,070 - INFO - validation batch 151, loss: 0.226, 4832/10016 datapoints
2025-03-06 18:59:23,225 - INFO - validation batch 201, loss: 0.463, 6432/10016 datapoints
2025-03-06 18:59:23,377 - INFO - validation batch 251, loss: 0.251, 8032/10016 datapoints
2025-03-06 18:59:23,529 - INFO - validation batch 301, loss: 0.351, 9632/10016 datapoints
2025-03-06 18:59:23,570 - INFO - Epoch 153/800 done.
2025-03-06 18:59:23,570 - INFO - Final validation performance:
Loss: 0.337, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:59:23,571 - INFO - Beginning epoch 154/800
2025-03-06 18:59:23,577 - INFO - training batch 1, loss: 0.323, 32/60000 datapoints
2025-03-06 18:59:23,774 - INFO - training batch 51, loss: 0.360, 1632/60000 datapoints
2025-03-06 18:59:23,979 - INFO - training batch 101, loss: 0.379, 3232/60000 datapoints
2025-03-06 18:59:24,170 - INFO - training batch 151, loss: 0.200, 4832/60000 datapoints
2025-03-06 18:59:24,366 - INFO - training batch 201, loss: 0.459, 6432/60000 datapoints
2025-03-06 18:59:24,567 - INFO - training batch 251, loss: 0.545, 8032/60000 datapoints
2025-03-06 18:59:24,764 - INFO - training batch 301, loss: 0.286, 9632/60000 datapoints
2025-03-06 18:59:24,964 - INFO - training batch 351, loss: 0.552, 11232/60000 datapoints
2025-03-06 18:59:25,157 - INFO - training batch 401, loss: 0.291, 12832/60000 datapoints
2025-03-06 18:59:25,355 - INFO - training batch 451, loss: 0.407, 14432/60000 datapoints
2025-03-06 18:59:25,552 - INFO - training batch 501, loss: 0.205, 16032/60000 datapoints
2025-03-06 18:59:25,748 - INFO - training batch 551, loss: 0.360, 17632/60000 datapoints
2025-03-06 18:59:25,942 - INFO - training batch 601, loss: 0.460, 19232/60000 datapoints
2025-03-06 18:59:26,136 - INFO - training batch 651, loss: 0.287, 20832/60000 datapoints
2025-03-06 18:59:26,333 - INFO - training batch 701, loss: 0.346, 22432/60000 datapoints
2025-03-06 18:59:26,525 - INFO - training batch 751, loss: 0.325, 24032/60000 datapoints
2025-03-06 18:59:26,721 - INFO - training batch 801, loss: 0.339, 25632/60000 datapoints
2025-03-06 18:59:26,915 - INFO - training batch 851, loss: 0.273, 27232/60000 datapoints
2025-03-06 18:59:27,109 - INFO - training batch 901, loss: 0.336, 28832/60000 datapoints
2025-03-06 18:59:27,303 - INFO - training batch 951, loss: 0.406, 30432/60000 datapoints
2025-03-06 18:59:27,496 - INFO - training batch 1001, loss: 0.217, 32032/60000 datapoints
2025-03-06 18:59:27,698 - INFO - training batch 1051, loss: 0.469, 33632/60000 datapoints
2025-03-06 18:59:27,892 - INFO - training batch 1101, loss: 0.464, 35232/60000 datapoints
2025-03-06 18:59:28,113 - INFO - training batch 1151, loss: 0.321, 36832/60000 datapoints
2025-03-06 18:59:28,306 - INFO - training batch 1201, loss: 0.657, 38432/60000 datapoints
2025-03-06 18:59:28,502 - INFO - training batch 1251, loss: 0.626, 40032/60000 datapoints
2025-03-06 18:59:28,697 - INFO - training batch 1301, loss: 0.312, 41632/60000 datapoints
2025-03-06 18:59:28,889 - INFO - training batch 1351, loss: 0.277, 43232/60000 datapoints
2025-03-06 18:59:29,083 - INFO - training batch 1401, loss: 0.275, 44832/60000 datapoints
2025-03-06 18:59:29,276 - INFO - training batch 1451, loss: 0.264, 46432/60000 datapoints
2025-03-06 18:59:29,478 - INFO - training batch 1501, loss: 0.218, 48032/60000 datapoints
2025-03-06 18:59:29,676 - INFO - training batch 1551, loss: 0.296, 49632/60000 datapoints
2025-03-06 18:59:29,872 - INFO - training batch 1601, loss: 0.470, 51232/60000 datapoints
2025-03-06 18:59:30,065 - INFO - training batch 1651, loss: 0.486, 52832/60000 datapoints
2025-03-06 18:59:30,259 - INFO - training batch 1701, loss: 0.682, 54432/60000 datapoints
2025-03-06 18:59:30,455 - INFO - training batch 1751, loss: 0.217, 56032/60000 datapoints
2025-03-06 18:59:30,654 - INFO - training batch 1801, loss: 0.452, 57632/60000 datapoints
2025-03-06 18:59:30,881 - INFO - training batch 1851, loss: 0.276, 59232/60000 datapoints
2025-03-06 18:59:30,981 - INFO - validation batch 1, loss: 0.395, 32/10016 datapoints
2025-03-06 18:59:31,130 - INFO - validation batch 51, loss: 0.345, 1632/10016 datapoints
2025-03-06 18:59:31,284 - INFO - validation batch 101, loss: 0.498, 3232/10016 datapoints
2025-03-06 18:59:31,437 - INFO - validation batch 151, loss: 0.305, 4832/10016 datapoints
2025-03-06 18:59:31,593 - INFO - validation batch 201, loss: 0.301, 6432/10016 datapoints
2025-03-06 18:59:31,748 - INFO - validation batch 251, loss: 0.222, 8032/10016 datapoints
2025-03-06 18:59:31,903 - INFO - validation batch 301, loss: 0.291, 9632/10016 datapoints
2025-03-06 18:59:31,938 - INFO - Epoch 154/800 done.
2025-03-06 18:59:31,938 - INFO - Final validation performance:
Loss: 0.337, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:59:31,939 - INFO - Beginning epoch 155/800
2025-03-06 18:59:31,947 - INFO - training batch 1, loss: 0.512, 32/60000 datapoints
2025-03-06 18:59:32,142 - INFO - training batch 51, loss: 0.220, 1632/60000 datapoints
2025-03-06 18:59:32,338 - INFO - training batch 101, loss: 0.244, 3232/60000 datapoints
2025-03-06 18:59:32,545 - INFO - training batch 151, loss: 0.648, 4832/60000 datapoints
2025-03-06 18:59:32,741 - INFO - training batch 201, loss: 0.455, 6432/60000 datapoints
2025-03-06 18:59:32,935 - INFO - training batch 251, loss: 0.552, 8032/60000 datapoints
2025-03-06 18:59:33,135 - INFO - training batch 301, loss: 0.376, 9632/60000 datapoints
2025-03-06 18:59:33,334 - INFO - training batch 351, loss: 0.566, 11232/60000 datapoints
2025-03-06 18:59:33,539 - INFO - training batch 401, loss: 0.238, 12832/60000 datapoints
2025-03-06 18:59:33,740 - INFO - training batch 451, loss: 0.167, 14432/60000 datapoints
2025-03-06 18:59:33,935 - INFO - training batch 501, loss: 0.345, 16032/60000 datapoints
2025-03-06 18:59:34,133 - INFO - training batch 551, loss: 0.427, 17632/60000 datapoints
2025-03-06 18:59:34,326 - INFO - training batch 601, loss: 0.312, 19232/60000 datapoints
2025-03-06 18:59:34,520 - INFO - training batch 651, loss: 0.313, 20832/60000 datapoints
2025-03-06 18:59:34,716 - INFO - training batch 701, loss: 0.304, 22432/60000 datapoints
2025-03-06 18:59:34,921 - INFO - training batch 751, loss: 0.519, 24032/60000 datapoints
2025-03-06 18:59:35,113 - INFO - training batch 801, loss: 0.375, 25632/60000 datapoints
2025-03-06 18:59:35,309 - INFO - training batch 851, loss: 0.490, 27232/60000 datapoints
2025-03-06 18:59:35,504 - INFO - training batch 901, loss: 0.210, 28832/60000 datapoints
2025-03-06 18:59:35,702 - INFO - training batch 951, loss: 0.594, 30432/60000 datapoints
2025-03-06 18:59:35,899 - INFO - training batch 1001, loss: 0.432, 32032/60000 datapoints
2025-03-06 18:59:36,092 - INFO - training batch 1051, loss: 0.447, 33632/60000 datapoints
2025-03-06 18:59:36,296 - INFO - training batch 1101, loss: 0.246, 35232/60000 datapoints
2025-03-06 18:59:36,493 - INFO - training batch 1151, loss: 0.202, 36832/60000 datapoints
2025-03-06 18:59:36,688 - INFO - training batch 1201, loss: 0.326, 38432/60000 datapoints
2025-03-06 18:59:36,884 - INFO - training batch 1251, loss: 0.419, 40032/60000 datapoints
2025-03-06 18:59:37,075 - INFO - training batch 1301, loss: 0.523, 41632/60000 datapoints
2025-03-06 18:59:37,269 - INFO - training batch 1351, loss: 0.480, 43232/60000 datapoints
2025-03-06 18:59:37,465 - INFO - training batch 1401, loss: 0.533, 44832/60000 datapoints
2025-03-06 18:59:37,680 - INFO - training batch 1451, loss: 0.425, 46432/60000 datapoints
2025-03-06 18:59:37,872 - INFO - training batch 1501, loss: 0.251, 48032/60000 datapoints
2025-03-06 18:59:38,087 - INFO - training batch 1551, loss: 0.432, 49632/60000 datapoints
2025-03-06 18:59:38,281 - INFO - training batch 1601, loss: 0.553, 51232/60000 datapoints
2025-03-06 18:59:38,477 - INFO - training batch 1651, loss: 0.346, 52832/60000 datapoints
2025-03-06 18:59:38,670 - INFO - training batch 1701, loss: 0.353, 54432/60000 datapoints
2025-03-06 18:59:38,866 - INFO - training batch 1751, loss: 0.385, 56032/60000 datapoints
2025-03-06 18:59:39,064 - INFO - training batch 1801, loss: 0.245, 57632/60000 datapoints
2025-03-06 18:59:39,255 - INFO - training batch 1851, loss: 0.422, 59232/60000 datapoints
2025-03-06 18:59:39,355 - INFO - validation batch 1, loss: 0.728, 32/10016 datapoints
2025-03-06 18:59:39,510 - INFO - validation batch 51, loss: 0.410, 1632/10016 datapoints
2025-03-06 18:59:39,666 - INFO - validation batch 101, loss: 0.491, 3232/10016 datapoints
2025-03-06 18:59:39,819 - INFO - validation batch 151, loss: 0.287, 4832/10016 datapoints
2025-03-06 18:59:39,971 - INFO - validation batch 201, loss: 0.257, 6432/10016 datapoints
2025-03-06 18:59:40,121 - INFO - validation batch 251, loss: 0.286, 8032/10016 datapoints
2025-03-06 18:59:40,272 - INFO - validation batch 301, loss: 0.361, 9632/10016 datapoints
2025-03-06 18:59:40,309 - INFO - Epoch 155/800 done.
2025-03-06 18:59:40,309 - INFO - Final validation performance:
Loss: 0.403, top-1 acc: 0.894top-5 acc: 0.894
2025-03-06 18:59:40,310 - INFO - Beginning epoch 156/800
2025-03-06 18:59:40,317 - INFO - training batch 1, loss: 0.451, 32/60000 datapoints
2025-03-06 18:59:40,527 - INFO - training batch 51, loss: 0.478, 1632/60000 datapoints
2025-03-06 18:59:40,725 - INFO - training batch 101, loss: 0.412, 3232/60000 datapoints
2025-03-06 18:59:40,924 - INFO - training batch 151, loss: 0.390, 4832/60000 datapoints
2025-03-06 18:59:41,119 - INFO - training batch 201, loss: 0.421, 6432/60000 datapoints
2025-03-06 18:59:41,317 - INFO - training batch 251, loss: 0.385, 8032/60000 datapoints
2025-03-06 18:59:41,513 - INFO - training batch 301, loss: 0.234, 9632/60000 datapoints
2025-03-06 18:59:41,716 - INFO - training batch 351, loss: 0.277, 11232/60000 datapoints
2025-03-06 18:59:41,909 - INFO - training batch 401, loss: 0.495, 12832/60000 datapoints
2025-03-06 18:59:42,105 - INFO - training batch 451, loss: 0.342, 14432/60000 datapoints
2025-03-06 18:59:42,300 - INFO - training batch 501, loss: 0.479, 16032/60000 datapoints
2025-03-06 18:59:42,496 - INFO - training batch 551, loss: 0.411, 17632/60000 datapoints
2025-03-06 18:59:42,691 - INFO - training batch 601, loss: 0.346, 19232/60000 datapoints
2025-03-06 18:59:42,885 - INFO - training batch 651, loss: 0.356, 20832/60000 datapoints
2025-03-06 18:59:43,079 - INFO - training batch 701, loss: 0.425, 22432/60000 datapoints
2025-03-06 18:59:43,273 - INFO - training batch 751, loss: 0.322, 24032/60000 datapoints
2025-03-06 18:59:43,468 - INFO - training batch 801, loss: 0.522, 25632/60000 datapoints
2025-03-06 18:59:43,665 - INFO - training batch 851, loss: 0.502, 27232/60000 datapoints
2025-03-06 18:59:43,860 - INFO - training batch 901, loss: 0.247, 28832/60000 datapoints
2025-03-06 18:59:44,055 - INFO - training batch 951, loss: 0.228, 30432/60000 datapoints
2025-03-06 18:59:44,247 - INFO - training batch 1001, loss: 0.776, 32032/60000 datapoints
2025-03-06 18:59:44,441 - INFO - training batch 1051, loss: 0.817, 33632/60000 datapoints
2025-03-06 18:59:44,635 - INFO - training batch 1101, loss: 0.608, 35232/60000 datapoints
2025-03-06 18:59:44,832 - INFO - training batch 1151, loss: 0.444, 36832/60000 datapoints
2025-03-06 18:59:45,030 - INFO - training batch 1201, loss: 0.438, 38432/60000 datapoints
2025-03-06 18:59:45,226 - INFO - training batch 1251, loss: 0.395, 40032/60000 datapoints
2025-03-06 18:59:45,417 - INFO - training batch 1301, loss: 0.307, 41632/60000 datapoints
2025-03-06 18:59:45,611 - INFO - training batch 1351, loss: 0.537, 43232/60000 datapoints
2025-03-06 18:59:45,806 - INFO - training batch 1401, loss: 0.431, 44832/60000 datapoints
2025-03-06 18:59:45,999 - INFO - training batch 1451, loss: 0.741, 46432/60000 datapoints
2025-03-06 18:59:46,193 - INFO - training batch 1501, loss: 0.301, 48032/60000 datapoints
2025-03-06 18:59:46,386 - INFO - training batch 1551, loss: 0.395, 49632/60000 datapoints
2025-03-06 18:59:46,578 - INFO - training batch 1601, loss: 0.205, 51232/60000 datapoints
2025-03-06 18:59:46,774 - INFO - training batch 1651, loss: 0.332, 52832/60000 datapoints
2025-03-06 18:59:46,970 - INFO - training batch 1701, loss: 0.365, 54432/60000 datapoints
2025-03-06 18:59:47,166 - INFO - training batch 1751, loss: 0.612, 56032/60000 datapoints
2025-03-06 18:59:47,359 - INFO - training batch 1801, loss: 0.259, 57632/60000 datapoints
2025-03-06 18:59:47,551 - INFO - training batch 1851, loss: 0.382, 59232/60000 datapoints
2025-03-06 18:59:47,656 - INFO - validation batch 1, loss: 0.687, 32/10016 datapoints
2025-03-06 18:59:47,812 - INFO - validation batch 51, loss: 0.357, 1632/10016 datapoints
2025-03-06 18:59:47,965 - INFO - validation batch 101, loss: 0.516, 3232/10016 datapoints
2025-03-06 18:59:48,122 - INFO - validation batch 151, loss: 0.303, 4832/10016 datapoints
2025-03-06 18:59:48,287 - INFO - validation batch 201, loss: 0.164, 6432/10016 datapoints
2025-03-06 18:59:48,439 - INFO - validation batch 251, loss: 0.435, 8032/10016 datapoints
2025-03-06 18:59:48,591 - INFO - validation batch 301, loss: 0.762, 9632/10016 datapoints
2025-03-06 18:59:48,630 - INFO - Epoch 156/800 done.
2025-03-06 18:59:48,630 - INFO - Final validation performance:
Loss: 0.461, top-1 acc: 0.895top-5 acc: 0.895
2025-03-06 18:59:48,631 - INFO - Beginning epoch 157/800
2025-03-06 18:59:48,638 - INFO - training batch 1, loss: 0.382, 32/60000 datapoints
2025-03-06 18:59:48,834 - INFO - training batch 51, loss: 0.639, 1632/60000 datapoints
2025-03-06 18:59:49,029 - INFO - training batch 101, loss: 0.671, 3232/60000 datapoints
2025-03-06 18:59:49,227 - INFO - training batch 151, loss: 0.219, 4832/60000 datapoints
2025-03-06 18:59:49,422 - INFO - training batch 201, loss: 0.312, 6432/60000 datapoints
2025-03-06 18:59:49,622 - INFO - training batch 251, loss: 0.319, 8032/60000 datapoints
2025-03-06 18:59:49,821 - INFO - training batch 301, loss: 0.434, 9632/60000 datapoints
2025-03-06 18:59:50,016 - INFO - training batch 351, loss: 0.319, 11232/60000 datapoints
2025-03-06 18:59:50,211 - INFO - training batch 401, loss: 0.155, 12832/60000 datapoints
2025-03-06 18:59:50,410 - INFO - training batch 451, loss: 0.499, 14432/60000 datapoints
2025-03-06 18:59:50,606 - INFO - training batch 501, loss: 0.412, 16032/60000 datapoints
2025-03-06 18:59:50,799 - INFO - training batch 551, loss: 0.414, 17632/60000 datapoints
2025-03-06 18:59:50,996 - INFO - training batch 601, loss: 0.363, 19232/60000 datapoints
2025-03-06 18:59:51,189 - INFO - training batch 651, loss: 0.412, 20832/60000 datapoints
2025-03-06 18:59:51,386 - INFO - training batch 701, loss: 0.189, 22432/60000 datapoints
2025-03-06 18:59:51,580 - INFO - training batch 751, loss: 0.700, 24032/60000 datapoints
2025-03-06 18:59:51,778 - INFO - training batch 801, loss: 0.399, 25632/60000 datapoints
2025-03-06 18:59:51,972 - INFO - training batch 851, loss: 0.289, 27232/60000 datapoints
2025-03-06 18:59:52,166 - INFO - training batch 901, loss: 0.211, 28832/60000 datapoints
2025-03-06 18:59:52,360 - INFO - training batch 951, loss: 0.397, 30432/60000 datapoints
2025-03-06 18:59:52,557 - INFO - training batch 1001, loss: 0.416, 32032/60000 datapoints
2025-03-06 18:59:52,753 - INFO - training batch 1051, loss: 0.299, 33632/60000 datapoints
2025-03-06 18:59:52,947 - INFO - training batch 1101, loss: 0.310, 35232/60000 datapoints
2025-03-06 18:59:53,141 - INFO - training batch 1151, loss: 0.271, 36832/60000 datapoints
2025-03-06 18:59:53,337 - INFO - training batch 1201, loss: 0.325, 38432/60000 datapoints
2025-03-06 18:59:53,529 - INFO - training batch 1251, loss: 0.519, 40032/60000 datapoints
2025-03-06 18:59:53,727 - INFO - training batch 1301, loss: 0.466, 41632/60000 datapoints
2025-03-06 18:59:53,919 - INFO - training batch 1351, loss: 0.350, 43232/60000 datapoints
2025-03-06 18:59:54,111 - INFO - training batch 1401, loss: 0.416, 44832/60000 datapoints
2025-03-06 18:59:54,305 - INFO - training batch 1451, loss: 0.605, 46432/60000 datapoints
2025-03-06 18:59:54,499 - INFO - training batch 1501, loss: 0.509, 48032/60000 datapoints
2025-03-06 18:59:54,693 - INFO - training batch 1551, loss: 0.358, 49632/60000 datapoints
2025-03-06 18:59:54,892 - INFO - training batch 1601, loss: 0.605, 51232/60000 datapoints
2025-03-06 18:59:55,086 - INFO - training batch 1651, loss: 0.510, 52832/60000 datapoints
2025-03-06 18:59:55,279 - INFO - training batch 1701, loss: 0.325, 54432/60000 datapoints
2025-03-06 18:59:55,473 - INFO - training batch 1751, loss: 0.620, 56032/60000 datapoints
2025-03-06 18:59:55,674 - INFO - training batch 1801, loss: 0.423, 57632/60000 datapoints
2025-03-06 18:59:55,865 - INFO - training batch 1851, loss: 0.374, 59232/60000 datapoints
2025-03-06 18:59:55,966 - INFO - validation batch 1, loss: 0.453, 32/10016 datapoints
2025-03-06 18:59:56,120 - INFO - validation batch 51, loss: 0.397, 1632/10016 datapoints
2025-03-06 18:59:56,274 - INFO - validation batch 101, loss: 0.213, 3232/10016 datapoints
2025-03-06 18:59:56,427 - INFO - validation batch 151, loss: 0.569, 4832/10016 datapoints
2025-03-06 18:59:56,580 - INFO - validation batch 201, loss: 0.551, 6432/10016 datapoints
2025-03-06 18:59:56,735 - INFO - validation batch 251, loss: 0.297, 8032/10016 datapoints
2025-03-06 18:59:56,887 - INFO - validation batch 301, loss: 0.288, 9632/10016 datapoints
2025-03-06 18:59:56,925 - INFO - Epoch 157/800 done.
2025-03-06 18:59:56,926 - INFO - Final validation performance:
Loss: 0.395, top-1 acc: 0.895top-5 acc: 0.895
2025-03-06 18:59:56,926 - INFO - Beginning epoch 158/800
2025-03-06 18:59:56,932 - INFO - training batch 1, loss: 0.415, 32/60000 datapoints
2025-03-06 18:59:57,129 - INFO - training batch 51, loss: 0.475, 1632/60000 datapoints
2025-03-06 18:59:57,324 - INFO - training batch 101, loss: 0.523, 3232/60000 datapoints
2025-03-06 18:59:57,525 - INFO - training batch 151, loss: 0.265, 4832/60000 datapoints
2025-03-06 18:59:57,729 - INFO - training batch 201, loss: 0.565, 6432/60000 datapoints
2025-03-06 18:59:57,923 - INFO - training batch 251, loss: 0.156, 8032/60000 datapoints
2025-03-06 18:59:58,123 - INFO - training batch 301, loss: 0.392, 9632/60000 datapoints
2025-03-06 18:59:58,338 - INFO - training batch 351, loss: 0.225, 11232/60000 datapoints
2025-03-06 18:59:58,538 - INFO - training batch 401, loss: 0.456, 12832/60000 datapoints
2025-03-06 18:59:58,734 - INFO - training batch 451, loss: 0.251, 14432/60000 datapoints
2025-03-06 18:59:58,928 - INFO - training batch 501, loss: 0.527, 16032/60000 datapoints
2025-03-06 18:59:59,120 - INFO - training batch 551, loss: 0.429, 17632/60000 datapoints
2025-03-06 18:59:59,313 - INFO - training batch 601, loss: 0.406, 19232/60000 datapoints
2025-03-06 18:59:59,507 - INFO - training batch 651, loss: 0.323, 20832/60000 datapoints
2025-03-06 18:59:59,706 - INFO - training batch 701, loss: 0.582, 22432/60000 datapoints
2025-03-06 18:59:59,899 - INFO - training batch 751, loss: 0.331, 24032/60000 datapoints
2025-03-06 19:00:00,097 - INFO - training batch 801, loss: 0.474, 25632/60000 datapoints
2025-03-06 19:00:00,295 - INFO - training batch 851, loss: 0.367, 27232/60000 datapoints
2025-03-06 19:00:00,487 - INFO - training batch 901, loss: 0.410, 28832/60000 datapoints
2025-03-06 19:00:00,683 - INFO - training batch 951, loss: 0.425, 30432/60000 datapoints
2025-03-06 19:00:00,876 - INFO - training batch 1001, loss: 0.431, 32032/60000 datapoints
2025-03-06 19:00:01,070 - INFO - training batch 1051, loss: 0.297, 33632/60000 datapoints
2025-03-06 19:00:01,263 - INFO - training batch 1101, loss: 0.419, 35232/60000 datapoints
2025-03-06 19:00:01,455 - INFO - training batch 1151, loss: 0.890, 36832/60000 datapoints
2025-03-06 19:00:01,651 - INFO - training batch 1201, loss: 0.553, 38432/60000 datapoints
2025-03-06 19:00:01,848 - INFO - training batch 1251, loss: 0.305, 40032/60000 datapoints
2025-03-06 19:00:02,042 - INFO - training batch 1301, loss: 0.218, 41632/60000 datapoints
2025-03-06 19:00:02,235 - INFO - training batch 1351, loss: 0.251, 43232/60000 datapoints
2025-03-06 19:00:02,429 - INFO - training batch 1401, loss: 0.283, 44832/60000 datapoints
2025-03-06 19:00:02,623 - INFO - training batch 1451, loss: 0.298, 46432/60000 datapoints
2025-03-06 19:00:02,818 - INFO - training batch 1501, loss: 0.459, 48032/60000 datapoints
2025-03-06 19:00:03,011 - INFO - training batch 1551, loss: 0.337, 49632/60000 datapoints
2025-03-06 19:00:03,204 - INFO - training batch 1601, loss: 0.575, 51232/60000 datapoints
2025-03-06 19:00:03,405 - INFO - training batch 1651, loss: 0.334, 52832/60000 datapoints
2025-03-06 19:00:03,599 - INFO - training batch 1701, loss: 0.412, 54432/60000 datapoints
2025-03-06 19:00:03,798 - INFO - training batch 1751, loss: 0.450, 56032/60000 datapoints
2025-03-06 19:00:03,991 - INFO - training batch 1801, loss: 0.422, 57632/60000 datapoints
2025-03-06 19:00:04,190 - INFO - training batch 1851, loss: 0.360, 59232/60000 datapoints
2025-03-06 19:00:04,291 - INFO - validation batch 1, loss: 0.263, 32/10016 datapoints
2025-03-06 19:00:04,444 - INFO - validation batch 51, loss: 0.319, 1632/10016 datapoints
2025-03-06 19:00:04,596 - INFO - validation batch 101, loss: 0.368, 3232/10016 datapoints
2025-03-06 19:00:04,752 - INFO - validation batch 151, loss: 0.439, 4832/10016 datapoints
2025-03-06 19:00:04,911 - INFO - validation batch 201, loss: 0.324, 6432/10016 datapoints
2025-03-06 19:00:05,062 - INFO - validation batch 251, loss: 0.397, 8032/10016 datapoints
2025-03-06 19:00:05,213 - INFO - validation batch 301, loss: 0.428, 9632/10016 datapoints
2025-03-06 19:00:05,252 - INFO - Epoch 158/800 done.
2025-03-06 19:00:05,252 - INFO - Final validation performance:
Loss: 0.363, top-1 acc: 0.895top-5 acc: 0.895
2025-03-06 19:00:05,253 - INFO - Beginning epoch 159/800
2025-03-06 19:00:05,259 - INFO - training batch 1, loss: 0.426, 32/60000 datapoints
2025-03-06 19:00:05,453 - INFO - training batch 51, loss: 0.342, 1632/60000 datapoints
2025-03-06 19:00:05,659 - INFO - training batch 101, loss: 0.241, 3232/60000 datapoints
2025-03-06 19:00:05,872 - INFO - training batch 151, loss: 0.413, 4832/60000 datapoints
2025-03-06 19:00:06,063 - INFO - training batch 201, loss: 0.278, 6432/60000 datapoints
2025-03-06 19:00:06,258 - INFO - training batch 251, loss: 0.542, 8032/60000 datapoints
2025-03-06 19:00:06,456 - INFO - training batch 301, loss: 0.428, 9632/60000 datapoints
2025-03-06 19:00:06,653 - INFO - training batch 351, loss: 0.480, 11232/60000 datapoints
2025-03-06 19:00:06,849 - INFO - training batch 401, loss: 0.408, 12832/60000 datapoints
2025-03-06 19:00:07,038 - INFO - training batch 451, loss: 0.362, 14432/60000 datapoints
2025-03-06 19:00:07,230 - INFO - training batch 501, loss: 0.470, 16032/60000 datapoints
2025-03-06 19:00:07,421 - INFO - training batch 551, loss: 0.341, 17632/60000 datapoints
2025-03-06 19:00:07,615 - INFO - training batch 601, loss: 0.490, 19232/60000 datapoints
2025-03-06 19:00:07,812 - INFO - training batch 651, loss: 0.284, 20832/60000 datapoints
2025-03-06 19:00:08,003 - INFO - training batch 701, loss: 0.264, 22432/60000 datapoints
2025-03-06 19:00:08,191 - INFO - training batch 751, loss: 0.497, 24032/60000 datapoints
2025-03-06 19:00:08,405 - INFO - training batch 801, loss: 0.640, 25632/60000 datapoints
2025-03-06 19:00:08,596 - INFO - training batch 851, loss: 0.335, 27232/60000 datapoints
2025-03-06 19:00:08,788 - INFO - training batch 901, loss: 0.256, 28832/60000 datapoints
2025-03-06 19:00:08,978 - INFO - training batch 951, loss: 0.247, 30432/60000 datapoints
2025-03-06 19:00:09,169 - INFO - training batch 1001, loss: 0.213, 32032/60000 datapoints
2025-03-06 19:00:09,362 - INFO - training batch 1051, loss: 0.285, 33632/60000 datapoints
2025-03-06 19:00:09,554 - INFO - training batch 1101, loss: 0.302, 35232/60000 datapoints
2025-03-06 19:00:09,749 - INFO - training batch 1151, loss: 0.367, 36832/60000 datapoints
2025-03-06 19:00:09,939 - INFO - training batch 1201, loss: 0.165, 38432/60000 datapoints
2025-03-06 19:00:10,132 - INFO - training batch 1251, loss: 0.340, 40032/60000 datapoints
2025-03-06 19:00:10,321 - INFO - training batch 1301, loss: 0.411, 41632/60000 datapoints
2025-03-06 19:00:10,513 - INFO - training batch 1351, loss: 0.315, 43232/60000 datapoints
2025-03-06 19:00:10,707 - INFO - training batch 1401, loss: 0.700, 44832/60000 datapoints
2025-03-06 19:00:10,899 - INFO - training batch 1451, loss: 0.318, 46432/60000 datapoints
2025-03-06 19:00:11,089 - INFO - training batch 1501, loss: 0.300, 48032/60000 datapoints
2025-03-06 19:00:11,280 - INFO - training batch 1551, loss: 0.364, 49632/60000 datapoints
2025-03-06 19:00:11,469 - INFO - training batch 1601, loss: 0.387, 51232/60000 datapoints
2025-03-06 19:00:11,663 - INFO - training batch 1651, loss: 0.338, 52832/60000 datapoints
2025-03-06 19:00:11,859 - INFO - training batch 1701, loss: 0.283, 54432/60000 datapoints
2025-03-06 19:00:12,048 - INFO - training batch 1751, loss: 0.382, 56032/60000 datapoints
2025-03-06 19:00:12,236 - INFO - training batch 1801, loss: 0.325, 57632/60000 datapoints
2025-03-06 19:00:12,427 - INFO - training batch 1851, loss: 0.525, 59232/60000 datapoints
2025-03-06 19:00:12,525 - INFO - validation batch 1, loss: 0.320, 32/10016 datapoints
2025-03-06 19:00:12,675 - INFO - validation batch 51, loss: 0.425, 1632/10016 datapoints
2025-03-06 19:00:12,827 - INFO - validation batch 101, loss: 0.371, 3232/10016 datapoints
2025-03-06 19:00:12,975 - INFO - validation batch 151, loss: 0.212, 4832/10016 datapoints
2025-03-06 19:00:13,123 - INFO - validation batch 201, loss: 0.238, 6432/10016 datapoints
2025-03-06 19:00:13,273 - INFO - validation batch 251, loss: 0.425, 8032/10016 datapoints
2025-03-06 19:00:13,425 - INFO - validation batch 301, loss: 0.260, 9632/10016 datapoints
2025-03-06 19:00:13,460 - INFO - Epoch 159/800 done.
2025-03-06 19:00:13,460 - INFO - Final validation performance:
Loss: 0.322, top-1 acc: 0.895top-5 acc: 0.895
2025-03-06 19:00:13,461 - INFO - Beginning epoch 160/800
2025-03-06 19:00:13,467 - INFO - training batch 1, loss: 0.444, 32/60000 datapoints
2025-03-06 19:00:13,673 - INFO - training batch 51, loss: 0.348, 1632/60000 datapoints
2025-03-06 19:00:13,867 - INFO - training batch 101, loss: 0.252, 3232/60000 datapoints
2025-03-06 19:00:14,064 - INFO - training batch 151, loss: 0.428, 4832/60000 datapoints
2025-03-06 19:00:14,256 - INFO - training batch 201, loss: 0.489, 6432/60000 datapoints
2025-03-06 19:00:14,451 - INFO - training batch 251, loss: 0.263, 8032/60000 datapoints
2025-03-06 19:00:14,644 - INFO - training batch 301, loss: 0.287, 9632/60000 datapoints
2025-03-06 19:00:14,833 - INFO - training batch 351, loss: 0.335, 11232/60000 datapoints
2025-03-06 19:00:15,029 - INFO - training batch 401, loss: 0.449, 12832/60000 datapoints
2025-03-06 19:00:15,220 - INFO - training batch 451, loss: 0.222, 14432/60000 datapoints
2025-03-06 19:00:15,414 - INFO - training batch 501, loss: 0.288, 16032/60000 datapoints
2025-03-06 19:00:15,606 - INFO - training batch 551, loss: 0.489, 17632/60000 datapoints
2025-03-06 19:00:15,801 - INFO - training batch 601, loss: 0.369, 19232/60000 datapoints
2025-03-06 19:00:15,994 - INFO - training batch 651, loss: 0.890, 20832/60000 datapoints
2025-03-06 19:00:16,190 - INFO - training batch 701, loss: 0.381, 22432/60000 datapoints
2025-03-06 19:00:16,386 - INFO - training batch 751, loss: 0.159, 24032/60000 datapoints
2025-03-06 19:00:16,579 - INFO - training batch 801, loss: 0.275, 25632/60000 datapoints
2025-03-06 19:00:16,776 - INFO - training batch 851, loss: 0.315, 27232/60000 datapoints
2025-03-06 19:00:16,974 - INFO - training batch 901, loss: 0.865, 28832/60000 datapoints
2025-03-06 19:00:17,166 - INFO - training batch 951, loss: 0.529, 30432/60000 datapoints
2025-03-06 19:00:17,363 - INFO - training batch 1001, loss: 0.350, 32032/60000 datapoints
2025-03-06 19:00:17,556 - INFO - training batch 1051, loss: 0.351, 33632/60000 datapoints
2025-03-06 19:00:17,760 - INFO - training batch 1101, loss: 0.329, 35232/60000 datapoints
2025-03-06 19:00:17,954 - INFO - training batch 1151, loss: 0.468, 36832/60000 datapoints
2025-03-06 19:00:18,147 - INFO - training batch 1201, loss: 0.176, 38432/60000 datapoints
2025-03-06 19:00:18,341 - INFO - training batch 1251, loss: 0.456, 40032/60000 datapoints
2025-03-06 19:00:18,555 - INFO - training batch 1301, loss: 0.240, 41632/60000 datapoints
2025-03-06 19:00:18,771 - INFO - training batch 1351, loss: 0.426, 43232/60000 datapoints
2025-03-06 19:00:18,967 - INFO - training batch 1401, loss: 0.308, 44832/60000 datapoints
2025-03-06 19:00:19,160 - INFO - training batch 1451, loss: 0.453, 46432/60000 datapoints
2025-03-06 19:00:19,351 - INFO - training batch 1501, loss: 0.259, 48032/60000 datapoints
2025-03-06 19:00:19,546 - INFO - training batch 1551, loss: 0.324, 49632/60000 datapoints
2025-03-06 19:00:19,742 - INFO - training batch 1601, loss: 0.425, 51232/60000 datapoints
2025-03-06 19:00:19,937 - INFO - training batch 1651, loss: 0.226, 52832/60000 datapoints
2025-03-06 19:00:20,130 - INFO - training batch 1701, loss: 0.494, 54432/60000 datapoints
2025-03-06 19:00:20,320 - INFO - training batch 1751, loss: 0.420, 56032/60000 datapoints
2025-03-06 19:00:20,514 - INFO - training batch 1801, loss: 0.464, 57632/60000 datapoints
2025-03-06 19:00:20,709 - INFO - training batch 1851, loss: 0.260, 59232/60000 datapoints
2025-03-06 19:00:20,810 - INFO - validation batch 1, loss: 0.279, 32/10016 datapoints
2025-03-06 19:00:20,965 - INFO - validation batch 51, loss: 0.228, 1632/10016 datapoints
2025-03-06 19:00:21,117 - INFO - validation batch 101, loss: 0.517, 3232/10016 datapoints
2025-03-06 19:00:21,269 - INFO - validation batch 151, loss: 0.220, 4832/10016 datapoints
2025-03-06 19:00:21,424 - INFO - validation batch 201, loss: 0.330, 6432/10016 datapoints
2025-03-06 19:00:21,575 - INFO - validation batch 251, loss: 0.335, 8032/10016 datapoints
2025-03-06 19:00:21,730 - INFO - validation batch 301, loss: 0.481, 9632/10016 datapoints
2025-03-06 19:00:21,770 - INFO - Epoch 160/800 done.
2025-03-06 19:00:21,770 - INFO - Final validation performance:
Loss: 0.341, top-1 acc: 0.896top-5 acc: 0.896
2025-03-06 19:00:21,771 - INFO - Beginning epoch 161/800
2025-03-06 19:00:21,777 - INFO - training batch 1, loss: 0.329, 32/60000 datapoints
2025-03-06 19:00:21,981 - INFO - training batch 51, loss: 0.432, 1632/60000 datapoints
2025-03-06 19:00:22,177 - INFO - training batch 101, loss: 0.407, 3232/60000 datapoints
2025-03-06 19:00:22,375 - INFO - training batch 151, loss: 0.444, 4832/60000 datapoints
2025-03-06 19:00:22,573 - INFO - training batch 201, loss: 0.450, 6432/60000 datapoints
2025-03-06 19:00:22,771 - INFO - training batch 251, loss: 0.427, 8032/60000 datapoints
2025-03-06 19:00:22,969 - INFO - training batch 301, loss: 0.420, 9632/60000 datapoints
2025-03-06 19:00:23,164 - INFO - training batch 351, loss: 0.671, 11232/60000 datapoints
2025-03-06 19:00:23,358 - INFO - training batch 401, loss: 0.289, 12832/60000 datapoints
2025-03-06 19:00:23,552 - INFO - training batch 451, loss: 0.217, 14432/60000 datapoints
2025-03-06 19:00:23,748 - INFO - training batch 501, loss: 0.718, 16032/60000 datapoints
2025-03-06 19:00:23,945 - INFO - training batch 551, loss: 0.278, 17632/60000 datapoints
2025-03-06 19:00:24,139 - INFO - training batch 601, loss: 0.244, 19232/60000 datapoints
2025-03-06 19:00:24,332 - INFO - training batch 651, loss: 0.335, 20832/60000 datapoints
2025-03-06 19:00:24,527 - INFO - training batch 701, loss: 0.271, 22432/60000 datapoints
2025-03-06 19:00:24,723 - INFO - training batch 751, loss: 0.408, 24032/60000 datapoints
2025-03-06 19:00:24,918 - INFO - training batch 801, loss: 0.264, 25632/60000 datapoints
2025-03-06 19:00:25,112 - INFO - training batch 851, loss: 0.383, 27232/60000 datapoints
2025-03-06 19:00:25,306 - INFO - training batch 901, loss: 0.344, 28832/60000 datapoints
2025-03-06 19:00:25,498 - INFO - training batch 951, loss: 0.357, 30432/60000 datapoints
2025-03-06 19:00:25,697 - INFO - training batch 1001, loss: 0.337, 32032/60000 datapoints
2025-03-06 19:00:25,893 - INFO - training batch 1051, loss: 0.244, 33632/60000 datapoints
2025-03-06 19:00:26,086 - INFO - training batch 1101, loss: 0.481, 35232/60000 datapoints
2025-03-06 19:00:26,280 - INFO - training batch 1151, loss: 0.316, 36832/60000 datapoints
2025-03-06 19:00:26,473 - INFO - training batch 1201, loss: 0.503, 38432/60000 datapoints
2025-03-06 19:00:26,674 - INFO - training batch 1251, loss: 0.311, 40032/60000 datapoints
2025-03-06 19:00:26,908 - INFO - training batch 1301, loss: 0.340, 41632/60000 datapoints
2025-03-06 19:00:27,101 - INFO - training batch 1351, loss: 0.487, 43232/60000 datapoints
2025-03-06 19:00:27,293 - INFO - training batch 1401, loss: 0.422, 44832/60000 datapoints
2025-03-06 19:00:27,487 - INFO - training batch 1451, loss: 0.296, 46432/60000 datapoints
2025-03-06 19:00:27,681 - INFO - training batch 1501, loss: 0.347, 48032/60000 datapoints
2025-03-06 19:00:27,877 - INFO - training batch 1551, loss: 0.377, 49632/60000 datapoints
2025-03-06 19:00:28,084 - INFO - training batch 1601, loss: 0.287, 51232/60000 datapoints
2025-03-06 19:00:28,280 - INFO - training batch 1651, loss: 0.346, 52832/60000 datapoints
2025-03-06 19:00:28,490 - INFO - training batch 1701, loss: 0.332, 54432/60000 datapoints
2025-03-06 19:00:28,690 - INFO - training batch 1751, loss: 0.149, 56032/60000 datapoints
2025-03-06 19:00:28,882 - INFO - training batch 1801, loss: 0.291, 57632/60000 datapoints
2025-03-06 19:00:29,074 - INFO - training batch 1851, loss: 0.524, 59232/60000 datapoints
2025-03-06 19:00:29,174 - INFO - validation batch 1, loss: 0.320, 32/10016 datapoints
2025-03-06 19:00:29,325 - INFO - validation batch 51, loss: 0.398, 1632/10016 datapoints
2025-03-06 19:00:29,478 - INFO - validation batch 101, loss: 0.340, 3232/10016 datapoints
2025-03-06 19:00:29,634 - INFO - validation batch 151, loss: 0.618, 4832/10016 datapoints
2025-03-06 19:00:29,786 - INFO - validation batch 201, loss: 0.252, 6432/10016 datapoints
2025-03-06 19:00:29,942 - INFO - validation batch 251, loss: 0.420, 8032/10016 datapoints
2025-03-06 19:00:30,095 - INFO - validation batch 301, loss: 0.555, 9632/10016 datapoints
2025-03-06 19:00:30,131 - INFO - Epoch 161/800 done.
2025-03-06 19:00:30,131 - INFO - Final validation performance:
Loss: 0.415, top-1 acc: 0.896top-5 acc: 0.896
2025-03-06 19:00:30,132 - INFO - Beginning epoch 162/800
2025-03-06 19:00:30,138 - INFO - training batch 1, loss: 0.262, 32/60000 datapoints
2025-03-06 19:00:30,337 - INFO - training batch 51, loss: 0.755, 1632/60000 datapoints
2025-03-06 19:00:30,536 - INFO - training batch 101, loss: 0.394, 3232/60000 datapoints
2025-03-06 19:00:30,747 - INFO - training batch 151, loss: 0.442, 4832/60000 datapoints
2025-03-06 19:00:30,942 - INFO - training batch 201, loss: 0.440, 6432/60000 datapoints
2025-03-06 19:00:31,137 - INFO - training batch 251, loss: 0.298, 8032/60000 datapoints
2025-03-06 19:00:31,336 - INFO - training batch 301, loss: 0.296, 9632/60000 datapoints
2025-03-06 19:00:31,528 - INFO - training batch 351, loss: 0.235, 11232/60000 datapoints
2025-03-06 19:00:31,728 - INFO - training batch 401, loss: 0.344, 12832/60000 datapoints
2025-03-06 19:00:31,925 - INFO - training batch 451, loss: 0.484, 14432/60000 datapoints
2025-03-06 19:00:32,118 - INFO - training batch 501, loss: 0.295, 16032/60000 datapoints
2025-03-06 19:00:32,314 - INFO - training batch 551, loss: 0.218, 17632/60000 datapoints
2025-03-06 19:00:32,508 - INFO - training batch 601, loss: 0.363, 19232/60000 datapoints
2025-03-06 19:00:32,705 - INFO - training batch 651, loss: 0.336, 20832/60000 datapoints
2025-03-06 19:00:32,898 - INFO - training batch 701, loss: 0.345, 22432/60000 datapoints
2025-03-06 19:00:33,092 - INFO - training batch 751, loss: 0.229, 24032/60000 datapoints
2025-03-06 19:00:33,287 - INFO - training batch 801, loss: 0.319, 25632/60000 datapoints
2025-03-06 19:00:33,482 - INFO - training batch 851, loss: 0.502, 27232/60000 datapoints
2025-03-06 19:00:33,678 - INFO - training batch 901, loss: 0.587, 28832/60000 datapoints
2025-03-06 19:00:33,878 - INFO - training batch 951, loss: 0.182, 30432/60000 datapoints
2025-03-06 19:00:34,072 - INFO - training batch 1001, loss: 0.294, 32032/60000 datapoints
2025-03-06 19:00:34,262 - INFO - training batch 1051, loss: 0.503, 33632/60000 datapoints
2025-03-06 19:00:34,453 - INFO - training batch 1101, loss: 0.439, 35232/60000 datapoints
2025-03-06 19:00:34,654 - INFO - training batch 1151, loss: 0.224, 36832/60000 datapoints
2025-03-06 19:00:34,847 - INFO - training batch 1201, loss: 0.525, 38432/60000 datapoints
2025-03-06 19:00:35,048 - INFO - training batch 1251, loss: 0.622, 40032/60000 datapoints
2025-03-06 19:00:35,242 - INFO - training batch 1301, loss: 0.668, 41632/60000 datapoints
2025-03-06 19:00:35,435 - INFO - training batch 1351, loss: 0.540, 43232/60000 datapoints
2025-03-06 19:00:35,631 - INFO - training batch 1401, loss: 0.296, 44832/60000 datapoints
2025-03-06 19:00:35,828 - INFO - training batch 1451, loss: 0.545, 46432/60000 datapoints
2025-03-06 19:00:36,024 - INFO - training batch 1501, loss: 0.389, 48032/60000 datapoints
2025-03-06 19:00:36,222 - INFO - training batch 1551, loss: 0.356, 49632/60000 datapoints
2025-03-06 19:00:36,414 - INFO - training batch 1601, loss: 0.147, 51232/60000 datapoints
2025-03-06 19:00:36,605 - INFO - training batch 1651, loss: 0.372, 52832/60000 datapoints
2025-03-06 19:00:36,799 - INFO - training batch 1701, loss: 0.388, 54432/60000 datapoints
2025-03-06 19:00:36,991 - INFO - training batch 1751, loss: 0.357, 56032/60000 datapoints
2025-03-06 19:00:37,185 - INFO - training batch 1801, loss: 0.279, 57632/60000 datapoints
2025-03-06 19:00:37,376 - INFO - training batch 1851, loss: 0.501, 59232/60000 datapoints
2025-03-06 19:00:37,476 - INFO - validation batch 1, loss: 0.354, 32/10016 datapoints
2025-03-06 19:00:37,648 - INFO - validation batch 51, loss: 0.284, 1632/10016 datapoints
2025-03-06 19:00:37,803 - INFO - validation batch 101, loss: 0.391, 3232/10016 datapoints
2025-03-06 19:00:37,959 - INFO - validation batch 151, loss: 0.372, 4832/10016 datapoints
2025-03-06 19:00:38,111 - INFO - validation batch 201, loss: 0.381, 6432/10016 datapoints
2025-03-06 19:00:38,262 - INFO - validation batch 251, loss: 0.555, 8032/10016 datapoints
2025-03-06 19:00:38,414 - INFO - validation batch 301, loss: 0.381, 9632/10016 datapoints
2025-03-06 19:00:38,451 - INFO - Epoch 162/800 done.
2025-03-06 19:00:38,451 - INFO - Final validation performance:
Loss: 0.389, top-1 acc: 0.896top-5 acc: 0.896
2025-03-06 19:00:38,452 - INFO - Beginning epoch 163/800
2025-03-06 19:00:38,459 - INFO - training batch 1, loss: 0.520, 32/60000 datapoints
2025-03-06 19:00:38,681 - INFO - training batch 51, loss: 0.750, 1632/60000 datapoints
2025-03-06 19:00:38,872 - INFO - training batch 101, loss: 0.382, 3232/60000 datapoints
2025-03-06 19:00:39,077 - INFO - training batch 151, loss: 0.349, 4832/60000 datapoints
2025-03-06 19:00:39,271 - INFO - training batch 201, loss: 0.365, 6432/60000 datapoints
2025-03-06 19:00:39,466 - INFO - training batch 251, loss: 0.325, 8032/60000 datapoints
2025-03-06 19:00:39,660 - INFO - training batch 301, loss: 0.462, 9632/60000 datapoints
2025-03-06 19:00:39,854 - INFO - training batch 351, loss: 0.158, 11232/60000 datapoints
2025-03-06 19:00:40,046 - INFO - training batch 401, loss: 0.274, 12832/60000 datapoints
2025-03-06 19:00:40,238 - INFO - training batch 451, loss: 0.148, 14432/60000 datapoints
2025-03-06 19:00:40,429 - INFO - training batch 501, loss: 0.437, 16032/60000 datapoints
2025-03-06 19:00:40,622 - INFO - training batch 551, loss: 0.547, 17632/60000 datapoints
2025-03-06 19:00:40,814 - INFO - training batch 601, loss: 0.195, 19232/60000 datapoints
2025-03-06 19:00:41,005 - INFO - training batch 651, loss: 0.303, 20832/60000 datapoints
2025-03-06 19:00:41,196 - INFO - training batch 701, loss: 0.547, 22432/60000 datapoints
2025-03-06 19:00:41,385 - INFO - training batch 751, loss: 0.566, 24032/60000 datapoints
2025-03-06 19:00:41,576 - INFO - training batch 801, loss: 0.377, 25632/60000 datapoints
2025-03-06 19:00:41,772 - INFO - training batch 851, loss: 0.514, 27232/60000 datapoints
2025-03-06 19:00:41,967 - INFO - training batch 901, loss: 0.401, 28832/60000 datapoints
2025-03-06 19:00:42,157 - INFO - training batch 951, loss: 0.439, 30432/60000 datapoints
2025-03-06 19:00:42,348 - INFO - training batch 1001, loss: 0.477, 32032/60000 datapoints
2025-03-06 19:00:42,539 - INFO - training batch 1051, loss: 0.653, 33632/60000 datapoints
2025-03-06 19:00:42,732 - INFO - training batch 1101, loss: 0.294, 35232/60000 datapoints
2025-03-06 19:00:42,923 - INFO - training batch 1151, loss: 0.357, 36832/60000 datapoints
2025-03-06 19:00:43,116 - INFO - training batch 1201, loss: 0.296, 38432/60000 datapoints
2025-03-06 19:00:43,304 - INFO - training batch 1251, loss: 0.324, 40032/60000 datapoints
2025-03-06 19:00:43,495 - INFO - training batch 1301, loss: 0.254, 41632/60000 datapoints
2025-03-06 19:00:43,689 - INFO - training batch 1351, loss: 0.398, 43232/60000 datapoints
2025-03-06 19:00:43,881 - INFO - training batch 1401, loss: 0.152, 44832/60000 datapoints
2025-03-06 19:00:44,074 - INFO - training batch 1451, loss: 0.178, 46432/60000 datapoints
2025-03-06 19:00:44,264 - INFO - training batch 1501, loss: 0.522, 48032/60000 datapoints
2025-03-06 19:00:44,454 - INFO - training batch 1551, loss: 0.478, 49632/60000 datapoints
2025-03-06 19:00:44,648 - INFO - training batch 1601, loss: 0.218, 51232/60000 datapoints
2025-03-06 19:00:44,838 - INFO - training batch 1651, loss: 0.286, 52832/60000 datapoints
2025-03-06 19:00:45,033 - INFO - training batch 1701, loss: 0.311, 54432/60000 datapoints
2025-03-06 19:00:45,225 - INFO - training batch 1751, loss: 0.281, 56032/60000 datapoints
2025-03-06 19:00:45,414 - INFO - training batch 1801, loss: 0.265, 57632/60000 datapoints
2025-03-06 19:00:45,606 - INFO - training batch 1851, loss: 0.358, 59232/60000 datapoints
2025-03-06 19:00:45,707 - INFO - validation batch 1, loss: 0.375, 32/10016 datapoints
2025-03-06 19:00:45,863 - INFO - validation batch 51, loss: 0.332, 1632/10016 datapoints
2025-03-06 19:00:46,013 - INFO - validation batch 101, loss: 0.382, 3232/10016 datapoints
2025-03-06 19:00:46,168 - INFO - validation batch 151, loss: 0.325, 4832/10016 datapoints
2025-03-06 19:00:46,319 - INFO - validation batch 201, loss: 0.260, 6432/10016 datapoints
2025-03-06 19:00:46,468 - INFO - validation batch 251, loss: 0.501, 8032/10016 datapoints
2025-03-06 19:00:46,623 - INFO - validation batch 301, loss: 0.439, 9632/10016 datapoints
2025-03-06 19:00:46,658 - INFO - Epoch 163/800 done.
2025-03-06 19:00:46,658 - INFO - Final validation performance:
Loss: 0.374, top-1 acc: 0.896top-5 acc: 0.896
2025-03-06 19:00:46,659 - INFO - Beginning epoch 164/800
2025-03-06 19:00:46,665 - INFO - training batch 1, loss: 0.416, 32/60000 datapoints
2025-03-06 19:00:46,867 - INFO - training batch 51, loss: 0.463, 1632/60000 datapoints
2025-03-06 19:00:47,058 - INFO - training batch 101, loss: 0.486, 3232/60000 datapoints
2025-03-06 19:00:47,255 - INFO - training batch 151, loss: 0.377, 4832/60000 datapoints
2025-03-06 19:00:47,450 - INFO - training batch 201, loss: 0.272, 6432/60000 datapoints
2025-03-06 19:00:47,647 - INFO - training batch 251, loss: 0.352, 8032/60000 datapoints
2025-03-06 19:00:47,844 - INFO - training batch 301, loss: 0.641, 9632/60000 datapoints
2025-03-06 19:00:48,039 - INFO - training batch 351, loss: 0.415, 11232/60000 datapoints
2025-03-06 19:00:48,231 - INFO - training batch 401, loss: 0.342, 12832/60000 datapoints
2025-03-06 19:00:48,428 - INFO - training batch 451, loss: 0.219, 14432/60000 datapoints
2025-03-06 19:00:48,629 - INFO - training batch 501, loss: 0.308, 16032/60000 datapoints
2025-03-06 19:00:48,834 - INFO - training batch 551, loss: 0.587, 17632/60000 datapoints
2025-03-06 19:00:49,025 - INFO - training batch 601, loss: 0.255, 19232/60000 datapoints
2025-03-06 19:00:49,218 - INFO - training batch 651, loss: 0.364, 20832/60000 datapoints
2025-03-06 19:00:49,408 - INFO - training batch 701, loss: 0.365, 22432/60000 datapoints
2025-03-06 19:00:49,601 - INFO - training batch 751, loss: 0.617, 24032/60000 datapoints
2025-03-06 19:00:49,796 - INFO - training batch 801, loss: 0.385, 25632/60000 datapoints
2025-03-06 19:00:49,989 - INFO - training batch 851, loss: 0.392, 27232/60000 datapoints
2025-03-06 19:00:50,180 - INFO - training batch 901, loss: 0.388, 28832/60000 datapoints
2025-03-06 19:00:50,371 - INFO - training batch 951, loss: 0.351, 30432/60000 datapoints
2025-03-06 19:00:50,562 - INFO - training batch 1001, loss: 0.535, 32032/60000 datapoints
2025-03-06 19:00:50,755 - INFO - training batch 1051, loss: 0.616, 33632/60000 datapoints
2025-03-06 19:00:50,948 - INFO - training batch 1101, loss: 0.386, 35232/60000 datapoints
2025-03-06 19:00:51,141 - INFO - training batch 1151, loss: 0.424, 36832/60000 datapoints
2025-03-06 19:00:51,331 - INFO - training batch 1201, loss: 0.333, 38432/60000 datapoints
2025-03-06 19:00:51,521 - INFO - training batch 1251, loss: 0.281, 40032/60000 datapoints
2025-03-06 19:00:51,714 - INFO - training batch 1301, loss: 0.355, 41632/60000 datapoints
2025-03-06 19:00:51,906 - INFO - training batch 1351, loss: 0.510, 43232/60000 datapoints
2025-03-06 19:00:52,098 - INFO - training batch 1401, loss: 0.177, 44832/60000 datapoints
2025-03-06 19:00:52,288 - INFO - training batch 1451, loss: 0.157, 46432/60000 datapoints
2025-03-06 19:00:52,479 - INFO - training batch 1501, loss: 0.354, 48032/60000 datapoints
2025-03-06 19:00:52,673 - INFO - training batch 1551, loss: 0.504, 49632/60000 datapoints
2025-03-06 19:00:52,886 - INFO - training batch 1601, loss: 0.357, 51232/60000 datapoints
2025-03-06 19:00:53,077 - INFO - training batch 1651, loss: 0.450, 52832/60000 datapoints
2025-03-06 19:00:53,268 - INFO - training batch 1701, loss: 0.544, 54432/60000 datapoints
2025-03-06 19:00:53,457 - INFO - training batch 1751, loss: 0.378, 56032/60000 datapoints
2025-03-06 19:00:53,648 - INFO - training batch 1801, loss: 0.432, 57632/60000 datapoints
2025-03-06 19:00:53,840 - INFO - training batch 1851, loss: 0.406, 59232/60000 datapoints
2025-03-06 19:00:53,940 - INFO - validation batch 1, loss: 0.274, 32/10016 datapoints
2025-03-06 19:00:54,090 - INFO - validation batch 51, loss: 0.181, 1632/10016 datapoints
2025-03-06 19:00:54,241 - INFO - validation batch 101, loss: 0.349, 3232/10016 datapoints
2025-03-06 19:00:54,391 - INFO - validation batch 151, loss: 0.565, 4832/10016 datapoints
2025-03-06 19:00:54,545 - INFO - validation batch 201, loss: 0.332, 6432/10016 datapoints
2025-03-06 19:00:54,697 - INFO - validation batch 251, loss: 0.483, 8032/10016 datapoints
2025-03-06 19:00:54,848 - INFO - validation batch 301, loss: 0.160, 9632/10016 datapoints
2025-03-06 19:00:54,886 - INFO - Epoch 164/800 done.
2025-03-06 19:00:54,886 - INFO - Final validation performance:
Loss: 0.335, top-1 acc: 0.896top-5 acc: 0.896
2025-03-06 19:00:54,887 - INFO - Beginning epoch 165/800
2025-03-06 19:00:54,894 - INFO - training batch 1, loss: 0.351, 32/60000 datapoints
2025-03-06 19:00:55,100 - INFO - training batch 51, loss: 0.277, 1632/60000 datapoints
2025-03-06 19:00:55,291 - INFO - training batch 101, loss: 0.587, 3232/60000 datapoints
2025-03-06 19:00:55,486 - INFO - training batch 151, loss: 0.352, 4832/60000 datapoints
2025-03-06 19:00:55,682 - INFO - training batch 201, loss: 0.320, 6432/60000 datapoints
2025-03-06 19:00:55,879 - INFO - training batch 251, loss: 0.424, 8032/60000 datapoints
2025-03-06 19:00:56,080 - INFO - training batch 301, loss: 0.514, 9632/60000 datapoints
2025-03-06 19:00:56,272 - INFO - training batch 351, loss: 0.262, 11232/60000 datapoints
2025-03-06 19:00:56,476 - INFO - training batch 401, loss: 0.553, 12832/60000 datapoints
2025-03-06 19:00:56,671 - INFO - training batch 451, loss: 0.468, 14432/60000 datapoints
2025-03-06 19:00:56,865 - INFO - training batch 501, loss: 0.280, 16032/60000 datapoints
2025-03-06 19:00:57,061 - INFO - training batch 551, loss: 0.323, 17632/60000 datapoints
2025-03-06 19:00:57,256 - INFO - training batch 601, loss: 0.237, 19232/60000 datapoints
2025-03-06 19:00:57,450 - INFO - training batch 651, loss: 0.494, 20832/60000 datapoints
2025-03-06 19:00:57,647 - INFO - training batch 701, loss: 0.250, 22432/60000 datapoints
2025-03-06 19:00:57,847 - INFO - training batch 751, loss: 0.287, 24032/60000 datapoints
2025-03-06 19:00:58,043 - INFO - training batch 801, loss: 0.421, 25632/60000 datapoints
2025-03-06 19:00:58,236 - INFO - training batch 851, loss: 0.266, 27232/60000 datapoints
2025-03-06 19:00:58,432 - INFO - training batch 901, loss: 0.372, 28832/60000 datapoints
2025-03-06 19:00:58,634 - INFO - training batch 951, loss: 0.296, 30432/60000 datapoints
2025-03-06 19:00:58,851 - INFO - training batch 1001, loss: 0.266, 32032/60000 datapoints
2025-03-06 19:00:59,051 - INFO - training batch 1051, loss: 0.544, 33632/60000 datapoints
2025-03-06 19:00:59,242 - INFO - training batch 1101, loss: 0.494, 35232/60000 datapoints
2025-03-06 19:00:59,435 - INFO - training batch 1151, loss: 0.565, 36832/60000 datapoints
2025-03-06 19:00:59,630 - INFO - training batch 1201, loss: 0.373, 38432/60000 datapoints
2025-03-06 19:00:59,821 - INFO - training batch 1251, loss: 0.376, 40032/60000 datapoints
2025-03-06 19:01:00,020 - INFO - training batch 1301, loss: 0.436, 41632/60000 datapoints
2025-03-06 19:01:00,209 - INFO - training batch 1351, loss: 0.232, 43232/60000 datapoints
2025-03-06 19:01:00,402 - INFO - training batch 1401, loss: 0.398, 44832/60000 datapoints
2025-03-06 19:01:00,595 - INFO - training batch 1451, loss: 0.531, 46432/60000 datapoints
2025-03-06 19:01:00,788 - INFO - training batch 1501, loss: 0.524, 48032/60000 datapoints
2025-03-06 19:01:00,984 - INFO - training batch 1551, loss: 0.563, 49632/60000 datapoints
2025-03-06 19:01:01,181 - INFO - training batch 1601, loss: 0.345, 51232/60000 datapoints
2025-03-06 19:01:01,375 - INFO - training batch 1651, loss: 0.585, 52832/60000 datapoints
2025-03-06 19:01:01,569 - INFO - training batch 1701, loss: 0.703, 54432/60000 datapoints
2025-03-06 19:01:01,764 - INFO - training batch 1751, loss: 0.356, 56032/60000 datapoints
2025-03-06 19:01:01,961 - INFO - training batch 1801, loss: 0.395, 57632/60000 datapoints
2025-03-06 19:01:02,155 - INFO - training batch 1851, loss: 0.509, 59232/60000 datapoints
2025-03-06 19:01:02,254 - INFO - validation batch 1, loss: 0.212, 32/10016 datapoints
2025-03-06 19:01:02,406 - INFO - validation batch 51, loss: 0.238, 1632/10016 datapoints
2025-03-06 19:01:02,558 - INFO - validation batch 101, loss: 0.498, 3232/10016 datapoints
2025-03-06 19:01:02,714 - INFO - validation batch 151, loss: 0.446, 4832/10016 datapoints
2025-03-06 19:01:02,867 - INFO - validation batch 201, loss: 0.403, 6432/10016 datapoints
2025-03-06 19:01:03,019 - INFO - validation batch 251, loss: 0.366, 8032/10016 datapoints
2025-03-06 19:01:03,171 - INFO - validation batch 301, loss: 0.656, 9632/10016 datapoints
2025-03-06 19:01:03,209 - INFO - Epoch 165/800 done.
2025-03-06 19:01:03,209 - INFO - Final validation performance:
Loss: 0.403, top-1 acc: 0.896top-5 acc: 0.896
2025-03-06 19:01:03,210 - INFO - Beginning epoch 166/800
2025-03-06 19:01:03,216 - INFO - training batch 1, loss: 0.615, 32/60000 datapoints
2025-03-06 19:01:03,422 - INFO - training batch 51, loss: 0.512, 1632/60000 datapoints
2025-03-06 19:01:03,620 - INFO - training batch 101, loss: 0.480, 3232/60000 datapoints
2025-03-06 19:01:03,814 - INFO - training batch 151, loss: 0.419, 4832/60000 datapoints
2025-03-06 19:01:04,011 - INFO - training batch 201, loss: 0.368, 6432/60000 datapoints
2025-03-06 19:01:04,210 - INFO - training batch 251, loss: 0.354, 8032/60000 datapoints
2025-03-06 19:01:04,408 - INFO - training batch 301, loss: 0.369, 9632/60000 datapoints
2025-03-06 19:01:04,610 - INFO - training batch 351, loss: 0.323, 11232/60000 datapoints
2025-03-06 19:01:04,804 - INFO - training batch 401, loss: 0.460, 12832/60000 datapoints
2025-03-06 19:01:05,002 - INFO - training batch 451, loss: 0.363, 14432/60000 datapoints
2025-03-06 19:01:05,195 - INFO - training batch 501, loss: 0.446, 16032/60000 datapoints
2025-03-06 19:01:05,389 - INFO - training batch 551, loss: 0.420, 17632/60000 datapoints
2025-03-06 19:01:05,581 - INFO - training batch 601, loss: 0.363, 19232/60000 datapoints
2025-03-06 19:01:05,776 - INFO - training batch 651, loss: 0.393, 20832/60000 datapoints
2025-03-06 19:01:05,976 - INFO - training batch 701, loss: 0.330, 22432/60000 datapoints
2025-03-06 19:01:06,183 - INFO - training batch 751, loss: 0.273, 24032/60000 datapoints
2025-03-06 19:01:06,416 - INFO - training batch 801, loss: 0.482, 25632/60000 datapoints
2025-03-06 19:01:06,633 - INFO - training batch 851, loss: 0.242, 27232/60000 datapoints
2025-03-06 19:01:06,849 - INFO - training batch 901, loss: 0.386, 28832/60000 datapoints
2025-03-06 19:01:07,060 - INFO - training batch 951, loss: 0.453, 30432/60000 datapoints
2025-03-06 19:01:07,268 - INFO - training batch 1001, loss: 0.612, 32032/60000 datapoints
2025-03-06 19:01:07,463 - INFO - training batch 1051, loss: 0.253, 33632/60000 datapoints
2025-03-06 19:01:07,663 - INFO - training batch 1101, loss: 0.285, 35232/60000 datapoints
2025-03-06 19:01:07,860 - INFO - training batch 1151, loss: 0.405, 36832/60000 datapoints
2025-03-06 19:01:08,056 - INFO - training batch 1201, loss: 0.353, 38432/60000 datapoints
2025-03-06 19:01:08,250 - INFO - training batch 1251, loss: 0.356, 40032/60000 datapoints
2025-03-06 19:01:08,445 - INFO - training batch 1301, loss: 0.239, 41632/60000 datapoints
2025-03-06 19:01:08,640 - INFO - training batch 1351, loss: 0.277, 43232/60000 datapoints
2025-03-06 19:01:08,848 - INFO - training batch 1401, loss: 0.352, 44832/60000 datapoints
2025-03-06 19:01:09,051 - INFO - training batch 1451, loss: 0.228, 46432/60000 datapoints
2025-03-06 19:01:09,246 - INFO - training batch 1501, loss: 0.645, 48032/60000 datapoints
2025-03-06 19:01:09,451 - INFO - training batch 1551, loss: 0.354, 49632/60000 datapoints
2025-03-06 19:01:09,653 - INFO - training batch 1601, loss: 0.283, 51232/60000 datapoints
2025-03-06 19:01:09,849 - INFO - training batch 1651, loss: 0.470, 52832/60000 datapoints
2025-03-06 19:01:10,050 - INFO - training batch 1701, loss: 0.234, 54432/60000 datapoints
2025-03-06 19:01:10,246 - INFO - training batch 1751, loss: 0.450, 56032/60000 datapoints
2025-03-06 19:01:10,441 - INFO - training batch 1801, loss: 0.590, 57632/60000 datapoints
2025-03-06 19:01:10,645 - INFO - training batch 1851, loss: 0.405, 59232/60000 datapoints
2025-03-06 19:01:10,748 - INFO - validation batch 1, loss: 0.404, 32/10016 datapoints
2025-03-06 19:01:10,903 - INFO - validation batch 51, loss: 0.519, 1632/10016 datapoints
2025-03-06 19:01:11,057 - INFO - validation batch 101, loss: 0.499, 3232/10016 datapoints
2025-03-06 19:01:11,208 - INFO - validation batch 151, loss: 0.264, 4832/10016 datapoints
2025-03-06 19:01:11,369 - INFO - validation batch 201, loss: 0.375, 6432/10016 datapoints
2025-03-06 19:01:11,534 - INFO - validation batch 251, loss: 0.424, 8032/10016 datapoints
2025-03-06 19:01:11,687 - INFO - validation batch 301, loss: 0.278, 9632/10016 datapoints
2025-03-06 19:01:11,724 - INFO - Epoch 166/800 done.
2025-03-06 19:01:11,724 - INFO - Final validation performance:
Loss: 0.395, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 19:01:11,725 - INFO - Beginning epoch 167/800
2025-03-06 19:01:11,731 - INFO - training batch 1, loss: 0.263, 32/60000 datapoints
2025-03-06 19:01:11,936 - INFO - training batch 51, loss: 0.416, 1632/60000 datapoints
2025-03-06 19:01:12,134 - INFO - training batch 101, loss: 0.301, 3232/60000 datapoints
2025-03-06 19:01:12,328 - INFO - training batch 151, loss: 0.304, 4832/60000 datapoints
2025-03-06 19:01:12,528 - INFO - training batch 201, loss: 0.469, 6432/60000 datapoints
2025-03-06 19:01:12,725 - INFO - training batch 251, loss: 0.157, 8032/60000 datapoints
2025-03-06 19:01:12,921 - INFO - training batch 301, loss: 0.434, 9632/60000 datapoints
2025-03-06 19:01:13,117 - INFO - training batch 351, loss: 0.368, 11232/60000 datapoints
2025-03-06 19:01:13,313 - INFO - training batch 401, loss: 0.160, 12832/60000 datapoints
2025-03-06 19:01:13,506 - INFO - training batch 451, loss: 0.524, 14432/60000 datapoints
2025-03-06 19:01:13,704 - INFO - training batch 501, loss: 0.231, 16032/60000 datapoints
2025-03-06 19:01:13,902 - INFO - training batch 551, loss: 0.472, 17632/60000 datapoints
2025-03-06 19:01:14,103 - INFO - training batch 601, loss: 0.480, 19232/60000 datapoints
2025-03-06 19:01:14,297 - INFO - training batch 651, loss: 0.415, 20832/60000 datapoints
2025-03-06 19:01:14,492 - INFO - training batch 701, loss: 0.434, 22432/60000 datapoints
2025-03-06 19:01:14,689 - INFO - training batch 751, loss: 0.563, 24032/60000 datapoints
2025-03-06 19:01:14,884 - INFO - training batch 801, loss: 0.325, 25632/60000 datapoints
2025-03-06 19:01:15,084 - INFO - training batch 851, loss: 0.288, 27232/60000 datapoints
2025-03-06 19:01:15,276 - INFO - training batch 901, loss: 0.582, 28832/60000 datapoints
2025-03-06 19:01:15,469 - INFO - training batch 951, loss: 0.294, 30432/60000 datapoints
2025-03-06 19:01:15,668 - INFO - training batch 1001, loss: 0.454, 32032/60000 datapoints
2025-03-06 19:01:15,862 - INFO - training batch 1051, loss: 0.279, 33632/60000 datapoints
2025-03-06 19:01:16,063 - INFO - training batch 1101, loss: 0.381, 35232/60000 datapoints
2025-03-06 19:01:16,260 - INFO - training batch 1151, loss: 0.334, 36832/60000 datapoints
2025-03-06 19:01:16,460 - INFO - training batch 1201, loss: 0.323, 38432/60000 datapoints
2025-03-06 19:01:16,660 - INFO - training batch 1251, loss: 0.413, 40032/60000 datapoints
2025-03-06 19:01:16,855 - INFO - training batch 1301, loss: 0.491, 41632/60000 datapoints
2025-03-06 19:01:17,049 - INFO - training batch 1351, loss: 0.448, 43232/60000 datapoints
2025-03-06 19:01:17,241 - INFO - training batch 1401, loss: 0.586, 44832/60000 datapoints
2025-03-06 19:01:17,440 - INFO - training batch 1451, loss: 0.300, 46432/60000 datapoints
2025-03-06 19:01:17,636 - INFO - training batch 1501, loss: 0.628, 48032/60000 datapoints
2025-03-06 19:01:17,833 - INFO - training batch 1551, loss: 0.298, 49632/60000 datapoints
2025-03-06 19:01:18,084 - INFO - training batch 1601, loss: 0.272, 51232/60000 datapoints
2025-03-06 19:01:18,279 - INFO - training batch 1651, loss: 0.263, 52832/60000 datapoints
2025-03-06 19:01:18,473 - INFO - training batch 1701, loss: 0.441, 54432/60000 datapoints
2025-03-06 19:01:18,670 - INFO - training batch 1751, loss: 0.236, 56032/60000 datapoints
2025-03-06 19:01:18,865 - INFO - training batch 1801, loss: 0.262, 57632/60000 datapoints
2025-03-06 19:01:19,078 - INFO - training batch 1851, loss: 0.354, 59232/60000 datapoints
2025-03-06 19:01:19,203 - INFO - validation batch 1, loss: 0.275, 32/10016 datapoints
2025-03-06 19:01:19,362 - INFO - validation batch 51, loss: 0.273, 1632/10016 datapoints
2025-03-06 19:01:19,517 - INFO - validation batch 101, loss: 0.678, 3232/10016 datapoints
2025-03-06 19:01:19,674 - INFO - validation batch 151, loss: 0.302, 4832/10016 datapoints
2025-03-06 19:01:19,825 - INFO - validation batch 201, loss: 0.285, 6432/10016 datapoints
2025-03-06 19:01:19,978 - INFO - validation batch 251, loss: 0.449, 8032/10016 datapoints
2025-03-06 19:01:20,134 - INFO - validation batch 301, loss: 0.340, 9632/10016 datapoints
2025-03-06 19:01:20,172 - INFO - Epoch 167/800 done.
2025-03-06 19:01:20,172 - INFO - Final validation performance:
Loss: 0.372, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 19:01:20,173 - INFO - Beginning epoch 168/800
2025-03-06 19:01:20,179 - INFO - training batch 1, loss: 0.174, 32/60000 datapoints
2025-03-06 19:01:20,372 - INFO - training batch 51, loss: 0.358, 1632/60000 datapoints
2025-03-06 19:01:20,577 - INFO - training batch 101, loss: 0.330, 3232/60000 datapoints
2025-03-06 19:01:20,776 - INFO - training batch 151, loss: 0.322, 4832/60000 datapoints
2025-03-06 19:01:20,975 - INFO - training batch 201, loss: 0.240, 6432/60000 datapoints
2025-03-06 19:01:21,170 - INFO - training batch 251, loss: 0.462, 8032/60000 datapoints
2025-03-06 19:01:21,370 - INFO - training batch 301, loss: 0.452, 9632/60000 datapoints
2025-03-06 19:01:21,564 - INFO - training batch 351, loss: 0.297, 11232/60000 datapoints
2025-03-06 19:01:21,765 - INFO - training batch 401, loss: 0.316, 12832/60000 datapoints
2025-03-06 19:01:21,961 - INFO - training batch 451, loss: 0.245, 14432/60000 datapoints
2025-03-06 19:01:22,159 - INFO - training batch 501, loss: 0.563, 16032/60000 datapoints
2025-03-06 19:01:22,354 - INFO - training batch 551, loss: 0.257, 17632/60000 datapoints
2025-03-06 19:01:22,548 - INFO - training batch 601, loss: 0.600, 19232/60000 datapoints
2025-03-06 19:01:22,746 - INFO - training batch 651, loss: 0.384, 20832/60000 datapoints
2025-03-06 19:01:22,940 - INFO - training batch 701, loss: 0.363, 22432/60000 datapoints
2025-03-06 19:01:23,134 - INFO - training batch 751, loss: 0.289, 24032/60000 datapoints
2025-03-06 19:01:23,328 - INFO - training batch 801, loss: 0.387, 25632/60000 datapoints
2025-03-06 19:01:23,522 - INFO - training batch 851, loss: 0.338, 27232/60000 datapoints
2025-03-06 19:01:23,720 - INFO - training batch 901, loss: 0.246, 28832/60000 datapoints
2025-03-06 19:01:23,914 - INFO - training batch 951, loss: 0.652, 30432/60000 datapoints
2025-03-06 19:01:24,111 - INFO - training batch 1001, loss: 0.247, 32032/60000 datapoints
2025-03-06 19:01:24,307 - INFO - training batch 1051, loss: 0.490, 33632/60000 datapoints
2025-03-06 19:01:24,499 - INFO - training batch 1101, loss: 0.276, 35232/60000 datapoints
2025-03-06 19:01:24,696 - INFO - training batch 1151, loss: 0.334, 36832/60000 datapoints
2025-03-06 19:01:24,893 - INFO - training batch 1201, loss: 0.188, 38432/60000 datapoints
2025-03-06 19:01:25,088 - INFO - training batch 1251, loss: 0.491, 40032/60000 datapoints
2025-03-06 19:01:25,282 - INFO - training batch 1301, loss: 0.381, 41632/60000 datapoints
2025-03-06 19:01:25,474 - INFO - training batch 1351, loss: 0.721, 43232/60000 datapoints
2025-03-06 19:01:25,671 - INFO - training batch 1401, loss: 0.186, 44832/60000 datapoints
2025-03-06 19:01:25,866 - INFO - training batch 1451, loss: 0.519, 46432/60000 datapoints
2025-03-06 19:01:26,065 - INFO - training batch 1501, loss: 0.356, 48032/60000 datapoints
2025-03-06 19:01:26,260 - INFO - training batch 1551, loss: 0.507, 49632/60000 datapoints
2025-03-06 19:01:26,455 - INFO - training batch 1601, loss: 0.181, 51232/60000 datapoints
2025-03-06 19:01:26,650 - INFO - training batch 1651, loss: 0.311, 52832/60000 datapoints
2025-03-06 19:01:26,845 - INFO - training batch 1701, loss: 0.422, 54432/60000 datapoints
2025-03-06 19:01:27,040 - INFO - training batch 1751, loss: 0.247, 56032/60000 datapoints
2025-03-06 19:01:27,232 - INFO - training batch 1801, loss: 0.420, 57632/60000 datapoints
2025-03-06 19:01:27,425 - INFO - training batch 1851, loss: 0.240, 59232/60000 datapoints
2025-03-06 19:01:27,525 - INFO - validation batch 1, loss: 0.537, 32/10016 datapoints
2025-03-06 19:01:27,681 - INFO - validation batch 51, loss: 0.390, 1632/10016 datapoints
2025-03-06 19:01:27,833 - INFO - validation batch 101, loss: 0.592, 3232/10016 datapoints
2025-03-06 19:01:27,990 - INFO - validation batch 151, loss: 0.246, 4832/10016 datapoints
2025-03-06 19:01:28,160 - INFO - validation batch 201, loss: 0.354, 6432/10016 datapoints
2025-03-06 19:01:28,311 - INFO - validation batch 251, loss: 0.236, 8032/10016 datapoints
2025-03-06 19:01:28,465 - INFO - validation batch 301, loss: 0.456, 9632/10016 datapoints
2025-03-06 19:01:28,502 - INFO - Epoch 168/800 done.
2025-03-06 19:01:28,502 - INFO - Final validation performance:
Loss: 0.402, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 19:01:28,503 - INFO - Beginning epoch 169/800
2025-03-06 19:01:28,510 - INFO - training batch 1, loss: 0.393, 32/60000 datapoints
2025-03-06 19:01:28,721 - INFO - training batch 51, loss: 0.251, 1632/60000 datapoints
2025-03-06 19:01:28,912 - INFO - training batch 101, loss: 0.355, 3232/60000 datapoints
2025-03-06 19:01:29,130 - INFO - training batch 151, loss: 0.485, 4832/60000 datapoints
2025-03-06 19:01:29,325 - INFO - training batch 201, loss: 0.210, 6432/60000 datapoints
2025-03-06 19:01:29,522 - INFO - training batch 251, loss: 0.470, 8032/60000 datapoints
2025-03-06 19:01:29,717 - INFO - training batch 301, loss: 0.606, 9632/60000 datapoints
2025-03-06 19:01:29,914 - INFO - training batch 351, loss: 0.354, 11232/60000 datapoints
2025-03-06 19:01:30,112 - INFO - training batch 401, loss: 0.212, 12832/60000 datapoints
2025-03-06 19:01:30,307 - INFO - training batch 451, loss: 0.389, 14432/60000 datapoints
2025-03-06 19:01:30,502 - INFO - training batch 501, loss: 0.367, 16032/60000 datapoints
2025-03-06 19:01:30,698 - INFO - training batch 551, loss: 0.263, 17632/60000 datapoints
2025-03-06 19:01:30,892 - INFO - training batch 601, loss: 0.305, 19232/60000 datapoints
2025-03-06 19:01:31,090 - INFO - training batch 651, loss: 0.462, 20832/60000 datapoints
2025-03-06 19:01:31,286 - INFO - training batch 701, loss: 0.427, 22432/60000 datapoints
2025-03-06 19:01:31,480 - INFO - training batch 751, loss: 0.284, 24032/60000 datapoints
2025-03-06 19:01:31,679 - INFO - training batch 801, loss: 0.475, 25632/60000 datapoints
2025-03-06 19:01:31,876 - INFO - training batch 851, loss: 0.644, 27232/60000 datapoints
2025-03-06 19:01:32,072 - INFO - training batch 901, loss: 0.329, 28832/60000 datapoints
2025-03-06 19:01:32,267 - INFO - training batch 951, loss: 0.487, 30432/60000 datapoints
2025-03-06 19:01:32,459 - INFO - training batch 1001, loss: 0.300, 32032/60000 datapoints
2025-03-06 19:01:32,654 - INFO - training batch 1051, loss: 0.243, 33632/60000 datapoints
2025-03-06 19:01:32,849 - INFO - training batch 1101, loss: 0.490, 35232/60000 datapoints
2025-03-06 19:01:33,044 - INFO - training batch 1151, loss: 0.230, 36832/60000 datapoints
2025-03-06 19:01:33,240 - INFO - training batch 1201, loss: 0.229, 38432/60000 datapoints
2025-03-06 19:01:33,431 - INFO - training batch 1251, loss: 0.331, 40032/60000 datapoints
2025-03-06 19:01:33,626 - INFO - training batch 1301, loss: 0.428, 41632/60000 datapoints
2025-03-06 19:01:33,821 - INFO - training batch 1351, loss: 0.271, 43232/60000 datapoints
2025-03-06 19:01:34,014 - INFO - training batch 1401, loss: 0.584, 44832/60000 datapoints
2025-03-06 19:01:34,214 - INFO - training batch 1451, loss: 0.150, 46432/60000 datapoints
2025-03-06 19:01:34,408 - INFO - training batch 1501, loss: 0.361, 48032/60000 datapoints
2025-03-06 19:01:34,601 - INFO - training batch 1551, loss: 0.442, 49632/60000 datapoints
2025-03-06 19:01:34,798 - INFO - training batch 1601, loss: 0.340, 51232/60000 datapoints
2025-03-06 19:01:34,996 - INFO - training batch 1651, loss: 0.542, 52832/60000 datapoints
2025-03-06 19:01:35,192 - INFO - training batch 1701, loss: 0.264, 54432/60000 datapoints
2025-03-06 19:01:35,384 - INFO - training batch 1751, loss: 0.236, 56032/60000 datapoints
2025-03-06 19:01:35,577 - INFO - training batch 1801, loss: 0.713, 57632/60000 datapoints
2025-03-06 19:01:35,775 - INFO - training batch 1851, loss: 0.576, 59232/60000 datapoints
2025-03-06 19:01:35,876 - INFO - validation batch 1, loss: 0.252, 32/10016 datapoints
2025-03-06 19:01:36,030 - INFO - validation batch 51, loss: 0.639, 1632/10016 datapoints
2025-03-06 19:01:36,186 - INFO - validation batch 101, loss: 0.738, 3232/10016 datapoints
2025-03-06 19:01:36,339 - INFO - validation batch 151, loss: 0.268, 4832/10016 datapoints
2025-03-06 19:01:36,495 - INFO - validation batch 201, loss: 0.663, 6432/10016 datapoints
2025-03-06 19:01:36,649 - INFO - validation batch 251, loss: 0.412, 8032/10016 datapoints
2025-03-06 19:01:36,811 - INFO - validation batch 301, loss: 0.417, 9632/10016 datapoints
2025-03-06 19:01:36,849 - INFO - Epoch 169/800 done.
2025-03-06 19:01:36,850 - INFO - Final validation performance:
Loss: 0.484, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 19:01:36,850 - INFO - Beginning epoch 170/800
2025-03-06 19:01:36,857 - INFO - training batch 1, loss: 0.791, 32/60000 datapoints
2025-03-06 19:01:37,048 - INFO - training batch 51, loss: 0.454, 1632/60000 datapoints
2025-03-06 19:01:37,240 - INFO - training batch 101, loss: 0.400, 3232/60000 datapoints
2025-03-06 19:01:37,439 - INFO - training batch 151, loss: 0.379, 4832/60000 datapoints
2025-03-06 19:01:37,648 - INFO - training batch 201, loss: 0.346, 6432/60000 datapoints
2025-03-06 19:01:37,841 - INFO - training batch 251, loss: 0.249, 8032/60000 datapoints
2025-03-06 19:01:38,042 - INFO - training batch 301, loss: 0.345, 9632/60000 datapoints
2025-03-06 19:01:38,240 - INFO - training batch 351, loss: 0.400, 11232/60000 datapoints
2025-03-06 19:01:38,434 - INFO - training batch 401, loss: 0.247, 12832/60000 datapoints
2025-03-06 19:01:38,627 - INFO - training batch 451, loss: 0.317, 14432/60000 datapoints
2025-03-06 19:01:38,821 - INFO - training batch 501, loss: 0.359, 16032/60000 datapoints
2025-03-06 19:01:39,015 - INFO - training batch 551, loss: 0.307, 17632/60000 datapoints
2025-03-06 19:01:39,225 - INFO - training batch 601, loss: 0.268, 19232/60000 datapoints
2025-03-06 19:01:39,421 - INFO - training batch 651, loss: 0.874, 20832/60000 datapoints
2025-03-06 19:01:39,618 - INFO - training batch 701, loss: 0.618, 22432/60000 datapoints
2025-03-06 19:01:39,813 - INFO - training batch 751, loss: 0.272, 24032/60000 datapoints
2025-03-06 19:01:40,009 - INFO - training batch 801, loss: 0.283, 25632/60000 datapoints
2025-03-06 19:01:40,205 - INFO - training batch 851, loss: 0.410, 27232/60000 datapoints
2025-03-06 19:01:40,398 - INFO - training batch 901, loss: 0.429, 28832/60000 datapoints
2025-03-06 19:01:40,589 - INFO - training batch 951, loss: 0.597, 30432/60000 datapoints
2025-03-06 19:01:40,783 - INFO - training batch 1001, loss: 0.244, 32032/60000 datapoints
2025-03-06 19:01:40,978 - INFO - training batch 1051, loss: 0.263, 33632/60000 datapoints
2025-03-06 19:01:41,171 - INFO - training batch 1101, loss: 0.424, 35232/60000 datapoints
2025-03-06 19:01:41,370 - INFO - training batch 1151, loss: 0.299, 36832/60000 datapoints
2025-03-06 19:01:41,564 - INFO - training batch 1201, loss: 0.314, 38432/60000 datapoints
2025-03-06 19:01:41,760 - INFO - training batch 1251, loss: 0.360, 40032/60000 datapoints
2025-03-06 19:01:41,953 - INFO - training batch 1301, loss: 0.410, 41632/60000 datapoints
2025-03-06 19:01:42,152 - INFO - training batch 1351, loss: 0.340, 43232/60000 datapoints
2025-03-06 19:01:42,349 - INFO - training batch 1401, loss: 0.626, 44832/60000 datapoints
2025-03-06 19:01:42,544 - INFO - training batch 1451, loss: 0.174, 46432/60000 datapoints
2025-03-06 19:01:42,741 - INFO - training batch 1501, loss: 0.288, 48032/60000 datapoints
2025-03-06 19:01:42,937 - INFO - training batch 1551, loss: 0.345, 49632/60000 datapoints
2025-03-06 19:01:43,132 - INFO - training batch 1601, loss: 0.421, 51232/60000 datapoints
2025-03-06 19:01:43,325 - INFO - training batch 1651, loss: 0.386, 52832/60000 datapoints
2025-03-06 19:01:43,525 - INFO - training batch 1701, loss: 0.395, 54432/60000 datapoints
2025-03-06 19:01:43,721 - INFO - training batch 1751, loss: 0.474, 56032/60000 datapoints
2025-03-06 19:01:43,918 - INFO - training batch 1801, loss: 0.371, 57632/60000 datapoints
2025-03-06 19:01:44,115 - INFO - training batch 1851, loss: 0.490, 59232/60000 datapoints
2025-03-06 19:01:44,217 - INFO - validation batch 1, loss: 0.425, 32/10016 datapoints
2025-03-06 19:01:44,368 - INFO - validation batch 51, loss: 0.430, 1632/10016 datapoints
2025-03-06 19:01:44,517 - INFO - validation batch 101, loss: 0.227, 3232/10016 datapoints
2025-03-06 19:01:44,672 - INFO - validation batch 151, loss: 0.254, 4832/10016 datapoints
2025-03-06 19:01:44,825 - INFO - validation batch 201, loss: 0.420, 6432/10016 datapoints
2025-03-06 19:01:44,983 - INFO - validation batch 251, loss: 0.633, 8032/10016 datapoints
2025-03-06 19:01:45,138 - INFO - validation batch 301, loss: 0.524, 9632/10016 datapoints
2025-03-06 19:01:45,177 - INFO - Epoch 170/800 done.
2025-03-06 19:01:45,177 - INFO - Final validation performance:
Loss: 0.416, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 19:01:45,178 - INFO - Beginning epoch 171/800
2025-03-06 19:01:45,185 - INFO - training batch 1, loss: 0.296, 32/60000 datapoints
2025-03-06 19:01:45,389 - INFO - training batch 51, loss: 0.576, 1632/60000 datapoints
2025-03-06 19:01:45,585 - INFO - training batch 101, loss: 0.405, 3232/60000 datapoints
2025-03-06 19:01:45,781 - INFO - training batch 151, loss: 0.309, 4832/60000 datapoints
2025-03-06 19:01:45,976 - INFO - training batch 201, loss: 0.579, 6432/60000 datapoints
2025-03-06 19:01:46,178 - INFO - training batch 251, loss: 0.301, 8032/60000 datapoints
2025-03-06 19:01:46,378 - INFO - training batch 301, loss: 0.381, 9632/60000 datapoints
2025-03-06 19:01:46,576 - INFO - training batch 351, loss: 0.330, 11232/60000 datapoints
2025-03-06 19:01:46,770 - INFO - training batch 401, loss: 0.301, 12832/60000 datapoints
2025-03-06 19:01:46,964 - INFO - training batch 451, loss: 0.519, 14432/60000 datapoints
2025-03-06 19:01:47,164 - INFO - training batch 501, loss: 0.447, 16032/60000 datapoints
2025-03-06 19:01:47,358 - INFO - training batch 551, loss: 0.270, 17632/60000 datapoints
2025-03-06 19:01:47,555 - INFO - training batch 601, loss: 0.206, 19232/60000 datapoints
2025-03-06 19:01:47,752 - INFO - training batch 651, loss: 0.334, 20832/60000 datapoints
2025-03-06 19:01:47,949 - INFO - training batch 701, loss: 0.751, 22432/60000 datapoints
2025-03-06 19:01:48,143 - INFO - training batch 751, loss: 0.301, 24032/60000 datapoints
2025-03-06 19:01:48,357 - INFO - training batch 801, loss: 0.210, 25632/60000 datapoints
2025-03-06 19:01:48,553 - INFO - training batch 851, loss: 0.634, 27232/60000 datapoints
2025-03-06 19:01:48,749 - INFO - training batch 901, loss: 0.250, 28832/60000 datapoints
2025-03-06 19:01:48,942 - INFO - training batch 951, loss: 0.271, 30432/60000 datapoints
2025-03-06 19:01:49,143 - INFO - training batch 1001, loss: 0.518, 32032/60000 datapoints
2025-03-06 19:01:49,348 - INFO - training batch 1051, loss: 0.221, 33632/60000 datapoints
2025-03-06 19:01:49,541 - INFO - training batch 1101, loss: 0.542, 35232/60000 datapoints
2025-03-06 19:01:49,741 - INFO - training batch 1151, loss: 0.380, 36832/60000 datapoints
2025-03-06 19:01:49,935 - INFO - training batch 1201, loss: 0.280, 38432/60000 datapoints
2025-03-06 19:01:50,132 - INFO - training batch 1251, loss: 0.653, 40032/60000 datapoints
2025-03-06 19:01:50,328 - INFO - training batch 1301, loss: 0.390, 41632/60000 datapoints
2025-03-06 19:01:50,522 - INFO - training batch 1351, loss: 0.673, 43232/60000 datapoints
2025-03-06 19:01:50,716 - INFO - training batch 1401, loss: 0.356, 44832/60000 datapoints
2025-03-06 19:01:50,911 - INFO - training batch 1451, loss: 0.436, 46432/60000 datapoints
2025-03-06 19:01:51,104 - INFO - training batch 1501, loss: 0.272, 48032/60000 datapoints
2025-03-06 19:01:51,299 - INFO - training batch 1551, loss: 0.305, 49632/60000 datapoints
2025-03-06 19:01:51,492 - INFO - training batch 1601, loss: 0.457, 51232/60000 datapoints
2025-03-06 19:01:51,686 - INFO - training batch 1651, loss: 0.315, 52832/60000 datapoints
2025-03-06 19:01:51,883 - INFO - training batch 1701, loss: 0.371, 54432/60000 datapoints
2025-03-06 19:01:52,079 - INFO - training batch 1751, loss: 0.651, 56032/60000 datapoints
2025-03-06 19:01:52,276 - INFO - training batch 1801, loss: 0.207, 57632/60000 datapoints
2025-03-06 19:01:52,470 - INFO - training batch 1851, loss: 0.422, 59232/60000 datapoints
2025-03-06 19:01:52,570 - INFO - validation batch 1, loss: 0.413, 32/10016 datapoints
2025-03-06 19:01:52,725 - INFO - validation batch 51, loss: 0.250, 1632/10016 datapoints
2025-03-06 19:01:52,877 - INFO - validation batch 101, loss: 0.554, 3232/10016 datapoints
2025-03-06 19:01:53,032 - INFO - validation batch 151, loss: 0.538, 4832/10016 datapoints
2025-03-06 19:01:53,185 - INFO - validation batch 201, loss: 0.430, 6432/10016 datapoints
2025-03-06 19:01:53,336 - INFO - validation batch 251, loss: 0.372, 8032/10016 datapoints
2025-03-06 19:01:53,489 - INFO - validation batch 301, loss: 0.358, 9632/10016 datapoints
2025-03-06 19:01:53,527 - INFO - Epoch 171/800 done.
2025-03-06 19:01:53,527 - INFO - Final validation performance:
Loss: 0.417, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 19:01:53,528 - INFO - Beginning epoch 172/800
2025-03-06 19:01:53,534 - INFO - training batch 1, loss: 0.342, 32/60000 datapoints
2025-03-06 19:01:53,741 - INFO - training batch 51, loss: 0.401, 1632/60000 datapoints
2025-03-06 19:01:53,934 - INFO - training batch 101, loss: 0.604, 3232/60000 datapoints
2025-03-06 19:01:54,138 - INFO - training batch 151, loss: 0.157, 4832/60000 datapoints
2025-03-06 19:01:54,341 - INFO - training batch 201, loss: 0.291, 6432/60000 datapoints
2025-03-06 19:01:54,537 - INFO - training batch 251, loss: 0.208, 8032/60000 datapoints
2025-03-06 19:01:54,743 - INFO - training batch 301, loss: 0.281, 9632/60000 datapoints
2025-03-06 19:01:54,947 - INFO - training batch 351, loss: 0.456, 11232/60000 datapoints
2025-03-06 19:01:55,145 - INFO - training batch 401, loss: 0.223, 12832/60000 datapoints
2025-03-06 19:01:55,342 - INFO - training batch 451, loss: 0.552, 14432/60000 datapoints
2025-03-06 19:01:55,538 - INFO - training batch 501, loss: 0.401, 16032/60000 datapoints
2025-03-06 19:01:55,734 - INFO - training batch 551, loss: 0.291, 17632/60000 datapoints
2025-03-06 19:01:55,927 - INFO - training batch 601, loss: 0.396, 19232/60000 datapoints
2025-03-06 19:01:56,122 - INFO - training batch 651, loss: 0.209, 20832/60000 datapoints
2025-03-06 19:01:56,317 - INFO - training batch 701, loss: 0.368, 22432/60000 datapoints
2025-03-06 19:01:56,511 - INFO - training batch 751, loss: 0.311, 24032/60000 datapoints
2025-03-06 19:01:56,705 - INFO - training batch 801, loss: 0.402, 25632/60000 datapoints
2025-03-06 19:01:56,897 - INFO - training batch 851, loss: 0.579, 27232/60000 datapoints
2025-03-06 19:01:57,091 - INFO - training batch 901, loss: 0.342, 28832/60000 datapoints
2025-03-06 19:01:57,284 - INFO - training batch 951, loss: 0.342, 30432/60000 datapoints
2025-03-06 19:01:57,480 - INFO - training batch 1001, loss: 0.418, 32032/60000 datapoints
2025-03-06 19:01:57,675 - INFO - training batch 1051, loss: 0.563, 33632/60000 datapoints
2025-03-06 19:01:57,869 - INFO - training batch 1101, loss: 0.272, 35232/60000 datapoints
2025-03-06 19:01:58,069 - INFO - training batch 1151, loss: 0.346, 36832/60000 datapoints
2025-03-06 19:01:58,266 - INFO - training batch 1201, loss: 0.293, 38432/60000 datapoints
2025-03-06 19:01:58,461 - INFO - training batch 1251, loss: 0.694, 40032/60000 datapoints
2025-03-06 19:01:58,660 - INFO - training batch 1301, loss: 0.500, 41632/60000 datapoints
2025-03-06 19:01:58,853 - INFO - training batch 1351, loss: 0.466, 43232/60000 datapoints
2025-03-06 19:01:59,048 - INFO - training batch 1401, loss: 0.300, 44832/60000 datapoints
2025-03-06 19:01:59,257 - INFO - training batch 1451, loss: 0.379, 46432/60000 datapoints
2025-03-06 19:01:59,454 - INFO - training batch 1501, loss: 0.506, 48032/60000 datapoints
2025-03-06 19:01:59,650 - INFO - training batch 1551, loss: 0.385, 49632/60000 datapoints
2025-03-06 19:01:59,846 - INFO - training batch 1601, loss: 0.315, 51232/60000 datapoints
2025-03-06 19:02:00,042 - INFO - training batch 1651, loss: 0.438, 52832/60000 datapoints
2025-03-06 19:02:00,238 - INFO - training batch 1701, loss: 0.397, 54432/60000 datapoints
2025-03-06 19:02:00,433 - INFO - training batch 1751, loss: 0.292, 56032/60000 datapoints
2025-03-06 19:02:00,629 - INFO - training batch 1801, loss: 0.427, 57632/60000 datapoints
2025-03-06 19:02:00,821 - INFO - training batch 1851, loss: 0.217, 59232/60000 datapoints
2025-03-06 19:02:00,923 - INFO - validation batch 1, loss: 0.685, 32/10016 datapoints
2025-03-06 19:02:01,077 - INFO - validation batch 51, loss: 0.605, 1632/10016 datapoints
2025-03-06 19:02:01,229 - INFO - validation batch 101, loss: 0.255, 3232/10016 datapoints
2025-03-06 19:02:01,583 - INFO - validation batch 151, loss: 0.365, 4832/10016 datapoints
2025-03-06 19:02:01,924 - INFO - validation batch 201, loss: 0.366, 6432/10016 datapoints
2025-03-06 19:02:02,206 - INFO - validation batch 251, loss: 0.364, 8032/10016 datapoints
2025-03-06 19:02:02,504 - INFO - validation batch 301, loss: 0.639, 9632/10016 datapoints
2025-03-06 19:02:02,579 - INFO - Epoch 172/800 done.
2025-03-06 19:02:02,579 - INFO - Final validation performance:
Loss: 0.468, top-1 acc: 0.897top-5 acc: 0.897
2025-03-06 19:02:02,580 - INFO - Beginning epoch 173/800
2025-03-06 19:02:02,592 - INFO - training batch 1, loss: 0.419, 32/60000 datapoints
2025-03-06 19:02:02,990 - INFO - training batch 51, loss: 0.552, 1632/60000 datapoints
2025-03-06 19:02:03,388 - INFO - training batch 101, loss: 0.387, 3232/60000 datapoints
2025-03-06 19:02:03,787 - INFO - training batch 151, loss: 0.386, 4832/60000 datapoints
2025-03-06 19:02:04,209 - INFO - training batch 201, loss: 0.274, 6432/60000 datapoints
2025-03-06 19:02:04,833 - INFO - training batch 251, loss: 0.725, 8032/60000 datapoints
2025-03-06 19:02:05,317 - INFO - training batch 301, loss: 0.535, 9632/60000 datapoints
2025-03-06 19:02:05,683 - INFO - training batch 351, loss: 0.388, 11232/60000 datapoints
2025-03-06 19:02:05,988 - INFO - training batch 401, loss: 0.250, 12832/60000 datapoints
2025-03-06 19:02:06,281 - INFO - training batch 451, loss: 0.448, 14432/60000 datapoints
2025-03-06 19:02:06,577 - INFO - training batch 501, loss: 0.230, 16032/60000 datapoints
2025-03-06 19:02:06,869 - INFO - training batch 551, loss: 0.354, 17632/60000 datapoints
2025-03-06 19:02:07,157 - INFO - training batch 601, loss: 0.575, 19232/60000 datapoints
2025-03-06 19:02:07,436 - INFO - training batch 651, loss: 0.233, 20832/60000 datapoints
2025-03-06 19:02:07,739 - INFO - training batch 701, loss: 0.224, 22432/60000 datapoints
2025-03-06 19:02:08,019 - INFO - training batch 751, loss: 0.478, 24032/60000 datapoints
2025-03-06 19:02:08,303 - INFO - training batch 801, loss: 0.690, 25632/60000 datapoints
2025-03-06 19:02:08,580 - INFO - training batch 851, loss: 0.387, 27232/60000 datapoints
2025-03-06 19:02:08,866 - INFO - training batch 901, loss: 0.268, 28832/60000 datapoints
2025-03-06 19:02:09,144 - INFO - training batch 951, loss: 0.192, 30432/60000 datapoints
2025-03-06 19:02:09,458 - INFO - training batch 1001, loss: 0.361, 32032/60000 datapoints
2025-03-06 19:02:09,732 - INFO - training batch 1051, loss: 0.250, 33632/60000 datapoints
2025-03-06 19:02:10,013 - INFO - training batch 1101, loss: 0.318, 35232/60000 datapoints
2025-03-06 19:02:10,297 - INFO - training batch 1151, loss: 0.291, 36832/60000 datapoints
2025-03-06 19:02:10,575 - INFO - training batch 1201, loss: 0.565, 38432/60000 datapoints
2025-03-06 19:02:10,858 - INFO - training batch 1251, loss: 0.225, 40032/60000 datapoints
2025-03-06 19:02:11,137 - INFO - training batch 1301, loss: 0.406, 41632/60000 datapoints
2025-03-06 19:02:11,412 - INFO - training batch 1351, loss: 0.412, 43232/60000 datapoints
2025-03-06 19:02:11,687 - INFO - training batch 1401, loss: 0.431, 44832/60000 datapoints
2025-03-06 19:02:11,967 - INFO - training batch 1451, loss: 0.402, 46432/60000 datapoints
2025-03-06 19:02:12,246 - INFO - training batch 1501, loss: 0.472, 48032/60000 datapoints
2025-03-06 19:02:12,525 - INFO - training batch 1551, loss: 0.440, 49632/60000 datapoints
2025-03-06 19:02:12,809 - INFO - training batch 1601, loss: 0.215, 51232/60000 datapoints
2025-03-06 19:02:13,088 - INFO - training batch 1651, loss: 0.631, 52832/60000 datapoints
2025-03-06 19:02:13,368 - INFO - training batch 1701, loss: 0.290, 54432/60000 datapoints
2025-03-06 19:02:13,652 - INFO - training batch 1751, loss: 0.954, 56032/60000 datapoints
2025-03-06 19:02:13,925 - INFO - training batch 1801, loss: 0.655, 57632/60000 datapoints
2025-03-06 19:02:14,208 - INFO - training batch 1851, loss: 0.207, 59232/60000 datapoints
2025-03-06 19:02:14,361 - INFO - validation batch 1, loss: 0.228, 32/10016 datapoints
2025-03-06 19:02:14,595 - INFO - validation batch 51, loss: 0.228, 1632/10016 datapoints
2025-03-06 19:02:14,836 - INFO - validation batch 101, loss: 0.176, 3232/10016 datapoints
2025-03-06 19:02:15,082 - INFO - validation batch 151, loss: 0.462, 4832/10016 datapoints
2025-03-06 19:02:15,321 - INFO - validation batch 201, loss: 0.271, 6432/10016 datapoints
2025-03-06 19:02:15,558 - INFO - validation batch 251, loss: 0.595, 8032/10016 datapoints
2025-03-06 19:02:15,795 - INFO - validation batch 301, loss: 0.394, 9632/10016 datapoints
2025-03-06 19:02:15,849 - INFO - Epoch 173/800 done.
2025-03-06 19:02:15,850 - INFO - Final validation performance:
Loss: 0.336, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 19:02:15,851 - INFO - Beginning epoch 174/800
2025-03-06 19:02:15,861 - INFO - training batch 1, loss: 0.481, 32/60000 datapoints
2025-03-06 19:02:16,174 - INFO - training batch 51, loss: 0.506, 1632/60000 datapoints
2025-03-06 19:02:16,459 - INFO - training batch 101, loss: 0.237, 3232/60000 datapoints
2025-03-06 19:02:16,776 - INFO - training batch 151, loss: 0.331, 4832/60000 datapoints
2025-03-06 19:02:17,068 - INFO - training batch 201, loss: 0.603, 6432/60000 datapoints
2025-03-06 19:02:17,369 - INFO - training batch 251, loss: 0.421, 8032/60000 datapoints
2025-03-06 19:02:17,668 - INFO - training batch 301, loss: 0.312, 9632/60000 datapoints
2025-03-06 19:02:17,958 - INFO - training batch 351, loss: 0.402, 11232/60000 datapoints
2025-03-06 19:02:18,251 - INFO - training batch 401, loss: 0.329, 12832/60000 datapoints
2025-03-06 19:02:18,548 - INFO - training batch 451, loss: 0.600, 14432/60000 datapoints
2025-03-06 19:02:18,833 - INFO - training batch 501, loss: 0.340, 16032/60000 datapoints
2025-03-06 19:02:19,109 - INFO - training batch 551, loss: 0.420, 17632/60000 datapoints
2025-03-06 19:02:19,387 - INFO - training batch 601, loss: 0.241, 19232/60000 datapoints
2025-03-06 19:02:19,744 - INFO - training batch 651, loss: 0.211, 20832/60000 datapoints
2025-03-06 19:02:20,024 - INFO - training batch 701, loss: 0.381, 22432/60000 datapoints
2025-03-06 19:02:20,313 - INFO - training batch 751, loss: 0.402, 24032/60000 datapoints
2025-03-06 19:02:20,592 - INFO - training batch 801, loss: 0.248, 25632/60000 datapoints
2025-03-06 19:02:20,874 - INFO - training batch 851, loss: 0.486, 27232/60000 datapoints
2025-03-06 19:02:21,152 - INFO - training batch 901, loss: 0.529, 28832/60000 datapoints
2025-03-06 19:02:21,433 - INFO - training batch 951, loss: 0.228, 30432/60000 datapoints
2025-03-06 19:02:21,711 - INFO - training batch 1001, loss: 0.529, 32032/60000 datapoints
2025-03-06 19:02:21,986 - INFO - training batch 1051, loss: 0.257, 33632/60000 datapoints
2025-03-06 19:02:22,265 - INFO - training batch 1101, loss: 0.193, 35232/60000 datapoints
2025-03-06 19:02:22,541 - INFO - training batch 1151, loss: 0.340, 36832/60000 datapoints
2025-03-06 19:02:22,839 - INFO - training batch 1201, loss: 0.289, 38432/60000 datapoints
2025-03-06 19:02:23,118 - INFO - training batch 1251, loss: 0.316, 40032/60000 datapoints
2025-03-06 19:02:23,392 - INFO - training batch 1301, loss: 0.312, 41632/60000 datapoints
2025-03-06 19:02:23,677 - INFO - training batch 1351, loss: 0.480, 43232/60000 datapoints
2025-03-06 19:02:23,951 - INFO - training batch 1401, loss: 0.568, 44832/60000 datapoints
2025-03-06 19:02:24,225 - INFO - training batch 1451, loss: 0.649, 46432/60000 datapoints
2025-03-06 19:02:24,499 - INFO - training batch 1501, loss: 0.341, 48032/60000 datapoints
2025-03-06 19:02:24,781 - INFO - training batch 1551, loss: 0.422, 49632/60000 datapoints
2025-03-06 19:02:25,064 - INFO - training batch 1601, loss: 0.624, 51232/60000 datapoints
2025-03-06 19:02:25,336 - INFO - training batch 1651, loss: 0.473, 52832/60000 datapoints
2025-03-06 19:02:25,621 - INFO - training batch 1701, loss: 0.353, 54432/60000 datapoints
2025-03-06 19:02:25,892 - INFO - training batch 1751, loss: 0.612, 56032/60000 datapoints
2025-03-06 19:02:26,167 - INFO - training batch 1801, loss: 0.286, 57632/60000 datapoints
2025-03-06 19:02:26,447 - INFO - training batch 1851, loss: 0.152, 59232/60000 datapoints
2025-03-06 19:02:26,613 - INFO - validation batch 1, loss: 0.275, 32/10016 datapoints
2025-03-06 19:02:26,838 - INFO - validation batch 51, loss: 0.395, 1632/10016 datapoints
2025-03-06 19:02:27,065 - INFO - validation batch 101, loss: 0.454, 3232/10016 datapoints
2025-03-06 19:02:27,288 - INFO - validation batch 151, loss: 0.246, 4832/10016 datapoints
2025-03-06 19:02:27,517 - INFO - validation batch 201, loss: 0.228, 6432/10016 datapoints
2025-03-06 19:02:27,748 - INFO - validation batch 251, loss: 0.503, 8032/10016 datapoints
2025-03-06 19:02:27,970 - INFO - validation batch 301, loss: 0.248, 9632/10016 datapoints
2025-03-06 19:02:28,025 - INFO - Epoch 174/800 done.
2025-03-06 19:02:28,025 - INFO - Final validation performance:
Loss: 0.335, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 19:02:28,026 - INFO - Beginning epoch 175/800
2025-03-06 19:02:28,035 - INFO - training batch 1, loss: 0.293, 32/60000 datapoints
2025-03-06 19:02:28,371 - INFO - training batch 51, loss: 0.269, 1632/60000 datapoints
2025-03-06 19:02:28,710 - INFO - training batch 101, loss: 0.380, 3232/60000 datapoints
2025-03-06 19:02:29,026 - INFO - training batch 151, loss: 0.350, 4832/60000 datapoints
2025-03-06 19:02:29,326 - INFO - training batch 201, loss: 0.354, 6432/60000 datapoints
2025-03-06 19:02:29,647 - INFO - training batch 251, loss: 0.537, 8032/60000 datapoints
2025-03-06 19:02:29,931 - INFO - training batch 301, loss: 0.567, 9632/60000 datapoints
2025-03-06 19:02:30,204 - INFO - training batch 351, loss: 0.334, 11232/60000 datapoints
2025-03-06 19:02:30,486 - INFO - training batch 401, loss: 0.364, 12832/60000 datapoints
2025-03-06 19:02:30,767 - INFO - training batch 451, loss: 0.216, 14432/60000 datapoints
2025-03-06 19:02:31,046 - INFO - training batch 501, loss: 0.306, 16032/60000 datapoints
2025-03-06 19:02:31,324 - INFO - training batch 551, loss: 0.236, 17632/60000 datapoints
2025-03-06 19:02:31,601 - INFO - training batch 601, loss: 0.732, 19232/60000 datapoints
2025-03-06 19:02:32,000 - INFO - training batch 651, loss: 0.429, 20832/60000 datapoints
2025-03-06 19:02:32,296 - INFO - training batch 701, loss: 0.428, 22432/60000 datapoints
2025-03-06 19:02:32,576 - INFO - training batch 751, loss: 0.209, 24032/60000 datapoints
2025-03-06 19:02:32,859 - INFO - training batch 801, loss: 0.368, 25632/60000 datapoints
2025-03-06 19:02:33,139 - INFO - training batch 851, loss: 0.705, 27232/60000 datapoints
2025-03-06 19:02:33,414 - INFO - training batch 901, loss: 0.691, 28832/60000 datapoints
2025-03-06 19:02:33,686 - INFO - training batch 951, loss: 0.232, 30432/60000 datapoints
2025-03-06 19:02:33,963 - INFO - training batch 1001, loss: 0.317, 32032/60000 datapoints
2025-03-06 19:02:34,238 - INFO - training batch 1051, loss: 0.508, 33632/60000 datapoints
2025-03-06 19:02:34,519 - INFO - training batch 1101, loss: 0.479, 35232/60000 datapoints
2025-03-06 19:02:34,806 - INFO - training batch 1151, loss: 0.348, 36832/60000 datapoints
2025-03-06 19:02:35,083 - INFO - training batch 1201, loss: 0.494, 38432/60000 datapoints
2025-03-06 19:02:35,362 - INFO - training batch 1251, loss: 0.105, 40032/60000 datapoints
2025-03-06 19:02:35,640 - INFO - training batch 1301, loss: 0.256, 41632/60000 datapoints
2025-03-06 19:02:35,912 - INFO - training batch 1351, loss: 0.382, 43232/60000 datapoints
2025-03-06 19:02:36,190 - INFO - training batch 1401, loss: 0.351, 44832/60000 datapoints
2025-03-06 19:02:36,471 - INFO - training batch 1451, loss: 0.296, 46432/60000 datapoints
2025-03-06 19:02:36,749 - INFO - training batch 1501, loss: 0.352, 48032/60000 datapoints
2025-03-06 19:02:37,020 - INFO - training batch 1551, loss: 0.361, 49632/60000 datapoints
2025-03-06 19:02:37,297 - INFO - training batch 1601, loss: 0.312, 51232/60000 datapoints
2025-03-06 19:02:37,569 - INFO - training batch 1651, loss: 0.474, 52832/60000 datapoints
2025-03-06 19:02:37,912 - INFO - training batch 1701, loss: 0.245, 54432/60000 datapoints
2025-03-06 19:02:38,189 - INFO - training batch 1751, loss: 0.422, 56032/60000 datapoints
2025-03-06 19:02:38,462 - INFO - training batch 1801, loss: 0.407, 57632/60000 datapoints
2025-03-06 19:02:38,739 - INFO - training batch 1851, loss: 0.301, 59232/60000 datapoints
2025-03-06 19:02:38,895 - INFO - validation batch 1, loss: 0.200, 32/10016 datapoints
2025-03-06 19:02:39,117 - INFO - validation batch 51, loss: 0.348, 1632/10016 datapoints
2025-03-06 19:02:39,340 - INFO - validation batch 101, loss: 0.530, 3232/10016 datapoints
2025-03-06 19:02:39,564 - INFO - validation batch 151, loss: 0.292, 4832/10016 datapoints
2025-03-06 19:02:39,818 - INFO - validation batch 201, loss: 0.248, 6432/10016 datapoints
2025-03-06 19:02:40,047 - INFO - validation batch 251, loss: 0.306, 8032/10016 datapoints
2025-03-06 19:02:40,286 - INFO - validation batch 301, loss: 0.213, 9632/10016 datapoints
2025-03-06 19:02:40,345 - INFO - Epoch 175/800 done.
2025-03-06 19:02:40,345 - INFO - Final validation performance:
Loss: 0.305, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 19:02:40,346 - INFO - Beginning epoch 176/800
2025-03-06 19:02:40,356 - INFO - training batch 1, loss: 0.506, 32/60000 datapoints
2025-03-06 19:02:40,647 - INFO - training batch 51, loss: 0.273, 1632/60000 datapoints
2025-03-06 19:02:40,939 - INFO - training batch 101, loss: 0.434, 3232/60000 datapoints
2025-03-06 19:02:41,215 - INFO - training batch 151, loss: 0.976, 4832/60000 datapoints
2025-03-06 19:02:41,500 - INFO - training batch 201, loss: 0.432, 6432/60000 datapoints
2025-03-06 19:02:41,780 - INFO - training batch 251, loss: 0.388, 8032/60000 datapoints
2025-03-06 19:02:42,054 - INFO - training batch 301, loss: 0.246, 9632/60000 datapoints
2025-03-06 19:02:42,345 - INFO - training batch 351, loss: 0.358, 11232/60000 datapoints
2025-03-06 19:02:42,625 - INFO - training batch 401, loss: 0.300, 12832/60000 datapoints
2025-03-06 19:02:42,903 - INFO - training batch 451, loss: 0.409, 14432/60000 datapoints
2025-03-06 19:02:43,184 - INFO - training batch 501, loss: 0.271, 16032/60000 datapoints
2025-03-06 19:02:43,459 - INFO - training batch 551, loss: 0.288, 17632/60000 datapoints
2025-03-06 19:02:43,740 - INFO - training batch 601, loss: 0.258, 19232/60000 datapoints
2025-03-06 19:02:44,015 - INFO - training batch 651, loss: 0.187, 20832/60000 datapoints
2025-03-06 19:02:44,304 - INFO - training batch 701, loss: 0.434, 22432/60000 datapoints
2025-03-06 19:02:44,586 - INFO - training batch 751, loss: 0.485, 24032/60000 datapoints
2025-03-06 19:02:44,867 - INFO - training batch 801, loss: 0.302, 25632/60000 datapoints
2025-03-06 19:02:45,149 - INFO - training batch 851, loss: 0.381, 27232/60000 datapoints
2025-03-06 19:02:45,425 - INFO - training batch 901, loss: 0.521, 28832/60000 datapoints
2025-03-06 19:02:45,702 - INFO - training batch 951, loss: 0.407, 30432/60000 datapoints
2025-03-06 19:02:45,977 - INFO - training batch 1001, loss: 0.559, 32032/60000 datapoints
2025-03-06 19:02:46,255 - INFO - training batch 1051, loss: 0.273, 33632/60000 datapoints
2025-03-06 19:02:46,528 - INFO - training batch 1101, loss: 0.374, 35232/60000 datapoints
2025-03-06 19:02:46,804 - INFO - training batch 1151, loss: 0.406, 36832/60000 datapoints
2025-03-06 19:02:47,075 - INFO - training batch 1201, loss: 0.446, 38432/60000 datapoints
2025-03-06 19:02:47,364 - INFO - training batch 1251, loss: 0.380, 40032/60000 datapoints
2025-03-06 19:02:47,737 - INFO - training batch 1301, loss: 0.741, 41632/60000 datapoints
2025-03-06 19:02:48,242 - INFO - training batch 1351, loss: 0.473, 43232/60000 datapoints
2025-03-06 19:02:48,650 - INFO - training batch 1401, loss: 0.255, 44832/60000 datapoints
2025-03-06 19:02:48,939 - INFO - training batch 1451, loss: 0.665, 46432/60000 datapoints
2025-03-06 19:02:49,239 - INFO - training batch 1501, loss: 0.364, 48032/60000 datapoints
2025-03-06 19:02:49,544 - INFO - training batch 1551, loss: 0.274, 49632/60000 datapoints
2025-03-06 19:02:50,215 - INFO - training batch 1601, loss: 0.209, 51232/60000 datapoints
2025-03-06 19:02:50,578 - INFO - training batch 1651, loss: 0.762, 52832/60000 datapoints
2025-03-06 19:02:50,894 - INFO - training batch 1701, loss: 0.298, 54432/60000 datapoints
2025-03-06 19:02:51,347 - INFO - training batch 1751, loss: 0.168, 56032/60000 datapoints
2025-03-06 19:02:51,806 - INFO - training batch 1801, loss: 0.626, 57632/60000 datapoints
2025-03-06 19:02:52,148 - INFO - training batch 1851, loss: 0.614, 59232/60000 datapoints
2025-03-06 19:02:52,301 - INFO - validation batch 1, loss: 0.267, 32/10016 datapoints
2025-03-06 19:02:52,560 - INFO - validation batch 51, loss: 0.270, 1632/10016 datapoints
2025-03-06 19:02:52,821 - INFO - validation batch 101, loss: 0.260, 3232/10016 datapoints
2025-03-06 19:02:53,053 - INFO - validation batch 151, loss: 0.212, 4832/10016 datapoints
2025-03-06 19:02:53,283 - INFO - validation batch 201, loss: 0.179, 6432/10016 datapoints
2025-03-06 19:02:53,554 - INFO - validation batch 251, loss: 0.310, 8032/10016 datapoints
2025-03-06 19:02:53,803 - INFO - validation batch 301, loss: 0.290, 9632/10016 datapoints
2025-03-06 19:02:53,859 - INFO - Epoch 176/800 done.
2025-03-06 19:02:53,859 - INFO - Final validation performance:
Loss: 0.256, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 19:02:53,860 - INFO - Beginning epoch 177/800
2025-03-06 19:02:53,871 - INFO - training batch 1, loss: 0.242, 32/60000 datapoints
2025-03-06 19:02:54,180 - INFO - training batch 51, loss: 0.326, 1632/60000 datapoints
2025-03-06 19:02:54,461 - INFO - training batch 101, loss: 0.518, 3232/60000 datapoints
2025-03-06 19:02:54,764 - INFO - training batch 151, loss: 0.447, 4832/60000 datapoints
2025-03-06 19:02:55,059 - INFO - training batch 201, loss: 0.266, 6432/60000 datapoints
2025-03-06 19:02:55,462 - INFO - training batch 251, loss: 0.350, 8032/60000 datapoints
2025-03-06 19:02:55,684 - INFO - training batch 301, loss: 0.154, 9632/60000 datapoints
2025-03-06 19:02:55,899 - INFO - training batch 351, loss: 0.218, 11232/60000 datapoints
2025-03-06 19:02:56,121 - INFO - training batch 401, loss: 0.190, 12832/60000 datapoints
2025-03-06 19:02:56,340 - INFO - training batch 451, loss: 0.449, 14432/60000 datapoints
2025-03-06 19:02:56,560 - INFO - training batch 501, loss: 0.673, 16032/60000 datapoints
2025-03-06 19:02:56,778 - INFO - training batch 551, loss: 0.420, 17632/60000 datapoints
2025-03-06 19:02:56,978 - INFO - training batch 601, loss: 0.431, 19232/60000 datapoints
2025-03-06 19:02:57,179 - INFO - training batch 651, loss: 0.472, 20832/60000 datapoints
2025-03-06 19:02:57,398 - INFO - training batch 701, loss: 0.714, 22432/60000 datapoints
2025-03-06 19:02:57,598 - INFO - training batch 751, loss: 0.435, 24032/60000 datapoints
2025-03-06 19:02:57,801 - INFO - training batch 801, loss: 0.517, 25632/60000 datapoints
2025-03-06 19:02:58,013 - INFO - training batch 851, loss: 0.547, 27232/60000 datapoints
2025-03-06 19:02:58,432 - INFO - training batch 901, loss: 0.500, 28832/60000 datapoints
2025-03-06 19:02:58,683 - INFO - training batch 951, loss: 0.338, 30432/60000 datapoints
2025-03-06 19:02:58,926 - INFO - training batch 1001, loss: 0.336, 32032/60000 datapoints
2025-03-06 19:02:59,186 - INFO - training batch 1051, loss: 0.273, 33632/60000 datapoints
2025-03-06 19:02:59,743 - INFO - training batch 1101, loss: 0.477, 35232/60000 datapoints
2025-03-06 19:03:00,017 - INFO - training batch 1151, loss: 0.655, 36832/60000 datapoints
2025-03-06 19:03:00,293 - INFO - training batch 1201, loss: 0.535, 38432/60000 datapoints
2025-03-06 19:03:00,558 - INFO - training batch 1251, loss: 0.475, 40032/60000 datapoints
2025-03-06 19:03:00,825 - INFO - training batch 1301, loss: 0.360, 41632/60000 datapoints
2025-03-06 19:03:01,061 - INFO - training batch 1351, loss: 0.497, 43232/60000 datapoints
2025-03-06 19:03:01,290 - INFO - training batch 1401, loss: 0.417, 44832/60000 datapoints
2025-03-06 19:03:01,553 - INFO - training batch 1451, loss: 0.167, 46432/60000 datapoints
2025-03-06 19:03:01,771 - INFO - training batch 1501, loss: 0.496, 48032/60000 datapoints
2025-03-06 19:03:01,969 - INFO - training batch 1551, loss: 0.152, 49632/60000 datapoints
2025-03-06 19:03:02,168 - INFO - training batch 1601, loss: 0.562, 51232/60000 datapoints
2025-03-06 19:03:02,380 - INFO - training batch 1651, loss: 0.164, 52832/60000 datapoints
2025-03-06 19:03:02,585 - INFO - training batch 1701, loss: 0.542, 54432/60000 datapoints
2025-03-06 19:03:02,790 - INFO - training batch 1751, loss: 0.212, 56032/60000 datapoints
2025-03-06 19:03:02,988 - INFO - training batch 1801, loss: 0.273, 57632/60000 datapoints
2025-03-06 19:03:03,191 - INFO - training batch 1851, loss: 0.576, 59232/60000 datapoints
2025-03-06 19:03:03,296 - INFO - validation batch 1, loss: 0.211, 32/10016 datapoints
2025-03-06 19:03:03,457 - INFO - validation batch 51, loss: 0.632, 1632/10016 datapoints
2025-03-06 19:03:03,616 - INFO - validation batch 101, loss: 0.337, 3232/10016 datapoints
2025-03-06 19:03:03,773 - INFO - validation batch 151, loss: 0.338, 4832/10016 datapoints
2025-03-06 19:03:03,929 - INFO - validation batch 201, loss: 0.380, 6432/10016 datapoints
2025-03-06 19:03:04,085 - INFO - validation batch 251, loss: 0.448, 8032/10016 datapoints
2025-03-06 19:03:04,241 - INFO - validation batch 301, loss: 0.184, 9632/10016 datapoints
2025-03-06 19:03:04,279 - INFO - Epoch 177/800 done.
2025-03-06 19:03:04,279 - INFO - Final validation performance:
Loss: 0.361, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 19:03:04,280 - INFO - Beginning epoch 178/800
2025-03-06 19:03:04,288 - INFO - training batch 1, loss: 0.366, 32/60000 datapoints
2025-03-06 19:03:04,492 - INFO - training batch 51, loss: 0.178, 1632/60000 datapoints
2025-03-06 19:03:04,708 - INFO - training batch 101, loss: 0.237, 3232/60000 datapoints
2025-03-06 19:03:04,908 - INFO - training batch 151, loss: 0.302, 4832/60000 datapoints
2025-03-06 19:03:05,115 - INFO - training batch 201, loss: 0.287, 6432/60000 datapoints
2025-03-06 19:03:05,320 - INFO - training batch 251, loss: 0.438, 8032/60000 datapoints
2025-03-06 19:03:05,529 - INFO - training batch 301, loss: 0.476, 9632/60000 datapoints
2025-03-06 19:03:05,726 - INFO - training batch 351, loss: 0.275, 11232/60000 datapoints
2025-03-06 19:03:05,921 - INFO - training batch 401, loss: 0.278, 12832/60000 datapoints
2025-03-06 19:03:06,123 - INFO - training batch 451, loss: 0.288, 14432/60000 datapoints
2025-03-06 19:03:06,325 - INFO - training batch 501, loss: 0.406, 16032/60000 datapoints
2025-03-06 19:03:06,534 - INFO - training batch 551, loss: 0.360, 17632/60000 datapoints
2025-03-06 19:03:06,782 - INFO - training batch 601, loss: 0.521, 19232/60000 datapoints
2025-03-06 19:03:07,038 - INFO - training batch 651, loss: 0.379, 20832/60000 datapoints
2025-03-06 19:03:07,250 - INFO - training batch 701, loss: 0.365, 22432/60000 datapoints
2025-03-06 19:03:07,452 - INFO - training batch 751, loss: 0.224, 24032/60000 datapoints
2025-03-06 19:03:07,654 - INFO - training batch 801, loss: 0.541, 25632/60000 datapoints
2025-03-06 19:03:07,854 - INFO - training batch 851, loss: 0.276, 27232/60000 datapoints
2025-03-06 19:03:08,051 - INFO - training batch 901, loss: 0.402, 28832/60000 datapoints
2025-03-06 19:03:08,287 - INFO - training batch 951, loss: 0.521, 30432/60000 datapoints
2025-03-06 19:03:08,540 - INFO - training batch 1001, loss: 0.577, 32032/60000 datapoints
2025-03-06 19:03:08,792 - INFO - training batch 1051, loss: 0.463, 33632/60000 datapoints
2025-03-06 19:03:09,009 - INFO - training batch 1101, loss: 0.305, 35232/60000 datapoints
2025-03-06 19:03:09,236 - INFO - training batch 1151, loss: 0.212, 36832/60000 datapoints
2025-03-06 19:03:09,560 - INFO - training batch 1201, loss: 0.907, 38432/60000 datapoints
2025-03-06 19:03:09,815 - INFO - training batch 1251, loss: 0.181, 40032/60000 datapoints
2025-03-06 19:03:10,048 - INFO - training batch 1301, loss: 0.453, 41632/60000 datapoints
2025-03-06 19:03:10,265 - INFO - training batch 1351, loss: 0.238, 43232/60000 datapoints
2025-03-06 19:03:10,469 - INFO - training batch 1401, loss: 0.461, 44832/60000 datapoints
2025-03-06 19:03:10,687 - INFO - training batch 1451, loss: 0.494, 46432/60000 datapoints
2025-03-06 19:03:10,883 - INFO - training batch 1501, loss: 0.249, 48032/60000 datapoints
2025-03-06 19:03:11,078 - INFO - training batch 1551, loss: 0.369, 49632/60000 datapoints
2025-03-06 19:03:11,274 - INFO - training batch 1601, loss: 0.354, 51232/60000 datapoints
2025-03-06 19:03:11,475 - INFO - training batch 1651, loss: 0.350, 52832/60000 datapoints
2025-03-06 19:03:11,700 - INFO - training batch 1701, loss: 0.503, 54432/60000 datapoints
2025-03-06 19:03:11,899 - INFO - training batch 1751, loss: 0.461, 56032/60000 datapoints
2025-03-06 19:03:12,095 - INFO - training batch 1801, loss: 0.876, 57632/60000 datapoints
2025-03-06 19:03:12,292 - INFO - training batch 1851, loss: 0.157, 59232/60000 datapoints
2025-03-06 19:03:12,399 - INFO - validation batch 1, loss: 0.485, 32/10016 datapoints
2025-03-06 19:03:12,554 - INFO - validation batch 51, loss: 0.285, 1632/10016 datapoints
2025-03-06 19:03:12,711 - INFO - validation batch 101, loss: 0.243, 3232/10016 datapoints
2025-03-06 19:03:12,882 - INFO - validation batch 151, loss: 0.401, 4832/10016 datapoints
2025-03-06 19:03:13,039 - INFO - validation batch 201, loss: 0.374, 6432/10016 datapoints
2025-03-06 19:03:13,192 - INFO - validation batch 251, loss: 0.425, 8032/10016 datapoints
2025-03-06 19:03:13,347 - INFO - validation batch 301, loss: 0.292, 9632/10016 datapoints
2025-03-06 19:03:13,387 - INFO - Epoch 178/800 done.
2025-03-06 19:03:13,387 - INFO - Final validation performance:
Loss: 0.358, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 19:03:13,388 - INFO - Beginning epoch 179/800
2025-03-06 19:03:13,396 - INFO - training batch 1, loss: 0.500, 32/60000 datapoints
2025-03-06 19:03:13,589 - INFO - training batch 51, loss: 0.460, 1632/60000 datapoints
2025-03-06 19:03:13,788 - INFO - training batch 101, loss: 0.447, 3232/60000 datapoints
2025-03-06 19:03:13,987 - INFO - training batch 151, loss: 0.281, 4832/60000 datapoints
2025-03-06 19:03:14,183 - INFO - training batch 201, loss: 0.745, 6432/60000 datapoints
2025-03-06 19:03:14,385 - INFO - training batch 251, loss: 0.414, 8032/60000 datapoints
2025-03-06 19:03:14,613 - INFO - training batch 301, loss: 0.338, 9632/60000 datapoints
2025-03-06 19:03:14,818 - INFO - training batch 351, loss: 0.348, 11232/60000 datapoints
2025-03-06 19:03:15,025 - INFO - training batch 401, loss: 0.402, 12832/60000 datapoints
2025-03-06 19:03:15,228 - INFO - training batch 451, loss: 0.438, 14432/60000 datapoints
2025-03-06 19:03:15,429 - INFO - training batch 501, loss: 0.261, 16032/60000 datapoints
2025-03-06 19:03:15,640 - INFO - training batch 551, loss: 0.533, 17632/60000 datapoints
2025-03-06 19:03:15,840 - INFO - training batch 601, loss: 0.328, 19232/60000 datapoints
2025-03-06 19:03:16,042 - INFO - training batch 651, loss: 0.238, 20832/60000 datapoints
2025-03-06 19:03:16,265 - INFO - training batch 701, loss: 0.620, 22432/60000 datapoints
2025-03-06 19:03:16,465 - INFO - training batch 751, loss: 0.354, 24032/60000 datapoints
2025-03-06 19:03:16,668 - INFO - training batch 801, loss: 0.342, 25632/60000 datapoints
2025-03-06 19:03:16,869 - INFO - training batch 851, loss: 0.202, 27232/60000 datapoints
2025-03-06 19:03:17,074 - INFO - training batch 901, loss: 0.653, 28832/60000 datapoints
2025-03-06 19:03:17,270 - INFO - training batch 951, loss: 0.260, 30432/60000 datapoints
2025-03-06 19:03:17,466 - INFO - training batch 1001, loss: 0.385, 32032/60000 datapoints
2025-03-06 19:03:17,800 - INFO - training batch 1051, loss: 0.396, 33632/60000 datapoints
2025-03-06 19:03:18,039 - INFO - training batch 1101, loss: 0.421, 35232/60000 datapoints
2025-03-06 19:03:18,237 - INFO - training batch 1151, loss: 0.160, 36832/60000 datapoints
2025-03-06 19:03:18,448 - INFO - training batch 1201, loss: 0.675, 38432/60000 datapoints
2025-03-06 19:03:18,646 - INFO - training batch 1251, loss: 0.441, 40032/60000 datapoints
2025-03-06 19:03:18,845 - INFO - training batch 1301, loss: 0.450, 41632/60000 datapoints
2025-03-06 19:03:19,047 - INFO - training batch 1351, loss: 0.762, 43232/60000 datapoints
2025-03-06 19:03:19,245 - INFO - training batch 1401, loss: 0.335, 44832/60000 datapoints
2025-03-06 19:03:19,447 - INFO - training batch 1451, loss: 0.517, 46432/60000 datapoints
2025-03-06 19:03:19,648 - INFO - training batch 1501, loss: 0.496, 48032/60000 datapoints
2025-03-06 19:03:19,849 - INFO - training batch 1551, loss: 0.509, 49632/60000 datapoints
2025-03-06 19:03:20,049 - INFO - training batch 1601, loss: 0.333, 51232/60000 datapoints
2025-03-06 19:03:20,268 - INFO - training batch 1651, loss: 0.433, 52832/60000 datapoints
2025-03-06 19:03:20,465 - INFO - training batch 1701, loss: 0.279, 54432/60000 datapoints
2025-03-06 19:03:20,662 - INFO - training batch 1751, loss: 0.308, 56032/60000 datapoints
2025-03-06 19:03:20,858 - INFO - training batch 1801, loss: 0.544, 57632/60000 datapoints
2025-03-06 19:03:21,054 - INFO - training batch 1851, loss: 0.194, 59232/60000 datapoints
2025-03-06 19:03:21,161 - INFO - validation batch 1, loss: 0.368, 32/10016 datapoints
2025-03-06 19:03:21,325 - INFO - validation batch 51, loss: 0.375, 1632/10016 datapoints
2025-03-06 19:03:21,484 - INFO - validation batch 101, loss: 0.593, 3232/10016 datapoints
2025-03-06 19:03:21,644 - INFO - validation batch 151, loss: 0.185, 4832/10016 datapoints
2025-03-06 19:03:21,835 - INFO - validation batch 201, loss: 0.358, 6432/10016 datapoints
2025-03-06 19:03:22,037 - INFO - validation batch 251, loss: 0.296, 8032/10016 datapoints
2025-03-06 19:03:22,202 - INFO - validation batch 301, loss: 0.452, 9632/10016 datapoints
2025-03-06 19:03:22,240 - INFO - Epoch 179/800 done.
2025-03-06 19:03:22,240 - INFO - Final validation performance:
Loss: 0.375, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 19:03:22,241 - INFO - Beginning epoch 180/800
2025-03-06 19:03:22,248 - INFO - training batch 1, loss: 0.206, 32/60000 datapoints
2025-03-06 19:03:22,468 - INFO - training batch 51, loss: 0.416, 1632/60000 datapoints
2025-03-06 19:03:22,670 - INFO - training batch 101, loss: 0.300, 3232/60000 datapoints
2025-03-06 19:03:22,878 - INFO - training batch 151, loss: 0.374, 4832/60000 datapoints
2025-03-06 19:03:23,099 - INFO - training batch 201, loss: 0.260, 6432/60000 datapoints
2025-03-06 19:03:23,310 - INFO - training batch 251, loss: 0.203, 8032/60000 datapoints
2025-03-06 19:03:23,504 - INFO - training batch 301, loss: 0.381, 9632/60000 datapoints
2025-03-06 19:03:23,703 - INFO - training batch 351, loss: 0.408, 11232/60000 datapoints
2025-03-06 19:03:23,896 - INFO - training batch 401, loss: 0.267, 12832/60000 datapoints
2025-03-06 19:03:24,093 - INFO - training batch 451, loss: 0.426, 14432/60000 datapoints
2025-03-06 19:03:24,287 - INFO - training batch 501, loss: 0.493, 16032/60000 datapoints
2025-03-06 19:03:24,481 - INFO - training batch 551, loss: 0.426, 17632/60000 datapoints
2025-03-06 19:03:24,679 - INFO - training batch 601, loss: 0.210, 19232/60000 datapoints
2025-03-06 19:03:24,876 - INFO - training batch 651, loss: 0.413, 20832/60000 datapoints
2025-03-06 19:03:25,073 - INFO - training batch 701, loss: 0.319, 22432/60000 datapoints
2025-03-06 19:03:25,274 - INFO - training batch 751, loss: 0.523, 24032/60000 datapoints
2025-03-06 19:03:25,539 - INFO - training batch 801, loss: 0.209, 25632/60000 datapoints
2025-03-06 19:03:25,758 - INFO - training batch 851, loss: 0.889, 27232/60000 datapoints
2025-03-06 19:03:25,954 - INFO - training batch 901, loss: 0.481, 28832/60000 datapoints
2025-03-06 19:03:26,146 - INFO - training batch 951, loss: 0.429, 30432/60000 datapoints
2025-03-06 19:03:26,337 - INFO - training batch 1001, loss: 0.463, 32032/60000 datapoints
2025-03-06 19:03:26,548 - INFO - training batch 1051, loss: 0.144, 33632/60000 datapoints
2025-03-06 19:03:26,764 - INFO - training batch 1101, loss: 0.303, 35232/60000 datapoints
2025-03-06 19:03:26,973 - INFO - training batch 1151, loss: 0.628, 36832/60000 datapoints
2025-03-06 19:03:27,177 - INFO - training batch 1201, loss: 0.347, 38432/60000 datapoints
2025-03-06 19:03:27,374 - INFO - training batch 1251, loss: 0.463, 40032/60000 datapoints
2025-03-06 19:03:27,573 - INFO - training batch 1301, loss: 0.296, 41632/60000 datapoints
2025-03-06 19:03:27,770 - INFO - training batch 1351, loss: 0.519, 43232/60000 datapoints
2025-03-06 19:03:27,966 - INFO - training batch 1401, loss: 0.577, 44832/60000 datapoints
2025-03-06 19:03:28,161 - INFO - training batch 1451, loss: 0.558, 46432/60000 datapoints
2025-03-06 19:03:28,356 - INFO - training batch 1501, loss: 0.416, 48032/60000 datapoints
2025-03-06 19:03:28,560 - INFO - training batch 1551, loss: 0.598, 49632/60000 datapoints
2025-03-06 19:03:28,769 - INFO - training batch 1601, loss: 0.374, 51232/60000 datapoints
2025-03-06 19:03:28,966 - INFO - training batch 1651, loss: 0.440, 52832/60000 datapoints
2025-03-06 19:03:29,160 - INFO - training batch 1701, loss: 0.413, 54432/60000 datapoints
2025-03-06 19:03:29,356 - INFO - training batch 1751, loss: 0.591, 56032/60000 datapoints
2025-03-06 19:03:29,550 - INFO - training batch 1801, loss: 0.476, 57632/60000 datapoints
2025-03-06 19:03:29,754 - INFO - training batch 1851, loss: 0.425, 59232/60000 datapoints
2025-03-06 19:03:29,859 - INFO - validation batch 1, loss: 0.948, 32/10016 datapoints
2025-03-06 19:03:30,173 - INFO - validation batch 51, loss: 0.246, 1632/10016 datapoints
2025-03-06 19:03:30,349 - INFO - validation batch 101, loss: 0.595, 3232/10016 datapoints
2025-03-06 19:03:30,508 - INFO - validation batch 151, loss: 0.485, 4832/10016 datapoints
2025-03-06 19:03:30,663 - INFO - validation batch 201, loss: 0.325, 6432/10016 datapoints
2025-03-06 19:03:30,815 - INFO - validation batch 251, loss: 0.626, 8032/10016 datapoints
2025-03-06 19:03:30,971 - INFO - validation batch 301, loss: 0.568, 9632/10016 datapoints
2025-03-06 19:03:31,008 - INFO - Epoch 180/800 done.
2025-03-06 19:03:31,009 - INFO - Final validation performance:
Loss: 0.542, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 19:03:31,009 - INFO - Beginning epoch 181/800
2025-03-06 19:03:31,016 - INFO - training batch 1, loss: 0.115, 32/60000 datapoints
2025-03-06 19:03:31,230 - INFO - training batch 51, loss: 0.403, 1632/60000 datapoints
2025-03-06 19:03:31,426 - INFO - training batch 101, loss: 0.231, 3232/60000 datapoints
2025-03-06 19:03:31,632 - INFO - training batch 151, loss: 0.252, 4832/60000 datapoints
2025-03-06 19:03:31,830 - INFO - training batch 201, loss: 0.288, 6432/60000 datapoints
2025-03-06 19:03:32,029 - INFO - training batch 251, loss: 0.363, 8032/60000 datapoints
2025-03-06 19:03:32,227 - INFO - training batch 301, loss: 0.251, 9632/60000 datapoints
2025-03-06 19:03:32,425 - INFO - training batch 351, loss: 0.356, 11232/60000 datapoints
2025-03-06 19:03:32,622 - INFO - training batch 401, loss: 0.505, 12832/60000 datapoints
2025-03-06 19:03:32,820 - INFO - training batch 451, loss: 0.505, 14432/60000 datapoints
2025-03-06 19:03:33,025 - INFO - training batch 501, loss: 0.610, 16032/60000 datapoints
2025-03-06 19:03:33,225 - INFO - training batch 551, loss: 0.422, 17632/60000 datapoints
2025-03-06 19:03:33,423 - INFO - training batch 601, loss: 0.538, 19232/60000 datapoints
2025-03-06 19:03:33,636 - INFO - training batch 651, loss: 0.479, 20832/60000 datapoints
2025-03-06 19:03:33,841 - INFO - training batch 701, loss: 0.367, 22432/60000 datapoints
2025-03-06 19:03:34,109 - INFO - training batch 751, loss: 0.370, 24032/60000 datapoints
2025-03-06 19:03:34,354 - INFO - training batch 801, loss: 0.366, 25632/60000 datapoints
2025-03-06 19:03:34,557 - INFO - training batch 851, loss: 0.283, 27232/60000 datapoints
2025-03-06 19:03:34,773 - INFO - training batch 901, loss: 0.430, 28832/60000 datapoints
2025-03-06 19:03:34,973 - INFO - training batch 951, loss: 0.414, 30432/60000 datapoints
2025-03-06 19:03:35,171 - INFO - training batch 1001, loss: 0.299, 32032/60000 datapoints
2025-03-06 19:03:35,366 - INFO - training batch 1051, loss: 0.271, 33632/60000 datapoints
2025-03-06 19:03:35,563 - INFO - training batch 1101, loss: 0.170, 35232/60000 datapoints
2025-03-06 19:03:35,760 - INFO - training batch 1151, loss: 0.246, 36832/60000 datapoints
2025-03-06 19:03:35,960 - INFO - training batch 1201, loss: 0.436, 38432/60000 datapoints
2025-03-06 19:03:36,158 - INFO - training batch 1251, loss: 0.369, 40032/60000 datapoints
2025-03-06 19:03:36,356 - INFO - training batch 1301, loss: 0.405, 41632/60000 datapoints
2025-03-06 19:03:36,553 - INFO - training batch 1351, loss: 0.662, 43232/60000 datapoints
2025-03-06 19:03:36,754 - INFO - training batch 1401, loss: 0.399, 44832/60000 datapoints
2025-03-06 19:03:36,950 - INFO - training batch 1451, loss: 0.412, 46432/60000 datapoints
2025-03-06 19:03:37,144 - INFO - training batch 1501, loss: 0.536, 48032/60000 datapoints
2025-03-06 19:03:37,338 - INFO - training batch 1551, loss: 0.397, 49632/60000 datapoints
2025-03-06 19:03:37,532 - INFO - training batch 1601, loss: 0.478, 51232/60000 datapoints
2025-03-06 19:03:37,751 - INFO - training batch 1651, loss: 0.445, 52832/60000 datapoints
2025-03-06 19:03:37,946 - INFO - training batch 1701, loss: 0.454, 54432/60000 datapoints
2025-03-06 19:03:38,139 - INFO - training batch 1751, loss: 0.465, 56032/60000 datapoints
2025-03-06 19:03:38,338 - INFO - training batch 1801, loss: 0.357, 57632/60000 datapoints
2025-03-06 19:03:38,544 - INFO - training batch 1851, loss: 0.459, 59232/60000 datapoints
2025-03-06 19:03:38,647 - INFO - validation batch 1, loss: 0.428, 32/10016 datapoints
2025-03-06 19:03:38,802 - INFO - validation batch 51, loss: 0.258, 1632/10016 datapoints
2025-03-06 19:03:38,960 - INFO - validation batch 101, loss: 0.529, 3232/10016 datapoints
2025-03-06 19:03:39,119 - INFO - validation batch 151, loss: 0.259, 4832/10016 datapoints
2025-03-06 19:03:39,271 - INFO - validation batch 201, loss: 0.593, 6432/10016 datapoints
2025-03-06 19:03:39,424 - INFO - validation batch 251, loss: 0.664, 8032/10016 datapoints
2025-03-06 19:03:39,579 - INFO - validation batch 301, loss: 0.389, 9632/10016 datapoints
2025-03-06 19:03:39,622 - INFO - Epoch 181/800 done.
2025-03-06 19:03:39,634 - INFO - Final validation performance:
Loss: 0.446, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 19:03:39,635 - INFO - Beginning epoch 182/800
2025-03-06 19:03:39,646 - INFO - training batch 1, loss: 0.299, 32/60000 datapoints
2025-03-06 19:03:39,855 - INFO - training batch 51, loss: 0.294, 1632/60000 datapoints
2025-03-06 19:03:40,048 - INFO - training batch 101, loss: 0.201, 3232/60000 datapoints
2025-03-06 19:03:40,256 - INFO - training batch 151, loss: 0.532, 4832/60000 datapoints
2025-03-06 19:03:40,478 - INFO - training batch 201, loss: 0.337, 6432/60000 datapoints
2025-03-06 19:03:40,686 - INFO - training batch 251, loss: 0.353, 8032/60000 datapoints
2025-03-06 19:03:40,883 - INFO - training batch 301, loss: 0.431, 9632/60000 datapoints
2025-03-06 19:03:41,078 - INFO - training batch 351, loss: 0.131, 11232/60000 datapoints
2025-03-06 19:03:41,277 - INFO - training batch 401, loss: 0.318, 12832/60000 datapoints
2025-03-06 19:03:41,470 - INFO - training batch 451, loss: 0.581, 14432/60000 datapoints
2025-03-06 19:03:41,666 - INFO - training batch 501, loss: 0.183, 16032/60000 datapoints
2025-03-06 19:03:41,862 - INFO - training batch 551, loss: 0.456, 17632/60000 datapoints
2025-03-06 19:03:42,054 - INFO - training batch 601, loss: 0.399, 19232/60000 datapoints
2025-03-06 19:03:42,249 - INFO - training batch 651, loss: 0.539, 20832/60000 datapoints
2025-03-06 19:03:42,449 - INFO - training batch 701, loss: 0.327, 22432/60000 datapoints
2025-03-06 19:03:42,648 - INFO - training batch 751, loss: 0.679, 24032/60000 datapoints
2025-03-06 19:03:42,842 - INFO - training batch 801, loss: 0.835, 25632/60000 datapoints
2025-03-06 19:03:43,038 - INFO - training batch 851, loss: 0.527, 27232/60000 datapoints
2025-03-06 19:03:43,232 - INFO - training batch 901, loss: 0.328, 28832/60000 datapoints
2025-03-06 19:03:43,427 - INFO - training batch 951, loss: 0.285, 30432/60000 datapoints
2025-03-06 19:03:43,647 - INFO - training batch 1001, loss: 0.374, 32032/60000 datapoints
2025-03-06 19:03:43,841 - INFO - training batch 1051, loss: 0.480, 33632/60000 datapoints
2025-03-06 19:03:44,038 - INFO - training batch 1101, loss: 0.293, 35232/60000 datapoints
2025-03-06 19:03:44,235 - INFO - training batch 1151, loss: 0.269, 36832/60000 datapoints
2025-03-06 19:03:44,431 - INFO - training batch 1201, loss: 0.341, 38432/60000 datapoints
2025-03-06 19:03:44,630 - INFO - training batch 1251, loss: 0.307, 40032/60000 datapoints
2025-03-06 19:03:44,834 - INFO - training batch 1301, loss: 0.184, 41632/60000 datapoints
2025-03-06 19:03:45,045 - INFO - training batch 1351, loss: 0.430, 43232/60000 datapoints
2025-03-06 19:03:45,242 - INFO - training batch 1401, loss: 0.511, 44832/60000 datapoints
2025-03-06 19:03:45,437 - INFO - training batch 1451, loss: 0.436, 46432/60000 datapoints
2025-03-06 19:03:45,631 - INFO - training batch 1501, loss: 0.600, 48032/60000 datapoints
2025-03-06 19:03:45,829 - INFO - training batch 1551, loss: 0.657, 49632/60000 datapoints
2025-03-06 19:03:46,025 - INFO - training batch 1601, loss: 0.246, 51232/60000 datapoints
2025-03-06 19:03:46,225 - INFO - training batch 1651, loss: 0.357, 52832/60000 datapoints
2025-03-06 19:03:46,422 - INFO - training batch 1701, loss: 0.275, 54432/60000 datapoints
2025-03-06 19:03:46,623 - INFO - training batch 1751, loss: 0.234, 56032/60000 datapoints
2025-03-06 19:03:46,819 - INFO - training batch 1801, loss: 0.515, 57632/60000 datapoints
2025-03-06 19:03:47,015 - INFO - training batch 1851, loss: 0.386, 59232/60000 datapoints
2025-03-06 19:03:47,116 - INFO - validation batch 1, loss: 0.567, 32/10016 datapoints
2025-03-06 19:03:47,269 - INFO - validation batch 51, loss: 0.362, 1632/10016 datapoints
2025-03-06 19:03:47,424 - INFO - validation batch 101, loss: 0.287, 3232/10016 datapoints
2025-03-06 19:03:47,579 - INFO - validation batch 151, loss: 0.206, 4832/10016 datapoints
2025-03-06 19:03:47,734 - INFO - validation batch 201, loss: 0.248, 6432/10016 datapoints
2025-03-06 19:03:47,887 - INFO - validation batch 251, loss: 0.283, 8032/10016 datapoints
2025-03-06 19:03:48,038 - INFO - validation batch 301, loss: 0.287, 9632/10016 datapoints
2025-03-06 19:03:48,075 - INFO - Epoch 182/800 done.
2025-03-06 19:03:48,075 - INFO - Final validation performance:
Loss: 0.320, top-1 acc: 0.898top-5 acc: 0.898
2025-03-06 19:03:48,075 - INFO - Beginning epoch 183/800
2025-03-06 19:03:48,081 - INFO - training batch 1, loss: 0.234, 32/60000 datapoints
2025-03-06 19:03:48,281 - INFO - training batch 51, loss: 0.399, 1632/60000 datapoints
2025-03-06 19:03:48,478 - INFO - training batch 101, loss: 0.397, 3232/60000 datapoints
2025-03-06 19:03:48,696 - INFO - training batch 151, loss: 0.165, 4832/60000 datapoints
2025-03-06 19:03:48,889 - INFO - training batch 201, loss: 0.476, 6432/60000 datapoints
2025-03-06 19:03:49,082 - INFO - training batch 251, loss: 0.575, 8032/60000 datapoints
2025-03-06 19:03:49,285 - INFO - training batch 301, loss: 0.276, 9632/60000 datapoints
2025-03-06 19:03:49,480 - INFO - training batch 351, loss: 0.815, 11232/60000 datapoints
2025-03-06 19:03:49,681 - INFO - training batch 401, loss: 0.281, 12832/60000 datapoints
2025-03-06 19:03:49,878 - INFO - training batch 451, loss: 0.163, 14432/60000 datapoints
2025-03-06 19:03:50,069 - INFO - training batch 501, loss: 0.246, 16032/60000 datapoints
2025-03-06 19:03:50,263 - INFO - training batch 551, loss: 0.439, 17632/60000 datapoints
2025-03-06 19:03:50,475 - INFO - training batch 601, loss: 0.231, 19232/60000 datapoints
2025-03-06 19:03:50,677 - INFO - training batch 651, loss: 0.323, 20832/60000 datapoints
2025-03-06 19:03:50,875 - INFO - training batch 701, loss: 0.647, 22432/60000 datapoints
2025-03-06 19:03:51,070 - INFO - training batch 751, loss: 0.288, 24032/60000 datapoints
2025-03-06 19:03:51,266 - INFO - training batch 801, loss: 0.281, 25632/60000 datapoints
2025-03-06 19:03:51,463 - INFO - training batch 851, loss: 0.376, 27232/60000 datapoints
2025-03-06 19:03:51,659 - INFO - training batch 901, loss: 0.326, 28832/60000 datapoints
2025-03-06 19:03:51,907 - INFO - training batch 951, loss: 0.251, 30432/60000 datapoints
2025-03-06 19:03:52,117 - INFO - training batch 1001, loss: 0.379, 32032/60000 datapoints
2025-03-06 19:03:52,335 - INFO - training batch 1051, loss: 0.552, 33632/60000 datapoints
2025-03-06 19:03:52,534 - INFO - training batch 1101, loss: 0.577, 35232/60000 datapoints
2025-03-06 19:03:52,733 - INFO - training batch 1151, loss: 0.452, 36832/60000 datapoints
2025-03-06 19:03:52,932 - INFO - training batch 1201, loss: 0.363, 38432/60000 datapoints
2025-03-06 19:03:53,129 - INFO - training batch 1251, loss: 0.328, 40032/60000 datapoints
2025-03-06 19:03:53,324 - INFO - training batch 1301, loss: 0.463, 41632/60000 datapoints
2025-03-06 19:03:53,521 - INFO - training batch 1351, loss: 0.379, 43232/60000 datapoints
2025-03-06 19:03:53,739 - INFO - training batch 1401, loss: 0.467, 44832/60000 datapoints
2025-03-06 19:03:53,934 - INFO - training batch 1451, loss: 0.209, 46432/60000 datapoints
2025-03-06 19:03:54,134 - INFO - training batch 1501, loss: 0.350, 48032/60000 datapoints
2025-03-06 19:03:54,333 - INFO - training batch 1551, loss: 0.325, 49632/60000 datapoints
2025-03-06 19:03:54,530 - INFO - training batch 1601, loss: 0.345, 51232/60000 datapoints
2025-03-06 19:03:54,727 - INFO - training batch 1651, loss: 0.241, 52832/60000 datapoints
2025-03-06 19:03:54,936 - INFO - training batch 1701, loss: 0.361, 54432/60000 datapoints
2025-03-06 19:03:55,133 - INFO - training batch 1751, loss: 0.832, 56032/60000 datapoints
2025-03-06 19:03:55,330 - INFO - training batch 1801, loss: 0.443, 57632/60000 datapoints
2025-03-06 19:03:55,525 - INFO - training batch 1851, loss: 0.288, 59232/60000 datapoints
2025-03-06 19:03:55,640 - INFO - validation batch 1, loss: 0.734, 32/10016 datapoints
2025-03-06 19:03:55,832 - INFO - validation batch 51, loss: 0.432, 1632/10016 datapoints
2025-03-06 19:03:55,997 - INFO - validation batch 101, loss: 0.228, 3232/10016 datapoints
2025-03-06 19:03:56,171 - INFO - validation batch 151, loss: 0.549, 4832/10016 datapoints
2025-03-06 19:03:56,347 - INFO - validation batch 201, loss: 0.536, 6432/10016 datapoints
2025-03-06 19:03:56,524 - INFO - validation batch 251, loss: 0.571, 8032/10016 datapoints
2025-03-06 19:03:56,695 - INFO - validation batch 301, loss: 0.216, 9632/10016 datapoints
2025-03-06 19:03:56,744 - INFO - Epoch 183/800 done.
2025-03-06 19:03:56,744 - INFO - Final validation performance:
Loss: 0.467, top-1 acc: 0.899top-5 acc: 0.899
2025-03-06 19:03:56,745 - INFO - Beginning epoch 184/800
2025-03-06 19:03:56,756 - INFO - training batch 1, loss: 0.307, 32/60000 datapoints
2025-03-06 19:03:57,033 - INFO - training batch 51, loss: 0.481, 1632/60000 datapoints
2025-03-06 19:03:57,301 - INFO - training batch 101, loss: 0.353, 3232/60000 datapoints
2025-03-06 19:03:57,553 - INFO - training batch 151, loss: 0.772, 4832/60000 datapoints
2025-03-06 19:03:57,807 - INFO - training batch 201, loss: 0.453, 6432/60000 datapoints
2025-03-06 19:03:58,086 - INFO - training batch 251, loss: 0.275, 8032/60000 datapoints
2025-03-06 19:03:58,339 - INFO - training batch 301, loss: 0.322, 9632/60000 datapoints
2025-03-06 19:03:58,628 - INFO - training batch 351, loss: 0.302, 11232/60000 datapoints
2025-03-06 19:03:58,902 - INFO - training batch 401, loss: 0.239, 12832/60000 datapoints
2025-03-06 19:03:59,259 - INFO - training batch 451, loss: 0.438, 14432/60000 datapoints
2025-03-06 19:03:59,552 - INFO - training batch 501, loss: 0.323, 16032/60000 datapoints
2025-03-06 19:03:59,880 - INFO - training batch 551, loss: 0.276, 17632/60000 datapoints
2025-03-06 19:04:00,232 - INFO - training batch 601, loss: 0.379, 19232/60000 datapoints
2025-03-06 19:04:00,737 - INFO - training batch 651, loss: 0.343, 20832/60000 datapoints
2025-03-06 19:04:01,004 - INFO - training batch 701, loss: 0.281, 22432/60000 datapoints
2025-03-06 19:04:01,251 - INFO - training batch 751, loss: 0.238, 24032/60000 datapoints
2025-03-06 19:04:01,530 - INFO - training batch 801, loss: 0.701, 25632/60000 datapoints
2025-03-06 19:04:01,777 - INFO - training batch 851, loss: 0.608, 27232/60000 datapoints
2025-03-06 19:04:01,995 - INFO - training batch 901, loss: 0.223, 28832/60000 datapoints
2025-03-06 19:04:02,224 - INFO - training batch 951, loss: 0.410, 30432/60000 datapoints
2025-03-06 19:04:02,442 - INFO - training batch 1001, loss: 0.364, 32032/60000 datapoints
2025-03-06 19:04:02,661 - INFO - training batch 1051, loss: 0.178, 33632/60000 datapoints
2025-03-06 19:04:02,865 - INFO - training batch 1101, loss: 0.280, 35232/60000 datapoints
2025-03-06 19:04:03,081 - INFO - training batch 1151, loss: 0.236, 36832/60000 datapoints
2025-03-06 19:04:03,301 - INFO - training batch 1201, loss: 0.270, 38432/60000 datapoints
2025-03-06 19:04:03,541 - INFO - training batch 1251, loss: 0.452, 40032/60000 datapoints
2025-03-06 19:04:03,745 - INFO - training batch 1301, loss: 0.246, 41632/60000 datapoints
2025-03-06 19:04:03,954 - INFO - training batch 1351, loss: 0.362, 43232/60000 datapoints
2025-03-06 19:04:04,167 - INFO - training batch 1401, loss: 0.240, 44832/60000 datapoints
2025-03-06 19:04:04,363 - INFO - training batch 1451, loss: 0.639, 46432/60000 datapoints
2025-03-06 19:04:04,566 - INFO - training batch 1501, loss: 0.333, 48032/60000 datapoints
2025-03-06 19:04:04,771 - INFO - training batch 1551, loss: 0.260, 49632/60000 datapoints
2025-03-06 19:04:04,967 - INFO - training batch 1601, loss: 0.474, 51232/60000 datapoints
2025-03-06 19:04:05,165 - INFO - training batch 1651, loss: 0.244, 52832/60000 datapoints
2025-03-06 19:04:05,367 - INFO - training batch 1701, loss: 0.562, 54432/60000 datapoints
2025-03-06 19:04:05,571 - INFO - training batch 1751, loss: 0.383, 56032/60000 datapoints
2025-03-06 19:04:05,765 - INFO - training batch 1801, loss: 0.182, 57632/60000 datapoints
2025-03-06 19:04:05,959 - INFO - training batch 1851, loss: 0.396, 59232/60000 datapoints
2025-03-06 19:04:06,059 - INFO - validation batch 1, loss: 0.405, 32/10016 datapoints
2025-03-06 19:04:06,214 - INFO - validation batch 51, loss: 0.427, 1632/10016 datapoints
2025-03-06 19:04:06,367 - INFO - validation batch 101, loss: 0.328, 3232/10016 datapoints
2025-03-06 19:04:06,521 - INFO - validation batch 151, loss: 0.271, 4832/10016 datapoints
2025-03-06 19:04:06,679 - INFO - validation batch 201, loss: 0.350, 6432/10016 datapoints
2025-03-06 19:04:06,837 - INFO - validation batch 251, loss: 0.290, 8032/10016 datapoints
2025-03-06 19:04:06,988 - INFO - validation batch 301, loss: 0.259, 9632/10016 datapoints
2025-03-06 19:04:07,026 - INFO - Epoch 184/800 done.
2025-03-06 19:04:07,026 - INFO - Final validation performance:
Loss: 0.333, top-1 acc: 0.899top-5 acc: 0.899
2025-03-06 19:04:07,027 - INFO - Beginning epoch 185/800
2025-03-06 19:04:07,033 - INFO - training batch 1, loss: 0.310, 32/60000 datapoints
2025-03-06 19:04:07,235 - INFO - training batch 51, loss: 0.198, 1632/60000 datapoints
2025-03-06 19:04:07,449 - INFO - training batch 101, loss: 0.320, 3232/60000 datapoints
2025-03-06 19:04:07,653 - INFO - training batch 151, loss: 0.451, 4832/60000 datapoints
2025-03-06 19:04:07,875 - INFO - training batch 201, loss: 0.394, 6432/60000 datapoints
2025-03-06 19:04:08,084 - INFO - training batch 251, loss: 0.247, 8032/60000 datapoints
2025-03-06 19:04:08,296 - INFO - training batch 301, loss: 0.312, 9632/60000 datapoints
2025-03-06 19:04:08,505 - INFO - training batch 351, loss: 0.292, 11232/60000 datapoints
2025-03-06 19:04:08,724 - INFO - training batch 401, loss: 0.307, 12832/60000 datapoints
2025-03-06 19:04:08,942 - INFO - training batch 451, loss: 0.190, 14432/60000 datapoints
2025-03-06 19:04:09,161 - INFO - training batch 501, loss: 0.429, 16032/60000 datapoints
2025-03-06 19:04:09,387 - INFO - training batch 551, loss: 0.346, 17632/60000 datapoints
2025-03-06 19:04:09,598 - INFO - training batch 601, loss: 0.296, 19232/60000 datapoints
2025-03-06 19:04:09,810 - INFO - training batch 651, loss: 0.281, 20832/60000 datapoints
2025-03-06 19:04:10,036 - INFO - training batch 701, loss: 0.283, 22432/60000 datapoints
2025-03-06 19:04:10,246 - INFO - training batch 751, loss: 0.360, 24032/60000 datapoints
2025-03-06 19:04:10,447 - INFO - training batch 801, loss: 0.219, 25632/60000 datapoints
2025-03-06 19:04:10,687 - INFO - training batch 851, loss: 0.275, 27232/60000 datapoints
2025-03-06 19:04:10,935 - INFO - training batch 901, loss: 0.360, 28832/60000 datapoints
2025-03-06 19:04:11,161 - INFO - training batch 951, loss: 0.319, 30432/60000 datapoints
2025-03-06 19:04:11,392 - INFO - training batch 1001, loss: 0.260, 32032/60000 datapoints
2025-03-06 19:04:11,611 - INFO - training batch 1051, loss: 0.505, 33632/60000 datapoints
2025-03-06 19:04:11,819 - INFO - training batch 1101, loss: 0.185, 35232/60000 datapoints
2025-03-06 19:04:12,029 - INFO - training batch 1151, loss: 0.682, 36832/60000 datapoints
2025-03-06 19:04:12,263 - INFO - training batch 1201, loss: 0.231, 38432/60000 datapoints
2025-03-06 19:04:12,527 - INFO - training batch 1251, loss: 0.220, 40032/60000 datapoints
2025-03-06 19:04:12,755 - INFO - training batch 1301, loss: 0.741, 41632/60000 datapoints
2025-03-06 19:04:12,970 - INFO - training batch 1351, loss: 0.287, 43232/60000 datapoints
2025-03-06 19:04:13,195 - INFO - training batch 1401, loss: 0.181, 44832/60000 datapoints
2025-03-06 19:04:13,416 - INFO - training batch 1451, loss: 0.411, 46432/60000 datapoints
2025-03-06 19:04:13,664 - INFO - training batch 1501, loss: 0.464, 48032/60000 datapoints
2025-03-06 19:04:13,902 - INFO - training batch 1551, loss: 0.226, 49632/60000 datapoints
2025-03-06 19:04:14,125 - INFO - training batch 1601, loss: 0.208, 51232/60000 datapoints
2025-03-06 19:04:14,343 - INFO - training batch 1651, loss: 0.308, 52832/60000 datapoints
2025-03-06 19:04:14,584 - INFO - training batch 1701, loss: 0.313, 54432/60000 datapoints
2025-03-06 19:04:14,795 - INFO - training batch 1751, loss: 0.207, 56032/60000 datapoints
2025-03-06 19:04:15,002 - INFO - training batch 1801, loss: 0.410, 57632/60000 datapoints
2025-03-06 19:04:15,219 - INFO - training batch 1851, loss: 0.351, 59232/60000 datapoints
2025-03-06 19:04:15,335 - INFO - validation batch 1, loss: 0.251, 32/10016 datapoints
2025-03-06 19:04:15,502 - INFO - validation batch 51, loss: 0.213, 1632/10016 datapoints
2025-03-06 19:04:15,680 - INFO - validation batch 101, loss: 0.269, 3232/10016 datapoints
2025-03-06 19:04:15,843 - INFO - validation batch 151, loss: 0.414, 4832/10016 datapoints
2025-03-06 19:04:16,021 - INFO - validation batch 201, loss: 0.433, 6432/10016 datapoints
2025-03-06 19:04:16,189 - INFO - validation batch 251, loss: 0.386, 8032/10016 datapoints
2025-03-06 19:04:16,359 - INFO - validation batch 301, loss: 0.216, 9632/10016 datapoints
2025-03-06 19:04:16,400 - INFO - Epoch 185/800 done.
2025-03-06 19:04:16,400 - INFO - Final validation performance:
Loss: 0.312, top-1 acc: 0.899top-5 acc: 0.899
2025-03-06 19:04:16,401 - INFO - Beginning epoch 186/800
2025-03-06 19:04:16,407 - INFO - training batch 1, loss: 0.198, 32/60000 datapoints
2025-03-06 19:04:16,618 - INFO - training batch 51, loss: 0.211, 1632/60000 datapoints
2025-03-06 19:04:16,834 - INFO - training batch 101, loss: 0.618, 3232/60000 datapoints
2025-03-06 19:04:17,034 - INFO - training batch 151, loss: 0.276, 4832/60000 datapoints
2025-03-06 19:04:17,245 - INFO - training batch 201, loss: 0.373, 6432/60000 datapoints
2025-03-06 19:04:17,476 - INFO - training batch 251, loss: 0.364, 8032/60000 datapoints
2025-03-06 19:04:17,687 - INFO - training batch 301, loss: 0.150, 9632/60000 datapoints
2025-03-06 19:04:17,892 - INFO - training batch 351, loss: 0.342, 11232/60000 datapoints
2025-03-06 19:04:18,087 - INFO - training batch 401, loss: 0.429, 12832/60000 datapoints
2025-03-06 19:04:18,282 - INFO - training batch 451, loss: 0.248, 14432/60000 datapoints
2025-03-06 19:04:18,479 - INFO - training batch 501, loss: 0.291, 16032/60000 datapoints
2025-03-06 19:04:18,681 - INFO - training batch 551, loss: 0.264, 17632/60000 datapoints
2025-03-06 19:04:18,876 - INFO - training batch 601, loss: 0.580, 19232/60000 datapoints
2025-03-06 19:04:19,082 - INFO - training batch 651, loss: 0.364, 20832/60000 datapoints
2025-03-06 19:04:19,287 - INFO - training batch 701, loss: 0.181, 22432/60000 datapoints
2025-03-06 19:04:19,496 - INFO - training batch 751, loss: 0.359, 24032/60000 datapoints
2025-03-06 19:04:19,697 - INFO - training batch 801, loss: 0.321, 25632/60000 datapoints
2025-03-06 19:04:19,899 - INFO - training batch 851, loss: 0.418, 27232/60000 datapoints
2025-03-06 19:04:20,094 - INFO - training batch 901, loss: 0.441, 28832/60000 datapoints
2025-03-06 19:04:20,294 - INFO - training batch 951, loss: 0.340, 30432/60000 datapoints
2025-03-06 19:04:20,493 - INFO - training batch 1001, loss: 0.304, 32032/60000 datapoints
2025-03-06 19:04:20,694 - INFO - training batch 1051, loss: 0.381, 33632/60000 datapoints
2025-03-06 19:04:20,925 - INFO - training batch 1101, loss: 0.613, 35232/60000 datapoints
2025-03-06 19:04:21,130 - INFO - training batch 1151, loss: 0.327, 36832/60000 datapoints
2025-03-06 19:04:21,355 - INFO - training batch 1201, loss: 0.230, 38432/60000 datapoints
2025-03-06 19:04:21,566 - INFO - training batch 1251, loss: 0.284, 40032/60000 datapoints
2025-03-06 19:04:21,785 - INFO - training batch 1301, loss: 0.423, 41632/60000 datapoints
2025-03-06 19:04:22,011 - INFO - training batch 1351, loss: 0.219, 43232/60000 datapoints
2025-03-06 19:04:22,328 - INFO - training batch 1401, loss: 0.436, 44832/60000 datapoints
2025-03-06 19:04:22,621 - INFO - training batch 1451, loss: 0.318, 46432/60000 datapoints
2025-03-06 19:04:22,849 - INFO - training batch 1501, loss: 0.324, 48032/60000 datapoints
2025-03-06 19:04:23,067 - INFO - training batch 1551, loss: 0.161, 49632/60000 datapoints
2025-03-06 19:04:23,326 - INFO - training batch 1601, loss: 0.218, 51232/60000 datapoints
2025-03-06 19:04:23,532 - INFO - training batch 1651, loss: 0.457, 52832/60000 datapoints
2025-03-06 19:04:23,740 - INFO - training batch 1701, loss: 0.380, 54432/60000 datapoints
2025-03-06 19:04:23,953 - INFO - training batch 1751, loss: 0.443, 56032/60000 datapoints
2025-03-06 19:04:24,178 - INFO - training batch 1801, loss: 0.209, 57632/60000 datapoints
2025-03-06 19:04:24,391 - INFO - training batch 1851, loss: 0.448, 59232/60000 datapoints
2025-03-06 19:04:24,499 - INFO - validation batch 1, loss: 0.115, 32/10016 datapoints
2025-03-06 19:04:24,688 - INFO - validation batch 51, loss: 0.271, 1632/10016 datapoints
2025-03-06 19:04:24,866 - INFO - validation batch 101, loss: 0.784, 3232/10016 datapoints
2025-03-06 19:04:25,032 - INFO - validation batch 151, loss: 0.437, 4832/10016 datapoints
2025-03-06 19:04:25,201 - INFO - validation batch 201, loss: 0.142, 6432/10016 datapoints
2025-03-06 19:04:25,359 - INFO - validation batch 251, loss: 0.282, 8032/10016 datapoints
2025-03-06 19:04:25,528 - INFO - validation batch 301, loss: 0.330, 9632/10016 datapoints
2025-03-06 19:04:25,569 - INFO - Epoch 186/800 done.
2025-03-06 19:04:25,569 - INFO - Final validation performance:
Loss: 0.337, top-1 acc: 0.899top-5 acc: 0.899
2025-03-06 19:04:25,570 - INFO - Beginning epoch 187/800
2025-03-06 19:04:25,576 - INFO - training batch 1, loss: 0.532, 32/60000 datapoints
2025-03-06 19:04:25,833 - INFO - training batch 51, loss: 0.677, 1632/60000 datapoints
2025-03-06 19:04:26,058 - INFO - training batch 101, loss: 0.215, 3232/60000 datapoints
2025-03-06 19:04:26,294 - INFO - training batch 151, loss: 0.373, 4832/60000 datapoints
2025-03-06 19:04:26,523 - INFO - training batch 201, loss: 0.435, 6432/60000 datapoints
2025-03-06 19:04:26,749 - INFO - training batch 251, loss: 0.329, 8032/60000 datapoints
2025-03-06 19:04:26,956 - INFO - training batch 301, loss: 0.653, 9632/60000 datapoints
2025-03-06 19:04:27,181 - INFO - training batch 351, loss: 0.395, 11232/60000 datapoints
2025-03-06 19:04:27,394 - INFO - training batch 401, loss: 0.797, 12832/60000 datapoints
2025-03-06 19:04:27,600 - INFO - training batch 451, loss: 0.172, 14432/60000 datapoints
2025-03-06 19:04:27,805 - INFO - training batch 501, loss: 0.656, 16032/60000 datapoints
2025-03-06 19:04:28,007 - INFO - training batch 551, loss: 0.526, 17632/60000 datapoints
2025-03-06 19:04:28,229 - INFO - training batch 601, loss: 0.209, 19232/60000 datapoints
2025-03-06 19:04:28,432 - INFO - training batch 651, loss: 0.233, 20832/60000 datapoints
2025-03-06 19:04:28,649 - INFO - training batch 701, loss: 0.393, 22432/60000 datapoints
2025-03-06 19:04:28,861 - INFO - training batch 751, loss: 0.326, 24032/60000 datapoints
2025-03-06 19:04:29,064 - INFO - training batch 801, loss: 0.555, 25632/60000 datapoints
2025-03-06 19:04:29,279 - INFO - training batch 851, loss: 0.212, 27232/60000 datapoints
2025-03-06 19:04:29,486 - INFO - training batch 901, loss: 0.497, 28832/60000 datapoints
2025-03-06 19:04:29,682 - INFO - training batch 951, loss: 0.259, 30432/60000 datapoints
2025-03-06 19:04:29,874 - INFO - training batch 1001, loss: 0.438, 32032/60000 datapoints
2025-03-06 19:04:30,067 - INFO - training batch 1051, loss: 0.316, 33632/60000 datapoints
2025-03-06 19:04:30,259 - INFO - training batch 1101, loss: 0.300, 35232/60000 datapoints
2025-03-06 19:04:30,455 - INFO - training batch 1151, loss: 0.215, 36832/60000 datapoints
2025-03-06 19:04:30,654 - INFO - training batch 1201, loss: 0.508, 38432/60000 datapoints
2025-03-06 19:04:30,853 - INFO - training batch 1251, loss: 0.559, 40032/60000 datapoints
2025-03-06 19:04:31,061 - INFO - training batch 1301, loss: 0.353, 41632/60000 datapoints
2025-03-06 19:04:31,254 - INFO - training batch 1351, loss: 0.356, 43232/60000 datapoints
2025-03-06 19:04:31,448 - INFO - training batch 1401, loss: 0.119, 44832/60000 datapoints
2025-03-06 19:04:31,646 - INFO - training batch 1451, loss: 0.457, 46432/60000 datapoints
2025-03-06 19:04:31,838 - INFO - training batch 1501, loss: 0.455, 48032/60000 datapoints
2025-03-06 19:04:32,034 - INFO - training batch 1551, loss: 0.279, 49632/60000 datapoints
2025-03-06 19:04:32,231 - INFO - training batch 1601, loss: 0.413, 51232/60000 datapoints
2025-03-06 19:04:32,424 - INFO - training batch 1651, loss: 0.587, 52832/60000 datapoints
2025-03-06 19:04:32,622 - INFO - training batch 1701, loss: 0.470, 54432/60000 datapoints
2025-03-06 19:04:32,817 - INFO - training batch 1751, loss: 0.342, 56032/60000 datapoints
2025-03-06 19:04:33,013 - INFO - training batch 1801, loss: 0.309, 57632/60000 datapoints
2025-03-06 19:04:33,214 - INFO - training batch 1851, loss: 0.257, 59232/60000 datapoints
2025-03-06 19:04:33,315 - INFO - validation batch 1, loss: 0.352, 32/10016 datapoints
2025-03-06 19:04:33,467 - INFO - validation batch 51, loss: 0.559, 1632/10016 datapoints
2025-03-06 19:04:33,625 - INFO - validation batch 101, loss: 0.183, 3232/10016 datapoints
2025-03-06 19:04:33,776 - INFO - validation batch 151, loss: 0.413, 4832/10016 datapoints
2025-03-06 19:04:33,930 - INFO - validation batch 201, loss: 0.295, 6432/10016 datapoints
2025-03-06 19:04:34,082 - INFO - validation batch 251, loss: 0.279, 8032/10016 datapoints
2025-03-06 19:04:34,235 - INFO - validation batch 301, loss: 0.436, 9632/10016 datapoints
2025-03-06 19:04:34,273 - INFO - Epoch 187/800 done.
2025-03-06 19:04:34,273 - INFO - Final validation performance:
Loss: 0.360, top-1 acc: 0.899top-5 acc: 0.899
2025-03-06 19:04:34,274 - INFO - Beginning epoch 188/800
2025-03-06 19:04:34,280 - INFO - training batch 1, loss: 0.465, 32/60000 datapoints
2025-03-06 19:04:34,478 - INFO - training batch 51, loss: 0.273, 1632/60000 datapoints
2025-03-06 19:04:34,690 - INFO - training batch 101, loss: 0.161, 3232/60000 datapoints
2025-03-06 19:04:34,890 - INFO - training batch 151, loss: 0.279, 4832/60000 datapoints
2025-03-06 19:04:35,094 - INFO - training batch 201, loss: 0.502, 6432/60000 datapoints
2025-03-06 19:04:35,291 - INFO - training batch 251, loss: 0.373, 8032/60000 datapoints
2025-03-06 19:04:35,490 - INFO - training batch 301, loss: 0.888, 9632/60000 datapoints
2025-03-06 19:04:35,689 - INFO - training batch 351, loss: 0.452, 11232/60000 datapoints
2025-03-06 19:04:35,881 - INFO - training batch 401, loss: 0.424, 12832/60000 datapoints
2025-03-06 19:04:36,077 - INFO - training batch 451, loss: 0.397, 14432/60000 datapoints
2025-03-06 19:04:36,274 - INFO - training batch 501, loss: 0.423, 16032/60000 datapoints
2025-03-06 19:04:36,467 - INFO - training batch 551, loss: 0.469, 17632/60000 datapoints
2025-03-06 19:04:36,667 - INFO - training batch 601, loss: 0.419, 19232/60000 datapoints
2025-03-06 19:04:36,860 - INFO - training batch 651, loss: 0.407, 20832/60000 datapoints
2025-03-06 19:04:37,055 - INFO - training batch 701, loss: 0.266, 22432/60000 datapoints
2025-03-06 19:04:37,248 - INFO - training batch 751, loss: 0.283, 24032/60000 datapoints
2025-03-06 19:04:37,442 - INFO - training batch 801, loss: 0.309, 25632/60000 datapoints
2025-03-06 19:04:37,653 - INFO - training batch 851, loss: 0.481, 27232/60000 datapoints
2025-03-06 19:04:37,845 - INFO - training batch 901, loss: 0.549, 28832/60000 datapoints
2025-03-06 19:04:38,063 - INFO - training batch 951, loss: 0.246, 30432/60000 datapoints
2025-03-06 19:04:38,293 - INFO - training batch 1001, loss: 0.332, 32032/60000 datapoints
2025-03-06 19:04:38,507 - INFO - training batch 1051, loss: 0.331, 33632/60000 datapoints
2025-03-06 19:04:38,731 - INFO - training batch 1101, loss: 0.342, 35232/60000 datapoints
2025-03-06 19:04:38,950 - INFO - training batch 1151, loss: 0.281, 36832/60000 datapoints
2025-03-06 19:04:39,149 - INFO - training batch 1201, loss: 0.372, 38432/60000 datapoints
2025-03-06 19:04:39,364 - INFO - training batch 1251, loss: 0.476, 40032/60000 datapoints
2025-03-06 19:04:39,572 - INFO - training batch 1301, loss: 0.458, 41632/60000 datapoints
2025-03-06 19:04:39,773 - INFO - training batch 1351, loss: 0.304, 43232/60000 datapoints
2025-03-06 19:04:39,972 - INFO - training batch 1401, loss: 0.595, 44832/60000 datapoints
2025-03-06 19:04:40,174 - INFO - training batch 1451, loss: 0.269, 46432/60000 datapoints
2025-03-06 19:04:40,372 - INFO - training batch 1501, loss: 0.218, 48032/60000 datapoints
2025-03-06 19:04:40,571 - INFO - training batch 1551, loss: 0.611, 49632/60000 datapoints
2025-03-06 19:04:40,769 - INFO - training batch 1601, loss: 0.367, 51232/60000 datapoints
2025-03-06 19:04:40,978 - INFO - training batch 1651, loss: 0.454, 52832/60000 datapoints
2025-03-06 19:04:41,176 - INFO - training batch 1701, loss: 0.198, 54432/60000 datapoints
2025-03-06 19:04:41,373 - INFO - training batch 1751, loss: 0.274, 56032/60000 datapoints
2025-03-06 19:04:41,567 - INFO - training batch 1801, loss: 0.292, 57632/60000 datapoints
2025-03-06 19:04:41,765 - INFO - training batch 1851, loss: 0.395, 59232/60000 datapoints
2025-03-06 19:04:41,866 - INFO - validation batch 1, loss: 0.268, 32/10016 datapoints
2025-03-06 19:04:42,019 - INFO - validation batch 51, loss: 0.486, 1632/10016 datapoints
2025-03-06 19:04:42,173 - INFO - validation batch 101, loss: 0.312, 3232/10016 datapoints
2025-03-06 19:04:42,324 - INFO - validation batch 151, loss: 0.444, 4832/10016 datapoints
2025-03-06 19:04:42,476 - INFO - validation batch 201, loss: 0.411, 6432/10016 datapoints
2025-03-06 19:04:42,635 - INFO - validation batch 251, loss: 0.258, 8032/10016 datapoints
2025-03-06 19:04:42,788 - INFO - validation batch 301, loss: 0.428, 9632/10016 datapoints
2025-03-06 19:04:42,826 - INFO - Epoch 188/800 done.
2025-03-06 19:04:42,826 - INFO - Final validation performance:
Loss: 0.373, top-1 acc: 0.900top-5 acc: 0.900
2025-03-06 19:04:42,827 - INFO - Beginning epoch 189/800
2025-03-06 19:04:42,833 - INFO - training batch 1, loss: 0.275, 32/60000 datapoints
2025-03-06 19:04:43,044 - INFO - training batch 51, loss: 0.363, 1632/60000 datapoints
2025-03-06 19:04:43,239 - INFO - training batch 101, loss: 0.519, 3232/60000 datapoints
2025-03-06 19:04:43,438 - INFO - training batch 151, loss: 0.546, 4832/60000 datapoints
2025-03-06 19:04:43,644 - INFO - training batch 201, loss: 0.241, 6432/60000 datapoints
2025-03-06 19:04:43,843 - INFO - training batch 251, loss: 0.225, 8032/60000 datapoints
2025-03-06 19:04:44,036 - INFO - training batch 301, loss: 0.310, 9632/60000 datapoints
2025-03-06 19:04:44,237 - INFO - training batch 351, loss: 0.394, 11232/60000 datapoints
2025-03-06 19:04:44,430 - INFO - training batch 401, loss: 0.327, 12832/60000 datapoints
2025-03-06 19:04:44,629 - INFO - training batch 451, loss: 0.324, 14432/60000 datapoints
2025-03-06 19:04:44,826 - INFO - training batch 501, loss: 0.324, 16032/60000 datapoints
2025-03-06 19:04:45,020 - INFO - training batch 551, loss: 0.520, 17632/60000 datapoints
2025-03-06 19:04:45,215 - INFO - training batch 601, loss: 0.283, 19232/60000 datapoints
2025-03-06 19:04:45,410 - INFO - training batch 651, loss: 0.336, 20832/60000 datapoints
2025-03-06 19:04:45,604 - INFO - training batch 701, loss: 0.253, 22432/60000 datapoints
2025-03-06 19:04:45,800 - INFO - training batch 751, loss: 0.187, 24032/60000 datapoints
2025-03-06 19:04:45,992 - INFO - training batch 801, loss: 0.316, 25632/60000 datapoints
2025-03-06 19:04:46,186 - INFO - training batch 851, loss: 0.414, 27232/60000 datapoints
2025-03-06 19:04:46,382 - INFO - training batch 901, loss: 0.271, 28832/60000 datapoints
2025-03-06 19:04:46,577 - INFO - training batch 951, loss: 0.464, 30432/60000 datapoints
2025-03-06 19:04:46,776 - INFO - training batch 1001, loss: 0.386, 32032/60000 datapoints
2025-03-06 19:04:46,971 - INFO - training batch 1051, loss: 0.340, 33632/60000 datapoints
2025-03-06 19:04:47,165 - INFO - training batch 1101, loss: 0.222, 35232/60000 datapoints
2025-03-06 19:04:47,357 - INFO - training batch 1151, loss: 0.606, 36832/60000 datapoints
2025-03-06 19:04:47,549 - INFO - training batch 1201, loss: 0.761, 38432/60000 datapoints
2025-03-06 19:04:47,747 - INFO - training batch 1251, loss: 0.268, 40032/60000 datapoints
2025-03-06 19:04:47,939 - INFO - training batch 1301, loss: 0.379, 41632/60000 datapoints
2025-03-06 19:04:48,132 - INFO - training batch 1351, loss: 0.206, 43232/60000 datapoints
2025-03-06 19:04:48,327 - INFO - training batch 1401, loss: 0.342, 44832/60000 datapoints
2025-03-06 19:04:48,520 - INFO - training batch 1451, loss: 0.547, 46432/60000 datapoints
2025-03-06 19:04:48,723 - INFO - training batch 1501, loss: 0.286, 48032/60000 datapoints
2025-03-06 19:04:48,917 - INFO - training batch 1551, loss: 0.140, 49632/60000 datapoints
2025-03-06 19:04:49,110 - INFO - training batch 1601, loss: 0.261, 51232/60000 datapoints
2025-03-06 19:04:49,302 - INFO - training batch 1651, loss: 0.282, 52832/60000 datapoints
2025-03-06 19:04:49,497 - INFO - training batch 1701, loss: 0.275, 54432/60000 datapoints
2025-03-06 19:04:49,695 - INFO - training batch 1751, loss: 0.288, 56032/60000 datapoints
2025-03-06 19:04:49,890 - INFO - training batch 1801, loss: 0.274, 57632/60000 datapoints
2025-03-06 19:04:50,085 - INFO - training batch 1851, loss: 0.455, 59232/60000 datapoints
2025-03-06 19:04:50,187 - INFO - validation batch 1, loss: 0.382, 32/10016 datapoints
2025-03-06 19:04:50,339 - INFO - validation batch 51, loss: 0.369, 1632/10016 datapoints
2025-03-06 19:04:50,490 - INFO - validation batch 101, loss: 0.484, 3232/10016 datapoints
2025-03-06 19:04:50,649 - INFO - validation batch 151, loss: 0.314, 4832/10016 datapoints
2025-03-06 19:04:50,801 - INFO - validation batch 201, loss: 0.220, 6432/10016 datapoints
2025-03-06 19:04:50,954 - INFO - validation batch 251, loss: 0.362, 8032/10016 datapoints
2025-03-06 19:04:51,130 - INFO - validation batch 301, loss: 0.255, 9632/10016 datapoints
2025-03-06 19:04:51,170 - INFO - Epoch 189/800 done.
2025-03-06 19:04:51,170 - INFO - Final validation performance:
Loss: 0.341, top-1 acc: 0.900top-5 acc: 0.900
2025-03-06 19:04:51,170 - INFO - Beginning epoch 190/800
2025-03-06 19:04:51,177 - INFO - training batch 1, loss: 0.355, 32/60000 datapoints
2025-03-06 19:04:51,386 - INFO - training batch 51, loss: 0.275, 1632/60000 datapoints
2025-03-06 19:04:51,581 - INFO - training batch 101, loss: 0.321, 3232/60000 datapoints
2025-03-06 19:04:51,786 - INFO - training batch 151, loss: 0.238, 4832/60000 datapoints
2025-03-06 19:04:51,989 - INFO - training batch 201, loss: 0.339, 6432/60000 datapoints
2025-03-06 19:04:52,197 - INFO - training batch 251, loss: 0.444, 8032/60000 datapoints
2025-03-06 19:04:52,391 - INFO - training batch 301, loss: 0.574, 9632/60000 datapoints
2025-03-06 19:04:52,586 - INFO - training batch 351, loss: 0.354, 11232/60000 datapoints
2025-03-06 19:04:52,786 - INFO - training batch 401, loss: 0.321, 12832/60000 datapoints
2025-03-06 19:04:52,978 - INFO - training batch 451, loss: 0.382, 14432/60000 datapoints
2025-03-06 19:04:53,181 - INFO - training batch 501, loss: 0.380, 16032/60000 datapoints
2025-03-06 19:04:53,381 - INFO - training batch 551, loss: 0.436, 17632/60000 datapoints
2025-03-06 19:04:53,575 - INFO - training batch 601, loss: 0.450, 19232/60000 datapoints
2025-03-06 19:04:53,772 - INFO - training batch 651, loss: 0.366, 20832/60000 datapoints
2025-03-06 19:04:53,964 - INFO - training batch 701, loss: 0.317, 22432/60000 datapoints
2025-03-06 19:04:54,159 - INFO - training batch 751, loss: 0.237, 24032/60000 datapoints
2025-03-06 19:04:54,351 - INFO - training batch 801, loss: 0.517, 25632/60000 datapoints
2025-03-06 19:04:54,543 - INFO - training batch 851, loss: 0.358, 27232/60000 datapoints
2025-03-06 19:04:54,743 - INFO - training batch 901, loss: 0.341, 28832/60000 datapoints
2025-03-06 19:04:54,939 - INFO - training batch 951, loss: 0.638, 30432/60000 datapoints
2025-03-06 19:04:55,132 - INFO - training batch 1001, loss: 0.252, 32032/60000 datapoints
2025-03-06 19:04:55,326 - INFO - training batch 1051, loss: 0.681, 33632/60000 datapoints
2025-03-06 19:04:55,522 - INFO - training batch 1101, loss: 0.298, 35232/60000 datapoints
2025-03-06 19:04:55,725 - INFO - training batch 1151, loss: 0.198, 36832/60000 datapoints
2025-03-06 19:04:55,922 - INFO - training batch 1201, loss: 0.362, 38432/60000 datapoints
2025-03-06 19:04:56,114 - INFO - training batch 1251, loss: 0.301, 40032/60000 datapoints
2025-03-06 19:04:56,309 - INFO - training batch 1301, loss: 0.351, 41632/60000 datapoints
2025-03-06 19:04:56,503 - INFO - training batch 1351, loss: 0.208, 43232/60000 datapoints
2025-03-06 19:04:56,733 - INFO - training batch 1401, loss: 0.391, 44832/60000 datapoints
2025-03-06 19:04:56,931 - INFO - training batch 1451, loss: 0.361, 46432/60000 datapoints
2025-03-06 19:04:57,124 - INFO - training batch 1501, loss: 0.389, 48032/60000 datapoints
2025-03-06 19:04:57,317 - INFO - training batch 1551, loss: 0.342, 49632/60000 datapoints
2025-03-06 19:04:57,512 - INFO - training batch 1601, loss: 0.283, 51232/60000 datapoints
2025-03-06 19:04:57,710 - INFO - training batch 1651, loss: 0.554, 52832/60000 datapoints
2025-03-06 19:04:57,903 - INFO - training batch 1701, loss: 0.705, 54432/60000 datapoints
2025-03-06 19:04:58,126 - INFO - training batch 1751, loss: 0.391, 56032/60000 datapoints
2025-03-06 19:04:58,336 - INFO - training batch 1801, loss: 0.354, 57632/60000 datapoints
2025-03-06 19:04:58,544 - INFO - training batch 1851, loss: 0.469, 59232/60000 datapoints
2025-03-06 19:04:58,654 - INFO - validation batch 1, loss: 0.262, 32/10016 datapoints
2025-03-06 19:04:58,829 - INFO - validation batch 51, loss: 0.295, 1632/10016 datapoints
2025-03-06 19:04:58,999 - INFO - validation batch 101, loss: 0.586, 3232/10016 datapoints
2025-03-06 19:04:59,157 - INFO - validation batch 151, loss: 0.440, 4832/10016 datapoints
2025-03-06 19:04:59,322 - INFO - validation batch 201, loss: 0.378, 6432/10016 datapoints
2025-03-06 19:04:59,501 - INFO - validation batch 251, loss: 0.328, 8032/10016 datapoints
2025-03-06 19:04:59,675 - INFO - validation batch 301, loss: 0.276, 9632/10016 datapoints
2025-03-06 19:04:59,715 - INFO - Epoch 190/800 done.
2025-03-06 19:04:59,716 - INFO - Final validation performance:
Loss: 0.366, top-1 acc: 0.900top-5 acc: 0.900
2025-03-06 19:04:59,716 - INFO - Beginning epoch 191/800
2025-03-06 19:04:59,723 - INFO - training batch 1, loss: 0.208, 32/60000 datapoints
2025-03-06 19:04:59,948 - INFO - training batch 51, loss: 0.439, 1632/60000 datapoints
2025-03-06 19:05:00,174 - INFO - training batch 101, loss: 0.456, 3232/60000 datapoints
2025-03-06 19:05:00,443 - INFO - training batch 151, loss: 0.581, 4832/60000 datapoints
2025-03-06 19:05:00,742 - INFO - training batch 201, loss: 0.373, 6432/60000 datapoints
2025-03-06 19:05:00,992 - INFO - training batch 251, loss: 0.211, 8032/60000 datapoints
2025-03-06 19:05:01,237 - INFO - training batch 301, loss: 0.396, 9632/60000 datapoints
2025-03-06 19:05:01,514 - INFO - training batch 351, loss: 0.613, 11232/60000 datapoints
2025-03-06 19:05:01,737 - INFO - training batch 401, loss: 0.390, 12832/60000 datapoints
2025-03-06 19:05:01,948 - INFO - training batch 451, loss: 0.329, 14432/60000 datapoints
2025-03-06 19:05:02,170 - INFO - training batch 501, loss: 0.328, 16032/60000 datapoints
2025-03-06 19:05:02,396 - INFO - training batch 551, loss: 0.300, 17632/60000 datapoints
2025-03-06 19:05:02,615 - INFO - training batch 601, loss: 0.355, 19232/60000 datapoints
2025-03-06 19:05:02,846 - INFO - training batch 651, loss: 0.243, 20832/60000 datapoints
2025-03-06 19:05:03,057 - INFO - training batch 701, loss: 0.458, 22432/60000 datapoints
2025-03-06 19:05:03,457 - INFO - training batch 751, loss: 0.517, 24032/60000 datapoints
2025-03-06 19:05:03,739 - INFO - training batch 801, loss: 0.597, 25632/60000 datapoints
2025-03-06 19:05:03,986 - INFO - training batch 851, loss: 0.150, 27232/60000 datapoints
2025-03-06 19:05:04,223 - INFO - training batch 901, loss: 0.311, 28832/60000 datapoints
2025-03-06 19:05:04,450 - INFO - training batch 951, loss: 0.204, 30432/60000 datapoints
2025-03-06 19:05:04,812 - INFO - training batch 1001, loss: 0.324, 32032/60000 datapoints
2025-03-06 19:05:05,043 - INFO - training batch 1051, loss: 0.221, 33632/60000 datapoints
2025-03-06 19:05:05,276 - INFO - training batch 1101, loss: 0.383, 35232/60000 datapoints
2025-03-06 19:05:05,507 - INFO - training batch 1151, loss: 0.213, 36832/60000 datapoints
2025-03-06 19:05:05,751 - INFO - training batch 1201, loss: 0.490, 38432/60000 datapoints
2025-03-06 19:05:05,981 - INFO - training batch 1251, loss: 0.754, 40032/60000 datapoints
2025-03-06 19:05:06,200 - INFO - training batch 1301, loss: 0.264, 41632/60000 datapoints
2025-03-06 19:05:06,417 - INFO - training batch 1351, loss: 0.207, 43232/60000 datapoints
2025-03-06 19:05:06,642 - INFO - training batch 1401, loss: 0.342, 44832/60000 datapoints
2025-03-06 19:05:06,872 - INFO - training batch 1451, loss: 0.540, 46432/60000 datapoints
2025-03-06 19:05:07,085 - INFO - training batch 1501, loss: 0.253, 48032/60000 datapoints
2025-03-06 19:05:07,311 - INFO - training batch 1551, loss: 0.379, 49632/60000 datapoints
2025-03-06 19:05:07,650 - INFO - training batch 1601, loss: 0.385, 51232/60000 datapoints
2025-03-06 19:05:07,931 - INFO - training batch 1651, loss: 0.392, 52832/60000 datapoints
2025-03-06 19:05:08,191 - INFO - training batch 1701, loss: 0.262, 54432/60000 datapoints
2025-03-06 19:05:09,404 - INFO - training batch 1751, loss: 0.385, 56032/60000 datapoints
2025-03-06 19:05:10,087 - INFO - training batch 1801, loss: 0.321, 57632/60000 datapoints
2025-03-06 19:05:10,382 - INFO - training batch 1851, loss: 0.451, 59232/60000 datapoints
2025-03-06 19:05:10,561 - INFO - validation batch 1, loss: 0.152, 32/10016 datapoints
2025-03-06 19:05:11,073 - INFO - validation batch 51, loss: 0.387, 1632/10016 datapoints
2025-03-06 19:05:11,328 - INFO - validation batch 101, loss: 0.308, 3232/10016 datapoints
2025-03-06 19:05:11,539 - INFO - validation batch 151, loss: 0.352, 4832/10016 datapoints
2025-03-06 19:05:11,731 - INFO - validation batch 201, loss: 0.928, 6432/10016 datapoints
2025-03-06 19:05:11,962 - INFO - validation batch 251, loss: 0.313, 8032/10016 datapoints
2025-03-06 19:05:12,169 - INFO - validation batch 301, loss: 0.333, 9632/10016 datapoints
2025-03-06 19:05:12,215 - INFO - Epoch 191/800 done.
2025-03-06 19:05:12,216 - INFO - Final validation performance:
Loss: 0.396, top-1 acc: 0.900top-5 acc: 0.900
2025-03-06 19:05:12,217 - INFO - Beginning epoch 192/800
2025-03-06 19:05:12,225 - INFO - training batch 1, loss: 0.643, 32/60000 datapoints
2025-03-06 19:05:12,485 - INFO - training batch 51, loss: 0.397, 1632/60000 datapoints
2025-03-06 19:05:12,721 - INFO - training batch 101, loss: 0.456, 3232/60000 datapoints
2025-03-06 19:05:12,962 - INFO - training batch 151, loss: 0.278, 4832/60000 datapoints
2025-03-06 19:05:13,228 - INFO - training batch 201, loss: 0.223, 6432/60000 datapoints
2025-03-06 19:05:13,477 - INFO - training batch 251, loss: 0.190, 8032/60000 datapoints
2025-03-06 19:05:13,706 - INFO - training batch 301, loss: 0.313, 9632/60000 datapoints
2025-03-06 19:05:13,983 - INFO - training batch 351, loss: 0.403, 11232/60000 datapoints
2025-03-06 19:05:14,449 - INFO - training batch 401, loss: 0.385, 12832/60000 datapoints
2025-03-06 19:05:14,687 - INFO - training batch 451, loss: 0.516, 14432/60000 datapoints
2025-03-06 19:05:14,934 - INFO - training batch 501, loss: 0.218, 16032/60000 datapoints
2025-03-06 19:05:15,178 - INFO - training batch 551, loss: 0.360, 17632/60000 datapoints
2025-03-06 19:05:15,435 - INFO - training batch 601, loss: 0.313, 19232/60000 datapoints
2025-03-06 19:05:15,668 - INFO - training batch 651, loss: 0.324, 20832/60000 datapoints
2025-03-06 19:05:16,035 - INFO - training batch 701, loss: 0.310, 22432/60000 datapoints
2025-03-06 19:05:16,295 - INFO - training batch 751, loss: 0.391, 24032/60000 datapoints
2025-03-06 19:05:16,522 - INFO - training batch 801, loss: 0.509, 25632/60000 datapoints
2025-03-06 19:05:16,769 - INFO - training batch 851, loss: 0.382, 27232/60000 datapoints
2025-03-06 19:05:16,996 - INFO - training batch 901, loss: 0.438, 28832/60000 datapoints
2025-03-06 19:05:17,233 - INFO - training batch 951, loss: 0.245, 30432/60000 datapoints
2025-03-06 19:05:17,477 - INFO - training batch 1001, loss: 0.464, 32032/60000 datapoints
2025-03-06 19:05:17,684 - INFO - training batch 1051, loss: 0.202, 33632/60000 datapoints
2025-03-06 19:05:17,925 - INFO - training batch 1101, loss: 0.141, 35232/60000 datapoints
2025-03-06 19:05:18,170 - INFO - training batch 1151, loss: 0.500, 36832/60000 datapoints
2025-03-06 19:05:18,449 - INFO - training batch 1201, loss: 0.243, 38432/60000 datapoints
2025-03-06 19:05:18,669 - INFO - training batch 1251, loss: 0.281, 40032/60000 datapoints
2025-03-06 19:05:18,902 - INFO - training batch 1301, loss: 0.386, 41632/60000 datapoints
2025-03-06 19:05:19,151 - INFO - training batch 1351, loss: 0.738, 43232/60000 datapoints
2025-03-06 19:05:19,376 - INFO - training batch 1401, loss: 0.171, 44832/60000 datapoints
2025-03-06 19:05:19,624 - INFO - training batch 1451, loss: 0.313, 46432/60000 datapoints
2025-03-06 19:05:19,854 - INFO - training batch 1501, loss: 0.196, 48032/60000 datapoints
2025-03-06 19:05:20,067 - INFO - training batch 1551, loss: 0.404, 49632/60000 datapoints
2025-03-06 19:05:20,279 - INFO - training batch 1601, loss: 0.645, 51232/60000 datapoints
2025-03-06 19:05:20,518 - INFO - training batch 1651, loss: 0.329, 52832/60000 datapoints
2025-03-06 19:05:20,745 - INFO - training batch 1701, loss: 0.566, 54432/60000 datapoints
2025-03-06 19:05:21,086 - INFO - training batch 1751, loss: 0.481, 56032/60000 datapoints
2025-03-06 19:05:21,321 - INFO - training batch 1801, loss: 0.352, 57632/60000 datapoints
2025-03-06 19:05:21,547 - INFO - training batch 1851, loss: 0.481, 59232/60000 datapoints
2025-03-06 19:05:21,681 - INFO - validation batch 1, loss: 0.660, 32/10016 datapoints
2025-03-06 19:05:21,865 - INFO - validation batch 51, loss: 0.554, 1632/10016 datapoints
2025-03-06 19:05:22,051 - INFO - validation batch 101, loss: 0.240, 3232/10016 datapoints
2025-03-06 19:05:22,235 - INFO - validation batch 151, loss: 0.347, 4832/10016 datapoints
2025-03-06 19:05:22,576 - INFO - validation batch 201, loss: 0.238, 6432/10016 datapoints
2025-03-06 19:05:22,746 - INFO - validation batch 251, loss: 0.244, 8032/10016 datapoints
2025-03-06 19:05:22,945 - INFO - validation batch 301, loss: 0.142, 9632/10016 datapoints
2025-03-06 19:05:22,993 - INFO - Epoch 192/800 done.
2025-03-06 19:05:22,993 - INFO - Final validation performance:
Loss: 0.346, top-1 acc: 0.900top-5 acc: 0.900
2025-03-06 19:05:22,994 - INFO - Beginning epoch 193/800
2025-03-06 19:05:23,003 - INFO - training batch 1, loss: 0.527, 32/60000 datapoints
2025-03-06 19:05:23,277 - INFO - training batch 51, loss: 0.400, 1632/60000 datapoints
2025-03-06 19:05:23,528 - INFO - training batch 101, loss: 0.516, 3232/60000 datapoints
2025-03-06 19:05:23,844 - INFO - training batch 151, loss: 0.255, 4832/60000 datapoints
2025-03-06 19:05:24,147 - INFO - training batch 201, loss: 0.430, 6432/60000 datapoints
2025-03-06 19:05:24,475 - INFO - training batch 251, loss: 0.256, 8032/60000 datapoints
2025-03-06 19:05:24,699 - INFO - training batch 301, loss: 0.127, 9632/60000 datapoints
2025-03-06 19:05:24,935 - INFO - training batch 351, loss: 0.547, 11232/60000 datapoints
2025-03-06 19:05:25,226 - INFO - training batch 401, loss: 0.399, 12832/60000 datapoints
2025-03-06 19:05:25,504 - INFO - training batch 451, loss: 0.230, 14432/60000 datapoints
2025-03-06 19:05:25,751 - INFO - training batch 501, loss: 0.229, 16032/60000 datapoints
2025-03-06 19:05:25,989 - INFO - training batch 551, loss: 0.315, 17632/60000 datapoints
2025-03-06 19:05:26,420 - INFO - training batch 601, loss: 0.593, 19232/60000 datapoints
2025-03-06 19:05:26,634 - INFO - training batch 651, loss: 0.249, 20832/60000 datapoints
2025-03-06 19:05:26,848 - INFO - training batch 701, loss: 0.619, 22432/60000 datapoints
2025-03-06 19:05:27,080 - INFO - training batch 751, loss: 0.539, 24032/60000 datapoints
2025-03-06 19:05:27,291 - INFO - training batch 801, loss: 0.373, 25632/60000 datapoints
2025-03-06 19:05:27,500 - INFO - training batch 851, loss: 0.354, 27232/60000 datapoints
2025-03-06 19:05:27,724 - INFO - training batch 901, loss: 0.265, 28832/60000 datapoints
2025-03-06 19:05:27,924 - INFO - training batch 951, loss: 0.430, 30432/60000 datapoints
2025-03-06 19:05:28,123 - INFO - training batch 1001, loss: 0.336, 32032/60000 datapoints
2025-03-06 19:05:28,327 - INFO - training batch 1051, loss: 0.231, 33632/60000 datapoints
2025-03-06 19:05:28,524 - INFO - training batch 1101, loss: 0.223, 35232/60000 datapoints
2025-03-06 19:05:28,729 - INFO - training batch 1151, loss: 0.555, 36832/60000 datapoints
2025-03-06 19:05:28,957 - INFO - training batch 1201, loss: 0.264, 38432/60000 datapoints
2025-03-06 19:05:29,191 - INFO - training batch 1251, loss: 0.363, 40032/60000 datapoints
2025-03-06 19:05:29,395 - INFO - training batch 1301, loss: 0.526, 41632/60000 datapoints
2025-03-06 19:05:29,619 - INFO - training batch 1351, loss: 0.372, 43232/60000 datapoints
2025-03-06 19:05:29,832 - INFO - training batch 1401, loss: 0.204, 44832/60000 datapoints
2025-03-06 19:05:30,035 - INFO - training batch 1451, loss: 0.254, 46432/60000 datapoints
2025-03-06 19:05:30,251 - INFO - training batch 1501, loss: 0.335, 48032/60000 datapoints
2025-03-06 19:05:30,463 - INFO - training batch 1551, loss: 0.450, 49632/60000 datapoints
2025-03-06 19:05:30,673 - INFO - training batch 1601, loss: 0.307, 51232/60000 datapoints
2025-03-06 19:05:30,898 - INFO - training batch 1651, loss: 0.345, 52832/60000 datapoints
2025-03-06 19:05:31,123 - INFO - training batch 1701, loss: 0.370, 54432/60000 datapoints
2025-03-06 19:05:31,343 - INFO - training batch 1751, loss: 0.245, 56032/60000 datapoints
2025-03-06 19:05:31,587 - INFO - training batch 1801, loss: 0.964, 57632/60000 datapoints
2025-03-06 19:05:31,918 - INFO - training batch 1851, loss: 0.426, 59232/60000 datapoints
2025-03-06 19:05:32,030 - INFO - validation batch 1, loss: 0.335, 32/10016 datapoints
2025-03-06 19:05:32,205 - INFO - validation batch 51, loss: 0.281, 1632/10016 datapoints
2025-03-06 19:05:32,390 - INFO - validation batch 101, loss: 0.363, 3232/10016 datapoints
2025-03-06 19:05:32,550 - INFO - validation batch 151, loss: 0.203, 4832/10016 datapoints
2025-03-06 19:05:32,707 - INFO - validation batch 201, loss: 0.548, 6432/10016 datapoints
2025-03-06 19:05:32,911 - INFO - validation batch 251, loss: 0.411, 8032/10016 datapoints
2025-03-06 19:05:33,081 - INFO - validation batch 301, loss: 0.392, 9632/10016 datapoints
2025-03-06 19:05:33,125 - INFO - Epoch 193/800 done.
2025-03-06 19:05:33,144 - INFO - Final validation performance:
Loss: 0.362, top-1 acc: 0.900top-5 acc: 0.900
2025-03-06 19:05:33,144 - INFO - Beginning epoch 194/800
2025-03-06 19:05:33,152 - INFO - training batch 1, loss: 0.198, 32/60000 datapoints
2025-03-06 19:05:33,388 - INFO - training batch 51, loss: 0.443, 1632/60000 datapoints
2025-03-06 19:05:33,656 - INFO - training batch 101, loss: 0.352, 3232/60000 datapoints
2025-03-06 19:05:34,225 - INFO - training batch 151, loss: 0.397, 4832/60000 datapoints
2025-03-06 19:05:34,597 - INFO - training batch 201, loss: 0.379, 6432/60000 datapoints
2025-03-06 19:05:34,889 - INFO - training batch 251, loss: 0.465, 8032/60000 datapoints
2025-03-06 19:05:35,145 - INFO - training batch 301, loss: 0.311, 9632/60000 datapoints
2025-03-06 19:05:35,422 - INFO - training batch 351, loss: 0.504, 11232/60000 datapoints
2025-03-06 19:05:35,703 - INFO - training batch 401, loss: 0.157, 12832/60000 datapoints
2025-03-06 19:05:35,960 - INFO - training batch 451, loss: 0.406, 14432/60000 datapoints
2025-03-06 19:05:36,223 - INFO - training batch 501, loss: 0.698, 16032/60000 datapoints
2025-03-06 19:05:36,478 - INFO - training batch 551, loss: 0.313, 17632/60000 datapoints
2025-03-06 19:05:36,718 - INFO - training batch 601, loss: 0.450, 19232/60000 datapoints
2025-03-06 19:05:36,949 - INFO - training batch 651, loss: 0.117, 20832/60000 datapoints
2025-03-06 19:05:37,180 - INFO - training batch 701, loss: 0.132, 22432/60000 datapoints
2025-03-06 19:05:37,409 - INFO - training batch 751, loss: 0.263, 24032/60000 datapoints
2025-03-06 19:05:37,656 - INFO - training batch 801, loss: 0.431, 25632/60000 datapoints
2025-03-06 19:05:37,891 - INFO - training batch 851, loss: 0.574, 27232/60000 datapoints
2025-03-06 19:05:38,114 - INFO - training batch 901, loss: 0.577, 28832/60000 datapoints
2025-03-06 19:05:38,360 - INFO - training batch 951, loss: 0.309, 30432/60000 datapoints
2025-03-06 19:05:38,601 - INFO - training batch 1001, loss: 0.509, 32032/60000 datapoints
2025-03-06 19:05:38,844 - INFO - training batch 1051, loss: 0.247, 33632/60000 datapoints
2025-03-06 19:05:39,089 - INFO - training batch 1101, loss: 0.318, 35232/60000 datapoints
2025-03-06 19:05:39,451 - INFO - training batch 1151, loss: 0.399, 36832/60000 datapoints
2025-03-06 19:05:40,054 - INFO - training batch 1201, loss: 0.504, 38432/60000 datapoints
2025-03-06 19:05:40,597 - INFO - training batch 1251, loss: 0.308, 40032/60000 datapoints
2025-03-06 19:05:41,151 - INFO - training batch 1301, loss: 0.515, 41632/60000 datapoints
2025-03-06 19:05:41,739 - INFO - training batch 1351, loss: 0.243, 43232/60000 datapoints
2025-03-06 19:05:42,290 - INFO - training batch 1401, loss: 0.608, 44832/60000 datapoints
2025-03-06 19:05:43,152 - INFO - training batch 1451, loss: 0.197, 46432/60000 datapoints
2025-03-06 19:05:43,479 - INFO - training batch 1501, loss: 0.178, 48032/60000 datapoints
2025-03-06 19:05:43,743 - INFO - training batch 1551, loss: 0.261, 49632/60000 datapoints
2025-03-06 19:05:43,981 - INFO - training batch 1601, loss: 0.471, 51232/60000 datapoints
2025-03-06 19:05:44,213 - INFO - training batch 1651, loss: 0.636, 52832/60000 datapoints
2025-03-06 19:05:44,443 - INFO - training batch 1701, loss: 0.151, 54432/60000 datapoints
2025-03-06 19:05:44,662 - INFO - training batch 1751, loss: 0.202, 56032/60000 datapoints
2025-03-06 19:05:44,875 - INFO - training batch 1801, loss: 0.367, 57632/60000 datapoints
2025-03-06 19:05:45,087 - INFO - training batch 1851, loss: 0.329, 59232/60000 datapoints
2025-03-06 19:05:45,197 - INFO - validation batch 1, loss: 0.332, 32/10016 datapoints
2025-03-06 19:05:45,488 - INFO - validation batch 51, loss: 0.610, 1632/10016 datapoints
2025-03-06 19:05:45,861 - INFO - validation batch 101, loss: 0.234, 3232/10016 datapoints
2025-03-06 19:05:46,262 - INFO - validation batch 151, loss: 0.470, 4832/10016 datapoints
2025-03-06 19:05:46,425 - INFO - validation batch 201, loss: 0.343, 6432/10016 datapoints
2025-03-06 19:05:46,600 - INFO - validation batch 251, loss: 0.170, 8032/10016 datapoints
2025-03-06 19:05:46,781 - INFO - validation batch 301, loss: 0.267, 9632/10016 datapoints
2025-03-06 19:05:46,828 - INFO - Epoch 194/800 done.
2025-03-06 19:05:46,828 - INFO - Final validation performance:
Loss: 0.347, top-1 acc: 0.900top-5 acc: 0.900
2025-03-06 19:05:46,829 - INFO - Beginning epoch 195/800
2025-03-06 19:05:46,838 - INFO - training batch 1, loss: 0.404, 32/60000 datapoints
2025-03-06 19:05:47,072 - INFO - training batch 51, loss: 0.432, 1632/60000 datapoints
2025-03-06 19:05:47,293 - INFO - training batch 101, loss: 0.433, 3232/60000 datapoints
2025-03-06 19:05:47,533 - INFO - training batch 151, loss: 0.507, 4832/60000 datapoints
2025-03-06 19:05:47,745 - INFO - training batch 201, loss: 0.245, 6432/60000 datapoints
2025-03-06 19:05:47,950 - INFO - training batch 251, loss: 0.496, 8032/60000 datapoints
2025-03-06 19:05:48,163 - INFO - training batch 301, loss: 0.357, 9632/60000 datapoints
2025-03-06 19:05:48,461 - INFO - training batch 351, loss: 0.349, 11232/60000 datapoints
2025-03-06 19:05:48,953 - INFO - training batch 401, loss: 0.377, 12832/60000 datapoints
2025-03-06 19:05:49,321 - INFO - training batch 451, loss: 0.215, 14432/60000 datapoints
2025-03-06 19:05:49,556 - INFO - training batch 501, loss: 0.258, 16032/60000 datapoints
2025-03-06 19:05:49,771 - INFO - training batch 551, loss: 0.273, 17632/60000 datapoints
2025-03-06 19:05:49,987 - INFO - training batch 601, loss: 0.279, 19232/60000 datapoints
2025-03-06 19:05:50,223 - INFO - training batch 651, loss: 0.284, 20832/60000 datapoints
2025-03-06 19:05:50,447 - INFO - training batch 701, loss: 0.514, 22432/60000 datapoints
2025-03-06 19:05:50,676 - INFO - training batch 751, loss: 0.199, 24032/60000 datapoints
2025-03-06 19:05:50,891 - INFO - training batch 801, loss: 0.383, 25632/60000 datapoints
2025-03-06 19:05:51,093 - INFO - training batch 851, loss: 0.298, 27232/60000 datapoints
2025-03-06 19:05:51,308 - INFO - training batch 901, loss: 0.271, 28832/60000 datapoints
2025-03-06 19:05:51,604 - INFO - training batch 951, loss: 0.295, 30432/60000 datapoints
2025-03-06 19:05:51,841 - INFO - training batch 1001, loss: 0.279, 32032/60000 datapoints
2025-03-06 19:05:52,054 - INFO - training batch 1051, loss: 0.540, 33632/60000 datapoints
2025-03-06 19:05:52,276 - INFO - training batch 1101, loss: 0.439, 35232/60000 datapoints
2025-03-06 19:05:52,482 - INFO - training batch 1151, loss: 0.387, 36832/60000 datapoints
2025-03-06 19:05:52,699 - INFO - training batch 1201, loss: 0.317, 38432/60000 datapoints
2025-03-06 19:05:52,922 - INFO - training batch 1251, loss: 0.461, 40032/60000 datapoints
2025-03-06 19:05:53,137 - INFO - training batch 1301, loss: 0.457, 41632/60000 datapoints
2025-03-06 19:05:53,471 - INFO - training batch 1351, loss: 0.308, 43232/60000 datapoints
2025-03-06 19:05:53,937 - INFO - training batch 1401, loss: 0.334, 44832/60000 datapoints
2025-03-06 19:05:54,314 - INFO - training batch 1451, loss: 0.425, 46432/60000 datapoints
2025-03-06 19:05:54,529 - INFO - training batch 1501, loss: 0.306, 48032/60000 datapoints
2025-03-06 19:05:54,778 - INFO - training batch 1551, loss: 0.229, 49632/60000 datapoints
2025-03-06 19:05:55,012 - INFO - training batch 1601, loss: 0.302, 51232/60000 datapoints
2025-03-06 19:05:55,237 - INFO - training batch 1651, loss: 0.504, 52832/60000 datapoints
2025-03-06 19:05:55,447 - INFO - training batch 1701, loss: 0.394, 54432/60000 datapoints
2025-03-06 19:05:55,690 - INFO - training batch 1751, loss: 0.264, 56032/60000 datapoints
2025-03-06 19:05:55,928 - INFO - training batch 1801, loss: 0.304, 57632/60000 datapoints
2025-03-06 19:05:56,187 - INFO - training batch 1851, loss: 0.343, 59232/60000 datapoints
2025-03-06 19:05:56,302 - INFO - validation batch 1, loss: 0.408, 32/10016 datapoints
2025-03-06 19:05:56,479 - INFO - validation batch 51, loss: 0.361, 1632/10016 datapoints
2025-03-06 19:05:56,642 - INFO - validation batch 101, loss: 0.285, 3232/10016 datapoints
2025-03-06 19:05:56,819 - INFO - validation batch 151, loss: 0.381, 4832/10016 datapoints
2025-03-06 19:05:56,992 - INFO - validation batch 201, loss: 0.420, 6432/10016 datapoints
2025-03-06 19:05:57,251 - INFO - validation batch 251, loss: 0.128, 8032/10016 datapoints
2025-03-06 19:05:57,654 - INFO - validation batch 301, loss: 0.590, 9632/10016 datapoints
2025-03-06 19:05:57,735 - INFO - Epoch 195/800 done.
2025-03-06 19:05:57,735 - INFO - Final validation performance:
Loss: 0.367, top-1 acc: 0.900top-5 acc: 0.900
2025-03-06 19:05:57,737 - INFO - Beginning epoch 196/800
2025-03-06 19:05:57,750 - INFO - training batch 1, loss: 0.440, 32/60000 datapoints
2025-03-06 19:05:58,169 - INFO - training batch 51, loss: 0.226, 1632/60000 datapoints
2025-03-06 19:05:58,425 - INFO - training batch 101, loss: 0.182, 3232/60000 datapoints
2025-03-06 19:05:58,636 - INFO - training batch 151, loss: 0.453, 4832/60000 datapoints
2025-03-06 19:05:58,844 - INFO - training batch 201, loss: 0.564, 6432/60000 datapoints
2025-03-06 19:05:59,072 - INFO - training batch 251, loss: 0.419, 8032/60000 datapoints
2025-03-06 19:05:59,267 - INFO - training batch 301, loss: 0.687, 9632/60000 datapoints
2025-03-06 19:05:59,469 - INFO - training batch 351, loss: 0.347, 11232/60000 datapoints
2025-03-06 19:05:59,669 - INFO - training batch 401, loss: 0.406, 12832/60000 datapoints
2025-03-06 19:05:59,866 - INFO - training batch 451, loss: 0.196, 14432/60000 datapoints
2025-03-06 19:06:00,065 - INFO - training batch 501, loss: 0.348, 16032/60000 datapoints
2025-03-06 19:06:00,260 - INFO - training batch 551, loss: 0.352, 17632/60000 datapoints
2025-03-06 19:06:00,457 - INFO - training batch 601, loss: 0.394, 19232/60000 datapoints
2025-03-06 19:06:00,657 - INFO - training batch 651, loss: 0.155, 20832/60000 datapoints
2025-03-06 19:06:00,850 - INFO - training batch 701, loss: 0.376, 22432/60000 datapoints
2025-03-06 19:06:01,049 - INFO - training batch 751, loss: 0.317, 24032/60000 datapoints
2025-03-06 19:06:01,245 - INFO - training batch 801, loss: 0.209, 25632/60000 datapoints
2025-03-06 19:06:01,442 - INFO - training batch 851, loss: 0.813, 27232/60000 datapoints
2025-03-06 19:06:01,641 - INFO - training batch 901, loss: 0.238, 28832/60000 datapoints
2025-03-06 19:06:01,849 - INFO - training batch 951, loss: 0.587, 30432/60000 datapoints
2025-03-06 19:06:02,051 - INFO - training batch 1001, loss: 0.288, 32032/60000 datapoints
2025-03-06 19:06:02,247 - INFO - training batch 1051, loss: 0.433, 33632/60000 datapoints
2025-03-06 19:06:02,450 - INFO - training batch 1101, loss: 0.409, 35232/60000 datapoints
2025-03-06 19:06:02,646 - INFO - training batch 1151, loss: 0.306, 36832/60000 datapoints
2025-03-06 19:06:02,839 - INFO - training batch 1201, loss: 0.249, 38432/60000 datapoints
2025-03-06 19:06:03,038 - INFO - training batch 1251, loss: 0.329, 40032/60000 datapoints
2025-03-06 19:06:03,231 - INFO - training batch 1301, loss: 0.330, 41632/60000 datapoints
2025-03-06 19:06:03,465 - INFO - training batch 1351, loss: 0.478, 43232/60000 datapoints
2025-03-06 19:06:03,665 - INFO - training batch 1401, loss: 0.353, 44832/60000 datapoints
2025-03-06 19:06:03,864 - INFO - training batch 1451, loss: 0.412, 46432/60000 datapoints
2025-03-06 19:06:04,063 - INFO - training batch 1501, loss: 0.292, 48032/60000 datapoints
2025-03-06 19:06:04,257 - INFO - training batch 1551, loss: 0.233, 49632/60000 datapoints
2025-03-06 19:06:04,454 - INFO - training batch 1601, loss: 0.310, 51232/60000 datapoints
2025-03-06 19:06:04,655 - INFO - training batch 1651, loss: 0.371, 52832/60000 datapoints
2025-03-06 19:06:04,859 - INFO - training batch 1701, loss: 0.363, 54432/60000 datapoints
2025-03-06 19:06:05,061 - INFO - training batch 1751, loss: 0.620, 56032/60000 datapoints
2025-03-06 19:06:05,256 - INFO - training batch 1801, loss: 0.473, 57632/60000 datapoints
2025-03-06 19:06:05,849 - INFO - training batch 1851, loss: 0.575, 59232/60000 datapoints
2025-03-06 19:06:06,055 - INFO - validation batch 1, loss: 0.522, 32/10016 datapoints
2025-03-06 19:06:06,417 - INFO - validation batch 51, loss: 0.392, 1632/10016 datapoints
2025-03-06 19:06:06,573 - INFO - validation batch 101, loss: 0.456, 3232/10016 datapoints
2025-03-06 19:06:06,731 - INFO - validation batch 151, loss: 0.420, 4832/10016 datapoints
2025-03-06 19:06:06,889 - INFO - validation batch 201, loss: 0.286, 6432/10016 datapoints
2025-03-06 19:06:07,049 - INFO - validation batch 251, loss: 0.187, 8032/10016 datapoints
2025-03-06 19:06:07,206 - INFO - validation batch 301, loss: 0.186, 9632/10016 datapoints
2025-03-06 19:06:07,244 - INFO - Epoch 196/800 done.
2025-03-06 19:06:07,245 - INFO - Final validation performance:
Loss: 0.350, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 19:06:07,245 - INFO - Beginning epoch 197/800
2025-03-06 19:06:07,259 - INFO - training batch 1, loss: 0.335, 32/60000 datapoints
2025-03-06 19:06:07,496 - INFO - training batch 51, loss: 0.285, 1632/60000 datapoints
2025-03-06 19:06:07,699 - INFO - training batch 101, loss: 0.307, 3232/60000 datapoints
2025-03-06 19:06:07,906 - INFO - training batch 151, loss: 0.269, 4832/60000 datapoints
2025-03-06 19:06:08,111 - INFO - training batch 201, loss: 0.330, 6432/60000 datapoints
2025-03-06 19:06:08,315 - INFO - training batch 251, loss: 0.358, 8032/60000 datapoints
2025-03-06 19:06:08,512 - INFO - training batch 301, loss: 0.285, 9632/60000 datapoints
2025-03-06 19:06:08,714 - INFO - training batch 351, loss: 0.498, 11232/60000 datapoints
2025-03-06 19:06:08,924 - INFO - training batch 401, loss: 0.590, 12832/60000 datapoints
2025-03-06 19:06:09,119 - INFO - training batch 451, loss: 0.488, 14432/60000 datapoints
2025-03-06 19:06:09,316 - INFO - training batch 501, loss: 0.228, 16032/60000 datapoints
2025-03-06 19:06:09,521 - INFO - training batch 551, loss: 0.229, 17632/60000 datapoints
2025-03-06 19:06:09,721 - INFO - training batch 601, loss: 0.310, 19232/60000 datapoints
2025-03-06 19:06:09,920 - INFO - training batch 651, loss: 0.241, 20832/60000 datapoints
2025-03-06 19:06:10,117 - INFO - training batch 701, loss: 0.325, 22432/60000 datapoints
2025-03-06 19:06:10,316 - INFO - training batch 751, loss: 0.215, 24032/60000 datapoints
2025-03-06 19:06:10,514 - INFO - training batch 801, loss: 0.617, 25632/60000 datapoints
2025-03-06 19:06:10,714 - INFO - training batch 851, loss: 0.497, 27232/60000 datapoints
2025-03-06 19:06:10,916 - INFO - training batch 901, loss: 0.393, 28832/60000 datapoints
2025-03-06 19:06:11,121 - INFO - training batch 951, loss: 0.350, 30432/60000 datapoints
2025-03-06 19:06:11,319 - INFO - training batch 1001, loss: 0.352, 32032/60000 datapoints
2025-03-06 19:06:11,520 - INFO - training batch 1051, loss: 0.289, 33632/60000 datapoints
2025-03-06 19:06:11,721 - INFO - training batch 1101, loss: 0.547, 35232/60000 datapoints
2025-03-06 19:06:11,945 - INFO - training batch 1151, loss: 0.522, 36832/60000 datapoints
2025-03-06 19:06:12,149 - INFO - training batch 1201, loss: 0.577, 38432/60000 datapoints
2025-03-06 19:06:12,348 - INFO - training batch 1251, loss: 0.223, 40032/60000 datapoints
2025-03-06 19:06:12,546 - INFO - training batch 1301, loss: 0.410, 41632/60000 datapoints
2025-03-06 19:06:12,749 - INFO - training batch 1351, loss: 0.310, 43232/60000 datapoints
2025-03-06 19:06:12,952 - INFO - training batch 1401, loss: 0.352, 44832/60000 datapoints
2025-03-06 19:06:13,152 - INFO - training batch 1451, loss: 0.612, 46432/60000 datapoints
2025-03-06 19:06:13,349 - INFO - training batch 1501, loss: 0.157, 48032/60000 datapoints
2025-03-06 19:06:13,547 - INFO - training batch 1551, loss: 0.443, 49632/60000 datapoints
2025-03-06 19:06:13,748 - INFO - training batch 1601, loss: 0.632, 51232/60000 datapoints
2025-03-06 19:06:13,952 - INFO - training batch 1651, loss: 0.291, 52832/60000 datapoints
2025-03-06 19:06:14,152 - INFO - training batch 1701, loss: 0.359, 54432/60000 datapoints
2025-03-06 19:06:14,346 - INFO - training batch 1751, loss: 0.193, 56032/60000 datapoints
2025-03-06 19:06:14,548 - INFO - training batch 1801, loss: 0.696, 57632/60000 datapoints
2025-03-06 19:06:14,745 - INFO - training batch 1851, loss: 0.310, 59232/60000 datapoints
2025-03-06 19:06:14,853 - INFO - validation batch 1, loss: 0.411, 32/10016 datapoints
2025-03-06 19:06:15,015 - INFO - validation batch 51, loss: 0.389, 1632/10016 datapoints
2025-03-06 19:06:15,173 - INFO - validation batch 101, loss: 0.234, 3232/10016 datapoints
2025-03-06 19:06:15,329 - INFO - validation batch 151, loss: 0.672, 4832/10016 datapoints
2025-03-06 19:06:15,490 - INFO - validation batch 201, loss: 0.542, 6432/10016 datapoints
2025-03-06 19:06:15,650 - INFO - validation batch 251, loss: 0.375, 8032/10016 datapoints
2025-03-06 19:06:15,804 - INFO - validation batch 301, loss: 0.252, 9632/10016 datapoints
2025-03-06 19:06:15,844 - INFO - Epoch 197/800 done.
2025-03-06 19:06:15,844 - INFO - Final validation performance:
Loss: 0.411, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 19:06:15,844 - INFO - Beginning epoch 198/800
2025-03-06 19:06:15,852 - INFO - training batch 1, loss: 0.381, 32/60000 datapoints
2025-03-06 19:06:16,148 - INFO - training batch 51, loss: 0.583, 1632/60000 datapoints
2025-03-06 19:06:16,378 - INFO - training batch 101, loss: 0.416, 3232/60000 datapoints
2025-03-06 19:06:16,574 - INFO - training batch 151, loss: 0.441, 4832/60000 datapoints
2025-03-06 19:06:16,774 - INFO - training batch 201, loss: 0.460, 6432/60000 datapoints
2025-03-06 19:06:16,979 - INFO - training batch 251, loss: 0.489, 8032/60000 datapoints
2025-03-06 19:06:17,176 - INFO - training batch 301, loss: 0.289, 9632/60000 datapoints
2025-03-06 19:06:17,493 - INFO - training batch 351, loss: 0.229, 11232/60000 datapoints
2025-03-06 19:06:17,892 - INFO - training batch 401, loss: 0.627, 12832/60000 datapoints
2025-03-06 19:06:18,287 - INFO - training batch 451, loss: 0.193, 14432/60000 datapoints
2025-03-06 19:06:18,508 - INFO - training batch 501, loss: 0.221, 16032/60000 datapoints
2025-03-06 19:06:18,711 - INFO - training batch 551, loss: 0.401, 17632/60000 datapoints
2025-03-06 19:06:18,913 - INFO - training batch 601, loss: 0.552, 19232/60000 datapoints
2025-03-06 19:06:19,114 - INFO - training batch 651, loss: 0.322, 20832/60000 datapoints
2025-03-06 19:06:19,389 - INFO - training batch 701, loss: 0.321, 22432/60000 datapoints
2025-03-06 19:06:19,603 - INFO - training batch 751, loss: 0.515, 24032/60000 datapoints
2025-03-06 19:06:19,804 - INFO - training batch 801, loss: 0.327, 25632/60000 datapoints
2025-03-06 19:06:20,004 - INFO - training batch 851, loss: 0.452, 27232/60000 datapoints
2025-03-06 19:06:20,202 - INFO - training batch 901, loss: 0.387, 28832/60000 datapoints
2025-03-06 19:06:20,409 - INFO - training batch 951, loss: 0.567, 30432/60000 datapoints
2025-03-06 19:06:20,610 - INFO - training batch 1001, loss: 0.327, 32032/60000 datapoints
2025-03-06 19:06:20,813 - INFO - training batch 1051, loss: 0.368, 33632/60000 datapoints
2025-03-06 19:06:21,017 - INFO - training batch 1101, loss: 0.617, 35232/60000 datapoints
2025-03-06 19:06:21,218 - INFO - training batch 1151, loss: 0.213, 36832/60000 datapoints
2025-03-06 19:06:21,423 - INFO - training batch 1201, loss: 0.395, 38432/60000 datapoints
2025-03-06 19:06:21,626 - INFO - training batch 1251, loss: 0.389, 40032/60000 datapoints
2025-03-06 19:06:21,827 - INFO - training batch 1301, loss: 0.267, 41632/60000 datapoints
2025-03-06 19:06:22,051 - INFO - training batch 1351, loss: 0.254, 43232/60000 datapoints
2025-03-06 19:06:22,254 - INFO - training batch 1401, loss: 0.751, 44832/60000 datapoints
2025-03-06 19:06:22,457 - INFO - training batch 1451, loss: 0.449, 46432/60000 datapoints
2025-03-06 19:06:22,661 - INFO - training batch 1501, loss: 0.139, 48032/60000 datapoints
2025-03-06 19:06:22,861 - INFO - training batch 1551, loss: 0.251, 49632/60000 datapoints
2025-03-06 19:06:23,068 - INFO - training batch 1601, loss: 0.422, 51232/60000 datapoints
2025-03-06 19:06:23,267 - INFO - training batch 1651, loss: 0.429, 52832/60000 datapoints
2025-03-06 19:06:23,471 - INFO - training batch 1701, loss: 0.495, 54432/60000 datapoints
2025-03-06 19:06:23,682 - INFO - training batch 1751, loss: 0.215, 56032/60000 datapoints
2025-03-06 19:06:23,918 - INFO - training batch 1801, loss: 0.488, 57632/60000 datapoints
2025-03-06 19:06:24,118 - INFO - training batch 1851, loss: 0.308, 59232/60000 datapoints
2025-03-06 19:06:24,224 - INFO - validation batch 1, loss: 0.314, 32/10016 datapoints
2025-03-06 19:06:24,534 - INFO - validation batch 51, loss: 0.593, 1632/10016 datapoints
2025-03-06 19:06:24,877 - INFO - validation batch 101, loss: 0.365, 3232/10016 datapoints
2025-03-06 19:06:25,207 - INFO - validation batch 151, loss: 0.400, 4832/10016 datapoints
2025-03-06 19:06:25,427 - INFO - validation batch 201, loss: 0.492, 6432/10016 datapoints
2025-03-06 19:06:25,590 - INFO - validation batch 251, loss: 0.316, 8032/10016 datapoints
2025-03-06 19:06:25,755 - INFO - validation batch 301, loss: 0.287, 9632/10016 datapoints
2025-03-06 19:06:25,797 - INFO - Epoch 198/800 done.
2025-03-06 19:06:25,797 - INFO - Final validation performance:
Loss: 0.395, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 19:06:25,798 - INFO - Beginning epoch 199/800
2025-03-06 19:06:25,805 - INFO - training batch 1, loss: 0.523, 32/60000 datapoints
2025-03-06 19:06:26,026 - INFO - training batch 51, loss: 0.365, 1632/60000 datapoints
2025-03-06 19:06:26,228 - INFO - training batch 101, loss: 0.204, 3232/60000 datapoints
2025-03-06 19:06:26,438 - INFO - training batch 151, loss: 0.727, 4832/60000 datapoints
2025-03-06 19:06:26,640 - INFO - training batch 201, loss: 0.291, 6432/60000 datapoints
2025-03-06 19:06:26,841 - INFO - training batch 251, loss: 0.220, 8032/60000 datapoints
2025-03-06 19:06:27,042 - INFO - training batch 301, loss: 0.405, 9632/60000 datapoints
2025-03-06 19:06:27,240 - INFO - training batch 351, loss: 0.475, 11232/60000 datapoints
2025-03-06 19:06:27,438 - INFO - training batch 401, loss: 0.455, 12832/60000 datapoints
2025-03-06 19:06:27,641 - INFO - training batch 451, loss: 0.226, 14432/60000 datapoints
2025-03-06 19:06:27,836 - INFO - training batch 501, loss: 0.440, 16032/60000 datapoints
2025-03-06 19:06:28,044 - INFO - training batch 551, loss: 0.353, 17632/60000 datapoints
2025-03-06 19:06:28,241 - INFO - training batch 601, loss: 0.250, 19232/60000 datapoints
2025-03-06 19:06:28,443 - INFO - training batch 651, loss: 0.215, 20832/60000 datapoints
2025-03-06 19:06:28,641 - INFO - training batch 701, loss: 0.296, 22432/60000 datapoints
2025-03-06 19:06:28,839 - INFO - training batch 751, loss: 0.297, 24032/60000 datapoints
2025-03-06 19:06:29,042 - INFO - training batch 801, loss: 0.315, 25632/60000 datapoints
2025-03-06 19:06:29,237 - INFO - training batch 851, loss: 0.342, 27232/60000 datapoints
2025-03-06 19:06:29,438 - INFO - training batch 901, loss: 0.169, 28832/60000 datapoints
2025-03-06 19:06:29,642 - INFO - training batch 951, loss: 0.764, 30432/60000 datapoints
2025-03-06 19:06:29,837 - INFO - training batch 1001, loss: 0.397, 32032/60000 datapoints
2025-03-06 19:06:30,038 - INFO - training batch 1051, loss: 0.461, 33632/60000 datapoints
2025-03-06 19:06:30,232 - INFO - training batch 1101, loss: 0.445, 35232/60000 datapoints
2025-03-06 19:06:30,433 - INFO - training batch 1151, loss: 0.618, 36832/60000 datapoints
2025-03-06 19:06:30,632 - INFO - training batch 1201, loss: 0.330, 38432/60000 datapoints
2025-03-06 19:06:30,826 - INFO - training batch 1251, loss: 0.202, 40032/60000 datapoints
2025-03-06 19:06:31,028 - INFO - training batch 1301, loss: 0.429, 41632/60000 datapoints
2025-03-06 19:06:31,225 - INFO - training batch 1351, loss: 0.349, 43232/60000 datapoints
2025-03-06 19:06:31,426 - INFO - training batch 1401, loss: 0.399, 44832/60000 datapoints
2025-03-06 19:06:31,626 - INFO - training batch 1451, loss: 0.472, 46432/60000 datapoints
2025-03-06 19:06:31,822 - INFO - training batch 1501, loss: 0.230, 48032/60000 datapoints
2025-03-06 19:06:32,020 - INFO - training batch 1551, loss: 0.427, 49632/60000 datapoints
2025-03-06 19:06:32,240 - INFO - training batch 1601, loss: 0.319, 51232/60000 datapoints
2025-03-06 19:06:32,443 - INFO - training batch 1651, loss: 0.365, 52832/60000 datapoints
2025-03-06 19:06:32,644 - INFO - training batch 1701, loss: 0.249, 54432/60000 datapoints
2025-03-06 19:06:32,841 - INFO - training batch 1751, loss: 0.566, 56032/60000 datapoints
2025-03-06 19:06:33,044 - INFO - training batch 1801, loss: 0.231, 57632/60000 datapoints
2025-03-06 19:06:33,242 - INFO - training batch 1851, loss: 0.177, 59232/60000 datapoints
2025-03-06 19:06:33,351 - INFO - validation batch 1, loss: 0.858, 32/10016 datapoints
2025-03-06 19:06:33,512 - INFO - validation batch 51, loss: 0.120, 1632/10016 datapoints
2025-03-06 19:06:33,672 - INFO - validation batch 101, loss: 0.443, 3232/10016 datapoints
2025-03-06 19:06:33,846 - INFO - validation batch 151, loss: 0.217, 4832/10016 datapoints
2025-03-06 19:06:34,011 - INFO - validation batch 201, loss: 0.274, 6432/10016 datapoints
2025-03-06 19:06:34,167 - INFO - validation batch 251, loss: 0.226, 8032/10016 datapoints
2025-03-06 19:06:34,318 - INFO - validation batch 301, loss: 0.369, 9632/10016 datapoints
2025-03-06 19:06:34,356 - INFO - Epoch 199/800 done.
2025-03-06 19:06:34,356 - INFO - Final validation performance:
Loss: 0.358, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 19:06:34,357 - INFO - Beginning epoch 200/800
2025-03-06 19:06:34,363 - INFO - training batch 1, loss: 0.591, 32/60000 datapoints
2025-03-06 19:06:34,561 - INFO - training batch 51, loss: 0.222, 1632/60000 datapoints
2025-03-06 19:06:34,767 - INFO - training batch 101, loss: 0.369, 3232/60000 datapoints
2025-03-06 19:06:34,965 - INFO - training batch 151, loss: 0.249, 4832/60000 datapoints
2025-03-06 19:06:35,163 - INFO - training batch 201, loss: 0.231, 6432/60000 datapoints
2025-03-06 19:06:35,363 - INFO - training batch 251, loss: 0.557, 8032/60000 datapoints
2025-03-06 19:06:35,561 - INFO - training batch 301, loss: 0.316, 9632/60000 datapoints
2025-03-06 19:06:35,763 - INFO - training batch 351, loss: 0.188, 11232/60000 datapoints
2025-03-06 19:06:35,955 - INFO - training batch 401, loss: 0.281, 12832/60000 datapoints
2025-03-06 19:06:36,148 - INFO - training batch 451, loss: 0.313, 14432/60000 datapoints
2025-03-06 19:06:36,443 - INFO - training batch 501, loss: 0.256, 16032/60000 datapoints
2025-03-06 19:06:36,824 - INFO - training batch 551, loss: 0.265, 17632/60000 datapoints
2025-03-06 19:06:37,212 - INFO - training batch 601, loss: 0.196, 19232/60000 datapoints
2025-03-06 19:06:37,460 - INFO - training batch 651, loss: 0.271, 20832/60000 datapoints
2025-03-06 19:06:37,663 - INFO - training batch 701, loss: 0.293, 22432/60000 datapoints
2025-03-06 19:06:37,855 - INFO - training batch 751, loss: 0.375, 24032/60000 datapoints
2025-03-06 19:06:38,064 - INFO - training batch 801, loss: 0.314, 25632/60000 datapoints
2025-03-06 19:06:38,260 - INFO - training batch 851, loss: 0.270, 27232/60000 datapoints
2025-03-06 19:06:38,461 - INFO - training batch 901, loss: 0.428, 28832/60000 datapoints
2025-03-06 19:06:38,663 - INFO - training batch 951, loss: 0.149, 30432/60000 datapoints
2025-03-06 19:06:38,867 - INFO - training batch 1001, loss: 0.526, 32032/60000 datapoints
2025-03-06 19:06:39,078 - INFO - training batch 1051, loss: 0.218, 33632/60000 datapoints
2025-03-06 19:06:39,273 - INFO - training batch 1101, loss: 0.330, 35232/60000 datapoints
2025-03-06 19:06:39,470 - INFO - training batch 1151, loss: 0.425, 36832/60000 datapoints
2025-03-06 19:06:39,672 - INFO - training batch 1201, loss: 0.276, 38432/60000 datapoints
2025-03-06 19:06:39,868 - INFO - training batch 1251, loss: 0.599, 40032/60000 datapoints
2025-03-06 19:06:40,069 - INFO - training batch 1301, loss: 0.372, 41632/60000 datapoints
2025-03-06 19:06:40,262 - INFO - training batch 1351, loss: 0.473, 43232/60000 datapoints
2025-03-06 19:06:40,458 - INFO - training batch 1401, loss: 0.510, 44832/60000 datapoints
2025-03-06 19:06:40,655 - INFO - training batch 1451, loss: 0.319, 46432/60000 datapoints
2025-03-06 19:06:40,849 - INFO - training batch 1501, loss: 0.433, 48032/60000 datapoints
2025-03-06 19:06:41,051 - INFO - training batch 1551, loss: 0.365, 49632/60000 datapoints
2025-03-06 19:06:41,245 - INFO - training batch 1601, loss: 0.300, 51232/60000 datapoints
2025-03-06 19:06:41,442 - INFO - training batch 1651, loss: 0.497, 52832/60000 datapoints
2025-03-06 19:06:41,641 - INFO - training batch 1701, loss: 0.279, 54432/60000 datapoints
2025-03-06 19:06:41,834 - INFO - training batch 1751, loss: 0.465, 56032/60000 datapoints
2025-03-06 19:06:42,030 - INFO - training batch 1801, loss: 0.419, 57632/60000 datapoints
2025-03-06 19:06:42,242 - INFO - training batch 1851, loss: 0.459, 59232/60000 datapoints
2025-03-06 19:06:42,346 - INFO - validation batch 1, loss: 0.423, 32/10016 datapoints
2025-03-06 19:06:42,504 - INFO - validation batch 51, loss: 0.522, 1632/10016 datapoints
2025-03-06 19:06:42,660 - INFO - validation batch 101, loss: 0.313, 3232/10016 datapoints
2025-03-06 19:06:42,828 - INFO - validation batch 151, loss: 0.344, 4832/10016 datapoints
2025-03-06 19:06:42,985 - INFO - validation batch 201, loss: 0.205, 6432/10016 datapoints
2025-03-06 19:06:43,141 - INFO - validation batch 251, loss: 0.260, 8032/10016 datapoints
2025-03-06 19:06:43,296 - INFO - validation batch 301, loss: 0.373, 9632/10016 datapoints
2025-03-06 19:06:43,334 - INFO - Epoch 200/800 done.
2025-03-06 19:06:43,334 - INFO - Final validation performance:
Loss: 0.349, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 19:06:43,335 - INFO - Beginning epoch 201/800
2025-03-06 19:06:43,341 - INFO - training batch 1, loss: 0.419, 32/60000 datapoints
2025-03-06 19:06:43,550 - INFO - training batch 51, loss: 0.464, 1632/60000 datapoints
2025-03-06 19:06:43,750 - INFO - training batch 101, loss: 0.145, 3232/60000 datapoints
2025-03-06 19:06:43,959 - INFO - training batch 151, loss: 0.620, 4832/60000 datapoints
2025-03-06 19:06:44,161 - INFO - training batch 201, loss: 0.300, 6432/60000 datapoints
2025-03-06 19:06:44,448 - INFO - training batch 251, loss: 0.297, 8032/60000 datapoints
2025-03-06 19:06:44,838 - INFO - training batch 301, loss: 0.321, 9632/60000 datapoints
2025-03-06 19:06:45,233 - INFO - training batch 351, loss: 0.360, 11232/60000 datapoints
2025-03-06 19:06:45,447 - INFO - training batch 401, loss: 0.360, 12832/60000 datapoints
2025-03-06 19:06:45,649 - INFO - training batch 451, loss: 0.186, 14432/60000 datapoints
2025-03-06 19:06:45,847 - INFO - training batch 501, loss: 0.203, 16032/60000 datapoints
2025-03-06 19:06:46,046 - INFO - training batch 551, loss: 0.140, 17632/60000 datapoints
2025-03-06 19:06:46,245 - INFO - training batch 601, loss: 0.474, 19232/60000 datapoints
2025-03-06 19:06:46,444 - INFO - training batch 651, loss: 0.463, 20832/60000 datapoints
2025-03-06 19:06:46,643 - INFO - training batch 701, loss: 0.605, 22432/60000 datapoints
2025-03-06 19:06:46,840 - INFO - training batch 751, loss: 0.160, 24032/60000 datapoints
2025-03-06 19:06:47,040 - INFO - training batch 801, loss: 0.250, 25632/60000 datapoints
2025-03-06 19:06:47,238 - INFO - training batch 851, loss: 0.303, 27232/60000 datapoints
2025-03-06 19:06:47,436 - INFO - training batch 901, loss: 0.400, 28832/60000 datapoints
2025-03-06 19:06:47,636 - INFO - training batch 951, loss: 0.516, 30432/60000 datapoints
2025-03-06 19:06:47,833 - INFO - training batch 1001, loss: 0.827, 32032/60000 datapoints
2025-03-06 19:06:48,029 - INFO - training batch 1051, loss: 0.184, 33632/60000 datapoints
2025-03-06 19:06:48,225 - INFO - training batch 1101, loss: 0.174, 35232/60000 datapoints
2025-03-06 19:06:48,425 - INFO - training batch 1151, loss: 0.127, 36832/60000 datapoints
2025-03-06 19:06:48,625 - INFO - training batch 1201, loss: 0.697, 38432/60000 datapoints
2025-03-06 19:06:48,819 - INFO - training batch 1251, loss: 0.495, 40032/60000 datapoints
2025-03-06 19:06:49,019 - INFO - training batch 1301, loss: 0.444, 41632/60000 datapoints
2025-03-06 19:06:49,216 - INFO - training batch 1351, loss: 0.129, 43232/60000 datapoints
2025-03-06 19:06:49,415 - INFO - training batch 1401, loss: 0.323, 44832/60000 datapoints
2025-03-06 19:06:49,620 - INFO - training batch 1451, loss: 0.362, 46432/60000 datapoints
2025-03-06 19:06:49,812 - INFO - training batch 1501, loss: 0.328, 48032/60000 datapoints
2025-03-06 19:06:50,011 - INFO - training batch 1551, loss: 0.355, 49632/60000 datapoints
2025-03-06 19:06:50,206 - INFO - training batch 1601, loss: 0.527, 51232/60000 datapoints
2025-03-06 19:06:50,411 - INFO - training batch 1651, loss: 0.344, 52832/60000 datapoints
2025-03-06 19:06:50,610 - INFO - training batch 1701, loss: 0.327, 54432/60000 datapoints
2025-03-06 19:06:50,820 - INFO - training batch 1751, loss: 0.261, 56032/60000 datapoints
2025-03-06 19:06:51,029 - INFO - training batch 1801, loss: 0.447, 57632/60000 datapoints
2025-03-06 19:06:51,235 - INFO - training batch 1851, loss: 0.294, 59232/60000 datapoints
2025-03-06 19:06:51,345 - INFO - validation batch 1, loss: 0.410, 32/10016 datapoints
2025-03-06 19:06:51,510 - INFO - validation batch 51, loss: 0.661, 1632/10016 datapoints
2025-03-06 19:06:51,679 - INFO - validation batch 101, loss: 0.205, 3232/10016 datapoints
2025-03-06 19:06:51,842 - INFO - validation batch 151, loss: 0.192, 4832/10016 datapoints
2025-03-06 19:06:52,006 - INFO - validation batch 201, loss: 0.412, 6432/10016 datapoints
2025-03-06 19:06:52,170 - INFO - validation batch 251, loss: 0.474, 8032/10016 datapoints
2025-03-06 19:06:52,352 - INFO - validation batch 301, loss: 0.358, 9632/10016 datapoints
2025-03-06 19:06:52,397 - INFO - Epoch 201/800 done.
2025-03-06 19:06:52,397 - INFO - Final validation performance:
Loss: 0.387, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 19:06:52,398 - INFO - Beginning epoch 202/800
2025-03-06 19:06:52,407 - INFO - training batch 1, loss: 0.402, 32/60000 datapoints
2025-03-06 19:06:52,628 - INFO - training batch 51, loss: 0.554, 1632/60000 datapoints
2025-03-06 19:06:52,827 - INFO - training batch 101, loss: 0.232, 3232/60000 datapoints
2025-03-06 19:06:53,044 - INFO - training batch 151, loss: 0.612, 4832/60000 datapoints
2025-03-06 19:06:53,248 - INFO - training batch 201, loss: 0.327, 6432/60000 datapoints
2025-03-06 19:06:53,460 - INFO - training batch 251, loss: 0.141, 8032/60000 datapoints
2025-03-06 19:06:53,666 - INFO - training batch 301, loss: 0.411, 9632/60000 datapoints
2025-03-06 19:06:53,866 - INFO - training batch 351, loss: 0.297, 11232/60000 datapoints
2025-03-06 19:06:54,063 - INFO - training batch 401, loss: 0.303, 12832/60000 datapoints
2025-03-06 19:06:54,264 - INFO - training batch 451, loss: 0.357, 14432/60000 datapoints
2025-03-06 19:06:54,465 - INFO - training batch 501, loss: 0.166, 16032/60000 datapoints
2025-03-06 19:06:54,664 - INFO - training batch 551, loss: 0.489, 17632/60000 datapoints
2025-03-06 19:06:54,865 - INFO - training batch 601, loss: 0.471, 19232/60000 datapoints
2025-03-06 19:06:55,072 - INFO - training batch 651, loss: 0.287, 20832/60000 datapoints
2025-03-06 19:06:55,271 - INFO - training batch 701, loss: 0.297, 22432/60000 datapoints
2025-03-06 19:06:55,467 - INFO - training batch 751, loss: 0.410, 24032/60000 datapoints
2025-03-06 19:06:55,672 - INFO - training batch 801, loss: 0.423, 25632/60000 datapoints
2025-03-06 19:06:55,869 - INFO - training batch 851, loss: 0.322, 27232/60000 datapoints
2025-03-06 19:06:56,070 - INFO - training batch 901, loss: 0.145, 28832/60000 datapoints
2025-03-06 19:06:56,271 - INFO - training batch 951, loss: 0.275, 30432/60000 datapoints
2025-03-06 19:06:56,471 - INFO - training batch 1001, loss: 0.277, 32032/60000 datapoints
2025-03-06 19:06:56,671 - INFO - training batch 1051, loss: 0.238, 33632/60000 datapoints
2025-03-06 19:06:56,867 - INFO - training batch 1101, loss: 0.605, 35232/60000 datapoints
2025-03-06 19:06:57,069 - INFO - training batch 1151, loss: 0.264, 36832/60000 datapoints
2025-03-06 19:06:57,264 - INFO - training batch 1201, loss: 0.281, 38432/60000 datapoints
2025-03-06 19:06:57,462 - INFO - training batch 1251, loss: 0.241, 40032/60000 datapoints
2025-03-06 19:06:57,660 - INFO - training batch 1301, loss: 0.316, 41632/60000 datapoints
2025-03-06 19:06:57,850 - INFO - training batch 1351, loss: 0.339, 43232/60000 datapoints
2025-03-06 19:06:58,043 - INFO - training batch 1401, loss: 0.268, 44832/60000 datapoints
2025-03-06 19:06:58,235 - INFO - training batch 1451, loss: 0.309, 46432/60000 datapoints
2025-03-06 19:06:58,442 - INFO - training batch 1501, loss: 0.263, 48032/60000 datapoints
2025-03-06 19:06:58,641 - INFO - training batch 1551, loss: 0.305, 49632/60000 datapoints
2025-03-06 19:06:58,839 - INFO - training batch 1601, loss: 0.445, 51232/60000 datapoints
2025-03-06 19:06:59,036 - INFO - training batch 1651, loss: 0.130, 52832/60000 datapoints
2025-03-06 19:06:59,235 - INFO - training batch 1701, loss: 0.538, 54432/60000 datapoints
2025-03-06 19:06:59,622 - INFO - training batch 1751, loss: 0.372, 56032/60000 datapoints
2025-03-06 19:07:00,007 - INFO - training batch 1801, loss: 0.182, 57632/60000 datapoints
2025-03-06 19:07:00,329 - INFO - training batch 1851, loss: 0.309, 59232/60000 datapoints
2025-03-06 19:07:00,434 - INFO - validation batch 1, loss: 0.595, 32/10016 datapoints
2025-03-06 19:07:00,592 - INFO - validation batch 51, loss: 0.373, 1632/10016 datapoints
2025-03-06 19:07:00,755 - INFO - validation batch 101, loss: 0.384, 3232/10016 datapoints
2025-03-06 19:07:00,915 - INFO - validation batch 151, loss: 0.264, 4832/10016 datapoints
2025-03-06 19:07:01,074 - INFO - validation batch 201, loss: 0.511, 6432/10016 datapoints
2025-03-06 19:07:01,232 - INFO - validation batch 251, loss: 0.333, 8032/10016 datapoints
2025-03-06 19:07:01,391 - INFO - validation batch 301, loss: 0.473, 9632/10016 datapoints
2025-03-06 19:07:01,436 - INFO - Epoch 202/800 done.
2025-03-06 19:07:01,436 - INFO - Final validation performance:
Loss: 0.419, top-1 acc: 0.901top-5 acc: 0.901
2025-03-06 19:07:01,436 - INFO - Beginning epoch 203/800
2025-03-06 19:07:01,443 - INFO - training batch 1, loss: 0.266, 32/60000 datapoints
2025-03-06 19:07:01,644 - INFO - training batch 51, loss: 0.277, 1632/60000 datapoints
2025-03-06 19:07:01,845 - INFO - training batch 101, loss: 0.544, 3232/60000 datapoints
2025-03-06 19:07:02,061 - INFO - training batch 151, loss: 0.245, 4832/60000 datapoints
2025-03-06 19:07:02,258 - INFO - training batch 201, loss: 0.529, 6432/60000 datapoints
2025-03-06 19:07:02,490 - INFO - training batch 251, loss: 0.613, 8032/60000 datapoints
2025-03-06 19:07:02,700 - INFO - training batch 301, loss: 0.343, 9632/60000 datapoints
2025-03-06 19:07:02,900 - INFO - training batch 351, loss: 0.328, 11232/60000 datapoints
2025-03-06 19:07:03,103 - INFO - training batch 401, loss: 0.369, 12832/60000 datapoints
2025-03-06 19:07:03,406 - INFO - training batch 451, loss: 0.557, 14432/60000 datapoints
2025-03-06 19:07:03,681 - INFO - training batch 501, loss: 0.480, 16032/60000 datapoints
2025-03-06 19:07:03,899 - INFO - training batch 551, loss: 0.560, 17632/60000 datapoints
2025-03-06 19:07:04,094 - INFO - training batch 601, loss: 0.530, 19232/60000 datapoints
2025-03-06 19:07:04,288 - INFO - training batch 651, loss: 0.305, 20832/60000 datapoints
2025-03-06 19:07:04,486 - INFO - training batch 701, loss: 0.552, 22432/60000 datapoints
2025-03-06 19:07:04,683 - INFO - training batch 751, loss: 0.297, 24032/60000 datapoints
2025-03-06 19:07:04,881 - INFO - training batch 801, loss: 0.267, 25632/60000 datapoints
2025-03-06 19:07:05,080 - INFO - training batch 851, loss: 0.412, 27232/60000 datapoints
2025-03-06 19:07:05,275 - INFO - training batch 901, loss: 0.210, 28832/60000 datapoints
2025-03-06 19:07:05,474 - INFO - training batch 951, loss: 0.367, 30432/60000 datapoints
2025-03-06 19:07:05,671 - INFO - training batch 1001, loss: 0.374, 32032/60000 datapoints
2025-03-06 19:07:05,867 - INFO - training batch 1051, loss: 0.309, 33632/60000 datapoints
2025-03-06 19:07:06,063 - INFO - training batch 1101, loss: 0.355, 35232/60000 datapoints
2025-03-06 19:07:06,261 - INFO - training batch 1151, loss: 0.222, 36832/60000 datapoints
2025-03-06 19:07:06,461 - INFO - training batch 1201, loss: 0.425, 38432/60000 datapoints
2025-03-06 19:07:06,657 - INFO - training batch 1251, loss: 0.294, 40032/60000 datapoints
2025-03-06 19:07:06,854 - INFO - training batch 1301, loss: 0.447, 41632/60000 datapoints
2025-03-06 19:07:07,049 - INFO - training batch 1351, loss: 0.548, 43232/60000 datapoints
2025-03-06 19:07:07,245 - INFO - training batch 1401, loss: 0.115, 44832/60000 datapoints
2025-03-06 19:07:07,443 - INFO - training batch 1451, loss: 0.357, 46432/60000 datapoints
2025-03-06 19:07:07,638 - INFO - training batch 1501, loss: 0.268, 48032/60000 datapoints
2025-03-06 19:07:07,833 - INFO - training batch 1551, loss: 0.250, 49632/60000 datapoints
2025-03-06 19:07:08,029 - INFO - training batch 1601, loss: 0.426, 51232/60000 datapoints
2025-03-06 19:07:08,225 - INFO - training batch 1651, loss: 0.238, 52832/60000 datapoints
2025-03-06 19:07:08,573 - INFO - training batch 1701, loss: 0.621, 54432/60000 datapoints
2025-03-06 19:07:08,971 - INFO - training batch 1751, loss: 0.166, 56032/60000 datapoints
2025-03-06 19:07:09,316 - INFO - training batch 1801, loss: 0.272, 57632/60000 datapoints
2025-03-06 19:07:09,514 - INFO - training batch 1851, loss: 0.176, 59232/60000 datapoints
2025-03-06 19:07:09,630 - INFO - validation batch 1, loss: 0.237, 32/10016 datapoints
2025-03-06 19:07:09,787 - INFO - validation batch 51, loss: 0.169, 1632/10016 datapoints
2025-03-06 19:07:09,947 - INFO - validation batch 101, loss: 0.399, 3232/10016 datapoints
2025-03-06 19:07:10,105 - INFO - validation batch 151, loss: 0.644, 4832/10016 datapoints
2025-03-06 19:07:10,264 - INFO - validation batch 201, loss: 0.184, 6432/10016 datapoints
2025-03-06 19:07:10,425 - INFO - validation batch 251, loss: 0.404, 8032/10016 datapoints
2025-03-06 19:07:10,582 - INFO - validation batch 301, loss: 0.265, 9632/10016 datapoints
2025-03-06 19:07:10,626 - INFO - Epoch 203/800 done.
2025-03-06 19:07:10,626 - INFO - Final validation performance:
Loss: 0.329, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 19:07:10,627 - INFO - Beginning epoch 204/800
2025-03-06 19:07:10,633 - INFO - training batch 1, loss: 0.459, 32/60000 datapoints
2025-03-06 19:07:10,850 - INFO - training batch 51, loss: 0.228, 1632/60000 datapoints
2025-03-06 19:07:11,049 - INFO - training batch 101, loss: 0.448, 3232/60000 datapoints
2025-03-06 19:07:11,261 - INFO - training batch 151, loss: 0.495, 4832/60000 datapoints
2025-03-06 19:07:11,466 - INFO - training batch 201, loss: 0.400, 6432/60000 datapoints
2025-03-06 19:07:11,670 - INFO - training batch 251, loss: 0.189, 8032/60000 datapoints
2025-03-06 19:07:11,867 - INFO - training batch 301, loss: 0.151, 9632/60000 datapoints
2025-03-06 19:07:12,066 - INFO - training batch 351, loss: 0.278, 11232/60000 datapoints
2025-03-06 19:07:12,262 - INFO - training batch 401, loss: 0.647, 12832/60000 datapoints
2025-03-06 19:07:12,522 - INFO - training batch 451, loss: 0.310, 14432/60000 datapoints
2025-03-06 19:07:12,721 - INFO - training batch 501, loss: 0.453, 16032/60000 datapoints
2025-03-06 19:07:12,917 - INFO - training batch 551, loss: 0.271, 17632/60000 datapoints
2025-03-06 19:07:13,118 - INFO - training batch 601, loss: 0.442, 19232/60000 datapoints
2025-03-06 19:07:13,313 - INFO - training batch 651, loss: 0.295, 20832/60000 datapoints
2025-03-06 19:07:13,515 - INFO - training batch 701, loss: 0.413, 22432/60000 datapoints
2025-03-06 19:07:13,713 - INFO - training batch 751, loss: 0.579, 24032/60000 datapoints
2025-03-06 19:07:13,910 - INFO - training batch 801, loss: 0.195, 25632/60000 datapoints
2025-03-06 19:07:14,105 - INFO - training batch 851, loss: 0.345, 27232/60000 datapoints
2025-03-06 19:07:14,299 - INFO - training batch 901, loss: 0.328, 28832/60000 datapoints
2025-03-06 19:07:14,499 - INFO - training batch 951, loss: 0.346, 30432/60000 datapoints
2025-03-06 19:07:14,696 - INFO - training batch 1001, loss: 0.304, 32032/60000 datapoints
2025-03-06 19:07:14,893 - INFO - training batch 1051, loss: 0.350, 33632/60000 datapoints
2025-03-06 19:07:15,090 - INFO - training batch 1101, loss: 0.289, 35232/60000 datapoints
2025-03-06 19:07:15,282 - INFO - training batch 1151, loss: 0.512, 36832/60000 datapoints
2025-03-06 19:07:15,480 - INFO - training batch 1201, loss: 0.504, 38432/60000 datapoints
2025-03-06 19:07:15,677 - INFO - training batch 1251, loss: 0.420, 40032/60000 datapoints
2025-03-06 19:07:15,874 - INFO - training batch 1301, loss: 0.534, 41632/60000 datapoints
2025-03-06 19:07:16,071 - INFO - training batch 1351, loss: 0.352, 43232/60000 datapoints
2025-03-06 19:07:16,269 - INFO - training batch 1401, loss: 0.164, 44832/60000 datapoints
2025-03-06 19:07:16,470 - INFO - training batch 1451, loss: 0.261, 46432/60000 datapoints
2025-03-06 19:07:16,667 - INFO - training batch 1501, loss: 0.233, 48032/60000 datapoints
2025-03-06 19:07:16,860 - INFO - training batch 1551, loss: 0.377, 49632/60000 datapoints
2025-03-06 19:07:17,056 - INFO - training batch 1601, loss: 0.683, 51232/60000 datapoints
2025-03-06 19:07:17,251 - INFO - training batch 1651, loss: 0.173, 52832/60000 datapoints
2025-03-06 19:07:17,447 - INFO - training batch 1701, loss: 0.398, 54432/60000 datapoints
2025-03-06 19:07:17,650 - INFO - training batch 1751, loss: 0.239, 56032/60000 datapoints
2025-03-06 19:07:17,843 - INFO - training batch 1801, loss: 0.284, 57632/60000 datapoints
2025-03-06 19:07:18,040 - INFO - training batch 1851, loss: 0.206, 59232/60000 datapoints
2025-03-06 19:07:18,144 - INFO - validation batch 1, loss: 0.419, 32/10016 datapoints
2025-03-06 19:07:18,293 - INFO - validation batch 51, loss: 0.290, 1632/10016 datapoints
2025-03-06 19:07:18,450 - INFO - validation batch 101, loss: 0.578, 3232/10016 datapoints
2025-03-06 19:07:18,605 - INFO - validation batch 151, loss: 0.337, 4832/10016 datapoints
2025-03-06 19:07:18,760 - INFO - validation batch 201, loss: 0.216, 6432/10016 datapoints
2025-03-06 19:07:18,918 - INFO - validation batch 251, loss: 0.554, 8032/10016 datapoints
2025-03-06 19:07:19,072 - INFO - validation batch 301, loss: 0.418, 9632/10016 datapoints
2025-03-06 19:07:19,111 - INFO - Epoch 204/800 done.
2025-03-06 19:07:19,112 - INFO - Final validation performance:
Loss: 0.402, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 19:07:19,112 - INFO - Beginning epoch 205/800
2025-03-06 19:07:19,119 - INFO - training batch 1, loss: 0.247, 32/60000 datapoints
2025-03-06 19:07:19,365 - INFO - training batch 51, loss: 0.582, 1632/60000 datapoints
2025-03-06 19:07:19,803 - INFO - training batch 101, loss: 0.467, 3232/60000 datapoints
2025-03-06 19:07:20,188 - INFO - training batch 151, loss: 0.531, 4832/60000 datapoints
2025-03-06 19:07:20,457 - INFO - training batch 201, loss: 0.313, 6432/60000 datapoints
2025-03-06 19:07:20,672 - INFO - training batch 251, loss: 0.303, 8032/60000 datapoints
2025-03-06 19:07:20,874 - INFO - training batch 301, loss: 0.424, 9632/60000 datapoints
2025-03-06 19:07:21,074 - INFO - training batch 351, loss: 0.289, 11232/60000 datapoints
2025-03-06 19:07:21,276 - INFO - training batch 401, loss: 0.369, 12832/60000 datapoints
2025-03-06 19:07:21,479 - INFO - training batch 451, loss: 0.346, 14432/60000 datapoints
2025-03-06 19:07:21,683 - INFO - training batch 501, loss: 0.429, 16032/60000 datapoints
2025-03-06 19:07:21,895 - INFO - training batch 551, loss: 0.439, 17632/60000 datapoints
2025-03-06 19:07:22,105 - INFO - training batch 601, loss: 0.464, 19232/60000 datapoints
2025-03-06 19:07:22,303 - INFO - training batch 651, loss: 0.460, 20832/60000 datapoints
2025-03-06 19:07:22,513 - INFO - training batch 701, loss: 0.231, 22432/60000 datapoints
2025-03-06 19:07:22,734 - INFO - training batch 751, loss: 0.595, 24032/60000 datapoints
2025-03-06 19:07:22,933 - INFO - training batch 801, loss: 0.505, 25632/60000 datapoints
2025-03-06 19:07:23,137 - INFO - training batch 851, loss: 0.435, 27232/60000 datapoints
2025-03-06 19:07:23,339 - INFO - training batch 901, loss: 0.590, 28832/60000 datapoints
2025-03-06 19:07:23,540 - INFO - training batch 951, loss: 0.424, 30432/60000 datapoints
2025-03-06 19:07:23,743 - INFO - training batch 1001, loss: 0.236, 32032/60000 datapoints
2025-03-06 19:07:23,944 - INFO - training batch 1051, loss: 0.504, 33632/60000 datapoints
2025-03-06 19:07:24,145 - INFO - training batch 1101, loss: 0.382, 35232/60000 datapoints
2025-03-06 19:07:24,342 - INFO - training batch 1151, loss: 0.529, 36832/60000 datapoints
2025-03-06 19:07:24,543 - INFO - training batch 1201, loss: 0.411, 38432/60000 datapoints
2025-03-06 19:07:24,748 - INFO - training batch 1251, loss: 0.381, 40032/60000 datapoints
2025-03-06 19:07:24,956 - INFO - training batch 1301, loss: 0.202, 41632/60000 datapoints
2025-03-06 19:07:25,156 - INFO - training batch 1351, loss: 0.172, 43232/60000 datapoints
2025-03-06 19:07:25,352 - INFO - training batch 1401, loss: 0.296, 44832/60000 datapoints
2025-03-06 19:07:25,562 - INFO - training batch 1451, loss: 0.461, 46432/60000 datapoints
2025-03-06 19:07:25,764 - INFO - training batch 1501, loss: 0.160, 48032/60000 datapoints
2025-03-06 19:07:25,965 - INFO - training batch 1551, loss: 0.355, 49632/60000 datapoints
2025-03-06 19:07:26,161 - INFO - training batch 1601, loss: 0.382, 51232/60000 datapoints
2025-03-06 19:07:26,363 - INFO - training batch 1651, loss: 0.312, 52832/60000 datapoints
2025-03-06 19:07:26,563 - INFO - training batch 1701, loss: 0.260, 54432/60000 datapoints
2025-03-06 19:07:26,764 - INFO - training batch 1751, loss: 0.472, 56032/60000 datapoints
2025-03-06 19:07:26,965 - INFO - training batch 1801, loss: 0.411, 57632/60000 datapoints
2025-03-06 19:07:27,165 - INFO - training batch 1851, loss: 0.478, 59232/60000 datapoints
2025-03-06 19:07:27,267 - INFO - validation batch 1, loss: 0.386, 32/10016 datapoints
2025-03-06 19:07:27,425 - INFO - validation batch 51, loss: 0.233, 1632/10016 datapoints
2025-03-06 19:07:27,581 - INFO - validation batch 101, loss: 0.257, 3232/10016 datapoints
2025-03-06 19:07:27,737 - INFO - validation batch 151, loss: 0.385, 4832/10016 datapoints
2025-03-06 19:07:27,888 - INFO - validation batch 201, loss: 0.989, 6432/10016 datapoints
2025-03-06 19:07:28,040 - INFO - validation batch 251, loss: 0.472, 8032/10016 datapoints
2025-03-06 19:07:28,191 - INFO - validation batch 301, loss: 0.380, 9632/10016 datapoints
2025-03-06 19:07:28,226 - INFO - Epoch 205/800 done.
2025-03-06 19:07:28,226 - INFO - Final validation performance:
Loss: 0.443, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 19:07:28,227 - INFO - Beginning epoch 206/800
2025-03-06 19:07:28,233 - INFO - training batch 1, loss: 0.312, 32/60000 datapoints
2025-03-06 19:07:28,444 - INFO - training batch 51, loss: 0.353, 1632/60000 datapoints
2025-03-06 19:07:28,637 - INFO - training batch 101, loss: 0.420, 3232/60000 datapoints
2025-03-06 19:07:28,852 - INFO - training batch 151, loss: 0.357, 4832/60000 datapoints
2025-03-06 19:07:29,059 - INFO - training batch 201, loss: 0.228, 6432/60000 datapoints
2025-03-06 19:07:29,285 - INFO - training batch 251, loss: 0.359, 8032/60000 datapoints
2025-03-06 19:07:29,523 - INFO - training batch 301, loss: 0.377, 9632/60000 datapoints
2025-03-06 19:07:29,748 - INFO - training batch 351, loss: 0.425, 11232/60000 datapoints
2025-03-06 19:07:29,977 - INFO - training batch 401, loss: 0.242, 12832/60000 datapoints
2025-03-06 19:07:30,176 - INFO - training batch 451, loss: 0.257, 14432/60000 datapoints
2025-03-06 19:07:30,400 - INFO - training batch 501, loss: 0.814, 16032/60000 datapoints
2025-03-06 19:07:30,609 - INFO - training batch 551, loss: 0.215, 17632/60000 datapoints
2025-03-06 19:07:30,804 - INFO - training batch 601, loss: 0.349, 19232/60000 datapoints
2025-03-06 19:07:31,009 - INFO - training batch 651, loss: 0.139, 20832/60000 datapoints
2025-03-06 19:07:31,209 - INFO - training batch 701, loss: 0.741, 22432/60000 datapoints
2025-03-06 19:07:31,572 - INFO - training batch 751, loss: 0.368, 24032/60000 datapoints
2025-03-06 19:07:31,999 - INFO - training batch 801, loss: 0.449, 25632/60000 datapoints
2025-03-06 19:07:32,345 - INFO - training batch 851, loss: 0.284, 27232/60000 datapoints
2025-03-06 19:07:32,551 - INFO - training batch 901, loss: 0.309, 28832/60000 datapoints
2025-03-06 19:07:32,783 - INFO - training batch 951, loss: 0.340, 30432/60000 datapoints
2025-03-06 19:07:32,983 - INFO - training batch 1001, loss: 0.386, 32032/60000 datapoints
2025-03-06 19:07:33,185 - INFO - training batch 1051, loss: 0.287, 33632/60000 datapoints
2025-03-06 19:07:33,379 - INFO - training batch 1101, loss: 0.323, 35232/60000 datapoints
2025-03-06 19:07:33,593 - INFO - training batch 1151, loss: 0.352, 36832/60000 datapoints
2025-03-06 19:07:33,796 - INFO - training batch 1201, loss: 0.361, 38432/60000 datapoints
2025-03-06 19:07:33,997 - INFO - training batch 1251, loss: 0.156, 40032/60000 datapoints
2025-03-06 19:07:34,194 - INFO - training batch 1301, loss: 0.276, 41632/60000 datapoints
2025-03-06 19:07:34,395 - INFO - training batch 1351, loss: 0.376, 43232/60000 datapoints
2025-03-06 19:07:34,593 - INFO - training batch 1401, loss: 0.494, 44832/60000 datapoints
2025-03-06 19:07:34,792 - INFO - training batch 1451, loss: 0.260, 46432/60000 datapoints
2025-03-06 19:07:34,999 - INFO - training batch 1501, loss: 0.298, 48032/60000 datapoints
2025-03-06 19:07:35,201 - INFO - training batch 1551, loss: 0.278, 49632/60000 datapoints
2025-03-06 19:07:35,397 - INFO - training batch 1601, loss: 0.437, 51232/60000 datapoints
2025-03-06 19:07:35,597 - INFO - training batch 1651, loss: 0.323, 52832/60000 datapoints
2025-03-06 19:07:35,795 - INFO - training batch 1701, loss: 0.471, 54432/60000 datapoints
2025-03-06 19:07:35,993 - INFO - training batch 1751, loss: 0.329, 56032/60000 datapoints
2025-03-06 19:07:36,193 - INFO - training batch 1801, loss: 0.375, 57632/60000 datapoints
2025-03-06 19:07:36,393 - INFO - training batch 1851, loss: 0.495, 59232/60000 datapoints
2025-03-06 19:07:36,503 - INFO - validation batch 1, loss: 0.394, 32/10016 datapoints
2025-03-06 19:07:36,664 - INFO - validation batch 51, loss: 0.326, 1632/10016 datapoints
2025-03-06 19:07:36,821 - INFO - validation batch 101, loss: 0.351, 3232/10016 datapoints
2025-03-06 19:07:36,982 - INFO - validation batch 151, loss: 0.569, 4832/10016 datapoints
2025-03-06 19:07:37,144 - INFO - validation batch 201, loss: 0.493, 6432/10016 datapoints
2025-03-06 19:07:37,298 - INFO - validation batch 251, loss: 0.224, 8032/10016 datapoints
2025-03-06 19:07:37,458 - INFO - validation batch 301, loss: 0.263, 9632/10016 datapoints
2025-03-06 19:07:37,497 - INFO - Epoch 206/800 done.
2025-03-06 19:07:37,497 - INFO - Final validation performance:
Loss: 0.374, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 19:07:37,498 - INFO - Beginning epoch 207/800
2025-03-06 19:07:37,511 - INFO - training batch 1, loss: 0.450, 32/60000 datapoints
2025-03-06 19:07:37,740 - INFO - training batch 51, loss: 0.245, 1632/60000 datapoints
2025-03-06 19:07:37,932 - INFO - training batch 101, loss: 0.499, 3232/60000 datapoints
2025-03-06 19:07:38,132 - INFO - training batch 151, loss: 0.504, 4832/60000 datapoints
2025-03-06 19:07:38,326 - INFO - training batch 201, loss: 0.348, 6432/60000 datapoints
2025-03-06 19:07:38,525 - INFO - training batch 251, loss: 0.250, 8032/60000 datapoints
2025-03-06 19:07:38,721 - INFO - training batch 301, loss: 0.236, 9632/60000 datapoints
2025-03-06 19:07:38,922 - INFO - training batch 351, loss: 0.595, 11232/60000 datapoints
2025-03-06 19:07:39,125 - INFO - training batch 401, loss: 0.375, 12832/60000 datapoints
2025-03-06 19:07:39,323 - INFO - training batch 451, loss: 0.437, 14432/60000 datapoints
2025-03-06 19:07:39,525 - INFO - training batch 501, loss: 0.158, 16032/60000 datapoints
2025-03-06 19:07:39,728 - INFO - training batch 551, loss: 0.362, 17632/60000 datapoints
2025-03-06 19:07:39,921 - INFO - training batch 601, loss: 0.431, 19232/60000 datapoints
2025-03-06 19:07:40,117 - INFO - training batch 651, loss: 0.289, 20832/60000 datapoints
2025-03-06 19:07:40,312 - INFO - training batch 701, loss: 0.200, 22432/60000 datapoints
2025-03-06 19:07:40,510 - INFO - training batch 751, loss: 0.259, 24032/60000 datapoints
2025-03-06 19:07:40,710 - INFO - training batch 801, loss: 0.332, 25632/60000 datapoints
2025-03-06 19:07:40,906 - INFO - training batch 851, loss: 0.473, 27232/60000 datapoints
2025-03-06 19:07:41,101 - INFO - training batch 901, loss: 0.255, 28832/60000 datapoints
2025-03-06 19:07:41,300 - INFO - training batch 951, loss: 0.240, 30432/60000 datapoints
2025-03-06 19:07:41,500 - INFO - training batch 1001, loss: 0.354, 32032/60000 datapoints
2025-03-06 19:07:41,699 - INFO - training batch 1051, loss: 0.541, 33632/60000 datapoints
2025-03-06 19:07:41,896 - INFO - training batch 1101, loss: 0.465, 35232/60000 datapoints
2025-03-06 19:07:42,092 - INFO - training batch 1151, loss: 0.169, 36832/60000 datapoints
2025-03-06 19:07:42,289 - INFO - training batch 1201, loss: 0.470, 38432/60000 datapoints
2025-03-06 19:07:42,487 - INFO - training batch 1251, loss: 0.477, 40032/60000 datapoints
2025-03-06 19:07:42,694 - INFO - training batch 1301, loss: 0.435, 41632/60000 datapoints
2025-03-06 19:07:42,908 - INFO - training batch 1351, loss: 0.242, 43232/60000 datapoints
2025-03-06 19:07:43,102 - INFO - training batch 1401, loss: 0.186, 44832/60000 datapoints
2025-03-06 19:07:43,331 - INFO - training batch 1451, loss: 0.214, 46432/60000 datapoints
2025-03-06 19:07:43,735 - INFO - training batch 1501, loss: 0.345, 48032/60000 datapoints
2025-03-06 19:07:44,127 - INFO - training batch 1551, loss: 0.446, 49632/60000 datapoints
2025-03-06 19:07:44,418 - INFO - training batch 1601, loss: 0.231, 51232/60000 datapoints
2025-03-06 19:07:44,619 - INFO - training batch 1651, loss: 0.551, 52832/60000 datapoints
2025-03-06 19:07:44,823 - INFO - training batch 1701, loss: 0.449, 54432/60000 datapoints
2025-03-06 19:07:45,022 - INFO - training batch 1751, loss: 0.284, 56032/60000 datapoints
2025-03-06 19:07:45,223 - INFO - training batch 1801, loss: 0.171, 57632/60000 datapoints
2025-03-06 19:07:45,419 - INFO - training batch 1851, loss: 0.460, 59232/60000 datapoints
2025-03-06 19:07:45,524 - INFO - validation batch 1, loss: 0.222, 32/10016 datapoints
2025-03-06 19:07:45,688 - INFO - validation batch 51, loss: 0.349, 1632/10016 datapoints
2025-03-06 19:07:45,845 - INFO - validation batch 101, loss: 0.161, 3232/10016 datapoints
2025-03-06 19:07:46,003 - INFO - validation batch 151, loss: 0.359, 4832/10016 datapoints
2025-03-06 19:07:46,165 - INFO - validation batch 201, loss: 0.329, 6432/10016 datapoints
2025-03-06 19:07:46,325 - INFO - validation batch 251, loss: 0.368, 8032/10016 datapoints
2025-03-06 19:07:46,489 - INFO - validation batch 301, loss: 0.269, 9632/10016 datapoints
2025-03-06 19:07:46,532 - INFO - Epoch 207/800 done.
2025-03-06 19:07:46,532 - INFO - Final validation performance:
Loss: 0.294, top-1 acc: 0.902top-5 acc: 0.902
2025-03-06 19:07:46,533 - INFO - Beginning epoch 208/800
2025-03-06 19:07:46,540 - INFO - training batch 1, loss: 0.356, 32/60000 datapoints
2025-03-06 19:07:46,743 - INFO - training batch 51, loss: 0.288, 1632/60000 datapoints
2025-03-06 19:07:46,941 - INFO - training batch 101, loss: 0.211, 3232/60000 datapoints
2025-03-06 19:07:47,156 - INFO - training batch 151, loss: 0.298, 4832/60000 datapoints
2025-03-06 19:07:47,352 - INFO - training batch 201, loss: 0.667, 6432/60000 datapoints
2025-03-06 19:07:47,549 - INFO - training batch 251, loss: 0.373, 8032/60000 datapoints
2025-03-06 19:07:47,757 - INFO - training batch 301, loss: 0.388, 9632/60000 datapoints
2025-03-06 19:07:47,960 - INFO - training batch 351, loss: 0.645, 11232/60000 datapoints
2025-03-06 19:07:48,160 - INFO - training batch 401, loss: 0.210, 12832/60000 datapoints
2025-03-06 19:07:48,356 - INFO - training batch 451, loss: 0.321, 14432/60000 datapoints
2025-03-06 19:07:48,553 - INFO - training batch 501, loss: 0.245, 16032/60000 datapoints
2025-03-06 19:07:48,832 - INFO - training batch 551, loss: 0.537, 17632/60000 datapoints
2025-03-06 19:07:49,030 - INFO - training batch 601, loss: 0.221, 19232/60000 datapoints
2025-03-06 19:07:49,227 - INFO - training batch 651, loss: 0.323, 20832/60000 datapoints
2025-03-06 19:07:49,421 - INFO - training batch 701, loss: 0.285, 22432/60000 datapoints
2025-03-06 19:07:49,622 - INFO - training batch 751, loss: 0.261, 24032/60000 datapoints
2025-03-06 19:07:49,825 - INFO - training batch 801, loss: 0.394, 25632/60000 datapoints
2025-03-06 19:07:50,020 - INFO - training batch 851, loss: 0.333, 27232/60000 datapoints
2025-03-06 19:07:50,217 - INFO - training batch 901, loss: 0.256, 28832/60000 datapoints
2025-03-06 19:07:50,413 - INFO - training batch 951, loss: 0.252, 30432/60000 datapoints
2025-03-06 19:07:50,610 - INFO - training batch 1001, loss: 0.325, 32032/60000 datapoints
2025-03-06 19:07:50,814 - INFO - training batch 1051, loss: 0.260, 33632/60000 datapoints
2025-03-06 19:07:51,015 - INFO - training batch 1101, loss: 0.568, 35232/60000 datapoints
2025-03-06 19:07:51,220 - INFO - training batch 1151, loss: 0.318, 36832/60000 datapoints
2025-03-06 19:07:51,423 - INFO - training batch 1201, loss: 0.276, 38432/60000 datapoints
2025-03-06 19:07:51,627 - INFO - training batch 1251, loss: 0.427, 40032/60000 datapoints
2025-03-06 19:07:51,831 - INFO - training batch 1301, loss: 0.597, 41632/60000 datapoints
2025-03-06 19:07:52,030 - INFO - training batch 1351, loss: 0.520, 43232/60000 datapoints
2025-03-06 19:07:52,230 - INFO - training batch 1401, loss: 0.148, 44832/60000 datapoints
2025-03-06 19:07:52,429 - INFO - training batch 1451, loss: 0.246, 46432/60000 datapoints
2025-03-06 19:07:52,656 - INFO - training batch 1501, loss: 0.405, 48032/60000 datapoints
2025-03-06 19:07:52,887 - INFO - training batch 1551, loss: 0.283, 49632/60000 datapoints
2025-03-06 19:07:53,089 - INFO - training batch 1601, loss: 0.415, 51232/60000 datapoints
2025-03-06 19:07:53,298 - INFO - training batch 1651, loss: 0.398, 52832/60000 datapoints
2025-03-06 19:07:53,521 - INFO - training batch 1701, loss: 0.539, 54432/60000 datapoints
2025-03-06 19:07:53,725 - INFO - training batch 1751, loss: 1.023, 56032/60000 datapoints
2025-03-06 19:07:53,926 - INFO - training batch 1801, loss: 0.358, 57632/60000 datapoints
2025-03-06 19:07:54,126 - INFO - training batch 1851, loss: 0.360, 59232/60000 datapoints
2025-03-06 19:07:54,230 - INFO - validation batch 1, loss: 0.489, 32/10016 datapoints
2025-03-06 19:07:54,516 - INFO - validation batch 51, loss: 0.468, 1632/10016 datapoints
2025-03-06 19:07:54,858 - INFO - validation batch 101, loss: 0.240, 3232/10016 datapoints
2025-03-06 19:07:55,185 - INFO - validation batch 151, loss: 0.155, 4832/10016 datapoints
2025-03-06 19:07:55,386 - INFO - validation batch 201, loss: 0.248, 6432/10016 datapoints
2025-03-06 19:07:55,550 - INFO - validation batch 251, loss: 0.154, 8032/10016 datapoints
2025-03-06 19:07:55,718 - INFO - validation batch 301, loss: 0.290, 9632/10016 datapoints
2025-03-06 19:07:55,760 - INFO - Epoch 208/800 done.
2025-03-06 19:07:55,760 - INFO - Final validation performance:
Loss: 0.292, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 19:07:55,763 - INFO - Beginning epoch 209/800
2025-03-06 19:07:55,772 - INFO - training batch 1, loss: 0.475, 32/60000 datapoints
2025-03-06 19:07:56,000 - INFO - training batch 51, loss: 0.591, 1632/60000 datapoints
2025-03-06 19:07:56,217 - INFO - training batch 101, loss: 0.309, 3232/60000 datapoints
2025-03-06 19:07:56,421 - INFO - training batch 151, loss: 0.230, 4832/60000 datapoints
2025-03-06 19:07:56,627 - INFO - training batch 201, loss: 0.240, 6432/60000 datapoints
2025-03-06 19:07:56,832 - INFO - training batch 251, loss: 0.362, 8032/60000 datapoints
2025-03-06 19:07:57,033 - INFO - training batch 301, loss: 0.360, 9632/60000 datapoints
2025-03-06 19:07:57,234 - INFO - training batch 351, loss: 0.367, 11232/60000 datapoints
2025-03-06 19:07:57,437 - INFO - training batch 401, loss: 0.625, 12832/60000 datapoints
2025-03-06 19:07:57,640 - INFO - training batch 451, loss: 0.242, 14432/60000 datapoints
2025-03-06 19:07:57,839 - INFO - training batch 501, loss: 0.339, 16032/60000 datapoints
2025-03-06 19:07:58,042 - INFO - training batch 551, loss: 0.183, 17632/60000 datapoints
2025-03-06 19:07:58,238 - INFO - training batch 601, loss: 0.697, 19232/60000 datapoints
2025-03-06 19:07:58,440 - INFO - training batch 651, loss: 0.435, 20832/60000 datapoints
2025-03-06 19:07:58,639 - INFO - training batch 701, loss: 0.277, 22432/60000 datapoints
2025-03-06 19:07:58,835 - INFO - training batch 751, loss: 0.659, 24032/60000 datapoints
2025-03-06 19:07:59,037 - INFO - training batch 801, loss: 0.500, 25632/60000 datapoints
2025-03-06 19:07:59,237 - INFO - training batch 851, loss: 0.434, 27232/60000 datapoints
2025-03-06 19:07:59,434 - INFO - training batch 901, loss: 0.290, 28832/60000 datapoints
2025-03-06 19:07:59,641 - INFO - training batch 951, loss: 0.468, 30432/60000 datapoints
2025-03-06 19:07:59,837 - INFO - training batch 1001, loss: 0.299, 32032/60000 datapoints
2025-03-06 19:08:00,033 - INFO - training batch 1051, loss: 0.344, 33632/60000 datapoints
2025-03-06 19:08:00,230 - INFO - training batch 1101, loss: 0.485, 35232/60000 datapoints
2025-03-06 19:08:00,426 - INFO - training batch 1151, loss: 0.406, 36832/60000 datapoints
2025-03-06 19:08:00,624 - INFO - training batch 1201, loss: 0.520, 38432/60000 datapoints
2025-03-06 19:08:00,821 - INFO - training batch 1251, loss: 0.320, 40032/60000 datapoints
2025-03-06 19:08:01,016 - INFO - training batch 1301, loss: 0.403, 41632/60000 datapoints
2025-03-06 19:08:01,223 - INFO - training batch 1351, loss: 0.200, 43232/60000 datapoints
2025-03-06 19:08:01,433 - INFO - training batch 1401, loss: 0.349, 44832/60000 datapoints
2025-03-06 19:08:01,638 - INFO - training batch 1451, loss: 0.536, 46432/60000 datapoints
2025-03-06 19:08:01,833 - INFO - training batch 1501, loss: 0.532, 48032/60000 datapoints
2025-03-06 19:08:02,030 - INFO - training batch 1551, loss: 0.405, 49632/60000 datapoints
2025-03-06 19:08:02,227 - INFO - training batch 1601, loss: 0.896, 51232/60000 datapoints
2025-03-06 19:08:02,584 - INFO - training batch 1651, loss: 0.328, 52832/60000 datapoints
2025-03-06 19:08:03,016 - INFO - training batch 1701, loss: 0.492, 54432/60000 datapoints
2025-03-06 19:08:03,379 - INFO - training batch 1751, loss: 0.387, 56032/60000 datapoints
2025-03-06 19:08:03,604 - INFO - training batch 1801, loss: 0.477, 57632/60000 datapoints
2025-03-06 19:08:03,855 - INFO - training batch 1851, loss: 0.486, 59232/60000 datapoints
2025-03-06 19:08:03,989 - INFO - validation batch 1, loss: 0.580, 32/10016 datapoints
2025-03-06 19:08:04,147 - INFO - validation batch 51, loss: 0.283, 1632/10016 datapoints
2025-03-06 19:08:04,302 - INFO - validation batch 101, loss: 0.520, 3232/10016 datapoints
2025-03-06 19:08:04,456 - INFO - validation batch 151, loss: 0.358, 4832/10016 datapoints
2025-03-06 19:08:04,614 - INFO - validation batch 201, loss: 0.218, 6432/10016 datapoints
2025-03-06 19:08:04,769 - INFO - validation batch 251, loss: 0.550, 8032/10016 datapoints
2025-03-06 19:08:04,934 - INFO - validation batch 301, loss: 0.311, 9632/10016 datapoints
2025-03-06 19:08:04,972 - INFO - Epoch 209/800 done.
2025-03-06 19:08:04,972 - INFO - Final validation performance:
Loss: 0.403, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 19:08:04,973 - INFO - Beginning epoch 210/800
2025-03-06 19:08:04,979 - INFO - training batch 1, loss: 0.267, 32/60000 datapoints
2025-03-06 19:08:05,177 - INFO - training batch 51, loss: 0.423, 1632/60000 datapoints
2025-03-06 19:08:05,376 - INFO - training batch 101, loss: 0.309, 3232/60000 datapoints
2025-03-06 19:08:05,581 - INFO - training batch 151, loss: 0.256, 4832/60000 datapoints
2025-03-06 19:08:05,782 - INFO - training batch 201, loss: 0.571, 6432/60000 datapoints
2025-03-06 19:08:05,985 - INFO - training batch 251, loss: 0.631, 8032/60000 datapoints
2025-03-06 19:08:06,189 - INFO - training batch 301, loss: 0.394, 9632/60000 datapoints
2025-03-06 19:08:06,390 - INFO - training batch 351, loss: 0.338, 11232/60000 datapoints
2025-03-06 19:08:06,592 - INFO - training batch 401, loss: 0.369, 12832/60000 datapoints
2025-03-06 19:08:06,792 - INFO - training batch 451, loss: 0.325, 14432/60000 datapoints
2025-03-06 19:08:06,992 - INFO - training batch 501, loss: 0.143, 16032/60000 datapoints
2025-03-06 19:08:07,191 - INFO - training batch 551, loss: 0.258, 17632/60000 datapoints
2025-03-06 19:08:07,395 - INFO - training batch 601, loss: 0.260, 19232/60000 datapoints
2025-03-06 19:08:07,597 - INFO - training batch 651, loss: 0.496, 20832/60000 datapoints
2025-03-06 19:08:07,804 - INFO - training batch 701, loss: 0.266, 22432/60000 datapoints
2025-03-06 19:08:08,006 - INFO - training batch 751, loss: 0.706, 24032/60000 datapoints
2025-03-06 19:08:08,206 - INFO - training batch 801, loss: 0.347, 25632/60000 datapoints
2025-03-06 19:08:08,408 - INFO - training batch 851, loss: 0.410, 27232/60000 datapoints
2025-03-06 19:08:08,612 - INFO - training batch 901, loss: 0.334, 28832/60000 datapoints
2025-03-06 19:08:08,818 - INFO - training batch 951, loss: 0.272, 30432/60000 datapoints
2025-03-06 19:08:09,032 - INFO - training batch 1001, loss: 0.403, 32032/60000 datapoints
2025-03-06 19:08:09,240 - INFO - training batch 1051, loss: 0.237, 33632/60000 datapoints
2025-03-06 19:08:09,454 - INFO - training batch 1101, loss: 0.328, 35232/60000 datapoints
2025-03-06 19:08:09,662 - INFO - training batch 1151, loss: 0.229, 36832/60000 datapoints
2025-03-06 19:08:09,879 - INFO - training batch 1201, loss: 0.724, 38432/60000 datapoints
2025-03-06 19:08:10,091 - INFO - training batch 1251, loss: 0.525, 40032/60000 datapoints
2025-03-06 19:08:10,305 - INFO - training batch 1301, loss: 0.348, 41632/60000 datapoints
2025-03-06 19:08:10,520 - INFO - training batch 1351, loss: 0.400, 43232/60000 datapoints
2025-03-06 19:08:10,734 - INFO - training batch 1401, loss: 0.358, 44832/60000 datapoints
2025-03-06 19:08:10,952 - INFO - training batch 1451, loss: 0.198, 46432/60000 datapoints
2025-03-06 19:08:11,171 - INFO - training batch 1501, loss: 0.440, 48032/60000 datapoints
2025-03-06 19:08:11,391 - INFO - training batch 1551, loss: 0.363, 49632/60000 datapoints
2025-03-06 19:08:11,611 - INFO - training batch 1601, loss: 0.537, 51232/60000 datapoints
2025-03-06 19:08:11,826 - INFO - training batch 1651, loss: 0.444, 52832/60000 datapoints
2025-03-06 19:08:12,042 - INFO - training batch 1701, loss: 0.317, 54432/60000 datapoints
2025-03-06 19:08:12,257 - INFO - training batch 1751, loss: 0.215, 56032/60000 datapoints
2025-03-06 19:08:12,475 - INFO - training batch 1801, loss: 0.303, 57632/60000 datapoints
2025-03-06 19:08:12,695 - INFO - training batch 1851, loss: 0.236, 59232/60000 datapoints
2025-03-06 19:08:12,815 - INFO - validation batch 1, loss: 0.598, 32/10016 datapoints
2025-03-06 19:08:12,992 - INFO - validation batch 51, loss: 0.246, 1632/10016 datapoints
2025-03-06 19:08:13,189 - INFO - validation batch 101, loss: 0.090, 3232/10016 datapoints
2025-03-06 19:08:13,368 - INFO - validation batch 151, loss: 0.338, 4832/10016 datapoints
2025-03-06 19:08:13,545 - INFO - validation batch 201, loss: 0.431, 6432/10016 datapoints
2025-03-06 19:08:13,732 - INFO - validation batch 251, loss: 0.328, 8032/10016 datapoints
2025-03-06 19:08:13,908 - INFO - validation batch 301, loss: 0.432, 9632/10016 datapoints
2025-03-06 19:08:13,950 - INFO - Epoch 210/800 done.
2025-03-06 19:08:13,966 - INFO - Final validation performance:
Loss: 0.352, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 19:08:13,967 - INFO - Beginning epoch 211/800
2025-03-06 19:08:14,000 - INFO - training batch 1, loss: 0.505, 32/60000 datapoints
2025-03-06 19:08:14,249 - INFO - training batch 51, loss: 0.468, 1632/60000 datapoints
2025-03-06 19:08:14,462 - INFO - training batch 101, loss: 0.298, 3232/60000 datapoints
2025-03-06 19:08:14,680 - INFO - training batch 151, loss: 0.504, 4832/60000 datapoints
2025-03-06 19:08:14,900 - INFO - training batch 201, loss: 0.454, 6432/60000 datapoints
2025-03-06 19:08:15,111 - INFO - training batch 251, loss: 0.514, 8032/60000 datapoints
2025-03-06 19:08:15,320 - INFO - training batch 301, loss: 0.199, 9632/60000 datapoints
2025-03-06 19:08:15,530 - INFO - training batch 351, loss: 0.349, 11232/60000 datapoints
2025-03-06 19:08:15,736 - INFO - training batch 401, loss: 0.682, 12832/60000 datapoints
2025-03-06 19:08:15,940 - INFO - training batch 451, loss: 0.309, 14432/60000 datapoints
2025-03-06 19:08:16,140 - INFO - training batch 501, loss: 0.514, 16032/60000 datapoints
2025-03-06 19:08:16,345 - INFO - training batch 551, loss: 0.406, 17632/60000 datapoints
2025-03-06 19:08:16,553 - INFO - training batch 601, loss: 0.389, 19232/60000 datapoints
2025-03-06 19:08:16,751 - INFO - training batch 651, loss: 0.324, 20832/60000 datapoints
2025-03-06 19:08:16,950 - INFO - training batch 701, loss: 0.228, 22432/60000 datapoints
2025-03-06 19:08:17,144 - INFO - training batch 751, loss: 0.331, 24032/60000 datapoints
2025-03-06 19:08:17,342 - INFO - training batch 801, loss: 0.519, 25632/60000 datapoints
2025-03-06 19:08:17,542 - INFO - training batch 851, loss: 0.231, 27232/60000 datapoints
2025-03-06 19:08:17,745 - INFO - training batch 901, loss: 0.226, 28832/60000 datapoints
2025-03-06 19:08:17,948 - INFO - training batch 951, loss: 0.337, 30432/60000 datapoints
2025-03-06 19:08:18,146 - INFO - training batch 1001, loss: 0.224, 32032/60000 datapoints
2025-03-06 19:08:18,339 - INFO - training batch 1051, loss: 0.545, 33632/60000 datapoints
2025-03-06 19:08:18,538 - INFO - training batch 1101, loss: 0.454, 35232/60000 datapoints
2025-03-06 19:08:18,736 - INFO - training batch 1151, loss: 0.495, 36832/60000 datapoints
2025-03-06 19:08:18,933 - INFO - training batch 1201, loss: 0.351, 38432/60000 datapoints
2025-03-06 19:08:19,129 - INFO - training batch 1251, loss: 0.441, 40032/60000 datapoints
2025-03-06 19:08:19,335 - INFO - training batch 1301, loss: 0.229, 41632/60000 datapoints
2025-03-06 19:08:19,544 - INFO - training batch 1351, loss: 0.198, 43232/60000 datapoints
2025-03-06 19:08:19,753 - INFO - training batch 1401, loss: 0.587, 44832/60000 datapoints
2025-03-06 19:08:19,964 - INFO - training batch 1451, loss: 0.318, 46432/60000 datapoints
2025-03-06 19:08:20,164 - INFO - training batch 1501, loss: 0.279, 48032/60000 datapoints
2025-03-06 19:08:20,365 - INFO - training batch 1551, loss: 0.269, 49632/60000 datapoints
2025-03-06 19:08:20,572 - INFO - training batch 1601, loss: 0.359, 51232/60000 datapoints
2025-03-06 19:08:20,791 - INFO - training batch 1651, loss: 0.491, 52832/60000 datapoints
2025-03-06 19:08:21,004 - INFO - training batch 1701, loss: 0.314, 54432/60000 datapoints
2025-03-06 19:08:21,214 - INFO - training batch 1751, loss: 0.427, 56032/60000 datapoints
2025-03-06 19:08:21,426 - INFO - training batch 1801, loss: 0.523, 57632/60000 datapoints
2025-03-06 19:08:21,638 - INFO - training batch 1851, loss: 0.470, 59232/60000 datapoints
2025-03-06 19:08:21,747 - INFO - validation batch 1, loss: 0.184, 32/10016 datapoints
2025-03-06 19:08:21,915 - INFO - validation batch 51, loss: 0.464, 1632/10016 datapoints
2025-03-06 19:08:22,081 - INFO - validation batch 101, loss: 0.277, 3232/10016 datapoints
2025-03-06 19:08:22,243 - INFO - validation batch 151, loss: 0.242, 4832/10016 datapoints
2025-03-06 19:08:22,409 - INFO - validation batch 201, loss: 0.302, 6432/10016 datapoints
2025-03-06 19:08:22,576 - INFO - validation batch 251, loss: 0.251, 8032/10016 datapoints
2025-03-06 19:08:22,739 - INFO - validation batch 301, loss: 0.284, 9632/10016 datapoints
2025-03-06 19:08:22,784 - INFO - Epoch 211/800 done.
2025-03-06 19:08:22,784 - INFO - Final validation performance:
Loss: 0.286, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 19:08:22,785 - INFO - Beginning epoch 212/800
2025-03-06 19:08:22,792 - INFO - training batch 1, loss: 0.409, 32/60000 datapoints
2025-03-06 19:08:23,002 - INFO - training batch 51, loss: 0.298, 1632/60000 datapoints
2025-03-06 19:08:23,226 - INFO - training batch 101, loss: 0.278, 3232/60000 datapoints
2025-03-06 19:08:23,461 - INFO - training batch 151, loss: 0.387, 4832/60000 datapoints
2025-03-06 19:08:23,668 - INFO - training batch 201, loss: 0.344, 6432/60000 datapoints
2025-03-06 19:08:23,886 - INFO - training batch 251, loss: 0.405, 8032/60000 datapoints
2025-03-06 19:08:24,098 - INFO - training batch 301, loss: 0.390, 9632/60000 datapoints
2025-03-06 19:08:24,310 - INFO - training batch 351, loss: 0.344, 11232/60000 datapoints
2025-03-06 19:08:24,516 - INFO - training batch 401, loss: 0.319, 12832/60000 datapoints
2025-03-06 19:08:24,727 - INFO - training batch 451, loss: 0.421, 14432/60000 datapoints
2025-03-06 19:08:24,943 - INFO - training batch 501, loss: 0.354, 16032/60000 datapoints
2025-03-06 19:08:25,145 - INFO - training batch 551, loss: 0.529, 17632/60000 datapoints
2025-03-06 19:08:25,359 - INFO - training batch 601, loss: 0.268, 19232/60000 datapoints
2025-03-06 19:08:25,573 - INFO - training batch 651, loss: 0.377, 20832/60000 datapoints
2025-03-06 19:08:25,785 - INFO - training batch 701, loss: 0.433, 22432/60000 datapoints
2025-03-06 19:08:25,996 - INFO - training batch 751, loss: 0.252, 24032/60000 datapoints
2025-03-06 19:08:26,208 - INFO - training batch 801, loss: 0.637, 25632/60000 datapoints
2025-03-06 19:08:26,428 - INFO - training batch 851, loss: 0.272, 27232/60000 datapoints
2025-03-06 19:08:26,641 - INFO - training batch 901, loss: 0.215, 28832/60000 datapoints
2025-03-06 19:08:26,849 - INFO - training batch 951, loss: 0.428, 30432/60000 datapoints
2025-03-06 19:08:27,065 - INFO - training batch 1001, loss: 0.218, 32032/60000 datapoints
2025-03-06 19:08:27,283 - INFO - training batch 1051, loss: 0.282, 33632/60000 datapoints
2025-03-06 19:08:27,502 - INFO - training batch 1101, loss: 0.421, 35232/60000 datapoints
2025-03-06 19:08:27,735 - INFO - training batch 1151, loss: 0.416, 36832/60000 datapoints
2025-03-06 19:08:28,098 - INFO - training batch 1201, loss: 0.468, 38432/60000 datapoints
2025-03-06 19:08:28,296 - INFO - training batch 1251, loss: 0.113, 40032/60000 datapoints
2025-03-06 19:08:28,500 - INFO - training batch 1301, loss: 0.522, 41632/60000 datapoints
2025-03-06 19:08:28,708 - INFO - training batch 1351, loss: 0.482, 43232/60000 datapoints
2025-03-06 19:08:28,914 - INFO - training batch 1401, loss: 0.316, 44832/60000 datapoints
2025-03-06 19:08:29,120 - INFO - training batch 1451, loss: 0.227, 46432/60000 datapoints
2025-03-06 19:08:29,327 - INFO - training batch 1501, loss: 0.296, 48032/60000 datapoints
2025-03-06 19:08:29,534 - INFO - training batch 1551, loss: 0.379, 49632/60000 datapoints
2025-03-06 19:08:29,739 - INFO - training batch 1601, loss: 0.412, 51232/60000 datapoints
2025-03-06 19:08:29,953 - INFO - training batch 1651, loss: 0.618, 52832/60000 datapoints
2025-03-06 19:08:30,161 - INFO - training batch 1701, loss: 0.421, 54432/60000 datapoints
2025-03-06 19:08:30,369 - INFO - training batch 1751, loss: 0.307, 56032/60000 datapoints
2025-03-06 19:08:30,624 - INFO - training batch 1801, loss: 0.262, 57632/60000 datapoints
2025-03-06 19:08:30,865 - INFO - training batch 1851, loss: 0.464, 59232/60000 datapoints
2025-03-06 19:08:30,993 - INFO - validation batch 1, loss: 0.454, 32/10016 datapoints
2025-03-06 19:08:31,176 - INFO - validation batch 51, loss: 0.596, 1632/10016 datapoints
2025-03-06 19:08:31,360 - INFO - validation batch 101, loss: 0.249, 3232/10016 datapoints
2025-03-06 19:08:31,620 - INFO - validation batch 151, loss: 0.189, 4832/10016 datapoints
2025-03-06 19:08:31,882 - INFO - validation batch 201, loss: 0.335, 6432/10016 datapoints
2025-03-06 19:08:32,071 - INFO - validation batch 251, loss: 0.272, 8032/10016 datapoints
2025-03-06 19:08:32,274 - INFO - validation batch 301, loss: 0.159, 9632/10016 datapoints
2025-03-06 19:08:32,342 - INFO - Epoch 212/800 done.
2025-03-06 19:08:32,342 - INFO - Final validation performance:
Loss: 0.322, top-1 acc: 0.903top-5 acc: 0.903
2025-03-06 19:08:32,343 - INFO - Beginning epoch 213/800
2025-03-06 19:08:32,351 - INFO - training batch 1, loss: 0.797, 32/60000 datapoints
2025-03-06 19:08:32,662 - INFO - training batch 51, loss: 0.527, 1632/60000 datapoints
2025-03-06 19:08:32,943 - INFO - training batch 101, loss: 0.197, 3232/60000 datapoints
2025-03-06 19:08:33,721 - INFO - training batch 151, loss: 0.261, 4832/60000 datapoints
2025-03-06 19:08:34,121 - INFO - training batch 201, loss: 0.351, 6432/60000 datapoints
2025-03-06 19:08:34,452 - INFO - training batch 251, loss: 0.431, 8032/60000 datapoints
2025-03-06 19:08:34,779 - INFO - training batch 301, loss: 0.291, 9632/60000 datapoints
2025-03-06 19:08:35,062 - INFO - training batch 351, loss: 0.175, 11232/60000 datapoints
2025-03-06 19:08:35,455 - INFO - training batch 401, loss: 0.325, 12832/60000 datapoints
2025-03-06 19:08:35,756 - INFO - training batch 451, loss: 0.302, 14432/60000 datapoints
2025-03-06 19:08:36,083 - INFO - training batch 501, loss: 0.523, 16032/60000 datapoints
2025-03-06 19:08:36,375 - INFO - training batch 551, loss: 0.202, 17632/60000 datapoints
2025-03-06 19:08:36,663 - INFO - training batch 601, loss: 0.213, 19232/60000 datapoints
2025-03-06 19:08:36,962 - INFO - training batch 651, loss: 0.298, 20832/60000 datapoints
2025-03-06 19:08:37,266 - INFO - training batch 701, loss: 0.655, 22432/60000 datapoints
2025-03-06 19:08:37,568 - INFO - training batch 751, loss: 0.316, 24032/60000 datapoints
2025-03-06 19:08:37,901 - INFO - training batch 801, loss: 0.320, 25632/60000 datapoints
2025-03-06 19:08:38,196 - INFO - training batch 851, loss: 0.305, 27232/60000 datapoints
2025-03-06 19:08:38,483 - INFO - training batch 901, loss: 0.443, 28832/60000 datapoints
2025-03-06 19:08:38,769 - INFO - training batch 951, loss: 0.391, 30432/60000 datapoints
2025-03-06 19:08:39,041 - INFO - training batch 1001, loss: 0.405, 32032/60000 datapoints
2025-03-06 19:08:39,277 - INFO - training batch 1051, loss: 0.630, 33632/60000 datapoints
2025-03-06 19:08:39,546 - INFO - training batch 1101, loss: 0.164, 35232/60000 datapoints
2025-03-06 19:08:39,820 - INFO - training batch 1151, loss: 0.394, 36832/60000 datapoints
2025-03-06 19:08:40,098 - INFO - training batch 1201, loss: 0.271, 38432/60000 datapoints
2025-03-06 19:08:40,360 - INFO - training batch 1251, loss: 0.478, 40032/60000 datapoints
2025-03-06 19:08:40,635 - INFO - training batch 1301, loss: 0.358, 41632/60000 datapoints
2025-03-06 19:08:40,898 - INFO - training batch 1351, loss: 0.234, 43232/60000 datapoints
2025-03-06 19:08:41,148 - INFO - training batch 1401, loss: 0.347, 44832/60000 datapoints
2025-03-06 19:08:41,367 - INFO - training batch 1451, loss: 0.337, 46432/60000 datapoints
2025-03-06 19:08:41,626 - INFO - training batch 1501, loss: 0.160, 48032/60000 datapoints
2025-03-06 19:08:41,931 - INFO - training batch 1551, loss: 0.179, 49632/60000 datapoints
2025-03-06 19:08:42,195 - INFO - training batch 1601, loss: 0.375, 51232/60000 datapoints
2025-03-06 19:08:42,437 - INFO - training batch 1651, loss: 0.422, 52832/60000 datapoints
2025-03-06 19:08:42,682 - INFO - training batch 1701, loss: 0.275, 54432/60000 datapoints
2025-03-06 19:08:42,932 - INFO - training batch 1751, loss: 0.416, 56032/60000 datapoints
2025-03-06 19:08:43,173 - INFO - training batch 1801, loss: 0.300, 57632/60000 datapoints
2025-03-06 19:08:43,412 - INFO - training batch 1851, loss: 0.518, 59232/60000 datapoints
2025-03-06 19:08:43,554 - INFO - validation batch 1, loss: 0.504, 32/10016 datapoints
2025-03-06 19:08:43,798 - INFO - validation batch 51, loss: 0.402, 1632/10016 datapoints
2025-03-06 19:08:43,991 - INFO - validation batch 101, loss: 0.318, 3232/10016 datapoints
2025-03-06 19:08:44,188 - INFO - validation batch 151, loss: 0.575, 4832/10016 datapoints
2025-03-06 19:08:44,368 - INFO - validation batch 201, loss: 0.508, 6432/10016 datapoints
2025-03-06 19:08:44,542 - INFO - validation batch 251, loss: 0.381, 8032/10016 datapoints
2025-03-06 19:08:44,715 - INFO - validation batch 301, loss: 0.530, 9632/10016 datapoints
2025-03-06 19:08:44,759 - INFO - Epoch 213/800 done.
2025-03-06 19:08:44,759 - INFO - Final validation performance:
Loss: 0.460, top-1 acc: 0.904top-5 acc: 0.904
2025-03-06 19:08:44,760 - INFO - Beginning epoch 214/800
2025-03-06 19:08:44,767 - INFO - training batch 1, loss: 0.474, 32/60000 datapoints
2025-03-06 19:08:45,003 - INFO - training batch 51, loss: 0.325, 1632/60000 datapoints
2025-03-06 19:08:45,218 - INFO - training batch 101, loss: 0.272, 3232/60000 datapoints
2025-03-06 19:08:45,441 - INFO - training batch 151, loss: 0.315, 4832/60000 datapoints
2025-03-06 19:08:45,679 - INFO - training batch 201, loss: 0.300, 6432/60000 datapoints
2025-03-06 19:08:45,904 - INFO - training batch 251, loss: 0.306, 8032/60000 datapoints
2025-03-06 19:08:46,133 - INFO - training batch 301, loss: 0.267, 9632/60000 datapoints
2025-03-06 19:08:46,343 - INFO - training batch 351, loss: 0.431, 11232/60000 datapoints
2025-03-06 19:08:46,555 - INFO - training batch 401, loss: 0.480, 12832/60000 datapoints
2025-03-06 19:08:46,769 - INFO - training batch 451, loss: 0.285, 14432/60000 datapoints
2025-03-06 19:08:46,977 - INFO - training batch 501, loss: 0.198, 16032/60000 datapoints
2025-03-06 19:08:47,183 - INFO - training batch 551, loss: 0.184, 17632/60000 datapoints
2025-03-06 19:08:47,391 - INFO - training batch 601, loss: 0.243, 19232/60000 datapoints
2025-03-06 19:08:47,616 - INFO - training batch 651, loss: 0.234, 20832/60000 datapoints
2025-03-06 19:08:47,851 - INFO - training batch 701, loss: 0.378, 22432/60000 datapoints
2025-03-06 19:08:48,093 - INFO - training batch 751, loss: 0.213, 24032/60000 datapoints
2025-03-06 19:08:48,320 - INFO - training batch 801, loss: 0.520, 25632/60000 datapoints
2025-03-06 19:08:48,562 - INFO - training batch 851, loss: 0.382, 27232/60000 datapoints
2025-03-06 19:08:48,805 - INFO - training batch 901, loss: 0.282, 28832/60000 datapoints
2025-03-06 19:08:49,041 - INFO - training batch 951, loss: 0.686, 30432/60000 datapoints
2025-03-06 19:08:49,276 - INFO - training batch 1001, loss: 0.194, 32032/60000 datapoints
2025-03-06 19:08:49,522 - INFO - training batch 1051, loss: 0.294, 33632/60000 datapoints
2025-03-06 19:08:49,762 - INFO - training batch 1101, loss: 0.346, 35232/60000 datapoints
2025-03-06 19:08:50,012 - INFO - training batch 1151, loss: 0.384, 36832/60000 datapoints
2025-03-06 19:08:50,260 - INFO - training batch 1201, loss: 0.512, 38432/60000 datapoints
2025-03-06 19:08:50,504 - INFO - training batch 1251, loss: 0.303, 40032/60000 datapoints
2025-03-06 19:08:50,749 - INFO - training batch 1301, loss: 0.388, 41632/60000 datapoints
2025-03-06 19:08:50,995 - INFO - training batch 1351, loss: 0.330, 43232/60000 datapoints
2025-03-06 19:08:51,238 - INFO - training batch 1401, loss: 0.336, 44832/60000 datapoints
2025-03-06 19:08:51,499 - INFO - training batch 1451, loss: 0.301, 46432/60000 datapoints
2025-03-06 19:08:51,757 - INFO - training batch 1501, loss: 0.413, 48032/60000 datapoints
2025-03-06 19:08:52,015 - INFO - training batch 1551, loss: 0.594, 49632/60000 datapoints
2025-03-06 19:08:52,267 - INFO - training batch 1601, loss: 0.429, 51232/60000 datapoints
2025-03-06 19:08:52,516 - INFO - training batch 1651, loss: 0.274, 52832/60000 datapoints
2025-03-06 19:08:52,799 - INFO - training batch 1701, loss: 0.298, 54432/60000 datapoints
2025-03-06 19:08:53,196 - INFO - training batch 1751, loss: 0.683, 56032/60000 datapoints
2025-03-06 19:08:53,546 - INFO - training batch 1801, loss: 0.186, 57632/60000 datapoints
2025-03-06 19:08:53,918 - INFO - training batch 1851, loss: 0.312, 59232/60000 datapoints
2025-03-06 19:08:54,193 - INFO - validation batch 1, loss: 0.305, 32/10016 datapoints
2025-03-06 19:08:54,453 - INFO - validation batch 51, loss: 0.487, 1632/10016 datapoints
2025-03-06 19:08:54,658 - INFO - validation batch 101, loss: 0.510, 3232/10016 datapoints
2025-03-06 19:08:54,880 - INFO - validation batch 151, loss: 0.412, 4832/10016 datapoints
2025-03-06 19:08:55,214 - INFO - validation batch 201, loss: 0.221, 6432/10016 datapoints
2025-03-06 19:08:55,429 - INFO - validation batch 251, loss: 0.410, 8032/10016 datapoints
2025-03-06 19:08:55,743 - INFO - validation batch 301, loss: 0.228, 9632/10016 datapoints
2025-03-06 19:08:55,823 - INFO - Epoch 214/800 done.
2025-03-06 19:08:55,823 - INFO - Final validation performance:
Loss: 0.367, top-1 acc: 0.904top-5 acc: 0.904
2025-03-06 19:08:55,824 - INFO - Beginning epoch 215/800
2025-03-06 19:08:55,834 - INFO - training batch 1, loss: 0.382, 32/60000 datapoints
2025-03-06 19:08:56,440 - INFO - training batch 51, loss: 0.611, 1632/60000 datapoints
2025-03-06 19:08:56,795 - INFO - training batch 101, loss: 0.170, 3232/60000 datapoints
2025-03-06 19:08:57,075 - INFO - training batch 151, loss: 0.330, 4832/60000 datapoints
2025-03-06 19:08:57,329 - INFO - training batch 201, loss: 0.104, 6432/60000 datapoints
2025-03-06 19:08:57,581 - INFO - training batch 251, loss: 0.359, 8032/60000 datapoints
2025-03-06 19:08:57,832 - INFO - training batch 301, loss: 0.518, 9632/60000 datapoints
2025-03-06 19:08:58,079 - INFO - training batch 351, loss: 0.395, 11232/60000 datapoints
2025-03-06 19:08:58,331 - INFO - training batch 401, loss: 0.404, 12832/60000 datapoints
2025-03-06 19:08:58,581 - INFO - training batch 451, loss: 0.229, 14432/60000 datapoints
2025-03-06 19:08:58,834 - INFO - training batch 501, loss: 0.393, 16032/60000 datapoints
2025-03-06 19:08:59,074 - INFO - training batch 551, loss: 0.351, 17632/60000 datapoints
2025-03-06 19:08:59,313 - INFO - training batch 601, loss: 0.439, 19232/60000 datapoints
2025-03-06 19:08:59,548 - INFO - training batch 651, loss: 0.521, 20832/60000 datapoints
2025-03-06 19:08:59,781 - INFO - training batch 701, loss: 0.445, 22432/60000 datapoints
2025-03-06 19:09:00,032 - INFO - training batch 751, loss: 0.533, 24032/60000 datapoints
2025-03-06 19:09:00,294 - INFO - training batch 801, loss: 0.294, 25632/60000 datapoints
2025-03-06 19:09:00,544 - INFO - training batch 851, loss: 0.426, 27232/60000 datapoints
2025-03-06 19:09:00,810 - INFO - training batch 901, loss: 0.216, 28832/60000 datapoints
2025-03-06 19:09:01,053 - INFO - training batch 951, loss: 0.576, 30432/60000 datapoints
2025-03-06 19:09:01,276 - INFO - training batch 1001, loss: 0.562, 32032/60000 datapoints
2025-03-06 19:09:01,517 - INFO - training batch 1051, loss: 0.369, 33632/60000 datapoints
2025-03-06 19:09:01,766 - INFO - training batch 1101, loss: 0.380, 35232/60000 datapoints
2025-03-06 19:09:02,006 - INFO - training batch 1151, loss: 0.212, 36832/60000 datapoints
2025-03-06 19:09:02,257 - INFO - training batch 1201, loss: 0.291, 38432/60000 datapoints
2025-03-06 19:09:02,496 - INFO - training batch 1251, loss: 0.329, 40032/60000 datapoints
2025-03-06 19:09:02,747 - INFO - training batch 1301, loss: 0.377, 41632/60000 datapoints
2025-03-06 19:09:03,005 - INFO - training batch 1351, loss: 0.429, 43232/60000 datapoints
2025-03-06 19:09:03,267 - INFO - training batch 1401, loss: 0.324, 44832/60000 datapoints
2025-03-06 19:09:03,511 - INFO - training batch 1451, loss: 0.565, 46432/60000 datapoints
2025-03-06 19:09:03,767 - INFO - training batch 1501, loss: 0.444, 48032/60000 datapoints
2025-03-06 19:09:04,039 - INFO - training batch 1551, loss: 0.225, 49632/60000 datapoints
2025-03-06 19:09:04,302 - INFO - training batch 1601, loss: 0.145, 51232/60000 datapoints
2025-03-06 19:09:04,549 - INFO - training batch 1651, loss: 0.422, 52832/60000 datapoints
2025-03-06 19:09:04,796 - INFO - training batch 1701, loss: 0.176, 54432/60000 datapoints
2025-03-06 19:09:05,023 - INFO - training batch 1751, loss: 0.191, 56032/60000 datapoints
2025-03-06 19:09:05,312 - INFO - training batch 1801, loss: 0.596, 57632/60000 datapoints
2025-03-06 19:09:05,592 - INFO - training batch 1851, loss: 0.248, 59232/60000 datapoints
2025-03-06 19:09:05,764 - INFO - validation batch 1, loss: 0.276, 32/10016 datapoints
2025-03-06 19:09:06,035 - INFO - validation batch 51, loss: 0.428, 1632/10016 datapoints
2025-03-06 19:09:06,318 - INFO - validation batch 101, loss: 0.292, 3232/10016 datapoints
2025-03-06 19:09:06,545 - INFO - validation batch 151, loss: 0.530, 4832/10016 datapoints
2025-03-06 19:09:06,774 - INFO - validation batch 201, loss: 0.395, 6432/10016 datapoints
2025-03-06 19:09:06,965 - INFO - validation batch 251, loss: 0.490, 8032/10016 datapoints
2025-03-06 19:09:07,183 - INFO - validation batch 301, loss: 0.404, 9632/10016 datapoints
2025-03-06 19:09:07,239 - INFO - Epoch 215/800 done.
2025-03-06 19:09:07,239 - INFO - Final validation performance:
Loss: 0.402, top-1 acc: 0.904top-5 acc: 0.904
2025-03-06 19:09:07,240 - INFO - Beginning epoch 216/800
2025-03-06 19:09:07,248 - INFO - training batch 1, loss: 0.166, 32/60000 datapoints
2025-03-06 19:09:07,491 - INFO - training batch 51, loss: 0.360, 1632/60000 datapoints
2025-03-06 19:09:07,746 - INFO - training batch 101, loss: 0.431, 3232/60000 datapoints
2025-03-06 19:09:07,981 - INFO - training batch 151, loss: 0.404, 4832/60000 datapoints
2025-03-06 19:09:08,223 - INFO - training batch 201, loss: 0.395, 6432/60000 datapoints
2025-03-06 19:09:08,657 - INFO - training batch 251, loss: 0.218, 8032/60000 datapoints
2025-03-06 19:09:08,936 - INFO - training batch 301, loss: 0.453, 9632/60000 datapoints
2025-03-06 19:09:09,270 - INFO - training batch 351, loss: 0.306, 11232/60000 datapoints
2025-03-06 19:09:09,513 - INFO - training batch 401, loss: 0.193, 12832/60000 datapoints
2025-03-06 19:09:09,777 - INFO - training batch 451, loss: 0.372, 14432/60000 datapoints
2025-03-06 19:09:10,039 - INFO - training batch 501, loss: 0.308, 16032/60000 datapoints
2025-03-06 19:09:10,420 - INFO - training batch 551, loss: 0.193, 17632/60000 datapoints
2025-03-06 19:09:10,766 - INFO - training batch 601, loss: 0.403, 19232/60000 datapoints
2025-03-06 19:09:11,121 - INFO - training batch 651, loss: 0.395, 20832/60000 datapoints
2025-03-06 19:09:11,594 - INFO - training batch 701, loss: 0.254, 22432/60000 datapoints
2025-03-06 19:09:11,903 - INFO - training batch 751, loss: 0.258, 24032/60000 datapoints
2025-03-06 19:09:12,230 - INFO - training batch 801, loss: 0.602, 25632/60000 datapoints
2025-03-06 19:09:12,473 - INFO - training batch 851, loss: 0.259, 27232/60000 datapoints
2025-03-06 19:09:12,718 - INFO - training batch 901, loss: 0.389, 28832/60000 datapoints
2025-03-06 19:09:12,986 - INFO - training batch 951, loss: 0.212, 30432/60000 datapoints
2025-03-06 19:09:13,235 - INFO - training batch 1001, loss: 0.178, 32032/60000 datapoints
2025-03-06 19:09:13,490 - INFO - training batch 1051, loss: 0.548, 33632/60000 datapoints
2025-03-06 19:09:13,744 - INFO - training batch 1101, loss: 0.184, 35232/60000 datapoints
2025-03-06 19:09:14,010 - INFO - training batch 1151, loss: 0.539, 36832/60000 datapoints
2025-03-06 19:09:14,276 - INFO - training batch 1201, loss: 0.612, 38432/60000 datapoints
2025-03-06 19:09:14,518 - INFO - training batch 1251, loss: 0.402, 40032/60000 datapoints
2025-03-06 19:09:14,753 - INFO - training batch 1301, loss: 0.531, 41632/60000 datapoints
2025-03-06 19:09:14,998 - INFO - training batch 1351, loss: 0.238, 43232/60000 datapoints
2025-03-06 19:09:15,232 - INFO - training batch 1401, loss: 0.276, 44832/60000 datapoints
2025-03-06 19:09:15,458 - INFO - training batch 1451, loss: 0.607, 46432/60000 datapoints
2025-03-06 19:09:15,688 - INFO - training batch 1501, loss: 0.453, 48032/60000 datapoints
2025-03-06 19:09:15,916 - INFO - training batch 1551, loss: 0.202, 49632/60000 datapoints
2025-03-06 19:09:16,141 - INFO - training batch 1601, loss: 0.290, 51232/60000 datapoints
2025-03-06 19:09:16,374 - INFO - training batch 1651, loss: 0.358, 52832/60000 datapoints
2025-03-06 19:09:16,600 - INFO - training batch 1701, loss: 0.217, 54432/60000 datapoints
2025-03-06 19:09:16,837 - INFO - training batch 1751, loss: 0.720, 56032/60000 datapoints
2025-03-06 19:09:17,063 - INFO - training batch 1801, loss: 0.305, 57632/60000 datapoints
2025-03-06 19:09:17,283 - INFO - training batch 1851, loss: 0.499, 59232/60000 datapoints
2025-03-06 19:09:17,400 - INFO - validation batch 1, loss: 0.435, 32/10016 datapoints
2025-03-06 19:09:17,570 - INFO - validation batch 51, loss: 0.579, 1632/10016 datapoints
2025-03-06 19:09:17,741 - INFO - validation batch 101, loss: 0.311, 3232/10016 datapoints
2025-03-06 19:09:17,911 - INFO - validation batch 151, loss: 0.197, 4832/10016 datapoints
2025-03-06 19:09:18,078 - INFO - validation batch 201, loss: 0.313, 6432/10016 datapoints
2025-03-06 19:09:18,258 - INFO - validation batch 251, loss: 0.297, 8032/10016 datapoints
2025-03-06 19:09:18,427 - INFO - validation batch 301, loss: 0.175, 9632/10016 datapoints
2025-03-06 19:09:18,470 - INFO - Epoch 216/800 done.
2025-03-06 19:09:18,471 - INFO - Final validation performance:
Loss: 0.329, top-1 acc: 0.904top-5 acc: 0.904
2025-03-06 19:09:18,471 - INFO - Beginning epoch 217/800
2025-03-06 19:09:18,478 - INFO - training batch 1, loss: 0.562, 32/60000 datapoints
2025-03-06 19:09:18,683 - INFO - training batch 51, loss: 0.347, 1632/60000 datapoints
2025-03-06 19:09:18,896 - INFO - training batch 101, loss: 0.369, 3232/60000 datapoints
2025-03-06 19:09:19,125 - INFO - training batch 151, loss: 0.187, 4832/60000 datapoints
2025-03-06 19:09:19,336 - INFO - training batch 201, loss: 0.417, 6432/60000 datapoints
2025-03-06 19:09:19,560 - INFO - training batch 251, loss: 0.278, 8032/60000 datapoints
2025-03-06 19:09:19,771 - INFO - training batch 301, loss: 0.375, 9632/60000 datapoints
2025-03-06 19:09:19,988 - INFO - training batch 351, loss: 0.403, 11232/60000 datapoints
2025-03-06 19:09:20,199 - INFO - training batch 401, loss: 0.575, 12832/60000 datapoints
2025-03-06 19:09:20,414 - INFO - training batch 451, loss: 0.443, 14432/60000 datapoints
2025-03-06 19:09:20,627 - INFO - training batch 501, loss: 0.304, 16032/60000 datapoints
2025-03-06 19:09:20,833 - INFO - training batch 551, loss: 0.329, 17632/60000 datapoints
2025-03-06 19:09:21,038 - INFO - training batch 601, loss: 0.409, 19232/60000 datapoints
2025-03-06 19:09:21,251 - INFO - training batch 651, loss: 0.677, 20832/60000 datapoints
2025-03-06 19:09:21,454 - INFO - training batch 701, loss: 0.306, 22432/60000 datapoints
2025-03-06 19:09:21,661 - INFO - training batch 751, loss: 0.374, 24032/60000 datapoints
2025-03-06 19:09:21,866 - INFO - training batch 801, loss: 0.193, 25632/60000 datapoints
2025-03-06 19:09:22,074 - INFO - training batch 851, loss: 0.632, 27232/60000 datapoints
2025-03-06 19:09:22,298 - INFO - training batch 901, loss: 0.575, 28832/60000 datapoints
2025-03-06 19:09:22,506 - INFO - training batch 951, loss: 0.390, 30432/60000 datapoints
2025-03-06 19:09:22,709 - INFO - training batch 1001, loss: 0.512, 32032/60000 datapoints
2025-03-06 19:09:22,912 - INFO - training batch 1051, loss: 0.255, 33632/60000 datapoints
2025-03-06 19:09:23,115 - INFO - training batch 1101, loss: 0.529, 35232/60000 datapoints
2025-03-06 19:09:23,315 - INFO - training batch 1151, loss: 0.291, 36832/60000 datapoints
2025-03-06 19:09:23,518 - INFO - training batch 1201, loss: 0.495, 38432/60000 datapoints
2025-03-06 19:09:23,722 - INFO - training batch 1251, loss: 0.547, 40032/60000 datapoints
2025-03-06 19:09:23,922 - INFO - training batch 1301, loss: 0.468, 41632/60000 datapoints
2025-03-06 19:09:24,129 - INFO - training batch 1351, loss: 0.420, 43232/60000 datapoints
2025-03-06 19:09:24,341 - INFO - training batch 1401, loss: 0.237, 44832/60000 datapoints
2025-03-06 19:09:24,539 - INFO - training batch 1451, loss: 0.339, 46432/60000 datapoints
2025-03-06 19:09:24,738 - INFO - training batch 1501, loss: 0.264, 48032/60000 datapoints
2025-03-06 19:09:24,949 - INFO - training batch 1551, loss: 0.502, 49632/60000 datapoints
2025-03-06 19:09:25,151 - INFO - training batch 1601, loss: 0.204, 51232/60000 datapoints
2025-03-06 19:09:25,357 - INFO - training batch 1651, loss: 0.563, 52832/60000 datapoints
2025-03-06 19:09:25,563 - INFO - training batch 1701, loss: 0.185, 54432/60000 datapoints
2025-03-06 19:09:25,771 - INFO - training batch 1751, loss: 0.446, 56032/60000 datapoints
2025-03-06 19:09:25,973 - INFO - training batch 1801, loss: 0.159, 57632/60000 datapoints
2025-03-06 19:09:26,183 - INFO - training batch 1851, loss: 0.560, 59232/60000 datapoints
2025-03-06 19:09:26,290 - INFO - validation batch 1, loss: 0.164, 32/10016 datapoints
2025-03-06 19:09:26,453 - INFO - validation batch 51, loss: 0.287, 1632/10016 datapoints
2025-03-06 19:09:26,619 - INFO - validation batch 101, loss: 0.419, 3232/10016 datapoints
2025-03-06 19:09:26,782 - INFO - validation batch 151, loss: 0.310, 4832/10016 datapoints
2025-03-06 19:09:26,947 - INFO - validation batch 201, loss: 0.360, 6432/10016 datapoints
2025-03-06 19:09:27,113 - INFO - validation batch 251, loss: 0.176, 8032/10016 datapoints
2025-03-06 19:09:27,280 - INFO - validation batch 301, loss: 0.309, 9632/10016 datapoints
2025-03-06 19:09:27,321 - INFO - Epoch 217/800 done.
2025-03-06 19:09:27,322 - INFO - Final validation performance:
Loss: 0.289, top-1 acc: 0.904top-5 acc: 0.904
2025-03-06 19:09:27,322 - INFO - Beginning epoch 218/800
2025-03-06 19:09:27,329 - INFO - training batch 1, loss: 0.338, 32/60000 datapoints
2025-03-06 19:09:27,549 - INFO - training batch 51, loss: 0.210, 1632/60000 datapoints
2025-03-06 19:09:27,765 - INFO - training batch 101, loss: 0.410, 3232/60000 datapoints
2025-03-06 19:09:27,979 - INFO - training batch 151, loss: 0.280, 4832/60000 datapoints
2025-03-06 19:09:28,194 - INFO - training batch 201, loss: 0.419, 6432/60000 datapoints
2025-03-06 19:09:28,407 - INFO - training batch 251, loss: 0.498, 8032/60000 datapoints
2025-03-06 19:09:28,623 - INFO - training batch 301, loss: 0.524, 9632/60000 datapoints
2025-03-06 19:09:28,834 - INFO - training batch 351, loss: 0.263, 11232/60000 datapoints
2025-03-06 19:09:29,045 - INFO - training batch 401, loss: 0.333, 12832/60000 datapoints
2025-03-06 19:09:29,261 - INFO - training batch 451, loss: 0.453, 14432/60000 datapoints
2025-03-06 19:09:29,486 - INFO - training batch 501, loss: 0.544, 16032/60000 datapoints
2025-03-06 19:09:29,710 - INFO - training batch 551, loss: 0.238, 17632/60000 datapoints
2025-03-06 19:09:29,931 - INFO - training batch 601, loss: 0.377, 19232/60000 datapoints
2025-03-06 19:09:30,173 - INFO - training batch 651, loss: 0.141, 20832/60000 datapoints
2025-03-06 19:09:30,409 - INFO - training batch 701, loss: 0.269, 22432/60000 datapoints
2025-03-06 19:09:30,633 - INFO - training batch 751, loss: 0.400, 24032/60000 datapoints
2025-03-06 19:09:30,853 - INFO - training batch 801, loss: 0.352, 25632/60000 datapoints
2025-03-06 19:09:31,073 - INFO - training batch 851, loss: 0.445, 27232/60000 datapoints
2025-03-06 19:09:31,291 - INFO - training batch 901, loss: 0.305, 28832/60000 datapoints
2025-03-06 19:09:31,510 - INFO - training batch 951, loss: 0.561, 30432/60000 datapoints
2025-03-06 19:09:31,738 - INFO - training batch 1001, loss: 0.376, 32032/60000 datapoints
2025-03-06 19:09:31,961 - INFO - training batch 1051, loss: 0.411, 33632/60000 datapoints
2025-03-06 19:09:32,179 - INFO - training batch 1101, loss: 0.454, 35232/60000 datapoints
2025-03-06 19:09:32,404 - INFO - training batch 1151, loss: 0.364, 36832/60000 datapoints
2025-03-06 19:09:32,629 - INFO - training batch 1201, loss: 0.427, 38432/60000 datapoints
2025-03-06 19:09:32,847 - INFO - training batch 1251, loss: 0.313, 40032/60000 datapoints
2025-03-06 19:09:33,068 - INFO - training batch 1301, loss: 0.353, 41632/60000 datapoints
2025-03-06 19:09:33,286 - INFO - training batch 1351, loss: 0.592, 43232/60000 datapoints
2025-03-06 19:09:33,505 - INFO - training batch 1401, loss: 0.266, 44832/60000 datapoints
2025-03-06 19:09:33,731 - INFO - training batch 1451, loss: 0.193, 46432/60000 datapoints
2025-03-06 19:09:33,966 - INFO - training batch 1501, loss: 0.447, 48032/60000 datapoints
2025-03-06 19:09:34,185 - INFO - training batch 1551, loss: 0.317, 49632/60000 datapoints
2025-03-06 19:09:34,416 - INFO - training batch 1601, loss: 0.271, 51232/60000 datapoints
2025-03-06 19:09:34,639 - INFO - training batch 1651, loss: 0.195, 52832/60000 datapoints
2025-03-06 19:09:34,860 - INFO - training batch 1701, loss: 0.268, 54432/60000 datapoints
2025-03-06 19:09:35,076 - INFO - training batch 1751, loss: 0.250, 56032/60000 datapoints
2025-03-06 19:09:35,286 - INFO - training batch 1801, loss: 0.358, 57632/60000 datapoints
2025-03-06 19:09:35,500 - INFO - training batch 1851, loss: 0.430, 59232/60000 datapoints
2025-03-06 19:09:35,617 - INFO - validation batch 1, loss: 0.217, 32/10016 datapoints
2025-03-06 19:09:35,786 - INFO - validation batch 51, loss: 0.381, 1632/10016 datapoints
2025-03-06 19:09:35,956 - INFO - validation batch 101, loss: 0.161, 3232/10016 datapoints
2025-03-06 19:09:36,126 - INFO - validation batch 151, loss: 0.267, 4832/10016 datapoints
2025-03-06 19:09:36,295 - INFO - validation batch 201, loss: 0.309, 6432/10016 datapoints
2025-03-06 19:09:36,466 - INFO - validation batch 251, loss: 0.596, 8032/10016 datapoints
2025-03-06 19:09:36,634 - INFO - validation batch 301, loss: 0.348, 9632/10016 datapoints
2025-03-06 19:09:36,679 - INFO - Epoch 218/800 done.
2025-03-06 19:09:36,679 - INFO - Final validation performance:
Loss: 0.325, top-1 acc: 0.905top-5 acc: 0.905
2025-03-06 19:09:36,679 - INFO - Beginning epoch 219/800
2025-03-06 19:09:36,687 - INFO - training batch 1, loss: 0.209, 32/60000 datapoints
2025-03-06 19:09:36,914 - INFO - training batch 51, loss: 0.630, 1632/60000 datapoints
2025-03-06 19:09:37,123 - INFO - training batch 101, loss: 0.396, 3232/60000 datapoints
2025-03-06 19:09:37,335 - INFO - training batch 151, loss: 0.405, 4832/60000 datapoints
2025-03-06 19:09:37,549 - INFO - training batch 201, loss: 0.262, 6432/60000 datapoints
2025-03-06 19:09:37,779 - INFO - training batch 251, loss: 0.587, 8032/60000 datapoints
2025-03-06 19:09:37,984 - INFO - training batch 301, loss: 0.312, 9632/60000 datapoints
2025-03-06 19:09:38,188 - INFO - training batch 351, loss: 0.186, 11232/60000 datapoints
2025-03-06 19:09:38,389 - INFO - training batch 401, loss: 0.406, 12832/60000 datapoints
2025-03-06 19:09:38,597 - INFO - training batch 451, loss: 0.288, 14432/60000 datapoints
2025-03-06 19:09:38,802 - INFO - training batch 501, loss: 0.444, 16032/60000 datapoints
2025-03-06 19:09:39,013 - INFO - training batch 551, loss: 0.477, 17632/60000 datapoints
2025-03-06 19:09:39,220 - INFO - training batch 601, loss: 0.411, 19232/60000 datapoints
2025-03-06 19:09:39,422 - INFO - training batch 651, loss: 0.303, 20832/60000 datapoints
2025-03-06 19:09:39,635 - INFO - training batch 701, loss: 0.603, 22432/60000 datapoints
2025-03-06 19:09:39,838 - INFO - training batch 751, loss: 0.562, 24032/60000 datapoints
2025-03-06 19:09:40,042 - INFO - training batch 801, loss: 0.144, 25632/60000 datapoints
2025-03-06 19:09:40,251 - INFO - training batch 851, loss: 0.243, 27232/60000 datapoints
2025-03-06 19:09:40,456 - INFO - training batch 901, loss: 0.267, 28832/60000 datapoints
2025-03-06 19:09:40,666 - INFO - training batch 951, loss: 0.352, 30432/60000 datapoints
2025-03-06 19:09:40,868 - INFO - training batch 1001, loss: 0.277, 32032/60000 datapoints
2025-03-06 19:09:41,074 - INFO - training batch 1051, loss: 0.207, 33632/60000 datapoints
2025-03-06 19:09:41,281 - INFO - training batch 1101, loss: 0.196, 35232/60000 datapoints
2025-03-06 19:09:41,484 - INFO - training batch 1151, loss: 0.207, 36832/60000 datapoints
2025-03-06 19:09:41,695 - INFO - training batch 1201, loss: 0.317, 38432/60000 datapoints
2025-03-06 19:09:41,906 - INFO - training batch 1251, loss: 0.339, 40032/60000 datapoints
2025-03-06 19:09:42,112 - INFO - training batch 1301, loss: 0.539, 41632/60000 datapoints
2025-03-06 19:09:42,316 - INFO - training batch 1351, loss: 0.208, 43232/60000 datapoints
2025-03-06 19:09:42,525 - INFO - training batch 1401, loss: 0.198, 44832/60000 datapoints
2025-03-06 19:09:42,736 - INFO - training batch 1451, loss: 0.108, 46432/60000 datapoints
2025-03-06 19:09:42,941 - INFO - training batch 1501, loss: 0.305, 48032/60000 datapoints
2025-03-06 19:09:43,146 - INFO - training batch 1551, loss: 0.520, 49632/60000 datapoints
2025-03-06 19:09:43,350 - INFO - training batch 1601, loss: 0.166, 51232/60000 datapoints
2025-03-06 19:09:43,555 - INFO - training batch 1651, loss: 0.263, 52832/60000 datapoints
2025-03-06 19:09:43,770 - INFO - training batch 1701, loss: 0.631, 54432/60000 datapoints
2025-03-06 19:09:43,984 - INFO - training batch 1751, loss: 0.438, 56032/60000 datapoints
2025-03-06 19:09:44,194 - INFO - training batch 1801, loss: 0.415, 57632/60000 datapoints
2025-03-06 19:09:44,427 - INFO - training batch 1851, loss: 0.203, 59232/60000 datapoints
2025-03-06 19:09:44,535 - INFO - validation batch 1, loss: 0.201, 32/10016 datapoints
2025-03-06 19:09:44,705 - INFO - validation batch 51, loss: 0.414, 1632/10016 datapoints
2025-03-06 19:09:44,875 - INFO - validation batch 101, loss: 0.349, 3232/10016 datapoints
2025-03-06 19:09:45,041 - INFO - validation batch 151, loss: 0.290, 4832/10016 datapoints
2025-03-06 19:09:45,213 - INFO - validation batch 201, loss: 0.182, 6432/10016 datapoints
2025-03-06 19:09:45,376 - INFO - validation batch 251, loss: 0.290, 8032/10016 datapoints
2025-03-06 19:09:45,541 - INFO - validation batch 301, loss: 0.523, 9632/10016 datapoints
2025-03-06 19:09:45,585 - INFO - Epoch 219/800 done.
2025-03-06 19:09:45,585 - INFO - Final validation performance:
Loss: 0.321, top-1 acc: 0.905top-5 acc: 0.905
2025-03-06 19:09:45,586 - INFO - Beginning epoch 220/800
2025-03-06 19:09:45,593 - INFO - training batch 1, loss: 0.184, 32/60000 datapoints
2025-03-06 19:09:45,816 - INFO - training batch 51, loss: 0.240, 1632/60000 datapoints
2025-03-06 19:09:46,023 - INFO - training batch 101, loss: 0.318, 3232/60000 datapoints
2025-03-06 19:09:46,237 - INFO - training batch 151, loss: 0.235, 4832/60000 datapoints
2025-03-06 19:09:46,444 - INFO - training batch 201, loss: 0.429, 6432/60000 datapoints
2025-03-06 19:09:46,660 - INFO - training batch 251, loss: 0.223, 8032/60000 datapoints
2025-03-06 19:09:46,867 - INFO - training batch 301, loss: 0.528, 9632/60000 datapoints
2025-03-06 19:09:47,076 - INFO - training batch 351, loss: 0.415, 11232/60000 datapoints
2025-03-06 19:09:47,283 - INFO - training batch 401, loss: 0.415, 12832/60000 datapoints
2025-03-06 19:09:47,488 - INFO - training batch 451, loss: 0.188, 14432/60000 datapoints
2025-03-06 19:09:47,702 - INFO - training batch 501, loss: 0.223, 16032/60000 datapoints
2025-03-06 19:09:47,917 - INFO - training batch 551, loss: 0.490, 17632/60000 datapoints
2025-03-06 19:09:48,127 - INFO - training batch 601, loss: 0.175, 19232/60000 datapoints
2025-03-06 19:09:48,330 - INFO - training batch 651, loss: 0.359, 20832/60000 datapoints
2025-03-06 19:09:48,537 - INFO - training batch 701, loss: 0.422, 22432/60000 datapoints
2025-03-06 19:09:48,748 - INFO - training batch 751, loss: 0.303, 24032/60000 datapoints
2025-03-06 19:09:48,955 - INFO - training batch 801, loss: 0.256, 25632/60000 datapoints
2025-03-06 19:09:49,170 - INFO - training batch 851, loss: 0.261, 27232/60000 datapoints
2025-03-06 19:09:49,382 - INFO - training batch 901, loss: 0.227, 28832/60000 datapoints
2025-03-06 19:09:49,593 - INFO - training batch 951, loss: 0.286, 30432/60000 datapoints
2025-03-06 19:09:49,811 - INFO - training batch 1001, loss: 0.498, 32032/60000 datapoints
2025-03-06 19:09:50,023 - INFO - training batch 1051, loss: 0.573, 33632/60000 datapoints
2025-03-06 19:09:50,245 - INFO - training batch 1101, loss: 0.420, 35232/60000 datapoints
2025-03-06 19:09:50,457 - INFO - training batch 1151, loss: 0.245, 36832/60000 datapoints
2025-03-06 19:09:50,673 - INFO - training batch 1201, loss: 0.302, 38432/60000 datapoints
2025-03-06 19:09:50,885 - INFO - training batch 1251, loss: 0.292, 40032/60000 datapoints
2025-03-06 19:09:51,097 - INFO - training batch 1301, loss: 0.219, 41632/60000 datapoints
2025-03-06 19:09:51,309 - INFO - training batch 1351, loss: 0.299, 43232/60000 datapoints
2025-03-06 19:09:51,523 - INFO - training batch 1401, loss: 0.229, 44832/60000 datapoints
2025-03-06 19:09:51,746 - INFO - training batch 1451, loss: 0.269, 46432/60000 datapoints
2025-03-06 19:09:51,967 - INFO - training batch 1501, loss: 0.633, 48032/60000 datapoints
2025-03-06 19:09:52,182 - INFO - training batch 1551, loss: 0.727, 49632/60000 datapoints
2025-03-06 19:09:52,394 - INFO - training batch 1601, loss: 0.373, 51232/60000 datapoints
2025-03-06 19:09:52,606 - INFO - training batch 1651, loss: 0.396, 52832/60000 datapoints
2025-03-06 19:09:52,818 - INFO - training batch 1701, loss: 0.254, 54432/60000 datapoints
2025-03-06 19:09:53,031 - INFO - training batch 1751, loss: 0.417, 56032/60000 datapoints
2025-03-06 19:09:53,244 - INFO - training batch 1801, loss: 0.348, 57632/60000 datapoints
2025-03-06 19:09:53,458 - INFO - training batch 1851, loss: 0.168, 59232/60000 datapoints
2025-03-06 19:09:53,572 - INFO - validation batch 1, loss: 0.130, 32/10016 datapoints
2025-03-06 19:09:53,752 - INFO - validation batch 51, loss: 0.385, 1632/10016 datapoints
2025-03-06 19:09:53,924 - INFO - validation batch 101, loss: 0.283, 3232/10016 datapoints
2025-03-06 19:09:54,098 - INFO - validation batch 151, loss: 0.409, 4832/10016 datapoints
2025-03-06 19:09:54,271 - INFO - validation batch 201, loss: 0.161, 6432/10016 datapoints
2025-03-06 19:09:54,466 - INFO - validation batch 251, loss: 0.223, 8032/10016 datapoints
2025-03-06 19:09:54,644 - INFO - validation batch 301, loss: 0.245, 9632/10016 datapoints
2025-03-06 19:09:54,688 - INFO - Epoch 220/800 done.
2025-03-06 19:09:54,688 - INFO - Final validation performance:
Loss: 0.262, top-1 acc: 0.905top-5 acc: 0.905
2025-03-06 19:09:54,689 - INFO - Beginning epoch 221/800
2025-03-06 19:09:54,696 - INFO - training batch 1, loss: 0.354, 32/60000 datapoints
2025-03-06 19:09:54,929 - INFO - training batch 51, loss: 0.495, 1632/60000 datapoints
2025-03-06 19:09:55,145 - INFO - training batch 101, loss: 0.608, 3232/60000 datapoints
2025-03-06 19:09:55,367 - INFO - training batch 151, loss: 0.591, 4832/60000 datapoints
2025-03-06 19:09:55,591 - INFO - training batch 201, loss: 0.333, 6432/60000 datapoints
2025-03-06 19:09:55,820 - INFO - training batch 251, loss: 0.305, 8032/60000 datapoints
2025-03-06 19:09:56,032 - INFO - training batch 301, loss: 0.232, 9632/60000 datapoints
2025-03-06 19:09:56,248 - INFO - training batch 351, loss: 0.502, 11232/60000 datapoints
2025-03-06 19:09:56,461 - INFO - training batch 401, loss: 0.378, 12832/60000 datapoints
2025-03-06 19:09:56,679 - INFO - training batch 451, loss: 0.432, 14432/60000 datapoints
2025-03-06 19:09:56,894 - INFO - training batch 501, loss: 0.491, 16032/60000 datapoints
2025-03-06 19:09:57,111 - INFO - training batch 551, loss: 0.486, 17632/60000 datapoints
2025-03-06 19:09:57,324 - INFO - training batch 601, loss: 0.369, 19232/60000 datapoints
2025-03-06 19:09:57,530 - INFO - training batch 651, loss: 0.384, 20832/60000 datapoints
2025-03-06 19:09:57,749 - INFO - training batch 701, loss: 0.316, 22432/60000 datapoints
2025-03-06 19:09:57,956 - INFO - training batch 751, loss: 0.286, 24032/60000 datapoints
2025-03-06 19:09:58,165 - INFO - training batch 801, loss: 0.338, 25632/60000 datapoints
2025-03-06 19:09:58,369 - INFO - training batch 851, loss: 0.280, 27232/60000 datapoints
2025-03-06 19:09:58,575 - INFO - training batch 901, loss: 0.294, 28832/60000 datapoints
2025-03-06 19:09:58,787 - INFO - training batch 951, loss: 0.195, 30432/60000 datapoints
2025-03-06 19:09:58,994 - INFO - training batch 1001, loss: 0.157, 32032/60000 datapoints
2025-03-06 19:09:59,204 - INFO - training batch 1051, loss: 0.438, 33632/60000 datapoints
2025-03-06 19:09:59,407 - INFO - training batch 1101, loss: 0.349, 35232/60000 datapoints
2025-03-06 19:09:59,619 - INFO - training batch 1151, loss: 0.494, 36832/60000 datapoints
2025-03-06 19:09:59,828 - INFO - training batch 1201, loss: 0.220, 38432/60000 datapoints
2025-03-06 19:10:00,038 - INFO - training batch 1251, loss: 0.379, 40032/60000 datapoints
2025-03-06 19:10:00,251 - INFO - training batch 1301, loss: 0.467, 41632/60000 datapoints
2025-03-06 19:10:00,456 - INFO - training batch 1351, loss: 0.449, 43232/60000 datapoints
2025-03-06 19:10:00,664 - INFO - training batch 1401, loss: 0.245, 44832/60000 datapoints
2025-03-06 19:10:00,864 - INFO - training batch 1451, loss: 0.459, 46432/60000 datapoints
2025-03-06 19:10:01,065 - INFO - training batch 1501, loss: 0.100, 48032/60000 datapoints
2025-03-06 19:10:01,270 - INFO - training batch 1551, loss: 0.436, 49632/60000 datapoints
2025-03-06 19:10:01,474 - INFO - training batch 1601, loss: 0.230, 51232/60000 datapoints
2025-03-06 19:10:01,680 - INFO - training batch 1651, loss: 0.414, 52832/60000 datapoints
2025-03-06 19:10:01,881 - INFO - training batch 1701, loss: 0.221, 54432/60000 datapoints
2025-03-06 19:10:02,084 - INFO - training batch 1751, loss: 0.409, 56032/60000 datapoints
2025-03-06 19:10:02,287 - INFO - training batch 1801, loss: 0.435, 57632/60000 datapoints
2025-03-06 19:10:02,489 - INFO - training batch 1851, loss: 0.329, 59232/60000 datapoints
2025-03-06 19:10:02,595 - INFO - validation batch 1, loss: 0.214, 32/10016 datapoints
2025-03-06 19:10:02,760 - INFO - validation batch 51, loss: 0.168, 1632/10016 datapoints
2025-03-06 19:10:02,925 - INFO - validation batch 101, loss: 0.362, 3232/10016 datapoints
2025-03-06 19:10:03,094 - INFO - validation batch 151, loss: 0.261, 4832/10016 datapoints
2025-03-06 19:10:03,264 - INFO - validation batch 201, loss: 0.382, 6432/10016 datapoints
2025-03-06 19:10:03,434 - INFO - validation batch 251, loss: 0.206, 8032/10016 datapoints
2025-03-06 19:10:03,596 - INFO - validation batch 301, loss: 0.259, 9632/10016 datapoints
2025-03-06 19:10:03,642 - INFO - Epoch 221/800 done.
2025-03-06 19:10:03,642 - INFO - Final validation performance:
Loss: 0.265, top-1 acc: 0.905top-5 acc: 0.905
2025-03-06 19:10:03,643 - INFO - Beginning epoch 222/800
2025-03-06 19:10:03,651 - INFO - training batch 1, loss: 0.196, 32/60000 datapoints
2025-03-06 19:10:03,874 - INFO - training batch 51, loss: 0.239, 1632/60000 datapoints
2025-03-06 19:10:04,085 - INFO - training batch 101, loss: 0.353, 3232/60000 datapoints
2025-03-06 19:10:04,315 - INFO - training batch 151, loss: 0.254, 4832/60000 datapoints
2025-03-06 19:10:04,569 - INFO - training batch 201, loss: 0.339, 6432/60000 datapoints
2025-03-06 19:10:04,777 - INFO - training batch 251, loss: 0.355, 8032/60000 datapoints
2025-03-06 19:10:04,988 - INFO - training batch 301, loss: 0.243, 9632/60000 datapoints
2025-03-06 19:10:05,191 - INFO - training batch 351, loss: 0.527, 11232/60000 datapoints
2025-03-06 19:10:05,390 - INFO - training batch 401, loss: 0.185, 12832/60000 datapoints
2025-03-06 19:10:05,738 - INFO - training batch 451, loss: 0.254, 14432/60000 datapoints
2025-03-06 19:10:05,944 - INFO - training batch 501, loss: 0.302, 16032/60000 datapoints
2025-03-06 19:10:06,146 - INFO - training batch 551, loss: 0.290, 17632/60000 datapoints
2025-03-06 19:10:06,356 - INFO - training batch 601, loss: 0.121, 19232/60000 datapoints
2025-03-06 19:10:06,562 - INFO - training batch 651, loss: 0.343, 20832/60000 datapoints
2025-03-06 19:10:06,777 - INFO - training batch 701, loss: 0.428, 22432/60000 datapoints
2025-03-06 19:10:06,991 - INFO - training batch 751, loss: 0.321, 24032/60000 datapoints
2025-03-06 19:10:07,211 - INFO - training batch 801, loss: 0.287, 25632/60000 datapoints
2025-03-06 19:10:07,431 - INFO - training batch 851, loss: 0.216, 27232/60000 datapoints
2025-03-06 19:10:07,659 - INFO - training batch 901, loss: 0.430, 28832/60000 datapoints
2025-03-06 19:10:07,885 - INFO - training batch 951, loss: 0.226, 30432/60000 datapoints
2025-03-06 19:10:08,105 - INFO - training batch 1001, loss: 0.329, 32032/60000 datapoints
2025-03-06 19:10:08,319 - INFO - training batch 1051, loss: 0.339, 33632/60000 datapoints
2025-03-06 19:10:08,529 - INFO - training batch 1101, loss: 0.496, 35232/60000 datapoints
2025-03-06 19:10:08,743 - INFO - training batch 1151, loss: 0.471, 36832/60000 datapoints
2025-03-06 19:10:08,961 - INFO - training batch 1201, loss: 0.136, 38432/60000 datapoints
2025-03-06 19:10:09,177 - INFO - training batch 1251, loss: 0.177, 40032/60000 datapoints
2025-03-06 19:10:09,391 - INFO - training batch 1301, loss: 0.563, 41632/60000 datapoints
2025-03-06 19:10:09,603 - INFO - training batch 1351, loss: 0.534, 43232/60000 datapoints
2025-03-06 19:10:09,820 - INFO - training batch 1401, loss: 0.489, 44832/60000 datapoints
2025-03-06 19:10:10,034 - INFO - training batch 1451, loss: 0.204, 46432/60000 datapoints
2025-03-06 19:10:10,257 - INFO - training batch 1501, loss: 0.338, 48032/60000 datapoints
2025-03-06 19:10:10,472 - INFO - training batch 1551, loss: 0.200, 49632/60000 datapoints
2025-03-06 19:10:10,689 - INFO - training batch 1601, loss: 0.544, 51232/60000 datapoints
2025-03-06 19:10:10,902 - INFO - training batch 1651, loss: 0.540, 52832/60000 datapoints
2025-03-06 19:10:11,117 - INFO - training batch 1701, loss: 0.320, 54432/60000 datapoints
2025-03-06 19:10:11,330 - INFO - training batch 1751, loss: 0.336, 56032/60000 datapoints
2025-03-06 19:10:11,542 - INFO - training batch 1801, loss: 0.323, 57632/60000 datapoints
2025-03-06 19:10:11,763 - INFO - training batch 1851, loss: 0.291, 59232/60000 datapoints
2025-03-06 19:10:11,879 - INFO - validation batch 1, loss: 0.373, 32/10016 datapoints
2025-03-06 19:10:12,052 - INFO - validation batch 51, loss: 0.470, 1632/10016 datapoints
2025-03-06 19:10:12,222 - INFO - validation batch 101, loss: 0.464, 3232/10016 datapoints
2025-03-06 19:10:12,397 - INFO - validation batch 151, loss: 0.258, 4832/10016 datapoints
2025-03-06 19:10:12,566 - INFO - validation batch 201, loss: 0.427, 6432/10016 datapoints
2025-03-06 19:10:12,739 - INFO - validation batch 251, loss: 0.321, 8032/10016 datapoints
2025-03-06 19:10:12,913 - INFO - validation batch 301, loss: 0.240, 9632/10016 datapoints
2025-03-06 19:10:12,954 - INFO - Epoch 222/800 done.
2025-03-06 19:10:12,955 - INFO - Final validation performance:
Loss: 0.365, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 19:10:12,955 - INFO - Beginning epoch 223/800
2025-03-06 19:10:12,966 - INFO - training batch 1, loss: 0.220, 32/60000 datapoints
2025-03-06 19:10:13,187 - INFO - training batch 51, loss: 0.280, 1632/60000 datapoints
2025-03-06 19:10:13,401 - INFO - training batch 101, loss: 0.121, 3232/60000 datapoints
2025-03-06 19:10:13,618 - INFO - training batch 151, loss: 0.370, 4832/60000 datapoints
2025-03-06 19:10:13,857 - INFO - training batch 201, loss: 0.322, 6432/60000 datapoints
2025-03-06 19:10:14,069 - INFO - training batch 251, loss: 0.485, 8032/60000 datapoints
2025-03-06 19:10:14,296 - INFO - training batch 301, loss: 0.143, 9632/60000 datapoints
2025-03-06 19:10:14,513 - INFO - training batch 351, loss: 0.096, 11232/60000 datapoints
2025-03-06 19:10:14,755 - INFO - training batch 401, loss: 0.515, 12832/60000 datapoints
2025-03-06 19:10:14,971 - INFO - training batch 451, loss: 0.314, 14432/60000 datapoints
2025-03-06 19:10:15,182 - INFO - training batch 501, loss: 0.236, 16032/60000 datapoints
2025-03-06 19:10:15,392 - INFO - training batch 551, loss: 0.411, 17632/60000 datapoints
2025-03-06 19:10:15,604 - INFO - training batch 601, loss: 0.264, 19232/60000 datapoints
2025-03-06 19:10:15,825 - INFO - training batch 651, loss: 0.356, 20832/60000 datapoints
2025-03-06 19:10:16,037 - INFO - training batch 701, loss: 0.986, 22432/60000 datapoints
2025-03-06 19:10:16,254 - INFO - training batch 751, loss: 0.181, 24032/60000 datapoints
2025-03-06 19:10:16,467 - INFO - training batch 801, loss: 0.508, 25632/60000 datapoints
2025-03-06 19:10:16,685 - INFO - training batch 851, loss: 0.178, 27232/60000 datapoints
2025-03-06 19:10:16,904 - INFO - training batch 901, loss: 0.320, 28832/60000 datapoints
2025-03-06 19:10:17,119 - INFO - training batch 951, loss: 0.266, 30432/60000 datapoints
2025-03-06 19:10:17,334 - INFO - training batch 1001, loss: 0.468, 32032/60000 datapoints
2025-03-06 19:10:17,541 - INFO - training batch 1051, loss: 0.365, 33632/60000 datapoints
2025-03-06 19:10:17,754 - INFO - training batch 1101, loss: 0.142, 35232/60000 datapoints
2025-03-06 19:10:17,973 - INFO - training batch 1151, loss: 0.112, 36832/60000 datapoints
2025-03-06 19:10:18,181 - INFO - training batch 1201, loss: 0.208, 38432/60000 datapoints
2025-03-06 19:10:18,387 - INFO - training batch 1251, loss: 0.258, 40032/60000 datapoints
2025-03-06 19:10:18,593 - INFO - training batch 1301, loss: 0.415, 41632/60000 datapoints
2025-03-06 19:10:18,807 - INFO - training batch 1351, loss: 0.267, 43232/60000 datapoints
2025-03-06 19:10:19,015 - INFO - training batch 1401, loss: 0.368, 44832/60000 datapoints
2025-03-06 19:10:19,223 - INFO - training batch 1451, loss: 0.304, 46432/60000 datapoints
2025-03-06 19:10:19,432 - INFO - training batch 1501, loss: 0.392, 48032/60000 datapoints
2025-03-06 19:10:19,640 - INFO - training batch 1551, loss: 0.650, 49632/60000 datapoints
2025-03-06 19:10:19,848 - INFO - training batch 1601, loss: 0.565, 51232/60000 datapoints
2025-03-06 19:10:20,056 - INFO - training batch 1651, loss: 0.417, 52832/60000 datapoints
2025-03-06 19:10:20,263 - INFO - training batch 1701, loss: 0.198, 54432/60000 datapoints
2025-03-06 19:10:20,464 - INFO - training batch 1751, loss: 0.125, 56032/60000 datapoints
2025-03-06 19:10:20,664 - INFO - training batch 1801, loss: 0.417, 57632/60000 datapoints
2025-03-06 19:10:20,862 - INFO - training batch 1851, loss: 0.267, 59232/60000 datapoints
2025-03-06 19:10:20,971 - INFO - validation batch 1, loss: 0.212, 32/10016 datapoints
2025-03-06 19:10:21,133 - INFO - validation batch 51, loss: 0.326, 1632/10016 datapoints
2025-03-06 19:10:21,295 - INFO - validation batch 101, loss: 0.289, 3232/10016 datapoints
2025-03-06 19:10:21,460 - INFO - validation batch 151, loss: 0.313, 4832/10016 datapoints
2025-03-06 19:10:21,635 - INFO - validation batch 201, loss: 0.282, 6432/10016 datapoints
2025-03-06 19:10:21,801 - INFO - validation batch 251, loss: 0.320, 8032/10016 datapoints
2025-03-06 19:10:21,967 - INFO - validation batch 301, loss: 0.380, 9632/10016 datapoints
2025-03-06 19:10:22,004 - INFO - Epoch 223/800 done.
2025-03-06 19:10:22,005 - INFO - Final validation performance:
Loss: 0.303, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 19:10:22,005 - INFO - Beginning epoch 224/800
2025-03-06 19:10:22,016 - INFO - training batch 1, loss: 0.503, 32/60000 datapoints
2025-03-06 19:10:22,254 - INFO - training batch 51, loss: 0.494, 1632/60000 datapoints
2025-03-06 19:10:22,457 - INFO - training batch 101, loss: 0.468, 3232/60000 datapoints
2025-03-06 19:10:22,683 - INFO - training batch 151, loss: 0.236, 4832/60000 datapoints
2025-03-06 19:10:22,885 - INFO - training batch 201, loss: 0.256, 6432/60000 datapoints
2025-03-06 19:10:23,086 - INFO - training batch 251, loss: 0.491, 8032/60000 datapoints
2025-03-06 19:10:23,290 - INFO - training batch 301, loss: 0.374, 9632/60000 datapoints
2025-03-06 19:10:23,489 - INFO - training batch 351, loss: 0.112, 11232/60000 datapoints
2025-03-06 19:10:23,689 - INFO - training batch 401, loss: 0.154, 12832/60000 datapoints
2025-03-06 19:10:23,890 - INFO - training batch 451, loss: 0.426, 14432/60000 datapoints
2025-03-06 19:10:24,089 - INFO - training batch 501, loss: 0.280, 16032/60000 datapoints
2025-03-06 19:10:24,291 - INFO - training batch 551, loss: 0.681, 17632/60000 datapoints
2025-03-06 19:10:24,499 - INFO - training batch 601, loss: 0.411, 19232/60000 datapoints
2025-03-06 19:10:24,722 - INFO - training batch 651, loss: 0.239, 20832/60000 datapoints
2025-03-06 19:10:24,936 - INFO - training batch 701, loss: 0.293, 22432/60000 datapoints
2025-03-06 19:10:25,140 - INFO - training batch 751, loss: 0.422, 24032/60000 datapoints
2025-03-06 19:10:25,345 - INFO - training batch 801, loss: 0.411, 25632/60000 datapoints
2025-03-06 19:10:25,555 - INFO - training batch 851, loss: 0.193, 27232/60000 datapoints
2025-03-06 19:10:25,787 - INFO - training batch 901, loss: 0.349, 28832/60000 datapoints
2025-03-06 19:10:26,047 - INFO - training batch 951, loss: 0.462, 30432/60000 datapoints
2025-03-06 19:10:26,267 - INFO - training batch 1001, loss: 0.397, 32032/60000 datapoints
2025-03-06 19:10:26,480 - INFO - training batch 1051, loss: 0.301, 33632/60000 datapoints
2025-03-06 19:10:26,707 - INFO - training batch 1101, loss: 0.542, 35232/60000 datapoints
2025-03-06 19:10:26,921 - INFO - training batch 1151, loss: 0.165, 36832/60000 datapoints
2025-03-06 19:10:27,138 - INFO - training batch 1201, loss: 0.234, 38432/60000 datapoints
2025-03-06 19:10:27,349 - INFO - training batch 1251, loss: 0.339, 40032/60000 datapoints
2025-03-06 19:10:27,563 - INFO - training batch 1301, loss: 0.434, 41632/60000 datapoints
2025-03-06 19:10:27,782 - INFO - training batch 1351, loss: 0.317, 43232/60000 datapoints
2025-03-06 19:10:28,004 - INFO - training batch 1401, loss: 0.522, 44832/60000 datapoints
2025-03-06 19:10:28,223 - INFO - training batch 1451, loss: 0.168, 46432/60000 datapoints
2025-03-06 19:10:28,443 - INFO - training batch 1501, loss: 0.379, 48032/60000 datapoints
2025-03-06 19:10:28,664 - INFO - training batch 1551, loss: 0.310, 49632/60000 datapoints
2025-03-06 19:10:28,883 - INFO - training batch 1601, loss: 0.203, 51232/60000 datapoints
2025-03-06 19:10:29,102 - INFO - training batch 1651, loss: 0.221, 52832/60000 datapoints
2025-03-06 19:10:29,320 - INFO - training batch 1701, loss: 0.240, 54432/60000 datapoints
2025-03-06 19:10:29,540 - INFO - training batch 1751, loss: 0.173, 56032/60000 datapoints
2025-03-06 19:10:29,768 - INFO - training batch 1801, loss: 0.409, 57632/60000 datapoints
2025-03-06 19:10:29,988 - INFO - training batch 1851, loss: 0.477, 59232/60000 datapoints
2025-03-06 19:10:30,104 - INFO - validation batch 1, loss: 0.170, 32/10016 datapoints
2025-03-06 19:10:30,283 - INFO - validation batch 51, loss: 0.511, 1632/10016 datapoints
2025-03-06 19:10:30,470 - INFO - validation batch 101, loss: 0.403, 3232/10016 datapoints
2025-03-06 19:10:30,648 - INFO - validation batch 151, loss: 0.252, 4832/10016 datapoints
2025-03-06 19:10:30,826 - INFO - validation batch 201, loss: 0.294, 6432/10016 datapoints
2025-03-06 19:10:31,002 - INFO - validation batch 251, loss: 0.219, 8032/10016 datapoints
2025-03-06 19:10:31,179 - INFO - validation batch 301, loss: 0.209, 9632/10016 datapoints
2025-03-06 19:10:31,226 - INFO - Epoch 224/800 done.
2025-03-06 19:10:31,226 - INFO - Final validation performance:
Loss: 0.294, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 19:10:31,227 - INFO - Beginning epoch 225/800
2025-03-06 19:10:31,234 - INFO - training batch 1, loss: 0.303, 32/60000 datapoints
2025-03-06 19:10:31,456 - INFO - training batch 51, loss: 0.226, 1632/60000 datapoints
2025-03-06 19:10:31,678 - INFO - training batch 101, loss: 0.308, 3232/60000 datapoints
2025-03-06 19:10:31,916 - INFO - training batch 151, loss: 0.374, 4832/60000 datapoints
2025-03-06 19:10:32,139 - INFO - training batch 201, loss: 0.198, 6432/60000 datapoints
2025-03-06 19:10:32,363 - INFO - training batch 251, loss: 0.265, 8032/60000 datapoints
2025-03-06 19:10:32,578 - INFO - training batch 301, loss: 0.491, 9632/60000 datapoints
2025-03-06 19:10:32,796 - INFO - training batch 351, loss: 0.385, 11232/60000 datapoints
2025-03-06 19:10:33,008 - INFO - training batch 401, loss: 0.423, 12832/60000 datapoints
2025-03-06 19:10:33,223 - INFO - training batch 451, loss: 0.285, 14432/60000 datapoints
2025-03-06 19:10:33,438 - INFO - training batch 501, loss: 0.262, 16032/60000 datapoints
2025-03-06 19:10:33,654 - INFO - training batch 551, loss: 0.328, 17632/60000 datapoints
2025-03-06 19:10:33,864 - INFO - training batch 601, loss: 0.121, 19232/60000 datapoints
2025-03-06 19:10:34,070 - INFO - training batch 651, loss: 0.323, 20832/60000 datapoints
2025-03-06 19:10:34,276 - INFO - training batch 701, loss: 0.503, 22432/60000 datapoints
2025-03-06 19:10:34,484 - INFO - training batch 751, loss: 0.214, 24032/60000 datapoints
2025-03-06 19:10:34,693 - INFO - training batch 801, loss: 0.267, 25632/60000 datapoints
2025-03-06 19:10:34,933 - INFO - training batch 851, loss: 0.405, 27232/60000 datapoints
2025-03-06 19:10:35,140 - INFO - training batch 901, loss: 0.470, 28832/60000 datapoints
2025-03-06 19:10:35,344 - INFO - training batch 951, loss: 0.355, 30432/60000 datapoints
2025-03-06 19:10:35,550 - INFO - training batch 1001, loss: 0.323, 32032/60000 datapoints
2025-03-06 19:10:35,761 - INFO - training batch 1051, loss: 0.258, 33632/60000 datapoints
2025-03-06 19:10:35,971 - INFO - training batch 1101, loss: 0.338, 35232/60000 datapoints
2025-03-06 19:10:36,177 - INFO - training batch 1151, loss: 0.482, 36832/60000 datapoints
2025-03-06 19:10:36,385 - INFO - training batch 1201, loss: 0.289, 38432/60000 datapoints
2025-03-06 19:10:36,593 - INFO - training batch 1251, loss: 0.842, 40032/60000 datapoints
2025-03-06 19:10:36,803 - INFO - training batch 1301, loss: 0.378, 41632/60000 datapoints
2025-03-06 19:10:37,008 - INFO - training batch 1351, loss: 0.490, 43232/60000 datapoints
2025-03-06 19:10:37,214 - INFO - training batch 1401, loss: 0.349, 44832/60000 datapoints
2025-03-06 19:10:37,413 - INFO - training batch 1451, loss: 0.301, 46432/60000 datapoints
2025-03-06 19:10:37,637 - INFO - training batch 1501, loss: 0.236, 48032/60000 datapoints
2025-03-06 19:10:37,844 - INFO - training batch 1551, loss: 0.451, 49632/60000 datapoints
2025-03-06 19:10:38,050 - INFO - training batch 1601, loss: 0.351, 51232/60000 datapoints
2025-03-06 19:10:38,252 - INFO - training batch 1651, loss: 0.529, 52832/60000 datapoints
2025-03-06 19:10:38,453 - INFO - training batch 1701, loss: 0.319, 54432/60000 datapoints
2025-03-06 19:10:38,661 - INFO - training batch 1751, loss: 0.882, 56032/60000 datapoints
2025-03-06 19:10:38,865 - INFO - training batch 1801, loss: 0.381, 57632/60000 datapoints
2025-03-06 19:10:39,072 - INFO - training batch 1851, loss: 0.228, 59232/60000 datapoints
2025-03-06 19:10:39,181 - INFO - validation batch 1, loss: 0.479, 32/10016 datapoints
2025-03-06 19:10:39,346 - INFO - validation batch 51, loss: 0.191, 1632/10016 datapoints
2025-03-06 19:10:39,508 - INFO - validation batch 101, loss: 0.343, 3232/10016 datapoints
2025-03-06 19:10:39,676 - INFO - validation batch 151, loss: 0.430, 4832/10016 datapoints
2025-03-06 19:10:39,844 - INFO - validation batch 201, loss: 0.122, 6432/10016 datapoints
2025-03-06 19:10:40,011 - INFO - validation batch 251, loss: 0.275, 8032/10016 datapoints
2025-03-06 19:10:40,175 - INFO - validation batch 301, loss: 0.222, 9632/10016 datapoints
2025-03-06 19:10:40,217 - INFO - Epoch 225/800 done.
2025-03-06 19:10:40,217 - INFO - Final validation performance:
Loss: 0.294, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 19:10:40,218 - INFO - Beginning epoch 226/800
2025-03-06 19:10:40,225 - INFO - training batch 1, loss: 0.396, 32/60000 datapoints
2025-03-06 19:10:40,453 - INFO - training batch 51, loss: 0.305, 1632/60000 datapoints
2025-03-06 19:10:40,666 - INFO - training batch 101, loss: 0.261, 3232/60000 datapoints
2025-03-06 19:10:40,876 - INFO - training batch 151, loss: 0.303, 4832/60000 datapoints
2025-03-06 19:10:41,088 - INFO - training batch 201, loss: 0.414, 6432/60000 datapoints
2025-03-06 19:10:41,296 - INFO - training batch 251, loss: 0.244, 8032/60000 datapoints
2025-03-06 19:10:41,501 - INFO - training batch 301, loss: 0.519, 9632/60000 datapoints
2025-03-06 19:10:41,713 - INFO - training batch 351, loss: 0.300, 11232/60000 datapoints
2025-03-06 19:10:41,915 - INFO - training batch 401, loss: 0.451, 12832/60000 datapoints
2025-03-06 19:10:42,135 - INFO - training batch 451, loss: 0.254, 14432/60000 datapoints
2025-03-06 19:10:42,338 - INFO - training batch 501, loss: 0.473, 16032/60000 datapoints
2025-03-06 19:10:42,542 - INFO - training batch 551, loss: 0.219, 17632/60000 datapoints
2025-03-06 19:10:42,752 - INFO - training batch 601, loss: 0.608, 19232/60000 datapoints
2025-03-06 19:10:42,956 - INFO - training batch 651, loss: 0.455, 20832/60000 datapoints
2025-03-06 19:10:43,164 - INFO - training batch 701, loss: 0.290, 22432/60000 datapoints
2025-03-06 19:10:43,373 - INFO - training batch 751, loss: 0.502, 24032/60000 datapoints
2025-03-06 19:10:43,583 - INFO - training batch 801, loss: 0.267, 25632/60000 datapoints
2025-03-06 19:10:43,796 - INFO - training batch 851, loss: 0.327, 27232/60000 datapoints
2025-03-06 19:10:44,006 - INFO - training batch 901, loss: 0.385, 28832/60000 datapoints
2025-03-06 19:10:44,218 - INFO - training batch 951, loss: 0.478, 30432/60000 datapoints
2025-03-06 19:10:44,426 - INFO - training batch 1001, loss: 0.643, 32032/60000 datapoints
2025-03-06 19:10:44,642 - INFO - training batch 1051, loss: 0.380, 33632/60000 datapoints
2025-03-06 19:10:44,872 - INFO - training batch 1101, loss: 0.382, 35232/60000 datapoints
2025-03-06 19:10:45,104 - INFO - training batch 1151, loss: 0.115, 36832/60000 datapoints
2025-03-06 19:10:45,317 - INFO - training batch 1201, loss: 0.454, 38432/60000 datapoints
2025-03-06 19:10:45,527 - INFO - training batch 1251, loss: 0.386, 40032/60000 datapoints
2025-03-06 19:10:45,743 - INFO - training batch 1301, loss: 0.268, 41632/60000 datapoints
2025-03-06 19:10:45,959 - INFO - training batch 1351, loss: 0.201, 43232/60000 datapoints
2025-03-06 19:10:46,174 - INFO - training batch 1401, loss: 0.530, 44832/60000 datapoints
2025-03-06 19:10:46,387 - INFO - training batch 1451, loss: 0.222, 46432/60000 datapoints
2025-03-06 19:10:46,612 - INFO - training batch 1501, loss: 0.284, 48032/60000 datapoints
2025-03-06 19:10:46,832 - INFO - training batch 1551, loss: 0.353, 49632/60000 datapoints
2025-03-06 19:10:47,045 - INFO - training batch 1601, loss: 0.173, 51232/60000 datapoints
2025-03-06 19:10:47,256 - INFO - training batch 1651, loss: 0.469, 52832/60000 datapoints
2025-03-06 19:10:47,470 - INFO - training batch 1701, loss: 0.242, 54432/60000 datapoints
2025-03-06 19:10:47,688 - INFO - training batch 1751, loss: 0.279, 56032/60000 datapoints
2025-03-06 19:10:47,905 - INFO - training batch 1801, loss: 0.513, 57632/60000 datapoints
2025-03-06 19:10:48,116 - INFO - training batch 1851, loss: 0.473, 59232/60000 datapoints
2025-03-06 19:10:48,230 - INFO - validation batch 1, loss: 0.284, 32/10016 datapoints
2025-03-06 19:10:48,402 - INFO - validation batch 51, loss: 0.385, 1632/10016 datapoints
2025-03-06 19:10:48,633 - INFO - validation batch 101, loss: 0.305, 3232/10016 datapoints
2025-03-06 19:10:48,828 - INFO - validation batch 151, loss: 0.367, 4832/10016 datapoints
2025-03-06 19:10:49,002 - INFO - validation batch 201, loss: 0.624, 6432/10016 datapoints
2025-03-06 19:10:49,176 - INFO - validation batch 251, loss: 0.288, 8032/10016 datapoints
2025-03-06 19:10:49,343 - INFO - validation batch 301, loss: 0.236, 9632/10016 datapoints
2025-03-06 19:10:49,387 - INFO - Epoch 226/800 done.
2025-03-06 19:10:49,388 - INFO - Final validation performance:
Loss: 0.355, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 19:10:49,388 - INFO - Beginning epoch 227/800
2025-03-06 19:10:49,395 - INFO - training batch 1, loss: 0.264, 32/60000 datapoints
2025-03-06 19:10:49,650 - INFO - training batch 51, loss: 0.451, 1632/60000 datapoints
2025-03-06 19:10:49,878 - INFO - training batch 101, loss: 0.365, 3232/60000 datapoints
2025-03-06 19:10:50,103 - INFO - training batch 151, loss: 0.283, 4832/60000 datapoints
2025-03-06 19:10:50,337 - INFO - training batch 201, loss: 0.353, 6432/60000 datapoints
2025-03-06 19:10:50,558 - INFO - training batch 251, loss: 0.197, 8032/60000 datapoints
2025-03-06 19:10:50,783 - INFO - training batch 301, loss: 0.449, 9632/60000 datapoints
2025-03-06 19:10:51,003 - INFO - training batch 351, loss: 0.386, 11232/60000 datapoints
2025-03-06 19:10:51,229 - INFO - training batch 401, loss: 0.535, 12832/60000 datapoints
2025-03-06 19:10:51,448 - INFO - training batch 451, loss: 0.198, 14432/60000 datapoints
2025-03-06 19:10:51,673 - INFO - training batch 501, loss: 0.234, 16032/60000 datapoints
2025-03-06 19:10:51,897 - INFO - training batch 551, loss: 0.376, 17632/60000 datapoints
2025-03-06 19:10:52,132 - INFO - training batch 601, loss: 0.126, 19232/60000 datapoints
2025-03-06 19:10:52,351 - INFO - training batch 651, loss: 0.180, 20832/60000 datapoints
2025-03-06 19:10:52,572 - INFO - training batch 701, loss: 0.334, 22432/60000 datapoints
2025-03-06 19:10:52,800 - INFO - training batch 751, loss: 0.467, 24032/60000 datapoints
2025-03-06 19:10:53,045 - INFO - training batch 801, loss: 0.253, 25632/60000 datapoints
2025-03-06 19:10:53,288 - INFO - training batch 851, loss: 0.468, 27232/60000 datapoints
2025-03-06 19:10:53,511 - INFO - training batch 901, loss: 0.253, 28832/60000 datapoints
2025-03-06 19:10:53,736 - INFO - training batch 951, loss: 0.459, 30432/60000 datapoints
2025-03-06 19:10:53,960 - INFO - training batch 1001, loss: 0.378, 32032/60000 datapoints
2025-03-06 19:10:54,180 - INFO - training batch 1051, loss: 0.370, 33632/60000 datapoints
2025-03-06 19:10:54,394 - INFO - training batch 1101, loss: 0.221, 35232/60000 datapoints
2025-03-06 19:10:54,628 - INFO - training batch 1151, loss: 0.398, 36832/60000 datapoints
2025-03-06 19:10:54,855 - INFO - training batch 1201, loss: 0.180, 38432/60000 datapoints
2025-03-06 19:10:55,105 - INFO - training batch 1251, loss: 0.231, 40032/60000 datapoints
2025-03-06 19:10:55,319 - INFO - training batch 1301, loss: 0.388, 41632/60000 datapoints
2025-03-06 19:10:55,534 - INFO - training batch 1351, loss: 0.393, 43232/60000 datapoints
2025-03-06 19:10:55,753 - INFO - training batch 1401, loss: 0.236, 44832/60000 datapoints
2025-03-06 19:10:55,968 - INFO - training batch 1451, loss: 0.236, 46432/60000 datapoints
2025-03-06 19:10:56,180 - INFO - training batch 1501, loss: 0.241, 48032/60000 datapoints
2025-03-06 19:10:56,387 - INFO - training batch 1551, loss: 0.314, 49632/60000 datapoints
2025-03-06 19:10:56,594 - INFO - training batch 1601, loss: 0.298, 51232/60000 datapoints
2025-03-06 19:10:56,802 - INFO - training batch 1651, loss: 0.451, 52832/60000 datapoints
2025-03-06 19:10:57,006 - INFO - training batch 1701, loss: 0.383, 54432/60000 datapoints
2025-03-06 19:10:57,213 - INFO - training batch 1751, loss: 0.181, 56032/60000 datapoints
2025-03-06 19:10:57,423 - INFO - training batch 1801, loss: 0.502, 57632/60000 datapoints
2025-03-06 19:10:57,631 - INFO - training batch 1851, loss: 0.313, 59232/60000 datapoints
2025-03-06 19:10:57,741 - INFO - validation batch 1, loss: 0.230, 32/10016 datapoints
2025-03-06 19:10:57,910 - INFO - validation batch 51, loss: 0.233, 1632/10016 datapoints
2025-03-06 19:10:58,075 - INFO - validation batch 101, loss: 0.290, 3232/10016 datapoints
2025-03-06 19:10:58,244 - INFO - validation batch 151, loss: 0.384, 4832/10016 datapoints
2025-03-06 19:10:58,411 - INFO - validation batch 201, loss: 0.314, 6432/10016 datapoints
2025-03-06 19:10:58,588 - INFO - validation batch 251, loss: 0.283, 8032/10016 datapoints
2025-03-06 19:10:58,770 - INFO - validation batch 301, loss: 0.217, 9632/10016 datapoints
2025-03-06 19:10:58,811 - INFO - Epoch 227/800 done.
2025-03-06 19:10:58,811 - INFO - Final validation performance:
Loss: 0.279, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 19:10:58,813 - INFO - Beginning epoch 228/800
2025-03-06 19:10:58,823 - INFO - training batch 1, loss: 0.196, 32/60000 datapoints
2025-03-06 19:10:59,031 - INFO - training batch 51, loss: 0.267, 1632/60000 datapoints
2025-03-06 19:10:59,232 - INFO - training batch 101, loss: 0.436, 3232/60000 datapoints
2025-03-06 19:10:59,442 - INFO - training batch 151, loss: 0.345, 4832/60000 datapoints
2025-03-06 19:10:59,639 - INFO - training batch 201, loss: 0.487, 6432/60000 datapoints
2025-03-06 19:10:59,847 - INFO - training batch 251, loss: 0.727, 8032/60000 datapoints
2025-03-06 19:11:00,054 - INFO - training batch 301, loss: 0.176, 9632/60000 datapoints
2025-03-06 19:11:00,256 - INFO - training batch 351, loss: 0.423, 11232/60000 datapoints
2025-03-06 19:11:00,458 - INFO - training batch 401, loss: 0.392, 12832/60000 datapoints
2025-03-06 19:11:00,655 - INFO - training batch 451, loss: 0.426, 14432/60000 datapoints
2025-03-06 19:11:00,851 - INFO - training batch 501, loss: 0.529, 16032/60000 datapoints
2025-03-06 19:11:01,046 - INFO - training batch 551, loss: 0.296, 17632/60000 datapoints
2025-03-06 19:11:01,241 - INFO - training batch 601, loss: 0.374, 19232/60000 datapoints
2025-03-06 19:11:01,436 - INFO - training batch 651, loss: 0.222, 20832/60000 datapoints
2025-03-06 19:11:01,639 - INFO - training batch 701, loss: 0.357, 22432/60000 datapoints
2025-03-06 19:11:01,839 - INFO - training batch 751, loss: 0.374, 24032/60000 datapoints
2025-03-06 19:11:02,036 - INFO - training batch 801, loss: 0.242, 25632/60000 datapoints
2025-03-06 19:11:02,234 - INFO - training batch 851, loss: 0.280, 27232/60000 datapoints
2025-03-06 19:11:02,432 - INFO - training batch 901, loss: 0.327, 28832/60000 datapoints
2025-03-06 19:11:02,634 - INFO - training batch 951, loss: 0.355, 30432/60000 datapoints
2025-03-06 19:11:02,830 - INFO - training batch 1001, loss: 0.517, 32032/60000 datapoints
2025-03-06 19:11:03,030 - INFO - training batch 1051, loss: 0.261, 33632/60000 datapoints
2025-03-06 19:11:03,234 - INFO - training batch 1101, loss: 0.428, 35232/60000 datapoints
2025-03-06 19:11:03,434 - INFO - training batch 1151, loss: 0.258, 36832/60000 datapoints
2025-03-06 19:11:03,636 - INFO - training batch 1201, loss: 0.391, 38432/60000 datapoints
2025-03-06 19:11:03,848 - INFO - training batch 1251, loss: 0.199, 40032/60000 datapoints
2025-03-06 19:11:04,047 - INFO - training batch 1301, loss: 0.133, 41632/60000 datapoints
2025-03-06 19:11:04,244 - INFO - training batch 1351, loss: 0.306, 43232/60000 datapoints
2025-03-06 19:11:04,438 - INFO - training batch 1401, loss: 0.219, 44832/60000 datapoints
2025-03-06 19:11:04,638 - INFO - training batch 1451, loss: 0.269, 46432/60000 datapoints
2025-03-06 19:11:04,887 - INFO - training batch 1501, loss: 0.225, 48032/60000 datapoints
2025-03-06 19:11:05,094 - INFO - training batch 1551, loss: 0.617, 49632/60000 datapoints
2025-03-06 19:11:05,315 - INFO - training batch 1601, loss: 0.277, 51232/60000 datapoints
2025-03-06 19:11:05,531 - INFO - training batch 1651, loss: 0.563, 52832/60000 datapoints
2025-03-06 19:11:05,738 - INFO - training batch 1701, loss: 0.150, 54432/60000 datapoints
2025-03-06 19:11:05,945 - INFO - training batch 1751, loss: 0.249, 56032/60000 datapoints
2025-03-06 19:11:06,150 - INFO - training batch 1801, loss: 0.280, 57632/60000 datapoints
2025-03-06 19:11:06,358 - INFO - training batch 1851, loss: 0.315, 59232/60000 datapoints
2025-03-06 19:11:06,469 - INFO - validation batch 1, loss: 0.437, 32/10016 datapoints
2025-03-06 19:11:06,638 - INFO - validation batch 51, loss: 0.187, 1632/10016 datapoints
2025-03-06 19:11:06,805 - INFO - validation batch 101, loss: 0.341, 3232/10016 datapoints
2025-03-06 19:11:06,979 - INFO - validation batch 151, loss: 0.362, 4832/10016 datapoints
2025-03-06 19:11:07,152 - INFO - validation batch 201, loss: 0.262, 6432/10016 datapoints
2025-03-06 19:11:07,323 - INFO - validation batch 251, loss: 0.312, 8032/10016 datapoints
2025-03-06 19:11:07,495 - INFO - validation batch 301, loss: 0.132, 9632/10016 datapoints
2025-03-06 19:11:07,540 - INFO - Epoch 228/800 done.
2025-03-06 19:11:07,540 - INFO - Final validation performance:
Loss: 0.291, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 19:11:07,541 - INFO - Beginning epoch 229/800
2025-03-06 19:11:07,548 - INFO - training batch 1, loss: 0.229, 32/60000 datapoints
2025-03-06 19:11:07,785 - INFO - training batch 51, loss: 0.540, 1632/60000 datapoints
2025-03-06 19:11:08,003 - INFO - training batch 101, loss: 0.357, 3232/60000 datapoints
2025-03-06 19:11:08,227 - INFO - training batch 151, loss: 0.226, 4832/60000 datapoints
2025-03-06 19:11:08,442 - INFO - training batch 201, loss: 0.544, 6432/60000 datapoints
2025-03-06 19:11:08,658 - INFO - training batch 251, loss: 0.299, 8032/60000 datapoints
2025-03-06 19:11:08,877 - INFO - training batch 301, loss: 0.498, 9632/60000 datapoints
2025-03-06 19:11:09,094 - INFO - training batch 351, loss: 0.189, 11232/60000 datapoints
2025-03-06 19:11:09,307 - INFO - training batch 401, loss: 0.605, 12832/60000 datapoints
2025-03-06 19:11:09,520 - INFO - training batch 451, loss: 0.271, 14432/60000 datapoints
2025-03-06 19:11:09,738 - INFO - training batch 501, loss: 0.484, 16032/60000 datapoints
2025-03-06 19:11:09,956 - INFO - training batch 551, loss: 0.387, 17632/60000 datapoints
2025-03-06 19:11:10,167 - INFO - training batch 601, loss: 0.531, 19232/60000 datapoints
2025-03-06 19:11:10,378 - INFO - training batch 651, loss: 0.270, 20832/60000 datapoints
2025-03-06 19:11:10,591 - INFO - training batch 701, loss: 0.375, 22432/60000 datapoints
2025-03-06 19:11:10,803 - INFO - training batch 751, loss: 0.436, 24032/60000 datapoints
2025-03-06 19:11:11,016 - INFO - training batch 801, loss: 0.376, 25632/60000 datapoints
2025-03-06 19:11:11,230 - INFO - training batch 851, loss: 0.445, 27232/60000 datapoints
2025-03-06 19:11:11,440 - INFO - training batch 901, loss: 0.350, 28832/60000 datapoints
2025-03-06 19:11:11,657 - INFO - training batch 951, loss: 0.312, 30432/60000 datapoints
2025-03-06 19:11:11,881 - INFO - training batch 1001, loss: 0.349, 32032/60000 datapoints
2025-03-06 19:11:12,099 - INFO - training batch 1051, loss: 0.381, 33632/60000 datapoints
2025-03-06 19:11:12,308 - INFO - training batch 1101, loss: 0.317, 35232/60000 datapoints
2025-03-06 19:11:12,519 - INFO - training batch 1151, loss: 0.434, 36832/60000 datapoints
2025-03-06 19:11:12,733 - INFO - training batch 1201, loss: 0.168, 38432/60000 datapoints
2025-03-06 19:11:12,944 - INFO - training batch 1251, loss: 0.330, 40032/60000 datapoints
2025-03-06 19:11:13,159 - INFO - training batch 1301, loss: 0.480, 41632/60000 datapoints
2025-03-06 19:11:13,384 - INFO - training batch 1351, loss: 0.352, 43232/60000 datapoints
2025-03-06 19:11:13,621 - INFO - training batch 1401, loss: 0.268, 44832/60000 datapoints
2025-03-06 19:11:13,837 - INFO - training batch 1451, loss: 0.261, 46432/60000 datapoints
2025-03-06 19:11:14,053 - INFO - training batch 1501, loss: 0.393, 48032/60000 datapoints
2025-03-06 19:11:14,279 - INFO - training batch 1551, loss: 0.361, 49632/60000 datapoints
2025-03-06 19:11:14,492 - INFO - training batch 1601, loss: 0.498, 51232/60000 datapoints
2025-03-06 19:11:14,709 - INFO - training batch 1651, loss: 0.192, 52832/60000 datapoints
2025-03-06 19:11:14,930 - INFO - training batch 1701, loss: 0.213, 54432/60000 datapoints
2025-03-06 19:11:15,145 - INFO - training batch 1751, loss: 0.148, 56032/60000 datapoints
2025-03-06 19:11:15,389 - INFO - training batch 1801, loss: 0.277, 57632/60000 datapoints
2025-03-06 19:11:15,600 - INFO - training batch 1851, loss: 0.723, 59232/60000 datapoints
2025-03-06 19:11:15,717 - INFO - validation batch 1, loss: 0.393, 32/10016 datapoints
2025-03-06 19:11:15,893 - INFO - validation batch 51, loss: 0.603, 1632/10016 datapoints
2025-03-06 19:11:16,075 - INFO - validation batch 101, loss: 0.470, 3232/10016 datapoints
2025-03-06 19:11:16,253 - INFO - validation batch 151, loss: 0.275, 4832/10016 datapoints
2025-03-06 19:11:16,425 - INFO - validation batch 201, loss: 0.582, 6432/10016 datapoints
2025-03-06 19:11:16,594 - INFO - validation batch 251, loss: 0.482, 8032/10016 datapoints
2025-03-06 19:11:16,770 - INFO - validation batch 301, loss: 0.293, 9632/10016 datapoints
2025-03-06 19:11:16,821 - INFO - Epoch 229/800 done.
2025-03-06 19:11:16,822 - INFO - Final validation performance:
Loss: 0.443, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 19:11:16,822 - INFO - Beginning epoch 230/800
2025-03-06 19:11:16,829 - INFO - training batch 1, loss: 0.295, 32/60000 datapoints
2025-03-06 19:11:17,041 - INFO - training batch 51, loss: 0.451, 1632/60000 datapoints
2025-03-06 19:11:17,252 - INFO - training batch 101, loss: 0.230, 3232/60000 datapoints
2025-03-06 19:11:17,496 - INFO - training batch 151, loss: 0.484, 4832/60000 datapoints
2025-03-06 19:11:17,742 - INFO - training batch 201, loss: 0.470, 6432/60000 datapoints
2025-03-06 19:11:17,967 - INFO - training batch 251, loss: 0.322, 8032/60000 datapoints
2025-03-06 19:11:18,211 - INFO - training batch 301, loss: 0.179, 9632/60000 datapoints
2025-03-06 19:11:18,456 - INFO - training batch 351, loss: 0.274, 11232/60000 datapoints
2025-03-06 19:11:18,686 - INFO - training batch 401, loss: 0.313, 12832/60000 datapoints
2025-03-06 19:11:18,896 - INFO - training batch 451, loss: 0.313, 14432/60000 datapoints
2025-03-06 19:11:19,108 - INFO - training batch 501, loss: 0.664, 16032/60000 datapoints
2025-03-06 19:11:19,326 - INFO - training batch 551, loss: 0.249, 17632/60000 datapoints
2025-03-06 19:11:19,537 - INFO - training batch 601, loss: 0.700, 19232/60000 datapoints
2025-03-06 19:11:19,756 - INFO - training batch 651, loss: 0.221, 20832/60000 datapoints
2025-03-06 19:11:19,977 - INFO - training batch 701, loss: 0.183, 22432/60000 datapoints
2025-03-06 19:11:20,188 - INFO - training batch 751, loss: 0.134, 24032/60000 datapoints
2025-03-06 19:11:20,402 - INFO - training batch 801, loss: 0.203, 25632/60000 datapoints
2025-03-06 19:11:20,617 - INFO - training batch 851, loss: 0.260, 27232/60000 datapoints
2025-03-06 19:11:20,828 - INFO - training batch 901, loss: 0.336, 28832/60000 datapoints
2025-03-06 19:11:21,041 - INFO - training batch 951, loss: 0.372, 30432/60000 datapoints
2025-03-06 19:11:21,253 - INFO - training batch 1001, loss: 0.180, 32032/60000 datapoints
2025-03-06 19:11:21,466 - INFO - training batch 1051, loss: 0.472, 33632/60000 datapoints
2025-03-06 19:11:21,679 - INFO - training batch 1101, loss: 0.621, 35232/60000 datapoints
2025-03-06 19:11:21,895 - INFO - training batch 1151, loss: 0.321, 36832/60000 datapoints
2025-03-06 19:11:22,104 - INFO - training batch 1201, loss: 0.338, 38432/60000 datapoints
2025-03-06 19:11:22,309 - INFO - training batch 1251, loss: 0.197, 40032/60000 datapoints
2025-03-06 19:11:22,515 - INFO - training batch 1301, loss: 0.327, 41632/60000 datapoints
2025-03-06 19:11:22,720 - INFO - training batch 1351, loss: 0.277, 43232/60000 datapoints
2025-03-06 19:11:22,929 - INFO - training batch 1401, loss: 0.311, 44832/60000 datapoints
2025-03-06 19:11:23,136 - INFO - training batch 1451, loss: 0.245, 46432/60000 datapoints
2025-03-06 19:11:23,344 - INFO - training batch 1501, loss: 0.298, 48032/60000 datapoints
2025-03-06 19:11:23,548 - INFO - training batch 1551, loss: 0.184, 49632/60000 datapoints
2025-03-06 19:11:23,757 - INFO - training batch 1601, loss: 0.287, 51232/60000 datapoints
2025-03-06 19:11:23,968 - INFO - training batch 1651, loss: 0.154, 52832/60000 datapoints
2025-03-06 19:11:24,176 - INFO - training batch 1701, loss: 0.239, 54432/60000 datapoints
2025-03-06 19:11:24,382 - INFO - training batch 1751, loss: 0.339, 56032/60000 datapoints
2025-03-06 19:11:24,584 - INFO - training batch 1801, loss: 0.401, 57632/60000 datapoints
2025-03-06 19:11:24,791 - INFO - training batch 1851, loss: 0.198, 59232/60000 datapoints
2025-03-06 19:11:24,915 - INFO - validation batch 1, loss: 0.158, 32/10016 datapoints
2025-03-06 19:11:25,090 - INFO - validation batch 51, loss: 0.302, 1632/10016 datapoints
2025-03-06 19:11:25,259 - INFO - validation batch 101, loss: 0.500, 3232/10016 datapoints
2025-03-06 19:11:25,454 - INFO - validation batch 151, loss: 0.420, 4832/10016 datapoints
2025-03-06 19:11:25,624 - INFO - validation batch 201, loss: 0.206, 6432/10016 datapoints
2025-03-06 19:11:25,790 - INFO - validation batch 251, loss: 0.323, 8032/10016 datapoints
2025-03-06 19:11:25,961 - INFO - validation batch 301, loss: 0.622, 9632/10016 datapoints
2025-03-06 19:11:26,001 - INFO - Epoch 230/800 done.
2025-03-06 19:11:26,001 - INFO - Final validation performance:
Loss: 0.361, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 19:11:26,002 - INFO - Beginning epoch 231/800
2025-03-06 19:11:26,013 - INFO - training batch 1, loss: 0.387, 32/60000 datapoints
2025-03-06 19:11:26,245 - INFO - training batch 51, loss: 0.263, 1632/60000 datapoints
2025-03-06 19:11:26,451 - INFO - training batch 101, loss: 0.229, 3232/60000 datapoints
2025-03-06 19:11:26,671 - INFO - training batch 151, loss: 0.259, 4832/60000 datapoints
2025-03-06 19:11:26,895 - INFO - training batch 201, loss: 0.147, 6432/60000 datapoints
2025-03-06 19:11:27,117 - INFO - training batch 251, loss: 0.375, 8032/60000 datapoints
2025-03-06 19:11:27,331 - INFO - training batch 301, loss: 0.284, 9632/60000 datapoints
2025-03-06 19:11:27,552 - INFO - training batch 351, loss: 0.263, 11232/60000 datapoints
2025-03-06 19:11:27,783 - INFO - training batch 401, loss: 0.415, 12832/60000 datapoints
2025-03-06 19:11:28,010 - INFO - training batch 451, loss: 0.250, 14432/60000 datapoints
2025-03-06 19:11:28,216 - INFO - training batch 501, loss: 0.356, 16032/60000 datapoints
2025-03-06 19:11:28,423 - INFO - training batch 551, loss: 0.400, 17632/60000 datapoints
2025-03-06 19:11:28,630 - INFO - training batch 601, loss: 0.271, 19232/60000 datapoints
2025-03-06 19:11:28,865 - INFO - training batch 651, loss: 0.356, 20832/60000 datapoints
2025-03-06 19:11:29,078 - INFO - training batch 701, loss: 0.187, 22432/60000 datapoints
2025-03-06 19:11:29,290 - INFO - training batch 751, loss: 0.359, 24032/60000 datapoints
2025-03-06 19:11:29,505 - INFO - training batch 801, loss: 0.261, 25632/60000 datapoints
2025-03-06 19:11:29,712 - INFO - training batch 851, loss: 0.189, 27232/60000 datapoints
2025-03-06 19:11:29,929 - INFO - training batch 901, loss: 0.128, 28832/60000 datapoints
2025-03-06 19:11:30,140 - INFO - training batch 951, loss: 0.416, 30432/60000 datapoints
2025-03-06 19:11:30,347 - INFO - training batch 1001, loss: 0.153, 32032/60000 datapoints
2025-03-06 19:11:30,559 - INFO - training batch 1051, loss: 0.407, 33632/60000 datapoints
2025-03-06 19:11:30,767 - INFO - training batch 1101, loss: 0.298, 35232/60000 datapoints
2025-03-06 19:11:30,977 - INFO - training batch 1151, loss: 0.178, 36832/60000 datapoints
2025-03-06 19:11:31,186 - INFO - training batch 1201, loss: 0.408, 38432/60000 datapoints
2025-03-06 19:11:31,394 - INFO - training batch 1251, loss: 0.464, 40032/60000 datapoints
2025-03-06 19:11:31,597 - INFO - training batch 1301, loss: 0.415, 41632/60000 datapoints
2025-03-06 19:11:31,807 - INFO - training batch 1351, loss: 0.454, 43232/60000 datapoints
2025-03-06 19:11:32,021 - INFO - training batch 1401, loss: 0.365, 44832/60000 datapoints
2025-03-06 19:11:32,226 - INFO - training batch 1451, loss: 0.232, 46432/60000 datapoints
2025-03-06 19:11:32,434 - INFO - training batch 1501, loss: 0.308, 48032/60000 datapoints
2025-03-06 19:11:32,640 - INFO - training batch 1551, loss: 0.433, 49632/60000 datapoints
2025-03-06 19:11:32,848 - INFO - training batch 1601, loss: 0.231, 51232/60000 datapoints
2025-03-06 19:11:33,064 - INFO - training batch 1651, loss: 0.145, 52832/60000 datapoints
2025-03-06 19:11:33,279 - INFO - training batch 1701, loss: 0.375, 54432/60000 datapoints
2025-03-06 19:11:33,491 - INFO - training batch 1751, loss: 0.674, 56032/60000 datapoints
2025-03-06 19:11:33,705 - INFO - training batch 1801, loss: 0.302, 57632/60000 datapoints
2025-03-06 19:11:33,920 - INFO - training batch 1851, loss: 0.222, 59232/60000 datapoints
2025-03-06 19:11:34,032 - INFO - validation batch 1, loss: 0.206, 32/10016 datapoints
2025-03-06 19:11:34,201 - INFO - validation batch 51, loss: 0.239, 1632/10016 datapoints
2025-03-06 19:11:34,374 - INFO - validation batch 101, loss: 0.275, 3232/10016 datapoints
2025-03-06 19:11:34,546 - INFO - validation batch 151, loss: 0.399, 4832/10016 datapoints
2025-03-06 19:11:34,725 - INFO - validation batch 201, loss: 0.445, 6432/10016 datapoints
2025-03-06 19:11:34,905 - INFO - validation batch 251, loss: 0.177, 8032/10016 datapoints
2025-03-06 19:11:35,079 - INFO - validation batch 301, loss: 0.374, 9632/10016 datapoints
2025-03-06 19:11:35,124 - INFO - Epoch 231/800 done.
2025-03-06 19:11:35,124 - INFO - Final validation performance:
Loss: 0.302, top-1 acc: 0.906top-5 acc: 0.906
2025-03-06 19:11:35,125 - INFO - Beginning epoch 232/800
2025-03-06 19:11:35,133 - INFO - training batch 1, loss: 0.422, 32/60000 datapoints
2025-03-06 19:11:35,354 - INFO - training batch 51, loss: 0.411, 1632/60000 datapoints
2025-03-06 19:11:35,610 - INFO - training batch 101, loss: 0.120, 3232/60000 datapoints
2025-03-06 19:11:35,823 - INFO - training batch 151, loss: 0.344, 4832/60000 datapoints
2025-03-06 19:11:36,051 - INFO - training batch 201, loss: 0.212, 6432/60000 datapoints
2025-03-06 19:11:36,269 - INFO - training batch 251, loss: 0.409, 8032/60000 datapoints
2025-03-06 19:11:36,481 - INFO - training batch 301, loss: 0.353, 9632/60000 datapoints
2025-03-06 19:11:36,693 - INFO - training batch 351, loss: 0.326, 11232/60000 datapoints
2025-03-06 19:11:36,905 - INFO - training batch 401, loss: 0.313, 12832/60000 datapoints
2025-03-06 19:11:37,118 - INFO - training batch 451, loss: 0.268, 14432/60000 datapoints
2025-03-06 19:11:37,329 - INFO - training batch 501, loss: 0.419, 16032/60000 datapoints
2025-03-06 19:11:37,542 - INFO - training batch 551, loss: 0.423, 17632/60000 datapoints
2025-03-06 19:11:37,772 - INFO - training batch 601, loss: 0.470, 19232/60000 datapoints
2025-03-06 19:11:37,991 - INFO - training batch 651, loss: 0.413, 20832/60000 datapoints
2025-03-06 19:11:38,206 - INFO - training batch 701, loss: 0.702, 22432/60000 datapoints
2025-03-06 19:11:38,418 - INFO - training batch 751, loss: 0.213, 24032/60000 datapoints
2025-03-06 19:11:38,633 - INFO - training batch 801, loss: 0.423, 25632/60000 datapoints
2025-03-06 19:11:38,854 - INFO - training batch 851, loss: 0.507, 27232/60000 datapoints
2025-03-06 19:11:39,074 - INFO - training batch 901, loss: 0.288, 28832/60000 datapoints
2025-03-06 19:11:39,287 - INFO - training batch 951, loss: 0.345, 30432/60000 datapoints
2025-03-06 19:11:39,503 - INFO - training batch 1001, loss: 0.299, 32032/60000 datapoints
2025-03-06 19:11:39,717 - INFO - training batch 1051, loss: 0.471, 33632/60000 datapoints
2025-03-06 19:11:39,933 - INFO - training batch 1101, loss: 0.160, 35232/60000 datapoints
2025-03-06 19:11:40,144 - INFO - training batch 1151, loss: 0.372, 36832/60000 datapoints
2025-03-06 19:11:40,355 - INFO - training batch 1201, loss: 0.366, 38432/60000 datapoints
2025-03-06 19:11:40,574 - INFO - training batch 1251, loss: 0.277, 40032/60000 datapoints
2025-03-06 19:11:40,786 - INFO - training batch 1301, loss: 0.554, 41632/60000 datapoints
2025-03-06 19:11:40,998 - INFO - training batch 1351, loss: 0.533, 43232/60000 datapoints
2025-03-06 19:11:41,210 - INFO - training batch 1401, loss: 0.259, 44832/60000 datapoints
2025-03-06 19:11:41,422 - INFO - training batch 1451, loss: 0.331, 46432/60000 datapoints
2025-03-06 19:11:41,637 - INFO - training batch 1501, loss: 0.589, 48032/60000 datapoints
2025-03-06 19:11:41,849 - INFO - training batch 1551, loss: 0.427, 49632/60000 datapoints
2025-03-06 19:11:42,068 - INFO - training batch 1601, loss: 0.419, 51232/60000 datapoints
2025-03-06 19:11:42,280 - INFO - training batch 1651, loss: 0.186, 52832/60000 datapoints
2025-03-06 19:11:42,492 - INFO - training batch 1701, loss: 0.283, 54432/60000 datapoints
2025-03-06 19:11:42,704 - INFO - training batch 1751, loss: 0.168, 56032/60000 datapoints
2025-03-06 19:11:42,916 - INFO - training batch 1801, loss: 0.379, 57632/60000 datapoints
2025-03-06 19:11:43,127 - INFO - training batch 1851, loss: 0.594, 59232/60000 datapoints
2025-03-06 19:11:43,240 - INFO - validation batch 1, loss: 0.118, 32/10016 datapoints
2025-03-06 19:11:43,410 - INFO - validation batch 51, loss: 0.324, 1632/10016 datapoints
2025-03-06 19:11:43,583 - INFO - validation batch 101, loss: 0.394, 3232/10016 datapoints
2025-03-06 19:11:43,759 - INFO - validation batch 151, loss: 0.332, 4832/10016 datapoints
2025-03-06 19:11:43,936 - INFO - validation batch 201, loss: 0.186, 6432/10016 datapoints
2025-03-06 19:11:44,112 - INFO - validation batch 251, loss: 0.277, 8032/10016 datapoints
2025-03-06 19:11:44,284 - INFO - validation batch 301, loss: 0.326, 9632/10016 datapoints
2025-03-06 19:11:44,329 - INFO - Epoch 232/800 done.
2025-03-06 19:11:44,329 - INFO - Final validation performance:
Loss: 0.280, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 19:11:44,330 - INFO - Beginning epoch 233/800
2025-03-06 19:11:44,337 - INFO - training batch 1, loss: 0.288, 32/60000 datapoints
2025-03-06 19:11:44,570 - INFO - training batch 51, loss: 0.259, 1632/60000 datapoints
2025-03-06 19:11:44,781 - INFO - training batch 101, loss: 0.343, 3232/60000 datapoints
2025-03-06 19:11:45,000 - INFO - training batch 151, loss: 0.250, 4832/60000 datapoints
2025-03-06 19:11:45,213 - INFO - training batch 201, loss: 0.348, 6432/60000 datapoints
2025-03-06 19:11:45,417 - INFO - training batch 251, loss: 0.391, 8032/60000 datapoints
2025-03-06 19:11:45,657 - INFO - training batch 301, loss: 0.276, 9632/60000 datapoints
2025-03-06 19:11:45,862 - INFO - training batch 351, loss: 0.272, 11232/60000 datapoints
2025-03-06 19:11:46,070 - INFO - training batch 401, loss: 0.292, 12832/60000 datapoints
2025-03-06 19:11:46,282 - INFO - training batch 451, loss: 0.456, 14432/60000 datapoints
2025-03-06 19:11:46,488 - INFO - training batch 501, loss: 0.435, 16032/60000 datapoints
2025-03-06 19:11:46,696 - INFO - training batch 551, loss: 0.308, 17632/60000 datapoints
2025-03-06 19:11:46,903 - INFO - training batch 601, loss: 0.387, 19232/60000 datapoints
2025-03-06 19:11:47,114 - INFO - training batch 651, loss: 0.306, 20832/60000 datapoints
2025-03-06 19:11:47,318 - INFO - training batch 701, loss: 0.456, 22432/60000 datapoints
2025-03-06 19:11:47,526 - INFO - training batch 751, loss: 0.361, 24032/60000 datapoints
2025-03-06 19:11:47,778 - INFO - training batch 801, loss: 0.405, 25632/60000 datapoints
2025-03-06 19:11:48,155 - INFO - training batch 851, loss: 0.273, 27232/60000 datapoints
2025-03-06 19:11:48,435 - INFO - training batch 901, loss: 0.170, 28832/60000 datapoints
2025-03-06 19:11:48,662 - INFO - training batch 951, loss: 0.214, 30432/60000 datapoints
2025-03-06 19:11:48,899 - INFO - training batch 1001, loss: 0.400, 32032/60000 datapoints
2025-03-06 19:11:49,164 - INFO - training batch 1051, loss: 0.577, 33632/60000 datapoints
2025-03-06 19:11:49,417 - INFO - training batch 1101, loss: 0.311, 35232/60000 datapoints
2025-03-06 19:11:49,684 - INFO - training batch 1151, loss: 0.303, 36832/60000 datapoints
2025-03-06 19:11:49,943 - INFO - training batch 1201, loss: 0.132, 38432/60000 datapoints
2025-03-06 19:11:50,201 - INFO - training batch 1251, loss: 0.284, 40032/60000 datapoints
2025-03-06 19:11:50,454 - INFO - training batch 1301, loss: 0.427, 41632/60000 datapoints
2025-03-06 19:11:50,719 - INFO - training batch 1351, loss: 0.318, 43232/60000 datapoints
2025-03-06 19:11:50,984 - INFO - training batch 1401, loss: 0.278, 44832/60000 datapoints
2025-03-06 19:11:51,282 - INFO - training batch 1451, loss: 0.128, 46432/60000 datapoints
2025-03-06 19:11:51,579 - INFO - training batch 1501, loss: 0.490, 48032/60000 datapoints
2025-03-06 19:11:51,860 - INFO - training batch 1551, loss: 0.204, 49632/60000 datapoints
2025-03-06 19:11:52,099 - INFO - training batch 1601, loss: 0.212, 51232/60000 datapoints
2025-03-06 19:11:52,333 - INFO - training batch 1651, loss: 0.278, 52832/60000 datapoints
2025-03-06 19:11:52,571 - INFO - training batch 1701, loss: 0.551, 54432/60000 datapoints
2025-03-06 19:11:52,813 - INFO - training batch 1751, loss: 0.394, 56032/60000 datapoints
2025-03-06 19:11:53,051 - INFO - training batch 1801, loss: 0.271, 57632/60000 datapoints
2025-03-06 19:11:53,285 - INFO - training batch 1851, loss: 0.242, 59232/60000 datapoints
2025-03-06 19:11:53,407 - INFO - validation batch 1, loss: 0.231, 32/10016 datapoints
2025-03-06 19:11:53,606 - INFO - validation batch 51, loss: 0.315, 1632/10016 datapoints
2025-03-06 19:11:53,796 - INFO - validation batch 101, loss: 0.436, 3232/10016 datapoints
2025-03-06 19:11:53,983 - INFO - validation batch 151, loss: 0.305, 4832/10016 datapoints
2025-03-06 19:11:54,194 - INFO - validation batch 201, loss: 0.524, 6432/10016 datapoints
2025-03-06 19:11:54,413 - INFO - validation batch 251, loss: 0.417, 8032/10016 datapoints
2025-03-06 19:11:54,639 - INFO - validation batch 301, loss: 0.248, 9632/10016 datapoints
2025-03-06 19:11:54,699 - INFO - Epoch 233/800 done.
2025-03-06 19:11:54,700 - INFO - Final validation performance:
Loss: 0.354, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 19:11:54,701 - INFO - Beginning epoch 234/800
2025-03-06 19:11:54,711 - INFO - training batch 1, loss: 0.596, 32/60000 datapoints
2025-03-06 19:11:55,009 - INFO - training batch 51, loss: 0.285, 1632/60000 datapoints
2025-03-06 19:11:55,301 - INFO - training batch 101, loss: 0.323, 3232/60000 datapoints
2025-03-06 19:11:55,576 - INFO - training batch 151, loss: 0.226, 4832/60000 datapoints
2025-03-06 19:11:55,876 - INFO - training batch 201, loss: 0.395, 6432/60000 datapoints
2025-03-06 19:11:56,142 - INFO - training batch 251, loss: 0.584, 8032/60000 datapoints
2025-03-06 19:11:56,486 - INFO - training batch 301, loss: 0.533, 9632/60000 datapoints
2025-03-06 19:11:56,803 - INFO - training batch 351, loss: 0.448, 11232/60000 datapoints
2025-03-06 19:11:57,129 - INFO - training batch 401, loss: 0.481, 12832/60000 datapoints
2025-03-06 19:11:57,584 - INFO - training batch 451, loss: 0.493, 14432/60000 datapoints
2025-03-06 19:11:57,843 - INFO - training batch 501, loss: 0.370, 16032/60000 datapoints
2025-03-06 19:11:58,111 - INFO - training batch 551, loss: 0.684, 17632/60000 datapoints
2025-03-06 19:11:58,373 - INFO - training batch 601, loss: 0.350, 19232/60000 datapoints
2025-03-06 19:11:58,620 - INFO - training batch 651, loss: 0.317, 20832/60000 datapoints
2025-03-06 19:11:58,862 - INFO - training batch 701, loss: 0.274, 22432/60000 datapoints
2025-03-06 19:11:59,120 - INFO - training batch 751, loss: 0.468, 24032/60000 datapoints
2025-03-06 19:11:59,382 - INFO - training batch 801, loss: 0.620, 25632/60000 datapoints
2025-03-06 19:11:59,648 - INFO - training batch 851, loss: 0.323, 27232/60000 datapoints
2025-03-06 19:11:59,910 - INFO - training batch 901, loss: 0.212, 28832/60000 datapoints
2025-03-06 19:12:00,152 - INFO - training batch 951, loss: 0.172, 30432/60000 datapoints
2025-03-06 19:12:00,381 - INFO - training batch 1001, loss: 0.303, 32032/60000 datapoints
2025-03-06 19:12:00,615 - INFO - training batch 1051, loss: 0.241, 33632/60000 datapoints
2025-03-06 19:12:00,847 - INFO - training batch 1101, loss: 0.334, 35232/60000 datapoints
2025-03-06 19:12:01,108 - INFO - training batch 1151, loss: 0.377, 36832/60000 datapoints
2025-03-06 19:12:01,350 - INFO - training batch 1201, loss: 0.536, 38432/60000 datapoints
2025-03-06 19:12:01,591 - INFO - training batch 1251, loss: 0.335, 40032/60000 datapoints
2025-03-06 19:12:01,845 - INFO - training batch 1301, loss: 0.177, 41632/60000 datapoints
2025-03-06 19:12:02,093 - INFO - training batch 1351, loss: 0.399, 43232/60000 datapoints
2025-03-06 19:12:02,382 - INFO - training batch 1401, loss: 0.504, 44832/60000 datapoints
2025-03-06 19:12:02,624 - INFO - training batch 1451, loss: 0.381, 46432/60000 datapoints
2025-03-06 19:12:02,866 - INFO - training batch 1501, loss: 0.500, 48032/60000 datapoints
2025-03-06 19:12:03,114 - INFO - training batch 1551, loss: 0.668, 49632/60000 datapoints
2025-03-06 19:12:03,449 - INFO - training batch 1601, loss: 0.397, 51232/60000 datapoints
2025-03-06 19:12:03,760 - INFO - training batch 1651, loss: 0.390, 52832/60000 datapoints
2025-03-06 19:12:03,991 - INFO - training batch 1701, loss: 0.224, 54432/60000 datapoints
2025-03-06 19:12:04,244 - INFO - training batch 1751, loss: 0.337, 56032/60000 datapoints
2025-03-06 19:12:04,671 - INFO - training batch 1801, loss: 0.254, 57632/60000 datapoints
2025-03-06 19:12:04,890 - INFO - training batch 1851, loss: 0.218, 59232/60000 datapoints
2025-03-06 19:12:05,008 - INFO - validation batch 1, loss: 0.218, 32/10016 datapoints
2025-03-06 19:12:05,195 - INFO - validation batch 51, loss: 0.245, 1632/10016 datapoints
2025-03-06 19:12:05,380 - INFO - validation batch 101, loss: 0.247, 3232/10016 datapoints
2025-03-06 19:12:05,574 - INFO - validation batch 151, loss: 0.550, 4832/10016 datapoints
2025-03-06 19:12:05,796 - INFO - validation batch 201, loss: 0.711, 6432/10016 datapoints
2025-03-06 19:12:05,996 - INFO - validation batch 251, loss: 0.336, 8032/10016 datapoints
2025-03-06 19:12:06,196 - INFO - validation batch 301, loss: 0.190, 9632/10016 datapoints
2025-03-06 19:12:06,250 - INFO - Epoch 234/800 done.
2025-03-06 19:12:06,251 - INFO - Final validation performance:
Loss: 0.357, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 19:12:06,252 - INFO - Beginning epoch 235/800
2025-03-06 19:12:06,262 - INFO - training batch 1, loss: 0.472, 32/60000 datapoints
2025-03-06 19:12:06,520 - INFO - training batch 51, loss: 0.530, 1632/60000 datapoints
2025-03-06 19:12:06,961 - INFO - training batch 101, loss: 0.339, 3232/60000 datapoints
2025-03-06 19:12:07,270 - INFO - training batch 151, loss: 0.615, 4832/60000 datapoints
2025-03-06 19:12:07,519 - INFO - training batch 201, loss: 0.332, 6432/60000 datapoints
2025-03-06 19:12:07,799 - INFO - training batch 251, loss: 0.363, 8032/60000 datapoints
2025-03-06 19:12:08,047 - INFO - training batch 301, loss: 0.570, 9632/60000 datapoints
2025-03-06 19:12:08,299 - INFO - training batch 351, loss: 0.238, 11232/60000 datapoints
2025-03-06 19:12:08,548 - INFO - training batch 401, loss: 0.351, 12832/60000 datapoints
2025-03-06 19:12:08,811 - INFO - training batch 451, loss: 0.088, 14432/60000 datapoints
2025-03-06 19:12:09,058 - INFO - training batch 501, loss: 0.291, 16032/60000 datapoints
2025-03-06 19:12:09,325 - INFO - training batch 551, loss: 0.290, 17632/60000 datapoints
2025-03-06 19:12:09,604 - INFO - training batch 601, loss: 0.605, 19232/60000 datapoints
2025-03-06 19:12:09,912 - INFO - training batch 651, loss: 0.254, 20832/60000 datapoints
2025-03-06 19:12:10,185 - INFO - training batch 701, loss: 0.300, 22432/60000 datapoints
2025-03-06 19:12:10,476 - INFO - training batch 751, loss: 0.374, 24032/60000 datapoints
2025-03-06 19:12:10,785 - INFO - training batch 801, loss: 0.258, 25632/60000 datapoints
2025-03-06 19:12:11,106 - INFO - training batch 851, loss: 0.309, 27232/60000 datapoints
2025-03-06 19:12:11,416 - INFO - training batch 901, loss: 0.325, 28832/60000 datapoints
2025-03-06 19:12:11,735 - INFO - training batch 951, loss: 0.422, 30432/60000 datapoints
2025-03-06 19:12:11,996 - INFO - training batch 1001, loss: 0.533, 32032/60000 datapoints
2025-03-06 19:12:12,345 - INFO - training batch 1051, loss: 0.247, 33632/60000 datapoints
2025-03-06 19:12:12,649 - INFO - training batch 1101, loss: 0.200, 35232/60000 datapoints
2025-03-06 19:12:12,911 - INFO - training batch 1151, loss: 0.277, 36832/60000 datapoints
2025-03-06 19:12:13,211 - INFO - training batch 1201, loss: 0.202, 38432/60000 datapoints
2025-03-06 19:12:13,447 - INFO - training batch 1251, loss: 0.148, 40032/60000 datapoints
2025-03-06 19:12:13,709 - INFO - training batch 1301, loss: 0.124, 41632/60000 datapoints
2025-03-06 19:12:13,973 - INFO - training batch 1351, loss: 0.479, 43232/60000 datapoints
2025-03-06 19:12:14,257 - INFO - training batch 1401, loss: 0.237, 44832/60000 datapoints
2025-03-06 19:12:14,493 - INFO - training batch 1451, loss: 0.336, 46432/60000 datapoints
2025-03-06 19:12:14,731 - INFO - training batch 1501, loss: 0.643, 48032/60000 datapoints
2025-03-06 19:12:14,979 - INFO - training batch 1551, loss: 0.264, 49632/60000 datapoints
2025-03-06 19:12:15,222 - INFO - training batch 1601, loss: 0.463, 51232/60000 datapoints
2025-03-06 19:12:15,487 - INFO - training batch 1651, loss: 0.343, 52832/60000 datapoints
2025-03-06 19:12:15,726 - INFO - training batch 1701, loss: 0.440, 54432/60000 datapoints
2025-03-06 19:12:15,993 - INFO - training batch 1751, loss: 0.278, 56032/60000 datapoints
2025-03-06 19:12:16,234 - INFO - training batch 1801, loss: 0.278, 57632/60000 datapoints
2025-03-06 19:12:16,479 - INFO - training batch 1851, loss: 0.388, 59232/60000 datapoints
2025-03-06 19:12:16,614 - INFO - validation batch 1, loss: 0.431, 32/10016 datapoints
2025-03-06 19:12:16,806 - INFO - validation batch 51, loss: 0.410, 1632/10016 datapoints
2025-03-06 19:12:16,995 - INFO - validation batch 101, loss: 0.188, 3232/10016 datapoints
2025-03-06 19:12:17,193 - INFO - validation batch 151, loss: 0.318, 4832/10016 datapoints
2025-03-06 19:12:17,381 - INFO - validation batch 201, loss: 0.312, 6432/10016 datapoints
2025-03-06 19:12:17,570 - INFO - validation batch 251, loss: 0.327, 8032/10016 datapoints
2025-03-06 19:12:17,766 - INFO - validation batch 301, loss: 0.363, 9632/10016 datapoints
2025-03-06 19:12:17,809 - INFO - Epoch 235/800 done.
2025-03-06 19:12:17,809 - INFO - Final validation performance:
Loss: 0.336, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 19:12:17,810 - INFO - Beginning epoch 236/800
2025-03-06 19:12:17,822 - INFO - training batch 1, loss: 0.280, 32/60000 datapoints
2025-03-06 19:12:18,050 - INFO - training batch 51, loss: 0.305, 1632/60000 datapoints
2025-03-06 19:12:18,311 - INFO - training batch 101, loss: 0.373, 3232/60000 datapoints
2025-03-06 19:12:18,551 - INFO - training batch 151, loss: 0.272, 4832/60000 datapoints
2025-03-06 19:12:18,772 - INFO - training batch 201, loss: 0.226, 6432/60000 datapoints
2025-03-06 19:12:19,014 - INFO - training batch 251, loss: 0.367, 8032/60000 datapoints
2025-03-06 19:12:19,308 - INFO - training batch 301, loss: 0.522, 9632/60000 datapoints
2025-03-06 19:12:19,558 - INFO - training batch 351, loss: 0.431, 11232/60000 datapoints
2025-03-06 19:12:19,816 - INFO - training batch 401, loss: 0.550, 12832/60000 datapoints
2025-03-06 19:12:20,101 - INFO - training batch 451, loss: 0.218, 14432/60000 datapoints
2025-03-06 19:12:20,357 - INFO - training batch 501, loss: 0.337, 16032/60000 datapoints
2025-03-06 19:12:20,592 - INFO - training batch 551, loss: 0.311, 17632/60000 datapoints
2025-03-06 19:12:20,831 - INFO - training batch 601, loss: 0.116, 19232/60000 datapoints
2025-03-06 19:12:21,063 - INFO - training batch 651, loss: 0.266, 20832/60000 datapoints
2025-03-06 19:12:21,302 - INFO - training batch 701, loss: 0.296, 22432/60000 datapoints
2025-03-06 19:12:21,532 - INFO - training batch 751, loss: 0.312, 24032/60000 datapoints
2025-03-06 19:12:21,775 - INFO - training batch 801, loss: 0.317, 25632/60000 datapoints
2025-03-06 19:12:22,029 - INFO - training batch 851, loss: 0.159, 27232/60000 datapoints
2025-03-06 19:12:22,279 - INFO - training batch 901, loss: 0.372, 28832/60000 datapoints
2025-03-06 19:12:22,526 - INFO - training batch 951, loss: 0.161, 30432/60000 datapoints
2025-03-06 19:12:22,745 - INFO - training batch 1001, loss: 0.486, 32032/60000 datapoints
2025-03-06 19:12:22,957 - INFO - training batch 1051, loss: 0.117, 33632/60000 datapoints
2025-03-06 19:12:23,205 - INFO - training batch 1101, loss: 0.292, 35232/60000 datapoints
2025-03-06 19:12:23,426 - INFO - training batch 1151, loss: 0.248, 36832/60000 datapoints
2025-03-06 19:12:23,649 - INFO - training batch 1201, loss: 0.365, 38432/60000 datapoints
2025-03-06 19:12:23,866 - INFO - training batch 1251, loss: 0.222, 40032/60000 datapoints
2025-03-06 19:12:24,088 - INFO - training batch 1301, loss: 0.302, 41632/60000 datapoints
2025-03-06 19:12:24,309 - INFO - training batch 1351, loss: 0.496, 43232/60000 datapoints
2025-03-06 19:12:24,532 - INFO - training batch 1401, loss: 0.315, 44832/60000 datapoints
2025-03-06 19:12:24,752 - INFO - training batch 1451, loss: 0.416, 46432/60000 datapoints
2025-03-06 19:12:24,978 - INFO - training batch 1501, loss: 0.372, 48032/60000 datapoints
2025-03-06 19:12:25,201 - INFO - training batch 1551, loss: 0.202, 49632/60000 datapoints
2025-03-06 19:12:25,421 - INFO - training batch 1601, loss: 0.340, 51232/60000 datapoints
2025-03-06 19:12:25,643 - INFO - training batch 1651, loss: 0.183, 52832/60000 datapoints
2025-03-06 19:12:25,860 - INFO - training batch 1701, loss: 0.480, 54432/60000 datapoints
2025-03-06 19:12:26,112 - INFO - training batch 1751, loss: 0.381, 56032/60000 datapoints
2025-03-06 19:12:26,333 - INFO - training batch 1801, loss: 0.465, 57632/60000 datapoints
2025-03-06 19:12:26,551 - INFO - training batch 1851, loss: 0.946, 59232/60000 datapoints
2025-03-06 19:12:26,670 - INFO - validation batch 1, loss: 0.258, 32/10016 datapoints
2025-03-06 19:12:26,847 - INFO - validation batch 51, loss: 0.417, 1632/10016 datapoints
2025-03-06 19:12:27,027 - INFO - validation batch 101, loss: 0.372, 3232/10016 datapoints
2025-03-06 19:12:27,204 - INFO - validation batch 151, loss: 0.477, 4832/10016 datapoints
2025-03-06 19:12:27,382 - INFO - validation batch 201, loss: 0.321, 6432/10016 datapoints
2025-03-06 19:12:27,558 - INFO - validation batch 251, loss: 0.193, 8032/10016 datapoints
2025-03-06 19:12:27,740 - INFO - validation batch 301, loss: 0.334, 9632/10016 datapoints
2025-03-06 19:12:27,786 - INFO - Epoch 236/800 done.
2025-03-06 19:12:27,786 - INFO - Final validation performance:
Loss: 0.339, top-1 acc: 0.907top-5 acc: 0.907
2025-03-06 19:12:27,787 - INFO - Beginning epoch 237/800
2025-03-06 19:12:27,794 - INFO - training batch 1, loss: 0.189, 32/60000 datapoints
2025-03-06 19:12:28,038 - INFO - training batch 51, loss: 0.435, 1632/60000 datapoints
2025-03-06 19:12:28,286 - INFO - training batch 101, loss: 0.431, 3232/60000 datapoints
2025-03-06 19:12:28,546 - INFO - training batch 151, loss: 0.320, 4832/60000 datapoints
2025-03-06 19:12:28,793 - INFO - training batch 201, loss: 0.359, 6432/60000 datapoints
2025-03-06 19:12:29,041 - INFO - training batch 251, loss: 0.387, 8032/60000 datapoints
2025-03-06 19:12:29,306 - INFO - training batch 301, loss: 0.263, 9632/60000 datapoints
2025-03-06 19:12:29,543 - INFO - training batch 351, loss: 0.378, 11232/60000 datapoints
2025-03-06 19:12:29,765 - INFO - training batch 401, loss: 0.277, 12832/60000 datapoints
2025-03-06 19:12:29,980 - INFO - training batch 451, loss: 0.642, 14432/60000 datapoints
2025-03-06 19:12:30,217 - INFO - training batch 501, loss: 0.391, 16032/60000 datapoints
2025-03-06 19:12:30,469 - INFO - training batch 551, loss: 0.317, 17632/60000 datapoints
2025-03-06 19:12:30,726 - INFO - training batch 601, loss: 0.399, 19232/60000 datapoints
2025-03-06 19:12:30,973 - INFO - training batch 651, loss: 0.113, 20832/60000 datapoints
2025-03-06 19:12:31,189 - INFO - training batch 701, loss: 0.329, 22432/60000 datapoints
2025-03-06 19:12:31,403 - INFO - training batch 751, loss: 0.361, 24032/60000 datapoints
2025-03-06 19:12:31,626 - INFO - training batch 801, loss: 0.344, 25632/60000 datapoints
2025-03-06 19:12:31,839 - INFO - training batch 851, loss: 0.195, 27232/60000 datapoints
2025-03-06 19:12:32,052 - INFO - training batch 901, loss: 0.142, 28832/60000 datapoints
2025-03-06 19:12:32,283 - INFO - training batch 951, loss: 0.636, 30432/60000 datapoints
2025-03-06 19:12:32,502 - INFO - training batch 1001, loss: 0.355, 32032/60000 datapoints
2025-03-06 19:12:32,717 - INFO - training batch 1051, loss: 0.159, 33632/60000 datapoints
2025-03-06 19:12:32,930 - INFO - training batch 1101, loss: 0.398, 35232/60000 datapoints
2025-03-06 19:12:33,140 - INFO - training batch 1151, loss: 0.317, 36832/60000 datapoints
2025-03-06 19:12:33,355 - INFO - training batch 1201, loss: 0.149, 38432/60000 datapoints
2025-03-06 19:12:33,568 - INFO - training batch 1251, loss: 0.481, 40032/60000 datapoints
2025-03-06 19:12:33,786 - INFO - training batch 1301, loss: 0.339, 41632/60000 datapoints
2025-03-06 19:12:34,001 - INFO - training batch 1351, loss: 0.262, 43232/60000 datapoints
2025-03-06 19:12:34,242 - INFO - training batch 1401, loss: 0.335, 44832/60000 datapoints
2025-03-06 19:12:34,480 - INFO - training batch 1451, loss: 0.267, 46432/60000 datapoints
2025-03-06 19:12:34,716 - INFO - training batch 1501, loss: 0.554, 48032/60000 datapoints
2025-03-06 19:12:34,933 - INFO - training batch 1551, loss: 0.280, 49632/60000 datapoints
2025-03-06 19:12:35,149 - INFO - training batch 1601, loss: 0.708, 51232/60000 datapoints
2025-03-06 19:12:35,386 - INFO - training batch 1651, loss: 0.272, 52832/60000 datapoints
2025-03-06 19:12:35,606 - INFO - training batch 1701, loss: 0.433, 54432/60000 datapoints
2025-03-06 19:12:35,845 - INFO - training batch 1751, loss: 0.246, 56032/60000 datapoints
2025-03-06 19:12:36,083 - INFO - training batch 1801, loss: 0.357, 57632/60000 datapoints
2025-03-06 19:12:36,308 - INFO - training batch 1851, loss: 0.385, 59232/60000 datapoints
2025-03-06 19:12:36,422 - INFO - validation batch 1, loss: 0.399, 32/10016 datapoints
2025-03-06 19:12:36,593 - INFO - validation batch 51, loss: 0.409, 1632/10016 datapoints
2025-03-06 19:12:36,765 - INFO - validation batch 101, loss: 0.227, 3232/10016 datapoints
2025-03-06 19:12:36,940 - INFO - validation batch 151, loss: 0.369, 4832/10016 datapoints
2025-03-06 19:12:37,111 - INFO - validation batch 201, loss: 0.206, 6432/10016 datapoints
2025-03-06 19:12:37,280 - INFO - validation batch 251, loss: 0.487, 8032/10016 datapoints
2025-03-06 19:12:37,448 - INFO - validation batch 301, loss: 0.375, 9632/10016 datapoints
2025-03-06 19:12:37,491 - INFO - Epoch 237/800 done.
2025-03-06 19:12:37,491 - INFO - Final validation performance:
Loss: 0.353, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 19:12:37,492 - INFO - Beginning epoch 238/800
2025-03-06 19:12:37,499 - INFO - training batch 1, loss: 0.440, 32/60000 datapoints
2025-03-06 19:12:37,765 - INFO - training batch 51, loss: 0.430, 1632/60000 datapoints
2025-03-06 19:12:37,970 - INFO - training batch 101, loss: 0.168, 3232/60000 datapoints
2025-03-06 19:12:38,184 - INFO - training batch 151, loss: 0.238, 4832/60000 datapoints
2025-03-06 19:12:38,386 - INFO - training batch 201, loss: 0.203, 6432/60000 datapoints
2025-03-06 19:12:38,592 - INFO - training batch 251, loss: 0.444, 8032/60000 datapoints
2025-03-06 19:12:38,795 - INFO - training batch 301, loss: 0.324, 9632/60000 datapoints
2025-03-06 19:12:39,006 - INFO - training batch 351, loss: 0.399, 11232/60000 datapoints
2025-03-06 19:12:39,208 - INFO - training batch 401, loss: 0.383, 12832/60000 datapoints
2025-03-06 19:12:39,416 - INFO - training batch 451, loss: 0.270, 14432/60000 datapoints
2025-03-06 19:12:39,626 - INFO - training batch 501, loss: 0.319, 16032/60000 datapoints
2025-03-06 19:12:39,829 - INFO - training batch 551, loss: 0.227, 17632/60000 datapoints
2025-03-06 19:12:40,036 - INFO - training batch 601, loss: 0.422, 19232/60000 datapoints
2025-03-06 19:12:40,242 - INFO - training batch 651, loss: 0.247, 20832/60000 datapoints
2025-03-06 19:12:40,450 - INFO - training batch 701, loss: 0.523, 22432/60000 datapoints
2025-03-06 19:12:40,655 - INFO - training batch 751, loss: 0.257, 24032/60000 datapoints
2025-03-06 19:12:40,861 - INFO - training batch 801, loss: 0.271, 25632/60000 datapoints
2025-03-06 19:12:41,069 - INFO - training batch 851, loss: 0.475, 27232/60000 datapoints
2025-03-06 19:12:41,277 - INFO - training batch 901, loss: 0.398, 28832/60000 datapoints
2025-03-06 19:12:41,488 - INFO - training batch 951, loss: 0.174, 30432/60000 datapoints
2025-03-06 19:12:41,701 - INFO - training batch 1001, loss: 0.392, 32032/60000 datapoints
2025-03-06 19:12:41,913 - INFO - training batch 1051, loss: 0.434, 33632/60000 datapoints
2025-03-06 19:12:42,133 - INFO - training batch 1101, loss: 0.381, 35232/60000 datapoints
2025-03-06 19:12:42,344 - INFO - training batch 1151, loss: 0.079, 36832/60000 datapoints
2025-03-06 19:12:42,558 - INFO - training batch 1201, loss: 0.557, 38432/60000 datapoints
2025-03-06 19:12:42,774 - INFO - training batch 1251, loss: 0.314, 40032/60000 datapoints
2025-03-06 19:12:43,025 - INFO - training batch 1301, loss: 0.304, 41632/60000 datapoints
2025-03-06 19:12:43,261 - INFO - training batch 1351, loss: 0.417, 43232/60000 datapoints
2025-03-06 19:12:43,501 - INFO - training batch 1401, loss: 0.451, 44832/60000 datapoints
2025-03-06 19:12:43,723 - INFO - training batch 1451, loss: 0.323, 46432/60000 datapoints
2025-03-06 19:12:43,942 - INFO - training batch 1501, loss: 0.255, 48032/60000 datapoints
2025-03-06 19:12:44,170 - INFO - training batch 1551, loss: 0.342, 49632/60000 datapoints
2025-03-06 19:12:44,393 - INFO - training batch 1601, loss: 0.206, 51232/60000 datapoints
2025-03-06 19:12:44,638 - INFO - training batch 1651, loss: 0.367, 52832/60000 datapoints
2025-03-06 19:12:44,868 - INFO - training batch 1701, loss: 0.433, 54432/60000 datapoints
2025-03-06 19:12:45,089 - INFO - training batch 1751, loss: 0.416, 56032/60000 datapoints
2025-03-06 19:12:45,306 - INFO - training batch 1801, loss: 0.138, 57632/60000 datapoints
2025-03-06 19:12:45,531 - INFO - training batch 1851, loss: 0.288, 59232/60000 datapoints
2025-03-06 19:12:45,652 - INFO - validation batch 1, loss: 0.258, 32/10016 datapoints
2025-03-06 19:12:45,830 - INFO - validation batch 51, loss: 0.592, 1632/10016 datapoints
2025-03-06 19:12:46,006 - INFO - validation batch 101, loss: 0.290, 3232/10016 datapoints
2025-03-06 19:12:46,215 - INFO - validation batch 151, loss: 0.290, 4832/10016 datapoints
2025-03-06 19:12:46,403 - INFO - validation batch 201, loss: 0.151, 6432/10016 datapoints
2025-03-06 19:12:46,581 - INFO - validation batch 251, loss: 0.166, 8032/10016 datapoints
2025-03-06 19:12:46,760 - INFO - validation batch 301, loss: 0.371, 9632/10016 datapoints
2025-03-06 19:12:46,803 - INFO - Epoch 238/800 done.
2025-03-06 19:12:46,803 - INFO - Final validation performance:
Loss: 0.303, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 19:12:46,804 - INFO - Beginning epoch 239/800
2025-03-06 19:12:46,817 - INFO - training batch 1, loss: 0.113, 32/60000 datapoints
2025-03-06 19:12:47,057 - INFO - training batch 51, loss: 0.283, 1632/60000 datapoints
2025-03-06 19:12:47,286 - INFO - training batch 101, loss: 0.404, 3232/60000 datapoints
2025-03-06 19:12:47,510 - INFO - training batch 151, loss: 0.474, 4832/60000 datapoints
2025-03-06 19:12:47,740 - INFO - training batch 201, loss: 0.323, 6432/60000 datapoints
2025-03-06 19:12:47,956 - INFO - training batch 251, loss: 0.391, 8032/60000 datapoints
2025-03-06 19:12:48,184 - INFO - training batch 301, loss: 0.322, 9632/60000 datapoints
2025-03-06 19:12:48,401 - INFO - training batch 351, loss: 0.270, 11232/60000 datapoints
2025-03-06 19:12:48,623 - INFO - training batch 401, loss: 0.615, 12832/60000 datapoints
2025-03-06 19:12:48,849 - INFO - training batch 451, loss: 0.229, 14432/60000 datapoints
2025-03-06 19:12:49,068 - INFO - training batch 501, loss: 0.352, 16032/60000 datapoints
2025-03-06 19:12:49,291 - INFO - training batch 551, loss: 0.335, 17632/60000 datapoints
2025-03-06 19:12:49,515 - INFO - training batch 601, loss: 0.277, 19232/60000 datapoints
2025-03-06 19:12:49,733 - INFO - training batch 651, loss: 0.466, 20832/60000 datapoints
2025-03-06 19:12:49,946 - INFO - training batch 701, loss: 0.330, 22432/60000 datapoints
2025-03-06 19:12:50,162 - INFO - training batch 751, loss: 0.328, 24032/60000 datapoints
2025-03-06 19:12:50,376 - INFO - training batch 801, loss: 0.666, 25632/60000 datapoints
2025-03-06 19:12:50,591 - INFO - training batch 851, loss: 0.407, 27232/60000 datapoints
2025-03-06 19:12:50,811 - INFO - training batch 901, loss: 0.300, 28832/60000 datapoints
2025-03-06 19:12:51,026 - INFO - training batch 951, loss: 0.360, 30432/60000 datapoints
2025-03-06 19:12:51,238 - INFO - training batch 1001, loss: 0.090, 32032/60000 datapoints
2025-03-06 19:12:51,450 - INFO - training batch 1051, loss: 0.432, 33632/60000 datapoints
2025-03-06 19:12:51,659 - INFO - training batch 1101, loss: 0.390, 35232/60000 datapoints
2025-03-06 19:12:51,887 - INFO - training batch 1151, loss: 0.320, 36832/60000 datapoints
2025-03-06 19:12:52,110 - INFO - training batch 1201, loss: 0.126, 38432/60000 datapoints
2025-03-06 19:12:52,327 - INFO - training batch 1251, loss: 0.694, 40032/60000 datapoints
2025-03-06 19:12:52,539 - INFO - training batch 1301, loss: 0.373, 41632/60000 datapoints
2025-03-06 19:12:52,759 - INFO - training batch 1351, loss: 0.461, 43232/60000 datapoints
2025-03-06 19:12:52,997 - INFO - training batch 1401, loss: 0.506, 44832/60000 datapoints
2025-03-06 19:12:53,253 - INFO - training batch 1451, loss: 0.152, 46432/60000 datapoints
2025-03-06 19:12:53,525 - INFO - training batch 1501, loss: 0.180, 48032/60000 datapoints
2025-03-06 19:12:53,800 - INFO - training batch 1551, loss: 0.246, 49632/60000 datapoints
2025-03-06 19:12:54,040 - INFO - training batch 1601, loss: 0.153, 51232/60000 datapoints
2025-03-06 19:12:54,262 - INFO - training batch 1651, loss: 0.678, 52832/60000 datapoints
2025-03-06 19:12:54,484 - INFO - training batch 1701, loss: 0.520, 54432/60000 datapoints
2025-03-06 19:12:54,696 - INFO - training batch 1751, loss: 0.252, 56032/60000 datapoints
2025-03-06 19:12:54,915 - INFO - training batch 1801, loss: 0.230, 57632/60000 datapoints
2025-03-06 19:12:55,127 - INFO - training batch 1851, loss: 0.486, 59232/60000 datapoints
2025-03-06 19:12:55,238 - INFO - validation batch 1, loss: 0.398, 32/10016 datapoints
2025-03-06 19:12:55,408 - INFO - validation batch 51, loss: 0.288, 1632/10016 datapoints
2025-03-06 19:12:55,587 - INFO - validation batch 101, loss: 0.265, 3232/10016 datapoints
2025-03-06 19:12:55,767 - INFO - validation batch 151, loss: 0.313, 4832/10016 datapoints
2025-03-06 19:12:55,940 - INFO - validation batch 201, loss: 0.077, 6432/10016 datapoints
2025-03-06 19:12:56,128 - INFO - validation batch 251, loss: 0.481, 8032/10016 datapoints
2025-03-06 19:12:56,332 - INFO - validation batch 301, loss: 0.244, 9632/10016 datapoints
2025-03-06 19:12:56,379 - INFO - Epoch 239/800 done.
2025-03-06 19:12:56,379 - INFO - Final validation performance:
Loss: 0.295, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 19:12:56,380 - INFO - Beginning epoch 240/800
2025-03-06 19:12:56,387 - INFO - training batch 1, loss: 0.267, 32/60000 datapoints
2025-03-06 19:12:56,609 - INFO - training batch 51, loss: 0.333, 1632/60000 datapoints
2025-03-06 19:12:56,847 - INFO - training batch 101, loss: 0.595, 3232/60000 datapoints
2025-03-06 19:12:57,085 - INFO - training batch 151, loss: 0.316, 4832/60000 datapoints
2025-03-06 19:12:57,319 - INFO - training batch 201, loss: 0.151, 6432/60000 datapoints
2025-03-06 19:12:57,562 - INFO - training batch 251, loss: 0.240, 8032/60000 datapoints
2025-03-06 19:12:57,807 - INFO - training batch 301, loss: 0.581, 9632/60000 datapoints
2025-03-06 19:12:58,043 - INFO - training batch 351, loss: 0.232, 11232/60000 datapoints
2025-03-06 19:12:58,276 - INFO - training batch 401, loss: 0.265, 12832/60000 datapoints
2025-03-06 19:12:58,495 - INFO - training batch 451, loss: 0.365, 14432/60000 datapoints
2025-03-06 19:12:58,722 - INFO - training batch 501, loss: 0.270, 16032/60000 datapoints
2025-03-06 19:12:58,954 - INFO - training batch 551, loss: 0.432, 17632/60000 datapoints
2025-03-06 19:12:59,175 - INFO - training batch 601, loss: 0.381, 19232/60000 datapoints
2025-03-06 19:12:59,399 - INFO - training batch 651, loss: 0.535, 20832/60000 datapoints
2025-03-06 19:12:59,647 - INFO - training batch 701, loss: 0.224, 22432/60000 datapoints
2025-03-06 19:12:59,897 - INFO - training batch 751, loss: 0.112, 24032/60000 datapoints
2025-03-06 19:13:00,143 - INFO - training batch 801, loss: 0.357, 25632/60000 datapoints
2025-03-06 19:13:00,380 - INFO - training batch 851, loss: 0.356, 27232/60000 datapoints
2025-03-06 19:13:00,625 - INFO - training batch 901, loss: 0.236, 28832/60000 datapoints
2025-03-06 19:13:00,856 - INFO - training batch 951, loss: 0.169, 30432/60000 datapoints
2025-03-06 19:13:01,079 - INFO - training batch 1001, loss: 0.301, 32032/60000 datapoints
2025-03-06 19:13:01,299 - INFO - training batch 1051, loss: 0.370, 33632/60000 datapoints
2025-03-06 19:13:01,523 - INFO - training batch 1101, loss: 0.105, 35232/60000 datapoints
2025-03-06 19:13:01,747 - INFO - training batch 1151, loss: 0.213, 36832/60000 datapoints
2025-03-06 19:13:01,967 - INFO - training batch 1201, loss: 0.325, 38432/60000 datapoints
2025-03-06 19:13:02,199 - INFO - training batch 1251, loss: 0.667, 40032/60000 datapoints
2025-03-06 19:13:02,443 - INFO - training batch 1301, loss: 0.288, 41632/60000 datapoints
2025-03-06 19:13:02,663 - INFO - training batch 1351, loss: 0.212, 43232/60000 datapoints
2025-03-06 19:13:02,883 - INFO - training batch 1401, loss: 0.305, 44832/60000 datapoints
2025-03-06 19:13:03,122 - INFO - training batch 1451, loss: 0.281, 46432/60000 datapoints
2025-03-06 19:13:03,344 - INFO - training batch 1501, loss: 0.400, 48032/60000 datapoints
2025-03-06 19:13:03,565 - INFO - training batch 1551, loss: 0.108, 49632/60000 datapoints
2025-03-06 19:13:03,788 - INFO - training batch 1601, loss: 0.491, 51232/60000 datapoints
2025-03-06 19:13:04,009 - INFO - training batch 1651, loss: 0.266, 52832/60000 datapoints
2025-03-06 19:13:04,245 - INFO - training batch 1701, loss: 0.233, 54432/60000 datapoints
2025-03-06 19:13:04,464 - INFO - training batch 1751, loss: 0.304, 56032/60000 datapoints
2025-03-06 19:13:04,685 - INFO - training batch 1801, loss: 0.390, 57632/60000 datapoints
2025-03-06 19:13:04,911 - INFO - training batch 1851, loss: 0.676, 59232/60000 datapoints
2025-03-06 19:13:05,026 - INFO - validation batch 1, loss: 0.275, 32/10016 datapoints
2025-03-06 19:13:05,244 - INFO - validation batch 51, loss: 0.450, 1632/10016 datapoints
2025-03-06 19:13:05,427 - INFO - validation batch 101, loss: 0.193, 3232/10016 datapoints
2025-03-06 19:13:05,602 - INFO - validation batch 151, loss: 0.197, 4832/10016 datapoints
2025-03-06 19:13:05,775 - INFO - validation batch 201, loss: 0.366, 6432/10016 datapoints
2025-03-06 19:13:05,948 - INFO - validation batch 251, loss: 0.312, 8032/10016 datapoints
2025-03-06 19:13:06,124 - INFO - validation batch 301, loss: 0.514, 9632/10016 datapoints
2025-03-06 19:13:06,173 - INFO - Epoch 240/800 done.
2025-03-06 19:13:06,173 - INFO - Final validation performance:
Loss: 0.330, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 19:13:06,174 - INFO - Beginning epoch 241/800
2025-03-06 19:13:06,182 - INFO - training batch 1, loss: 0.378, 32/60000 datapoints
2025-03-06 19:13:06,428 - INFO - training batch 51, loss: 0.566, 1632/60000 datapoints
2025-03-06 19:13:06,644 - INFO - training batch 101, loss: 0.455, 3232/60000 datapoints
2025-03-06 19:13:06,876 - INFO - training batch 151, loss: 0.388, 4832/60000 datapoints
2025-03-06 19:13:07,090 - INFO - training batch 201, loss: 0.292, 6432/60000 datapoints
2025-03-06 19:13:07,311 - INFO - training batch 251, loss: 0.249, 8032/60000 datapoints
2025-03-06 19:13:07,528 - INFO - training batch 301, loss: 0.223, 9632/60000 datapoints
2025-03-06 19:13:07,744 - INFO - training batch 351, loss: 0.222, 11232/60000 datapoints
2025-03-06 19:13:07,955 - INFO - training batch 401, loss: 0.319, 12832/60000 datapoints
2025-03-06 19:13:08,174 - INFO - training batch 451, loss: 0.220, 14432/60000 datapoints
2025-03-06 19:13:08,388 - INFO - training batch 501, loss: 0.308, 16032/60000 datapoints
2025-03-06 19:13:08,651 - INFO - training batch 551, loss: 0.392, 17632/60000 datapoints
2025-03-06 19:13:08,881 - INFO - training batch 601, loss: 0.328, 19232/60000 datapoints
2025-03-06 19:13:09,103 - INFO - training batch 651, loss: 0.362, 20832/60000 datapoints
2025-03-06 19:13:09,312 - INFO - training batch 701, loss: 0.416, 22432/60000 datapoints
2025-03-06 19:13:09,531 - INFO - training batch 751, loss: 0.349, 24032/60000 datapoints
2025-03-06 19:13:09,768 - INFO - training batch 801, loss: 0.332, 25632/60000 datapoints
2025-03-06 19:13:09,995 - INFO - training batch 851, loss: 0.356, 27232/60000 datapoints
2025-03-06 19:13:10,224 - INFO - training batch 901, loss: 0.306, 28832/60000 datapoints
2025-03-06 19:13:10,455 - INFO - training batch 951, loss: 0.146, 30432/60000 datapoints
2025-03-06 19:13:10,681 - INFO - training batch 1001, loss: 0.216, 32032/60000 datapoints
2025-03-06 19:13:10,892 - INFO - training batch 1051, loss: 0.277, 33632/60000 datapoints
2025-03-06 19:13:11,117 - INFO - training batch 1101, loss: 0.273, 35232/60000 datapoints
2025-03-06 19:13:11,358 - INFO - training batch 1151, loss: 0.566, 36832/60000 datapoints
2025-03-06 19:13:11,576 - INFO - training batch 1201, loss: 0.165, 38432/60000 datapoints
2025-03-06 19:13:11,799 - INFO - training batch 1251, loss: 0.236, 40032/60000 datapoints
2025-03-06 19:13:12,048 - INFO - training batch 1301, loss: 0.476, 41632/60000 datapoints
2025-03-06 19:13:12,283 - INFO - training batch 1351, loss: 0.596, 43232/60000 datapoints
2025-03-06 19:13:12,544 - INFO - training batch 1401, loss: 0.378, 44832/60000 datapoints
2025-03-06 19:13:12,828 - INFO - training batch 1451, loss: 0.467, 46432/60000 datapoints
2025-03-06 19:13:13,086 - INFO - training batch 1501, loss: 0.463, 48032/60000 datapoints
2025-03-06 19:13:13,364 - INFO - training batch 1551, loss: 0.253, 49632/60000 datapoints
2025-03-06 19:13:13,596 - INFO - training batch 1601, loss: 0.662, 51232/60000 datapoints
2025-03-06 19:13:13,832 - INFO - training batch 1651, loss: 0.371, 52832/60000 datapoints
2025-03-06 19:13:14,076 - INFO - training batch 1701, loss: 0.366, 54432/60000 datapoints
2025-03-06 19:13:14,349 - INFO - training batch 1751, loss: 0.343, 56032/60000 datapoints
2025-03-06 19:13:14,582 - INFO - training batch 1801, loss: 0.547, 57632/60000 datapoints
2025-03-06 19:13:14,831 - INFO - training batch 1851, loss: 0.125, 59232/60000 datapoints
2025-03-06 19:13:14,965 - INFO - validation batch 1, loss: 0.464, 32/10016 datapoints
2025-03-06 19:13:15,161 - INFO - validation batch 51, loss: 0.400, 1632/10016 datapoints
2025-03-06 19:13:15,355 - INFO - validation batch 101, loss: 0.178, 3232/10016 datapoints
2025-03-06 19:13:15,549 - INFO - validation batch 151, loss: 0.147, 4832/10016 datapoints
2025-03-06 19:13:15,754 - INFO - validation batch 201, loss: 0.497, 6432/10016 datapoints
2025-03-06 19:13:15,970 - INFO - validation batch 251, loss: 0.375, 8032/10016 datapoints
2025-03-06 19:13:16,183 - INFO - validation batch 301, loss: 0.149, 9632/10016 datapoints
2025-03-06 19:13:16,251 - INFO - Epoch 241/800 done.
2025-03-06 19:13:16,251 - INFO - Final validation performance:
Loss: 0.316, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 19:13:16,252 - INFO - Beginning epoch 242/800
2025-03-06 19:13:16,263 - INFO - training batch 1, loss: 0.285, 32/60000 datapoints
2025-03-06 19:13:16,553 - INFO - training batch 51, loss: 0.226, 1632/60000 datapoints
2025-03-06 19:13:16,797 - INFO - training batch 101, loss: 0.368, 3232/60000 datapoints
2025-03-06 19:13:17,057 - INFO - training batch 151, loss: 0.300, 4832/60000 datapoints
2025-03-06 19:13:17,310 - INFO - training batch 201, loss: 0.233, 6432/60000 datapoints
2025-03-06 19:13:17,581 - INFO - training batch 251, loss: 0.230, 8032/60000 datapoints
2025-03-06 19:13:17,868 - INFO - training batch 301, loss: 0.243, 9632/60000 datapoints
2025-03-06 19:13:18,104 - INFO - training batch 351, loss: 0.287, 11232/60000 datapoints
2025-03-06 19:13:18,349 - INFO - training batch 401, loss: 0.540, 12832/60000 datapoints
2025-03-06 19:13:18,596 - INFO - training batch 451, loss: 0.218, 14432/60000 datapoints
2025-03-06 19:13:18,859 - INFO - training batch 501, loss: 0.188, 16032/60000 datapoints
2025-03-06 19:13:19,102 - INFO - training batch 551, loss: 0.606, 17632/60000 datapoints
2025-03-06 19:13:19,360 - INFO - training batch 601, loss: 0.290, 19232/60000 datapoints
2025-03-06 19:13:19,617 - INFO - training batch 651, loss: 0.242, 20832/60000 datapoints
2025-03-06 19:13:19,878 - INFO - training batch 701, loss: 0.420, 22432/60000 datapoints
2025-03-06 19:13:20,137 - INFO - training batch 751, loss: 0.378, 24032/60000 datapoints
2025-03-06 19:13:20,392 - INFO - training batch 801, loss: 0.194, 25632/60000 datapoints
2025-03-06 19:13:20,641 - INFO - training batch 851, loss: 0.228, 27232/60000 datapoints
2025-03-06 19:13:20,891 - INFO - training batch 901, loss: 0.495, 28832/60000 datapoints
2025-03-06 19:13:21,132 - INFO - training batch 951, loss: 0.247, 30432/60000 datapoints
2025-03-06 19:13:21,372 - INFO - training batch 1001, loss: 0.386, 32032/60000 datapoints
2025-03-06 19:13:21,632 - INFO - training batch 1051, loss: 0.359, 33632/60000 datapoints
2025-03-06 19:13:21,875 - INFO - training batch 1101, loss: 0.287, 35232/60000 datapoints
2025-03-06 19:13:22,098 - INFO - training batch 1151, loss: 0.433, 36832/60000 datapoints
2025-03-06 19:13:22,333 - INFO - training batch 1201, loss: 0.295, 38432/60000 datapoints
2025-03-06 19:13:22,550 - INFO - training batch 1251, loss: 0.213, 40032/60000 datapoints
2025-03-06 19:13:22,782 - INFO - training batch 1301, loss: 0.448, 41632/60000 datapoints
2025-03-06 19:13:23,002 - INFO - training batch 1351, loss: 0.084, 43232/60000 datapoints
2025-03-06 19:13:23,219 - INFO - training batch 1401, loss: 0.733, 44832/60000 datapoints
2025-03-06 19:13:23,437 - INFO - training batch 1451, loss: 0.404, 46432/60000 datapoints
2025-03-06 19:13:23,661 - INFO - training batch 1501, loss: 0.168, 48032/60000 datapoints
2025-03-06 19:13:23,881 - INFO - training batch 1551, loss: 0.394, 49632/60000 datapoints
2025-03-06 19:13:24,091 - INFO - training batch 1601, loss: 0.192, 51232/60000 datapoints
2025-03-06 19:13:24,329 - INFO - training batch 1651, loss: 0.161, 52832/60000 datapoints
2025-03-06 19:13:24,542 - INFO - training batch 1701, loss: 0.188, 54432/60000 datapoints
2025-03-06 19:13:24,778 - INFO - training batch 1751, loss: 0.456, 56032/60000 datapoints
2025-03-06 19:13:24,993 - INFO - training batch 1801, loss: 0.781, 57632/60000 datapoints
2025-03-06 19:13:25,197 - INFO - training batch 1851, loss: 0.241, 59232/60000 datapoints
2025-03-06 19:13:25,303 - INFO - validation batch 1, loss: 0.222, 32/10016 datapoints
2025-03-06 19:13:25,467 - INFO - validation batch 51, loss: 0.420, 1632/10016 datapoints
2025-03-06 19:13:25,632 - INFO - validation batch 101, loss: 0.243, 3232/10016 datapoints
2025-03-06 19:13:25,794 - INFO - validation batch 151, loss: 0.455, 4832/10016 datapoints
2025-03-06 19:13:25,954 - INFO - validation batch 201, loss: 0.217, 6432/10016 datapoints
2025-03-06 19:13:26,114 - INFO - validation batch 251, loss: 0.415, 8032/10016 datapoints
2025-03-06 19:13:26,284 - INFO - validation batch 301, loss: 0.471, 9632/10016 datapoints
2025-03-06 19:13:26,324 - INFO - Epoch 242/800 done.
2025-03-06 19:13:26,324 - INFO - Final validation performance:
Loss: 0.349, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 19:13:26,325 - INFO - Beginning epoch 243/800
2025-03-06 19:13:26,332 - INFO - training batch 1, loss: 0.299, 32/60000 datapoints
2025-03-06 19:13:26,571 - INFO - training batch 51, loss: 0.503, 1632/60000 datapoints
2025-03-06 19:13:26,781 - INFO - training batch 101, loss: 0.634, 3232/60000 datapoints
2025-03-06 19:13:27,008 - INFO - training batch 151, loss: 0.455, 4832/60000 datapoints
2025-03-06 19:13:27,211 - INFO - training batch 201, loss: 0.232, 6432/60000 datapoints
2025-03-06 19:13:27,424 - INFO - training batch 251, loss: 0.276, 8032/60000 datapoints
2025-03-06 19:13:27,635 - INFO - training batch 301, loss: 0.316, 9632/60000 datapoints
2025-03-06 19:13:27,845 - INFO - training batch 351, loss: 0.369, 11232/60000 datapoints
2025-03-06 19:13:28,051 - INFO - training batch 401, loss: 0.444, 12832/60000 datapoints
2025-03-06 19:13:28,255 - INFO - training batch 451, loss: 0.285, 14432/60000 datapoints
2025-03-06 19:13:28,467 - INFO - training batch 501, loss: 0.259, 16032/60000 datapoints
2025-03-06 19:13:28,677 - INFO - training batch 551, loss: 0.721, 17632/60000 datapoints
2025-03-06 19:13:28,919 - INFO - training batch 601, loss: 0.222, 19232/60000 datapoints
2025-03-06 19:13:29,127 - INFO - training batch 651, loss: 0.353, 20832/60000 datapoints
2025-03-06 19:13:29,336 - INFO - training batch 701, loss: 0.095, 22432/60000 datapoints
2025-03-06 19:13:29,546 - INFO - training batch 751, loss: 0.267, 24032/60000 datapoints
2025-03-06 19:13:29,755 - INFO - training batch 801, loss: 0.576, 25632/60000 datapoints
2025-03-06 19:13:29,963 - INFO - training batch 851, loss: 0.245, 27232/60000 datapoints
2025-03-06 19:13:30,169 - INFO - training batch 901, loss: 0.439, 28832/60000 datapoints
2025-03-06 19:13:30,381 - INFO - training batch 951, loss: 0.241, 30432/60000 datapoints
2025-03-06 19:13:30,585 - INFO - training batch 1001, loss: 0.358, 32032/60000 datapoints
2025-03-06 19:13:30,799 - INFO - training batch 1051, loss: 0.177, 33632/60000 datapoints
2025-03-06 19:13:31,015 - INFO - training batch 1101, loss: 0.275, 35232/60000 datapoints
2025-03-06 19:13:31,232 - INFO - training batch 1151, loss: 0.510, 36832/60000 datapoints
2025-03-06 19:13:31,441 - INFO - training batch 1201, loss: 0.508, 38432/60000 datapoints
2025-03-06 19:13:31,655 - INFO - training batch 1251, loss: 0.282, 40032/60000 datapoints
2025-03-06 19:13:31,863 - INFO - training batch 1301, loss: 0.348, 41632/60000 datapoints
2025-03-06 19:13:32,083 - INFO - training batch 1351, loss: 0.338, 43232/60000 datapoints
2025-03-06 19:13:32,297 - INFO - training batch 1401, loss: 0.235, 44832/60000 datapoints
2025-03-06 19:13:32,508 - INFO - training batch 1451, loss: 0.397, 46432/60000 datapoints
2025-03-06 19:13:32,729 - INFO - training batch 1501, loss: 0.545, 48032/60000 datapoints
2025-03-06 19:13:32,937 - INFO - training batch 1551, loss: 0.298, 49632/60000 datapoints
2025-03-06 19:13:33,153 - INFO - training batch 1601, loss: 0.257, 51232/60000 datapoints
2025-03-06 19:13:33,368 - INFO - training batch 1651, loss: 0.246, 52832/60000 datapoints
2025-03-06 19:13:33,575 - INFO - training batch 1701, loss: 0.228, 54432/60000 datapoints
2025-03-06 19:13:33,791 - INFO - training batch 1751, loss: 0.239, 56032/60000 datapoints
2025-03-06 19:13:34,009 - INFO - training batch 1801, loss: 0.381, 57632/60000 datapoints
2025-03-06 19:13:34,220 - INFO - training batch 1851, loss: 0.283, 59232/60000 datapoints
2025-03-06 19:13:34,339 - INFO - validation batch 1, loss: 0.368, 32/10016 datapoints
2025-03-06 19:13:34,511 - INFO - validation batch 51, loss: 0.417, 1632/10016 datapoints
2025-03-06 19:13:34,684 - INFO - validation batch 101, loss: 0.235, 3232/10016 datapoints
2025-03-06 19:13:34,859 - INFO - validation batch 151, loss: 0.337, 4832/10016 datapoints
2025-03-06 19:13:35,033 - INFO - validation batch 201, loss: 0.291, 6432/10016 datapoints
2025-03-06 19:13:35,213 - INFO - validation batch 251, loss: 0.406, 8032/10016 datapoints
2025-03-06 19:13:35,386 - INFO - validation batch 301, loss: 0.300, 9632/10016 datapoints
2025-03-06 19:13:35,431 - INFO - Epoch 243/800 done.
2025-03-06 19:13:35,431 - INFO - Final validation performance:
Loss: 0.336, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 19:13:35,432 - INFO - Beginning epoch 244/800
2025-03-06 19:13:35,439 - INFO - training batch 1, loss: 0.290, 32/60000 datapoints
2025-03-06 19:13:35,671 - INFO - training batch 51, loss: 0.218, 1632/60000 datapoints
2025-03-06 19:13:35,890 - INFO - training batch 101, loss: 0.311, 3232/60000 datapoints
2025-03-06 19:13:36,111 - INFO - training batch 151, loss: 0.230, 4832/60000 datapoints
2025-03-06 19:13:36,340 - INFO - training batch 201, loss: 0.267, 6432/60000 datapoints
2025-03-06 19:13:36,567 - INFO - training batch 251, loss: 0.200, 8032/60000 datapoints
2025-03-06 19:13:36,901 - INFO - training batch 301, loss: 0.164, 9632/60000 datapoints
2025-03-06 19:13:37,141 - INFO - training batch 351, loss: 0.344, 11232/60000 datapoints
2025-03-06 19:13:37,403 - INFO - training batch 401, loss: 0.203, 12832/60000 datapoints
2025-03-06 19:13:37,695 - INFO - training batch 451, loss: 0.494, 14432/60000 datapoints
2025-03-06 19:13:37,925 - INFO - training batch 501, loss: 0.515, 16032/60000 datapoints
2025-03-06 19:13:38,166 - INFO - training batch 551, loss: 0.258, 17632/60000 datapoints
2025-03-06 19:13:38,405 - INFO - training batch 601, loss: 0.404, 19232/60000 datapoints
2025-03-06 19:13:38,666 - INFO - training batch 651, loss: 0.521, 20832/60000 datapoints
2025-03-06 19:13:38,905 - INFO - training batch 701, loss: 0.299, 22432/60000 datapoints
2025-03-06 19:13:39,143 - INFO - training batch 751, loss: 0.220, 24032/60000 datapoints
2025-03-06 19:13:39,373 - INFO - training batch 801, loss: 0.394, 25632/60000 datapoints
2025-03-06 19:13:39,636 - INFO - training batch 851, loss: 0.241, 27232/60000 datapoints
2025-03-06 19:13:39,886 - INFO - training batch 901, loss: 0.127, 28832/60000 datapoints
2025-03-06 19:13:40,138 - INFO - training batch 951, loss: 0.245, 30432/60000 datapoints
2025-03-06 19:13:40,377 - INFO - training batch 1001, loss: 0.223, 32032/60000 datapoints
2025-03-06 19:13:40,644 - INFO - training batch 1051, loss: 0.594, 33632/60000 datapoints
2025-03-06 19:13:40,933 - INFO - training batch 1101, loss: 0.128, 35232/60000 datapoints
2025-03-06 19:13:41,170 - INFO - training batch 1151, loss: 0.297, 36832/60000 datapoints
2025-03-06 19:13:41,429 - INFO - training batch 1201, loss: 0.277, 38432/60000 datapoints
2025-03-06 19:13:41,691 - INFO - training batch 1251, loss: 0.265, 40032/60000 datapoints
2025-03-06 19:13:41,951 - INFO - training batch 1301, loss: 0.414, 41632/60000 datapoints
2025-03-06 19:13:42,191 - INFO - training batch 1351, loss: 0.261, 43232/60000 datapoints
2025-03-06 19:13:42,461 - INFO - training batch 1401, loss: 0.243, 44832/60000 datapoints
2025-03-06 19:13:42,701 - INFO - training batch 1451, loss: 0.405, 46432/60000 datapoints
2025-03-06 19:13:42,944 - INFO - training batch 1501, loss: 0.314, 48032/60000 datapoints
2025-03-06 19:13:43,237 - INFO - training batch 1551, loss: 0.273, 49632/60000 datapoints
2025-03-06 19:13:43,511 - INFO - training batch 1601, loss: 0.558, 51232/60000 datapoints
2025-03-06 19:13:43,776 - INFO - training batch 1651, loss: 0.199, 52832/60000 datapoints
2025-03-06 19:13:44,042 - INFO - training batch 1701, loss: 0.261, 54432/60000 datapoints
2025-03-06 19:13:44,312 - INFO - training batch 1751, loss: 0.464, 56032/60000 datapoints
2025-03-06 19:13:44,551 - INFO - training batch 1801, loss: 0.178, 57632/60000 datapoints
2025-03-06 19:13:44,790 - INFO - training batch 1851, loss: 0.446, 59232/60000 datapoints
2025-03-06 19:13:44,917 - INFO - validation batch 1, loss: 0.285, 32/10016 datapoints
2025-03-06 19:13:45,104 - INFO - validation batch 51, loss: 0.183, 1632/10016 datapoints
2025-03-06 19:13:45,290 - INFO - validation batch 101, loss: 0.292, 3232/10016 datapoints
2025-03-06 19:13:45,481 - INFO - validation batch 151, loss: 0.180, 4832/10016 datapoints
2025-03-06 19:13:45,672 - INFO - validation batch 201, loss: 0.238, 6432/10016 datapoints
2025-03-06 19:13:45,857 - INFO - validation batch 251, loss: 0.174, 8032/10016 datapoints
2025-03-06 19:13:46,042 - INFO - validation batch 301, loss: 0.408, 9632/10016 datapoints
2025-03-06 19:13:46,086 - INFO - Epoch 244/800 done.
2025-03-06 19:13:46,086 - INFO - Final validation performance:
Loss: 0.252, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:13:46,087 - INFO - Beginning epoch 245/800
2025-03-06 19:13:46,095 - INFO - training batch 1, loss: 0.212, 32/60000 datapoints
2025-03-06 19:13:46,338 - INFO - training batch 51, loss: 0.418, 1632/60000 datapoints
2025-03-06 19:13:46,589 - INFO - training batch 101, loss: 0.377, 3232/60000 datapoints
2025-03-06 19:13:46,849 - INFO - training batch 151, loss: 0.175, 4832/60000 datapoints
2025-03-06 19:13:47,127 - INFO - training batch 201, loss: 0.504, 6432/60000 datapoints
2025-03-06 19:13:47,367 - INFO - training batch 251, loss: 0.462, 8032/60000 datapoints
2025-03-06 19:13:47,647 - INFO - training batch 301, loss: 0.288, 9632/60000 datapoints
2025-03-06 19:13:47,912 - INFO - training batch 351, loss: 0.228, 11232/60000 datapoints
2025-03-06 19:13:48,177 - INFO - training batch 401, loss: 0.444, 12832/60000 datapoints
2025-03-06 19:13:48,458 - INFO - training batch 451, loss: 0.242, 14432/60000 datapoints
2025-03-06 19:13:48,719 - INFO - training batch 501, loss: 0.468, 16032/60000 datapoints
2025-03-06 19:13:48,998 - INFO - training batch 551, loss: 0.829, 17632/60000 datapoints
2025-03-06 19:13:49,250 - INFO - training batch 601, loss: 0.261, 19232/60000 datapoints
2025-03-06 19:13:49,494 - INFO - training batch 651, loss: 0.658, 20832/60000 datapoints
2025-03-06 19:13:49,749 - INFO - training batch 701, loss: 0.361, 22432/60000 datapoints
2025-03-06 19:13:49,971 - INFO - training batch 751, loss: 0.445, 24032/60000 datapoints
2025-03-06 19:13:50,204 - INFO - training batch 801, loss: 0.249, 25632/60000 datapoints
2025-03-06 19:13:50,436 - INFO - training batch 851, loss: 0.350, 27232/60000 datapoints
2025-03-06 19:13:50,667 - INFO - training batch 901, loss: 0.216, 28832/60000 datapoints
2025-03-06 19:13:50,886 - INFO - training batch 951, loss: 0.257, 30432/60000 datapoints
2025-03-06 19:13:51,114 - INFO - training batch 1001, loss: 0.388, 32032/60000 datapoints
2025-03-06 19:13:51,343 - INFO - training batch 1051, loss: 0.224, 33632/60000 datapoints
2025-03-06 19:13:51,568 - INFO - training batch 1101, loss: 0.182, 35232/60000 datapoints
2025-03-06 19:13:51,803 - INFO - training batch 1151, loss: 0.187, 36832/60000 datapoints
2025-03-06 19:13:52,145 - INFO - training batch 1201, loss: 0.456, 38432/60000 datapoints
2025-03-06 19:13:52,449 - INFO - training batch 1251, loss: 0.412, 40032/60000 datapoints
2025-03-06 19:13:52,747 - INFO - training batch 1301, loss: 0.375, 41632/60000 datapoints
2025-03-06 19:13:53,012 - INFO - training batch 1351, loss: 0.337, 43232/60000 datapoints
2025-03-06 19:13:53,286 - INFO - training batch 1401, loss: 0.214, 44832/60000 datapoints
2025-03-06 19:13:53,531 - INFO - training batch 1451, loss: 0.334, 46432/60000 datapoints
2025-03-06 19:13:53,780 - INFO - training batch 1501, loss: 0.495, 48032/60000 datapoints
2025-03-06 19:13:54,023 - INFO - training batch 1551, loss: 0.370, 49632/60000 datapoints
2025-03-06 19:13:54,287 - INFO - training batch 1601, loss: 0.269, 51232/60000 datapoints
2025-03-06 19:13:54,574 - INFO - training batch 1651, loss: 0.149, 52832/60000 datapoints
2025-03-06 19:13:54,846 - INFO - training batch 1701, loss: 0.464, 54432/60000 datapoints
2025-03-06 19:13:55,095 - INFO - training batch 1751, loss: 0.360, 56032/60000 datapoints
2025-03-06 19:13:55,357 - INFO - training batch 1801, loss: 0.213, 57632/60000 datapoints
2025-03-06 19:13:55,648 - INFO - training batch 1851, loss: 0.318, 59232/60000 datapoints
2025-03-06 19:13:55,793 - INFO - validation batch 1, loss: 0.346, 32/10016 datapoints
2025-03-06 19:13:56,029 - INFO - validation batch 51, loss: 0.193, 1632/10016 datapoints
2025-03-06 19:13:56,340 - INFO - validation batch 101, loss: 0.335, 3232/10016 datapoints
2025-03-06 19:13:56,580 - INFO - validation batch 151, loss: 0.134, 4832/10016 datapoints
2025-03-06 19:13:56,868 - INFO - validation batch 201, loss: 0.328, 6432/10016 datapoints
2025-03-06 19:13:57,099 - INFO - validation batch 251, loss: 0.238, 8032/10016 datapoints
2025-03-06 19:13:57,313 - INFO - validation batch 301, loss: 0.466, 9632/10016 datapoints
2025-03-06 19:13:57,374 - INFO - Epoch 245/800 done.
2025-03-06 19:13:57,374 - INFO - Final validation performance:
Loss: 0.292, top-1 acc: 0.908top-5 acc: 0.908
2025-03-06 19:13:57,375 - INFO - Beginning epoch 246/800
2025-03-06 19:13:57,387 - INFO - training batch 1, loss: 0.234, 32/60000 datapoints
2025-03-06 19:13:57,708 - INFO - training batch 51, loss: 0.226, 1632/60000 datapoints
2025-03-06 19:13:57,955 - INFO - training batch 101, loss: 0.203, 3232/60000 datapoints
2025-03-06 19:13:58,226 - INFO - training batch 151, loss: 0.583, 4832/60000 datapoints
2025-03-06 19:13:58,508 - INFO - training batch 201, loss: 0.660, 6432/60000 datapoints
2025-03-06 19:13:58,774 - INFO - training batch 251, loss: 0.252, 8032/60000 datapoints
2025-03-06 19:13:59,019 - INFO - training batch 301, loss: 0.264, 9632/60000 datapoints
2025-03-06 19:13:59,256 - INFO - training batch 351, loss: 0.126, 11232/60000 datapoints
2025-03-06 19:13:59,496 - INFO - training batch 401, loss: 0.407, 12832/60000 datapoints
2025-03-06 19:13:59,746 - INFO - training batch 451, loss: 0.470, 14432/60000 datapoints
2025-03-06 19:13:59,992 - INFO - training batch 501, loss: 0.430, 16032/60000 datapoints
2025-03-06 19:14:00,237 - INFO - training batch 551, loss: 0.281, 17632/60000 datapoints
2025-03-06 19:14:00,538 - INFO - training batch 601, loss: 0.561, 19232/60000 datapoints
2025-03-06 19:14:00,849 - INFO - training batch 651, loss: 0.328, 20832/60000 datapoints
2025-03-06 19:14:01,116 - INFO - training batch 701, loss: 0.393, 22432/60000 datapoints
2025-03-06 19:14:01,390 - INFO - training batch 751, loss: 0.340, 24032/60000 datapoints
2025-03-06 19:14:01,677 - INFO - training batch 801, loss: 0.351, 25632/60000 datapoints
2025-03-06 19:14:01,967 - INFO - training batch 851, loss: 0.178, 27232/60000 datapoints
2025-03-06 19:14:02,244 - INFO - training batch 901, loss: 0.288, 28832/60000 datapoints
2025-03-06 19:14:02,498 - INFO - training batch 951, loss: 0.615, 30432/60000 datapoints
2025-03-06 19:14:02,772 - INFO - training batch 1001, loss: 0.302, 32032/60000 datapoints
2025-03-06 19:14:03,044 - INFO - training batch 1051, loss: 0.190, 33632/60000 datapoints
2025-03-06 19:14:03,369 - INFO - training batch 1101, loss: 0.128, 35232/60000 datapoints
2025-03-06 19:14:03,664 - INFO - training batch 1151, loss: 0.382, 36832/60000 datapoints
2025-03-06 19:14:03,968 - INFO - training batch 1201, loss: 0.456, 38432/60000 datapoints
2025-03-06 19:14:04,225 - INFO - training batch 1251, loss: 0.433, 40032/60000 datapoints
2025-03-06 19:14:04,513 - INFO - training batch 1301, loss: 0.198, 41632/60000 datapoints
2025-03-06 19:14:04,797 - INFO - training batch 1351, loss: 0.426, 43232/60000 datapoints
2025-03-06 19:14:05,061 - INFO - training batch 1401, loss: 0.221, 44832/60000 datapoints
2025-03-06 19:14:05,335 - INFO - training batch 1451, loss: 0.416, 46432/60000 datapoints
2025-03-06 19:14:05,644 - INFO - training batch 1501, loss: 0.341, 48032/60000 datapoints
2025-03-06 19:14:05,894 - INFO - training batch 1551, loss: 0.416, 49632/60000 datapoints
2025-03-06 19:14:06,179 - INFO - training batch 1601, loss: 0.226, 51232/60000 datapoints
2025-03-06 19:14:06,441 - INFO - training batch 1651, loss: 0.424, 52832/60000 datapoints
2025-03-06 19:14:06,693 - INFO - training batch 1701, loss: 0.174, 54432/60000 datapoints
2025-03-06 19:14:06,975 - INFO - training batch 1751, loss: 0.549, 56032/60000 datapoints
2025-03-06 19:14:07,231 - INFO - training batch 1801, loss: 0.229, 57632/60000 datapoints
2025-03-06 19:14:07,472 - INFO - training batch 1851, loss: 0.197, 59232/60000 datapoints
2025-03-06 19:14:07,605 - INFO - validation batch 1, loss: 0.201, 32/10016 datapoints
2025-03-06 19:14:07,793 - INFO - validation batch 51, loss: 0.207, 1632/10016 datapoints
2025-03-06 19:14:07,984 - INFO - validation batch 101, loss: 0.112, 3232/10016 datapoints
2025-03-06 19:14:08,199 - INFO - validation batch 151, loss: 0.335, 4832/10016 datapoints
2025-03-06 19:14:08,385 - INFO - validation batch 201, loss: 0.264, 6432/10016 datapoints
2025-03-06 19:14:08,579 - INFO - validation batch 251, loss: 0.442, 8032/10016 datapoints
2025-03-06 19:14:08,782 - INFO - validation batch 301, loss: 0.421, 9632/10016 datapoints
2025-03-06 19:14:08,841 - INFO - Epoch 246/800 done.
2025-03-06 19:14:08,841 - INFO - Final validation performance:
Loss: 0.283, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:14:08,842 - INFO - Beginning epoch 247/800
2025-03-06 19:14:08,852 - INFO - training batch 1, loss: 0.225, 32/60000 datapoints
2025-03-06 19:14:09,109 - INFO - training batch 51, loss: 0.261, 1632/60000 datapoints
2025-03-06 19:14:09,333 - INFO - training batch 101, loss: 0.816, 3232/60000 datapoints
2025-03-06 19:14:09,575 - INFO - training batch 151, loss: 0.143, 4832/60000 datapoints
2025-03-06 19:14:09,813 - INFO - training batch 201, loss: 0.456, 6432/60000 datapoints
2025-03-06 19:14:10,039 - INFO - training batch 251, loss: 0.449, 8032/60000 datapoints
2025-03-06 19:14:10,265 - INFO - training batch 301, loss: 0.142, 9632/60000 datapoints
2025-03-06 19:14:10,499 - INFO - training batch 351, loss: 0.317, 11232/60000 datapoints
2025-03-06 19:14:10,739 - INFO - training batch 401, loss: 0.381, 12832/60000 datapoints
2025-03-06 19:14:10,990 - INFO - training batch 451, loss: 0.189, 14432/60000 datapoints
2025-03-06 19:14:11,225 - INFO - training batch 501, loss: 0.351, 16032/60000 datapoints
2025-03-06 19:14:11,470 - INFO - training batch 551, loss: 0.365, 17632/60000 datapoints
2025-03-06 19:14:11,726 - INFO - training batch 601, loss: 0.316, 19232/60000 datapoints
2025-03-06 19:14:11,972 - INFO - training batch 651, loss: 0.198, 20832/60000 datapoints
2025-03-06 19:14:12,224 - INFO - training batch 701, loss: 0.383, 22432/60000 datapoints
2025-03-06 19:14:12,508 - INFO - training batch 751, loss: 0.246, 24032/60000 datapoints
2025-03-06 19:14:12,811 - INFO - training batch 801, loss: 0.236, 25632/60000 datapoints
2025-03-06 19:14:13,102 - INFO - training batch 851, loss: 0.303, 27232/60000 datapoints
2025-03-06 19:14:13,446 - INFO - training batch 901, loss: 0.129, 28832/60000 datapoints
2025-03-06 19:14:13,705 - INFO - training batch 951, loss: 0.250, 30432/60000 datapoints
2025-03-06 19:14:13,953 - INFO - training batch 1001, loss: 0.188, 32032/60000 datapoints
2025-03-06 19:14:14,192 - INFO - training batch 1051, loss: 0.962, 33632/60000 datapoints
2025-03-06 19:14:14,444 - INFO - training batch 1101, loss: 0.624, 35232/60000 datapoints
2025-03-06 19:14:14,692 - INFO - training batch 1151, loss: 0.351, 36832/60000 datapoints
2025-03-06 19:14:14,963 - INFO - training batch 1201, loss: 0.197, 38432/60000 datapoints
2025-03-06 19:14:15,226 - INFO - training batch 1251, loss: 0.247, 40032/60000 datapoints
2025-03-06 19:14:15,487 - INFO - training batch 1301, loss: 0.547, 41632/60000 datapoints
2025-03-06 19:14:15,767 - INFO - training batch 1351, loss: 0.445, 43232/60000 datapoints
2025-03-06 19:14:16,049 - INFO - training batch 1401, loss: 0.183, 44832/60000 datapoints
2025-03-06 19:14:16,326 - INFO - training batch 1451, loss: 0.446, 46432/60000 datapoints
2025-03-06 19:14:16,647 - INFO - training batch 1501, loss: 0.084, 48032/60000 datapoints
2025-03-06 19:14:16,901 - INFO - training batch 1551, loss: 0.288, 49632/60000 datapoints
2025-03-06 19:14:17,237 - INFO - training batch 1601, loss: 0.155, 51232/60000 datapoints
2025-03-06 19:14:17,641 - INFO - training batch 1651, loss: 0.127, 52832/60000 datapoints
2025-03-06 19:14:17,962 - INFO - training batch 1701, loss: 0.287, 54432/60000 datapoints
2025-03-06 19:14:18,261 - INFO - training batch 1751, loss: 0.325, 56032/60000 datapoints
2025-03-06 19:14:18,709 - INFO - training batch 1801, loss: 0.285, 57632/60000 datapoints
2025-03-06 19:14:19,077 - INFO - training batch 1851, loss: 0.233, 59232/60000 datapoints
2025-03-06 19:14:19,281 - INFO - validation batch 1, loss: 0.352, 32/10016 datapoints
2025-03-06 19:14:19,636 - INFO - validation batch 51, loss: 0.234, 1632/10016 datapoints
2025-03-06 19:14:19,930 - INFO - validation batch 101, loss: 0.344, 3232/10016 datapoints
2025-03-06 19:14:20,371 - INFO - validation batch 151, loss: 0.446, 4832/10016 datapoints
2025-03-06 19:14:20,789 - INFO - validation batch 201, loss: 0.361, 6432/10016 datapoints
2025-03-06 19:14:21,051 - INFO - validation batch 251, loss: 0.355, 8032/10016 datapoints
2025-03-06 19:14:21,318 - INFO - validation batch 301, loss: 0.175, 9632/10016 datapoints
2025-03-06 19:14:21,370 - INFO - Epoch 247/800 done.
2025-03-06 19:14:21,371 - INFO - Final validation performance:
Loss: 0.324, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:14:21,373 - INFO - Beginning epoch 248/800
2025-03-06 19:14:21,382 - INFO - training batch 1, loss: 0.175, 32/60000 datapoints
2025-03-06 19:14:21,720 - INFO - training batch 51, loss: 0.428, 1632/60000 datapoints
2025-03-06 19:14:22,114 - INFO - training batch 101, loss: 0.380, 3232/60000 datapoints
2025-03-06 19:14:22,446 - INFO - training batch 151, loss: 0.237, 4832/60000 datapoints
2025-03-06 19:14:22,792 - INFO - training batch 201, loss: 0.655, 6432/60000 datapoints
2025-03-06 19:14:23,101 - INFO - training batch 251, loss: 0.456, 8032/60000 datapoints
2025-03-06 19:14:23,398 - INFO - training batch 301, loss: 0.402, 9632/60000 datapoints
2025-03-06 19:14:23,719 - INFO - training batch 351, loss: 0.559, 11232/60000 datapoints
2025-03-06 19:14:24,031 - INFO - training batch 401, loss: 0.600, 12832/60000 datapoints
2025-03-06 19:14:24,397 - INFO - training batch 451, loss: 0.225, 14432/60000 datapoints
2025-03-06 19:14:24,769 - INFO - training batch 501, loss: 0.724, 16032/60000 datapoints
2025-03-06 19:14:25,131 - INFO - training batch 551, loss: 0.214, 17632/60000 datapoints
2025-03-06 19:14:25,441 - INFO - training batch 601, loss: 0.171, 19232/60000 datapoints
2025-03-06 19:14:25,788 - INFO - training batch 651, loss: 0.226, 20832/60000 datapoints
2025-03-06 19:14:26,046 - INFO - training batch 701, loss: 0.428, 22432/60000 datapoints
2025-03-06 19:14:26,344 - INFO - training batch 751, loss: 0.246, 24032/60000 datapoints
2025-03-06 19:14:26,616 - INFO - training batch 801, loss: 0.187, 25632/60000 datapoints
2025-03-06 19:14:26,875 - INFO - training batch 851, loss: 0.218, 27232/60000 datapoints
2025-03-06 19:14:27,219 - INFO - training batch 901, loss: 0.366, 28832/60000 datapoints
2025-03-06 19:14:27,508 - INFO - training batch 951, loss: 0.276, 30432/60000 datapoints
2025-03-06 19:14:27,774 - INFO - training batch 1001, loss: 0.320, 32032/60000 datapoints
2025-03-06 19:14:28,053 - INFO - training batch 1051, loss: 0.298, 33632/60000 datapoints
2025-03-06 19:14:28,328 - INFO - training batch 1101, loss: 0.163, 35232/60000 datapoints
2025-03-06 19:14:28,578 - INFO - training batch 1151, loss: 0.377, 36832/60000 datapoints
2025-03-06 19:14:28,846 - INFO - training batch 1201, loss: 0.256, 38432/60000 datapoints
2025-03-06 19:14:29,102 - INFO - training batch 1251, loss: 0.216, 40032/60000 datapoints
2025-03-06 19:14:29,377 - INFO - training batch 1301, loss: 0.421, 41632/60000 datapoints
2025-03-06 19:14:29,644 - INFO - training batch 1351, loss: 0.467, 43232/60000 datapoints
2025-03-06 19:14:29,905 - INFO - training batch 1401, loss: 0.282, 44832/60000 datapoints
2025-03-06 19:14:30,151 - INFO - training batch 1451, loss: 0.285, 46432/60000 datapoints
2025-03-06 19:14:30,388 - INFO - training batch 1501, loss: 0.663, 48032/60000 datapoints
2025-03-06 19:14:30,646 - INFO - training batch 1551, loss: 0.743, 49632/60000 datapoints
2025-03-06 19:14:30,886 - INFO - training batch 1601, loss: 0.333, 51232/60000 datapoints
2025-03-06 19:14:31,123 - INFO - training batch 1651, loss: 0.139, 52832/60000 datapoints
2025-03-06 19:14:31,353 - INFO - training batch 1701, loss: 0.269, 54432/60000 datapoints
2025-03-06 19:14:31,582 - INFO - training batch 1751, loss: 0.532, 56032/60000 datapoints
2025-03-06 19:14:31,832 - INFO - training batch 1801, loss: 0.317, 57632/60000 datapoints
2025-03-06 19:14:32,068 - INFO - training batch 1851, loss: 0.342, 59232/60000 datapoints
2025-03-06 19:14:32,210 - INFO - validation batch 1, loss: 0.302, 32/10016 datapoints
2025-03-06 19:14:32,392 - INFO - validation batch 51, loss: 0.255, 1632/10016 datapoints
2025-03-06 19:14:32,582 - INFO - validation batch 101, loss: 0.340, 3232/10016 datapoints
2025-03-06 19:14:32,764 - INFO - validation batch 151, loss: 0.131, 4832/10016 datapoints
2025-03-06 19:14:32,939 - INFO - validation batch 201, loss: 0.577, 6432/10016 datapoints
2025-03-06 19:14:33,119 - INFO - validation batch 251, loss: 0.279, 8032/10016 datapoints
2025-03-06 19:14:33,294 - INFO - validation batch 301, loss: 0.219, 9632/10016 datapoints
2025-03-06 19:14:33,340 - INFO - Epoch 248/800 done.
2025-03-06 19:14:33,340 - INFO - Final validation performance:
Loss: 0.300, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:14:33,341 - INFO - Beginning epoch 249/800
2025-03-06 19:14:33,348 - INFO - training batch 1, loss: 0.398, 32/60000 datapoints
2025-03-06 19:14:33,587 - INFO - training batch 51, loss: 0.266, 1632/60000 datapoints
2025-03-06 19:14:33,858 - INFO - training batch 101, loss: 0.181, 3232/60000 datapoints
2025-03-06 19:14:34,092 - INFO - training batch 151, loss: 0.450, 4832/60000 datapoints
2025-03-06 19:14:34,323 - INFO - training batch 201, loss: 0.169, 6432/60000 datapoints
2025-03-06 19:14:34,589 - INFO - training batch 251, loss: 0.624, 8032/60000 datapoints
2025-03-06 19:14:34,814 - INFO - training batch 301, loss: 0.246, 9632/60000 datapoints
2025-03-06 19:14:35,048 - INFO - training batch 351, loss: 0.228, 11232/60000 datapoints
2025-03-06 19:14:35,276 - INFO - training batch 401, loss: 0.466, 12832/60000 datapoints
2025-03-06 19:14:35,506 - INFO - training batch 451, loss: 0.280, 14432/60000 datapoints
2025-03-06 19:14:35,740 - INFO - training batch 501, loss: 0.336, 16032/60000 datapoints
2025-03-06 19:14:35,990 - INFO - training batch 551, loss: 0.435, 17632/60000 datapoints
2025-03-06 19:14:36,220 - INFO - training batch 601, loss: 0.264, 19232/60000 datapoints
2025-03-06 19:14:36,448 - INFO - training batch 651, loss: 0.260, 20832/60000 datapoints
2025-03-06 19:14:36,705 - INFO - training batch 701, loss: 0.435, 22432/60000 datapoints
2025-03-06 19:14:36,963 - INFO - training batch 751, loss: 0.199, 24032/60000 datapoints
2025-03-06 19:14:37,201 - INFO - training batch 801, loss: 0.422, 25632/60000 datapoints
2025-03-06 19:14:37,508 - INFO - training batch 851, loss: 0.334, 27232/60000 datapoints
2025-03-06 19:14:37,885 - INFO - training batch 901, loss: 0.460, 28832/60000 datapoints
2025-03-06 19:14:38,178 - INFO - training batch 951, loss: 0.258, 30432/60000 datapoints
2025-03-06 19:14:38,457 - INFO - training batch 1001, loss: 0.499, 32032/60000 datapoints
2025-03-06 19:14:38,724 - INFO - training batch 1051, loss: 0.557, 33632/60000 datapoints
2025-03-06 19:14:39,014 - INFO - training batch 1101, loss: 0.245, 35232/60000 datapoints
2025-03-06 19:14:39,304 - INFO - training batch 1151, loss: 0.187, 36832/60000 datapoints
2025-03-06 19:14:39,582 - INFO - training batch 1201, loss: 0.153, 38432/60000 datapoints
2025-03-06 19:14:39,867 - INFO - training batch 1251, loss: 0.596, 40032/60000 datapoints
2025-03-06 19:14:40,157 - INFO - training batch 1301, loss: 0.342, 41632/60000 datapoints
2025-03-06 19:14:40,442 - INFO - training batch 1351, loss: 0.288, 43232/60000 datapoints
2025-03-06 19:14:40,737 - INFO - training batch 1401, loss: 0.564, 44832/60000 datapoints
2025-03-06 19:14:41,021 - INFO - training batch 1451, loss: 0.158, 46432/60000 datapoints
2025-03-06 19:14:41,277 - INFO - training batch 1501, loss: 0.280, 48032/60000 datapoints
2025-03-06 19:14:41,533 - INFO - training batch 1551, loss: 0.229, 49632/60000 datapoints
2025-03-06 19:14:41,828 - INFO - training batch 1601, loss: 0.493, 51232/60000 datapoints
2025-03-06 19:14:42,123 - INFO - training batch 1651, loss: 0.315, 52832/60000 datapoints
2025-03-06 19:14:42,450 - INFO - training batch 1701, loss: 0.259, 54432/60000 datapoints
2025-03-06 19:14:42,732 - INFO - training batch 1751, loss: 0.151, 56032/60000 datapoints
2025-03-06 19:14:43,026 - INFO - training batch 1801, loss: 0.244, 57632/60000 datapoints
2025-03-06 19:14:43,313 - INFO - training batch 1851, loss: 0.484, 59232/60000 datapoints
2025-03-06 19:14:43,491 - INFO - validation batch 1, loss: 0.430, 32/10016 datapoints
2025-03-06 19:14:43,837 - INFO - validation batch 51, loss: 0.573, 1632/10016 datapoints
2025-03-06 19:14:44,229 - INFO - validation batch 101, loss: 0.362, 3232/10016 datapoints
2025-03-06 19:14:44,616 - INFO - validation batch 151, loss: 0.551, 4832/10016 datapoints
2025-03-06 19:14:45,043 - INFO - validation batch 201, loss: 0.532, 6432/10016 datapoints
2025-03-06 19:14:45,522 - INFO - validation batch 251, loss: 0.436, 8032/10016 datapoints
2025-03-06 19:14:45,779 - INFO - validation batch 301, loss: 0.498, 9632/10016 datapoints
2025-03-06 19:14:45,836 - INFO - Epoch 249/800 done.
2025-03-06 19:14:45,837 - INFO - Final validation performance:
Loss: 0.483, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:14:45,838 - INFO - Beginning epoch 250/800
2025-03-06 19:14:45,846 - INFO - training batch 1, loss: 0.460, 32/60000 datapoints
2025-03-06 19:14:46,233 - INFO - training batch 51, loss: 0.362, 1632/60000 datapoints
2025-03-06 19:14:46,573 - INFO - training batch 101, loss: 0.482, 3232/60000 datapoints
2025-03-06 19:14:46,902 - INFO - training batch 151, loss: 0.407, 4832/60000 datapoints
2025-03-06 19:14:47,279 - INFO - training batch 201, loss: 0.140, 6432/60000 datapoints
2025-03-06 19:14:47,645 - INFO - training batch 251, loss: 0.259, 8032/60000 datapoints
2025-03-06 19:14:47,979 - INFO - training batch 301, loss: 0.313, 9632/60000 datapoints
2025-03-06 19:14:48,322 - INFO - training batch 351, loss: 0.297, 11232/60000 datapoints
2025-03-06 19:14:48,692 - INFO - training batch 401, loss: 0.326, 12832/60000 datapoints
2025-03-06 19:14:49,084 - INFO - training batch 451, loss: 0.492, 14432/60000 datapoints
2025-03-06 19:14:49,444 - INFO - training batch 501, loss: 0.441, 16032/60000 datapoints
2025-03-06 19:14:49,804 - INFO - training batch 551, loss: 0.196, 17632/60000 datapoints
2025-03-06 19:14:50,192 - INFO - training batch 601, loss: 0.338, 19232/60000 datapoints
2025-03-06 19:14:50,670 - INFO - training batch 651, loss: 0.249, 20832/60000 datapoints
2025-03-06 19:14:51,025 - INFO - training batch 701, loss: 0.278, 22432/60000 datapoints
2025-03-06 19:14:51,351 - INFO - training batch 751, loss: 0.395, 24032/60000 datapoints
2025-03-06 19:14:51,674 - INFO - training batch 801, loss: 0.601, 25632/60000 datapoints
2025-03-06 19:14:52,098 - INFO - training batch 851, loss: 0.384, 27232/60000 datapoints
2025-03-06 19:14:52,442 - INFO - training batch 901, loss: 0.264, 28832/60000 datapoints
2025-03-06 19:14:52,842 - INFO - training batch 951, loss: 0.306, 30432/60000 datapoints
2025-03-06 19:14:53,161 - INFO - training batch 1001, loss: 0.314, 32032/60000 datapoints
2025-03-06 19:14:53,494 - INFO - training batch 1051, loss: 0.333, 33632/60000 datapoints
2025-03-06 19:14:53,848 - INFO - training batch 1101, loss: 0.478, 35232/60000 datapoints
2025-03-06 19:14:54,142 - INFO - training batch 1151, loss: 0.384, 36832/60000 datapoints
2025-03-06 19:14:54,484 - INFO - training batch 1201, loss: 0.293, 38432/60000 datapoints
2025-03-06 19:14:54,836 - INFO - training batch 1251, loss: 0.377, 40032/60000 datapoints
2025-03-06 19:14:55,209 - INFO - training batch 1301, loss: 0.499, 41632/60000 datapoints
2025-03-06 19:14:55,539 - INFO - training batch 1351, loss: 0.246, 43232/60000 datapoints
2025-03-06 19:14:55,914 - INFO - training batch 1401, loss: 0.512, 44832/60000 datapoints
2025-03-06 19:14:56,235 - INFO - training batch 1451, loss: 0.472, 46432/60000 datapoints
2025-03-06 19:14:56,523 - INFO - training batch 1501, loss: 0.231, 48032/60000 datapoints
2025-03-06 19:14:56,825 - INFO - training batch 1551, loss: 0.315, 49632/60000 datapoints
2025-03-06 19:14:57,101 - INFO - training batch 1601, loss: 0.680, 51232/60000 datapoints
2025-03-06 19:14:57,399 - INFO - training batch 1651, loss: 0.319, 52832/60000 datapoints
2025-03-06 19:14:57,715 - INFO - training batch 1701, loss: 0.333, 54432/60000 datapoints
2025-03-06 19:14:57,989 - INFO - training batch 1751, loss: 0.472, 56032/60000 datapoints
2025-03-06 19:14:58,283 - INFO - training batch 1801, loss: 0.184, 57632/60000 datapoints
2025-03-06 19:14:58,551 - INFO - training batch 1851, loss: 0.735, 59232/60000 datapoints
2025-03-06 19:14:58,690 - INFO - validation batch 1, loss: 0.206, 32/10016 datapoints
2025-03-06 19:14:58,897 - INFO - validation batch 51, loss: 0.248, 1632/10016 datapoints
2025-03-06 19:14:59,097 - INFO - validation batch 101, loss: 0.266, 3232/10016 datapoints
2025-03-06 19:14:59,294 - INFO - validation batch 151, loss: 0.717, 4832/10016 datapoints
2025-03-06 19:14:59,493 - INFO - validation batch 201, loss: 0.237, 6432/10016 datapoints
2025-03-06 19:14:59,708 - INFO - validation batch 251, loss: 0.286, 8032/10016 datapoints
2025-03-06 19:14:59,916 - INFO - validation batch 301, loss: 0.421, 9632/10016 datapoints
2025-03-06 19:14:59,961 - INFO - Epoch 250/800 done.
2025-03-06 19:14:59,962 - INFO - Final validation performance:
Loss: 0.340, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:14:59,963 - INFO - Beginning epoch 251/800
2025-03-06 19:14:59,974 - INFO - training batch 1, loss: 0.219, 32/60000 datapoints
2025-03-06 19:15:00,265 - INFO - training batch 51, loss: 0.315, 1632/60000 datapoints
2025-03-06 19:15:00,516 - INFO - training batch 101, loss: 0.080, 3232/60000 datapoints
2025-03-06 19:15:00,769 - INFO - training batch 151, loss: 0.507, 4832/60000 datapoints
2025-03-06 19:15:01,019 - INFO - training batch 201, loss: 0.308, 6432/60000 datapoints
2025-03-06 19:15:01,296 - INFO - training batch 251, loss: 0.280, 8032/60000 datapoints
2025-03-06 19:15:01,706 - INFO - training batch 301, loss: 0.214, 9632/60000 datapoints
2025-03-06 19:15:02,055 - INFO - training batch 351, loss: 0.396, 11232/60000 datapoints
2025-03-06 19:15:02,418 - INFO - training batch 401, loss: 0.236, 12832/60000 datapoints
2025-03-06 19:15:02,726 - INFO - training batch 451, loss: 0.214, 14432/60000 datapoints
2025-03-06 19:15:03,008 - INFO - training batch 501, loss: 0.313, 16032/60000 datapoints
2025-03-06 19:15:03,250 - INFO - training batch 551, loss: 0.380, 17632/60000 datapoints
2025-03-06 19:15:03,487 - INFO - training batch 601, loss: 0.135, 19232/60000 datapoints
2025-03-06 19:15:03,719 - INFO - training batch 651, loss: 0.216, 20832/60000 datapoints
2025-03-06 19:15:03,993 - INFO - training batch 701, loss: 0.125, 22432/60000 datapoints
2025-03-06 19:15:04,272 - INFO - training batch 751, loss: 0.278, 24032/60000 datapoints
2025-03-06 19:15:04,547 - INFO - training batch 801, loss: 0.280, 25632/60000 datapoints
2025-03-06 19:15:04,886 - INFO - training batch 851, loss: 0.508, 27232/60000 datapoints
2025-03-06 19:15:05,210 - INFO - training batch 901, loss: 0.157, 28832/60000 datapoints
2025-03-06 19:15:05,487 - INFO - training batch 951, loss: 0.356, 30432/60000 datapoints
2025-03-06 19:15:05,764 - INFO - training batch 1001, loss: 0.334, 32032/60000 datapoints
2025-03-06 19:15:06,135 - INFO - training batch 1051, loss: 0.623, 33632/60000 datapoints
2025-03-06 19:15:06,399 - INFO - training batch 1101, loss: 0.293, 35232/60000 datapoints
2025-03-06 19:15:06,711 - INFO - training batch 1151, loss: 0.267, 36832/60000 datapoints
2025-03-06 19:15:06,963 - INFO - training batch 1201, loss: 0.508, 38432/60000 datapoints
2025-03-06 19:15:07,264 - INFO - training batch 1251, loss: 0.164, 40032/60000 datapoints
2025-03-06 19:15:07,528 - INFO - training batch 1301, loss: 0.368, 41632/60000 datapoints
2025-03-06 19:15:07,879 - INFO - training batch 1351, loss: 0.468, 43232/60000 datapoints
2025-03-06 19:15:08,175 - INFO - training batch 1401, loss: 0.308, 44832/60000 datapoints
2025-03-06 19:15:08,453 - INFO - training batch 1451, loss: 0.284, 46432/60000 datapoints
2025-03-06 19:15:08,822 - INFO - training batch 1501, loss: 0.519, 48032/60000 datapoints
2025-03-06 19:15:09,200 - INFO - training batch 1551, loss: 0.607, 49632/60000 datapoints
2025-03-06 19:15:09,522 - INFO - training batch 1601, loss: 0.411, 51232/60000 datapoints
2025-03-06 19:15:09,981 - INFO - training batch 1651, loss: 0.331, 52832/60000 datapoints
2025-03-06 19:15:10,458 - INFO - training batch 1701, loss: 0.149, 54432/60000 datapoints
2025-03-06 19:15:10,813 - INFO - training batch 1751, loss: 0.423, 56032/60000 datapoints
2025-03-06 19:15:11,132 - INFO - training batch 1801, loss: 0.543, 57632/60000 datapoints
2025-03-06 19:15:11,432 - INFO - training batch 1851, loss: 0.293, 59232/60000 datapoints
2025-03-06 19:15:11,596 - INFO - validation batch 1, loss: 0.371, 32/10016 datapoints
2025-03-06 19:15:11,849 - INFO - validation batch 51, loss: 0.197, 1632/10016 datapoints
2025-03-06 19:15:12,097 - INFO - validation batch 101, loss: 0.296, 3232/10016 datapoints
2025-03-06 19:15:12,315 - INFO - validation batch 151, loss: 0.295, 4832/10016 datapoints
2025-03-06 19:15:12,546 - INFO - validation batch 201, loss: 0.140, 6432/10016 datapoints
2025-03-06 19:15:12,850 - INFO - validation batch 251, loss: 0.523, 8032/10016 datapoints
2025-03-06 19:15:13,163 - INFO - validation batch 301, loss: 0.365, 9632/10016 datapoints
2025-03-06 19:15:13,247 - INFO - Epoch 251/800 done.
2025-03-06 19:15:13,248 - INFO - Final validation performance:
Loss: 0.313, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:15:13,249 - INFO - Beginning epoch 252/800
2025-03-06 19:15:13,269 - INFO - training batch 1, loss: 0.272, 32/60000 datapoints
2025-03-06 19:15:13,833 - INFO - training batch 51, loss: 0.452, 1632/60000 datapoints
2025-03-06 19:15:14,733 - INFO - training batch 101, loss: 0.101, 3232/60000 datapoints
2025-03-06 19:15:15,331 - INFO - training batch 151, loss: 0.557, 4832/60000 datapoints
2025-03-06 19:15:15,803 - INFO - training batch 201, loss: 0.226, 6432/60000 datapoints
2025-03-06 19:15:16,319 - INFO - training batch 251, loss: 0.559, 8032/60000 datapoints
2025-03-06 19:15:16,736 - INFO - training batch 301, loss: 0.371, 9632/60000 datapoints
2025-03-06 19:15:17,172 - INFO - training batch 351, loss: 0.686, 11232/60000 datapoints
2025-03-06 19:15:17,605 - INFO - training batch 401, loss: 0.271, 12832/60000 datapoints
2025-03-06 19:15:17,974 - INFO - training batch 451, loss: 0.209, 14432/60000 datapoints
2025-03-06 19:15:18,339 - INFO - training batch 501, loss: 0.257, 16032/60000 datapoints
2025-03-06 19:15:18,740 - INFO - training batch 551, loss: 0.238, 17632/60000 datapoints
2025-03-06 19:15:19,090 - INFO - training batch 601, loss: 0.350, 19232/60000 datapoints
2025-03-06 19:15:19,421 - INFO - training batch 651, loss: 0.401, 20832/60000 datapoints
2025-03-06 19:15:19,872 - INFO - training batch 701, loss: 0.350, 22432/60000 datapoints
2025-03-06 19:15:20,288 - INFO - training batch 751, loss: 0.329, 24032/60000 datapoints
2025-03-06 19:15:20,830 - INFO - training batch 801, loss: 0.197, 25632/60000 datapoints
2025-03-06 19:15:21,184 - INFO - training batch 851, loss: 0.253, 27232/60000 datapoints
2025-03-06 19:15:21,496 - INFO - training batch 901, loss: 0.209, 28832/60000 datapoints
2025-03-06 19:15:21,840 - INFO - training batch 951, loss: 0.521, 30432/60000 datapoints
2025-03-06 19:15:22,172 - INFO - training batch 1001, loss: 0.559, 32032/60000 datapoints
2025-03-06 19:15:22,488 - INFO - training batch 1051, loss: 0.165, 33632/60000 datapoints
2025-03-06 19:15:22,788 - INFO - training batch 1101, loss: 0.208, 35232/60000 datapoints
2025-03-06 19:15:23,137 - INFO - training batch 1151, loss: 0.517, 36832/60000 datapoints
2025-03-06 19:15:23,434 - INFO - training batch 1201, loss: 0.228, 38432/60000 datapoints
2025-03-06 19:15:23,730 - INFO - training batch 1251, loss: 0.227, 40032/60000 datapoints
2025-03-06 19:15:24,019 - INFO - training batch 1301, loss: 0.453, 41632/60000 datapoints
2025-03-06 19:15:24,290 - INFO - training batch 1351, loss: 0.471, 43232/60000 datapoints
2025-03-06 19:15:24,672 - INFO - training batch 1401, loss: 0.375, 44832/60000 datapoints
2025-03-06 19:15:25,073 - INFO - training batch 1451, loss: 0.431, 46432/60000 datapoints
2025-03-06 19:15:25,371 - INFO - training batch 1501, loss: 0.214, 48032/60000 datapoints
2025-03-06 19:15:25,630 - INFO - training batch 1551, loss: 0.325, 49632/60000 datapoints
2025-03-06 19:15:25,891 - INFO - training batch 1601, loss: 0.627, 51232/60000 datapoints
2025-03-06 19:15:26,162 - INFO - training batch 1651, loss: 0.196, 52832/60000 datapoints
2025-03-06 19:15:26,392 - INFO - training batch 1701, loss: 0.638, 54432/60000 datapoints
2025-03-06 19:15:26,670 - INFO - training batch 1751, loss: 0.130, 56032/60000 datapoints
2025-03-06 19:15:26,908 - INFO - training batch 1801, loss: 0.333, 57632/60000 datapoints
2025-03-06 19:15:27,147 - INFO - training batch 1851, loss: 0.337, 59232/60000 datapoints
2025-03-06 19:15:27,270 - INFO - validation batch 1, loss: 0.456, 32/10016 datapoints
2025-03-06 19:15:27,449 - INFO - validation batch 51, loss: 0.353, 1632/10016 datapoints
2025-03-06 19:15:27,638 - INFO - validation batch 101, loss: 0.348, 3232/10016 datapoints
2025-03-06 19:15:27,821 - INFO - validation batch 151, loss: 0.291, 4832/10016 datapoints
2025-03-06 19:15:28,035 - INFO - validation batch 201, loss: 0.438, 6432/10016 datapoints
2025-03-06 19:15:28,219 - INFO - validation batch 251, loss: 0.418, 8032/10016 datapoints
2025-03-06 19:15:28,399 - INFO - validation batch 301, loss: 0.134, 9632/10016 datapoints
2025-03-06 19:15:28,447 - INFO - Epoch 252/800 done.
2025-03-06 19:15:28,447 - INFO - Final validation performance:
Loss: 0.348, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:15:28,448 - INFO - Beginning epoch 253/800
2025-03-06 19:15:28,458 - INFO - training batch 1, loss: 0.293, 32/60000 datapoints
2025-03-06 19:15:28,722 - INFO - training batch 51, loss: 0.163, 1632/60000 datapoints
2025-03-06 19:15:29,008 - INFO - training batch 101, loss: 0.224, 3232/60000 datapoints
2025-03-06 19:15:29,258 - INFO - training batch 151, loss: 0.192, 4832/60000 datapoints
2025-03-06 19:15:29,489 - INFO - training batch 201, loss: 0.159, 6432/60000 datapoints
2025-03-06 19:15:29,789 - INFO - training batch 251, loss: 0.511, 8032/60000 datapoints
2025-03-06 19:15:30,087 - INFO - training batch 301, loss: 0.406, 9632/60000 datapoints
2025-03-06 19:15:30,350 - INFO - training batch 351, loss: 0.254, 11232/60000 datapoints
2025-03-06 19:15:30,565 - INFO - training batch 401, loss: 0.223, 12832/60000 datapoints
2025-03-06 19:15:30,790 - INFO - training batch 451, loss: 0.129, 14432/60000 datapoints
2025-03-06 19:15:31,014 - INFO - training batch 501, loss: 0.402, 16032/60000 datapoints
2025-03-06 19:15:31,229 - INFO - training batch 551, loss: 0.559, 17632/60000 datapoints
2025-03-06 19:15:31,447 - INFO - training batch 601, loss: 0.312, 19232/60000 datapoints
2025-03-06 19:15:31,664 - INFO - training batch 651, loss: 0.437, 20832/60000 datapoints
2025-03-06 19:15:31,912 - INFO - training batch 701, loss: 0.284, 22432/60000 datapoints
2025-03-06 19:15:32,143 - INFO - training batch 751, loss: 0.135, 24032/60000 datapoints
2025-03-06 19:15:32,356 - INFO - training batch 801, loss: 0.231, 25632/60000 datapoints
2025-03-06 19:15:32,570 - INFO - training batch 851, loss: 0.431, 27232/60000 datapoints
2025-03-06 19:15:32,798 - INFO - training batch 901, loss: 0.418, 28832/60000 datapoints
2025-03-06 19:15:33,079 - INFO - training batch 951, loss: 0.595, 30432/60000 datapoints
2025-03-06 19:15:33,319 - INFO - training batch 1001, loss: 0.090, 32032/60000 datapoints
2025-03-06 19:15:33,573 - INFO - training batch 1051, loss: 0.358, 33632/60000 datapoints
2025-03-06 19:15:33,802 - INFO - training batch 1101, loss: 0.210, 35232/60000 datapoints
2025-03-06 19:15:34,037 - INFO - training batch 1151, loss: 0.465, 36832/60000 datapoints
2025-03-06 19:15:34,344 - INFO - training batch 1201, loss: 0.404, 38432/60000 datapoints
2025-03-06 19:15:34,578 - INFO - training batch 1251, loss: 0.592, 40032/60000 datapoints
2025-03-06 19:15:34,822 - INFO - training batch 1301, loss: 0.148, 41632/60000 datapoints
2025-03-06 19:15:35,070 - INFO - training batch 1351, loss: 0.250, 43232/60000 datapoints
2025-03-06 19:15:35,310 - INFO - training batch 1401, loss: 0.346, 44832/60000 datapoints
2025-03-06 19:15:35,549 - INFO - training batch 1451, loss: 0.263, 46432/60000 datapoints
2025-03-06 19:15:35,797 - INFO - training batch 1501, loss: 0.320, 48032/60000 datapoints
2025-03-06 19:15:36,039 - INFO - training batch 1551, loss: 0.372, 49632/60000 datapoints
2025-03-06 19:15:36,280 - INFO - training batch 1601, loss: 0.138, 51232/60000 datapoints
2025-03-06 19:15:36,509 - INFO - training batch 1651, loss: 0.274, 52832/60000 datapoints
2025-03-06 19:15:36,781 - INFO - training batch 1701, loss: 0.313, 54432/60000 datapoints
2025-03-06 19:15:37,048 - INFO - training batch 1751, loss: 0.244, 56032/60000 datapoints
2025-03-06 19:15:37,279 - INFO - training batch 1801, loss: 0.263, 57632/60000 datapoints
2025-03-06 19:15:37,519 - INFO - training batch 1851, loss: 0.324, 59232/60000 datapoints
2025-03-06 19:15:37,709 - INFO - validation batch 1, loss: 0.261, 32/10016 datapoints
2025-03-06 19:15:37,947 - INFO - validation batch 51, loss: 0.136, 1632/10016 datapoints
2025-03-06 19:15:38,161 - INFO - validation batch 101, loss: 0.443, 3232/10016 datapoints
2025-03-06 19:15:38,349 - INFO - validation batch 151, loss: 0.161, 4832/10016 datapoints
2025-03-06 19:15:38,600 - INFO - validation batch 201, loss: 0.213, 6432/10016 datapoints
2025-03-06 19:15:38,828 - INFO - validation batch 251, loss: 0.310, 8032/10016 datapoints
2025-03-06 19:15:39,075 - INFO - validation batch 301, loss: 0.729, 9632/10016 datapoints
2025-03-06 19:15:39,146 - INFO - Epoch 253/800 done.
2025-03-06 19:15:39,146 - INFO - Final validation performance:
Loss: 0.322, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:15:39,148 - INFO - Beginning epoch 254/800
2025-03-06 19:15:39,170 - INFO - training batch 1, loss: 0.526, 32/60000 datapoints
2025-03-06 19:15:39,521 - INFO - training batch 51, loss: 0.257, 1632/60000 datapoints
2025-03-06 19:15:39,805 - INFO - training batch 101, loss: 0.237, 3232/60000 datapoints
2025-03-06 19:15:40,119 - INFO - training batch 151, loss: 0.171, 4832/60000 datapoints
2025-03-06 19:15:40,395 - INFO - training batch 201, loss: 0.576, 6432/60000 datapoints
2025-03-06 19:15:40,708 - INFO - training batch 251, loss: 0.282, 8032/60000 datapoints
2025-03-06 19:15:41,021 - INFO - training batch 301, loss: 0.283, 9632/60000 datapoints
2025-03-06 19:15:41,307 - INFO - training batch 351, loss: 0.617, 11232/60000 datapoints
2025-03-06 19:15:41,682 - INFO - training batch 401, loss: 0.239, 12832/60000 datapoints
2025-03-06 19:15:42,265 - INFO - training batch 451, loss: 0.206, 14432/60000 datapoints
2025-03-06 19:15:42,644 - INFO - training batch 501, loss: 0.349, 16032/60000 datapoints
2025-03-06 19:15:42,953 - INFO - training batch 551, loss: 0.385, 17632/60000 datapoints
2025-03-06 19:15:43,251 - INFO - training batch 601, loss: 0.335, 19232/60000 datapoints
2025-03-06 19:15:43,545 - INFO - training batch 651, loss: 0.356, 20832/60000 datapoints
2025-03-06 19:15:43,833 - INFO - training batch 701, loss: 0.270, 22432/60000 datapoints
2025-03-06 19:15:44,158 - INFO - training batch 751, loss: 0.331, 24032/60000 datapoints
2025-03-06 19:15:44,473 - INFO - training batch 801, loss: 0.504, 25632/60000 datapoints
2025-03-06 19:15:44,743 - INFO - training batch 851, loss: 0.532, 27232/60000 datapoints
2025-03-06 19:15:45,023 - INFO - training batch 901, loss: 0.211, 28832/60000 datapoints
2025-03-06 19:15:45,361 - INFO - training batch 951, loss: 0.559, 30432/60000 datapoints
2025-03-06 19:15:45,700 - INFO - training batch 1001, loss: 0.343, 32032/60000 datapoints
2025-03-06 19:15:45,985 - INFO - training batch 1051, loss: 0.137, 33632/60000 datapoints
2025-03-06 19:15:46,265 - INFO - training batch 1101, loss: 0.228, 35232/60000 datapoints
2025-03-06 19:15:46,570 - INFO - training batch 1151, loss: 0.293, 36832/60000 datapoints
2025-03-06 19:15:46,876 - INFO - training batch 1201, loss: 0.615, 38432/60000 datapoints
2025-03-06 19:15:47,171 - INFO - training batch 1251, loss: 0.278, 40032/60000 datapoints
2025-03-06 19:15:47,439 - INFO - training batch 1301, loss: 0.184, 41632/60000 datapoints
2025-03-06 19:15:47,738 - INFO - training batch 1351, loss: 0.499, 43232/60000 datapoints
2025-03-06 19:15:48,006 - INFO - training batch 1401, loss: 0.304, 44832/60000 datapoints
2025-03-06 19:15:48,293 - INFO - training batch 1451, loss: 0.529, 46432/60000 datapoints
2025-03-06 19:15:48,565 - INFO - training batch 1501, loss: 0.211, 48032/60000 datapoints
2025-03-06 19:15:48,835 - INFO - training batch 1551, loss: 0.505, 49632/60000 datapoints
2025-03-06 19:15:49,089 - INFO - training batch 1601, loss: 0.220, 51232/60000 datapoints
2025-03-06 19:15:49,355 - INFO - training batch 1651, loss: 0.265, 52832/60000 datapoints
2025-03-06 19:15:49,703 - INFO - training batch 1701, loss: 0.141, 54432/60000 datapoints
2025-03-06 19:15:50,068 - INFO - training batch 1751, loss: 0.192, 56032/60000 datapoints
2025-03-06 19:15:50,376 - INFO - training batch 1801, loss: 0.302, 57632/60000 datapoints
2025-03-06 19:15:50,646 - INFO - training batch 1851, loss: 0.206, 59232/60000 datapoints
2025-03-06 19:15:50,804 - INFO - validation batch 1, loss: 0.431, 32/10016 datapoints
2025-03-06 19:15:51,023 - INFO - validation batch 51, loss: 0.350, 1632/10016 datapoints
2025-03-06 19:15:51,257 - INFO - validation batch 101, loss: 0.268, 3232/10016 datapoints
2025-03-06 19:15:51,451 - INFO - validation batch 151, loss: 0.303, 4832/10016 datapoints
2025-03-06 19:15:51,645 - INFO - validation batch 201, loss: 0.164, 6432/10016 datapoints
2025-03-06 19:15:51,834 - INFO - validation batch 251, loss: 0.398, 8032/10016 datapoints
2025-03-06 19:15:52,038 - INFO - validation batch 301, loss: 0.176, 9632/10016 datapoints
2025-03-06 19:15:52,086 - INFO - Epoch 254/800 done.
2025-03-06 19:15:52,086 - INFO - Final validation performance:
Loss: 0.299, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:15:52,087 - INFO - Beginning epoch 255/800
2025-03-06 19:15:52,094 - INFO - training batch 1, loss: 0.331, 32/60000 datapoints
2025-03-06 19:15:52,361 - INFO - training batch 51, loss: 0.287, 1632/60000 datapoints
2025-03-06 19:15:52,597 - INFO - training batch 101, loss: 0.126, 3232/60000 datapoints
2025-03-06 19:15:52,872 - INFO - training batch 151, loss: 0.223, 4832/60000 datapoints
2025-03-06 19:15:53,146 - INFO - training batch 201, loss: 0.590, 6432/60000 datapoints
2025-03-06 19:15:53,431 - INFO - training batch 251, loss: 0.473, 8032/60000 datapoints
2025-03-06 19:15:53,673 - INFO - training batch 301, loss: 0.217, 9632/60000 datapoints
2025-03-06 19:15:53,956 - INFO - training batch 351, loss: 0.240, 11232/60000 datapoints
2025-03-06 19:15:54,192 - INFO - training batch 401, loss: 0.199, 12832/60000 datapoints
2025-03-06 19:15:54,450 - INFO - training batch 451, loss: 0.418, 14432/60000 datapoints
2025-03-06 19:15:54,708 - INFO - training batch 501, loss: 0.302, 16032/60000 datapoints
2025-03-06 19:15:54,966 - INFO - training batch 551, loss: 0.348, 17632/60000 datapoints
2025-03-06 19:15:55,211 - INFO - training batch 601, loss: 0.457, 19232/60000 datapoints
2025-03-06 19:15:55,467 - INFO - training batch 651, loss: 0.447, 20832/60000 datapoints
2025-03-06 19:15:55,742 - INFO - training batch 701, loss: 0.228, 22432/60000 datapoints
2025-03-06 19:15:56,039 - INFO - training batch 751, loss: 0.379, 24032/60000 datapoints
2025-03-06 19:15:56,308 - INFO - training batch 801, loss: 0.240, 25632/60000 datapoints
2025-03-06 19:15:56,594 - INFO - training batch 851, loss: 0.372, 27232/60000 datapoints
2025-03-06 19:15:56,891 - INFO - training batch 901, loss: 0.445, 28832/60000 datapoints
2025-03-06 19:15:57,187 - INFO - training batch 951, loss: 0.284, 30432/60000 datapoints
2025-03-06 19:15:57,523 - INFO - training batch 1001, loss: 0.415, 32032/60000 datapoints
2025-03-06 19:15:58,075 - INFO - training batch 1051, loss: 0.134, 33632/60000 datapoints
2025-03-06 19:15:58,446 - INFO - training batch 1101, loss: 0.378, 35232/60000 datapoints
2025-03-06 19:15:58,735 - INFO - training batch 1151, loss: 0.415, 36832/60000 datapoints
2025-03-06 19:15:59,021 - INFO - training batch 1201, loss: 0.323, 38432/60000 datapoints
2025-03-06 19:15:59,292 - INFO - training batch 1251, loss: 0.355, 40032/60000 datapoints
2025-03-06 19:15:59,574 - INFO - training batch 1301, loss: 0.558, 41632/60000 datapoints
2025-03-06 19:16:00,034 - INFO - training batch 1351, loss: 0.311, 43232/60000 datapoints
2025-03-06 19:16:00,371 - INFO - training batch 1401, loss: 0.422, 44832/60000 datapoints
2025-03-06 19:16:00,666 - INFO - training batch 1451, loss: 0.522, 46432/60000 datapoints
2025-03-06 19:16:00,952 - INFO - training batch 1501, loss: 0.610, 48032/60000 datapoints
2025-03-06 19:16:01,243 - INFO - training batch 1551, loss: 0.170, 49632/60000 datapoints
2025-03-06 19:16:01,568 - INFO - training batch 1601, loss: 0.291, 51232/60000 datapoints
2025-03-06 19:16:01,920 - INFO - training batch 1651, loss: 0.372, 52832/60000 datapoints
2025-03-06 19:16:02,267 - INFO - training batch 1701, loss: 0.311, 54432/60000 datapoints
2025-03-06 19:16:02,544 - INFO - training batch 1751, loss: 0.155, 56032/60000 datapoints
2025-03-06 19:16:02,843 - INFO - training batch 1801, loss: 0.280, 57632/60000 datapoints
2025-03-06 19:16:03,141 - INFO - training batch 1851, loss: 0.195, 59232/60000 datapoints
2025-03-06 19:16:03,295 - INFO - validation batch 1, loss: 0.383, 32/10016 datapoints
2025-03-06 19:16:03,540 - INFO - validation batch 51, loss: 0.428, 1632/10016 datapoints
2025-03-06 19:16:03,792 - INFO - validation batch 101, loss: 0.162, 3232/10016 datapoints
2025-03-06 19:16:04,024 - INFO - validation batch 151, loss: 0.339, 4832/10016 datapoints
2025-03-06 19:16:04,251 - INFO - validation batch 201, loss: 0.210, 6432/10016 datapoints
2025-03-06 19:16:04,622 - INFO - validation batch 251, loss: 0.193, 8032/10016 datapoints
2025-03-06 19:16:05,001 - INFO - validation batch 301, loss: 0.421, 9632/10016 datapoints
2025-03-06 19:16:05,076 - INFO - Epoch 255/800 done.
2025-03-06 19:16:05,077 - INFO - Final validation performance:
Loss: 0.305, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:16:05,079 - INFO - Beginning epoch 256/800
2025-03-06 19:16:05,089 - INFO - training batch 1, loss: 0.480, 32/60000 datapoints
2025-03-06 19:16:05,462 - INFO - training batch 51, loss: 0.163, 1632/60000 datapoints
2025-03-06 19:16:05,784 - INFO - training batch 101, loss: 0.289, 3232/60000 datapoints
2025-03-06 19:16:06,065 - INFO - training batch 151, loss: 0.571, 4832/60000 datapoints
2025-03-06 19:16:06,393 - INFO - training batch 201, loss: 0.423, 6432/60000 datapoints
2025-03-06 19:16:06,697 - INFO - training batch 251, loss: 0.288, 8032/60000 datapoints
2025-03-06 19:16:06,987 - INFO - training batch 301, loss: 0.362, 9632/60000 datapoints
2025-03-06 19:16:07,271 - INFO - training batch 351, loss: 0.444, 11232/60000 datapoints
2025-03-06 19:16:07,549 - INFO - training batch 401, loss: 0.677, 12832/60000 datapoints
2025-03-06 19:16:07,848 - INFO - training batch 451, loss: 0.329, 14432/60000 datapoints
2025-03-06 19:16:08,116 - INFO - training batch 501, loss: 0.266, 16032/60000 datapoints
2025-03-06 19:16:08,389 - INFO - training batch 551, loss: 0.174, 17632/60000 datapoints
2025-03-06 19:16:08,655 - INFO - training batch 601, loss: 0.195, 19232/60000 datapoints
2025-03-06 19:16:08,923 - INFO - training batch 651, loss: 0.349, 20832/60000 datapoints
2025-03-06 19:16:09,185 - INFO - training batch 701, loss: 0.410, 22432/60000 datapoints
2025-03-06 19:16:09,436 - INFO - training batch 751, loss: 0.282, 24032/60000 datapoints
2025-03-06 19:16:09,867 - INFO - training batch 801, loss: 0.153, 25632/60000 datapoints
2025-03-06 19:16:10,168 - INFO - training batch 851, loss: 0.757, 27232/60000 datapoints
2025-03-06 19:16:10,402 - INFO - training batch 901, loss: 0.232, 28832/60000 datapoints
2025-03-06 19:16:10,651 - INFO - training batch 951, loss: 0.283, 30432/60000 datapoints
2025-03-06 19:16:10,883 - INFO - training batch 1001, loss: 0.167, 32032/60000 datapoints
2025-03-06 19:16:11,129 - INFO - training batch 1051, loss: 0.163, 33632/60000 datapoints
2025-03-06 19:16:11,369 - INFO - training batch 1101, loss: 0.384, 35232/60000 datapoints
2025-03-06 19:16:11,603 - INFO - training batch 1151, loss: 0.473, 36832/60000 datapoints
2025-03-06 19:16:11,844 - INFO - training batch 1201, loss: 0.339, 38432/60000 datapoints
2025-03-06 19:16:12,070 - INFO - training batch 1251, loss: 0.186, 40032/60000 datapoints
2025-03-06 19:16:12,289 - INFO - training batch 1301, loss: 0.364, 41632/60000 datapoints
2025-03-06 19:16:12,509 - INFO - training batch 1351, loss: 0.450, 43232/60000 datapoints
2025-03-06 19:16:12,742 - INFO - training batch 1401, loss: 0.150, 44832/60000 datapoints
2025-03-06 19:16:12,964 - INFO - training batch 1451, loss: 0.328, 46432/60000 datapoints
2025-03-06 19:16:13,186 - INFO - training batch 1501, loss: 0.144, 48032/60000 datapoints
2025-03-06 19:16:13,409 - INFO - training batch 1551, loss: 0.558, 49632/60000 datapoints
2025-03-06 19:16:13,630 - INFO - training batch 1601, loss: 0.359, 51232/60000 datapoints
2025-03-06 19:16:13,844 - INFO - training batch 1651, loss: 0.503, 52832/60000 datapoints
2025-03-06 19:16:14,149 - INFO - training batch 1701, loss: 0.333, 54432/60000 datapoints
2025-03-06 19:16:14,430 - INFO - training batch 1751, loss: 0.345, 56032/60000 datapoints
2025-03-06 19:16:14,722 - INFO - training batch 1801, loss: 0.359, 57632/60000 datapoints
2025-03-06 19:16:15,047 - INFO - training batch 1851, loss: 0.365, 59232/60000 datapoints
2025-03-06 19:16:15,204 - INFO - validation batch 1, loss: 0.222, 32/10016 datapoints
2025-03-06 19:16:15,456 - INFO - validation batch 51, loss: 0.345, 1632/10016 datapoints
2025-03-06 19:16:15,686 - INFO - validation batch 101, loss: 0.609, 3232/10016 datapoints
2025-03-06 19:16:15,931 - INFO - validation batch 151, loss: 0.470, 4832/10016 datapoints
2025-03-06 19:16:16,149 - INFO - validation batch 201, loss: 0.396, 6432/10016 datapoints
2025-03-06 19:16:16,410 - INFO - validation batch 251, loss: 0.488, 8032/10016 datapoints
2025-03-06 19:16:16,687 - INFO - validation batch 301, loss: 0.435, 9632/10016 datapoints
2025-03-06 19:16:16,775 - INFO - Epoch 256/800 done.
2025-03-06 19:16:16,775 - INFO - Final validation performance:
Loss: 0.423, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:16:16,776 - INFO - Beginning epoch 257/800
2025-03-06 19:16:16,783 - INFO - training batch 1, loss: 0.302, 32/60000 datapoints
2025-03-06 19:16:17,119 - INFO - training batch 51, loss: 0.130, 1632/60000 datapoints
2025-03-06 19:16:17,463 - INFO - training batch 101, loss: 0.491, 3232/60000 datapoints
2025-03-06 19:16:17,747 - INFO - training batch 151, loss: 0.313, 4832/60000 datapoints
2025-03-06 19:16:18,029 - INFO - training batch 201, loss: 0.456, 6432/60000 datapoints
2025-03-06 19:16:18,343 - INFO - training batch 251, loss: 0.136, 8032/60000 datapoints
2025-03-06 19:16:18,653 - INFO - training batch 301, loss: 0.176, 9632/60000 datapoints
2025-03-06 19:16:18,941 - INFO - training batch 351, loss: 0.383, 11232/60000 datapoints
2025-03-06 19:16:19,186 - INFO - training batch 401, loss: 0.247, 12832/60000 datapoints
2025-03-06 19:16:19,436 - INFO - training batch 451, loss: 0.607, 14432/60000 datapoints
2025-03-06 19:16:19,695 - INFO - training batch 501, loss: 0.471, 16032/60000 datapoints
2025-03-06 19:16:19,928 - INFO - training batch 551, loss: 0.166, 17632/60000 datapoints
2025-03-06 19:16:20,174 - INFO - training batch 601, loss: 0.633, 19232/60000 datapoints
2025-03-06 19:16:20,409 - INFO - training batch 651, loss: 0.447, 20832/60000 datapoints
2025-03-06 19:16:20,648 - INFO - training batch 701, loss: 0.149, 22432/60000 datapoints
2025-03-06 19:16:20,898 - INFO - training batch 751, loss: 0.465, 24032/60000 datapoints
2025-03-06 19:16:21,139 - INFO - training batch 801, loss: 0.552, 25632/60000 datapoints
2025-03-06 19:16:21,387 - INFO - training batch 851, loss: 0.170, 27232/60000 datapoints
2025-03-06 19:16:21,640 - INFO - training batch 901, loss: 0.329, 28832/60000 datapoints
2025-03-06 19:16:21,884 - INFO - training batch 951, loss: 0.554, 30432/60000 datapoints
2025-03-06 19:16:22,126 - INFO - training batch 1001, loss: 0.312, 32032/60000 datapoints
2025-03-06 19:16:22,395 - INFO - training batch 1051, loss: 0.210, 33632/60000 datapoints
2025-03-06 19:16:22,646 - INFO - training batch 1101, loss: 0.327, 35232/60000 datapoints
2025-03-06 19:16:22,905 - INFO - training batch 1151, loss: 0.290, 36832/60000 datapoints
2025-03-06 19:16:23,151 - INFO - training batch 1201, loss: 0.149, 38432/60000 datapoints
2025-03-06 19:16:23,404 - INFO - training batch 1251, loss: 0.308, 40032/60000 datapoints
2025-03-06 19:16:23,654 - INFO - training batch 1301, loss: 0.487, 41632/60000 datapoints
2025-03-06 19:16:23,899 - INFO - training batch 1351, loss: 0.359, 43232/60000 datapoints
2025-03-06 19:16:24,146 - INFO - training batch 1401, loss: 0.351, 44832/60000 datapoints
2025-03-06 19:16:24,400 - INFO - training batch 1451, loss: 0.330, 46432/60000 datapoints
2025-03-06 19:16:24,646 - INFO - training batch 1501, loss: 0.291, 48032/60000 datapoints
2025-03-06 19:16:24,900 - INFO - training batch 1551, loss: 0.293, 49632/60000 datapoints
2025-03-06 19:16:25,145 - INFO - training batch 1601, loss: 0.631, 51232/60000 datapoints
2025-03-06 19:16:25,395 - INFO - training batch 1651, loss: 0.230, 52832/60000 datapoints
2025-03-06 19:16:25,643 - INFO - training batch 1701, loss: 0.281, 54432/60000 datapoints
2025-03-06 19:16:25,888 - INFO - training batch 1751, loss: 0.188, 56032/60000 datapoints
2025-03-06 19:16:26,131 - INFO - training batch 1801, loss: 0.154, 57632/60000 datapoints
2025-03-06 19:16:26,373 - INFO - training batch 1851, loss: 0.226, 59232/60000 datapoints
2025-03-06 19:16:26,501 - INFO - validation batch 1, loss: 0.298, 32/10016 datapoints
2025-03-06 19:16:26,706 - INFO - validation batch 51, loss: 0.135, 1632/10016 datapoints
2025-03-06 19:16:26,900 - INFO - validation batch 101, loss: 0.384, 3232/10016 datapoints
2025-03-06 19:16:27,089 - INFO - validation batch 151, loss: 0.169, 4832/10016 datapoints
2025-03-06 19:16:27,286 - INFO - validation batch 201, loss: 0.237, 6432/10016 datapoints
2025-03-06 19:16:27,495 - INFO - validation batch 251, loss: 0.206, 8032/10016 datapoints
2025-03-06 19:16:27,700 - INFO - validation batch 301, loss: 0.205, 9632/10016 datapoints
2025-03-06 19:16:27,750 - INFO - Epoch 257/800 done.
2025-03-06 19:16:27,750 - INFO - Final validation performance:
Loss: 0.233, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:16:27,750 - INFO - Beginning epoch 258/800
2025-03-06 19:16:27,761 - INFO - training batch 1, loss: 0.374, 32/60000 datapoints
2025-03-06 19:16:28,022 - INFO - training batch 51, loss: 0.701, 1632/60000 datapoints
2025-03-06 19:16:28,266 - INFO - training batch 101, loss: 0.364, 3232/60000 datapoints
2025-03-06 19:16:28,517 - INFO - training batch 151, loss: 0.242, 4832/60000 datapoints
2025-03-06 19:16:28,791 - INFO - training batch 201, loss: 0.263, 6432/60000 datapoints
2025-03-06 19:16:29,029 - INFO - training batch 251, loss: 0.204, 8032/60000 datapoints
2025-03-06 19:16:29,258 - INFO - training batch 301, loss: 0.323, 9632/60000 datapoints
2025-03-06 19:16:29,494 - INFO - training batch 351, loss: 0.168, 11232/60000 datapoints
2025-03-06 19:16:29,740 - INFO - training batch 401, loss: 0.256, 12832/60000 datapoints
2025-03-06 19:16:29,996 - INFO - training batch 451, loss: 0.226, 14432/60000 datapoints
2025-03-06 19:16:30,276 - INFO - training batch 501, loss: 0.194, 16032/60000 datapoints
2025-03-06 19:16:30,512 - INFO - training batch 551, loss: 0.247, 17632/60000 datapoints
2025-03-06 19:16:30,759 - INFO - training batch 601, loss: 0.408, 19232/60000 datapoints
2025-03-06 19:16:31,006 - INFO - training batch 651, loss: 0.402, 20832/60000 datapoints
2025-03-06 19:16:31,260 - INFO - training batch 701, loss: 0.371, 22432/60000 datapoints
2025-03-06 19:16:31,500 - INFO - training batch 751, loss: 0.412, 24032/60000 datapoints
2025-03-06 19:16:31,738 - INFO - training batch 801, loss: 0.232, 25632/60000 datapoints
2025-03-06 19:16:31,983 - INFO - training batch 851, loss: 0.294, 27232/60000 datapoints
2025-03-06 19:16:32,242 - INFO - training batch 901, loss: 0.295, 28832/60000 datapoints
2025-03-06 19:16:32,511 - INFO - training batch 951, loss: 0.301, 30432/60000 datapoints
2025-03-06 19:16:32,766 - INFO - training batch 1001, loss: 0.378, 32032/60000 datapoints
2025-03-06 19:16:33,007 - INFO - training batch 1051, loss: 0.239, 33632/60000 datapoints
2025-03-06 19:16:33,268 - INFO - training batch 1101, loss: 0.325, 35232/60000 datapoints
2025-03-06 19:16:33,533 - INFO - training batch 1151, loss: 0.520, 36832/60000 datapoints
2025-03-06 19:16:33,794 - INFO - training batch 1201, loss: 0.404, 38432/60000 datapoints
2025-03-06 19:16:34,046 - INFO - training batch 1251, loss: 0.191, 40032/60000 datapoints
2025-03-06 19:16:34,297 - INFO - training batch 1301, loss: 0.160, 41632/60000 datapoints
2025-03-06 19:16:34,563 - INFO - training batch 1351, loss: 0.596, 43232/60000 datapoints
2025-03-06 19:16:34,848 - INFO - training batch 1401, loss: 0.403, 44832/60000 datapoints
2025-03-06 19:16:35,087 - INFO - training batch 1451, loss: 0.262, 46432/60000 datapoints
2025-03-06 19:16:35,328 - INFO - training batch 1501, loss: 0.335, 48032/60000 datapoints
2025-03-06 19:16:35,590 - INFO - training batch 1551, loss: 0.240, 49632/60000 datapoints
2025-03-06 19:16:35,838 - INFO - training batch 1601, loss: 0.281, 51232/60000 datapoints
2025-03-06 19:16:36,084 - INFO - training batch 1651, loss: 0.229, 52832/60000 datapoints
2025-03-06 19:16:36,335 - INFO - training batch 1701, loss: 0.341, 54432/60000 datapoints
2025-03-06 19:16:36,564 - INFO - training batch 1751, loss: 0.679, 56032/60000 datapoints
2025-03-06 19:16:37,520 - INFO - training batch 1801, loss: 0.375, 57632/60000 datapoints
2025-03-06 19:16:37,940 - INFO - training batch 1851, loss: 0.655, 59232/60000 datapoints
2025-03-06 19:16:38,075 - INFO - validation batch 1, loss: 0.320, 32/10016 datapoints
2025-03-06 19:16:38,283 - INFO - validation batch 51, loss: 0.335, 1632/10016 datapoints
2025-03-06 19:16:38,481 - INFO - validation batch 101, loss: 0.415, 3232/10016 datapoints
2025-03-06 19:16:38,684 - INFO - validation batch 151, loss: 0.471, 4832/10016 datapoints
2025-03-06 19:16:38,905 - INFO - validation batch 201, loss: 0.372, 6432/10016 datapoints
2025-03-06 19:16:39,094 - INFO - validation batch 251, loss: 0.457, 8032/10016 datapoints
2025-03-06 19:16:39,300 - INFO - validation batch 301, loss: 0.428, 9632/10016 datapoints
2025-03-06 19:16:39,360 - INFO - Epoch 258/800 done.
2025-03-06 19:16:39,360 - INFO - Final validation performance:
Loss: 0.400, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:16:39,361 - INFO - Beginning epoch 259/800
2025-03-06 19:16:39,399 - INFO - training batch 1, loss: 0.253, 32/60000 datapoints
2025-03-06 19:16:39,678 - INFO - training batch 51, loss: 0.250, 1632/60000 datapoints
2025-03-06 19:16:39,936 - INFO - training batch 101, loss: 0.259, 3232/60000 datapoints
2025-03-06 19:16:40,184 - INFO - training batch 151, loss: 0.252, 4832/60000 datapoints
2025-03-06 19:16:40,859 - INFO - training batch 201, loss: 0.257, 6432/60000 datapoints
2025-03-06 19:16:41,321 - INFO - training batch 251, loss: 0.204, 8032/60000 datapoints
2025-03-06 19:16:41,557 - INFO - training batch 301, loss: 0.402, 9632/60000 datapoints
2025-03-06 19:16:41,785 - INFO - training batch 351, loss: 0.374, 11232/60000 datapoints
2025-03-06 19:16:42,003 - INFO - training batch 401, loss: 0.631, 12832/60000 datapoints
2025-03-06 19:16:42,225 - INFO - training batch 451, loss: 0.271, 14432/60000 datapoints
2025-03-06 19:16:42,446 - INFO - training batch 501, loss: 0.082, 16032/60000 datapoints
2025-03-06 19:16:42,662 - INFO - training batch 551, loss: 0.149, 17632/60000 datapoints
2025-03-06 19:16:42,873 - INFO - training batch 601, loss: 1.109, 19232/60000 datapoints
2025-03-06 19:16:43,084 - INFO - training batch 651, loss: 0.116, 20832/60000 datapoints
2025-03-06 19:16:43,296 - INFO - training batch 701, loss: 0.336, 22432/60000 datapoints
2025-03-06 19:16:43,516 - INFO - training batch 751, loss: 0.435, 24032/60000 datapoints
2025-03-06 19:16:43,735 - INFO - training batch 801, loss: 0.474, 25632/60000 datapoints
2025-03-06 19:16:43,946 - INFO - training batch 851, loss: 0.240, 27232/60000 datapoints
2025-03-06 19:16:44,159 - INFO - training batch 901, loss: 0.234, 28832/60000 datapoints
2025-03-06 19:16:44,381 - INFO - training batch 951, loss: 0.245, 30432/60000 datapoints
2025-03-06 19:16:44,600 - INFO - training batch 1001, loss: 0.308, 32032/60000 datapoints
2025-03-06 19:16:44,814 - INFO - training batch 1051, loss: 0.391, 33632/60000 datapoints
2025-03-06 19:16:45,032 - INFO - training batch 1101, loss: 0.542, 35232/60000 datapoints
2025-03-06 19:16:45,240 - INFO - training batch 1151, loss: 0.198, 36832/60000 datapoints
2025-03-06 19:16:45,454 - INFO - training batch 1201, loss: 0.258, 38432/60000 datapoints
2025-03-06 19:16:45,675 - INFO - training batch 1251, loss: 0.352, 40032/60000 datapoints
2025-03-06 19:16:45,888 - INFO - training batch 1301, loss: 0.300, 41632/60000 datapoints
2025-03-06 19:16:46,095 - INFO - training batch 1351, loss: 0.242, 43232/60000 datapoints
2025-03-06 19:16:46,309 - INFO - training batch 1401, loss: 0.316, 44832/60000 datapoints
2025-03-06 19:16:46,524 - INFO - training batch 1451, loss: 0.300, 46432/60000 datapoints
2025-03-06 19:16:46,742 - INFO - training batch 1501, loss: 0.317, 48032/60000 datapoints
2025-03-06 19:16:46,952 - INFO - training batch 1551, loss: 0.312, 49632/60000 datapoints
2025-03-06 19:16:47,168 - INFO - training batch 1601, loss: 0.290, 51232/60000 datapoints
2025-03-06 19:16:47,385 - INFO - training batch 1651, loss: 0.344, 52832/60000 datapoints
2025-03-06 19:16:47,599 - INFO - training batch 1701, loss: 0.500, 54432/60000 datapoints
2025-03-06 19:16:47,811 - INFO - training batch 1751, loss: 0.246, 56032/60000 datapoints
2025-03-06 19:16:48,024 - INFO - training batch 1801, loss: 0.290, 57632/60000 datapoints
2025-03-06 19:16:48,246 - INFO - training batch 1851, loss: 0.342, 59232/60000 datapoints
2025-03-06 19:16:48,360 - INFO - validation batch 1, loss: 0.317, 32/10016 datapoints
2025-03-06 19:16:48,534 - INFO - validation batch 51, loss: 0.261, 1632/10016 datapoints
2025-03-06 19:16:48,718 - INFO - validation batch 101, loss: 0.307, 3232/10016 datapoints
2025-03-06 19:16:48,919 - INFO - validation batch 151, loss: 0.203, 4832/10016 datapoints
2025-03-06 19:16:49,098 - INFO - validation batch 201, loss: 0.135, 6432/10016 datapoints
2025-03-06 19:16:49,269 - INFO - validation batch 251, loss: 0.201, 8032/10016 datapoints
2025-03-06 19:16:49,447 - INFO - validation batch 301, loss: 0.192, 9632/10016 datapoints
2025-03-06 19:16:49,496 - INFO - Epoch 259/800 done.
2025-03-06 19:16:49,496 - INFO - Final validation performance:
Loss: 0.231, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:16:49,497 - INFO - Beginning epoch 260/800
2025-03-06 19:16:49,505 - INFO - training batch 1, loss: 0.316, 32/60000 datapoints
2025-03-06 19:16:49,737 - INFO - training batch 51, loss: 0.228, 1632/60000 datapoints
2025-03-06 19:16:49,969 - INFO - training batch 101, loss: 0.452, 3232/60000 datapoints
2025-03-06 19:16:50,201 - INFO - training batch 151, loss: 0.148, 4832/60000 datapoints
2025-03-06 19:16:50,427 - INFO - training batch 201, loss: 0.184, 6432/60000 datapoints
2025-03-06 19:16:50,662 - INFO - training batch 251, loss: 0.370, 8032/60000 datapoints
2025-03-06 19:16:50,889 - INFO - training batch 301, loss: 0.526, 9632/60000 datapoints
2025-03-06 19:16:51,118 - INFO - training batch 351, loss: 0.567, 11232/60000 datapoints
2025-03-06 19:16:51,344 - INFO - training batch 401, loss: 0.515, 12832/60000 datapoints
2025-03-06 19:16:51,576 - INFO - training batch 451, loss: 0.450, 14432/60000 datapoints
2025-03-06 19:16:51,823 - INFO - training batch 501, loss: 0.335, 16032/60000 datapoints
2025-03-06 19:16:52,057 - INFO - training batch 551, loss: 0.433, 17632/60000 datapoints
2025-03-06 19:16:52,283 - INFO - training batch 601, loss: 0.681, 19232/60000 datapoints
2025-03-06 19:16:52,516 - INFO - training batch 651, loss: 0.221, 20832/60000 datapoints
2025-03-06 19:16:52,754 - INFO - training batch 701, loss: 0.422, 22432/60000 datapoints
2025-03-06 19:16:52,987 - INFO - training batch 751, loss: 0.518, 24032/60000 datapoints
2025-03-06 19:16:53,214 - INFO - training batch 801, loss: 0.167, 25632/60000 datapoints
2025-03-06 19:16:53,453 - INFO - training batch 851, loss: 0.323, 27232/60000 datapoints
2025-03-06 19:16:53,694 - INFO - training batch 901, loss: 0.250, 28832/60000 datapoints
2025-03-06 19:16:53,942 - INFO - training batch 951, loss: 0.258, 30432/60000 datapoints
2025-03-06 19:16:54,176 - INFO - training batch 1001, loss: 0.229, 32032/60000 datapoints
2025-03-06 19:16:54,400 - INFO - training batch 1051, loss: 0.121, 33632/60000 datapoints
2025-03-06 19:16:54,640 - INFO - training batch 1101, loss: 0.320, 35232/60000 datapoints
2025-03-06 19:16:54,872 - INFO - training batch 1151, loss: 0.408, 36832/60000 datapoints
2025-03-06 19:16:55,101 - INFO - training batch 1201, loss: 0.427, 38432/60000 datapoints
2025-03-06 19:16:55,328 - INFO - training batch 1251, loss: 0.297, 40032/60000 datapoints
2025-03-06 19:16:55,561 - INFO - training batch 1301, loss: 0.673, 41632/60000 datapoints
2025-03-06 19:16:55,795 - INFO - training batch 1351, loss: 0.367, 43232/60000 datapoints
2025-03-06 19:16:56,022 - INFO - training batch 1401, loss: 0.265, 44832/60000 datapoints
2025-03-06 19:16:56,250 - INFO - training batch 1451, loss: 0.369, 46432/60000 datapoints
2025-03-06 19:16:56,479 - INFO - training batch 1501, loss: 0.228, 48032/60000 datapoints
2025-03-06 19:16:56,711 - INFO - training batch 1551, loss: 0.311, 49632/60000 datapoints
2025-03-06 19:16:56,940 - INFO - training batch 1601, loss: 0.239, 51232/60000 datapoints
2025-03-06 19:16:57,172 - INFO - training batch 1651, loss: 0.511, 52832/60000 datapoints
2025-03-06 19:16:57,399 - INFO - training batch 1701, loss: 0.300, 54432/60000 datapoints
2025-03-06 19:16:57,628 - INFO - training batch 1751, loss: 0.422, 56032/60000 datapoints
2025-03-06 19:16:57,858 - INFO - training batch 1801, loss: 0.145, 57632/60000 datapoints
2025-03-06 19:16:58,082 - INFO - training batch 1851, loss: 0.308, 59232/60000 datapoints
2025-03-06 19:16:58,210 - INFO - validation batch 1, loss: 0.382, 32/10016 datapoints
2025-03-06 19:16:58,396 - INFO - validation batch 51, loss: 0.461, 1632/10016 datapoints
2025-03-06 19:16:58,584 - INFO - validation batch 101, loss: 0.452, 3232/10016 datapoints
2025-03-06 19:16:58,771 - INFO - validation batch 151, loss: 0.121, 4832/10016 datapoints
2025-03-06 19:16:58,967 - INFO - validation batch 201, loss: 0.264, 6432/10016 datapoints
2025-03-06 19:16:59,174 - INFO - validation batch 251, loss: 0.272, 8032/10016 datapoints
2025-03-06 19:16:59,360 - INFO - validation batch 301, loss: 0.443, 9632/10016 datapoints
2025-03-06 19:16:59,407 - INFO - Epoch 260/800 done.
2025-03-06 19:16:59,407 - INFO - Final validation performance:
Loss: 0.342, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:16:59,408 - INFO - Beginning epoch 261/800
2025-03-06 19:16:59,420 - INFO - training batch 1, loss: 0.309, 32/60000 datapoints
2025-03-06 19:16:59,663 - INFO - training batch 51, loss: 0.355, 1632/60000 datapoints
2025-03-06 19:16:59,894 - INFO - training batch 101, loss: 0.143, 3232/60000 datapoints
2025-03-06 19:17:00,122 - INFO - training batch 151, loss: 0.431, 4832/60000 datapoints
2025-03-06 19:17:00,347 - INFO - training batch 201, loss: 0.221, 6432/60000 datapoints
2025-03-06 19:17:00,565 - INFO - training batch 251, loss: 0.308, 8032/60000 datapoints
2025-03-06 19:17:00,788 - INFO - training batch 301, loss: 0.418, 9632/60000 datapoints
2025-03-06 19:17:01,012 - INFO - training batch 351, loss: 0.260, 11232/60000 datapoints
2025-03-06 19:17:01,235 - INFO - training batch 401, loss: 0.394, 12832/60000 datapoints
2025-03-06 19:17:01,458 - INFO - training batch 451, loss: 0.213, 14432/60000 datapoints
2025-03-06 19:17:01,682 - INFO - training batch 501, loss: 0.471, 16032/60000 datapoints
2025-03-06 19:17:01,903 - INFO - training batch 551, loss: 0.380, 17632/60000 datapoints
2025-03-06 19:17:02,114 - INFO - training batch 601, loss: 0.400, 19232/60000 datapoints
2025-03-06 19:17:02,338 - INFO - training batch 651, loss: 0.241, 20832/60000 datapoints
2025-03-06 19:17:02,550 - INFO - training batch 701, loss: 0.460, 22432/60000 datapoints
2025-03-06 19:17:02,768 - INFO - training batch 751, loss: 0.497, 24032/60000 datapoints
2025-03-06 19:17:02,980 - INFO - training batch 801, loss: 0.291, 25632/60000 datapoints
2025-03-06 19:17:03,205 - INFO - training batch 851, loss: 0.191, 27232/60000 datapoints
2025-03-06 19:17:03,425 - INFO - training batch 901, loss: 0.269, 28832/60000 datapoints
2025-03-06 19:17:03,642 - INFO - training batch 951, loss: 0.542, 30432/60000 datapoints
2025-03-06 19:17:03,858 - INFO - training batch 1001, loss: 0.230, 32032/60000 datapoints
2025-03-06 19:17:04,074 - INFO - training batch 1051, loss: 0.387, 33632/60000 datapoints
2025-03-06 19:17:04,290 - INFO - training batch 1101, loss: 0.095, 35232/60000 datapoints
2025-03-06 19:17:04,498 - INFO - training batch 1151, loss: 0.139, 36832/60000 datapoints
2025-03-06 19:17:04,719 - INFO - training batch 1201, loss: 0.141, 38432/60000 datapoints
2025-03-06 19:17:04,934 - INFO - training batch 1251, loss: 0.558, 40032/60000 datapoints
2025-03-06 19:17:05,143 - INFO - training batch 1301, loss: 0.253, 41632/60000 datapoints
2025-03-06 19:17:05,357 - INFO - training batch 1351, loss: 0.540, 43232/60000 datapoints
2025-03-06 19:17:05,576 - INFO - training batch 1401, loss: 0.417, 44832/60000 datapoints
2025-03-06 19:17:05,791 - INFO - training batch 1451, loss: 0.281, 46432/60000 datapoints
2025-03-06 19:17:05,997 - INFO - training batch 1501, loss: 0.566, 48032/60000 datapoints
2025-03-06 19:17:06,201 - INFO - training batch 1551, loss: 0.210, 49632/60000 datapoints
2025-03-06 19:17:06,404 - INFO - training batch 1601, loss: 0.376, 51232/60000 datapoints
2025-03-06 19:17:06,613 - INFO - training batch 1651, loss: 0.127, 52832/60000 datapoints
2025-03-06 19:17:06,821 - INFO - training batch 1701, loss: 0.430, 54432/60000 datapoints
2025-03-06 19:17:07,028 - INFO - training batch 1751, loss: 0.384, 56032/60000 datapoints
2025-03-06 19:17:07,235 - INFO - training batch 1801, loss: 0.136, 57632/60000 datapoints
2025-03-06 19:17:07,444 - INFO - training batch 1851, loss: 0.919, 59232/60000 datapoints
2025-03-06 19:17:07,549 - INFO - validation batch 1, loss: 0.086, 32/10016 datapoints
2025-03-06 19:17:07,721 - INFO - validation batch 51, loss: 0.244, 1632/10016 datapoints
2025-03-06 19:17:07,890 - INFO - validation batch 101, loss: 0.270, 3232/10016 datapoints
2025-03-06 19:17:08,057 - INFO - validation batch 151, loss: 0.179, 4832/10016 datapoints
2025-03-06 19:17:08,223 - INFO - validation batch 201, loss: 0.355, 6432/10016 datapoints
2025-03-06 19:17:08,389 - INFO - validation batch 251, loss: 0.107, 8032/10016 datapoints
2025-03-06 19:17:08,555 - INFO - validation batch 301, loss: 0.224, 9632/10016 datapoints
2025-03-06 19:17:08,596 - INFO - Epoch 261/800 done.
2025-03-06 19:17:08,596 - INFO - Final validation performance:
Loss: 0.209, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:17:08,597 - INFO - Beginning epoch 262/800
2025-03-06 19:17:08,604 - INFO - training batch 1, loss: 0.346, 32/60000 datapoints
2025-03-06 19:17:08,837 - INFO - training batch 51, loss: 0.339, 1632/60000 datapoints
2025-03-06 19:17:09,073 - INFO - training batch 101, loss: 0.287, 3232/60000 datapoints
2025-03-06 19:17:09,323 - INFO - training batch 151, loss: 0.295, 4832/60000 datapoints
2025-03-06 19:17:09,538 - INFO - training batch 201, loss: 0.606, 6432/60000 datapoints
2025-03-06 19:17:09,758 - INFO - training batch 251, loss: 0.253, 8032/60000 datapoints
2025-03-06 19:17:09,969 - INFO - training batch 301, loss: 0.395, 9632/60000 datapoints
2025-03-06 19:17:10,176 - INFO - training batch 351, loss: 0.365, 11232/60000 datapoints
2025-03-06 19:17:10,383 - INFO - training batch 401, loss: 0.394, 12832/60000 datapoints
2025-03-06 19:17:10,592 - INFO - training batch 451, loss: 0.311, 14432/60000 datapoints
2025-03-06 19:17:10,808 - INFO - training batch 501, loss: 0.127, 16032/60000 datapoints
2025-03-06 19:17:11,041 - INFO - training batch 551, loss: 0.370, 17632/60000 datapoints
2025-03-06 19:17:11,253 - INFO - training batch 601, loss: 0.257, 19232/60000 datapoints
2025-03-06 19:17:11,484 - INFO - training batch 651, loss: 0.310, 20832/60000 datapoints
2025-03-06 19:17:11,708 - INFO - training batch 701, loss: 0.390, 22432/60000 datapoints
2025-03-06 19:17:11,939 - INFO - training batch 751, loss: 0.357, 24032/60000 datapoints
2025-03-06 19:17:12,171 - INFO - training batch 801, loss: 0.241, 25632/60000 datapoints
2025-03-06 19:17:12,398 - INFO - training batch 851, loss: 0.197, 27232/60000 datapoints
2025-03-06 19:17:12,635 - INFO - training batch 901, loss: 0.437, 28832/60000 datapoints
2025-03-06 19:17:12,860 - INFO - training batch 951, loss: 0.155, 30432/60000 datapoints
2025-03-06 19:17:13,084 - INFO - training batch 1001, loss: 0.392, 32032/60000 datapoints
2025-03-06 19:17:13,310 - INFO - training batch 1051, loss: 0.265, 33632/60000 datapoints
2025-03-06 19:17:13,540 - INFO - training batch 1101, loss: 0.352, 35232/60000 datapoints
2025-03-06 19:17:13,775 - INFO - training batch 1151, loss: 0.246, 36832/60000 datapoints
2025-03-06 19:17:14,002 - INFO - training batch 1201, loss: 0.268, 38432/60000 datapoints
2025-03-06 19:17:14,231 - INFO - training batch 1251, loss: 0.206, 40032/60000 datapoints
2025-03-06 19:17:14,460 - INFO - training batch 1301, loss: 0.193, 41632/60000 datapoints
2025-03-06 19:17:14,702 - INFO - training batch 1351, loss: 0.285, 43232/60000 datapoints
2025-03-06 19:17:14,984 - INFO - training batch 1401, loss: 0.389, 44832/60000 datapoints
2025-03-06 19:17:15,268 - INFO - training batch 1451, loss: 0.341, 46432/60000 datapoints
2025-03-06 19:17:15,496 - INFO - training batch 1501, loss: 0.150, 48032/60000 datapoints
2025-03-06 19:17:15,727 - INFO - training batch 1551, loss: 0.250, 49632/60000 datapoints
2025-03-06 19:17:15,954 - INFO - training batch 1601, loss: 0.128, 51232/60000 datapoints
2025-03-06 19:17:16,180 - INFO - training batch 1651, loss: 0.317, 52832/60000 datapoints
2025-03-06 19:17:16,411 - INFO - training batch 1701, loss: 0.402, 54432/60000 datapoints
2025-03-06 19:17:16,641 - INFO - training batch 1751, loss: 0.289, 56032/60000 datapoints
2025-03-06 19:17:16,867 - INFO - training batch 1801, loss: 0.300, 57632/60000 datapoints
2025-03-06 19:17:17,109 - INFO - training batch 1851, loss: 0.164, 59232/60000 datapoints
2025-03-06 19:17:17,229 - INFO - validation batch 1, loss: 0.396, 32/10016 datapoints
2025-03-06 19:17:17,412 - INFO - validation batch 51, loss: 0.586, 1632/10016 datapoints
2025-03-06 19:17:17,596 - INFO - validation batch 101, loss: 0.443, 3232/10016 datapoints
2025-03-06 19:17:17,781 - INFO - validation batch 151, loss: 0.274, 4832/10016 datapoints
2025-03-06 19:17:17,963 - INFO - validation batch 201, loss: 0.216, 6432/10016 datapoints
2025-03-06 19:17:18,144 - INFO - validation batch 251, loss: 0.482, 8032/10016 datapoints
2025-03-06 19:17:18,327 - INFO - validation batch 301, loss: 0.407, 9632/10016 datapoints
2025-03-06 19:17:18,377 - INFO - Epoch 262/800 done.
2025-03-06 19:17:18,377 - INFO - Final validation performance:
Loss: 0.401, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:17:18,378 - INFO - Beginning epoch 263/800
2025-03-06 19:17:18,387 - INFO - training batch 1, loss: 0.436, 32/60000 datapoints
2025-03-06 19:17:18,617 - INFO - training batch 51, loss: 1.063, 1632/60000 datapoints
2025-03-06 19:17:18,858 - INFO - training batch 101, loss: 0.176, 3232/60000 datapoints
2025-03-06 19:17:19,092 - INFO - training batch 151, loss: 0.214, 4832/60000 datapoints
2025-03-06 19:17:19,359 - INFO - training batch 201, loss: 0.290, 6432/60000 datapoints
2025-03-06 19:17:19,594 - INFO - training batch 251, loss: 0.327, 8032/60000 datapoints
2025-03-06 19:17:19,827 - INFO - training batch 301, loss: 0.222, 9632/60000 datapoints
2025-03-06 19:17:20,061 - INFO - training batch 351, loss: 0.453, 11232/60000 datapoints
2025-03-06 19:17:20,291 - INFO - training batch 401, loss: 0.119, 12832/60000 datapoints
2025-03-06 19:17:20,518 - INFO - training batch 451, loss: 0.552, 14432/60000 datapoints
2025-03-06 19:17:20,746 - INFO - training batch 501, loss: 0.575, 16032/60000 datapoints
2025-03-06 19:17:20,975 - INFO - training batch 551, loss: 0.446, 17632/60000 datapoints
2025-03-06 19:17:21,199 - INFO - training batch 601, loss: 0.366, 19232/60000 datapoints
2025-03-06 19:17:21,425 - INFO - training batch 651, loss: 0.258, 20832/60000 datapoints
2025-03-06 19:17:21,653 - INFO - training batch 701, loss: 0.359, 22432/60000 datapoints
2025-03-06 19:17:21,898 - INFO - training batch 751, loss: 0.129, 24032/60000 datapoints
2025-03-06 19:17:22,127 - INFO - training batch 801, loss: 0.441, 25632/60000 datapoints
2025-03-06 19:17:22,353 - INFO - training batch 851, loss: 0.338, 27232/60000 datapoints
2025-03-06 19:17:22,579 - INFO - training batch 901, loss: 0.400, 28832/60000 datapoints
2025-03-06 19:17:22,815 - INFO - training batch 951, loss: 0.531, 30432/60000 datapoints
2025-03-06 19:17:23,274 - INFO - training batch 1001, loss: 0.164, 32032/60000 datapoints
2025-03-06 19:17:23,495 - INFO - training batch 1051, loss: 0.467, 33632/60000 datapoints
2025-03-06 19:17:23,727 - INFO - training batch 1101, loss: 0.216, 35232/60000 datapoints
2025-03-06 19:17:23,954 - INFO - training batch 1151, loss: 0.358, 36832/60000 datapoints
2025-03-06 19:17:24,178 - INFO - training batch 1201, loss: 0.277, 38432/60000 datapoints
2025-03-06 19:17:24,442 - INFO - training batch 1251, loss: 0.438, 40032/60000 datapoints
2025-03-06 19:17:24,674 - INFO - training batch 1301, loss: 0.407, 41632/60000 datapoints
2025-03-06 19:17:24,891 - INFO - training batch 1351, loss: 0.345, 43232/60000 datapoints
2025-03-06 19:17:25,117 - INFO - training batch 1401, loss: 0.238, 44832/60000 datapoints
2025-03-06 19:17:25,331 - INFO - training batch 1451, loss: 0.874, 46432/60000 datapoints
2025-03-06 19:17:25,545 - INFO - training batch 1501, loss: 0.500, 48032/60000 datapoints
2025-03-06 19:17:25,767 - INFO - training batch 1551, loss: 0.310, 49632/60000 datapoints
2025-03-06 19:17:25,980 - INFO - training batch 1601, loss: 0.386, 51232/60000 datapoints
2025-03-06 19:17:26,194 - INFO - training batch 1651, loss: 0.302, 52832/60000 datapoints
2025-03-06 19:17:26,408 - INFO - training batch 1701, loss: 0.374, 54432/60000 datapoints
2025-03-06 19:17:26,629 - INFO - training batch 1751, loss: 0.443, 56032/60000 datapoints
2025-03-06 19:17:26,843 - INFO - training batch 1801, loss: 0.366, 57632/60000 datapoints
2025-03-06 19:17:27,059 - INFO - training batch 1851, loss: 0.312, 59232/60000 datapoints
2025-03-06 19:17:27,173 - INFO - validation batch 1, loss: 0.112, 32/10016 datapoints
2025-03-06 19:17:27,347 - INFO - validation batch 51, loss: 0.249, 1632/10016 datapoints
2025-03-06 19:17:27,520 - INFO - validation batch 101, loss: 0.272, 3232/10016 datapoints
2025-03-06 19:17:27,695 - INFO - validation batch 151, loss: 0.428, 4832/10016 datapoints
2025-03-06 19:17:27,870 - INFO - validation batch 201, loss: 0.448, 6432/10016 datapoints
2025-03-06 19:17:28,041 - INFO - validation batch 251, loss: 0.583, 8032/10016 datapoints
2025-03-06 19:17:28,212 - INFO - validation batch 301, loss: 0.238, 9632/10016 datapoints
2025-03-06 19:17:28,254 - INFO - Epoch 263/800 done.
2025-03-06 19:17:28,255 - INFO - Final validation performance:
Loss: 0.333, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:17:28,255 - INFO - Beginning epoch 264/800
2025-03-06 19:17:28,266 - INFO - training batch 1, loss: 0.345, 32/60000 datapoints
2025-03-06 19:17:28,495 - INFO - training batch 51, loss: 0.168, 1632/60000 datapoints
2025-03-06 19:17:28,718 - INFO - training batch 101, loss: 0.259, 3232/60000 datapoints
2025-03-06 19:17:28,940 - INFO - training batch 151, loss: 0.336, 4832/60000 datapoints
2025-03-06 19:17:29,162 - INFO - training batch 201, loss: 0.473, 6432/60000 datapoints
2025-03-06 19:17:29,385 - INFO - training batch 251, loss: 0.263, 8032/60000 datapoints
2025-03-06 19:17:29,626 - INFO - training batch 301, loss: 0.404, 9632/60000 datapoints
2025-03-06 19:17:29,844 - INFO - training batch 351, loss: 0.248, 11232/60000 datapoints
2025-03-06 19:17:30,065 - INFO - training batch 401, loss: 0.429, 12832/60000 datapoints
2025-03-06 19:17:30,286 - INFO - training batch 451, loss: 0.296, 14432/60000 datapoints
2025-03-06 19:17:30,510 - INFO - training batch 501, loss: 0.336, 16032/60000 datapoints
2025-03-06 19:17:30,730 - INFO - training batch 551, loss: 0.483, 17632/60000 datapoints
2025-03-06 19:17:30,954 - INFO - training batch 601, loss: 0.235, 19232/60000 datapoints
2025-03-06 19:17:31,179 - INFO - training batch 651, loss: 0.219, 20832/60000 datapoints
2025-03-06 19:17:31,431 - INFO - training batch 701, loss: 0.246, 22432/60000 datapoints
2025-03-06 19:17:31,685 - INFO - training batch 751, loss: 0.212, 24032/60000 datapoints
2025-03-06 19:17:31,946 - INFO - training batch 801, loss: 0.220, 25632/60000 datapoints
2025-03-06 19:17:32,189 - INFO - training batch 851, loss: 0.213, 27232/60000 datapoints
2025-03-06 19:17:32,432 - INFO - training batch 901, loss: 0.361, 28832/60000 datapoints
2025-03-06 19:17:32,673 - INFO - training batch 951, loss: 0.511, 30432/60000 datapoints
2025-03-06 19:17:32,896 - INFO - training batch 1001, loss: 0.279, 32032/60000 datapoints
2025-03-06 19:17:33,127 - INFO - training batch 1051, loss: 0.424, 33632/60000 datapoints
2025-03-06 19:17:33,374 - INFO - training batch 1101, loss: 0.324, 35232/60000 datapoints
2025-03-06 19:17:33,590 - INFO - training batch 1151, loss: 0.408, 36832/60000 datapoints
2025-03-06 19:17:33,821 - INFO - training batch 1201, loss: 0.212, 38432/60000 datapoints
2025-03-06 19:17:34,037 - INFO - training batch 1251, loss: 0.168, 40032/60000 datapoints
2025-03-06 19:17:34,258 - INFO - training batch 1301, loss: 0.242, 41632/60000 datapoints
2025-03-06 19:17:34,481 - INFO - training batch 1351, loss: 0.150, 43232/60000 datapoints
2025-03-06 19:17:34,703 - INFO - training batch 1401, loss: 0.242, 44832/60000 datapoints
2025-03-06 19:17:34,926 - INFO - training batch 1451, loss: 0.341, 46432/60000 datapoints
2025-03-06 19:17:35,154 - INFO - training batch 1501, loss: 0.212, 48032/60000 datapoints
2025-03-06 19:17:35,375 - INFO - training batch 1551, loss: 0.130, 49632/60000 datapoints
2025-03-06 19:17:35,596 - INFO - training batch 1601, loss: 0.556, 51232/60000 datapoints
2025-03-06 19:17:35,817 - INFO - training batch 1651, loss: 0.187, 52832/60000 datapoints
2025-03-06 19:17:36,036 - INFO - training batch 1701, loss: 0.382, 54432/60000 datapoints
2025-03-06 19:17:36,253 - INFO - training batch 1751, loss: 0.314, 56032/60000 datapoints
2025-03-06 19:17:36,474 - INFO - training batch 1801, loss: 0.572, 57632/60000 datapoints
2025-03-06 19:17:36,696 - INFO - training batch 1851, loss: 0.230, 59232/60000 datapoints
2025-03-06 19:17:36,813 - INFO - validation batch 1, loss: 0.170, 32/10016 datapoints
2025-03-06 19:17:36,990 - INFO - validation batch 51, loss: 0.383, 1632/10016 datapoints
2025-03-06 19:17:37,166 - INFO - validation batch 101, loss: 0.312, 3232/10016 datapoints
2025-03-06 19:17:37,342 - INFO - validation batch 151, loss: 0.987, 4832/10016 datapoints
2025-03-06 19:17:37,520 - INFO - validation batch 201, loss: 0.267, 6432/10016 datapoints
2025-03-06 19:17:37,736 - INFO - validation batch 251, loss: 0.163, 8032/10016 datapoints
2025-03-06 19:17:37,912 - INFO - validation batch 301, loss: 0.394, 9632/10016 datapoints
2025-03-06 19:17:37,955 - INFO - Epoch 264/800 done.
2025-03-06 19:17:37,955 - INFO - Final validation performance:
Loss: 0.382, top-1 acc: 0.909top-5 acc: 0.909
2025-03-06 19:17:37,956 - INFO - Beginning epoch 265/800
2025-03-06 19:17:37,968 - INFO - training batch 1, loss: 0.310, 32/60000 datapoints
2025-03-06 19:17:38,211 - INFO - training batch 51, loss: 0.454, 1632/60000 datapoints
2025-03-06 19:17:38,429 - INFO - training batch 101, loss: 0.390, 3232/60000 datapoints
2025-03-06 19:17:38,660 - INFO - training batch 151, loss: 0.330, 4832/60000 datapoints
2025-03-06 19:17:38,880 - INFO - training batch 201, loss: 0.235, 6432/60000 datapoints
2025-03-06 19:17:39,120 - INFO - training batch 251, loss: 0.408, 8032/60000 datapoints
2025-03-06 19:17:39,343 - INFO - training batch 301, loss: 0.222, 9632/60000 datapoints
2025-03-06 19:17:39,586 - INFO - training batch 351, loss: 0.254, 11232/60000 datapoints
2025-03-06 19:17:39,808 - INFO - training batch 401, loss: 0.305, 12832/60000 datapoints
2025-03-06 19:17:40,022 - INFO - training batch 451, loss: 0.239, 14432/60000 datapoints
2025-03-06 19:17:40,239 - INFO - training batch 501, loss: 0.210, 16032/60000 datapoints
2025-03-06 19:17:40,452 - INFO - training batch 551, loss: 0.548, 17632/60000 datapoints
2025-03-06 19:17:40,668 - INFO - training batch 601, loss: 0.215, 19232/60000 datapoints
2025-03-06 19:17:40,897 - INFO - training batch 651, loss: 0.255, 20832/60000 datapoints
2025-03-06 19:17:41,124 - INFO - training batch 701, loss: 0.392, 22432/60000 datapoints
2025-03-06 19:17:41,349 - INFO - training batch 751, loss: 0.282, 24032/60000 datapoints
2025-03-06 19:17:41,580 - INFO - training batch 801, loss: 0.357, 25632/60000 datapoints
2025-03-06 19:17:41,815 - INFO - training batch 851, loss: 0.249, 27232/60000 datapoints
2025-03-06 19:17:42,047 - INFO - training batch 901, loss: 0.158, 28832/60000 datapoints
2025-03-06 19:17:42,275 - INFO - training batch 951, loss: 0.178, 30432/60000 datapoints
2025-03-06 19:17:42,499 - INFO - training batch 1001, loss: 0.398, 32032/60000 datapoints
2025-03-06 19:17:42,728 - INFO - training batch 1051, loss: 0.191, 33632/60000 datapoints
2025-03-06 19:17:42,951 - INFO - training batch 1101, loss: 0.261, 35232/60000 datapoints
2025-03-06 19:17:43,178 - INFO - training batch 1151, loss: 0.345, 36832/60000 datapoints
2025-03-06 19:17:43,404 - INFO - training batch 1201, loss: 0.295, 38432/60000 datapoints
2025-03-06 19:17:43,635 - INFO - training batch 1251, loss: 0.456, 40032/60000 datapoints
2025-03-06 19:17:43,861 - INFO - training batch 1301, loss: 0.780, 41632/60000 datapoints
2025-03-06 19:17:44,087 - INFO - training batch 1351, loss: 0.323, 43232/60000 datapoints
2025-03-06 19:17:44,318 - INFO - training batch 1401, loss: 0.383, 44832/60000 datapoints
2025-03-06 19:17:44,543 - INFO - training batch 1451, loss: 0.321, 46432/60000 datapoints
2025-03-06 19:17:44,770 - INFO - training batch 1501, loss: 0.337, 48032/60000 datapoints
2025-03-06 19:17:45,003 - INFO - training batch 1551, loss: 0.260, 49632/60000 datapoints
2025-03-06 19:17:45,230 - INFO - training batch 1601, loss: 0.207, 51232/60000 datapoints
2025-03-06 19:17:45,453 - INFO - training batch 1651, loss: 0.497, 52832/60000 datapoints
2025-03-06 19:17:45,688 - INFO - training batch 1701, loss: 0.256, 54432/60000 datapoints
2025-03-06 19:17:45,912 - INFO - training batch 1751, loss: 0.435, 56032/60000 datapoints
2025-03-06 19:17:46,137 - INFO - training batch 1801, loss: 0.129, 57632/60000 datapoints
2025-03-06 19:17:46,362 - INFO - training batch 1851, loss: 0.229, 59232/60000 datapoints
2025-03-06 19:17:46,483 - INFO - validation batch 1, loss: 0.384, 32/10016 datapoints
2025-03-06 19:17:46,668 - INFO - validation batch 51, loss: 0.122, 1632/10016 datapoints
2025-03-06 19:17:46,850 - INFO - validation batch 101, loss: 0.258, 3232/10016 datapoints
2025-03-06 19:17:47,025 - INFO - validation batch 151, loss: 0.380, 4832/10016 datapoints
2025-03-06 19:17:47,201 - INFO - validation batch 201, loss: 0.195, 6432/10016 datapoints
2025-03-06 19:17:47,377 - INFO - validation batch 251, loss: 0.561, 8032/10016 datapoints
2025-03-06 19:17:47,551 - INFO - validation batch 301, loss: 0.278, 9632/10016 datapoints
2025-03-06 19:17:47,600 - INFO - Epoch 265/800 done.
2025-03-06 19:17:47,600 - INFO - Final validation performance:
Loss: 0.311, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 19:17:47,601 - INFO - Beginning epoch 266/800
2025-03-06 19:17:47,614 - INFO - training batch 1, loss: 0.144, 32/60000 datapoints
2025-03-06 19:17:47,848 - INFO - training batch 51, loss: 0.213, 1632/60000 datapoints
2025-03-06 19:17:48,057 - INFO - training batch 101, loss: 0.364, 3232/60000 datapoints
2025-03-06 19:17:48,287 - INFO - training batch 151, loss: 0.204, 4832/60000 datapoints
2025-03-06 19:17:48,505 - INFO - training batch 201, loss: 0.357, 6432/60000 datapoints
2025-03-06 19:17:48,717 - INFO - training batch 251, loss: 0.225, 8032/60000 datapoints
2025-03-06 19:17:48,939 - INFO - training batch 301, loss: 0.397, 9632/60000 datapoints
2025-03-06 19:17:49,172 - INFO - training batch 351, loss: 0.340, 11232/60000 datapoints
2025-03-06 19:17:49,394 - INFO - training batch 401, loss: 0.550, 12832/60000 datapoints
2025-03-06 19:17:49,647 - INFO - training batch 451, loss: 0.220, 14432/60000 datapoints
2025-03-06 19:17:49,873 - INFO - training batch 501, loss: 0.161, 16032/60000 datapoints
2025-03-06 19:17:50,084 - INFO - training batch 551, loss: 0.480, 17632/60000 datapoints
2025-03-06 19:17:50,300 - INFO - training batch 601, loss: 0.320, 19232/60000 datapoints
2025-03-06 19:17:50,516 - INFO - training batch 651, loss: 0.354, 20832/60000 datapoints
2025-03-06 19:17:50,729 - INFO - training batch 701, loss: 0.415, 22432/60000 datapoints
2025-03-06 19:17:50,943 - INFO - training batch 751, loss: 0.502, 24032/60000 datapoints
2025-03-06 19:17:51,154 - INFO - training batch 801, loss: 0.341, 25632/60000 datapoints
2025-03-06 19:17:51,369 - INFO - training batch 851, loss: 0.383, 27232/60000 datapoints
2025-03-06 19:17:51,585 - INFO - training batch 901, loss: 0.291, 28832/60000 datapoints
2025-03-06 19:17:51,805 - INFO - training batch 951, loss: 0.493, 30432/60000 datapoints
2025-03-06 19:17:52,027 - INFO - training batch 1001, loss: 0.248, 32032/60000 datapoints
2025-03-06 19:17:52,244 - INFO - training batch 1051, loss: 0.468, 33632/60000 datapoints
2025-03-06 19:17:52,453 - INFO - training batch 1101, loss: 0.263, 35232/60000 datapoints
2025-03-06 19:17:52,667 - INFO - training batch 1151, loss: 0.442, 36832/60000 datapoints
2025-03-06 19:17:52,895 - INFO - training batch 1201, loss: 0.153, 38432/60000 datapoints
2025-03-06 19:17:53,127 - INFO - training batch 1251, loss: 0.265, 40032/60000 datapoints
2025-03-06 19:17:53,383 - INFO - training batch 1301, loss: 0.367, 41632/60000 datapoints
2025-03-06 19:17:53,676 - INFO - training batch 1351, loss: 0.199, 43232/60000 datapoints
2025-03-06 19:17:53,964 - INFO - training batch 1401, loss: 0.206, 44832/60000 datapoints
2025-03-06 19:17:54,211 - INFO - training batch 1451, loss: 0.223, 46432/60000 datapoints
2025-03-06 19:17:54,424 - INFO - training batch 1501, loss: 0.505, 48032/60000 datapoints
2025-03-06 19:17:54,641 - INFO - training batch 1551, loss: 0.832, 49632/60000 datapoints
2025-03-06 19:17:54,858 - INFO - training batch 1601, loss: 0.706, 51232/60000 datapoints
2025-03-06 19:17:55,097 - INFO - training batch 1651, loss: 0.245, 52832/60000 datapoints
2025-03-06 19:17:55,320 - INFO - training batch 1701, loss: 0.570, 54432/60000 datapoints
2025-03-06 19:17:55,545 - INFO - training batch 1751, loss: 0.181, 56032/60000 datapoints
2025-03-06 19:17:55,784 - INFO - training batch 1801, loss: 0.204, 57632/60000 datapoints
2025-03-06 19:17:56,010 - INFO - training batch 1851, loss: 0.135, 59232/60000 datapoints
2025-03-06 19:17:56,130 - INFO - validation batch 1, loss: 0.271, 32/10016 datapoints
2025-03-06 19:17:56,312 - INFO - validation batch 51, loss: 0.272, 1632/10016 datapoints
2025-03-06 19:17:56,492 - INFO - validation batch 101, loss: 0.255, 3232/10016 datapoints
2025-03-06 19:17:56,684 - INFO - validation batch 151, loss: 0.700, 4832/10016 datapoints
2025-03-06 19:17:56,888 - INFO - validation batch 201, loss: 0.734, 6432/10016 datapoints
2025-03-06 19:17:57,077 - INFO - validation batch 251, loss: 0.223, 8032/10016 datapoints
2025-03-06 19:17:57,259 - INFO - validation batch 301, loss: 0.177, 9632/10016 datapoints
2025-03-06 19:17:57,303 - INFO - Epoch 266/800 done.
2025-03-06 19:17:57,303 - INFO - Final validation performance:
Loss: 0.376, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 19:17:57,304 - INFO - Beginning epoch 267/800
2025-03-06 19:17:57,317 - INFO - training batch 1, loss: 0.416, 32/60000 datapoints
2025-03-06 19:17:57,579 - INFO - training batch 51, loss: 0.181, 1632/60000 datapoints
2025-03-06 19:17:57,811 - INFO - training batch 101, loss: 0.243, 3232/60000 datapoints
2025-03-06 19:17:58,045 - INFO - training batch 151, loss: 0.193, 4832/60000 datapoints
2025-03-06 19:17:58,274 - INFO - training batch 201, loss: 0.346, 6432/60000 datapoints
2025-03-06 19:17:58,501 - INFO - training batch 251, loss: 0.234, 8032/60000 datapoints
2025-03-06 19:17:58,728 - INFO - training batch 301, loss: 0.346, 9632/60000 datapoints
2025-03-06 19:17:58,955 - INFO - training batch 351, loss: 0.150, 11232/60000 datapoints
2025-03-06 19:17:59,182 - INFO - training batch 401, loss: 0.510, 12832/60000 datapoints
2025-03-06 19:17:59,409 - INFO - training batch 451, loss: 0.304, 14432/60000 datapoints
2025-03-06 19:17:59,638 - INFO - training batch 501, loss: 0.403, 16032/60000 datapoints
2025-03-06 19:17:59,888 - INFO - training batch 551, loss: 0.480, 17632/60000 datapoints
2025-03-06 19:18:00,123 - INFO - training batch 601, loss: 0.180, 19232/60000 datapoints
2025-03-06 19:18:00,348 - INFO - training batch 651, loss: 0.136, 20832/60000 datapoints
2025-03-06 19:18:00,577 - INFO - training batch 701, loss: 0.276, 22432/60000 datapoints
2025-03-06 19:18:00,824 - INFO - training batch 751, loss: 0.378, 24032/60000 datapoints
2025-03-06 19:18:01,050 - INFO - training batch 801, loss: 0.550, 25632/60000 datapoints
2025-03-06 19:18:01,276 - INFO - training batch 851, loss: 0.319, 27232/60000 datapoints
2025-03-06 19:18:01,503 - INFO - training batch 901, loss: 0.588, 28832/60000 datapoints
2025-03-06 19:18:01,732 - INFO - training batch 951, loss: 0.248, 30432/60000 datapoints
2025-03-06 19:18:01,963 - INFO - training batch 1001, loss: 0.414, 32032/60000 datapoints
2025-03-06 19:18:02,197 - INFO - training batch 1051, loss: 0.160, 33632/60000 datapoints
2025-03-06 19:18:02,424 - INFO - training batch 1101, loss: 0.707, 35232/60000 datapoints
2025-03-06 19:18:02,650 - INFO - training batch 1151, loss: 0.237, 36832/60000 datapoints
2025-03-06 19:18:02,873 - INFO - training batch 1201, loss: 0.338, 38432/60000 datapoints
2025-03-06 19:18:03,114 - INFO - training batch 1251, loss: 0.329, 40032/60000 datapoints
2025-03-06 19:18:03,357 - INFO - training batch 1301, loss: 0.440, 41632/60000 datapoints
2025-03-06 19:18:03,603 - INFO - training batch 1351, loss: 0.177, 43232/60000 datapoints
2025-03-06 19:18:03,839 - INFO - training batch 1401, loss: 0.310, 44832/60000 datapoints
2025-03-06 19:18:04,066 - INFO - training batch 1451, loss: 0.465, 46432/60000 datapoints
2025-03-06 19:18:04,291 - INFO - training batch 1501, loss: 0.295, 48032/60000 datapoints
2025-03-06 19:18:04,519 - INFO - training batch 1551, loss: 0.431, 49632/60000 datapoints
2025-03-06 19:18:04,749 - INFO - training batch 1601, loss: 0.301, 51232/60000 datapoints
2025-03-06 19:18:04,983 - INFO - training batch 1651, loss: 0.208, 52832/60000 datapoints
2025-03-06 19:18:05,207 - INFO - training batch 1701, loss: 0.703, 54432/60000 datapoints
2025-03-06 19:18:05,432 - INFO - training batch 1751, loss: 0.186, 56032/60000 datapoints
2025-03-06 19:18:05,660 - INFO - training batch 1801, loss: 0.421, 57632/60000 datapoints
2025-03-06 19:18:05,887 - INFO - training batch 1851, loss: 0.578, 59232/60000 datapoints
2025-03-06 19:18:06,013 - INFO - validation batch 1, loss: 0.553, 32/10016 datapoints
2025-03-06 19:18:06,189 - INFO - validation batch 51, loss: 0.402, 1632/10016 datapoints
2025-03-06 19:18:06,368 - INFO - validation batch 101, loss: 0.274, 3232/10016 datapoints
2025-03-06 19:18:06,544 - INFO - validation batch 151, loss: 0.308, 4832/10016 datapoints
2025-03-06 19:18:06,720 - INFO - validation batch 201, loss: 0.153, 6432/10016 datapoints
2025-03-06 19:18:06,892 - INFO - validation batch 251, loss: 0.352, 8032/10016 datapoints
2025-03-06 19:18:07,105 - INFO - validation batch 301, loss: 0.387, 9632/10016 datapoints
2025-03-06 19:18:07,163 - INFO - Epoch 267/800 done.
2025-03-06 19:18:07,163 - INFO - Final validation performance:
Loss: 0.347, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 19:18:07,164 - INFO - Beginning epoch 268/800
2025-03-06 19:18:07,174 - INFO - training batch 1, loss: 0.274, 32/60000 datapoints
2025-03-06 19:18:07,398 - INFO - training batch 51, loss: 0.130, 1632/60000 datapoints
2025-03-06 19:18:07,643 - INFO - training batch 101, loss: 0.286, 3232/60000 datapoints
2025-03-06 19:18:07,864 - INFO - training batch 151, loss: 0.216, 4832/60000 datapoints
2025-03-06 19:18:08,093 - INFO - training batch 201, loss: 0.137, 6432/60000 datapoints
2025-03-06 19:18:08,315 - INFO - training batch 251, loss: 0.144, 8032/60000 datapoints
2025-03-06 19:18:08,530 - INFO - training batch 301, loss: 0.192, 9632/60000 datapoints
2025-03-06 19:18:08,745 - INFO - training batch 351, loss: 0.410, 11232/60000 datapoints
2025-03-06 19:18:08,988 - INFO - training batch 401, loss: 0.126, 12832/60000 datapoints
2025-03-06 19:18:09,195 - INFO - training batch 451, loss: 0.242, 14432/60000 datapoints
2025-03-06 19:18:09,411 - INFO - training batch 501, loss: 0.397, 16032/60000 datapoints
2025-03-06 19:18:09,630 - INFO - training batch 551, loss: 0.255, 17632/60000 datapoints
2025-03-06 19:18:09,874 - INFO - training batch 601, loss: 0.336, 19232/60000 datapoints
2025-03-06 19:18:10,109 - INFO - training batch 651, loss: 0.344, 20832/60000 datapoints
2025-03-06 19:18:10,324 - INFO - training batch 701, loss: 0.413, 22432/60000 datapoints
2025-03-06 19:18:10,535 - INFO - training batch 751, loss: 0.275, 24032/60000 datapoints
2025-03-06 19:18:10,750 - INFO - training batch 801, loss: 0.318, 25632/60000 datapoints
2025-03-06 19:18:10,959 - INFO - training batch 851, loss: 0.313, 27232/60000 datapoints
2025-03-06 19:18:11,172 - INFO - training batch 901, loss: 0.297, 28832/60000 datapoints
2025-03-06 19:18:11,385 - INFO - training batch 951, loss: 0.373, 30432/60000 datapoints
2025-03-06 19:18:11,599 - INFO - training batch 1001, loss: 0.357, 32032/60000 datapoints
2025-03-06 19:18:11,817 - INFO - training batch 1051, loss: 0.185, 33632/60000 datapoints
2025-03-06 19:18:12,029 - INFO - training batch 1101, loss: 0.438, 35232/60000 datapoints
2025-03-06 19:18:12,247 - INFO - training batch 1151, loss: 0.165, 36832/60000 datapoints
2025-03-06 19:18:12,459 - INFO - training batch 1201, loss: 0.290, 38432/60000 datapoints
2025-03-06 19:18:12,675 - INFO - training batch 1251, loss: 0.292, 40032/60000 datapoints
2025-03-06 19:18:12,887 - INFO - training batch 1301, loss: 0.600, 41632/60000 datapoints
2025-03-06 19:18:13,101 - INFO - training batch 1351, loss: 0.158, 43232/60000 datapoints
2025-03-06 19:18:13,312 - INFO - training batch 1401, loss: 0.221, 44832/60000 datapoints
2025-03-06 19:18:13,524 - INFO - training batch 1451, loss: 0.186, 46432/60000 datapoints
2025-03-06 19:18:13,741 - INFO - training batch 1501, loss: 0.315, 48032/60000 datapoints
2025-03-06 19:18:13,959 - INFO - training batch 1551, loss: 0.174, 49632/60000 datapoints
2025-03-06 19:18:14,174 - INFO - training batch 1601, loss: 0.292, 51232/60000 datapoints
2025-03-06 19:18:14,386 - INFO - training batch 1651, loss: 0.188, 52832/60000 datapoints
2025-03-06 19:18:14,597 - INFO - training batch 1701, loss: 0.243, 54432/60000 datapoints
2025-03-06 19:18:14,809 - INFO - training batch 1751, loss: 0.454, 56032/60000 datapoints
2025-03-06 19:18:15,034 - INFO - training batch 1801, loss: 0.538, 57632/60000 datapoints
2025-03-06 19:18:15,250 - INFO - training batch 1851, loss: 0.329, 59232/60000 datapoints
2025-03-06 19:18:15,367 - INFO - validation batch 1, loss: 0.797, 32/10016 datapoints
2025-03-06 19:18:15,546 - INFO - validation batch 51, loss: 0.155, 1632/10016 datapoints
2025-03-06 19:18:15,727 - INFO - validation batch 101, loss: 0.387, 3232/10016 datapoints
2025-03-06 19:18:15,903 - INFO - validation batch 151, loss: 0.184, 4832/10016 datapoints
2025-03-06 19:18:16,083 - INFO - validation batch 201, loss: 0.117, 6432/10016 datapoints
2025-03-06 19:18:16,262 - INFO - validation batch 251, loss: 0.118, 8032/10016 datapoints
2025-03-06 19:18:16,443 - INFO - validation batch 301, loss: 0.159, 9632/10016 datapoints
2025-03-06 19:18:16,489 - INFO - Epoch 268/800 done.
2025-03-06 19:18:16,490 - INFO - Final validation performance:
Loss: 0.274, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 19:18:16,490 - INFO - Beginning epoch 269/800
2025-03-06 19:18:16,497 - INFO - training batch 1, loss: 0.306, 32/60000 datapoints
2025-03-06 19:18:16,741 - INFO - training batch 51, loss: 0.478, 1632/60000 datapoints
2025-03-06 19:18:16,963 - INFO - training batch 101, loss: 0.132, 3232/60000 datapoints
2025-03-06 19:18:17,191 - INFO - training batch 151, loss: 0.413, 4832/60000 datapoints
2025-03-06 19:18:17,419 - INFO - training batch 201, loss: 0.359, 6432/60000 datapoints
2025-03-06 19:18:17,644 - INFO - training batch 251, loss: 0.235, 8032/60000 datapoints
2025-03-06 19:18:17,864 - INFO - training batch 301, loss: 0.186, 9632/60000 datapoints
2025-03-06 19:18:18,082 - INFO - training batch 351, loss: 0.260, 11232/60000 datapoints
2025-03-06 19:18:18,298 - INFO - training batch 401, loss: 0.352, 12832/60000 datapoints
2025-03-06 19:18:18,514 - INFO - training batch 451, loss: 0.110, 14432/60000 datapoints
2025-03-06 19:18:18,740 - INFO - training batch 501, loss: 0.305, 16032/60000 datapoints
2025-03-06 19:18:18,957 - INFO - training batch 551, loss: 0.388, 17632/60000 datapoints
2025-03-06 19:18:19,178 - INFO - training batch 601, loss: 0.373, 19232/60000 datapoints
2025-03-06 19:18:19,406 - INFO - training batch 651, loss: 0.173, 20832/60000 datapoints
2025-03-06 19:18:19,632 - INFO - training batch 701, loss: 0.179, 22432/60000 datapoints
2025-03-06 19:18:19,873 - INFO - training batch 751, loss: 0.884, 24032/60000 datapoints
2025-03-06 19:18:20,138 - INFO - training batch 801, loss: 0.395, 25632/60000 datapoints
2025-03-06 19:18:20,381 - INFO - training batch 851, loss: 0.122, 27232/60000 datapoints
2025-03-06 19:18:20,628 - INFO - training batch 901, loss: 0.221, 28832/60000 datapoints
2025-03-06 19:18:20,869 - INFO - training batch 951, loss: 0.154, 30432/60000 datapoints
2025-03-06 19:18:21,109 - INFO - training batch 1001, loss: 0.220, 32032/60000 datapoints
2025-03-06 19:18:21,349 - INFO - training batch 1051, loss: 0.191, 33632/60000 datapoints
2025-03-06 19:18:21,590 - INFO - training batch 1101, loss: 0.304, 35232/60000 datapoints
2025-03-06 19:18:21,834 - INFO - training batch 1151, loss: 0.294, 36832/60000 datapoints
2025-03-06 19:18:22,078 - INFO - training batch 1201, loss: 0.290, 38432/60000 datapoints
2025-03-06 19:18:22,350 - INFO - training batch 1251, loss: 0.140, 40032/60000 datapoints
2025-03-06 19:18:22,576 - INFO - training batch 1301, loss: 0.268, 41632/60000 datapoints
2025-03-06 19:18:22,808 - INFO - training batch 1351, loss: 0.232, 43232/60000 datapoints
2025-03-06 19:18:23,034 - INFO - training batch 1401, loss: 0.333, 44832/60000 datapoints
2025-03-06 19:18:23,256 - INFO - training batch 1451, loss: 0.409, 46432/60000 datapoints
2025-03-06 19:18:23,483 - INFO - training batch 1501, loss: 0.466, 48032/60000 datapoints
2025-03-06 19:18:23,713 - INFO - training batch 1551, loss: 0.324, 49632/60000 datapoints
2025-03-06 19:18:23,939 - INFO - training batch 1601, loss: 0.242, 51232/60000 datapoints
2025-03-06 19:18:24,180 - INFO - training batch 1651, loss: 0.384, 52832/60000 datapoints
2025-03-06 19:18:24,401 - INFO - training batch 1701, loss: 0.229, 54432/60000 datapoints
2025-03-06 19:18:24,627 - INFO - training batch 1751, loss: 0.244, 56032/60000 datapoints
2025-03-06 19:18:24,855 - INFO - training batch 1801, loss: 0.371, 57632/60000 datapoints
2025-03-06 19:18:25,088 - INFO - training batch 1851, loss: 0.228, 59232/60000 datapoints
2025-03-06 19:18:25,209 - INFO - validation batch 1, loss: 0.291, 32/10016 datapoints
2025-03-06 19:18:25,392 - INFO - validation batch 51, loss: 0.356, 1632/10016 datapoints
2025-03-06 19:18:25,577 - INFO - validation batch 101, loss: 0.524, 3232/10016 datapoints
2025-03-06 19:18:25,770 - INFO - validation batch 151, loss: 0.647, 4832/10016 datapoints
2025-03-06 19:18:25,952 - INFO - validation batch 201, loss: 0.519, 6432/10016 datapoints
2025-03-06 19:18:26,141 - INFO - validation batch 251, loss: 0.460, 8032/10016 datapoints
2025-03-06 19:18:26,329 - INFO - validation batch 301, loss: 0.268, 9632/10016 datapoints
2025-03-06 19:18:26,376 - INFO - Epoch 269/800 done.
2025-03-06 19:18:26,377 - INFO - Final validation performance:
Loss: 0.438, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 19:18:26,377 - INFO - Beginning epoch 270/800
2025-03-06 19:18:26,384 - INFO - training batch 1, loss: 0.316, 32/60000 datapoints
2025-03-06 19:18:26,624 - INFO - training batch 51, loss: 0.342, 1632/60000 datapoints
2025-03-06 19:18:26,853 - INFO - training batch 101, loss: 0.372, 3232/60000 datapoints
2025-03-06 19:18:27,072 - INFO - training batch 151, loss: 0.121, 4832/60000 datapoints
2025-03-06 19:18:27,300 - INFO - training batch 201, loss: 0.241, 6432/60000 datapoints
2025-03-06 19:18:27,519 - INFO - training batch 251, loss: 0.488, 8032/60000 datapoints
2025-03-06 19:18:27,745 - INFO - training batch 301, loss: 0.265, 9632/60000 datapoints
2025-03-06 19:18:27,969 - INFO - training batch 351, loss: 0.281, 11232/60000 datapoints
2025-03-06 19:18:28,184 - INFO - training batch 401, loss: 0.277, 12832/60000 datapoints
2025-03-06 19:18:28,405 - INFO - training batch 451, loss: 0.411, 14432/60000 datapoints
2025-03-06 19:18:28,628 - INFO - training batch 501, loss: 0.141, 16032/60000 datapoints
2025-03-06 19:18:28,843 - INFO - training batch 551, loss: 0.097, 17632/60000 datapoints
2025-03-06 19:18:29,052 - INFO - training batch 601, loss: 0.526, 19232/60000 datapoints
2025-03-06 19:18:29,263 - INFO - training batch 651, loss: 0.399, 20832/60000 datapoints
2025-03-06 19:18:29,479 - INFO - training batch 701, loss: 0.328, 22432/60000 datapoints
2025-03-06 19:18:29,696 - INFO - training batch 751, loss: 0.375, 24032/60000 datapoints
2025-03-06 19:18:30,098 - INFO - training batch 801, loss: 0.391, 25632/60000 datapoints
2025-03-06 19:18:30,339 - INFO - training batch 851, loss: 0.200, 27232/60000 datapoints
2025-03-06 19:18:30,546 - INFO - training batch 901, loss: 0.179, 28832/60000 datapoints
2025-03-06 19:18:30,756 - INFO - training batch 951, loss: 0.289, 30432/60000 datapoints
2025-03-06 19:18:30,965 - INFO - training batch 1001, loss: 0.489, 32032/60000 datapoints
2025-03-06 19:18:31,177 - INFO - training batch 1051, loss: 0.225, 33632/60000 datapoints
2025-03-06 19:18:31,388 - INFO - training batch 1101, loss: 0.381, 35232/60000 datapoints
2025-03-06 19:18:31,596 - INFO - training batch 1151, loss: 0.318, 36832/60000 datapoints
2025-03-06 19:18:31,813 - INFO - training batch 1201, loss: 0.625, 38432/60000 datapoints
2025-03-06 19:18:32,024 - INFO - training batch 1251, loss: 0.310, 40032/60000 datapoints
2025-03-06 19:18:32,231 - INFO - training batch 1301, loss: 0.190, 41632/60000 datapoints
2025-03-06 19:18:32,439 - INFO - training batch 1351, loss: 0.865, 43232/60000 datapoints
2025-03-06 19:18:32,648 - INFO - training batch 1401, loss: 0.402, 44832/60000 datapoints
2025-03-06 19:18:32,858 - INFO - training batch 1451, loss: 0.157, 46432/60000 datapoints
2025-03-06 19:18:33,068 - INFO - training batch 1501, loss: 0.387, 48032/60000 datapoints
2025-03-06 19:18:33,278 - INFO - training batch 1551, loss: 0.302, 49632/60000 datapoints
2025-03-06 19:18:33,491 - INFO - training batch 1601, loss: 0.328, 51232/60000 datapoints
2025-03-06 19:18:33,709 - INFO - training batch 1651, loss: 0.249, 52832/60000 datapoints
2025-03-06 19:18:33,928 - INFO - training batch 1701, loss: 0.530, 54432/60000 datapoints
2025-03-06 19:18:34,142 - INFO - training batch 1751, loss: 0.384, 56032/60000 datapoints
2025-03-06 19:18:34,363 - INFO - training batch 1801, loss: 0.171, 57632/60000 datapoints
2025-03-06 19:18:34,582 - INFO - training batch 1851, loss: 0.400, 59232/60000 datapoints
2025-03-06 19:18:34,698 - INFO - validation batch 1, loss: 0.482, 32/10016 datapoints
2025-03-06 19:18:34,878 - INFO - validation batch 51, loss: 0.369, 1632/10016 datapoints
2025-03-06 19:18:35,056 - INFO - validation batch 101, loss: 0.399, 3232/10016 datapoints
2025-03-06 19:18:35,233 - INFO - validation batch 151, loss: 0.322, 4832/10016 datapoints
2025-03-06 19:18:35,409 - INFO - validation batch 201, loss: 0.433, 6432/10016 datapoints
2025-03-06 19:18:35,585 - INFO - validation batch 251, loss: 0.228, 8032/10016 datapoints
2025-03-06 19:18:35,772 - INFO - validation batch 301, loss: 0.240, 9632/10016 datapoints
2025-03-06 19:18:35,816 - INFO - Epoch 270/800 done.
2025-03-06 19:18:35,816 - INFO - Final validation performance:
Loss: 0.353, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 19:18:35,817 - INFO - Beginning epoch 271/800
2025-03-06 19:18:35,827 - INFO - training batch 1, loss: 0.213, 32/60000 datapoints
2025-03-06 19:18:36,058 - INFO - training batch 51, loss: 0.248, 1632/60000 datapoints
2025-03-06 19:18:36,289 - INFO - training batch 101, loss: 0.463, 3232/60000 datapoints
2025-03-06 19:18:36,524 - INFO - training batch 151, loss: 0.414, 4832/60000 datapoints
2025-03-06 19:18:36,754 - INFO - training batch 201, loss: 0.257, 6432/60000 datapoints
2025-03-06 19:18:36,982 - INFO - training batch 251, loss: 0.408, 8032/60000 datapoints
2025-03-06 19:18:37,209 - INFO - training batch 301, loss: 0.342, 9632/60000 datapoints
2025-03-06 19:18:37,437 - INFO - training batch 351, loss: 0.330, 11232/60000 datapoints
2025-03-06 19:18:37,686 - INFO - training batch 401, loss: 0.349, 12832/60000 datapoints
2025-03-06 19:18:37,909 - INFO - training batch 451, loss: 0.576, 14432/60000 datapoints
2025-03-06 19:18:38,127 - INFO - training batch 501, loss: 0.264, 16032/60000 datapoints
2025-03-06 19:18:38,347 - INFO - training batch 551, loss: 0.369, 17632/60000 datapoints
2025-03-06 19:18:38,567 - INFO - training batch 601, loss: 0.812, 19232/60000 datapoints
2025-03-06 19:18:38,786 - INFO - training batch 651, loss: 0.255, 20832/60000 datapoints
2025-03-06 19:18:39,011 - INFO - training batch 701, loss: 0.474, 22432/60000 datapoints
2025-03-06 19:18:39,242 - INFO - training batch 751, loss: 0.371, 24032/60000 datapoints
2025-03-06 19:18:39,465 - INFO - training batch 801, loss: 0.529, 25632/60000 datapoints
2025-03-06 19:18:39,685 - INFO - training batch 851, loss: 0.390, 27232/60000 datapoints
2025-03-06 19:18:39,906 - INFO - training batch 901, loss: 0.496, 28832/60000 datapoints
2025-03-06 19:18:40,130 - INFO - training batch 951, loss: 0.245, 30432/60000 datapoints
2025-03-06 19:18:40,377 - INFO - training batch 1001, loss: 0.309, 32032/60000 datapoints
2025-03-06 19:18:40,594 - INFO - training batch 1051, loss: 0.276, 33632/60000 datapoints
2025-03-06 19:18:40,814 - INFO - training batch 1101, loss: 0.310, 35232/60000 datapoints
2025-03-06 19:18:41,032 - INFO - training batch 1151, loss: 0.260, 36832/60000 datapoints
2025-03-06 19:18:41,245 - INFO - training batch 1201, loss: 0.332, 38432/60000 datapoints
2025-03-06 19:18:41,458 - INFO - training batch 1251, loss: 0.111, 40032/60000 datapoints
2025-03-06 19:18:41,672 - INFO - training batch 1301, loss: 0.354, 41632/60000 datapoints
2025-03-06 19:18:41,887 - INFO - training batch 1351, loss: 0.270, 43232/60000 datapoints
2025-03-06 19:18:42,098 - INFO - training batch 1401, loss: 0.385, 44832/60000 datapoints
2025-03-06 19:18:42,317 - INFO - training batch 1451, loss: 0.332, 46432/60000 datapoints
2025-03-06 19:18:42,529 - INFO - training batch 1501, loss: 0.262, 48032/60000 datapoints
2025-03-06 19:18:42,740 - INFO - training batch 1551, loss: 0.181, 49632/60000 datapoints
2025-03-06 19:18:42,945 - INFO - training batch 1601, loss: 0.384, 51232/60000 datapoints
2025-03-06 19:18:43,150 - INFO - training batch 1651, loss: 0.242, 52832/60000 datapoints
2025-03-06 19:18:43,357 - INFO - training batch 1701, loss: 0.466, 54432/60000 datapoints
2025-03-06 19:18:43,565 - INFO - training batch 1751, loss: 0.093, 56032/60000 datapoints
2025-03-06 19:18:43,774 - INFO - training batch 1801, loss: 0.207, 57632/60000 datapoints
2025-03-06 19:18:43,981 - INFO - training batch 1851, loss: 0.242, 59232/60000 datapoints
2025-03-06 19:18:44,088 - INFO - validation batch 1, loss: 0.431, 32/10016 datapoints
2025-03-06 19:18:44,249 - INFO - validation batch 51, loss: 0.475, 1632/10016 datapoints
2025-03-06 19:18:44,416 - INFO - validation batch 101, loss: 0.245, 3232/10016 datapoints
2025-03-06 19:18:44,580 - INFO - validation batch 151, loss: 0.436, 4832/10016 datapoints
2025-03-06 19:18:44,759 - INFO - validation batch 201, loss: 0.381, 6432/10016 datapoints
2025-03-06 19:18:44,932 - INFO - validation batch 251, loss: 0.221, 8032/10016 datapoints
2025-03-06 19:18:45,096 - INFO - validation batch 301, loss: 0.175, 9632/10016 datapoints
2025-03-06 19:18:45,134 - INFO - Epoch 271/800 done.
2025-03-06 19:18:45,135 - INFO - Final validation performance:
Loss: 0.338, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 19:18:45,135 - INFO - Beginning epoch 272/800
2025-03-06 19:18:45,142 - INFO - training batch 1, loss: 0.196, 32/60000 datapoints
2025-03-06 19:18:45,349 - INFO - training batch 51, loss: 0.259, 1632/60000 datapoints
2025-03-06 19:18:45,571 - INFO - training batch 101, loss: 0.335, 3232/60000 datapoints
2025-03-06 19:18:45,784 - INFO - training batch 151, loss: 0.128, 4832/60000 datapoints
2025-03-06 19:18:45,998 - INFO - training batch 201, loss: 0.456, 6432/60000 datapoints
2025-03-06 19:18:46,210 - INFO - training batch 251, loss: 0.146, 8032/60000 datapoints
2025-03-06 19:18:46,421 - INFO - training batch 301, loss: 0.595, 9632/60000 datapoints
2025-03-06 19:18:46,633 - INFO - training batch 351, loss: 0.220, 11232/60000 datapoints
2025-03-06 19:18:46,834 - INFO - training batch 401, loss: 0.192, 12832/60000 datapoints
2025-03-06 19:18:47,036 - INFO - training batch 451, loss: 0.232, 14432/60000 datapoints
2025-03-06 19:18:47,238 - INFO - training batch 501, loss: 0.132, 16032/60000 datapoints
2025-03-06 19:18:47,443 - INFO - training batch 551, loss: 0.249, 17632/60000 datapoints
2025-03-06 19:18:47,652 - INFO - training batch 601, loss: 0.143, 19232/60000 datapoints
2025-03-06 19:18:47,859 - INFO - training batch 651, loss: 0.145, 20832/60000 datapoints
2025-03-06 19:18:48,067 - INFO - training batch 701, loss: 0.197, 22432/60000 datapoints
2025-03-06 19:18:48,269 - INFO - training batch 751, loss: 0.154, 24032/60000 datapoints
2025-03-06 19:18:48,476 - INFO - training batch 801, loss: 0.717, 25632/60000 datapoints
2025-03-06 19:18:48,682 - INFO - training batch 851, loss: 0.345, 27232/60000 datapoints
2025-03-06 19:18:48,885 - INFO - training batch 901, loss: 0.232, 28832/60000 datapoints
2025-03-06 19:18:49,092 - INFO - training batch 951, loss: 0.256, 30432/60000 datapoints
2025-03-06 19:18:49,303 - INFO - training batch 1001, loss: 0.283, 32032/60000 datapoints
2025-03-06 19:18:49,510 - INFO - training batch 1051, loss: 0.251, 33632/60000 datapoints
2025-03-06 19:18:49,744 - INFO - training batch 1101, loss: 0.374, 35232/60000 datapoints
2025-03-06 19:18:49,957 - INFO - training batch 1151, loss: 0.421, 36832/60000 datapoints
2025-03-06 19:18:50,168 - INFO - training batch 1201, loss: 0.421, 38432/60000 datapoints
2025-03-06 19:18:50,406 - INFO - training batch 1251, loss: 0.305, 40032/60000 datapoints
2025-03-06 19:18:50,623 - INFO - training batch 1301, loss: 0.165, 41632/60000 datapoints
2025-03-06 19:18:50,833 - INFO - training batch 1351, loss: 0.516, 43232/60000 datapoints
2025-03-06 19:18:51,044 - INFO - training batch 1401, loss: 0.482, 44832/60000 datapoints
2025-03-06 19:18:51,253 - INFO - training batch 1451, loss: 0.311, 46432/60000 datapoints
2025-03-06 19:18:51,469 - INFO - training batch 1501, loss: 0.614, 48032/60000 datapoints
2025-03-06 19:18:51,685 - INFO - training batch 1551, loss: 0.214, 49632/60000 datapoints
2025-03-06 19:18:51,902 - INFO - training batch 1601, loss: 0.469, 51232/60000 datapoints
2025-03-06 19:18:52,111 - INFO - training batch 1651, loss: 0.143, 52832/60000 datapoints
2025-03-06 19:18:52,329 - INFO - training batch 1701, loss: 0.567, 54432/60000 datapoints
2025-03-06 19:18:52,554 - INFO - training batch 1751, loss: 0.375, 56032/60000 datapoints
2025-03-06 19:18:52,771 - INFO - training batch 1801, loss: 0.240, 57632/60000 datapoints
2025-03-06 19:18:52,986 - INFO - training batch 1851, loss: 0.225, 59232/60000 datapoints
2025-03-06 19:18:53,099 - INFO - validation batch 1, loss: 0.140, 32/10016 datapoints
2025-03-06 19:18:53,273 - INFO - validation batch 51, loss: 0.373, 1632/10016 datapoints
2025-03-06 19:18:53,445 - INFO - validation batch 101, loss: 0.231, 3232/10016 datapoints
2025-03-06 19:18:53,634 - INFO - validation batch 151, loss: 0.481, 4832/10016 datapoints
2025-03-06 19:18:53,806 - INFO - validation batch 201, loss: 0.180, 6432/10016 datapoints
2025-03-06 19:18:53,986 - INFO - validation batch 251, loss: 0.458, 8032/10016 datapoints
2025-03-06 19:18:54,165 - INFO - validation batch 301, loss: 0.494, 9632/10016 datapoints
2025-03-06 19:18:54,206 - INFO - Epoch 272/800 done.
2025-03-06 19:18:54,207 - INFO - Final validation performance:
Loss: 0.337, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 19:18:54,208 - INFO - Beginning epoch 273/800
2025-03-06 19:18:54,217 - INFO - training batch 1, loss: 0.245, 32/60000 datapoints
2025-03-06 19:18:54,455 - INFO - training batch 51, loss: 0.462, 1632/60000 datapoints
2025-03-06 19:18:54,670 - INFO - training batch 101, loss: 0.408, 3232/60000 datapoints
2025-03-06 19:18:54,890 - INFO - training batch 151, loss: 0.109, 4832/60000 datapoints
2025-03-06 19:18:55,115 - INFO - training batch 201, loss: 0.298, 6432/60000 datapoints
2025-03-06 19:18:55,329 - INFO - training batch 251, loss: 0.518, 8032/60000 datapoints
2025-03-06 19:18:55,547 - INFO - training batch 301, loss: 0.210, 9632/60000 datapoints
2025-03-06 19:18:55,803 - INFO - training batch 351, loss: 0.522, 11232/60000 datapoints
2025-03-06 19:18:56,070 - INFO - training batch 401, loss: 0.300, 12832/60000 datapoints
2025-03-06 19:18:56,358 - INFO - training batch 451, loss: 0.206, 14432/60000 datapoints
2025-03-06 19:18:56,578 - INFO - training batch 501, loss: 0.308, 16032/60000 datapoints
2025-03-06 19:18:56,797 - INFO - training batch 551, loss: 0.228, 17632/60000 datapoints
2025-03-06 19:18:57,022 - INFO - training batch 601, loss: 0.199, 19232/60000 datapoints
2025-03-06 19:18:57,244 - INFO - training batch 651, loss: 0.284, 20832/60000 datapoints
2025-03-06 19:18:57,472 - INFO - training batch 701, loss: 0.281, 22432/60000 datapoints
2025-03-06 19:18:57,701 - INFO - training batch 751, loss: 0.155, 24032/60000 datapoints
2025-03-06 19:18:57,927 - INFO - training batch 801, loss: 0.349, 25632/60000 datapoints
2025-03-06 19:18:58,151 - INFO - training batch 851, loss: 0.187, 27232/60000 datapoints
2025-03-06 19:18:58,373 - INFO - training batch 901, loss: 0.383, 28832/60000 datapoints
2025-03-06 19:18:58,600 - INFO - training batch 951, loss: 0.388, 30432/60000 datapoints
2025-03-06 19:18:58,830 - INFO - training batch 1001, loss: 0.449, 32032/60000 datapoints
2025-03-06 19:18:59,053 - INFO - training batch 1051, loss: 0.527, 33632/60000 datapoints
2025-03-06 19:18:59,278 - INFO - training batch 1101, loss: 0.096, 35232/60000 datapoints
2025-03-06 19:18:59,502 - INFO - training batch 1151, loss: 0.589, 36832/60000 datapoints
2025-03-06 19:18:59,733 - INFO - training batch 1201, loss: 0.170, 38432/60000 datapoints
2025-03-06 19:18:59,958 - INFO - training batch 1251, loss: 0.131, 40032/60000 datapoints
2025-03-06 19:19:00,180 - INFO - training batch 1301, loss: 0.227, 41632/60000 datapoints
2025-03-06 19:19:00,429 - INFO - training batch 1351, loss: 0.269, 43232/60000 datapoints
2025-03-06 19:19:00,665 - INFO - training batch 1401, loss: 0.253, 44832/60000 datapoints
2025-03-06 19:19:00,887 - INFO - training batch 1451, loss: 0.167, 46432/60000 datapoints
2025-03-06 19:19:01,109 - INFO - training batch 1501, loss: 0.268, 48032/60000 datapoints
2025-03-06 19:19:01,339 - INFO - training batch 1551, loss: 0.361, 49632/60000 datapoints
2025-03-06 19:19:01,570 - INFO - training batch 1601, loss: 0.275, 51232/60000 datapoints
2025-03-06 19:19:01,795 - INFO - training batch 1651, loss: 0.243, 52832/60000 datapoints
2025-03-06 19:19:02,021 - INFO - training batch 1701, loss: 0.152, 54432/60000 datapoints
2025-03-06 19:19:02,241 - INFO - training batch 1751, loss: 0.177, 56032/60000 datapoints
2025-03-06 19:19:02,471 - INFO - training batch 1801, loss: 0.372, 57632/60000 datapoints
2025-03-06 19:19:02,698 - INFO - training batch 1851, loss: 0.290, 59232/60000 datapoints
2025-03-06 19:19:02,818 - INFO - validation batch 1, loss: 0.472, 32/10016 datapoints
2025-03-06 19:19:02,998 - INFO - validation batch 51, loss: 0.291, 1632/10016 datapoints
2025-03-06 19:19:03,179 - INFO - validation batch 101, loss: 0.225, 3232/10016 datapoints
2025-03-06 19:19:03,355 - INFO - validation batch 151, loss: 0.292, 4832/10016 datapoints
2025-03-06 19:19:03,532 - INFO - validation batch 201, loss: 0.346, 6432/10016 datapoints
2025-03-06 19:19:03,709 - INFO - validation batch 251, loss: 0.650, 8032/10016 datapoints
2025-03-06 19:19:03,893 - INFO - validation batch 301, loss: 0.514, 9632/10016 datapoints
2025-03-06 19:19:03,940 - INFO - Epoch 273/800 done.
2025-03-06 19:19:03,940 - INFO - Final validation performance:
Loss: 0.399, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 19:19:03,941 - INFO - Beginning epoch 274/800
2025-03-06 19:19:03,948 - INFO - training batch 1, loss: 0.290, 32/60000 datapoints
2025-03-06 19:19:04,169 - INFO - training batch 51, loss: 0.121, 1632/60000 datapoints
2025-03-06 19:19:04,400 - INFO - training batch 101, loss: 0.199, 3232/60000 datapoints
2025-03-06 19:19:04,628 - INFO - training batch 151, loss: 0.205, 4832/60000 datapoints
2025-03-06 19:19:04,843 - INFO - training batch 201, loss: 0.196, 6432/60000 datapoints
2025-03-06 19:19:05,073 - INFO - training batch 251, loss: 0.553, 8032/60000 datapoints
2025-03-06 19:19:05,289 - INFO - training batch 301, loss: 0.247, 9632/60000 datapoints
2025-03-06 19:19:05,502 - INFO - training batch 351, loss: 0.395, 11232/60000 datapoints
2025-03-06 19:19:05,722 - INFO - training batch 401, loss: 0.756, 12832/60000 datapoints
2025-03-06 19:19:05,938 - INFO - training batch 451, loss: 0.267, 14432/60000 datapoints
2025-03-06 19:19:06,147 - INFO - training batch 501, loss: 0.199, 16032/60000 datapoints
2025-03-06 19:19:06,367 - INFO - training batch 551, loss: 0.470, 17632/60000 datapoints
2025-03-06 19:19:06,581 - INFO - training batch 601, loss: 0.119, 19232/60000 datapoints
2025-03-06 19:19:06,800 - INFO - training batch 651, loss: 0.210, 20832/60000 datapoints
2025-03-06 19:19:07,019 - INFO - training batch 701, loss: 0.884, 22432/60000 datapoints
2025-03-06 19:19:07,232 - INFO - training batch 751, loss: 0.312, 24032/60000 datapoints
2025-03-06 19:19:07,473 - INFO - training batch 801, loss: 0.408, 25632/60000 datapoints
2025-03-06 19:19:07,701 - INFO - training batch 851, loss: 0.252, 27232/60000 datapoints
2025-03-06 19:19:07,914 - INFO - training batch 901, loss: 0.347, 28832/60000 datapoints
2025-03-06 19:19:08,125 - INFO - training batch 951, loss: 0.437, 30432/60000 datapoints
2025-03-06 19:19:08,336 - INFO - training batch 1001, loss: 0.286, 32032/60000 datapoints
2025-03-06 19:19:08,546 - INFO - training batch 1051, loss: 0.314, 33632/60000 datapoints
2025-03-06 19:19:08,760 - INFO - training batch 1101, loss: 0.306, 35232/60000 datapoints
2025-03-06 19:19:08,977 - INFO - training batch 1151, loss: 0.244, 36832/60000 datapoints
2025-03-06 19:19:09,204 - INFO - training batch 1201, loss: 0.217, 38432/60000 datapoints
2025-03-06 19:19:09,413 - INFO - training batch 1251, loss: 0.342, 40032/60000 datapoints
2025-03-06 19:19:09,627 - INFO - training batch 1301, loss: 0.374, 41632/60000 datapoints
2025-03-06 19:19:09,839 - INFO - training batch 1351, loss: 0.207, 43232/60000 datapoints
2025-03-06 19:19:10,050 - INFO - training batch 1401, loss: 0.544, 44832/60000 datapoints
2025-03-06 19:19:10,260 - INFO - training batch 1451, loss: 0.282, 46432/60000 datapoints
2025-03-06 19:19:10,479 - INFO - training batch 1501, loss: 0.335, 48032/60000 datapoints
2025-03-06 19:19:10,711 - INFO - training batch 1551, loss: 0.173, 49632/60000 datapoints
2025-03-06 19:19:10,922 - INFO - training batch 1601, loss: 0.098, 51232/60000 datapoints
2025-03-06 19:19:11,133 - INFO - training batch 1651, loss: 0.207, 52832/60000 datapoints
2025-03-06 19:19:11,344 - INFO - training batch 1701, loss: 0.158, 54432/60000 datapoints
2025-03-06 19:19:11,560 - INFO - training batch 1751, loss: 0.387, 56032/60000 datapoints
2025-03-06 19:19:11,777 - INFO - training batch 1801, loss: 0.341, 57632/60000 datapoints
2025-03-06 19:19:11,992 - INFO - training batch 1851, loss: 0.326, 59232/60000 datapoints
2025-03-06 19:19:12,105 - INFO - validation batch 1, loss: 0.454, 32/10016 datapoints
2025-03-06 19:19:12,277 - INFO - validation batch 51, loss: 0.371, 1632/10016 datapoints
2025-03-06 19:19:12,446 - INFO - validation batch 101, loss: 0.159, 3232/10016 datapoints
2025-03-06 19:19:12,624 - INFO - validation batch 151, loss: 0.238, 4832/10016 datapoints
2025-03-06 19:19:12,795 - INFO - validation batch 201, loss: 0.269, 6432/10016 datapoints
2025-03-06 19:19:12,963 - INFO - validation batch 251, loss: 0.543, 8032/10016 datapoints
2025-03-06 19:19:13,135 - INFO - validation batch 301, loss: 0.137, 9632/10016 datapoints
2025-03-06 19:19:13,184 - INFO - Epoch 274/800 done.
2025-03-06 19:19:13,185 - INFO - Final validation performance:
Loss: 0.310, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 19:19:13,185 - INFO - Beginning epoch 275/800
2025-03-06 19:19:13,192 - INFO - training batch 1, loss: 0.247, 32/60000 datapoints
2025-03-06 19:19:13,426 - INFO - training batch 51, loss: 0.305, 1632/60000 datapoints
2025-03-06 19:19:13,645 - INFO - training batch 101, loss: 0.097, 3232/60000 datapoints
2025-03-06 19:19:13,873 - INFO - training batch 151, loss: 0.359, 4832/60000 datapoints
2025-03-06 19:19:14,088 - INFO - training batch 201, loss: 0.359, 6432/60000 datapoints
2025-03-06 19:19:14,300 - INFO - training batch 251, loss: 0.223, 8032/60000 datapoints
2025-03-06 19:19:14,516 - INFO - training batch 301, loss: 0.183, 9632/60000 datapoints
2025-03-06 19:19:14,734 - INFO - training batch 351, loss: 0.164, 11232/60000 datapoints
2025-03-06 19:19:14,959 - INFO - training batch 401, loss: 0.282, 12832/60000 datapoints
2025-03-06 19:19:15,176 - INFO - training batch 451, loss: 0.778, 14432/60000 datapoints
2025-03-06 19:19:15,399 - INFO - training batch 501, loss: 0.339, 16032/60000 datapoints
2025-03-06 19:19:15,622 - INFO - training batch 551, loss: 0.283, 17632/60000 datapoints
2025-03-06 19:19:15,839 - INFO - training batch 601, loss: 0.264, 19232/60000 datapoints
2025-03-06 19:19:16,059 - INFO - training batch 651, loss: 0.198, 20832/60000 datapoints
2025-03-06 19:19:16,278 - INFO - training batch 701, loss: 0.306, 22432/60000 datapoints
2025-03-06 19:19:16,500 - INFO - training batch 751, loss: 0.289, 24032/60000 datapoints
2025-03-06 19:19:16,724 - INFO - training batch 801, loss: 0.222, 25632/60000 datapoints
2025-03-06 19:19:16,941 - INFO - training batch 851, loss: 0.181, 27232/60000 datapoints
2025-03-06 19:19:17,158 - INFO - training batch 901, loss: 0.247, 28832/60000 datapoints
2025-03-06 19:19:17,377 - INFO - training batch 951, loss: 0.831, 30432/60000 datapoints
2025-03-06 19:19:17,595 - INFO - training batch 1001, loss: 0.222, 32032/60000 datapoints
2025-03-06 19:19:17,820 - INFO - training batch 1051, loss: 0.362, 33632/60000 datapoints
2025-03-06 19:19:18,044 - INFO - training batch 1101, loss: 0.613, 35232/60000 datapoints
2025-03-06 19:19:18,265 - INFO - training batch 1151, loss: 0.212, 36832/60000 datapoints
2025-03-06 19:19:18,485 - INFO - training batch 1201, loss: 0.411, 38432/60000 datapoints
2025-03-06 19:19:18,709 - INFO - training batch 1251, loss: 0.139, 40032/60000 datapoints
2025-03-06 19:19:18,935 - INFO - training batch 1301, loss: 0.375, 41632/60000 datapoints
2025-03-06 19:19:19,153 - INFO - training batch 1351, loss: 0.308, 43232/60000 datapoints
2025-03-06 19:19:19,383 - INFO - training batch 1401, loss: 0.185, 44832/60000 datapoints
2025-03-06 19:19:19,618 - INFO - training batch 1451, loss: 0.210, 46432/60000 datapoints
2025-03-06 19:19:19,838 - INFO - training batch 1501, loss: 0.531, 48032/60000 datapoints
2025-03-06 19:19:20,060 - INFO - training batch 1551, loss: 0.305, 49632/60000 datapoints
2025-03-06 19:19:20,275 - INFO - training batch 1601, loss: 0.771, 51232/60000 datapoints
2025-03-06 19:19:20,488 - INFO - training batch 1651, loss: 0.140, 52832/60000 datapoints
2025-03-06 19:19:20,728 - INFO - training batch 1701, loss: 0.252, 54432/60000 datapoints
2025-03-06 19:19:20,942 - INFO - training batch 1751, loss: 0.460, 56032/60000 datapoints
2025-03-06 19:19:21,158 - INFO - training batch 1801, loss: 0.189, 57632/60000 datapoints
2025-03-06 19:19:21,378 - INFO - training batch 1851, loss: 0.337, 59232/60000 datapoints
2025-03-06 19:19:21,492 - INFO - validation batch 1, loss: 0.199, 32/10016 datapoints
2025-03-06 19:19:21,669 - INFO - validation batch 51, loss: 0.254, 1632/10016 datapoints
2025-03-06 19:19:21,856 - INFO - validation batch 101, loss: 0.211, 3232/10016 datapoints
2025-03-06 19:19:22,041 - INFO - validation batch 151, loss: 0.225, 4832/10016 datapoints
2025-03-06 19:19:22,224 - INFO - validation batch 201, loss: 0.240, 6432/10016 datapoints
2025-03-06 19:19:22,406 - INFO - validation batch 251, loss: 0.279, 8032/10016 datapoints
2025-03-06 19:19:22,592 - INFO - validation batch 301, loss: 0.252, 9632/10016 datapoints
2025-03-06 19:19:22,643 - INFO - Epoch 275/800 done.
2025-03-06 19:19:22,663 - INFO - Final validation performance:
Loss: 0.237, top-1 acc: 0.910top-5 acc: 0.910
2025-03-06 19:19:22,666 - INFO - Beginning epoch 276/800
2025-03-06 19:19:22,699 - INFO - training batch 1, loss: 0.317, 32/60000 datapoints
2025-03-06 19:19:22,995 - INFO - training batch 51, loss: 0.378, 1632/60000 datapoints
2025-03-06 19:19:23,247 - INFO - training batch 101, loss: 0.368, 3232/60000 datapoints
2025-03-06 19:19:23,479 - INFO - training batch 151, loss: 0.373, 4832/60000 datapoints
2025-03-06 19:19:23,709 - INFO - training batch 201, loss: 0.644, 6432/60000 datapoints
2025-03-06 19:19:23,934 - INFO - training batch 251, loss: 0.447, 8032/60000 datapoints
2025-03-06 19:19:24,161 - INFO - training batch 301, loss: 0.290, 9632/60000 datapoints
2025-03-06 19:19:24,387 - INFO - training batch 351, loss: 0.194, 11232/60000 datapoints
2025-03-06 19:19:24,632 - INFO - training batch 401, loss: 0.175, 12832/60000 datapoints
2025-03-06 19:19:24,857 - INFO - training batch 451, loss: 0.205, 14432/60000 datapoints
2025-03-06 19:19:25,088 - INFO - training batch 501, loss: 0.526, 16032/60000 datapoints
2025-03-06 19:19:25,312 - INFO - training batch 551, loss: 0.355, 17632/60000 datapoints
2025-03-06 19:19:25,539 - INFO - training batch 601, loss: 0.478, 19232/60000 datapoints
2025-03-06 19:19:25,772 - INFO - training batch 651, loss: 0.533, 20832/60000 datapoints
2025-03-06 19:19:25,999 - INFO - training batch 701, loss: 0.187, 22432/60000 datapoints
2025-03-06 19:19:26,214 - INFO - training batch 751, loss: 0.269, 24032/60000 datapoints
2025-03-06 19:19:26,433 - INFO - training batch 801, loss: 0.145, 25632/60000 datapoints
2025-03-06 19:19:26,655 - INFO - training batch 851, loss: 0.304, 27232/60000 datapoints
2025-03-06 19:19:26,875 - INFO - training batch 901, loss: 0.256, 28832/60000 datapoints
2025-03-06 19:19:27,087 - INFO - training batch 951, loss: 0.462, 30432/60000 datapoints
2025-03-06 19:19:27,307 - INFO - training batch 1001, loss: 0.437, 32032/60000 datapoints
2025-03-06 19:19:27,524 - INFO - training batch 1051, loss: 0.229, 33632/60000 datapoints
2025-03-06 19:19:27,739 - INFO - training batch 1101, loss: 0.557, 35232/60000 datapoints
2025-03-06 19:19:27,950 - INFO - training batch 1151, loss: 0.193, 36832/60000 datapoints
2025-03-06 19:19:28,163 - INFO - training batch 1201, loss: 0.391, 38432/60000 datapoints
2025-03-06 19:19:28,375 - INFO - training batch 1251, loss: 0.506, 40032/60000 datapoints
2025-03-06 19:19:28,586 - INFO - training batch 1301, loss: 0.517, 41632/60000 datapoints
2025-03-06 19:19:28,801 - INFO - training batch 1351, loss: 0.141, 43232/60000 datapoints
2025-03-06 19:19:29,022 - INFO - training batch 1401, loss: 0.262, 44832/60000 datapoints
2025-03-06 19:19:29,229 - INFO - training batch 1451, loss: 0.575, 46432/60000 datapoints
2025-03-06 19:19:29,440 - INFO - training batch 1501, loss: 0.351, 48032/60000 datapoints
2025-03-06 19:19:29,656 - INFO - training batch 1551, loss: 0.249, 49632/60000 datapoints
2025-03-06 19:19:29,879 - INFO - training batch 1601, loss: 0.227, 51232/60000 datapoints
2025-03-06 19:19:30,091 - INFO - training batch 1651, loss: 0.295, 52832/60000 datapoints
2025-03-06 19:19:30,300 - INFO - training batch 1701, loss: 0.161, 54432/60000 datapoints
2025-03-06 19:19:30,514 - INFO - training batch 1751, loss: 0.189, 56032/60000 datapoints
2025-03-06 19:19:30,755 - INFO - training batch 1801, loss: 0.340, 57632/60000 datapoints
2025-03-06 19:19:30,970 - INFO - training batch 1851, loss: 0.196, 59232/60000 datapoints
2025-03-06 19:19:31,084 - INFO - validation batch 1, loss: 0.128, 32/10016 datapoints
2025-03-06 19:19:31,251 - INFO - validation batch 51, loss: 0.402, 1632/10016 datapoints
2025-03-06 19:19:31,426 - INFO - validation batch 101, loss: 0.347, 3232/10016 datapoints
2025-03-06 19:19:31,599 - INFO - validation batch 151, loss: 0.104, 4832/10016 datapoints
2025-03-06 19:19:31,772 - INFO - validation batch 201, loss: 0.357, 6432/10016 datapoints
2025-03-06 19:19:31,941 - INFO - validation batch 251, loss: 0.386, 8032/10016 datapoints
2025-03-06 19:19:32,109 - INFO - validation batch 301, loss: 0.349, 9632/10016 datapoints
2025-03-06 19:19:32,149 - INFO - Epoch 276/800 done.
2025-03-06 19:19:32,149 - INFO - Final validation performance:
Loss: 0.296, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 19:19:32,149 - INFO - Beginning epoch 277/800
2025-03-06 19:19:32,158 - INFO - training batch 1, loss: 0.288, 32/60000 datapoints
2025-03-06 19:19:32,389 - INFO - training batch 51, loss: 0.396, 1632/60000 datapoints
2025-03-06 19:19:32,595 - INFO - training batch 101, loss: 0.308, 3232/60000 datapoints
2025-03-06 19:19:32,816 - INFO - training batch 151, loss: 0.418, 4832/60000 datapoints
2025-03-06 19:19:33,017 - INFO - training batch 201, loss: 0.151, 6432/60000 datapoints
2025-03-06 19:19:33,225 - INFO - training batch 251, loss: 0.366, 8032/60000 datapoints
2025-03-06 19:19:33,430 - INFO - training batch 301, loss: 0.739, 9632/60000 datapoints
2025-03-06 19:19:33,637 - INFO - training batch 351, loss: 0.263, 11232/60000 datapoints
2025-03-06 19:19:33,836 - INFO - training batch 401, loss: 0.333, 12832/60000 datapoints
2025-03-06 19:19:34,037 - INFO - training batch 451, loss: 0.202, 14432/60000 datapoints
2025-03-06 19:19:34,234 - INFO - training batch 501, loss: 0.206, 16032/60000 datapoints
2025-03-06 19:19:34,431 - INFO - training batch 551, loss: 0.229, 17632/60000 datapoints
2025-03-06 19:19:34,636 - INFO - training batch 601, loss: 0.170, 19232/60000 datapoints
2025-03-06 19:19:34,835 - INFO - training batch 651, loss: 0.388, 20832/60000 datapoints
2025-03-06 19:19:35,044 - INFO - training batch 701, loss: 0.469, 22432/60000 datapoints
2025-03-06 19:19:35,240 - INFO - training batch 751, loss: 0.282, 24032/60000 datapoints
2025-03-06 19:19:35,438 - INFO - training batch 801, loss: 0.227, 25632/60000 datapoints
2025-03-06 19:19:35,643 - INFO - training batch 851, loss: 0.571, 27232/60000 datapoints
2025-03-06 19:19:35,841 - INFO - training batch 901, loss: 0.577, 28832/60000 datapoints
2025-03-06 19:19:36,042 - INFO - training batch 951, loss: 0.133, 30432/60000 datapoints
2025-03-06 19:19:36,240 - INFO - training batch 1001, loss: 0.185, 32032/60000 datapoints
2025-03-06 19:19:36,438 - INFO - training batch 1051, loss: 0.363, 33632/60000 datapoints
2025-03-06 19:19:36,638 - INFO - training batch 1101, loss: 0.151, 35232/60000 datapoints
2025-03-06 19:19:36,839 - INFO - training batch 1151, loss: 0.227, 36832/60000 datapoints
2025-03-06 19:19:37,040 - INFO - training batch 1201, loss: 0.145, 38432/60000 datapoints
2025-03-06 19:19:37,242 - INFO - training batch 1251, loss: 0.976, 40032/60000 datapoints
2025-03-06 19:19:37,455 - INFO - training batch 1301, loss: 0.410, 41632/60000 datapoints
2025-03-06 19:19:37,673 - INFO - training batch 1351, loss: 0.636, 43232/60000 datapoints
2025-03-06 19:19:37,895 - INFO - training batch 1401, loss: 0.237, 44832/60000 datapoints
2025-03-06 19:19:38,100 - INFO - training batch 1451, loss: 0.386, 46432/60000 datapoints
2025-03-06 19:19:38,303 - INFO - training batch 1501, loss: 0.267, 48032/60000 datapoints
2025-03-06 19:19:38,526 - INFO - training batch 1551, loss: 0.209, 49632/60000 datapoints
2025-03-06 19:19:38,724 - INFO - training batch 1601, loss: 0.544, 51232/60000 datapoints
2025-03-06 19:19:38,922 - INFO - training batch 1651, loss: 0.388, 52832/60000 datapoints
2025-03-06 19:19:39,133 - INFO - training batch 1701, loss: 0.271, 54432/60000 datapoints
2025-03-06 19:19:39,330 - INFO - training batch 1751, loss: 0.294, 56032/60000 datapoints
2025-03-06 19:19:39,529 - INFO - training batch 1801, loss: 0.377, 57632/60000 datapoints
2025-03-06 19:19:39,732 - INFO - training batch 1851, loss: 0.431, 59232/60000 datapoints
2025-03-06 19:19:39,834 - INFO - validation batch 1, loss: 0.241, 32/10016 datapoints
2025-03-06 19:19:39,990 - INFO - validation batch 51, loss: 0.395, 1632/10016 datapoints
2025-03-06 19:19:40,148 - INFO - validation batch 101, loss: 0.214, 3232/10016 datapoints
2025-03-06 19:19:40,303 - INFO - validation batch 151, loss: 0.421, 4832/10016 datapoints
2025-03-06 19:19:40,459 - INFO - validation batch 201, loss: 0.570, 6432/10016 datapoints
2025-03-06 19:19:40,616 - INFO - validation batch 251, loss: 0.365, 8032/10016 datapoints
2025-03-06 19:19:40,769 - INFO - validation batch 301, loss: 0.458, 9632/10016 datapoints
2025-03-06 19:19:40,814 - INFO - Epoch 277/800 done.
2025-03-06 19:19:40,814 - INFO - Final validation performance:
Loss: 0.380, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 19:19:40,815 - INFO - Beginning epoch 278/800
2025-03-06 19:19:40,823 - INFO - training batch 1, loss: 0.248, 32/60000 datapoints
2025-03-06 19:19:41,034 - INFO - training batch 51, loss: 0.254, 1632/60000 datapoints
2025-03-06 19:19:41,229 - INFO - training batch 101, loss: 0.232, 3232/60000 datapoints
2025-03-06 19:19:41,436 - INFO - training batch 151, loss: 0.239, 4832/60000 datapoints
2025-03-06 19:19:41,639 - INFO - training batch 201, loss: 0.179, 6432/60000 datapoints
2025-03-06 19:19:41,838 - INFO - training batch 251, loss: 0.163, 8032/60000 datapoints
2025-03-06 19:19:42,048 - INFO - training batch 301, loss: 0.173, 9632/60000 datapoints
2025-03-06 19:19:42,255 - INFO - training batch 351, loss: 0.181, 11232/60000 datapoints
2025-03-06 19:19:42,473 - INFO - training batch 401, loss: 0.366, 12832/60000 datapoints
2025-03-06 19:19:42,676 - INFO - training batch 451, loss: 0.236, 14432/60000 datapoints
2025-03-06 19:19:42,890 - INFO - training batch 501, loss: 0.370, 16032/60000 datapoints
2025-03-06 19:19:43,101 - INFO - training batch 551, loss: 0.169, 17632/60000 datapoints
2025-03-06 19:19:43,315 - INFO - training batch 601, loss: 0.208, 19232/60000 datapoints
2025-03-06 19:19:43,537 - INFO - training batch 651, loss: 0.424, 20832/60000 datapoints
2025-03-06 19:19:43,757 - INFO - training batch 701, loss: 0.295, 22432/60000 datapoints
2025-03-06 19:19:43,986 - INFO - training batch 751, loss: 0.197, 24032/60000 datapoints
2025-03-06 19:19:44,191 - INFO - training batch 801, loss: 0.449, 25632/60000 datapoints
2025-03-06 19:19:44,391 - INFO - training batch 851, loss: 0.094, 27232/60000 datapoints
2025-03-06 19:19:44,598 - INFO - training batch 901, loss: 0.499, 28832/60000 datapoints
2025-03-06 19:19:44,802 - INFO - training batch 951, loss: 0.349, 30432/60000 datapoints
2025-03-06 19:19:45,017 - INFO - training batch 1001, loss: 0.244, 32032/60000 datapoints
2025-03-06 19:19:45,218 - INFO - training batch 1051, loss: 0.666, 33632/60000 datapoints
2025-03-06 19:19:45,420 - INFO - training batch 1101, loss: 0.383, 35232/60000 datapoints
2025-03-06 19:19:45,626 - INFO - training batch 1151, loss: 0.262, 36832/60000 datapoints
2025-03-06 19:19:45,823 - INFO - training batch 1201, loss: 0.452, 38432/60000 datapoints
2025-03-06 19:19:46,022 - INFO - training batch 1251, loss: 0.147, 40032/60000 datapoints
2025-03-06 19:19:46,222 - INFO - training batch 1301, loss: 0.285, 41632/60000 datapoints
2025-03-06 19:19:46,425 - INFO - training batch 1351, loss: 0.467, 43232/60000 datapoints
2025-03-06 19:19:46,629 - INFO - training batch 1401, loss: 0.227, 44832/60000 datapoints
2025-03-06 19:19:46,830 - INFO - training batch 1451, loss: 0.150, 46432/60000 datapoints
2025-03-06 19:19:47,027 - INFO - training batch 1501, loss: 0.298, 48032/60000 datapoints
2025-03-06 19:19:47,225 - INFO - training batch 1551, loss: 0.335, 49632/60000 datapoints
2025-03-06 19:19:47,423 - INFO - training batch 1601, loss: 0.286, 51232/60000 datapoints
2025-03-06 19:19:47,624 - INFO - training batch 1651, loss: 0.205, 52832/60000 datapoints
2025-03-06 19:19:47,823 - INFO - training batch 1701, loss: 0.339, 54432/60000 datapoints
2025-03-06 19:19:48,020 - INFO - training batch 1751, loss: 0.275, 56032/60000 datapoints
2025-03-06 19:19:48,223 - INFO - training batch 1801, loss: 0.172, 57632/60000 datapoints
2025-03-06 19:19:48,418 - INFO - training batch 1851, loss: 0.329, 59232/60000 datapoints
2025-03-06 19:19:48,522 - INFO - validation batch 1, loss: 0.470, 32/10016 datapoints
2025-03-06 19:19:48,679 - INFO - validation batch 51, loss: 0.252, 1632/10016 datapoints
2025-03-06 19:19:48,834 - INFO - validation batch 101, loss: 0.205, 3232/10016 datapoints
2025-03-06 19:19:48,993 - INFO - validation batch 151, loss: 0.274, 4832/10016 datapoints
2025-03-06 19:19:49,152 - INFO - validation batch 201, loss: 0.102, 6432/10016 datapoints
2025-03-06 19:19:49,307 - INFO - validation batch 251, loss: 0.169, 8032/10016 datapoints
2025-03-06 19:19:49,463 - INFO - validation batch 301, loss: 0.419, 9632/10016 datapoints
2025-03-06 19:19:49,499 - INFO - Epoch 278/800 done.
2025-03-06 19:19:49,499 - INFO - Final validation performance:
Loss: 0.270, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 19:19:49,500 - INFO - Beginning epoch 279/800
2025-03-06 19:19:49,507 - INFO - training batch 1, loss: 0.209, 32/60000 datapoints
2025-03-06 19:19:49,712 - INFO - training batch 51, loss: 0.242, 1632/60000 datapoints
2025-03-06 19:19:49,912 - INFO - training batch 101, loss: 0.344, 3232/60000 datapoints
2025-03-06 19:19:50,131 - INFO - training batch 151, loss: 0.402, 4832/60000 datapoints
2025-03-06 19:19:50,335 - INFO - training batch 201, loss: 0.235, 6432/60000 datapoints
2025-03-06 19:19:50,543 - INFO - training batch 251, loss: 0.472, 8032/60000 datapoints
2025-03-06 19:19:50,752 - INFO - training batch 301, loss: 0.322, 9632/60000 datapoints
2025-03-06 19:19:50,979 - INFO - training batch 351, loss: 0.073, 11232/60000 datapoints
2025-03-06 19:19:51,186 - INFO - training batch 401, loss: 0.150, 12832/60000 datapoints
2025-03-06 19:19:51,388 - INFO - training batch 451, loss: 0.502, 14432/60000 datapoints
2025-03-06 19:19:51,591 - INFO - training batch 501, loss: 0.325, 16032/60000 datapoints
2025-03-06 19:19:51,798 - INFO - training batch 551, loss: 0.195, 17632/60000 datapoints
2025-03-06 19:19:52,002 - INFO - training batch 601, loss: 0.162, 19232/60000 datapoints
2025-03-06 19:19:52,210 - INFO - training batch 651, loss: 0.357, 20832/60000 datapoints
2025-03-06 19:19:52,413 - INFO - training batch 701, loss: 0.342, 22432/60000 datapoints
2025-03-06 19:19:52,624 - INFO - training batch 751, loss: 0.299, 24032/60000 datapoints
2025-03-06 19:19:52,834 - INFO - training batch 801, loss: 0.245, 25632/60000 datapoints
2025-03-06 19:19:53,034 - INFO - training batch 851, loss: 0.238, 27232/60000 datapoints
2025-03-06 19:19:53,239 - INFO - training batch 901, loss: 0.218, 28832/60000 datapoints
2025-03-06 19:19:53,448 - INFO - training batch 951, loss: 0.294, 30432/60000 datapoints
2025-03-06 19:19:53,657 - INFO - training batch 1001, loss: 0.189, 32032/60000 datapoints
2025-03-06 19:19:53,881 - INFO - training batch 1051, loss: 0.336, 33632/60000 datapoints
2025-03-06 19:19:54,087 - INFO - training batch 1101, loss: 0.470, 35232/60000 datapoints
2025-03-06 19:19:54,292 - INFO - training batch 1151, loss: 0.322, 36832/60000 datapoints
2025-03-06 19:19:54,504 - INFO - training batch 1201, loss: 0.224, 38432/60000 datapoints
2025-03-06 19:19:54,715 - INFO - training batch 1251, loss: 0.144, 40032/60000 datapoints
2025-03-06 19:19:54,931 - INFO - training batch 1301, loss: 0.511, 41632/60000 datapoints
2025-03-06 19:19:55,142 - INFO - training batch 1351, loss: 0.271, 43232/60000 datapoints
2025-03-06 19:19:55,351 - INFO - training batch 1401, loss: 0.205, 44832/60000 datapoints
2025-03-06 19:19:55,557 - INFO - training batch 1451, loss: 0.425, 46432/60000 datapoints
2025-03-06 19:19:55,765 - INFO - training batch 1501, loss: 0.278, 48032/60000 datapoints
2025-03-06 19:19:55,968 - INFO - training batch 1551, loss: 0.225, 49632/60000 datapoints
2025-03-06 19:19:56,176 - INFO - training batch 1601, loss: 0.145, 51232/60000 datapoints
2025-03-06 19:19:56,378 - INFO - training batch 1651, loss: 0.224, 52832/60000 datapoints
2025-03-06 19:19:56,580 - INFO - training batch 1701, loss: 0.301, 54432/60000 datapoints
2025-03-06 19:19:56,786 - INFO - training batch 1751, loss: 0.301, 56032/60000 datapoints
2025-03-06 19:19:56,987 - INFO - training batch 1801, loss: 0.357, 57632/60000 datapoints
2025-03-06 19:19:57,196 - INFO - training batch 1851, loss: 0.559, 59232/60000 datapoints
2025-03-06 19:19:57,300 - INFO - validation batch 1, loss: 0.416, 32/10016 datapoints
2025-03-06 19:19:57,462 - INFO - validation batch 51, loss: 0.174, 1632/10016 datapoints
2025-03-06 19:19:57,625 - INFO - validation batch 101, loss: 0.247, 3232/10016 datapoints
2025-03-06 19:19:57,790 - INFO - validation batch 151, loss: 0.173, 4832/10016 datapoints
2025-03-06 19:19:57,950 - INFO - validation batch 201, loss: 0.359, 6432/10016 datapoints
2025-03-06 19:19:58,114 - INFO - validation batch 251, loss: 0.274, 8032/10016 datapoints
2025-03-06 19:19:58,281 - INFO - validation batch 301, loss: 0.413, 9632/10016 datapoints
2025-03-06 19:19:58,322 - INFO - Epoch 279/800 done.
2025-03-06 19:19:58,322 - INFO - Final validation performance:
Loss: 0.294, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 19:19:58,323 - INFO - Beginning epoch 280/800
2025-03-06 19:19:58,329 - INFO - training batch 1, loss: 0.257, 32/60000 datapoints
2025-03-06 19:19:58,540 - INFO - training batch 51, loss: 0.323, 1632/60000 datapoints
2025-03-06 19:19:58,751 - INFO - training batch 101, loss: 0.217, 3232/60000 datapoints
2025-03-06 19:19:58,970 - INFO - training batch 151, loss: 0.419, 4832/60000 datapoints
2025-03-06 19:19:59,169 - INFO - training batch 201, loss: 0.168, 6432/60000 datapoints
2025-03-06 19:19:59,375 - INFO - training batch 251, loss: 0.266, 8032/60000 datapoints
2025-03-06 19:19:59,580 - INFO - training batch 301, loss: 0.189, 9632/60000 datapoints
2025-03-06 19:19:59,789 - INFO - training batch 351, loss: 0.414, 11232/60000 datapoints
2025-03-06 19:19:59,995 - INFO - training batch 401, loss: 0.148, 12832/60000 datapoints
2025-03-06 19:20:00,201 - INFO - training batch 451, loss: 0.329, 14432/60000 datapoints
2025-03-06 19:20:00,404 - INFO - training batch 501, loss: 0.548, 16032/60000 datapoints
2025-03-06 19:20:00,604 - INFO - training batch 551, loss: 0.457, 17632/60000 datapoints
2025-03-06 19:20:00,809 - INFO - training batch 601, loss: 0.385, 19232/60000 datapoints
2025-03-06 19:20:01,028 - INFO - training batch 651, loss: 0.292, 20832/60000 datapoints
2025-03-06 19:20:01,236 - INFO - training batch 701, loss: 0.530, 22432/60000 datapoints
2025-03-06 19:20:01,437 - INFO - training batch 751, loss: 0.154, 24032/60000 datapoints
2025-03-06 19:20:01,640 - INFO - training batch 801, loss: 0.331, 25632/60000 datapoints
2025-03-06 19:20:01,848 - INFO - training batch 851, loss: 0.175, 27232/60000 datapoints
2025-03-06 19:20:02,047 - INFO - training batch 901, loss: 0.397, 28832/60000 datapoints
2025-03-06 19:20:02,249 - INFO - training batch 951, loss: 0.517, 30432/60000 datapoints
2025-03-06 19:20:02,448 - INFO - training batch 1001, loss: 0.454, 32032/60000 datapoints
2025-03-06 19:20:02,649 - INFO - training batch 1051, loss: 0.241, 33632/60000 datapoints
2025-03-06 19:20:02,856 - INFO - training batch 1101, loss: 0.217, 35232/60000 datapoints
2025-03-06 19:20:03,076 - INFO - training batch 1151, loss: 0.255, 36832/60000 datapoints
2025-03-06 19:20:03,296 - INFO - training batch 1201, loss: 0.328, 38432/60000 datapoints
2025-03-06 19:20:03,498 - INFO - training batch 1251, loss: 0.667, 40032/60000 datapoints
2025-03-06 19:20:03,705 - INFO - training batch 1301, loss: 0.414, 41632/60000 datapoints
2025-03-06 19:20:03,919 - INFO - training batch 1351, loss: 0.241, 43232/60000 datapoints
2025-03-06 19:20:04,248 - INFO - training batch 1401, loss: 0.590, 44832/60000 datapoints
2025-03-06 19:20:04,454 - INFO - training batch 1451, loss: 0.398, 46432/60000 datapoints
2025-03-06 19:20:04,665 - INFO - training batch 1501, loss: 0.314, 48032/60000 datapoints
2025-03-06 19:20:04,877 - INFO - training batch 1551, loss: 0.398, 49632/60000 datapoints
2025-03-06 19:20:05,086 - INFO - training batch 1601, loss: 0.211, 51232/60000 datapoints
2025-03-06 19:20:05,295 - INFO - training batch 1651, loss: 0.174, 52832/60000 datapoints
2025-03-06 19:20:05,501 - INFO - training batch 1701, loss: 0.223, 54432/60000 datapoints
2025-03-06 19:20:05,707 - INFO - training batch 1751, loss: 0.225, 56032/60000 datapoints
2025-03-06 19:20:05,914 - INFO - training batch 1801, loss: 0.314, 57632/60000 datapoints
2025-03-06 19:20:06,117 - INFO - training batch 1851, loss: 0.181, 59232/60000 datapoints
2025-03-06 19:20:06,228 - INFO - validation batch 1, loss: 0.375, 32/10016 datapoints
2025-03-06 19:20:06,391 - INFO - validation batch 51, loss: 0.284, 1632/10016 datapoints
2025-03-06 19:20:06,551 - INFO - validation batch 101, loss: 0.154, 3232/10016 datapoints
2025-03-06 19:20:06,722 - INFO - validation batch 151, loss: 0.338, 4832/10016 datapoints
2025-03-06 19:20:06,893 - INFO - validation batch 201, loss: 0.405, 6432/10016 datapoints
2025-03-06 19:20:07,055 - INFO - validation batch 251, loss: 0.495, 8032/10016 datapoints
2025-03-06 19:20:07,221 - INFO - validation batch 301, loss: 0.233, 9632/10016 datapoints
2025-03-06 19:20:07,261 - INFO - Epoch 280/800 done.
2025-03-06 19:20:07,261 - INFO - Final validation performance:
Loss: 0.326, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 19:20:07,262 - INFO - Beginning epoch 281/800
2025-03-06 19:20:07,272 - INFO - training batch 1, loss: 0.163, 32/60000 datapoints
2025-03-06 19:20:07,493 - INFO - training batch 51, loss: 0.290, 1632/60000 datapoints
2025-03-06 19:20:07,697 - INFO - training batch 101, loss: 0.247, 3232/60000 datapoints
2025-03-06 19:20:07,949 - INFO - training batch 151, loss: 0.274, 4832/60000 datapoints
2025-03-06 19:20:08,156 - INFO - training batch 201, loss: 0.278, 6432/60000 datapoints
2025-03-06 19:20:08,354 - INFO - training batch 251, loss: 0.362, 8032/60000 datapoints
2025-03-06 19:20:08,548 - INFO - training batch 301, loss: 0.668, 9632/60000 datapoints
2025-03-06 19:20:08,747 - INFO - training batch 351, loss: 0.213, 11232/60000 datapoints
2025-03-06 19:20:08,946 - INFO - training batch 401, loss: 0.378, 12832/60000 datapoints
2025-03-06 19:20:09,167 - INFO - training batch 451, loss: 0.228, 14432/60000 datapoints
2025-03-06 19:20:09,467 - INFO - training batch 501, loss: 0.210, 16032/60000 datapoints
2025-03-06 19:20:09,663 - INFO - training batch 551, loss: 0.564, 17632/60000 datapoints
2025-03-06 19:20:09,864 - INFO - training batch 601, loss: 0.521, 19232/60000 datapoints
2025-03-06 19:20:10,064 - INFO - training batch 651, loss: 0.648, 20832/60000 datapoints
2025-03-06 19:20:10,276 - INFO - training batch 701, loss: 0.158, 22432/60000 datapoints
2025-03-06 19:20:10,481 - INFO - training batch 751, loss: 0.325, 24032/60000 datapoints
2025-03-06 19:20:10,684 - INFO - training batch 801, loss: 0.231, 25632/60000 datapoints
2025-03-06 19:20:10,890 - INFO - training batch 851, loss: 0.389, 27232/60000 datapoints
2025-03-06 19:20:11,103 - INFO - training batch 901, loss: 0.304, 28832/60000 datapoints
2025-03-06 19:20:11,321 - INFO - training batch 951, loss: 0.150, 30432/60000 datapoints
2025-03-06 19:20:11,522 - INFO - training batch 1001, loss: 0.264, 32032/60000 datapoints
2025-03-06 19:20:11,729 - INFO - training batch 1051, loss: 0.225, 33632/60000 datapoints
2025-03-06 19:20:11,936 - INFO - training batch 1101, loss: 0.367, 35232/60000 datapoints
2025-03-06 19:20:12,150 - INFO - training batch 1151, loss: 0.217, 36832/60000 datapoints
2025-03-06 19:20:12,357 - INFO - training batch 1201, loss: 0.192, 38432/60000 datapoints
2025-03-06 19:20:12,560 - INFO - training batch 1251, loss: 0.282, 40032/60000 datapoints
2025-03-06 19:20:12,768 - INFO - training batch 1301, loss: 0.194, 41632/60000 datapoints
2025-03-06 19:20:12,971 - INFO - training batch 1351, loss: 0.306, 43232/60000 datapoints
2025-03-06 19:20:13,175 - INFO - training batch 1401, loss: 0.199, 44832/60000 datapoints
2025-03-06 19:20:13,379 - INFO - training batch 1451, loss: 0.501, 46432/60000 datapoints
2025-03-06 19:20:13,580 - INFO - training batch 1501, loss: 0.421, 48032/60000 datapoints
2025-03-06 19:20:13,785 - INFO - training batch 1551, loss: 0.381, 49632/60000 datapoints
2025-03-06 19:20:13,988 - INFO - training batch 1601, loss: 0.209, 51232/60000 datapoints
2025-03-06 19:20:14,195 - INFO - training batch 1651, loss: 0.175, 52832/60000 datapoints
2025-03-06 19:20:14,398 - INFO - training batch 1701, loss: 0.148, 54432/60000 datapoints
2025-03-06 19:20:14,599 - INFO - training batch 1751, loss: 0.343, 56032/60000 datapoints
2025-03-06 19:20:14,802 - INFO - training batch 1801, loss: 0.359, 57632/60000 datapoints
2025-03-06 19:20:15,013 - INFO - training batch 1851, loss: 0.550, 59232/60000 datapoints
2025-03-06 19:20:15,123 - INFO - validation batch 1, loss: 0.324, 32/10016 datapoints
2025-03-06 19:20:15,290 - INFO - validation batch 51, loss: 0.263, 1632/10016 datapoints
2025-03-06 19:20:15,455 - INFO - validation batch 101, loss: 0.170, 3232/10016 datapoints
2025-03-06 19:20:15,623 - INFO - validation batch 151, loss: 0.294, 4832/10016 datapoints
2025-03-06 19:20:15,789 - INFO - validation batch 201, loss: 0.206, 6432/10016 datapoints
2025-03-06 19:20:15,955 - INFO - validation batch 251, loss: 0.145, 8032/10016 datapoints
2025-03-06 19:20:16,118 - INFO - validation batch 301, loss: 0.279, 9632/10016 datapoints
2025-03-06 19:20:16,162 - INFO - Epoch 281/800 done.
2025-03-06 19:20:16,162 - INFO - Final validation performance:
Loss: 0.240, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 19:20:16,163 - INFO - Beginning epoch 282/800
2025-03-06 19:20:16,173 - INFO - training batch 1, loss: 0.595, 32/60000 datapoints
2025-03-06 19:20:16,377 - INFO - training batch 51, loss: 0.290, 1632/60000 datapoints
2025-03-06 19:20:16,587 - INFO - training batch 101, loss: 0.126, 3232/60000 datapoints
2025-03-06 19:20:16,795 - INFO - training batch 151, loss: 0.267, 4832/60000 datapoints
2025-03-06 19:20:16,998 - INFO - training batch 201, loss: 0.478, 6432/60000 datapoints
2025-03-06 19:20:17,208 - INFO - training batch 251, loss: 0.460, 8032/60000 datapoints
2025-03-06 19:20:17,415 - INFO - training batch 301, loss: 0.325, 9632/60000 datapoints
2025-03-06 19:20:17,619 - INFO - training batch 351, loss: 0.223, 11232/60000 datapoints
2025-03-06 19:20:17,818 - INFO - training batch 401, loss: 0.110, 12832/60000 datapoints
2025-03-06 19:20:18,018 - INFO - training batch 451, loss: 0.257, 14432/60000 datapoints
2025-03-06 19:20:18,221 - INFO - training batch 501, loss: 0.463, 16032/60000 datapoints
2025-03-06 19:20:18,414 - INFO - training batch 551, loss: 0.184, 17632/60000 datapoints
2025-03-06 19:20:18,618 - INFO - training batch 601, loss: 0.378, 19232/60000 datapoints
2025-03-06 19:20:18,814 - INFO - training batch 651, loss: 0.174, 20832/60000 datapoints
2025-03-06 19:20:19,013 - INFO - training batch 701, loss: 0.496, 22432/60000 datapoints
2025-03-06 19:20:19,209 - INFO - training batch 751, loss: 0.523, 24032/60000 datapoints
2025-03-06 19:20:19,402 - INFO - training batch 801, loss: 0.155, 25632/60000 datapoints
2025-03-06 19:20:19,597 - INFO - training batch 851, loss: 0.410, 27232/60000 datapoints
2025-03-06 19:20:19,797 - INFO - training batch 901, loss: 0.749, 28832/60000 datapoints
2025-03-06 19:20:20,000 - INFO - training batch 951, loss: 0.379, 30432/60000 datapoints
2025-03-06 19:20:20,198 - INFO - training batch 1001, loss: 0.184, 32032/60000 datapoints
2025-03-06 19:20:20,394 - INFO - training batch 1051, loss: 0.355, 33632/60000 datapoints
2025-03-06 19:20:20,589 - INFO - training batch 1101, loss: 0.460, 35232/60000 datapoints
2025-03-06 19:20:20,793 - INFO - training batch 1151, loss: 0.253, 36832/60000 datapoints
2025-03-06 19:20:20,995 - INFO - training batch 1201, loss: 0.175, 38432/60000 datapoints
2025-03-06 19:20:21,197 - INFO - training batch 1251, loss: 0.217, 40032/60000 datapoints
2025-03-06 19:20:21,412 - INFO - training batch 1301, loss: 0.214, 41632/60000 datapoints
2025-03-06 19:20:21,607 - INFO - training batch 1351, loss: 0.248, 43232/60000 datapoints
2025-03-06 19:20:21,806 - INFO - training batch 1401, loss: 0.255, 44832/60000 datapoints
2025-03-06 19:20:22,014 - INFO - training batch 1451, loss: 0.343, 46432/60000 datapoints
2025-03-06 19:20:22,221 - INFO - training batch 1501, loss: 0.216, 48032/60000 datapoints
2025-03-06 19:20:22,420 - INFO - training batch 1551, loss: 0.488, 49632/60000 datapoints
2025-03-06 19:20:22,623 - INFO - training batch 1601, loss: 0.285, 51232/60000 datapoints
2025-03-06 19:20:22,826 - INFO - training batch 1651, loss: 0.146, 52832/60000 datapoints
2025-03-06 19:20:23,029 - INFO - training batch 1701, loss: 0.190, 54432/60000 datapoints
2025-03-06 19:20:23,229 - INFO - training batch 1751, loss: 0.527, 56032/60000 datapoints
2025-03-06 19:20:23,426 - INFO - training batch 1801, loss: 0.271, 57632/60000 datapoints
2025-03-06 19:20:23,631 - INFO - training batch 1851, loss: 0.169, 59232/60000 datapoints
2025-03-06 19:20:23,733 - INFO - validation batch 1, loss: 0.298, 32/10016 datapoints
2025-03-06 19:20:23,896 - INFO - validation batch 51, loss: 0.430, 1632/10016 datapoints
2025-03-06 19:20:24,054 - INFO - validation batch 101, loss: 0.702, 3232/10016 datapoints
2025-03-06 19:20:24,225 - INFO - validation batch 151, loss: 0.407, 4832/10016 datapoints
2025-03-06 19:20:24,386 - INFO - validation batch 201, loss: 0.241, 6432/10016 datapoints
2025-03-06 19:20:24,543 - INFO - validation batch 251, loss: 0.154, 8032/10016 datapoints
2025-03-06 19:20:24,700 - INFO - validation batch 301, loss: 0.312, 9632/10016 datapoints
2025-03-06 19:20:24,738 - INFO - Epoch 282/800 done.
2025-03-06 19:20:24,739 - INFO - Final validation performance:
Loss: 0.364, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 19:20:24,739 - INFO - Beginning epoch 283/800
2025-03-06 19:20:24,746 - INFO - training batch 1, loss: 0.181, 32/60000 datapoints
2025-03-06 19:20:24,987 - INFO - training batch 51, loss: 0.455, 1632/60000 datapoints
2025-03-06 19:20:25,190 - INFO - training batch 101, loss: 0.342, 3232/60000 datapoints
2025-03-06 19:20:25,406 - INFO - training batch 151, loss: 0.467, 4832/60000 datapoints
2025-03-06 19:20:25,619 - INFO - training batch 201, loss: 0.383, 6432/60000 datapoints
2025-03-06 19:20:25,816 - INFO - training batch 251, loss: 0.219, 8032/60000 datapoints
2025-03-06 19:20:26,014 - INFO - training batch 301, loss: 0.251, 9632/60000 datapoints
2025-03-06 19:20:26,214 - INFO - training batch 351, loss: 0.161, 11232/60000 datapoints
2025-03-06 19:20:26,422 - INFO - training batch 401, loss: 0.177, 12832/60000 datapoints
2025-03-06 19:20:26,623 - INFO - training batch 451, loss: 0.234, 14432/60000 datapoints
2025-03-06 19:20:26,823 - INFO - training batch 501, loss: 0.604, 16032/60000 datapoints
2025-03-06 19:20:27,027 - INFO - training batch 551, loss: 0.514, 17632/60000 datapoints
2025-03-06 19:20:27,225 - INFO - training batch 601, loss: 0.511, 19232/60000 datapoints
2025-03-06 19:20:27,425 - INFO - training batch 651, loss: 0.245, 20832/60000 datapoints
2025-03-06 19:20:27,629 - INFO - training batch 701, loss: 0.192, 22432/60000 datapoints
2025-03-06 19:20:27,830 - INFO - training batch 751, loss: 0.514, 24032/60000 datapoints
2025-03-06 19:20:28,037 - INFO - training batch 801, loss: 0.314, 25632/60000 datapoints
2025-03-06 19:20:28,239 - INFO - training batch 851, loss: 0.562, 27232/60000 datapoints
2025-03-06 19:20:28,438 - INFO - training batch 901, loss: 0.321, 28832/60000 datapoints
2025-03-06 19:20:28,638 - INFO - training batch 951, loss: 0.355, 30432/60000 datapoints
2025-03-06 19:20:28,832 - INFO - training batch 1001, loss: 0.397, 32032/60000 datapoints
2025-03-06 19:20:29,043 - INFO - training batch 1051, loss: 0.491, 33632/60000 datapoints
2025-03-06 19:20:29,243 - INFO - training batch 1101, loss: 0.106, 35232/60000 datapoints
2025-03-06 19:20:29,441 - INFO - training batch 1151, loss: 0.438, 36832/60000 datapoints
2025-03-06 19:20:29,643 - INFO - training batch 1201, loss: 0.498, 38432/60000 datapoints
2025-03-06 19:20:29,842 - INFO - training batch 1251, loss: 0.257, 40032/60000 datapoints
2025-03-06 19:20:30,045 - INFO - training batch 1301, loss: 0.206, 41632/60000 datapoints
2025-03-06 19:20:30,253 - INFO - training batch 1351, loss: 0.292, 43232/60000 datapoints
2025-03-06 19:20:30,468 - INFO - training batch 1401, loss: 0.324, 44832/60000 datapoints
2025-03-06 19:20:30,673 - INFO - training batch 1451, loss: 0.400, 46432/60000 datapoints
2025-03-06 19:20:30,876 - INFO - training batch 1501, loss: 0.164, 48032/60000 datapoints
2025-03-06 19:20:31,095 - INFO - training batch 1551, loss: 0.321, 49632/60000 datapoints
2025-03-06 19:20:31,315 - INFO - training batch 1601, loss: 0.409, 51232/60000 datapoints
2025-03-06 19:20:31,534 - INFO - training batch 1651, loss: 0.121, 52832/60000 datapoints
2025-03-06 19:20:31,760 - INFO - training batch 1701, loss: 0.341, 54432/60000 datapoints
2025-03-06 19:20:31,968 - INFO - training batch 1751, loss: 0.248, 56032/60000 datapoints
2025-03-06 19:20:32,177 - INFO - training batch 1801, loss: 0.451, 57632/60000 datapoints
2025-03-06 19:20:32,386 - INFO - training batch 1851, loss: 0.320, 59232/60000 datapoints
2025-03-06 19:20:32,494 - INFO - validation batch 1, loss: 0.258, 32/10016 datapoints
2025-03-06 19:20:32,662 - INFO - validation batch 51, loss: 0.155, 1632/10016 datapoints
2025-03-06 19:20:32,828 - INFO - validation batch 101, loss: 0.203, 3232/10016 datapoints
2025-03-06 19:20:32,991 - INFO - validation batch 151, loss: 0.128, 4832/10016 datapoints
2025-03-06 19:20:33,153 - INFO - validation batch 201, loss: 0.420, 6432/10016 datapoints
2025-03-06 19:20:33,318 - INFO - validation batch 251, loss: 0.178, 8032/10016 datapoints
2025-03-06 19:20:33,482 - INFO - validation batch 301, loss: 0.202, 9632/10016 datapoints
2025-03-06 19:20:33,524 - INFO - Epoch 283/800 done.
2025-03-06 19:20:33,524 - INFO - Final validation performance:
Loss: 0.221, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 19:20:33,524 - INFO - Beginning epoch 284/800
2025-03-06 19:20:33,533 - INFO - training batch 1, loss: 0.189, 32/60000 datapoints
2025-03-06 19:20:33,762 - INFO - training batch 51, loss: 0.462, 1632/60000 datapoints
2025-03-06 19:20:33,972 - INFO - training batch 101, loss: 0.439, 3232/60000 datapoints
2025-03-06 19:20:34,183 - INFO - training batch 151, loss: 0.557, 4832/60000 datapoints
2025-03-06 19:20:34,394 - INFO - training batch 201, loss: 0.435, 6432/60000 datapoints
2025-03-06 19:20:34,603 - INFO - training batch 251, loss: 0.430, 8032/60000 datapoints
2025-03-06 19:20:34,812 - INFO - training batch 301, loss: 0.410, 9632/60000 datapoints
2025-03-06 19:20:35,022 - INFO - training batch 351, loss: 0.289, 11232/60000 datapoints
2025-03-06 19:20:35,231 - INFO - training batch 401, loss: 0.147, 12832/60000 datapoints
2025-03-06 19:20:35,435 - INFO - training batch 451, loss: 0.389, 14432/60000 datapoints
2025-03-06 19:20:35,643 - INFO - training batch 501, loss: 0.384, 16032/60000 datapoints
2025-03-06 19:20:35,850 - INFO - training batch 551, loss: 0.121, 17632/60000 datapoints
2025-03-06 19:20:36,053 - INFO - training batch 601, loss: 0.447, 19232/60000 datapoints
2025-03-06 19:20:36,259 - INFO - training batch 651, loss: 0.209, 20832/60000 datapoints
2025-03-06 19:20:36,463 - INFO - training batch 701, loss: 0.302, 22432/60000 datapoints
2025-03-06 19:20:36,671 - INFO - training batch 751, loss: 0.335, 24032/60000 datapoints
2025-03-06 19:20:36,877 - INFO - training batch 801, loss: 0.432, 25632/60000 datapoints
2025-03-06 19:20:37,082 - INFO - training batch 851, loss: 0.402, 27232/60000 datapoints
2025-03-06 19:20:37,285 - INFO - training batch 901, loss: 0.322, 28832/60000 datapoints
2025-03-06 19:20:37,488 - INFO - training batch 951, loss: 0.464, 30432/60000 datapoints
2025-03-06 19:20:37,715 - INFO - training batch 1001, loss: 0.327, 32032/60000 datapoints
2025-03-06 19:20:37,920 - INFO - training batch 1051, loss: 0.299, 33632/60000 datapoints
2025-03-06 19:20:38,126 - INFO - training batch 1101, loss: 0.219, 35232/60000 datapoints
2025-03-06 19:20:38,336 - INFO - training batch 1151, loss: 0.522, 36832/60000 datapoints
2025-03-06 19:20:38,538 - INFO - training batch 1201, loss: 0.273, 38432/60000 datapoints
2025-03-06 19:20:38,743 - INFO - training batch 1251, loss: 0.300, 40032/60000 datapoints
2025-03-06 19:20:38,947 - INFO - training batch 1301, loss: 0.258, 41632/60000 datapoints
2025-03-06 19:20:39,168 - INFO - training batch 1351, loss: 0.526, 43232/60000 datapoints
2025-03-06 19:20:39,372 - INFO - training batch 1401, loss: 0.310, 44832/60000 datapoints
2025-03-06 19:20:39,573 - INFO - training batch 1451, loss: 0.231, 46432/60000 datapoints
2025-03-06 19:20:39,779 - INFO - training batch 1501, loss: 0.171, 48032/60000 datapoints
2025-03-06 19:20:39,988 - INFO - training batch 1551, loss: 0.481, 49632/60000 datapoints
2025-03-06 19:20:40,190 - INFO - training batch 1601, loss: 0.294, 51232/60000 datapoints
2025-03-06 19:20:40,392 - INFO - training batch 1651, loss: 0.278, 52832/60000 datapoints
2025-03-06 19:20:40,596 - INFO - training batch 1701, loss: 0.267, 54432/60000 datapoints
2025-03-06 19:20:40,803 - INFO - training batch 1751, loss: 0.375, 56032/60000 datapoints
2025-03-06 19:20:41,004 - INFO - training batch 1801, loss: 0.309, 57632/60000 datapoints
2025-03-06 19:20:41,206 - INFO - training batch 1851, loss: 0.278, 59232/60000 datapoints
2025-03-06 19:20:41,314 - INFO - validation batch 1, loss: 0.600, 32/10016 datapoints
2025-03-06 19:20:41,498 - INFO - validation batch 51, loss: 0.195, 1632/10016 datapoints
2025-03-06 19:20:41,666 - INFO - validation batch 101, loss: 0.175, 3232/10016 datapoints
2025-03-06 19:20:41,831 - INFO - validation batch 151, loss: 0.139, 4832/10016 datapoints
2025-03-06 19:20:41,998 - INFO - validation batch 201, loss: 0.436, 6432/10016 datapoints
2025-03-06 19:20:42,159 - INFO - validation batch 251, loss: 0.230, 8032/10016 datapoints
2025-03-06 19:20:42,327 - INFO - validation batch 301, loss: 0.343, 9632/10016 datapoints
2025-03-06 19:20:42,369 - INFO - Epoch 284/800 done.
2025-03-06 19:20:42,370 - INFO - Final validation performance:
Loss: 0.302, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 19:20:42,371 - INFO - Beginning epoch 285/800
2025-03-06 19:20:42,378 - INFO - training batch 1, loss: 0.191, 32/60000 datapoints
2025-03-06 19:20:42,602 - INFO - training batch 51, loss: 0.332, 1632/60000 datapoints
2025-03-06 19:20:42,814 - INFO - training batch 101, loss: 0.207, 3232/60000 datapoints
2025-03-06 19:20:43,028 - INFO - training batch 151, loss: 0.496, 4832/60000 datapoints
2025-03-06 19:20:43,237 - INFO - training batch 201, loss: 0.408, 6432/60000 datapoints
2025-03-06 19:20:43,446 - INFO - training batch 251, loss: 0.287, 8032/60000 datapoints
2025-03-06 19:20:43,654 - INFO - training batch 301, loss: 0.559, 9632/60000 datapoints
2025-03-06 19:20:43,859 - INFO - training batch 351, loss: 0.288, 11232/60000 datapoints
2025-03-06 19:20:44,064 - INFO - training batch 401, loss: 0.426, 12832/60000 datapoints
2025-03-06 19:20:44,273 - INFO - training batch 451, loss: 0.378, 14432/60000 datapoints
2025-03-06 19:20:44,477 - INFO - training batch 501, loss: 0.182, 16032/60000 datapoints
2025-03-06 19:20:44,680 - INFO - training batch 551, loss: 0.280, 17632/60000 datapoints
2025-03-06 19:20:44,884 - INFO - training batch 601, loss: 0.139, 19232/60000 datapoints
2025-03-06 19:20:45,093 - INFO - training batch 651, loss: 0.250, 20832/60000 datapoints
2025-03-06 19:20:45,298 - INFO - training batch 701, loss: 0.473, 22432/60000 datapoints
2025-03-06 19:20:45,502 - INFO - training batch 751, loss: 0.258, 24032/60000 datapoints
2025-03-06 19:20:45,710 - INFO - training batch 801, loss: 0.343, 25632/60000 datapoints
2025-03-06 19:20:45,912 - INFO - training batch 851, loss: 0.672, 27232/60000 datapoints
2025-03-06 19:20:46,114 - INFO - training batch 901, loss: 0.520, 28832/60000 datapoints
2025-03-06 19:20:46,322 - INFO - training batch 951, loss: 0.297, 30432/60000 datapoints
2025-03-06 19:20:46,522 - INFO - training batch 1001, loss: 0.250, 32032/60000 datapoints
2025-03-06 19:20:46,731 - INFO - training batch 1051, loss: 0.436, 33632/60000 datapoints
2025-03-06 19:20:46,937 - INFO - training batch 1101, loss: 0.184, 35232/60000 datapoints
2025-03-06 19:20:47,139 - INFO - training batch 1151, loss: 0.367, 36832/60000 datapoints
2025-03-06 19:20:47,337 - INFO - training batch 1201, loss: 0.533, 38432/60000 datapoints
2025-03-06 19:20:47,537 - INFO - training batch 1251, loss: 0.308, 40032/60000 datapoints
2025-03-06 19:20:47,744 - INFO - training batch 1301, loss: 0.367, 41632/60000 datapoints
2025-03-06 19:20:47,948 - INFO - training batch 1351, loss: 0.238, 43232/60000 datapoints
2025-03-06 19:20:48,152 - INFO - training batch 1401, loss: 0.292, 44832/60000 datapoints
2025-03-06 19:20:48,382 - INFO - training batch 1451, loss: 0.326, 46432/60000 datapoints
2025-03-06 19:20:48,585 - INFO - training batch 1501, loss: 0.419, 48032/60000 datapoints
2025-03-06 19:20:48,792 - INFO - training batch 1551, loss: 0.535, 49632/60000 datapoints
2025-03-06 19:20:48,994 - INFO - training batch 1601, loss: 0.419, 51232/60000 datapoints
2025-03-06 19:20:49,204 - INFO - training batch 1651, loss: 0.501, 52832/60000 datapoints
2025-03-06 19:20:49,411 - INFO - training batch 1701, loss: 0.360, 54432/60000 datapoints
2025-03-06 19:20:49,628 - INFO - training batch 1751, loss: 0.542, 56032/60000 datapoints
2025-03-06 19:20:49,834 - INFO - training batch 1801, loss: 0.240, 57632/60000 datapoints
2025-03-06 19:20:50,042 - INFO - training batch 1851, loss: 0.131, 59232/60000 datapoints
2025-03-06 19:20:50,149 - INFO - validation batch 1, loss: 0.336, 32/10016 datapoints
2025-03-06 19:20:50,322 - INFO - validation batch 51, loss: 0.078, 1632/10016 datapoints
2025-03-06 19:20:50,488 - INFO - validation batch 101, loss: 0.348, 3232/10016 datapoints
2025-03-06 19:20:50,657 - INFO - validation batch 151, loss: 0.201, 4832/10016 datapoints
2025-03-06 19:20:50,823 - INFO - validation batch 201, loss: 0.197, 6432/10016 datapoints
2025-03-06 19:20:50,989 - INFO - validation batch 251, loss: 0.243, 8032/10016 datapoints
2025-03-06 19:20:51,151 - INFO - validation batch 301, loss: 0.146, 9632/10016 datapoints
2025-03-06 19:20:51,197 - INFO - Epoch 285/800 done.
2025-03-06 19:20:51,197 - INFO - Final validation performance:
Loss: 0.221, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 19:20:51,197 - INFO - Beginning epoch 286/800
2025-03-06 19:20:51,205 - INFO - training batch 1, loss: 0.503, 32/60000 datapoints
2025-03-06 19:20:51,420 - INFO - training batch 51, loss: 0.131, 1632/60000 datapoints
2025-03-06 19:20:51,648 - INFO - training batch 101, loss: 0.071, 3232/60000 datapoints
2025-03-06 19:20:51,875 - INFO - training batch 151, loss: 0.481, 4832/60000 datapoints
2025-03-06 19:20:52,088 - INFO - training batch 201, loss: 0.360, 6432/60000 datapoints
2025-03-06 19:20:52,315 - INFO - training batch 251, loss: 0.358, 8032/60000 datapoints
2025-03-06 19:20:52,527 - INFO - training batch 301, loss: 0.382, 9632/60000 datapoints
2025-03-06 19:20:52,736 - INFO - training batch 351, loss: 0.335, 11232/60000 datapoints
2025-03-06 19:20:52,942 - INFO - training batch 401, loss: 0.302, 12832/60000 datapoints
2025-03-06 19:20:53,143 - INFO - training batch 451, loss: 0.411, 14432/60000 datapoints
2025-03-06 19:20:53,349 - INFO - training batch 501, loss: 0.165, 16032/60000 datapoints
2025-03-06 19:20:53,556 - INFO - training batch 551, loss: 0.217, 17632/60000 datapoints
2025-03-06 19:20:53,763 - INFO - training batch 601, loss: 0.347, 19232/60000 datapoints
2025-03-06 19:20:53,985 - INFO - training batch 651, loss: 0.591, 20832/60000 datapoints
2025-03-06 19:20:54,196 - INFO - training batch 701, loss: 0.227, 22432/60000 datapoints
2025-03-06 19:20:54,404 - INFO - training batch 751, loss: 0.223, 24032/60000 datapoints
2025-03-06 19:20:54,604 - INFO - training batch 801, loss: 0.269, 25632/60000 datapoints
2025-03-06 19:20:54,818 - INFO - training batch 851, loss: 0.318, 27232/60000 datapoints
2025-03-06 19:20:55,022 - INFO - training batch 901, loss: 0.249, 28832/60000 datapoints
2025-03-06 19:20:55,225 - INFO - training batch 951, loss: 0.268, 30432/60000 datapoints
2025-03-06 19:20:55,428 - INFO - training batch 1001, loss: 0.211, 32032/60000 datapoints
2025-03-06 19:20:55,635 - INFO - training batch 1051, loss: 0.421, 33632/60000 datapoints
2025-03-06 19:20:55,838 - INFO - training batch 1101, loss: 0.312, 35232/60000 datapoints
2025-03-06 19:20:56,040 - INFO - training batch 1151, loss: 0.812, 36832/60000 datapoints
2025-03-06 19:20:56,248 - INFO - training batch 1201, loss: 0.308, 38432/60000 datapoints
2025-03-06 19:20:56,454 - INFO - training batch 1251, loss: 0.423, 40032/60000 datapoints
2025-03-06 19:20:56,660 - INFO - training batch 1301, loss: 0.517, 41632/60000 datapoints
2025-03-06 19:20:56,860 - INFO - training batch 1351, loss: 0.295, 43232/60000 datapoints
2025-03-06 19:20:57,065 - INFO - training batch 1401, loss: 0.529, 44832/60000 datapoints
2025-03-06 19:20:57,274 - INFO - training batch 1451, loss: 0.145, 46432/60000 datapoints
2025-03-06 19:20:57,479 - INFO - training batch 1501, loss: 0.638, 48032/60000 datapoints
2025-03-06 19:20:57,684 - INFO - training batch 1551, loss: 0.359, 49632/60000 datapoints
2025-03-06 19:20:57,884 - INFO - training batch 1601, loss: 0.397, 51232/60000 datapoints
2025-03-06 19:20:58,086 - INFO - training batch 1651, loss: 0.506, 52832/60000 datapoints
2025-03-06 19:20:58,287 - INFO - training batch 1701, loss: 0.283, 54432/60000 datapoints
2025-03-06 19:20:58,490 - INFO - training batch 1751, loss: 0.425, 56032/60000 datapoints
2025-03-06 19:20:58,708 - INFO - training batch 1801, loss: 0.616, 57632/60000 datapoints
2025-03-06 19:20:58,925 - INFO - training batch 1851, loss: 0.394, 59232/60000 datapoints
2025-03-06 19:20:59,038 - INFO - validation batch 1, loss: 0.226, 32/10016 datapoints
2025-03-06 19:20:59,200 - INFO - validation batch 51, loss: 0.175, 1632/10016 datapoints
2025-03-06 19:20:59,362 - INFO - validation batch 101, loss: 0.381, 3232/10016 datapoints
2025-03-06 19:20:59,523 - INFO - validation batch 151, loss: 0.355, 4832/10016 datapoints
2025-03-06 19:20:59,688 - INFO - validation batch 201, loss: 0.360, 6432/10016 datapoints
2025-03-06 19:20:59,847 - INFO - validation batch 251, loss: 0.432, 8032/10016 datapoints
2025-03-06 19:21:00,010 - INFO - validation batch 301, loss: 0.191, 9632/10016 datapoints
2025-03-06 19:21:00,048 - INFO - Epoch 286/800 done.
2025-03-06 19:21:00,049 - INFO - Final validation performance:
Loss: 0.303, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 19:21:00,049 - INFO - Beginning epoch 287/800
2025-03-06 19:21:00,057 - INFO - training batch 1, loss: 0.175, 32/60000 datapoints
2025-03-06 19:21:00,279 - INFO - training batch 51, loss: 0.252, 1632/60000 datapoints
2025-03-06 19:21:00,479 - INFO - training batch 101, loss: 0.165, 3232/60000 datapoints
2025-03-06 19:21:00,691 - INFO - training batch 151, loss: 0.273, 4832/60000 datapoints
2025-03-06 19:21:00,890 - INFO - training batch 201, loss: 0.154, 6432/60000 datapoints
2025-03-06 19:21:01,096 - INFO - training batch 251, loss: 0.484, 8032/60000 datapoints
2025-03-06 19:21:01,306 - INFO - training batch 301, loss: 0.299, 9632/60000 datapoints
2025-03-06 19:21:01,511 - INFO - training batch 351, loss: 0.195, 11232/60000 datapoints
2025-03-06 19:21:01,735 - INFO - training batch 401, loss: 0.298, 12832/60000 datapoints
2025-03-06 19:21:01,939 - INFO - training batch 451, loss: 0.404, 14432/60000 datapoints
2025-03-06 19:21:02,139 - INFO - training batch 501, loss: 0.168, 16032/60000 datapoints
2025-03-06 19:21:02,341 - INFO - training batch 551, loss: 0.351, 17632/60000 datapoints
2025-03-06 19:21:02,545 - INFO - training batch 601, loss: 0.403, 19232/60000 datapoints
2025-03-06 19:21:02,745 - INFO - training batch 651, loss: 0.445, 20832/60000 datapoints
2025-03-06 19:21:02,949 - INFO - training batch 701, loss: 0.173, 22432/60000 datapoints
2025-03-06 19:21:03,150 - INFO - training batch 751, loss: 0.215, 24032/60000 datapoints
2025-03-06 19:21:03,350 - INFO - training batch 801, loss: 0.609, 25632/60000 datapoints
2025-03-06 19:21:03,562 - INFO - training batch 851, loss: 0.774, 27232/60000 datapoints
2025-03-06 19:21:03,765 - INFO - training batch 901, loss: 0.133, 28832/60000 datapoints
2025-03-06 19:21:03,968 - INFO - training batch 951, loss: 0.107, 30432/60000 datapoints
2025-03-06 19:21:04,166 - INFO - training batch 1001, loss: 0.328, 32032/60000 datapoints
2025-03-06 19:21:04,374 - INFO - training batch 1051, loss: 0.185, 33632/60000 datapoints
2025-03-06 19:21:04,576 - INFO - training batch 1101, loss: 0.296, 35232/60000 datapoints
2025-03-06 19:21:04,790 - INFO - training batch 1151, loss: 0.256, 36832/60000 datapoints
2025-03-06 19:21:04,997 - INFO - training batch 1201, loss: 0.357, 38432/60000 datapoints
2025-03-06 19:21:05,199 - INFO - training batch 1251, loss: 0.365, 40032/60000 datapoints
2025-03-06 19:21:05,396 - INFO - training batch 1301, loss: 0.170, 41632/60000 datapoints
2025-03-06 19:21:05,593 - INFO - training batch 1351, loss: 0.214, 43232/60000 datapoints
2025-03-06 19:21:05,802 - INFO - training batch 1401, loss: 0.155, 44832/60000 datapoints
2025-03-06 19:21:06,002 - INFO - training batch 1451, loss: 0.209, 46432/60000 datapoints
2025-03-06 19:21:06,201 - INFO - training batch 1501, loss: 0.434, 48032/60000 datapoints
2025-03-06 19:21:06,406 - INFO - training batch 1551, loss: 0.477, 49632/60000 datapoints
2025-03-06 19:21:06,606 - INFO - training batch 1601, loss: 0.094, 51232/60000 datapoints
2025-03-06 19:21:06,808 - INFO - training batch 1651, loss: 0.264, 52832/60000 datapoints
2025-03-06 19:21:07,008 - INFO - training batch 1701, loss: 0.483, 54432/60000 datapoints
2025-03-06 19:21:07,215 - INFO - training batch 1751, loss: 0.754, 56032/60000 datapoints
2025-03-06 19:21:07,416 - INFO - training batch 1801, loss: 0.276, 57632/60000 datapoints
2025-03-06 19:21:07,618 - INFO - training batch 1851, loss: 0.470, 59232/60000 datapoints
2025-03-06 19:21:07,724 - INFO - validation batch 1, loss: 0.191, 32/10016 datapoints
2025-03-06 19:21:07,886 - INFO - validation batch 51, loss: 0.251, 1632/10016 datapoints
2025-03-06 19:21:08,055 - INFO - validation batch 101, loss: 0.238, 3232/10016 datapoints
2025-03-06 19:21:08,249 - INFO - validation batch 151, loss: 0.473, 4832/10016 datapoints
2025-03-06 19:21:08,421 - INFO - validation batch 201, loss: 0.272, 6432/10016 datapoints
2025-03-06 19:21:08,584 - INFO - validation batch 251, loss: 0.364, 8032/10016 datapoints
2025-03-06 19:21:08,750 - INFO - validation batch 301, loss: 0.198, 9632/10016 datapoints
2025-03-06 19:21:08,790 - INFO - Epoch 287/800 done.
2025-03-06 19:21:08,790 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.911top-5 acc: 0.911
2025-03-06 19:21:08,791 - INFO - Beginning epoch 288/800
2025-03-06 19:21:08,797 - INFO - training batch 1, loss: 0.312, 32/60000 datapoints
2025-03-06 19:21:09,029 - INFO - training batch 51, loss: 0.457, 1632/60000 datapoints
2025-03-06 19:21:09,231 - INFO - training batch 101, loss: 0.515, 3232/60000 datapoints
2025-03-06 19:21:09,439 - INFO - training batch 151, loss: 0.464, 4832/60000 datapoints
2025-03-06 19:21:09,648 - INFO - training batch 201, loss: 0.572, 6432/60000 datapoints
2025-03-06 19:21:09,854 - INFO - training batch 251, loss: 0.347, 8032/60000 datapoints
2025-03-06 19:21:10,060 - INFO - training batch 301, loss: 0.533, 9632/60000 datapoints
2025-03-06 19:21:10,266 - INFO - training batch 351, loss: 0.244, 11232/60000 datapoints
2025-03-06 19:21:10,472 - INFO - training batch 401, loss: 0.352, 12832/60000 datapoints
2025-03-06 19:21:10,688 - INFO - training batch 451, loss: 0.543, 14432/60000 datapoints
2025-03-06 19:21:10,892 - INFO - training batch 501, loss: 0.197, 16032/60000 datapoints
2025-03-06 19:21:11,096 - INFO - training batch 551, loss: 0.319, 17632/60000 datapoints
2025-03-06 19:21:11,304 - INFO - training batch 601, loss: 0.140, 19232/60000 datapoints
2025-03-06 19:21:11,512 - INFO - training batch 651, loss: 0.234, 20832/60000 datapoints
2025-03-06 19:21:11,720 - INFO - training batch 701, loss: 0.176, 22432/60000 datapoints
2025-03-06 19:21:11,931 - INFO - training batch 751, loss: 0.285, 24032/60000 datapoints
2025-03-06 19:21:12,134 - INFO - training batch 801, loss: 0.360, 25632/60000 datapoints
2025-03-06 19:21:12,337 - INFO - training batch 851, loss: 0.222, 27232/60000 datapoints
2025-03-06 19:21:12,541 - INFO - training batch 901, loss: 0.220, 28832/60000 datapoints
2025-03-06 19:21:12,742 - INFO - training batch 951, loss: 0.171, 30432/60000 datapoints
2025-03-06 19:21:12,943 - INFO - training batch 1001, loss: 0.485, 32032/60000 datapoints
2025-03-06 19:21:13,143 - INFO - training batch 1051, loss: 0.389, 33632/60000 datapoints
2025-03-06 19:21:13,347 - INFO - training batch 1101, loss: 0.352, 35232/60000 datapoints
2025-03-06 19:21:13,545 - INFO - training batch 1151, loss: 0.386, 36832/60000 datapoints
2025-03-06 19:21:13,749 - INFO - training batch 1201, loss: 0.322, 38432/60000 datapoints
2025-03-06 19:21:13,954 - INFO - training batch 1251, loss: 0.224, 40032/60000 datapoints
2025-03-06 19:21:14,154 - INFO - training batch 1301, loss: 0.301, 41632/60000 datapoints
2025-03-06 19:21:14,353 - INFO - training batch 1351, loss: 0.288, 43232/60000 datapoints
2025-03-06 19:21:14,558 - INFO - training batch 1401, loss: 0.259, 44832/60000 datapoints
2025-03-06 19:21:14,759 - INFO - training batch 1451, loss: 0.309, 46432/60000 datapoints
2025-03-06 19:21:14,966 - INFO - training batch 1501, loss: 0.264, 48032/60000 datapoints
2025-03-06 19:21:15,167 - INFO - training batch 1551, loss: 0.173, 49632/60000 datapoints
2025-03-06 19:21:15,366 - INFO - training batch 1601, loss: 0.246, 51232/60000 datapoints
2025-03-06 19:21:15,566 - INFO - training batch 1651, loss: 0.221, 52832/60000 datapoints
2025-03-06 19:21:15,768 - INFO - training batch 1701, loss: 0.157, 54432/60000 datapoints
2025-03-06 19:21:15,966 - INFO - training batch 1751, loss: 0.338, 56032/60000 datapoints
2025-03-06 19:21:16,166 - INFO - training batch 1801, loss: 0.387, 57632/60000 datapoints
2025-03-06 19:21:16,367 - INFO - training batch 1851, loss: 0.678, 59232/60000 datapoints
2025-03-06 19:21:16,474 - INFO - validation batch 1, loss: 0.151, 32/10016 datapoints
2025-03-06 19:21:16,637 - INFO - validation batch 51, loss: 0.205, 1632/10016 datapoints
2025-03-06 19:21:16,792 - INFO - validation batch 101, loss: 0.279, 3232/10016 datapoints
2025-03-06 19:21:16,949 - INFO - validation batch 151, loss: 0.265, 4832/10016 datapoints
2025-03-06 19:21:17,112 - INFO - validation batch 201, loss: 0.332, 6432/10016 datapoints
2025-03-06 19:21:17,273 - INFO - validation batch 251, loss: 0.479, 8032/10016 datapoints
2025-03-06 19:21:17,435 - INFO - validation batch 301, loss: 0.190, 9632/10016 datapoints
2025-03-06 19:21:17,475 - INFO - Epoch 288/800 done.
2025-03-06 19:21:17,476 - INFO - Final validation performance:
Loss: 0.272, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 19:21:17,476 - INFO - Beginning epoch 289/800
2025-03-06 19:21:17,482 - INFO - training batch 1, loss: 0.316, 32/60000 datapoints
2025-03-06 19:21:17,688 - INFO - training batch 51, loss: 0.341, 1632/60000 datapoints
2025-03-06 19:21:17,948 - INFO - training batch 101, loss: 0.327, 3232/60000 datapoints
2025-03-06 19:21:18,180 - INFO - training batch 151, loss: 0.361, 4832/60000 datapoints
2025-03-06 19:21:18,394 - INFO - training batch 201, loss: 0.483, 6432/60000 datapoints
2025-03-06 19:21:18,599 - INFO - training batch 251, loss: 0.126, 8032/60000 datapoints
2025-03-06 19:21:18,804 - INFO - training batch 301, loss: 0.191, 9632/60000 datapoints
2025-03-06 19:21:19,006 - INFO - training batch 351, loss: 0.243, 11232/60000 datapoints
2025-03-06 19:21:19,209 - INFO - training batch 401, loss: 0.330, 12832/60000 datapoints
2025-03-06 19:21:19,408 - INFO - training batch 451, loss: 0.298, 14432/60000 datapoints
2025-03-06 19:21:19,609 - INFO - training batch 501, loss: 0.183, 16032/60000 datapoints
2025-03-06 19:21:19,810 - INFO - training batch 551, loss: 0.474, 17632/60000 datapoints
2025-03-06 19:21:20,017 - INFO - training batch 601, loss: 0.299, 19232/60000 datapoints
2025-03-06 19:21:20,217 - INFO - training batch 651, loss: 0.286, 20832/60000 datapoints
2025-03-06 19:21:20,421 - INFO - training batch 701, loss: 0.426, 22432/60000 datapoints
2025-03-06 19:21:20,626 - INFO - training batch 751, loss: 0.205, 24032/60000 datapoints
2025-03-06 19:21:20,829 - INFO - training batch 801, loss: 0.214, 25632/60000 datapoints
2025-03-06 19:21:21,032 - INFO - training batch 851, loss: 0.253, 27232/60000 datapoints
2025-03-06 19:21:21,236 - INFO - training batch 901, loss: 0.216, 28832/60000 datapoints
2025-03-06 19:21:21,443 - INFO - training batch 951, loss: 0.202, 30432/60000 datapoints
2025-03-06 19:21:21,649 - INFO - training batch 1001, loss: 0.211, 32032/60000 datapoints
2025-03-06 19:21:21,876 - INFO - training batch 1051, loss: 0.123, 33632/60000 datapoints
2025-03-06 19:21:22,081 - INFO - training batch 1101, loss: 0.129, 35232/60000 datapoints
2025-03-06 19:21:22,286 - INFO - training batch 1151, loss: 0.372, 36832/60000 datapoints
2025-03-06 19:21:22,492 - INFO - training batch 1201, loss: 0.223, 38432/60000 datapoints
2025-03-06 19:21:22,700 - INFO - training batch 1251, loss: 0.172, 40032/60000 datapoints
2025-03-06 19:21:22,900 - INFO - training batch 1301, loss: 0.338, 41632/60000 datapoints
2025-03-06 19:21:23,108 - INFO - training batch 1351, loss: 0.228, 43232/60000 datapoints
2025-03-06 19:21:23,310 - INFO - training batch 1401, loss: 0.093, 44832/60000 datapoints
2025-03-06 19:21:23,507 - INFO - training batch 1451, loss: 0.476, 46432/60000 datapoints
2025-03-06 19:21:23,709 - INFO - training batch 1501, loss: 0.174, 48032/60000 datapoints
2025-03-06 19:21:23,912 - INFO - training batch 1551, loss: 0.420, 49632/60000 datapoints
2025-03-06 19:21:24,116 - INFO - training batch 1601, loss: 0.321, 51232/60000 datapoints
2025-03-06 19:21:24,317 - INFO - training batch 1651, loss: 0.222, 52832/60000 datapoints
2025-03-06 19:21:24,518 - INFO - training batch 1701, loss: 0.182, 54432/60000 datapoints
2025-03-06 19:21:24,724 - INFO - training batch 1751, loss: 0.323, 56032/60000 datapoints
2025-03-06 19:21:24,931 - INFO - training batch 1801, loss: 0.407, 57632/60000 datapoints
2025-03-06 19:21:25,149 - INFO - training batch 1851, loss: 0.208, 59232/60000 datapoints
2025-03-06 19:21:25,262 - INFO - validation batch 1, loss: 0.101, 32/10016 datapoints
2025-03-06 19:21:25,420 - INFO - validation batch 51, loss: 0.190, 1632/10016 datapoints
2025-03-06 19:21:25,581 - INFO - validation batch 101, loss: 0.487, 3232/10016 datapoints
2025-03-06 19:21:25,741 - INFO - validation batch 151, loss: 0.341, 4832/10016 datapoints
2025-03-06 19:21:25,901 - INFO - validation batch 201, loss: 0.384, 6432/10016 datapoints
2025-03-06 19:21:26,058 - INFO - validation batch 251, loss: 0.122, 8032/10016 datapoints
2025-03-06 19:21:26,224 - INFO - validation batch 301, loss: 0.125, 9632/10016 datapoints
2025-03-06 19:21:26,263 - INFO - Epoch 289/800 done.
2025-03-06 19:21:26,263 - INFO - Final validation performance:
Loss: 0.250, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 19:21:26,263 - INFO - Beginning epoch 290/800
2025-03-06 19:21:26,271 - INFO - training batch 1, loss: 0.520, 32/60000 datapoints
2025-03-06 19:21:26,473 - INFO - training batch 51, loss: 0.580, 1632/60000 datapoints
2025-03-06 19:21:26,685 - INFO - training batch 101, loss: 0.447, 3232/60000 datapoints
2025-03-06 19:21:26,884 - INFO - training batch 151, loss: 0.420, 4832/60000 datapoints
2025-03-06 19:21:27,078 - INFO - training batch 201, loss: 0.128, 6432/60000 datapoints
2025-03-06 19:21:27,288 - INFO - training batch 251, loss: 0.283, 8032/60000 datapoints
2025-03-06 19:21:27,506 - INFO - training batch 301, loss: 0.341, 9632/60000 datapoints
2025-03-06 19:21:27,720 - INFO - training batch 351, loss: 0.187, 11232/60000 datapoints
2025-03-06 19:21:27,919 - INFO - training batch 401, loss: 0.179, 12832/60000 datapoints
2025-03-06 19:21:28,121 - INFO - training batch 451, loss: 0.284, 14432/60000 datapoints
2025-03-06 19:21:28,319 - INFO - training batch 501, loss: 0.394, 16032/60000 datapoints
2025-03-06 19:21:28,519 - INFO - training batch 551, loss: 0.285, 17632/60000 datapoints
2025-03-06 19:21:28,724 - INFO - training batch 601, loss: 0.592, 19232/60000 datapoints
2025-03-06 19:21:28,926 - INFO - training batch 651, loss: 0.193, 20832/60000 datapoints
2025-03-06 19:21:29,127 - INFO - training batch 701, loss: 0.736, 22432/60000 datapoints
2025-03-06 19:21:29,326 - INFO - training batch 751, loss: 0.442, 24032/60000 datapoints
2025-03-06 19:21:29,526 - INFO - training batch 801, loss: 0.453, 25632/60000 datapoints
2025-03-06 19:21:29,730 - INFO - training batch 851, loss: 0.525, 27232/60000 datapoints
2025-03-06 19:21:29,928 - INFO - training batch 901, loss: 0.186, 28832/60000 datapoints
2025-03-06 19:21:30,128 - INFO - training batch 951, loss: 0.393, 30432/60000 datapoints
2025-03-06 19:21:30,326 - INFO - training batch 1001, loss: 0.518, 32032/60000 datapoints
2025-03-06 19:21:30,525 - INFO - training batch 1051, loss: 0.152, 33632/60000 datapoints
2025-03-06 19:21:30,722 - INFO - training batch 1101, loss: 0.230, 35232/60000 datapoints
2025-03-06 19:21:30,924 - INFO - training batch 1151, loss: 0.430, 36832/60000 datapoints
2025-03-06 19:21:31,123 - INFO - training batch 1201, loss: 0.159, 38432/60000 datapoints
2025-03-06 19:21:31,323 - INFO - training batch 1251, loss: 0.369, 40032/60000 datapoints
2025-03-06 19:21:31,521 - INFO - training batch 1301, loss: 0.518, 41632/60000 datapoints
2025-03-06 19:21:31,728 - INFO - training batch 1351, loss: 0.362, 43232/60000 datapoints
2025-03-06 19:21:31,939 - INFO - training batch 1401, loss: 0.315, 44832/60000 datapoints
2025-03-06 19:21:32,145 - INFO - training batch 1451, loss: 0.463, 46432/60000 datapoints
2025-03-06 19:21:32,352 - INFO - training batch 1501, loss: 0.310, 48032/60000 datapoints
2025-03-06 19:21:32,560 - INFO - training batch 1551, loss: 0.390, 49632/60000 datapoints
2025-03-06 19:21:32,762 - INFO - training batch 1601, loss: 0.188, 51232/60000 datapoints
2025-03-06 19:21:32,976 - INFO - training batch 1651, loss: 0.276, 52832/60000 datapoints
2025-03-06 19:21:33,188 - INFO - training batch 1701, loss: 0.313, 54432/60000 datapoints
2025-03-06 19:21:33,390 - INFO - training batch 1751, loss: 0.318, 56032/60000 datapoints
2025-03-06 19:21:33,587 - INFO - training batch 1801, loss: 0.287, 57632/60000 datapoints
2025-03-06 19:21:33,788 - INFO - training batch 1851, loss: 0.237, 59232/60000 datapoints
2025-03-06 19:21:33,893 - INFO - validation batch 1, loss: 0.309, 32/10016 datapoints
2025-03-06 19:21:34,052 - INFO - validation batch 51, loss: 0.248, 1632/10016 datapoints
2025-03-06 19:21:34,211 - INFO - validation batch 101, loss: 0.219, 3232/10016 datapoints
2025-03-06 19:21:34,372 - INFO - validation batch 151, loss: 0.453, 4832/10016 datapoints
2025-03-06 19:21:34,532 - INFO - validation batch 201, loss: 0.332, 6432/10016 datapoints
2025-03-06 19:21:34,692 - INFO - validation batch 251, loss: 0.366, 8032/10016 datapoints
2025-03-06 19:21:34,852 - INFO - validation batch 301, loss: 0.260, 9632/10016 datapoints
2025-03-06 19:21:34,895 - INFO - Epoch 290/800 done.
2025-03-06 19:21:34,895 - INFO - Final validation performance:
Loss: 0.312, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 19:21:34,896 - INFO - Beginning epoch 291/800
2025-03-06 19:21:34,905 - INFO - training batch 1, loss: 0.169, 32/60000 datapoints
2025-03-06 19:21:35,121 - INFO - training batch 51, loss: 0.301, 1632/60000 datapoints
2025-03-06 19:21:35,520 - INFO - training batch 101, loss: 0.307, 3232/60000 datapoints
2025-03-06 19:21:35,729 - INFO - training batch 151, loss: 0.067, 4832/60000 datapoints
2025-03-06 19:21:35,929 - INFO - training batch 201, loss: 0.275, 6432/60000 datapoints
2025-03-06 19:21:36,127 - INFO - training batch 251, loss: 0.229, 8032/60000 datapoints
2025-03-06 19:21:36,328 - INFO - training batch 301, loss: 0.369, 9632/60000 datapoints
2025-03-06 19:21:36,531 - INFO - training batch 351, loss: 0.301, 11232/60000 datapoints
2025-03-06 19:21:36,730 - INFO - training batch 401, loss: 0.180, 12832/60000 datapoints
2025-03-06 19:21:36,932 - INFO - training batch 451, loss: 0.215, 14432/60000 datapoints
2025-03-06 19:21:37,132 - INFO - training batch 501, loss: 0.176, 16032/60000 datapoints
2025-03-06 19:21:37,334 - INFO - training batch 551, loss: 0.779, 17632/60000 datapoints
2025-03-06 19:21:37,534 - INFO - training batch 601, loss: 0.455, 19232/60000 datapoints
2025-03-06 19:21:37,757 - INFO - training batch 651, loss: 0.106, 20832/60000 datapoints
2025-03-06 19:21:37,956 - INFO - training batch 701, loss: 0.563, 22432/60000 datapoints
2025-03-06 19:21:38,159 - INFO - training batch 751, loss: 0.308, 24032/60000 datapoints
2025-03-06 19:21:38,356 - INFO - training batch 801, loss: 0.593, 25632/60000 datapoints
2025-03-06 19:21:38,558 - INFO - training batch 851, loss: 0.519, 27232/60000 datapoints
2025-03-06 19:21:38,760 - INFO - training batch 901, loss: 0.190, 28832/60000 datapoints
2025-03-06 19:21:38,963 - INFO - training batch 951, loss: 0.389, 30432/60000 datapoints
2025-03-06 19:21:39,178 - INFO - training batch 1001, loss: 0.331, 32032/60000 datapoints
2025-03-06 19:21:39,375 - INFO - training batch 1051, loss: 0.309, 33632/60000 datapoints
2025-03-06 19:21:39,576 - INFO - training batch 1101, loss: 0.242, 35232/60000 datapoints
2025-03-06 19:21:39,776 - INFO - training batch 1151, loss: 0.351, 36832/60000 datapoints
2025-03-06 19:21:39,980 - INFO - training batch 1201, loss: 0.378, 38432/60000 datapoints
2025-03-06 19:21:40,186 - INFO - training batch 1251, loss: 0.558, 40032/60000 datapoints
2025-03-06 19:21:40,721 - INFO - training batch 1301, loss: 0.106, 41632/60000 datapoints
2025-03-06 19:21:40,922 - INFO - training batch 1351, loss: 0.364, 43232/60000 datapoints
2025-03-06 19:21:41,125 - INFO - training batch 1401, loss: 0.286, 44832/60000 datapoints
2025-03-06 19:21:41,326 - INFO - training batch 1451, loss: 0.298, 46432/60000 datapoints
2025-03-06 19:21:41,528 - INFO - training batch 1501, loss: 0.370, 48032/60000 datapoints
2025-03-06 19:21:41,732 - INFO - training batch 1551, loss: 0.117, 49632/60000 datapoints
2025-03-06 19:21:41,933 - INFO - training batch 1601, loss: 0.186, 51232/60000 datapoints
2025-03-06 19:21:42,165 - INFO - training batch 1651, loss: 0.535, 52832/60000 datapoints
2025-03-06 19:21:42,367 - INFO - training batch 1701, loss: 0.362, 54432/60000 datapoints
2025-03-06 19:21:42,578 - INFO - training batch 1751, loss: 0.364, 56032/60000 datapoints
2025-03-06 19:21:42,783 - INFO - training batch 1801, loss: 0.344, 57632/60000 datapoints
2025-03-06 19:21:42,990 - INFO - training batch 1851, loss: 0.148, 59232/60000 datapoints
2025-03-06 19:21:43,119 - INFO - validation batch 1, loss: 0.151, 32/10016 datapoints
2025-03-06 19:21:43,282 - INFO - validation batch 51, loss: 0.363, 1632/10016 datapoints
2025-03-06 19:21:43,450 - INFO - validation batch 101, loss: 0.174, 3232/10016 datapoints
2025-03-06 19:21:43,617 - INFO - validation batch 151, loss: 0.428, 4832/10016 datapoints
2025-03-06 19:21:43,778 - INFO - validation batch 201, loss: 0.299, 6432/10016 datapoints
2025-03-06 19:21:43,941 - INFO - validation batch 251, loss: 0.631, 8032/10016 datapoints
2025-03-06 19:21:44,106 - INFO - validation batch 301, loss: 0.237, 9632/10016 datapoints
2025-03-06 19:21:44,148 - INFO - Epoch 291/800 done.
2025-03-06 19:21:44,148 - INFO - Final validation performance:
Loss: 0.326, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 19:21:44,149 - INFO - Beginning epoch 292/800
2025-03-06 19:21:44,156 - INFO - training batch 1, loss: 0.113, 32/60000 datapoints
2025-03-06 19:21:44,387 - INFO - training batch 51, loss: 0.150, 1632/60000 datapoints
2025-03-06 19:21:44,598 - INFO - training batch 101, loss: 0.265, 3232/60000 datapoints
2025-03-06 19:21:44,813 - INFO - training batch 151, loss: 0.451, 4832/60000 datapoints
2025-03-06 19:21:45,026 - INFO - training batch 201, loss: 0.136, 6432/60000 datapoints
2025-03-06 19:21:45,234 - INFO - training batch 251, loss: 0.288, 8032/60000 datapoints
2025-03-06 19:21:45,445 - INFO - training batch 301, loss: 0.437, 9632/60000 datapoints
2025-03-06 19:21:45,658 - INFO - training batch 351, loss: 0.247, 11232/60000 datapoints
2025-03-06 19:21:45,860 - INFO - training batch 401, loss: 0.201, 12832/60000 datapoints
2025-03-06 19:21:46,064 - INFO - training batch 451, loss: 0.221, 14432/60000 datapoints
2025-03-06 19:21:46,267 - INFO - training batch 501, loss: 0.371, 16032/60000 datapoints
2025-03-06 19:21:46,475 - INFO - training batch 551, loss: 0.327, 17632/60000 datapoints
2025-03-06 19:21:46,687 - INFO - training batch 601, loss: 0.242, 19232/60000 datapoints
2025-03-06 19:21:46,887 - INFO - training batch 651, loss: 0.720, 20832/60000 datapoints
2025-03-06 19:21:47,089 - INFO - training batch 701, loss: 0.259, 22432/60000 datapoints
2025-03-06 19:21:47,290 - INFO - training batch 751, loss: 0.264, 24032/60000 datapoints
2025-03-06 19:21:47,490 - INFO - training batch 801, loss: 0.534, 25632/60000 datapoints
2025-03-06 19:21:47,694 - INFO - training batch 851, loss: 0.240, 27232/60000 datapoints
2025-03-06 19:21:47,895 - INFO - training batch 901, loss: 0.601, 28832/60000 datapoints
2025-03-06 19:21:48,112 - INFO - training batch 951, loss: 0.135, 30432/60000 datapoints
2025-03-06 19:21:48,312 - INFO - training batch 1001, loss: 0.220, 32032/60000 datapoints
2025-03-06 19:21:48,517 - INFO - training batch 1051, loss: 0.214, 33632/60000 datapoints
2025-03-06 19:21:48,726 - INFO - training batch 1101, loss: 0.394, 35232/60000 datapoints
2025-03-06 19:21:48,928 - INFO - training batch 1151, loss: 0.337, 36832/60000 datapoints
2025-03-06 19:21:49,131 - INFO - training batch 1201, loss: 0.548, 38432/60000 datapoints
2025-03-06 19:21:49,336 - INFO - training batch 1251, loss: 0.386, 40032/60000 datapoints
2025-03-06 19:21:49,540 - INFO - training batch 1301, loss: 0.362, 41632/60000 datapoints
2025-03-06 19:21:49,749 - INFO - training batch 1351, loss: 0.246, 43232/60000 datapoints
2025-03-06 19:21:49,952 - INFO - training batch 1401, loss: 0.430, 44832/60000 datapoints
2025-03-06 19:21:50,155 - INFO - training batch 1451, loss: 0.442, 46432/60000 datapoints
2025-03-06 19:21:50,354 - INFO - training batch 1501, loss: 0.192, 48032/60000 datapoints
2025-03-06 19:21:50,560 - INFO - training batch 1551, loss: 0.178, 49632/60000 datapoints
2025-03-06 19:21:50,784 - INFO - training batch 1601, loss: 0.653, 51232/60000 datapoints
2025-03-06 19:21:50,985 - INFO - training batch 1651, loss: 0.382, 52832/60000 datapoints
2025-03-06 19:21:51,198 - INFO - training batch 1701, loss: 0.165, 54432/60000 datapoints
2025-03-06 19:21:51,443 - INFO - training batch 1751, loss: 0.594, 56032/60000 datapoints
2025-03-06 19:21:51,648 - INFO - training batch 1801, loss: 0.373, 57632/60000 datapoints
2025-03-06 19:21:51,850 - INFO - training batch 1851, loss: 0.513, 59232/60000 datapoints
2025-03-06 19:21:51,954 - INFO - validation batch 1, loss: 0.464, 32/10016 datapoints
2025-03-06 19:21:52,115 - INFO - validation batch 51, loss: 0.323, 1632/10016 datapoints
2025-03-06 19:21:52,289 - INFO - validation batch 101, loss: 0.392, 3232/10016 datapoints
2025-03-06 19:21:52,445 - INFO - validation batch 151, loss: 0.240, 4832/10016 datapoints
2025-03-06 19:21:52,606 - INFO - validation batch 201, loss: 0.256, 6432/10016 datapoints
2025-03-06 19:21:52,766 - INFO - validation batch 251, loss: 0.342, 8032/10016 datapoints
2025-03-06 19:21:52,921 - INFO - validation batch 301, loss: 0.456, 9632/10016 datapoints
2025-03-06 19:21:52,958 - INFO - Epoch 292/800 done.
2025-03-06 19:21:52,958 - INFO - Final validation performance:
Loss: 0.353, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 19:21:52,959 - INFO - Beginning epoch 293/800
2025-03-06 19:21:52,966 - INFO - training batch 1, loss: 0.269, 32/60000 datapoints
2025-03-06 19:21:53,170 - INFO - training batch 51, loss: 0.179, 1632/60000 datapoints
2025-03-06 19:21:53,365 - INFO - training batch 101, loss: 0.210, 3232/60000 datapoints
2025-03-06 19:21:53,579 - INFO - training batch 151, loss: 0.239, 4832/60000 datapoints
2025-03-06 19:21:53,776 - INFO - training batch 201, loss: 0.222, 6432/60000 datapoints
2025-03-06 19:21:53,974 - INFO - training batch 251, loss: 0.167, 8032/60000 datapoints
2025-03-06 19:21:54,178 - INFO - training batch 301, loss: 0.154, 9632/60000 datapoints
2025-03-06 19:21:54,375 - INFO - training batch 351, loss: 0.343, 11232/60000 datapoints
2025-03-06 19:21:54,582 - INFO - training batch 401, loss: 0.312, 12832/60000 datapoints
2025-03-06 19:21:54,781 - INFO - training batch 451, loss: 0.097, 14432/60000 datapoints
2025-03-06 19:21:54,981 - INFO - training batch 501, loss: 0.145, 16032/60000 datapoints
2025-03-06 19:21:55,178 - INFO - training batch 551, loss: 0.273, 17632/60000 datapoints
2025-03-06 19:21:55,372 - INFO - training batch 601, loss: 0.345, 19232/60000 datapoints
2025-03-06 19:21:55,565 - INFO - training batch 651, loss: 0.263, 20832/60000 datapoints
2025-03-06 19:21:55,766 - INFO - training batch 701, loss: 0.187, 22432/60000 datapoints
2025-03-06 19:21:55,961 - INFO - training batch 751, loss: 0.814, 24032/60000 datapoints
2025-03-06 19:21:56,160 - INFO - training batch 801, loss: 0.310, 25632/60000 datapoints
2025-03-06 19:21:56,356 - INFO - training batch 851, loss: 0.312, 27232/60000 datapoints
2025-03-06 19:21:56,551 - INFO - training batch 901, loss: 0.356, 28832/60000 datapoints
2025-03-06 19:21:56,752 - INFO - training batch 951, loss: 0.255, 30432/60000 datapoints
2025-03-06 19:21:56,948 - INFO - training batch 1001, loss: 0.376, 32032/60000 datapoints
2025-03-06 19:21:57,142 - INFO - training batch 1051, loss: 0.585, 33632/60000 datapoints
2025-03-06 19:21:57,335 - INFO - training batch 1101, loss: 0.131, 35232/60000 datapoints
2025-03-06 19:21:57,535 - INFO - training batch 1151, loss: 0.270, 36832/60000 datapoints
2025-03-06 19:21:57,737 - INFO - training batch 1201, loss: 0.290, 38432/60000 datapoints
2025-03-06 19:21:57,933 - INFO - training batch 1251, loss: 0.272, 40032/60000 datapoints
2025-03-06 19:21:58,131 - INFO - training batch 1301, loss: 0.413, 41632/60000 datapoints
2025-03-06 19:21:58,337 - INFO - training batch 1351, loss: 0.295, 43232/60000 datapoints
2025-03-06 19:21:58,541 - INFO - training batch 1401, loss: 0.227, 44832/60000 datapoints
2025-03-06 19:21:58,746 - INFO - training batch 1451, loss: 0.184, 46432/60000 datapoints
2025-03-06 19:21:58,945 - INFO - training batch 1501, loss: 0.200, 48032/60000 datapoints
2025-03-06 19:21:59,144 - INFO - training batch 1551, loss: 0.340, 49632/60000 datapoints
2025-03-06 19:21:59,341 - INFO - training batch 1601, loss: 0.213, 51232/60000 datapoints
2025-03-06 19:21:59,540 - INFO - training batch 1651, loss: 0.202, 52832/60000 datapoints
2025-03-06 19:21:59,742 - INFO - training batch 1701, loss: 0.335, 54432/60000 datapoints
2025-03-06 19:21:59,938 - INFO - training batch 1751, loss: 0.232, 56032/60000 datapoints
2025-03-06 19:22:00,137 - INFO - training batch 1801, loss: 0.226, 57632/60000 datapoints
2025-03-06 19:22:00,333 - INFO - training batch 1851, loss: 0.257, 59232/60000 datapoints
2025-03-06 19:22:00,434 - INFO - validation batch 1, loss: 0.471, 32/10016 datapoints
2025-03-06 19:22:00,589 - INFO - validation batch 51, loss: 0.134, 1632/10016 datapoints
2025-03-06 19:22:00,748 - INFO - validation batch 101, loss: 0.177, 3232/10016 datapoints
2025-03-06 19:22:00,904 - INFO - validation batch 151, loss: 0.504, 4832/10016 datapoints
2025-03-06 19:22:01,060 - INFO - validation batch 201, loss: 0.128, 6432/10016 datapoints
2025-03-06 19:22:01,216 - INFO - validation batch 251, loss: 0.245, 8032/10016 datapoints
2025-03-06 19:22:01,375 - INFO - validation batch 301, loss: 0.449, 9632/10016 datapoints
2025-03-06 19:22:01,411 - INFO - Epoch 293/800 done.
2025-03-06 19:22:01,411 - INFO - Final validation performance:
Loss: 0.301, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 19:22:01,412 - INFO - Beginning epoch 294/800
2025-03-06 19:22:01,419 - INFO - training batch 1, loss: 0.096, 32/60000 datapoints
2025-03-06 19:22:01,622 - INFO - training batch 51, loss: 0.210, 1632/60000 datapoints
2025-03-06 19:22:01,822 - INFO - training batch 101, loss: 0.186, 3232/60000 datapoints
2025-03-06 19:22:02,033 - INFO - training batch 151, loss: 0.183, 4832/60000 datapoints
2025-03-06 19:22:02,255 - INFO - training batch 201, loss: 0.330, 6432/60000 datapoints
2025-03-06 19:22:02,464 - INFO - training batch 251, loss: 0.399, 8032/60000 datapoints
2025-03-06 19:22:02,670 - INFO - training batch 301, loss: 0.879, 9632/60000 datapoints
2025-03-06 19:22:02,870 - INFO - training batch 351, loss: 0.166, 11232/60000 datapoints
2025-03-06 19:22:03,067 - INFO - training batch 401, loss: 0.270, 12832/60000 datapoints
2025-03-06 19:22:03,268 - INFO - training batch 451, loss: 0.285, 14432/60000 datapoints
2025-03-06 19:22:03,463 - INFO - training batch 501, loss: 0.178, 16032/60000 datapoints
2025-03-06 19:22:03,660 - INFO - training batch 551, loss: 0.341, 17632/60000 datapoints
2025-03-06 19:22:03,854 - INFO - training batch 601, loss: 0.347, 19232/60000 datapoints
2025-03-06 19:22:04,048 - INFO - training batch 651, loss: 0.330, 20832/60000 datapoints
2025-03-06 19:22:04,247 - INFO - training batch 701, loss: 0.773, 22432/60000 datapoints
2025-03-06 19:22:04,445 - INFO - training batch 751, loss: 0.421, 24032/60000 datapoints
2025-03-06 19:22:04,647 - INFO - training batch 801, loss: 0.270, 25632/60000 datapoints
2025-03-06 19:22:04,844 - INFO - training batch 851, loss: 0.263, 27232/60000 datapoints
2025-03-06 19:22:05,043 - INFO - training batch 901, loss: 0.531, 28832/60000 datapoints
2025-03-06 19:22:05,242 - INFO - training batch 951, loss: 0.573, 30432/60000 datapoints
2025-03-06 19:22:05,444 - INFO - training batch 1001, loss: 0.478, 32032/60000 datapoints
2025-03-06 19:22:05,644 - INFO - training batch 1051, loss: 0.202, 33632/60000 datapoints
2025-03-06 19:22:05,843 - INFO - training batch 1101, loss: 0.144, 35232/60000 datapoints
2025-03-06 19:22:06,038 - INFO - training batch 1151, loss: 0.246, 36832/60000 datapoints
2025-03-06 19:22:06,235 - INFO - training batch 1201, loss: 0.291, 38432/60000 datapoints
2025-03-06 19:22:06,432 - INFO - training batch 1251, loss: 0.211, 40032/60000 datapoints
2025-03-06 19:22:06,631 - INFO - training batch 1301, loss: 0.304, 41632/60000 datapoints
2025-03-06 19:22:06,830 - INFO - training batch 1351, loss: 0.263, 43232/60000 datapoints
2025-03-06 19:22:07,027 - INFO - training batch 1401, loss: 0.530, 44832/60000 datapoints
2025-03-06 19:22:07,222 - INFO - training batch 1451, loss: 0.217, 46432/60000 datapoints
2025-03-06 19:22:07,421 - INFO - training batch 1501, loss: 0.166, 48032/60000 datapoints
2025-03-06 19:22:07,628 - INFO - training batch 1551, loss: 0.356, 49632/60000 datapoints
2025-03-06 19:22:07,832 - INFO - training batch 1601, loss: 0.298, 51232/60000 datapoints
2025-03-06 19:22:08,025 - INFO - training batch 1651, loss: 0.312, 52832/60000 datapoints
2025-03-06 19:22:08,221 - INFO - training batch 1701, loss: 0.279, 54432/60000 datapoints
2025-03-06 19:22:08,421 - INFO - training batch 1751, loss: 0.494, 56032/60000 datapoints
2025-03-06 19:22:08,647 - INFO - training batch 1801, loss: 0.384, 57632/60000 datapoints
2025-03-06 19:22:08,851 - INFO - training batch 1851, loss: 0.365, 59232/60000 datapoints
2025-03-06 19:22:08,950 - INFO - validation batch 1, loss: 0.296, 32/10016 datapoints
2025-03-06 19:22:09,120 - INFO - validation batch 51, loss: 0.381, 1632/10016 datapoints
2025-03-06 19:22:09,276 - INFO - validation batch 101, loss: 0.356, 3232/10016 datapoints
2025-03-06 19:22:09,428 - INFO - validation batch 151, loss: 0.432, 4832/10016 datapoints
2025-03-06 19:22:09,583 - INFO - validation batch 201, loss: 0.299, 6432/10016 datapoints
2025-03-06 19:22:09,750 - INFO - validation batch 251, loss: 0.142, 8032/10016 datapoints
2025-03-06 19:22:09,908 - INFO - validation batch 301, loss: 0.311, 9632/10016 datapoints
2025-03-06 19:22:09,944 - INFO - Epoch 294/800 done.
2025-03-06 19:22:09,945 - INFO - Final validation performance:
Loss: 0.317, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 19:22:09,945 - INFO - Beginning epoch 295/800
2025-03-06 19:22:09,951 - INFO - training batch 1, loss: 0.169, 32/60000 datapoints
2025-03-06 19:22:10,149 - INFO - training batch 51, loss: 0.277, 1632/60000 datapoints
2025-03-06 19:22:10,354 - INFO - training batch 101, loss: 0.157, 3232/60000 datapoints
2025-03-06 19:22:10,548 - INFO - training batch 151, loss: 0.420, 4832/60000 datapoints
2025-03-06 19:22:10,760 - INFO - training batch 201, loss: 0.266, 6432/60000 datapoints
2025-03-06 19:22:10,958 - INFO - training batch 251, loss: 0.436, 8032/60000 datapoints
2025-03-06 19:22:11,152 - INFO - training batch 301, loss: 0.234, 9632/60000 datapoints
2025-03-06 19:22:11,348 - INFO - training batch 351, loss: 0.318, 11232/60000 datapoints
2025-03-06 19:22:11,556 - INFO - training batch 401, loss: 0.251, 12832/60000 datapoints
2025-03-06 19:22:11,757 - INFO - training batch 451, loss: 0.616, 14432/60000 datapoints
2025-03-06 19:22:11,953 - INFO - training batch 501, loss: 0.236, 16032/60000 datapoints
2025-03-06 19:22:12,150 - INFO - training batch 551, loss: 0.132, 17632/60000 datapoints
2025-03-06 19:22:12,360 - INFO - training batch 601, loss: 0.243, 19232/60000 datapoints
2025-03-06 19:22:12,576 - INFO - training batch 651, loss: 0.221, 20832/60000 datapoints
2025-03-06 19:22:12,778 - INFO - training batch 701, loss: 0.607, 22432/60000 datapoints
2025-03-06 19:22:12,974 - INFO - training batch 751, loss: 0.371, 24032/60000 datapoints
2025-03-06 19:22:13,172 - INFO - training batch 801, loss: 0.613, 25632/60000 datapoints
2025-03-06 19:22:13,372 - INFO - training batch 851, loss: 0.158, 27232/60000 datapoints
2025-03-06 19:22:13,569 - INFO - training batch 901, loss: 0.310, 28832/60000 datapoints
2025-03-06 19:22:13,768 - INFO - training batch 951, loss: 0.275, 30432/60000 datapoints
2025-03-06 19:22:13,967 - INFO - training batch 1001, loss: 0.348, 32032/60000 datapoints
2025-03-06 19:22:14,163 - INFO - training batch 1051, loss: 0.157, 33632/60000 datapoints
2025-03-06 19:22:14,361 - INFO - training batch 1101, loss: 0.398, 35232/60000 datapoints
2025-03-06 19:22:14,563 - INFO - training batch 1151, loss: 0.294, 36832/60000 datapoints
2025-03-06 19:22:14,766 - INFO - training batch 1201, loss: 0.246, 38432/60000 datapoints
2025-03-06 19:22:14,972 - INFO - training batch 1251, loss: 0.319, 40032/60000 datapoints
2025-03-06 19:22:15,171 - INFO - training batch 1301, loss: 0.403, 41632/60000 datapoints
2025-03-06 19:22:15,366 - INFO - training batch 1351, loss: 0.176, 43232/60000 datapoints
2025-03-06 19:22:15,563 - INFO - training batch 1401, loss: 0.148, 44832/60000 datapoints
2025-03-06 19:22:15,762 - INFO - training batch 1451, loss: 0.513, 46432/60000 datapoints
2025-03-06 19:22:15,963 - INFO - training batch 1501, loss: 0.186, 48032/60000 datapoints
2025-03-06 19:22:16,160 - INFO - training batch 1551, loss: 0.273, 49632/60000 datapoints
2025-03-06 19:22:16,358 - INFO - training batch 1601, loss: 0.113, 51232/60000 datapoints
2025-03-06 19:22:16,558 - INFO - training batch 1651, loss: 0.163, 52832/60000 datapoints
2025-03-06 19:22:16,760 - INFO - training batch 1701, loss: 0.140, 54432/60000 datapoints
2025-03-06 19:22:16,958 - INFO - training batch 1751, loss: 0.313, 56032/60000 datapoints
2025-03-06 19:22:17,156 - INFO - training batch 1801, loss: 0.263, 57632/60000 datapoints
2025-03-06 19:22:17,352 - INFO - training batch 1851, loss: 0.165, 59232/60000 datapoints
2025-03-06 19:22:17,458 - INFO - validation batch 1, loss: 0.180, 32/10016 datapoints
2025-03-06 19:22:17,618 - INFO - validation batch 51, loss: 0.213, 1632/10016 datapoints
2025-03-06 19:22:17,774 - INFO - validation batch 101, loss: 0.284, 3232/10016 datapoints
2025-03-06 19:22:17,931 - INFO - validation batch 151, loss: 0.122, 4832/10016 datapoints
2025-03-06 19:22:18,088 - INFO - validation batch 201, loss: 0.508, 6432/10016 datapoints
2025-03-06 19:22:18,245 - INFO - validation batch 251, loss: 0.631, 8032/10016 datapoints
2025-03-06 19:22:18,400 - INFO - validation batch 301, loss: 0.346, 9632/10016 datapoints
2025-03-06 19:22:18,442 - INFO - Epoch 295/800 done.
2025-03-06 19:22:18,442 - INFO - Final validation performance:
Loss: 0.326, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 19:22:18,442 - INFO - Beginning epoch 296/800
2025-03-06 19:22:18,448 - INFO - training batch 1, loss: 0.236, 32/60000 datapoints
2025-03-06 19:22:18,665 - INFO - training batch 51, loss: 0.141, 1632/60000 datapoints
2025-03-06 19:22:18,865 - INFO - training batch 101, loss: 0.238, 3232/60000 datapoints
2025-03-06 19:22:19,071 - INFO - training batch 151, loss: 0.185, 4832/60000 datapoints
2025-03-06 19:22:19,270 - INFO - training batch 201, loss: 0.239, 6432/60000 datapoints
2025-03-06 19:22:19,471 - INFO - training batch 251, loss: 0.425, 8032/60000 datapoints
2025-03-06 19:22:19,682 - INFO - training batch 301, loss: 0.156, 9632/60000 datapoints
2025-03-06 19:22:19,918 - INFO - training batch 351, loss: 0.257, 11232/60000 datapoints
2025-03-06 19:22:20,155 - INFO - training batch 401, loss: 0.191, 12832/60000 datapoints
2025-03-06 19:22:20,396 - INFO - training batch 451, loss: 0.101, 14432/60000 datapoints
2025-03-06 19:22:20,597 - INFO - training batch 501, loss: 0.364, 16032/60000 datapoints
2025-03-06 19:22:20,803 - INFO - training batch 551, loss: 0.209, 17632/60000 datapoints
2025-03-06 19:22:21,011 - INFO - training batch 601, loss: 0.133, 19232/60000 datapoints
2025-03-06 19:22:21,227 - INFO - training batch 651, loss: 0.323, 20832/60000 datapoints
2025-03-06 19:22:21,432 - INFO - training batch 701, loss: 0.443, 22432/60000 datapoints
2025-03-06 19:22:21,639 - INFO - training batch 751, loss: 0.309, 24032/60000 datapoints
2025-03-06 19:22:21,847 - INFO - training batch 801, loss: 0.315, 25632/60000 datapoints
2025-03-06 19:22:22,051 - INFO - training batch 851, loss: 0.265, 27232/60000 datapoints
2025-03-06 19:22:22,264 - INFO - training batch 901, loss: 0.541, 28832/60000 datapoints
2025-03-06 19:22:22,518 - INFO - training batch 951, loss: 0.260, 30432/60000 datapoints
2025-03-06 19:22:22,758 - INFO - training batch 1001, loss: 0.254, 32032/60000 datapoints
2025-03-06 19:22:22,996 - INFO - training batch 1051, loss: 0.091, 33632/60000 datapoints
2025-03-06 19:22:23,218 - INFO - training batch 1101, loss: 0.298, 35232/60000 datapoints
2025-03-06 19:22:23,444 - INFO - training batch 1151, loss: 0.148, 36832/60000 datapoints
2025-03-06 19:22:23,694 - INFO - training batch 1201, loss: 0.203, 38432/60000 datapoints
2025-03-06 19:22:23,919 - INFO - training batch 1251, loss: 0.212, 40032/60000 datapoints
2025-03-06 19:22:24,141 - INFO - training batch 1301, loss: 0.163, 41632/60000 datapoints
2025-03-06 19:22:24,352 - INFO - training batch 1351, loss: 0.840, 43232/60000 datapoints
2025-03-06 19:22:24,567 - INFO - training batch 1401, loss: 0.274, 44832/60000 datapoints
2025-03-06 19:22:24,818 - INFO - training batch 1451, loss: 0.476, 46432/60000 datapoints
2025-03-06 19:22:25,091 - INFO - training batch 1501, loss: 0.292, 48032/60000 datapoints
2025-03-06 19:22:25,300 - INFO - training batch 1551, loss: 0.400, 49632/60000 datapoints
2025-03-06 19:22:25,502 - INFO - training batch 1601, loss: 0.351, 51232/60000 datapoints
2025-03-06 19:22:25,718 - INFO - training batch 1651, loss: 0.209, 52832/60000 datapoints
2025-03-06 19:22:25,928 - INFO - training batch 1701, loss: 0.196, 54432/60000 datapoints
2025-03-06 19:22:26,127 - INFO - training batch 1751, loss: 0.153, 56032/60000 datapoints
2025-03-06 19:22:26,330 - INFO - training batch 1801, loss: 0.438, 57632/60000 datapoints
2025-03-06 19:22:26,530 - INFO - training batch 1851, loss: 0.514, 59232/60000 datapoints
2025-03-06 19:22:26,635 - INFO - validation batch 1, loss: 0.453, 32/10016 datapoints
2025-03-06 19:22:26,794 - INFO - validation batch 51, loss: 0.292, 1632/10016 datapoints
2025-03-06 19:22:26,954 - INFO - validation batch 101, loss: 0.209, 3232/10016 datapoints
2025-03-06 19:22:27,118 - INFO - validation batch 151, loss: 0.478, 4832/10016 datapoints
2025-03-06 19:22:27,286 - INFO - validation batch 201, loss: 0.134, 6432/10016 datapoints
2025-03-06 19:22:27,453 - INFO - validation batch 251, loss: 0.385, 8032/10016 datapoints
2025-03-06 19:22:27,623 - INFO - validation batch 301, loss: 0.136, 9632/10016 datapoints
2025-03-06 19:22:27,663 - INFO - Epoch 296/800 done.
2025-03-06 19:22:27,663 - INFO - Final validation performance:
Loss: 0.298, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 19:22:27,664 - INFO - Beginning epoch 297/800
2025-03-06 19:22:27,672 - INFO - training batch 1, loss: 0.339, 32/60000 datapoints
2025-03-06 19:22:27,877 - INFO - training batch 51, loss: 0.314, 1632/60000 datapoints
2025-03-06 19:22:28,085 - INFO - training batch 101, loss: 0.558, 3232/60000 datapoints
2025-03-06 19:22:28,305 - INFO - training batch 151, loss: 0.396, 4832/60000 datapoints
2025-03-06 19:22:28,515 - INFO - training batch 201, loss: 0.348, 6432/60000 datapoints
2025-03-06 19:22:28,734 - INFO - training batch 251, loss: 0.162, 8032/60000 datapoints
2025-03-06 19:22:28,940 - INFO - training batch 301, loss: 0.129, 9632/60000 datapoints
2025-03-06 19:22:29,151 - INFO - training batch 351, loss: 0.263, 11232/60000 datapoints
2025-03-06 19:22:29,354 - INFO - training batch 401, loss: 0.604, 12832/60000 datapoints
2025-03-06 19:22:29,557 - INFO - training batch 451, loss: 0.284, 14432/60000 datapoints
2025-03-06 19:22:29,764 - INFO - training batch 501, loss: 0.523, 16032/60000 datapoints
2025-03-06 19:22:29,974 - INFO - training batch 551, loss: 0.282, 17632/60000 datapoints
2025-03-06 19:22:30,181 - INFO - training batch 601, loss: 0.647, 19232/60000 datapoints
2025-03-06 19:22:30,380 - INFO - training batch 651, loss: 0.279, 20832/60000 datapoints
2025-03-06 19:22:30,630 - INFO - training batch 701, loss: 0.298, 22432/60000 datapoints
2025-03-06 19:22:30,873 - INFO - training batch 751, loss: 0.263, 24032/60000 datapoints
2025-03-06 19:22:31,099 - INFO - training batch 801, loss: 0.342, 25632/60000 datapoints
2025-03-06 19:22:31,296 - INFO - training batch 851, loss: 0.204, 27232/60000 datapoints
2025-03-06 19:22:31,497 - INFO - training batch 901, loss: 0.644, 28832/60000 datapoints
2025-03-06 19:22:31,700 - INFO - training batch 951, loss: 0.224, 30432/60000 datapoints
2025-03-06 19:22:31,901 - INFO - training batch 1001, loss: 0.259, 32032/60000 datapoints
2025-03-06 19:22:32,102 - INFO - training batch 1051, loss: 0.600, 33632/60000 datapoints
2025-03-06 19:22:32,305 - INFO - training batch 1101, loss: 0.301, 35232/60000 datapoints
2025-03-06 19:22:32,502 - INFO - training batch 1151, loss: 0.180, 36832/60000 datapoints
2025-03-06 19:22:32,729 - INFO - training batch 1201, loss: 0.437, 38432/60000 datapoints
2025-03-06 19:22:32,929 - INFO - training batch 1251, loss: 0.292, 40032/60000 datapoints
2025-03-06 19:22:33,128 - INFO - training batch 1301, loss: 0.227, 41632/60000 datapoints
2025-03-06 19:22:33,331 - INFO - training batch 1351, loss: 0.140, 43232/60000 datapoints
2025-03-06 19:22:33,532 - INFO - training batch 1401, loss: 0.185, 44832/60000 datapoints
2025-03-06 19:22:33,737 - INFO - training batch 1451, loss: 0.199, 46432/60000 datapoints
2025-03-06 19:22:33,949 - INFO - training batch 1501, loss: 0.322, 48032/60000 datapoints
2025-03-06 19:22:34,182 - INFO - training batch 1551, loss: 0.265, 49632/60000 datapoints
2025-03-06 19:22:34,415 - INFO - training batch 1601, loss: 0.615, 51232/60000 datapoints
2025-03-06 19:22:34,638 - INFO - training batch 1651, loss: 0.509, 52832/60000 datapoints
2025-03-06 19:22:34,851 - INFO - training batch 1701, loss: 0.401, 54432/60000 datapoints
2025-03-06 19:22:35,064 - INFO - training batch 1751, loss: 0.298, 56032/60000 datapoints
2025-03-06 19:22:35,268 - INFO - training batch 1801, loss: 0.149, 57632/60000 datapoints
2025-03-06 19:22:35,467 - INFO - training batch 1851, loss: 0.431, 59232/60000 datapoints
2025-03-06 19:22:35,574 - INFO - validation batch 1, loss: 0.288, 32/10016 datapoints
2025-03-06 19:22:35,734 - INFO - validation batch 51, loss: 0.146, 1632/10016 datapoints
2025-03-06 19:22:35,891 - INFO - validation batch 101, loss: 0.301, 3232/10016 datapoints
2025-03-06 19:22:36,049 - INFO - validation batch 151, loss: 0.208, 4832/10016 datapoints
2025-03-06 19:22:36,228 - INFO - validation batch 201, loss: 0.218, 6432/10016 datapoints
2025-03-06 19:22:36,391 - INFO - validation batch 251, loss: 0.499, 8032/10016 datapoints
2025-03-06 19:22:36,552 - INFO - validation batch 301, loss: 0.293, 9632/10016 datapoints
2025-03-06 19:22:36,593 - INFO - Epoch 297/800 done.
2025-03-06 19:22:36,593 - INFO - Final validation performance:
Loss: 0.279, top-1 acc: 0.912top-5 acc: 0.912
2025-03-06 19:22:36,594 - INFO - Beginning epoch 298/800
2025-03-06 19:22:36,600 - INFO - training batch 1, loss: 0.211, 32/60000 datapoints
2025-03-06 19:22:36,815 - INFO - training batch 51, loss: 0.215, 1632/60000 datapoints
2025-03-06 19:22:37,043 - INFO - training batch 101, loss: 0.297, 3232/60000 datapoints
2025-03-06 19:22:37,255 - INFO - training batch 151, loss: 0.519, 4832/60000 datapoints
2025-03-06 19:22:37,462 - INFO - training batch 201, loss: 0.352, 6432/60000 datapoints
2025-03-06 19:22:37,709 - INFO - training batch 251, loss: 0.197, 8032/60000 datapoints
2025-03-06 19:22:37,920 - INFO - training batch 301, loss: 0.353, 9632/60000 datapoints
2025-03-06 19:22:38,148 - INFO - training batch 351, loss: 0.527, 11232/60000 datapoints
2025-03-06 19:22:38,357 - INFO - training batch 401, loss: 0.279, 12832/60000 datapoints
2025-03-06 19:22:38,579 - INFO - training batch 451, loss: 0.152, 14432/60000 datapoints
2025-03-06 19:22:38,793 - INFO - training batch 501, loss: 0.202, 16032/60000 datapoints
2025-03-06 19:22:39,016 - INFO - training batch 551, loss: 0.238, 17632/60000 datapoints
2025-03-06 19:22:39,241 - INFO - training batch 601, loss: 0.464, 19232/60000 datapoints
2025-03-06 19:22:39,461 - INFO - training batch 651, loss: 0.214, 20832/60000 datapoints
2025-03-06 19:22:39,699 - INFO - training batch 701, loss: 0.425, 22432/60000 datapoints
2025-03-06 19:22:39,901 - INFO - training batch 751, loss: 0.509, 24032/60000 datapoints
2025-03-06 19:22:40,115 - INFO - training batch 801, loss: 0.218, 25632/60000 datapoints
2025-03-06 19:22:40,323 - INFO - training batch 851, loss: 0.549, 27232/60000 datapoints
2025-03-06 19:22:40,535 - INFO - training batch 901, loss: 0.146, 28832/60000 datapoints
2025-03-06 19:22:40,749 - INFO - training batch 951, loss: 0.514, 30432/60000 datapoints
2025-03-06 19:22:40,959 - INFO - training batch 1001, loss: 0.565, 32032/60000 datapoints
2025-03-06 19:22:41,165 - INFO - training batch 1051, loss: 0.215, 33632/60000 datapoints
2025-03-06 19:22:41,376 - INFO - training batch 1101, loss: 0.322, 35232/60000 datapoints
2025-03-06 19:22:41,602 - INFO - training batch 1151, loss: 0.177, 36832/60000 datapoints
2025-03-06 19:22:41,830 - INFO - training batch 1201, loss: 0.353, 38432/60000 datapoints
2025-03-06 19:22:42,027 - INFO - training batch 1251, loss: 0.169, 40032/60000 datapoints
2025-03-06 19:22:42,225 - INFO - training batch 1301, loss: 0.161, 41632/60000 datapoints
2025-03-06 19:22:42,423 - INFO - training batch 1351, loss: 0.153, 43232/60000 datapoints
2025-03-06 19:22:42,624 - INFO - training batch 1401, loss: 0.252, 44832/60000 datapoints
2025-03-06 19:22:42,848 - INFO - training batch 1451, loss: 0.578, 46432/60000 datapoints
2025-03-06 19:22:43,049 - INFO - training batch 1501, loss: 0.520, 48032/60000 datapoints
2025-03-06 19:22:43,250 - INFO - training batch 1551, loss: 0.211, 49632/60000 datapoints
2025-03-06 19:22:43,456 - INFO - training batch 1601, loss: 0.385, 51232/60000 datapoints
2025-03-06 19:22:43,666 - INFO - training batch 1651, loss: 0.222, 52832/60000 datapoints
2025-03-06 19:22:43,871 - INFO - training batch 1701, loss: 0.182, 54432/60000 datapoints
2025-03-06 19:22:44,076 - INFO - training batch 1751, loss: 0.380, 56032/60000 datapoints
2025-03-06 19:22:44,278 - INFO - training batch 1801, loss: 0.232, 57632/60000 datapoints
2025-03-06 19:22:44,477 - INFO - training batch 1851, loss: 0.101, 59232/60000 datapoints
2025-03-06 19:22:44,580 - INFO - validation batch 1, loss: 0.377, 32/10016 datapoints
2025-03-06 19:22:44,739 - INFO - validation batch 51, loss: 0.144, 1632/10016 datapoints
2025-03-06 19:22:44,906 - INFO - validation batch 101, loss: 0.431, 3232/10016 datapoints
2025-03-06 19:22:45,068 - INFO - validation batch 151, loss: 0.613, 4832/10016 datapoints
2025-03-06 19:22:45,225 - INFO - validation batch 201, loss: 0.236, 6432/10016 datapoints
2025-03-06 19:22:45,383 - INFO - validation batch 251, loss: 0.286, 8032/10016 datapoints
2025-03-06 19:22:45,538 - INFO - validation batch 301, loss: 0.168, 9632/10016 datapoints
2025-03-06 19:22:45,577 - INFO - Epoch 298/800 done.
2025-03-06 19:22:45,578 - INFO - Final validation performance:
Loss: 0.322, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:22:45,578 - INFO - Beginning epoch 299/800
2025-03-06 19:22:45,584 - INFO - training batch 1, loss: 0.546, 32/60000 datapoints
2025-03-06 19:22:45,798 - INFO - training batch 51, loss: 0.410, 1632/60000 datapoints
2025-03-06 19:22:45,994 - INFO - training batch 101, loss: 0.670, 3232/60000 datapoints
2025-03-06 19:22:46,200 - INFO - training batch 151, loss: 0.281, 4832/60000 datapoints
2025-03-06 19:22:46,401 - INFO - training batch 201, loss: 0.236, 6432/60000 datapoints
2025-03-06 19:22:46,601 - INFO - training batch 251, loss: 0.280, 8032/60000 datapoints
2025-03-06 19:22:46,807 - INFO - training batch 301, loss: 0.223, 9632/60000 datapoints
2025-03-06 19:22:47,004 - INFO - training batch 351, loss: 0.121, 11232/60000 datapoints
2025-03-06 19:22:47,200 - INFO - training batch 401, loss: 0.307, 12832/60000 datapoints
2025-03-06 19:22:47,404 - INFO - training batch 451, loss: 0.320, 14432/60000 datapoints
2025-03-06 19:22:47,600 - INFO - training batch 501, loss: 0.165, 16032/60000 datapoints
2025-03-06 19:22:47,803 - INFO - training batch 551, loss: 0.309, 17632/60000 datapoints
2025-03-06 19:22:48,002 - INFO - training batch 601, loss: 0.360, 19232/60000 datapoints
2025-03-06 19:22:48,218 - INFO - training batch 651, loss: 0.293, 20832/60000 datapoints
2025-03-06 19:22:48,416 - INFO - training batch 701, loss: 0.289, 22432/60000 datapoints
2025-03-06 19:22:48,613 - INFO - training batch 751, loss: 0.177, 24032/60000 datapoints
2025-03-06 19:22:48,817 - INFO - training batch 801, loss: 0.252, 25632/60000 datapoints
2025-03-06 19:22:49,015 - INFO - training batch 851, loss: 0.379, 27232/60000 datapoints
2025-03-06 19:22:49,216 - INFO - training batch 901, loss: 0.254, 28832/60000 datapoints
2025-03-06 19:22:49,418 - INFO - training batch 951, loss: 0.187, 30432/60000 datapoints
2025-03-06 19:22:49,618 - INFO - training batch 1001, loss: 0.178, 32032/60000 datapoints
2025-03-06 19:22:49,820 - INFO - training batch 1051, loss: 0.390, 33632/60000 datapoints
2025-03-06 19:22:50,018 - INFO - training batch 1101, loss: 0.210, 35232/60000 datapoints
2025-03-06 19:22:50,218 - INFO - training batch 1151, loss: 0.326, 36832/60000 datapoints
2025-03-06 19:22:50,423 - INFO - training batch 1201, loss: 0.151, 38432/60000 datapoints
2025-03-06 19:22:50,623 - INFO - training batch 1251, loss: 0.529, 40032/60000 datapoints
2025-03-06 19:22:50,822 - INFO - training batch 1301, loss: 0.495, 41632/60000 datapoints
2025-03-06 19:22:51,022 - INFO - training batch 1351, loss: 0.495, 43232/60000 datapoints
2025-03-06 19:22:51,222 - INFO - training batch 1401, loss: 0.645, 44832/60000 datapoints
2025-03-06 19:22:51,421 - INFO - training batch 1451, loss: 0.325, 46432/60000 datapoints
2025-03-06 19:22:51,622 - INFO - training batch 1501, loss: 0.217, 48032/60000 datapoints
2025-03-06 19:22:51,821 - INFO - training batch 1551, loss: 0.332, 49632/60000 datapoints
2025-03-06 19:22:52,019 - INFO - training batch 1601, loss: 0.216, 51232/60000 datapoints
2025-03-06 19:22:52,218 - INFO - training batch 1651, loss: 0.243, 52832/60000 datapoints
2025-03-06 19:22:52,421 - INFO - training batch 1701, loss: 0.375, 54432/60000 datapoints
2025-03-06 19:22:52,626 - INFO - training batch 1751, loss: 0.381, 56032/60000 datapoints
2025-03-06 19:22:52,846 - INFO - training batch 1801, loss: 0.249, 57632/60000 datapoints
2025-03-06 19:22:53,044 - INFO - training batch 1851, loss: 0.096, 59232/60000 datapoints
2025-03-06 19:22:53,147 - INFO - validation batch 1, loss: 0.290, 32/10016 datapoints
2025-03-06 19:22:53,305 - INFO - validation batch 51, loss: 0.266, 1632/10016 datapoints
2025-03-06 19:22:53,466 - INFO - validation batch 101, loss: 0.304, 3232/10016 datapoints
2025-03-06 19:22:53,622 - INFO - validation batch 151, loss: 0.397, 4832/10016 datapoints
2025-03-06 19:22:53,778 - INFO - validation batch 201, loss: 0.298, 6432/10016 datapoints
2025-03-06 19:22:53,934 - INFO - validation batch 251, loss: 0.317, 8032/10016 datapoints
2025-03-06 19:22:54,093 - INFO - validation batch 301, loss: 0.251, 9632/10016 datapoints
2025-03-06 19:22:54,133 - INFO - Epoch 299/800 done.
2025-03-06 19:22:54,133 - INFO - Final validation performance:
Loss: 0.303, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:22:54,134 - INFO - Beginning epoch 300/800
2025-03-06 19:22:54,140 - INFO - training batch 1, loss: 0.652, 32/60000 datapoints
2025-03-06 19:22:54,355 - INFO - training batch 51, loss: 0.145, 1632/60000 datapoints
2025-03-06 19:22:54,551 - INFO - training batch 101, loss: 0.337, 3232/60000 datapoints
2025-03-06 19:22:54,755 - INFO - training batch 151, loss: 0.409, 4832/60000 datapoints
2025-03-06 19:22:54,963 - INFO - training batch 201, loss: 0.550, 6432/60000 datapoints
2025-03-06 19:22:55,163 - INFO - training batch 251, loss: 0.277, 8032/60000 datapoints
2025-03-06 19:22:55,366 - INFO - training batch 301, loss: 0.415, 9632/60000 datapoints
2025-03-06 19:22:55,563 - INFO - training batch 351, loss: 0.294, 11232/60000 datapoints
2025-03-06 19:22:55,767 - INFO - training batch 401, loss: 0.142, 12832/60000 datapoints
2025-03-06 19:22:55,964 - INFO - training batch 451, loss: 0.514, 14432/60000 datapoints
2025-03-06 19:22:56,161 - INFO - training batch 501, loss: 0.239, 16032/60000 datapoints
2025-03-06 19:22:56,359 - INFO - training batch 551, loss: 0.222, 17632/60000 datapoints
2025-03-06 19:22:56,555 - INFO - training batch 601, loss: 0.318, 19232/60000 datapoints
2025-03-06 19:22:56,756 - INFO - training batch 651, loss: 0.487, 20832/60000 datapoints
2025-03-06 19:22:56,958 - INFO - training batch 701, loss: 0.459, 22432/60000 datapoints
2025-03-06 19:22:57,154 - INFO - training batch 751, loss: 0.442, 24032/60000 datapoints
2025-03-06 19:22:57,352 - INFO - training batch 801, loss: 0.230, 25632/60000 datapoints
2025-03-06 19:22:57,549 - INFO - training batch 851, loss: 0.436, 27232/60000 datapoints
2025-03-06 19:22:57,759 - INFO - training batch 901, loss: 0.114, 28832/60000 datapoints
2025-03-06 19:22:57,960 - INFO - training batch 951, loss: 0.448, 30432/60000 datapoints
2025-03-06 19:22:58,160 - INFO - training batch 1001, loss: 0.126, 32032/60000 datapoints
2025-03-06 19:22:58,361 - INFO - training batch 1051, loss: 0.169, 33632/60000 datapoints
2025-03-06 19:22:58,561 - INFO - training batch 1101, loss: 0.496, 35232/60000 datapoints
2025-03-06 19:22:58,761 - INFO - training batch 1151, loss: 0.488, 36832/60000 datapoints
2025-03-06 19:22:58,966 - INFO - training batch 1201, loss: 0.249, 38432/60000 datapoints
2025-03-06 19:22:59,167 - INFO - training batch 1251, loss: 0.182, 40032/60000 datapoints
2025-03-06 19:22:59,365 - INFO - training batch 1301, loss: 0.482, 41632/60000 datapoints
2025-03-06 19:22:59,563 - INFO - training batch 1351, loss: 0.296, 43232/60000 datapoints
2025-03-06 19:22:59,761 - INFO - training batch 1401, loss: 0.222, 44832/60000 datapoints
2025-03-06 19:22:59,958 - INFO - training batch 1451, loss: 0.336, 46432/60000 datapoints
2025-03-06 19:23:00,157 - INFO - training batch 1501, loss: 0.216, 48032/60000 datapoints
2025-03-06 19:23:00,356 - INFO - training batch 1551, loss: 0.313, 49632/60000 datapoints
2025-03-06 19:23:00,554 - INFO - training batch 1601, loss: 0.426, 51232/60000 datapoints
2025-03-06 19:23:00,769 - INFO - training batch 1651, loss: 0.481, 52832/60000 datapoints
2025-03-06 19:23:00,985 - INFO - training batch 1701, loss: 0.234, 54432/60000 datapoints
2025-03-06 19:23:01,191 - INFO - training batch 1751, loss: 0.354, 56032/60000 datapoints
2025-03-06 19:23:01,394 - INFO - training batch 1801, loss: 0.325, 57632/60000 datapoints
2025-03-06 19:23:01,596 - INFO - training batch 1851, loss: 0.464, 59232/60000 datapoints
2025-03-06 19:23:01,704 - INFO - validation batch 1, loss: 0.332, 32/10016 datapoints
2025-03-06 19:23:01,865 - INFO - validation batch 51, loss: 0.301, 1632/10016 datapoints
2025-03-06 19:23:02,023 - INFO - validation batch 101, loss: 0.200, 3232/10016 datapoints
2025-03-06 19:23:02,182 - INFO - validation batch 151, loss: 0.204, 4832/10016 datapoints
2025-03-06 19:23:02,340 - INFO - validation batch 201, loss: 0.139, 6432/10016 datapoints
2025-03-06 19:23:02,500 - INFO - validation batch 251, loss: 0.340, 8032/10016 datapoints
2025-03-06 19:23:02,663 - INFO - validation batch 301, loss: 0.292, 9632/10016 datapoints
2025-03-06 19:23:02,700 - INFO - Epoch 300/800 done.
2025-03-06 19:23:02,700 - INFO - Final validation performance:
Loss: 0.258, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:23:02,701 - INFO - Beginning epoch 301/800
2025-03-06 19:23:02,709 - INFO - training batch 1, loss: 0.543, 32/60000 datapoints
2025-03-06 19:23:02,952 - INFO - training batch 51, loss: 0.307, 1632/60000 datapoints
2025-03-06 19:23:03,154 - INFO - training batch 101, loss: 0.160, 3232/60000 datapoints
2025-03-06 19:23:03,363 - INFO - training batch 151, loss: 0.271, 4832/60000 datapoints
2025-03-06 19:23:03,575 - INFO - training batch 201, loss: 0.305, 6432/60000 datapoints
2025-03-06 19:23:03,783 - INFO - training batch 251, loss: 0.232, 8032/60000 datapoints
2025-03-06 19:23:03,988 - INFO - training batch 301, loss: 0.237, 9632/60000 datapoints
2025-03-06 19:23:04,186 - INFO - training batch 351, loss: 0.243, 11232/60000 datapoints
2025-03-06 19:23:04,384 - INFO - training batch 401, loss: 0.308, 12832/60000 datapoints
2025-03-06 19:23:04,593 - INFO - training batch 451, loss: 0.385, 14432/60000 datapoints
2025-03-06 19:23:04,811 - INFO - training batch 501, loss: 0.375, 16032/60000 datapoints
2025-03-06 19:23:05,020 - INFO - training batch 551, loss: 0.156, 17632/60000 datapoints
2025-03-06 19:23:05,226 - INFO - training batch 601, loss: 0.328, 19232/60000 datapoints
2025-03-06 19:23:05,432 - INFO - training batch 651, loss: 0.180, 20832/60000 datapoints
2025-03-06 19:23:05,639 - INFO - training batch 701, loss: 0.345, 22432/60000 datapoints
2025-03-06 19:23:05,843 - INFO - training batch 751, loss: 0.282, 24032/60000 datapoints
2025-03-06 19:23:06,048 - INFO - training batch 801, loss: 0.341, 25632/60000 datapoints
2025-03-06 19:23:06,250 - INFO - training batch 851, loss: 0.592, 27232/60000 datapoints
2025-03-06 19:23:06,457 - INFO - training batch 901, loss: 0.174, 28832/60000 datapoints
2025-03-06 19:23:06,659 - INFO - training batch 951, loss: 0.201, 30432/60000 datapoints
2025-03-06 19:23:06,860 - INFO - training batch 1001, loss: 0.149, 32032/60000 datapoints
2025-03-06 19:23:07,059 - INFO - training batch 1051, loss: 0.160, 33632/60000 datapoints
2025-03-06 19:23:07,256 - INFO - training batch 1101, loss: 0.349, 35232/60000 datapoints
2025-03-06 19:23:07,454 - INFO - training batch 1151, loss: 0.323, 36832/60000 datapoints
2025-03-06 19:23:07,655 - INFO - training batch 1201, loss: 0.292, 38432/60000 datapoints
2025-03-06 19:23:07,857 - INFO - training batch 1251, loss: 0.114, 40032/60000 datapoints
2025-03-06 19:23:08,065 - INFO - training batch 1301, loss: 0.335, 41632/60000 datapoints
2025-03-06 19:23:08,263 - INFO - training batch 1351, loss: 0.773, 43232/60000 datapoints
2025-03-06 19:23:08,462 - INFO - training batch 1401, loss: 0.325, 44832/60000 datapoints
2025-03-06 19:23:08,664 - INFO - training batch 1451, loss: 0.259, 46432/60000 datapoints
2025-03-06 19:23:08,871 - INFO - training batch 1501, loss: 0.222, 48032/60000 datapoints
2025-03-06 19:23:09,122 - INFO - training batch 1551, loss: 0.326, 49632/60000 datapoints
2025-03-06 19:23:09,360 - INFO - training batch 1601, loss: 0.362, 51232/60000 datapoints
2025-03-06 19:23:09,603 - INFO - training batch 1651, loss: 0.335, 52832/60000 datapoints
2025-03-06 19:23:09,853 - INFO - training batch 1701, loss: 0.250, 54432/60000 datapoints
2025-03-06 19:23:10,065 - INFO - training batch 1751, loss: 0.263, 56032/60000 datapoints
2025-03-06 19:23:10,269 - INFO - training batch 1801, loss: 0.247, 57632/60000 datapoints
2025-03-06 19:23:10,476 - INFO - training batch 1851, loss: 0.384, 59232/60000 datapoints
2025-03-06 19:23:10,580 - INFO - validation batch 1, loss: 0.226, 32/10016 datapoints
2025-03-06 19:23:10,749 - INFO - validation batch 51, loss: 0.219, 1632/10016 datapoints
2025-03-06 19:23:10,913 - INFO - validation batch 101, loss: 0.538, 3232/10016 datapoints
2025-03-06 19:23:11,077 - INFO - validation batch 151, loss: 0.363, 4832/10016 datapoints
2025-03-06 19:23:11,237 - INFO - validation batch 201, loss: 0.205, 6432/10016 datapoints
2025-03-06 19:23:11,399 - INFO - validation batch 251, loss: 0.347, 8032/10016 datapoints
2025-03-06 19:23:11,563 - INFO - validation batch 301, loss: 0.162, 9632/10016 datapoints
2025-03-06 19:23:11,602 - INFO - Epoch 301/800 done.
2025-03-06 19:23:11,602 - INFO - Final validation performance:
Loss: 0.294, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:23:11,603 - INFO - Beginning epoch 302/800
2025-03-06 19:23:11,615 - INFO - training batch 1, loss: 0.196, 32/60000 datapoints
2025-03-06 19:23:11,837 - INFO - training batch 51, loss: 0.231, 1632/60000 datapoints
2025-03-06 19:23:12,043 - INFO - training batch 101, loss: 0.204, 3232/60000 datapoints
2025-03-06 19:23:12,249 - INFO - training batch 151, loss: 0.314, 4832/60000 datapoints
2025-03-06 19:23:12,449 - INFO - training batch 201, loss: 0.642, 6432/60000 datapoints
2025-03-06 19:23:12,654 - INFO - training batch 251, loss: 0.521, 8032/60000 datapoints
2025-03-06 19:23:12,869 - INFO - training batch 301, loss: 0.249, 9632/60000 datapoints
2025-03-06 19:23:13,094 - INFO - training batch 351, loss: 0.106, 11232/60000 datapoints
2025-03-06 19:23:13,298 - INFO - training batch 401, loss: 0.262, 12832/60000 datapoints
2025-03-06 19:23:13,502 - INFO - training batch 451, loss: 0.243, 14432/60000 datapoints
2025-03-06 19:23:13,702 - INFO - training batch 501, loss: 0.178, 16032/60000 datapoints
2025-03-06 19:23:13,898 - INFO - training batch 551, loss: 0.434, 17632/60000 datapoints
2025-03-06 19:23:14,099 - INFO - training batch 601, loss: 0.160, 19232/60000 datapoints
2025-03-06 19:23:14,300 - INFO - training batch 651, loss: 0.534, 20832/60000 datapoints
2025-03-06 19:23:14,501 - INFO - training batch 701, loss: 0.637, 22432/60000 datapoints
2025-03-06 19:23:14,700 - INFO - training batch 751, loss: 0.362, 24032/60000 datapoints
2025-03-06 19:23:14,903 - INFO - training batch 801, loss: 0.104, 25632/60000 datapoints
2025-03-06 19:23:15,105 - INFO - training batch 851, loss: 0.099, 27232/60000 datapoints
2025-03-06 19:23:15,303 - INFO - training batch 901, loss: 0.473, 28832/60000 datapoints
2025-03-06 19:23:15,502 - INFO - training batch 951, loss: 0.382, 30432/60000 datapoints
2025-03-06 19:23:15,706 - INFO - training batch 1001, loss: 0.708, 32032/60000 datapoints
2025-03-06 19:23:15,903 - INFO - training batch 1051, loss: 0.129, 33632/60000 datapoints
2025-03-06 19:23:16,100 - INFO - training batch 1101, loss: 0.194, 35232/60000 datapoints
2025-03-06 19:23:16,294 - INFO - training batch 1151, loss: 0.186, 36832/60000 datapoints
2025-03-06 19:23:16,500 - INFO - training batch 1201, loss: 0.458, 38432/60000 datapoints
2025-03-06 19:23:16,702 - INFO - training batch 1251, loss: 0.566, 40032/60000 datapoints
2025-03-06 19:23:16,908 - INFO - training batch 1301, loss: 0.365, 41632/60000 datapoints
2025-03-06 19:23:17,115 - INFO - training batch 1351, loss: 0.227, 43232/60000 datapoints
2025-03-06 19:23:17,314 - INFO - training batch 1401, loss: 0.342, 44832/60000 datapoints
2025-03-06 19:23:17,512 - INFO - training batch 1451, loss: 0.209, 46432/60000 datapoints
2025-03-06 19:23:17,714 - INFO - training batch 1501, loss: 0.415, 48032/60000 datapoints
2025-03-06 19:23:17,912 - INFO - training batch 1551, loss: 0.112, 49632/60000 datapoints
2025-03-06 19:23:18,115 - INFO - training batch 1601, loss: 0.313, 51232/60000 datapoints
2025-03-06 19:23:18,311 - INFO - training batch 1651, loss: 0.326, 52832/60000 datapoints
2025-03-06 19:23:18,511 - INFO - training batch 1701, loss: 0.223, 54432/60000 datapoints
2025-03-06 19:23:18,713 - INFO - training batch 1751, loss: 0.130, 56032/60000 datapoints
2025-03-06 19:23:18,911 - INFO - training batch 1801, loss: 0.142, 57632/60000 datapoints
2025-03-06 19:23:19,118 - INFO - training batch 1851, loss: 0.256, 59232/60000 datapoints
2025-03-06 19:23:19,223 - INFO - validation batch 1, loss: 0.220, 32/10016 datapoints
2025-03-06 19:23:19,378 - INFO - validation batch 51, loss: 0.225, 1632/10016 datapoints
2025-03-06 19:23:19,535 - INFO - validation batch 101, loss: 0.316, 3232/10016 datapoints
2025-03-06 19:23:19,695 - INFO - validation batch 151, loss: 0.277, 4832/10016 datapoints
2025-03-06 19:23:19,854 - INFO - validation batch 201, loss: 0.432, 6432/10016 datapoints
2025-03-06 19:23:20,009 - INFO - validation batch 251, loss: 0.127, 8032/10016 datapoints
2025-03-06 19:23:20,168 - INFO - validation batch 301, loss: 0.444, 9632/10016 datapoints
2025-03-06 19:23:20,204 - INFO - Epoch 302/800 done.
2025-03-06 19:23:20,204 - INFO - Final validation performance:
Loss: 0.292, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:23:20,205 - INFO - Beginning epoch 303/800
2025-03-06 19:23:20,213 - INFO - training batch 1, loss: 0.305, 32/60000 datapoints
2025-03-06 19:23:20,435 - INFO - training batch 51, loss: 0.402, 1632/60000 datapoints
2025-03-06 19:23:20,639 - INFO - training batch 101, loss: 0.211, 3232/60000 datapoints
2025-03-06 19:23:20,844 - INFO - training batch 151, loss: 0.342, 4832/60000 datapoints
2025-03-06 19:23:21,047 - INFO - training batch 201, loss: 0.151, 6432/60000 datapoints
2025-03-06 19:23:21,249 - INFO - training batch 251, loss: 0.380, 8032/60000 datapoints
2025-03-06 19:23:21,453 - INFO - training batch 301, loss: 0.339, 9632/60000 datapoints
2025-03-06 19:23:21,680 - INFO - training batch 351, loss: 0.311, 11232/60000 datapoints
2025-03-06 19:23:21,905 - INFO - training batch 401, loss: 0.371, 12832/60000 datapoints
2025-03-06 19:23:22,132 - INFO - training batch 451, loss: 0.253, 14432/60000 datapoints
2025-03-06 19:23:22,358 - INFO - training batch 501, loss: 0.395, 16032/60000 datapoints
2025-03-06 19:23:22,574 - INFO - training batch 551, loss: 0.128, 17632/60000 datapoints
2025-03-06 19:23:22,798 - INFO - training batch 601, loss: 0.323, 19232/60000 datapoints
2025-03-06 19:23:23,002 - INFO - training batch 651, loss: 0.256, 20832/60000 datapoints
2025-03-06 19:23:23,218 - INFO - training batch 701, loss: 0.204, 22432/60000 datapoints
2025-03-06 19:23:23,416 - INFO - training batch 751, loss: 0.406, 24032/60000 datapoints
2025-03-06 19:23:23,620 - INFO - training batch 801, loss: 0.247, 25632/60000 datapoints
2025-03-06 19:23:23,820 - INFO - training batch 851, loss: 0.212, 27232/60000 datapoints
2025-03-06 19:23:24,018 - INFO - training batch 901, loss: 0.326, 28832/60000 datapoints
2025-03-06 19:23:24,219 - INFO - training batch 951, loss: 0.445, 30432/60000 datapoints
2025-03-06 19:23:24,419 - INFO - training batch 1001, loss: 0.722, 32032/60000 datapoints
2025-03-06 19:23:24,616 - INFO - training batch 1051, loss: 0.608, 33632/60000 datapoints
2025-03-06 19:23:24,812 - INFO - training batch 1101, loss: 0.843, 35232/60000 datapoints
2025-03-06 19:23:25,013 - INFO - training batch 1151, loss: 0.332, 36832/60000 datapoints
2025-03-06 19:23:25,210 - INFO - training batch 1201, loss: 0.386, 38432/60000 datapoints
2025-03-06 19:23:25,403 - INFO - training batch 1251, loss: 0.191, 40032/60000 datapoints
2025-03-06 19:23:25,597 - INFO - training batch 1301, loss: 0.207, 41632/60000 datapoints
2025-03-06 19:23:25,801 - INFO - training batch 1351, loss: 0.267, 43232/60000 datapoints
2025-03-06 19:23:25,998 - INFO - training batch 1401, loss: 0.318, 44832/60000 datapoints
2025-03-06 19:23:26,202 - INFO - training batch 1451, loss: 0.157, 46432/60000 datapoints
2025-03-06 19:23:26,398 - INFO - training batch 1501, loss: 0.358, 48032/60000 datapoints
2025-03-06 19:23:26,591 - INFO - training batch 1551, loss: 0.237, 49632/60000 datapoints
2025-03-06 19:23:26,790 - INFO - training batch 1601, loss: 0.129, 51232/60000 datapoints
2025-03-06 19:23:26,987 - INFO - training batch 1651, loss: 0.354, 52832/60000 datapoints
2025-03-06 19:23:27,181 - INFO - training batch 1701, loss: 0.108, 54432/60000 datapoints
2025-03-06 19:23:27,375 - INFO - training batch 1751, loss: 0.420, 56032/60000 datapoints
2025-03-06 19:23:27,571 - INFO - training batch 1801, loss: 0.093, 57632/60000 datapoints
2025-03-06 19:23:27,776 - INFO - training batch 1851, loss: 0.338, 59232/60000 datapoints
2025-03-06 19:23:27,877 - INFO - validation batch 1, loss: 0.194, 32/10016 datapoints
2025-03-06 19:23:28,030 - INFO - validation batch 51, loss: 0.187, 1632/10016 datapoints
2025-03-06 19:23:28,182 - INFO - validation batch 101, loss: 0.306, 3232/10016 datapoints
2025-03-06 19:23:28,333 - INFO - validation batch 151, loss: 0.082, 4832/10016 datapoints
2025-03-06 19:23:28,486 - INFO - validation batch 201, loss: 0.279, 6432/10016 datapoints
2025-03-06 19:23:28,641 - INFO - validation batch 251, loss: 0.092, 8032/10016 datapoints
2025-03-06 19:23:28,794 - INFO - validation batch 301, loss: 0.163, 9632/10016 datapoints
2025-03-06 19:23:28,831 - INFO - Epoch 303/800 done.
2025-03-06 19:23:28,831 - INFO - Final validation performance:
Loss: 0.186, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:23:28,832 - INFO - Beginning epoch 304/800
2025-03-06 19:23:28,838 - INFO - training batch 1, loss: 0.267, 32/60000 datapoints
2025-03-06 19:23:29,036 - INFO - training batch 51, loss: 0.294, 1632/60000 datapoints
2025-03-06 19:23:29,234 - INFO - training batch 101, loss: 0.340, 3232/60000 datapoints
2025-03-06 19:23:29,440 - INFO - training batch 151, loss: 0.287, 4832/60000 datapoints
2025-03-06 19:23:29,638 - INFO - training batch 201, loss: 0.305, 6432/60000 datapoints
2025-03-06 19:23:29,834 - INFO - training batch 251, loss: 0.624, 8032/60000 datapoints
2025-03-06 19:23:30,037 - INFO - training batch 301, loss: 0.279, 9632/60000 datapoints
2025-03-06 19:23:30,236 - INFO - training batch 351, loss: 0.118, 11232/60000 datapoints
2025-03-06 19:23:30,432 - INFO - training batch 401, loss: 0.260, 12832/60000 datapoints
2025-03-06 19:23:30,634 - INFO - training batch 451, loss: 0.270, 14432/60000 datapoints
2025-03-06 19:23:30,861 - INFO - training batch 501, loss: 0.387, 16032/60000 datapoints
2025-03-06 19:23:31,060 - INFO - training batch 551, loss: 0.583, 17632/60000 datapoints
2025-03-06 19:23:31,254 - INFO - training batch 601, loss: 0.384, 19232/60000 datapoints
2025-03-06 19:23:31,449 - INFO - training batch 651, loss: 0.389, 20832/60000 datapoints
2025-03-06 19:23:31,649 - INFO - training batch 701, loss: 0.498, 22432/60000 datapoints
2025-03-06 19:23:31,847 - INFO - training batch 751, loss: 0.438, 24032/60000 datapoints
2025-03-06 19:23:32,044 - INFO - training batch 801, loss: 0.687, 25632/60000 datapoints
2025-03-06 19:23:32,239 - INFO - training batch 851, loss: 0.822, 27232/60000 datapoints
2025-03-06 19:23:32,434 - INFO - training batch 901, loss: 0.374, 28832/60000 datapoints
2025-03-06 19:23:32,636 - INFO - training batch 951, loss: 0.208, 30432/60000 datapoints
2025-03-06 19:23:32,834 - INFO - training batch 1001, loss: 0.251, 32032/60000 datapoints
2025-03-06 19:23:33,034 - INFO - training batch 1051, loss: 0.237, 33632/60000 datapoints
2025-03-06 19:23:33,248 - INFO - training batch 1101, loss: 0.277, 35232/60000 datapoints
2025-03-06 19:23:33,449 - INFO - training batch 1151, loss: 0.453, 36832/60000 datapoints
2025-03-06 19:23:33,657 - INFO - training batch 1201, loss: 0.160, 38432/60000 datapoints
2025-03-06 19:23:33,852 - INFO - training batch 1251, loss: 0.470, 40032/60000 datapoints
2025-03-06 19:23:34,046 - INFO - training batch 1301, loss: 0.179, 41632/60000 datapoints
2025-03-06 19:23:34,241 - INFO - training batch 1351, loss: 0.231, 43232/60000 datapoints
2025-03-06 19:23:34,432 - INFO - training batch 1401, loss: 0.168, 44832/60000 datapoints
2025-03-06 19:23:34,632 - INFO - training batch 1451, loss: 0.249, 46432/60000 datapoints
2025-03-06 19:23:34,832 - INFO - training batch 1501, loss: 0.237, 48032/60000 datapoints
2025-03-06 19:23:35,037 - INFO - training batch 1551, loss: 0.189, 49632/60000 datapoints
2025-03-06 19:23:35,234 - INFO - training batch 1601, loss: 0.470, 51232/60000 datapoints
2025-03-06 19:23:35,434 - INFO - training batch 1651, loss: 0.276, 52832/60000 datapoints
2025-03-06 19:23:35,635 - INFO - training batch 1701, loss: 0.271, 54432/60000 datapoints
2025-03-06 19:23:35,831 - INFO - training batch 1751, loss: 0.506, 56032/60000 datapoints
2025-03-06 19:23:36,031 - INFO - training batch 1801, loss: 0.402, 57632/60000 datapoints
2025-03-06 19:23:36,228 - INFO - training batch 1851, loss: 0.206, 59232/60000 datapoints
2025-03-06 19:23:36,332 - INFO - validation batch 1, loss: 0.341, 32/10016 datapoints
2025-03-06 19:23:36,485 - INFO - validation batch 51, loss: 0.353, 1632/10016 datapoints
2025-03-06 19:23:36,641 - INFO - validation batch 101, loss: 0.110, 3232/10016 datapoints
2025-03-06 19:23:36,794 - INFO - validation batch 151, loss: 0.221, 4832/10016 datapoints
2025-03-06 19:23:36,946 - INFO - validation batch 201, loss: 0.240, 6432/10016 datapoints
2025-03-06 19:23:37,103 - INFO - validation batch 251, loss: 0.189, 8032/10016 datapoints
2025-03-06 19:23:37,256 - INFO - validation batch 301, loss: 0.267, 9632/10016 datapoints
2025-03-06 19:23:37,295 - INFO - Epoch 304/800 done.
2025-03-06 19:23:37,295 - INFO - Final validation performance:
Loss: 0.246, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:23:37,295 - INFO - Beginning epoch 305/800
2025-03-06 19:23:37,302 - INFO - training batch 1, loss: 0.402, 32/60000 datapoints
2025-03-06 19:23:37,509 - INFO - training batch 51, loss: 0.174, 1632/60000 datapoints
2025-03-06 19:23:37,732 - INFO - training batch 101, loss: 0.513, 3232/60000 datapoints
2025-03-06 19:23:37,935 - INFO - training batch 151, loss: 0.509, 4832/60000 datapoints
2025-03-06 19:23:38,137 - INFO - training batch 201, loss: 0.232, 6432/60000 datapoints
2025-03-06 19:23:38,356 - INFO - training batch 251, loss: 0.224, 8032/60000 datapoints
2025-03-06 19:23:38,581 - INFO - training batch 301, loss: 0.310, 9632/60000 datapoints
2025-03-06 19:23:38,797 - INFO - training batch 351, loss: 0.148, 11232/60000 datapoints
2025-03-06 19:23:38,997 - INFO - training batch 401, loss: 0.557, 12832/60000 datapoints
2025-03-06 19:23:39,198 - INFO - training batch 451, loss: 0.211, 14432/60000 datapoints
2025-03-06 19:23:39,395 - INFO - training batch 501, loss: 0.349, 16032/60000 datapoints
2025-03-06 19:23:39,590 - INFO - training batch 551, loss: 0.260, 17632/60000 datapoints
2025-03-06 19:23:39,788 - INFO - training batch 601, loss: 0.211, 19232/60000 datapoints
2025-03-06 19:23:39,984 - INFO - training batch 651, loss: 0.291, 20832/60000 datapoints
2025-03-06 19:23:40,178 - INFO - training batch 701, loss: 0.510, 22432/60000 datapoints
2025-03-06 19:23:40,373 - INFO - training batch 751, loss: 0.316, 24032/60000 datapoints
2025-03-06 19:23:40,569 - INFO - training batch 801, loss: 0.303, 25632/60000 datapoints
2025-03-06 19:23:40,767 - INFO - training batch 851, loss: 0.303, 27232/60000 datapoints
2025-03-06 19:23:40,972 - INFO - training batch 901, loss: 0.214, 28832/60000 datapoints
2025-03-06 19:23:41,172 - INFO - training batch 951, loss: 0.311, 30432/60000 datapoints
2025-03-06 19:23:41,371 - INFO - training batch 1001, loss: 0.261, 32032/60000 datapoints
2025-03-06 19:23:41,564 - INFO - training batch 1051, loss: 0.566, 33632/60000 datapoints
2025-03-06 19:23:41,761 - INFO - training batch 1101, loss: 0.524, 35232/60000 datapoints
2025-03-06 19:23:41,961 - INFO - training batch 1151, loss: 0.144, 36832/60000 datapoints
2025-03-06 19:23:42,156 - INFO - training batch 1201, loss: 0.654, 38432/60000 datapoints
2025-03-06 19:23:42,351 - INFO - training batch 1251, loss: 0.122, 40032/60000 datapoints
2025-03-06 19:23:42,546 - INFO - training batch 1301, loss: 0.510, 41632/60000 datapoints
2025-03-06 19:23:42,743 - INFO - training batch 1351, loss: 0.514, 43232/60000 datapoints
2025-03-06 19:23:42,938 - INFO - training batch 1401, loss: 0.262, 44832/60000 datapoints
2025-03-06 19:23:43,133 - INFO - training batch 1451, loss: 0.707, 46432/60000 datapoints
2025-03-06 19:23:43,349 - INFO - training batch 1501, loss: 0.354, 48032/60000 datapoints
2025-03-06 19:23:43,545 - INFO - training batch 1551, loss: 0.399, 49632/60000 datapoints
2025-03-06 19:23:43,744 - INFO - training batch 1601, loss: 0.354, 51232/60000 datapoints
2025-03-06 19:23:43,947 - INFO - training batch 1651, loss: 0.319, 52832/60000 datapoints
2025-03-06 19:23:44,151 - INFO - training batch 1701, loss: 0.203, 54432/60000 datapoints
2025-03-06 19:23:44,348 - INFO - training batch 1751, loss: 0.262, 56032/60000 datapoints
2025-03-06 19:23:44,544 - INFO - training batch 1801, loss: 0.245, 57632/60000 datapoints
2025-03-06 19:23:44,740 - INFO - training batch 1851, loss: 0.076, 59232/60000 datapoints
2025-03-06 19:23:44,841 - INFO - validation batch 1, loss: 0.339, 32/10016 datapoints
2025-03-06 19:23:45,001 - INFO - validation batch 51, loss: 0.257, 1632/10016 datapoints
2025-03-06 19:23:45,154 - INFO - validation batch 101, loss: 0.548, 3232/10016 datapoints
2025-03-06 19:23:45,310 - INFO - validation batch 151, loss: 0.661, 4832/10016 datapoints
2025-03-06 19:23:45,462 - INFO - validation batch 201, loss: 0.217, 6432/10016 datapoints
2025-03-06 19:23:45,616 - INFO - validation batch 251, loss: 0.308, 8032/10016 datapoints
2025-03-06 19:23:45,769 - INFO - validation batch 301, loss: 0.356, 9632/10016 datapoints
2025-03-06 19:23:45,805 - INFO - Epoch 305/800 done.
2025-03-06 19:23:45,805 - INFO - Final validation performance:
Loss: 0.384, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:23:45,806 - INFO - Beginning epoch 306/800
2025-03-06 19:23:45,813 - INFO - training batch 1, loss: 0.540, 32/60000 datapoints
2025-03-06 19:23:46,011 - INFO - training batch 51, loss: 0.312, 1632/60000 datapoints
2025-03-06 19:23:46,216 - INFO - training batch 101, loss: 0.111, 3232/60000 datapoints
2025-03-06 19:23:46,408 - INFO - training batch 151, loss: 0.359, 4832/60000 datapoints
2025-03-06 19:23:46,599 - INFO - training batch 201, loss: 0.740, 6432/60000 datapoints
2025-03-06 19:23:46,799 - INFO - training batch 251, loss: 0.789, 8032/60000 datapoints
2025-03-06 19:23:46,994 - INFO - training batch 301, loss: 0.962, 9632/60000 datapoints
2025-03-06 19:23:47,194 - INFO - training batch 351, loss: 0.305, 11232/60000 datapoints
2025-03-06 19:23:47,388 - INFO - training batch 401, loss: 0.342, 12832/60000 datapoints
2025-03-06 19:23:47,580 - INFO - training batch 451, loss: 0.528, 14432/60000 datapoints
2025-03-06 19:23:47,777 - INFO - training batch 501, loss: 0.310, 16032/60000 datapoints
2025-03-06 19:23:47,970 - INFO - training batch 551, loss: 0.487, 17632/60000 datapoints
2025-03-06 19:23:48,194 - INFO - training batch 601, loss: 0.442, 19232/60000 datapoints
2025-03-06 19:23:48,389 - INFO - training batch 651, loss: 0.100, 20832/60000 datapoints
2025-03-06 19:23:48,581 - INFO - training batch 701, loss: 0.162, 22432/60000 datapoints
2025-03-06 19:23:48,776 - INFO - training batch 751, loss: 0.462, 24032/60000 datapoints
2025-03-06 19:23:48,968 - INFO - training batch 801, loss: 0.398, 25632/60000 datapoints
2025-03-06 19:23:49,164 - INFO - training batch 851, loss: 0.173, 27232/60000 datapoints
2025-03-06 19:23:49,359 - INFO - training batch 901, loss: 0.793, 28832/60000 datapoints
2025-03-06 19:23:49,551 - INFO - training batch 951, loss: 0.202, 30432/60000 datapoints
2025-03-06 19:23:49,747 - INFO - training batch 1001, loss: 0.273, 32032/60000 datapoints
2025-03-06 19:23:49,940 - INFO - training batch 1051, loss: 0.225, 33632/60000 datapoints
2025-03-06 19:23:50,133 - INFO - training batch 1101, loss: 0.579, 35232/60000 datapoints
2025-03-06 19:23:50,325 - INFO - training batch 1151, loss: 0.655, 36832/60000 datapoints
2025-03-06 19:23:50,516 - INFO - training batch 1201, loss: 0.194, 38432/60000 datapoints
2025-03-06 19:23:50,710 - INFO - training batch 1251, loss: 0.240, 40032/60000 datapoints
2025-03-06 19:23:50,902 - INFO - training batch 1301, loss: 0.153, 41632/60000 datapoints
2025-03-06 19:23:51,099 - INFO - training batch 1351, loss: 0.321, 43232/60000 datapoints
2025-03-06 19:23:51,291 - INFO - training batch 1401, loss: 0.383, 44832/60000 datapoints
2025-03-06 19:23:51,485 - INFO - training batch 1451, loss: 0.325, 46432/60000 datapoints
2025-03-06 19:23:51,679 - INFO - training batch 1501, loss: 0.237, 48032/60000 datapoints
2025-03-06 19:23:51,872 - INFO - training batch 1551, loss: 0.300, 49632/60000 datapoints
2025-03-06 19:23:52,063 - INFO - training batch 1601, loss: 0.352, 51232/60000 datapoints
2025-03-06 19:23:52,260 - INFO - training batch 1651, loss: 0.273, 52832/60000 datapoints
2025-03-06 19:23:52,457 - INFO - training batch 1701, loss: 0.279, 54432/60000 datapoints
2025-03-06 19:23:52,661 - INFO - training batch 1751, loss: 0.212, 56032/60000 datapoints
2025-03-06 19:23:52,858 - INFO - training batch 1801, loss: 0.241, 57632/60000 datapoints
2025-03-06 19:23:53,055 - INFO - training batch 1851, loss: 0.382, 59232/60000 datapoints
2025-03-06 19:23:53,157 - INFO - validation batch 1, loss: 0.259, 32/10016 datapoints
2025-03-06 19:23:53,330 - INFO - validation batch 51, loss: 0.342, 1632/10016 datapoints
2025-03-06 19:23:53,486 - INFO - validation batch 101, loss: 0.172, 3232/10016 datapoints
2025-03-06 19:23:53,645 - INFO - validation batch 151, loss: 0.142, 4832/10016 datapoints
2025-03-06 19:23:53,798 - INFO - validation batch 201, loss: 0.432, 6432/10016 datapoints
2025-03-06 19:23:53,951 - INFO - validation batch 251, loss: 0.444, 8032/10016 datapoints
2025-03-06 19:23:54,106 - INFO - validation batch 301, loss: 0.370, 9632/10016 datapoints
2025-03-06 19:23:54,144 - INFO - Epoch 306/800 done.
2025-03-06 19:23:54,144 - INFO - Final validation performance:
Loss: 0.309, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:23:54,144 - INFO - Beginning epoch 307/800
2025-03-06 19:23:54,151 - INFO - training batch 1, loss: 0.278, 32/60000 datapoints
2025-03-06 19:23:54,347 - INFO - training batch 51, loss: 0.138, 1632/60000 datapoints
2025-03-06 19:23:54,545 - INFO - training batch 101, loss: 0.488, 3232/60000 datapoints
2025-03-06 19:23:54,751 - INFO - training batch 151, loss: 0.292, 4832/60000 datapoints
2025-03-06 19:23:54,949 - INFO - training batch 201, loss: 0.153, 6432/60000 datapoints
2025-03-06 19:23:55,156 - INFO - training batch 251, loss: 0.232, 8032/60000 datapoints
2025-03-06 19:23:55,355 - INFO - training batch 301, loss: 0.131, 9632/60000 datapoints
2025-03-06 19:23:55,552 - INFO - training batch 351, loss: 0.345, 11232/60000 datapoints
2025-03-06 19:23:55,748 - INFO - training batch 401, loss: 0.254, 12832/60000 datapoints
2025-03-06 19:23:55,943 - INFO - training batch 451, loss: 0.335, 14432/60000 datapoints
2025-03-06 19:23:56,137 - INFO - training batch 501, loss: 0.255, 16032/60000 datapoints
2025-03-06 19:23:56,335 - INFO - training batch 551, loss: 0.247, 17632/60000 datapoints
2025-03-06 19:23:56,531 - INFO - training batch 601, loss: 0.137, 19232/60000 datapoints
2025-03-06 19:23:56,729 - INFO - training batch 651, loss: 0.163, 20832/60000 datapoints
2025-03-06 19:23:56,924 - INFO - training batch 701, loss: 0.358, 22432/60000 datapoints
2025-03-06 19:23:57,120 - INFO - training batch 751, loss: 0.507, 24032/60000 datapoints
2025-03-06 19:23:57,315 - INFO - training batch 801, loss: 0.268, 25632/60000 datapoints
2025-03-06 19:23:57,511 - INFO - training batch 851, loss: 0.704, 27232/60000 datapoints
2025-03-06 19:23:57,709 - INFO - training batch 901, loss: 0.341, 28832/60000 datapoints
2025-03-06 19:23:57,923 - INFO - training batch 951, loss: 0.266, 30432/60000 datapoints
2025-03-06 19:23:58,119 - INFO - training batch 1001, loss: 0.250, 32032/60000 datapoints
2025-03-06 19:23:58,314 - INFO - training batch 1051, loss: 0.179, 33632/60000 datapoints
2025-03-06 19:23:58,512 - INFO - training batch 1101, loss: 0.204, 35232/60000 datapoints
2025-03-06 19:23:58,712 - INFO - training batch 1151, loss: 0.406, 36832/60000 datapoints
2025-03-06 19:23:58,910 - INFO - training batch 1201, loss: 0.187, 38432/60000 datapoints
2025-03-06 19:23:59,108 - INFO - training batch 1251, loss: 0.487, 40032/60000 datapoints
2025-03-06 19:23:59,300 - INFO - training batch 1301, loss: 0.373, 41632/60000 datapoints
2025-03-06 19:23:59,495 - INFO - training batch 1351, loss: 0.159, 43232/60000 datapoints
2025-03-06 19:23:59,694 - INFO - training batch 1401, loss: 0.233, 44832/60000 datapoints
2025-03-06 19:23:59,892 - INFO - training batch 1451, loss: 0.102, 46432/60000 datapoints
2025-03-06 19:24:00,089 - INFO - training batch 1501, loss: 0.163, 48032/60000 datapoints
2025-03-06 19:24:00,284 - INFO - training batch 1551, loss: 0.328, 49632/60000 datapoints
2025-03-06 19:24:00,478 - INFO - training batch 1601, loss: 0.408, 51232/60000 datapoints
2025-03-06 19:24:00,675 - INFO - training batch 1651, loss: 0.566, 52832/60000 datapoints
2025-03-06 19:24:00,868 - INFO - training batch 1701, loss: 0.172, 54432/60000 datapoints
2025-03-06 19:24:01,064 - INFO - training batch 1751, loss: 0.494, 56032/60000 datapoints
2025-03-06 19:24:01,257 - INFO - training batch 1801, loss: 0.453, 57632/60000 datapoints
2025-03-06 19:24:01,454 - INFO - training batch 1851, loss: 0.300, 59232/60000 datapoints
2025-03-06 19:24:01,557 - INFO - validation batch 1, loss: 0.411, 32/10016 datapoints
2025-03-06 19:24:01,711 - INFO - validation batch 51, loss: 0.323, 1632/10016 datapoints
2025-03-06 19:24:01,865 - INFO - validation batch 101, loss: 0.236, 3232/10016 datapoints
2025-03-06 19:24:02,017 - INFO - validation batch 151, loss: 0.404, 4832/10016 datapoints
2025-03-06 19:24:02,171 - INFO - validation batch 201, loss: 0.331, 6432/10016 datapoints
2025-03-06 19:24:02,324 - INFO - validation batch 251, loss: 0.203, 8032/10016 datapoints
2025-03-06 19:24:02,477 - INFO - validation batch 301, loss: 0.212, 9632/10016 datapoints
2025-03-06 19:24:02,512 - INFO - Epoch 307/800 done.
2025-03-06 19:24:02,513 - INFO - Final validation performance:
Loss: 0.303, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:24:02,513 - INFO - Beginning epoch 308/800
2025-03-06 19:24:02,520 - INFO - training batch 1, loss: 0.412, 32/60000 datapoints
2025-03-06 19:24:02,724 - INFO - training batch 51, loss: 0.504, 1632/60000 datapoints
2025-03-06 19:24:02,923 - INFO - training batch 101, loss: 0.151, 3232/60000 datapoints
2025-03-06 19:24:03,118 - INFO - training batch 151, loss: 0.113, 4832/60000 datapoints
2025-03-06 19:24:03,316 - INFO - training batch 201, loss: 0.433, 6432/60000 datapoints
2025-03-06 19:24:03,529 - INFO - training batch 251, loss: 0.200, 8032/60000 datapoints
2025-03-06 19:24:03,735 - INFO - training batch 301, loss: 0.381, 9632/60000 datapoints
2025-03-06 19:24:03,926 - INFO - training batch 351, loss: 0.279, 11232/60000 datapoints
2025-03-06 19:24:04,120 - INFO - training batch 401, loss: 0.344, 12832/60000 datapoints
2025-03-06 19:24:04,313 - INFO - training batch 451, loss: 0.198, 14432/60000 datapoints
2025-03-06 19:24:04,504 - INFO - training batch 501, loss: 0.152, 16032/60000 datapoints
2025-03-06 19:24:04,698 - INFO - training batch 551, loss: 0.261, 17632/60000 datapoints
2025-03-06 19:24:04,894 - INFO - training batch 601, loss: 0.532, 19232/60000 datapoints
2025-03-06 19:24:05,086 - INFO - training batch 651, loss: 0.494, 20832/60000 datapoints
2025-03-06 19:24:05,280 - INFO - training batch 701, loss: 0.231, 22432/60000 datapoints
2025-03-06 19:24:05,471 - INFO - training batch 751, loss: 0.291, 24032/60000 datapoints
2025-03-06 19:24:05,664 - INFO - training batch 801, loss: 0.345, 25632/60000 datapoints
2025-03-06 19:24:05,854 - INFO - training batch 851, loss: 0.239, 27232/60000 datapoints
2025-03-06 19:24:06,044 - INFO - training batch 901, loss: 0.306, 28832/60000 datapoints
2025-03-06 19:24:06,236 - INFO - training batch 951, loss: 0.101, 30432/60000 datapoints
2025-03-06 19:24:06,428 - INFO - training batch 1001, loss: 0.370, 32032/60000 datapoints
2025-03-06 19:24:06,621 - INFO - training batch 1051, loss: 0.481, 33632/60000 datapoints
2025-03-06 19:24:06,813 - INFO - training batch 1101, loss: 0.202, 35232/60000 datapoints
2025-03-06 19:24:07,004 - INFO - training batch 1151, loss: 0.265, 36832/60000 datapoints
2025-03-06 19:24:07,198 - INFO - training batch 1201, loss: 0.333, 38432/60000 datapoints
2025-03-06 19:24:07,390 - INFO - training batch 1251, loss: 0.340, 40032/60000 datapoints
2025-03-06 19:24:07,580 - INFO - training batch 1301, loss: 0.231, 41632/60000 datapoints
2025-03-06 19:24:07,776 - INFO - training batch 1351, loss: 0.267, 43232/60000 datapoints
2025-03-06 19:24:07,967 - INFO - training batch 1401, loss: 0.484, 44832/60000 datapoints
2025-03-06 19:24:08,157 - INFO - training batch 1451, loss: 0.602, 46432/60000 datapoints
2025-03-06 19:24:08,353 - INFO - training batch 1501, loss: 0.163, 48032/60000 datapoints
2025-03-06 19:24:08,547 - INFO - training batch 1551, loss: 0.401, 49632/60000 datapoints
2025-03-06 19:24:08,740 - INFO - training batch 1601, loss: 0.203, 51232/60000 datapoints
2025-03-06 19:24:08,932 - INFO - training batch 1651, loss: 0.276, 52832/60000 datapoints
2025-03-06 19:24:09,132 - INFO - training batch 1701, loss: 0.131, 54432/60000 datapoints
2025-03-06 19:24:09,328 - INFO - training batch 1751, loss: 0.300, 56032/60000 datapoints
2025-03-06 19:24:09,546 - INFO - training batch 1801, loss: 0.160, 57632/60000 datapoints
2025-03-06 19:24:09,741 - INFO - training batch 1851, loss: 0.364, 59232/60000 datapoints
2025-03-06 19:24:09,839 - INFO - validation batch 1, loss: 0.155, 32/10016 datapoints
2025-03-06 19:24:09,992 - INFO - validation batch 51, loss: 0.617, 1632/10016 datapoints
2025-03-06 19:24:10,141 - INFO - validation batch 101, loss: 0.321, 3232/10016 datapoints
2025-03-06 19:24:10,293 - INFO - validation batch 151, loss: 0.287, 4832/10016 datapoints
2025-03-06 19:24:10,443 - INFO - validation batch 201, loss: 0.431, 6432/10016 datapoints
2025-03-06 19:24:10,593 - INFO - validation batch 251, loss: 0.565, 8032/10016 datapoints
2025-03-06 19:24:10,749 - INFO - validation batch 301, loss: 0.316, 9632/10016 datapoints
2025-03-06 19:24:10,785 - INFO - Epoch 308/800 done.
2025-03-06 19:24:10,785 - INFO - Final validation performance:
Loss: 0.385, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:24:10,786 - INFO - Beginning epoch 309/800
2025-03-06 19:24:10,792 - INFO - training batch 1, loss: 0.378, 32/60000 datapoints
2025-03-06 19:24:10,983 - INFO - training batch 51, loss: 0.186, 1632/60000 datapoints
2025-03-06 19:24:11,178 - INFO - training batch 101, loss: 0.394, 3232/60000 datapoints
2025-03-06 19:24:11,378 - INFO - training batch 151, loss: 0.212, 4832/60000 datapoints
2025-03-06 19:24:11,576 - INFO - training batch 201, loss: 0.099, 6432/60000 datapoints
2025-03-06 19:24:11,779 - INFO - training batch 251, loss: 0.356, 8032/60000 datapoints
2025-03-06 19:24:11,975 - INFO - training batch 301, loss: 0.552, 9632/60000 datapoints
2025-03-06 19:24:12,171 - INFO - training batch 351, loss: 0.358, 11232/60000 datapoints
2025-03-06 19:24:12,368 - INFO - training batch 401, loss: 0.251, 12832/60000 datapoints
2025-03-06 19:24:12,558 - INFO - training batch 451, loss: 0.358, 14432/60000 datapoints
2025-03-06 19:24:12,753 - INFO - training batch 501, loss: 0.451, 16032/60000 datapoints
2025-03-06 19:24:12,944 - INFO - training batch 551, loss: 0.151, 17632/60000 datapoints
2025-03-06 19:24:13,136 - INFO - training batch 601, loss: 0.165, 19232/60000 datapoints
2025-03-06 19:24:13,331 - INFO - training batch 651, loss: 0.146, 20832/60000 datapoints
2025-03-06 19:24:13,544 - INFO - training batch 701, loss: 0.289, 22432/60000 datapoints
2025-03-06 19:24:13,747 - INFO - training batch 751, loss: 0.283, 24032/60000 datapoints
2025-03-06 19:24:13,958 - INFO - training batch 801, loss: 0.474, 25632/60000 datapoints
2025-03-06 19:24:14,148 - INFO - training batch 851, loss: 0.122, 27232/60000 datapoints
2025-03-06 19:24:14,339 - INFO - training batch 901, loss: 0.294, 28832/60000 datapoints
2025-03-06 19:24:14,532 - INFO - training batch 951, loss: 0.253, 30432/60000 datapoints
2025-03-06 19:24:14,726 - INFO - training batch 1001, loss: 0.394, 32032/60000 datapoints
2025-03-06 19:24:14,922 - INFO - training batch 1051, loss: 0.213, 33632/60000 datapoints
2025-03-06 19:24:15,115 - INFO - training batch 1101, loss: 0.210, 35232/60000 datapoints
2025-03-06 19:24:15,308 - INFO - training batch 1151, loss: 0.206, 36832/60000 datapoints
2025-03-06 19:24:15,500 - INFO - training batch 1201, loss: 0.351, 38432/60000 datapoints
2025-03-06 19:24:15,693 - INFO - training batch 1251, loss: 0.481, 40032/60000 datapoints
2025-03-06 19:24:15,886 - INFO - training batch 1301, loss: 0.334, 41632/60000 datapoints
2025-03-06 19:24:16,077 - INFO - training batch 1351, loss: 0.270, 43232/60000 datapoints
2025-03-06 19:24:16,269 - INFO - training batch 1401, loss: 0.241, 44832/60000 datapoints
2025-03-06 19:24:16,460 - INFO - training batch 1451, loss: 0.268, 46432/60000 datapoints
2025-03-06 19:24:16,655 - INFO - training batch 1501, loss: 0.201, 48032/60000 datapoints
2025-03-06 19:24:16,848 - INFO - training batch 1551, loss: 0.721, 49632/60000 datapoints
2025-03-06 19:24:17,040 - INFO - training batch 1601, loss: 0.476, 51232/60000 datapoints
2025-03-06 19:24:17,235 - INFO - training batch 1651, loss: 0.324, 52832/60000 datapoints
2025-03-06 19:24:17,427 - INFO - training batch 1701, loss: 0.425, 54432/60000 datapoints
2025-03-06 19:24:17,621 - INFO - training batch 1751, loss: 0.312, 56032/60000 datapoints
2025-03-06 19:24:17,814 - INFO - training batch 1801, loss: 0.193, 57632/60000 datapoints
2025-03-06 19:24:18,006 - INFO - training batch 1851, loss: 0.365, 59232/60000 datapoints
2025-03-06 19:24:18,104 - INFO - validation batch 1, loss: 0.338, 32/10016 datapoints
2025-03-06 19:24:18,254 - INFO - validation batch 51, loss: 0.216, 1632/10016 datapoints
2025-03-06 19:24:18,405 - INFO - validation batch 101, loss: 0.156, 3232/10016 datapoints
2025-03-06 19:24:18,554 - INFO - validation batch 151, loss: 0.373, 4832/10016 datapoints
2025-03-06 19:24:18,706 - INFO - validation batch 201, loss: 0.617, 6432/10016 datapoints
2025-03-06 19:24:18,856 - INFO - validation batch 251, loss: 0.123, 8032/10016 datapoints
2025-03-06 19:24:19,006 - INFO - validation batch 301, loss: 0.415, 9632/10016 datapoints
2025-03-06 19:24:19,041 - INFO - Epoch 309/800 done.
2025-03-06 19:24:19,041 - INFO - Final validation performance:
Loss: 0.320, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:24:19,042 - INFO - Beginning epoch 310/800
2025-03-06 19:24:19,048 - INFO - training batch 1, loss: 0.211, 32/60000 datapoints
2025-03-06 19:24:19,244 - INFO - training batch 51, loss: 0.216, 1632/60000 datapoints
2025-03-06 19:24:19,435 - INFO - training batch 101, loss: 0.299, 3232/60000 datapoints
2025-03-06 19:24:19,634 - INFO - training batch 151, loss: 0.546, 4832/60000 datapoints
2025-03-06 19:24:19,833 - INFO - training batch 201, loss: 0.138, 6432/60000 datapoints
2025-03-06 19:24:20,024 - INFO - training batch 251, loss: 0.486, 8032/60000 datapoints
2025-03-06 19:24:20,221 - INFO - training batch 301, loss: 0.156, 9632/60000 datapoints
2025-03-06 19:24:20,415 - INFO - training batch 351, loss: 0.143, 11232/60000 datapoints
2025-03-06 19:24:20,609 - INFO - training batch 401, loss: 0.248, 12832/60000 datapoints
2025-03-06 19:24:20,805 - INFO - training batch 451, loss: 0.182, 14432/60000 datapoints
2025-03-06 19:24:20,996 - INFO - training batch 501, loss: 0.860, 16032/60000 datapoints
2025-03-06 19:24:21,189 - INFO - training batch 551, loss: 0.268, 17632/60000 datapoints
2025-03-06 19:24:21,380 - INFO - training batch 601, loss: 0.396, 19232/60000 datapoints
2025-03-06 19:24:21,570 - INFO - training batch 651, loss: 0.222, 20832/60000 datapoints
2025-03-06 19:24:21,763 - INFO - training batch 701, loss: 0.354, 22432/60000 datapoints
2025-03-06 19:24:21,956 - INFO - training batch 751, loss: 0.242, 24032/60000 datapoints
2025-03-06 19:24:22,147 - INFO - training batch 801, loss: 0.187, 25632/60000 datapoints
2025-03-06 19:24:22,339 - INFO - training batch 851, loss: 0.362, 27232/60000 datapoints
2025-03-06 19:24:22,530 - INFO - training batch 901, loss: 0.320, 28832/60000 datapoints
2025-03-06 19:24:22,725 - INFO - training batch 951, loss: 0.152, 30432/60000 datapoints
2025-03-06 19:24:22,917 - INFO - training batch 1001, loss: 0.133, 32032/60000 datapoints
2025-03-06 19:24:23,108 - INFO - training batch 1051, loss: 0.423, 33632/60000 datapoints
2025-03-06 19:24:23,303 - INFO - training batch 1101, loss: 0.265, 35232/60000 datapoints
2025-03-06 19:24:23,502 - INFO - training batch 1151, loss: 0.227, 36832/60000 datapoints
2025-03-06 19:24:23,714 - INFO - training batch 1201, loss: 0.496, 38432/60000 datapoints
2025-03-06 19:24:23,907 - INFO - training batch 1251, loss: 0.394, 40032/60000 datapoints
2025-03-06 19:24:24,101 - INFO - training batch 1301, loss: 0.239, 41632/60000 datapoints
2025-03-06 19:24:24,294 - INFO - training batch 1351, loss: 0.350, 43232/60000 datapoints
2025-03-06 19:24:24,487 - INFO - training batch 1401, loss: 0.222, 44832/60000 datapoints
2025-03-06 19:24:24,679 - INFO - training batch 1451, loss: 0.583, 46432/60000 datapoints
2025-03-06 19:24:24,873 - INFO - training batch 1501, loss: 0.496, 48032/60000 datapoints
2025-03-06 19:24:25,070 - INFO - training batch 1551, loss: 0.110, 49632/60000 datapoints
2025-03-06 19:24:25,263 - INFO - training batch 1601, loss: 0.319, 51232/60000 datapoints
2025-03-06 19:24:25,455 - INFO - training batch 1651, loss: 0.149, 52832/60000 datapoints
2025-03-06 19:24:25,649 - INFO - training batch 1701, loss: 0.268, 54432/60000 datapoints
2025-03-06 19:24:25,843 - INFO - training batch 1751, loss: 0.260, 56032/60000 datapoints
2025-03-06 19:24:26,034 - INFO - training batch 1801, loss: 0.479, 57632/60000 datapoints
2025-03-06 19:24:26,224 - INFO - training batch 1851, loss: 0.453, 59232/60000 datapoints
2025-03-06 19:24:26,322 - INFO - validation batch 1, loss: 0.294, 32/10016 datapoints
2025-03-06 19:24:26,476 - INFO - validation batch 51, loss: 0.643, 1632/10016 datapoints
2025-03-06 19:24:26,628 - INFO - validation batch 101, loss: 0.225, 3232/10016 datapoints
2025-03-06 19:24:26,781 - INFO - validation batch 151, loss: 0.360, 4832/10016 datapoints
2025-03-06 19:24:26,935 - INFO - validation batch 201, loss: 0.244, 6432/10016 datapoints
2025-03-06 19:24:27,087 - INFO - validation batch 251, loss: 0.292, 8032/10016 datapoints
2025-03-06 19:24:27,238 - INFO - validation batch 301, loss: 0.182, 9632/10016 datapoints
2025-03-06 19:24:27,273 - INFO - Epoch 310/800 done.
2025-03-06 19:24:27,273 - INFO - Final validation performance:
Loss: 0.320, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:24:27,274 - INFO - Beginning epoch 311/800
2025-03-06 19:24:27,280 - INFO - training batch 1, loss: 0.317, 32/60000 datapoints
2025-03-06 19:24:27,488 - INFO - training batch 51, loss: 0.204, 1632/60000 datapoints
2025-03-06 19:24:27,682 - INFO - training batch 101, loss: 0.135, 3232/60000 datapoints
2025-03-06 19:24:27,879 - INFO - training batch 151, loss: 0.205, 4832/60000 datapoints
2025-03-06 19:24:28,076 - INFO - training batch 201, loss: 0.408, 6432/60000 datapoints
2025-03-06 19:24:28,271 - INFO - training batch 251, loss: 0.295, 8032/60000 datapoints
2025-03-06 19:24:28,464 - INFO - training batch 301, loss: 0.377, 9632/60000 datapoints
2025-03-06 19:24:28,661 - INFO - training batch 351, loss: 0.215, 11232/60000 datapoints
2025-03-06 19:24:28,854 - INFO - training batch 401, loss: 0.290, 12832/60000 datapoints
2025-03-06 19:24:29,046 - INFO - training batch 451, loss: 0.286, 14432/60000 datapoints
2025-03-06 19:24:29,241 - INFO - training batch 501, loss: 0.236, 16032/60000 datapoints
2025-03-06 19:24:29,433 - INFO - training batch 551, loss: 0.130, 17632/60000 datapoints
2025-03-06 19:24:29,628 - INFO - training batch 601, loss: 0.135, 19232/60000 datapoints
2025-03-06 19:24:29,821 - INFO - training batch 651, loss: 0.255, 20832/60000 datapoints
2025-03-06 19:24:30,014 - INFO - training batch 701, loss: 0.397, 22432/60000 datapoints
2025-03-06 19:24:30,208 - INFO - training batch 751, loss: 0.421, 24032/60000 datapoints
2025-03-06 19:24:30,400 - INFO - training batch 801, loss: 0.569, 25632/60000 datapoints
2025-03-06 19:24:30,591 - INFO - training batch 851, loss: 0.379, 27232/60000 datapoints
2025-03-06 19:24:30,783 - INFO - training batch 901, loss: 0.410, 28832/60000 datapoints
2025-03-06 19:24:30,974 - INFO - training batch 951, loss: 0.195, 30432/60000 datapoints
2025-03-06 19:24:31,166 - INFO - training batch 1001, loss: 0.176, 32032/60000 datapoints
2025-03-06 19:24:31,358 - INFO - training batch 1051, loss: 0.191, 33632/60000 datapoints
2025-03-06 19:24:31,548 - INFO - training batch 1101, loss: 0.509, 35232/60000 datapoints
2025-03-06 19:24:31,740 - INFO - training batch 1151, loss: 0.820, 36832/60000 datapoints
2025-03-06 19:24:31,933 - INFO - training batch 1201, loss: 0.112, 38432/60000 datapoints
2025-03-06 19:24:32,125 - INFO - training batch 1251, loss: 0.305, 40032/60000 datapoints
2025-03-06 19:24:32,314 - INFO - training batch 1301, loss: 0.291, 41632/60000 datapoints
2025-03-06 19:24:32,510 - INFO - training batch 1351, loss: 0.244, 43232/60000 datapoints
2025-03-06 19:24:32,707 - INFO - training batch 1401, loss: 0.188, 44832/60000 datapoints
2025-03-06 19:24:32,900 - INFO - training batch 1451, loss: 0.242, 46432/60000 datapoints
2025-03-06 19:24:33,094 - INFO - training batch 1501, loss: 0.553, 48032/60000 datapoints
2025-03-06 19:24:33,290 - INFO - training batch 1551, loss: 0.172, 49632/60000 datapoints
2025-03-06 19:24:33,489 - INFO - training batch 1601, loss: 0.174, 51232/60000 datapoints
2025-03-06 19:24:33,710 - INFO - training batch 1651, loss: 0.142, 52832/60000 datapoints
2025-03-06 19:24:33,905 - INFO - training batch 1701, loss: 0.136, 54432/60000 datapoints
2025-03-06 19:24:34,098 - INFO - training batch 1751, loss: 0.369, 56032/60000 datapoints
2025-03-06 19:24:34,291 - INFO - training batch 1801, loss: 0.362, 57632/60000 datapoints
2025-03-06 19:24:34,488 - INFO - training batch 1851, loss: 0.427, 59232/60000 datapoints
2025-03-06 19:24:34,589 - INFO - validation batch 1, loss: 0.303, 32/10016 datapoints
2025-03-06 19:24:34,764 - INFO - validation batch 51, loss: 0.206, 1632/10016 datapoints
2025-03-06 19:24:34,943 - INFO - validation batch 101, loss: 0.093, 3232/10016 datapoints
2025-03-06 19:24:35,119 - INFO - validation batch 151, loss: 0.272, 4832/10016 datapoints
2025-03-06 19:24:35,274 - INFO - validation batch 201, loss: 0.165, 6432/10016 datapoints
2025-03-06 19:24:35,426 - INFO - validation batch 251, loss: 0.195, 8032/10016 datapoints
2025-03-06 19:24:35,579 - INFO - validation batch 301, loss: 0.381, 9632/10016 datapoints
2025-03-06 19:24:35,618 - INFO - Epoch 311/800 done.
2025-03-06 19:24:35,618 - INFO - Final validation performance:
Loss: 0.231, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:24:35,619 - INFO - Beginning epoch 312/800
2025-03-06 19:24:35,625 - INFO - training batch 1, loss: 0.273, 32/60000 datapoints
2025-03-06 19:24:35,829 - INFO - training batch 51, loss: 0.380, 1632/60000 datapoints
2025-03-06 19:24:36,023 - INFO - training batch 101, loss: 0.203, 3232/60000 datapoints
2025-03-06 19:24:36,222 - INFO - training batch 151, loss: 0.446, 4832/60000 datapoints
2025-03-06 19:24:36,417 - INFO - training batch 201, loss: 0.329, 6432/60000 datapoints
2025-03-06 19:24:36,613 - INFO - training batch 251, loss: 0.352, 8032/60000 datapoints
2025-03-06 19:24:36,804 - INFO - training batch 301, loss: 0.223, 9632/60000 datapoints
2025-03-06 19:24:36,997 - INFO - training batch 351, loss: 0.147, 11232/60000 datapoints
2025-03-06 19:24:37,190 - INFO - training batch 401, loss: 0.167, 12832/60000 datapoints
2025-03-06 19:24:37,380 - INFO - training batch 451, loss: 0.110, 14432/60000 datapoints
2025-03-06 19:24:37,574 - INFO - training batch 501, loss: 0.175, 16032/60000 datapoints
2025-03-06 19:24:37,785 - INFO - training batch 551, loss: 0.153, 17632/60000 datapoints
2025-03-06 19:24:37,978 - INFO - training batch 601, loss: 0.250, 19232/60000 datapoints
2025-03-06 19:24:38,172 - INFO - training batch 651, loss: 0.122, 20832/60000 datapoints
2025-03-06 19:24:38,363 - INFO - training batch 701, loss: 0.229, 22432/60000 datapoints
2025-03-06 19:24:38,556 - INFO - training batch 751, loss: 0.178, 24032/60000 datapoints
2025-03-06 19:24:38,749 - INFO - training batch 801, loss: 0.497, 25632/60000 datapoints
2025-03-06 19:24:38,942 - INFO - training batch 851, loss: 0.192, 27232/60000 datapoints
2025-03-06 19:24:39,143 - INFO - training batch 901, loss: 0.474, 28832/60000 datapoints
2025-03-06 19:24:39,337 - INFO - training batch 951, loss: 0.224, 30432/60000 datapoints
2025-03-06 19:24:39,529 - INFO - training batch 1001, loss: 0.349, 32032/60000 datapoints
2025-03-06 19:24:39,723 - INFO - training batch 1051, loss: 0.356, 33632/60000 datapoints
2025-03-06 19:24:39,915 - INFO - training batch 1101, loss: 0.374, 35232/60000 datapoints
2025-03-06 19:24:40,108 - INFO - training batch 1151, loss: 0.497, 36832/60000 datapoints
2025-03-06 19:24:40,300 - INFO - training batch 1201, loss: 0.242, 38432/60000 datapoints
2025-03-06 19:24:40,493 - INFO - training batch 1251, loss: 0.455, 40032/60000 datapoints
2025-03-06 19:24:40,687 - INFO - training batch 1301, loss: 0.425, 41632/60000 datapoints
2025-03-06 19:24:40,879 - INFO - training batch 1351, loss: 0.278, 43232/60000 datapoints
2025-03-06 19:24:41,071 - INFO - training batch 1401, loss: 0.238, 44832/60000 datapoints
2025-03-06 19:24:41,264 - INFO - training batch 1451, loss: 0.187, 46432/60000 datapoints
2025-03-06 19:24:41,457 - INFO - training batch 1501, loss: 0.364, 48032/60000 datapoints
2025-03-06 19:24:41,653 - INFO - training batch 1551, loss: 0.228, 49632/60000 datapoints
2025-03-06 19:24:41,844 - INFO - training batch 1601, loss: 0.189, 51232/60000 datapoints
2025-03-06 19:24:42,041 - INFO - training batch 1651, loss: 0.294, 52832/60000 datapoints
2025-03-06 19:24:42,233 - INFO - training batch 1701, loss: 0.424, 54432/60000 datapoints
2025-03-06 19:24:42,424 - INFO - training batch 1751, loss: 0.360, 56032/60000 datapoints
2025-03-06 19:24:42,619 - INFO - training batch 1801, loss: 0.405, 57632/60000 datapoints
2025-03-06 19:24:42,810 - INFO - training batch 1851, loss: 0.509, 59232/60000 datapoints
2025-03-06 19:24:42,909 - INFO - validation batch 1, loss: 0.546, 32/10016 datapoints
2025-03-06 19:24:43,060 - INFO - validation batch 51, loss: 0.247, 1632/10016 datapoints
2025-03-06 19:24:43,209 - INFO - validation batch 101, loss: 0.168, 3232/10016 datapoints
2025-03-06 19:24:43,361 - INFO - validation batch 151, loss: 0.174, 4832/10016 datapoints
2025-03-06 19:24:43,512 - INFO - validation batch 201, loss: 0.117, 6432/10016 datapoints
2025-03-06 19:24:43,677 - INFO - validation batch 251, loss: 0.210, 8032/10016 datapoints
2025-03-06 19:24:43,837 - INFO - validation batch 301, loss: 0.125, 9632/10016 datapoints
2025-03-06 19:24:43,873 - INFO - Epoch 312/800 done.
2025-03-06 19:24:43,873 - INFO - Final validation performance:
Loss: 0.227, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:24:43,874 - INFO - Beginning epoch 313/800
2025-03-06 19:24:43,880 - INFO - training batch 1, loss: 0.188, 32/60000 datapoints
2025-03-06 19:24:44,074 - INFO - training batch 51, loss: 0.338, 1632/60000 datapoints
2025-03-06 19:24:44,274 - INFO - training batch 101, loss: 0.136, 3232/60000 datapoints
2025-03-06 19:24:44,469 - INFO - training batch 151, loss: 0.307, 4832/60000 datapoints
2025-03-06 19:24:44,664 - INFO - training batch 201, loss: 0.231, 6432/60000 datapoints
2025-03-06 19:24:44,862 - INFO - training batch 251, loss: 0.418, 8032/60000 datapoints
2025-03-06 19:24:45,064 - INFO - training batch 301, loss: 0.305, 9632/60000 datapoints
2025-03-06 19:24:45,262 - INFO - training batch 351, loss: 0.415, 11232/60000 datapoints
2025-03-06 19:24:45,455 - INFO - training batch 401, loss: 0.407, 12832/60000 datapoints
2025-03-06 19:24:45,650 - INFO - training batch 451, loss: 0.202, 14432/60000 datapoints
2025-03-06 19:24:45,840 - INFO - training batch 501, loss: 0.533, 16032/60000 datapoints
2025-03-06 19:24:46,032 - INFO - training batch 551, loss: 0.230, 17632/60000 datapoints
2025-03-06 19:24:46,224 - INFO - training batch 601, loss: 0.245, 19232/60000 datapoints
2025-03-06 19:24:46,416 - INFO - training batch 651, loss: 0.594, 20832/60000 datapoints
2025-03-06 19:24:46,609 - INFO - training batch 701, loss: 0.391, 22432/60000 datapoints
2025-03-06 19:24:46,804 - INFO - training batch 751, loss: 0.439, 24032/60000 datapoints
2025-03-06 19:24:46,996 - INFO - training batch 801, loss: 0.226, 25632/60000 datapoints
2025-03-06 19:24:47,190 - INFO - training batch 851, loss: 0.240, 27232/60000 datapoints
2025-03-06 19:24:47,385 - INFO - training batch 901, loss: 0.080, 28832/60000 datapoints
2025-03-06 19:24:47,577 - INFO - training batch 951, loss: 0.090, 30432/60000 datapoints
2025-03-06 19:24:47,773 - INFO - training batch 1001, loss: 0.408, 32032/60000 datapoints
2025-03-06 19:24:47,964 - INFO - training batch 1051, loss: 0.166, 33632/60000 datapoints
2025-03-06 19:24:48,162 - INFO - training batch 1101, loss: 0.463, 35232/60000 datapoints
2025-03-06 19:24:48,354 - INFO - training batch 1151, loss: 0.214, 36832/60000 datapoints
2025-03-06 19:24:48,546 - INFO - training batch 1201, loss: 0.262, 38432/60000 datapoints
2025-03-06 19:24:48,742 - INFO - training batch 1251, loss: 0.760, 40032/60000 datapoints
2025-03-06 19:24:48,934 - INFO - training batch 1301, loss: 0.345, 41632/60000 datapoints
2025-03-06 19:24:49,126 - INFO - training batch 1351, loss: 0.312, 43232/60000 datapoints
2025-03-06 19:24:49,322 - INFO - training batch 1401, loss: 0.266, 44832/60000 datapoints
2025-03-06 19:24:49,514 - INFO - training batch 1451, loss: 0.294, 46432/60000 datapoints
2025-03-06 19:24:49,710 - INFO - training batch 1501, loss: 0.380, 48032/60000 datapoints
2025-03-06 19:24:49,902 - INFO - training batch 1551, loss: 0.177, 49632/60000 datapoints
2025-03-06 19:24:50,096 - INFO - training batch 1601, loss: 0.226, 51232/60000 datapoints
2025-03-06 19:24:50,289 - INFO - training batch 1651, loss: 0.342, 52832/60000 datapoints
2025-03-06 19:24:50,483 - INFO - training batch 1701, loss: 0.503, 54432/60000 datapoints
2025-03-06 19:24:50,677 - INFO - training batch 1751, loss: 0.267, 56032/60000 datapoints
2025-03-06 19:24:50,869 - INFO - training batch 1801, loss: 0.391, 57632/60000 datapoints
2025-03-06 19:24:51,060 - INFO - training batch 1851, loss: 0.216, 59232/60000 datapoints
2025-03-06 19:24:51,160 - INFO - validation batch 1, loss: 0.081, 32/10016 datapoints
2025-03-06 19:24:51,313 - INFO - validation batch 51, loss: 0.302, 1632/10016 datapoints
2025-03-06 19:24:51,463 - INFO - validation batch 101, loss: 0.205, 3232/10016 datapoints
2025-03-06 19:24:51,613 - INFO - validation batch 151, loss: 0.479, 4832/10016 datapoints
2025-03-06 19:24:51,765 - INFO - validation batch 201, loss: 0.384, 6432/10016 datapoints
2025-03-06 19:24:51,915 - INFO - validation batch 251, loss: 0.301, 8032/10016 datapoints
2025-03-06 19:24:52,064 - INFO - validation batch 301, loss: 0.186, 9632/10016 datapoints
2025-03-06 19:24:52,099 - INFO - Epoch 313/800 done.
2025-03-06 19:24:52,099 - INFO - Final validation performance:
Loss: 0.277, top-1 acc: 0.913top-5 acc: 0.913
2025-03-06 19:24:52,100 - INFO - Beginning epoch 314/800
2025-03-06 19:24:52,106 - INFO - training batch 1, loss: 0.393, 32/60000 datapoints
2025-03-06 19:24:52,310 - INFO - training batch 51, loss: 0.156, 1632/60000 datapoints
2025-03-06 19:24:52,503 - INFO - training batch 101, loss: 0.432, 3232/60000 datapoints
2025-03-06 19:24:52,708 - INFO - training batch 151, loss: 0.300, 4832/60000 datapoints
2025-03-06 19:24:52,904 - INFO - training batch 201, loss: 0.161, 6432/60000 datapoints
2025-03-06 19:24:53,102 - INFO - training batch 251, loss: 0.272, 8032/60000 datapoints
2025-03-06 19:24:53,304 - INFO - training batch 301, loss: 0.456, 9632/60000 datapoints
2025-03-06 19:24:53,498 - INFO - training batch 351, loss: 0.254, 11232/60000 datapoints
2025-03-06 19:24:53,699 - INFO - training batch 401, loss: 0.259, 12832/60000 datapoints
2025-03-06 19:24:53,916 - INFO - training batch 451, loss: 0.153, 14432/60000 datapoints
2025-03-06 19:24:54,112 - INFO - training batch 501, loss: 0.320, 16032/60000 datapoints
2025-03-06 19:24:54,309 - INFO - training batch 551, loss: 0.466, 17632/60000 datapoints
2025-03-06 19:24:54,503 - INFO - training batch 601, loss: 0.379, 19232/60000 datapoints
2025-03-06 19:24:54,698 - INFO - training batch 651, loss: 0.185, 20832/60000 datapoints
2025-03-06 19:24:54,896 - INFO - training batch 701, loss: 0.292, 22432/60000 datapoints
2025-03-06 19:24:55,090 - INFO - training batch 751, loss: 0.342, 24032/60000 datapoints
2025-03-06 19:24:55,289 - INFO - training batch 801, loss: 0.184, 25632/60000 datapoints
2025-03-06 19:24:55,482 - INFO - training batch 851, loss: 0.341, 27232/60000 datapoints
2025-03-06 19:24:55,678 - INFO - training batch 901, loss: 0.359, 28832/60000 datapoints
2025-03-06 19:24:55,873 - INFO - training batch 951, loss: 0.193, 30432/60000 datapoints
2025-03-06 19:24:56,066 - INFO - training batch 1001, loss: 0.289, 32032/60000 datapoints
2025-03-06 19:24:56,261 - INFO - training batch 1051, loss: 0.279, 33632/60000 datapoints
2025-03-06 19:24:56,456 - INFO - training batch 1101, loss: 0.107, 35232/60000 datapoints
2025-03-06 19:24:56,652 - INFO - training batch 1151, loss: 0.723, 36832/60000 datapoints
2025-03-06 19:24:56,847 - INFO - training batch 1201, loss: 0.240, 38432/60000 datapoints
2025-03-06 19:24:57,041 - INFO - training batch 1251, loss: 0.412, 40032/60000 datapoints
2025-03-06 19:24:57,240 - INFO - training batch 1301, loss: 0.483, 41632/60000 datapoints
2025-03-06 19:24:57,440 - INFO - training batch 1351, loss: 0.489, 43232/60000 datapoints
2025-03-06 19:24:57,638 - INFO - training batch 1401, loss: 0.345, 44832/60000 datapoints
2025-03-06 19:24:57,834 - INFO - training batch 1451, loss: 0.149, 46432/60000 datapoints
2025-03-06 19:24:58,027 - INFO - training batch 1501, loss: 0.157, 48032/60000 datapoints
2025-03-06 19:24:58,220 - INFO - training batch 1551, loss: 0.250, 49632/60000 datapoints
2025-03-06 19:24:58,413 - INFO - training batch 1601, loss: 0.403, 51232/60000 datapoints
2025-03-06 19:24:58,608 - INFO - training batch 1651, loss: 0.279, 52832/60000 datapoints
2025-03-06 19:24:58,810 - INFO - training batch 1701, loss: 0.348, 54432/60000 datapoints
2025-03-06 19:24:59,004 - INFO - training batch 1751, loss: 0.183, 56032/60000 datapoints
2025-03-06 19:24:59,199 - INFO - training batch 1801, loss: 0.632, 57632/60000 datapoints
2025-03-06 19:24:59,397 - INFO - training batch 1851, loss: 0.162, 59232/60000 datapoints
2025-03-06 19:24:59,497 - INFO - validation batch 1, loss: 0.465, 32/10016 datapoints
2025-03-06 19:24:59,653 - INFO - validation batch 51, loss: 0.187, 1632/10016 datapoints
2025-03-06 19:24:59,813 - INFO - validation batch 101, loss: 0.316, 3232/10016 datapoints
2025-03-06 19:24:59,966 - INFO - validation batch 151, loss: 0.255, 4832/10016 datapoints
2025-03-06 19:25:00,120 - INFO - validation batch 201, loss: 0.153, 6432/10016 datapoints
2025-03-06 19:25:00,275 - INFO - validation batch 251, loss: 0.345, 8032/10016 datapoints
2025-03-06 19:25:00,428 - INFO - validation batch 301, loss: 0.349, 9632/10016 datapoints
2025-03-06 19:25:00,465 - INFO - Epoch 314/800 done.
2025-03-06 19:25:00,465 - INFO - Final validation performance:
Loss: 0.296, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 19:25:00,466 - INFO - Beginning epoch 315/800
2025-03-06 19:25:00,473 - INFO - training batch 1, loss: 0.103, 32/60000 datapoints
2025-03-06 19:25:00,682 - INFO - training batch 51, loss: 0.205, 1632/60000 datapoints
2025-03-06 19:25:00,877 - INFO - training batch 101, loss: 0.397, 3232/60000 datapoints
2025-03-06 19:25:01,076 - INFO - training batch 151, loss: 0.234, 4832/60000 datapoints
2025-03-06 19:25:01,272 - INFO - training batch 201, loss: 0.550, 6432/60000 datapoints
2025-03-06 19:25:01,467 - INFO - training batch 251, loss: 0.196, 8032/60000 datapoints
2025-03-06 19:25:01,661 - INFO - training batch 301, loss: 0.171, 9632/60000 datapoints
2025-03-06 19:25:01,853 - INFO - training batch 351, loss: 0.342, 11232/60000 datapoints
2025-03-06 19:25:02,045 - INFO - training batch 401, loss: 0.276, 12832/60000 datapoints
2025-03-06 19:25:02,235 - INFO - training batch 451, loss: 0.426, 14432/60000 datapoints
2025-03-06 19:25:02,427 - INFO - training batch 501, loss: 0.363, 16032/60000 datapoints
2025-03-06 19:25:02,619 - INFO - training batch 551, loss: 0.547, 17632/60000 datapoints
2025-03-06 19:25:02,813 - INFO - training batch 601, loss: 0.366, 19232/60000 datapoints
2025-03-06 19:25:03,006 - INFO - training batch 651, loss: 0.414, 20832/60000 datapoints
2025-03-06 19:25:03,199 - INFO - training batch 701, loss: 0.252, 22432/60000 datapoints
2025-03-06 19:25:03,393 - INFO - training batch 751, loss: 0.241, 24032/60000 datapoints
2025-03-06 19:25:03,584 - INFO - training batch 801, loss: 0.126, 25632/60000 datapoints
2025-03-06 19:25:03,784 - INFO - training batch 851, loss: 0.297, 27232/60000 datapoints
2025-03-06 19:25:03,995 - INFO - training batch 901, loss: 0.277, 28832/60000 datapoints
2025-03-06 19:25:04,186 - INFO - training batch 951, loss: 0.349, 30432/60000 datapoints
2025-03-06 19:25:04,380 - INFO - training batch 1001, loss: 0.199, 32032/60000 datapoints
2025-03-06 19:25:04,573 - INFO - training batch 1051, loss: 0.338, 33632/60000 datapoints
2025-03-06 19:25:04,767 - INFO - training batch 1101, loss: 0.455, 35232/60000 datapoints
2025-03-06 19:25:04,966 - INFO - training batch 1151, loss: 0.438, 36832/60000 datapoints
2025-03-06 19:25:05,157 - INFO - training batch 1201, loss: 0.224, 38432/60000 datapoints
2025-03-06 19:25:05,351 - INFO - training batch 1251, loss: 0.215, 40032/60000 datapoints
2025-03-06 19:25:05,542 - INFO - training batch 1301, loss: 0.347, 41632/60000 datapoints
2025-03-06 19:25:05,736 - INFO - training batch 1351, loss: 0.275, 43232/60000 datapoints
2025-03-06 19:25:05,926 - INFO - training batch 1401, loss: 0.217, 44832/60000 datapoints
2025-03-06 19:25:06,116 - INFO - training batch 1451, loss: 0.567, 46432/60000 datapoints
2025-03-06 19:25:06,308 - INFO - training batch 1501, loss: 0.181, 48032/60000 datapoints
2025-03-06 19:25:06,500 - INFO - training batch 1551, loss: 0.420, 49632/60000 datapoints
2025-03-06 19:25:06,691 - INFO - training batch 1601, loss: 0.203, 51232/60000 datapoints
2025-03-06 19:25:06,888 - INFO - training batch 1651, loss: 0.259, 52832/60000 datapoints
2025-03-06 19:25:07,079 - INFO - training batch 1701, loss: 0.670, 54432/60000 datapoints
2025-03-06 19:25:07,268 - INFO - training batch 1751, loss: 0.281, 56032/60000 datapoints
2025-03-06 19:25:07,463 - INFO - training batch 1801, loss: 0.213, 57632/60000 datapoints
2025-03-06 19:25:07,657 - INFO - training batch 1851, loss: 0.346, 59232/60000 datapoints
2025-03-06 19:25:07,756 - INFO - validation batch 1, loss: 0.369, 32/10016 datapoints
2025-03-06 19:25:07,907 - INFO - validation batch 51, loss: 0.233, 1632/10016 datapoints
2025-03-06 19:25:08,056 - INFO - validation batch 101, loss: 0.480, 3232/10016 datapoints
2025-03-06 19:25:08,207 - INFO - validation batch 151, loss: 0.281, 4832/10016 datapoints
2025-03-06 19:25:08,356 - INFO - validation batch 201, loss: 0.325, 6432/10016 datapoints
2025-03-06 19:25:08,508 - INFO - validation batch 251, loss: 0.413, 8032/10016 datapoints
2025-03-06 19:25:08,659 - INFO - validation batch 301, loss: 0.110, 9632/10016 datapoints
2025-03-06 19:25:08,695 - INFO - Epoch 315/800 done.
2025-03-06 19:25:08,696 - INFO - Final validation performance:
Loss: 0.316, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 19:25:08,696 - INFO - Beginning epoch 316/800
2025-03-06 19:25:08,703 - INFO - training batch 1, loss: 0.291, 32/60000 datapoints
2025-03-06 19:25:08,905 - INFO - training batch 51, loss: 0.176, 1632/60000 datapoints
2025-03-06 19:25:09,110 - INFO - training batch 101, loss: 0.261, 3232/60000 datapoints
2025-03-06 19:25:09,304 - INFO - training batch 151, loss: 0.320, 4832/60000 datapoints
2025-03-06 19:25:09,505 - INFO - training batch 201, loss: 0.429, 6432/60000 datapoints
2025-03-06 19:25:09,701 - INFO - training batch 251, loss: 0.534, 8032/60000 datapoints
2025-03-06 19:25:09,926 - INFO - training batch 301, loss: 0.228, 9632/60000 datapoints
2025-03-06 19:25:10,119 - INFO - training batch 351, loss: 0.347, 11232/60000 datapoints
2025-03-06 19:25:10,311 - INFO - training batch 401, loss: 0.279, 12832/60000 datapoints
2025-03-06 19:25:10,505 - INFO - training batch 451, loss: 0.161, 14432/60000 datapoints
2025-03-06 19:25:10,698 - INFO - training batch 501, loss: 0.169, 16032/60000 datapoints
2025-03-06 19:25:10,892 - INFO - training batch 551, loss: 0.237, 17632/60000 datapoints
2025-03-06 19:25:11,082 - INFO - training batch 601, loss: 0.127, 19232/60000 datapoints
2025-03-06 19:25:11,274 - INFO - training batch 651, loss: 0.208, 20832/60000 datapoints
2025-03-06 19:25:11,469 - INFO - training batch 701, loss: 0.678, 22432/60000 datapoints
2025-03-06 19:25:11,663 - INFO - training batch 751, loss: 0.491, 24032/60000 datapoints
2025-03-06 19:25:11,854 - INFO - training batch 801, loss: 0.248, 25632/60000 datapoints
2025-03-06 19:25:12,046 - INFO - training batch 851, loss: 0.277, 27232/60000 datapoints
2025-03-06 19:25:12,236 - INFO - training batch 901, loss: 0.605, 28832/60000 datapoints
2025-03-06 19:25:12,431 - INFO - training batch 951, loss: 0.204, 30432/60000 datapoints
2025-03-06 19:25:12,624 - INFO - training batch 1001, loss: 0.277, 32032/60000 datapoints
2025-03-06 19:25:12,824 - INFO - training batch 1051, loss: 0.317, 33632/60000 datapoints
2025-03-06 19:25:13,023 - INFO - training batch 1101, loss: 0.529, 35232/60000 datapoints
2025-03-06 19:25:13,215 - INFO - training batch 1151, loss: 0.359, 36832/60000 datapoints
2025-03-06 19:25:13,414 - INFO - training batch 1201, loss: 0.209, 38432/60000 datapoints
2025-03-06 19:25:13,608 - INFO - training batch 1251, loss: 0.379, 40032/60000 datapoints
2025-03-06 19:25:13,811 - INFO - training batch 1301, loss: 0.264, 41632/60000 datapoints
2025-03-06 19:25:14,024 - INFO - training batch 1351, loss: 0.436, 43232/60000 datapoints
2025-03-06 19:25:14,218 - INFO - training batch 1401, loss: 0.401, 44832/60000 datapoints
2025-03-06 19:25:14,416 - INFO - training batch 1451, loss: 0.377, 46432/60000 datapoints
2025-03-06 19:25:14,608 - INFO - training batch 1501, loss: 0.487, 48032/60000 datapoints
2025-03-06 19:25:14,805 - INFO - training batch 1551, loss: 0.308, 49632/60000 datapoints
2025-03-06 19:25:15,018 - INFO - training batch 1601, loss: 0.136, 51232/60000 datapoints
2025-03-06 19:25:15,229 - INFO - training batch 1651, loss: 0.353, 52832/60000 datapoints
2025-03-06 19:25:15,427 - INFO - training batch 1701, loss: 0.178, 54432/60000 datapoints
2025-03-06 19:25:15,621 - INFO - training batch 1751, loss: 0.190, 56032/60000 datapoints
2025-03-06 19:25:15,816 - INFO - training batch 1801, loss: 0.355, 57632/60000 datapoints
2025-03-06 19:25:16,013 - INFO - training batch 1851, loss: 0.693, 59232/60000 datapoints
2025-03-06 19:25:16,113 - INFO - validation batch 1, loss: 0.149, 32/10016 datapoints
2025-03-06 19:25:16,266 - INFO - validation batch 51, loss: 0.461, 1632/10016 datapoints
2025-03-06 19:25:16,419 - INFO - validation batch 101, loss: 0.097, 3232/10016 datapoints
2025-03-06 19:25:16,572 - INFO - validation batch 151, loss: 0.185, 4832/10016 datapoints
2025-03-06 19:25:16,728 - INFO - validation batch 201, loss: 0.319, 6432/10016 datapoints
2025-03-06 19:25:16,883 - INFO - validation batch 251, loss: 0.242, 8032/10016 datapoints
2025-03-06 19:25:17,038 - INFO - validation batch 301, loss: 0.532, 9632/10016 datapoints
2025-03-06 19:25:17,076 - INFO - Epoch 316/800 done.
2025-03-06 19:25:17,076 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 19:25:17,077 - INFO - Beginning epoch 317/800
2025-03-06 19:25:17,083 - INFO - training batch 1, loss: 0.209, 32/60000 datapoints
2025-03-06 19:25:17,278 - INFO - training batch 51, loss: 0.336, 1632/60000 datapoints
2025-03-06 19:25:17,483 - INFO - training batch 101, loss: 0.606, 3232/60000 datapoints
2025-03-06 19:25:17,684 - INFO - training batch 151, loss: 0.268, 4832/60000 datapoints
2025-03-06 19:25:17,880 - INFO - training batch 201, loss: 0.309, 6432/60000 datapoints
2025-03-06 19:25:18,081 - INFO - training batch 251, loss: 0.243, 8032/60000 datapoints
2025-03-06 19:25:18,277 - INFO - training batch 301, loss: 0.452, 9632/60000 datapoints
2025-03-06 19:25:18,474 - INFO - training batch 351, loss: 0.130, 11232/60000 datapoints
2025-03-06 19:25:18,670 - INFO - training batch 401, loss: 0.409, 12832/60000 datapoints
2025-03-06 19:25:18,869 - INFO - training batch 451, loss: 0.130, 14432/60000 datapoints
2025-03-06 19:25:19,065 - INFO - training batch 501, loss: 0.310, 16032/60000 datapoints
2025-03-06 19:25:19,265 - INFO - training batch 551, loss: 0.177, 17632/60000 datapoints
2025-03-06 19:25:19,470 - INFO - training batch 601, loss: 0.423, 19232/60000 datapoints
2025-03-06 19:25:19,668 - INFO - training batch 651, loss: 0.313, 20832/60000 datapoints
2025-03-06 19:25:19,866 - INFO - training batch 701, loss: 0.179, 22432/60000 datapoints
2025-03-06 19:25:20,062 - INFO - training batch 751, loss: 0.381, 24032/60000 datapoints
2025-03-06 19:25:20,258 - INFO - training batch 801, loss: 0.107, 25632/60000 datapoints
2025-03-06 19:25:20,454 - INFO - training batch 851, loss: 0.400, 27232/60000 datapoints
2025-03-06 19:25:20,650 - INFO - training batch 901, loss: 0.358, 28832/60000 datapoints
2025-03-06 19:25:20,845 - INFO - training batch 951, loss: 0.208, 30432/60000 datapoints
2025-03-06 19:25:21,042 - INFO - training batch 1001, loss: 0.428, 32032/60000 datapoints
2025-03-06 19:25:21,236 - INFO - training batch 1051, loss: 0.613, 33632/60000 datapoints
2025-03-06 19:25:21,432 - INFO - training batch 1101, loss: 0.225, 35232/60000 datapoints
2025-03-06 19:25:21,636 - INFO - training batch 1151, loss: 0.520, 36832/60000 datapoints
2025-03-06 19:25:21,833 - INFO - training batch 1201, loss: 0.214, 38432/60000 datapoints
2025-03-06 19:25:22,028 - INFO - training batch 1251, loss: 0.511, 40032/60000 datapoints
2025-03-06 19:25:22,222 - INFO - training batch 1301, loss: 0.142, 41632/60000 datapoints
2025-03-06 19:25:22,416 - INFO - training batch 1351, loss: 0.201, 43232/60000 datapoints
2025-03-06 19:25:22,613 - INFO - training batch 1401, loss: 0.437, 44832/60000 datapoints
2025-03-06 19:25:22,809 - INFO - training batch 1451, loss: 0.223, 46432/60000 datapoints
2025-03-06 19:25:23,005 - INFO - training batch 1501, loss: 0.158, 48032/60000 datapoints
2025-03-06 19:25:23,197 - INFO - training batch 1551, loss: 0.219, 49632/60000 datapoints
2025-03-06 19:25:23,392 - INFO - training batch 1601, loss: 0.340, 51232/60000 datapoints
2025-03-06 19:25:23,586 - INFO - training batch 1651, loss: 0.286, 52832/60000 datapoints
2025-03-06 19:25:23,785 - INFO - training batch 1701, loss: 0.314, 54432/60000 datapoints
2025-03-06 19:25:23,991 - INFO - training batch 1751, loss: 0.293, 56032/60000 datapoints
2025-03-06 19:25:24,201 - INFO - training batch 1801, loss: 0.388, 57632/60000 datapoints
2025-03-06 19:25:24,395 - INFO - training batch 1851, loss: 0.237, 59232/60000 datapoints
2025-03-06 19:25:24,497 - INFO - validation batch 1, loss: 0.163, 32/10016 datapoints
2025-03-06 19:25:24,656 - INFO - validation batch 51, loss: 0.198, 1632/10016 datapoints
2025-03-06 19:25:24,807 - INFO - validation batch 101, loss: 0.447, 3232/10016 datapoints
2025-03-06 19:25:24,968 - INFO - validation batch 151, loss: 0.296, 4832/10016 datapoints
2025-03-06 19:25:25,124 - INFO - validation batch 201, loss: 0.311, 6432/10016 datapoints
2025-03-06 19:25:25,277 - INFO - validation batch 251, loss: 0.208, 8032/10016 datapoints
2025-03-06 19:25:25,445 - INFO - validation batch 301, loss: 0.180, 9632/10016 datapoints
2025-03-06 19:25:25,483 - INFO - Epoch 317/800 done.
2025-03-06 19:25:25,483 - INFO - Final validation performance:
Loss: 0.258, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 19:25:25,484 - INFO - Beginning epoch 318/800
2025-03-06 19:25:25,490 - INFO - training batch 1, loss: 0.314, 32/60000 datapoints
2025-03-06 19:25:25,699 - INFO - training batch 51, loss: 0.121, 1632/60000 datapoints
2025-03-06 19:25:25,896 - INFO - training batch 101, loss: 0.325, 3232/60000 datapoints
2025-03-06 19:25:26,097 - INFO - training batch 151, loss: 0.132, 4832/60000 datapoints
2025-03-06 19:25:26,295 - INFO - training batch 201, loss: 0.283, 6432/60000 datapoints
2025-03-06 19:25:26,494 - INFO - training batch 251, loss: 0.358, 8032/60000 datapoints
2025-03-06 19:25:26,692 - INFO - training batch 301, loss: 0.208, 9632/60000 datapoints
2025-03-06 19:25:26,896 - INFO - training batch 351, loss: 0.320, 11232/60000 datapoints
2025-03-06 19:25:27,091 - INFO - training batch 401, loss: 0.527, 12832/60000 datapoints
2025-03-06 19:25:27,285 - INFO - training batch 451, loss: 0.398, 14432/60000 datapoints
2025-03-06 19:25:27,482 - INFO - training batch 501, loss: 0.374, 16032/60000 datapoints
2025-03-06 19:25:27,686 - INFO - training batch 551, loss: 0.445, 17632/60000 datapoints
2025-03-06 19:25:27,883 - INFO - training batch 601, loss: 0.458, 19232/60000 datapoints
2025-03-06 19:25:28,079 - INFO - training batch 651, loss: 0.088, 20832/60000 datapoints
2025-03-06 19:25:28,273 - INFO - training batch 701, loss: 0.384, 22432/60000 datapoints
2025-03-06 19:25:28,468 - INFO - training batch 751, loss: 0.424, 24032/60000 datapoints
2025-03-06 19:25:28,667 - INFO - training batch 801, loss: 0.296, 25632/60000 datapoints
2025-03-06 19:25:28,862 - INFO - training batch 851, loss: 0.369, 27232/60000 datapoints
2025-03-06 19:25:29,058 - INFO - training batch 901, loss: 0.309, 28832/60000 datapoints
2025-03-06 19:25:29,254 - INFO - training batch 951, loss: 0.253, 30432/60000 datapoints
2025-03-06 19:25:29,450 - INFO - training batch 1001, loss: 0.192, 32032/60000 datapoints
2025-03-06 19:25:29,646 - INFO - training batch 1051, loss: 0.241, 33632/60000 datapoints
2025-03-06 19:25:29,840 - INFO - training batch 1101, loss: 0.487, 35232/60000 datapoints
2025-03-06 19:25:30,034 - INFO - training batch 1151, loss: 0.246, 36832/60000 datapoints
2025-03-06 19:25:30,229 - INFO - training batch 1201, loss: 0.433, 38432/60000 datapoints
2025-03-06 19:25:30,437 - INFO - training batch 1251, loss: 0.393, 40032/60000 datapoints
2025-03-06 19:25:30,637 - INFO - training batch 1301, loss: 0.151, 41632/60000 datapoints
2025-03-06 19:25:30,831 - INFO - training batch 1351, loss: 0.283, 43232/60000 datapoints
2025-03-06 19:25:31,029 - INFO - training batch 1401, loss: 0.433, 44832/60000 datapoints
2025-03-06 19:25:31,253 - INFO - training batch 1451, loss: 0.294, 46432/60000 datapoints
2025-03-06 19:25:31,478 - INFO - training batch 1501, loss: 0.493, 48032/60000 datapoints
2025-03-06 19:25:31,718 - INFO - training batch 1551, loss: 0.435, 49632/60000 datapoints
2025-03-06 19:25:31,916 - INFO - training batch 1601, loss: 0.474, 51232/60000 datapoints
2025-03-06 19:25:32,111 - INFO - training batch 1651, loss: 0.107, 52832/60000 datapoints
2025-03-06 19:25:32,305 - INFO - training batch 1701, loss: 0.323, 54432/60000 datapoints
2025-03-06 19:25:32,499 - INFO - training batch 1751, loss: 0.298, 56032/60000 datapoints
2025-03-06 19:25:32,699 - INFO - training batch 1801, loss: 0.152, 57632/60000 datapoints
2025-03-06 19:25:32,895 - INFO - training batch 1851, loss: 0.197, 59232/60000 datapoints
2025-03-06 19:25:32,997 - INFO - validation batch 1, loss: 0.396, 32/10016 datapoints
2025-03-06 19:25:33,150 - INFO - validation batch 51, loss: 0.419, 1632/10016 datapoints
2025-03-06 19:25:33,303 - INFO - validation batch 101, loss: 0.329, 3232/10016 datapoints
2025-03-06 19:25:33,457 - INFO - validation batch 151, loss: 0.558, 4832/10016 datapoints
2025-03-06 19:25:33,610 - INFO - validation batch 201, loss: 0.190, 6432/10016 datapoints
2025-03-06 19:25:33,764 - INFO - validation batch 251, loss: 0.226, 8032/10016 datapoints
2025-03-06 19:25:33,922 - INFO - validation batch 301, loss: 0.301, 9632/10016 datapoints
2025-03-06 19:25:33,961 - INFO - Epoch 318/800 done.
2025-03-06 19:25:33,961 - INFO - Final validation performance:
Loss: 0.346, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 19:25:33,962 - INFO - Beginning epoch 319/800
2025-03-06 19:25:33,970 - INFO - training batch 1, loss: 0.410, 32/60000 datapoints
2025-03-06 19:25:34,211 - INFO - training batch 51, loss: 0.454, 1632/60000 datapoints
2025-03-06 19:25:34,404 - INFO - training batch 101, loss: 0.336, 3232/60000 datapoints
2025-03-06 19:25:34,606 - INFO - training batch 151, loss: 0.208, 4832/60000 datapoints
2025-03-06 19:25:34,807 - INFO - training batch 201, loss: 0.345, 6432/60000 datapoints
2025-03-06 19:25:35,011 - INFO - training batch 251, loss: 0.237, 8032/60000 datapoints
2025-03-06 19:25:35,206 - INFO - training batch 301, loss: 0.177, 9632/60000 datapoints
2025-03-06 19:25:35,406 - INFO - training batch 351, loss: 0.440, 11232/60000 datapoints
2025-03-06 19:25:35,602 - INFO - training batch 401, loss: 0.255, 12832/60000 datapoints
2025-03-06 19:25:35,798 - INFO - training batch 451, loss: 0.245, 14432/60000 datapoints
2025-03-06 19:25:35,993 - INFO - training batch 501, loss: 0.237, 16032/60000 datapoints
2025-03-06 19:25:36,189 - INFO - training batch 551, loss: 0.389, 17632/60000 datapoints
2025-03-06 19:25:36,386 - INFO - training batch 601, loss: 0.257, 19232/60000 datapoints
2025-03-06 19:25:36,579 - INFO - training batch 651, loss: 0.422, 20832/60000 datapoints
2025-03-06 19:25:36,834 - INFO - training batch 701, loss: 0.114, 22432/60000 datapoints
2025-03-06 19:25:37,032 - INFO - training batch 751, loss: 0.204, 24032/60000 datapoints
2025-03-06 19:25:37,226 - INFO - training batch 801, loss: 0.326, 25632/60000 datapoints
2025-03-06 19:25:37,422 - INFO - training batch 851, loss: 0.400, 27232/60000 datapoints
2025-03-06 19:25:37,630 - INFO - training batch 901, loss: 0.316, 28832/60000 datapoints
2025-03-06 19:25:37,828 - INFO - training batch 951, loss: 0.160, 30432/60000 datapoints
2025-03-06 19:25:38,026 - INFO - training batch 1001, loss: 0.229, 32032/60000 datapoints
2025-03-06 19:25:38,223 - INFO - training batch 1051, loss: 0.255, 33632/60000 datapoints
2025-03-06 19:25:38,416 - INFO - training batch 1101, loss: 0.580, 35232/60000 datapoints
2025-03-06 19:25:38,611 - INFO - training batch 1151, loss: 0.399, 36832/60000 datapoints
2025-03-06 19:25:38,808 - INFO - training batch 1201, loss: 0.408, 38432/60000 datapoints
2025-03-06 19:25:39,003 - INFO - training batch 1251, loss: 0.259, 40032/60000 datapoints
2025-03-06 19:25:39,206 - INFO - training batch 1301, loss: 0.194, 41632/60000 datapoints
2025-03-06 19:25:39,402 - INFO - training batch 1351, loss: 0.420, 43232/60000 datapoints
2025-03-06 19:25:39,598 - INFO - training batch 1401, loss: 0.201, 44832/60000 datapoints
2025-03-06 19:25:39,794 - INFO - training batch 1451, loss: 0.314, 46432/60000 datapoints
2025-03-06 19:25:39,988 - INFO - training batch 1501, loss: 0.330, 48032/60000 datapoints
2025-03-06 19:25:40,183 - INFO - training batch 1551, loss: 0.301, 49632/60000 datapoints
2025-03-06 19:25:40,380 - INFO - training batch 1601, loss: 0.235, 51232/60000 datapoints
2025-03-06 19:25:40,575 - INFO - training batch 1651, loss: 0.231, 52832/60000 datapoints
2025-03-06 19:25:40,772 - INFO - training batch 1701, loss: 0.468, 54432/60000 datapoints
2025-03-06 19:25:40,966 - INFO - training batch 1751, loss: 0.474, 56032/60000 datapoints
2025-03-06 19:25:41,161 - INFO - training batch 1801, loss: 0.410, 57632/60000 datapoints
2025-03-06 19:25:41,355 - INFO - training batch 1851, loss: 0.274, 59232/60000 datapoints
2025-03-06 19:25:41,459 - INFO - validation batch 1, loss: 0.386, 32/10016 datapoints
2025-03-06 19:25:41,615 - INFO - validation batch 51, loss: 0.166, 1632/10016 datapoints
2025-03-06 19:25:41,770 - INFO - validation batch 101, loss: 0.378, 3232/10016 datapoints
2025-03-06 19:25:41,924 - INFO - validation batch 151, loss: 0.396, 4832/10016 datapoints
2025-03-06 19:25:42,076 - INFO - validation batch 201, loss: 0.087, 6432/10016 datapoints
2025-03-06 19:25:42,229 - INFO - validation batch 251, loss: 0.371, 8032/10016 datapoints
2025-03-06 19:25:42,381 - INFO - validation batch 301, loss: 0.400, 9632/10016 datapoints
2025-03-06 19:25:42,418 - INFO - Epoch 319/800 done.
2025-03-06 19:25:42,419 - INFO - Final validation performance:
Loss: 0.312, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 19:25:42,419 - INFO - Beginning epoch 320/800
2025-03-06 19:25:42,426 - INFO - training batch 1, loss: 0.288, 32/60000 datapoints
2025-03-06 19:25:42,618 - INFO - training batch 51, loss: 0.212, 1632/60000 datapoints
2025-03-06 19:25:42,814 - INFO - training batch 101, loss: 0.255, 3232/60000 datapoints
2025-03-06 19:25:43,021 - INFO - training batch 151, loss: 0.154, 4832/60000 datapoints
2025-03-06 19:25:43,217 - INFO - training batch 201, loss: 0.631, 6432/60000 datapoints
2025-03-06 19:25:43,414 - INFO - training batch 251, loss: 0.264, 8032/60000 datapoints
2025-03-06 19:25:43,617 - INFO - training batch 301, loss: 0.283, 9632/60000 datapoints
2025-03-06 19:25:43,811 - INFO - training batch 351, loss: 0.182, 11232/60000 datapoints
2025-03-06 19:25:44,014 - INFO - training batch 401, loss: 0.187, 12832/60000 datapoints
2025-03-06 19:25:44,231 - INFO - training batch 451, loss: 0.211, 14432/60000 datapoints
2025-03-06 19:25:44,429 - INFO - training batch 501, loss: 0.467, 16032/60000 datapoints
2025-03-06 19:25:44,621 - INFO - training batch 551, loss: 0.259, 17632/60000 datapoints
2025-03-06 19:25:44,812 - INFO - training batch 601, loss: 0.186, 19232/60000 datapoints
2025-03-06 19:25:45,009 - INFO - training batch 651, loss: 0.562, 20832/60000 datapoints
2025-03-06 19:25:45,200 - INFO - training batch 701, loss: 0.481, 22432/60000 datapoints
2025-03-06 19:25:45,395 - INFO - training batch 751, loss: 0.213, 24032/60000 datapoints
2025-03-06 19:25:45,591 - INFO - training batch 801, loss: 0.201, 25632/60000 datapoints
2025-03-06 19:25:45,784 - INFO - training batch 851, loss: 0.198, 27232/60000 datapoints
2025-03-06 19:25:45,975 - INFO - training batch 901, loss: 0.374, 28832/60000 datapoints
2025-03-06 19:25:46,166 - INFO - training batch 951, loss: 0.325, 30432/60000 datapoints
2025-03-06 19:25:46,360 - INFO - training batch 1001, loss: 0.223, 32032/60000 datapoints
2025-03-06 19:25:46,551 - INFO - training batch 1051, loss: 0.242, 33632/60000 datapoints
2025-03-06 19:25:46,746 - INFO - training batch 1101, loss: 0.169, 35232/60000 datapoints
2025-03-06 19:25:46,939 - INFO - training batch 1151, loss: 0.400, 36832/60000 datapoints
2025-03-06 19:25:47,134 - INFO - training batch 1201, loss: 0.506, 38432/60000 datapoints
2025-03-06 19:25:47,325 - INFO - training batch 1251, loss: 0.322, 40032/60000 datapoints
2025-03-06 19:25:47,517 - INFO - training batch 1301, loss: 0.158, 41632/60000 datapoints
2025-03-06 19:25:47,710 - INFO - training batch 1351, loss: 0.437, 43232/60000 datapoints
2025-03-06 19:25:47,902 - INFO - training batch 1401, loss: 0.104, 44832/60000 datapoints
2025-03-06 19:25:48,100 - INFO - training batch 1451, loss: 0.388, 46432/60000 datapoints
2025-03-06 19:25:48,291 - INFO - training batch 1501, loss: 0.465, 48032/60000 datapoints
2025-03-06 19:25:48,482 - INFO - training batch 1551, loss: 0.422, 49632/60000 datapoints
2025-03-06 19:25:48,674 - INFO - training batch 1601, loss: 0.117, 51232/60000 datapoints
2025-03-06 19:25:48,865 - INFO - training batch 1651, loss: 0.403, 52832/60000 datapoints
2025-03-06 19:25:49,057 - INFO - training batch 1701, loss: 0.210, 54432/60000 datapoints
2025-03-06 19:25:49,248 - INFO - training batch 1751, loss: 0.206, 56032/60000 datapoints
2025-03-06 19:25:49,439 - INFO - training batch 1801, loss: 0.215, 57632/60000 datapoints
2025-03-06 19:25:49,641 - INFO - training batch 1851, loss: 0.185, 59232/60000 datapoints
2025-03-06 19:25:49,740 - INFO - validation batch 1, loss: 0.899, 32/10016 datapoints
2025-03-06 19:25:49,890 - INFO - validation batch 51, loss: 0.446, 1632/10016 datapoints
2025-03-06 19:25:50,054 - INFO - validation batch 101, loss: 0.526, 3232/10016 datapoints
2025-03-06 19:25:50,229 - INFO - validation batch 151, loss: 0.319, 4832/10016 datapoints
2025-03-06 19:25:50,396 - INFO - validation batch 201, loss: 0.277, 6432/10016 datapoints
2025-03-06 19:25:50,566 - INFO - validation batch 251, loss: 0.260, 8032/10016 datapoints
2025-03-06 19:25:50,723 - INFO - validation batch 301, loss: 0.151, 9632/10016 datapoints
2025-03-06 19:25:50,759 - INFO - Epoch 320/800 done.
2025-03-06 19:25:50,759 - INFO - Final validation performance:
Loss: 0.411, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 19:25:50,760 - INFO - Beginning epoch 321/800
2025-03-06 19:25:50,766 - INFO - training batch 1, loss: 0.328, 32/60000 datapoints
2025-03-06 19:25:50,977 - INFO - training batch 51, loss: 0.214, 1632/60000 datapoints
2025-03-06 19:25:51,177 - INFO - training batch 101, loss: 0.586, 3232/60000 datapoints
2025-03-06 19:25:51,379 - INFO - training batch 151, loss: 0.244, 4832/60000 datapoints
2025-03-06 19:25:51,593 - INFO - training batch 201, loss: 0.509, 6432/60000 datapoints
2025-03-06 19:25:51,808 - INFO - training batch 251, loss: 0.232, 8032/60000 datapoints
2025-03-06 19:25:52,002 - INFO - training batch 301, loss: 0.336, 9632/60000 datapoints
2025-03-06 19:25:52,196 - INFO - training batch 351, loss: 0.087, 11232/60000 datapoints
2025-03-06 19:25:52,387 - INFO - training batch 401, loss: 0.610, 12832/60000 datapoints
2025-03-06 19:25:52,579 - INFO - training batch 451, loss: 0.117, 14432/60000 datapoints
2025-03-06 19:25:52,777 - INFO - training batch 501, loss: 0.263, 16032/60000 datapoints
2025-03-06 19:25:52,970 - INFO - training batch 551, loss: 0.346, 17632/60000 datapoints
2025-03-06 19:25:53,165 - INFO - training batch 601, loss: 0.174, 19232/60000 datapoints
2025-03-06 19:25:53,357 - INFO - training batch 651, loss: 0.139, 20832/60000 datapoints
2025-03-06 19:25:53,551 - INFO - training batch 701, loss: 0.110, 22432/60000 datapoints
2025-03-06 19:25:53,744 - INFO - training batch 751, loss: 0.333, 24032/60000 datapoints
2025-03-06 19:25:53,939 - INFO - training batch 801, loss: 0.200, 25632/60000 datapoints
2025-03-06 19:25:54,132 - INFO - training batch 851, loss: 0.221, 27232/60000 datapoints
2025-03-06 19:25:54,345 - INFO - training batch 901, loss: 0.189, 28832/60000 datapoints
2025-03-06 19:25:54,535 - INFO - training batch 951, loss: 0.642, 30432/60000 datapoints
2025-03-06 19:25:54,727 - INFO - training batch 1001, loss: 0.194, 32032/60000 datapoints
2025-03-06 19:25:54,922 - INFO - training batch 1051, loss: 0.316, 33632/60000 datapoints
2025-03-06 19:25:55,116 - INFO - training batch 1101, loss: 0.566, 35232/60000 datapoints
2025-03-06 19:25:55,310 - INFO - training batch 1151, loss: 0.232, 36832/60000 datapoints
2025-03-06 19:25:55,505 - INFO - training batch 1201, loss: 0.297, 38432/60000 datapoints
2025-03-06 19:25:55,699 - INFO - training batch 1251, loss: 0.394, 40032/60000 datapoints
2025-03-06 19:25:55,890 - INFO - training batch 1301, loss: 0.604, 41632/60000 datapoints
2025-03-06 19:25:56,082 - INFO - training batch 1351, loss: 0.339, 43232/60000 datapoints
2025-03-06 19:25:56,274 - INFO - training batch 1401, loss: 0.670, 44832/60000 datapoints
2025-03-06 19:25:56,468 - INFO - training batch 1451, loss: 0.273, 46432/60000 datapoints
2025-03-06 19:25:56,661 - INFO - training batch 1501, loss: 0.169, 48032/60000 datapoints
2025-03-06 19:25:56,853 - INFO - training batch 1551, loss: 0.335, 49632/60000 datapoints
2025-03-06 19:25:57,047 - INFO - training batch 1601, loss: 0.799, 51232/60000 datapoints
2025-03-06 19:25:57,239 - INFO - training batch 1651, loss: 0.444, 52832/60000 datapoints
2025-03-06 19:25:57,433 - INFO - training batch 1701, loss: 0.202, 54432/60000 datapoints
2025-03-06 19:25:57,628 - INFO - training batch 1751, loss: 0.168, 56032/60000 datapoints
2025-03-06 19:25:57,820 - INFO - training batch 1801, loss: 0.173, 57632/60000 datapoints
2025-03-06 19:25:58,011 - INFO - training batch 1851, loss: 0.359, 59232/60000 datapoints
2025-03-06 19:25:58,115 - INFO - validation batch 1, loss: 0.276, 32/10016 datapoints
2025-03-06 19:25:58,264 - INFO - validation batch 51, loss: 0.231, 1632/10016 datapoints
2025-03-06 19:25:58,414 - INFO - validation batch 101, loss: 0.152, 3232/10016 datapoints
2025-03-06 19:25:58,567 - INFO - validation batch 151, loss: 0.517, 4832/10016 datapoints
2025-03-06 19:25:58,719 - INFO - validation batch 201, loss: 0.449, 6432/10016 datapoints
2025-03-06 19:25:58,884 - INFO - validation batch 251, loss: 0.174, 8032/10016 datapoints
2025-03-06 19:25:59,042 - INFO - validation batch 301, loss: 0.240, 9632/10016 datapoints
2025-03-06 19:25:59,079 - INFO - Epoch 321/800 done.
2025-03-06 19:25:59,079 - INFO - Final validation performance:
Loss: 0.291, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 19:25:59,079 - INFO - Beginning epoch 322/800
2025-03-06 19:25:59,085 - INFO - training batch 1, loss: 0.176, 32/60000 datapoints
2025-03-06 19:25:59,281 - INFO - training batch 51, loss: 0.293, 1632/60000 datapoints
2025-03-06 19:25:59,480 - INFO - training batch 101, loss: 0.386, 3232/60000 datapoints
2025-03-06 19:25:59,696 - INFO - training batch 151, loss: 0.246, 4832/60000 datapoints
2025-03-06 19:25:59,894 - INFO - training batch 201, loss: 0.461, 6432/60000 datapoints
2025-03-06 19:26:00,097 - INFO - training batch 251, loss: 0.502, 8032/60000 datapoints
2025-03-06 19:26:00,296 - INFO - training batch 301, loss: 0.331, 9632/60000 datapoints
2025-03-06 19:26:00,499 - INFO - training batch 351, loss: 0.150, 11232/60000 datapoints
2025-03-06 19:26:00,696 - INFO - training batch 401, loss: 0.171, 12832/60000 datapoints
2025-03-06 19:26:00,887 - INFO - training batch 451, loss: 0.458, 14432/60000 datapoints
2025-03-06 19:26:01,082 - INFO - training batch 501, loss: 0.546, 16032/60000 datapoints
2025-03-06 19:26:01,274 - INFO - training batch 551, loss: 0.342, 17632/60000 datapoints
2025-03-06 19:26:01,469 - INFO - training batch 601, loss: 0.256, 19232/60000 datapoints
2025-03-06 19:26:01,669 - INFO - training batch 651, loss: 0.331, 20832/60000 datapoints
2025-03-06 19:26:01,864 - INFO - training batch 701, loss: 0.275, 22432/60000 datapoints
2025-03-06 19:26:02,057 - INFO - training batch 751, loss: 0.275, 24032/60000 datapoints
2025-03-06 19:26:02,254 - INFO - training batch 801, loss: 0.262, 25632/60000 datapoints
2025-03-06 19:26:02,448 - INFO - training batch 851, loss: 0.273, 27232/60000 datapoints
2025-03-06 19:26:02,645 - INFO - training batch 901, loss: 0.230, 28832/60000 datapoints
2025-03-06 19:26:02,842 - INFO - training batch 951, loss: 0.145, 30432/60000 datapoints
2025-03-06 19:26:03,038 - INFO - training batch 1001, loss: 0.352, 32032/60000 datapoints
2025-03-06 19:26:03,233 - INFO - training batch 1051, loss: 0.430, 33632/60000 datapoints
2025-03-06 19:26:03,428 - INFO - training batch 1101, loss: 0.258, 35232/60000 datapoints
2025-03-06 19:26:03,628 - INFO - training batch 1151, loss: 0.247, 36832/60000 datapoints
2025-03-06 19:26:03,823 - INFO - training batch 1201, loss: 0.290, 38432/60000 datapoints
2025-03-06 19:26:04,020 - INFO - training batch 1251, loss: 0.725, 40032/60000 datapoints
2025-03-06 19:26:04,216 - INFO - training batch 1301, loss: 0.240, 41632/60000 datapoints
2025-03-06 19:26:04,430 - INFO - training batch 1351, loss: 0.687, 43232/60000 datapoints
2025-03-06 19:26:04,627 - INFO - training batch 1401, loss: 0.193, 44832/60000 datapoints
2025-03-06 19:26:04,821 - INFO - training batch 1451, loss: 0.367, 46432/60000 datapoints
2025-03-06 19:26:05,018 - INFO - training batch 1501, loss: 0.137, 48032/60000 datapoints
2025-03-06 19:26:05,214 - INFO - training batch 1551, loss: 0.203, 49632/60000 datapoints
2025-03-06 19:26:05,408 - INFO - training batch 1601, loss: 0.194, 51232/60000 datapoints
2025-03-06 19:26:05,604 - INFO - training batch 1651, loss: 0.346, 52832/60000 datapoints
2025-03-06 19:26:05,800 - INFO - training batch 1701, loss: 0.188, 54432/60000 datapoints
2025-03-06 19:26:05,994 - INFO - training batch 1751, loss: 0.296, 56032/60000 datapoints
2025-03-06 19:26:06,191 - INFO - training batch 1801, loss: 0.210, 57632/60000 datapoints
2025-03-06 19:26:06,387 - INFO - training batch 1851, loss: 0.267, 59232/60000 datapoints
2025-03-06 19:26:06,488 - INFO - validation batch 1, loss: 0.458, 32/10016 datapoints
2025-03-06 19:26:06,644 - INFO - validation batch 51, loss: 0.377, 1632/10016 datapoints
2025-03-06 19:26:06,797 - INFO - validation batch 101, loss: 0.497, 3232/10016 datapoints
2025-03-06 19:26:06,950 - INFO - validation batch 151, loss: 0.259, 4832/10016 datapoints
2025-03-06 19:26:07,105 - INFO - validation batch 201, loss: 0.407, 6432/10016 datapoints
2025-03-06 19:26:07,261 - INFO - validation batch 251, loss: 0.322, 8032/10016 datapoints
2025-03-06 19:26:07,413 - INFO - validation batch 301, loss: 0.212, 9632/10016 datapoints
2025-03-06 19:26:07,450 - INFO - Epoch 322/800 done.
2025-03-06 19:26:07,451 - INFO - Final validation performance:
Loss: 0.362, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 19:26:07,451 - INFO - Beginning epoch 323/800
2025-03-06 19:26:07,458 - INFO - training batch 1, loss: 0.176, 32/60000 datapoints
2025-03-06 19:26:07,681 - INFO - training batch 51, loss: 0.342, 1632/60000 datapoints
2025-03-06 19:26:07,880 - INFO - training batch 101, loss: 0.292, 3232/60000 datapoints
2025-03-06 19:26:08,082 - INFO - training batch 151, loss: 0.482, 4832/60000 datapoints
2025-03-06 19:26:08,281 - INFO - training batch 201, loss: 0.107, 6432/60000 datapoints
2025-03-06 19:26:08,476 - INFO - training batch 251, loss: 0.274, 8032/60000 datapoints
2025-03-06 19:26:08,672 - INFO - training batch 301, loss: 0.455, 9632/60000 datapoints
2025-03-06 19:26:08,867 - INFO - training batch 351, loss: 0.147, 11232/60000 datapoints
2025-03-06 19:26:09,067 - INFO - training batch 401, loss: 0.255, 12832/60000 datapoints
2025-03-06 19:26:09,268 - INFO - training batch 451, loss: 0.412, 14432/60000 datapoints
2025-03-06 19:26:09,464 - INFO - training batch 501, loss: 0.399, 16032/60000 datapoints
2025-03-06 19:26:09,664 - INFO - training batch 551, loss: 0.234, 17632/60000 datapoints
2025-03-06 19:26:09,860 - INFO - training batch 601, loss: 0.375, 19232/60000 datapoints
2025-03-06 19:26:10,055 - INFO - training batch 651, loss: 0.062, 20832/60000 datapoints
2025-03-06 19:26:10,275 - INFO - training batch 701, loss: 0.228, 22432/60000 datapoints
2025-03-06 19:26:10,476 - INFO - training batch 751, loss: 0.339, 24032/60000 datapoints
2025-03-06 19:26:10,673 - INFO - training batch 801, loss: 0.349, 25632/60000 datapoints
2025-03-06 19:26:10,864 - INFO - training batch 851, loss: 0.266, 27232/60000 datapoints
2025-03-06 19:26:11,057 - INFO - training batch 901, loss: 0.313, 28832/60000 datapoints
2025-03-06 19:26:11,252 - INFO - training batch 951, loss: 0.375, 30432/60000 datapoints
2025-03-06 19:26:11,447 - INFO - training batch 1001, loss: 0.357, 32032/60000 datapoints
2025-03-06 19:26:11,646 - INFO - training batch 1051, loss: 0.180, 33632/60000 datapoints
2025-03-06 19:26:11,841 - INFO - training batch 1101, loss: 0.339, 35232/60000 datapoints
2025-03-06 19:26:12,035 - INFO - training batch 1151, loss: 0.178, 36832/60000 datapoints
2025-03-06 19:26:12,234 - INFO - training batch 1201, loss: 0.283, 38432/60000 datapoints
2025-03-06 19:26:12,431 - INFO - training batch 1251, loss: 0.530, 40032/60000 datapoints
2025-03-06 19:26:12,628 - INFO - training batch 1301, loss: 0.208, 41632/60000 datapoints
2025-03-06 19:26:12,822 - INFO - training batch 1351, loss: 0.495, 43232/60000 datapoints
2025-03-06 19:26:13,016 - INFO - training batch 1401, loss: 0.222, 44832/60000 datapoints
2025-03-06 19:26:13,212 - INFO - training batch 1451, loss: 0.146, 46432/60000 datapoints
2025-03-06 19:26:13,413 - INFO - training batch 1501, loss: 0.186, 48032/60000 datapoints
2025-03-06 19:26:13,639 - INFO - training batch 1551, loss: 0.365, 49632/60000 datapoints
2025-03-06 19:26:13,833 - INFO - training batch 1601, loss: 0.534, 51232/60000 datapoints
2025-03-06 19:26:14,030 - INFO - training batch 1651, loss: 0.299, 52832/60000 datapoints
2025-03-06 19:26:14,228 - INFO - training batch 1701, loss: 0.210, 54432/60000 datapoints
2025-03-06 19:26:14,438 - INFO - training batch 1751, loss: 0.415, 56032/60000 datapoints
2025-03-06 19:26:14,641 - INFO - training batch 1801, loss: 0.533, 57632/60000 datapoints
2025-03-06 19:26:14,835 - INFO - training batch 1851, loss: 0.439, 59232/60000 datapoints
2025-03-06 19:26:14,943 - INFO - validation batch 1, loss: 0.283, 32/10016 datapoints
2025-03-06 19:26:15,097 - INFO - validation batch 51, loss: 0.208, 1632/10016 datapoints
2025-03-06 19:26:15,250 - INFO - validation batch 101, loss: 0.566, 3232/10016 datapoints
2025-03-06 19:26:15,402 - INFO - validation batch 151, loss: 0.198, 4832/10016 datapoints
2025-03-06 19:26:15,557 - INFO - validation batch 201, loss: 0.536, 6432/10016 datapoints
2025-03-06 19:26:15,714 - INFO - validation batch 251, loss: 0.264, 8032/10016 datapoints
2025-03-06 19:26:15,866 - INFO - validation batch 301, loss: 0.253, 9632/10016 datapoints
2025-03-06 19:26:15,903 - INFO - Epoch 323/800 done.
2025-03-06 19:26:15,903 - INFO - Final validation performance:
Loss: 0.330, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 19:26:15,903 - INFO - Beginning epoch 324/800
2025-03-06 19:26:15,910 - INFO - training batch 1, loss: 0.229, 32/60000 datapoints
2025-03-06 19:26:16,126 - INFO - training batch 51, loss: 0.424, 1632/60000 datapoints
2025-03-06 19:26:16,327 - INFO - training batch 101, loss: 0.330, 3232/60000 datapoints
2025-03-06 19:26:16,528 - INFO - training batch 151, loss: 0.286, 4832/60000 datapoints
2025-03-06 19:26:16,728 - INFO - training batch 201, loss: 0.152, 6432/60000 datapoints
2025-03-06 19:26:16,927 - INFO - training batch 251, loss: 0.225, 8032/60000 datapoints
2025-03-06 19:26:17,120 - INFO - training batch 301, loss: 0.198, 9632/60000 datapoints
2025-03-06 19:26:17,317 - INFO - training batch 351, loss: 0.102, 11232/60000 datapoints
2025-03-06 19:26:17,512 - INFO - training batch 401, loss: 0.448, 12832/60000 datapoints
2025-03-06 19:26:17,711 - INFO - training batch 451, loss: 0.378, 14432/60000 datapoints
2025-03-06 19:26:17,906 - INFO - training batch 501, loss: 0.368, 16032/60000 datapoints
2025-03-06 19:26:18,101 - INFO - training batch 551, loss: 0.262, 17632/60000 datapoints
2025-03-06 19:26:18,298 - INFO - training batch 601, loss: 0.411, 19232/60000 datapoints
2025-03-06 19:26:18,494 - INFO - training batch 651, loss: 0.245, 20832/60000 datapoints
2025-03-06 19:26:18,691 - INFO - training batch 701, loss: 0.526, 22432/60000 datapoints
2025-03-06 19:26:18,886 - INFO - training batch 751, loss: 0.251, 24032/60000 datapoints
2025-03-06 19:26:19,081 - INFO - training batch 801, loss: 0.264, 25632/60000 datapoints
2025-03-06 19:26:19,284 - INFO - training batch 851, loss: 0.194, 27232/60000 datapoints
2025-03-06 19:26:19,484 - INFO - training batch 901, loss: 0.199, 28832/60000 datapoints
2025-03-06 19:26:19,684 - INFO - training batch 951, loss: 0.220, 30432/60000 datapoints
2025-03-06 19:26:19,878 - INFO - training batch 1001, loss: 0.108, 32032/60000 datapoints
2025-03-06 19:26:20,073 - INFO - training batch 1051, loss: 0.232, 33632/60000 datapoints
2025-03-06 19:26:20,269 - INFO - training batch 1101, loss: 0.129, 35232/60000 datapoints
2025-03-06 19:26:20,462 - INFO - training batch 1151, loss: 0.214, 36832/60000 datapoints
2025-03-06 19:26:20,658 - INFO - training batch 1201, loss: 0.919, 38432/60000 datapoints
2025-03-06 19:26:20,851 - INFO - training batch 1251, loss: 0.342, 40032/60000 datapoints
2025-03-06 19:26:21,045 - INFO - training batch 1301, loss: 0.404, 41632/60000 datapoints
2025-03-06 19:26:21,241 - INFO - training batch 1351, loss: 0.170, 43232/60000 datapoints
2025-03-06 19:26:21,435 - INFO - training batch 1401, loss: 0.136, 44832/60000 datapoints
2025-03-06 19:26:21,634 - INFO - training batch 1451, loss: 0.112, 46432/60000 datapoints
2025-03-06 19:26:21,830 - INFO - training batch 1501, loss: 0.229, 48032/60000 datapoints
2025-03-06 19:26:22,023 - INFO - training batch 1551, loss: 0.243, 49632/60000 datapoints
2025-03-06 19:26:22,217 - INFO - training batch 1601, loss: 0.134, 51232/60000 datapoints
2025-03-06 19:26:22,411 - INFO - training batch 1651, loss: 0.440, 52832/60000 datapoints
2025-03-06 19:26:22,602 - INFO - training batch 1701, loss: 0.478, 54432/60000 datapoints
2025-03-06 19:26:22,799 - INFO - training batch 1751, loss: 0.244, 56032/60000 datapoints
2025-03-06 19:26:22,992 - INFO - training batch 1801, loss: 0.208, 57632/60000 datapoints
2025-03-06 19:26:23,187 - INFO - training batch 1851, loss: 0.136, 59232/60000 datapoints
2025-03-06 19:26:23,292 - INFO - validation batch 1, loss: 0.204, 32/10016 datapoints
2025-03-06 19:26:23,443 - INFO - validation batch 51, loss: 0.150, 1632/10016 datapoints
2025-03-06 19:26:23,598 - INFO - validation batch 101, loss: 0.111, 3232/10016 datapoints
2025-03-06 19:26:23,757 - INFO - validation batch 151, loss: 0.281, 4832/10016 datapoints
2025-03-06 19:26:23,909 - INFO - validation batch 201, loss: 0.328, 6432/10016 datapoints
2025-03-06 19:26:24,066 - INFO - validation batch 251, loss: 0.407, 8032/10016 datapoints
2025-03-06 19:26:24,218 - INFO - validation batch 301, loss: 0.412, 9632/10016 datapoints
2025-03-06 19:26:24,259 - INFO - Epoch 324/800 done.
2025-03-06 19:26:24,260 - INFO - Final validation performance:
Loss: 0.271, top-1 acc: 0.914top-5 acc: 0.914
2025-03-06 19:26:24,260 - INFO - Beginning epoch 325/800
2025-03-06 19:26:24,268 - INFO - training batch 1, loss: 0.298, 32/60000 datapoints
2025-03-06 19:26:24,488 - INFO - training batch 51, loss: 0.172, 1632/60000 datapoints
2025-03-06 19:26:24,698 - INFO - training batch 101, loss: 0.188, 3232/60000 datapoints
2025-03-06 19:26:24,901 - INFO - training batch 151, loss: 0.447, 4832/60000 datapoints
2025-03-06 19:26:25,100 - INFO - training batch 201, loss: 0.196, 6432/60000 datapoints
2025-03-06 19:26:25,303 - INFO - training batch 251, loss: 0.253, 8032/60000 datapoints
2025-03-06 19:26:25,499 - INFO - training batch 301, loss: 0.250, 9632/60000 datapoints
2025-03-06 19:26:25,699 - INFO - training batch 351, loss: 0.315, 11232/60000 datapoints
2025-03-06 19:26:25,894 - INFO - training batch 401, loss: 0.175, 12832/60000 datapoints
2025-03-06 19:26:26,090 - INFO - training batch 451, loss: 0.199, 14432/60000 datapoints
2025-03-06 19:26:26,287 - INFO - training batch 501, loss: 0.158, 16032/60000 datapoints
2025-03-06 19:26:26,482 - INFO - training batch 551, loss: 0.273, 17632/60000 datapoints
2025-03-06 19:26:26,678 - INFO - training batch 601, loss: 0.127, 19232/60000 datapoints
2025-03-06 19:26:26,873 - INFO - training batch 651, loss: 0.392, 20832/60000 datapoints
2025-03-06 19:26:27,068 - INFO - training batch 701, loss: 0.408, 22432/60000 datapoints
2025-03-06 19:26:27,265 - INFO - training batch 751, loss: 0.601, 24032/60000 datapoints
2025-03-06 19:26:27,467 - INFO - training batch 801, loss: 0.145, 25632/60000 datapoints
2025-03-06 19:26:27,666 - INFO - training batch 851, loss: 0.357, 27232/60000 datapoints
2025-03-06 19:26:27,864 - INFO - training batch 901, loss: 0.243, 28832/60000 datapoints
2025-03-06 19:26:28,060 - INFO - training batch 951, loss: 0.255, 30432/60000 datapoints
2025-03-06 19:26:28,254 - INFO - training batch 1001, loss: 0.344, 32032/60000 datapoints
2025-03-06 19:26:28,452 - INFO - training batch 1051, loss: 0.242, 33632/60000 datapoints
2025-03-06 19:26:28,649 - INFO - training batch 1101, loss: 0.197, 35232/60000 datapoints
2025-03-06 19:26:28,849 - INFO - training batch 1151, loss: 0.412, 36832/60000 datapoints
2025-03-06 19:26:29,045 - INFO - training batch 1201, loss: 0.247, 38432/60000 datapoints
2025-03-06 19:26:29,239 - INFO - training batch 1251, loss: 0.526, 40032/60000 datapoints
2025-03-06 19:26:29,437 - INFO - training batch 1301, loss: 0.357, 41632/60000 datapoints
2025-03-06 19:26:29,638 - INFO - training batch 1351, loss: 0.117, 43232/60000 datapoints
2025-03-06 19:26:29,834 - INFO - training batch 1401, loss: 0.168, 44832/60000 datapoints
2025-03-06 19:26:30,030 - INFO - training batch 1451, loss: 0.096, 46432/60000 datapoints
2025-03-06 19:26:30,224 - INFO - training batch 1501, loss: 0.459, 48032/60000 datapoints
2025-03-06 19:26:30,419 - INFO - training batch 1551, loss: 0.524, 49632/60000 datapoints
2025-03-06 19:26:30,613 - INFO - training batch 1601, loss: 0.196, 51232/60000 datapoints
2025-03-06 19:26:30,808 - INFO - training batch 1651, loss: 0.181, 52832/60000 datapoints
2025-03-06 19:26:31,002 - INFO - training batch 1701, loss: 0.678, 54432/60000 datapoints
2025-03-06 19:26:31,195 - INFO - training batch 1751, loss: 0.290, 56032/60000 datapoints
2025-03-06 19:26:31,389 - INFO - training batch 1801, loss: 0.127, 57632/60000 datapoints
2025-03-06 19:26:31,583 - INFO - training batch 1851, loss: 0.507, 59232/60000 datapoints
2025-03-06 19:26:31,690 - INFO - validation batch 1, loss: 0.597, 32/10016 datapoints
2025-03-06 19:26:31,844 - INFO - validation batch 51, loss: 0.284, 1632/10016 datapoints
2025-03-06 19:26:31,996 - INFO - validation batch 101, loss: 0.194, 3232/10016 datapoints
2025-03-06 19:26:32,149 - INFO - validation batch 151, loss: 0.557, 4832/10016 datapoints
2025-03-06 19:26:32,302 - INFO - validation batch 201, loss: 0.230, 6432/10016 datapoints
2025-03-06 19:26:32,455 - INFO - validation batch 251, loss: 0.205, 8032/10016 datapoints
2025-03-06 19:26:32,609 - INFO - validation batch 301, loss: 0.272, 9632/10016 datapoints
2025-03-06 19:26:32,648 - INFO - Epoch 325/800 done.
2025-03-06 19:26:32,649 - INFO - Final validation performance:
Loss: 0.334, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:26:32,649 - INFO - Beginning epoch 326/800
2025-03-06 19:26:32,656 - INFO - training batch 1, loss: 0.335, 32/60000 datapoints
2025-03-06 19:26:32,867 - INFO - training batch 51, loss: 0.444, 1632/60000 datapoints
2025-03-06 19:26:33,060 - INFO - training batch 101, loss: 0.141, 3232/60000 datapoints
2025-03-06 19:26:33,266 - INFO - training batch 151, loss: 0.477, 4832/60000 datapoints
2025-03-06 19:26:33,467 - INFO - training batch 201, loss: 0.301, 6432/60000 datapoints
2025-03-06 19:26:33,672 - INFO - training batch 251, loss: 0.212, 8032/60000 datapoints
2025-03-06 19:26:33,867 - INFO - training batch 301, loss: 0.156, 9632/60000 datapoints
2025-03-06 19:26:34,072 - INFO - training batch 351, loss: 0.411, 11232/60000 datapoints
2025-03-06 19:26:34,272 - INFO - training batch 401, loss: 0.363, 12832/60000 datapoints
2025-03-06 19:26:34,466 - INFO - training batch 451, loss: 0.358, 14432/60000 datapoints
2025-03-06 19:26:34,686 - INFO - training batch 501, loss: 0.355, 16032/60000 datapoints
2025-03-06 19:26:34,886 - INFO - training batch 551, loss: 0.392, 17632/60000 datapoints
2025-03-06 19:26:35,081 - INFO - training batch 601, loss: 0.180, 19232/60000 datapoints
2025-03-06 19:26:35,275 - INFO - training batch 651, loss: 0.229, 20832/60000 datapoints
2025-03-06 19:26:35,474 - INFO - training batch 701, loss: 0.480, 22432/60000 datapoints
2025-03-06 19:26:35,675 - INFO - training batch 751, loss: 0.163, 24032/60000 datapoints
2025-03-06 19:26:35,872 - INFO - training batch 801, loss: 0.186, 25632/60000 datapoints
2025-03-06 19:26:36,068 - INFO - training batch 851, loss: 0.354, 27232/60000 datapoints
2025-03-06 19:26:36,268 - INFO - training batch 901, loss: 0.585, 28832/60000 datapoints
2025-03-06 19:26:36,473 - INFO - training batch 951, loss: 0.275, 30432/60000 datapoints
2025-03-06 19:26:36,670 - INFO - training batch 1001, loss: 0.175, 32032/60000 datapoints
2025-03-06 19:26:36,863 - INFO - training batch 1051, loss: 0.545, 33632/60000 datapoints
2025-03-06 19:26:37,061 - INFO - training batch 1101, loss: 0.318, 35232/60000 datapoints
2025-03-06 19:26:37,260 - INFO - training batch 1151, loss: 0.397, 36832/60000 datapoints
2025-03-06 19:26:37,455 - INFO - training batch 1201, loss: 0.589, 38432/60000 datapoints
2025-03-06 19:26:37,677 - INFO - training batch 1251, loss: 0.322, 40032/60000 datapoints
2025-03-06 19:26:37,875 - INFO - training batch 1301, loss: 0.594, 41632/60000 datapoints
2025-03-06 19:26:38,071 - INFO - training batch 1351, loss: 0.289, 43232/60000 datapoints
2025-03-06 19:26:38,267 - INFO - training batch 1401, loss: 0.094, 44832/60000 datapoints
2025-03-06 19:26:38,464 - INFO - training batch 1451, loss: 0.384, 46432/60000 datapoints
2025-03-06 19:26:38,659 - INFO - training batch 1501, loss: 0.241, 48032/60000 datapoints
2025-03-06 19:26:38,854 - INFO - training batch 1551, loss: 0.480, 49632/60000 datapoints
2025-03-06 19:26:39,060 - INFO - training batch 1601, loss: 0.265, 51232/60000 datapoints
2025-03-06 19:26:39,256 - INFO - training batch 1651, loss: 0.479, 52832/60000 datapoints
2025-03-06 19:26:39,454 - INFO - training batch 1701, loss: 0.302, 54432/60000 datapoints
2025-03-06 19:26:39,652 - INFO - training batch 1751, loss: 0.497, 56032/60000 datapoints
2025-03-06 19:26:39,846 - INFO - training batch 1801, loss: 0.235, 57632/60000 datapoints
2025-03-06 19:26:40,043 - INFO - training batch 1851, loss: 0.247, 59232/60000 datapoints
2025-03-06 19:26:40,144 - INFO - validation batch 1, loss: 0.178, 32/10016 datapoints
2025-03-06 19:26:40,297 - INFO - validation batch 51, loss: 0.109, 1632/10016 datapoints
2025-03-06 19:26:40,456 - INFO - validation batch 101, loss: 0.221, 3232/10016 datapoints
2025-03-06 19:26:40,609 - INFO - validation batch 151, loss: 0.247, 4832/10016 datapoints
2025-03-06 19:26:40,764 - INFO - validation batch 201, loss: 0.228, 6432/10016 datapoints
2025-03-06 19:26:40,919 - INFO - validation batch 251, loss: 0.076, 8032/10016 datapoints
2025-03-06 19:26:41,073 - INFO - validation batch 301, loss: 0.247, 9632/10016 datapoints
2025-03-06 19:26:41,109 - INFO - Epoch 326/800 done.
2025-03-06 19:26:41,110 - INFO - Final validation performance:
Loss: 0.186, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:26:41,110 - INFO - Beginning epoch 327/800
2025-03-06 19:26:41,117 - INFO - training batch 1, loss: 0.182, 32/60000 datapoints
2025-03-06 19:26:41,312 - INFO - training batch 51, loss: 0.396, 1632/60000 datapoints
2025-03-06 19:26:41,514 - INFO - training batch 101, loss: 0.340, 3232/60000 datapoints
2025-03-06 19:26:41,716 - INFO - training batch 151, loss: 0.146, 4832/60000 datapoints
2025-03-06 19:26:41,909 - INFO - training batch 201, loss: 0.683, 6432/60000 datapoints
2025-03-06 19:26:42,108 - INFO - training batch 251, loss: 0.324, 8032/60000 datapoints
2025-03-06 19:26:42,303 - INFO - training batch 301, loss: 0.266, 9632/60000 datapoints
2025-03-06 19:26:42,502 - INFO - training batch 351, loss: 0.453, 11232/60000 datapoints
2025-03-06 19:26:42,694 - INFO - training batch 401, loss: 0.143, 12832/60000 datapoints
2025-03-06 19:26:42,888 - INFO - training batch 451, loss: 0.288, 14432/60000 datapoints
2025-03-06 19:26:43,080 - INFO - training batch 501, loss: 0.544, 16032/60000 datapoints
2025-03-06 19:26:43,272 - INFO - training batch 551, loss: 0.346, 17632/60000 datapoints
2025-03-06 19:26:43,465 - INFO - training batch 601, loss: 0.691, 19232/60000 datapoints
2025-03-06 19:26:43,659 - INFO - training batch 651, loss: 0.216, 20832/60000 datapoints
2025-03-06 19:26:43,854 - INFO - training batch 701, loss: 0.178, 22432/60000 datapoints
2025-03-06 19:26:44,077 - INFO - training batch 751, loss: 0.199, 24032/60000 datapoints
2025-03-06 19:26:44,275 - INFO - training batch 801, loss: 0.213, 25632/60000 datapoints
2025-03-06 19:26:44,469 - INFO - training batch 851, loss: 0.118, 27232/60000 datapoints
2025-03-06 19:26:44,677 - INFO - training batch 901, loss: 0.226, 28832/60000 datapoints
2025-03-06 19:26:44,874 - INFO - training batch 951, loss: 0.182, 30432/60000 datapoints
2025-03-06 19:26:45,071 - INFO - training batch 1001, loss: 0.449, 32032/60000 datapoints
2025-03-06 19:26:45,264 - INFO - training batch 1051, loss: 0.266, 33632/60000 datapoints
2025-03-06 19:26:45,457 - INFO - training batch 1101, loss: 0.428, 35232/60000 datapoints
2025-03-06 19:26:45,661 - INFO - training batch 1151, loss: 0.378, 36832/60000 datapoints
2025-03-06 19:26:45,856 - INFO - training batch 1201, loss: 0.331, 38432/60000 datapoints
2025-03-06 19:26:46,049 - INFO - training batch 1251, loss: 0.303, 40032/60000 datapoints
2025-03-06 19:26:46,239 - INFO - training batch 1301, loss: 0.131, 41632/60000 datapoints
2025-03-06 19:26:46,431 - INFO - training batch 1351, loss: 0.161, 43232/60000 datapoints
2025-03-06 19:26:46,627 - INFO - training batch 1401, loss: 0.128, 44832/60000 datapoints
2025-03-06 19:26:46,819 - INFO - training batch 1451, loss: 0.703, 46432/60000 datapoints
2025-03-06 19:26:47,012 - INFO - training batch 1501, loss: 0.168, 48032/60000 datapoints
2025-03-06 19:26:47,203 - INFO - training batch 1551, loss: 0.503, 49632/60000 datapoints
2025-03-06 19:26:47,395 - INFO - training batch 1601, loss: 0.591, 51232/60000 datapoints
2025-03-06 19:26:47,588 - INFO - training batch 1651, loss: 0.222, 52832/60000 datapoints
2025-03-06 19:26:47,785 - INFO - training batch 1701, loss: 0.203, 54432/60000 datapoints
2025-03-06 19:26:47,980 - INFO - training batch 1751, loss: 0.236, 56032/60000 datapoints
2025-03-06 19:26:48,176 - INFO - training batch 1801, loss: 0.392, 57632/60000 datapoints
2025-03-06 19:26:48,368 - INFO - training batch 1851, loss: 0.201, 59232/60000 datapoints
2025-03-06 19:26:48,468 - INFO - validation batch 1, loss: 0.140, 32/10016 datapoints
2025-03-06 19:26:48,619 - INFO - validation batch 51, loss: 0.184, 1632/10016 datapoints
2025-03-06 19:26:48,771 - INFO - validation batch 101, loss: 0.194, 3232/10016 datapoints
2025-03-06 19:26:48,921 - INFO - validation batch 151, loss: 0.481, 4832/10016 datapoints
2025-03-06 19:26:49,075 - INFO - validation batch 201, loss: 0.345, 6432/10016 datapoints
2025-03-06 19:26:49,225 - INFO - validation batch 251, loss: 0.326, 8032/10016 datapoints
2025-03-06 19:26:49,376 - INFO - validation batch 301, loss: 0.366, 9632/10016 datapoints
2025-03-06 19:26:49,412 - INFO - Epoch 327/800 done.
2025-03-06 19:26:49,412 - INFO - Final validation performance:
Loss: 0.291, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:26:49,413 - INFO - Beginning epoch 328/800
2025-03-06 19:26:49,419 - INFO - training batch 1, loss: 0.513, 32/60000 datapoints
2025-03-06 19:26:49,630 - INFO - training batch 51, loss: 0.319, 1632/60000 datapoints
2025-03-06 19:26:49,824 - INFO - training batch 101, loss: 0.349, 3232/60000 datapoints
2025-03-06 19:26:50,025 - INFO - training batch 151, loss: 0.271, 4832/60000 datapoints
2025-03-06 19:26:50,221 - INFO - training batch 201, loss: 0.191, 6432/60000 datapoints
2025-03-06 19:26:50,418 - INFO - training batch 251, loss: 0.339, 8032/60000 datapoints
2025-03-06 19:26:50,611 - INFO - training batch 301, loss: 0.188, 9632/60000 datapoints
2025-03-06 19:26:50,804 - INFO - training batch 351, loss: 0.091, 11232/60000 datapoints
2025-03-06 19:26:50,995 - INFO - training batch 401, loss: 0.222, 12832/60000 datapoints
2025-03-06 19:26:51,189 - INFO - training batch 451, loss: 0.219, 14432/60000 datapoints
2025-03-06 19:26:51,381 - INFO - training batch 501, loss: 0.356, 16032/60000 datapoints
2025-03-06 19:26:51,573 - INFO - training batch 551, loss: 0.156, 17632/60000 datapoints
2025-03-06 19:26:51,768 - INFO - training batch 601, loss: 0.158, 19232/60000 datapoints
2025-03-06 19:26:51,962 - INFO - training batch 651, loss: 0.087, 20832/60000 datapoints
2025-03-06 19:26:52,155 - INFO - training batch 701, loss: 0.166, 22432/60000 datapoints
2025-03-06 19:26:52,346 - INFO - training batch 751, loss: 0.156, 24032/60000 datapoints
2025-03-06 19:26:52,538 - INFO - training batch 801, loss: 0.276, 25632/60000 datapoints
2025-03-06 19:26:52,733 - INFO - training batch 851, loss: 0.578, 27232/60000 datapoints
2025-03-06 19:26:52,927 - INFO - training batch 901, loss: 0.131, 28832/60000 datapoints
2025-03-06 19:26:53,118 - INFO - training batch 951, loss: 0.364, 30432/60000 datapoints
2025-03-06 19:26:53,312 - INFO - training batch 1001, loss: 0.191, 32032/60000 datapoints
2025-03-06 19:26:53,504 - INFO - training batch 1051, loss: 0.270, 33632/60000 datapoints
2025-03-06 19:26:53,698 - INFO - training batch 1101, loss: 0.226, 35232/60000 datapoints
2025-03-06 19:26:53,893 - INFO - training batch 1151, loss: 0.464, 36832/60000 datapoints
2025-03-06 19:26:54,089 - INFO - training batch 1201, loss: 0.531, 38432/60000 datapoints
2025-03-06 19:26:54,280 - INFO - training batch 1251, loss: 0.256, 40032/60000 datapoints
2025-03-06 19:26:54,472 - INFO - training batch 1301, loss: 0.311, 41632/60000 datapoints
2025-03-06 19:26:54,667 - INFO - training batch 1351, loss: 0.287, 43232/60000 datapoints
2025-03-06 19:26:54,880 - INFO - training batch 1401, loss: 0.480, 44832/60000 datapoints
2025-03-06 19:26:55,075 - INFO - training batch 1451, loss: 0.353, 46432/60000 datapoints
2025-03-06 19:26:55,268 - INFO - training batch 1501, loss: 0.247, 48032/60000 datapoints
2025-03-06 19:26:55,461 - INFO - training batch 1551, loss: 0.195, 49632/60000 datapoints
2025-03-06 19:26:55,658 - INFO - training batch 1601, loss: 0.281, 51232/60000 datapoints
2025-03-06 19:26:55,851 - INFO - training batch 1651, loss: 0.288, 52832/60000 datapoints
2025-03-06 19:26:56,043 - INFO - training batch 1701, loss: 0.136, 54432/60000 datapoints
2025-03-06 19:26:56,236 - INFO - training batch 1751, loss: 0.364, 56032/60000 datapoints
2025-03-06 19:26:56,428 - INFO - training batch 1801, loss: 0.565, 57632/60000 datapoints
2025-03-06 19:26:56,623 - INFO - training batch 1851, loss: 0.240, 59232/60000 datapoints
2025-03-06 19:26:56,721 - INFO - validation batch 1, loss: 0.393, 32/10016 datapoints
2025-03-06 19:26:56,871 - INFO - validation batch 51, loss: 0.275, 1632/10016 datapoints
2025-03-06 19:26:57,021 - INFO - validation batch 101, loss: 0.307, 3232/10016 datapoints
2025-03-06 19:26:57,172 - INFO - validation batch 151, loss: 0.284, 4832/10016 datapoints
2025-03-06 19:26:57,321 - INFO - validation batch 201, loss: 0.163, 6432/10016 datapoints
2025-03-06 19:26:57,470 - INFO - validation batch 251, loss: 0.096, 8032/10016 datapoints
2025-03-06 19:26:57,624 - INFO - validation batch 301, loss: 0.423, 9632/10016 datapoints
2025-03-06 19:26:57,660 - INFO - Epoch 328/800 done.
2025-03-06 19:26:57,660 - INFO - Final validation performance:
Loss: 0.277, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:26:57,661 - INFO - Beginning epoch 329/800
2025-03-06 19:26:57,667 - INFO - training batch 1, loss: 0.437, 32/60000 datapoints
2025-03-06 19:26:57,862 - INFO - training batch 51, loss: 0.290, 1632/60000 datapoints
2025-03-06 19:26:58,056 - INFO - training batch 101, loss: 0.159, 3232/60000 datapoints
2025-03-06 19:26:58,260 - INFO - training batch 151, loss: 0.255, 4832/60000 datapoints
2025-03-06 19:26:58,459 - INFO - training batch 201, loss: 0.376, 6432/60000 datapoints
2025-03-06 19:26:58,691 - INFO - training batch 251, loss: 0.346, 8032/60000 datapoints
2025-03-06 19:26:58,892 - INFO - training batch 301, loss: 0.186, 9632/60000 datapoints
2025-03-06 19:26:59,085 - INFO - training batch 351, loss: 0.308, 11232/60000 datapoints
2025-03-06 19:26:59,283 - INFO - training batch 401, loss: 0.213, 12832/60000 datapoints
2025-03-06 19:26:59,486 - INFO - training batch 451, loss: 0.354, 14432/60000 datapoints
2025-03-06 19:26:59,683 - INFO - training batch 501, loss: 0.073, 16032/60000 datapoints
2025-03-06 19:26:59,880 - INFO - training batch 551, loss: 0.365, 17632/60000 datapoints
2025-03-06 19:27:00,087 - INFO - training batch 601, loss: 0.152, 19232/60000 datapoints
2025-03-06 19:27:00,284 - INFO - training batch 651, loss: 0.510, 20832/60000 datapoints
2025-03-06 19:27:00,481 - INFO - training batch 701, loss: 0.263, 22432/60000 datapoints
2025-03-06 19:27:00,691 - INFO - training batch 751, loss: 0.423, 24032/60000 datapoints
2025-03-06 19:27:00,909 - INFO - training batch 801, loss: 0.312, 25632/60000 datapoints
2025-03-06 19:27:01,102 - INFO - training batch 851, loss: 0.340, 27232/60000 datapoints
2025-03-06 19:27:01,297 - INFO - training batch 901, loss: 0.258, 28832/60000 datapoints
2025-03-06 19:27:01,493 - INFO - training batch 951, loss: 0.172, 30432/60000 datapoints
2025-03-06 19:27:01,692 - INFO - training batch 1001, loss: 0.663, 32032/60000 datapoints
2025-03-06 19:27:01,908 - INFO - training batch 1051, loss: 0.226, 33632/60000 datapoints
2025-03-06 19:27:02,100 - INFO - training batch 1101, loss: 0.590, 35232/60000 datapoints
2025-03-06 19:27:02,294 - INFO - training batch 1151, loss: 0.399, 36832/60000 datapoints
2025-03-06 19:27:02,489 - INFO - training batch 1201, loss: 0.358, 38432/60000 datapoints
2025-03-06 19:27:02,687 - INFO - training batch 1251, loss: 0.127, 40032/60000 datapoints
2025-03-06 19:27:02,882 - INFO - training batch 1301, loss: 0.176, 41632/60000 datapoints
2025-03-06 19:27:03,076 - INFO - training batch 1351, loss: 0.286, 43232/60000 datapoints
2025-03-06 19:27:03,269 - INFO - training batch 1401, loss: 0.340, 44832/60000 datapoints
2025-03-06 19:27:03,464 - INFO - training batch 1451, loss: 0.497, 46432/60000 datapoints
2025-03-06 19:27:03,660 - INFO - training batch 1501, loss: 0.192, 48032/60000 datapoints
2025-03-06 19:27:03,857 - INFO - training batch 1551, loss: 0.299, 49632/60000 datapoints
2025-03-06 19:27:04,056 - INFO - training batch 1601, loss: 0.431, 51232/60000 datapoints
2025-03-06 19:27:04,251 - INFO - training batch 1651, loss: 0.202, 52832/60000 datapoints
2025-03-06 19:27:04,447 - INFO - training batch 1701, loss: 0.331, 54432/60000 datapoints
2025-03-06 19:27:04,645 - INFO - training batch 1751, loss: 0.133, 56032/60000 datapoints
2025-03-06 19:27:04,862 - INFO - training batch 1801, loss: 0.157, 57632/60000 datapoints
2025-03-06 19:27:05,062 - INFO - training batch 1851, loss: 0.580, 59232/60000 datapoints
2025-03-06 19:27:05,165 - INFO - validation batch 1, loss: 0.305, 32/10016 datapoints
2025-03-06 19:27:05,317 - INFO - validation batch 51, loss: 0.188, 1632/10016 datapoints
2025-03-06 19:27:05,471 - INFO - validation batch 101, loss: 0.301, 3232/10016 datapoints
2025-03-06 19:27:05,625 - INFO - validation batch 151, loss: 0.176, 4832/10016 datapoints
2025-03-06 19:27:05,781 - INFO - validation batch 201, loss: 0.065, 6432/10016 datapoints
2025-03-06 19:27:05,945 - INFO - validation batch 251, loss: 0.203, 8032/10016 datapoints
2025-03-06 19:27:06,096 - INFO - validation batch 301, loss: 0.168, 9632/10016 datapoints
2025-03-06 19:27:06,133 - INFO - Epoch 329/800 done.
2025-03-06 19:27:06,133 - INFO - Final validation performance:
Loss: 0.201, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:27:06,133 - INFO - Beginning epoch 330/800
2025-03-06 19:27:06,139 - INFO - training batch 1, loss: 0.457, 32/60000 datapoints
2025-03-06 19:27:06,336 - INFO - training batch 51, loss: 0.217, 1632/60000 datapoints
2025-03-06 19:27:06,531 - INFO - training batch 101, loss: 0.217, 3232/60000 datapoints
2025-03-06 19:27:06,739 - INFO - training batch 151, loss: 0.348, 4832/60000 datapoints
2025-03-06 19:27:06,938 - INFO - training batch 201, loss: 0.444, 6432/60000 datapoints
2025-03-06 19:27:07,152 - INFO - training batch 251, loss: 0.255, 8032/60000 datapoints
2025-03-06 19:27:07,355 - INFO - training batch 301, loss: 0.070, 9632/60000 datapoints
2025-03-06 19:27:07,552 - INFO - training batch 351, loss: 0.485, 11232/60000 datapoints
2025-03-06 19:27:07,752 - INFO - training batch 401, loss: 0.452, 12832/60000 datapoints
2025-03-06 19:27:07,956 - INFO - training batch 451, loss: 0.450, 14432/60000 datapoints
2025-03-06 19:27:08,176 - INFO - training batch 501, loss: 0.330, 16032/60000 datapoints
2025-03-06 19:27:08,372 - INFO - training batch 551, loss: 0.389, 17632/60000 datapoints
2025-03-06 19:27:08,568 - INFO - training batch 601, loss: 0.357, 19232/60000 datapoints
2025-03-06 19:27:08,765 - INFO - training batch 651, loss: 0.276, 20832/60000 datapoints
2025-03-06 19:27:08,957 - INFO - training batch 701, loss: 0.620, 22432/60000 datapoints
2025-03-06 19:27:09,153 - INFO - training batch 751, loss: 0.321, 24032/60000 datapoints
2025-03-06 19:27:09,350 - INFO - training batch 801, loss: 0.297, 25632/60000 datapoints
2025-03-06 19:27:09,550 - INFO - training batch 851, loss: 0.144, 27232/60000 datapoints
2025-03-06 19:27:09,756 - INFO - training batch 901, loss: 0.241, 28832/60000 datapoints
2025-03-06 19:27:09,953 - INFO - training batch 951, loss: 0.432, 30432/60000 datapoints
2025-03-06 19:27:10,149 - INFO - training batch 1001, loss: 0.259, 32032/60000 datapoints
2025-03-06 19:27:10,343 - INFO - training batch 1051, loss: 0.465, 33632/60000 datapoints
2025-03-06 19:27:10,538 - INFO - training batch 1101, loss: 0.197, 35232/60000 datapoints
2025-03-06 19:27:10,770 - INFO - training batch 1151, loss: 0.337, 36832/60000 datapoints
2025-03-06 19:27:10,966 - INFO - training batch 1201, loss: 0.420, 38432/60000 datapoints
2025-03-06 19:27:11,162 - INFO - training batch 1251, loss: 0.257, 40032/60000 datapoints
2025-03-06 19:27:11,357 - INFO - training batch 1301, loss: 0.144, 41632/60000 datapoints
2025-03-06 19:27:11,551 - INFO - training batch 1351, loss: 0.205, 43232/60000 datapoints
2025-03-06 19:27:11,747 - INFO - training batch 1401, loss: 0.127, 44832/60000 datapoints
2025-03-06 19:27:11,943 - INFO - training batch 1451, loss: 0.190, 46432/60000 datapoints
2025-03-06 19:27:12,143 - INFO - training batch 1501, loss: 0.195, 48032/60000 datapoints
2025-03-06 19:27:12,340 - INFO - training batch 1551, loss: 0.129, 49632/60000 datapoints
2025-03-06 19:27:12,533 - INFO - training batch 1601, loss: 0.454, 51232/60000 datapoints
2025-03-06 19:27:12,727 - INFO - training batch 1651, loss: 0.253, 52832/60000 datapoints
2025-03-06 19:27:12,922 - INFO - training batch 1701, loss: 0.405, 54432/60000 datapoints
2025-03-06 19:27:13,116 - INFO - training batch 1751, loss: 0.269, 56032/60000 datapoints
2025-03-06 19:27:13,311 - INFO - training batch 1801, loss: 0.261, 57632/60000 datapoints
2025-03-06 19:27:13,505 - INFO - training batch 1851, loss: 0.190, 59232/60000 datapoints
2025-03-06 19:27:13,608 - INFO - validation batch 1, loss: 0.243, 32/10016 datapoints
2025-03-06 19:27:13,763 - INFO - validation batch 51, loss: 0.323, 1632/10016 datapoints
2025-03-06 19:27:13,918 - INFO - validation batch 101, loss: 0.597, 3232/10016 datapoints
2025-03-06 19:27:14,075 - INFO - validation batch 151, loss: 0.190, 4832/10016 datapoints
2025-03-06 19:27:14,227 - INFO - validation batch 201, loss: 0.302, 6432/10016 datapoints
2025-03-06 19:27:14,385 - INFO - validation batch 251, loss: 0.216, 8032/10016 datapoints
2025-03-06 19:27:14,542 - INFO - validation batch 301, loss: 0.189, 9632/10016 datapoints
2025-03-06 19:27:14,579 - INFO - Epoch 330/800 done.
2025-03-06 19:27:14,579 - INFO - Final validation performance:
Loss: 0.294, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:27:14,580 - INFO - Beginning epoch 331/800
2025-03-06 19:27:14,588 - INFO - training batch 1, loss: 0.249, 32/60000 datapoints
2025-03-06 19:27:14,798 - INFO - training batch 51, loss: 0.482, 1632/60000 datapoints
2025-03-06 19:27:15,017 - INFO - training batch 101, loss: 0.257, 3232/60000 datapoints
2025-03-06 19:27:15,214 - INFO - training batch 151, loss: 0.126, 4832/60000 datapoints
2025-03-06 19:27:15,412 - INFO - training batch 201, loss: 0.309, 6432/60000 datapoints
2025-03-06 19:27:15,609 - INFO - training batch 251, loss: 0.438, 8032/60000 datapoints
2025-03-06 19:27:15,808 - INFO - training batch 301, loss: 0.212, 9632/60000 datapoints
2025-03-06 19:27:16,005 - INFO - training batch 351, loss: 0.264, 11232/60000 datapoints
2025-03-06 19:27:16,200 - INFO - training batch 401, loss: 0.409, 12832/60000 datapoints
2025-03-06 19:27:16,394 - INFO - training batch 451, loss: 0.173, 14432/60000 datapoints
2025-03-06 19:27:16,591 - INFO - training batch 501, loss: 0.421, 16032/60000 datapoints
2025-03-06 19:27:16,788 - INFO - training batch 551, loss: 0.235, 17632/60000 datapoints
2025-03-06 19:27:16,983 - INFO - training batch 601, loss: 0.403, 19232/60000 datapoints
2025-03-06 19:27:17,178 - INFO - training batch 651, loss: 0.293, 20832/60000 datapoints
2025-03-06 19:27:17,373 - INFO - training batch 701, loss: 0.231, 22432/60000 datapoints
2025-03-06 19:27:17,569 - INFO - training batch 751, loss: 0.607, 24032/60000 datapoints
2025-03-06 19:27:17,764 - INFO - training batch 801, loss: 0.382, 25632/60000 datapoints
2025-03-06 19:27:17,963 - INFO - training batch 851, loss: 0.183, 27232/60000 datapoints
2025-03-06 19:27:18,157 - INFO - training batch 901, loss: 0.164, 28832/60000 datapoints
2025-03-06 19:27:18,349 - INFO - training batch 951, loss: 0.188, 30432/60000 datapoints
2025-03-06 19:27:18,543 - INFO - training batch 1001, loss: 0.282, 32032/60000 datapoints
2025-03-06 19:27:18,737 - INFO - training batch 1051, loss: 0.309, 33632/60000 datapoints
2025-03-06 19:27:18,932 - INFO - training batch 1101, loss: 0.195, 35232/60000 datapoints
2025-03-06 19:27:19,126 - INFO - training batch 1151, loss: 0.579, 36832/60000 datapoints
2025-03-06 19:27:19,321 - INFO - training batch 1201, loss: 0.175, 38432/60000 datapoints
2025-03-06 19:27:19,515 - INFO - training batch 1251, loss: 0.150, 40032/60000 datapoints
2025-03-06 19:27:19,718 - INFO - training batch 1301, loss: 0.132, 41632/60000 datapoints
2025-03-06 19:27:19,916 - INFO - training batch 1351, loss: 0.240, 43232/60000 datapoints
2025-03-06 19:27:20,112 - INFO - training batch 1401, loss: 0.236, 44832/60000 datapoints
2025-03-06 19:27:20,304 - INFO - training batch 1451, loss: 0.334, 46432/60000 datapoints
2025-03-06 19:27:20,499 - INFO - training batch 1501, loss: 0.325, 48032/60000 datapoints
2025-03-06 19:27:20,697 - INFO - training batch 1551, loss: 0.513, 49632/60000 datapoints
2025-03-06 19:27:20,894 - INFO - training batch 1601, loss: 0.375, 51232/60000 datapoints
2025-03-06 19:27:21,089 - INFO - training batch 1651, loss: 0.188, 52832/60000 datapoints
2025-03-06 19:27:21,284 - INFO - training batch 1701, loss: 0.350, 54432/60000 datapoints
2025-03-06 19:27:21,481 - INFO - training batch 1751, loss: 0.189, 56032/60000 datapoints
2025-03-06 19:27:21,677 - INFO - training batch 1801, loss: 0.225, 57632/60000 datapoints
2025-03-06 19:27:21,874 - INFO - training batch 1851, loss: 0.506, 59232/60000 datapoints
2025-03-06 19:27:21,978 - INFO - validation batch 1, loss: 0.258, 32/10016 datapoints
2025-03-06 19:27:22,132 - INFO - validation batch 51, loss: 0.106, 1632/10016 datapoints
2025-03-06 19:27:22,283 - INFO - validation batch 101, loss: 0.360, 3232/10016 datapoints
2025-03-06 19:27:22,437 - INFO - validation batch 151, loss: 0.306, 4832/10016 datapoints
2025-03-06 19:27:22,588 - INFO - validation batch 201, loss: 0.645, 6432/10016 datapoints
2025-03-06 19:27:22,742 - INFO - validation batch 251, loss: 0.278, 8032/10016 datapoints
2025-03-06 19:27:22,895 - INFO - validation batch 301, loss: 0.347, 9632/10016 datapoints
2025-03-06 19:27:22,935 - INFO - Epoch 331/800 done.
2025-03-06 19:27:22,935 - INFO - Final validation performance:
Loss: 0.329, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:27:22,935 - INFO - Beginning epoch 332/800
2025-03-06 19:27:22,942 - INFO - training batch 1, loss: 0.457, 32/60000 datapoints
2025-03-06 19:27:23,136 - INFO - training batch 51, loss: 0.143, 1632/60000 datapoints
2025-03-06 19:27:23,330 - INFO - training batch 101, loss: 0.376, 3232/60000 datapoints
2025-03-06 19:27:23,541 - INFO - training batch 151, loss: 0.204, 4832/60000 datapoints
2025-03-06 19:27:23,739 - INFO - training batch 201, loss: 0.178, 6432/60000 datapoints
2025-03-06 19:27:23,938 - INFO - training batch 251, loss: 0.237, 8032/60000 datapoints
2025-03-06 19:27:24,144 - INFO - training batch 301, loss: 0.567, 9632/60000 datapoints
2025-03-06 19:27:24,341 - INFO - training batch 351, loss: 0.337, 11232/60000 datapoints
2025-03-06 19:27:24,539 - INFO - training batch 401, loss: 0.217, 12832/60000 datapoints
2025-03-06 19:27:24,739 - INFO - training batch 451, loss: 0.327, 14432/60000 datapoints
2025-03-06 19:27:24,940 - INFO - training batch 501, loss: 0.205, 16032/60000 datapoints
2025-03-06 19:27:25,153 - INFO - training batch 551, loss: 0.446, 17632/60000 datapoints
2025-03-06 19:27:25,347 - INFO - training batch 601, loss: 0.318, 19232/60000 datapoints
2025-03-06 19:27:25,545 - INFO - training batch 651, loss: 0.434, 20832/60000 datapoints
2025-03-06 19:27:25,743 - INFO - training batch 701, loss: 0.372, 22432/60000 datapoints
2025-03-06 19:27:25,938 - INFO - training batch 751, loss: 0.277, 24032/60000 datapoints
2025-03-06 19:27:26,135 - INFO - training batch 801, loss: 0.390, 25632/60000 datapoints
2025-03-06 19:27:26,329 - INFO - training batch 851, loss: 0.353, 27232/60000 datapoints
2025-03-06 19:27:26,523 - INFO - training batch 901, loss: 0.162, 28832/60000 datapoints
2025-03-06 19:27:26,717 - INFO - training batch 951, loss: 0.654, 30432/60000 datapoints
2025-03-06 19:27:26,911 - INFO - training batch 1001, loss: 0.273, 32032/60000 datapoints
2025-03-06 19:27:27,103 - INFO - training batch 1051, loss: 0.377, 33632/60000 datapoints
2025-03-06 19:27:27,295 - INFO - training batch 1101, loss: 0.267, 35232/60000 datapoints
2025-03-06 19:27:27,487 - INFO - training batch 1151, loss: 0.241, 36832/60000 datapoints
2025-03-06 19:27:27,688 - INFO - training batch 1201, loss: 0.197, 38432/60000 datapoints
2025-03-06 19:27:27,886 - INFO - training batch 1251, loss: 0.319, 40032/60000 datapoints
2025-03-06 19:27:28,080 - INFO - training batch 1301, loss: 0.404, 41632/60000 datapoints
2025-03-06 19:27:28,274 - INFO - training batch 1351, loss: 0.439, 43232/60000 datapoints
2025-03-06 19:27:28,467 - INFO - training batch 1401, loss: 0.201, 44832/60000 datapoints
2025-03-06 19:27:28,667 - INFO - training batch 1451, loss: 0.344, 46432/60000 datapoints
2025-03-06 19:27:28,861 - INFO - training batch 1501, loss: 0.250, 48032/60000 datapoints
2025-03-06 19:27:29,057 - INFO - training batch 1551, loss: 0.272, 49632/60000 datapoints
2025-03-06 19:27:29,251 - INFO - training batch 1601, loss: 0.467, 51232/60000 datapoints
2025-03-06 19:27:29,446 - INFO - training batch 1651, loss: 0.377, 52832/60000 datapoints
2025-03-06 19:27:29,649 - INFO - training batch 1701, loss: 0.344, 54432/60000 datapoints
2025-03-06 19:27:29,844 - INFO - training batch 1751, loss: 0.248, 56032/60000 datapoints
2025-03-06 19:27:30,042 - INFO - training batch 1801, loss: 0.159, 57632/60000 datapoints
2025-03-06 19:27:30,236 - INFO - training batch 1851, loss: 0.445, 59232/60000 datapoints
2025-03-06 19:27:30,339 - INFO - validation batch 1, loss: 0.315, 32/10016 datapoints
2025-03-06 19:27:30,493 - INFO - validation batch 51, loss: 0.231, 1632/10016 datapoints
2025-03-06 19:27:30,650 - INFO - validation batch 101, loss: 0.073, 3232/10016 datapoints
2025-03-06 19:27:30,803 - INFO - validation batch 151, loss: 0.406, 4832/10016 datapoints
2025-03-06 19:27:30,954 - INFO - validation batch 201, loss: 0.279, 6432/10016 datapoints
2025-03-06 19:27:31,108 - INFO - validation batch 251, loss: 0.244, 8032/10016 datapoints
2025-03-06 19:27:31,262 - INFO - validation batch 301, loss: 0.162, 9632/10016 datapoints
2025-03-06 19:27:31,298 - INFO - Epoch 332/800 done.
2025-03-06 19:27:31,298 - INFO - Final validation performance:
Loss: 0.244, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:27:31,299 - INFO - Beginning epoch 333/800
2025-03-06 19:27:31,305 - INFO - training batch 1, loss: 0.282, 32/60000 datapoints
2025-03-06 19:27:31,511 - INFO - training batch 51, loss: 0.345, 1632/60000 datapoints
2025-03-06 19:27:31,707 - INFO - training batch 101, loss: 0.255, 3232/60000 datapoints
2025-03-06 19:27:31,920 - INFO - training batch 151, loss: 0.582, 4832/60000 datapoints
2025-03-06 19:27:32,114 - INFO - training batch 201, loss: 0.182, 6432/60000 datapoints
2025-03-06 19:27:32,312 - INFO - training batch 251, loss: 0.259, 8032/60000 datapoints
2025-03-06 19:27:32,507 - INFO - training batch 301, loss: 0.352, 9632/60000 datapoints
2025-03-06 19:27:32,709 - INFO - training batch 351, loss: 0.306, 11232/60000 datapoints
2025-03-06 19:27:32,904 - INFO - training batch 401, loss: 0.351, 12832/60000 datapoints
2025-03-06 19:27:33,099 - INFO - training batch 451, loss: 0.182, 14432/60000 datapoints
2025-03-06 19:27:33,293 - INFO - training batch 501, loss: 0.493, 16032/60000 datapoints
2025-03-06 19:27:33,485 - INFO - training batch 551, loss: 0.146, 17632/60000 datapoints
2025-03-06 19:27:33,681 - INFO - training batch 601, loss: 0.554, 19232/60000 datapoints
2025-03-06 19:27:33,887 - INFO - training batch 651, loss: 0.250, 20832/60000 datapoints
2025-03-06 19:27:34,087 - INFO - training batch 701, loss: 0.194, 22432/60000 datapoints
2025-03-06 19:27:34,283 - INFO - training batch 751, loss: 0.262, 24032/60000 datapoints
2025-03-06 19:27:34,479 - INFO - training batch 801, loss: 0.166, 25632/60000 datapoints
2025-03-06 19:27:34,677 - INFO - training batch 851, loss: 0.224, 27232/60000 datapoints
2025-03-06 19:27:34,873 - INFO - training batch 901, loss: 0.223, 28832/60000 datapoints
2025-03-06 19:27:35,085 - INFO - training batch 951, loss: 0.213, 30432/60000 datapoints
2025-03-06 19:27:35,284 - INFO - training batch 1001, loss: 0.462, 32032/60000 datapoints
2025-03-06 19:27:35,482 - INFO - training batch 1051, loss: 0.266, 33632/60000 datapoints
2025-03-06 19:27:35,681 - INFO - training batch 1101, loss: 0.270, 35232/60000 datapoints
2025-03-06 19:27:35,878 - INFO - training batch 1151, loss: 0.223, 36832/60000 datapoints
2025-03-06 19:27:36,073 - INFO - training batch 1201, loss: 0.407, 38432/60000 datapoints
2025-03-06 19:27:36,282 - INFO - training batch 1251, loss: 0.290, 40032/60000 datapoints
2025-03-06 19:27:36,511 - INFO - training batch 1301, loss: 0.386, 41632/60000 datapoints
2025-03-06 19:27:36,735 - INFO - training batch 1351, loss: 0.503, 43232/60000 datapoints
2025-03-06 19:27:36,932 - INFO - training batch 1401, loss: 0.388, 44832/60000 datapoints
2025-03-06 19:27:37,126 - INFO - training batch 1451, loss: 0.129, 46432/60000 datapoints
2025-03-06 19:27:37,321 - INFO - training batch 1501, loss: 0.153, 48032/60000 datapoints
2025-03-06 19:27:37,517 - INFO - training batch 1551, loss: 0.116, 49632/60000 datapoints
2025-03-06 19:27:37,734 - INFO - training batch 1601, loss: 0.390, 51232/60000 datapoints
2025-03-06 19:27:37,932 - INFO - training batch 1651, loss: 0.301, 52832/60000 datapoints
2025-03-06 19:27:38,134 - INFO - training batch 1701, loss: 0.082, 54432/60000 datapoints
2025-03-06 19:27:38,329 - INFO - training batch 1751, loss: 0.288, 56032/60000 datapoints
2025-03-06 19:27:38,522 - INFO - training batch 1801, loss: 0.108, 57632/60000 datapoints
2025-03-06 19:27:38,719 - INFO - training batch 1851, loss: 0.214, 59232/60000 datapoints
2025-03-06 19:27:38,820 - INFO - validation batch 1, loss: 0.203, 32/10016 datapoints
2025-03-06 19:27:38,976 - INFO - validation batch 51, loss: 0.444, 1632/10016 datapoints
2025-03-06 19:27:39,138 - INFO - validation batch 101, loss: 0.262, 3232/10016 datapoints
2025-03-06 19:27:39,292 - INFO - validation batch 151, loss: 0.134, 4832/10016 datapoints
2025-03-06 19:27:39,444 - INFO - validation batch 201, loss: 0.155, 6432/10016 datapoints
2025-03-06 19:27:39,599 - INFO - validation batch 251, loss: 0.525, 8032/10016 datapoints
2025-03-06 19:27:39,754 - INFO - validation batch 301, loss: 0.164, 9632/10016 datapoints
2025-03-06 19:27:39,791 - INFO - Epoch 333/800 done.
2025-03-06 19:27:39,791 - INFO - Final validation performance:
Loss: 0.270, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:27:39,792 - INFO - Beginning epoch 334/800
2025-03-06 19:27:39,798 - INFO - training batch 1, loss: 0.282, 32/60000 datapoints
2025-03-06 19:27:40,007 - INFO - training batch 51, loss: 0.216, 1632/60000 datapoints
2025-03-06 19:27:40,204 - INFO - training batch 101, loss: 0.546, 3232/60000 datapoints
2025-03-06 19:27:40,405 - INFO - training batch 151, loss: 0.208, 4832/60000 datapoints
2025-03-06 19:27:40,601 - INFO - training batch 201, loss: 0.386, 6432/60000 datapoints
2025-03-06 19:27:40,805 - INFO - training batch 251, loss: 0.549, 8032/60000 datapoints
2025-03-06 19:27:41,000 - INFO - training batch 301, loss: 0.436, 9632/60000 datapoints
2025-03-06 19:27:41,194 - INFO - training batch 351, loss: 0.194, 11232/60000 datapoints
2025-03-06 19:27:41,388 - INFO - training batch 401, loss: 0.249, 12832/60000 datapoints
2025-03-06 19:27:41,581 - INFO - training batch 451, loss: 0.088, 14432/60000 datapoints
2025-03-06 19:27:41,780 - INFO - training batch 501, loss: 0.269, 16032/60000 datapoints
2025-03-06 19:27:41,977 - INFO - training batch 551, loss: 0.338, 17632/60000 datapoints
2025-03-06 19:27:42,173 - INFO - training batch 601, loss: 0.289, 19232/60000 datapoints
2025-03-06 19:27:42,368 - INFO - training batch 651, loss: 0.209, 20832/60000 datapoints
2025-03-06 19:27:42,563 - INFO - training batch 701, loss: 0.475, 22432/60000 datapoints
2025-03-06 19:27:42,762 - INFO - training batch 751, loss: 0.387, 24032/60000 datapoints
2025-03-06 19:27:42,958 - INFO - training batch 801, loss: 0.265, 25632/60000 datapoints
2025-03-06 19:27:43,152 - INFO - training batch 851, loss: 0.300, 27232/60000 datapoints
2025-03-06 19:27:43,347 - INFO - training batch 901, loss: 0.373, 28832/60000 datapoints
2025-03-06 19:27:43,543 - INFO - training batch 951, loss: 0.301, 30432/60000 datapoints
2025-03-06 19:27:43,744 - INFO - training batch 1001, loss: 0.082, 32032/60000 datapoints
2025-03-06 19:27:43,943 - INFO - training batch 1051, loss: 0.148, 33632/60000 datapoints
2025-03-06 19:27:44,143 - INFO - training batch 1101, loss: 0.774, 35232/60000 datapoints
2025-03-06 19:27:44,337 - INFO - training batch 1151, loss: 0.205, 36832/60000 datapoints
2025-03-06 19:27:44,534 - INFO - training batch 1201, loss: 0.132, 38432/60000 datapoints
2025-03-06 19:27:44,731 - INFO - training batch 1251, loss: 0.510, 40032/60000 datapoints
2025-03-06 19:27:44,931 - INFO - training batch 1301, loss: 0.444, 41632/60000 datapoints
2025-03-06 19:27:45,135 - INFO - training batch 1351, loss: 0.353, 43232/60000 datapoints
2025-03-06 19:27:45,339 - INFO - training batch 1401, loss: 0.207, 44832/60000 datapoints
2025-03-06 19:27:45,533 - INFO - training batch 1451, loss: 0.380, 46432/60000 datapoints
2025-03-06 19:27:45,734 - INFO - training batch 1501, loss: 0.334, 48032/60000 datapoints
2025-03-06 19:27:45,931 - INFO - training batch 1551, loss: 0.379, 49632/60000 datapoints
2025-03-06 19:27:46,125 - INFO - training batch 1601, loss: 0.538, 51232/60000 datapoints
2025-03-06 19:27:46,321 - INFO - training batch 1651, loss: 0.474, 52832/60000 datapoints
2025-03-06 19:27:46,515 - INFO - training batch 1701, loss: 0.132, 54432/60000 datapoints
2025-03-06 19:27:46,713 - INFO - training batch 1751, loss: 0.541, 56032/60000 datapoints
2025-03-06 19:27:46,906 - INFO - training batch 1801, loss: 0.122, 57632/60000 datapoints
2025-03-06 19:27:47,101 - INFO - training batch 1851, loss: 0.231, 59232/60000 datapoints
2025-03-06 19:27:47,202 - INFO - validation batch 1, loss: 0.307, 32/10016 datapoints
2025-03-06 19:27:47,352 - INFO - validation batch 51, loss: 0.116, 1632/10016 datapoints
2025-03-06 19:27:47,504 - INFO - validation batch 101, loss: 0.074, 3232/10016 datapoints
2025-03-06 19:27:47,659 - INFO - validation batch 151, loss: 0.403, 4832/10016 datapoints
2025-03-06 19:27:47,814 - INFO - validation batch 201, loss: 0.251, 6432/10016 datapoints
2025-03-06 19:27:47,971 - INFO - validation batch 251, loss: 0.292, 8032/10016 datapoints
2025-03-06 19:27:48,146 - INFO - validation batch 301, loss: 0.171, 9632/10016 datapoints
2025-03-06 19:27:48,190 - INFO - Epoch 334/800 done.
2025-03-06 19:27:48,190 - INFO - Final validation performance:
Loss: 0.230, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:27:48,191 - INFO - Beginning epoch 335/800
2025-03-06 19:27:48,197 - INFO - training batch 1, loss: 0.265, 32/60000 datapoints
2025-03-06 19:27:48,398 - INFO - training batch 51, loss: 0.148, 1632/60000 datapoints
2025-03-06 19:27:48,595 - INFO - training batch 101, loss: 0.142, 3232/60000 datapoints
2025-03-06 19:27:48,806 - INFO - training batch 151, loss: 0.226, 4832/60000 datapoints
2025-03-06 19:27:48,998 - INFO - training batch 201, loss: 0.177, 6432/60000 datapoints
2025-03-06 19:27:49,198 - INFO - training batch 251, loss: 0.379, 8032/60000 datapoints
2025-03-06 19:27:49,395 - INFO - training batch 301, loss: 0.599, 9632/60000 datapoints
2025-03-06 19:27:49,593 - INFO - training batch 351, loss: 0.123, 11232/60000 datapoints
2025-03-06 19:27:49,791 - INFO - training batch 401, loss: 0.205, 12832/60000 datapoints
2025-03-06 19:27:49,988 - INFO - training batch 451, loss: 0.442, 14432/60000 datapoints
2025-03-06 19:27:50,184 - INFO - training batch 501, loss: 0.171, 16032/60000 datapoints
2025-03-06 19:27:50,381 - INFO - training batch 551, loss: 0.083, 17632/60000 datapoints
2025-03-06 19:27:50,575 - INFO - training batch 601, loss: 0.179, 19232/60000 datapoints
2025-03-06 19:27:50,773 - INFO - training batch 651, loss: 0.344, 20832/60000 datapoints
2025-03-06 19:27:50,968 - INFO - training batch 701, loss: 0.168, 22432/60000 datapoints
2025-03-06 19:27:51,165 - INFO - training batch 751, loss: 0.282, 24032/60000 datapoints
2025-03-06 19:27:51,360 - INFO - training batch 801, loss: 0.196, 25632/60000 datapoints
2025-03-06 19:27:51,554 - INFO - training batch 851, loss: 0.274, 27232/60000 datapoints
2025-03-06 19:27:51,752 - INFO - training batch 901, loss: 0.177, 28832/60000 datapoints
2025-03-06 19:27:51,951 - INFO - training batch 951, loss: 0.479, 30432/60000 datapoints
2025-03-06 19:27:52,146 - INFO - training batch 1001, loss: 0.391, 32032/60000 datapoints
2025-03-06 19:27:52,342 - INFO - training batch 1051, loss: 0.131, 33632/60000 datapoints
2025-03-06 19:27:52,536 - INFO - training batch 1101, loss: 0.263, 35232/60000 datapoints
2025-03-06 19:27:52,733 - INFO - training batch 1151, loss: 0.564, 36832/60000 datapoints
2025-03-06 19:27:52,930 - INFO - training batch 1201, loss: 0.358, 38432/60000 datapoints
2025-03-06 19:27:53,127 - INFO - training batch 1251, loss: 0.164, 40032/60000 datapoints
2025-03-06 19:27:53,322 - INFO - training batch 1301, loss: 0.385, 41632/60000 datapoints
2025-03-06 19:27:53,519 - INFO - training batch 1351, loss: 0.315, 43232/60000 datapoints
2025-03-06 19:27:53,715 - INFO - training batch 1401, loss: 0.245, 44832/60000 datapoints
2025-03-06 19:27:53,911 - INFO - training batch 1451, loss: 0.239, 46432/60000 datapoints
2025-03-06 19:27:54,108 - INFO - training batch 1501, loss: 0.173, 48032/60000 datapoints
2025-03-06 19:27:54,307 - INFO - training batch 1551, loss: 0.362, 49632/60000 datapoints
2025-03-06 19:27:54,503 - INFO - training batch 1601, loss: 0.254, 51232/60000 datapoints
2025-03-06 19:27:54,702 - INFO - training batch 1651, loss: 0.352, 52832/60000 datapoints
2025-03-06 19:27:54,908 - INFO - training batch 1701, loss: 0.153, 54432/60000 datapoints
2025-03-06 19:27:55,102 - INFO - training batch 1751, loss: 0.284, 56032/60000 datapoints
2025-03-06 19:27:55,316 - INFO - training batch 1801, loss: 0.468, 57632/60000 datapoints
2025-03-06 19:27:55,515 - INFO - training batch 1851, loss: 0.179, 59232/60000 datapoints
2025-03-06 19:27:55,618 - INFO - validation batch 1, loss: 0.152, 32/10016 datapoints
2025-03-06 19:27:55,770 - INFO - validation batch 51, loss: 0.244, 1632/10016 datapoints
2025-03-06 19:27:55,924 - INFO - validation batch 101, loss: 0.285, 3232/10016 datapoints
2025-03-06 19:27:56,079 - INFO - validation batch 151, loss: 0.233, 4832/10016 datapoints
2025-03-06 19:27:56,231 - INFO - validation batch 201, loss: 0.688, 6432/10016 datapoints
2025-03-06 19:27:56,385 - INFO - validation batch 251, loss: 0.231, 8032/10016 datapoints
2025-03-06 19:27:56,538 - INFO - validation batch 301, loss: 0.253, 9632/10016 datapoints
2025-03-06 19:27:56,576 - INFO - Epoch 335/800 done.
2025-03-06 19:27:56,576 - INFO - Final validation performance:
Loss: 0.298, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:27:56,577 - INFO - Beginning epoch 336/800
2025-03-06 19:27:56,583 - INFO - training batch 1, loss: 0.175, 32/60000 datapoints
2025-03-06 19:27:56,779 - INFO - training batch 51, loss: 0.083, 1632/60000 datapoints
2025-03-06 19:27:56,980 - INFO - training batch 101, loss: 0.254, 3232/60000 datapoints
2025-03-06 19:27:57,189 - INFO - training batch 151, loss: 0.420, 4832/60000 datapoints
2025-03-06 19:27:57,383 - INFO - training batch 201, loss: 0.348, 6432/60000 datapoints
2025-03-06 19:27:57,577 - INFO - training batch 251, loss: 0.131, 8032/60000 datapoints
2025-03-06 19:27:57,780 - INFO - training batch 301, loss: 0.320, 9632/60000 datapoints
2025-03-06 19:27:57,984 - INFO - training batch 351, loss: 0.199, 11232/60000 datapoints
2025-03-06 19:27:58,182 - INFO - training batch 401, loss: 0.710, 12832/60000 datapoints
2025-03-06 19:27:58,376 - INFO - training batch 451, loss: 0.163, 14432/60000 datapoints
2025-03-06 19:27:58,570 - INFO - training batch 501, loss: 0.377, 16032/60000 datapoints
2025-03-06 19:27:58,767 - INFO - training batch 551, loss: 0.360, 17632/60000 datapoints
2025-03-06 19:27:58,963 - INFO - training batch 601, loss: 0.401, 19232/60000 datapoints
2025-03-06 19:27:59,158 - INFO - training batch 651, loss: 0.207, 20832/60000 datapoints
2025-03-06 19:27:59,356 - INFO - training batch 701, loss: 0.561, 22432/60000 datapoints
2025-03-06 19:27:59,551 - INFO - training batch 751, loss: 0.320, 24032/60000 datapoints
2025-03-06 19:27:59,749 - INFO - training batch 801, loss: 0.455, 25632/60000 datapoints
2025-03-06 19:27:59,946 - INFO - training batch 851, loss: 0.261, 27232/60000 datapoints
2025-03-06 19:28:00,151 - INFO - training batch 901, loss: 0.171, 28832/60000 datapoints
2025-03-06 19:28:00,347 - INFO - training batch 951, loss: 0.146, 30432/60000 datapoints
2025-03-06 19:28:00,540 - INFO - training batch 1001, loss: 0.119, 32032/60000 datapoints
2025-03-06 19:28:00,748 - INFO - training batch 1051, loss: 0.378, 33632/60000 datapoints
2025-03-06 19:28:00,941 - INFO - training batch 1101, loss: 0.355, 35232/60000 datapoints
2025-03-06 19:28:01,136 - INFO - training batch 1151, loss: 0.508, 36832/60000 datapoints
2025-03-06 19:28:01,341 - INFO - training batch 1201, loss: 0.215, 38432/60000 datapoints
2025-03-06 19:28:01,535 - INFO - training batch 1251, loss: 0.062, 40032/60000 datapoints
2025-03-06 19:28:01,732 - INFO - training batch 1301, loss: 0.289, 41632/60000 datapoints
2025-03-06 19:28:01,929 - INFO - training batch 1351, loss: 0.365, 43232/60000 datapoints
2025-03-06 19:28:02,123 - INFO - training batch 1401, loss: 0.272, 44832/60000 datapoints
2025-03-06 19:28:02,317 - INFO - training batch 1451, loss: 0.466, 46432/60000 datapoints
2025-03-06 19:28:02,513 - INFO - training batch 1501, loss: 0.115, 48032/60000 datapoints
2025-03-06 19:28:02,708 - INFO - training batch 1551, loss: 0.159, 49632/60000 datapoints
2025-03-06 19:28:02,910 - INFO - training batch 1601, loss: 0.331, 51232/60000 datapoints
2025-03-06 19:28:03,111 - INFO - training batch 1651, loss: 0.259, 52832/60000 datapoints
2025-03-06 19:28:03,304 - INFO - training batch 1701, loss: 0.285, 54432/60000 datapoints
2025-03-06 19:28:03,499 - INFO - training batch 1751, loss: 0.281, 56032/60000 datapoints
2025-03-06 19:28:03,695 - INFO - training batch 1801, loss: 0.284, 57632/60000 datapoints
2025-03-06 19:28:03,889 - INFO - training batch 1851, loss: 0.224, 59232/60000 datapoints
2025-03-06 19:28:03,992 - INFO - validation batch 1, loss: 0.344, 32/10016 datapoints
2025-03-06 19:28:04,147 - INFO - validation batch 51, loss: 0.105, 1632/10016 datapoints
2025-03-06 19:28:04,304 - INFO - validation batch 101, loss: 0.216, 3232/10016 datapoints
2025-03-06 19:28:04,460 - INFO - validation batch 151, loss: 0.143, 4832/10016 datapoints
2025-03-06 19:28:04,614 - INFO - validation batch 201, loss: 0.354, 6432/10016 datapoints
2025-03-06 19:28:04,768 - INFO - validation batch 251, loss: 0.165, 8032/10016 datapoints
2025-03-06 19:28:04,927 - INFO - validation batch 301, loss: 0.189, 9632/10016 datapoints
2025-03-06 19:28:04,965 - INFO - Epoch 336/800 done.
2025-03-06 19:28:04,966 - INFO - Final validation performance:
Loss: 0.217, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:28:04,966 - INFO - Beginning epoch 337/800
2025-03-06 19:28:04,974 - INFO - training batch 1, loss: 0.444, 32/60000 datapoints
2025-03-06 19:28:05,175 - INFO - training batch 51, loss: 0.573, 1632/60000 datapoints
2025-03-06 19:28:05,400 - INFO - training batch 101, loss: 0.350, 3232/60000 datapoints
2025-03-06 19:28:05,596 - INFO - training batch 151, loss: 0.284, 4832/60000 datapoints
2025-03-06 19:28:05,816 - INFO - training batch 201, loss: 0.226, 6432/60000 datapoints
2025-03-06 19:28:06,024 - INFO - training batch 251, loss: 0.462, 8032/60000 datapoints
2025-03-06 19:28:06,218 - INFO - training batch 301, loss: 0.217, 9632/60000 datapoints
2025-03-06 19:28:06,413 - INFO - training batch 351, loss: 0.119, 11232/60000 datapoints
2025-03-06 19:28:06,607 - INFO - training batch 401, loss: 0.435, 12832/60000 datapoints
2025-03-06 19:28:06,802 - INFO - training batch 451, loss: 0.525, 14432/60000 datapoints
2025-03-06 19:28:06,998 - INFO - training batch 501, loss: 0.373, 16032/60000 datapoints
2025-03-06 19:28:07,192 - INFO - training batch 551, loss: 0.300, 17632/60000 datapoints
2025-03-06 19:28:07,386 - INFO - training batch 601, loss: 0.336, 19232/60000 datapoints
2025-03-06 19:28:07,579 - INFO - training batch 651, loss: 0.321, 20832/60000 datapoints
2025-03-06 19:28:07,775 - INFO - training batch 701, loss: 0.373, 22432/60000 datapoints
2025-03-06 19:28:07,971 - INFO - training batch 751, loss: 0.250, 24032/60000 datapoints
2025-03-06 19:28:08,167 - INFO - training batch 801, loss: 0.284, 25632/60000 datapoints
2025-03-06 19:28:08,363 - INFO - training batch 851, loss: 0.371, 27232/60000 datapoints
2025-03-06 19:28:08,558 - INFO - training batch 901, loss: 0.326, 28832/60000 datapoints
2025-03-06 19:28:08,757 - INFO - training batch 951, loss: 0.315, 30432/60000 datapoints
2025-03-06 19:28:08,952 - INFO - training batch 1001, loss: 0.289, 32032/60000 datapoints
2025-03-06 19:28:09,147 - INFO - training batch 1051, loss: 0.230, 33632/60000 datapoints
2025-03-06 19:28:09,342 - INFO - training batch 1101, loss: 0.196, 35232/60000 datapoints
2025-03-06 19:28:09,539 - INFO - training batch 1151, loss: 0.170, 36832/60000 datapoints
2025-03-06 19:28:09,738 - INFO - training batch 1201, loss: 0.206, 38432/60000 datapoints
2025-03-06 19:28:09,933 - INFO - training batch 1251, loss: 0.070, 40032/60000 datapoints
2025-03-06 19:28:10,139 - INFO - training batch 1301, loss: 0.337, 41632/60000 datapoints
2025-03-06 19:28:10,333 - INFO - training batch 1351, loss: 0.399, 43232/60000 datapoints
2025-03-06 19:28:10,528 - INFO - training batch 1401, loss: 0.208, 44832/60000 datapoints
2025-03-06 19:28:10,725 - INFO - training batch 1451, loss: 0.169, 46432/60000 datapoints
2025-03-06 19:28:10,920 - INFO - training batch 1501, loss: 0.131, 48032/60000 datapoints
2025-03-06 19:28:11,144 - INFO - training batch 1551, loss: 0.258, 49632/60000 datapoints
2025-03-06 19:28:11,340 - INFO - training batch 1601, loss: 0.064, 51232/60000 datapoints
2025-03-06 19:28:11,536 - INFO - training batch 1651, loss: 0.394, 52832/60000 datapoints
2025-03-06 19:28:11,732 - INFO - training batch 1701, loss: 0.311, 54432/60000 datapoints
2025-03-06 19:28:11,928 - INFO - training batch 1751, loss: 0.504, 56032/60000 datapoints
2025-03-06 19:28:12,124 - INFO - training batch 1801, loss: 0.258, 57632/60000 datapoints
2025-03-06 19:28:12,317 - INFO - training batch 1851, loss: 0.224, 59232/60000 datapoints
2025-03-06 19:28:12,418 - INFO - validation batch 1, loss: 0.407, 32/10016 datapoints
2025-03-06 19:28:12,570 - INFO - validation batch 51, loss: 0.191, 1632/10016 datapoints
2025-03-06 19:28:12,725 - INFO - validation batch 101, loss: 0.332, 3232/10016 datapoints
2025-03-06 19:28:12,878 - INFO - validation batch 151, loss: 0.155, 4832/10016 datapoints
2025-03-06 19:28:13,035 - INFO - validation batch 201, loss: 0.198, 6432/10016 datapoints
2025-03-06 19:28:13,189 - INFO - validation batch 251, loss: 0.189, 8032/10016 datapoints
2025-03-06 19:28:13,339 - INFO - validation batch 301, loss: 0.290, 9632/10016 datapoints
2025-03-06 19:28:13,377 - INFO - Epoch 337/800 done.
2025-03-06 19:28:13,377 - INFO - Final validation performance:
Loss: 0.252, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:28:13,377 - INFO - Beginning epoch 338/800
2025-03-06 19:28:13,384 - INFO - training batch 1, loss: 0.342, 32/60000 datapoints
2025-03-06 19:28:13,579 - INFO - training batch 51, loss: 0.279, 1632/60000 datapoints
2025-03-06 19:28:13,774 - INFO - training batch 101, loss: 0.287, 3232/60000 datapoints
2025-03-06 19:28:13,974 - INFO - training batch 151, loss: 0.145, 4832/60000 datapoints
2025-03-06 19:28:14,175 - INFO - training batch 201, loss: 0.284, 6432/60000 datapoints
2025-03-06 19:28:14,376 - INFO - training batch 251, loss: 0.321, 8032/60000 datapoints
2025-03-06 19:28:14,578 - INFO - training batch 301, loss: 0.167, 9632/60000 datapoints
2025-03-06 19:28:14,778 - INFO - training batch 351, loss: 0.218, 11232/60000 datapoints
2025-03-06 19:28:14,977 - INFO - training batch 401, loss: 0.182, 12832/60000 datapoints
2025-03-06 19:28:15,176 - INFO - training batch 451, loss: 0.321, 14432/60000 datapoints
2025-03-06 19:28:15,388 - INFO - training batch 501, loss: 0.266, 16032/60000 datapoints
2025-03-06 19:28:15,591 - INFO - training batch 551, loss: 0.316, 17632/60000 datapoints
2025-03-06 19:28:15,789 - INFO - training batch 601, loss: 0.525, 19232/60000 datapoints
2025-03-06 19:28:15,984 - INFO - training batch 651, loss: 0.321, 20832/60000 datapoints
2025-03-06 19:28:16,181 - INFO - training batch 701, loss: 0.441, 22432/60000 datapoints
2025-03-06 19:28:16,376 - INFO - training batch 751, loss: 0.359, 24032/60000 datapoints
2025-03-06 19:28:16,571 - INFO - training batch 801, loss: 0.138, 25632/60000 datapoints
2025-03-06 19:28:16,766 - INFO - training batch 851, loss: 0.264, 27232/60000 datapoints
2025-03-06 19:28:16,960 - INFO - training batch 901, loss: 0.264, 28832/60000 datapoints
2025-03-06 19:28:17,156 - INFO - training batch 951, loss: 0.171, 30432/60000 datapoints
2025-03-06 19:28:17,349 - INFO - training batch 1001, loss: 0.204, 32032/60000 datapoints
2025-03-06 19:28:17,544 - INFO - training batch 1051, loss: 0.181, 33632/60000 datapoints
2025-03-06 19:28:17,743 - INFO - training batch 1101, loss: 0.578, 35232/60000 datapoints
2025-03-06 19:28:17,935 - INFO - training batch 1151, loss: 0.216, 36832/60000 datapoints
2025-03-06 19:28:18,132 - INFO - training batch 1201, loss: 0.573, 38432/60000 datapoints
2025-03-06 19:28:18,325 - INFO - training batch 1251, loss: 0.230, 40032/60000 datapoints
2025-03-06 19:28:18,520 - INFO - training batch 1301, loss: 0.148, 41632/60000 datapoints
2025-03-06 19:28:18,718 - INFO - training batch 1351, loss: 0.378, 43232/60000 datapoints
2025-03-06 19:28:18,910 - INFO - training batch 1401, loss: 0.285, 44832/60000 datapoints
2025-03-06 19:28:19,109 - INFO - training batch 1451, loss: 0.385, 46432/60000 datapoints
2025-03-06 19:28:19,299 - INFO - training batch 1501, loss: 0.340, 48032/60000 datapoints
2025-03-06 19:28:19,493 - INFO - training batch 1551, loss: 0.382, 49632/60000 datapoints
2025-03-06 19:28:19,690 - INFO - training batch 1601, loss: 0.259, 51232/60000 datapoints
2025-03-06 19:28:19,886 - INFO - training batch 1651, loss: 0.227, 52832/60000 datapoints
2025-03-06 19:28:20,086 - INFO - training batch 1701, loss: 0.245, 54432/60000 datapoints
2025-03-06 19:28:20,279 - INFO - training batch 1751, loss: 0.387, 56032/60000 datapoints
2025-03-06 19:28:20,473 - INFO - training batch 1801, loss: 0.352, 57632/60000 datapoints
2025-03-06 19:28:20,669 - INFO - training batch 1851, loss: 0.205, 59232/60000 datapoints
2025-03-06 19:28:20,768 - INFO - validation batch 1, loss: 0.174, 32/10016 datapoints
2025-03-06 19:28:20,921 - INFO - validation batch 51, loss: 0.230, 1632/10016 datapoints
2025-03-06 19:28:21,076 - INFO - validation batch 101, loss: 0.350, 3232/10016 datapoints
2025-03-06 19:28:21,230 - INFO - validation batch 151, loss: 0.152, 4832/10016 datapoints
2025-03-06 19:28:21,384 - INFO - validation batch 201, loss: 0.269, 6432/10016 datapoints
2025-03-06 19:28:21,537 - INFO - validation batch 251, loss: 0.129, 8032/10016 datapoints
2025-03-06 19:28:21,695 - INFO - validation batch 301, loss: 0.334, 9632/10016 datapoints
2025-03-06 19:28:21,733 - INFO - Epoch 338/800 done.
2025-03-06 19:28:21,733 - INFO - Final validation performance:
Loss: 0.234, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:28:21,733 - INFO - Beginning epoch 339/800
2025-03-06 19:28:21,740 - INFO - training batch 1, loss: 0.353, 32/60000 datapoints
2025-03-06 19:28:21,943 - INFO - training batch 51, loss: 0.186, 1632/60000 datapoints
2025-03-06 19:28:22,147 - INFO - training batch 101, loss: 0.183, 3232/60000 datapoints
2025-03-06 19:28:22,354 - INFO - training batch 151, loss: 0.092, 4832/60000 datapoints
2025-03-06 19:28:22,548 - INFO - training batch 201, loss: 0.180, 6432/60000 datapoints
2025-03-06 19:28:22,751 - INFO - training batch 251, loss: 0.220, 8032/60000 datapoints
2025-03-06 19:28:22,950 - INFO - training batch 301, loss: 0.278, 9632/60000 datapoints
2025-03-06 19:28:23,150 - INFO - training batch 351, loss: 0.241, 11232/60000 datapoints
2025-03-06 19:28:23,344 - INFO - training batch 401, loss: 0.233, 12832/60000 datapoints
2025-03-06 19:28:23,538 - INFO - training batch 451, loss: 0.157, 14432/60000 datapoints
2025-03-06 19:28:23,735 - INFO - training batch 501, loss: 0.318, 16032/60000 datapoints
2025-03-06 19:28:23,929 - INFO - training batch 551, loss: 0.431, 17632/60000 datapoints
2025-03-06 19:28:24,127 - INFO - training batch 601, loss: 0.257, 19232/60000 datapoints
2025-03-06 19:28:24,324 - INFO - training batch 651, loss: 0.261, 20832/60000 datapoints
2025-03-06 19:28:24,519 - INFO - training batch 701, loss: 0.387, 22432/60000 datapoints
2025-03-06 19:28:24,717 - INFO - training batch 751, loss: 0.444, 24032/60000 datapoints
2025-03-06 19:28:24,917 - INFO - training batch 801, loss: 0.218, 25632/60000 datapoints
2025-03-06 19:28:25,115 - INFO - training batch 851, loss: 0.231, 27232/60000 datapoints
2025-03-06 19:28:25,310 - INFO - training batch 901, loss: 0.297, 28832/60000 datapoints
2025-03-06 19:28:25,525 - INFO - training batch 951, loss: 0.131, 30432/60000 datapoints
2025-03-06 19:28:25,724 - INFO - training batch 1001, loss: 0.162, 32032/60000 datapoints
2025-03-06 19:28:25,919 - INFO - training batch 1051, loss: 0.319, 33632/60000 datapoints
2025-03-06 19:28:26,123 - INFO - training batch 1101, loss: 0.337, 35232/60000 datapoints
2025-03-06 19:28:26,324 - INFO - training batch 1151, loss: 0.107, 36832/60000 datapoints
2025-03-06 19:28:26,518 - INFO - training batch 1201, loss: 0.209, 38432/60000 datapoints
2025-03-06 19:28:26,714 - INFO - training batch 1251, loss: 0.365, 40032/60000 datapoints
2025-03-06 19:28:26,906 - INFO - training batch 1301, loss: 0.147, 41632/60000 datapoints
2025-03-06 19:28:27,101 - INFO - training batch 1351, loss: 0.252, 43232/60000 datapoints
2025-03-06 19:28:27,293 - INFO - training batch 1401, loss: 0.143, 44832/60000 datapoints
2025-03-06 19:28:27,487 - INFO - training batch 1451, loss: 0.335, 46432/60000 datapoints
2025-03-06 19:28:27,682 - INFO - training batch 1501, loss: 0.492, 48032/60000 datapoints
2025-03-06 19:28:27,876 - INFO - training batch 1551, loss: 0.173, 49632/60000 datapoints
2025-03-06 19:28:28,077 - INFO - training batch 1601, loss: 0.312, 51232/60000 datapoints
2025-03-06 19:28:28,278 - INFO - training batch 1651, loss: 0.317, 52832/60000 datapoints
2025-03-06 19:28:28,471 - INFO - training batch 1701, loss: 0.136, 54432/60000 datapoints
2025-03-06 19:28:28,667 - INFO - training batch 1751, loss: 0.336, 56032/60000 datapoints
2025-03-06 19:28:28,863 - INFO - training batch 1801, loss: 0.294, 57632/60000 datapoints
2025-03-06 19:28:29,057 - INFO - training batch 1851, loss: 0.369, 59232/60000 datapoints
2025-03-06 19:28:29,159 - INFO - validation batch 1, loss: 0.443, 32/10016 datapoints
2025-03-06 19:28:29,314 - INFO - validation batch 51, loss: 0.147, 1632/10016 datapoints
2025-03-06 19:28:29,467 - INFO - validation batch 101, loss: 0.210, 3232/10016 datapoints
2025-03-06 19:28:29,622 - INFO - validation batch 151, loss: 0.337, 4832/10016 datapoints
2025-03-06 19:28:29,777 - INFO - validation batch 201, loss: 0.107, 6432/10016 datapoints
2025-03-06 19:28:29,929 - INFO - validation batch 251, loss: 0.257, 8032/10016 datapoints
2025-03-06 19:28:30,086 - INFO - validation batch 301, loss: 0.481, 9632/10016 datapoints
2025-03-06 19:28:30,125 - INFO - Epoch 339/800 done.
2025-03-06 19:28:30,125 - INFO - Final validation performance:
Loss: 0.283, top-1 acc: 0.915top-5 acc: 0.915
2025-03-06 19:28:30,126 - INFO - Beginning epoch 340/800
2025-03-06 19:28:30,132 - INFO - training batch 1, loss: 0.265, 32/60000 datapoints
2025-03-06 19:28:30,328 - INFO - training batch 51, loss: 0.308, 1632/60000 datapoints
2025-03-06 19:28:30,523 - INFO - training batch 101, loss: 0.902, 3232/60000 datapoints
2025-03-06 19:28:30,732 - INFO - training batch 151, loss: 0.227, 4832/60000 datapoints
2025-03-06 19:28:30,932 - INFO - training batch 201, loss: 0.208, 6432/60000 datapoints
2025-03-06 19:28:31,131 - INFO - training batch 251, loss: 0.296, 8032/60000 datapoints
2025-03-06 19:28:31,332 - INFO - training batch 301, loss: 0.203, 9632/60000 datapoints
2025-03-06 19:28:31,530 - INFO - training batch 351, loss: 0.308, 11232/60000 datapoints
2025-03-06 19:28:31,744 - INFO - training batch 401, loss: 0.118, 12832/60000 datapoints
2025-03-06 19:28:32,053 - INFO - training batch 451, loss: 0.209, 14432/60000 datapoints
2025-03-06 19:28:32,251 - INFO - training batch 501, loss: 0.375, 16032/60000 datapoints
2025-03-06 19:28:32,446 - INFO - training batch 551, loss: 0.142, 17632/60000 datapoints
2025-03-06 19:28:32,640 - INFO - training batch 601, loss: 0.591, 19232/60000 datapoints
2025-03-06 19:28:32,836 - INFO - training batch 651, loss: 0.494, 20832/60000 datapoints
2025-03-06 19:28:33,029 - INFO - training batch 701, loss: 0.209, 22432/60000 datapoints
2025-03-06 19:28:33,238 - INFO - training batch 751, loss: 0.487, 24032/60000 datapoints
2025-03-06 19:28:33,433 - INFO - training batch 801, loss: 0.145, 25632/60000 datapoints
2025-03-06 19:28:33,628 - INFO - training batch 851, loss: 0.302, 27232/60000 datapoints
2025-03-06 19:28:33,823 - INFO - training batch 901, loss: 0.287, 28832/60000 datapoints
2025-03-06 19:28:34,015 - INFO - training batch 951, loss: 0.276, 30432/60000 datapoints
2025-03-06 19:28:34,210 - INFO - training batch 1001, loss: 0.615, 32032/60000 datapoints
2025-03-06 19:28:34,408 - INFO - training batch 1051, loss: 0.181, 33632/60000 datapoints
2025-03-06 19:28:34,602 - INFO - training batch 1101, loss: 0.231, 35232/60000 datapoints
2025-03-06 19:28:34,799 - INFO - training batch 1151, loss: 0.209, 36832/60000 datapoints
2025-03-06 19:28:34,999 - INFO - training batch 1201, loss: 0.486, 38432/60000 datapoints
2025-03-06 19:28:35,194 - INFO - training batch 1251, loss: 0.703, 40032/60000 datapoints
2025-03-06 19:28:35,388 - INFO - training batch 1301, loss: 0.104, 41632/60000 datapoints
2025-03-06 19:28:35,602 - INFO - training batch 1351, loss: 0.155, 43232/60000 datapoints
2025-03-06 19:28:35,805 - INFO - training batch 1401, loss: 0.185, 44832/60000 datapoints
2025-03-06 19:28:36,016 - INFO - training batch 1451, loss: 0.253, 46432/60000 datapoints
2025-03-06 19:28:36,215 - INFO - training batch 1501, loss: 0.171, 48032/60000 datapoints
2025-03-06 19:28:36,413 - INFO - training batch 1551, loss: 0.455, 49632/60000 datapoints
2025-03-06 19:28:36,606 - INFO - training batch 1601, loss: 0.544, 51232/60000 datapoints
2025-03-06 19:28:36,804 - INFO - training batch 1651, loss: 0.360, 52832/60000 datapoints
2025-03-06 19:28:37,001 - INFO - training batch 1701, loss: 0.215, 54432/60000 datapoints
2025-03-06 19:28:37,196 - INFO - training batch 1751, loss: 0.240, 56032/60000 datapoints
2025-03-06 19:28:37,391 - INFO - training batch 1801, loss: 0.097, 57632/60000 datapoints
2025-03-06 19:28:37,585 - INFO - training batch 1851, loss: 0.099, 59232/60000 datapoints
2025-03-06 19:28:37,703 - INFO - validation batch 1, loss: 0.440, 32/10016 datapoints
2025-03-06 19:28:37,858 - INFO - validation batch 51, loss: 0.161, 1632/10016 datapoints
2025-03-06 19:28:38,012 - INFO - validation batch 101, loss: 0.115, 3232/10016 datapoints
2025-03-06 19:28:38,168 - INFO - validation batch 151, loss: 0.413, 4832/10016 datapoints
2025-03-06 19:28:38,322 - INFO - validation batch 201, loss: 0.095, 6432/10016 datapoints
2025-03-06 19:28:38,472 - INFO - validation batch 251, loss: 0.257, 8032/10016 datapoints
2025-03-06 19:28:38,624 - INFO - validation batch 301, loss: 0.316, 9632/10016 datapoints
2025-03-06 19:28:38,662 - INFO - Epoch 340/800 done.
2025-03-06 19:28:38,662 - INFO - Final validation performance:
Loss: 0.257, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 19:28:38,663 - INFO - Beginning epoch 341/800
2025-03-06 19:28:38,671 - INFO - training batch 1, loss: 0.336, 32/60000 datapoints
2025-03-06 19:28:38,888 - INFO - training batch 51, loss: 0.334, 1632/60000 datapoints
2025-03-06 19:28:39,086 - INFO - training batch 101, loss: 0.338, 3232/60000 datapoints
2025-03-06 19:28:39,284 - INFO - training batch 151, loss: 0.284, 4832/60000 datapoints
2025-03-06 19:28:39,481 - INFO - training batch 201, loss: 0.488, 6432/60000 datapoints
2025-03-06 19:28:39,682 - INFO - training batch 251, loss: 0.164, 8032/60000 datapoints
2025-03-06 19:28:39,878 - INFO - training batch 301, loss: 0.325, 9632/60000 datapoints
2025-03-06 19:28:40,077 - INFO - training batch 351, loss: 0.308, 11232/60000 datapoints
2025-03-06 19:28:40,273 - INFO - training batch 401, loss: 0.214, 12832/60000 datapoints
2025-03-06 19:28:40,467 - INFO - training batch 451, loss: 0.134, 14432/60000 datapoints
2025-03-06 19:28:40,666 - INFO - training batch 501, loss: 0.582, 16032/60000 datapoints
2025-03-06 19:28:40,861 - INFO - training batch 551, loss: 0.162, 17632/60000 datapoints
2025-03-06 19:28:41,057 - INFO - training batch 601, loss: 0.392, 19232/60000 datapoints
2025-03-06 19:28:41,252 - INFO - training batch 651, loss: 0.347, 20832/60000 datapoints
2025-03-06 19:28:41,448 - INFO - training batch 701, loss: 0.179, 22432/60000 datapoints
2025-03-06 19:28:41,642 - INFO - training batch 751, loss: 0.297, 24032/60000 datapoints
2025-03-06 19:28:41,835 - INFO - training batch 801, loss: 0.303, 25632/60000 datapoints
2025-03-06 19:28:42,028 - INFO - training batch 851, loss: 0.168, 27232/60000 datapoints
2025-03-06 19:28:42,221 - INFO - training batch 901, loss: 0.234, 28832/60000 datapoints
2025-03-06 19:28:42,417 - INFO - training batch 951, loss: 0.676, 30432/60000 datapoints
2025-03-06 19:28:42,610 - INFO - training batch 1001, loss: 0.327, 32032/60000 datapoints
2025-03-06 19:28:42,805 - INFO - training batch 1051, loss: 0.307, 33632/60000 datapoints
2025-03-06 19:28:42,999 - INFO - training batch 1101, loss: 0.234, 35232/60000 datapoints
2025-03-06 19:28:43,194 - INFO - training batch 1151, loss: 0.267, 36832/60000 datapoints
2025-03-06 19:28:43,388 - INFO - training batch 1201, loss: 0.171, 38432/60000 datapoints
2025-03-06 19:28:43,581 - INFO - training batch 1251, loss: 0.649, 40032/60000 datapoints
2025-03-06 19:28:43,777 - INFO - training batch 1301, loss: 0.262, 41632/60000 datapoints
2025-03-06 19:28:43,973 - INFO - training batch 1351, loss: 0.275, 43232/60000 datapoints
2025-03-06 19:28:44,169 - INFO - training batch 1401, loss: 0.334, 44832/60000 datapoints
2025-03-06 19:28:44,366 - INFO - training batch 1451, loss: 0.235, 46432/60000 datapoints
2025-03-06 19:28:44,562 - INFO - training batch 1501, loss: 0.264, 48032/60000 datapoints
2025-03-06 19:28:44,758 - INFO - training batch 1551, loss: 0.149, 49632/60000 datapoints
2025-03-06 19:28:44,954 - INFO - training batch 1601, loss: 0.265, 51232/60000 datapoints
2025-03-06 19:28:45,152 - INFO - training batch 1651, loss: 0.121, 52832/60000 datapoints
2025-03-06 19:28:45,344 - INFO - training batch 1701, loss: 0.367, 54432/60000 datapoints
2025-03-06 19:28:45,540 - INFO - training batch 1751, loss: 0.303, 56032/60000 datapoints
2025-03-06 19:28:45,760 - INFO - training batch 1801, loss: 0.245, 57632/60000 datapoints
2025-03-06 19:28:45,954 - INFO - training batch 1851, loss: 0.087, 59232/60000 datapoints
2025-03-06 19:28:46,058 - INFO - validation batch 1, loss: 0.192, 32/10016 datapoints
2025-03-06 19:28:46,216 - INFO - validation batch 51, loss: 0.341, 1632/10016 datapoints
2025-03-06 19:28:46,371 - INFO - validation batch 101, loss: 0.699, 3232/10016 datapoints
2025-03-06 19:28:46,523 - INFO - validation batch 151, loss: 0.313, 4832/10016 datapoints
2025-03-06 19:28:46,676 - INFO - validation batch 201, loss: 0.339, 6432/10016 datapoints
2025-03-06 19:28:46,828 - INFO - validation batch 251, loss: 0.311, 8032/10016 datapoints
2025-03-06 19:28:46,983 - INFO - validation batch 301, loss: 0.247, 9632/10016 datapoints
2025-03-06 19:28:47,020 - INFO - Epoch 341/800 done.
2025-03-06 19:28:47,021 - INFO - Final validation performance:
Loss: 0.349, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 19:28:47,021 - INFO - Beginning epoch 342/800
2025-03-06 19:28:47,028 - INFO - training batch 1, loss: 0.222, 32/60000 datapoints
2025-03-06 19:28:47,221 - INFO - training batch 51, loss: 0.407, 1632/60000 datapoints
2025-03-06 19:28:47,418 - INFO - training batch 101, loss: 0.221, 3232/60000 datapoints
2025-03-06 19:28:47,622 - INFO - training batch 151, loss: 0.499, 4832/60000 datapoints
2025-03-06 19:28:47,816 - INFO - training batch 201, loss: 0.351, 6432/60000 datapoints
2025-03-06 19:28:48,012 - INFO - training batch 251, loss: 0.210, 8032/60000 datapoints
2025-03-06 19:28:48,225 - INFO - training batch 301, loss: 0.332, 9632/60000 datapoints
2025-03-06 19:28:48,423 - INFO - training batch 351, loss: 0.301, 11232/60000 datapoints
2025-03-06 19:28:48,622 - INFO - training batch 401, loss: 0.191, 12832/60000 datapoints
2025-03-06 19:28:48,814 - INFO - training batch 451, loss: 0.176, 14432/60000 datapoints
2025-03-06 19:28:49,010 - INFO - training batch 501, loss: 0.260, 16032/60000 datapoints
2025-03-06 19:28:49,203 - INFO - training batch 551, loss: 0.502, 17632/60000 datapoints
2025-03-06 19:28:49,397 - INFO - training batch 601, loss: 0.308, 19232/60000 datapoints
2025-03-06 19:28:49,591 - INFO - training batch 651, loss: 0.262, 20832/60000 datapoints
2025-03-06 19:28:49,788 - INFO - training batch 701, loss: 0.307, 22432/60000 datapoints
2025-03-06 19:28:49,983 - INFO - training batch 751, loss: 0.164, 24032/60000 datapoints
2025-03-06 19:28:50,182 - INFO - training batch 801, loss: 0.368, 25632/60000 datapoints
2025-03-06 19:28:50,375 - INFO - training batch 851, loss: 0.135, 27232/60000 datapoints
2025-03-06 19:28:50,571 - INFO - training batch 901, loss: 0.717, 28832/60000 datapoints
2025-03-06 19:28:50,765 - INFO - training batch 951, loss: 0.122, 30432/60000 datapoints
2025-03-06 19:28:50,959 - INFO - training batch 1001, loss: 0.418, 32032/60000 datapoints
2025-03-06 19:28:51,155 - INFO - training batch 1051, loss: 0.230, 33632/60000 datapoints
2025-03-06 19:28:51,351 - INFO - training batch 1101, loss: 0.267, 35232/60000 datapoints
2025-03-06 19:28:51,545 - INFO - training batch 1151, loss: 0.119, 36832/60000 datapoints
2025-03-06 19:28:51,742 - INFO - training batch 1201, loss: 0.267, 38432/60000 datapoints
2025-03-06 19:28:51,938 - INFO - training batch 1251, loss: 0.419, 40032/60000 datapoints
2025-03-06 19:28:52,131 - INFO - training batch 1301, loss: 0.337, 41632/60000 datapoints
2025-03-06 19:28:52,326 - INFO - training batch 1351, loss: 0.295, 43232/60000 datapoints
2025-03-06 19:28:52,521 - INFO - training batch 1401, loss: 0.629, 44832/60000 datapoints
2025-03-06 19:28:52,718 - INFO - training batch 1451, loss: 0.118, 46432/60000 datapoints
2025-03-06 19:28:52,911 - INFO - training batch 1501, loss: 0.452, 48032/60000 datapoints
2025-03-06 19:28:53,107 - INFO - training batch 1551, loss: 0.219, 49632/60000 datapoints
2025-03-06 19:28:53,300 - INFO - training batch 1601, loss: 0.087, 51232/60000 datapoints
2025-03-06 19:28:53,494 - INFO - training batch 1651, loss: 0.123, 52832/60000 datapoints
2025-03-06 19:28:53,690 - INFO - training batch 1701, loss: 0.513, 54432/60000 datapoints
2025-03-06 19:28:53,885 - INFO - training batch 1751, loss: 0.184, 56032/60000 datapoints
2025-03-06 19:28:54,080 - INFO - training batch 1801, loss: 0.326, 57632/60000 datapoints
2025-03-06 19:28:54,278 - INFO - training batch 1851, loss: 0.322, 59232/60000 datapoints
2025-03-06 19:28:54,381 - INFO - validation batch 1, loss: 0.188, 32/10016 datapoints
2025-03-06 19:28:54,535 - INFO - validation batch 51, loss: 0.248, 1632/10016 datapoints
2025-03-06 19:28:54,689 - INFO - validation batch 101, loss: 0.568, 3232/10016 datapoints
2025-03-06 19:28:54,841 - INFO - validation batch 151, loss: 0.257, 4832/10016 datapoints
2025-03-06 19:28:54,998 - INFO - validation batch 201, loss: 0.621, 6432/10016 datapoints
2025-03-06 19:28:55,151 - INFO - validation batch 251, loss: 0.385, 8032/10016 datapoints
2025-03-06 19:28:55,303 - INFO - validation batch 301, loss: 0.639, 9632/10016 datapoints
2025-03-06 19:28:55,340 - INFO - Epoch 342/800 done.
2025-03-06 19:28:55,340 - INFO - Final validation performance:
Loss: 0.415, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 19:28:55,341 - INFO - Beginning epoch 343/800
2025-03-06 19:28:55,347 - INFO - training batch 1, loss: 0.296, 32/60000 datapoints
2025-03-06 19:28:55,554 - INFO - training batch 51, loss: 0.676, 1632/60000 datapoints
2025-03-06 19:28:55,770 - INFO - training batch 101, loss: 0.215, 3232/60000 datapoints
2025-03-06 19:28:55,971 - INFO - training batch 151, loss: 0.426, 4832/60000 datapoints
2025-03-06 19:28:56,169 - INFO - training batch 201, loss: 0.117, 6432/60000 datapoints
2025-03-06 19:28:56,368 - INFO - training batch 251, loss: 0.274, 8032/60000 datapoints
2025-03-06 19:28:56,563 - INFO - training batch 301, loss: 0.311, 9632/60000 datapoints
2025-03-06 19:28:56,756 - INFO - training batch 351, loss: 0.117, 11232/60000 datapoints
2025-03-06 19:28:56,951 - INFO - training batch 401, loss: 0.317, 12832/60000 datapoints
2025-03-06 19:28:57,145 - INFO - training batch 451, loss: 0.213, 14432/60000 datapoints
2025-03-06 19:28:57,338 - INFO - training batch 501, loss: 0.309, 16032/60000 datapoints
2025-03-06 19:28:57,533 - INFO - training batch 551, loss: 0.360, 17632/60000 datapoints
2025-03-06 19:28:57,729 - INFO - training batch 601, loss: 0.239, 19232/60000 datapoints
2025-03-06 19:28:57,925 - INFO - training batch 651, loss: 0.215, 20832/60000 datapoints
2025-03-06 19:28:58,121 - INFO - training batch 701, loss: 0.177, 22432/60000 datapoints
2025-03-06 19:28:58,317 - INFO - training batch 751, loss: 0.371, 24032/60000 datapoints
2025-03-06 19:28:58,513 - INFO - training batch 801, loss: 0.312, 25632/60000 datapoints
2025-03-06 19:28:58,706 - INFO - training batch 851, loss: 0.152, 27232/60000 datapoints
2025-03-06 19:28:58,897 - INFO - training batch 901, loss: 0.315, 28832/60000 datapoints
2025-03-06 19:28:59,089 - INFO - training batch 951, loss: 0.304, 30432/60000 datapoints
2025-03-06 19:28:59,281 - INFO - training batch 1001, loss: 0.254, 32032/60000 datapoints
2025-03-06 19:28:59,477 - INFO - training batch 1051, loss: 0.380, 33632/60000 datapoints
2025-03-06 19:28:59,674 - INFO - training batch 1101, loss: 0.311, 35232/60000 datapoints
2025-03-06 19:28:59,865 - INFO - training batch 1151, loss: 0.363, 36832/60000 datapoints
2025-03-06 19:29:00,062 - INFO - training batch 1201, loss: 0.682, 38432/60000 datapoints
2025-03-06 19:29:00,263 - INFO - training batch 1251, loss: 0.265, 40032/60000 datapoints
2025-03-06 19:29:00,458 - INFO - training batch 1301, loss: 0.222, 41632/60000 datapoints
2025-03-06 19:29:00,654 - INFO - training batch 1351, loss: 0.171, 43232/60000 datapoints
2025-03-06 19:29:00,846 - INFO - training batch 1401, loss: 0.315, 44832/60000 datapoints
2025-03-06 19:29:01,039 - INFO - training batch 1451, loss: 0.279, 46432/60000 datapoints
2025-03-06 19:29:01,235 - INFO - training batch 1501, loss: 0.104, 48032/60000 datapoints
2025-03-06 19:29:01,430 - INFO - training batch 1551, loss: 0.337, 49632/60000 datapoints
2025-03-06 19:29:01,629 - INFO - training batch 1601, loss: 0.198, 51232/60000 datapoints
2025-03-06 19:29:01,822 - INFO - training batch 1651, loss: 0.303, 52832/60000 datapoints
2025-03-06 19:29:02,017 - INFO - training batch 1701, loss: 0.140, 54432/60000 datapoints
2025-03-06 19:29:02,214 - INFO - training batch 1751, loss: 0.245, 56032/60000 datapoints
2025-03-06 19:29:02,410 - INFO - training batch 1801, loss: 0.365, 57632/60000 datapoints
2025-03-06 19:29:02,606 - INFO - training batch 1851, loss: 0.214, 59232/60000 datapoints
2025-03-06 19:29:02,708 - INFO - validation batch 1, loss: 0.229, 32/10016 datapoints
2025-03-06 19:29:02,858 - INFO - validation batch 51, loss: 0.588, 1632/10016 datapoints
2025-03-06 19:29:03,010 - INFO - validation batch 101, loss: 0.494, 3232/10016 datapoints
2025-03-06 19:29:03,165 - INFO - validation batch 151, loss: 0.354, 4832/10016 datapoints
2025-03-06 19:29:03,317 - INFO - validation batch 201, loss: 0.219, 6432/10016 datapoints
2025-03-06 19:29:03,470 - INFO - validation batch 251, loss: 0.503, 8032/10016 datapoints
2025-03-06 19:29:03,627 - INFO - validation batch 301, loss: 0.276, 9632/10016 datapoints
2025-03-06 19:29:03,680 - INFO - Epoch 343/800 done.
2025-03-06 19:29:03,682 - INFO - Final validation performance:
Loss: 0.381, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 19:29:03,684 - INFO - Beginning epoch 344/800
2025-03-06 19:29:03,710 - INFO - training batch 1, loss: 0.336, 32/60000 datapoints
2025-03-06 19:29:03,914 - INFO - training batch 51, loss: 0.232, 1632/60000 datapoints
2025-03-06 19:29:04,107 - INFO - training batch 101, loss: 0.157, 3232/60000 datapoints
2025-03-06 19:29:04,305 - INFO - training batch 151, loss: 0.308, 4832/60000 datapoints
2025-03-06 19:29:04,502 - INFO - training batch 201, loss: 0.270, 6432/60000 datapoints
2025-03-06 19:29:04,700 - INFO - training batch 251, loss: 0.271, 8032/60000 datapoints
2025-03-06 19:29:04,899 - INFO - training batch 301, loss: 0.095, 9632/60000 datapoints
2025-03-06 19:29:05,092 - INFO - training batch 351, loss: 0.487, 11232/60000 datapoints
2025-03-06 19:29:05,284 - INFO - training batch 401, loss: 0.412, 12832/60000 datapoints
2025-03-06 19:29:05,477 - INFO - training batch 451, loss: 0.149, 14432/60000 datapoints
2025-03-06 19:29:05,672 - INFO - training batch 501, loss: 0.238, 16032/60000 datapoints
2025-03-06 19:29:05,882 - INFO - training batch 551, loss: 0.240, 17632/60000 datapoints
2025-03-06 19:29:06,076 - INFO - training batch 601, loss: 0.553, 19232/60000 datapoints
2025-03-06 19:29:06,271 - INFO - training batch 651, loss: 0.165, 20832/60000 datapoints
2025-03-06 19:29:06,466 - INFO - training batch 701, loss: 0.244, 22432/60000 datapoints
2025-03-06 19:29:06,659 - INFO - training batch 751, loss: 0.227, 24032/60000 datapoints
2025-03-06 19:29:06,850 - INFO - training batch 801, loss: 0.291, 25632/60000 datapoints
2025-03-06 19:29:07,043 - INFO - training batch 851, loss: 0.301, 27232/60000 datapoints
2025-03-06 19:29:07,236 - INFO - training batch 901, loss: 0.258, 28832/60000 datapoints
2025-03-06 19:29:07,427 - INFO - training batch 951, loss: 0.342, 30432/60000 datapoints
2025-03-06 19:29:07,622 - INFO - training batch 1001, loss: 0.187, 32032/60000 datapoints
2025-03-06 19:29:07,813 - INFO - training batch 1051, loss: 0.359, 33632/60000 datapoints
2025-03-06 19:29:08,005 - INFO - training batch 1101, loss: 0.736, 35232/60000 datapoints
2025-03-06 19:29:08,199 - INFO - training batch 1151, loss: 0.144, 36832/60000 datapoints
2025-03-06 19:29:08,394 - INFO - training batch 1201, loss: 0.191, 38432/60000 datapoints
2025-03-06 19:29:08,583 - INFO - training batch 1251, loss: 0.450, 40032/60000 datapoints
2025-03-06 19:29:08,777 - INFO - training batch 1301, loss: 0.090, 41632/60000 datapoints
2025-03-06 19:29:08,966 - INFO - training batch 1351, loss: 0.381, 43232/60000 datapoints
2025-03-06 19:29:09,159 - INFO - training batch 1401, loss: 0.273, 44832/60000 datapoints
2025-03-06 19:29:09,349 - INFO - training batch 1451, loss: 0.289, 46432/60000 datapoints
2025-03-06 19:29:09,541 - INFO - training batch 1501, loss: 0.362, 48032/60000 datapoints
2025-03-06 19:29:09,735 - INFO - training batch 1551, loss: 0.701, 49632/60000 datapoints
2025-03-06 19:29:09,927 - INFO - training batch 1601, loss: 0.481, 51232/60000 datapoints
2025-03-06 19:29:10,118 - INFO - training batch 1651, loss: 0.311, 52832/60000 datapoints
2025-03-06 19:29:10,313 - INFO - training batch 1701, loss: 0.166, 54432/60000 datapoints
2025-03-06 19:29:10,508 - INFO - training batch 1751, loss: 0.164, 56032/60000 datapoints
2025-03-06 19:29:10,707 - INFO - training batch 1801, loss: 0.659, 57632/60000 datapoints
2025-03-06 19:29:10,898 - INFO - training batch 1851, loss: 0.157, 59232/60000 datapoints
2025-03-06 19:29:10,996 - INFO - validation batch 1, loss: 0.280, 32/10016 datapoints
2025-03-06 19:29:11,147 - INFO - validation batch 51, loss: 0.419, 1632/10016 datapoints
2025-03-06 19:29:11,298 - INFO - validation batch 101, loss: 0.225, 3232/10016 datapoints
2025-03-06 19:29:11,449 - INFO - validation batch 151, loss: 0.416, 4832/10016 datapoints
2025-03-06 19:29:11,627 - INFO - validation batch 201, loss: 0.606, 6432/10016 datapoints
2025-03-06 19:29:11,780 - INFO - validation batch 251, loss: 0.285, 8032/10016 datapoints
2025-03-06 19:29:11,930 - INFO - validation batch 301, loss: 0.341, 9632/10016 datapoints
2025-03-06 19:29:11,966 - INFO - Epoch 344/800 done.
2025-03-06 19:29:11,966 - INFO - Final validation performance:
Loss: 0.367, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 19:29:11,967 - INFO - Beginning epoch 345/800
2025-03-06 19:29:11,973 - INFO - training batch 1, loss: 0.148, 32/60000 datapoints
2025-03-06 19:29:12,168 - INFO - training batch 51, loss: 0.302, 1632/60000 datapoints
2025-03-06 19:29:12,371 - INFO - training batch 101, loss: 0.599, 3232/60000 datapoints
2025-03-06 19:29:12,562 - INFO - training batch 151, loss: 0.245, 4832/60000 datapoints
2025-03-06 19:29:12,757 - INFO - training batch 201, loss: 0.397, 6432/60000 datapoints
2025-03-06 19:29:12,955 - INFO - training batch 251, loss: 0.464, 8032/60000 datapoints
2025-03-06 19:29:13,151 - INFO - training batch 301, loss: 0.354, 9632/60000 datapoints
2025-03-06 19:29:13,349 - INFO - training batch 351, loss: 0.342, 11232/60000 datapoints
2025-03-06 19:29:13,539 - INFO - training batch 401, loss: 0.233, 12832/60000 datapoints
2025-03-06 19:29:13,734 - INFO - training batch 451, loss: 0.669, 14432/60000 datapoints
2025-03-06 19:29:13,925 - INFO - training batch 501, loss: 0.259, 16032/60000 datapoints
2025-03-06 19:29:14,116 - INFO - training batch 551, loss: 0.432, 17632/60000 datapoints
2025-03-06 19:29:14,313 - INFO - training batch 601, loss: 0.230, 19232/60000 datapoints
2025-03-06 19:29:14,512 - INFO - training batch 651, loss: 0.253, 20832/60000 datapoints
2025-03-06 19:29:14,705 - INFO - training batch 701, loss: 0.256, 22432/60000 datapoints
2025-03-06 19:29:14,899 - INFO - training batch 751, loss: 0.122, 24032/60000 datapoints
2025-03-06 19:29:15,093 - INFO - training batch 801, loss: 0.203, 25632/60000 datapoints
2025-03-06 19:29:15,285 - INFO - training batch 851, loss: 0.196, 27232/60000 datapoints
2025-03-06 19:29:15,477 - INFO - training batch 901, loss: 0.139, 28832/60000 datapoints
2025-03-06 19:29:15,673 - INFO - training batch 951, loss: 0.324, 30432/60000 datapoints
2025-03-06 19:29:15,875 - INFO - training batch 1001, loss: 0.511, 32032/60000 datapoints
2025-03-06 19:29:16,073 - INFO - training batch 1051, loss: 0.245, 33632/60000 datapoints
2025-03-06 19:29:16,268 - INFO - training batch 1101, loss: 0.206, 35232/60000 datapoints
2025-03-06 19:29:16,465 - INFO - training batch 1151, loss: 0.166, 36832/60000 datapoints
2025-03-06 19:29:16,660 - INFO - training batch 1201, loss: 0.480, 38432/60000 datapoints
2025-03-06 19:29:16,853 - INFO - training batch 1251, loss: 0.440, 40032/60000 datapoints
2025-03-06 19:29:17,043 - INFO - training batch 1301, loss: 0.524, 41632/60000 datapoints
2025-03-06 19:29:17,234 - INFO - training batch 1351, loss: 0.385, 43232/60000 datapoints
2025-03-06 19:29:17,427 - INFO - training batch 1401, loss: 0.394, 44832/60000 datapoints
2025-03-06 19:29:17,619 - INFO - training batch 1451, loss: 0.315, 46432/60000 datapoints
2025-03-06 19:29:17,812 - INFO - training batch 1501, loss: 0.168, 48032/60000 datapoints
2025-03-06 19:29:18,003 - INFO - training batch 1551, loss: 0.528, 49632/60000 datapoints
2025-03-06 19:29:18,196 - INFO - training batch 1601, loss: 0.271, 51232/60000 datapoints
2025-03-06 19:29:18,391 - INFO - training batch 1651, loss: 0.499, 52832/60000 datapoints
2025-03-06 19:29:18,583 - INFO - training batch 1701, loss: 0.400, 54432/60000 datapoints
2025-03-06 19:29:18,777 - INFO - training batch 1751, loss: 0.217, 56032/60000 datapoints
2025-03-06 19:29:18,967 - INFO - training batch 1801, loss: 0.395, 57632/60000 datapoints
2025-03-06 19:29:19,158 - INFO - training batch 1851, loss: 0.195, 59232/60000 datapoints
2025-03-06 19:29:19,256 - INFO - validation batch 1, loss: 0.235, 32/10016 datapoints
2025-03-06 19:29:19,407 - INFO - validation batch 51, loss: 0.336, 1632/10016 datapoints
2025-03-06 19:29:19,556 - INFO - validation batch 101, loss: 0.294, 3232/10016 datapoints
2025-03-06 19:29:19,709 - INFO - validation batch 151, loss: 0.297, 4832/10016 datapoints
2025-03-06 19:29:19,860 - INFO - validation batch 201, loss: 0.317, 6432/10016 datapoints
2025-03-06 19:29:20,010 - INFO - validation batch 251, loss: 0.183, 8032/10016 datapoints
2025-03-06 19:29:20,159 - INFO - validation batch 301, loss: 0.323, 9632/10016 datapoints
2025-03-06 19:29:20,195 - INFO - Epoch 345/800 done.
2025-03-06 19:29:20,195 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 19:29:20,196 - INFO - Beginning epoch 346/800
2025-03-06 19:29:20,202 - INFO - training batch 1, loss: 0.367, 32/60000 datapoints
2025-03-06 19:29:20,415 - INFO - training batch 51, loss: 0.096, 1632/60000 datapoints
2025-03-06 19:29:20,621 - INFO - training batch 101, loss: 0.197, 3232/60000 datapoints
2025-03-06 19:29:20,825 - INFO - training batch 151, loss: 0.807, 4832/60000 datapoints
2025-03-06 19:29:21,020 - INFO - training batch 201, loss: 0.395, 6432/60000 datapoints
2025-03-06 19:29:21,218 - INFO - training batch 251, loss: 0.362, 8032/60000 datapoints
2025-03-06 19:29:21,414 - INFO - training batch 301, loss: 0.543, 9632/60000 datapoints
2025-03-06 19:29:21,609 - INFO - training batch 351, loss: 0.424, 11232/60000 datapoints
2025-03-06 19:29:21,807 - INFO - training batch 401, loss: 0.279, 12832/60000 datapoints
2025-03-06 19:29:22,000 - INFO - training batch 451, loss: 0.388, 14432/60000 datapoints
2025-03-06 19:29:22,194 - INFO - training batch 501, loss: 0.307, 16032/60000 datapoints
2025-03-06 19:29:22,393 - INFO - training batch 551, loss: 0.296, 17632/60000 datapoints
2025-03-06 19:29:22,587 - INFO - training batch 601, loss: 0.222, 19232/60000 datapoints
2025-03-06 19:29:22,785 - INFO - training batch 651, loss: 0.156, 20832/60000 datapoints
2025-03-06 19:29:22,979 - INFO - training batch 701, loss: 0.482, 22432/60000 datapoints
2025-03-06 19:29:23,173 - INFO - training batch 751, loss: 0.293, 24032/60000 datapoints
2025-03-06 19:29:23,367 - INFO - training batch 801, loss: 0.406, 25632/60000 datapoints
2025-03-06 19:29:23,561 - INFO - training batch 851, loss: 0.235, 27232/60000 datapoints
2025-03-06 19:29:23,759 - INFO - training batch 901, loss: 0.275, 28832/60000 datapoints
2025-03-06 19:29:23,955 - INFO - training batch 951, loss: 0.276, 30432/60000 datapoints
2025-03-06 19:29:24,150 - INFO - training batch 1001, loss: 0.433, 32032/60000 datapoints
2025-03-06 19:29:24,349 - INFO - training batch 1051, loss: 0.289, 33632/60000 datapoints
2025-03-06 19:29:24,545 - INFO - training batch 1101, loss: 0.141, 35232/60000 datapoints
2025-03-06 19:29:24,741 - INFO - training batch 1151, loss: 0.311, 36832/60000 datapoints
2025-03-06 19:29:24,944 - INFO - training batch 1201, loss: 0.215, 38432/60000 datapoints
2025-03-06 19:29:25,139 - INFO - training batch 1251, loss: 0.222, 40032/60000 datapoints
2025-03-06 19:29:25,334 - INFO - training batch 1301, loss: 0.433, 41632/60000 datapoints
2025-03-06 19:29:25,526 - INFO - training batch 1351, loss: 0.192, 43232/60000 datapoints
2025-03-06 19:29:25,724 - INFO - training batch 1401, loss: 0.305, 44832/60000 datapoints
2025-03-06 19:29:25,923 - INFO - training batch 1451, loss: 0.113, 46432/60000 datapoints
2025-03-06 19:29:26,132 - INFO - training batch 1501, loss: 0.166, 48032/60000 datapoints
2025-03-06 19:29:26,330 - INFO - training batch 1551, loss: 0.563, 49632/60000 datapoints
2025-03-06 19:29:26,524 - INFO - training batch 1601, loss: 0.223, 51232/60000 datapoints
2025-03-06 19:29:26,731 - INFO - training batch 1651, loss: 0.249, 52832/60000 datapoints
2025-03-06 19:29:26,927 - INFO - training batch 1701, loss: 0.326, 54432/60000 datapoints
2025-03-06 19:29:27,120 - INFO - training batch 1751, loss: 0.528, 56032/60000 datapoints
2025-03-06 19:29:27,313 - INFO - training batch 1801, loss: 0.251, 57632/60000 datapoints
2025-03-06 19:29:27,507 - INFO - training batch 1851, loss: 0.281, 59232/60000 datapoints
2025-03-06 19:29:27,608 - INFO - validation batch 1, loss: 0.479, 32/10016 datapoints
2025-03-06 19:29:27,762 - INFO - validation batch 51, loss: 0.433, 1632/10016 datapoints
2025-03-06 19:29:27,916 - INFO - validation batch 101, loss: 0.417, 3232/10016 datapoints
2025-03-06 19:29:28,069 - INFO - validation batch 151, loss: 0.353, 4832/10016 datapoints
2025-03-06 19:29:28,221 - INFO - validation batch 201, loss: 0.153, 6432/10016 datapoints
2025-03-06 19:29:28,377 - INFO - validation batch 251, loss: 0.143, 8032/10016 datapoints
2025-03-06 19:29:28,528 - INFO - validation batch 301, loss: 0.235, 9632/10016 datapoints
2025-03-06 19:29:28,565 - INFO - Epoch 346/800 done.
2025-03-06 19:29:28,566 - INFO - Final validation performance:
Loss: 0.316, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 19:29:28,566 - INFO - Beginning epoch 347/800
2025-03-06 19:29:28,573 - INFO - training batch 1, loss: 0.143, 32/60000 datapoints
2025-03-06 19:29:28,776 - INFO - training batch 51, loss: 0.229, 1632/60000 datapoints
2025-03-06 19:29:28,972 - INFO - training batch 101, loss: 0.322, 3232/60000 datapoints
2025-03-06 19:29:29,174 - INFO - training batch 151, loss: 0.320, 4832/60000 datapoints
2025-03-06 19:29:29,366 - INFO - training batch 201, loss: 0.308, 6432/60000 datapoints
2025-03-06 19:29:29,559 - INFO - training batch 251, loss: 0.216, 8032/60000 datapoints
2025-03-06 19:29:29,760 - INFO - training batch 301, loss: 0.151, 9632/60000 datapoints
2025-03-06 19:29:29,955 - INFO - training batch 351, loss: 0.412, 11232/60000 datapoints
2025-03-06 19:29:30,150 - INFO - training batch 401, loss: 0.238, 12832/60000 datapoints
2025-03-06 19:29:30,346 - INFO - training batch 451, loss: 0.204, 14432/60000 datapoints
2025-03-06 19:29:30,537 - INFO - training batch 501, loss: 0.279, 16032/60000 datapoints
2025-03-06 19:29:30,733 - INFO - training batch 551, loss: 0.572, 17632/60000 datapoints
2025-03-06 19:29:30,925 - INFO - training batch 601, loss: 0.437, 19232/60000 datapoints
2025-03-06 19:29:31,116 - INFO - training batch 651, loss: 0.319, 20832/60000 datapoints
2025-03-06 19:29:31,307 - INFO - training batch 701, loss: 0.155, 22432/60000 datapoints
2025-03-06 19:29:31,558 - INFO - training batch 751, loss: 0.115, 24032/60000 datapoints
2025-03-06 19:29:31,756 - INFO - training batch 801, loss: 0.134, 25632/60000 datapoints
2025-03-06 19:29:31,948 - INFO - training batch 851, loss: 0.362, 27232/60000 datapoints
2025-03-06 19:29:32,139 - INFO - training batch 901, loss: 0.413, 28832/60000 datapoints
2025-03-06 19:29:32,332 - INFO - training batch 951, loss: 0.454, 30432/60000 datapoints
2025-03-06 19:29:32,523 - INFO - training batch 1001, loss: 0.318, 32032/60000 datapoints
2025-03-06 19:29:32,715 - INFO - training batch 1051, loss: 0.126, 33632/60000 datapoints
2025-03-06 19:29:32,907 - INFO - training batch 1101, loss: 0.426, 35232/60000 datapoints
2025-03-06 19:29:33,099 - INFO - training batch 1151, loss: 0.106, 36832/60000 datapoints
2025-03-06 19:29:33,291 - INFO - training batch 1201, loss: 0.260, 38432/60000 datapoints
2025-03-06 19:29:33,483 - INFO - training batch 1251, loss: 0.141, 40032/60000 datapoints
2025-03-06 19:29:33,677 - INFO - training batch 1301, loss: 0.246, 41632/60000 datapoints
2025-03-06 19:29:33,868 - INFO - training batch 1351, loss: 0.203, 43232/60000 datapoints
2025-03-06 19:29:34,062 - INFO - training batch 1401, loss: 0.351, 44832/60000 datapoints
2025-03-06 19:29:34,253 - INFO - training batch 1451, loss: 0.406, 46432/60000 datapoints
2025-03-06 19:29:34,450 - INFO - training batch 1501, loss: 0.305, 48032/60000 datapoints
2025-03-06 19:29:34,645 - INFO - training batch 1551, loss: 0.757, 49632/60000 datapoints
2025-03-06 19:29:34,856 - INFO - training batch 1601, loss: 0.249, 51232/60000 datapoints
2025-03-06 19:29:35,052 - INFO - training batch 1651, loss: 0.470, 52832/60000 datapoints
2025-03-06 19:29:35,242 - INFO - training batch 1701, loss: 0.231, 54432/60000 datapoints
2025-03-06 19:29:35,432 - INFO - training batch 1751, loss: 0.310, 56032/60000 datapoints
2025-03-06 19:29:35,626 - INFO - training batch 1801, loss: 0.135, 57632/60000 datapoints
2025-03-06 19:29:35,819 - INFO - training batch 1851, loss: 0.397, 59232/60000 datapoints
2025-03-06 19:29:35,918 - INFO - validation batch 1, loss: 0.584, 32/10016 datapoints
2025-03-06 19:29:36,086 - INFO - validation batch 51, loss: 0.421, 1632/10016 datapoints
2025-03-06 19:29:36,235 - INFO - validation batch 101, loss: 0.134, 3232/10016 datapoints
2025-03-06 19:29:36,387 - INFO - validation batch 151, loss: 0.192, 4832/10016 datapoints
2025-03-06 19:29:36,538 - INFO - validation batch 201, loss: 0.358, 6432/10016 datapoints
2025-03-06 19:29:36,687 - INFO - validation batch 251, loss: 0.624, 8032/10016 datapoints
2025-03-06 19:29:36,849 - INFO - validation batch 301, loss: 0.125, 9632/10016 datapoints
2025-03-06 19:29:36,887 - INFO - Epoch 347/800 done.
2025-03-06 19:29:36,888 - INFO - Final validation performance:
Loss: 0.348, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 19:29:36,888 - INFO - Beginning epoch 348/800
2025-03-06 19:29:36,895 - INFO - training batch 1, loss: 0.193, 32/60000 datapoints
2025-03-06 19:29:37,114 - INFO - training batch 51, loss: 0.306, 1632/60000 datapoints
2025-03-06 19:29:37,308 - INFO - training batch 101, loss: 0.210, 3232/60000 datapoints
2025-03-06 19:29:37,509 - INFO - training batch 151, loss: 0.247, 4832/60000 datapoints
2025-03-06 19:29:37,727 - INFO - training batch 201, loss: 0.301, 6432/60000 datapoints
2025-03-06 19:29:37,924 - INFO - training batch 251, loss: 0.201, 8032/60000 datapoints
2025-03-06 19:29:38,119 - INFO - training batch 301, loss: 0.221, 9632/60000 datapoints
2025-03-06 19:29:38,312 - INFO - training batch 351, loss: 0.211, 11232/60000 datapoints
2025-03-06 19:29:38,510 - INFO - training batch 401, loss: 0.159, 12832/60000 datapoints
2025-03-06 19:29:38,704 - INFO - training batch 451, loss: 0.373, 14432/60000 datapoints
2025-03-06 19:29:38,897 - INFO - training batch 501, loss: 0.445, 16032/60000 datapoints
2025-03-06 19:29:39,092 - INFO - training batch 551, loss: 0.596, 17632/60000 datapoints
2025-03-06 19:29:39,286 - INFO - training batch 601, loss: 0.638, 19232/60000 datapoints
2025-03-06 19:29:39,482 - INFO - training batch 651, loss: 0.461, 20832/60000 datapoints
2025-03-06 19:29:39,677 - INFO - training batch 701, loss: 0.174, 22432/60000 datapoints
2025-03-06 19:29:39,869 - INFO - training batch 751, loss: 0.137, 24032/60000 datapoints
2025-03-06 19:29:40,066 - INFO - training batch 801, loss: 0.401, 25632/60000 datapoints
2025-03-06 19:29:40,260 - INFO - training batch 851, loss: 0.251, 27232/60000 datapoints
2025-03-06 19:29:40,456 - INFO - training batch 901, loss: 0.137, 28832/60000 datapoints
2025-03-06 19:29:40,653 - INFO - training batch 951, loss: 0.365, 30432/60000 datapoints
2025-03-06 19:29:40,848 - INFO - training batch 1001, loss: 0.273, 32032/60000 datapoints
2025-03-06 19:29:41,042 - INFO - training batch 1051, loss: 0.173, 33632/60000 datapoints
2025-03-06 19:29:41,239 - INFO - training batch 1101, loss: 0.294, 35232/60000 datapoints
2025-03-06 19:29:41,435 - INFO - training batch 1151, loss: 0.443, 36832/60000 datapoints
2025-03-06 19:29:41,633 - INFO - training batch 1201, loss: 0.574, 38432/60000 datapoints
2025-03-06 19:29:41,827 - INFO - training batch 1251, loss: 0.292, 40032/60000 datapoints
2025-03-06 19:29:42,022 - INFO - training batch 1301, loss: 0.523, 41632/60000 datapoints
2025-03-06 19:29:42,218 - INFO - training batch 1351, loss: 0.284, 43232/60000 datapoints
2025-03-06 19:29:42,414 - INFO - training batch 1401, loss: 0.279, 44832/60000 datapoints
2025-03-06 19:29:42,611 - INFO - training batch 1451, loss: 0.511, 46432/60000 datapoints
2025-03-06 19:29:42,805 - INFO - training batch 1501, loss: 0.356, 48032/60000 datapoints
2025-03-06 19:29:43,000 - INFO - training batch 1551, loss: 0.362, 49632/60000 datapoints
2025-03-06 19:29:43,192 - INFO - training batch 1601, loss: 0.274, 51232/60000 datapoints
2025-03-06 19:29:43,387 - INFO - training batch 1651, loss: 0.180, 52832/60000 datapoints
2025-03-06 19:29:43,578 - INFO - training batch 1701, loss: 0.500, 54432/60000 datapoints
2025-03-06 19:29:43,775 - INFO - training batch 1751, loss: 0.118, 56032/60000 datapoints
2025-03-06 19:29:43,969 - INFO - training batch 1801, loss: 0.149, 57632/60000 datapoints
2025-03-06 19:29:44,166 - INFO - training batch 1851, loss: 0.717, 59232/60000 datapoints
2025-03-06 19:29:44,267 - INFO - validation batch 1, loss: 0.399, 32/10016 datapoints
2025-03-06 19:29:44,426 - INFO - validation batch 51, loss: 0.217, 1632/10016 datapoints
2025-03-06 19:29:44,579 - INFO - validation batch 101, loss: 0.186, 3232/10016 datapoints
2025-03-06 19:29:44,734 - INFO - validation batch 151, loss: 0.145, 4832/10016 datapoints
2025-03-06 19:29:44,897 - INFO - validation batch 201, loss: 0.343, 6432/10016 datapoints
2025-03-06 19:29:45,051 - INFO - validation batch 251, loss: 0.387, 8032/10016 datapoints
2025-03-06 19:29:45,205 - INFO - validation batch 301, loss: 0.182, 9632/10016 datapoints
2025-03-06 19:29:45,243 - INFO - Epoch 348/800 done.
2025-03-06 19:29:45,243 - INFO - Final validation performance:
Loss: 0.265, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 19:29:45,243 - INFO - Beginning epoch 349/800
2025-03-06 19:29:45,249 - INFO - training batch 1, loss: 0.469, 32/60000 datapoints
2025-03-06 19:29:45,456 - INFO - training batch 51, loss: 0.504, 1632/60000 datapoints
2025-03-06 19:29:45,664 - INFO - training batch 101, loss: 0.185, 3232/60000 datapoints
2025-03-06 19:29:45,865 - INFO - training batch 151, loss: 0.189, 4832/60000 datapoints
2025-03-06 19:29:46,062 - INFO - training batch 201, loss: 0.172, 6432/60000 datapoints
2025-03-06 19:29:46,278 - INFO - training batch 251, loss: 0.242, 8032/60000 datapoints
2025-03-06 19:29:46,476 - INFO - training batch 301, loss: 0.291, 9632/60000 datapoints
2025-03-06 19:29:46,673 - INFO - training batch 351, loss: 0.415, 11232/60000 datapoints
2025-03-06 19:29:46,868 - INFO - training batch 401, loss: 0.192, 12832/60000 datapoints
2025-03-06 19:29:47,068 - INFO - training batch 451, loss: 0.282, 14432/60000 datapoints
2025-03-06 19:29:47,266 - INFO - training batch 501, loss: 0.300, 16032/60000 datapoints
2025-03-06 19:29:47,458 - INFO - training batch 551, loss: 0.134, 17632/60000 datapoints
2025-03-06 19:29:47,652 - INFO - training batch 601, loss: 0.174, 19232/60000 datapoints
2025-03-06 19:29:47,846 - INFO - training batch 651, loss: 0.355, 20832/60000 datapoints
2025-03-06 19:29:48,046 - INFO - training batch 701, loss: 0.869, 22432/60000 datapoints
2025-03-06 19:29:48,243 - INFO - training batch 751, loss: 0.214, 24032/60000 datapoints
2025-03-06 19:29:48,438 - INFO - training batch 801, loss: 0.204, 25632/60000 datapoints
2025-03-06 19:29:48,635 - INFO - training batch 851, loss: 0.296, 27232/60000 datapoints
2025-03-06 19:29:48,829 - INFO - training batch 901, loss: 0.207, 28832/60000 datapoints
2025-03-06 19:29:49,024 - INFO - training batch 951, loss: 0.142, 30432/60000 datapoints
2025-03-06 19:29:49,217 - INFO - training batch 1001, loss: 0.239, 32032/60000 datapoints
2025-03-06 19:29:49,410 - INFO - training batch 1051, loss: 0.267, 33632/60000 datapoints
2025-03-06 19:29:49,604 - INFO - training batch 1101, loss: 0.195, 35232/60000 datapoints
2025-03-06 19:29:49,803 - INFO - training batch 1151, loss: 0.179, 36832/60000 datapoints
2025-03-06 19:29:49,998 - INFO - training batch 1201, loss: 0.215, 38432/60000 datapoints
2025-03-06 19:29:50,193 - INFO - training batch 1251, loss: 0.223, 40032/60000 datapoints
2025-03-06 19:29:50,390 - INFO - training batch 1301, loss: 0.241, 41632/60000 datapoints
2025-03-06 19:29:50,584 - INFO - training batch 1351, loss: 0.174, 43232/60000 datapoints
2025-03-06 19:29:50,780 - INFO - training batch 1401, loss: 0.143, 44832/60000 datapoints
2025-03-06 19:29:50,974 - INFO - training batch 1451, loss: 0.406, 46432/60000 datapoints
2025-03-06 19:29:51,168 - INFO - training batch 1501, loss: 0.223, 48032/60000 datapoints
2025-03-06 19:29:51,363 - INFO - training batch 1551, loss: 0.248, 49632/60000 datapoints
2025-03-06 19:29:51,556 - INFO - training batch 1601, loss: 0.676, 51232/60000 datapoints
2025-03-06 19:29:51,758 - INFO - training batch 1651, loss: 0.593, 52832/60000 datapoints
2025-03-06 19:29:51,953 - INFO - training batch 1701, loss: 0.167, 54432/60000 datapoints
2025-03-06 19:29:52,149 - INFO - training batch 1751, loss: 0.500, 56032/60000 datapoints
2025-03-06 19:29:52,342 - INFO - training batch 1801, loss: 0.578, 57632/60000 datapoints
2025-03-06 19:29:52,542 - INFO - training batch 1851, loss: 0.150, 59232/60000 datapoints
2025-03-06 19:29:52,645 - INFO - validation batch 1, loss: 0.169, 32/10016 datapoints
2025-03-06 19:29:52,801 - INFO - validation batch 51, loss: 0.291, 1632/10016 datapoints
2025-03-06 19:29:52,954 - INFO - validation batch 101, loss: 0.176, 3232/10016 datapoints
2025-03-06 19:29:53,106 - INFO - validation batch 151, loss: 0.367, 4832/10016 datapoints
2025-03-06 19:29:53,260 - INFO - validation batch 201, loss: 0.456, 6432/10016 datapoints
2025-03-06 19:29:53,414 - INFO - validation batch 251, loss: 0.250, 8032/10016 datapoints
2025-03-06 19:29:53,568 - INFO - validation batch 301, loss: 0.393, 9632/10016 datapoints
2025-03-06 19:29:53,603 - INFO - Epoch 349/800 done.
2025-03-06 19:29:53,603 - INFO - Final validation performance:
Loss: 0.300, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 19:29:53,604 - INFO - Beginning epoch 350/800
2025-03-06 19:29:53,611 - INFO - training batch 1, loss: 0.673, 32/60000 datapoints
2025-03-06 19:29:53,816 - INFO - training batch 51, loss: 0.280, 1632/60000 datapoints
2025-03-06 19:29:54,008 - INFO - training batch 101, loss: 0.312, 3232/60000 datapoints
2025-03-06 19:29:54,212 - INFO - training batch 151, loss: 0.322, 4832/60000 datapoints
2025-03-06 19:29:54,408 - INFO - training batch 201, loss: 0.484, 6432/60000 datapoints
2025-03-06 19:29:54,608 - INFO - training batch 251, loss: 0.206, 8032/60000 datapoints
2025-03-06 19:29:54,809 - INFO - training batch 301, loss: 0.177, 9632/60000 datapoints
2025-03-06 19:29:55,012 - INFO - training batch 351, loss: 0.059, 11232/60000 datapoints
2025-03-06 19:29:55,212 - INFO - training batch 401, loss: 0.338, 12832/60000 datapoints
2025-03-06 19:29:55,409 - INFO - training batch 451, loss: 0.158, 14432/60000 datapoints
2025-03-06 19:29:55,603 - INFO - training batch 501, loss: 0.246, 16032/60000 datapoints
2025-03-06 19:29:55,800 - INFO - training batch 551, loss: 0.339, 17632/60000 datapoints
2025-03-06 19:29:55,995 - INFO - training batch 601, loss: 0.452, 19232/60000 datapoints
2025-03-06 19:29:56,200 - INFO - training batch 651, loss: 0.527, 20832/60000 datapoints
2025-03-06 19:29:56,409 - INFO - training batch 701, loss: 0.377, 22432/60000 datapoints
2025-03-06 19:29:56,605 - INFO - training batch 751, loss: 0.236, 24032/60000 datapoints
2025-03-06 19:29:56,803 - INFO - training batch 801, loss: 0.577, 25632/60000 datapoints
2025-03-06 19:29:56,999 - INFO - training batch 851, loss: 0.202, 27232/60000 datapoints
2025-03-06 19:29:57,195 - INFO - training batch 901, loss: 0.278, 28832/60000 datapoints
2025-03-06 19:29:57,390 - INFO - training batch 951, loss: 0.379, 30432/60000 datapoints
2025-03-06 19:29:57,584 - INFO - training batch 1001, loss: 0.328, 32032/60000 datapoints
2025-03-06 19:29:57,782 - INFO - training batch 1051, loss: 0.223, 33632/60000 datapoints
2025-03-06 19:29:57,977 - INFO - training batch 1101, loss: 0.168, 35232/60000 datapoints
2025-03-06 19:29:58,172 - INFO - training batch 1151, loss: 0.384, 36832/60000 datapoints
2025-03-06 19:29:58,369 - INFO - training batch 1201, loss: 0.362, 38432/60000 datapoints
2025-03-06 19:29:58,568 - INFO - training batch 1251, loss: 0.215, 40032/60000 datapoints
2025-03-06 19:29:58,763 - INFO - training batch 1301, loss: 0.154, 41632/60000 datapoints
2025-03-06 19:29:58,957 - INFO - training batch 1351, loss: 0.282, 43232/60000 datapoints
2025-03-06 19:29:59,174 - INFO - training batch 1401, loss: 0.127, 44832/60000 datapoints
2025-03-06 19:29:59,400 - INFO - training batch 1451, loss: 0.351, 46432/60000 datapoints
2025-03-06 19:29:59,627 - INFO - training batch 1501, loss: 0.752, 48032/60000 datapoints
2025-03-06 19:29:59,852 - INFO - training batch 1551, loss: 0.233, 49632/60000 datapoints
2025-03-06 19:30:00,067 - INFO - training batch 1601, loss: 0.312, 51232/60000 datapoints
2025-03-06 19:30:00,263 - INFO - training batch 1651, loss: 0.406, 52832/60000 datapoints
2025-03-06 19:30:00,459 - INFO - training batch 1701, loss: 0.229, 54432/60000 datapoints
2025-03-06 19:30:00,657 - INFO - training batch 1751, loss: 0.222, 56032/60000 datapoints
2025-03-06 19:30:00,852 - INFO - training batch 1801, loss: 0.139, 57632/60000 datapoints
2025-03-06 19:30:01,047 - INFO - training batch 1851, loss: 0.207, 59232/60000 datapoints
2025-03-06 19:30:01,149 - INFO - validation batch 1, loss: 0.585, 32/10016 datapoints
2025-03-06 19:30:01,303 - INFO - validation batch 51, loss: 0.248, 1632/10016 datapoints
2025-03-06 19:30:01,454 - INFO - validation batch 101, loss: 0.155, 3232/10016 datapoints
2025-03-06 19:30:01,609 - INFO - validation batch 151, loss: 0.453, 4832/10016 datapoints
2025-03-06 19:30:01,766 - INFO - validation batch 201, loss: 0.288, 6432/10016 datapoints
2025-03-06 19:30:01,920 - INFO - validation batch 251, loss: 0.280, 8032/10016 datapoints
2025-03-06 19:30:02,072 - INFO - validation batch 301, loss: 0.097, 9632/10016 datapoints
2025-03-06 19:30:02,109 - INFO - Epoch 350/800 done.
2025-03-06 19:30:02,109 - INFO - Final validation performance:
Loss: 0.301, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 19:30:02,110 - INFO - Beginning epoch 351/800
2025-03-06 19:30:02,117 - INFO - training batch 1, loss: 0.362, 32/60000 datapoints
2025-03-06 19:30:02,329 - INFO - training batch 51, loss: 0.124, 1632/60000 datapoints
2025-03-06 19:30:02,524 - INFO - training batch 101, loss: 0.222, 3232/60000 datapoints
2025-03-06 19:30:02,716 - INFO - training batch 151, loss: 0.291, 4832/60000 datapoints
2025-03-06 19:30:02,915 - INFO - training batch 201, loss: 0.185, 6432/60000 datapoints
2025-03-06 19:30:03,111 - INFO - training batch 251, loss: 0.588, 8032/60000 datapoints
2025-03-06 19:30:03,309 - INFO - training batch 301, loss: 0.347, 9632/60000 datapoints
2025-03-06 19:30:03,501 - INFO - training batch 351, loss: 0.174, 11232/60000 datapoints
2025-03-06 19:30:03,696 - INFO - training batch 401, loss: 0.252, 12832/60000 datapoints
2025-03-06 19:30:03,888 - INFO - training batch 451, loss: 0.329, 14432/60000 datapoints
2025-03-06 19:30:04,081 - INFO - training batch 501, loss: 0.537, 16032/60000 datapoints
2025-03-06 19:30:04,272 - INFO - training batch 551, loss: 0.752, 17632/60000 datapoints
2025-03-06 19:30:04,476 - INFO - training batch 601, loss: 0.218, 19232/60000 datapoints
2025-03-06 19:30:04,670 - INFO - training batch 651, loss: 0.267, 20832/60000 datapoints
2025-03-06 19:30:04,865 - INFO - training batch 701, loss: 0.471, 22432/60000 datapoints
2025-03-06 19:30:05,065 - INFO - training batch 751, loss: 0.263, 24032/60000 datapoints
2025-03-06 19:30:05,258 - INFO - training batch 801, loss: 0.197, 25632/60000 datapoints
2025-03-06 19:30:05,452 - INFO - training batch 851, loss: 0.094, 27232/60000 datapoints
2025-03-06 19:30:05,649 - INFO - training batch 901, loss: 0.396, 28832/60000 datapoints
2025-03-06 19:30:05,853 - INFO - training batch 951, loss: 0.432, 30432/60000 datapoints
2025-03-06 19:30:06,044 - INFO - training batch 1001, loss: 0.295, 32032/60000 datapoints
2025-03-06 19:30:06,236 - INFO - training batch 1051, loss: 0.170, 33632/60000 datapoints
2025-03-06 19:30:06,447 - INFO - training batch 1101, loss: 0.458, 35232/60000 datapoints
2025-03-06 19:30:06,640 - INFO - training batch 1151, loss: 0.160, 36832/60000 datapoints
2025-03-06 19:30:06,830 - INFO - training batch 1201, loss: 0.851, 38432/60000 datapoints
2025-03-06 19:30:07,023 - INFO - training batch 1251, loss: 0.393, 40032/60000 datapoints
2025-03-06 19:30:07,215 - INFO - training batch 1301, loss: 0.147, 41632/60000 datapoints
2025-03-06 19:30:07,408 - INFO - training batch 1351, loss: 0.351, 43232/60000 datapoints
2025-03-06 19:30:07,601 - INFO - training batch 1401, loss: 0.413, 44832/60000 datapoints
2025-03-06 19:30:07,798 - INFO - training batch 1451, loss: 0.171, 46432/60000 datapoints
2025-03-06 19:30:07,989 - INFO - training batch 1501, loss: 0.216, 48032/60000 datapoints
2025-03-06 19:30:08,180 - INFO - training batch 1551, loss: 0.276, 49632/60000 datapoints
2025-03-06 19:30:08,372 - INFO - training batch 1601, loss: 0.443, 51232/60000 datapoints
2025-03-06 19:30:08,567 - INFO - training batch 1651, loss: 0.553, 52832/60000 datapoints
2025-03-06 19:30:08,759 - INFO - training batch 1701, loss: 0.294, 54432/60000 datapoints
2025-03-06 19:30:08,953 - INFO - training batch 1751, loss: 0.476, 56032/60000 datapoints
2025-03-06 19:30:09,147 - INFO - training batch 1801, loss: 0.332, 57632/60000 datapoints
2025-03-06 19:30:09,340 - INFO - training batch 1851, loss: 0.292, 59232/60000 datapoints
2025-03-06 19:30:09,439 - INFO - validation batch 1, loss: 0.647, 32/10016 datapoints
2025-03-06 19:30:09,589 - INFO - validation batch 51, loss: 0.346, 1632/10016 datapoints
2025-03-06 19:30:09,741 - INFO - validation batch 101, loss: 0.305, 3232/10016 datapoints
2025-03-06 19:30:09,893 - INFO - validation batch 151, loss: 0.644, 4832/10016 datapoints
2025-03-06 19:30:10,043 - INFO - validation batch 201, loss: 0.130, 6432/10016 datapoints
2025-03-06 19:30:10,195 - INFO - validation batch 251, loss: 0.300, 8032/10016 datapoints
2025-03-06 19:30:10,345 - INFO - validation batch 301, loss: 0.300, 9632/10016 datapoints
2025-03-06 19:30:10,382 - INFO - Epoch 351/800 done.
2025-03-06 19:30:10,382 - INFO - Final validation performance:
Loss: 0.382, top-1 acc: 0.916top-5 acc: 0.916
2025-03-06 19:30:10,383 - INFO - Beginning epoch 352/800
2025-03-06 19:30:10,389 - INFO - training batch 1, loss: 0.485, 32/60000 datapoints
2025-03-06 19:30:10,585 - INFO - training batch 51, loss: 0.542, 1632/60000 datapoints
2025-03-06 19:30:10,782 - INFO - training batch 101, loss: 0.242, 3232/60000 datapoints
2025-03-06 19:30:10,994 - INFO - training batch 151, loss: 0.309, 4832/60000 datapoints
2025-03-06 19:30:11,196 - INFO - training batch 201, loss: 0.316, 6432/60000 datapoints
2025-03-06 19:30:11,390 - INFO - training batch 251, loss: 0.360, 8032/60000 datapoints
2025-03-06 19:30:11,590 - INFO - training batch 301, loss: 0.270, 9632/60000 datapoints
2025-03-06 19:30:11,786 - INFO - training batch 351, loss: 0.352, 11232/60000 datapoints
2025-03-06 19:30:11,981 - INFO - training batch 401, loss: 0.317, 12832/60000 datapoints
2025-03-06 19:30:12,200 - INFO - training batch 451, loss: 0.116, 14432/60000 datapoints
2025-03-06 19:30:12,392 - INFO - training batch 501, loss: 0.188, 16032/60000 datapoints
2025-03-06 19:30:12,587 - INFO - training batch 551, loss: 0.401, 17632/60000 datapoints
2025-03-06 19:30:12,781 - INFO - training batch 601, loss: 0.361, 19232/60000 datapoints
2025-03-06 19:30:12,973 - INFO - training batch 651, loss: 0.137, 20832/60000 datapoints
2025-03-06 19:30:13,167 - INFO - training batch 701, loss: 0.270, 22432/60000 datapoints
2025-03-06 19:30:13,363 - INFO - training batch 751, loss: 0.309, 24032/60000 datapoints
2025-03-06 19:30:13,557 - INFO - training batch 801, loss: 0.183, 25632/60000 datapoints
2025-03-06 19:30:13,754 - INFO - training batch 851, loss: 0.460, 27232/60000 datapoints
2025-03-06 19:30:13,945 - INFO - training batch 901, loss: 0.331, 28832/60000 datapoints
2025-03-06 19:30:14,136 - INFO - training batch 951, loss: 0.186, 30432/60000 datapoints
2025-03-06 19:30:14,328 - INFO - training batch 1001, loss: 0.181, 32032/60000 datapoints
2025-03-06 19:30:14,530 - INFO - training batch 1051, loss: 0.180, 33632/60000 datapoints
2025-03-06 19:30:14,723 - INFO - training batch 1101, loss: 0.404, 35232/60000 datapoints
2025-03-06 19:30:14,920 - INFO - training batch 1151, loss: 0.191, 36832/60000 datapoints
2025-03-06 19:30:15,115 - INFO - training batch 1201, loss: 0.385, 38432/60000 datapoints
2025-03-06 19:30:15,307 - INFO - training batch 1251, loss: 0.240, 40032/60000 datapoints
2025-03-06 19:30:15,501 - INFO - training batch 1301, loss: 0.364, 41632/60000 datapoints
2025-03-06 19:30:15,696 - INFO - training batch 1351, loss: 0.253, 43232/60000 datapoints
2025-03-06 19:30:15,889 - INFO - training batch 1401, loss: 0.145, 44832/60000 datapoints
2025-03-06 19:30:16,083 - INFO - training batch 1451, loss: 0.209, 46432/60000 datapoints
2025-03-06 19:30:16,276 - INFO - training batch 1501, loss: 0.335, 48032/60000 datapoints
2025-03-06 19:30:16,492 - INFO - training batch 1551, loss: 0.139, 49632/60000 datapoints
2025-03-06 19:30:16,684 - INFO - training batch 1601, loss: 0.198, 51232/60000 datapoints
2025-03-06 19:30:16,882 - INFO - training batch 1651, loss: 0.229, 52832/60000 datapoints
2025-03-06 19:30:17,082 - INFO - training batch 1701, loss: 0.177, 54432/60000 datapoints
2025-03-06 19:30:17,276 - INFO - training batch 1751, loss: 0.331, 56032/60000 datapoints
2025-03-06 19:30:17,479 - INFO - training batch 1801, loss: 0.472, 57632/60000 datapoints
2025-03-06 19:30:17,680 - INFO - training batch 1851, loss: 0.958, 59232/60000 datapoints
2025-03-06 19:30:17,783 - INFO - validation batch 1, loss: 0.182, 32/10016 datapoints
2025-03-06 19:30:17,935 - INFO - validation batch 51, loss: 0.201, 1632/10016 datapoints
2025-03-06 19:30:18,089 - INFO - validation batch 101, loss: 0.110, 3232/10016 datapoints
2025-03-06 19:30:18,243 - INFO - validation batch 151, loss: 0.219, 4832/10016 datapoints
2025-03-06 19:30:18,396 - INFO - validation batch 201, loss: 0.146, 6432/10016 datapoints
2025-03-06 19:30:18,553 - INFO - validation batch 251, loss: 0.297, 8032/10016 datapoints
2025-03-06 19:30:18,707 - INFO - validation batch 301, loss: 0.296, 9632/10016 datapoints
2025-03-06 19:30:18,745 - INFO - Epoch 352/800 done.
2025-03-06 19:30:18,745 - INFO - Final validation performance:
Loss: 0.207, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:30:18,745 - INFO - Beginning epoch 353/800
2025-03-06 19:30:18,752 - INFO - training batch 1, loss: 0.327, 32/60000 datapoints
2025-03-06 19:30:18,946 - INFO - training batch 51, loss: 0.587, 1632/60000 datapoints
2025-03-06 19:30:19,152 - INFO - training batch 101, loss: 0.264, 3232/60000 datapoints
2025-03-06 19:30:19,346 - INFO - training batch 151, loss: 0.406, 4832/60000 datapoints
2025-03-06 19:30:19,547 - INFO - training batch 201, loss: 0.456, 6432/60000 datapoints
2025-03-06 19:30:19,751 - INFO - training batch 251, loss: 0.363, 8032/60000 datapoints
2025-03-06 19:30:19,948 - INFO - training batch 301, loss: 0.105, 9632/60000 datapoints
2025-03-06 19:30:20,145 - INFO - training batch 351, loss: 0.197, 11232/60000 datapoints
2025-03-06 19:30:20,338 - INFO - training batch 401, loss: 0.120, 12832/60000 datapoints
2025-03-06 19:30:20,539 - INFO - training batch 451, loss: 0.276, 14432/60000 datapoints
2025-03-06 19:30:20,738 - INFO - training batch 501, loss: 0.283, 16032/60000 datapoints
2025-03-06 19:30:20,932 - INFO - training batch 551, loss: 0.135, 17632/60000 datapoints
2025-03-06 19:30:21,126 - INFO - training batch 601, loss: 0.297, 19232/60000 datapoints
2025-03-06 19:30:21,321 - INFO - training batch 651, loss: 0.216, 20832/60000 datapoints
2025-03-06 19:30:21,516 - INFO - training batch 701, loss: 0.345, 22432/60000 datapoints
2025-03-06 19:30:21,712 - INFO - training batch 751, loss: 0.435, 24032/60000 datapoints
2025-03-06 19:30:21,909 - INFO - training batch 801, loss: 0.163, 25632/60000 datapoints
2025-03-06 19:30:22,103 - INFO - training batch 851, loss: 0.449, 27232/60000 datapoints
2025-03-06 19:30:22,298 - INFO - training batch 901, loss: 0.243, 28832/60000 datapoints
2025-03-06 19:30:22,494 - INFO - training batch 951, loss: 0.274, 30432/60000 datapoints
2025-03-06 19:30:22,691 - INFO - training batch 1001, loss: 0.298, 32032/60000 datapoints
2025-03-06 19:30:22,885 - INFO - training batch 1051, loss: 0.351, 33632/60000 datapoints
2025-03-06 19:30:23,081 - INFO - training batch 1101, loss: 0.569, 35232/60000 datapoints
2025-03-06 19:30:23,276 - INFO - training batch 1151, loss: 0.758, 36832/60000 datapoints
2025-03-06 19:30:23,469 - INFO - training batch 1201, loss: 0.337, 38432/60000 datapoints
2025-03-06 19:30:23,669 - INFO - training batch 1251, loss: 0.269, 40032/60000 datapoints
2025-03-06 19:30:23,860 - INFO - training batch 1301, loss: 0.499, 41632/60000 datapoints
2025-03-06 19:30:24,054 - INFO - training batch 1351, loss: 0.501, 43232/60000 datapoints
2025-03-06 19:30:24,249 - INFO - training batch 1401, loss: 0.237, 44832/60000 datapoints
2025-03-06 19:30:24,444 - INFO - training batch 1451, loss: 0.302, 46432/60000 datapoints
2025-03-06 19:30:24,647 - INFO - training batch 1501, loss: 0.505, 48032/60000 datapoints
2025-03-06 19:30:24,843 - INFO - training batch 1551, loss: 0.176, 49632/60000 datapoints
2025-03-06 19:30:25,039 - INFO - training batch 1601, loss: 0.207, 51232/60000 datapoints
2025-03-06 19:30:25,233 - INFO - training batch 1651, loss: 0.242, 52832/60000 datapoints
2025-03-06 19:30:25,429 - INFO - training batch 1701, loss: 0.137, 54432/60000 datapoints
2025-03-06 19:30:25,628 - INFO - training batch 1751, loss: 0.500, 56032/60000 datapoints
2025-03-06 19:30:25,824 - INFO - training batch 1801, loss: 0.326, 57632/60000 datapoints
2025-03-06 19:30:26,019 - INFO - training batch 1851, loss: 0.261, 59232/60000 datapoints
2025-03-06 19:30:26,121 - INFO - validation batch 1, loss: 0.451, 32/10016 datapoints
2025-03-06 19:30:26,275 - INFO - validation batch 51, loss: 0.228, 1632/10016 datapoints
2025-03-06 19:30:26,448 - INFO - validation batch 101, loss: 0.330, 3232/10016 datapoints
2025-03-06 19:30:26,609 - INFO - validation batch 151, loss: 0.123, 4832/10016 datapoints
2025-03-06 19:30:26,763 - INFO - validation batch 201, loss: 0.657, 6432/10016 datapoints
2025-03-06 19:30:26,915 - INFO - validation batch 251, loss: 0.251, 8032/10016 datapoints
2025-03-06 19:30:27,070 - INFO - validation batch 301, loss: 0.217, 9632/10016 datapoints
2025-03-06 19:30:27,108 - INFO - Epoch 353/800 done.
2025-03-06 19:30:27,108 - INFO - Final validation performance:
Loss: 0.322, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:30:27,109 - INFO - Beginning epoch 354/800
2025-03-06 19:30:27,117 - INFO - training batch 1, loss: 0.514, 32/60000 datapoints
2025-03-06 19:30:27,322 - INFO - training batch 51, loss: 0.287, 1632/60000 datapoints
2025-03-06 19:30:27,515 - INFO - training batch 101, loss: 0.284, 3232/60000 datapoints
2025-03-06 19:30:27,725 - INFO - training batch 151, loss: 0.255, 4832/60000 datapoints
2025-03-06 19:30:27,919 - INFO - training batch 201, loss: 0.452, 6432/60000 datapoints
2025-03-06 19:30:28,118 - INFO - training batch 251, loss: 0.632, 8032/60000 datapoints
2025-03-06 19:30:28,321 - INFO - training batch 301, loss: 0.243, 9632/60000 datapoints
2025-03-06 19:30:28,522 - INFO - training batch 351, loss: 0.191, 11232/60000 datapoints
2025-03-06 19:30:28,721 - INFO - training batch 401, loss: 0.241, 12832/60000 datapoints
2025-03-06 19:30:28,915 - INFO - training batch 451, loss: 0.180, 14432/60000 datapoints
2025-03-06 19:30:29,111 - INFO - training batch 501, loss: 0.418, 16032/60000 datapoints
2025-03-06 19:30:29,309 - INFO - training batch 551, loss: 0.229, 17632/60000 datapoints
2025-03-06 19:30:29,504 - INFO - training batch 601, loss: 0.207, 19232/60000 datapoints
2025-03-06 19:30:29,702 - INFO - training batch 651, loss: 0.515, 20832/60000 datapoints
2025-03-06 19:30:29,900 - INFO - training batch 701, loss: 0.389, 22432/60000 datapoints
2025-03-06 19:30:30,097 - INFO - training batch 751, loss: 0.584, 24032/60000 datapoints
2025-03-06 19:30:30,291 - INFO - training batch 801, loss: 0.372, 25632/60000 datapoints
2025-03-06 19:30:30,486 - INFO - training batch 851, loss: 0.213, 27232/60000 datapoints
2025-03-06 19:30:30,684 - INFO - training batch 901, loss: 0.292, 28832/60000 datapoints
2025-03-06 19:30:30,880 - INFO - training batch 951, loss: 0.166, 30432/60000 datapoints
2025-03-06 19:30:31,075 - INFO - training batch 1001, loss: 0.420, 32032/60000 datapoints
2025-03-06 19:30:31,271 - INFO - training batch 1051, loss: 0.433, 33632/60000 datapoints
2025-03-06 19:30:31,466 - INFO - training batch 1101, loss: 0.139, 35232/60000 datapoints
2025-03-06 19:30:31,663 - INFO - training batch 1151, loss: 0.241, 36832/60000 datapoints
2025-03-06 19:30:31,856 - INFO - training batch 1201, loss: 0.449, 38432/60000 datapoints
2025-03-06 19:30:32,053 - INFO - training batch 1251, loss: 0.155, 40032/60000 datapoints
2025-03-06 19:30:32,249 - INFO - training batch 1301, loss: 0.413, 41632/60000 datapoints
2025-03-06 19:30:32,445 - INFO - training batch 1351, loss: 0.267, 43232/60000 datapoints
2025-03-06 19:30:32,649 - INFO - training batch 1401, loss: 0.279, 44832/60000 datapoints
2025-03-06 19:30:32,845 - INFO - training batch 1451, loss: 0.162, 46432/60000 datapoints
2025-03-06 19:30:33,039 - INFO - training batch 1501, loss: 0.384, 48032/60000 datapoints
2025-03-06 19:30:33,237 - INFO - training batch 1551, loss: 0.057, 49632/60000 datapoints
2025-03-06 19:30:33,429 - INFO - training batch 1601, loss: 0.254, 51232/60000 datapoints
2025-03-06 19:30:33,626 - INFO - training batch 1651, loss: 0.304, 52832/60000 datapoints
2025-03-06 19:30:33,819 - INFO - training batch 1701, loss: 0.117, 54432/60000 datapoints
2025-03-06 19:30:34,017 - INFO - training batch 1751, loss: 0.252, 56032/60000 datapoints
2025-03-06 19:30:34,214 - INFO - training batch 1801, loss: 0.403, 57632/60000 datapoints
2025-03-06 19:30:34,409 - INFO - training batch 1851, loss: 0.248, 59232/60000 datapoints
2025-03-06 19:30:34,511 - INFO - validation batch 1, loss: 0.301, 32/10016 datapoints
2025-03-06 19:30:34,674 - INFO - validation batch 51, loss: 0.288, 1632/10016 datapoints
2025-03-06 19:30:34,826 - INFO - validation batch 101, loss: 0.282, 3232/10016 datapoints
2025-03-06 19:30:34,982 - INFO - validation batch 151, loss: 0.110, 4832/10016 datapoints
2025-03-06 19:30:35,136 - INFO - validation batch 201, loss: 0.192, 6432/10016 datapoints
2025-03-06 19:30:35,288 - INFO - validation batch 251, loss: 0.235, 8032/10016 datapoints
2025-03-06 19:30:35,441 - INFO - validation batch 301, loss: 0.354, 9632/10016 datapoints
2025-03-06 19:30:35,480 - INFO - Epoch 354/800 done.
2025-03-06 19:30:35,481 - INFO - Final validation performance:
Loss: 0.252, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:30:35,481 - INFO - Beginning epoch 355/800
2025-03-06 19:30:35,487 - INFO - training batch 1, loss: 0.333, 32/60000 datapoints
2025-03-06 19:30:35,695 - INFO - training batch 51, loss: 0.174, 1632/60000 datapoints
2025-03-06 19:30:35,911 - INFO - training batch 101, loss: 0.556, 3232/60000 datapoints
2025-03-06 19:30:36,109 - INFO - training batch 151, loss: 0.601, 4832/60000 datapoints
2025-03-06 19:30:36,312 - INFO - training batch 201, loss: 0.284, 6432/60000 datapoints
2025-03-06 19:30:36,515 - INFO - training batch 251, loss: 0.304, 8032/60000 datapoints
2025-03-06 19:30:36,730 - INFO - training batch 301, loss: 0.210, 9632/60000 datapoints
2025-03-06 19:30:36,927 - INFO - training batch 351, loss: 0.213, 11232/60000 datapoints
2025-03-06 19:30:37,124 - INFO - training batch 401, loss: 0.333, 12832/60000 datapoints
2025-03-06 19:30:37,320 - INFO - training batch 451, loss: 0.356, 14432/60000 datapoints
2025-03-06 19:30:37,514 - INFO - training batch 501, loss: 0.309, 16032/60000 datapoints
2025-03-06 19:30:37,725 - INFO - training batch 551, loss: 0.427, 17632/60000 datapoints
2025-03-06 19:30:37,920 - INFO - training batch 601, loss: 0.170, 19232/60000 datapoints
2025-03-06 19:30:38,114 - INFO - training batch 651, loss: 0.209, 20832/60000 datapoints
2025-03-06 19:30:38,309 - INFO - training batch 701, loss: 0.492, 22432/60000 datapoints
2025-03-06 19:30:38,504 - INFO - training batch 751, loss: 0.240, 24032/60000 datapoints
2025-03-06 19:30:38,705 - INFO - training batch 801, loss: 0.334, 25632/60000 datapoints
2025-03-06 19:30:38,899 - INFO - training batch 851, loss: 0.204, 27232/60000 datapoints
2025-03-06 19:30:39,093 - INFO - training batch 901, loss: 0.178, 28832/60000 datapoints
2025-03-06 19:30:39,290 - INFO - training batch 951, loss: 0.278, 30432/60000 datapoints
2025-03-06 19:30:39,485 - INFO - training batch 1001, loss: 0.295, 32032/60000 datapoints
2025-03-06 19:30:39,684 - INFO - training batch 1051, loss: 0.204, 33632/60000 datapoints
2025-03-06 19:30:39,879 - INFO - training batch 1101, loss: 0.337, 35232/60000 datapoints
2025-03-06 19:30:40,077 - INFO - training batch 1151, loss: 0.298, 36832/60000 datapoints
2025-03-06 19:30:40,274 - INFO - training batch 1201, loss: 0.298, 38432/60000 datapoints
2025-03-06 19:30:40,470 - INFO - training batch 1251, loss: 0.320, 40032/60000 datapoints
2025-03-06 19:30:40,671 - INFO - training batch 1301, loss: 0.217, 41632/60000 datapoints
2025-03-06 19:30:40,864 - INFO - training batch 1351, loss: 0.231, 43232/60000 datapoints
2025-03-06 19:30:41,058 - INFO - training batch 1401, loss: 0.371, 44832/60000 datapoints
2025-03-06 19:30:41,252 - INFO - training batch 1451, loss: 0.377, 46432/60000 datapoints
2025-03-06 19:30:41,447 - INFO - training batch 1501, loss: 0.480, 48032/60000 datapoints
2025-03-06 19:30:41,641 - INFO - training batch 1551, loss: 0.188, 49632/60000 datapoints
2025-03-06 19:30:41,837 - INFO - training batch 1601, loss: 0.209, 51232/60000 datapoints
2025-03-06 19:30:42,032 - INFO - training batch 1651, loss: 0.325, 52832/60000 datapoints
2025-03-06 19:30:42,226 - INFO - training batch 1701, loss: 0.515, 54432/60000 datapoints
2025-03-06 19:30:42,420 - INFO - training batch 1751, loss: 0.222, 56032/60000 datapoints
2025-03-06 19:30:42,617 - INFO - training batch 1801, loss: 0.238, 57632/60000 datapoints
2025-03-06 19:30:42,811 - INFO - training batch 1851, loss: 0.093, 59232/60000 datapoints
2025-03-06 19:30:42,912 - INFO - validation batch 1, loss: 0.147, 32/10016 datapoints
2025-03-06 19:30:43,064 - INFO - validation batch 51, loss: 0.229, 1632/10016 datapoints
2025-03-06 19:30:43,218 - INFO - validation batch 101, loss: 0.446, 3232/10016 datapoints
2025-03-06 19:30:43,369 - INFO - validation batch 151, loss: 0.362, 4832/10016 datapoints
2025-03-06 19:30:43,523 - INFO - validation batch 201, loss: 0.496, 6432/10016 datapoints
2025-03-06 19:30:43,680 - INFO - validation batch 251, loss: 0.246, 8032/10016 datapoints
2025-03-06 19:30:43,832 - INFO - validation batch 301, loss: 0.222, 9632/10016 datapoints
2025-03-06 19:30:43,870 - INFO - Epoch 355/800 done.
2025-03-06 19:30:43,870 - INFO - Final validation performance:
Loss: 0.307, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:30:43,870 - INFO - Beginning epoch 356/800
2025-03-06 19:30:43,877 - INFO - training batch 1, loss: 0.244, 32/60000 datapoints
2025-03-06 19:30:44,070 - INFO - training batch 51, loss: 0.379, 1632/60000 datapoints
2025-03-06 19:30:44,265 - INFO - training batch 101, loss: 0.281, 3232/60000 datapoints
2025-03-06 19:30:44,467 - INFO - training batch 151, loss: 0.178, 4832/60000 datapoints
2025-03-06 19:30:44,671 - INFO - training batch 201, loss: 0.271, 6432/60000 datapoints
2025-03-06 19:30:44,870 - INFO - training batch 251, loss: 0.199, 8032/60000 datapoints
2025-03-06 19:30:45,075 - INFO - training batch 301, loss: 0.072, 9632/60000 datapoints
2025-03-06 19:30:45,271 - INFO - training batch 351, loss: 0.599, 11232/60000 datapoints
2025-03-06 19:30:45,465 - INFO - training batch 401, loss: 0.280, 12832/60000 datapoints
2025-03-06 19:30:45,659 - INFO - training batch 451, loss: 0.275, 14432/60000 datapoints
2025-03-06 19:30:45,853 - INFO - training batch 501, loss: 0.222, 16032/60000 datapoints
2025-03-06 19:30:46,045 - INFO - training batch 551, loss: 0.287, 17632/60000 datapoints
2025-03-06 19:30:46,239 - INFO - training batch 601, loss: 0.143, 19232/60000 datapoints
2025-03-06 19:30:46,430 - INFO - training batch 651, loss: 0.149, 20832/60000 datapoints
2025-03-06 19:30:46,645 - INFO - training batch 701, loss: 0.359, 22432/60000 datapoints
2025-03-06 19:30:46,841 - INFO - training batch 751, loss: 0.146, 24032/60000 datapoints
2025-03-06 19:30:47,032 - INFO - training batch 801, loss: 0.181, 25632/60000 datapoints
2025-03-06 19:30:47,225 - INFO - training batch 851, loss: 0.120, 27232/60000 datapoints
2025-03-06 19:30:47,416 - INFO - training batch 901, loss: 0.254, 28832/60000 datapoints
2025-03-06 19:30:47,609 - INFO - training batch 951, loss: 0.446, 30432/60000 datapoints
2025-03-06 19:30:47,804 - INFO - training batch 1001, loss: 0.348, 32032/60000 datapoints
2025-03-06 19:30:48,001 - INFO - training batch 1051, loss: 0.410, 33632/60000 datapoints
2025-03-06 19:30:48,203 - INFO - training batch 1101, loss: 0.435, 35232/60000 datapoints
2025-03-06 19:30:48,397 - INFO - training batch 1151, loss: 0.147, 36832/60000 datapoints
2025-03-06 19:30:48,593 - INFO - training batch 1201, loss: 0.429, 38432/60000 datapoints
2025-03-06 19:30:48,787 - INFO - training batch 1251, loss: 0.209, 40032/60000 datapoints
2025-03-06 19:30:48,981 - INFO - training batch 1301, loss: 0.139, 41632/60000 datapoints
2025-03-06 19:30:49,175 - INFO - training batch 1351, loss: 0.247, 43232/60000 datapoints
2025-03-06 19:30:49,367 - INFO - training batch 1401, loss: 0.189, 44832/60000 datapoints
2025-03-06 19:30:49,560 - INFO - training batch 1451, loss: 0.406, 46432/60000 datapoints
2025-03-06 19:30:49,757 - INFO - training batch 1501, loss: 0.460, 48032/60000 datapoints
2025-03-06 19:30:49,950 - INFO - training batch 1551, loss: 0.470, 49632/60000 datapoints
2025-03-06 19:30:50,144 - INFO - training batch 1601, loss: 0.215, 51232/60000 datapoints
2025-03-06 19:30:50,335 - INFO - training batch 1651, loss: 0.262, 52832/60000 datapoints
2025-03-06 19:30:50,529 - INFO - training batch 1701, loss: 0.202, 54432/60000 datapoints
2025-03-06 19:30:50,727 - INFO - training batch 1751, loss: 0.262, 56032/60000 datapoints
2025-03-06 19:30:50,921 - INFO - training batch 1801, loss: 0.317, 57632/60000 datapoints
2025-03-06 19:30:51,111 - INFO - training batch 1851, loss: 0.260, 59232/60000 datapoints
2025-03-06 19:30:51,210 - INFO - validation batch 1, loss: 0.334, 32/10016 datapoints
2025-03-06 19:30:51,360 - INFO - validation batch 51, loss: 0.328, 1632/10016 datapoints
2025-03-06 19:30:51,512 - INFO - validation batch 101, loss: 0.502, 3232/10016 datapoints
2025-03-06 19:30:51,664 - INFO - validation batch 151, loss: 0.417, 4832/10016 datapoints
2025-03-06 19:30:51,816 - INFO - validation batch 201, loss: 0.100, 6432/10016 datapoints
2025-03-06 19:30:51,967 - INFO - validation batch 251, loss: 0.176, 8032/10016 datapoints
2025-03-06 19:30:52,117 - INFO - validation batch 301, loss: 0.377, 9632/10016 datapoints
2025-03-06 19:30:52,152 - INFO - Epoch 356/800 done.
2025-03-06 19:30:52,153 - INFO - Final validation performance:
Loss: 0.319, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:30:52,153 - INFO - Beginning epoch 357/800
2025-03-06 19:30:52,160 - INFO - training batch 1, loss: 0.173, 32/60000 datapoints
2025-03-06 19:30:52,367 - INFO - training batch 51, loss: 0.449, 1632/60000 datapoints
2025-03-06 19:30:52,560 - INFO - training batch 101, loss: 0.196, 3232/60000 datapoints
2025-03-06 19:30:52,763 - INFO - training batch 151, loss: 0.291, 4832/60000 datapoints
2025-03-06 19:30:52,956 - INFO - training batch 201, loss: 0.047, 6432/60000 datapoints
2025-03-06 19:30:53,151 - INFO - training batch 251, loss: 0.238, 8032/60000 datapoints
2025-03-06 19:30:53,347 - INFO - training batch 301, loss: 0.519, 9632/60000 datapoints
2025-03-06 19:30:53,539 - INFO - training batch 351, loss: 0.235, 11232/60000 datapoints
2025-03-06 19:30:53,734 - INFO - training batch 401, loss: 0.242, 12832/60000 datapoints
2025-03-06 19:30:53,927 - INFO - training batch 451, loss: 0.206, 14432/60000 datapoints
2025-03-06 19:30:54,119 - INFO - training batch 501, loss: 0.546, 16032/60000 datapoints
2025-03-06 19:30:54,310 - INFO - training batch 551, loss: 0.193, 17632/60000 datapoints
2025-03-06 19:30:54,504 - INFO - training batch 601, loss: 0.289, 19232/60000 datapoints
2025-03-06 19:30:54,705 - INFO - training batch 651, loss: 0.174, 20832/60000 datapoints
2025-03-06 19:30:54,902 - INFO - training batch 701, loss: 0.437, 22432/60000 datapoints
2025-03-06 19:30:55,095 - INFO - training batch 751, loss: 0.427, 24032/60000 datapoints
2025-03-06 19:30:55,287 - INFO - training batch 801, loss: 0.292, 25632/60000 datapoints
2025-03-06 19:30:55,480 - INFO - training batch 851, loss: 0.557, 27232/60000 datapoints
2025-03-06 19:30:55,673 - INFO - training batch 901, loss: 0.797, 28832/60000 datapoints
2025-03-06 19:30:55,881 - INFO - training batch 951, loss: 0.213, 30432/60000 datapoints
2025-03-06 19:30:56,074 - INFO - training batch 1001, loss: 0.212, 32032/60000 datapoints
2025-03-06 19:30:56,267 - INFO - training batch 1051, loss: 0.284, 33632/60000 datapoints
2025-03-06 19:30:56,462 - INFO - training batch 1101, loss: 0.237, 35232/60000 datapoints
2025-03-06 19:30:56,665 - INFO - training batch 1151, loss: 0.425, 36832/60000 datapoints
2025-03-06 19:30:56,872 - INFO - training batch 1201, loss: 0.156, 38432/60000 datapoints
2025-03-06 19:30:57,062 - INFO - training batch 1251, loss: 0.570, 40032/60000 datapoints
2025-03-06 19:30:57,262 - INFO - training batch 1301, loss: 0.253, 41632/60000 datapoints
2025-03-06 19:30:57,465 - INFO - training batch 1351, loss: 0.374, 43232/60000 datapoints
2025-03-06 19:30:57,661 - INFO - training batch 1401, loss: 0.227, 44832/60000 datapoints
2025-03-06 19:30:57,857 - INFO - training batch 1451, loss: 0.343, 46432/60000 datapoints
2025-03-06 19:30:58,052 - INFO - training batch 1501, loss: 0.273, 48032/60000 datapoints
2025-03-06 19:30:58,251 - INFO - training batch 1551, loss: 0.096, 49632/60000 datapoints
2025-03-06 19:30:58,449 - INFO - training batch 1601, loss: 0.291, 51232/60000 datapoints
2025-03-06 19:30:58,654 - INFO - training batch 1651, loss: 0.138, 52832/60000 datapoints
2025-03-06 19:30:58,854 - INFO - training batch 1701, loss: 0.258, 54432/60000 datapoints
2025-03-06 19:30:59,053 - INFO - training batch 1751, loss: 0.168, 56032/60000 datapoints
2025-03-06 19:30:59,248 - INFO - training batch 1801, loss: 0.181, 57632/60000 datapoints
2025-03-06 19:30:59,445 - INFO - training batch 1851, loss: 0.574, 59232/60000 datapoints
2025-03-06 19:30:59,546 - INFO - validation batch 1, loss: 0.314, 32/10016 datapoints
2025-03-06 19:30:59,701 - INFO - validation batch 51, loss: 0.380, 1632/10016 datapoints
2025-03-06 19:30:59,853 - INFO - validation batch 101, loss: 0.217, 3232/10016 datapoints
2025-03-06 19:31:00,010 - INFO - validation batch 151, loss: 0.125, 4832/10016 datapoints
2025-03-06 19:31:00,167 - INFO - validation batch 201, loss: 0.225, 6432/10016 datapoints
2025-03-06 19:31:00,319 - INFO - validation batch 251, loss: 0.336, 8032/10016 datapoints
2025-03-06 19:31:00,476 - INFO - validation batch 301, loss: 0.326, 9632/10016 datapoints
2025-03-06 19:31:00,513 - INFO - Epoch 357/800 done.
2025-03-06 19:31:00,513 - INFO - Final validation performance:
Loss: 0.275, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:31:00,514 - INFO - Beginning epoch 358/800
2025-03-06 19:31:00,521 - INFO - training batch 1, loss: 0.445, 32/60000 datapoints
2025-03-06 19:31:00,729 - INFO - training batch 51, loss: 0.133, 1632/60000 datapoints
2025-03-06 19:31:00,929 - INFO - training batch 101, loss: 0.625, 3232/60000 datapoints
2025-03-06 19:31:01,121 - INFO - training batch 151, loss: 0.332, 4832/60000 datapoints
2025-03-06 19:31:01,321 - INFO - training batch 201, loss: 0.142, 6432/60000 datapoints
2025-03-06 19:31:01,522 - INFO - training batch 251, loss: 0.245, 8032/60000 datapoints
2025-03-06 19:31:01,720 - INFO - training batch 301, loss: 0.399, 9632/60000 datapoints
2025-03-06 19:31:01,916 - INFO - training batch 351, loss: 0.212, 11232/60000 datapoints
2025-03-06 19:31:02,108 - INFO - training batch 401, loss: 0.379, 12832/60000 datapoints
2025-03-06 19:31:02,302 - INFO - training batch 451, loss: 0.164, 14432/60000 datapoints
2025-03-06 19:31:02,498 - INFO - training batch 501, loss: 0.199, 16032/60000 datapoints
2025-03-06 19:31:02,695 - INFO - training batch 551, loss: 0.291, 17632/60000 datapoints
2025-03-06 19:31:02,887 - INFO - training batch 601, loss: 0.171, 19232/60000 datapoints
2025-03-06 19:31:03,079 - INFO - training batch 651, loss: 0.191, 20832/60000 datapoints
2025-03-06 19:31:03,281 - INFO - training batch 701, loss: 0.168, 22432/60000 datapoints
2025-03-06 19:31:03,482 - INFO - training batch 751, loss: 0.092, 24032/60000 datapoints
2025-03-06 19:31:03,676 - INFO - training batch 801, loss: 0.087, 25632/60000 datapoints
2025-03-06 19:31:03,869 - INFO - training batch 851, loss: 0.318, 27232/60000 datapoints
2025-03-06 19:31:04,059 - INFO - training batch 901, loss: 0.218, 28832/60000 datapoints
2025-03-06 19:31:04,249 - INFO - training batch 951, loss: 0.279, 30432/60000 datapoints
2025-03-06 19:31:04,440 - INFO - training batch 1001, loss: 0.502, 32032/60000 datapoints
2025-03-06 19:31:04,637 - INFO - training batch 1051, loss: 0.422, 33632/60000 datapoints
2025-03-06 19:31:04,831 - INFO - training batch 1101, loss: 0.387, 35232/60000 datapoints
2025-03-06 19:31:05,029 - INFO - training batch 1151, loss: 0.172, 36832/60000 datapoints
2025-03-06 19:31:05,223 - INFO - training batch 1201, loss: 0.294, 38432/60000 datapoints
2025-03-06 19:31:05,413 - INFO - training batch 1251, loss: 0.095, 40032/60000 datapoints
2025-03-06 19:31:05,607 - INFO - training batch 1301, loss: 0.459, 41632/60000 datapoints
2025-03-06 19:31:05,803 - INFO - training batch 1351, loss: 0.436, 43232/60000 datapoints
2025-03-06 19:31:05,997 - INFO - training batch 1401, loss: 0.358, 44832/60000 datapoints
2025-03-06 19:31:06,190 - INFO - training batch 1451, loss: 0.574, 46432/60000 datapoints
2025-03-06 19:31:06,382 - INFO - training batch 1501, loss: 0.263, 48032/60000 datapoints
2025-03-06 19:31:06,575 - INFO - training batch 1551, loss: 0.243, 49632/60000 datapoints
2025-03-06 19:31:06,794 - INFO - training batch 1601, loss: 0.399, 51232/60000 datapoints
2025-03-06 19:31:06,988 - INFO - training batch 1651, loss: 0.277, 52832/60000 datapoints
2025-03-06 19:31:07,181 - INFO - training batch 1701, loss: 0.369, 54432/60000 datapoints
2025-03-06 19:31:07,374 - INFO - training batch 1751, loss: 0.375, 56032/60000 datapoints
2025-03-06 19:31:07,567 - INFO - training batch 1801, loss: 0.191, 57632/60000 datapoints
2025-03-06 19:31:07,764 - INFO - training batch 1851, loss: 0.221, 59232/60000 datapoints
2025-03-06 19:31:07,863 - INFO - validation batch 1, loss: 0.510, 32/10016 datapoints
2025-03-06 19:31:08,016 - INFO - validation batch 51, loss: 0.253, 1632/10016 datapoints
2025-03-06 19:31:08,165 - INFO - validation batch 101, loss: 0.251, 3232/10016 datapoints
2025-03-06 19:31:08,316 - INFO - validation batch 151, loss: 0.278, 4832/10016 datapoints
2025-03-06 19:31:08,467 - INFO - validation batch 201, loss: 0.400, 6432/10016 datapoints
2025-03-06 19:31:08,620 - INFO - validation batch 251, loss: 0.109, 8032/10016 datapoints
2025-03-06 19:31:08,773 - INFO - validation batch 301, loss: 0.344, 9632/10016 datapoints
2025-03-06 19:31:08,808 - INFO - Epoch 358/800 done.
2025-03-06 19:31:08,808 - INFO - Final validation performance:
Loss: 0.306, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:31:08,809 - INFO - Beginning epoch 359/800
2025-03-06 19:31:08,815 - INFO - training batch 1, loss: 0.760, 32/60000 datapoints
2025-03-06 19:31:09,026 - INFO - training batch 51, loss: 0.504, 1632/60000 datapoints
2025-03-06 19:31:09,219 - INFO - training batch 101, loss: 0.240, 3232/60000 datapoints
2025-03-06 19:31:09,419 - INFO - training batch 151, loss: 0.124, 4832/60000 datapoints
2025-03-06 19:31:09,620 - INFO - training batch 201, loss: 0.173, 6432/60000 datapoints
2025-03-06 19:31:09,816 - INFO - training batch 251, loss: 0.257, 8032/60000 datapoints
2025-03-06 19:31:10,010 - INFO - training batch 301, loss: 0.131, 9632/60000 datapoints
2025-03-06 19:31:10,203 - INFO - training batch 351, loss: 0.325, 11232/60000 datapoints
2025-03-06 19:31:10,396 - INFO - training batch 401, loss: 0.237, 12832/60000 datapoints
2025-03-06 19:31:10,591 - INFO - training batch 451, loss: 0.693, 14432/60000 datapoints
2025-03-06 19:31:10,792 - INFO - training batch 501, loss: 0.238, 16032/60000 datapoints
2025-03-06 19:31:10,983 - INFO - training batch 551, loss: 0.301, 17632/60000 datapoints
2025-03-06 19:31:11,174 - INFO - training batch 601, loss: 0.097, 19232/60000 datapoints
2025-03-06 19:31:11,367 - INFO - training batch 651, loss: 0.367, 20832/60000 datapoints
2025-03-06 19:31:11,567 - INFO - training batch 701, loss: 0.114, 22432/60000 datapoints
2025-03-06 19:31:11,760 - INFO - training batch 751, loss: 0.283, 24032/60000 datapoints
2025-03-06 19:31:11,952 - INFO - training batch 801, loss: 0.312, 25632/60000 datapoints
2025-03-06 19:31:12,145 - INFO - training batch 851, loss: 0.224, 27232/60000 datapoints
2025-03-06 19:31:12,335 - INFO - training batch 901, loss: 0.447, 28832/60000 datapoints
2025-03-06 19:31:12,531 - INFO - training batch 951, loss: 0.294, 30432/60000 datapoints
2025-03-06 19:31:12,750 - INFO - training batch 1001, loss: 0.388, 32032/60000 datapoints
2025-03-06 19:31:12,944 - INFO - training batch 1051, loss: 0.132, 33632/60000 datapoints
2025-03-06 19:31:13,135 - INFO - training batch 1101, loss: 0.722, 35232/60000 datapoints
2025-03-06 19:31:13,326 - INFO - training batch 1151, loss: 0.184, 36832/60000 datapoints
2025-03-06 19:31:13,519 - INFO - training batch 1201, loss: 0.399, 38432/60000 datapoints
2025-03-06 19:31:13,714 - INFO - training batch 1251, loss: 0.187, 40032/60000 datapoints
2025-03-06 19:31:13,907 - INFO - training batch 1301, loss: 0.391, 41632/60000 datapoints
2025-03-06 19:31:14,099 - INFO - training batch 1351, loss: 0.168, 43232/60000 datapoints
2025-03-06 19:31:14,291 - INFO - training batch 1401, loss: 0.109, 44832/60000 datapoints
2025-03-06 19:31:14,486 - INFO - training batch 1451, loss: 0.353, 46432/60000 datapoints
2025-03-06 19:31:14,686 - INFO - training batch 1501, loss: 0.233, 48032/60000 datapoints
2025-03-06 19:31:14,882 - INFO - training batch 1551, loss: 0.396, 49632/60000 datapoints
2025-03-06 19:31:15,078 - INFO - training batch 1601, loss: 0.264, 51232/60000 datapoints
2025-03-06 19:31:15,274 - INFO - training batch 1651, loss: 0.186, 52832/60000 datapoints
2025-03-06 19:31:15,467 - INFO - training batch 1701, loss: 0.503, 54432/60000 datapoints
2025-03-06 19:31:15,663 - INFO - training batch 1751, loss: 0.152, 56032/60000 datapoints
2025-03-06 19:31:15,854 - INFO - training batch 1801, loss: 0.393, 57632/60000 datapoints
2025-03-06 19:31:16,046 - INFO - training batch 1851, loss: 0.468, 59232/60000 datapoints
2025-03-06 19:31:16,144 - INFO - validation batch 1, loss: 0.110, 32/10016 datapoints
2025-03-06 19:31:16,295 - INFO - validation batch 51, loss: 0.075, 1632/10016 datapoints
2025-03-06 19:31:16,447 - INFO - validation batch 101, loss: 0.283, 3232/10016 datapoints
2025-03-06 19:31:16,601 - INFO - validation batch 151, loss: 0.137, 4832/10016 datapoints
2025-03-06 19:31:16,756 - INFO - validation batch 201, loss: 0.328, 6432/10016 datapoints
2025-03-06 19:31:16,925 - INFO - validation batch 251, loss: 0.125, 8032/10016 datapoints
2025-03-06 19:31:17,077 - INFO - validation batch 301, loss: 0.271, 9632/10016 datapoints
2025-03-06 19:31:17,114 - INFO - Epoch 359/800 done.
2025-03-06 19:31:17,115 - INFO - Final validation performance:
Loss: 0.190, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:31:17,115 - INFO - Beginning epoch 360/800
2025-03-06 19:31:17,121 - INFO - training batch 1, loss: 0.727, 32/60000 datapoints
2025-03-06 19:31:17,321 - INFO - training batch 51, loss: 0.187, 1632/60000 datapoints
2025-03-06 19:31:17,525 - INFO - training batch 101, loss: 0.278, 3232/60000 datapoints
2025-03-06 19:31:17,763 - INFO - training batch 151, loss: 0.194, 4832/60000 datapoints
2025-03-06 19:31:17,985 - INFO - training batch 201, loss: 0.352, 6432/60000 datapoints
2025-03-06 19:31:18,189 - INFO - training batch 251, loss: 0.395, 8032/60000 datapoints
2025-03-06 19:31:18,388 - INFO - training batch 301, loss: 0.181, 9632/60000 datapoints
2025-03-06 19:31:18,590 - INFO - training batch 351, loss: 0.290, 11232/60000 datapoints
2025-03-06 19:31:18,788 - INFO - training batch 401, loss: 0.250, 12832/60000 datapoints
2025-03-06 19:31:18,983 - INFO - training batch 451, loss: 0.233, 14432/60000 datapoints
2025-03-06 19:31:19,178 - INFO - training batch 501, loss: 0.419, 16032/60000 datapoints
2025-03-06 19:31:19,375 - INFO - training batch 551, loss: 0.524, 17632/60000 datapoints
2025-03-06 19:31:19,569 - INFO - training batch 601, loss: 0.451, 19232/60000 datapoints
2025-03-06 19:31:19,768 - INFO - training batch 651, loss: 0.500, 20832/60000 datapoints
2025-03-06 19:31:19,965 - INFO - training batch 701, loss: 0.554, 22432/60000 datapoints
2025-03-06 19:31:20,162 - INFO - training batch 751, loss: 0.220, 24032/60000 datapoints
2025-03-06 19:31:20,356 - INFO - training batch 801, loss: 0.881, 25632/60000 datapoints
2025-03-06 19:31:20,553 - INFO - training batch 851, loss: 0.459, 27232/60000 datapoints
2025-03-06 19:31:20,757 - INFO - training batch 901, loss: 0.848, 28832/60000 datapoints
2025-03-06 19:31:20,952 - INFO - training batch 951, loss: 0.071, 30432/60000 datapoints
2025-03-06 19:31:21,146 - INFO - training batch 1001, loss: 0.147, 32032/60000 datapoints
2025-03-06 19:31:21,341 - INFO - training batch 1051, loss: 0.618, 33632/60000 datapoints
2025-03-06 19:31:21,535 - INFO - training batch 1101, loss: 0.292, 35232/60000 datapoints
2025-03-06 19:31:21,734 - INFO - training batch 1151, loss: 0.284, 36832/60000 datapoints
2025-03-06 19:31:21,929 - INFO - training batch 1201, loss: 0.462, 38432/60000 datapoints
2025-03-06 19:31:22,124 - INFO - training batch 1251, loss: 0.121, 40032/60000 datapoints
2025-03-06 19:31:22,323 - INFO - training batch 1301, loss: 0.525, 41632/60000 datapoints
2025-03-06 19:31:22,521 - INFO - training batch 1351, loss: 0.689, 43232/60000 datapoints
2025-03-06 19:31:22,720 - INFO - training batch 1401, loss: 0.385, 44832/60000 datapoints
2025-03-06 19:31:22,918 - INFO - training batch 1451, loss: 0.137, 46432/60000 datapoints
2025-03-06 19:31:23,110 - INFO - training batch 1501, loss: 0.596, 48032/60000 datapoints
2025-03-06 19:31:23,308 - INFO - training batch 1551, loss: 0.283, 49632/60000 datapoints
2025-03-06 19:31:23,501 - INFO - training batch 1601, loss: 0.222, 51232/60000 datapoints
2025-03-06 19:31:23,700 - INFO - training batch 1651, loss: 0.193, 52832/60000 datapoints
2025-03-06 19:31:23,893 - INFO - training batch 1701, loss: 0.218, 54432/60000 datapoints
2025-03-06 19:31:24,087 - INFO - training batch 1751, loss: 0.415, 56032/60000 datapoints
2025-03-06 19:31:24,281 - INFO - training batch 1801, loss: 0.309, 57632/60000 datapoints
2025-03-06 19:31:24,475 - INFO - training batch 1851, loss: 0.103, 59232/60000 datapoints
2025-03-06 19:31:24,576 - INFO - validation batch 1, loss: 0.285, 32/10016 datapoints
2025-03-06 19:31:24,735 - INFO - validation batch 51, loss: 0.286, 1632/10016 datapoints
2025-03-06 19:31:24,889 - INFO - validation batch 101, loss: 0.351, 3232/10016 datapoints
2025-03-06 19:31:25,044 - INFO - validation batch 151, loss: 0.352, 4832/10016 datapoints
2025-03-06 19:31:25,200 - INFO - validation batch 201, loss: 0.193, 6432/10016 datapoints
2025-03-06 19:31:25,351 - INFO - validation batch 251, loss: 0.075, 8032/10016 datapoints
2025-03-06 19:31:25,506 - INFO - validation batch 301, loss: 0.344, 9632/10016 datapoints
2025-03-06 19:31:25,543 - INFO - Epoch 360/800 done.
2025-03-06 19:31:25,543 - INFO - Final validation performance:
Loss: 0.269, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:31:25,544 - INFO - Beginning epoch 361/800
2025-03-06 19:31:25,550 - INFO - training batch 1, loss: 0.246, 32/60000 datapoints
2025-03-06 19:31:25,762 - INFO - training batch 51, loss: 0.093, 1632/60000 datapoints
2025-03-06 19:31:25,957 - INFO - training batch 101, loss: 0.588, 3232/60000 datapoints
2025-03-06 19:31:26,159 - INFO - training batch 151, loss: 0.504, 4832/60000 datapoints
2025-03-06 19:31:26,358 - INFO - training batch 201, loss: 0.242, 6432/60000 datapoints
2025-03-06 19:31:26,561 - INFO - training batch 251, loss: 0.232, 8032/60000 datapoints
2025-03-06 19:31:26,766 - INFO - training batch 301, loss: 0.178, 9632/60000 datapoints
2025-03-06 19:31:26,982 - INFO - training batch 351, loss: 0.317, 11232/60000 datapoints
2025-03-06 19:31:27,180 - INFO - training batch 401, loss: 0.451, 12832/60000 datapoints
2025-03-06 19:31:27,375 - INFO - training batch 451, loss: 0.146, 14432/60000 datapoints
2025-03-06 19:31:27,569 - INFO - training batch 501, loss: 0.195, 16032/60000 datapoints
2025-03-06 19:31:27,768 - INFO - training batch 551, loss: 0.324, 17632/60000 datapoints
2025-03-06 19:31:27,965 - INFO - training batch 601, loss: 0.302, 19232/60000 datapoints
2025-03-06 19:31:28,161 - INFO - training batch 651, loss: 0.240, 20832/60000 datapoints
2025-03-06 19:31:28,359 - INFO - training batch 701, loss: 0.169, 22432/60000 datapoints
2025-03-06 19:31:28,553 - INFO - training batch 751, loss: 0.334, 24032/60000 datapoints
2025-03-06 19:31:28,754 - INFO - training batch 801, loss: 0.337, 25632/60000 datapoints
2025-03-06 19:31:28,956 - INFO - training batch 851, loss: 0.319, 27232/60000 datapoints
2025-03-06 19:31:29,149 - INFO - training batch 901, loss: 0.434, 28832/60000 datapoints
2025-03-06 19:31:29,343 - INFO - training batch 951, loss: 0.120, 30432/60000 datapoints
2025-03-06 19:31:29,540 - INFO - training batch 1001, loss: 0.261, 32032/60000 datapoints
2025-03-06 19:31:29,743 - INFO - training batch 1051, loss: 0.418, 33632/60000 datapoints
2025-03-06 19:31:29,941 - INFO - training batch 1101, loss: 0.109, 35232/60000 datapoints
2025-03-06 19:31:30,137 - INFO - training batch 1151, loss: 0.241, 36832/60000 datapoints
2025-03-06 19:31:30,335 - INFO - training batch 1201, loss: 0.346, 38432/60000 datapoints
2025-03-06 19:31:30,528 - INFO - training batch 1251, loss: 0.156, 40032/60000 datapoints
2025-03-06 19:31:30,726 - INFO - training batch 1301, loss: 0.214, 41632/60000 datapoints
2025-03-06 19:31:30,926 - INFO - training batch 1351, loss: 0.318, 43232/60000 datapoints
2025-03-06 19:31:31,124 - INFO - training batch 1401, loss: 0.309, 44832/60000 datapoints
2025-03-06 19:31:31,318 - INFO - training batch 1451, loss: 0.242, 46432/60000 datapoints
2025-03-06 19:31:31,514 - INFO - training batch 1501, loss: 0.221, 48032/60000 datapoints
2025-03-06 19:31:31,713 - INFO - training batch 1551, loss: 0.120, 49632/60000 datapoints
2025-03-06 19:31:31,910 - INFO - training batch 1601, loss: 0.484, 51232/60000 datapoints
2025-03-06 19:31:32,105 - INFO - training batch 1651, loss: 0.305, 52832/60000 datapoints
2025-03-06 19:31:32,303 - INFO - training batch 1701, loss: 0.100, 54432/60000 datapoints
2025-03-06 19:31:32,496 - INFO - training batch 1751, loss: 0.629, 56032/60000 datapoints
2025-03-06 19:31:32,694 - INFO - training batch 1801, loss: 0.165, 57632/60000 datapoints
2025-03-06 19:31:32,894 - INFO - training batch 1851, loss: 0.186, 59232/60000 datapoints
2025-03-06 19:31:32,996 - INFO - validation batch 1, loss: 0.405, 32/10016 datapoints
2025-03-06 19:31:33,148 - INFO - validation batch 51, loss: 0.158, 1632/10016 datapoints
2025-03-06 19:31:33,301 - INFO - validation batch 101, loss: 0.165, 3232/10016 datapoints
2025-03-06 19:31:33,453 - INFO - validation batch 151, loss: 0.164, 4832/10016 datapoints
2025-03-06 19:31:33,610 - INFO - validation batch 201, loss: 0.206, 6432/10016 datapoints
2025-03-06 19:31:33,765 - INFO - validation batch 251, loss: 0.150, 8032/10016 datapoints
2025-03-06 19:31:33,920 - INFO - validation batch 301, loss: 0.292, 9632/10016 datapoints
2025-03-06 19:31:33,955 - INFO - Epoch 361/800 done.
2025-03-06 19:31:33,956 - INFO - Final validation performance:
Loss: 0.220, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:31:33,956 - INFO - Beginning epoch 362/800
2025-03-06 19:31:33,964 - INFO - training batch 1, loss: 0.347, 32/60000 datapoints
2025-03-06 19:31:34,171 - INFO - training batch 51, loss: 0.279, 1632/60000 datapoints
2025-03-06 19:31:34,375 - INFO - training batch 101, loss: 0.291, 3232/60000 datapoints
2025-03-06 19:31:34,583 - INFO - training batch 151, loss: 0.308, 4832/60000 datapoints
2025-03-06 19:31:34,790 - INFO - training batch 201, loss: 0.425, 6432/60000 datapoints
2025-03-06 19:31:34,998 - INFO - training batch 251, loss: 0.468, 8032/60000 datapoints
2025-03-06 19:31:35,203 - INFO - training batch 301, loss: 0.427, 9632/60000 datapoints
2025-03-06 19:31:35,420 - INFO - training batch 351, loss: 0.272, 11232/60000 datapoints
2025-03-06 19:31:35,640 - INFO - training batch 401, loss: 0.206, 12832/60000 datapoints
2025-03-06 19:31:35,862 - INFO - training batch 451, loss: 0.233, 14432/60000 datapoints
2025-03-06 19:31:36,068 - INFO - training batch 501, loss: 0.520, 16032/60000 datapoints
2025-03-06 19:31:36,282 - INFO - training batch 551, loss: 0.183, 17632/60000 datapoints
2025-03-06 19:31:36,478 - INFO - training batch 601, loss: 0.411, 19232/60000 datapoints
2025-03-06 19:31:36,673 - INFO - training batch 651, loss: 0.156, 20832/60000 datapoints
2025-03-06 19:31:36,875 - INFO - training batch 701, loss: 0.351, 22432/60000 datapoints
2025-03-06 19:31:37,091 - INFO - training batch 751, loss: 0.140, 24032/60000 datapoints
2025-03-06 19:31:37,289 - INFO - training batch 801, loss: 0.414, 25632/60000 datapoints
2025-03-06 19:31:37,494 - INFO - training batch 851, loss: 0.313, 27232/60000 datapoints
2025-03-06 19:31:37,709 - INFO - training batch 901, loss: 0.435, 28832/60000 datapoints
2025-03-06 19:31:37,909 - INFO - training batch 951, loss: 0.614, 30432/60000 datapoints
2025-03-06 19:31:38,105 - INFO - training batch 1001, loss: 0.290, 32032/60000 datapoints
2025-03-06 19:31:38,300 - INFO - training batch 1051, loss: 0.149, 33632/60000 datapoints
2025-03-06 19:31:38,497 - INFO - training batch 1101, loss: 0.326, 35232/60000 datapoints
2025-03-06 19:31:38,697 - INFO - training batch 1151, loss: 0.348, 36832/60000 datapoints
2025-03-06 19:31:38,896 - INFO - training batch 1201, loss: 0.337, 38432/60000 datapoints
2025-03-06 19:31:39,106 - INFO - training batch 1251, loss: 0.262, 40032/60000 datapoints
2025-03-06 19:31:39,302 - INFO - training batch 1301, loss: 0.329, 41632/60000 datapoints
2025-03-06 19:31:39,500 - INFO - training batch 1351, loss: 0.153, 43232/60000 datapoints
2025-03-06 19:31:39,697 - INFO - training batch 1401, loss: 0.335, 44832/60000 datapoints
2025-03-06 19:31:39,892 - INFO - training batch 1451, loss: 0.345, 46432/60000 datapoints
2025-03-06 19:31:40,088 - INFO - training batch 1501, loss: 0.292, 48032/60000 datapoints
2025-03-06 19:31:40,283 - INFO - training batch 1551, loss: 0.414, 49632/60000 datapoints
2025-03-06 19:31:40,479 - INFO - training batch 1601, loss: 0.227, 51232/60000 datapoints
2025-03-06 19:31:40,677 - INFO - training batch 1651, loss: 0.589, 52832/60000 datapoints
2025-03-06 19:31:40,877 - INFO - training batch 1701, loss: 0.182, 54432/60000 datapoints
2025-03-06 19:31:41,070 - INFO - training batch 1751, loss: 0.223, 56032/60000 datapoints
2025-03-06 19:31:41,267 - INFO - training batch 1801, loss: 0.402, 57632/60000 datapoints
2025-03-06 19:31:41,465 - INFO - training batch 1851, loss: 0.213, 59232/60000 datapoints
2025-03-06 19:31:41,569 - INFO - validation batch 1, loss: 0.325, 32/10016 datapoints
2025-03-06 19:31:41,726 - INFO - validation batch 51, loss: 0.531, 1632/10016 datapoints
2025-03-06 19:31:41,882 - INFO - validation batch 101, loss: 0.147, 3232/10016 datapoints
2025-03-06 19:31:42,034 - INFO - validation batch 151, loss: 0.084, 4832/10016 datapoints
2025-03-06 19:31:42,188 - INFO - validation batch 201, loss: 0.271, 6432/10016 datapoints
2025-03-06 19:31:42,341 - INFO - validation batch 251, loss: 0.454, 8032/10016 datapoints
2025-03-06 19:31:42,494 - INFO - validation batch 301, loss: 0.098, 9632/10016 datapoints
2025-03-06 19:31:42,531 - INFO - Epoch 362/800 done.
2025-03-06 19:31:42,531 - INFO - Final validation performance:
Loss: 0.273, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:31:42,532 - INFO - Beginning epoch 363/800
2025-03-06 19:31:42,538 - INFO - training batch 1, loss: 0.074, 32/60000 datapoints
2025-03-06 19:31:42,736 - INFO - training batch 51, loss: 0.512, 1632/60000 datapoints
2025-03-06 19:31:42,937 - INFO - training batch 101, loss: 0.209, 3232/60000 datapoints
2025-03-06 19:31:43,147 - INFO - training batch 151, loss: 0.206, 4832/60000 datapoints
2025-03-06 19:31:43,344 - INFO - training batch 201, loss: 0.194, 6432/60000 datapoints
2025-03-06 19:31:43,547 - INFO - training batch 251, loss: 0.238, 8032/60000 datapoints
2025-03-06 19:31:43,750 - INFO - training batch 301, loss: 0.342, 9632/60000 datapoints
2025-03-06 19:31:43,949 - INFO - training batch 351, loss: 0.152, 11232/60000 datapoints
2025-03-06 19:31:44,145 - INFO - training batch 401, loss: 0.263, 12832/60000 datapoints
2025-03-06 19:31:44,339 - INFO - training batch 451, loss: 0.527, 14432/60000 datapoints
2025-03-06 19:31:44,536 - INFO - training batch 501, loss: 0.144, 16032/60000 datapoints
2025-03-06 19:31:44,738 - INFO - training batch 551, loss: 0.423, 17632/60000 datapoints
2025-03-06 19:31:44,950 - INFO - training batch 601, loss: 0.335, 19232/60000 datapoints
2025-03-06 19:31:45,160 - INFO - training batch 651, loss: 0.359, 20832/60000 datapoints
2025-03-06 19:31:45,357 - INFO - training batch 701, loss: 0.272, 22432/60000 datapoints
2025-03-06 19:31:45,556 - INFO - training batch 751, loss: 0.104, 24032/60000 datapoints
2025-03-06 19:31:45,754 - INFO - training batch 801, loss: 0.146, 25632/60000 datapoints
2025-03-06 19:31:45,950 - INFO - training batch 851, loss: 0.267, 27232/60000 datapoints
2025-03-06 19:31:46,146 - INFO - training batch 901, loss: 0.192, 28832/60000 datapoints
2025-03-06 19:31:46,341 - INFO - training batch 951, loss: 0.137, 30432/60000 datapoints
2025-03-06 19:31:46,537 - INFO - training batch 1001, loss: 0.168, 32032/60000 datapoints
2025-03-06 19:31:46,733 - INFO - training batch 1051, loss: 0.294, 33632/60000 datapoints
2025-03-06 19:31:46,933 - INFO - training batch 1101, loss: 0.189, 35232/60000 datapoints
2025-03-06 19:31:47,144 - INFO - training batch 1151, loss: 0.142, 36832/60000 datapoints
2025-03-06 19:31:47,337 - INFO - training batch 1201, loss: 0.312, 38432/60000 datapoints
2025-03-06 19:31:47,534 - INFO - training batch 1251, loss: 0.256, 40032/60000 datapoints
2025-03-06 19:31:47,731 - INFO - training batch 1301, loss: 0.484, 41632/60000 datapoints
2025-03-06 19:31:47,926 - INFO - training batch 1351, loss: 0.164, 43232/60000 datapoints
2025-03-06 19:31:48,126 - INFO - training batch 1401, loss: 0.192, 44832/60000 datapoints
2025-03-06 19:31:48,323 - INFO - training batch 1451, loss: 0.094, 46432/60000 datapoints
2025-03-06 19:31:48,520 - INFO - training batch 1501, loss: 0.401, 48032/60000 datapoints
2025-03-06 19:31:48,719 - INFO - training batch 1551, loss: 0.603, 49632/60000 datapoints
2025-03-06 19:31:48,918 - INFO - training batch 1601, loss: 0.306, 51232/60000 datapoints
2025-03-06 19:31:49,115 - INFO - training batch 1651, loss: 0.522, 52832/60000 datapoints
2025-03-06 19:31:49,311 - INFO - training batch 1701, loss: 0.468, 54432/60000 datapoints
2025-03-06 19:31:49,508 - INFO - training batch 1751, loss: 0.317, 56032/60000 datapoints
2025-03-06 19:31:49,704 - INFO - training batch 1801, loss: 0.402, 57632/60000 datapoints
2025-03-06 19:31:49,901 - INFO - training batch 1851, loss: 0.407, 59232/60000 datapoints
2025-03-06 19:31:50,005 - INFO - validation batch 1, loss: 0.324, 32/10016 datapoints
2025-03-06 19:31:50,157 - INFO - validation batch 51, loss: 0.356, 1632/10016 datapoints
2025-03-06 19:31:50,310 - INFO - validation batch 101, loss: 0.189, 3232/10016 datapoints
2025-03-06 19:31:50,463 - INFO - validation batch 151, loss: 0.061, 4832/10016 datapoints
2025-03-06 19:31:50,621 - INFO - validation batch 201, loss: 0.427, 6432/10016 datapoints
2025-03-06 19:31:50,776 - INFO - validation batch 251, loss: 0.356, 8032/10016 datapoints
2025-03-06 19:31:50,933 - INFO - validation batch 301, loss: 0.398, 9632/10016 datapoints
2025-03-06 19:31:50,974 - INFO - Epoch 363/800 done.
2025-03-06 19:31:50,974 - INFO - Final validation performance:
Loss: 0.302, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:31:50,975 - INFO - Beginning epoch 364/800
2025-03-06 19:31:50,981 - INFO - training batch 1, loss: 0.223, 32/60000 datapoints
2025-03-06 19:31:51,190 - INFO - training batch 51, loss: 0.156, 1632/60000 datapoints
2025-03-06 19:31:51,382 - INFO - training batch 101, loss: 0.186, 3232/60000 datapoints
2025-03-06 19:31:51,587 - INFO - training batch 151, loss: 0.244, 4832/60000 datapoints
2025-03-06 19:31:51,785 - INFO - training batch 201, loss: 0.301, 6432/60000 datapoints
2025-03-06 19:31:51,983 - INFO - training batch 251, loss: 0.220, 8032/60000 datapoints
2025-03-06 19:31:52,178 - INFO - training batch 301, loss: 0.362, 9632/60000 datapoints
2025-03-06 19:31:52,371 - INFO - training batch 351, loss: 0.246, 11232/60000 datapoints
2025-03-06 19:31:52,564 - INFO - training batch 401, loss: 0.353, 12832/60000 datapoints
2025-03-06 19:31:52,759 - INFO - training batch 451, loss: 0.194, 14432/60000 datapoints
2025-03-06 19:31:52,959 - INFO - training batch 501, loss: 0.271, 16032/60000 datapoints
2025-03-06 19:31:53,153 - INFO - training batch 551, loss: 0.283, 17632/60000 datapoints
2025-03-06 19:31:53,352 - INFO - training batch 601, loss: 0.179, 19232/60000 datapoints
2025-03-06 19:31:53,549 - INFO - training batch 651, loss: 0.450, 20832/60000 datapoints
2025-03-06 19:31:53,744 - INFO - training batch 701, loss: 0.426, 22432/60000 datapoints
2025-03-06 19:31:53,940 - INFO - training batch 751, loss: 0.266, 24032/60000 datapoints
2025-03-06 19:31:54,136 - INFO - training batch 801, loss: 0.448, 25632/60000 datapoints
2025-03-06 19:31:54,331 - INFO - training batch 851, loss: 0.199, 27232/60000 datapoints
2025-03-06 19:31:54,527 - INFO - training batch 901, loss: 0.331, 28832/60000 datapoints
2025-03-06 19:31:54,723 - INFO - training batch 951, loss: 0.131, 30432/60000 datapoints
2025-03-06 19:31:54,928 - INFO - training batch 1001, loss: 0.371, 32032/60000 datapoints
2025-03-06 19:31:55,124 - INFO - training batch 1051, loss: 0.394, 33632/60000 datapoints
2025-03-06 19:31:55,320 - INFO - training batch 1101, loss: 0.433, 35232/60000 datapoints
2025-03-06 19:31:55,516 - INFO - training batch 1151, loss: 0.123, 36832/60000 datapoints
2025-03-06 19:31:55,715 - INFO - training batch 1201, loss: 0.543, 38432/60000 datapoints
2025-03-06 19:31:55,912 - INFO - training batch 1251, loss: 0.207, 40032/60000 datapoints
2025-03-06 19:31:56,110 - INFO - training batch 1301, loss: 0.394, 41632/60000 datapoints
2025-03-06 19:31:56,304 - INFO - training batch 1351, loss: 0.118, 43232/60000 datapoints
2025-03-06 19:31:56,501 - INFO - training batch 1401, loss: 0.331, 44832/60000 datapoints
2025-03-06 19:31:56,699 - INFO - training batch 1451, loss: 0.144, 46432/60000 datapoints
2025-03-06 19:31:56,900 - INFO - training batch 1501, loss: 0.216, 48032/60000 datapoints
2025-03-06 19:31:57,096 - INFO - training batch 1551, loss: 0.311, 49632/60000 datapoints
2025-03-06 19:31:57,309 - INFO - training batch 1601, loss: 0.164, 51232/60000 datapoints
2025-03-06 19:31:57,505 - INFO - training batch 1651, loss: 0.112, 52832/60000 datapoints
2025-03-06 19:31:57,701 - INFO - training batch 1701, loss: 0.477, 54432/60000 datapoints
2025-03-06 19:31:57,896 - INFO - training batch 1751, loss: 0.209, 56032/60000 datapoints
2025-03-06 19:31:58,091 - INFO - training batch 1801, loss: 0.794, 57632/60000 datapoints
2025-03-06 19:31:58,285 - INFO - training batch 1851, loss: 0.331, 59232/60000 datapoints
2025-03-06 19:31:58,387 - INFO - validation batch 1, loss: 0.645, 32/10016 datapoints
2025-03-06 19:31:58,542 - INFO - validation batch 51, loss: 0.204, 1632/10016 datapoints
2025-03-06 19:31:58,699 - INFO - validation batch 101, loss: 0.228, 3232/10016 datapoints
2025-03-06 19:31:58,852 - INFO - validation batch 151, loss: 0.202, 4832/10016 datapoints
2025-03-06 19:31:59,007 - INFO - validation batch 201, loss: 0.208, 6432/10016 datapoints
2025-03-06 19:31:59,163 - INFO - validation batch 251, loss: 0.173, 8032/10016 datapoints
2025-03-06 19:31:59,317 - INFO - validation batch 301, loss: 0.153, 9632/10016 datapoints
2025-03-06 19:31:59,353 - INFO - Epoch 364/800 done.
2025-03-06 19:31:59,353 - INFO - Final validation performance:
Loss: 0.259, top-1 acc: 0.918top-5 acc: 0.918
2025-03-06 19:31:59,354 - INFO - Beginning epoch 365/800
2025-03-06 19:31:59,362 - INFO - training batch 1, loss: 0.178, 32/60000 datapoints
2025-03-06 19:31:59,576 - INFO - training batch 51, loss: 0.339, 1632/60000 datapoints
2025-03-06 19:31:59,774 - INFO - training batch 101, loss: 0.286, 3232/60000 datapoints
2025-03-06 19:31:59,975 - INFO - training batch 151, loss: 0.526, 4832/60000 datapoints
2025-03-06 19:32:00,177 - INFO - training batch 201, loss: 0.146, 6432/60000 datapoints
2025-03-06 19:32:00,375 - INFO - training batch 251, loss: 0.395, 8032/60000 datapoints
2025-03-06 19:32:00,567 - INFO - training batch 301, loss: 0.418, 9632/60000 datapoints
2025-03-06 19:32:00,764 - INFO - training batch 351, loss: 0.279, 11232/60000 datapoints
2025-03-06 19:32:00,962 - INFO - training batch 401, loss: 0.257, 12832/60000 datapoints
2025-03-06 19:32:01,154 - INFO - training batch 451, loss: 0.484, 14432/60000 datapoints
2025-03-06 19:32:01,351 - INFO - training batch 501, loss: 0.148, 16032/60000 datapoints
2025-03-06 19:32:01,546 - INFO - training batch 551, loss: 0.323, 17632/60000 datapoints
2025-03-06 19:32:01,745 - INFO - training batch 601, loss: 0.162, 19232/60000 datapoints
2025-03-06 19:32:01,942 - INFO - training batch 651, loss: 0.081, 20832/60000 datapoints
2025-03-06 19:32:02,138 - INFO - training batch 701, loss: 0.224, 22432/60000 datapoints
2025-03-06 19:32:02,332 - INFO - training batch 751, loss: 0.126, 24032/60000 datapoints
2025-03-06 19:32:02,527 - INFO - training batch 801, loss: 0.211, 25632/60000 datapoints
2025-03-06 19:32:02,723 - INFO - training batch 851, loss: 0.305, 27232/60000 datapoints
2025-03-06 19:32:02,921 - INFO - training batch 901, loss: 0.174, 28832/60000 datapoints
2025-03-06 19:32:03,115 - INFO - training batch 951, loss: 0.182, 30432/60000 datapoints
2025-03-06 19:32:03,308 - INFO - training batch 1001, loss: 0.113, 32032/60000 datapoints
2025-03-06 19:32:03,503 - INFO - training batch 1051, loss: 0.273, 33632/60000 datapoints
2025-03-06 19:32:03,701 - INFO - training batch 1101, loss: 0.384, 35232/60000 datapoints
2025-03-06 19:32:03,897 - INFO - training batch 1151, loss: 0.527, 36832/60000 datapoints
2025-03-06 19:32:04,091 - INFO - training batch 1201, loss: 0.197, 38432/60000 datapoints
2025-03-06 19:32:04,290 - INFO - training batch 1251, loss: 0.272, 40032/60000 datapoints
2025-03-06 19:32:04,485 - INFO - training batch 1301, loss: 0.425, 41632/60000 datapoints
2025-03-06 19:32:04,681 - INFO - training batch 1351, loss: 0.290, 43232/60000 datapoints
2025-03-06 19:32:04,885 - INFO - training batch 1401, loss: 0.091, 44832/60000 datapoints
2025-03-06 19:32:05,084 - INFO - training batch 1451, loss: 0.410, 46432/60000 datapoints
2025-03-06 19:32:05,278 - INFO - training batch 1501, loss: 0.134, 48032/60000 datapoints
2025-03-06 19:32:05,474 - INFO - training batch 1551, loss: 0.196, 49632/60000 datapoints
2025-03-06 19:32:05,671 - INFO - training batch 1601, loss: 0.632, 51232/60000 datapoints
2025-03-06 19:32:05,866 - INFO - training batch 1651, loss: 0.152, 52832/60000 datapoints
2025-03-06 19:32:06,061 - INFO - training batch 1701, loss: 0.385, 54432/60000 datapoints
2025-03-06 19:32:06,257 - INFO - training batch 1751, loss: 0.177, 56032/60000 datapoints
2025-03-06 19:32:06,452 - INFO - training batch 1801, loss: 0.406, 57632/60000 datapoints
2025-03-06 19:32:06,649 - INFO - training batch 1851, loss: 0.369, 59232/60000 datapoints
2025-03-06 19:32:06,750 - INFO - validation batch 1, loss: 0.324, 32/10016 datapoints
2025-03-06 19:32:06,905 - INFO - validation batch 51, loss: 0.257, 1632/10016 datapoints
2025-03-06 19:32:07,059 - INFO - validation batch 101, loss: 0.424, 3232/10016 datapoints
2025-03-06 19:32:07,214 - INFO - validation batch 151, loss: 0.194, 4832/10016 datapoints
2025-03-06 19:32:07,383 - INFO - validation batch 201, loss: 0.444, 6432/10016 datapoints
2025-03-06 19:32:07,537 - INFO - validation batch 251, loss: 0.261, 8032/10016 datapoints
2025-03-06 19:32:07,695 - INFO - validation batch 301, loss: 0.099, 9632/10016 datapoints
2025-03-06 19:32:07,735 - INFO - Epoch 365/800 done.
2025-03-06 19:32:07,742 - INFO - Final validation performance:
Loss: 0.286, top-1 acc: 0.917top-5 acc: 0.917
2025-03-06 19:32:07,743 - INFO - Beginning epoch 366/800
2025-03-06 19:32:07,768 - INFO - training batch 1, loss: 0.245, 32/60000 datapoints
2025-03-06 19:32:07,976 - INFO - training batch 51, loss: 0.307, 1632/60000 datapoints
2025-03-06 19:32:08,168 - INFO - training batch 101, loss: 0.291, 3232/60000 datapoints
2025-03-06 19:32:08,375 - INFO - training batch 151, loss: 0.271, 4832/60000 datapoints
2025-03-06 19:32:08,578 - INFO - training batch 201, loss: 0.386, 6432/60000 datapoints
2025-03-06 19:32:08,773 - INFO - training batch 251, loss: 0.599, 8032/60000 datapoints
2025-03-06 19:32:08,973 - INFO - training batch 301, loss: 0.413, 9632/60000 datapoints
2025-03-06 19:32:09,175 - INFO - training batch 351, loss: 0.196, 11232/60000 datapoints
2025-03-06 19:32:09,372 - INFO - training batch 401, loss: 0.210, 12832/60000 datapoints
2025-03-06 19:32:09,562 - INFO - training batch 451, loss: 0.555, 14432/60000 datapoints
2025-03-06 19:32:09,760 - INFO - training batch 501, loss: 0.160, 16032/60000 datapoints
2025-03-06 19:32:09,950 - INFO - training batch 551, loss: 0.120, 17632/60000 datapoints
2025-03-06 19:32:10,142 - INFO - training batch 601, loss: 0.149, 19232/60000 datapoints
2025-03-06 19:32:10,335 - INFO - training batch 651, loss: 0.698, 20832/60000 datapoints
2025-03-06 19:32:10,528 - INFO - training batch 701, loss: 0.604, 22432/60000 datapoints
2025-03-06 19:32:10,724 - INFO - training batch 751, loss: 1.079, 24032/60000 datapoints
2025-03-06 19:32:10,920 - INFO - training batch 801, loss: 0.456, 25632/60000 datapoints
2025-03-06 19:32:11,111 - INFO - training batch 851, loss: 0.295, 27232/60000 datapoints
2025-03-06 19:32:11,304 - INFO - training batch 901, loss: 0.092, 28832/60000 datapoints
2025-03-06 19:32:11,497 - INFO - training batch 951, loss: 0.325, 30432/60000 datapoints
2025-03-06 19:32:11,690 - INFO - training batch 1001, loss: 0.351, 32032/60000 datapoints
2025-03-06 19:32:11,884 - INFO - training batch 1051, loss: 0.227, 33632/60000 datapoints
2025-03-06 19:32:12,083 - INFO - training batch 1101, loss: 0.243, 35232/60000 datapoints
2025-03-06 19:32:12,277 - INFO - training batch 1151, loss: 0.424, 36832/60000 datapoints
2025-03-06 19:32:12,467 - INFO - training batch 1201, loss: 0.299, 38432/60000 datapoints
2025-03-06 19:32:12,662 - INFO - training batch 1251, loss: 0.274, 40032/60000 datapoints
2025-03-06 19:32:12,855 - INFO - training batch 1301, loss: 0.249, 41632/60000 datapoints
2025-03-06 19:32:13,068 - INFO - training batch 1351, loss: 0.144, 43232/60000 datapoints
2025-03-06 19:32:13,268 - INFO - training batch 1401, loss: 0.143, 44832/60000 datapoints
2025-03-06 19:32:13,461 - INFO - training batch 1451, loss: 0.243, 46432/60000 datapoints
2025-03-06 19:32:13,655 - INFO - training batch 1501, loss: 0.188, 48032/60000 datapoints
2025-03-06 19:32:13,847 - INFO - training batch 1551, loss: 0.328, 49632/60000 datapoints
2025-03-06 19:32:14,039 - INFO - training batch 1601, loss: 0.148, 51232/60000 datapoints
2025-03-06 19:32:14,232 - INFO - training batch 1651, loss: 0.164, 52832/60000 datapoints
2025-03-06 19:32:14,423 - INFO - training batch 1701, loss: 0.147, 54432/60000 datapoints
2025-03-06 19:32:14,619 - INFO - training batch 1751, loss: 0.250, 56032/60000 datapoints
2025-03-06 19:32:14,815 - INFO - training batch 1801, loss: 0.262, 57632/60000 datapoints
2025-03-06 19:32:15,014 - INFO - training batch 1851, loss: 0.198, 59232/60000 datapoints
2025-03-06 19:32:15,113 - INFO - validation batch 1, loss: 0.154, 32/10016 datapoints
2025-03-06 19:32:15,266 - INFO - validation batch 51, loss: 0.302, 1632/10016 datapoints
2025-03-06 19:32:15,417 - INFO - validation batch 101, loss: 0.196, 3232/10016 datapoints
2025-03-06 19:32:15,567 - INFO - validation batch 151, loss: 0.338, 4832/10016 datapoints
2025-03-06 19:32:15,719 - INFO - validation batch 201, loss: 0.261, 6432/10016 datapoints
2025-03-06 19:32:15,871 - INFO - validation batch 251, loss: 0.236, 8032/10016 datapoints
2025-03-06 19:32:16,021 - INFO - validation batch 301, loss: 0.461, 9632/10016 datapoints
2025-03-06 19:32:16,058 - INFO - Epoch 366/800 done.
2025-03-06 19:32:16,058 - INFO - Final validation performance:
Loss: 0.279, top-1 acc: 0.918top-5 acc: 0.918
2025-03-06 19:32:16,059 - INFO - Beginning epoch 367/800
2025-03-06 19:32:16,064 - INFO - training batch 1, loss: 0.216, 32/60000 datapoints
2025-03-06 19:32:16,272 - INFO - training batch 51, loss: 0.199, 1632/60000 datapoints
2025-03-06 19:32:16,466 - INFO - training batch 101, loss: 0.102, 3232/60000 datapoints
2025-03-06 19:32:16,664 - INFO - training batch 151, loss: 0.410, 4832/60000 datapoints
2025-03-06 19:32:16,865 - INFO - training batch 201, loss: 0.182, 6432/60000 datapoints
2025-03-06 19:32:17,066 - INFO - training batch 251, loss: 0.164, 8032/60000 datapoints
2025-03-06 19:32:17,260 - INFO - training batch 301, loss: 0.348, 9632/60000 datapoints
2025-03-06 19:32:17,469 - INFO - training batch 351, loss: 0.404, 11232/60000 datapoints
2025-03-06 19:32:17,667 - INFO - training batch 401, loss: 0.413, 12832/60000 datapoints
2025-03-06 19:32:17,863 - INFO - training batch 451, loss: 0.329, 14432/60000 datapoints
2025-03-06 19:32:18,059 - INFO - training batch 501, loss: 0.820, 16032/60000 datapoints
2025-03-06 19:32:18,251 - INFO - training batch 551, loss: 0.189, 17632/60000 datapoints
2025-03-06 19:32:18,446 - INFO - training batch 601, loss: 0.261, 19232/60000 datapoints
2025-03-06 19:32:18,641 - INFO - training batch 651, loss: 0.247, 20832/60000 datapoints
2025-03-06 19:32:18,836 - INFO - training batch 701, loss: 0.152, 22432/60000 datapoints
2025-03-06 19:32:19,032 - INFO - training batch 751, loss: 0.144, 24032/60000 datapoints
2025-03-06 19:32:19,224 - INFO - training batch 801, loss: 0.137, 25632/60000 datapoints
2025-03-06 19:32:19,424 - INFO - training batch 851, loss: 0.425, 27232/60000 datapoints
2025-03-06 19:32:19,621 - INFO - training batch 901, loss: 0.415, 28832/60000 datapoints
2025-03-06 19:32:19,819 - INFO - training batch 951, loss: 0.157, 30432/60000 datapoints
2025-03-06 19:32:20,017 - INFO - training batch 1001, loss: 0.279, 32032/60000 datapoints
2025-03-06 19:32:20,210 - INFO - training batch 1051, loss: 0.202, 33632/60000 datapoints
2025-03-06 19:32:20,405 - INFO - training batch 1101, loss: 0.213, 35232/60000 datapoints
2025-03-06 19:32:20,601 - INFO - training batch 1151, loss: 0.628, 36832/60000 datapoints
2025-03-06 19:32:20,799 - INFO - training batch 1201, loss: 0.212, 38432/60000 datapoints
2025-03-06 19:32:21,000 - INFO - training batch 1251, loss: 0.231, 40032/60000 datapoints
2025-03-06 19:32:21,194 - INFO - training batch 1301, loss: 0.196, 41632/60000 datapoints
2025-03-06 19:32:21,389 - INFO - training batch 1351, loss: 0.197, 43232/60000 datapoints
2025-03-06 19:32:21,585 - INFO - training batch 1401, loss: 0.331, 44832/60000 datapoints
2025-03-06 19:32:21,782 - INFO - training batch 1451, loss: 0.226, 46432/60000 datapoints
2025-03-06 19:32:21,979 - INFO - training batch 1501, loss: 0.152, 48032/60000 datapoints
2025-03-06 19:32:22,172 - INFO - training batch 1551, loss: 0.249, 49632/60000 datapoints
2025-03-06 19:32:22,368 - INFO - training batch 1601, loss: 0.374, 51232/60000 datapoints
2025-03-06 19:32:22,563 - INFO - training batch 1651, loss: 0.104, 52832/60000 datapoints
2025-03-06 19:32:22,760 - INFO - training batch 1701, loss: 0.682, 54432/60000 datapoints
2025-03-06 19:32:22,957 - INFO - training batch 1751, loss: 0.450, 56032/60000 datapoints
2025-03-06 19:32:23,151 - INFO - training batch 1801, loss: 0.461, 57632/60000 datapoints
2025-03-06 19:32:23,347 - INFO - training batch 1851, loss: 0.311, 59232/60000 datapoints
2025-03-06 19:32:23,450 - INFO - validation batch 1, loss: 0.428, 32/10016 datapoints
2025-03-06 19:32:23,603 - INFO - validation batch 51, loss: 0.190, 1632/10016 datapoints
2025-03-06 19:32:23,757 - INFO - validation batch 101, loss: 0.124, 3232/10016 datapoints
2025-03-06 19:32:23,910 - INFO - validation batch 151, loss: 0.264, 4832/10016 datapoints
2025-03-06 19:32:24,067 - INFO - validation batch 201, loss: 0.073, 6432/10016 datapoints
2025-03-06 19:32:24,218 - INFO - validation batch 251, loss: 0.059, 8032/10016 datapoints
2025-03-06 19:32:24,369 - INFO - validation batch 301, loss: 0.462, 9632/10016 datapoints
2025-03-06 19:32:24,406 - INFO - Epoch 367/800 done.
2025-03-06 19:32:24,406 - INFO - Final validation performance:
Loss: 0.228, top-1 acc: 0.918top-5 acc: 0.918
2025-03-06 19:32:24,407 - INFO - Beginning epoch 368/800
2025-03-06 19:32:24,414 - INFO - training batch 1, loss: 0.125, 32/60000 datapoints
2025-03-06 19:32:24,629 - INFO - training batch 51, loss: 0.236, 1632/60000 datapoints
2025-03-06 19:32:24,829 - INFO - training batch 101, loss: 0.134, 3232/60000 datapoints
2025-03-06 19:32:25,042 - INFO - training batch 151, loss: 0.158, 4832/60000 datapoints
2025-03-06 19:32:25,239 - INFO - training batch 201, loss: 0.105, 6432/60000 datapoints
2025-03-06 19:32:25,440 - INFO - training batch 251, loss: 0.207, 8032/60000 datapoints
2025-03-06 19:32:25,637 - INFO - training batch 301, loss: 0.219, 9632/60000 datapoints
2025-03-06 19:32:25,834 - INFO - training batch 351, loss: 0.229, 11232/60000 datapoints
2025-03-06 19:32:26,031 - INFO - training batch 401, loss: 0.155, 12832/60000 datapoints
2025-03-06 19:32:26,227 - INFO - training batch 451, loss: 0.269, 14432/60000 datapoints
2025-03-06 19:32:26,424 - INFO - training batch 501, loss: 0.277, 16032/60000 datapoints
2025-03-06 19:32:26,621 - INFO - training batch 551, loss: 0.168, 17632/60000 datapoints
2025-03-06 19:32:26,816 - INFO - training batch 601, loss: 0.210, 19232/60000 datapoints
2025-03-06 19:32:27,014 - INFO - training batch 651, loss: 0.136, 20832/60000 datapoints
2025-03-06 19:32:27,208 - INFO - training batch 701, loss: 0.284, 22432/60000 datapoints
2025-03-06 19:32:27,408 - INFO - training batch 751, loss: 0.355, 24032/60000 datapoints
2025-03-06 19:32:27,618 - INFO - training batch 801, loss: 0.223, 25632/60000 datapoints
2025-03-06 19:32:27,815 - INFO - training batch 851, loss: 0.456, 27232/60000 datapoints
2025-03-06 19:32:28,011 - INFO - training batch 901, loss: 0.159, 28832/60000 datapoints
2025-03-06 19:32:28,204 - INFO - training batch 951, loss: 0.316, 30432/60000 datapoints
2025-03-06 19:32:28,399 - INFO - training batch 1001, loss: 0.182, 32032/60000 datapoints
2025-03-06 19:32:28,594 - INFO - training batch 1051, loss: 0.217, 33632/60000 datapoints
2025-03-06 19:32:28,789 - INFO - training batch 1101, loss: 0.332, 35232/60000 datapoints
2025-03-06 19:32:28,985 - INFO - training batch 1151, loss: 0.197, 36832/60000 datapoints
2025-03-06 19:32:29,183 - INFO - training batch 1201, loss: 0.150, 38432/60000 datapoints
2025-03-06 19:32:29,377 - INFO - training batch 1251, loss: 0.214, 40032/60000 datapoints
2025-03-06 19:32:29,573 - INFO - training batch 1301, loss: 0.278, 41632/60000 datapoints
2025-03-06 19:32:29,773 - INFO - training batch 1351, loss: 0.343, 43232/60000 datapoints
2025-03-06 19:32:29,971 - INFO - training batch 1401, loss: 0.211, 44832/60000 datapoints
2025-03-06 19:32:30,170 - INFO - training batch 1451, loss: 0.490, 46432/60000 datapoints
2025-03-06 19:32:30,365 - INFO - training batch 1501, loss: 0.380, 48032/60000 datapoints
2025-03-06 19:32:30,564 - INFO - training batch 1551, loss: 0.311, 49632/60000 datapoints
2025-03-06 19:32:30,768 - INFO - training batch 1601, loss: 0.495, 51232/60000 datapoints
2025-03-06 19:32:30,967 - INFO - training batch 1651, loss: 0.330, 52832/60000 datapoints
2025-03-06 19:32:31,161 - INFO - training batch 1701, loss: 0.360, 54432/60000 datapoints
2025-03-06 19:32:31,357 - INFO - training batch 1751, loss: 0.338, 56032/60000 datapoints
2025-03-06 19:32:31,553 - INFO - training batch 1801, loss: 0.219, 57632/60000 datapoints
2025-03-06 19:32:31,753 - INFO - training batch 1851, loss: 0.203, 59232/60000 datapoints
2025-03-06 19:32:31,855 - INFO - validation batch 1, loss: 0.158, 32/10016 datapoints
2025-03-06 19:32:32,009 - INFO - validation batch 51, loss: 0.205, 1632/10016 datapoints
2025-03-06 19:32:32,163 - INFO - validation batch 101, loss: 0.303, 3232/10016 datapoints
2025-03-06 19:32:32,313 - INFO - validation batch 151, loss: 0.404, 4832/10016 datapoints
2025-03-06 19:32:32,470 - INFO - validation batch 201, loss: 0.330, 6432/10016 datapoints
2025-03-06 19:32:32,622 - INFO - validation batch 251, loss: 0.292, 8032/10016 datapoints
2025-03-06 19:32:32,776 - INFO - validation batch 301, loss: 0.187, 9632/10016 datapoints
2025-03-06 19:32:32,814 - INFO - Epoch 368/800 done.
2025-03-06 19:32:32,814 - INFO - Final validation performance:
Loss: 0.268, top-1 acc: 0.918top-5 acc: 0.918
2025-03-06 19:32:32,814 - INFO - Beginning epoch 369/800
2025-03-06 19:32:32,822 - INFO - training batch 1, loss: 0.383, 32/60000 datapoints
2025-03-06 19:32:33,032 - INFO - training batch 51, loss: 0.617, 1632/60000 datapoints
2025-03-06 19:32:33,225 - INFO - training batch 101, loss: 0.300, 3232/60000 datapoints
2025-03-06 19:32:33,425 - INFO - training batch 151, loss: 0.377, 4832/60000 datapoints
2025-03-06 19:32:33,623 - INFO - training batch 201, loss: 0.220, 6432/60000 datapoints
2025-03-06 19:32:33,818 - INFO - training batch 251, loss: 0.391, 8032/60000 datapoints
2025-03-06 19:32:34,012 - INFO - training batch 301, loss: 0.372, 9632/60000 datapoints
2025-03-06 19:32:34,205 - INFO - training batch 351, loss: 0.243, 11232/60000 datapoints
2025-03-06 19:32:34,397 - INFO - training batch 401, loss: 0.310, 12832/60000 datapoints
2025-03-06 19:32:34,591 - INFO - training batch 451, loss: 0.266, 14432/60000 datapoints
2025-03-06 19:32:34,787 - INFO - training batch 501, loss: 0.184, 16032/60000 datapoints
2025-03-06 19:32:34,988 - INFO - training batch 551, loss: 0.602, 17632/60000 datapoints
2025-03-06 19:32:35,184 - INFO - training batch 601, loss: 0.193, 19232/60000 datapoints
2025-03-06 19:32:35,378 - INFO - training batch 651, loss: 0.289, 20832/60000 datapoints
2025-03-06 19:32:35,571 - INFO - training batch 701, loss: 0.256, 22432/60000 datapoints
2025-03-06 19:32:35,766 - INFO - training batch 751, loss: 0.275, 24032/60000 datapoints
2025-03-06 19:32:35,958 - INFO - training batch 801, loss: 0.257, 25632/60000 datapoints
2025-03-06 19:32:36,151 - INFO - training batch 851, loss: 0.260, 27232/60000 datapoints
2025-03-06 19:32:36,339 - INFO - training batch 901, loss: 0.113, 28832/60000 datapoints
2025-03-06 19:32:36,534 - INFO - training batch 951, loss: 0.189, 30432/60000 datapoints
2025-03-06 19:32:36,725 - INFO - training batch 1001, loss: 0.436, 32032/60000 datapoints
2025-03-06 19:32:36,917 - INFO - training batch 1051, loss: 0.235, 33632/60000 datapoints
2025-03-06 19:32:37,113 - INFO - training batch 1101, loss: 0.224, 35232/60000 datapoints
2025-03-06 19:32:37,303 - INFO - training batch 1151, loss: 0.258, 36832/60000 datapoints
2025-03-06 19:32:37,508 - INFO - training batch 1201, loss: 0.257, 38432/60000 datapoints
2025-03-06 19:32:37,728 - INFO - training batch 1251, loss: 0.287, 40032/60000 datapoints
2025-03-06 19:32:37,929 - INFO - training batch 1301, loss: 0.270, 41632/60000 datapoints
2025-03-06 19:32:38,153 - INFO - training batch 1351, loss: 0.323, 43232/60000 datapoints
2025-03-06 19:32:38,376 - INFO - training batch 1401, loss: 0.193, 44832/60000 datapoints
2025-03-06 19:32:38,589 - INFO - training batch 1451, loss: 0.168, 46432/60000 datapoints
2025-03-06 19:32:38,788 - INFO - training batch 1501, loss: 0.325, 48032/60000 datapoints
2025-03-06 19:32:38,984 - INFO - training batch 1551, loss: 0.209, 49632/60000 datapoints
2025-03-06 19:32:39,188 - INFO - training batch 1601, loss: 0.187, 51232/60000 datapoints
2025-03-06 19:32:39,383 - INFO - training batch 1651, loss: 0.135, 52832/60000 datapoints
2025-03-06 19:32:39,579 - INFO - training batch 1701, loss: 0.527, 54432/60000 datapoints
2025-03-06 19:32:39,784 - INFO - training batch 1751, loss: 0.315, 56032/60000 datapoints
2025-03-06 19:32:39,980 - INFO - training batch 1801, loss: 0.130, 57632/60000 datapoints
2025-03-06 19:32:40,177 - INFO - training batch 1851, loss: 0.239, 59232/60000 datapoints
2025-03-06 19:32:40,280 - INFO - validation batch 1, loss: 0.155, 32/10016 datapoints
2025-03-06 19:32:40,433 - INFO - validation batch 51, loss: 0.178, 1632/10016 datapoints
2025-03-06 19:32:40,591 - INFO - validation batch 101, loss: 0.289, 3232/10016 datapoints
2025-03-06 19:32:40,748 - INFO - validation batch 151, loss: 0.367, 4832/10016 datapoints
2025-03-06 19:32:40,901 - INFO - validation batch 201, loss: 0.335, 6432/10016 datapoints
2025-03-06 19:32:41,056 - INFO - validation batch 251, loss: 0.324, 8032/10016 datapoints
2025-03-06 19:32:41,212 - INFO - validation batch 301, loss: 0.188, 9632/10016 datapoints
2025-03-06 19:32:41,248 - INFO - Epoch 369/800 done.
2025-03-06 19:32:41,249 - INFO - Final validation performance:
Loss: 0.262, top-1 acc: 0.918top-5 acc: 0.918
2025-03-06 19:32:41,249 - INFO - Beginning epoch 370/800
2025-03-06 19:32:41,256 - INFO - training batch 1, loss: 0.303, 32/60000 datapoints
2025-03-06 19:32:41,458 - INFO - training batch 51, loss: 0.498, 1632/60000 datapoints
2025-03-06 19:32:41,672 - INFO - training batch 101, loss: 0.260, 3232/60000 datapoints
2025-03-06 19:32:41,870 - INFO - training batch 151, loss: 0.201, 4832/60000 datapoints
2025-03-06 19:32:42,070 - INFO - training batch 201, loss: 0.213, 6432/60000 datapoints
2025-03-06 19:32:42,271 - INFO - training batch 251, loss: 0.195, 8032/60000 datapoints
2025-03-06 19:32:42,469 - INFO - training batch 301, loss: 0.437, 9632/60000 datapoints
2025-03-06 19:32:42,671 - INFO - training batch 351, loss: 0.222, 11232/60000 datapoints
2025-03-06 19:32:42,898 - INFO - training batch 401, loss: 0.510, 12832/60000 datapoints
2025-03-06 19:32:43,095 - INFO - training batch 451, loss: 0.479, 14432/60000 datapoints
2025-03-06 19:32:43,288 - INFO - training batch 501, loss: 0.660, 16032/60000 datapoints
2025-03-06 19:32:43,485 - INFO - training batch 551, loss: 0.200, 17632/60000 datapoints
2025-03-06 19:32:43,682 - INFO - training batch 601, loss: 0.143, 19232/60000 datapoints
2025-03-06 19:32:43,876 - INFO - training batch 651, loss: 0.228, 20832/60000 datapoints
2025-03-06 19:32:44,070 - INFO - training batch 701, loss: 0.264, 22432/60000 datapoints
2025-03-06 19:32:44,265 - INFO - training batch 751, loss: 0.342, 24032/60000 datapoints
2025-03-06 19:32:44,457 - INFO - training batch 801, loss: 0.162, 25632/60000 datapoints
2025-03-06 19:32:44,652 - INFO - training batch 851, loss: 0.318, 27232/60000 datapoints
2025-03-06 19:32:44,848 - INFO - training batch 901, loss: 0.312, 28832/60000 datapoints
2025-03-06 19:32:45,053 - INFO - training batch 951, loss: 0.297, 30432/60000 datapoints
2025-03-06 19:32:45,251 - INFO - training batch 1001, loss: 0.312, 32032/60000 datapoints
2025-03-06 19:32:45,445 - INFO - training batch 1051, loss: 0.391, 33632/60000 datapoints
2025-03-06 19:32:45,643 - INFO - training batch 1101, loss: 0.093, 35232/60000 datapoints
2025-03-06 19:32:45,840 - INFO - training batch 1151, loss: 0.346, 36832/60000 datapoints
2025-03-06 19:32:46,034 - INFO - training batch 1201, loss: 0.187, 38432/60000 datapoints
2025-03-06 19:32:46,229 - INFO - training batch 1251, loss: 0.295, 40032/60000 datapoints
2025-03-06 19:32:46,424 - INFO - training batch 1301, loss: 0.423, 41632/60000 datapoints
2025-03-06 19:32:46,618 - INFO - training batch 1351, loss: 0.123, 43232/60000 datapoints
2025-03-06 19:32:46,813 - INFO - training batch 1401, loss: 0.316, 44832/60000 datapoints
2025-03-06 19:32:47,008 - INFO - training batch 1451, loss: 0.114, 46432/60000 datapoints
2025-03-06 19:32:47,207 - INFO - training batch 1501, loss: 0.152, 48032/60000 datapoints
2025-03-06 19:32:47,404 - INFO - training batch 1551, loss: 0.397, 49632/60000 datapoints
2025-03-06 19:32:47,620 - INFO - training batch 1601, loss: 0.116, 51232/60000 datapoints
2025-03-06 19:32:47,815 - INFO - training batch 1651, loss: 0.177, 52832/60000 datapoints
2025-03-06 19:32:48,015 - INFO - training batch 1701, loss: 0.224, 54432/60000 datapoints
2025-03-06 19:32:48,218 - INFO - training batch 1751, loss: 0.431, 56032/60000 datapoints
2025-03-06 19:32:48,412 - INFO - training batch 1801, loss: 0.391, 57632/60000 datapoints
2025-03-06 19:32:48,609 - INFO - training batch 1851, loss: 0.194, 59232/60000 datapoints
2025-03-06 19:32:48,713 - INFO - validation batch 1, loss: 0.267, 32/10016 datapoints
2025-03-06 19:32:48,867 - INFO - validation batch 51, loss: 0.748, 1632/10016 datapoints
2025-03-06 19:32:49,023 - INFO - validation batch 101, loss: 0.283, 3232/10016 datapoints
2025-03-06 19:32:49,178 - INFO - validation batch 151, loss: 0.242, 4832/10016 datapoints
2025-03-06 19:32:49,331 - INFO - validation batch 201, loss: 0.225, 6432/10016 datapoints
2025-03-06 19:32:49,487 - INFO - validation batch 251, loss: 0.142, 8032/10016 datapoints
2025-03-06 19:32:49,646 - INFO - validation batch 301, loss: 0.291, 9632/10016 datapoints
2025-03-06 19:32:49,685 - INFO - Epoch 370/800 done.
2025-03-06 19:32:49,685 - INFO - Final validation performance:
Loss: 0.314, top-1 acc: 0.918top-5 acc: 0.918
2025-03-06 19:32:49,685 - INFO - Beginning epoch 371/800
2025-03-06 19:32:49,692 - INFO - training batch 1, loss: 0.280, 32/60000 datapoints
2025-03-06 19:32:49,904 - INFO - training batch 51, loss: 0.250, 1632/60000 datapoints
2025-03-06 19:32:50,101 - INFO - training batch 101, loss: 0.090, 3232/60000 datapoints
2025-03-06 19:32:50,303 - INFO - training batch 151, loss: 0.339, 4832/60000 datapoints
2025-03-06 19:32:50,502 - INFO - training batch 201, loss: 0.285, 6432/60000 datapoints
2025-03-06 19:32:50,704 - INFO - training batch 251, loss: 0.277, 8032/60000 datapoints
2025-03-06 19:32:50,901 - INFO - training batch 301, loss: 0.267, 9632/60000 datapoints
2025-03-06 19:32:51,098 - INFO - training batch 351, loss: 0.275, 11232/60000 datapoints
2025-03-06 19:32:51,293 - INFO - training batch 401, loss: 0.360, 12832/60000 datapoints
2025-03-06 19:32:51,489 - INFO - training batch 451, loss: 0.271, 14432/60000 datapoints
2025-03-06 19:32:51,688 - INFO - training batch 501, loss: 0.188, 16032/60000 datapoints
2025-03-06 19:32:51,883 - INFO - training batch 551, loss: 0.192, 17632/60000 datapoints
2025-03-06 19:32:52,079 - INFO - training batch 601, loss: 0.268, 19232/60000 datapoints
2025-03-06 19:32:52,276 - INFO - training batch 651, loss: 0.167, 20832/60000 datapoints
2025-03-06 19:32:52,471 - INFO - training batch 701, loss: 0.423, 22432/60000 datapoints
2025-03-06 19:32:52,669 - INFO - training batch 751, loss: 0.653, 24032/60000 datapoints
2025-03-06 19:32:52,862 - INFO - training batch 801, loss: 0.405, 25632/60000 datapoints
2025-03-06 19:32:53,061 - INFO - training batch 851, loss: 0.420, 27232/60000 datapoints
2025-03-06 19:32:53,259 - INFO - training batch 901, loss: 0.323, 28832/60000 datapoints
2025-03-06 19:32:53,454 - INFO - training batch 951, loss: 0.128, 30432/60000 datapoints
2025-03-06 19:32:53,651 - INFO - training batch 1001, loss: 0.298, 32032/60000 datapoints
2025-03-06 19:32:53,845 - INFO - training batch 1051, loss: 0.087, 33632/60000 datapoints
2025-03-06 19:32:54,040 - INFO - training batch 1101, loss: 0.217, 35232/60000 datapoints
2025-03-06 19:32:54,236 - INFO - training batch 1151, loss: 0.189, 36832/60000 datapoints
2025-03-06 19:32:54,431 - INFO - training batch 1201, loss: 0.159, 38432/60000 datapoints
2025-03-06 19:32:54,626 - INFO - training batch 1251, loss: 0.340, 40032/60000 datapoints
2025-03-06 19:32:54,823 - INFO - training batch 1301, loss: 0.585, 41632/60000 datapoints
2025-03-06 19:32:55,028 - INFO - training batch 1351, loss: 0.189, 43232/60000 datapoints
2025-03-06 19:32:55,226 - INFO - training batch 1401, loss: 0.251, 44832/60000 datapoints
2025-03-06 19:32:55,423 - INFO - training batch 1451, loss: 0.216, 46432/60000 datapoints
2025-03-06 19:32:55,622 - INFO - training batch 1501, loss: 0.363, 48032/60000 datapoints
2025-03-06 19:32:55,817 - INFO - training batch 1551, loss: 0.365, 49632/60000 datapoints
2025-03-06 19:32:56,013 - INFO - training batch 1601, loss: 0.191, 51232/60000 datapoints
2025-03-06 19:32:56,208 - INFO - training batch 1651, loss: 0.595, 52832/60000 datapoints
2025-03-06 19:32:56,402 - INFO - training batch 1701, loss: 0.249, 54432/60000 datapoints
2025-03-06 19:32:56,595 - INFO - training batch 1751, loss: 0.100, 56032/60000 datapoints
2025-03-06 19:32:56,791 - INFO - training batch 1801, loss: 0.382, 57632/60000 datapoints
2025-03-06 19:32:56,986 - INFO - training batch 1851, loss: 0.361, 59232/60000 datapoints
2025-03-06 19:32:57,090 - INFO - validation batch 1, loss: 0.237, 32/10016 datapoints
2025-03-06 19:32:57,242 - INFO - validation batch 51, loss: 0.513, 1632/10016 datapoints
2025-03-06 19:32:57,397 - INFO - validation batch 101, loss: 0.651, 3232/10016 datapoints
2025-03-06 19:32:57,552 - INFO - validation batch 151, loss: 0.247, 4832/10016 datapoints
2025-03-06 19:32:57,727 - INFO - validation batch 201, loss: 0.241, 6432/10016 datapoints
2025-03-06 19:32:57,881 - INFO - validation batch 251, loss: 0.228, 8032/10016 datapoints
2025-03-06 19:32:58,036 - INFO - validation batch 301, loss: 0.741, 9632/10016 datapoints
2025-03-06 19:32:58,074 - INFO - Epoch 371/800 done.
2025-03-06 19:32:58,074 - INFO - Final validation performance:
Loss: 0.408, top-1 acc: 0.918top-5 acc: 0.918
2025-03-06 19:32:58,075 - INFO - Beginning epoch 372/800
2025-03-06 19:32:58,081 - INFO - training batch 1, loss: 0.202, 32/60000 datapoints
2025-03-06 19:32:58,278 - INFO - training batch 51, loss: 0.142, 1632/60000 datapoints
2025-03-06 19:32:58,480 - INFO - training batch 101, loss: 0.236, 3232/60000 datapoints
2025-03-06 19:32:58,693 - INFO - training batch 151, loss: 0.216, 4832/60000 datapoints
2025-03-06 19:32:58,891 - INFO - training batch 201, loss: 0.160, 6432/60000 datapoints
2025-03-06 19:32:59,101 - INFO - training batch 251, loss: 0.285, 8032/60000 datapoints
2025-03-06 19:32:59,299 - INFO - training batch 301, loss: 0.264, 9632/60000 datapoints
2025-03-06 19:32:59,496 - INFO - training batch 351, loss: 0.211, 11232/60000 datapoints
2025-03-06 19:32:59,698 - INFO - training batch 401, loss: 0.458, 12832/60000 datapoints
2025-03-06 19:32:59,891 - INFO - training batch 451, loss: 0.140, 14432/60000 datapoints
2025-03-06 19:33:00,089 - INFO - training batch 501, loss: 0.227, 16032/60000 datapoints
2025-03-06 19:33:00,284 - INFO - training batch 551, loss: 0.461, 17632/60000 datapoints
2025-03-06 19:33:00,486 - INFO - training batch 601, loss: 0.184, 19232/60000 datapoints
2025-03-06 19:33:00,683 - INFO - training batch 651, loss: 0.178, 20832/60000 datapoints
2025-03-06 19:33:00,876 - INFO - training batch 701, loss: 0.097, 22432/60000 datapoints
2025-03-06 19:33:01,073 - INFO - training batch 751, loss: 0.192, 24032/60000 datapoints
2025-03-06 19:33:01,268 - INFO - training batch 801, loss: 0.086, 25632/60000 datapoints
2025-03-06 19:33:01,465 - INFO - training batch 851, loss: 0.241, 27232/60000 datapoints
2025-03-06 19:33:01,665 - INFO - training batch 901, loss: 0.283, 28832/60000 datapoints
2025-03-06 19:33:01,862 - INFO - training batch 951, loss: 0.203, 30432/60000 datapoints
2025-03-06 19:33:02,060 - INFO - training batch 1001, loss: 0.322, 32032/60000 datapoints
2025-03-06 19:33:02,260 - INFO - training batch 1051, loss: 0.244, 33632/60000 datapoints
2025-03-06 19:33:02,457 - INFO - training batch 1101, loss: 0.295, 35232/60000 datapoints
2025-03-06 19:33:02,652 - INFO - training batch 1151, loss: 0.130, 36832/60000 datapoints
2025-03-06 19:33:02,845 - INFO - training batch 1201, loss: 0.232, 38432/60000 datapoints
2025-03-06 19:33:03,042 - INFO - training batch 1251, loss: 0.179, 40032/60000 datapoints
2025-03-06 19:33:03,239 - INFO - training batch 1301, loss: 0.721, 41632/60000 datapoints
2025-03-06 19:33:03,437 - INFO - training batch 1351, loss: 0.227, 43232/60000 datapoints
2025-03-06 19:33:03,634 - INFO - training batch 1401, loss: 0.192, 44832/60000 datapoints
2025-03-06 19:33:03,827 - INFO - training batch 1451, loss: 0.230, 46432/60000 datapoints
2025-03-06 19:33:04,025 - INFO - training batch 1501, loss: 0.341, 48032/60000 datapoints
2025-03-06 19:33:04,219 - INFO - training batch 1551, loss: 0.105, 49632/60000 datapoints
2025-03-06 19:33:04,414 - INFO - training batch 1601, loss: 0.096, 51232/60000 datapoints
2025-03-06 19:33:04,611 - INFO - training batch 1651, loss: 0.149, 52832/60000 datapoints
2025-03-06 19:33:04,811 - INFO - training batch 1701, loss: 0.862, 54432/60000 datapoints
2025-03-06 19:33:05,020 - INFO - training batch 1751, loss: 0.155, 56032/60000 datapoints
2025-03-06 19:33:05,221 - INFO - training batch 1801, loss: 0.499, 57632/60000 datapoints
2025-03-06 19:33:05,415 - INFO - training batch 1851, loss: 0.178, 59232/60000 datapoints
2025-03-06 19:33:05,518 - INFO - validation batch 1, loss: 0.328, 32/10016 datapoints
2025-03-06 19:33:05,674 - INFO - validation batch 51, loss: 0.121, 1632/10016 datapoints
2025-03-06 19:33:05,824 - INFO - validation batch 101, loss: 0.335, 3232/10016 datapoints
2025-03-06 19:33:05,978 - INFO - validation batch 151, loss: 0.319, 4832/10016 datapoints
2025-03-06 19:33:06,128 - INFO - validation batch 201, loss: 0.319, 6432/10016 datapoints
2025-03-06 19:33:06,281 - INFO - validation batch 251, loss: 0.188, 8032/10016 datapoints
2025-03-06 19:33:06,434 - INFO - validation batch 301, loss: 0.388, 9632/10016 datapoints
2025-03-06 19:33:06,472 - INFO - Epoch 372/800 done.
2025-03-06 19:33:06,472 - INFO - Final validation performance:
Loss: 0.286, top-1 acc: 0.918top-5 acc: 0.918
2025-03-06 19:33:06,472 - INFO - Beginning epoch 373/800
2025-03-06 19:33:06,479 - INFO - training batch 1, loss: 0.103, 32/60000 datapoints
2025-03-06 19:33:06,676 - INFO - training batch 51, loss: 0.139, 1632/60000 datapoints
2025-03-06 19:33:06,882 - INFO - training batch 101, loss: 0.067, 3232/60000 datapoints
2025-03-06 19:33:07,107 - INFO - training batch 151, loss: 0.474, 4832/60000 datapoints
2025-03-06 19:33:07,307 - INFO - training batch 201, loss: 0.504, 6432/60000 datapoints
2025-03-06 19:33:07,508 - INFO - training batch 251, loss: 0.205, 8032/60000 datapoints
2025-03-06 19:33:07,718 - INFO - training batch 301, loss: 0.127, 9632/60000 datapoints
2025-03-06 19:33:07,931 - INFO - training batch 351, loss: 0.086, 11232/60000 datapoints
2025-03-06 19:33:08,129 - INFO - training batch 401, loss: 0.289, 12832/60000 datapoints
2025-03-06 19:33:08,323 - INFO - training batch 451, loss: 0.245, 14432/60000 datapoints
2025-03-06 19:33:08,523 - INFO - training batch 501, loss: 0.144, 16032/60000 datapoints
2025-03-06 19:33:08,721 - INFO - training batch 551, loss: 0.158, 17632/60000 datapoints
2025-03-06 19:33:08,915 - INFO - training batch 601, loss: 0.222, 19232/60000 datapoints
2025-03-06 19:33:09,115 - INFO - training batch 651, loss: 0.140, 20832/60000 datapoints
2025-03-06 19:33:09,312 - INFO - training batch 701, loss: 0.192, 22432/60000 datapoints
2025-03-06 19:33:09,507 - INFO - training batch 751, loss: 0.158, 24032/60000 datapoints
2025-03-06 19:33:09,717 - INFO - training batch 801, loss: 0.154, 25632/60000 datapoints
2025-03-06 19:33:09,925 - INFO - training batch 851, loss: 0.421, 27232/60000 datapoints
2025-03-06 19:33:10,121 - INFO - training batch 901, loss: 0.369, 28832/60000 datapoints
2025-03-06 19:33:10,316 - INFO - training batch 951, loss: 0.171, 30432/60000 datapoints
2025-03-06 19:33:10,509 - INFO - training batch 1001, loss: 0.363, 32032/60000 datapoints
2025-03-06 19:33:10,707 - INFO - training batch 1051, loss: 0.259, 33632/60000 datapoints
2025-03-06 19:33:10,899 - INFO - training batch 1101, loss: 0.291, 35232/60000 datapoints
2025-03-06 19:33:11,094 - INFO - training batch 1151, loss: 0.109, 36832/60000 datapoints
2025-03-06 19:33:11,292 - INFO - training batch 1201, loss: 0.587, 38432/60000 datapoints
2025-03-06 19:33:11,487 - INFO - training batch 1251, loss: 0.579, 40032/60000 datapoints
2025-03-06 19:33:11,684 - INFO - training batch 1301, loss: 0.293, 41632/60000 datapoints
2025-03-06 19:33:11,882 - INFO - training batch 1351, loss: 0.271, 43232/60000 datapoints
2025-03-06 19:33:12,076 - INFO - training batch 1401, loss: 0.211, 44832/60000 datapoints
2025-03-06 19:33:12,268 - INFO - training batch 1451, loss: 0.380, 46432/60000 datapoints
2025-03-06 19:33:12,469 - INFO - training batch 1501, loss: 0.200, 48032/60000 datapoints
2025-03-06 19:33:12,667 - INFO - training batch 1551, loss: 0.455, 49632/60000 datapoints
2025-03-06 19:33:12,861 - INFO - training batch 1601, loss: 0.219, 51232/60000 datapoints
2025-03-06 19:33:13,057 - INFO - training batch 1651, loss: 0.350, 52832/60000 datapoints
2025-03-06 19:33:13,254 - INFO - training batch 1701, loss: 0.180, 54432/60000 datapoints
2025-03-06 19:33:13,477 - INFO - training batch 1751, loss: 0.250, 56032/60000 datapoints
2025-03-06 19:33:13,676 - INFO - training batch 1801, loss: 0.551, 57632/60000 datapoints
2025-03-06 19:33:13,870 - INFO - training batch 1851, loss: 0.208, 59232/60000 datapoints
2025-03-06 19:33:13,971 - INFO - validation batch 1, loss: 0.385, 32/10016 datapoints
2025-03-06 19:33:14,127 - INFO - validation batch 51, loss: 0.376, 1632/10016 datapoints
2025-03-06 19:33:14,280 - INFO - validation batch 101, loss: 0.222, 3232/10016 datapoints
2025-03-06 19:33:14,436 - INFO - validation batch 151, loss: 0.337, 4832/10016 datapoints
2025-03-06 19:33:14,590 - INFO - validation batch 201, loss: 0.143, 6432/10016 datapoints
2025-03-06 19:33:14,744 - INFO - validation batch 251, loss: 0.377, 8032/10016 datapoints
2025-03-06 19:33:14,898 - INFO - validation batch 301, loss: 0.113, 9632/10016 datapoints
2025-03-06 19:33:14,938 - INFO - Epoch 373/800 done.
2025-03-06 19:33:14,938 - INFO - Final validation performance:
Loss: 0.279, top-1 acc: 0.919top-5 acc: 0.919
2025-03-06 19:33:14,939 - INFO - Beginning epoch 374/800
2025-03-06 19:33:14,945 - INFO - training batch 1, loss: 0.237, 32/60000 datapoints
2025-03-06 19:33:15,160 - INFO - training batch 51, loss: 0.210, 1632/60000 datapoints
2025-03-06 19:33:15,356 - INFO - training batch 101, loss: 0.301, 3232/60000 datapoints
2025-03-06 19:33:15,558 - INFO - training batch 151, loss: 0.301, 4832/60000 datapoints
2025-03-06 19:33:15,758 - INFO - training batch 201, loss: 0.303, 6432/60000 datapoints
2025-03-06 19:33:15,958 - INFO - training batch 251, loss: 0.291, 8032/60000 datapoints
2025-03-06 19:33:16,155 - INFO - training batch 301, loss: 0.333, 9632/60000 datapoints
2025-03-06 19:33:16,349 - INFO - training batch 351, loss: 0.352, 11232/60000 datapoints
2025-03-06 19:33:16,544 - INFO - training batch 401, loss: 0.387, 12832/60000 datapoints
2025-03-06 19:33:16,741 - INFO - training batch 451, loss: 0.329, 14432/60000 datapoints
2025-03-06 19:33:16,934 - INFO - training batch 501, loss: 0.251, 16032/60000 datapoints
2025-03-06 19:33:17,127 - INFO - training batch 551, loss: 0.152, 17632/60000 datapoints
2025-03-06 19:33:17,327 - INFO - training batch 601, loss: 0.080, 19232/60000 datapoints
2025-03-06 19:33:17,519 - INFO - training batch 651, loss: 0.302, 20832/60000 datapoints
2025-03-06 19:33:17,719 - INFO - training batch 701, loss: 0.199, 22432/60000 datapoints
2025-03-06 19:33:17,933 - INFO - training batch 751, loss: 0.286, 24032/60000 datapoints
2025-03-06 19:33:18,127 - INFO - training batch 801, loss: 0.214, 25632/60000 datapoints
2025-03-06 19:33:18,322 - INFO - training batch 851, loss: 0.391, 27232/60000 datapoints
2025-03-06 19:33:18,518 - INFO - training batch 901, loss: 0.312, 28832/60000 datapoints
2025-03-06 19:33:18,715 - INFO - training batch 951, loss: 0.371, 30432/60000 datapoints
2025-03-06 19:33:18,911 - INFO - training batch 1001, loss: 0.321, 32032/60000 datapoints
2025-03-06 19:33:19,107 - INFO - training batch 1051, loss: 0.221, 33632/60000 datapoints
2025-03-06 19:33:19,303 - INFO - training batch 1101, loss: 0.164, 35232/60000 datapoints
2025-03-06 19:33:19,496 - INFO - training batch 1151, loss: 0.160, 36832/60000 datapoints
2025-03-06 19:33:19,693 - INFO - training batch 1201, loss: 0.363, 38432/60000 datapoints
2025-03-06 19:33:19,885 - INFO - training batch 1251, loss: 0.446, 40032/60000 datapoints
2025-03-06 19:33:20,078 - INFO - training batch 1301, loss: 0.391, 41632/60000 datapoints
2025-03-06 19:33:20,277 - INFO - training batch 1351, loss: 0.150, 43232/60000 datapoints
2025-03-06 19:33:20,470 - INFO - training batch 1401, loss: 0.358, 44832/60000 datapoints
2025-03-06 19:33:20,669 - INFO - training batch 1451, loss: 0.206, 46432/60000 datapoints
2025-03-06 19:33:20,863 - INFO - training batch 1501, loss: 0.257, 48032/60000 datapoints
2025-03-06 19:33:21,055 - INFO - training batch 1551, loss: 0.218, 49632/60000 datapoints
2025-03-06 19:33:21,254 - INFO - training batch 1601, loss: 0.228, 51232/60000 datapoints
2025-03-06 19:33:21,449 - INFO - training batch 1651, loss: 0.175, 52832/60000 datapoints
2025-03-06 19:33:21,646 - INFO - training batch 1701, loss: 0.314, 54432/60000 datapoints
2025-03-06 19:33:21,843 - INFO - training batch 1751, loss: 0.502, 56032/60000 datapoints
2025-03-06 19:33:22,038 - INFO - training batch 1801, loss: 0.187, 57632/60000 datapoints
2025-03-06 19:33:22,231 - INFO - training batch 1851, loss: 0.282, 59232/60000 datapoints
2025-03-06 19:33:22,332 - INFO - validation batch 1, loss: 0.207, 32/10016 datapoints
2025-03-06 19:33:22,487 - INFO - validation batch 51, loss: 0.162, 1632/10016 datapoints
2025-03-06 19:33:22,645 - INFO - validation batch 101, loss: 0.138, 3232/10016 datapoints
2025-03-06 19:33:22,795 - INFO - validation batch 151, loss: 0.054, 4832/10016 datapoints
2025-03-06 19:33:22,950 - INFO - validation batch 201, loss: 0.264, 6432/10016 datapoints
2025-03-06 19:33:23,102 - INFO - validation batch 251, loss: 0.195, 8032/10016 datapoints
2025-03-06 19:33:23,259 - INFO - validation batch 301, loss: 0.152, 9632/10016 datapoints
2025-03-06 19:33:23,296 - INFO - Epoch 374/800 done.
2025-03-06 19:33:23,296 - INFO - Final validation performance:
Loss: 0.168, top-1 acc: 0.918top-5 acc: 0.918
2025-03-06 19:33:23,297 - INFO - Beginning epoch 375/800
2025-03-06 19:33:23,303 - INFO - training batch 1, loss: 0.212, 32/60000 datapoints
2025-03-06 19:33:23,500 - INFO - training batch 51, loss: 0.349, 1632/60000 datapoints
2025-03-06 19:33:23,698 - INFO - training batch 101, loss: 0.307, 3232/60000 datapoints
2025-03-06 19:33:23,903 - INFO - training batch 151, loss: 0.336, 4832/60000 datapoints
2025-03-06 19:33:24,097 - INFO - training batch 201, loss: 0.444, 6432/60000 datapoints
2025-03-06 19:33:24,299 - INFO - training batch 251, loss: 0.164, 8032/60000 datapoints
2025-03-06 19:33:24,501 - INFO - training batch 301, loss: 0.542, 9632/60000 datapoints
2025-03-06 19:33:24,702 - INFO - training batch 351, loss: 0.406, 11232/60000 datapoints
2025-03-06 19:33:24,902 - INFO - training batch 401, loss: 0.256, 12832/60000 datapoints
2025-03-06 19:33:25,103 - INFO - training batch 451, loss: 0.365, 14432/60000 datapoints
2025-03-06 19:33:25,301 - INFO - training batch 501, loss: 0.278, 16032/60000 datapoints
2025-03-06 19:33:25,497 - INFO - training batch 551, loss: 0.313, 17632/60000 datapoints
2025-03-06 19:33:25,696 - INFO - training batch 601, loss: 0.297, 19232/60000 datapoints
2025-03-06 19:33:25,890 - INFO - training batch 651, loss: 0.279, 20832/60000 datapoints
2025-03-06 19:33:26,087 - INFO - training batch 701, loss: 0.281, 22432/60000 datapoints
2025-03-06 19:33:26,283 - INFO - training batch 751, loss: 0.210, 24032/60000 datapoints
2025-03-06 19:33:26,482 - INFO - training batch 801, loss: 0.091, 25632/60000 datapoints
2025-03-06 19:33:26,679 - INFO - training batch 851, loss: 0.210, 27232/60000 datapoints
2025-03-06 19:33:26,875 - INFO - training batch 901, loss: 0.299, 28832/60000 datapoints
2025-03-06 19:33:27,070 - INFO - training batch 951, loss: 0.138, 30432/60000 datapoints
2025-03-06 19:33:27,266 - INFO - training batch 1001, loss: 0.378, 32032/60000 datapoints
2025-03-06 19:33:27,462 - INFO - training batch 1051, loss: 0.237, 33632/60000 datapoints
2025-03-06 19:33:27,661 - INFO - training batch 1101, loss: 0.207, 35232/60000 datapoints
2025-03-06 19:33:27,858 - INFO - training batch 1151, loss: 0.388, 36832/60000 datapoints
2025-03-06 19:33:28,070 - INFO - training batch 1201, loss: 0.542, 38432/60000 datapoints
2025-03-06 19:33:28,264 - INFO - training batch 1251, loss: 0.107, 40032/60000 datapoints
2025-03-06 19:33:28,458 - INFO - training batch 1301, loss: 0.248, 41632/60000 datapoints
2025-03-06 19:33:28,653 - INFO - training batch 1351, loss: 0.083, 43232/60000 datapoints
2025-03-06 19:33:28,848 - INFO - training batch 1401, loss: 0.196, 44832/60000 datapoints
2025-03-06 19:33:29,046 - INFO - training batch 1451, loss: 0.443, 46432/60000 datapoints
2025-03-06 19:33:29,243 - INFO - training batch 1501, loss: 0.162, 48032/60000 datapoints
2025-03-06 19:33:29,437 - INFO - training batch 1551, loss: 0.252, 49632/60000 datapoints
2025-03-06 19:33:29,638 - INFO - training batch 1601, loss: 0.245, 51232/60000 datapoints
2025-03-06 19:33:29,833 - INFO - training batch 1651, loss: 0.184, 52832/60000 datapoints
2025-03-06 19:33:30,027 - INFO - training batch 1701, loss: 0.299, 54432/60000 datapoints
2025-03-06 19:33:30,223 - INFO - training batch 1751, loss: 0.045, 56032/60000 datapoints
2025-03-06 19:33:30,422 - INFO - training batch 1801, loss: 0.495, 57632/60000 datapoints
2025-03-06 19:33:30,617 - INFO - training batch 1851, loss: 0.294, 59232/60000 datapoints
2025-03-06 19:33:30,778 - INFO - validation batch 1, loss: 0.069, 32/10016 datapoints
2025-03-06 19:33:30,933 - INFO - validation batch 51, loss: 0.378, 1632/10016 datapoints
2025-03-06 19:33:31,085 - INFO - validation batch 101, loss: 0.326, 3232/10016 datapoints
2025-03-06 19:33:31,238 - INFO - validation batch 151, loss: 0.457, 4832/10016 datapoints
2025-03-06 19:33:31,390 - INFO - validation batch 201, loss: 0.183, 6432/10016 datapoints
2025-03-06 19:33:31,543 - INFO - validation batch 251, loss: 0.141, 8032/10016 datapoints
2025-03-06 19:33:31,700 - INFO - validation batch 301, loss: 0.114, 9632/10016 datapoints
2025-03-06 19:33:31,737 - INFO - Epoch 375/800 done.
2025-03-06 19:33:31,737 - INFO - Final validation performance:
Loss: 0.238, top-1 acc: 0.919top-5 acc: 0.919
2025-03-06 19:33:31,738 - INFO - Beginning epoch 376/800
2025-03-06 19:33:31,744 - INFO - training batch 1, loss: 0.143, 32/60000 datapoints
2025-03-06 19:33:31,941 - INFO - training batch 51, loss: 0.836, 1632/60000 datapoints
2025-03-06 19:33:32,139 - INFO - training batch 101, loss: 0.470, 3232/60000 datapoints
2025-03-06 19:33:32,345 - INFO - training batch 151, loss: 0.319, 4832/60000 datapoints
2025-03-06 19:33:32,543 - INFO - training batch 201, loss: 0.087, 6432/60000 datapoints
2025-03-06 19:33:32,742 - INFO - training batch 251, loss: 0.064, 8032/60000 datapoints
2025-03-06 19:33:32,945 - INFO - training batch 301, loss: 0.133, 9632/60000 datapoints
2025-03-06 19:33:33,162 - INFO - training batch 351, loss: 0.457, 11232/60000 datapoints
2025-03-06 19:33:33,363 - INFO - training batch 401, loss: 0.399, 12832/60000 datapoints
2025-03-06 19:33:33,562 - INFO - training batch 451, loss: 0.837, 14432/60000 datapoints
2025-03-06 19:33:33,762 - INFO - training batch 501, loss: 0.363, 16032/60000 datapoints
2025-03-06 19:33:33,959 - INFO - training batch 551, loss: 0.106, 17632/60000 datapoints
2025-03-06 19:33:34,153 - INFO - training batch 601, loss: 0.391, 19232/60000 datapoints
2025-03-06 19:33:34,347 - INFO - training batch 651, loss: 0.177, 20832/60000 datapoints
2025-03-06 19:33:34,542 - INFO - training batch 701, loss: 0.571, 22432/60000 datapoints
2025-03-06 19:33:34,739 - INFO - training batch 751, loss: 0.217, 24032/60000 datapoints
2025-03-06 19:33:34,939 - INFO - training batch 801, loss: 0.382, 25632/60000 datapoints
2025-03-06 19:33:35,137 - INFO - training batch 851, loss: 0.402, 27232/60000 datapoints
2025-03-06 19:33:35,336 - INFO - training batch 901, loss: 0.233, 28832/60000 datapoints
2025-03-06 19:33:35,536 - INFO - training batch 951, loss: 0.401, 30432/60000 datapoints
2025-03-06 19:33:35,735 - INFO - training batch 1001, loss: 0.302, 32032/60000 datapoints
2025-03-06 19:33:35,931 - INFO - training batch 1051, loss: 0.236, 33632/60000 datapoints
2025-03-06 19:33:36,123 - INFO - training batch 1101, loss: 0.158, 35232/60000 datapoints
2025-03-06 19:33:36,317 - INFO - training batch 1151, loss: 0.323, 36832/60000 datapoints
2025-03-06 19:33:36,512 - INFO - training batch 1201, loss: 0.380, 38432/60000 datapoints
2025-03-06 19:33:36,706 - INFO - training batch 1251, loss: 0.169, 40032/60000 datapoints
2025-03-06 19:33:36,901 - INFO - training batch 1301, loss: 0.198, 41632/60000 datapoints
2025-03-06 19:33:37,097 - INFO - training batch 1351, loss: 0.418, 43232/60000 datapoints
2025-03-06 19:33:37,294 - INFO - training batch 1401, loss: 0.316, 44832/60000 datapoints
2025-03-06 19:33:37,491 - INFO - training batch 1451, loss: 0.355, 46432/60000 datapoints
2025-03-06 19:33:37,702 - INFO - training batch 1501, loss: 0.555, 48032/60000 datapoints
2025-03-06 19:33:37,902 - INFO - training batch 1551, loss: 0.172, 49632/60000 datapoints
2025-03-06 19:33:38,116 - INFO - training batch 1601, loss: 0.415, 51232/60000 datapoints
2025-03-06 19:33:38,312 - INFO - training batch 1651, loss: 0.202, 52832/60000 datapoints
2025-03-06 19:33:38,510 - INFO - training batch 1701, loss: 0.110, 54432/60000 datapoints
2025-03-06 19:33:38,710 - INFO - training batch 1751, loss: 0.054, 56032/60000 datapoints
2025-03-06 19:33:38,904 - INFO - training batch 1801, loss: 0.142, 57632/60000 datapoints
2025-03-06 19:33:39,103 - INFO - training batch 1851, loss: 0.194, 59232/60000 datapoints
2025-03-06 19:33:39,204 - INFO - validation batch 1, loss: 0.280, 32/10016 datapoints
2025-03-06 19:33:39,358 - INFO - validation batch 51, loss: 0.510, 1632/10016 datapoints
2025-03-06 19:33:39,515 - INFO - validation batch 101, loss: 0.399, 3232/10016 datapoints
2025-03-06 19:33:39,671 - INFO - validation batch 151, loss: 0.080, 4832/10016 datapoints
2025-03-06 19:33:39,824 - INFO - validation batch 201, loss: 0.397, 6432/10016 datapoints
2025-03-06 19:33:39,976 - INFO - validation batch 251, loss: 0.246, 8032/10016 datapoints
2025-03-06 19:33:40,131 - INFO - validation batch 301, loss: 0.398, 9632/10016 datapoints
2025-03-06 19:33:40,168 - INFO - Epoch 376/800 done.
2025-03-06 19:33:40,168 - INFO - Final validation performance:
Loss: 0.330, top-1 acc: 0.919top-5 acc: 0.919
2025-03-06 19:33:40,169 - INFO - Beginning epoch 377/800
2025-03-06 19:33:40,176 - INFO - training batch 1, loss: 0.301, 32/60000 datapoints
2025-03-06 19:33:40,383 - INFO - training batch 51, loss: 0.064, 1632/60000 datapoints
2025-03-06 19:33:40,573 - INFO - training batch 101, loss: 0.175, 3232/60000 datapoints
2025-03-06 19:33:40,775 - INFO - training batch 151, loss: 0.201, 4832/60000 datapoints
2025-03-06 19:33:40,972 - INFO - training batch 201, loss: 0.255, 6432/60000 datapoints
2025-03-06 19:33:41,168 - INFO - training batch 251, loss: 0.176, 8032/60000 datapoints
2025-03-06 19:33:41,367 - INFO - training batch 301, loss: 0.091, 9632/60000 datapoints
2025-03-06 19:33:41,560 - INFO - training batch 351, loss: 0.658, 11232/60000 datapoints
2025-03-06 19:33:41,756 - INFO - training batch 401, loss: 0.417, 12832/60000 datapoints
2025-03-06 19:33:41,949 - INFO - training batch 451, loss: 0.280, 14432/60000 datapoints
2025-03-06 19:33:42,142 - INFO - training batch 501, loss: 0.149, 16032/60000 datapoints
2025-03-06 19:33:42,330 - INFO - training batch 551, loss: 0.222, 17632/60000 datapoints
2025-03-06 19:33:42,526 - INFO - training batch 601, loss: 0.162, 19232/60000 datapoints
2025-03-06 19:33:42,719 - INFO - training batch 651, loss: 0.167, 20832/60000 datapoints
2025-03-06 19:33:42,912 - INFO - training batch 701, loss: 0.203, 22432/60000 datapoints
2025-03-06 19:33:43,100 - INFO - training batch 751, loss: 0.105, 24032/60000 datapoints
2025-03-06 19:33:43,291 - INFO - training batch 801, loss: 0.363, 25632/60000 datapoints
2025-03-06 19:33:43,486 - INFO - training batch 851, loss: 0.087, 27232/60000 datapoints
2025-03-06 19:33:43,690 - INFO - training batch 901, loss: 0.277, 28832/60000 datapoints
2025-03-06 19:33:43,918 - INFO - training batch 951, loss: 0.133, 30432/60000 datapoints
2025-03-06 19:33:44,108 - INFO - training batch 1001, loss: 0.407, 32032/60000 datapoints
2025-03-06 19:33:44,298 - INFO - training batch 1051, loss: 0.183, 33632/60000 datapoints
2025-03-06 19:33:44,493 - INFO - training batch 1101, loss: 0.254, 35232/60000 datapoints
2025-03-06 19:33:44,687 - INFO - training batch 1151, loss: 0.348, 36832/60000 datapoints
2025-03-06 19:33:44,878 - INFO - training batch 1201, loss: 0.231, 38432/60000 datapoints
2025-03-06 19:33:45,079 - INFO - training batch 1251, loss: 0.411, 40032/60000 datapoints
2025-03-06 19:33:45,276 - INFO - training batch 1301, loss: 0.286, 41632/60000 datapoints
2025-03-06 19:33:45,470 - INFO - training batch 1351, loss: 0.156, 43232/60000 datapoints
2025-03-06 19:33:45,665 - INFO - training batch 1401, loss: 0.308, 44832/60000 datapoints
2025-03-06 19:33:45,856 - INFO - training batch 1451, loss: 0.314, 46432/60000 datapoints
2025-03-06 19:33:46,049 - INFO - training batch 1501, loss: 0.348, 48032/60000 datapoints
2025-03-06 19:33:46,242 - INFO - training batch 1551, loss: 0.365, 49632/60000 datapoints
2025-03-06 19:33:46,437 - INFO - training batch 1601, loss: 0.204, 51232/60000 datapoints
2025-03-06 19:33:46,632 - INFO - training batch 1651, loss: 0.254, 52832/60000 datapoints
2025-03-06 19:33:46,825 - INFO - training batch 1701, loss: 0.228, 54432/60000 datapoints
2025-03-06 19:33:47,018 - INFO - training batch 1751, loss: 0.310, 56032/60000 datapoints
2025-03-06 19:33:47,210 - INFO - training batch 1801, loss: 0.243, 57632/60000 datapoints
2025-03-06 19:33:47,403 - INFO - training batch 1851, loss: 0.362, 59232/60000 datapoints
2025-03-06 19:33:47,502 - INFO - validation batch 1, loss: 0.122, 32/10016 datapoints
2025-03-06 19:33:47,656 - INFO - validation batch 51, loss: 0.437, 1632/10016 datapoints
2025-03-06 19:33:47,804 - INFO - validation batch 101, loss: 0.275, 3232/10016 datapoints
2025-03-06 19:33:47,957 - INFO - validation batch 151, loss: 0.535, 4832/10016 datapoints
2025-03-06 19:33:48,146 - INFO - validation batch 201, loss: 0.520, 6432/10016 datapoints
2025-03-06 19:33:48,297 - INFO - validation batch 251, loss: 0.300, 8032/10016 datapoints
2025-03-06 19:33:48,450 - INFO - validation batch 301, loss: 0.568, 9632/10016 datapoints
2025-03-06 19:33:48,486 - INFO - Epoch 377/800 done.
2025-03-06 19:33:48,486 - INFO - Final validation performance:
Loss: 0.394, top-1 acc: 0.919top-5 acc: 0.919
2025-03-06 19:33:48,486 - INFO - Beginning epoch 378/800
2025-03-06 19:33:48,493 - INFO - training batch 1, loss: 0.346, 32/60000 datapoints
2025-03-06 19:33:48,699 - INFO - training batch 51, loss: 0.147, 1632/60000 datapoints
2025-03-06 19:33:48,896 - INFO - training batch 101, loss: 0.160, 3232/60000 datapoints
2025-03-06 19:33:49,091 - INFO - training batch 151, loss: 0.238, 4832/60000 datapoints
2025-03-06 19:33:49,292 - INFO - training batch 201, loss: 0.206, 6432/60000 datapoints
2025-03-06 19:33:49,487 - INFO - training batch 251, loss: 0.295, 8032/60000 datapoints
2025-03-06 19:33:49,690 - INFO - training batch 301, loss: 0.453, 9632/60000 datapoints
2025-03-06 19:33:49,883 - INFO - training batch 351, loss: 0.237, 11232/60000 datapoints
2025-03-06 19:33:50,076 - INFO - training batch 401, loss: 0.412, 12832/60000 datapoints
2025-03-06 19:33:50,269 - INFO - training batch 451, loss: 0.225, 14432/60000 datapoints
2025-03-06 19:33:50,463 - INFO - training batch 501, loss: 0.267, 16032/60000 datapoints
2025-03-06 19:33:50,656 - INFO - training batch 551, loss: 0.367, 17632/60000 datapoints
2025-03-06 19:33:50,853 - INFO - training batch 601, loss: 0.759, 19232/60000 datapoints
2025-03-06 19:33:51,046 - INFO - training batch 651, loss: 0.323, 20832/60000 datapoints
2025-03-06 19:33:51,237 - INFO - training batch 701, loss: 0.212, 22432/60000 datapoints
2025-03-06 19:33:51,431 - INFO - training batch 751, loss: 0.290, 24032/60000 datapoints
2025-03-06 19:33:51,625 - INFO - training batch 801, loss: 0.502, 25632/60000 datapoints
2025-03-06 19:33:51,816 - INFO - training batch 851, loss: 0.177, 27232/60000 datapoints
2025-03-06 19:33:52,008 - INFO - training batch 901, loss: 0.344, 28832/60000 datapoints
2025-03-06 19:33:52,198 - INFO - training batch 951, loss: 0.330, 30432/60000 datapoints
2025-03-06 19:33:52,390 - INFO - training batch 1001, loss: 0.211, 32032/60000 datapoints
2025-03-06 19:33:52,585 - INFO - training batch 1051, loss: 0.252, 33632/60000 datapoints
2025-03-06 19:33:52,780 - INFO - training batch 1101, loss: 0.146, 35232/60000 datapoints
2025-03-06 19:33:52,972 - INFO - training batch 1151, loss: 0.464, 36832/60000 datapoints
2025-03-06 19:33:53,162 - INFO - training batch 1201, loss: 0.383, 38432/60000 datapoints
2025-03-06 19:33:53,354 - INFO - training batch 1251, loss: 0.308, 40032/60000 datapoints
2025-03-06 19:33:53,547 - INFO - training batch 1301, loss: 0.442, 41632/60000 datapoints
2025-03-06 19:33:53,739 - INFO - training batch 1351, loss: 0.210, 43232/60000 datapoints
2025-03-06 19:33:53,932 - INFO - training batch 1401, loss: 0.241, 44832/60000 datapoints
2025-03-06 19:33:54,125 - INFO - training batch 1451, loss: 0.240, 46432/60000 datapoints
2025-03-06 19:33:54,315 - INFO - training batch 1501, loss: 0.201, 48032/60000 datapoints
2025-03-06 19:33:54,505 - INFO - training batch 1551, loss: 0.475, 49632/60000 datapoints
2025-03-06 19:33:54,700 - INFO - training batch 1601, loss: 0.342, 51232/60000 datapoints
2025-03-06 19:33:54,895 - INFO - training batch 1651, loss: 0.236, 52832/60000 datapoints
2025-03-06 19:33:55,091 - INFO - training batch 1701, loss: 0.261, 54432/60000 datapoints
2025-03-06 19:33:55,283 - INFO - training batch 1751, loss: 0.211, 56032/60000 datapoints
2025-03-06 19:33:55,480 - INFO - training batch 1801, loss: 0.248, 57632/60000 datapoints
2025-03-06 19:33:55,677 - INFO - training batch 1851, loss: 0.233, 59232/60000 datapoints
2025-03-06 19:33:55,776 - INFO - validation batch 1, loss: 0.213, 32/10016 datapoints
2025-03-06 19:33:55,925 - INFO - validation batch 51, loss: 0.402, 1632/10016 datapoints
2025-03-06 19:33:56,076 - INFO - validation batch 101, loss: 0.362, 3232/10016 datapoints
2025-03-06 19:33:56,227 - INFO - validation batch 151, loss: 0.396, 4832/10016 datapoints
2025-03-06 19:33:56,379 - INFO - validation batch 201, loss: 0.158, 6432/10016 datapoints
2025-03-06 19:33:56,527 - INFO - validation batch 251, loss: 0.340, 8032/10016 datapoints
2025-03-06 19:33:56,684 - INFO - validation batch 301, loss: 0.231, 9632/10016 datapoints
2025-03-06 19:33:56,719 - INFO - Epoch 378/800 done.
2025-03-06 19:33:56,719 - INFO - Final validation performance:
Loss: 0.300, top-1 acc: 0.919top-5 acc: 0.919
2025-03-06 19:33:56,720 - INFO - Beginning epoch 379/800
2025-03-06 19:33:56,726 - INFO - training batch 1, loss: 0.233, 32/60000 datapoints
2025-03-06 19:33:56,930 - INFO - training batch 51, loss: 0.150, 1632/60000 datapoints
2025-03-06 19:33:57,124 - INFO - training batch 101, loss: 0.221, 3232/60000 datapoints
2025-03-06 19:33:57,327 - INFO - training batch 151, loss: 0.332, 4832/60000 datapoints
2025-03-06 19:33:57,520 - INFO - training batch 201, loss: 0.562, 6432/60000 datapoints
2025-03-06 19:33:57,733 - INFO - training batch 251, loss: 0.590, 8032/60000 datapoints
2025-03-06 19:33:57,927 - INFO - training batch 301, loss: 0.407, 9632/60000 datapoints
2025-03-06 19:33:58,127 - INFO - training batch 351, loss: 0.251, 11232/60000 datapoints
2025-03-06 19:33:58,342 - INFO - training batch 401, loss: 0.174, 12832/60000 datapoints
2025-03-06 19:33:58,540 - INFO - training batch 451, loss: 0.095, 14432/60000 datapoints
2025-03-06 19:33:58,739 - INFO - training batch 501, loss: 0.175, 16032/60000 datapoints
2025-03-06 19:33:58,937 - INFO - training batch 551, loss: 0.207, 17632/60000 datapoints
2025-03-06 19:33:59,131 - INFO - training batch 601, loss: 0.213, 19232/60000 datapoints
2025-03-06 19:33:59,329 - INFO - training batch 651, loss: 0.244, 20832/60000 datapoints
2025-03-06 19:33:59,523 - INFO - training batch 701, loss: 0.110, 22432/60000 datapoints
2025-03-06 19:33:59,721 - INFO - training batch 751, loss: 0.366, 24032/60000 datapoints
2025-03-06 19:33:59,913 - INFO - training batch 801, loss: 0.399, 25632/60000 datapoints
2025-03-06 19:34:00,111 - INFO - training batch 851, loss: 0.180, 27232/60000 datapoints
2025-03-06 19:34:00,307 - INFO - training batch 901, loss: 0.259, 28832/60000 datapoints
2025-03-06 19:34:00,503 - INFO - training batch 951, loss: 0.162, 30432/60000 datapoints
2025-03-06 19:34:00,700 - INFO - training batch 1001, loss: 0.174, 32032/60000 datapoints
2025-03-06 19:34:00,899 - INFO - training batch 1051, loss: 0.451, 33632/60000 datapoints
2025-03-06 19:34:01,097 - INFO - training batch 1101, loss: 0.308, 35232/60000 datapoints
2025-03-06 19:34:01,292 - INFO - training batch 1151, loss: 0.612, 36832/60000 datapoints
2025-03-06 19:34:01,487 - INFO - training batch 1201, loss: 0.410, 38432/60000 datapoints
2025-03-06 19:34:01,685 - INFO - training batch 1251, loss: 0.471, 40032/60000 datapoints
2025-03-06 19:34:01,880 - INFO - training batch 1301, loss: 0.473, 41632/60000 datapoints
2025-03-06 19:34:02,073 - INFO - training batch 1351, loss: 0.263, 43232/60000 datapoints
2025-03-06 19:34:02,272 - INFO - training batch 1401, loss: 0.204, 44832/60000 datapoints
2025-03-06 19:34:02,466 - INFO - training batch 1451, loss: 0.179, 46432/60000 datapoints
2025-03-06 19:34:02,666 - INFO - training batch 1501, loss: 0.293, 48032/60000 datapoints
2025-03-06 19:34:02,862 - INFO - training batch 1551, loss: 0.149, 49632/60000 datapoints
2025-03-06 19:34:03,057 - INFO - training batch 1601, loss: 0.105, 51232/60000 datapoints
2025-03-06 19:34:03,255 - INFO - training batch 1651, loss: 0.339, 52832/60000 datapoints
2025-03-06 19:34:03,454 - INFO - training batch 1701, loss: 0.480, 54432/60000 datapoints
2025-03-06 19:34:03,653 - INFO - training batch 1751, loss: 0.396, 56032/60000 datapoints
2025-03-06 19:34:03,846 - INFO - training batch 1801, loss: 0.399, 57632/60000 datapoints
2025-03-06 19:34:04,043 - INFO - training batch 1851, loss: 0.185, 59232/60000 datapoints
2025-03-06 19:34:04,145 - INFO - validation batch 1, loss: 0.265, 32/10016 datapoints
2025-03-06 19:34:04,296 - INFO - validation batch 51, loss: 0.370, 1632/10016 datapoints
2025-03-06 19:34:04,448 - INFO - validation batch 101, loss: 0.504, 3232/10016 datapoints
2025-03-06 19:34:04,602 - INFO - validation batch 151, loss: 0.224, 4832/10016 datapoints
2025-03-06 19:34:04,756 - INFO - validation batch 201, loss: 0.241, 6432/10016 datapoints
2025-03-06 19:34:04,913 - INFO - validation batch 251, loss: 0.326, 8032/10016 datapoints
2025-03-06 19:34:05,066 - INFO - validation batch 301, loss: 0.464, 9632/10016 datapoints
2025-03-06 19:34:05,105 - INFO - Epoch 379/800 done.
2025-03-06 19:34:05,105 - INFO - Final validation performance:
Loss: 0.342, top-1 acc: 0.919top-5 acc: 0.919
2025-03-06 19:34:05,106 - INFO - Beginning epoch 380/800
2025-03-06 19:34:05,114 - INFO - training batch 1, loss: 0.282, 32/60000 datapoints
2025-03-06 19:34:05,318 - INFO - training batch 51, loss: 0.846, 1632/60000 datapoints
2025-03-06 19:34:05,516 - INFO - training batch 101, loss: 0.260, 3232/60000 datapoints
2025-03-06 19:34:05,727 - INFO - training batch 151, loss: 0.284, 4832/60000 datapoints
2025-03-06 19:34:05,923 - INFO - training batch 201, loss: 0.169, 6432/60000 datapoints
2025-03-06 19:34:06,128 - INFO - training batch 251, loss: 0.316, 8032/60000 datapoints
2025-03-06 19:34:06,327 - INFO - training batch 301, loss: 0.149, 9632/60000 datapoints
2025-03-06 19:34:06,526 - INFO - training batch 351, loss: 0.465, 11232/60000 datapoints
2025-03-06 19:34:06,722 - INFO - training batch 401, loss: 0.285, 12832/60000 datapoints
2025-03-06 19:34:06,916 - INFO - training batch 451, loss: 0.266, 14432/60000 datapoints
2025-03-06 19:34:07,109 - INFO - training batch 501, loss: 0.488, 16032/60000 datapoints
2025-03-06 19:34:07,304 - INFO - training batch 551, loss: 0.127, 17632/60000 datapoints
2025-03-06 19:34:07,499 - INFO - training batch 601, loss: 0.666, 19232/60000 datapoints
2025-03-06 19:34:07,701 - INFO - training batch 651, loss: 0.187, 20832/60000 datapoints
2025-03-06 19:34:07,891 - INFO - training batch 701, loss: 0.586, 22432/60000 datapoints
2025-03-06 19:34:08,085 - INFO - training batch 751, loss: 0.201, 24032/60000 datapoints
2025-03-06 19:34:08,298 - INFO - training batch 801, loss: 0.140, 25632/60000 datapoints
2025-03-06 19:34:08,492 - INFO - training batch 851, loss: 0.341, 27232/60000 datapoints
2025-03-06 19:34:08,695 - INFO - training batch 901, loss: 0.262, 28832/60000 datapoints
2025-03-06 19:34:08,886 - INFO - training batch 951, loss: 0.246, 30432/60000 datapoints
2025-03-06 19:34:09,082 - INFO - training batch 1001, loss: 0.679, 32032/60000 datapoints
2025-03-06 19:34:09,277 - INFO - training batch 1051, loss: 0.213, 33632/60000 datapoints
2025-03-06 19:34:09,474 - INFO - training batch 1101, loss: 0.370, 35232/60000 datapoints
2025-03-06 19:34:09,673 - INFO - training batch 1151, loss: 0.460, 36832/60000 datapoints
2025-03-06 19:34:09,875 - INFO - training batch 1201, loss: 0.153, 38432/60000 datapoints
2025-03-06 19:34:10,070 - INFO - training batch 1251, loss: 0.072, 40032/60000 datapoints
2025-03-06 19:34:10,266 - INFO - training batch 1301, loss: 0.349, 41632/60000 datapoints
2025-03-06 19:34:10,461 - INFO - training batch 1351, loss: 0.277, 43232/60000 datapoints
2025-03-06 19:34:10,659 - INFO - training batch 1401, loss: 0.268, 44832/60000 datapoints
2025-03-06 19:34:10,853 - INFO - training batch 1451, loss: 0.249, 46432/60000 datapoints
2025-03-06 19:34:11,050 - INFO - training batch 1501, loss: 0.305, 48032/60000 datapoints
2025-03-06 19:34:11,244 - INFO - training batch 1551, loss: 0.110, 49632/60000 datapoints
2025-03-06 19:34:11,439 - INFO - training batch 1601, loss: 0.244, 51232/60000 datapoints
2025-03-06 19:34:11,634 - INFO - training batch 1651, loss: 0.291, 52832/60000 datapoints
2025-03-06 19:34:11,828 - INFO - training batch 1701, loss: 0.244, 54432/60000 datapoints
2025-03-06 19:34:12,020 - INFO - training batch 1751, loss: 0.174, 56032/60000 datapoints
2025-03-06 19:34:12,215 - INFO - training batch 1801, loss: 0.281, 57632/60000 datapoints
2025-03-06 19:34:12,409 - INFO - training batch 1851, loss: 0.297, 59232/60000 datapoints
2025-03-06 19:34:12,509 - INFO - validation batch 1, loss: 0.173, 32/10016 datapoints
2025-03-06 19:34:12,663 - INFO - validation batch 51, loss: 0.177, 1632/10016 datapoints
2025-03-06 19:34:12,818 - INFO - validation batch 101, loss: 0.225, 3232/10016 datapoints
2025-03-06 19:34:12,971 - INFO - validation batch 151, loss: 0.251, 4832/10016 datapoints
2025-03-06 19:34:13,123 - INFO - validation batch 201, loss: 0.179, 6432/10016 datapoints
2025-03-06 19:34:13,280 - INFO - validation batch 251, loss: 0.537, 8032/10016 datapoints
2025-03-06 19:34:13,437 - INFO - validation batch 301, loss: 0.331, 9632/10016 datapoints
2025-03-06 19:34:13,477 - INFO - Epoch 380/800 done.
2025-03-06 19:34:13,477 - INFO - Final validation performance:
Loss: 0.267, top-1 acc: 0.919top-5 acc: 0.919
2025-03-06 19:34:13,478 - INFO - Beginning epoch 381/800
2025-03-06 19:34:13,484 - INFO - training batch 1, loss: 0.271, 32/60000 datapoints
2025-03-06 19:34:13,684 - INFO - training batch 51, loss: 0.132, 1632/60000 datapoints
2025-03-06 19:34:13,878 - INFO - training batch 101, loss: 0.580, 3232/60000 datapoints
2025-03-06 19:34:14,082 - INFO - training batch 151, loss: 0.082, 4832/60000 datapoints
2025-03-06 19:34:14,279 - INFO - training batch 201, loss: 0.298, 6432/60000 datapoints
2025-03-06 19:34:14,474 - INFO - training batch 251, loss: 0.208, 8032/60000 datapoints
2025-03-06 19:34:14,672 - INFO - training batch 301, loss: 0.231, 9632/60000 datapoints
2025-03-06 19:34:14,874 - INFO - training batch 351, loss: 0.174, 11232/60000 datapoints
2025-03-06 19:34:15,080 - INFO - training batch 401, loss: 0.143, 12832/60000 datapoints
2025-03-06 19:34:15,279 - INFO - training batch 451, loss: 0.235, 14432/60000 datapoints
2025-03-06 19:34:15,475 - INFO - training batch 501, loss: 0.320, 16032/60000 datapoints
2025-03-06 19:34:15,672 - INFO - training batch 551, loss: 0.548, 17632/60000 datapoints
2025-03-06 19:34:15,865 - INFO - training batch 601, loss: 0.388, 19232/60000 datapoints
2025-03-06 19:34:16,062 - INFO - training batch 651, loss: 0.380, 20832/60000 datapoints
2025-03-06 19:34:16,255 - INFO - training batch 701, loss: 0.348, 22432/60000 datapoints
2025-03-06 19:34:16,450 - INFO - training batch 751, loss: 0.097, 24032/60000 datapoints
2025-03-06 19:34:16,644 - INFO - training batch 801, loss: 0.204, 25632/60000 datapoints
2025-03-06 19:34:16,841 - INFO - training batch 851, loss: 0.205, 27232/60000 datapoints
2025-03-06 19:34:17,037 - INFO - training batch 901, loss: 0.191, 28832/60000 datapoints
2025-03-06 19:34:17,228 - INFO - training batch 951, loss: 0.132, 30432/60000 datapoints
2025-03-06 19:34:17,442 - INFO - training batch 1001, loss: 0.533, 32032/60000 datapoints
2025-03-06 19:34:17,637 - INFO - training batch 1051, loss: 0.328, 33632/60000 datapoints
2025-03-06 19:34:17,832 - INFO - training batch 1101, loss: 0.133, 35232/60000 datapoints
2025-03-06 19:34:18,026 - INFO - training batch 1151, loss: 0.215, 36832/60000 datapoints
2025-03-06 19:34:18,220 - INFO - training batch 1201, loss: 0.481, 38432/60000 datapoints
2025-03-06 19:34:18,436 - INFO - training batch 1251, loss: 0.571, 40032/60000 datapoints
2025-03-06 19:34:18,632 - INFO - training batch 1301, loss: 0.557, 41632/60000 datapoints
2025-03-06 19:34:18,827 - INFO - training batch 1351, loss: 0.197, 43232/60000 datapoints
2025-03-06 19:34:19,023 - INFO - training batch 1401, loss: 0.182, 44832/60000 datapoints
2025-03-06 19:34:19,216 - INFO - training batch 1451, loss: 0.457, 46432/60000 datapoints
2025-03-06 19:34:19,414 - INFO - training batch 1501, loss: 0.236, 48032/60000 datapoints
2025-03-06 19:34:19,610 - INFO - training batch 1551, loss: 0.172, 49632/60000 datapoints
2025-03-06 19:34:19,804 - INFO - training batch 1601, loss: 0.146, 51232/60000 datapoints
2025-03-06 19:34:19,998 - INFO - training batch 1651, loss: 0.526, 52832/60000 datapoints
2025-03-06 19:34:20,193 - INFO - training batch 1701, loss: 0.232, 54432/60000 datapoints
2025-03-06 19:34:20,387 - INFO - training batch 1751, loss: 0.082, 56032/60000 datapoints
2025-03-06 19:34:20,583 - INFO - training batch 1801, loss: 0.358, 57632/60000 datapoints
2025-03-06 19:34:20,778 - INFO - training batch 1851, loss: 0.212, 59232/60000 datapoints
2025-03-06 19:34:20,881 - INFO - validation batch 1, loss: 0.224, 32/10016 datapoints
2025-03-06 19:34:21,034 - INFO - validation batch 51, loss: 0.222, 1632/10016 datapoints
2025-03-06 19:34:21,188 - INFO - validation batch 101, loss: 0.118, 3232/10016 datapoints
2025-03-06 19:34:21,346 - INFO - validation batch 151, loss: 0.083, 4832/10016 datapoints
2025-03-06 19:34:21,504 - INFO - validation batch 201, loss: 0.565, 6432/10016 datapoints
2025-03-06 19:34:21,657 - INFO - validation batch 251, loss: 0.436, 8032/10016 datapoints
2025-03-06 19:34:21,809 - INFO - validation batch 301, loss: 0.351, 9632/10016 datapoints
2025-03-06 19:34:21,847 - INFO - Epoch 381/800 done.
2025-03-06 19:34:21,847 - INFO - Final validation performance:
Loss: 0.286, top-1 acc: 0.919top-5 acc: 0.919
2025-03-06 19:34:21,847 - INFO - Beginning epoch 382/800
2025-03-06 19:34:21,854 - INFO - training batch 1, loss: 0.253, 32/60000 datapoints
2025-03-06 19:34:22,048 - INFO - training batch 51, loss: 0.162, 1632/60000 datapoints
2025-03-06 19:34:22,241 - INFO - training batch 101, loss: 0.233, 3232/60000 datapoints
2025-03-06 19:34:22,447 - INFO - training batch 151, loss: 0.240, 4832/60000 datapoints
2025-03-06 19:34:22,645 - INFO - training batch 201, loss: 0.176, 6432/60000 datapoints
2025-03-06 19:34:22,855 - INFO - training batch 251, loss: 0.445, 8032/60000 datapoints
2025-03-06 19:34:23,051 - INFO - training batch 301, loss: 0.489, 9632/60000 datapoints
2025-03-06 19:34:23,248 - INFO - training batch 351, loss: 0.420, 11232/60000 datapoints
2025-03-06 19:34:23,449 - INFO - training batch 401, loss: 0.177, 12832/60000 datapoints
2025-03-06 19:34:23,650 - INFO - training batch 451, loss: 0.312, 14432/60000 datapoints
2025-03-06 19:34:23,845 - INFO - training batch 501, loss: 0.254, 16032/60000 datapoints
2025-03-06 19:34:24,036 - INFO - training batch 551, loss: 0.046, 17632/60000 datapoints
2025-03-06 19:34:24,230 - INFO - training batch 601, loss: 0.412, 19232/60000 datapoints
2025-03-06 19:34:24,426 - INFO - training batch 651, loss: 0.228, 20832/60000 datapoints
2025-03-06 19:34:24,621 - INFO - training batch 701, loss: 0.233, 22432/60000 datapoints
2025-03-06 19:34:24,817 - INFO - training batch 751, loss: 0.350, 24032/60000 datapoints
2025-03-06 19:34:25,015 - INFO - training batch 801, loss: 0.229, 25632/60000 datapoints
2025-03-06 19:34:25,214 - INFO - training batch 851, loss: 0.259, 27232/60000 datapoints
2025-03-06 19:34:25,408 - INFO - training batch 901, loss: 0.408, 28832/60000 datapoints
2025-03-06 19:34:25,601 - INFO - training batch 951, loss: 0.565, 30432/60000 datapoints
2025-03-06 19:34:25,799 - INFO - training batch 1001, loss: 0.139, 32032/60000 datapoints
2025-03-06 19:34:25,996 - INFO - training batch 1051, loss: 0.311, 33632/60000 datapoints
2025-03-06 19:34:26,189 - INFO - training batch 1101, loss: 0.419, 35232/60000 datapoints
2025-03-06 19:34:26,385 - INFO - training batch 1151, loss: 0.342, 36832/60000 datapoints
2025-03-06 19:34:26,577 - INFO - training batch 1201, loss: 0.099, 38432/60000 datapoints
2025-03-06 19:34:26,774 - INFO - training batch 1251, loss: 0.283, 40032/60000 datapoints
2025-03-06 19:34:26,968 - INFO - training batch 1301, loss: 0.479, 41632/60000 datapoints
2025-03-06 19:34:27,162 - INFO - training batch 1351, loss: 0.176, 43232/60000 datapoints
2025-03-06 19:34:27,353 - INFO - training batch 1401, loss: 0.298, 44832/60000 datapoints
2025-03-06 19:34:27,551 - INFO - training batch 1451, loss: 0.142, 46432/60000 datapoints
2025-03-06 19:34:27,747 - INFO - training batch 1501, loss: 0.354, 48032/60000 datapoints
2025-03-06 19:34:27,940 - INFO - training batch 1551, loss: 0.196, 49632/60000 datapoints
2025-03-06 19:34:28,135 - INFO - training batch 1601, loss: 0.688, 51232/60000 datapoints
2025-03-06 19:34:28,329 - INFO - training batch 1651, loss: 0.163, 52832/60000 datapoints
2025-03-06 19:34:28,547 - INFO - training batch 1701, loss: 0.269, 54432/60000 datapoints
2025-03-06 19:34:28,749 - INFO - training batch 1751, loss: 0.151, 56032/60000 datapoints
2025-03-06 19:34:28,942 - INFO - training batch 1801, loss: 0.489, 57632/60000 datapoints
2025-03-06 19:34:29,133 - INFO - training batch 1851, loss: 0.295, 59232/60000 datapoints
2025-03-06 19:34:29,232 - INFO - validation batch 1, loss: 0.519, 32/10016 datapoints
2025-03-06 19:34:29,386 - INFO - validation batch 51, loss: 0.332, 1632/10016 datapoints
2025-03-06 19:34:29,539 - INFO - validation batch 101, loss: 0.262, 3232/10016 datapoints
2025-03-06 19:34:29,695 - INFO - validation batch 151, loss: 0.406, 4832/10016 datapoints
2025-03-06 19:34:29,845 - INFO - validation batch 201, loss: 0.605, 6432/10016 datapoints
2025-03-06 19:34:29,999 - INFO - validation batch 251, loss: 0.300, 8032/10016 datapoints
2025-03-06 19:34:30,153 - INFO - validation batch 301, loss: 0.162, 9632/10016 datapoints
2025-03-06 19:34:30,192 - INFO - Epoch 382/800 done.
2025-03-06 19:34:30,192 - INFO - Final validation performance:
Loss: 0.370, top-1 acc: 0.919top-5 acc: 0.919
2025-03-06 19:34:30,192 - INFO - Beginning epoch 383/800
2025-03-06 19:34:30,199 - INFO - training batch 1, loss: 0.360, 32/60000 datapoints
2025-03-06 19:34:30,395 - INFO - training batch 51, loss: 0.197, 1632/60000 datapoints
2025-03-06 19:34:30,588 - INFO - training batch 101, loss: 0.196, 3232/60000 datapoints
2025-03-06 19:34:30,803 - INFO - training batch 151, loss: 0.534, 4832/60000 datapoints
2025-03-06 19:34:30,999 - INFO - training batch 201, loss: 0.284, 6432/60000 datapoints
2025-03-06 19:34:31,197 - INFO - training batch 251, loss: 0.652, 8032/60000 datapoints
2025-03-06 19:34:31,397 - INFO - training batch 301, loss: 0.233, 9632/60000 datapoints
2025-03-06 19:34:31,597 - INFO - training batch 351, loss: 0.161, 11232/60000 datapoints
2025-03-06 19:34:31,810 - INFO - training batch 401, loss: 0.214, 12832/60000 datapoints
2025-03-06 19:34:32,018 - INFO - training batch 451, loss: 0.149, 14432/60000 datapoints
2025-03-06 19:34:32,216 - INFO - training batch 501, loss: 0.368, 16032/60000 datapoints
2025-03-06 19:34:32,410 - INFO - training batch 551, loss: 0.384, 17632/60000 datapoints
2025-03-06 19:34:32,602 - INFO - training batch 601, loss: 0.319, 19232/60000 datapoints
2025-03-06 19:34:32,799 - INFO - training batch 651, loss: 0.173, 20832/60000 datapoints
2025-03-06 19:34:32,995 - INFO - training batch 701, loss: 0.336, 22432/60000 datapoints
2025-03-06 19:34:33,191 - INFO - training batch 751, loss: 0.291, 24032/60000 datapoints
2025-03-06 19:34:33,383 - INFO - training batch 801, loss: 0.293, 25632/60000 datapoints
2025-03-06 19:34:33,580 - INFO - training batch 851, loss: 0.245, 27232/60000 datapoints
2025-03-06 19:34:33,775 - INFO - training batch 901, loss: 0.217, 28832/60000 datapoints
2025-03-06 19:34:33,969 - INFO - training batch 951, loss: 0.113, 30432/60000 datapoints
2025-03-06 19:34:34,163 - INFO - training batch 1001, loss: 0.136, 32032/60000 datapoints
2025-03-06 19:34:34,359 - INFO - training batch 1051, loss: 0.285, 33632/60000 datapoints
2025-03-06 19:34:34,555 - INFO - training batch 1101, loss: 0.123, 35232/60000 datapoints
2025-03-06 19:34:34,751 - INFO - training batch 1151, loss: 0.397, 36832/60000 datapoints
2025-03-06 19:34:34,948 - INFO - training batch 1201, loss: 0.239, 38432/60000 datapoints
2025-03-06 19:34:35,142 - INFO - training batch 1251, loss: 0.226, 40032/60000 datapoints
2025-03-06 19:34:35,340 - INFO - training batch 1301, loss: 0.181, 41632/60000 datapoints
2025-03-06 19:34:35,536 - INFO - training batch 1351, loss: 0.249, 43232/60000 datapoints
2025-03-06 19:34:35,732 - INFO - training batch 1401, loss: 0.154, 44832/60000 datapoints
2025-03-06 19:34:35,927 - INFO - training batch 1451, loss: 0.337, 46432/60000 datapoints
2025-03-06 19:34:36,120 - INFO - training batch 1501, loss: 0.342, 48032/60000 datapoints
2025-03-06 19:34:36,316 - INFO - training batch 1551, loss: 0.168, 49632/60000 datapoints
2025-03-06 19:34:36,516 - INFO - training batch 1601, loss: 0.287, 51232/60000 datapoints
2025-03-06 19:34:36,712 - INFO - training batch 1651, loss: 0.067, 52832/60000 datapoints
2025-03-06 19:34:36,907 - INFO - training batch 1701, loss: 0.560, 54432/60000 datapoints
2025-03-06 19:34:37,098 - INFO - training batch 1751, loss: 0.273, 56032/60000 datapoints
2025-03-06 19:34:37,288 - INFO - training batch 1801, loss: 0.206, 57632/60000 datapoints
2025-03-06 19:34:37,488 - INFO - training batch 1851, loss: 0.694, 59232/60000 datapoints
2025-03-06 19:34:37,590 - INFO - validation batch 1, loss: 0.400, 32/10016 datapoints
2025-03-06 19:34:37,757 - INFO - validation batch 51, loss: 0.358, 1632/10016 datapoints
2025-03-06 19:34:37,910 - INFO - validation batch 101, loss: 0.253, 3232/10016 datapoints
2025-03-06 19:34:38,065 - INFO - validation batch 151, loss: 0.298, 4832/10016 datapoints
2025-03-06 19:34:38,218 - INFO - validation batch 201, loss: 0.621, 6432/10016 datapoints
2025-03-06 19:34:38,371 - INFO - validation batch 251, loss: 0.183, 8032/10016 datapoints
2025-03-06 19:34:38,544 - INFO - validation batch 301, loss: 0.246, 9632/10016 datapoints
2025-03-06 19:34:38,584 - INFO - Epoch 383/800 done.
2025-03-06 19:34:38,584 - INFO - Final validation performance:
Loss: 0.337, top-1 acc: 0.919top-5 acc: 0.919
2025-03-06 19:34:38,584 - INFO - Beginning epoch 384/800
2025-03-06 19:34:38,591 - INFO - training batch 1, loss: 0.144, 32/60000 datapoints
2025-03-06 19:34:38,786 - INFO - training batch 51, loss: 0.380, 1632/60000 datapoints
2025-03-06 19:34:38,983 - INFO - training batch 101, loss: 0.265, 3232/60000 datapoints
2025-03-06 19:34:39,188 - INFO - training batch 151, loss: 0.144, 4832/60000 datapoints
2025-03-06 19:34:39,384 - INFO - training batch 201, loss: 0.416, 6432/60000 datapoints
2025-03-06 19:34:39,588 - INFO - training batch 251, loss: 0.128, 8032/60000 datapoints
2025-03-06 19:34:39,790 - INFO - training batch 301, loss: 0.350, 9632/60000 datapoints
2025-03-06 19:34:39,989 - INFO - training batch 351, loss: 0.110, 11232/60000 datapoints
2025-03-06 19:34:40,183 - INFO - training batch 401, loss: 0.577, 12832/60000 datapoints
2025-03-06 19:34:40,378 - INFO - training batch 451, loss: 0.238, 14432/60000 datapoints
2025-03-06 19:34:40,576 - INFO - training batch 501, loss: 0.288, 16032/60000 datapoints
2025-03-06 19:34:40,776 - INFO - training batch 551, loss: 0.186, 17632/60000 datapoints
2025-03-06 19:34:40,973 - INFO - training batch 601, loss: 0.368, 19232/60000 datapoints
2025-03-06 19:34:41,165 - INFO - training batch 651, loss: 0.362, 20832/60000 datapoints
2025-03-06 19:34:41,359 - INFO - training batch 701, loss: 0.156, 22432/60000 datapoints
2025-03-06 19:34:41,556 - INFO - training batch 751, loss: 0.242, 24032/60000 datapoints
2025-03-06 19:34:41,754 - INFO - training batch 801, loss: 0.247, 25632/60000 datapoints
2025-03-06 19:34:41,949 - INFO - training batch 851, loss: 0.379, 27232/60000 datapoints
2025-03-06 19:34:42,141 - INFO - training batch 901, loss: 0.289, 28832/60000 datapoints
2025-03-06 19:34:42,335 - INFO - training batch 951, loss: 0.280, 30432/60000 datapoints
2025-03-06 19:34:42,530 - INFO - training batch 1001, loss: 0.261, 32032/60000 datapoints
2025-03-06 19:34:42,726 - INFO - training batch 1051, loss: 0.462, 33632/60000 datapoints
2025-03-06 19:34:42,922 - INFO - training batch 1101, loss: 0.126, 35232/60000 datapoints
2025-03-06 19:34:43,117 - INFO - training batch 1151, loss: 0.260, 36832/60000 datapoints
2025-03-06 19:34:43,311 - INFO - training batch 1201, loss: 0.325, 38432/60000 datapoints
2025-03-06 19:34:43,506 - INFO - training batch 1251, loss: 0.265, 40032/60000 datapoints
2025-03-06 19:34:43,706 - INFO - training batch 1301, loss: 0.273, 41632/60000 datapoints
2025-03-06 19:34:43,899 - INFO - training batch 1351, loss: 0.183, 43232/60000 datapoints
2025-03-06 19:34:44,092 - INFO - training batch 1401, loss: 0.191, 44832/60000 datapoints
2025-03-06 19:34:44,289 - INFO - training batch 1451, loss: 0.155, 46432/60000 datapoints
2025-03-06 19:34:44,485 - INFO - training batch 1501, loss: 0.324, 48032/60000 datapoints
2025-03-06 19:34:44,682 - INFO - training batch 1551, loss: 0.736, 49632/60000 datapoints
2025-03-06 19:34:44,876 - INFO - training batch 1601, loss: 0.384, 51232/60000 datapoints
2025-03-06 19:34:45,075 - INFO - training batch 1651, loss: 0.382, 52832/60000 datapoints
2025-03-06 19:34:45,272 - INFO - training batch 1701, loss: 0.231, 54432/60000 datapoints
2025-03-06 19:34:45,467 - INFO - training batch 1751, loss: 0.290, 56032/60000 datapoints
2025-03-06 19:34:45,668 - INFO - training batch 1801, loss: 0.456, 57632/60000 datapoints
2025-03-06 19:34:45,865 - INFO - training batch 1851, loss: 0.231, 59232/60000 datapoints
2025-03-06 19:34:45,968 - INFO - validation batch 1, loss: 0.087, 32/10016 datapoints
2025-03-06 19:34:46,123 - INFO - validation batch 51, loss: 0.351, 1632/10016 datapoints
2025-03-06 19:34:46,273 - INFO - validation batch 101, loss: 0.213, 3232/10016 datapoints
2025-03-06 19:34:46,427 - INFO - validation batch 151, loss: 0.244, 4832/10016 datapoints
2025-03-06 19:34:46,584 - INFO - validation batch 201, loss: 0.123, 6432/10016 datapoints
2025-03-06 19:34:46,739 - INFO - validation batch 251, loss: 0.500, 8032/10016 datapoints
2025-03-06 19:34:46,892 - INFO - validation batch 301, loss: 0.283, 9632/10016 datapoints
2025-03-06 19:34:46,929 - INFO - Epoch 384/800 done.
2025-03-06 19:34:46,929 - INFO - Final validation performance:
Loss: 0.257, top-1 acc: 0.919top-5 acc: 0.919
2025-03-06 19:34:46,930 - INFO - Beginning epoch 385/800
2025-03-06 19:34:46,936 - INFO - training batch 1, loss: 0.267, 32/60000 datapoints
2025-03-06 19:34:47,133 - INFO - training batch 51, loss: 0.555, 1632/60000 datapoints
2025-03-06 19:34:47,325 - INFO - training batch 101, loss: 0.303, 3232/60000 datapoints
2025-03-06 19:34:47,531 - INFO - training batch 151, loss: 0.230, 4832/60000 datapoints
2025-03-06 19:34:47,731 - INFO - training batch 201, loss: 0.280, 6432/60000 datapoints
2025-03-06 19:34:47,924 - INFO - training batch 251, loss: 0.258, 8032/60000 datapoints
2025-03-06 19:34:48,147 - INFO - training batch 301, loss: 0.292, 9632/60000 datapoints
2025-03-06 19:34:48,355 - INFO - training batch 351, loss: 0.433, 11232/60000 datapoints
2025-03-06 19:34:48,572 - INFO - training batch 401, loss: 0.259, 12832/60000 datapoints
2025-03-06 19:34:48,772 - INFO - training batch 451, loss: 0.145, 14432/60000 datapoints
2025-03-06 19:34:48,967 - INFO - training batch 501, loss: 0.205, 16032/60000 datapoints
2025-03-06 19:34:49,162 - INFO - training batch 551, loss: 0.280, 17632/60000 datapoints
2025-03-06 19:34:49,354 - INFO - training batch 601, loss: 0.184, 19232/60000 datapoints
2025-03-06 19:34:49,551 - INFO - training batch 651, loss: 0.234, 20832/60000 datapoints
2025-03-06 19:34:49,750 - INFO - training batch 701, loss: 0.258, 22432/60000 datapoints
2025-03-06 19:34:49,944 - INFO - training batch 751, loss: 0.170, 24032/60000 datapoints
2025-03-06 19:34:50,141 - INFO - training batch 801, loss: 0.182, 25632/60000 datapoints
2025-03-06 19:34:50,334 - INFO - training batch 851, loss: 0.204, 27232/60000 datapoints
2025-03-06 19:34:50,530 - INFO - training batch 901, loss: 0.467, 28832/60000 datapoints
2025-03-06 19:34:50,724 - INFO - training batch 951, loss: 0.106, 30432/60000 datapoints
2025-03-06 19:34:50,918 - INFO - training batch 1001, loss: 0.259, 32032/60000 datapoints
2025-03-06 19:34:51,113 - INFO - training batch 1051, loss: 0.439, 33632/60000 datapoints
2025-03-06 19:34:51,306 - INFO - training batch 1101, loss: 0.167, 35232/60000 datapoints
2025-03-06 19:34:51,505 - INFO - training batch 1151, loss: 0.100, 36832/60000 datapoints
2025-03-06 19:34:51,709 - INFO - training batch 1201, loss: 0.135, 38432/60000 datapoints
2025-03-06 19:34:51,907 - INFO - training batch 1251, loss: 0.272, 40032/60000 datapoints
2025-03-06 19:34:52,103 - INFO - training batch 1301, loss: 0.278, 41632/60000 datapoints
2025-03-06 19:34:52,297 - INFO - training batch 1351, loss: 0.565, 43232/60000 datapoints
2025-03-06 19:34:52,494 - INFO - training batch 1401, loss: 0.196, 44832/60000 datapoints
2025-03-06 19:34:52,689 - INFO - training batch 1451, loss: 0.195, 46432/60000 datapoints
2025-03-06 19:34:52,884 - INFO - training batch 1501, loss: 0.230, 48032/60000 datapoints
2025-03-06 19:34:53,079 - INFO - training batch 1551, loss: 0.182, 49632/60000 datapoints
2025-03-06 19:34:53,274 - INFO - training batch 1601, loss: 0.361, 51232/60000 datapoints
2025-03-06 19:34:53,469 - INFO - training batch 1651, loss: 0.431, 52832/60000 datapoints
2025-03-06 19:34:53,669 - INFO - training batch 1701, loss: 0.243, 54432/60000 datapoints
2025-03-06 19:34:53,865 - INFO - training batch 1751, loss: 0.227, 56032/60000 datapoints
2025-03-06 19:34:54,059 - INFO - training batch 1801, loss: 0.286, 57632/60000 datapoints
2025-03-06 19:34:54,254 - INFO - training batch 1851, loss: 0.209, 59232/60000 datapoints
2025-03-06 19:34:54,354 - INFO - validation batch 1, loss: 0.208, 32/10016 datapoints
2025-03-06 19:34:54,507 - INFO - validation batch 51, loss: 0.266, 1632/10016 datapoints
2025-03-06 19:34:54,664 - INFO - validation batch 101, loss: 0.347, 3232/10016 datapoints
2025-03-06 19:34:54,815 - INFO - validation batch 151, loss: 0.248, 4832/10016 datapoints
2025-03-06 19:34:54,970 - INFO - validation batch 201, loss: 0.369, 6432/10016 datapoints
2025-03-06 19:34:55,123 - INFO - validation batch 251, loss: 0.503, 8032/10016 datapoints
2025-03-06 19:34:55,290 - INFO - validation batch 301, loss: 0.288, 9632/10016 datapoints
2025-03-06 19:34:55,330 - INFO - Epoch 385/800 done.
2025-03-06 19:34:55,331 - INFO - Final validation performance:
Loss: 0.318, top-1 acc: 0.919top-5 acc: 0.919
2025-03-06 19:34:55,331 - INFO - Beginning epoch 386/800
2025-03-06 19:34:55,338 - INFO - training batch 1, loss: 0.394, 32/60000 datapoints
2025-03-06 19:34:55,555 - INFO - training batch 51, loss: 0.109, 1632/60000 datapoints
2025-03-06 19:34:55,754 - INFO - training batch 101, loss: 0.133, 3232/60000 datapoints
2025-03-06 19:34:55,949 - INFO - training batch 151, loss: 0.266, 4832/60000 datapoints
2025-03-06 19:34:56,151 - INFO - training batch 201, loss: 0.342, 6432/60000 datapoints
2025-03-06 19:34:56,348 - INFO - training batch 251, loss: 0.226, 8032/60000 datapoints
2025-03-06 19:34:56,548 - INFO - training batch 301, loss: 0.243, 9632/60000 datapoints
2025-03-06 19:34:56,744 - INFO - training batch 351, loss: 0.132, 11232/60000 datapoints
2025-03-06 19:34:56,940 - INFO - training batch 401, loss: 0.229, 12832/60000 datapoints
2025-03-06 19:34:57,135 - INFO - training batch 451, loss: 0.446, 14432/60000 datapoints
2025-03-06 19:34:57,329 - INFO - training batch 501, loss: 0.241, 16032/60000 datapoints
2025-03-06 19:34:57,522 - INFO - training batch 551, loss: 0.272, 17632/60000 datapoints
2025-03-06 19:34:57,723 - INFO - training batch 601, loss: 0.310, 19232/60000 datapoints
2025-03-06 19:34:57,920 - INFO - training batch 651, loss: 0.211, 20832/60000 datapoints
2025-03-06 19:34:58,114 - INFO - training batch 701, loss: 0.283, 22432/60000 datapoints
2025-03-06 19:34:58,313 - INFO - training batch 751, loss: 0.116, 24032/60000 datapoints
2025-03-06 19:34:58,512 - INFO - training batch 801, loss: 0.259, 25632/60000 datapoints
2025-03-06 19:34:58,730 - INFO - training batch 851, loss: 0.257, 27232/60000 datapoints
2025-03-06 19:34:58,926 - INFO - training batch 901, loss: 0.323, 28832/60000 datapoints
2025-03-06 19:34:59,119 - INFO - training batch 951, loss: 0.239, 30432/60000 datapoints
2025-03-06 19:34:59,316 - INFO - training batch 1001, loss: 0.345, 32032/60000 datapoints
2025-03-06 19:34:59,511 - INFO - training batch 1051, loss: 0.164, 33632/60000 datapoints
2025-03-06 19:34:59,709 - INFO - training batch 1101, loss: 0.232, 35232/60000 datapoints
2025-03-06 19:34:59,903 - INFO - training batch 1151, loss: 0.402, 36832/60000 datapoints
2025-03-06 19:35:00,098 - INFO - training batch 1201, loss: 0.377, 38432/60000 datapoints
2025-03-06 19:35:00,295 - INFO - training batch 1251, loss: 0.372, 40032/60000 datapoints
2025-03-06 19:35:00,490 - INFO - training batch 1301, loss: 0.525, 41632/60000 datapoints
2025-03-06 19:35:00,685 - INFO - training batch 1351, loss: 0.319, 43232/60000 datapoints
2025-03-06 19:35:00,878 - INFO - training batch 1401, loss: 0.208, 44832/60000 datapoints
2025-03-06 19:35:01,074 - INFO - training batch 1451, loss: 0.186, 46432/60000 datapoints
2025-03-06 19:35:01,268 - INFO - training batch 1501, loss: 0.109, 48032/60000 datapoints
2025-03-06 19:35:01,469 - INFO - training batch 1551, loss: 0.155, 49632/60000 datapoints
2025-03-06 19:35:01,667 - INFO - training batch 1601, loss: 0.880, 51232/60000 datapoints
2025-03-06 19:35:01,863 - INFO - training batch 1651, loss: 0.165, 52832/60000 datapoints
2025-03-06 19:35:02,055 - INFO - training batch 1701, loss: 0.164, 54432/60000 datapoints
2025-03-06 19:35:02,250 - INFO - training batch 1751, loss: 0.248, 56032/60000 datapoints
2025-03-06 19:35:02,445 - INFO - training batch 1801, loss: 0.184, 57632/60000 datapoints
2025-03-06 19:35:02,640 - INFO - training batch 1851, loss: 0.156, 59232/60000 datapoints
2025-03-06 19:35:02,740 - INFO - validation batch 1, loss: 0.243, 32/10016 datapoints
2025-03-06 19:35:02,891 - INFO - validation batch 51, loss: 0.242, 1632/10016 datapoints
2025-03-06 19:35:03,044 - INFO - validation batch 101, loss: 0.137, 3232/10016 datapoints
2025-03-06 19:35:03,196 - INFO - validation batch 151, loss: 0.605, 4832/10016 datapoints
2025-03-06 19:35:03,349 - INFO - validation batch 201, loss: 0.630, 6432/10016 datapoints
2025-03-06 19:35:03,500 - INFO - validation batch 251, loss: 0.242, 8032/10016 datapoints
2025-03-06 19:35:03,658 - INFO - validation batch 301, loss: 0.341, 9632/10016 datapoints
2025-03-06 19:35:03,697 - INFO - Epoch 386/800 done.
2025-03-06 19:35:03,697 - INFO - Final validation performance:
Loss: 0.349, top-1 acc: 0.920top-5 acc: 0.920
2025-03-06 19:35:03,697 - INFO - Beginning epoch 387/800
2025-03-06 19:35:03,704 - INFO - training batch 1, loss: 0.328, 32/60000 datapoints
2025-03-06 19:35:03,920 - INFO - training batch 51, loss: 0.519, 1632/60000 datapoints
2025-03-06 19:35:04,114 - INFO - training batch 101, loss: 0.416, 3232/60000 datapoints
2025-03-06 19:35:04,321 - INFO - training batch 151, loss: 0.131, 4832/60000 datapoints
2025-03-06 19:35:04,517 - INFO - training batch 201, loss: 0.218, 6432/60000 datapoints
2025-03-06 19:35:04,719 - INFO - training batch 251, loss: 0.365, 8032/60000 datapoints
2025-03-06 19:35:04,923 - INFO - training batch 301, loss: 0.205, 9632/60000 datapoints
2025-03-06 19:35:05,117 - INFO - training batch 351, loss: 0.193, 11232/60000 datapoints
2025-03-06 19:35:05,317 - INFO - training batch 401, loss: 0.081, 12832/60000 datapoints
2025-03-06 19:35:05,512 - INFO - training batch 451, loss: 0.406, 14432/60000 datapoints
2025-03-06 19:35:05,712 - INFO - training batch 501, loss: 0.422, 16032/60000 datapoints
2025-03-06 19:35:05,908 - INFO - training batch 551, loss: 0.319, 17632/60000 datapoints
2025-03-06 19:35:06,103 - INFO - training batch 601, loss: 0.388, 19232/60000 datapoints
2025-03-06 19:35:06,298 - INFO - training batch 651, loss: 0.162, 20832/60000 datapoints
2025-03-06 19:35:06,491 - INFO - training batch 701, loss: 0.340, 22432/60000 datapoints
2025-03-06 19:35:06,686 - INFO - training batch 751, loss: 0.377, 24032/60000 datapoints
2025-03-06 19:35:06,886 - INFO - training batch 801, loss: 0.316, 25632/60000 datapoints
2025-03-06 19:35:07,081 - INFO - training batch 851, loss: 0.252, 27232/60000 datapoints
2025-03-06 19:35:07,276 - INFO - training batch 901, loss: 0.204, 28832/60000 datapoints
2025-03-06 19:35:07,470 - INFO - training batch 951, loss: 0.504, 30432/60000 datapoints
2025-03-06 19:35:07,672 - INFO - training batch 1001, loss: 0.343, 32032/60000 datapoints
2025-03-06 19:35:07,866 - INFO - training batch 1051, loss: 0.363, 33632/60000 datapoints
2025-03-06 19:35:08,063 - INFO - training batch 1101, loss: 0.396, 35232/60000 datapoints
2025-03-06 19:35:08,257 - INFO - training batch 1151, loss: 0.519, 36832/60000 datapoints
2025-03-06 19:35:08,450 - INFO - training batch 1201, loss: 0.277, 38432/60000 datapoints
2025-03-06 19:35:08,648 - INFO - training batch 1251, loss: 0.092, 40032/60000 datapoints
2025-03-06 19:35:08,866 - INFO - training batch 1301, loss: 0.157, 41632/60000 datapoints
2025-03-06 19:35:09,062 - INFO - training batch 1351, loss: 0.160, 43232/60000 datapoints
2025-03-06 19:35:09,254 - INFO - training batch 1401, loss: 0.314, 44832/60000 datapoints
2025-03-06 19:35:09,449 - INFO - training batch 1451, loss: 0.110, 46432/60000 datapoints
2025-03-06 19:35:09,648 - INFO - training batch 1501, loss: 0.478, 48032/60000 datapoints
2025-03-06 19:35:09,842 - INFO - training batch 1551, loss: 0.214, 49632/60000 datapoints
2025-03-06 19:35:10,039 - INFO - training batch 1601, loss: 0.375, 51232/60000 datapoints
2025-03-06 19:35:10,231 - INFO - training batch 1651, loss: 0.150, 52832/60000 datapoints
2025-03-06 19:35:10,430 - INFO - training batch 1701, loss: 0.221, 54432/60000 datapoints
2025-03-06 19:35:10,627 - INFO - training batch 1751, loss: 0.289, 56032/60000 datapoints
2025-03-06 19:35:10,823 - INFO - training batch 1801, loss: 0.231, 57632/60000 datapoints
2025-03-06 19:35:11,017 - INFO - training batch 1851, loss: 0.321, 59232/60000 datapoints
2025-03-06 19:35:11,129 - INFO - validation batch 1, loss: 0.153, 32/10016 datapoints
2025-03-06 19:35:11,280 - INFO - validation batch 51, loss: 0.447, 1632/10016 datapoints
2025-03-06 19:35:11,433 - INFO - validation batch 101, loss: 0.472, 3232/10016 datapoints
2025-03-06 19:35:11,590 - INFO - validation batch 151, loss: 0.189, 4832/10016 datapoints
2025-03-06 19:35:11,745 - INFO - validation batch 201, loss: 0.389, 6432/10016 datapoints
2025-03-06 19:35:11,898 - INFO - validation batch 251, loss: 0.227, 8032/10016 datapoints
2025-03-06 19:35:12,049 - INFO - validation batch 301, loss: 0.341, 9632/10016 datapoints
2025-03-06 19:35:12,086 - INFO - Epoch 387/800 done.
2025-03-06 19:35:12,087 - INFO - Final validation performance:
Loss: 0.317, top-1 acc: 0.920top-5 acc: 0.920
2025-03-06 19:35:12,087 - INFO - Beginning epoch 388/800
2025-03-06 19:35:12,094 - INFO - training batch 1, loss: 0.339, 32/60000 datapoints
2025-03-06 19:35:12,289 - INFO - training batch 51, loss: 0.134, 1632/60000 datapoints
2025-03-06 19:35:12,487 - INFO - training batch 101, loss: 0.401, 3232/60000 datapoints
2025-03-06 19:35:12,694 - INFO - training batch 151, loss: 0.078, 4832/60000 datapoints
2025-03-06 19:35:12,889 - INFO - training batch 201, loss: 0.087, 6432/60000 datapoints
2025-03-06 19:35:13,088 - INFO - training batch 251, loss: 0.190, 8032/60000 datapoints
2025-03-06 19:35:13,288 - INFO - training batch 301, loss: 0.150, 9632/60000 datapoints
2025-03-06 19:35:13,487 - INFO - training batch 351, loss: 0.291, 11232/60000 datapoints
2025-03-06 19:35:13,687 - INFO - training batch 401, loss: 0.538, 12832/60000 datapoints
2025-03-06 19:35:13,882 - INFO - training batch 451, loss: 0.118, 14432/60000 datapoints
2025-03-06 19:35:14,077 - INFO - training batch 501, loss: 0.321, 16032/60000 datapoints
2025-03-06 19:35:14,274 - INFO - training batch 551, loss: 0.237, 17632/60000 datapoints
2025-03-06 19:35:14,469 - INFO - training batch 601, loss: 0.183, 19232/60000 datapoints
2025-03-06 19:35:14,667 - INFO - training batch 651, loss: 0.294, 20832/60000 datapoints
2025-03-06 19:35:14,864 - INFO - training batch 701, loss: 0.115, 22432/60000 datapoints
2025-03-06 19:35:15,063 - INFO - training batch 751, loss: 0.165, 24032/60000 datapoints
2025-03-06 19:35:15,267 - INFO - training batch 801, loss: 0.311, 25632/60000 datapoints
2025-03-06 19:35:15,464 - INFO - training batch 851, loss: 0.395, 27232/60000 datapoints
2025-03-06 19:35:15,664 - INFO - training batch 901, loss: 0.174, 28832/60000 datapoints
2025-03-06 19:35:15,859 - INFO - training batch 951, loss: 0.439, 30432/60000 datapoints
2025-03-06 19:35:16,054 - INFO - training batch 1001, loss: 0.356, 32032/60000 datapoints
2025-03-06 19:35:16,246 - INFO - training batch 1051, loss: 0.165, 33632/60000 datapoints
2025-03-06 19:35:16,441 - INFO - training batch 1101, loss: 0.620, 35232/60000 datapoints
2025-03-06 19:35:16,639 - INFO - training batch 1151, loss: 0.277, 36832/60000 datapoints
2025-03-06 19:35:16,832 - INFO - training batch 1201, loss: 0.150, 38432/60000 datapoints
2025-03-06 19:35:17,025 - INFO - training batch 1251, loss: 0.369, 40032/60000 datapoints
2025-03-06 19:35:17,218 - INFO - training batch 1301, loss: 0.545, 41632/60000 datapoints
2025-03-06 19:35:17,413 - INFO - training batch 1351, loss: 0.545, 43232/60000 datapoints
2025-03-06 19:35:17,609 - INFO - training batch 1401, loss: 0.310, 44832/60000 datapoints
2025-03-06 19:35:17,803 - INFO - training batch 1451, loss: 0.353, 46432/60000 datapoints
2025-03-06 19:35:17,997 - INFO - training batch 1501, loss: 0.187, 48032/60000 datapoints
2025-03-06 19:35:18,191 - INFO - training batch 1551, loss: 0.170, 49632/60000 datapoints
2025-03-06 19:35:18,381 - INFO - training batch 1601, loss: 0.250, 51232/60000 datapoints
2025-03-06 19:35:18,577 - INFO - training batch 1651, loss: 0.358, 52832/60000 datapoints
2025-03-06 19:35:18,777 - INFO - training batch 1701, loss: 0.078, 54432/60000 datapoints
2025-03-06 19:35:18,988 - INFO - training batch 1751, loss: 0.278, 56032/60000 datapoints
2025-03-06 19:35:19,179 - INFO - training batch 1801, loss: 0.094, 57632/60000 datapoints
2025-03-06 19:35:19,373 - INFO - training batch 1851, loss: 0.216, 59232/60000 datapoints
2025-03-06 19:35:19,475 - INFO - validation batch 1, loss: 0.100, 32/10016 datapoints
2025-03-06 19:35:19,631 - INFO - validation batch 51, loss: 0.341, 1632/10016 datapoints
2025-03-06 19:35:19,782 - INFO - validation batch 101, loss: 0.415, 3232/10016 datapoints
2025-03-06 19:35:19,935 - INFO - validation batch 151, loss: 0.135, 4832/10016 datapoints
2025-03-06 19:35:20,088 - INFO - validation batch 201, loss: 0.135, 6432/10016 datapoints
2025-03-06 19:35:20,240 - INFO - validation batch 251, loss: 0.306, 8032/10016 datapoints
2025-03-06 19:35:20,393 - INFO - validation batch 301, loss: 0.132, 9632/10016 datapoints
2025-03-06 19:35:20,433 - INFO - Epoch 388/800 done.
2025-03-06 19:35:20,433 - INFO - Final validation performance:
Loss: 0.224, top-1 acc: 0.920top-5 acc: 0.920
2025-03-06 19:35:20,434 - INFO - Beginning epoch 389/800
2025-03-06 19:35:20,440 - INFO - training batch 1, loss: 0.161, 32/60000 datapoints
2025-03-06 19:35:20,647 - INFO - training batch 51, loss: 0.161, 1632/60000 datapoints
2025-03-06 19:35:20,840 - INFO - training batch 101, loss: 0.141, 3232/60000 datapoints
2025-03-06 19:35:21,040 - INFO - training batch 151, loss: 0.206, 4832/60000 datapoints
2025-03-06 19:35:21,237 - INFO - training batch 201, loss: 0.249, 6432/60000 datapoints
2025-03-06 19:35:21,438 - INFO - training batch 251, loss: 0.410, 8032/60000 datapoints
2025-03-06 19:35:21,642 - INFO - training batch 301, loss: 0.114, 9632/60000 datapoints
2025-03-06 19:35:21,837 - INFO - training batch 351, loss: 0.415, 11232/60000 datapoints
2025-03-06 19:35:22,031 - INFO - training batch 401, loss: 0.189, 12832/60000 datapoints
2025-03-06 19:35:22,228 - INFO - training batch 451, loss: 0.196, 14432/60000 datapoints
2025-03-06 19:35:22,430 - INFO - training batch 501, loss: 0.236, 16032/60000 datapoints
2025-03-06 19:35:22,629 - INFO - training batch 551, loss: 0.312, 17632/60000 datapoints
2025-03-06 19:35:22,823 - INFO - training batch 601, loss: 0.187, 19232/60000 datapoints
2025-03-06 19:35:23,017 - INFO - training batch 651, loss: 0.137, 20832/60000 datapoints
2025-03-06 19:35:23,214 - INFO - training batch 701, loss: 0.471, 22432/60000 datapoints
2025-03-06 19:35:23,409 - INFO - training batch 751, loss: 0.238, 24032/60000 datapoints
2025-03-06 19:35:23,607 - INFO - training batch 801, loss: 0.555, 25632/60000 datapoints
2025-03-06 19:35:23,808 - INFO - training batch 851, loss: 0.195, 27232/60000 datapoints
2025-03-06 19:35:24,001 - INFO - training batch 901, loss: 0.198, 28832/60000 datapoints
2025-03-06 19:35:24,195 - INFO - training batch 951, loss: 0.292, 30432/60000 datapoints
2025-03-06 19:35:24,387 - INFO - training batch 1001, loss: 0.154, 32032/60000 datapoints
2025-03-06 19:35:24,580 - INFO - training batch 1051, loss: 0.323, 33632/60000 datapoints
2025-03-06 19:35:24,775 - INFO - training batch 1101, loss: 0.488, 35232/60000 datapoints
2025-03-06 19:35:24,974 - INFO - training batch 1151, loss: 0.269, 36832/60000 datapoints
2025-03-06 19:35:25,167 - INFO - training batch 1201, loss: 0.329, 38432/60000 datapoints
2025-03-06 19:35:25,365 - INFO - training batch 1251, loss: 0.583, 40032/60000 datapoints
2025-03-06 19:35:25,559 - INFO - training batch 1301, loss: 0.384, 41632/60000 datapoints
2025-03-06 19:35:25,760 - INFO - training batch 1351, loss: 0.144, 43232/60000 datapoints
2025-03-06 19:35:25,955 - INFO - training batch 1401, loss: 0.294, 44832/60000 datapoints
2025-03-06 19:35:26,151 - INFO - training batch 1451, loss: 0.093, 46432/60000 datapoints
2025-03-06 19:35:26,345 - INFO - training batch 1501, loss: 0.299, 48032/60000 datapoints
2025-03-06 19:35:26,540 - INFO - training batch 1551, loss: 0.730, 49632/60000 datapoints
2025-03-06 19:35:26,735 - INFO - training batch 1601, loss: 0.127, 51232/60000 datapoints
2025-03-06 19:35:26,930 - INFO - training batch 1651, loss: 0.270, 52832/60000 datapoints
2025-03-06 19:35:27,126 - INFO - training batch 1701, loss: 0.240, 54432/60000 datapoints
2025-03-06 19:35:27,319 - INFO - training batch 1751, loss: 0.613, 56032/60000 datapoints
2025-03-06 19:35:27,514 - INFO - training batch 1801, loss: 0.386, 57632/60000 datapoints
2025-03-06 19:35:27,714 - INFO - training batch 1851, loss: 0.529, 59232/60000 datapoints
2025-03-06 19:35:27,816 - INFO - validation batch 1, loss: 0.129, 32/10016 datapoints
2025-03-06 19:35:27,967 - INFO - validation batch 51, loss: 0.424, 1632/10016 datapoints
2025-03-06 19:35:28,120 - INFO - validation batch 101, loss: 0.220, 3232/10016 datapoints
2025-03-06 19:35:28,275 - INFO - validation batch 151, loss: 0.278, 4832/10016 datapoints
2025-03-06 19:35:28,426 - INFO - validation batch 201, loss: 0.419, 6432/10016 datapoints
2025-03-06 19:35:28,581 - INFO - validation batch 251, loss: 0.258, 8032/10016 datapoints
2025-03-06 19:35:28,734 - INFO - validation batch 301, loss: 0.346, 9632/10016 datapoints
2025-03-06 19:35:28,772 - INFO - Epoch 389/800 done.
2025-03-06 19:35:28,772 - INFO - Final validation performance:
Loss: 0.296, top-1 acc: 0.920top-5 acc: 0.920
2025-03-06 19:35:28,772 - INFO - Beginning epoch 390/800
2025-03-06 19:35:28,779 - INFO - training batch 1, loss: 0.211, 32/60000 datapoints
2025-03-06 19:35:28,991 - INFO - training batch 51, loss: 0.263, 1632/60000 datapoints
2025-03-06 19:35:29,190 - INFO - training batch 101, loss: 0.302, 3232/60000 datapoints
2025-03-06 19:35:29,396 - INFO - training batch 151, loss: 0.286, 4832/60000 datapoints
2025-03-06 19:35:29,590 - INFO - training batch 201, loss: 0.140, 6432/60000 datapoints
2025-03-06 19:35:29,793 - INFO - training batch 251, loss: 0.264, 8032/60000 datapoints
2025-03-06 19:35:29,990 - INFO - training batch 301, loss: 0.095, 9632/60000 datapoints
2025-03-06 19:35:30,187 - INFO - training batch 351, loss: 0.300, 11232/60000 datapoints
2025-03-06 19:35:30,382 - INFO - training batch 401, loss: 0.430, 12832/60000 datapoints
2025-03-06 19:35:30,578 - INFO - training batch 451, loss: 0.259, 14432/60000 datapoints
2025-03-06 19:35:30,779 - INFO - training batch 501, loss: 0.190, 16032/60000 datapoints
2025-03-06 19:35:30,972 - INFO - training batch 551, loss: 0.435, 17632/60000 datapoints
2025-03-06 19:35:31,172 - INFO - training batch 601, loss: 0.289, 19232/60000 datapoints
2025-03-06 19:35:31,368 - INFO - training batch 651, loss: 0.415, 20832/60000 datapoints
2025-03-06 19:35:31,564 - INFO - training batch 701, loss: 0.285, 22432/60000 datapoints
2025-03-06 19:35:31,766 - INFO - training batch 751, loss: 0.468, 24032/60000 datapoints
2025-03-06 19:35:31,960 - INFO - training batch 801, loss: 0.346, 25632/60000 datapoints
2025-03-06 19:35:32,158 - INFO - training batch 851, loss: 0.337, 27232/60000 datapoints
2025-03-06 19:35:32,379 - INFO - training batch 901, loss: 0.185, 28832/60000 datapoints
2025-03-06 19:35:32,574 - INFO - training batch 951, loss: 0.389, 30432/60000 datapoints
2025-03-06 19:35:32,767 - INFO - training batch 1001, loss: 0.373, 32032/60000 datapoints
2025-03-06 19:35:32,962 - INFO - training batch 1051, loss: 0.394, 33632/60000 datapoints
2025-03-06 19:35:33,158 - INFO - training batch 1101, loss: 0.260, 35232/60000 datapoints
2025-03-06 19:35:33,350 - INFO - training batch 1151, loss: 0.113, 36832/60000 datapoints
2025-03-06 19:35:33,543 - INFO - training batch 1201, loss: 0.142, 38432/60000 datapoints
2025-03-06 19:35:33,742 - INFO - training batch 1251, loss: 0.319, 40032/60000 datapoints
2025-03-06 19:35:33,935 - INFO - training batch 1301, loss: 0.290, 41632/60000 datapoints
2025-03-06 19:35:34,130 - INFO - training batch 1351, loss: 0.264, 43232/60000 datapoints
2025-03-06 19:35:34,324 - INFO - training batch 1401, loss: 0.358, 44832/60000 datapoints
2025-03-06 19:35:34,518 - INFO - training batch 1451, loss: 0.598, 46432/60000 datapoints
2025-03-06 19:35:34,716 - INFO - training batch 1501, loss: 0.490, 48032/60000 datapoints
2025-03-06 19:35:34,912 - INFO - training batch 1551, loss: 0.315, 49632/60000 datapoints
2025-03-06 19:35:35,106 - INFO - training batch 1601, loss: 0.241, 51232/60000 datapoints
2025-03-06 19:35:35,302 - INFO - training batch 1651, loss: 0.381, 52832/60000 datapoints
2025-03-06 19:35:35,506 - INFO - training batch 1701, loss: 0.382, 54432/60000 datapoints
2025-03-06 19:35:35,707 - INFO - training batch 1751, loss: 0.431, 56032/60000 datapoints
2025-03-06 19:35:35,901 - INFO - training batch 1801, loss: 0.176, 57632/60000 datapoints
2025-03-06 19:35:36,097 - INFO - training batch 1851, loss: 0.429, 59232/60000 datapoints
2025-03-06 19:35:36,199 - INFO - validation batch 1, loss: 0.174, 32/10016 datapoints
2025-03-06 19:35:36,351 - INFO - validation batch 51, loss: 0.305, 1632/10016 datapoints
2025-03-06 19:35:36,505 - INFO - validation batch 101, loss: 0.168, 3232/10016 datapoints
2025-03-06 19:35:36,659 - INFO - validation batch 151, loss: 0.139, 4832/10016 datapoints
2025-03-06 19:35:36,810 - INFO - validation batch 201, loss: 0.324, 6432/10016 datapoints
2025-03-06 19:35:36,962 - INFO - validation batch 251, loss: 0.195, 8032/10016 datapoints
2025-03-06 19:35:37,113 - INFO - validation batch 301, loss: 0.200, 9632/10016 datapoints
2025-03-06 19:35:37,152 - INFO - Epoch 390/800 done.
2025-03-06 19:35:37,152 - INFO - Final validation performance:
Loss: 0.215, top-1 acc: 0.920top-5 acc: 0.920
2025-03-06 19:35:37,153 - INFO - Beginning epoch 391/800
2025-03-06 19:35:37,160 - INFO - training batch 1, loss: 0.215, 32/60000 datapoints
2025-03-06 19:35:37,366 - INFO - training batch 51, loss: 0.323, 1632/60000 datapoints
2025-03-06 19:35:37,573 - INFO - training batch 101, loss: 0.186, 3232/60000 datapoints
2025-03-06 19:35:37,797 - INFO - training batch 151, loss: 0.248, 4832/60000 datapoints
2025-03-06 19:35:37,997 - INFO - training batch 201, loss: 0.340, 6432/60000 datapoints
2025-03-06 19:35:38,194 - INFO - training batch 251, loss: 0.256, 8032/60000 datapoints
2025-03-06 19:35:38,386 - INFO - training batch 301, loss: 0.081, 9632/60000 datapoints
2025-03-06 19:35:38,581 - INFO - training batch 351, loss: 0.225, 11232/60000 datapoints
2025-03-06 19:35:38,777 - INFO - training batch 401, loss: 0.112, 12832/60000 datapoints
2025-03-06 19:35:38,987 - INFO - training batch 451, loss: 0.533, 14432/60000 datapoints
2025-03-06 19:35:39,189 - INFO - training batch 501, loss: 0.129, 16032/60000 datapoints
2025-03-06 19:35:39,380 - INFO - training batch 551, loss: 0.237, 17632/60000 datapoints
2025-03-06 19:35:39,573 - INFO - training batch 601, loss: 0.160, 19232/60000 datapoints
2025-03-06 19:35:39,776 - INFO - training batch 651, loss: 0.179, 20832/60000 datapoints
2025-03-06 19:35:39,971 - INFO - training batch 701, loss: 0.319, 22432/60000 datapoints
2025-03-06 19:35:40,167 - INFO - training batch 751, loss: 0.242, 24032/60000 datapoints
2025-03-06 19:35:40,359 - INFO - training batch 801, loss: 0.240, 25632/60000 datapoints
2025-03-06 19:35:40,553 - INFO - training batch 851, loss: 0.266, 27232/60000 datapoints
2025-03-06 19:35:40,749 - INFO - training batch 901, loss: 0.292, 28832/60000 datapoints
2025-03-06 19:35:40,942 - INFO - training batch 951, loss: 0.340, 30432/60000 datapoints
2025-03-06 19:35:41,135 - INFO - training batch 1001, loss: 0.159, 32032/60000 datapoints
2025-03-06 19:35:41,329 - INFO - training batch 1051, loss: 0.344, 33632/60000 datapoints
2025-03-06 19:35:41,522 - INFO - training batch 1101, loss: 0.333, 35232/60000 datapoints
2025-03-06 19:35:41,721 - INFO - training batch 1151, loss: 0.234, 36832/60000 datapoints
2025-03-06 19:35:41,915 - INFO - training batch 1201, loss: 0.267, 38432/60000 datapoints
2025-03-06 19:35:42,108 - INFO - training batch 1251, loss: 0.163, 40032/60000 datapoints
2025-03-06 19:35:42,300 - INFO - training batch 1301, loss: 0.216, 41632/60000 datapoints
2025-03-06 19:35:42,493 - INFO - training batch 1351, loss: 0.316, 43232/60000 datapoints
2025-03-06 19:35:42,689 - INFO - training batch 1401, loss: 0.258, 44832/60000 datapoints
2025-03-06 19:35:42,884 - INFO - training batch 1451, loss: 0.327, 46432/60000 datapoints
2025-03-06 19:35:43,074 - INFO - training batch 1501, loss: 0.180, 48032/60000 datapoints
2025-03-06 19:35:43,267 - INFO - training batch 1551, loss: 0.290, 49632/60000 datapoints
2025-03-06 19:35:43,460 - INFO - training batch 1601, loss: 0.396, 51232/60000 datapoints
2025-03-06 19:35:43,657 - INFO - training batch 1651, loss: 0.224, 52832/60000 datapoints
2025-03-06 19:35:43,854 - INFO - training batch 1701, loss: 0.262, 54432/60000 datapoints
2025-03-06 19:35:44,047 - INFO - training batch 1751, loss: 0.553, 56032/60000 datapoints
2025-03-06 19:35:44,243 - INFO - training batch 1801, loss: 0.307, 57632/60000 datapoints
2025-03-06 19:35:44,435 - INFO - training batch 1851, loss: 0.309, 59232/60000 datapoints
2025-03-06 19:35:44,537 - INFO - validation batch 1, loss: 0.363, 32/10016 datapoints
2025-03-06 19:35:44,694 - INFO - validation batch 51, loss: 0.207, 1632/10016 datapoints
2025-03-06 19:35:44,845 - INFO - validation batch 101, loss: 0.130, 3232/10016 datapoints
2025-03-06 19:35:45,003 - INFO - validation batch 151, loss: 0.646, 4832/10016 datapoints
2025-03-06 19:35:45,156 - INFO - validation batch 201, loss: 0.285, 6432/10016 datapoints
2025-03-06 19:35:45,315 - INFO - validation batch 251, loss: 0.450, 8032/10016 datapoints
2025-03-06 19:35:45,468 - INFO - validation batch 301, loss: 0.095, 9632/10016 datapoints
2025-03-06 19:35:45,504 - INFO - Epoch 391/800 done.
2025-03-06 19:35:45,505 - INFO - Final validation performance:
Loss: 0.311, top-1 acc: 0.920top-5 acc: 0.920
2025-03-06 19:35:45,505 - INFO - Beginning epoch 392/800
2025-03-06 19:35:45,512 - INFO - training batch 1, loss: 0.112, 32/60000 datapoints
2025-03-06 19:35:45,735 - INFO - training batch 51, loss: 0.542, 1632/60000 datapoints
2025-03-06 19:35:45,930 - INFO - training batch 101, loss: 0.206, 3232/60000 datapoints
2025-03-06 19:35:46,132 - INFO - training batch 151, loss: 0.338, 4832/60000 datapoints
2025-03-06 19:35:46,330 - INFO - training batch 201, loss: 0.350, 6432/60000 datapoints
2025-03-06 19:35:46,528 - INFO - training batch 251, loss: 0.256, 8032/60000 datapoints
2025-03-06 19:35:46,727 - INFO - training batch 301, loss: 0.291, 9632/60000 datapoints
2025-03-06 19:35:46,920 - INFO - training batch 351, loss: 0.224, 11232/60000 datapoints
2025-03-06 19:35:47,115 - INFO - training batch 401, loss: 0.233, 12832/60000 datapoints
2025-03-06 19:35:47,311 - INFO - training batch 451, loss: 0.175, 14432/60000 datapoints
2025-03-06 19:35:47,503 - INFO - training batch 501, loss: 0.496, 16032/60000 datapoints
2025-03-06 19:35:47,698 - INFO - training batch 551, loss: 0.342, 17632/60000 datapoints
2025-03-06 19:35:47,897 - INFO - training batch 601, loss: 0.354, 19232/60000 datapoints
2025-03-06 19:35:48,098 - INFO - training batch 651, loss: 0.258, 20832/60000 datapoints
2025-03-06 19:35:48,293 - INFO - training batch 701, loss: 0.158, 22432/60000 datapoints
2025-03-06 19:35:48,487 - INFO - training batch 751, loss: 0.159, 24032/60000 datapoints
2025-03-06 19:35:48,683 - INFO - training batch 801, loss: 0.134, 25632/60000 datapoints
2025-03-06 19:35:48,879 - INFO - training batch 851, loss: 0.404, 27232/60000 datapoints
2025-03-06 19:35:49,092 - INFO - training batch 901, loss: 0.127, 28832/60000 datapoints
2025-03-06 19:35:49,289 - INFO - training batch 951, loss: 0.164, 30432/60000 datapoints
2025-03-06 19:35:49,481 - INFO - training batch 1001, loss: 0.422, 32032/60000 datapoints
2025-03-06 19:35:49,676 - INFO - training batch 1051, loss: 0.135, 33632/60000 datapoints
2025-03-06 19:35:49,872 - INFO - training batch 1101, loss: 0.234, 35232/60000 datapoints
2025-03-06 19:35:50,065 - INFO - training batch 1151, loss: 0.315, 36832/60000 datapoints
2025-03-06 19:35:50,257 - INFO - training batch 1201, loss: 0.375, 38432/60000 datapoints
2025-03-06 19:35:50,451 - INFO - training batch 1251, loss: 0.251, 40032/60000 datapoints
2025-03-06 19:35:50,647 - INFO - training batch 1301, loss: 0.389, 41632/60000 datapoints
2025-03-06 19:35:50,840 - INFO - training batch 1351, loss: 0.374, 43232/60000 datapoints
2025-03-06 19:35:51,034 - INFO - training batch 1401, loss: 0.374, 44832/60000 datapoints
2025-03-06 19:35:51,231 - INFO - training batch 1451, loss: 0.173, 46432/60000 datapoints
2025-03-06 19:35:51,427 - INFO - training batch 1501, loss: 0.320, 48032/60000 datapoints
2025-03-06 19:35:51,627 - INFO - training batch 1551, loss: 0.347, 49632/60000 datapoints
2025-03-06 19:35:51,824 - INFO - training batch 1601, loss: 0.304, 51232/60000 datapoints
2025-03-06 19:35:52,016 - INFO - training batch 1651, loss: 0.281, 52832/60000 datapoints
2025-03-06 19:35:52,209 - INFO - training batch 1701, loss: 0.203, 54432/60000 datapoints
2025-03-06 19:35:52,404 - INFO - training batch 1751, loss: 0.171, 56032/60000 datapoints
2025-03-06 19:35:52,599 - INFO - training batch 1801, loss: 0.177, 57632/60000 datapoints
2025-03-06 19:35:52,799 - INFO - training batch 1851, loss: 0.192, 59232/60000 datapoints
2025-03-06 19:35:52,901 - INFO - validation batch 1, loss: 0.333, 32/10016 datapoints
2025-03-06 19:35:53,054 - INFO - validation batch 51, loss: 0.207, 1632/10016 datapoints
2025-03-06 19:35:53,207 - INFO - validation batch 101, loss: 0.220, 3232/10016 datapoints
2025-03-06 19:35:53,363 - INFO - validation batch 151, loss: 0.213, 4832/10016 datapoints
2025-03-06 19:35:53,516 - INFO - validation batch 201, loss: 0.233, 6432/10016 datapoints
2025-03-06 19:35:53,673 - INFO - validation batch 251, loss: 0.338, 8032/10016 datapoints
2025-03-06 19:35:53,829 - INFO - validation batch 301, loss: 0.223, 9632/10016 datapoints
2025-03-06 19:35:53,865 - INFO - Epoch 392/800 done.
2025-03-06 19:35:53,865 - INFO - Final validation performance:
Loss: 0.252, top-1 acc: 0.920top-5 acc: 0.920
2025-03-06 19:35:53,865 - INFO - Beginning epoch 393/800
2025-03-06 19:35:53,872 - INFO - training batch 1, loss: 0.218, 32/60000 datapoints
2025-03-06 19:35:54,076 - INFO - training batch 51, loss: 0.458, 1632/60000 datapoints
2025-03-06 19:35:54,268 - INFO - training batch 101, loss: 0.243, 3232/60000 datapoints
2025-03-06 19:35:54,467 - INFO - training batch 151, loss: 0.233, 4832/60000 datapoints
2025-03-06 19:35:54,665 - INFO - training batch 201, loss: 0.312, 6432/60000 datapoints
2025-03-06 19:35:54,861 - INFO - training batch 251, loss: 0.253, 8032/60000 datapoints
2025-03-06 19:35:55,055 - INFO - training batch 301, loss: 0.251, 9632/60000 datapoints
2025-03-06 19:35:55,251 - INFO - training batch 351, loss: 0.097, 11232/60000 datapoints
2025-03-06 19:35:55,447 - INFO - training batch 401, loss: 0.271, 12832/60000 datapoints
2025-03-06 19:35:55,643 - INFO - training batch 451, loss: 0.208, 14432/60000 datapoints
2025-03-06 19:35:55,839 - INFO - training batch 501, loss: 0.269, 16032/60000 datapoints
2025-03-06 19:35:56,030 - INFO - training batch 551, loss: 0.099, 17632/60000 datapoints
2025-03-06 19:35:56,222 - INFO - training batch 601, loss: 0.269, 19232/60000 datapoints
2025-03-06 19:35:56,417 - INFO - training batch 651, loss: 0.324, 20832/60000 datapoints
2025-03-06 19:35:56,609 - INFO - training batch 701, loss: 0.304, 22432/60000 datapoints
2025-03-06 19:35:56,803 - INFO - training batch 751, loss: 0.127, 24032/60000 datapoints
2025-03-06 19:35:56,995 - INFO - training batch 801, loss: 0.214, 25632/60000 datapoints
2025-03-06 19:35:57,185 - INFO - training batch 851, loss: 0.471, 27232/60000 datapoints
2025-03-06 19:35:57,378 - INFO - training batch 901, loss: 0.130, 28832/60000 datapoints
2025-03-06 19:35:57,567 - INFO - training batch 951, loss: 0.305, 30432/60000 datapoints
2025-03-06 19:35:57,764 - INFO - training batch 1001, loss: 0.142, 32032/60000 datapoints
2025-03-06 19:35:57,957 - INFO - training batch 1051, loss: 0.121, 33632/60000 datapoints
2025-03-06 19:35:58,149 - INFO - training batch 1101, loss: 0.364, 35232/60000 datapoints
2025-03-06 19:35:58,341 - INFO - training batch 1151, loss: 0.126, 36832/60000 datapoints
2025-03-06 19:35:58,534 - INFO - training batch 1201, loss: 0.140, 38432/60000 datapoints
2025-03-06 19:35:58,729 - INFO - training batch 1251, loss: 0.200, 40032/60000 datapoints
2025-03-06 19:35:58,922 - INFO - training batch 1301, loss: 0.390, 41632/60000 datapoints
2025-03-06 19:35:59,127 - INFO - training batch 1351, loss: 0.202, 43232/60000 datapoints
2025-03-06 19:35:59,343 - INFO - training batch 1401, loss: 0.055, 44832/60000 datapoints
2025-03-06 19:35:59,537 - INFO - training batch 1451, loss: 0.122, 46432/60000 datapoints
2025-03-06 19:35:59,735 - INFO - training batch 1501, loss: 0.301, 48032/60000 datapoints
2025-03-06 19:35:59,935 - INFO - training batch 1551, loss: 0.183, 49632/60000 datapoints
2025-03-06 19:36:00,129 - INFO - training batch 1601, loss: 0.282, 51232/60000 datapoints
2025-03-06 19:36:00,321 - INFO - training batch 1651, loss: 0.192, 52832/60000 datapoints
2025-03-06 19:36:00,516 - INFO - training batch 1701, loss: 0.191, 54432/60000 datapoints
2025-03-06 19:36:00,710 - INFO - training batch 1751, loss: 0.203, 56032/60000 datapoints
2025-03-06 19:36:00,906 - INFO - training batch 1801, loss: 0.301, 57632/60000 datapoints
2025-03-06 19:36:01,100 - INFO - training batch 1851, loss: 0.326, 59232/60000 datapoints
2025-03-06 19:36:01,204 - INFO - validation batch 1, loss: 0.268, 32/10016 datapoints
2025-03-06 19:36:01,357 - INFO - validation batch 51, loss: 0.302, 1632/10016 datapoints
2025-03-06 19:36:01,515 - INFO - validation batch 101, loss: 0.243, 3232/10016 datapoints
2025-03-06 19:36:01,673 - INFO - validation batch 151, loss: 0.652, 4832/10016 datapoints
2025-03-06 19:36:01,828 - INFO - validation batch 201, loss: 0.217, 6432/10016 datapoints
2025-03-06 19:36:01,981 - INFO - validation batch 251, loss: 0.420, 8032/10016 datapoints
2025-03-06 19:36:02,136 - INFO - validation batch 301, loss: 0.087, 9632/10016 datapoints
2025-03-06 19:36:02,175 - INFO - Epoch 393/800 done.
2025-03-06 19:36:02,175 - INFO - Final validation performance:
Loss: 0.313, top-1 acc: 0.920top-5 acc: 0.920
2025-03-06 19:36:02,176 - INFO - Beginning epoch 394/800
2025-03-06 19:36:02,182 - INFO - training batch 1, loss: 0.181, 32/60000 datapoints
2025-03-06 19:36:02,377 - INFO - training batch 51, loss: 0.180, 1632/60000 datapoints
2025-03-06 19:36:02,573 - INFO - training batch 101, loss: 0.404, 3232/60000 datapoints
2025-03-06 19:36:02,779 - INFO - training batch 151, loss: 0.149, 4832/60000 datapoints
2025-03-06 19:36:02,974 - INFO - training batch 201, loss: 0.372, 6432/60000 datapoints
2025-03-06 19:36:03,173 - INFO - training batch 251, loss: 0.247, 8032/60000 datapoints
2025-03-06 19:36:03,371 - INFO - training batch 301, loss: 0.098, 9632/60000 datapoints
2025-03-06 19:36:03,571 - INFO - training batch 351, loss: 0.376, 11232/60000 datapoints
2025-03-06 19:36:03,768 - INFO - training batch 401, loss: 0.366, 12832/60000 datapoints
2025-03-06 19:36:03,965 - INFO - training batch 451, loss: 0.335, 14432/60000 datapoints
2025-03-06 19:36:04,160 - INFO - training batch 501, loss: 0.175, 16032/60000 datapoints
2025-03-06 19:36:04,356 - INFO - training batch 551, loss: 0.186, 17632/60000 datapoints
2025-03-06 19:36:04,549 - INFO - training batch 601, loss: 0.288, 19232/60000 datapoints
2025-03-06 19:36:04,744 - INFO - training batch 651, loss: 0.164, 20832/60000 datapoints
2025-03-06 19:36:04,943 - INFO - training batch 701, loss: 0.469, 22432/60000 datapoints
2025-03-06 19:36:05,140 - INFO - training batch 751, loss: 0.205, 24032/60000 datapoints
2025-03-06 19:36:05,338 - INFO - training batch 801, loss: 0.249, 25632/60000 datapoints
2025-03-06 19:36:05,536 - INFO - training batch 851, loss: 0.290, 27232/60000 datapoints
2025-03-06 19:36:05,731 - INFO - training batch 901, loss: 0.281, 28832/60000 datapoints
2025-03-06 19:36:05,925 - INFO - training batch 951, loss: 0.151, 30432/60000 datapoints
2025-03-06 19:36:06,119 - INFO - training batch 1001, loss: 0.305, 32032/60000 datapoints
2025-03-06 19:36:06,315 - INFO - training batch 1051, loss: 0.212, 33632/60000 datapoints
2025-03-06 19:36:06,512 - INFO - training batch 1101, loss: 0.246, 35232/60000 datapoints
2025-03-06 19:36:06,709 - INFO - training batch 1151, loss: 0.402, 36832/60000 datapoints
2025-03-06 19:36:06,902 - INFO - training batch 1201, loss: 0.192, 38432/60000 datapoints
2025-03-06 19:36:07,096 - INFO - training batch 1251, loss: 0.216, 40032/60000 datapoints
2025-03-06 19:36:07,290 - INFO - training batch 1301, loss: 0.430, 41632/60000 datapoints
2025-03-06 19:36:07,484 - INFO - training batch 1351, loss: 0.190, 43232/60000 datapoints
2025-03-06 19:36:07,683 - INFO - training batch 1401, loss: 0.316, 44832/60000 datapoints
2025-03-06 19:36:07,879 - INFO - training batch 1451, loss: 0.269, 46432/60000 datapoints
2025-03-06 19:36:08,077 - INFO - training batch 1501, loss: 0.143, 48032/60000 datapoints
2025-03-06 19:36:08,271 - INFO - training batch 1551, loss: 0.324, 49632/60000 datapoints
2025-03-06 19:36:08,464 - INFO - training batch 1601, loss: 0.189, 51232/60000 datapoints
2025-03-06 19:36:08,660 - INFO - training batch 1651, loss: 0.237, 52832/60000 datapoints
2025-03-06 19:36:08,856 - INFO - training batch 1701, loss: 0.348, 54432/60000 datapoints
2025-03-06 19:36:09,052 - INFO - training batch 1751, loss: 0.161, 56032/60000 datapoints
2025-03-06 19:36:09,259 - INFO - training batch 1801, loss: 0.148, 57632/60000 datapoints
2025-03-06 19:36:09,455 - INFO - training batch 1851, loss: 0.207, 59232/60000 datapoints
2025-03-06 19:36:09,555 - INFO - validation batch 1, loss: 0.251, 32/10016 datapoints
2025-03-06 19:36:09,709 - INFO - validation batch 51, loss: 0.117, 1632/10016 datapoints
2025-03-06 19:36:09,863 - INFO - validation batch 101, loss: 0.285, 3232/10016 datapoints
2025-03-06 19:36:10,018 - INFO - validation batch 151, loss: 0.222, 4832/10016 datapoints
2025-03-06 19:36:10,170 - INFO - validation batch 201, loss: 0.258, 6432/10016 datapoints
2025-03-06 19:36:10,322 - INFO - validation batch 251, loss: 0.314, 8032/10016 datapoints
2025-03-06 19:36:10,476 - INFO - validation batch 301, loss: 0.270, 9632/10016 datapoints
2025-03-06 19:36:10,511 - INFO - Epoch 394/800 done.
2025-03-06 19:36:10,512 - INFO - Final validation performance:
Loss: 0.246, top-1 acc: 0.920top-5 acc: 0.920
2025-03-06 19:36:10,512 - INFO - Beginning epoch 395/800
2025-03-06 19:36:10,521 - INFO - training batch 1, loss: 0.304, 32/60000 datapoints
2025-03-06 19:36:10,718 - INFO - training batch 51, loss: 0.336, 1632/60000 datapoints
2025-03-06 19:36:10,910 - INFO - training batch 101, loss: 0.633, 3232/60000 datapoints
2025-03-06 19:36:11,110 - INFO - training batch 151, loss: 0.162, 4832/60000 datapoints
2025-03-06 19:36:11,303 - INFO - training batch 201, loss: 0.108, 6432/60000 datapoints
2025-03-06 19:36:11,495 - INFO - training batch 251, loss: 0.532, 8032/60000 datapoints
2025-03-06 19:36:11,701 - INFO - training batch 301, loss: 0.178, 9632/60000 datapoints
2025-03-06 19:36:11,902 - INFO - training batch 351, loss: 0.274, 11232/60000 datapoints
2025-03-06 19:36:12,096 - INFO - training batch 401, loss: 0.256, 12832/60000 datapoints
2025-03-06 19:36:12,288 - INFO - training batch 451, loss: 0.190, 14432/60000 datapoints
2025-03-06 19:36:12,478 - INFO - training batch 501, loss: 0.147, 16032/60000 datapoints
2025-03-06 19:36:12,671 - INFO - training batch 551, loss: 0.114, 17632/60000 datapoints
2025-03-06 19:36:12,864 - INFO - training batch 601, loss: 0.328, 19232/60000 datapoints
2025-03-06 19:36:13,054 - INFO - training batch 651, loss: 0.239, 20832/60000 datapoints
2025-03-06 19:36:13,249 - INFO - training batch 701, loss: 0.144, 22432/60000 datapoints
2025-03-06 19:36:13,441 - INFO - training batch 751, loss: 0.268, 24032/60000 datapoints
2025-03-06 19:36:13,636 - INFO - training batch 801, loss: 0.243, 25632/60000 datapoints
2025-03-06 19:36:13,827 - INFO - training batch 851, loss: 0.166, 27232/60000 datapoints
2025-03-06 19:36:14,020 - INFO - training batch 901, loss: 0.238, 28832/60000 datapoints
2025-03-06 19:36:14,211 - INFO - training batch 951, loss: 0.262, 30432/60000 datapoints
2025-03-06 19:36:14,404 - INFO - training batch 1001, loss: 0.435, 32032/60000 datapoints
2025-03-06 19:36:14,594 - INFO - training batch 1051, loss: 0.110, 33632/60000 datapoints
2025-03-06 19:36:14,787 - INFO - training batch 1101, loss: 0.116, 35232/60000 datapoints
2025-03-06 19:36:14,982 - INFO - training batch 1151, loss: 0.271, 36832/60000 datapoints
2025-03-06 19:36:15,176 - INFO - training batch 1201, loss: 0.265, 38432/60000 datapoints
2025-03-06 19:36:15,370 - INFO - training batch 1251, loss: 0.578, 40032/60000 datapoints
2025-03-06 19:36:15,570 - INFO - training batch 1301, loss: 0.159, 41632/60000 datapoints
2025-03-06 19:36:15,764 - INFO - training batch 1351, loss: 0.256, 43232/60000 datapoints
2025-03-06 19:36:15,958 - INFO - training batch 1401, loss: 0.278, 44832/60000 datapoints
2025-03-06 19:36:16,150 - INFO - training batch 1451, loss: 0.229, 46432/60000 datapoints
2025-03-06 19:36:16,340 - INFO - training batch 1501, loss: 0.322, 48032/60000 datapoints
2025-03-06 19:36:16,529 - INFO - training batch 1551, loss: 0.134, 49632/60000 datapoints
2025-03-06 19:36:16,729 - INFO - training batch 1601, loss: 0.282, 51232/60000 datapoints
2025-03-06 19:36:16,924 - INFO - training batch 1651, loss: 0.152, 52832/60000 datapoints
2025-03-06 19:36:17,119 - INFO - training batch 1701, loss: 0.541, 54432/60000 datapoints
2025-03-06 19:36:17,311 - INFO - training batch 1751, loss: 0.443, 56032/60000 datapoints
2025-03-06 19:36:17,501 - INFO - training batch 1801, loss: 0.245, 57632/60000 datapoints
2025-03-06 19:36:17,698 - INFO - training batch 1851, loss: 0.545, 59232/60000 datapoints
2025-03-06 19:36:17,797 - INFO - validation batch 1, loss: 0.197, 32/10016 datapoints
2025-03-06 19:36:17,949 - INFO - validation batch 51, loss: 0.107, 1632/10016 datapoints
2025-03-06 19:36:18,102 - INFO - validation batch 101, loss: 0.544, 3232/10016 datapoints
2025-03-06 19:36:18,256 - INFO - validation batch 151, loss: 0.235, 4832/10016 datapoints
2025-03-06 19:36:18,407 - INFO - validation batch 201, loss: 0.281, 6432/10016 datapoints
2025-03-06 19:36:18,556 - INFO - validation batch 251, loss: 0.209, 8032/10016 datapoints
2025-03-06 19:36:18,711 - INFO - validation batch 301, loss: 0.423, 9632/10016 datapoints
2025-03-06 19:36:18,748 - INFO - Epoch 395/800 done.
2025-03-06 19:36:18,748 - INFO - Final validation performance:
Loss: 0.285, top-1 acc: 0.920top-5 acc: 0.920
2025-03-06 19:36:18,749 - INFO - Beginning epoch 396/800
2025-03-06 19:36:18,755 - INFO - training batch 1, loss: 0.334, 32/60000 datapoints
2025-03-06 19:36:18,962 - INFO - training batch 51, loss: 0.253, 1632/60000 datapoints
2025-03-06 19:36:19,158 - INFO - training batch 101, loss: 0.174, 3232/60000 datapoints
2025-03-06 19:36:19,384 - INFO - training batch 151, loss: 0.402, 4832/60000 datapoints
2025-03-06 19:36:19,593 - INFO - training batch 201, loss: 0.203, 6432/60000 datapoints
2025-03-06 19:36:19,796 - INFO - training batch 251, loss: 0.222, 8032/60000 datapoints
2025-03-06 19:36:19,994 - INFO - training batch 301, loss: 0.380, 9632/60000 datapoints
2025-03-06 19:36:20,193 - INFO - training batch 351, loss: 0.068, 11232/60000 datapoints
2025-03-06 19:36:20,386 - INFO - training batch 401, loss: 0.195, 12832/60000 datapoints
2025-03-06 19:36:20,581 - INFO - training batch 451, loss: 0.428, 14432/60000 datapoints
2025-03-06 19:36:20,778 - INFO - training batch 501, loss: 0.627, 16032/60000 datapoints
2025-03-06 19:36:20,973 - INFO - training batch 551, loss: 0.461, 17632/60000 datapoints
2025-03-06 19:36:21,170 - INFO - training batch 601, loss: 0.252, 19232/60000 datapoints
2025-03-06 19:36:21,369 - INFO - training batch 651, loss: 0.282, 20832/60000 datapoints
2025-03-06 19:36:21,565 - INFO - training batch 701, loss: 0.171, 22432/60000 datapoints
2025-03-06 19:36:21,763 - INFO - training batch 751, loss: 0.227, 24032/60000 datapoints
2025-03-06 19:36:21,963 - INFO - training batch 801, loss: 0.368, 25632/60000 datapoints
2025-03-06 19:36:22,156 - INFO - training batch 851, loss: 0.346, 27232/60000 datapoints
2025-03-06 19:36:22,351 - INFO - training batch 901, loss: 0.204, 28832/60000 datapoints
2025-03-06 19:36:22,544 - INFO - training batch 951, loss: 0.430, 30432/60000 datapoints
2025-03-06 19:36:22,740 - INFO - training batch 1001, loss: 0.125, 32032/60000 datapoints
2025-03-06 19:36:22,933 - INFO - training batch 1051, loss: 0.447, 33632/60000 datapoints
2025-03-06 19:36:23,126 - INFO - training batch 1101, loss: 0.377, 35232/60000 datapoints
2025-03-06 19:36:23,324 - INFO - training batch 1151, loss: 0.208, 36832/60000 datapoints
2025-03-06 19:36:23,520 - INFO - training batch 1201, loss: 0.610, 38432/60000 datapoints
2025-03-06 19:36:23,716 - INFO - training batch 1251, loss: 0.381, 40032/60000 datapoints
2025-03-06 19:36:23,912 - INFO - training batch 1301, loss: 0.116, 41632/60000 datapoints
2025-03-06 19:36:24,108 - INFO - training batch 1351, loss: 0.256, 43232/60000 datapoints
2025-03-06 19:36:24,302 - INFO - training batch 1401, loss: 0.422, 44832/60000 datapoints
2025-03-06 19:36:24,497 - INFO - training batch 1451, loss: 0.477, 46432/60000 datapoints
2025-03-06 19:36:24,692 - INFO - training batch 1501, loss: 0.422, 48032/60000 datapoints
2025-03-06 19:36:24,890 - INFO - training batch 1551, loss: 0.266, 49632/60000 datapoints
2025-03-06 19:36:25,092 - INFO - training batch 1601, loss: 0.249, 51232/60000 datapoints
2025-03-06 19:36:25,314 - INFO - training batch 1651, loss: 0.514, 52832/60000 datapoints
2025-03-06 19:36:25,535 - INFO - training batch 1701, loss: 0.193, 54432/60000 datapoints
2025-03-06 19:36:25,733 - INFO - training batch 1751, loss: 0.115, 56032/60000 datapoints
2025-03-06 19:36:25,929 - INFO - training batch 1801, loss: 0.559, 57632/60000 datapoints
2025-03-06 19:36:26,123 - INFO - training batch 1851, loss: 0.222, 59232/60000 datapoints
2025-03-06 19:36:26,224 - INFO - validation batch 1, loss: 0.152, 32/10016 datapoints
2025-03-06 19:36:26,377 - INFO - validation batch 51, loss: 0.093, 1632/10016 datapoints
2025-03-06 19:36:26,531 - INFO - validation batch 101, loss: 0.231, 3232/10016 datapoints
2025-03-06 19:36:26,685 - INFO - validation batch 151, loss: 0.166, 4832/10016 datapoints
2025-03-06 19:36:26,841 - INFO - validation batch 201, loss: 0.165, 6432/10016 datapoints
2025-03-06 19:36:26,995 - INFO - validation batch 251, loss: 0.160, 8032/10016 datapoints
2025-03-06 19:36:27,146 - INFO - validation batch 301, loss: 0.360, 9632/10016 datapoints
2025-03-06 19:36:27,184 - INFO - Epoch 396/800 done.
2025-03-06 19:36:27,184 - INFO - Final validation performance:
Loss: 0.190, top-1 acc: 0.920top-5 acc: 0.920
2025-03-06 19:36:27,185 - INFO - Beginning epoch 397/800
2025-03-06 19:36:27,191 - INFO - training batch 1, loss: 0.455, 32/60000 datapoints
2025-03-06 19:36:27,397 - INFO - training batch 51, loss: 0.173, 1632/60000 datapoints
2025-03-06 19:36:27,595 - INFO - training batch 101, loss: 0.209, 3232/60000 datapoints
2025-03-06 19:36:27,794 - INFO - training batch 151, loss: 0.140, 4832/60000 datapoints
2025-03-06 19:36:28,001 - INFO - training batch 201, loss: 0.257, 6432/60000 datapoints
2025-03-06 19:36:28,197 - INFO - training batch 251, loss: 0.190, 8032/60000 datapoints
2025-03-06 19:36:28,393 - INFO - training batch 301, loss: 0.160, 9632/60000 datapoints
2025-03-06 19:36:28,587 - INFO - training batch 351, loss: 0.192, 11232/60000 datapoints
2025-03-06 19:36:28,787 - INFO - training batch 401, loss: 0.219, 12832/60000 datapoints
2025-03-06 19:36:28,984 - INFO - training batch 451, loss: 0.433, 14432/60000 datapoints
2025-03-06 19:36:29,178 - INFO - training batch 501, loss: 0.365, 16032/60000 datapoints
2025-03-06 19:36:29,379 - INFO - training batch 551, loss: 0.132, 17632/60000 datapoints
2025-03-06 19:36:29,590 - INFO - training batch 601, loss: 0.377, 19232/60000 datapoints
2025-03-06 19:36:29,789 - INFO - training batch 651, loss: 0.253, 20832/60000 datapoints
2025-03-06 19:36:29,987 - INFO - training batch 701, loss: 0.346, 22432/60000 datapoints
2025-03-06 19:36:30,179 - INFO - training batch 751, loss: 0.275, 24032/60000 datapoints
2025-03-06 19:36:30,374 - INFO - training batch 801, loss: 0.108, 25632/60000 datapoints
2025-03-06 19:36:30,569 - INFO - training batch 851, loss: 0.179, 27232/60000 datapoints
2025-03-06 19:36:30,766 - INFO - training batch 901, loss: 0.222, 28832/60000 datapoints
2025-03-06 19:36:30,960 - INFO - training batch 951, loss: 0.335, 30432/60000 datapoints
2025-03-06 19:36:31,154 - INFO - training batch 1001, loss: 0.167, 32032/60000 datapoints
2025-03-06 19:36:31,349 - INFO - training batch 1051, loss: 0.164, 33632/60000 datapoints
2025-03-06 19:36:31,549 - INFO - training batch 1101, loss: 0.279, 35232/60000 datapoints
2025-03-06 19:36:31,746 - INFO - training batch 1151, loss: 0.377, 36832/60000 datapoints
2025-03-06 19:36:31,943 - INFO - training batch 1201, loss: 0.263, 38432/60000 datapoints
2025-03-06 19:36:32,136 - INFO - training batch 1251, loss: 0.190, 40032/60000 datapoints
2025-03-06 19:36:32,330 - INFO - training batch 1301, loss: 0.316, 41632/60000 datapoints
2025-03-06 19:36:32,524 - INFO - training batch 1351, loss: 0.487, 43232/60000 datapoints
2025-03-06 19:36:32,745 - INFO - training batch 1401, loss: 0.220, 44832/60000 datapoints
2025-03-06 19:36:32,941 - INFO - training batch 1451, loss: 0.337, 46432/60000 datapoints
2025-03-06 19:36:33,135 - INFO - training batch 1501, loss: 0.286, 48032/60000 datapoints
2025-03-06 19:36:33,330 - INFO - training batch 1551, loss: 0.247, 49632/60000 datapoints
2025-03-06 19:36:33,526 - INFO - training batch 1601, loss: 0.188, 51232/60000 datapoints
2025-03-06 19:36:33,724 - INFO - training batch 1651, loss: 0.220, 52832/60000 datapoints
2025-03-06 19:36:33,921 - INFO - training batch 1701, loss: 0.448, 54432/60000 datapoints
2025-03-06 19:36:34,113 - INFO - training batch 1751, loss: 0.135, 56032/60000 datapoints
2025-03-06 19:36:34,306 - INFO - training batch 1801, loss: 0.638, 57632/60000 datapoints
2025-03-06 19:36:34,501 - INFO - training batch 1851, loss: 0.304, 59232/60000 datapoints
2025-03-06 19:36:34,604 - INFO - validation batch 1, loss: 0.227, 32/10016 datapoints
2025-03-06 19:36:34,756 - INFO - validation batch 51, loss: 0.326, 1632/10016 datapoints
2025-03-06 19:36:34,912 - INFO - validation batch 101, loss: 0.198, 3232/10016 datapoints
2025-03-06 19:36:35,064 - INFO - validation batch 151, loss: 0.188, 4832/10016 datapoints
2025-03-06 19:36:35,215 - INFO - validation batch 201, loss: 0.419, 6432/10016 datapoints
2025-03-06 19:36:35,374 - INFO - validation batch 251, loss: 0.232, 8032/10016 datapoints
2025-03-06 19:36:35,531 - INFO - validation batch 301, loss: 0.398, 9632/10016 datapoints
2025-03-06 19:36:35,568 - INFO - Epoch 397/800 done.
2025-03-06 19:36:35,568 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.920top-5 acc: 0.920
2025-03-06 19:36:35,569 - INFO - Beginning epoch 398/800
2025-03-06 19:36:35,575 - INFO - training batch 1, loss: 0.690, 32/60000 datapoints
2025-03-06 19:36:35,777 - INFO - training batch 51, loss: 0.369, 1632/60000 datapoints
2025-03-06 19:36:35,981 - INFO - training batch 101, loss: 0.322, 3232/60000 datapoints
2025-03-06 19:36:36,180 - INFO - training batch 151, loss: 0.388, 4832/60000 datapoints
2025-03-06 19:36:36,376 - INFO - training batch 201, loss: 0.610, 6432/60000 datapoints
2025-03-06 19:36:36,573 - INFO - training batch 251, loss: 0.518, 8032/60000 datapoints
2025-03-06 19:36:36,768 - INFO - training batch 301, loss: 0.379, 9632/60000 datapoints
2025-03-06 19:36:36,963 - INFO - training batch 351, loss: 0.512, 11232/60000 datapoints
2025-03-06 19:36:37,153 - INFO - training batch 401, loss: 0.134, 12832/60000 datapoints
2025-03-06 19:36:37,345 - INFO - training batch 451, loss: 0.200, 14432/60000 datapoints
2025-03-06 19:36:37,536 - INFO - training batch 501, loss: 0.264, 16032/60000 datapoints
2025-03-06 19:36:37,746 - INFO - training batch 551, loss: 0.094, 17632/60000 datapoints
2025-03-06 19:36:37,943 - INFO - training batch 601, loss: 0.325, 19232/60000 datapoints
2025-03-06 19:36:38,136 - INFO - training batch 651, loss: 0.172, 20832/60000 datapoints
2025-03-06 19:36:38,327 - INFO - training batch 701, loss: 0.302, 22432/60000 datapoints
2025-03-06 19:36:38,525 - INFO - training batch 751, loss: 0.179, 24032/60000 datapoints
2025-03-06 19:36:38,719 - INFO - training batch 801, loss: 0.447, 25632/60000 datapoints
2025-03-06 19:36:38,912 - INFO - training batch 851, loss: 0.426, 27232/60000 datapoints
2025-03-06 19:36:39,105 - INFO - training batch 901, loss: 0.442, 28832/60000 datapoints
2025-03-06 19:36:39,297 - INFO - training batch 951, loss: 0.561, 30432/60000 datapoints
2025-03-06 19:36:39,514 - INFO - training batch 1001, loss: 0.196, 32032/60000 datapoints
2025-03-06 19:36:39,714 - INFO - training batch 1051, loss: 0.218, 33632/60000 datapoints
2025-03-06 19:36:39,909 - INFO - training batch 1101, loss: 0.367, 35232/60000 datapoints
2025-03-06 19:36:40,107 - INFO - training batch 1151, loss: 0.274, 36832/60000 datapoints
2025-03-06 19:36:40,301 - INFO - training batch 1201, loss: 0.206, 38432/60000 datapoints
2025-03-06 19:36:40,496 - INFO - training batch 1251, loss: 0.325, 40032/60000 datapoints
2025-03-06 19:36:40,693 - INFO - training batch 1301, loss: 0.109, 41632/60000 datapoints
2025-03-06 19:36:40,888 - INFO - training batch 1351, loss: 0.198, 43232/60000 datapoints
2025-03-06 19:36:41,083 - INFO - training batch 1401, loss: 0.304, 44832/60000 datapoints
2025-03-06 19:36:41,279 - INFO - training batch 1451, loss: 0.160, 46432/60000 datapoints
2025-03-06 19:36:41,475 - INFO - training batch 1501, loss: 0.272, 48032/60000 datapoints
2025-03-06 19:36:41,671 - INFO - training batch 1551, loss: 0.141, 49632/60000 datapoints
2025-03-06 19:36:41,865 - INFO - training batch 1601, loss: 0.199, 51232/60000 datapoints
2025-03-06 19:36:42,061 - INFO - training batch 1651, loss: 0.214, 52832/60000 datapoints
2025-03-06 19:36:42,255 - INFO - training batch 1701, loss: 0.451, 54432/60000 datapoints
2025-03-06 19:36:42,464 - INFO - training batch 1751, loss: 0.222, 56032/60000 datapoints
2025-03-06 19:36:42,660 - INFO - training batch 1801, loss: 0.410, 57632/60000 datapoints
2025-03-06 19:36:42,859 - INFO - training batch 1851, loss: 0.267, 59232/60000 datapoints
2025-03-06 19:36:42,962 - INFO - validation batch 1, loss: 0.203, 32/10016 datapoints
2025-03-06 19:36:43,116 - INFO - validation batch 51, loss: 0.326, 1632/10016 datapoints
2025-03-06 19:36:43,270 - INFO - validation batch 101, loss: 0.345, 3232/10016 datapoints
2025-03-06 19:36:43,424 - INFO - validation batch 151, loss: 0.114, 4832/10016 datapoints
2025-03-06 19:36:43,577 - INFO - validation batch 201, loss: 0.269, 6432/10016 datapoints
2025-03-06 19:36:43,733 - INFO - validation batch 251, loss: 0.328, 8032/10016 datapoints
2025-03-06 19:36:43,886 - INFO - validation batch 301, loss: 0.591, 9632/10016 datapoints
2025-03-06 19:36:43,927 - INFO - Epoch 398/800 done.
2025-03-06 19:36:43,927 - INFO - Final validation performance:
Loss: 0.311, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:36:43,928 - INFO - Beginning epoch 399/800
2025-03-06 19:36:43,934 - INFO - training batch 1, loss: 0.189, 32/60000 datapoints
2025-03-06 19:36:44,140 - INFO - training batch 51, loss: 0.237, 1632/60000 datapoints
2025-03-06 19:36:44,331 - INFO - training batch 101, loss: 0.234, 3232/60000 datapoints
2025-03-06 19:36:44,533 - INFO - training batch 151, loss: 0.186, 4832/60000 datapoints
2025-03-06 19:36:44,734 - INFO - training batch 201, loss: 0.214, 6432/60000 datapoints
2025-03-06 19:36:44,936 - INFO - training batch 251, loss: 0.261, 8032/60000 datapoints
2025-03-06 19:36:45,129 - INFO - training batch 301, loss: 0.084, 9632/60000 datapoints
2025-03-06 19:36:45,323 - INFO - training batch 351, loss: 0.177, 11232/60000 datapoints
2025-03-06 19:36:45,521 - INFO - training batch 401, loss: 0.110, 12832/60000 datapoints
2025-03-06 19:36:45,717 - INFO - training batch 451, loss: 0.148, 14432/60000 datapoints
2025-03-06 19:36:45,916 - INFO - training batch 501, loss: 0.132, 16032/60000 datapoints
2025-03-06 19:36:46,123 - INFO - training batch 551, loss: 0.401, 17632/60000 datapoints
2025-03-06 19:36:46,319 - INFO - training batch 601, loss: 0.054, 19232/60000 datapoints
2025-03-06 19:36:46,513 - INFO - training batch 651, loss: 0.491, 20832/60000 datapoints
2025-03-06 19:36:46,709 - INFO - training batch 701, loss: 0.255, 22432/60000 datapoints
2025-03-06 19:36:46,901 - INFO - training batch 751, loss: 0.229, 24032/60000 datapoints
2025-03-06 19:36:47,096 - INFO - training batch 801, loss: 0.147, 25632/60000 datapoints
2025-03-06 19:36:47,289 - INFO - training batch 851, loss: 0.381, 27232/60000 datapoints
2025-03-06 19:36:47,483 - INFO - training batch 901, loss: 0.612, 28832/60000 datapoints
2025-03-06 19:36:47,680 - INFO - training batch 951, loss: 1.223, 30432/60000 datapoints
2025-03-06 19:36:47,872 - INFO - training batch 1001, loss: 0.388, 32032/60000 datapoints
2025-03-06 19:36:48,077 - INFO - training batch 1051, loss: 0.516, 33632/60000 datapoints
2025-03-06 19:36:48,272 - INFO - training batch 1101, loss: 0.209, 35232/60000 datapoints
2025-03-06 19:36:48,466 - INFO - training batch 1151, loss: 0.302, 36832/60000 datapoints
2025-03-06 19:36:48,666 - INFO - training batch 1201, loss: 0.771, 38432/60000 datapoints
2025-03-06 19:36:48,860 - INFO - training batch 1251, loss: 0.367, 40032/60000 datapoints
2025-03-06 19:36:49,056 - INFO - training batch 1301, loss: 0.141, 41632/60000 datapoints
2025-03-06 19:36:49,250 - INFO - training batch 1351, loss: 0.131, 43232/60000 datapoints
2025-03-06 19:36:49,445 - INFO - training batch 1401, loss: 0.180, 44832/60000 datapoints
2025-03-06 19:36:49,669 - INFO - training batch 1451, loss: 0.153, 46432/60000 datapoints
2025-03-06 19:36:49,864 - INFO - training batch 1501, loss: 0.479, 48032/60000 datapoints
2025-03-06 19:36:50,065 - INFO - training batch 1551, loss: 0.421, 49632/60000 datapoints
2025-03-06 19:36:50,257 - INFO - training batch 1601, loss: 0.232, 51232/60000 datapoints
2025-03-06 19:36:50,453 - INFO - training batch 1651, loss: 0.303, 52832/60000 datapoints
2025-03-06 19:36:50,649 - INFO - training batch 1701, loss: 0.299, 54432/60000 datapoints
2025-03-06 19:36:50,844 - INFO - training batch 1751, loss: 0.304, 56032/60000 datapoints
2025-03-06 19:36:51,038 - INFO - training batch 1801, loss: 0.252, 57632/60000 datapoints
2025-03-06 19:36:51,235 - INFO - training batch 1851, loss: 0.360, 59232/60000 datapoints
2025-03-06 19:36:51,336 - INFO - validation batch 1, loss: 0.151, 32/10016 datapoints
2025-03-06 19:36:51,491 - INFO - validation batch 51, loss: 0.223, 1632/10016 datapoints
2025-03-06 19:36:51,647 - INFO - validation batch 101, loss: 0.130, 3232/10016 datapoints
2025-03-06 19:36:51,801 - INFO - validation batch 151, loss: 0.186, 4832/10016 datapoints
2025-03-06 19:36:51,958 - INFO - validation batch 201, loss: 0.206, 6432/10016 datapoints
2025-03-06 19:36:52,116 - INFO - validation batch 251, loss: 0.130, 8032/10016 datapoints
2025-03-06 19:36:52,270 - INFO - validation batch 301, loss: 0.368, 9632/10016 datapoints
2025-03-06 19:36:52,306 - INFO - Epoch 399/800 done.
2025-03-06 19:36:52,306 - INFO - Final validation performance:
Loss: 0.199, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:36:52,307 - INFO - Beginning epoch 400/800
2025-03-06 19:36:52,314 - INFO - training batch 1, loss: 0.163, 32/60000 datapoints
2025-03-06 19:36:52,511 - INFO - training batch 51, loss: 0.179, 1632/60000 datapoints
2025-03-06 19:36:52,710 - INFO - training batch 101, loss: 0.338, 3232/60000 datapoints
2025-03-06 19:36:52,914 - INFO - training batch 151, loss: 0.162, 4832/60000 datapoints
2025-03-06 19:36:53,113 - INFO - training batch 201, loss: 0.357, 6432/60000 datapoints
2025-03-06 19:36:53,307 - INFO - training batch 251, loss: 0.233, 8032/60000 datapoints
2025-03-06 19:36:53,508 - INFO - training batch 301, loss: 0.290, 9632/60000 datapoints
2025-03-06 19:36:53,707 - INFO - training batch 351, loss: 0.344, 11232/60000 datapoints
2025-03-06 19:36:53,905 - INFO - training batch 401, loss: 0.198, 12832/60000 datapoints
2025-03-06 19:36:54,104 - INFO - training batch 451, loss: 0.315, 14432/60000 datapoints
2025-03-06 19:36:54,294 - INFO - training batch 501, loss: 0.298, 16032/60000 datapoints
2025-03-06 19:36:54,489 - INFO - training batch 551, loss: 0.272, 17632/60000 datapoints
2025-03-06 19:36:54,684 - INFO - training batch 601, loss: 0.259, 19232/60000 datapoints
2025-03-06 19:36:54,878 - INFO - training batch 651, loss: 0.209, 20832/60000 datapoints
2025-03-06 19:36:55,079 - INFO - training batch 701, loss: 0.100, 22432/60000 datapoints
2025-03-06 19:36:55,274 - INFO - training batch 751, loss: 0.425, 24032/60000 datapoints
2025-03-06 19:36:55,468 - INFO - training batch 801, loss: 0.335, 25632/60000 datapoints
2025-03-06 19:36:55,669 - INFO - training batch 851, loss: 0.168, 27232/60000 datapoints
2025-03-06 19:36:55,862 - INFO - training batch 901, loss: 0.788, 28832/60000 datapoints
2025-03-06 19:36:56,059 - INFO - training batch 951, loss: 0.203, 30432/60000 datapoints
2025-03-06 19:36:56,254 - INFO - training batch 1001, loss: 0.273, 32032/60000 datapoints
2025-03-06 19:36:56,447 - INFO - training batch 1051, loss: 0.210, 33632/60000 datapoints
2025-03-06 19:36:56,645 - INFO - training batch 1101, loss: 0.285, 35232/60000 datapoints
2025-03-06 19:36:56,840 - INFO - training batch 1151, loss: 0.186, 36832/60000 datapoints
2025-03-06 19:36:57,036 - INFO - training batch 1201, loss: 0.154, 38432/60000 datapoints
2025-03-06 19:36:57,230 - INFO - training batch 1251, loss: 0.558, 40032/60000 datapoints
2025-03-06 19:36:57,422 - INFO - training batch 1301, loss: 0.334, 41632/60000 datapoints
2025-03-06 19:36:57,619 - INFO - training batch 1351, loss: 0.257, 43232/60000 datapoints
2025-03-06 19:36:57,814 - INFO - training batch 1401, loss: 0.146, 44832/60000 datapoints
2025-03-06 19:36:58,012 - INFO - training batch 1451, loss: 0.204, 46432/60000 datapoints
2025-03-06 19:36:58,207 - INFO - training batch 1501, loss: 0.267, 48032/60000 datapoints
2025-03-06 19:36:58,401 - INFO - training batch 1551, loss: 0.354, 49632/60000 datapoints
2025-03-06 19:36:58,597 - INFO - training batch 1601, loss: 0.525, 51232/60000 datapoints
2025-03-06 19:36:58,798 - INFO - training batch 1651, loss: 0.352, 52832/60000 datapoints
2025-03-06 19:36:58,993 - INFO - training batch 1701, loss: 0.326, 54432/60000 datapoints
2025-03-06 19:36:59,190 - INFO - training batch 1751, loss: 0.124, 56032/60000 datapoints
2025-03-06 19:36:59,384 - INFO - training batch 1801, loss: 0.059, 57632/60000 datapoints
2025-03-06 19:36:59,577 - INFO - training batch 1851, loss: 0.494, 59232/60000 datapoints
2025-03-06 19:36:59,703 - INFO - validation batch 1, loss: 0.297, 32/10016 datapoints
2025-03-06 19:36:59,854 - INFO - validation batch 51, loss: 0.233, 1632/10016 datapoints
2025-03-06 19:37:00,011 - INFO - validation batch 101, loss: 0.185, 3232/10016 datapoints
2025-03-06 19:37:00,166 - INFO - validation batch 151, loss: 0.152, 4832/10016 datapoints
2025-03-06 19:37:00,317 - INFO - validation batch 201, loss: 0.232, 6432/10016 datapoints
2025-03-06 19:37:00,468 - INFO - validation batch 251, loss: 0.328, 8032/10016 datapoints
2025-03-06 19:37:00,625 - INFO - validation batch 301, loss: 0.203, 9632/10016 datapoints
2025-03-06 19:37:00,661 - INFO - Epoch 400/800 done.
2025-03-06 19:37:00,661 - INFO - Final validation performance:
Loss: 0.233, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:37:00,662 - INFO - Beginning epoch 401/800
2025-03-06 19:37:00,668 - INFO - training batch 1, loss: 0.246, 32/60000 datapoints
2025-03-06 19:37:00,862 - INFO - training batch 51, loss: 0.124, 1632/60000 datapoints
2025-03-06 19:37:01,054 - INFO - training batch 101, loss: 0.309, 3232/60000 datapoints
2025-03-06 19:37:01,266 - INFO - training batch 151, loss: 0.232, 4832/60000 datapoints
2025-03-06 19:37:01,467 - INFO - training batch 201, loss: 0.445, 6432/60000 datapoints
2025-03-06 19:37:01,660 - INFO - training batch 251, loss: 0.143, 8032/60000 datapoints
2025-03-06 19:37:01,860 - INFO - training batch 301, loss: 0.314, 9632/60000 datapoints
2025-03-06 19:37:02,058 - INFO - training batch 351, loss: 0.194, 11232/60000 datapoints
2025-03-06 19:37:02,257 - INFO - training batch 401, loss: 0.166, 12832/60000 datapoints
2025-03-06 19:37:02,447 - INFO - training batch 451, loss: 0.289, 14432/60000 datapoints
2025-03-06 19:37:02,639 - INFO - training batch 501, loss: 0.492, 16032/60000 datapoints
2025-03-06 19:37:02,828 - INFO - training batch 551, loss: 0.203, 17632/60000 datapoints
2025-03-06 19:37:03,019 - INFO - training batch 601, loss: 0.149, 19232/60000 datapoints
2025-03-06 19:37:03,215 - INFO - training batch 651, loss: 0.311, 20832/60000 datapoints
2025-03-06 19:37:03,406 - INFO - training batch 701, loss: 0.222, 22432/60000 datapoints
2025-03-06 19:37:03,596 - INFO - training batch 751, loss: 0.051, 24032/60000 datapoints
2025-03-06 19:37:03,792 - INFO - training batch 801, loss: 0.293, 25632/60000 datapoints
2025-03-06 19:37:03,982 - INFO - training batch 851, loss: 0.298, 27232/60000 datapoints
2025-03-06 19:37:04,176 - INFO - training batch 901, loss: 0.401, 28832/60000 datapoints
2025-03-06 19:37:04,365 - INFO - training batch 951, loss: 0.322, 30432/60000 datapoints
2025-03-06 19:37:04,555 - INFO - training batch 1001, loss: 0.153, 32032/60000 datapoints
2025-03-06 19:37:04,750 - INFO - training batch 1051, loss: 0.275, 33632/60000 datapoints
2025-03-06 19:37:04,944 - INFO - training batch 1101, loss: 0.477, 35232/60000 datapoints
2025-03-06 19:37:05,135 - INFO - training batch 1151, loss: 0.231, 36832/60000 datapoints
2025-03-06 19:37:05,326 - INFO - training batch 1201, loss: 0.440, 38432/60000 datapoints
2025-03-06 19:37:05,516 - INFO - training batch 1251, loss: 0.364, 40032/60000 datapoints
2025-03-06 19:37:05,715 - INFO - training batch 1301, loss: 0.214, 41632/60000 datapoints
2025-03-06 19:37:05,906 - INFO - training batch 1351, loss: 0.645, 43232/60000 datapoints
2025-03-06 19:37:06,101 - INFO - training batch 1401, loss: 0.084, 44832/60000 datapoints
2025-03-06 19:37:06,294 - INFO - training batch 1451, loss: 0.183, 46432/60000 datapoints
2025-03-06 19:37:06,485 - INFO - training batch 1501, loss: 0.316, 48032/60000 datapoints
2025-03-06 19:37:06,678 - INFO - training batch 1551, loss: 0.349, 49632/60000 datapoints
2025-03-06 19:37:06,871 - INFO - training batch 1601, loss: 0.160, 51232/60000 datapoints
2025-03-06 19:37:07,061 - INFO - training batch 1651, loss: 0.196, 52832/60000 datapoints
2025-03-06 19:37:07,252 - INFO - training batch 1701, loss: 0.282, 54432/60000 datapoints
2025-03-06 19:37:07,441 - INFO - training batch 1751, loss: 0.071, 56032/60000 datapoints
2025-03-06 19:37:07,633 - INFO - training batch 1801, loss: 0.231, 57632/60000 datapoints
2025-03-06 19:37:07,827 - INFO - training batch 1851, loss: 0.076, 59232/60000 datapoints
2025-03-06 19:37:07,926 - INFO - validation batch 1, loss: 0.210, 32/10016 datapoints
2025-03-06 19:37:08,080 - INFO - validation batch 51, loss: 0.476, 1632/10016 datapoints
2025-03-06 19:37:08,231 - INFO - validation batch 101, loss: 0.362, 3232/10016 datapoints
2025-03-06 19:37:08,386 - INFO - validation batch 151, loss: 0.239, 4832/10016 datapoints
2025-03-06 19:37:08,540 - INFO - validation batch 201, loss: 0.193, 6432/10016 datapoints
2025-03-06 19:37:08,699 - INFO - validation batch 251, loss: 0.103, 8032/10016 datapoints
2025-03-06 19:37:08,851 - INFO - validation batch 301, loss: 0.487, 9632/10016 datapoints
2025-03-06 19:37:08,889 - INFO - Epoch 401/800 done.
2025-03-06 19:37:08,890 - INFO - Final validation performance:
Loss: 0.296, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:37:08,890 - INFO - Beginning epoch 402/800
2025-03-06 19:37:08,896 - INFO - training batch 1, loss: 0.413, 32/60000 datapoints
2025-03-06 19:37:09,104 - INFO - training batch 51, loss: 0.359, 1632/60000 datapoints
2025-03-06 19:37:09,303 - INFO - training batch 101, loss: 0.391, 3232/60000 datapoints
2025-03-06 19:37:09,507 - INFO - training batch 151, loss: 0.244, 4832/60000 datapoints
2025-03-06 19:37:09,707 - INFO - training batch 201, loss: 0.322, 6432/60000 datapoints
2025-03-06 19:37:09,920 - INFO - training batch 251, loss: 0.206, 8032/60000 datapoints
2025-03-06 19:37:10,127 - INFO - training batch 301, loss: 0.377, 9632/60000 datapoints
2025-03-06 19:37:10,325 - INFO - training batch 351, loss: 0.345, 11232/60000 datapoints
2025-03-06 19:37:10,522 - INFO - training batch 401, loss: 0.287, 12832/60000 datapoints
2025-03-06 19:37:10,719 - INFO - training batch 451, loss: 0.593, 14432/60000 datapoints
2025-03-06 19:37:10,914 - INFO - training batch 501, loss: 0.538, 16032/60000 datapoints
2025-03-06 19:37:11,108 - INFO - training batch 551, loss: 0.294, 17632/60000 datapoints
2025-03-06 19:37:11,304 - INFO - training batch 601, loss: 0.267, 19232/60000 datapoints
2025-03-06 19:37:11,499 - INFO - training batch 651, loss: 0.576, 20832/60000 datapoints
2025-03-06 19:37:11,699 - INFO - training batch 701, loss: 0.124, 22432/60000 datapoints
2025-03-06 19:37:11,896 - INFO - training batch 751, loss: 0.379, 24032/60000 datapoints
2025-03-06 19:37:12,093 - INFO - training batch 801, loss: 0.319, 25632/60000 datapoints
2025-03-06 19:37:12,286 - INFO - training batch 851, loss: 0.066, 27232/60000 datapoints
2025-03-06 19:37:12,478 - INFO - training batch 901, loss: 0.139, 28832/60000 datapoints
2025-03-06 19:37:12,673 - INFO - training batch 951, loss: 0.180, 30432/60000 datapoints
2025-03-06 19:37:12,867 - INFO - training batch 1001, loss: 0.316, 32032/60000 datapoints
2025-03-06 19:37:13,059 - INFO - training batch 1051, loss: 0.421, 33632/60000 datapoints
2025-03-06 19:37:13,254 - INFO - training batch 1101, loss: 0.635, 35232/60000 datapoints
2025-03-06 19:37:13,449 - INFO - training batch 1151, loss: 0.285, 36832/60000 datapoints
2025-03-06 19:37:13,646 - INFO - training batch 1201, loss: 0.542, 38432/60000 datapoints
2025-03-06 19:37:13,859 - INFO - training batch 1251, loss: 0.305, 40032/60000 datapoints
2025-03-06 19:37:14,054 - INFO - training batch 1301, loss: 0.155, 41632/60000 datapoints
2025-03-06 19:37:14,247 - INFO - training batch 1351, loss: 0.159, 43232/60000 datapoints
2025-03-06 19:37:14,443 - INFO - training batch 1401, loss: 0.286, 44832/60000 datapoints
2025-03-06 19:37:14,637 - INFO - training batch 1451, loss: 0.277, 46432/60000 datapoints
2025-03-06 19:37:14,833 - INFO - training batch 1501, loss: 0.283, 48032/60000 datapoints
2025-03-06 19:37:15,032 - INFO - training batch 1551, loss: 0.229, 49632/60000 datapoints
2025-03-06 19:37:15,226 - INFO - training batch 1601, loss: 0.170, 51232/60000 datapoints
2025-03-06 19:37:15,421 - INFO - training batch 1651, loss: 0.624, 52832/60000 datapoints
2025-03-06 19:37:15,623 - INFO - training batch 1701, loss: 0.360, 54432/60000 datapoints
2025-03-06 19:37:15,819 - INFO - training batch 1751, loss: 0.268, 56032/60000 datapoints
2025-03-06 19:37:16,016 - INFO - training batch 1801, loss: 0.165, 57632/60000 datapoints
2025-03-06 19:37:16,215 - INFO - training batch 1851, loss: 0.596, 59232/60000 datapoints
2025-03-06 19:37:16,318 - INFO - validation batch 1, loss: 0.119, 32/10016 datapoints
2025-03-06 19:37:16,469 - INFO - validation batch 51, loss: 0.151, 1632/10016 datapoints
2025-03-06 19:37:16,623 - INFO - validation batch 101, loss: 0.114, 3232/10016 datapoints
2025-03-06 19:37:16,797 - INFO - validation batch 151, loss: 0.173, 4832/10016 datapoints
2025-03-06 19:37:16,982 - INFO - validation batch 201, loss: 0.150, 6432/10016 datapoints
2025-03-06 19:37:17,133 - INFO - validation batch 251, loss: 0.556, 8032/10016 datapoints
2025-03-06 19:37:17,286 - INFO - validation batch 301, loss: 0.134, 9632/10016 datapoints
2025-03-06 19:37:17,323 - INFO - Epoch 402/800 done.
2025-03-06 19:37:17,323 - INFO - Final validation performance:
Loss: 0.200, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:37:17,324 - INFO - Beginning epoch 403/800
2025-03-06 19:37:17,331 - INFO - training batch 1, loss: 0.248, 32/60000 datapoints
2025-03-06 19:37:17,523 - INFO - training batch 51, loss: 0.277, 1632/60000 datapoints
2025-03-06 19:37:17,718 - INFO - training batch 101, loss: 0.128, 3232/60000 datapoints
2025-03-06 19:37:17,922 - INFO - training batch 151, loss: 0.256, 4832/60000 datapoints
2025-03-06 19:37:18,122 - INFO - training batch 201, loss: 0.164, 6432/60000 datapoints
2025-03-06 19:37:18,318 - INFO - training batch 251, loss: 0.202, 8032/60000 datapoints
2025-03-06 19:37:18,514 - INFO - training batch 301, loss: 0.474, 9632/60000 datapoints
2025-03-06 19:37:18,718 - INFO - training batch 351, loss: 0.470, 11232/60000 datapoints
2025-03-06 19:37:18,917 - INFO - training batch 401, loss: 0.249, 12832/60000 datapoints
2025-03-06 19:37:19,107 - INFO - training batch 451, loss: 0.192, 14432/60000 datapoints
2025-03-06 19:37:19,301 - INFO - training batch 501, loss: 0.258, 16032/60000 datapoints
2025-03-06 19:37:19,495 - INFO - training batch 551, loss: 0.427, 17632/60000 datapoints
2025-03-06 19:37:19,690 - INFO - training batch 601, loss: 0.549, 19232/60000 datapoints
2025-03-06 19:37:19,902 - INFO - training batch 651, loss: 0.327, 20832/60000 datapoints
2025-03-06 19:37:20,098 - INFO - training batch 701, loss: 0.432, 22432/60000 datapoints
2025-03-06 19:37:20,292 - INFO - training batch 751, loss: 0.103, 24032/60000 datapoints
2025-03-06 19:37:20,488 - INFO - training batch 801, loss: 0.628, 25632/60000 datapoints
2025-03-06 19:37:20,686 - INFO - training batch 851, loss: 0.375, 27232/60000 datapoints
2025-03-06 19:37:20,881 - INFO - training batch 901, loss: 0.164, 28832/60000 datapoints
2025-03-06 19:37:21,075 - INFO - training batch 951, loss: 0.423, 30432/60000 datapoints
2025-03-06 19:37:21,271 - INFO - training batch 1001, loss: 0.627, 32032/60000 datapoints
2025-03-06 19:37:21,464 - INFO - training batch 1051, loss: 0.270, 33632/60000 datapoints
2025-03-06 19:37:21,660 - INFO - training batch 1101, loss: 0.395, 35232/60000 datapoints
2025-03-06 19:37:21,853 - INFO - training batch 1151, loss: 0.199, 36832/60000 datapoints
2025-03-06 19:37:22,048 - INFO - training batch 1201, loss: 0.268, 38432/60000 datapoints
2025-03-06 19:37:22,247 - INFO - training batch 1251, loss: 0.389, 40032/60000 datapoints
2025-03-06 19:37:22,441 - INFO - training batch 1301, loss: 0.244, 41632/60000 datapoints
2025-03-06 19:37:22,636 - INFO - training batch 1351, loss: 0.161, 43232/60000 datapoints
2025-03-06 19:37:22,830 - INFO - training batch 1401, loss: 0.193, 44832/60000 datapoints
2025-03-06 19:37:23,023 - INFO - training batch 1451, loss: 0.182, 46432/60000 datapoints
2025-03-06 19:37:23,223 - INFO - training batch 1501, loss: 0.224, 48032/60000 datapoints
2025-03-06 19:37:23,421 - INFO - training batch 1551, loss: 0.229, 49632/60000 datapoints
2025-03-06 19:37:23,614 - INFO - training batch 1601, loss: 0.169, 51232/60000 datapoints
2025-03-06 19:37:23,809 - INFO - training batch 1651, loss: 0.205, 52832/60000 datapoints
2025-03-06 19:37:24,005 - INFO - training batch 1701, loss: 0.368, 54432/60000 datapoints
2025-03-06 19:37:24,202 - INFO - training batch 1751, loss: 0.254, 56032/60000 datapoints
2025-03-06 19:37:24,401 - INFO - training batch 1801, loss: 0.438, 57632/60000 datapoints
2025-03-06 19:37:24,595 - INFO - training batch 1851, loss: 0.291, 59232/60000 datapoints
2025-03-06 19:37:24,697 - INFO - validation batch 1, loss: 0.208, 32/10016 datapoints
2025-03-06 19:37:24,850 - INFO - validation batch 51, loss: 0.185, 1632/10016 datapoints
2025-03-06 19:37:25,007 - INFO - validation batch 101, loss: 0.439, 3232/10016 datapoints
2025-03-06 19:37:25,158 - INFO - validation batch 151, loss: 0.336, 4832/10016 datapoints
2025-03-06 19:37:25,309 - INFO - validation batch 201, loss: 0.170, 6432/10016 datapoints
2025-03-06 19:37:25,521 - INFO - validation batch 251, loss: 0.181, 8032/10016 datapoints
2025-03-06 19:37:25,681 - INFO - validation batch 301, loss: 0.333, 9632/10016 datapoints
2025-03-06 19:37:25,718 - INFO - Epoch 403/800 done.
2025-03-06 19:37:25,719 - INFO - Final validation performance:
Loss: 0.265, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:37:25,719 - INFO - Beginning epoch 404/800
2025-03-06 19:37:25,727 - INFO - training batch 1, loss: 0.224, 32/60000 datapoints
2025-03-06 19:37:25,929 - INFO - training batch 51, loss: 0.295, 1632/60000 datapoints
2025-03-06 19:37:26,133 - INFO - training batch 101, loss: 0.184, 3232/60000 datapoints
2025-03-06 19:37:26,325 - INFO - training batch 151, loss: 0.219, 4832/60000 datapoints
2025-03-06 19:37:26,525 - INFO - training batch 201, loss: 0.145, 6432/60000 datapoints
2025-03-06 19:37:26,726 - INFO - training batch 251, loss: 0.147, 8032/60000 datapoints
2025-03-06 19:37:26,925 - INFO - training batch 301, loss: 0.288, 9632/60000 datapoints
2025-03-06 19:37:27,116 - INFO - training batch 351, loss: 0.234, 11232/60000 datapoints
2025-03-06 19:37:27,308 - INFO - training batch 401, loss: 0.141, 12832/60000 datapoints
2025-03-06 19:37:27,500 - INFO - training batch 451, loss: 0.266, 14432/60000 datapoints
2025-03-06 19:37:27,695 - INFO - training batch 501, loss: 0.142, 16032/60000 datapoints
2025-03-06 19:37:27,886 - INFO - training batch 551, loss: 0.283, 17632/60000 datapoints
2025-03-06 19:37:28,078 - INFO - training batch 601, loss: 0.162, 19232/60000 datapoints
2025-03-06 19:37:28,273 - INFO - training batch 651, loss: 0.127, 20832/60000 datapoints
2025-03-06 19:37:28,465 - INFO - training batch 701, loss: 0.220, 22432/60000 datapoints
2025-03-06 19:37:28,666 - INFO - training batch 751, loss: 0.759, 24032/60000 datapoints
2025-03-06 19:37:28,860 - INFO - training batch 801, loss: 0.322, 25632/60000 datapoints
2025-03-06 19:37:29,056 - INFO - training batch 851, loss: 0.242, 27232/60000 datapoints
2025-03-06 19:37:29,253 - INFO - training batch 901, loss: 0.240, 28832/60000 datapoints
2025-03-06 19:37:29,446 - INFO - training batch 951, loss: 0.196, 30432/60000 datapoints
2025-03-06 19:37:29,642 - INFO - training batch 1001, loss: 0.623, 32032/60000 datapoints
2025-03-06 19:37:29,841 - INFO - training batch 1051, loss: 0.321, 33632/60000 datapoints
2025-03-06 19:37:30,054 - INFO - training batch 1101, loss: 0.290, 35232/60000 datapoints
2025-03-06 19:37:30,252 - INFO - training batch 1151, loss: 0.488, 36832/60000 datapoints
2025-03-06 19:37:30,446 - INFO - training batch 1201, loss: 0.416, 38432/60000 datapoints
2025-03-06 19:37:30,643 - INFO - training batch 1251, loss: 0.216, 40032/60000 datapoints
2025-03-06 19:37:30,837 - INFO - training batch 1301, loss: 0.338, 41632/60000 datapoints
2025-03-06 19:37:31,034 - INFO - training batch 1351, loss: 0.190, 43232/60000 datapoints
2025-03-06 19:37:31,229 - INFO - training batch 1401, loss: 0.262, 44832/60000 datapoints
2025-03-06 19:37:31,423 - INFO - training batch 1451, loss: 0.738, 46432/60000 datapoints
2025-03-06 19:37:31,622 - INFO - training batch 1501, loss: 0.245, 48032/60000 datapoints
2025-03-06 19:37:31,816 - INFO - training batch 1551, loss: 0.280, 49632/60000 datapoints
2025-03-06 19:37:32,019 - INFO - training batch 1601, loss: 0.162, 51232/60000 datapoints
2025-03-06 19:37:32,218 - INFO - training batch 1651, loss: 0.255, 52832/60000 datapoints
2025-03-06 19:37:32,411 - INFO - training batch 1701, loss: 0.101, 54432/60000 datapoints
2025-03-06 19:37:32,606 - INFO - training batch 1751, loss: 0.291, 56032/60000 datapoints
2025-03-06 19:37:32,803 - INFO - training batch 1801, loss: 0.077, 57632/60000 datapoints
2025-03-06 19:37:33,020 - INFO - training batch 1851, loss: 0.343, 59232/60000 datapoints
2025-03-06 19:37:33,122 - INFO - validation batch 1, loss: 0.477, 32/10016 datapoints
2025-03-06 19:37:33,280 - INFO - validation batch 51, loss: 0.296, 1632/10016 datapoints
2025-03-06 19:37:33,432 - INFO - validation batch 101, loss: 0.471, 3232/10016 datapoints
2025-03-06 19:37:33,587 - INFO - validation batch 151, loss: 0.313, 4832/10016 datapoints
2025-03-06 19:37:33,745 - INFO - validation batch 201, loss: 0.252, 6432/10016 datapoints
2025-03-06 19:37:33,897 - INFO - validation batch 251, loss: 0.355, 8032/10016 datapoints
2025-03-06 19:37:34,051 - INFO - validation batch 301, loss: 0.282, 9632/10016 datapoints
2025-03-06 19:37:34,089 - INFO - Epoch 404/800 done.
2025-03-06 19:37:34,089 - INFO - Final validation performance:
Loss: 0.349, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:37:34,089 - INFO - Beginning epoch 405/800
2025-03-06 19:37:34,096 - INFO - training batch 1, loss: 0.106, 32/60000 datapoints
2025-03-06 19:37:34,303 - INFO - training batch 51, loss: 0.335, 1632/60000 datapoints
2025-03-06 19:37:34,498 - INFO - training batch 101, loss: 0.289, 3232/60000 datapoints
2025-03-06 19:37:34,704 - INFO - training batch 151, loss: 0.072, 4832/60000 datapoints
2025-03-06 19:37:34,904 - INFO - training batch 201, loss: 0.114, 6432/60000 datapoints
2025-03-06 19:37:35,103 - INFO - training batch 251, loss: 0.180, 8032/60000 datapoints
2025-03-06 19:37:35,298 - INFO - training batch 301, loss: 0.375, 9632/60000 datapoints
2025-03-06 19:37:35,489 - INFO - training batch 351, loss: 0.380, 11232/60000 datapoints
2025-03-06 19:37:35,689 - INFO - training batch 401, loss: 0.208, 12832/60000 datapoints
2025-03-06 19:37:35,880 - INFO - training batch 451, loss: 0.333, 14432/60000 datapoints
2025-03-06 19:37:36,075 - INFO - training batch 501, loss: 0.440, 16032/60000 datapoints
2025-03-06 19:37:36,286 - INFO - training batch 551, loss: 0.473, 17632/60000 datapoints
2025-03-06 19:37:36,481 - INFO - training batch 601, loss: 0.760, 19232/60000 datapoints
2025-03-06 19:37:36,678 - INFO - training batch 651, loss: 0.231, 20832/60000 datapoints
2025-03-06 19:37:36,874 - INFO - training batch 701, loss: 0.106, 22432/60000 datapoints
2025-03-06 19:37:37,069 - INFO - training batch 751, loss: 0.345, 24032/60000 datapoints
2025-03-06 19:37:37,262 - INFO - training batch 801, loss: 0.174, 25632/60000 datapoints
2025-03-06 19:37:37,453 - INFO - training batch 851, loss: 0.131, 27232/60000 datapoints
2025-03-06 19:37:37,661 - INFO - training batch 901, loss: 0.138, 28832/60000 datapoints
2025-03-06 19:37:37,856 - INFO - training batch 951, loss: 0.307, 30432/60000 datapoints
2025-03-06 19:37:38,048 - INFO - training batch 1001, loss: 0.221, 32032/60000 datapoints
2025-03-06 19:37:38,249 - INFO - training batch 1051, loss: 0.358, 33632/60000 datapoints
2025-03-06 19:37:38,443 - INFO - training batch 1101, loss: 0.239, 35232/60000 datapoints
2025-03-06 19:37:38,639 - INFO - training batch 1151, loss: 0.238, 36832/60000 datapoints
2025-03-06 19:37:38,832 - INFO - training batch 1201, loss: 0.118, 38432/60000 datapoints
2025-03-06 19:37:39,033 - INFO - training batch 1251, loss: 0.374, 40032/60000 datapoints
2025-03-06 19:37:39,228 - INFO - training batch 1301, loss: 0.138, 41632/60000 datapoints
2025-03-06 19:37:39,423 - INFO - training batch 1351, loss: 0.277, 43232/60000 datapoints
2025-03-06 19:37:39,627 - INFO - training batch 1401, loss: 0.047, 44832/60000 datapoints
2025-03-06 19:37:39,820 - INFO - training batch 1451, loss: 0.213, 46432/60000 datapoints
2025-03-06 19:37:40,046 - INFO - training batch 1501, loss: 0.236, 48032/60000 datapoints
2025-03-06 19:37:40,276 - INFO - training batch 1551, loss: 0.327, 49632/60000 datapoints
2025-03-06 19:37:40,490 - INFO - training batch 1601, loss: 0.208, 51232/60000 datapoints
2025-03-06 19:37:40,701 - INFO - training batch 1651, loss: 0.102, 52832/60000 datapoints
2025-03-06 19:37:40,895 - INFO - training batch 1701, loss: 0.190, 54432/60000 datapoints
2025-03-06 19:37:41,091 - INFO - training batch 1751, loss: 0.273, 56032/60000 datapoints
2025-03-06 19:37:41,285 - INFO - training batch 1801, loss: 0.317, 57632/60000 datapoints
2025-03-06 19:37:41,477 - INFO - training batch 1851, loss: 0.181, 59232/60000 datapoints
2025-03-06 19:37:41,578 - INFO - validation batch 1, loss: 0.563, 32/10016 datapoints
2025-03-06 19:37:41,734 - INFO - validation batch 51, loss: 0.330, 1632/10016 datapoints
2025-03-06 19:37:41,890 - INFO - validation batch 101, loss: 0.315, 3232/10016 datapoints
2025-03-06 19:37:42,044 - INFO - validation batch 151, loss: 0.389, 4832/10016 datapoints
2025-03-06 19:37:42,199 - INFO - validation batch 201, loss: 0.085, 6432/10016 datapoints
2025-03-06 19:37:42,352 - INFO - validation batch 251, loss: 0.242, 8032/10016 datapoints
2025-03-06 19:37:42,505 - INFO - validation batch 301, loss: 0.109, 9632/10016 datapoints
2025-03-06 19:37:42,543 - INFO - Epoch 405/800 done.
2025-03-06 19:37:42,543 - INFO - Final validation performance:
Loss: 0.290, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:37:42,544 - INFO - Beginning epoch 406/800
2025-03-06 19:37:42,550 - INFO - training batch 1, loss: 0.335, 32/60000 datapoints
2025-03-06 19:37:42,747 - INFO - training batch 51, loss: 0.248, 1632/60000 datapoints
2025-03-06 19:37:42,943 - INFO - training batch 101, loss: 0.221, 3232/60000 datapoints
2025-03-06 19:37:43,148 - INFO - training batch 151, loss: 0.386, 4832/60000 datapoints
2025-03-06 19:37:43,342 - INFO - training batch 201, loss: 0.096, 6432/60000 datapoints
2025-03-06 19:37:43,536 - INFO - training batch 251, loss: 0.165, 8032/60000 datapoints
2025-03-06 19:37:43,743 - INFO - training batch 301, loss: 0.409, 9632/60000 datapoints
2025-03-06 19:37:43,950 - INFO - training batch 351, loss: 0.095, 11232/60000 datapoints
2025-03-06 19:37:44,148 - INFO - training batch 401, loss: 0.119, 12832/60000 datapoints
2025-03-06 19:37:44,345 - INFO - training batch 451, loss: 0.398, 14432/60000 datapoints
2025-03-06 19:37:44,540 - INFO - training batch 501, loss: 0.142, 16032/60000 datapoints
2025-03-06 19:37:44,738 - INFO - training batch 551, loss: 0.183, 17632/60000 datapoints
2025-03-06 19:37:44,938 - INFO - training batch 601, loss: 0.130, 19232/60000 datapoints
2025-03-06 19:37:45,132 - INFO - training batch 651, loss: 0.129, 20832/60000 datapoints
2025-03-06 19:37:45,329 - INFO - training batch 701, loss: 0.153, 22432/60000 datapoints
2025-03-06 19:37:45,523 - INFO - training batch 751, loss: 0.364, 24032/60000 datapoints
2025-03-06 19:37:45,721 - INFO - training batch 801, loss: 0.171, 25632/60000 datapoints
2025-03-06 19:37:45,916 - INFO - training batch 851, loss: 0.192, 27232/60000 datapoints
2025-03-06 19:37:46,110 - INFO - training batch 901, loss: 0.321, 28832/60000 datapoints
2025-03-06 19:37:46,308 - INFO - training batch 951, loss: 0.191, 30432/60000 datapoints
2025-03-06 19:37:46,501 - INFO - training batch 1001, loss: 0.213, 32032/60000 datapoints
2025-03-06 19:37:46,696 - INFO - training batch 1051, loss: 0.327, 33632/60000 datapoints
2025-03-06 19:37:46,889 - INFO - training batch 1101, loss: 0.306, 35232/60000 datapoints
2025-03-06 19:37:47,085 - INFO - training batch 1151, loss: 0.460, 36832/60000 datapoints
2025-03-06 19:37:47,281 - INFO - training batch 1201, loss: 0.202, 38432/60000 datapoints
2025-03-06 19:37:47,474 - INFO - training batch 1251, loss: 0.182, 40032/60000 datapoints
2025-03-06 19:37:47,672 - INFO - training batch 1301, loss: 0.285, 41632/60000 datapoints
2025-03-06 19:37:47,866 - INFO - training batch 1351, loss: 0.132, 43232/60000 datapoints
2025-03-06 19:37:48,075 - INFO - training batch 1401, loss: 0.200, 44832/60000 datapoints
2025-03-06 19:37:48,280 - INFO - training batch 1451, loss: 0.458, 46432/60000 datapoints
2025-03-06 19:37:48,472 - INFO - training batch 1501, loss: 0.331, 48032/60000 datapoints
2025-03-06 19:37:48,670 - INFO - training batch 1551, loss: 0.296, 49632/60000 datapoints
2025-03-06 19:37:48,866 - INFO - training batch 1601, loss: 0.263, 51232/60000 datapoints
2025-03-06 19:37:49,057 - INFO - training batch 1651, loss: 0.327, 52832/60000 datapoints
2025-03-06 19:37:49,253 - INFO - training batch 1701, loss: 0.134, 54432/60000 datapoints
2025-03-06 19:37:49,447 - INFO - training batch 1751, loss: 0.106, 56032/60000 datapoints
2025-03-06 19:37:49,641 - INFO - training batch 1801, loss: 0.143, 57632/60000 datapoints
2025-03-06 19:37:49,836 - INFO - training batch 1851, loss: 0.307, 59232/60000 datapoints
2025-03-06 19:37:49,937 - INFO - validation batch 1, loss: 0.276, 32/10016 datapoints
2025-03-06 19:37:50,106 - INFO - validation batch 51, loss: 0.318, 1632/10016 datapoints
2025-03-06 19:37:50,263 - INFO - validation batch 101, loss: 0.169, 3232/10016 datapoints
2025-03-06 19:37:50,414 - INFO - validation batch 151, loss: 0.420, 4832/10016 datapoints
2025-03-06 19:37:50,566 - INFO - validation batch 201, loss: 0.202, 6432/10016 datapoints
2025-03-06 19:37:50,723 - INFO - validation batch 251, loss: 0.197, 8032/10016 datapoints
2025-03-06 19:37:50,874 - INFO - validation batch 301, loss: 0.145, 9632/10016 datapoints
2025-03-06 19:37:50,910 - INFO - Epoch 406/800 done.
2025-03-06 19:37:50,911 - INFO - Final validation performance:
Loss: 0.247, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:37:50,911 - INFO - Beginning epoch 407/800
2025-03-06 19:37:50,918 - INFO - training batch 1, loss: 0.135, 32/60000 datapoints
2025-03-06 19:37:51,123 - INFO - training batch 51, loss: 0.117, 1632/60000 datapoints
2025-03-06 19:37:51,319 - INFO - training batch 101, loss: 0.164, 3232/60000 datapoints
2025-03-06 19:37:51,514 - INFO - training batch 151, loss: 0.338, 4832/60000 datapoints
2025-03-06 19:37:51,715 - INFO - training batch 201, loss: 0.374, 6432/60000 datapoints
2025-03-06 19:37:51,913 - INFO - training batch 251, loss: 0.167, 8032/60000 datapoints
2025-03-06 19:37:52,103 - INFO - training batch 301, loss: 0.270, 9632/60000 datapoints
2025-03-06 19:37:52,300 - INFO - training batch 351, loss: 0.124, 11232/60000 datapoints
2025-03-06 19:37:52,491 - INFO - training batch 401, loss: 0.309, 12832/60000 datapoints
2025-03-06 19:37:52,685 - INFO - training batch 451, loss: 0.338, 14432/60000 datapoints
2025-03-06 19:37:52,878 - INFO - training batch 501, loss: 0.201, 16032/60000 datapoints
2025-03-06 19:37:53,073 - INFO - training batch 551, loss: 0.179, 17632/60000 datapoints
2025-03-06 19:37:53,267 - INFO - training batch 601, loss: 0.355, 19232/60000 datapoints
2025-03-06 19:37:53,455 - INFO - training batch 651, loss: 0.836, 20832/60000 datapoints
2025-03-06 19:37:53,648 - INFO - training batch 701, loss: 0.437, 22432/60000 datapoints
2025-03-06 19:37:53,842 - INFO - training batch 751, loss: 0.187, 24032/60000 datapoints
2025-03-06 19:37:54,035 - INFO - training batch 801, loss: 0.473, 25632/60000 datapoints
2025-03-06 19:37:54,229 - INFO - training batch 851, loss: 0.529, 27232/60000 datapoints
2025-03-06 19:37:54,421 - INFO - training batch 901, loss: 0.374, 28832/60000 datapoints
2025-03-06 19:37:54,612 - INFO - training batch 951, loss: 0.308, 30432/60000 datapoints
2025-03-06 19:37:54,810 - INFO - training batch 1001, loss: 0.206, 32032/60000 datapoints
2025-03-06 19:37:55,009 - INFO - training batch 1051, loss: 0.144, 33632/60000 datapoints
2025-03-06 19:37:55,199 - INFO - training batch 1101, loss: 0.384, 35232/60000 datapoints
2025-03-06 19:37:55,392 - INFO - training batch 1151, loss: 0.246, 36832/60000 datapoints
2025-03-06 19:37:55,584 - INFO - training batch 1201, loss: 0.457, 38432/60000 datapoints
2025-03-06 19:37:55,784 - INFO - training batch 1251, loss: 0.168, 40032/60000 datapoints
2025-03-06 19:37:55,977 - INFO - training batch 1301, loss: 0.100, 41632/60000 datapoints
2025-03-06 19:37:56,167 - INFO - training batch 1351, loss: 0.659, 43232/60000 datapoints
2025-03-06 19:37:56,362 - INFO - training batch 1401, loss: 0.202, 44832/60000 datapoints
2025-03-06 19:37:56,552 - INFO - training batch 1451, loss: 0.269, 46432/60000 datapoints
2025-03-06 19:37:56,744 - INFO - training batch 1501, loss: 0.368, 48032/60000 datapoints
2025-03-06 19:37:56,939 - INFO - training batch 1551, loss: 0.405, 49632/60000 datapoints
2025-03-06 19:37:57,128 - INFO - training batch 1601, loss: 0.342, 51232/60000 datapoints
2025-03-06 19:37:57,319 - INFO - training batch 1651, loss: 0.202, 52832/60000 datapoints
2025-03-06 19:37:57,511 - INFO - training batch 1701, loss: 0.354, 54432/60000 datapoints
2025-03-06 19:37:57,704 - INFO - training batch 1751, loss: 0.219, 56032/60000 datapoints
2025-03-06 19:37:57,895 - INFO - training batch 1801, loss: 0.361, 57632/60000 datapoints
2025-03-06 19:37:58,086 - INFO - training batch 1851, loss: 0.458, 59232/60000 datapoints
2025-03-06 19:37:58,183 - INFO - validation batch 1, loss: 0.649, 32/10016 datapoints
2025-03-06 19:37:58,336 - INFO - validation batch 51, loss: 0.249, 1632/10016 datapoints
2025-03-06 19:37:58,488 - INFO - validation batch 101, loss: 0.327, 3232/10016 datapoints
2025-03-06 19:37:58,642 - INFO - validation batch 151, loss: 0.118, 4832/10016 datapoints
2025-03-06 19:37:58,794 - INFO - validation batch 201, loss: 0.088, 6432/10016 datapoints
2025-03-06 19:37:58,945 - INFO - validation batch 251, loss: 0.209, 8032/10016 datapoints
2025-03-06 19:37:59,093 - INFO - validation batch 301, loss: 0.189, 9632/10016 datapoints
2025-03-06 19:37:59,129 - INFO - Epoch 407/800 done.
2025-03-06 19:37:59,130 - INFO - Final validation performance:
Loss: 0.261, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:37:59,130 - INFO - Beginning epoch 408/800
2025-03-06 19:37:59,136 - INFO - training batch 1, loss: 0.316, 32/60000 datapoints
2025-03-06 19:37:59,332 - INFO - training batch 51, loss: 0.351, 1632/60000 datapoints
2025-03-06 19:37:59,522 - INFO - training batch 101, loss: 0.239, 3232/60000 datapoints
2025-03-06 19:37:59,725 - INFO - training batch 151, loss: 0.152, 4832/60000 datapoints
2025-03-06 19:37:59,921 - INFO - training batch 201, loss: 0.427, 6432/60000 datapoints
2025-03-06 19:38:00,114 - INFO - training batch 251, loss: 0.133, 8032/60000 datapoints
2025-03-06 19:38:00,331 - INFO - training batch 301, loss: 0.296, 9632/60000 datapoints
2025-03-06 19:38:00,525 - INFO - training batch 351, loss: 0.361, 11232/60000 datapoints
2025-03-06 19:38:00,722 - INFO - training batch 401, loss: 0.208, 12832/60000 datapoints
2025-03-06 19:38:00,916 - INFO - training batch 451, loss: 0.568, 14432/60000 datapoints
2025-03-06 19:38:01,106 - INFO - training batch 501, loss: 0.205, 16032/60000 datapoints
2025-03-06 19:38:01,296 - INFO - training batch 551, loss: 0.256, 17632/60000 datapoints
2025-03-06 19:38:01,490 - INFO - training batch 601, loss: 0.150, 19232/60000 datapoints
2025-03-06 19:38:01,683 - INFO - training batch 651, loss: 0.147, 20832/60000 datapoints
2025-03-06 19:38:01,876 - INFO - training batch 701, loss: 0.337, 22432/60000 datapoints
2025-03-06 19:38:02,069 - INFO - training batch 751, loss: 0.231, 24032/60000 datapoints
2025-03-06 19:38:02,262 - INFO - training batch 801, loss: 0.226, 25632/60000 datapoints
2025-03-06 19:38:02,455 - INFO - training batch 851, loss: 0.191, 27232/60000 datapoints
2025-03-06 19:38:02,648 - INFO - training batch 901, loss: 0.548, 28832/60000 datapoints
2025-03-06 19:38:02,840 - INFO - training batch 951, loss: 0.301, 30432/60000 datapoints
2025-03-06 19:38:03,032 - INFO - training batch 1001, loss: 0.130, 32032/60000 datapoints
2025-03-06 19:38:03,223 - INFO - training batch 1051, loss: 0.301, 33632/60000 datapoints
2025-03-06 19:38:03,416 - INFO - training batch 1101, loss: 0.143, 35232/60000 datapoints
2025-03-06 19:38:03,608 - INFO - training batch 1151, loss: 0.268, 36832/60000 datapoints
2025-03-06 19:38:03,802 - INFO - training batch 1201, loss: 0.277, 38432/60000 datapoints
2025-03-06 19:38:03,996 - INFO - training batch 1251, loss: 0.227, 40032/60000 datapoints
2025-03-06 19:38:04,186 - INFO - training batch 1301, loss: 0.327, 41632/60000 datapoints
2025-03-06 19:38:04,381 - INFO - training batch 1351, loss: 0.112, 43232/60000 datapoints
2025-03-06 19:38:04,573 - INFO - training batch 1401, loss: 0.235, 44832/60000 datapoints
2025-03-06 19:38:04,765 - INFO - training batch 1451, loss: 0.151, 46432/60000 datapoints
2025-03-06 19:38:04,965 - INFO - training batch 1501, loss: 0.519, 48032/60000 datapoints
2025-03-06 19:38:05,158 - INFO - training batch 1551, loss: 0.384, 49632/60000 datapoints
2025-03-06 19:38:05,349 - INFO - training batch 1601, loss: 0.268, 51232/60000 datapoints
2025-03-06 19:38:05,542 - INFO - training batch 1651, loss: 0.377, 52832/60000 datapoints
2025-03-06 19:38:05,737 - INFO - training batch 1701, loss: 0.330, 54432/60000 datapoints
2025-03-06 19:38:05,930 - INFO - training batch 1751, loss: 0.518, 56032/60000 datapoints
2025-03-06 19:38:06,121 - INFO - training batch 1801, loss: 0.175, 57632/60000 datapoints
2025-03-06 19:38:06,316 - INFO - training batch 1851, loss: 0.494, 59232/60000 datapoints
2025-03-06 19:38:06,414 - INFO - validation batch 1, loss: 0.149, 32/10016 datapoints
2025-03-06 19:38:06,563 - INFO - validation batch 51, loss: 0.344, 1632/10016 datapoints
2025-03-06 19:38:06,714 - INFO - validation batch 101, loss: 0.397, 3232/10016 datapoints
2025-03-06 19:38:06,863 - INFO - validation batch 151, loss: 0.248, 4832/10016 datapoints
2025-03-06 19:38:07,018 - INFO - validation batch 201, loss: 0.179, 6432/10016 datapoints
2025-03-06 19:38:07,169 - INFO - validation batch 251, loss: 0.159, 8032/10016 datapoints
2025-03-06 19:38:07,333 - INFO - validation batch 301, loss: 0.481, 9632/10016 datapoints
2025-03-06 19:38:07,373 - INFO - Epoch 408/800 done.
2025-03-06 19:38:07,373 - INFO - Final validation performance:
Loss: 0.280, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:38:07,374 - INFO - Beginning epoch 409/800
2025-03-06 19:38:07,381 - INFO - training batch 1, loss: 0.462, 32/60000 datapoints
2025-03-06 19:38:07,598 - INFO - training batch 51, loss: 0.104, 1632/60000 datapoints
2025-03-06 19:38:07,794 - INFO - training batch 101, loss: 0.101, 3232/60000 datapoints
2025-03-06 19:38:07,995 - INFO - training batch 151, loss: 0.153, 4832/60000 datapoints
2025-03-06 19:38:08,188 - INFO - training batch 201, loss: 0.236, 6432/60000 datapoints
2025-03-06 19:38:08,387 - INFO - training batch 251, loss: 0.227, 8032/60000 datapoints
2025-03-06 19:38:08,580 - INFO - training batch 301, loss: 0.338, 9632/60000 datapoints
2025-03-06 19:38:08,779 - INFO - training batch 351, loss: 0.371, 11232/60000 datapoints
2025-03-06 19:38:08,973 - INFO - training batch 401, loss: 0.145, 12832/60000 datapoints
2025-03-06 19:38:09,172 - INFO - training batch 451, loss: 0.217, 14432/60000 datapoints
2025-03-06 19:38:09,368 - INFO - training batch 501, loss: 0.176, 16032/60000 datapoints
2025-03-06 19:38:09,563 - INFO - training batch 551, loss: 0.417, 17632/60000 datapoints
2025-03-06 19:38:09,762 - INFO - training batch 601, loss: 0.226, 19232/60000 datapoints
2025-03-06 19:38:09,960 - INFO - training batch 651, loss: 0.562, 20832/60000 datapoints
2025-03-06 19:38:10,158 - INFO - training batch 701, loss: 0.212, 22432/60000 datapoints
2025-03-06 19:38:10,375 - INFO - training batch 751, loss: 0.179, 24032/60000 datapoints
2025-03-06 19:38:10,570 - INFO - training batch 801, loss: 1.058, 25632/60000 datapoints
2025-03-06 19:38:10,768 - INFO - training batch 851, loss: 0.196, 27232/60000 datapoints
2025-03-06 19:38:10,963 - INFO - training batch 901, loss: 0.092, 28832/60000 datapoints
2025-03-06 19:38:11,155 - INFO - training batch 951, loss: 0.299, 30432/60000 datapoints
2025-03-06 19:38:11,351 - INFO - training batch 1001, loss: 0.262, 32032/60000 datapoints
2025-03-06 19:38:11,547 - INFO - training batch 1051, loss: 0.175, 33632/60000 datapoints
2025-03-06 19:38:11,741 - INFO - training batch 1101, loss: 0.173, 35232/60000 datapoints
2025-03-06 19:38:11,938 - INFO - training batch 1151, loss: 0.152, 36832/60000 datapoints
2025-03-06 19:38:12,132 - INFO - training batch 1201, loss: 0.178, 38432/60000 datapoints
2025-03-06 19:38:12,328 - INFO - training batch 1251, loss: 0.218, 40032/60000 datapoints
2025-03-06 19:38:12,525 - INFO - training batch 1301, loss: 0.276, 41632/60000 datapoints
2025-03-06 19:38:12,719 - INFO - training batch 1351, loss: 0.076, 43232/60000 datapoints
2025-03-06 19:38:12,914 - INFO - training batch 1401, loss: 0.248, 44832/60000 datapoints
2025-03-06 19:38:13,107 - INFO - training batch 1451, loss: 0.659, 46432/60000 datapoints
2025-03-06 19:38:13,299 - INFO - training batch 1501, loss: 0.609, 48032/60000 datapoints
2025-03-06 19:38:13,496 - INFO - training batch 1551, loss: 0.240, 49632/60000 datapoints
2025-03-06 19:38:13,693 - INFO - training batch 1601, loss: 0.442, 51232/60000 datapoints
2025-03-06 19:38:13,890 - INFO - training batch 1651, loss: 0.187, 52832/60000 datapoints
2025-03-06 19:38:14,086 - INFO - training batch 1701, loss: 0.176, 54432/60000 datapoints
2025-03-06 19:38:14,282 - INFO - training batch 1751, loss: 0.184, 56032/60000 datapoints
2025-03-06 19:38:14,476 - INFO - training batch 1801, loss: 0.131, 57632/60000 datapoints
2025-03-06 19:38:14,675 - INFO - training batch 1851, loss: 0.289, 59232/60000 datapoints
2025-03-06 19:38:14,784 - INFO - validation batch 1, loss: 0.155, 32/10016 datapoints
2025-03-06 19:38:14,942 - INFO - validation batch 51, loss: 0.400, 1632/10016 datapoints
2025-03-06 19:38:15,097 - INFO - validation batch 101, loss: 0.099, 3232/10016 datapoints
2025-03-06 19:38:15,250 - INFO - validation batch 151, loss: 0.478, 4832/10016 datapoints
2025-03-06 19:38:15,404 - INFO - validation batch 201, loss: 0.258, 6432/10016 datapoints
2025-03-06 19:38:15,560 - INFO - validation batch 251, loss: 0.523, 8032/10016 datapoints
2025-03-06 19:38:15,714 - INFO - validation batch 301, loss: 0.164, 9632/10016 datapoints
2025-03-06 19:38:15,753 - INFO - Epoch 409/800 done.
2025-03-06 19:38:15,753 - INFO - Final validation performance:
Loss: 0.297, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:38:15,754 - INFO - Beginning epoch 410/800
2025-03-06 19:38:15,761 - INFO - training batch 1, loss: 0.543, 32/60000 datapoints
2025-03-06 19:38:15,974 - INFO - training batch 51, loss: 0.335, 1632/60000 datapoints
2025-03-06 19:38:16,168 - INFO - training batch 101, loss: 0.243, 3232/60000 datapoints
2025-03-06 19:38:16,368 - INFO - training batch 151, loss: 0.567, 4832/60000 datapoints
2025-03-06 19:38:16,565 - INFO - training batch 201, loss: 0.448, 6432/60000 datapoints
2025-03-06 19:38:16,763 - INFO - training batch 251, loss: 0.208, 8032/60000 datapoints
2025-03-06 19:38:16,954 - INFO - training batch 301, loss: 0.329, 9632/60000 datapoints
2025-03-06 19:38:17,147 - INFO - training batch 351, loss: 0.295, 11232/60000 datapoints
2025-03-06 19:38:17,339 - INFO - training batch 401, loss: 0.154, 12832/60000 datapoints
2025-03-06 19:38:17,531 - INFO - training batch 451, loss: 0.240, 14432/60000 datapoints
2025-03-06 19:38:17,724 - INFO - training batch 501, loss: 0.258, 16032/60000 datapoints
2025-03-06 19:38:17,915 - INFO - training batch 551, loss: 0.538, 17632/60000 datapoints
2025-03-06 19:38:18,108 - INFO - training batch 601, loss: 0.136, 19232/60000 datapoints
2025-03-06 19:38:18,303 - INFO - training batch 651, loss: 0.231, 20832/60000 datapoints
2025-03-06 19:38:18,495 - INFO - training batch 701, loss: 0.402, 22432/60000 datapoints
2025-03-06 19:38:18,688 - INFO - training batch 751, loss: 0.430, 24032/60000 datapoints
2025-03-06 19:38:18,881 - INFO - training batch 801, loss: 0.225, 25632/60000 datapoints
2025-03-06 19:38:19,072 - INFO - training batch 851, loss: 0.237, 27232/60000 datapoints
2025-03-06 19:38:19,265 - INFO - training batch 901, loss: 0.684, 28832/60000 datapoints
2025-03-06 19:38:19,454 - INFO - training batch 951, loss: 0.263, 30432/60000 datapoints
2025-03-06 19:38:19,647 - INFO - training batch 1001, loss: 0.194, 32032/60000 datapoints
2025-03-06 19:38:19,838 - INFO - training batch 1051, loss: 0.516, 33632/60000 datapoints
2025-03-06 19:38:20,030 - INFO - training batch 1101, loss: 0.110, 35232/60000 datapoints
2025-03-06 19:38:20,222 - INFO - training batch 1151, loss: 0.113, 36832/60000 datapoints
2025-03-06 19:38:20,435 - INFO - training batch 1201, loss: 0.273, 38432/60000 datapoints
2025-03-06 19:38:20,631 - INFO - training batch 1251, loss: 0.370, 40032/60000 datapoints
2025-03-06 19:38:20,821 - INFO - training batch 1301, loss: 0.399, 41632/60000 datapoints
2025-03-06 19:38:21,014 - INFO - training batch 1351, loss: 0.238, 43232/60000 datapoints
2025-03-06 19:38:21,207 - INFO - training batch 1401, loss: 0.278, 44832/60000 datapoints
2025-03-06 19:38:21,399 - INFO - training batch 1451, loss: 0.333, 46432/60000 datapoints
2025-03-06 19:38:21,592 - INFO - training batch 1501, loss: 0.199, 48032/60000 datapoints
2025-03-06 19:38:21,787 - INFO - training batch 1551, loss: 0.280, 49632/60000 datapoints
2025-03-06 19:38:21,983 - INFO - training batch 1601, loss: 0.164, 51232/60000 datapoints
2025-03-06 19:38:22,176 - INFO - training batch 1651, loss: 0.205, 52832/60000 datapoints
2025-03-06 19:38:22,371 - INFO - training batch 1701, loss: 0.425, 54432/60000 datapoints
2025-03-06 19:38:22,565 - INFO - training batch 1751, loss: 0.493, 56032/60000 datapoints
2025-03-06 19:38:22,759 - INFO - training batch 1801, loss: 0.419, 57632/60000 datapoints
2025-03-06 19:38:22,949 - INFO - training batch 1851, loss: 0.235, 59232/60000 datapoints
2025-03-06 19:38:23,047 - INFO - validation batch 1, loss: 0.307, 32/10016 datapoints
2025-03-06 19:38:23,198 - INFO - validation batch 51, loss: 0.107, 1632/10016 datapoints
2025-03-06 19:38:23,348 - INFO - validation batch 101, loss: 0.242, 3232/10016 datapoints
2025-03-06 19:38:23,499 - INFO - validation batch 151, loss: 0.202, 4832/10016 datapoints
2025-03-06 19:38:23,658 - INFO - validation batch 201, loss: 0.292, 6432/10016 datapoints
2025-03-06 19:38:23,807 - INFO - validation batch 251, loss: 0.297, 8032/10016 datapoints
2025-03-06 19:38:23,960 - INFO - validation batch 301, loss: 0.261, 9632/10016 datapoints
2025-03-06 19:38:23,996 - INFO - Epoch 410/800 done.
2025-03-06 19:38:23,996 - INFO - Final validation performance:
Loss: 0.244, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:38:23,996 - INFO - Beginning epoch 411/800
2025-03-06 19:38:24,002 - INFO - training batch 1, loss: 0.077, 32/60000 datapoints
2025-03-06 19:38:24,197 - INFO - training batch 51, loss: 0.383, 1632/60000 datapoints
2025-03-06 19:38:24,390 - INFO - training batch 101, loss: 0.373, 3232/60000 datapoints
2025-03-06 19:38:24,594 - INFO - training batch 151, loss: 0.245, 4832/60000 datapoints
2025-03-06 19:38:24,786 - INFO - training batch 201, loss: 0.083, 6432/60000 datapoints
2025-03-06 19:38:24,981 - INFO - training batch 251, loss: 0.117, 8032/60000 datapoints
2025-03-06 19:38:25,181 - INFO - training batch 301, loss: 0.164, 9632/60000 datapoints
2025-03-06 19:38:25,378 - INFO - training batch 351, loss: 0.268, 11232/60000 datapoints
2025-03-06 19:38:25,573 - INFO - training batch 401, loss: 0.203, 12832/60000 datapoints
2025-03-06 19:38:25,769 - INFO - training batch 451, loss: 0.184, 14432/60000 datapoints
2025-03-06 19:38:25,959 - INFO - training batch 501, loss: 0.573, 16032/60000 datapoints
2025-03-06 19:38:26,150 - INFO - training batch 551, loss: 0.381, 17632/60000 datapoints
2025-03-06 19:38:26,342 - INFO - training batch 601, loss: 0.461, 19232/60000 datapoints
2025-03-06 19:38:26,536 - INFO - training batch 651, loss: 0.174, 20832/60000 datapoints
2025-03-06 19:38:26,729 - INFO - training batch 701, loss: 0.455, 22432/60000 datapoints
2025-03-06 19:38:26,920 - INFO - training batch 751, loss: 0.551, 24032/60000 datapoints
2025-03-06 19:38:27,112 - INFO - training batch 801, loss: 0.382, 25632/60000 datapoints
2025-03-06 19:38:27,302 - INFO - training batch 851, loss: 0.475, 27232/60000 datapoints
2025-03-06 19:38:27,494 - INFO - training batch 901, loss: 0.155, 28832/60000 datapoints
2025-03-06 19:38:27,691 - INFO - training batch 951, loss: 0.352, 30432/60000 datapoints
2025-03-06 19:38:27,883 - INFO - training batch 1001, loss: 0.164, 32032/60000 datapoints
2025-03-06 19:38:28,076 - INFO - training batch 1051, loss: 0.417, 33632/60000 datapoints
2025-03-06 19:38:28,269 - INFO - training batch 1101, loss: 0.320, 35232/60000 datapoints
2025-03-06 19:38:28,463 - INFO - training batch 1151, loss: 0.221, 36832/60000 datapoints
2025-03-06 19:38:28,656 - INFO - training batch 1201, loss: 0.460, 38432/60000 datapoints
2025-03-06 19:38:28,853 - INFO - training batch 1251, loss: 0.165, 40032/60000 datapoints
2025-03-06 19:38:29,045 - INFO - training batch 1301, loss: 0.484, 41632/60000 datapoints
2025-03-06 19:38:29,241 - INFO - training batch 1351, loss: 0.169, 43232/60000 datapoints
2025-03-06 19:38:29,434 - INFO - training batch 1401, loss: 0.240, 44832/60000 datapoints
2025-03-06 19:38:29,636 - INFO - training batch 1451, loss: 0.377, 46432/60000 datapoints
2025-03-06 19:38:29,831 - INFO - training batch 1501, loss: 0.178, 48032/60000 datapoints
2025-03-06 19:38:30,026 - INFO - training batch 1551, loss: 0.116, 49632/60000 datapoints
2025-03-06 19:38:30,220 - INFO - training batch 1601, loss: 0.268, 51232/60000 datapoints
2025-03-06 19:38:30,433 - INFO - training batch 1651, loss: 0.250, 52832/60000 datapoints
2025-03-06 19:38:30,632 - INFO - training batch 1701, loss: 0.509, 54432/60000 datapoints
2025-03-06 19:38:30,823 - INFO - training batch 1751, loss: 0.655, 56032/60000 datapoints
2025-03-06 19:38:31,017 - INFO - training batch 1801, loss: 0.315, 57632/60000 datapoints
2025-03-06 19:38:31,213 - INFO - training batch 1851, loss: 0.241, 59232/60000 datapoints
2025-03-06 19:38:31,316 - INFO - validation batch 1, loss: 0.469, 32/10016 datapoints
2025-03-06 19:38:31,469 - INFO - validation batch 51, loss: 0.470, 1632/10016 datapoints
2025-03-06 19:38:31,622 - INFO - validation batch 101, loss: 0.113, 3232/10016 datapoints
2025-03-06 19:38:31,777 - INFO - validation batch 151, loss: 0.299, 4832/10016 datapoints
2025-03-06 19:38:31,930 - INFO - validation batch 201, loss: 0.218, 6432/10016 datapoints
2025-03-06 19:38:32,082 - INFO - validation batch 251, loss: 0.363, 8032/10016 datapoints
2025-03-06 19:38:32,240 - INFO - validation batch 301, loss: 0.272, 9632/10016 datapoints
2025-03-06 19:38:32,278 - INFO - Epoch 411/800 done.
2025-03-06 19:38:32,278 - INFO - Final validation performance:
Loss: 0.315, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:38:32,279 - INFO - Beginning epoch 412/800
2025-03-06 19:38:32,285 - INFO - training batch 1, loss: 0.274, 32/60000 datapoints
2025-03-06 19:38:32,495 - INFO - training batch 51, loss: 0.255, 1632/60000 datapoints
2025-03-06 19:38:32,694 - INFO - training batch 101, loss: 0.215, 3232/60000 datapoints
2025-03-06 19:38:32,896 - INFO - training batch 151, loss: 0.463, 4832/60000 datapoints
2025-03-06 19:38:33,092 - INFO - training batch 201, loss: 0.496, 6432/60000 datapoints
2025-03-06 19:38:33,310 - INFO - training batch 251, loss: 0.101, 8032/60000 datapoints
2025-03-06 19:38:33,513 - INFO - training batch 301, loss: 0.246, 9632/60000 datapoints
2025-03-06 19:38:33,724 - INFO - training batch 351, loss: 0.172, 11232/60000 datapoints
2025-03-06 19:38:33,982 - INFO - training batch 401, loss: 0.251, 12832/60000 datapoints
2025-03-06 19:38:34,177 - INFO - training batch 451, loss: 0.319, 14432/60000 datapoints
2025-03-06 19:38:34,373 - INFO - training batch 501, loss: 0.417, 16032/60000 datapoints
2025-03-06 19:38:34,570 - INFO - training batch 551, loss: 0.530, 17632/60000 datapoints
2025-03-06 19:38:34,768 - INFO - training batch 601, loss: 0.168, 19232/60000 datapoints
2025-03-06 19:38:34,965 - INFO - training batch 651, loss: 0.228, 20832/60000 datapoints
2025-03-06 19:38:35,160 - INFO - training batch 701, loss: 0.227, 22432/60000 datapoints
2025-03-06 19:38:35,355 - INFO - training batch 751, loss: 0.144, 24032/60000 datapoints
2025-03-06 19:38:35,553 - INFO - training batch 801, loss: 0.326, 25632/60000 datapoints
2025-03-06 19:38:35,751 - INFO - training batch 851, loss: 0.321, 27232/60000 datapoints
2025-03-06 19:38:35,949 - INFO - training batch 901, loss: 0.131, 28832/60000 datapoints
2025-03-06 19:38:36,145 - INFO - training batch 951, loss: 0.144, 30432/60000 datapoints
2025-03-06 19:38:36,345 - INFO - training batch 1001, loss: 0.297, 32032/60000 datapoints
2025-03-06 19:38:36,540 - INFO - training batch 1051, loss: 0.424, 33632/60000 datapoints
2025-03-06 19:38:36,734 - INFO - training batch 1101, loss: 0.150, 35232/60000 datapoints
2025-03-06 19:38:36,927 - INFO - training batch 1151, loss: 0.276, 36832/60000 datapoints
2025-03-06 19:38:37,120 - INFO - training batch 1201, loss: 0.153, 38432/60000 datapoints
2025-03-06 19:38:37,315 - INFO - training batch 1251, loss: 0.229, 40032/60000 datapoints
2025-03-06 19:38:37,506 - INFO - training batch 1301, loss: 0.133, 41632/60000 datapoints
2025-03-06 19:38:37,715 - INFO - training batch 1351, loss: 0.262, 43232/60000 datapoints
2025-03-06 19:38:37,908 - INFO - training batch 1401, loss: 0.329, 44832/60000 datapoints
2025-03-06 19:38:38,099 - INFO - training batch 1451, loss: 0.332, 46432/60000 datapoints
2025-03-06 19:38:38,291 - INFO - training batch 1501, loss: 0.152, 48032/60000 datapoints
2025-03-06 19:38:38,488 - INFO - training batch 1551, loss: 0.306, 49632/60000 datapoints
2025-03-06 19:38:38,684 - INFO - training batch 1601, loss: 0.348, 51232/60000 datapoints
2025-03-06 19:38:38,880 - INFO - training batch 1651, loss: 0.707, 52832/60000 datapoints
2025-03-06 19:38:39,078 - INFO - training batch 1701, loss: 0.169, 54432/60000 datapoints
2025-03-06 19:38:39,270 - INFO - training batch 1751, loss: 0.219, 56032/60000 datapoints
2025-03-06 19:38:39,461 - INFO - training batch 1801, loss: 0.430, 57632/60000 datapoints
2025-03-06 19:38:39,659 - INFO - training batch 1851, loss: 0.153, 59232/60000 datapoints
2025-03-06 19:38:39,762 - INFO - validation batch 1, loss: 0.199, 32/10016 datapoints
2025-03-06 19:38:39,915 - INFO - validation batch 51, loss: 0.163, 1632/10016 datapoints
2025-03-06 19:38:40,069 - INFO - validation batch 101, loss: 0.304, 3232/10016 datapoints
2025-03-06 19:38:40,221 - INFO - validation batch 151, loss: 0.209, 4832/10016 datapoints
2025-03-06 19:38:40,378 - INFO - validation batch 201, loss: 0.593, 6432/10016 datapoints
2025-03-06 19:38:40,550 - INFO - validation batch 251, loss: 0.223, 8032/10016 datapoints
2025-03-06 19:38:40,706 - INFO - validation batch 301, loss: 0.173, 9632/10016 datapoints
2025-03-06 19:38:40,744 - INFO - Epoch 412/800 done.
2025-03-06 19:38:40,744 - INFO - Final validation performance:
Loss: 0.266, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:38:40,745 - INFO - Beginning epoch 413/800
2025-03-06 19:38:40,751 - INFO - training batch 1, loss: 0.509, 32/60000 datapoints
2025-03-06 19:38:40,948 - INFO - training batch 51, loss: 0.436, 1632/60000 datapoints
2025-03-06 19:38:41,140 - INFO - training batch 101, loss: 0.580, 3232/60000 datapoints
2025-03-06 19:38:41,346 - INFO - training batch 151, loss: 0.434, 4832/60000 datapoints
2025-03-06 19:38:41,543 - INFO - training batch 201, loss: 0.127, 6432/60000 datapoints
2025-03-06 19:38:41,742 - INFO - training batch 251, loss: 0.338, 8032/60000 datapoints
2025-03-06 19:38:41,940 - INFO - training batch 301, loss: 0.191, 9632/60000 datapoints
2025-03-06 19:38:42,137 - INFO - training batch 351, loss: 0.304, 11232/60000 datapoints
2025-03-06 19:38:42,333 - INFO - training batch 401, loss: 0.420, 12832/60000 datapoints
2025-03-06 19:38:42,531 - INFO - training batch 451, loss: 0.471, 14432/60000 datapoints
2025-03-06 19:38:42,726 - INFO - training batch 501, loss: 0.337, 16032/60000 datapoints
2025-03-06 19:38:42,922 - INFO - training batch 551, loss: 0.159, 17632/60000 datapoints
2025-03-06 19:38:43,115 - INFO - training batch 601, loss: 0.429, 19232/60000 datapoints
2025-03-06 19:38:43,309 - INFO - training batch 651, loss: 0.396, 20832/60000 datapoints
2025-03-06 19:38:43,502 - INFO - training batch 701, loss: 0.055, 22432/60000 datapoints
2025-03-06 19:38:43,697 - INFO - training batch 751, loss: 0.200, 24032/60000 datapoints
2025-03-06 19:38:43,893 - INFO - training batch 801, loss: 0.441, 25632/60000 datapoints
2025-03-06 19:38:44,087 - INFO - training batch 851, loss: 0.151, 27232/60000 datapoints
2025-03-06 19:38:44,278 - INFO - training batch 901, loss: 0.145, 28832/60000 datapoints
2025-03-06 19:38:44,476 - INFO - training batch 951, loss: 0.208, 30432/60000 datapoints
2025-03-06 19:38:44,675 - INFO - training batch 1001, loss: 0.285, 32032/60000 datapoints
2025-03-06 19:38:44,872 - INFO - training batch 1051, loss: 0.146, 33632/60000 datapoints
2025-03-06 19:38:45,068 - INFO - training batch 1101, loss: 0.241, 35232/60000 datapoints
2025-03-06 19:38:45,263 - INFO - training batch 1151, loss: 0.205, 36832/60000 datapoints
2025-03-06 19:38:45,455 - INFO - training batch 1201, loss: 0.172, 38432/60000 datapoints
2025-03-06 19:38:45,652 - INFO - training batch 1251, loss: 0.402, 40032/60000 datapoints
2025-03-06 19:38:45,867 - INFO - training batch 1301, loss: 0.271, 41632/60000 datapoints
2025-03-06 19:38:46,062 - INFO - training batch 1351, loss: 0.196, 43232/60000 datapoints
2025-03-06 19:38:46,255 - INFO - training batch 1401, loss: 0.228, 44832/60000 datapoints
2025-03-06 19:38:46,452 - INFO - training batch 1451, loss: 0.128, 46432/60000 datapoints
2025-03-06 19:38:46,647 - INFO - training batch 1501, loss: 0.583, 48032/60000 datapoints
2025-03-06 19:38:46,841 - INFO - training batch 1551, loss: 0.248, 49632/60000 datapoints
2025-03-06 19:38:47,034 - INFO - training batch 1601, loss: 0.720, 51232/60000 datapoints
2025-03-06 19:38:47,228 - INFO - training batch 1651, loss: 0.205, 52832/60000 datapoints
2025-03-06 19:38:47,421 - INFO - training batch 1701, loss: 0.231, 54432/60000 datapoints
2025-03-06 19:38:47,615 - INFO - training batch 1751, loss: 0.296, 56032/60000 datapoints
2025-03-06 19:38:47,812 - INFO - training batch 1801, loss: 0.279, 57632/60000 datapoints
2025-03-06 19:38:48,009 - INFO - training batch 1851, loss: 0.384, 59232/60000 datapoints
2025-03-06 19:38:48,125 - INFO - validation batch 1, loss: 0.146, 32/10016 datapoints
2025-03-06 19:38:48,275 - INFO - validation batch 51, loss: 0.367, 1632/10016 datapoints
2025-03-06 19:38:48,433 - INFO - validation batch 101, loss: 0.194, 3232/10016 datapoints
2025-03-06 19:38:48,586 - INFO - validation batch 151, loss: 0.453, 4832/10016 datapoints
2025-03-06 19:38:48,740 - INFO - validation batch 201, loss: 0.178, 6432/10016 datapoints
2025-03-06 19:38:48,895 - INFO - validation batch 251, loss: 0.113, 8032/10016 datapoints
2025-03-06 19:38:49,048 - INFO - validation batch 301, loss: 0.219, 9632/10016 datapoints
2025-03-06 19:38:49,085 - INFO - Epoch 413/800 done.
2025-03-06 19:38:49,085 - INFO - Final validation performance:
Loss: 0.238, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:38:49,086 - INFO - Beginning epoch 414/800
2025-03-06 19:38:49,092 - INFO - training batch 1, loss: 0.087, 32/60000 datapoints
2025-03-06 19:38:49,288 - INFO - training batch 51, loss: 0.195, 1632/60000 datapoints
2025-03-06 19:38:49,483 - INFO - training batch 101, loss: 0.228, 3232/60000 datapoints
2025-03-06 19:38:49,690 - INFO - training batch 151, loss: 0.206, 4832/60000 datapoints
2025-03-06 19:38:49,883 - INFO - training batch 201, loss: 0.252, 6432/60000 datapoints
2025-03-06 19:38:50,082 - INFO - training batch 251, loss: 0.330, 8032/60000 datapoints
2025-03-06 19:38:50,279 - INFO - training batch 301, loss: 0.118, 9632/60000 datapoints
2025-03-06 19:38:50,481 - INFO - training batch 351, loss: 0.143, 11232/60000 datapoints
2025-03-06 19:38:50,696 - INFO - training batch 401, loss: 0.202, 12832/60000 datapoints
2025-03-06 19:38:50,890 - INFO - training batch 451, loss: 0.107, 14432/60000 datapoints
2025-03-06 19:38:51,084 - INFO - training batch 501, loss: 0.164, 16032/60000 datapoints
2025-03-06 19:38:51,278 - INFO - training batch 551, loss: 0.385, 17632/60000 datapoints
2025-03-06 19:38:51,473 - INFO - training batch 601, loss: 0.221, 19232/60000 datapoints
2025-03-06 19:38:51,671 - INFO - training batch 651, loss: 0.273, 20832/60000 datapoints
2025-03-06 19:38:51,869 - INFO - training batch 701, loss: 0.296, 22432/60000 datapoints
2025-03-06 19:38:52,065 - INFO - training batch 751, loss: 0.456, 24032/60000 datapoints
2025-03-06 19:38:52,260 - INFO - training batch 801, loss: 0.239, 25632/60000 datapoints
2025-03-06 19:38:52,459 - INFO - training batch 851, loss: 0.141, 27232/60000 datapoints
2025-03-06 19:38:52,656 - INFO - training batch 901, loss: 0.146, 28832/60000 datapoints
2025-03-06 19:38:52,849 - INFO - training batch 951, loss: 0.154, 30432/60000 datapoints
2025-03-06 19:38:53,044 - INFO - training batch 1001, loss: 0.451, 32032/60000 datapoints
2025-03-06 19:38:53,237 - INFO - training batch 1051, loss: 0.363, 33632/60000 datapoints
2025-03-06 19:38:53,432 - INFO - training batch 1101, loss: 0.295, 35232/60000 datapoints
2025-03-06 19:38:53,628 - INFO - training batch 1151, loss: 0.135, 36832/60000 datapoints
2025-03-06 19:38:53,821 - INFO - training batch 1201, loss: 0.232, 38432/60000 datapoints
2025-03-06 19:38:54,015 - INFO - training batch 1251, loss: 0.199, 40032/60000 datapoints
2025-03-06 19:38:54,209 - INFO - training batch 1301, loss: 0.185, 41632/60000 datapoints
2025-03-06 19:38:54,406 - INFO - training batch 1351, loss: 0.280, 43232/60000 datapoints
2025-03-06 19:38:54,600 - INFO - training batch 1401, loss: 0.196, 44832/60000 datapoints
2025-03-06 19:38:54,794 - INFO - training batch 1451, loss: 0.359, 46432/60000 datapoints
2025-03-06 19:38:54,996 - INFO - training batch 1501, loss: 0.267, 48032/60000 datapoints
2025-03-06 19:38:55,189 - INFO - training batch 1551, loss: 0.366, 49632/60000 datapoints
2025-03-06 19:38:55,383 - INFO - training batch 1601, loss: 0.227, 51232/60000 datapoints
2025-03-06 19:38:55,578 - INFO - training batch 1651, loss: 0.193, 52832/60000 datapoints
2025-03-06 19:38:55,782 - INFO - training batch 1701, loss: 0.311, 54432/60000 datapoints
2025-03-06 19:38:55,990 - INFO - training batch 1751, loss: 0.098, 56032/60000 datapoints
2025-03-06 19:38:56,185 - INFO - training batch 1801, loss: 0.193, 57632/60000 datapoints
2025-03-06 19:38:56,379 - INFO - training batch 1851, loss: 0.400, 59232/60000 datapoints
2025-03-06 19:38:56,483 - INFO - validation batch 1, loss: 0.162, 32/10016 datapoints
2025-03-06 19:38:56,639 - INFO - validation batch 51, loss: 0.205, 1632/10016 datapoints
2025-03-06 19:38:56,791 - INFO - validation batch 101, loss: 0.243, 3232/10016 datapoints
2025-03-06 19:38:56,945 - INFO - validation batch 151, loss: 0.155, 4832/10016 datapoints
2025-03-06 19:38:57,097 - INFO - validation batch 201, loss: 0.452, 6432/10016 datapoints
2025-03-06 19:38:57,249 - INFO - validation batch 251, loss: 0.477, 8032/10016 datapoints
2025-03-06 19:38:57,403 - INFO - validation batch 301, loss: 0.412, 9632/10016 datapoints
2025-03-06 19:38:57,440 - INFO - Epoch 414/800 done.
2025-03-06 19:38:57,440 - INFO - Final validation performance:
Loss: 0.301, top-1 acc: 0.921top-5 acc: 0.921
2025-03-06 19:38:57,441 - INFO - Beginning epoch 415/800
2025-03-06 19:38:57,449 - INFO - training batch 1, loss: 0.172, 32/60000 datapoints
2025-03-06 19:38:57,646 - INFO - training batch 51, loss: 0.158, 1632/60000 datapoints
2025-03-06 19:38:57,841 - INFO - training batch 101, loss: 0.184, 3232/60000 datapoints
2025-03-06 19:38:58,044 - INFO - training batch 151, loss: 0.226, 4832/60000 datapoints
2025-03-06 19:38:58,240 - INFO - training batch 201, loss: 0.190, 6432/60000 datapoints
2025-03-06 19:38:58,445 - INFO - training batch 251, loss: 0.146, 8032/60000 datapoints
2025-03-06 19:38:58,650 - INFO - training batch 301, loss: 0.403, 9632/60000 datapoints
2025-03-06 19:38:58,852 - INFO - training batch 351, loss: 0.392, 11232/60000 datapoints
2025-03-06 19:38:59,048 - INFO - training batch 401, loss: 0.168, 12832/60000 datapoints
2025-03-06 19:38:59,242 - INFO - training batch 451, loss: 0.704, 14432/60000 datapoints
2025-03-06 19:38:59,437 - INFO - training batch 501, loss: 0.290, 16032/60000 datapoints
2025-03-06 19:38:59,634 - INFO - training batch 551, loss: 0.475, 17632/60000 datapoints
2025-03-06 19:38:59,831 - INFO - training batch 601, loss: 0.502, 19232/60000 datapoints
2025-03-06 19:39:00,027 - INFO - training batch 651, loss: 0.181, 20832/60000 datapoints
2025-03-06 19:39:00,221 - INFO - training batch 701, loss: 0.363, 22432/60000 datapoints
2025-03-06 19:39:00,418 - INFO - training batch 751, loss: 0.184, 24032/60000 datapoints
2025-03-06 19:39:00,624 - INFO - training batch 801, loss: 0.251, 25632/60000 datapoints
2025-03-06 19:39:00,829 - INFO - training batch 851, loss: 0.698, 27232/60000 datapoints
2025-03-06 19:39:01,021 - INFO - training batch 901, loss: 0.313, 28832/60000 datapoints
2025-03-06 19:39:01,214 - INFO - training batch 951, loss: 0.181, 30432/60000 datapoints
2025-03-06 19:39:01,409 - INFO - training batch 1001, loss: 0.338, 32032/60000 datapoints
2025-03-06 19:39:01,601 - INFO - training batch 1051, loss: 0.272, 33632/60000 datapoints
2025-03-06 19:39:01,797 - INFO - training batch 1101, loss: 0.330, 35232/60000 datapoints
2025-03-06 19:39:01,991 - INFO - training batch 1151, loss: 0.247, 36832/60000 datapoints
2025-03-06 19:39:02,187 - INFO - training batch 1201, loss: 0.396, 38432/60000 datapoints
2025-03-06 19:39:02,378 - INFO - training batch 1251, loss: 0.429, 40032/60000 datapoints
2025-03-06 19:39:02,574 - INFO - training batch 1301, loss: 0.202, 41632/60000 datapoints
2025-03-06 19:39:02,768 - INFO - training batch 1351, loss: 0.397, 43232/60000 datapoints
2025-03-06 19:39:02,962 - INFO - training batch 1401, loss: 0.346, 44832/60000 datapoints
2025-03-06 19:39:03,157 - INFO - training batch 1451, loss: 0.345, 46432/60000 datapoints
2025-03-06 19:39:03,353 - INFO - training batch 1501, loss: 0.242, 48032/60000 datapoints
2025-03-06 19:39:03,550 - INFO - training batch 1551, loss: 0.187, 49632/60000 datapoints
2025-03-06 19:39:03,746 - INFO - training batch 1601, loss: 0.384, 51232/60000 datapoints
2025-03-06 19:39:03,942 - INFO - training batch 1651, loss: 0.132, 52832/60000 datapoints
2025-03-06 19:39:04,136 - INFO - training batch 1701, loss: 0.239, 54432/60000 datapoints
2025-03-06 19:39:04,329 - INFO - training batch 1751, loss: 0.306, 56032/60000 datapoints
2025-03-06 19:39:04,525 - INFO - training batch 1801, loss: 0.151, 57632/60000 datapoints
2025-03-06 19:39:04,720 - INFO - training batch 1851, loss: 0.059, 59232/60000 datapoints
2025-03-06 19:39:04,821 - INFO - validation batch 1, loss: 0.373, 32/10016 datapoints
2025-03-06 19:39:04,978 - INFO - validation batch 51, loss: 0.151, 1632/10016 datapoints
2025-03-06 19:39:05,131 - INFO - validation batch 101, loss: 0.397, 3232/10016 datapoints
2025-03-06 19:39:05,288 - INFO - validation batch 151, loss: 0.100, 4832/10016 datapoints
2025-03-06 19:39:05,440 - INFO - validation batch 201, loss: 0.164, 6432/10016 datapoints
2025-03-06 19:39:05,595 - INFO - validation batch 251, loss: 0.139, 8032/10016 datapoints
2025-03-06 19:39:05,751 - INFO - validation batch 301, loss: 0.239, 9632/10016 datapoints
2025-03-06 19:39:05,788 - INFO - Epoch 415/800 done.
2025-03-06 19:39:05,788 - INFO - Final validation performance:
Loss: 0.223, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:39:05,789 - INFO - Beginning epoch 416/800
2025-03-06 19:39:05,795 - INFO - training batch 1, loss: 0.342, 32/60000 datapoints
2025-03-06 19:39:06,008 - INFO - training batch 51, loss: 0.407, 1632/60000 datapoints
2025-03-06 19:39:06,203 - INFO - training batch 101, loss: 0.390, 3232/60000 datapoints
2025-03-06 19:39:06,401 - INFO - training batch 151, loss: 0.401, 4832/60000 datapoints
2025-03-06 19:39:06,602 - INFO - training batch 201, loss: 0.278, 6432/60000 datapoints
2025-03-06 19:39:06,799 - INFO - training batch 251, loss: 0.067, 8032/60000 datapoints
2025-03-06 19:39:06,994 - INFO - training batch 301, loss: 0.242, 9632/60000 datapoints
2025-03-06 19:39:07,188 - INFO - training batch 351, loss: 0.624, 11232/60000 datapoints
2025-03-06 19:39:07,383 - INFO - training batch 401, loss: 0.288, 12832/60000 datapoints
2025-03-06 19:39:07,576 - INFO - training batch 451, loss: 0.163, 14432/60000 datapoints
2025-03-06 19:39:07,777 - INFO - training batch 501, loss: 0.179, 16032/60000 datapoints
2025-03-06 19:39:07,971 - INFO - training batch 551, loss: 0.277, 17632/60000 datapoints
2025-03-06 19:39:08,167 - INFO - training batch 601, loss: 0.458, 19232/60000 datapoints
2025-03-06 19:39:08,362 - INFO - training batch 651, loss: 0.396, 20832/60000 datapoints
2025-03-06 19:39:08,560 - INFO - training batch 701, loss: 0.222, 22432/60000 datapoints
2025-03-06 19:39:08,757 - INFO - training batch 751, loss: 0.307, 24032/60000 datapoints
2025-03-06 19:39:08,951 - INFO - training batch 801, loss: 0.137, 25632/60000 datapoints
2025-03-06 19:39:09,150 - INFO - training batch 851, loss: 0.292, 27232/60000 datapoints
2025-03-06 19:39:09,341 - INFO - training batch 901, loss: 0.243, 28832/60000 datapoints
2025-03-06 19:39:09,535 - INFO - training batch 951, loss: 0.103, 30432/60000 datapoints
2025-03-06 19:39:09,731 - INFO - training batch 1001, loss: 0.304, 32032/60000 datapoints
2025-03-06 19:39:09,923 - INFO - training batch 1051, loss: 0.218, 33632/60000 datapoints
2025-03-06 19:39:10,117 - INFO - training batch 1101, loss: 0.190, 35232/60000 datapoints
2025-03-06 19:39:10,312 - INFO - training batch 1151, loss: 0.248, 36832/60000 datapoints
2025-03-06 19:39:10,508 - INFO - training batch 1201, loss: 0.704, 38432/60000 datapoints
2025-03-06 19:39:10,718 - INFO - training batch 1251, loss: 0.134, 40032/60000 datapoints
2025-03-06 19:39:10,926 - INFO - training batch 1301, loss: 0.177, 41632/60000 datapoints
2025-03-06 19:39:11,122 - INFO - training batch 1351, loss: 0.348, 43232/60000 datapoints
2025-03-06 19:39:11,319 - INFO - training batch 1401, loss: 0.664, 44832/60000 datapoints
2025-03-06 19:39:11,513 - INFO - training batch 1451, loss: 0.392, 46432/60000 datapoints
2025-03-06 19:39:11,712 - INFO - training batch 1501, loss: 0.552, 48032/60000 datapoints
2025-03-06 19:39:11,906 - INFO - training batch 1551, loss: 0.772, 49632/60000 datapoints
2025-03-06 19:39:12,099 - INFO - training batch 1601, loss: 0.313, 51232/60000 datapoints
2025-03-06 19:39:12,294 - INFO - training batch 1651, loss: 0.215, 52832/60000 datapoints
2025-03-06 19:39:12,490 - INFO - training batch 1701, loss: 0.376, 54432/60000 datapoints
2025-03-06 19:39:12,686 - INFO - training batch 1751, loss: 0.208, 56032/60000 datapoints
2025-03-06 19:39:12,880 - INFO - training batch 1801, loss: 0.234, 57632/60000 datapoints
2025-03-06 19:39:13,074 - INFO - training batch 1851, loss: 0.198, 59232/60000 datapoints
2025-03-06 19:39:13,176 - INFO - validation batch 1, loss: 0.355, 32/10016 datapoints
2025-03-06 19:39:13,329 - INFO - validation batch 51, loss: 0.078, 1632/10016 datapoints
2025-03-06 19:39:13,484 - INFO - validation batch 101, loss: 0.247, 3232/10016 datapoints
2025-03-06 19:39:13,640 - INFO - validation batch 151, loss: 0.102, 4832/10016 datapoints
2025-03-06 19:39:13,793 - INFO - validation batch 201, loss: 0.409, 6432/10016 datapoints
2025-03-06 19:39:13,947 - INFO - validation batch 251, loss: 0.169, 8032/10016 datapoints
2025-03-06 19:39:14,100 - INFO - validation batch 301, loss: 0.266, 9632/10016 datapoints
2025-03-06 19:39:14,138 - INFO - Epoch 416/800 done.
2025-03-06 19:39:14,138 - INFO - Final validation performance:
Loss: 0.232, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:39:14,139 - INFO - Beginning epoch 417/800
2025-03-06 19:39:14,145 - INFO - training batch 1, loss: 0.435, 32/60000 datapoints
2025-03-06 19:39:14,352 - INFO - training batch 51, loss: 0.203, 1632/60000 datapoints
2025-03-06 19:39:14,546 - INFO - training batch 101, loss: 0.678, 3232/60000 datapoints
2025-03-06 19:39:14,748 - INFO - training batch 151, loss: 0.222, 4832/60000 datapoints
2025-03-06 19:39:14,948 - INFO - training batch 201, loss: 0.299, 6432/60000 datapoints
2025-03-06 19:39:15,146 - INFO - training batch 251, loss: 0.198, 8032/60000 datapoints
2025-03-06 19:39:15,341 - INFO - training batch 301, loss: 0.170, 9632/60000 datapoints
2025-03-06 19:39:15,535 - INFO - training batch 351, loss: 0.210, 11232/60000 datapoints
2025-03-06 19:39:15,731 - INFO - training batch 401, loss: 0.092, 12832/60000 datapoints
2025-03-06 19:39:15,928 - INFO - training batch 451, loss: 0.169, 14432/60000 datapoints
2025-03-06 19:39:16,123 - INFO - training batch 501, loss: 0.216, 16032/60000 datapoints
2025-03-06 19:39:16,321 - INFO - training batch 551, loss: 0.243, 17632/60000 datapoints
2025-03-06 19:39:16,536 - INFO - training batch 601, loss: 0.154, 19232/60000 datapoints
2025-03-06 19:39:16,734 - INFO - training batch 651, loss: 0.131, 20832/60000 datapoints
2025-03-06 19:39:16,930 - INFO - training batch 701, loss: 0.378, 22432/60000 datapoints
2025-03-06 19:39:17,124 - INFO - training batch 751, loss: 0.291, 24032/60000 datapoints
2025-03-06 19:39:17,319 - INFO - training batch 801, loss: 0.155, 25632/60000 datapoints
2025-03-06 19:39:17,513 - INFO - training batch 851, loss: 0.200, 27232/60000 datapoints
2025-03-06 19:39:17,711 - INFO - training batch 901, loss: 0.330, 28832/60000 datapoints
2025-03-06 19:39:17,906 - INFO - training batch 951, loss: 0.358, 30432/60000 datapoints
2025-03-06 19:39:18,099 - INFO - training batch 1001, loss: 0.401, 32032/60000 datapoints
2025-03-06 19:39:18,294 - INFO - training batch 1051, loss: 0.120, 33632/60000 datapoints
2025-03-06 19:39:18,489 - INFO - training batch 1101, loss: 0.285, 35232/60000 datapoints
2025-03-06 19:39:18,686 - INFO - training batch 1151, loss: 0.115, 36832/60000 datapoints
2025-03-06 19:39:18,885 - INFO - training batch 1201, loss: 0.188, 38432/60000 datapoints
2025-03-06 19:39:19,079 - INFO - training batch 1251, loss: 0.225, 40032/60000 datapoints
2025-03-06 19:39:19,275 - INFO - training batch 1301, loss: 0.545, 41632/60000 datapoints
2025-03-06 19:39:19,469 - INFO - training batch 1351, loss: 0.377, 43232/60000 datapoints
2025-03-06 19:39:19,665 - INFO - training batch 1401, loss: 0.273, 44832/60000 datapoints
2025-03-06 19:39:19,860 - INFO - training batch 1451, loss: 0.267, 46432/60000 datapoints
2025-03-06 19:39:20,055 - INFO - training batch 1501, loss: 0.317, 48032/60000 datapoints
2025-03-06 19:39:20,250 - INFO - training batch 1551, loss: 0.597, 49632/60000 datapoints
2025-03-06 19:39:20,445 - INFO - training batch 1601, loss: 0.382, 51232/60000 datapoints
2025-03-06 19:39:20,646 - INFO - training batch 1651, loss: 0.124, 52832/60000 datapoints
2025-03-06 19:39:20,860 - INFO - training batch 1701, loss: 0.173, 54432/60000 datapoints
2025-03-06 19:39:21,055 - INFO - training batch 1751, loss: 0.164, 56032/60000 datapoints
2025-03-06 19:39:21,250 - INFO - training batch 1801, loss: 0.423, 57632/60000 datapoints
2025-03-06 19:39:21,443 - INFO - training batch 1851, loss: 0.118, 59232/60000 datapoints
2025-03-06 19:39:21,541 - INFO - validation batch 1, loss: 0.575, 32/10016 datapoints
2025-03-06 19:39:21,696 - INFO - validation batch 51, loss: 0.312, 1632/10016 datapoints
2025-03-06 19:39:21,850 - INFO - validation batch 101, loss: 0.139, 3232/10016 datapoints
2025-03-06 19:39:22,001 - INFO - validation batch 151, loss: 0.420, 4832/10016 datapoints
2025-03-06 19:39:22,152 - INFO - validation batch 201, loss: 0.266, 6432/10016 datapoints
2025-03-06 19:39:22,305 - INFO - validation batch 251, loss: 0.318, 8032/10016 datapoints
2025-03-06 19:39:22,457 - INFO - validation batch 301, loss: 0.169, 9632/10016 datapoints
2025-03-06 19:39:22,495 - INFO - Epoch 417/800 done.
2025-03-06 19:39:22,496 - INFO - Final validation performance:
Loss: 0.314, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:39:22,496 - INFO - Beginning epoch 418/800
2025-03-06 19:39:22,503 - INFO - training batch 1, loss: 0.146, 32/60000 datapoints
2025-03-06 19:39:22,701 - INFO - training batch 51, loss: 0.111, 1632/60000 datapoints
2025-03-06 19:39:22,902 - INFO - training batch 101, loss: 0.580, 3232/60000 datapoints
2025-03-06 19:39:23,105 - INFO - training batch 151, loss: 0.263, 4832/60000 datapoints
2025-03-06 19:39:23,301 - INFO - training batch 201, loss: 0.198, 6432/60000 datapoints
2025-03-06 19:39:23,500 - INFO - training batch 251, loss: 0.125, 8032/60000 datapoints
2025-03-06 19:39:23,704 - INFO - training batch 301, loss: 0.145, 9632/60000 datapoints
2025-03-06 19:39:23,904 - INFO - training batch 351, loss: 0.439, 11232/60000 datapoints
2025-03-06 19:39:24,098 - INFO - training batch 401, loss: 0.175, 12832/60000 datapoints
2025-03-06 19:39:24,292 - INFO - training batch 451, loss: 0.426, 14432/60000 datapoints
2025-03-06 19:39:24,485 - INFO - training batch 501, loss: 0.259, 16032/60000 datapoints
2025-03-06 19:39:24,683 - INFO - training batch 551, loss: 0.224, 17632/60000 datapoints
2025-03-06 19:39:24,880 - INFO - training batch 601, loss: 0.221, 19232/60000 datapoints
2025-03-06 19:39:25,075 - INFO - training batch 651, loss: 0.393, 20832/60000 datapoints
2025-03-06 19:39:25,270 - INFO - training batch 701, loss: 0.132, 22432/60000 datapoints
2025-03-06 19:39:25,466 - INFO - training batch 751, loss: 0.757, 24032/60000 datapoints
2025-03-06 19:39:25,665 - INFO - training batch 801, loss: 0.516, 25632/60000 datapoints
2025-03-06 19:39:25,860 - INFO - training batch 851, loss: 0.206, 27232/60000 datapoints
2025-03-06 19:39:26,055 - INFO - training batch 901, loss: 0.244, 28832/60000 datapoints
2025-03-06 19:39:26,251 - INFO - training batch 951, loss: 0.225, 30432/60000 datapoints
2025-03-06 19:39:26,450 - INFO - training batch 1001, loss: 0.421, 32032/60000 datapoints
2025-03-06 19:39:26,650 - INFO - training batch 1051, loss: 0.204, 33632/60000 datapoints
2025-03-06 19:39:26,844 - INFO - training batch 1101, loss: 0.353, 35232/60000 datapoints
2025-03-06 19:39:27,038 - INFO - training batch 1151, loss: 0.183, 36832/60000 datapoints
2025-03-06 19:39:27,232 - INFO - training batch 1201, loss: 0.211, 38432/60000 datapoints
2025-03-06 19:39:27,426 - INFO - training batch 1251, loss: 0.414, 40032/60000 datapoints
2025-03-06 19:39:27,623 - INFO - training batch 1301, loss: 0.512, 41632/60000 datapoints
2025-03-06 19:39:27,818 - INFO - training batch 1351, loss: 0.341, 43232/60000 datapoints
2025-03-06 19:39:28,014 - INFO - training batch 1401, loss: 0.101, 44832/60000 datapoints
2025-03-06 19:39:28,207 - INFO - training batch 1451, loss: 0.144, 46432/60000 datapoints
2025-03-06 19:39:28,401 - INFO - training batch 1501, loss: 0.100, 48032/60000 datapoints
2025-03-06 19:39:28,597 - INFO - training batch 1551, loss: 0.392, 49632/60000 datapoints
2025-03-06 19:39:28,793 - INFO - training batch 1601, loss: 0.363, 51232/60000 datapoints
2025-03-06 19:39:28,990 - INFO - training batch 1651, loss: 0.664, 52832/60000 datapoints
2025-03-06 19:39:29,184 - INFO - training batch 1701, loss: 0.288, 54432/60000 datapoints
2025-03-06 19:39:29,381 - INFO - training batch 1751, loss: 0.339, 56032/60000 datapoints
2025-03-06 19:39:29,580 - INFO - training batch 1801, loss: 0.266, 57632/60000 datapoints
2025-03-06 19:39:29,777 - INFO - training batch 1851, loss: 0.164, 59232/60000 datapoints
2025-03-06 19:39:29,879 - INFO - validation batch 1, loss: 0.300, 32/10016 datapoints
2025-03-06 19:39:30,036 - INFO - validation batch 51, loss: 0.093, 1632/10016 datapoints
2025-03-06 19:39:30,189 - INFO - validation batch 101, loss: 0.249, 3232/10016 datapoints
2025-03-06 19:39:30,343 - INFO - validation batch 151, loss: 0.224, 4832/10016 datapoints
2025-03-06 19:39:30,498 - INFO - validation batch 201, loss: 0.151, 6432/10016 datapoints
2025-03-06 19:39:30,655 - INFO - validation batch 251, loss: 0.541, 8032/10016 datapoints
2025-03-06 19:39:30,809 - INFO - validation batch 301, loss: 0.117, 9632/10016 datapoints
2025-03-06 19:39:30,850 - INFO - Epoch 418/800 done.
2025-03-06 19:39:30,850 - INFO - Final validation performance:
Loss: 0.239, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:39:30,851 - INFO - Beginning epoch 419/800
2025-03-06 19:39:30,858 - INFO - training batch 1, loss: 0.311, 32/60000 datapoints
2025-03-06 19:39:31,077 - INFO - training batch 51, loss: 0.113, 1632/60000 datapoints
2025-03-06 19:39:31,269 - INFO - training batch 101, loss: 0.172, 3232/60000 datapoints
2025-03-06 19:39:31,477 - INFO - training batch 151, loss: 0.660, 4832/60000 datapoints
2025-03-06 19:39:31,677 - INFO - training batch 201, loss: 0.330, 6432/60000 datapoints
2025-03-06 19:39:31,876 - INFO - training batch 251, loss: 0.264, 8032/60000 datapoints
2025-03-06 19:39:32,078 - INFO - training batch 301, loss: 0.276, 9632/60000 datapoints
2025-03-06 19:39:32,278 - INFO - training batch 351, loss: 0.130, 11232/60000 datapoints
2025-03-06 19:39:32,471 - INFO - training batch 401, loss: 0.288, 12832/60000 datapoints
2025-03-06 19:39:32,673 - INFO - training batch 451, loss: 0.304, 14432/60000 datapoints
2025-03-06 19:39:32,867 - INFO - training batch 501, loss: 0.151, 16032/60000 datapoints
2025-03-06 19:39:33,062 - INFO - training batch 551, loss: 0.569, 17632/60000 datapoints
2025-03-06 19:39:33,258 - INFO - training batch 601, loss: 0.265, 19232/60000 datapoints
2025-03-06 19:39:33,454 - INFO - training batch 651, loss: 0.285, 20832/60000 datapoints
2025-03-06 19:39:33,677 - INFO - training batch 701, loss: 0.335, 22432/60000 datapoints
2025-03-06 19:39:33,872 - INFO - training batch 751, loss: 0.270, 24032/60000 datapoints
2025-03-06 19:39:34,067 - INFO - training batch 801, loss: 0.643, 25632/60000 datapoints
2025-03-06 19:39:34,259 - INFO - training batch 851, loss: 0.290, 27232/60000 datapoints
2025-03-06 19:39:34,451 - INFO - training batch 901, loss: 0.128, 28832/60000 datapoints
2025-03-06 19:39:34,651 - INFO - training batch 951, loss: 0.169, 30432/60000 datapoints
2025-03-06 19:39:34,843 - INFO - training batch 1001, loss: 0.455, 32032/60000 datapoints
2025-03-06 19:39:35,040 - INFO - training batch 1051, loss: 0.380, 33632/60000 datapoints
2025-03-06 19:39:35,236 - INFO - training batch 1101, loss: 0.293, 35232/60000 datapoints
2025-03-06 19:39:35,430 - INFO - training batch 1151, loss: 0.312, 36832/60000 datapoints
2025-03-06 19:39:35,628 - INFO - training batch 1201, loss: 0.181, 38432/60000 datapoints
2025-03-06 19:39:35,822 - INFO - training batch 1251, loss: 0.292, 40032/60000 datapoints
2025-03-06 19:39:36,021 - INFO - training batch 1301, loss: 0.387, 41632/60000 datapoints
2025-03-06 19:39:36,215 - INFO - training batch 1351, loss: 0.246, 43232/60000 datapoints
2025-03-06 19:39:36,408 - INFO - training batch 1401, loss: 0.313, 44832/60000 datapoints
2025-03-06 19:39:36,605 - INFO - training batch 1451, loss: 0.285, 46432/60000 datapoints
2025-03-06 19:39:36,798 - INFO - training batch 1501, loss: 0.184, 48032/60000 datapoints
2025-03-06 19:39:36,993 - INFO - training batch 1551, loss: 0.120, 49632/60000 datapoints
2025-03-06 19:39:37,187 - INFO - training batch 1601, loss: 0.155, 51232/60000 datapoints
2025-03-06 19:39:37,384 - INFO - training batch 1651, loss: 0.270, 52832/60000 datapoints
2025-03-06 19:39:37,579 - INFO - training batch 1701, loss: 0.278, 54432/60000 datapoints
2025-03-06 19:39:37,790 - INFO - training batch 1751, loss: 0.198, 56032/60000 datapoints
2025-03-06 19:39:37,985 - INFO - training batch 1801, loss: 0.093, 57632/60000 datapoints
2025-03-06 19:39:38,181 - INFO - training batch 1851, loss: 0.397, 59232/60000 datapoints
2025-03-06 19:39:38,281 - INFO - validation batch 1, loss: 0.187, 32/10016 datapoints
2025-03-06 19:39:38,434 - INFO - validation batch 51, loss: 0.219, 1632/10016 datapoints
2025-03-06 19:39:38,589 - INFO - validation batch 101, loss: 0.142, 3232/10016 datapoints
2025-03-06 19:39:38,744 - INFO - validation batch 151, loss: 0.359, 4832/10016 datapoints
2025-03-06 19:39:38,895 - INFO - validation batch 201, loss: 0.662, 6432/10016 datapoints
2025-03-06 19:39:39,054 - INFO - validation batch 251, loss: 0.131, 8032/10016 datapoints
2025-03-06 19:39:39,206 - INFO - validation batch 301, loss: 0.419, 9632/10016 datapoints
2025-03-06 19:39:39,244 - INFO - Epoch 419/800 done.
2025-03-06 19:39:39,244 - INFO - Final validation performance:
Loss: 0.303, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:39:39,245 - INFO - Beginning epoch 420/800
2025-03-06 19:39:39,251 - INFO - training batch 1, loss: 0.509, 32/60000 datapoints
2025-03-06 19:39:39,448 - INFO - training batch 51, loss: 0.202, 1632/60000 datapoints
2025-03-06 19:39:39,645 - INFO - training batch 101, loss: 0.409, 3232/60000 datapoints
2025-03-06 19:39:39,851 - INFO - training batch 151, loss: 0.408, 4832/60000 datapoints
2025-03-06 19:39:40,046 - INFO - training batch 201, loss: 0.201, 6432/60000 datapoints
2025-03-06 19:39:40,242 - INFO - training batch 251, loss: 0.258, 8032/60000 datapoints
2025-03-06 19:39:40,440 - INFO - training batch 301, loss: 0.361, 9632/60000 datapoints
2025-03-06 19:39:40,645 - INFO - training batch 351, loss: 0.361, 11232/60000 datapoints
2025-03-06 19:39:40,840 - INFO - training batch 401, loss: 0.141, 12832/60000 datapoints
2025-03-06 19:39:41,055 - INFO - training batch 451, loss: 0.108, 14432/60000 datapoints
2025-03-06 19:39:41,248 - INFO - training batch 501, loss: 0.293, 16032/60000 datapoints
2025-03-06 19:39:41,442 - INFO - training batch 551, loss: 0.378, 17632/60000 datapoints
2025-03-06 19:39:41,643 - INFO - training batch 601, loss: 0.260, 19232/60000 datapoints
2025-03-06 19:39:41,839 - INFO - training batch 651, loss: 0.298, 20832/60000 datapoints
2025-03-06 19:39:42,035 - INFO - training batch 701, loss: 0.370, 22432/60000 datapoints
2025-03-06 19:39:42,229 - INFO - training batch 751, loss: 0.383, 24032/60000 datapoints
2025-03-06 19:39:42,425 - INFO - training batch 801, loss: 0.285, 25632/60000 datapoints
2025-03-06 19:39:42,625 - INFO - training batch 851, loss: 0.240, 27232/60000 datapoints
2025-03-06 19:39:42,818 - INFO - training batch 901, loss: 0.609, 28832/60000 datapoints
2025-03-06 19:39:43,014 - INFO - training batch 951, loss: 0.303, 30432/60000 datapoints
2025-03-06 19:39:43,209 - INFO - training batch 1001, loss: 0.315, 32032/60000 datapoints
2025-03-06 19:39:43,402 - INFO - training batch 1051, loss: 0.127, 33632/60000 datapoints
2025-03-06 19:39:43,597 - INFO - training batch 1101, loss: 0.168, 35232/60000 datapoints
2025-03-06 19:39:43,793 - INFO - training batch 1151, loss: 0.392, 36832/60000 datapoints
2025-03-06 19:39:43,990 - INFO - training batch 1201, loss: 0.218, 38432/60000 datapoints
2025-03-06 19:39:44,184 - INFO - training batch 1251, loss: 0.075, 40032/60000 datapoints
2025-03-06 19:39:44,377 - INFO - training batch 1301, loss: 0.256, 41632/60000 datapoints
2025-03-06 19:39:44,574 - INFO - training batch 1351, loss: 0.398, 43232/60000 datapoints
2025-03-06 19:39:44,774 - INFO - training batch 1401, loss: 0.376, 44832/60000 datapoints
2025-03-06 19:39:44,975 - INFO - training batch 1451, loss: 0.226, 46432/60000 datapoints
2025-03-06 19:39:45,171 - INFO - training batch 1501, loss: 0.281, 48032/60000 datapoints
2025-03-06 19:39:45,365 - INFO - training batch 1551, loss: 0.249, 49632/60000 datapoints
2025-03-06 19:39:45,559 - INFO - training batch 1601, loss: 0.260, 51232/60000 datapoints
2025-03-06 19:39:45,755 - INFO - training batch 1651, loss: 0.277, 52832/60000 datapoints
2025-03-06 19:39:45,948 - INFO - training batch 1701, loss: 0.143, 54432/60000 datapoints
2025-03-06 19:39:46,145 - INFO - training batch 1751, loss: 0.119, 56032/60000 datapoints
2025-03-06 19:39:46,339 - INFO - training batch 1801, loss: 0.481, 57632/60000 datapoints
2025-03-06 19:39:46,535 - INFO - training batch 1851, loss: 0.469, 59232/60000 datapoints
2025-03-06 19:39:46,642 - INFO - validation batch 1, loss: 0.609, 32/10016 datapoints
2025-03-06 19:39:46,793 - INFO - validation batch 51, loss: 0.070, 1632/10016 datapoints
2025-03-06 19:39:46,946 - INFO - validation batch 101, loss: 0.334, 3232/10016 datapoints
2025-03-06 19:39:47,101 - INFO - validation batch 151, loss: 0.192, 4832/10016 datapoints
2025-03-06 19:39:47,254 - INFO - validation batch 201, loss: 0.654, 6432/10016 datapoints
2025-03-06 19:39:47,407 - INFO - validation batch 251, loss: 0.176, 8032/10016 datapoints
2025-03-06 19:39:47,560 - INFO - validation batch 301, loss: 0.186, 9632/10016 datapoints
2025-03-06 19:39:47,599 - INFO - Epoch 420/800 done.
2025-03-06 19:39:47,599 - INFO - Final validation performance:
Loss: 0.317, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:39:47,600 - INFO - Beginning epoch 421/800
2025-03-06 19:39:47,606 - INFO - training batch 1, loss: 0.518, 32/60000 datapoints
2025-03-06 19:39:47,818 - INFO - training batch 51, loss: 0.256, 1632/60000 datapoints
2025-03-06 19:39:48,014 - INFO - training batch 101, loss: 0.272, 3232/60000 datapoints
2025-03-06 19:39:48,224 - INFO - training batch 151, loss: 0.421, 4832/60000 datapoints
2025-03-06 19:39:48,419 - INFO - training batch 201, loss: 0.299, 6432/60000 datapoints
2025-03-06 19:39:48,624 - INFO - training batch 251, loss: 0.246, 8032/60000 datapoints
2025-03-06 19:39:48,818 - INFO - training batch 301, loss: 0.391, 9632/60000 datapoints
2025-03-06 19:39:49,014 - INFO - training batch 351, loss: 0.535, 11232/60000 datapoints
2025-03-06 19:39:49,207 - INFO - training batch 401, loss: 0.112, 12832/60000 datapoints
2025-03-06 19:39:49,402 - INFO - training batch 451, loss: 0.229, 14432/60000 datapoints
2025-03-06 19:39:49,598 - INFO - training batch 501, loss: 0.303, 16032/60000 datapoints
2025-03-06 19:39:49,799 - INFO - training batch 551, loss: 0.156, 17632/60000 datapoints
2025-03-06 19:39:49,994 - INFO - training batch 601, loss: 0.202, 19232/60000 datapoints
2025-03-06 19:39:50,189 - INFO - training batch 651, loss: 0.265, 20832/60000 datapoints
2025-03-06 19:39:50,383 - INFO - training batch 701, loss: 0.521, 22432/60000 datapoints
2025-03-06 19:39:50,578 - INFO - training batch 751, loss: 0.219, 24032/60000 datapoints
2025-03-06 19:39:50,777 - INFO - training batch 801, loss: 0.370, 25632/60000 datapoints
2025-03-06 19:39:50,971 - INFO - training batch 851, loss: 0.246, 27232/60000 datapoints
2025-03-06 19:39:51,187 - INFO - training batch 901, loss: 0.491, 28832/60000 datapoints
2025-03-06 19:39:51,384 - INFO - training batch 951, loss: 0.362, 30432/60000 datapoints
2025-03-06 19:39:51,579 - INFO - training batch 1001, loss: 0.222, 32032/60000 datapoints
2025-03-06 19:39:51,777 - INFO - training batch 1051, loss: 0.133, 33632/60000 datapoints
2025-03-06 19:39:51,970 - INFO - training batch 1101, loss: 0.344, 35232/60000 datapoints
2025-03-06 19:39:52,167 - INFO - training batch 1151, loss: 0.091, 36832/60000 datapoints
2025-03-06 19:39:52,362 - INFO - training batch 1201, loss: 0.092, 38432/60000 datapoints
2025-03-06 19:39:52,555 - INFO - training batch 1251, loss: 0.148, 40032/60000 datapoints
2025-03-06 19:39:52,756 - INFO - training batch 1301, loss: 0.268, 41632/60000 datapoints
2025-03-06 19:39:52,950 - INFO - training batch 1351, loss: 0.163, 43232/60000 datapoints
2025-03-06 19:39:53,143 - INFO - training batch 1401, loss: 0.148, 44832/60000 datapoints
2025-03-06 19:39:53,334 - INFO - training batch 1451, loss: 0.334, 46432/60000 datapoints
2025-03-06 19:39:53,528 - INFO - training batch 1501, loss: 0.288, 48032/60000 datapoints
2025-03-06 19:39:53,725 - INFO - training batch 1551, loss: 0.295, 49632/60000 datapoints
2025-03-06 19:39:53,921 - INFO - training batch 1601, loss: 0.263, 51232/60000 datapoints
2025-03-06 19:39:54,120 - INFO - training batch 1651, loss: 0.382, 52832/60000 datapoints
2025-03-06 19:39:54,313 - INFO - training batch 1701, loss: 0.155, 54432/60000 datapoints
2025-03-06 19:39:54,506 - INFO - training batch 1751, loss: 0.262, 56032/60000 datapoints
2025-03-06 19:39:54,705 - INFO - training batch 1801, loss: 0.159, 57632/60000 datapoints
2025-03-06 19:39:54,904 - INFO - training batch 1851, loss: 0.362, 59232/60000 datapoints
2025-03-06 19:39:55,007 - INFO - validation batch 1, loss: 0.465, 32/10016 datapoints
2025-03-06 19:39:55,163 - INFO - validation batch 51, loss: 0.361, 1632/10016 datapoints
2025-03-06 19:39:55,316 - INFO - validation batch 101, loss: 0.163, 3232/10016 datapoints
2025-03-06 19:39:55,468 - INFO - validation batch 151, loss: 0.115, 4832/10016 datapoints
2025-03-06 19:39:55,623 - INFO - validation batch 201, loss: 0.397, 6432/10016 datapoints
2025-03-06 19:39:55,777 - INFO - validation batch 251, loss: 0.184, 8032/10016 datapoints
2025-03-06 19:39:55,928 - INFO - validation batch 301, loss: 0.576, 9632/10016 datapoints
2025-03-06 19:39:55,964 - INFO - Epoch 421/800 done.
2025-03-06 19:39:55,964 - INFO - Final validation performance:
Loss: 0.323, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:39:55,965 - INFO - Beginning epoch 422/800
2025-03-06 19:39:55,973 - INFO - training batch 1, loss: 0.405, 32/60000 datapoints
2025-03-06 19:39:56,202 - INFO - training batch 51, loss: 0.415, 1632/60000 datapoints
2025-03-06 19:39:56,400 - INFO - training batch 101, loss: 0.311, 3232/60000 datapoints
2025-03-06 19:39:56,601 - INFO - training batch 151, loss: 0.260, 4832/60000 datapoints
2025-03-06 19:39:56,800 - INFO - training batch 201, loss: 0.146, 6432/60000 datapoints
2025-03-06 19:39:57,002 - INFO - training batch 251, loss: 0.293, 8032/60000 datapoints
2025-03-06 19:39:57,198 - INFO - training batch 301, loss: 0.137, 9632/60000 datapoints
2025-03-06 19:39:57,393 - INFO - training batch 351, loss: 0.253, 11232/60000 datapoints
2025-03-06 19:39:57,587 - INFO - training batch 401, loss: 0.383, 12832/60000 datapoints
2025-03-06 19:39:57,786 - INFO - training batch 451, loss: 0.227, 14432/60000 datapoints
2025-03-06 19:39:57,981 - INFO - training batch 501, loss: 0.196, 16032/60000 datapoints
2025-03-06 19:39:58,175 - INFO - training batch 551, loss: 0.255, 17632/60000 datapoints
2025-03-06 19:39:58,370 - INFO - training batch 601, loss: 0.204, 19232/60000 datapoints
2025-03-06 19:39:58,568 - INFO - training batch 651, loss: 0.361, 20832/60000 datapoints
2025-03-06 19:39:58,767 - INFO - training batch 701, loss: 0.201, 22432/60000 datapoints
2025-03-06 19:39:58,962 - INFO - training batch 751, loss: 0.501, 24032/60000 datapoints
2025-03-06 19:39:59,155 - INFO - training batch 801, loss: 0.241, 25632/60000 datapoints
2025-03-06 19:39:59,349 - INFO - training batch 851, loss: 0.293, 27232/60000 datapoints
2025-03-06 19:39:59,542 - INFO - training batch 901, loss: 0.234, 28832/60000 datapoints
2025-03-06 19:39:59,738 - INFO - training batch 951, loss: 0.253, 30432/60000 datapoints
2025-03-06 19:39:59,932 - INFO - training batch 1001, loss: 0.300, 32032/60000 datapoints
2025-03-06 19:40:00,130 - INFO - training batch 1051, loss: 0.246, 33632/60000 datapoints
2025-03-06 19:40:00,324 - INFO - training batch 1101, loss: 0.118, 35232/60000 datapoints
2025-03-06 19:40:00,522 - INFO - training batch 1151, loss: 0.195, 36832/60000 datapoints
2025-03-06 19:40:00,720 - INFO - training batch 1201, loss: 0.198, 38432/60000 datapoints
2025-03-06 19:40:00,913 - INFO - training batch 1251, loss: 0.456, 40032/60000 datapoints
2025-03-06 19:40:01,111 - INFO - training batch 1301, loss: 0.331, 41632/60000 datapoints
2025-03-06 19:40:01,319 - INFO - training batch 1351, loss: 0.141, 43232/60000 datapoints
2025-03-06 19:40:01,512 - INFO - training batch 1401, loss: 0.401, 44832/60000 datapoints
2025-03-06 19:40:01,708 - INFO - training batch 1451, loss: 0.365, 46432/60000 datapoints
2025-03-06 19:40:01,903 - INFO - training batch 1501, loss: 0.305, 48032/60000 datapoints
2025-03-06 19:40:02,095 - INFO - training batch 1551, loss: 0.123, 49632/60000 datapoints
2025-03-06 19:40:02,290 - INFO - training batch 1601, loss: 0.251, 51232/60000 datapoints
2025-03-06 19:40:02,483 - INFO - training batch 1651, loss: 0.303, 52832/60000 datapoints
2025-03-06 19:40:02,681 - INFO - training batch 1701, loss: 0.496, 54432/60000 datapoints
2025-03-06 19:40:02,875 - INFO - training batch 1751, loss: 0.197, 56032/60000 datapoints
2025-03-06 19:40:03,067 - INFO - training batch 1801, loss: 0.273, 57632/60000 datapoints
2025-03-06 19:40:03,263 - INFO - training batch 1851, loss: 0.235, 59232/60000 datapoints
2025-03-06 19:40:03,363 - INFO - validation batch 1, loss: 0.277, 32/10016 datapoints
2025-03-06 19:40:03,516 - INFO - validation batch 51, loss: 0.197, 1632/10016 datapoints
2025-03-06 19:40:03,671 - INFO - validation batch 101, loss: 0.292, 3232/10016 datapoints
2025-03-06 19:40:03,826 - INFO - validation batch 151, loss: 0.219, 4832/10016 datapoints
2025-03-06 19:40:03,979 - INFO - validation batch 201, loss: 0.270, 6432/10016 datapoints
2025-03-06 19:40:04,134 - INFO - validation batch 251, loss: 0.180, 8032/10016 datapoints
2025-03-06 19:40:04,289 - INFO - validation batch 301, loss: 0.458, 9632/10016 datapoints
2025-03-06 19:40:04,327 - INFO - Epoch 422/800 done.
2025-03-06 19:40:04,327 - INFO - Final validation performance:
Loss: 0.271, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:40:04,328 - INFO - Beginning epoch 423/800
2025-03-06 19:40:04,334 - INFO - training batch 1, loss: 0.531, 32/60000 datapoints
2025-03-06 19:40:04,528 - INFO - training batch 51, loss: 0.130, 1632/60000 datapoints
2025-03-06 19:40:04,738 - INFO - training batch 101, loss: 0.197, 3232/60000 datapoints
2025-03-06 19:40:04,939 - INFO - training batch 151, loss: 0.173, 4832/60000 datapoints
2025-03-06 19:40:05,141 - INFO - training batch 201, loss: 0.518, 6432/60000 datapoints
2025-03-06 19:40:05,340 - INFO - training batch 251, loss: 0.354, 8032/60000 datapoints
2025-03-06 19:40:05,539 - INFO - training batch 301, loss: 0.296, 9632/60000 datapoints
2025-03-06 19:40:05,739 - INFO - training batch 351, loss: 0.242, 11232/60000 datapoints
2025-03-06 19:40:05,936 - INFO - training batch 401, loss: 0.122, 12832/60000 datapoints
2025-03-06 19:40:06,136 - INFO - training batch 451, loss: 0.534, 14432/60000 datapoints
2025-03-06 19:40:06,329 - INFO - training batch 501, loss: 0.092, 16032/60000 datapoints
2025-03-06 19:40:06,524 - INFO - training batch 551, loss: 0.350, 17632/60000 datapoints
2025-03-06 19:40:06,723 - INFO - training batch 601, loss: 0.586, 19232/60000 datapoints
2025-03-06 19:40:06,917 - INFO - training batch 651, loss: 0.531, 20832/60000 datapoints
2025-03-06 19:40:07,112 - INFO - training batch 701, loss: 0.454, 22432/60000 datapoints
2025-03-06 19:40:07,307 - INFO - training batch 751, loss: 0.308, 24032/60000 datapoints
2025-03-06 19:40:07,501 - INFO - training batch 801, loss: 0.203, 25632/60000 datapoints
2025-03-06 19:40:07,699 - INFO - training batch 851, loss: 0.226, 27232/60000 datapoints
2025-03-06 19:40:07,895 - INFO - training batch 901, loss: 0.177, 28832/60000 datapoints
2025-03-06 19:40:08,087 - INFO - training batch 951, loss: 0.371, 30432/60000 datapoints
2025-03-06 19:40:08,286 - INFO - training batch 1001, loss: 0.155, 32032/60000 datapoints
2025-03-06 19:40:08,480 - INFO - training batch 1051, loss: 0.348, 33632/60000 datapoints
2025-03-06 19:40:08,676 - INFO - training batch 1101, loss: 0.191, 35232/60000 datapoints
2025-03-06 19:40:08,872 - INFO - training batch 1151, loss: 0.127, 36832/60000 datapoints
2025-03-06 19:40:09,073 - INFO - training batch 1201, loss: 0.237, 38432/60000 datapoints
2025-03-06 19:40:09,267 - INFO - training batch 1251, loss: 0.195, 40032/60000 datapoints
2025-03-06 19:40:09,461 - INFO - training batch 1301, loss: 0.341, 41632/60000 datapoints
2025-03-06 19:40:09,657 - INFO - training batch 1351, loss: 0.180, 43232/60000 datapoints
2025-03-06 19:40:09,849 - INFO - training batch 1401, loss: 0.131, 44832/60000 datapoints
2025-03-06 19:40:10,044 - INFO - training batch 1451, loss: 0.256, 46432/60000 datapoints
2025-03-06 19:40:10,238 - INFO - training batch 1501, loss: 0.187, 48032/60000 datapoints
2025-03-06 19:40:10,430 - INFO - training batch 1551, loss: 0.258, 49632/60000 datapoints
2025-03-06 19:40:10,627 - INFO - training batch 1601, loss: 0.253, 51232/60000 datapoints
2025-03-06 19:40:10,823 - INFO - training batch 1651, loss: 0.137, 52832/60000 datapoints
2025-03-06 19:40:11,019 - INFO - training batch 1701, loss: 0.199, 54432/60000 datapoints
2025-03-06 19:40:11,229 - INFO - training batch 1751, loss: 0.154, 56032/60000 datapoints
2025-03-06 19:40:11,429 - INFO - training batch 1801, loss: 0.439, 57632/60000 datapoints
2025-03-06 19:40:11,624 - INFO - training batch 1851, loss: 0.303, 59232/60000 datapoints
2025-03-06 19:40:11,725 - INFO - validation batch 1, loss: 0.220, 32/10016 datapoints
2025-03-06 19:40:11,879 - INFO - validation batch 51, loss: 0.127, 1632/10016 datapoints
2025-03-06 19:40:12,030 - INFO - validation batch 101, loss: 0.085, 3232/10016 datapoints
2025-03-06 19:40:12,187 - INFO - validation batch 151, loss: 0.182, 4832/10016 datapoints
2025-03-06 19:40:12,337 - INFO - validation batch 201, loss: 0.245, 6432/10016 datapoints
2025-03-06 19:40:12,493 - INFO - validation batch 251, loss: 0.336, 8032/10016 datapoints
2025-03-06 19:40:12,646 - INFO - validation batch 301, loss: 0.285, 9632/10016 datapoints
2025-03-06 19:40:12,683 - INFO - Epoch 423/800 done.
2025-03-06 19:40:12,684 - INFO - Final validation performance:
Loss: 0.211, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:40:12,684 - INFO - Beginning epoch 424/800
2025-03-06 19:40:12,691 - INFO - training batch 1, loss: 0.366, 32/60000 datapoints
2025-03-06 19:40:12,890 - INFO - training batch 51, loss: 0.141, 1632/60000 datapoints
2025-03-06 19:40:13,097 - INFO - training batch 101, loss: 0.582, 3232/60000 datapoints
2025-03-06 19:40:13,291 - INFO - training batch 151, loss: 0.397, 4832/60000 datapoints
2025-03-06 19:40:13,493 - INFO - training batch 201, loss: 0.230, 6432/60000 datapoints
2025-03-06 19:40:13,694 - INFO - training batch 251, loss: 0.306, 8032/60000 datapoints
2025-03-06 19:40:13,890 - INFO - training batch 301, loss: 0.187, 9632/60000 datapoints
2025-03-06 19:40:14,083 - INFO - training batch 351, loss: 0.278, 11232/60000 datapoints
2025-03-06 19:40:14,275 - INFO - training batch 401, loss: 0.086, 12832/60000 datapoints
2025-03-06 19:40:14,471 - INFO - training batch 451, loss: 0.073, 14432/60000 datapoints
2025-03-06 19:40:14,669 - INFO - training batch 501, loss: 0.080, 16032/60000 datapoints
2025-03-06 19:40:14,866 - INFO - training batch 551, loss: 0.372, 17632/60000 datapoints
2025-03-06 19:40:15,062 - INFO - training batch 601, loss: 0.368, 19232/60000 datapoints
2025-03-06 19:40:15,256 - INFO - training batch 651, loss: 0.264, 20832/60000 datapoints
2025-03-06 19:40:15,449 - INFO - training batch 701, loss: 0.204, 22432/60000 datapoints
2025-03-06 19:40:15,647 - INFO - training batch 751, loss: 0.262, 24032/60000 datapoints
2025-03-06 19:40:15,842 - INFO - training batch 801, loss: 0.228, 25632/60000 datapoints
2025-03-06 19:40:16,040 - INFO - training batch 851, loss: 0.223, 27232/60000 datapoints
2025-03-06 19:40:16,240 - INFO - training batch 901, loss: 0.445, 28832/60000 datapoints
2025-03-06 19:40:16,436 - INFO - training batch 951, loss: 0.170, 30432/60000 datapoints
2025-03-06 19:40:16,635 - INFO - training batch 1001, loss: 0.263, 32032/60000 datapoints
2025-03-06 19:40:16,843 - INFO - training batch 1051, loss: 0.133, 33632/60000 datapoints
2025-03-06 19:40:17,039 - INFO - training batch 1101, loss: 0.266, 35232/60000 datapoints
2025-03-06 19:40:17,232 - INFO - training batch 1151, loss: 0.247, 36832/60000 datapoints
2025-03-06 19:40:17,427 - INFO - training batch 1201, loss: 0.408, 38432/60000 datapoints
2025-03-06 19:40:17,628 - INFO - training batch 1251, loss: 0.633, 40032/60000 datapoints
2025-03-06 19:40:17,822 - INFO - training batch 1301, loss: 0.105, 41632/60000 datapoints
2025-03-06 19:40:18,019 - INFO - training batch 1351, loss: 0.300, 43232/60000 datapoints
2025-03-06 19:40:18,214 - INFO - training batch 1401, loss: 0.320, 44832/60000 datapoints
2025-03-06 19:40:18,407 - INFO - training batch 1451, loss: 0.158, 46432/60000 datapoints
2025-03-06 19:40:18,605 - INFO - training batch 1501, loss: 0.195, 48032/60000 datapoints
2025-03-06 19:40:18,801 - INFO - training batch 1551, loss: 0.357, 49632/60000 datapoints
2025-03-06 19:40:18,997 - INFO - training batch 1601, loss: 0.561, 51232/60000 datapoints
2025-03-06 19:40:19,192 - INFO - training batch 1651, loss: 0.216, 52832/60000 datapoints
2025-03-06 19:40:19,386 - INFO - training batch 1701, loss: 0.189, 54432/60000 datapoints
2025-03-06 19:40:19,581 - INFO - training batch 1751, loss: 0.163, 56032/60000 datapoints
2025-03-06 19:40:19,779 - INFO - training batch 1801, loss: 0.164, 57632/60000 datapoints
2025-03-06 19:40:19,975 - INFO - training batch 1851, loss: 0.401, 59232/60000 datapoints
2025-03-06 19:40:20,075 - INFO - validation batch 1, loss: 0.397, 32/10016 datapoints
2025-03-06 19:40:20,227 - INFO - validation batch 51, loss: 0.036, 1632/10016 datapoints
2025-03-06 19:40:20,379 - INFO - validation batch 101, loss: 0.139, 3232/10016 datapoints
2025-03-06 19:40:20,533 - INFO - validation batch 151, loss: 0.283, 4832/10016 datapoints
2025-03-06 19:40:20,686 - INFO - validation batch 201, loss: 0.554, 6432/10016 datapoints
2025-03-06 19:40:20,841 - INFO - validation batch 251, loss: 0.617, 8032/10016 datapoints
2025-03-06 19:40:20,995 - INFO - validation batch 301, loss: 0.404, 9632/10016 datapoints
2025-03-06 19:40:21,035 - INFO - Epoch 424/800 done.
2025-03-06 19:40:21,035 - INFO - Final validation performance:
Loss: 0.347, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:40:21,035 - INFO - Beginning epoch 425/800
2025-03-06 19:40:21,042 - INFO - training batch 1, loss: 0.211, 32/60000 datapoints
2025-03-06 19:40:21,239 - INFO - training batch 51, loss: 0.254, 1632/60000 datapoints
2025-03-06 19:40:21,462 - INFO - training batch 101, loss: 0.595, 3232/60000 datapoints
2025-03-06 19:40:21,663 - INFO - training batch 151, loss: 0.404, 4832/60000 datapoints
2025-03-06 19:40:21,862 - INFO - training batch 201, loss: 0.307, 6432/60000 datapoints
2025-03-06 19:40:22,063 - INFO - training batch 251, loss: 0.191, 8032/60000 datapoints
2025-03-06 19:40:22,260 - INFO - training batch 301, loss: 0.095, 9632/60000 datapoints
2025-03-06 19:40:22,455 - INFO - training batch 351, loss: 0.332, 11232/60000 datapoints
2025-03-06 19:40:22,652 - INFO - training batch 401, loss: 0.328, 12832/60000 datapoints
2025-03-06 19:40:22,847 - INFO - training batch 451, loss: 0.164, 14432/60000 datapoints
2025-03-06 19:40:23,044 - INFO - training batch 501, loss: 0.437, 16032/60000 datapoints
2025-03-06 19:40:23,238 - INFO - training batch 551, loss: 0.333, 17632/60000 datapoints
2025-03-06 19:40:23,432 - INFO - training batch 601, loss: 0.343, 19232/60000 datapoints
2025-03-06 19:40:23,635 - INFO - training batch 651, loss: 0.177, 20832/60000 datapoints
2025-03-06 19:40:23,831 - INFO - training batch 701, loss: 0.148, 22432/60000 datapoints
2025-03-06 19:40:24,029 - INFO - training batch 751, loss: 0.226, 24032/60000 datapoints
2025-03-06 19:40:24,224 - INFO - training batch 801, loss: 0.200, 25632/60000 datapoints
2025-03-06 19:40:24,417 - INFO - training batch 851, loss: 0.284, 27232/60000 datapoints
2025-03-06 19:40:24,612 - INFO - training batch 901, loss: 0.185, 28832/60000 datapoints
2025-03-06 19:40:24,810 - INFO - training batch 951, loss: 0.260, 30432/60000 datapoints
2025-03-06 19:40:25,010 - INFO - training batch 1001, loss: 0.213, 32032/60000 datapoints
2025-03-06 19:40:25,207 - INFO - training batch 1051, loss: 0.071, 33632/60000 datapoints
2025-03-06 19:40:25,400 - INFO - training batch 1101, loss: 0.279, 35232/60000 datapoints
2025-03-06 19:40:25,595 - INFO - training batch 1151, loss: 0.165, 36832/60000 datapoints
2025-03-06 19:40:25,790 - INFO - training batch 1201, loss: 0.218, 38432/60000 datapoints
2025-03-06 19:40:25,986 - INFO - training batch 1251, loss: 0.339, 40032/60000 datapoints
2025-03-06 19:40:26,182 - INFO - training batch 1301, loss: 0.285, 41632/60000 datapoints
2025-03-06 19:40:26,377 - INFO - training batch 1351, loss: 0.443, 43232/60000 datapoints
2025-03-06 19:40:26,572 - INFO - training batch 1401, loss: 0.252, 44832/60000 datapoints
2025-03-06 19:40:26,769 - INFO - training batch 1451, loss: 0.256, 46432/60000 datapoints
2025-03-06 19:40:26,965 - INFO - training batch 1501, loss: 0.251, 48032/60000 datapoints
2025-03-06 19:40:27,160 - INFO - training batch 1551, loss: 0.330, 49632/60000 datapoints
2025-03-06 19:40:27,353 - INFO - training batch 1601, loss: 0.341, 51232/60000 datapoints
2025-03-06 19:40:27,552 - INFO - training batch 1651, loss: 0.204, 52832/60000 datapoints
2025-03-06 19:40:27,749 - INFO - training batch 1701, loss: 0.258, 54432/60000 datapoints
2025-03-06 19:40:27,944 - INFO - training batch 1751, loss: 0.372, 56032/60000 datapoints
2025-03-06 19:40:28,137 - INFO - training batch 1801, loss: 0.099, 57632/60000 datapoints
2025-03-06 19:40:28,336 - INFO - training batch 1851, loss: 0.296, 59232/60000 datapoints
2025-03-06 19:40:28,436 - INFO - validation batch 1, loss: 0.306, 32/10016 datapoints
2025-03-06 19:40:28,589 - INFO - validation batch 51, loss: 0.550, 1632/10016 datapoints
2025-03-06 19:40:28,743 - INFO - validation batch 101, loss: 0.454, 3232/10016 datapoints
2025-03-06 19:40:28,897 - INFO - validation batch 151, loss: 0.153, 4832/10016 datapoints
2025-03-06 19:40:29,051 - INFO - validation batch 201, loss: 0.563, 6432/10016 datapoints
2025-03-06 19:40:29,205 - INFO - validation batch 251, loss: 0.240, 8032/10016 datapoints
2025-03-06 19:40:29,356 - INFO - validation batch 301, loss: 0.131, 9632/10016 datapoints
2025-03-06 19:40:29,394 - INFO - Epoch 425/800 done.
2025-03-06 19:40:29,394 - INFO - Final validation performance:
Loss: 0.342, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:40:29,395 - INFO - Beginning epoch 426/800
2025-03-06 19:40:29,402 - INFO - training batch 1, loss: 0.362, 32/60000 datapoints
2025-03-06 19:40:29,613 - INFO - training batch 51, loss: 0.379, 1632/60000 datapoints
2025-03-06 19:40:29,809 - INFO - training batch 101, loss: 0.292, 3232/60000 datapoints
2025-03-06 19:40:30,003 - INFO - training batch 151, loss: 0.098, 4832/60000 datapoints
2025-03-06 19:40:30,206 - INFO - training batch 201, loss: 0.335, 6432/60000 datapoints
2025-03-06 19:40:30,405 - INFO - training batch 251, loss: 0.116, 8032/60000 datapoints
2025-03-06 19:40:30,603 - INFO - training batch 301, loss: 0.929, 9632/60000 datapoints
2025-03-06 19:40:30,803 - INFO - training batch 351, loss: 0.179, 11232/60000 datapoints
2025-03-06 19:40:30,998 - INFO - training batch 401, loss: 0.243, 12832/60000 datapoints
2025-03-06 19:40:31,192 - INFO - training batch 451, loss: 0.187, 14432/60000 datapoints
2025-03-06 19:40:31,406 - INFO - training batch 501, loss: 0.131, 16032/60000 datapoints
2025-03-06 19:40:31,605 - INFO - training batch 551, loss: 0.419, 17632/60000 datapoints
2025-03-06 19:40:31,804 - INFO - training batch 601, loss: 0.837, 19232/60000 datapoints
2025-03-06 19:40:31,998 - INFO - training batch 651, loss: 0.383, 20832/60000 datapoints
2025-03-06 19:40:32,194 - INFO - training batch 701, loss: 0.094, 22432/60000 datapoints
2025-03-06 19:40:32,387 - INFO - training batch 751, loss: 0.357, 24032/60000 datapoints
2025-03-06 19:40:32,582 - INFO - training batch 801, loss: 0.554, 25632/60000 datapoints
2025-03-06 19:40:32,779 - INFO - training batch 851, loss: 0.248, 27232/60000 datapoints
2025-03-06 19:40:32,978 - INFO - training batch 901, loss: 0.141, 28832/60000 datapoints
2025-03-06 19:40:33,173 - INFO - training batch 951, loss: 0.315, 30432/60000 datapoints
2025-03-06 19:40:33,367 - INFO - training batch 1001, loss: 0.322, 32032/60000 datapoints
2025-03-06 19:40:33,561 - INFO - training batch 1051, loss: 0.441, 33632/60000 datapoints
2025-03-06 19:40:33,757 - INFO - training batch 1101, loss: 0.487, 35232/60000 datapoints
2025-03-06 19:40:33,974 - INFO - training batch 1151, loss: 0.329, 36832/60000 datapoints
2025-03-06 19:40:34,179 - INFO - training batch 1201, loss: 0.097, 38432/60000 datapoints
2025-03-06 19:40:34,372 - INFO - training batch 1251, loss: 0.139, 40032/60000 datapoints
2025-03-06 19:40:34,566 - INFO - training batch 1301, loss: 0.302, 41632/60000 datapoints
2025-03-06 19:40:34,761 - INFO - training batch 1351, loss: 0.219, 43232/60000 datapoints
2025-03-06 19:40:34,966 - INFO - training batch 1401, loss: 0.354, 44832/60000 datapoints
2025-03-06 19:40:35,161 - INFO - training batch 1451, loss: 0.246, 46432/60000 datapoints
2025-03-06 19:40:35,356 - INFO - training batch 1501, loss: 0.479, 48032/60000 datapoints
2025-03-06 19:40:35,549 - INFO - training batch 1551, loss: 0.221, 49632/60000 datapoints
2025-03-06 19:40:35,746 - INFO - training batch 1601, loss: 0.126, 51232/60000 datapoints
2025-03-06 19:40:35,942 - INFO - training batch 1651, loss: 0.580, 52832/60000 datapoints
2025-03-06 19:40:36,140 - INFO - training batch 1701, loss: 0.511, 54432/60000 datapoints
2025-03-06 19:40:36,344 - INFO - training batch 1751, loss: 0.246, 56032/60000 datapoints
2025-03-06 19:40:36,538 - INFO - training batch 1801, loss: 0.432, 57632/60000 datapoints
2025-03-06 19:40:36,736 - INFO - training batch 1851, loss: 0.140, 59232/60000 datapoints
2025-03-06 19:40:36,839 - INFO - validation batch 1, loss: 0.162, 32/10016 datapoints
2025-03-06 19:40:36,991 - INFO - validation batch 51, loss: 0.144, 1632/10016 datapoints
2025-03-06 19:40:37,144 - INFO - validation batch 101, loss: 0.559, 3232/10016 datapoints
2025-03-06 19:40:37,295 - INFO - validation batch 151, loss: 0.185, 4832/10016 datapoints
2025-03-06 19:40:37,448 - INFO - validation batch 201, loss: 0.280, 6432/10016 datapoints
2025-03-06 19:40:37,601 - INFO - validation batch 251, loss: 0.540, 8032/10016 datapoints
2025-03-06 19:40:37,794 - INFO - validation batch 301, loss: 0.252, 9632/10016 datapoints
2025-03-06 19:40:37,832 - INFO - Epoch 426/800 done.
2025-03-06 19:40:37,832 - INFO - Final validation performance:
Loss: 0.303, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:40:37,833 - INFO - Beginning epoch 427/800
2025-03-06 19:40:37,839 - INFO - training batch 1, loss: 0.338, 32/60000 datapoints
2025-03-06 19:40:38,048 - INFO - training batch 51, loss: 0.126, 1632/60000 datapoints
2025-03-06 19:40:38,246 - INFO - training batch 101, loss: 0.106, 3232/60000 datapoints
2025-03-06 19:40:38,446 - INFO - training batch 151, loss: 0.206, 4832/60000 datapoints
2025-03-06 19:40:38,643 - INFO - training batch 201, loss: 0.249, 6432/60000 datapoints
2025-03-06 19:40:38,845 - INFO - training batch 251, loss: 0.329, 8032/60000 datapoints
2025-03-06 19:40:39,042 - INFO - training batch 301, loss: 0.450, 9632/60000 datapoints
2025-03-06 19:40:39,235 - INFO - training batch 351, loss: 0.342, 11232/60000 datapoints
2025-03-06 19:40:39,429 - INFO - training batch 401, loss: 0.235, 12832/60000 datapoints
2025-03-06 19:40:39,624 - INFO - training batch 451, loss: 0.158, 14432/60000 datapoints
2025-03-06 19:40:39,819 - INFO - training batch 501, loss: 0.366, 16032/60000 datapoints
2025-03-06 19:40:40,015 - INFO - training batch 551, loss: 0.127, 17632/60000 datapoints
2025-03-06 19:40:40,212 - INFO - training batch 601, loss: 0.099, 19232/60000 datapoints
2025-03-06 19:40:40,404 - INFO - training batch 651, loss: 0.456, 20832/60000 datapoints
2025-03-06 19:40:40,598 - INFO - training batch 701, loss: 0.394, 22432/60000 datapoints
2025-03-06 19:40:40,798 - INFO - training batch 751, loss: 0.244, 24032/60000 datapoints
2025-03-06 19:40:40,995 - INFO - training batch 801, loss: 0.149, 25632/60000 datapoints
2025-03-06 19:40:41,192 - INFO - training batch 851, loss: 0.286, 27232/60000 datapoints
2025-03-06 19:40:41,388 - INFO - training batch 901, loss: 0.518, 28832/60000 datapoints
2025-03-06 19:40:41,600 - INFO - training batch 951, loss: 0.254, 30432/60000 datapoints
2025-03-06 19:40:41,797 - INFO - training batch 1001, loss: 0.129, 32032/60000 datapoints
2025-03-06 19:40:41,990 - INFO - training batch 1051, loss: 0.135, 33632/60000 datapoints
2025-03-06 19:40:42,184 - INFO - training batch 1101, loss: 0.446, 35232/60000 datapoints
2025-03-06 19:40:42,379 - INFO - training batch 1151, loss: 0.185, 36832/60000 datapoints
2025-03-06 19:40:42,572 - INFO - training batch 1201, loss: 0.661, 38432/60000 datapoints
2025-03-06 19:40:42,775 - INFO - training batch 1251, loss: 0.331, 40032/60000 datapoints
2025-03-06 19:40:42,972 - INFO - training batch 1301, loss: 0.214, 41632/60000 datapoints
2025-03-06 19:40:43,165 - INFO - training batch 1351, loss: 0.268, 43232/60000 datapoints
2025-03-06 19:40:43,358 - INFO - training batch 1401, loss: 0.153, 44832/60000 datapoints
2025-03-06 19:40:43,553 - INFO - training batch 1451, loss: 0.107, 46432/60000 datapoints
2025-03-06 19:40:43,749 - INFO - training batch 1501, loss: 0.145, 48032/60000 datapoints
2025-03-06 19:40:43,944 - INFO - training batch 1551, loss: 0.083, 49632/60000 datapoints
2025-03-06 19:40:44,138 - INFO - training batch 1601, loss: 0.328, 51232/60000 datapoints
2025-03-06 19:40:44,333 - INFO - training batch 1651, loss: 0.138, 52832/60000 datapoints
2025-03-06 19:40:44,528 - INFO - training batch 1701, loss: 0.168, 54432/60000 datapoints
2025-03-06 19:40:44,726 - INFO - training batch 1751, loss: 0.240, 56032/60000 datapoints
2025-03-06 19:40:44,931 - INFO - training batch 1801, loss: 0.113, 57632/60000 datapoints
2025-03-06 19:40:45,126 - INFO - training batch 1851, loss: 0.535, 59232/60000 datapoints
2025-03-06 19:40:45,226 - INFO - validation batch 1, loss: 0.192, 32/10016 datapoints
2025-03-06 19:40:45,379 - INFO - validation batch 51, loss: 0.149, 1632/10016 datapoints
2025-03-06 19:40:45,532 - INFO - validation batch 101, loss: 0.301, 3232/10016 datapoints
2025-03-06 19:40:45,687 - INFO - validation batch 151, loss: 0.162, 4832/10016 datapoints
2025-03-06 19:40:45,840 - INFO - validation batch 201, loss: 0.219, 6432/10016 datapoints
2025-03-06 19:40:45,993 - INFO - validation batch 251, loss: 0.326, 8032/10016 datapoints
2025-03-06 19:40:46,149 - INFO - validation batch 301, loss: 0.210, 9632/10016 datapoints
2025-03-06 19:40:46,187 - INFO - Epoch 427/800 done.
2025-03-06 19:40:46,188 - INFO - Final validation performance:
Loss: 0.223, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:40:46,188 - INFO - Beginning epoch 428/800
2025-03-06 19:40:46,195 - INFO - training batch 1, loss: 0.361, 32/60000 datapoints
2025-03-06 19:40:46,393 - INFO - training batch 51, loss: 0.438, 1632/60000 datapoints
2025-03-06 19:40:46,599 - INFO - training batch 101, loss: 0.263, 3232/60000 datapoints
2025-03-06 19:40:46,799 - INFO - training batch 151, loss: 0.086, 4832/60000 datapoints
2025-03-06 19:40:47,002 - INFO - training batch 201, loss: 0.089, 6432/60000 datapoints
2025-03-06 19:40:47,200 - INFO - training batch 251, loss: 0.245, 8032/60000 datapoints
2025-03-06 19:40:47,397 - INFO - training batch 301, loss: 0.137, 9632/60000 datapoints
2025-03-06 19:40:47,592 - INFO - training batch 351, loss: 0.127, 11232/60000 datapoints
2025-03-06 19:40:47,789 - INFO - training batch 401, loss: 0.599, 12832/60000 datapoints
2025-03-06 19:40:47,983 - INFO - training batch 451, loss: 0.396, 14432/60000 datapoints
2025-03-06 19:40:48,205 - INFO - training batch 501, loss: 0.696, 16032/60000 datapoints
2025-03-06 19:40:48,411 - INFO - training batch 551, loss: 0.398, 17632/60000 datapoints
2025-03-06 19:40:48,604 - INFO - training batch 601, loss: 0.115, 19232/60000 datapoints
2025-03-06 19:40:48,801 - INFO - training batch 651, loss: 0.194, 20832/60000 datapoints
2025-03-06 19:40:49,000 - INFO - training batch 701, loss: 0.213, 22432/60000 datapoints
2025-03-06 19:40:49,195 - INFO - training batch 751, loss: 0.200, 24032/60000 datapoints
2025-03-06 19:40:49,389 - INFO - training batch 801, loss: 0.278, 25632/60000 datapoints
2025-03-06 19:40:49,581 - INFO - training batch 851, loss: 0.167, 27232/60000 datapoints
2025-03-06 19:40:49,780 - INFO - training batch 901, loss: 0.120, 28832/60000 datapoints
2025-03-06 19:40:49,975 - INFO - training batch 951, loss: 0.376, 30432/60000 datapoints
2025-03-06 19:40:50,168 - INFO - training batch 1001, loss: 0.273, 32032/60000 datapoints
2025-03-06 19:40:50,363 - INFO - training batch 1051, loss: 0.306, 33632/60000 datapoints
2025-03-06 19:40:50,556 - INFO - training batch 1101, loss: 0.388, 35232/60000 datapoints
2025-03-06 19:40:50,756 - INFO - training batch 1151, loss: 0.199, 36832/60000 datapoints
2025-03-06 19:40:50,953 - INFO - training batch 1201, loss: 0.334, 38432/60000 datapoints
2025-03-06 19:40:51,147 - INFO - training batch 1251, loss: 0.244, 40032/60000 datapoints
2025-03-06 19:40:51,341 - INFO - training batch 1301, loss: 0.252, 41632/60000 datapoints
2025-03-06 19:40:51,554 - INFO - training batch 1351, loss: 0.376, 43232/60000 datapoints
2025-03-06 19:40:51,750 - INFO - training batch 1401, loss: 0.324, 44832/60000 datapoints
2025-03-06 19:40:51,947 - INFO - training batch 1451, loss: 0.227, 46432/60000 datapoints
2025-03-06 19:40:52,141 - INFO - training batch 1501, loss: 0.212, 48032/60000 datapoints
2025-03-06 19:40:52,336 - INFO - training batch 1551, loss: 0.092, 49632/60000 datapoints
2025-03-06 19:40:52,532 - INFO - training batch 1601, loss: 0.469, 51232/60000 datapoints
2025-03-06 19:40:52,729 - INFO - training batch 1651, loss: 0.291, 52832/60000 datapoints
2025-03-06 19:40:52,927 - INFO - training batch 1701, loss: 0.227, 54432/60000 datapoints
2025-03-06 19:40:53,122 - INFO - training batch 1751, loss: 0.507, 56032/60000 datapoints
2025-03-06 19:40:53,317 - INFO - training batch 1801, loss: 0.435, 57632/60000 datapoints
2025-03-06 19:40:53,513 - INFO - training batch 1851, loss: 0.429, 59232/60000 datapoints
2025-03-06 19:40:53,615 - INFO - validation batch 1, loss: 0.363, 32/10016 datapoints
2025-03-06 19:40:53,769 - INFO - validation batch 51, loss: 0.129, 1632/10016 datapoints
2025-03-06 19:40:53,923 - INFO - validation batch 101, loss: 0.188, 3232/10016 datapoints
2025-03-06 19:40:54,078 - INFO - validation batch 151, loss: 0.424, 4832/10016 datapoints
2025-03-06 19:40:54,230 - INFO - validation batch 201, loss: 0.360, 6432/10016 datapoints
2025-03-06 19:40:54,385 - INFO - validation batch 251, loss: 0.194, 8032/10016 datapoints
2025-03-06 19:40:54,535 - INFO - validation batch 301, loss: 0.241, 9632/10016 datapoints
2025-03-06 19:40:54,574 - INFO - Epoch 428/800 done.
2025-03-06 19:40:54,574 - INFO - Final validation performance:
Loss: 0.271, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:40:54,575 - INFO - Beginning epoch 429/800
2025-03-06 19:40:54,581 - INFO - training batch 1, loss: 0.095, 32/60000 datapoints
2025-03-06 19:40:54,837 - INFO - training batch 51, loss: 0.337, 1632/60000 datapoints
2025-03-06 19:40:55,042 - INFO - training batch 101, loss: 0.140, 3232/60000 datapoints
2025-03-06 19:40:55,244 - INFO - training batch 151, loss: 0.191, 4832/60000 datapoints
2025-03-06 19:40:55,442 - INFO - training batch 201, loss: 0.320, 6432/60000 datapoints
2025-03-06 19:40:55,643 - INFO - training batch 251, loss: 0.314, 8032/60000 datapoints
2025-03-06 19:40:55,839 - INFO - training batch 301, loss: 0.298, 9632/60000 datapoints
2025-03-06 19:40:56,034 - INFO - training batch 351, loss: 0.272, 11232/60000 datapoints
2025-03-06 19:40:56,234 - INFO - training batch 401, loss: 0.267, 12832/60000 datapoints
2025-03-06 19:40:56,432 - INFO - training batch 451, loss: 0.175, 14432/60000 datapoints
2025-03-06 19:40:56,627 - INFO - training batch 501, loss: 0.374, 16032/60000 datapoints
2025-03-06 19:40:56,821 - INFO - training batch 551, loss: 0.290, 17632/60000 datapoints
2025-03-06 19:40:57,017 - INFO - training batch 601, loss: 0.162, 19232/60000 datapoints
2025-03-06 19:40:57,213 - INFO - training batch 651, loss: 0.361, 20832/60000 datapoints
2025-03-06 19:40:57,409 - INFO - training batch 701, loss: 0.342, 22432/60000 datapoints
2025-03-06 19:40:57,602 - INFO - training batch 751, loss: 0.326, 24032/60000 datapoints
2025-03-06 19:40:57,799 - INFO - training batch 801, loss: 0.350, 25632/60000 datapoints
2025-03-06 19:40:58,000 - INFO - training batch 851, loss: 0.181, 27232/60000 datapoints
2025-03-06 19:40:58,197 - INFO - training batch 901, loss: 0.449, 28832/60000 datapoints
2025-03-06 19:40:58,395 - INFO - training batch 951, loss: 0.202, 30432/60000 datapoints
2025-03-06 19:40:58,600 - INFO - training batch 1001, loss: 0.291, 32032/60000 datapoints
2025-03-06 19:40:58,801 - INFO - training batch 1051, loss: 0.169, 33632/60000 datapoints
2025-03-06 19:40:59,014 - INFO - training batch 1101, loss: 0.217, 35232/60000 datapoints
2025-03-06 19:40:59,214 - INFO - training batch 1151, loss: 0.202, 36832/60000 datapoints
2025-03-06 19:40:59,408 - INFO - training batch 1201, loss: 0.206, 38432/60000 datapoints
2025-03-06 19:40:59,603 - INFO - training batch 1251, loss: 0.245, 40032/60000 datapoints
2025-03-06 19:40:59,799 - INFO - training batch 1301, loss: 0.444, 41632/60000 datapoints
2025-03-06 19:40:59,993 - INFO - training batch 1351, loss: 0.323, 43232/60000 datapoints
2025-03-06 19:41:00,189 - INFO - training batch 1401, loss: 0.319, 44832/60000 datapoints
2025-03-06 19:41:00,386 - INFO - training batch 1451, loss: 0.590, 46432/60000 datapoints
2025-03-06 19:41:00,586 - INFO - training batch 1501, loss: 0.124, 48032/60000 datapoints
2025-03-06 19:41:00,781 - INFO - training batch 1551, loss: 0.152, 49632/60000 datapoints
2025-03-06 19:41:00,979 - INFO - training batch 1601, loss: 0.123, 51232/60000 datapoints
2025-03-06 19:41:01,174 - INFO - training batch 1651, loss: 0.266, 52832/60000 datapoints
2025-03-06 19:41:01,369 - INFO - training batch 1701, loss: 0.113, 54432/60000 datapoints
2025-03-06 19:41:01,566 - INFO - training batch 1751, loss: 0.292, 56032/60000 datapoints
2025-03-06 19:41:01,780 - INFO - training batch 1801, loss: 0.289, 57632/60000 datapoints
2025-03-06 19:41:01,977 - INFO - training batch 1851, loss: 0.324, 59232/60000 datapoints
2025-03-06 19:41:02,078 - INFO - validation batch 1, loss: 0.172, 32/10016 datapoints
2025-03-06 19:41:02,232 - INFO - validation batch 51, loss: 0.191, 1632/10016 datapoints
2025-03-06 19:41:02,386 - INFO - validation batch 101, loss: 0.383, 3232/10016 datapoints
2025-03-06 19:41:02,539 - INFO - validation batch 151, loss: 0.241, 4832/10016 datapoints
2025-03-06 19:41:02,695 - INFO - validation batch 201, loss: 0.225, 6432/10016 datapoints
2025-03-06 19:41:02,847 - INFO - validation batch 251, loss: 0.455, 8032/10016 datapoints
2025-03-06 19:41:03,003 - INFO - validation batch 301, loss: 0.105, 9632/10016 datapoints
2025-03-06 19:41:03,040 - INFO - Epoch 429/800 done.
2025-03-06 19:41:03,041 - INFO - Final validation performance:
Loss: 0.253, top-1 acc: 0.922top-5 acc: 0.922
2025-03-06 19:41:03,041 - INFO - Beginning epoch 430/800
2025-03-06 19:41:03,048 - INFO - training batch 1, loss: 0.247, 32/60000 datapoints
2025-03-06 19:41:03,259 - INFO - training batch 51, loss: 0.320, 1632/60000 datapoints
2025-03-06 19:41:03,453 - INFO - training batch 101, loss: 0.222, 3232/60000 datapoints
2025-03-06 19:41:03,657 - INFO - training batch 151, loss: 0.062, 4832/60000 datapoints
2025-03-06 19:41:03,854 - INFO - training batch 201, loss: 0.448, 6432/60000 datapoints
2025-03-06 19:41:04,051 - INFO - training batch 251, loss: 0.204, 8032/60000 datapoints
2025-03-06 19:41:04,245 - INFO - training batch 301, loss: 0.093, 9632/60000 datapoints
2025-03-06 19:41:04,440 - INFO - training batch 351, loss: 0.504, 11232/60000 datapoints
2025-03-06 19:41:04,638 - INFO - training batch 401, loss: 0.246, 12832/60000 datapoints
2025-03-06 19:41:04,832 - INFO - training batch 451, loss: 0.202, 14432/60000 datapoints
2025-03-06 19:41:05,037 - INFO - training batch 501, loss: 0.293, 16032/60000 datapoints
2025-03-06 19:41:05,233 - INFO - training batch 551, loss: 0.324, 17632/60000 datapoints
2025-03-06 19:41:05,427 - INFO - training batch 601, loss: 0.142, 19232/60000 datapoints
2025-03-06 19:41:05,630 - INFO - training batch 651, loss: 0.500, 20832/60000 datapoints
2025-03-06 19:41:05,824 - INFO - training batch 701, loss: 0.366, 22432/60000 datapoints
2025-03-06 19:41:06,018 - INFO - training batch 751, loss: 0.211, 24032/60000 datapoints
2025-03-06 19:41:06,215 - INFO - training batch 801, loss: 0.200, 25632/60000 datapoints
2025-03-06 19:41:06,409 - INFO - training batch 851, loss: 0.519, 27232/60000 datapoints
2025-03-06 19:41:06,604 - INFO - training batch 901, loss: 0.430, 28832/60000 datapoints
2025-03-06 19:41:06,799 - INFO - training batch 951, loss: 0.139, 30432/60000 datapoints
2025-03-06 19:41:06,994 - INFO - training batch 1001, loss: 0.203, 32032/60000 datapoints
2025-03-06 19:41:07,189 - INFO - training batch 1051, loss: 0.262, 33632/60000 datapoints
2025-03-06 19:41:07,382 - INFO - training batch 1101, loss: 0.173, 35232/60000 datapoints
2025-03-06 19:41:07,578 - INFO - training batch 1151, loss: 0.227, 36832/60000 datapoints
2025-03-06 19:41:07,777 - INFO - training batch 1201, loss: 0.324, 38432/60000 datapoints
2025-03-06 19:41:07,973 - INFO - training batch 1251, loss: 0.193, 40032/60000 datapoints
2025-03-06 19:41:08,169 - INFO - training batch 1301, loss: 0.257, 41632/60000 datapoints
2025-03-06 19:41:08,364 - INFO - training batch 1351, loss: 0.552, 43232/60000 datapoints
2025-03-06 19:41:08,558 - INFO - training batch 1401, loss: 0.276, 44832/60000 datapoints
2025-03-06 19:41:08,753 - INFO - training batch 1451, loss: 0.253, 46432/60000 datapoints
2025-03-06 19:41:08,948 - INFO - training batch 1501, loss: 0.388, 48032/60000 datapoints
2025-03-06 19:41:09,148 - INFO - training batch 1551, loss: 0.361, 49632/60000 datapoints
2025-03-06 19:41:09,341 - INFO - training batch 1601, loss: 0.276, 51232/60000 datapoints
2025-03-06 19:41:09,534 - INFO - training batch 1651, loss: 0.124, 52832/60000 datapoints
2025-03-06 19:41:09,729 - INFO - training batch 1701, loss: 0.462, 54432/60000 datapoints
2025-03-06 19:41:09,922 - INFO - training batch 1751, loss: 0.135, 56032/60000 datapoints
2025-03-06 19:41:10,117 - INFO - training batch 1801, loss: 0.269, 57632/60000 datapoints
2025-03-06 19:41:10,312 - INFO - training batch 1851, loss: 0.179, 59232/60000 datapoints
2025-03-06 19:41:10,414 - INFO - validation batch 1, loss: 0.347, 32/10016 datapoints
2025-03-06 19:41:10,568 - INFO - validation batch 51, loss: 0.186, 1632/10016 datapoints
2025-03-06 19:41:10,722 - INFO - validation batch 101, loss: 0.156, 3232/10016 datapoints
2025-03-06 19:41:10,875 - INFO - validation batch 151, loss: 0.322, 4832/10016 datapoints
2025-03-06 19:41:11,034 - INFO - validation batch 201, loss: 0.494, 6432/10016 datapoints
2025-03-06 19:41:11,188 - INFO - validation batch 251, loss: 0.535, 8032/10016 datapoints
2025-03-06 19:41:11,342 - INFO - validation batch 301, loss: 0.061, 9632/10016 datapoints
2025-03-06 19:41:11,380 - INFO - Epoch 430/800 done.
2025-03-06 19:41:11,380 - INFO - Final validation performance:
Loss: 0.300, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:41:11,381 - INFO - Beginning epoch 431/800
2025-03-06 19:41:11,387 - INFO - training batch 1, loss: 0.126, 32/60000 datapoints
2025-03-06 19:41:11,597 - INFO - training batch 51, loss: 0.323, 1632/60000 datapoints
2025-03-06 19:41:11,813 - INFO - training batch 101, loss: 0.247, 3232/60000 datapoints
2025-03-06 19:41:12,012 - INFO - training batch 151, loss: 0.224, 4832/60000 datapoints
2025-03-06 19:41:12,213 - INFO - training batch 201, loss: 0.642, 6432/60000 datapoints
2025-03-06 19:41:12,411 - INFO - training batch 251, loss: 0.217, 8032/60000 datapoints
2025-03-06 19:41:12,606 - INFO - training batch 301, loss: 0.749, 9632/60000 datapoints
2025-03-06 19:41:12,802 - INFO - training batch 351, loss: 0.122, 11232/60000 datapoints
2025-03-06 19:41:12,999 - INFO - training batch 401, loss: 0.182, 12832/60000 datapoints
2025-03-06 19:41:13,206 - INFO - training batch 451, loss: 0.359, 14432/60000 datapoints
2025-03-06 19:41:13,417 - INFO - training batch 501, loss: 0.181, 16032/60000 datapoints
2025-03-06 19:41:13,630 - INFO - training batch 551, loss: 0.452, 17632/60000 datapoints
2025-03-06 19:41:13,824 - INFO - training batch 601, loss: 0.190, 19232/60000 datapoints
2025-03-06 19:41:14,019 - INFO - training batch 651, loss: 0.693, 20832/60000 datapoints
2025-03-06 19:41:14,233 - INFO - training batch 701, loss: 0.429, 22432/60000 datapoints
2025-03-06 19:41:14,438 - INFO - training batch 751, loss: 0.124, 24032/60000 datapoints
2025-03-06 19:41:14,635 - INFO - training batch 801, loss: 0.147, 25632/60000 datapoints
2025-03-06 19:41:14,830 - INFO - training batch 851, loss: 0.166, 27232/60000 datapoints
2025-03-06 19:41:15,031 - INFO - training batch 901, loss: 0.447, 28832/60000 datapoints
2025-03-06 19:41:15,226 - INFO - training batch 951, loss: 0.247, 30432/60000 datapoints
2025-03-06 19:41:15,420 - INFO - training batch 1001, loss: 0.421, 32032/60000 datapoints
2025-03-06 19:41:15,616 - INFO - training batch 1051, loss: 0.250, 33632/60000 datapoints
2025-03-06 19:41:15,812 - INFO - training batch 1101, loss: 0.257, 35232/60000 datapoints
2025-03-06 19:41:16,006 - INFO - training batch 1151, loss: 0.165, 36832/60000 datapoints
2025-03-06 19:41:16,208 - INFO - training batch 1201, loss: 0.273, 38432/60000 datapoints
2025-03-06 19:41:16,410 - INFO - training batch 1251, loss: 0.265, 40032/60000 datapoints
2025-03-06 19:41:16,602 - INFO - training batch 1301, loss: 0.175, 41632/60000 datapoints
2025-03-06 19:41:16,798 - INFO - training batch 1351, loss: 0.220, 43232/60000 datapoints
2025-03-06 19:41:16,995 - INFO - training batch 1401, loss: 0.569, 44832/60000 datapoints
2025-03-06 19:41:17,187 - INFO - training batch 1451, loss: 0.158, 46432/60000 datapoints
2025-03-06 19:41:17,382 - INFO - training batch 1501, loss: 0.142, 48032/60000 datapoints
2025-03-06 19:41:17,583 - INFO - training batch 1551, loss: 0.217, 49632/60000 datapoints
2025-03-06 19:41:17,803 - INFO - training batch 1601, loss: 0.420, 51232/60000 datapoints
2025-03-06 19:41:17,998 - INFO - training batch 1651, loss: 0.099, 52832/60000 datapoints
2025-03-06 19:41:18,194 - INFO - training batch 1701, loss: 0.325, 54432/60000 datapoints
2025-03-06 19:41:18,390 - INFO - training batch 1751, loss: 0.453, 56032/60000 datapoints
2025-03-06 19:41:18,586 - INFO - training batch 1801, loss: 0.375, 57632/60000 datapoints
2025-03-06 19:41:18,784 - INFO - training batch 1851, loss: 0.561, 59232/60000 datapoints
2025-03-06 19:41:18,884 - INFO - validation batch 1, loss: 0.146, 32/10016 datapoints
2025-03-06 19:41:19,040 - INFO - validation batch 51, loss: 0.143, 1632/10016 datapoints
2025-03-06 19:41:19,193 - INFO - validation batch 101, loss: 0.344, 3232/10016 datapoints
2025-03-06 19:41:19,347 - INFO - validation batch 151, loss: 0.220, 4832/10016 datapoints
2025-03-06 19:41:19,499 - INFO - validation batch 201, loss: 0.234, 6432/10016 datapoints
2025-03-06 19:41:19,656 - INFO - validation batch 251, loss: 0.452, 8032/10016 datapoints
2025-03-06 19:41:19,808 - INFO - validation batch 301, loss: 0.350, 9632/10016 datapoints
2025-03-06 19:41:19,845 - INFO - Epoch 431/800 done.
2025-03-06 19:41:19,846 - INFO - Final validation performance:
Loss: 0.270, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:41:19,846 - INFO - Beginning epoch 432/800
2025-03-06 19:41:19,853 - INFO - training batch 1, loss: 0.368, 32/60000 datapoints
2025-03-06 19:41:20,048 - INFO - training batch 51, loss: 0.235, 1632/60000 datapoints
2025-03-06 19:41:20,245 - INFO - training batch 101, loss: 0.196, 3232/60000 datapoints
2025-03-06 19:41:20,451 - INFO - training batch 151, loss: 0.441, 4832/60000 datapoints
2025-03-06 19:41:20,648 - INFO - training batch 201, loss: 0.231, 6432/60000 datapoints
2025-03-06 19:41:20,841 - INFO - training batch 251, loss: 0.253, 8032/60000 datapoints
2025-03-06 19:41:21,045 - INFO - training batch 301, loss: 0.244, 9632/60000 datapoints
2025-03-06 19:41:21,243 - INFO - training batch 351, loss: 0.125, 11232/60000 datapoints
2025-03-06 19:41:21,441 - INFO - training batch 401, loss: 0.420, 12832/60000 datapoints
2025-03-06 19:41:21,638 - INFO - training batch 451, loss: 0.315, 14432/60000 datapoints
2025-03-06 19:41:21,853 - INFO - training batch 501, loss: 0.347, 16032/60000 datapoints
2025-03-06 19:41:22,046 - INFO - training batch 551, loss: 0.167, 17632/60000 datapoints
2025-03-06 19:41:22,242 - INFO - training batch 601, loss: 0.645, 19232/60000 datapoints
2025-03-06 19:41:22,436 - INFO - training batch 651, loss: 0.309, 20832/60000 datapoints
2025-03-06 19:41:22,633 - INFO - training batch 701, loss: 0.374, 22432/60000 datapoints
2025-03-06 19:41:22,827 - INFO - training batch 751, loss: 0.210, 24032/60000 datapoints
2025-03-06 19:41:23,025 - INFO - training batch 801, loss: 0.484, 25632/60000 datapoints
2025-03-06 19:41:23,221 - INFO - training batch 851, loss: 0.120, 27232/60000 datapoints
2025-03-06 19:41:23,414 - INFO - training batch 901, loss: 0.151, 28832/60000 datapoints
2025-03-06 19:41:23,610 - INFO - training batch 951, loss: 0.105, 30432/60000 datapoints
2025-03-06 19:41:23,810 - INFO - training batch 1001, loss: 0.323, 32032/60000 datapoints
2025-03-06 19:41:24,004 - INFO - training batch 1051, loss: 0.132, 33632/60000 datapoints
2025-03-06 19:41:24,199 - INFO - training batch 1101, loss: 0.553, 35232/60000 datapoints
2025-03-06 19:41:24,395 - INFO - training batch 1151, loss: 0.172, 36832/60000 datapoints
2025-03-06 19:41:24,588 - INFO - training batch 1201, loss: 0.491, 38432/60000 datapoints
2025-03-06 19:41:24,786 - INFO - training batch 1251, loss: 0.548, 40032/60000 datapoints
2025-03-06 19:41:24,988 - INFO - training batch 1301, loss: 0.480, 41632/60000 datapoints
2025-03-06 19:41:25,184 - INFO - training batch 1351, loss: 0.456, 43232/60000 datapoints
2025-03-06 19:41:25,379 - INFO - training batch 1401, loss: 0.156, 44832/60000 datapoints
2025-03-06 19:41:25,575 - INFO - training batch 1451, loss: 0.506, 46432/60000 datapoints
2025-03-06 19:41:25,772 - INFO - training batch 1501, loss: 0.180, 48032/60000 datapoints
2025-03-06 19:41:25,967 - INFO - training batch 1551, loss: 0.141, 49632/60000 datapoints
2025-03-06 19:41:26,160 - INFO - training batch 1601, loss: 0.232, 51232/60000 datapoints
2025-03-06 19:41:26,418 - INFO - training batch 1651, loss: 0.160, 52832/60000 datapoints
2025-03-06 19:41:26,613 - INFO - training batch 1701, loss: 0.095, 54432/60000 datapoints
2025-03-06 19:41:26,810 - INFO - training batch 1751, loss: 0.214, 56032/60000 datapoints
2025-03-06 19:41:27,008 - INFO - training batch 1801, loss: 0.433, 57632/60000 datapoints
2025-03-06 19:41:27,202 - INFO - training batch 1851, loss: 0.174, 59232/60000 datapoints
2025-03-06 19:41:27,304 - INFO - validation batch 1, loss: 0.310, 32/10016 datapoints
2025-03-06 19:41:27,456 - INFO - validation batch 51, loss: 0.083, 1632/10016 datapoints
2025-03-06 19:41:27,608 - INFO - validation batch 101, loss: 0.228, 3232/10016 datapoints
2025-03-06 19:41:27,765 - INFO - validation batch 151, loss: 0.211, 4832/10016 datapoints
2025-03-06 19:41:27,919 - INFO - validation batch 201, loss: 0.236, 6432/10016 datapoints
2025-03-06 19:41:28,073 - INFO - validation batch 251, loss: 0.241, 8032/10016 datapoints
2025-03-06 19:41:28,229 - INFO - validation batch 301, loss: 0.317, 9632/10016 datapoints
2025-03-06 19:41:28,267 - INFO - Epoch 432/800 done.
2025-03-06 19:41:28,267 - INFO - Final validation performance:
Loss: 0.232, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:41:28,268 - INFO - Beginning epoch 433/800
2025-03-06 19:41:28,275 - INFO - training batch 1, loss: 0.152, 32/60000 datapoints
2025-03-06 19:41:28,490 - INFO - training batch 51, loss: 0.292, 1632/60000 datapoints
2025-03-06 19:41:28,687 - INFO - training batch 101, loss: 0.363, 3232/60000 datapoints
2025-03-06 19:41:28,889 - INFO - training batch 151, loss: 0.430, 4832/60000 datapoints
2025-03-06 19:41:29,093 - INFO - training batch 201, loss: 0.523, 6432/60000 datapoints
2025-03-06 19:41:29,293 - INFO - training batch 251, loss: 0.211, 8032/60000 datapoints
2025-03-06 19:41:29,488 - INFO - training batch 301, loss: 0.169, 9632/60000 datapoints
2025-03-06 19:41:29,684 - INFO - training batch 351, loss: 0.259, 11232/60000 datapoints
2025-03-06 19:41:29,878 - INFO - training batch 401, loss: 0.131, 12832/60000 datapoints
2025-03-06 19:41:30,074 - INFO - training batch 451, loss: 0.291, 14432/60000 datapoints
2025-03-06 19:41:30,269 - INFO - training batch 501, loss: 0.421, 16032/60000 datapoints
2025-03-06 19:41:30,465 - INFO - training batch 551, loss: 0.207, 17632/60000 datapoints
2025-03-06 19:41:30,665 - INFO - training batch 601, loss: 0.811, 19232/60000 datapoints
2025-03-06 19:41:30,861 - INFO - training batch 651, loss: 0.138, 20832/60000 datapoints
2025-03-06 19:41:31,058 - INFO - training batch 701, loss: 0.454, 22432/60000 datapoints
2025-03-06 19:41:31,255 - INFO - training batch 751, loss: 0.188, 24032/60000 datapoints
2025-03-06 19:41:31,451 - INFO - training batch 801, loss: 0.288, 25632/60000 datapoints
2025-03-06 19:41:31,649 - INFO - training batch 851, loss: 0.460, 27232/60000 datapoints
2025-03-06 19:41:31,857 - INFO - training batch 901, loss: 0.399, 28832/60000 datapoints
2025-03-06 19:41:32,060 - INFO - training batch 951, loss: 0.225, 30432/60000 datapoints
2025-03-06 19:41:32,257 - INFO - training batch 1001, loss: 0.191, 32032/60000 datapoints
2025-03-06 19:41:32,452 - INFO - training batch 1051, loss: 0.223, 33632/60000 datapoints
2025-03-06 19:41:32,647 - INFO - training batch 1101, loss: 0.401, 35232/60000 datapoints
2025-03-06 19:41:32,841 - INFO - training batch 1151, loss: 0.386, 36832/60000 datapoints
2025-03-06 19:41:33,039 - INFO - training batch 1201, loss: 0.195, 38432/60000 datapoints
2025-03-06 19:41:33,238 - INFO - training batch 1251, loss: 0.168, 40032/60000 datapoints
2025-03-06 19:41:33,437 - INFO - training batch 1301, loss: 0.416, 41632/60000 datapoints
2025-03-06 19:41:33,634 - INFO - training batch 1351, loss: 0.298, 43232/60000 datapoints
2025-03-06 19:41:33,830 - INFO - training batch 1401, loss: 0.185, 44832/60000 datapoints
2025-03-06 19:41:34,028 - INFO - training batch 1451, loss: 0.186, 46432/60000 datapoints
2025-03-06 19:41:34,223 - INFO - training batch 1501, loss: 0.088, 48032/60000 datapoints
2025-03-06 19:41:34,418 - INFO - training batch 1551, loss: 0.262, 49632/60000 datapoints
2025-03-06 19:41:34,613 - INFO - training batch 1601, loss: 0.437, 51232/60000 datapoints
2025-03-06 19:41:34,809 - INFO - training batch 1651, loss: 0.148, 52832/60000 datapoints
2025-03-06 19:41:35,011 - INFO - training batch 1701, loss: 0.245, 54432/60000 datapoints
2025-03-06 19:41:35,206 - INFO - training batch 1751, loss: 0.246, 56032/60000 datapoints
2025-03-06 19:41:35,401 - INFO - training batch 1801, loss: 0.343, 57632/60000 datapoints
2025-03-06 19:41:35,595 - INFO - training batch 1851, loss: 0.145, 59232/60000 datapoints
2025-03-06 19:41:35,699 - INFO - validation batch 1, loss: 0.166, 32/10016 datapoints
2025-03-06 19:41:35,851 - INFO - validation batch 51, loss: 0.215, 1632/10016 datapoints
2025-03-06 19:41:36,004 - INFO - validation batch 101, loss: 0.184, 3232/10016 datapoints
2025-03-06 19:41:36,155 - INFO - validation batch 151, loss: 0.119, 4832/10016 datapoints
2025-03-06 19:41:36,320 - INFO - validation batch 201, loss: 0.351, 6432/10016 datapoints
2025-03-06 19:41:36,473 - INFO - validation batch 251, loss: 0.270, 8032/10016 datapoints
2025-03-06 19:41:36,629 - INFO - validation batch 301, loss: 0.308, 9632/10016 datapoints
2025-03-06 19:41:36,666 - INFO - Epoch 433/800 done.
2025-03-06 19:41:36,666 - INFO - Final validation performance:
Loss: 0.231, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:41:36,667 - INFO - Beginning epoch 434/800
2025-03-06 19:41:36,674 - INFO - training batch 1, loss: 0.253, 32/60000 datapoints
2025-03-06 19:41:36,886 - INFO - training batch 51, loss: 0.244, 1632/60000 datapoints
2025-03-06 19:41:37,085 - INFO - training batch 101, loss: 0.262, 3232/60000 datapoints
2025-03-06 19:41:37,285 - INFO - training batch 151, loss: 0.501, 4832/60000 datapoints
2025-03-06 19:41:37,483 - INFO - training batch 201, loss: 0.103, 6432/60000 datapoints
2025-03-06 19:41:37,693 - INFO - training batch 251, loss: 0.551, 8032/60000 datapoints
2025-03-06 19:41:37,884 - INFO - training batch 301, loss: 0.193, 9632/60000 datapoints
2025-03-06 19:41:38,077 - INFO - training batch 351, loss: 0.432, 11232/60000 datapoints
2025-03-06 19:41:38,277 - INFO - training batch 401, loss: 0.073, 12832/60000 datapoints
2025-03-06 19:41:38,470 - INFO - training batch 451, loss: 0.113, 14432/60000 datapoints
2025-03-06 19:41:38,665 - INFO - training batch 501, loss: 0.183, 16032/60000 datapoints
2025-03-06 19:41:38,859 - INFO - training batch 551, loss: 0.189, 17632/60000 datapoints
2025-03-06 19:41:39,059 - INFO - training batch 601, loss: 0.227, 19232/60000 datapoints
2025-03-06 19:41:39,251 - INFO - training batch 651, loss: 0.251, 20832/60000 datapoints
2025-03-06 19:41:39,445 - INFO - training batch 701, loss: 0.160, 22432/60000 datapoints
2025-03-06 19:41:39,640 - INFO - training batch 751, loss: 0.090, 24032/60000 datapoints
2025-03-06 19:41:39,830 - INFO - training batch 801, loss: 0.497, 25632/60000 datapoints
2025-03-06 19:41:40,023 - INFO - training batch 851, loss: 0.201, 27232/60000 datapoints
2025-03-06 19:41:40,215 - INFO - training batch 901, loss: 0.400, 28832/60000 datapoints
2025-03-06 19:41:40,407 - INFO - training batch 951, loss: 0.205, 30432/60000 datapoints
2025-03-06 19:41:40,599 - INFO - training batch 1001, loss: 0.314, 32032/60000 datapoints
2025-03-06 19:41:40,796 - INFO - training batch 1051, loss: 0.304, 33632/60000 datapoints
2025-03-06 19:41:40,988 - INFO - training batch 1101, loss: 0.310, 35232/60000 datapoints
2025-03-06 19:41:41,182 - INFO - training batch 1151, loss: 0.227, 36832/60000 datapoints
2025-03-06 19:41:41,375 - INFO - training batch 1201, loss: 0.132, 38432/60000 datapoints
2025-03-06 19:41:41,565 - INFO - training batch 1251, loss: 0.390, 40032/60000 datapoints
2025-03-06 19:41:41,760 - INFO - training batch 1301, loss: 0.500, 41632/60000 datapoints
2025-03-06 19:41:41,970 - INFO - training batch 1351, loss: 0.185, 43232/60000 datapoints
2025-03-06 19:41:42,162 - INFO - training batch 1401, loss: 0.146, 44832/60000 datapoints
2025-03-06 19:41:42,353 - INFO - training batch 1451, loss: 0.419, 46432/60000 datapoints
2025-03-06 19:41:42,546 - INFO - training batch 1501, loss: 0.255, 48032/60000 datapoints
2025-03-06 19:41:42,739 - INFO - training batch 1551, loss: 0.282, 49632/60000 datapoints
2025-03-06 19:41:42,931 - INFO - training batch 1601, loss: 0.364, 51232/60000 datapoints
2025-03-06 19:41:43,127 - INFO - training batch 1651, loss: 0.121, 52832/60000 datapoints
2025-03-06 19:41:43,320 - INFO - training batch 1701, loss: 0.242, 54432/60000 datapoints
2025-03-06 19:41:43,513 - INFO - training batch 1751, loss: 0.421, 56032/60000 datapoints
2025-03-06 19:41:43,708 - INFO - training batch 1801, loss: 0.101, 57632/60000 datapoints
2025-03-06 19:41:43,907 - INFO - training batch 1851, loss: 0.190, 59232/60000 datapoints
2025-03-06 19:41:44,008 - INFO - validation batch 1, loss: 0.652, 32/10016 datapoints
2025-03-06 19:41:44,159 - INFO - validation batch 51, loss: 0.628, 1632/10016 datapoints
2025-03-06 19:41:44,307 - INFO - validation batch 101, loss: 0.323, 3232/10016 datapoints
2025-03-06 19:41:44,458 - INFO - validation batch 151, loss: 0.115, 4832/10016 datapoints
2025-03-06 19:41:44,609 - INFO - validation batch 201, loss: 0.206, 6432/10016 datapoints
2025-03-06 19:41:44,763 - INFO - validation batch 251, loss: 0.220, 8032/10016 datapoints
2025-03-06 19:41:44,920 - INFO - validation batch 301, loss: 0.185, 9632/10016 datapoints
2025-03-06 19:41:44,961 - INFO - Epoch 434/800 done.
2025-03-06 19:41:44,961 - INFO - Final validation performance:
Loss: 0.333, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:41:44,962 - INFO - Beginning epoch 435/800
2025-03-06 19:41:44,969 - INFO - training batch 1, loss: 0.177, 32/60000 datapoints
2025-03-06 19:41:45,169 - INFO - training batch 51, loss: 0.367, 1632/60000 datapoints
2025-03-06 19:41:45,369 - INFO - training batch 101, loss: 0.119, 3232/60000 datapoints
2025-03-06 19:41:45,569 - INFO - training batch 151, loss: 0.274, 4832/60000 datapoints
2025-03-06 19:41:45,768 - INFO - training batch 201, loss: 0.451, 6432/60000 datapoints
2025-03-06 19:41:45,966 - INFO - training batch 251, loss: 0.577, 8032/60000 datapoints
2025-03-06 19:41:46,157 - INFO - training batch 301, loss: 0.273, 9632/60000 datapoints
2025-03-06 19:41:46,357 - INFO - training batch 351, loss: 0.304, 11232/60000 datapoints
2025-03-06 19:41:46,551 - INFO - training batch 401, loss: 0.165, 12832/60000 datapoints
2025-03-06 19:41:46,746 - INFO - training batch 451, loss: 0.169, 14432/60000 datapoints
2025-03-06 19:41:46,952 - INFO - training batch 501, loss: 0.153, 16032/60000 datapoints
2025-03-06 19:41:47,148 - INFO - training batch 551, loss: 0.146, 17632/60000 datapoints
2025-03-06 19:41:47,342 - INFO - training batch 601, loss: 0.443, 19232/60000 datapoints
2025-03-06 19:41:47,536 - INFO - training batch 651, loss: 0.226, 20832/60000 datapoints
2025-03-06 19:41:47,732 - INFO - training batch 701, loss: 0.693, 22432/60000 datapoints
2025-03-06 19:41:47,923 - INFO - training batch 751, loss: 0.167, 24032/60000 datapoints
2025-03-06 19:41:48,122 - INFO - training batch 801, loss: 0.145, 25632/60000 datapoints
2025-03-06 19:41:48,318 - INFO - training batch 851, loss: 0.466, 27232/60000 datapoints
2025-03-06 19:41:48,522 - INFO - training batch 901, loss: 0.169, 28832/60000 datapoints
2025-03-06 19:41:48,719 - INFO - training batch 951, loss: 0.146, 30432/60000 datapoints
2025-03-06 19:41:48,908 - INFO - training batch 1001, loss: 0.172, 32032/60000 datapoints
2025-03-06 19:41:49,106 - INFO - training batch 1051, loss: 0.123, 33632/60000 datapoints
2025-03-06 19:41:49,296 - INFO - training batch 1101, loss: 0.386, 35232/60000 datapoints
2025-03-06 19:41:49,489 - INFO - training batch 1151, loss: 0.281, 36832/60000 datapoints
2025-03-06 19:41:49,683 - INFO - training batch 1201, loss: 0.183, 38432/60000 datapoints
2025-03-06 19:41:49,875 - INFO - training batch 1251, loss: 0.225, 40032/60000 datapoints
2025-03-06 19:41:50,069 - INFO - training batch 1301, loss: 0.366, 41632/60000 datapoints
2025-03-06 19:41:50,260 - INFO - training batch 1351, loss: 0.129, 43232/60000 datapoints
2025-03-06 19:41:50,455 - INFO - training batch 1401, loss: 0.261, 44832/60000 datapoints
2025-03-06 19:41:50,649 - INFO - training batch 1451, loss: 0.200, 46432/60000 datapoints
2025-03-06 19:41:50,849 - INFO - training batch 1501, loss: 0.502, 48032/60000 datapoints
2025-03-06 19:41:51,048 - INFO - training batch 1551, loss: 0.185, 49632/60000 datapoints
2025-03-06 19:41:51,244 - INFO - training batch 1601, loss: 0.117, 51232/60000 datapoints
2025-03-06 19:41:51,438 - INFO - training batch 1651, loss: 0.228, 52832/60000 datapoints
2025-03-06 19:41:51,641 - INFO - training batch 1701, loss: 0.177, 54432/60000 datapoints
2025-03-06 19:41:51,837 - INFO - training batch 1751, loss: 0.215, 56032/60000 datapoints
2025-03-06 19:41:52,058 - INFO - training batch 1801, loss: 0.040, 57632/60000 datapoints
2025-03-06 19:41:52,256 - INFO - training batch 1851, loss: 0.228, 59232/60000 datapoints
2025-03-06 19:41:52,359 - INFO - validation batch 1, loss: 0.567, 32/10016 datapoints
2025-03-06 19:41:52,516 - INFO - validation batch 51, loss: 0.194, 1632/10016 datapoints
2025-03-06 19:41:52,674 - INFO - validation batch 101, loss: 0.165, 3232/10016 datapoints
2025-03-06 19:41:52,826 - INFO - validation batch 151, loss: 0.198, 4832/10016 datapoints
2025-03-06 19:41:52,981 - INFO - validation batch 201, loss: 0.219, 6432/10016 datapoints
2025-03-06 19:41:53,137 - INFO - validation batch 251, loss: 0.156, 8032/10016 datapoints
2025-03-06 19:41:53,292 - INFO - validation batch 301, loss: 0.336, 9632/10016 datapoints
2025-03-06 19:41:53,330 - INFO - Epoch 435/800 done.
2025-03-06 19:41:53,330 - INFO - Final validation performance:
Loss: 0.262, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:41:53,331 - INFO - Beginning epoch 436/800
2025-03-06 19:41:53,337 - INFO - training batch 1, loss: 0.272, 32/60000 datapoints
2025-03-06 19:41:53,548 - INFO - training batch 51, loss: 0.149, 1632/60000 datapoints
2025-03-06 19:41:53,756 - INFO - training batch 101, loss: 0.531, 3232/60000 datapoints
2025-03-06 19:41:53,962 - INFO - training batch 151, loss: 0.210, 4832/60000 datapoints
2025-03-06 19:41:54,162 - INFO - training batch 201, loss: 0.253, 6432/60000 datapoints
2025-03-06 19:41:54,358 - INFO - training batch 251, loss: 0.262, 8032/60000 datapoints
2025-03-06 19:41:54,561 - INFO - training batch 301, loss: 0.320, 9632/60000 datapoints
2025-03-06 19:41:54,764 - INFO - training batch 351, loss: 0.189, 11232/60000 datapoints
2025-03-06 19:41:54,964 - INFO - training batch 401, loss: 0.125, 12832/60000 datapoints
2025-03-06 19:41:55,168 - INFO - training batch 451, loss: 0.230, 14432/60000 datapoints
2025-03-06 19:41:55,368 - INFO - training batch 501, loss: 0.629, 16032/60000 datapoints
2025-03-06 19:41:55,563 - INFO - training batch 551, loss: 0.108, 17632/60000 datapoints
2025-03-06 19:41:55,768 - INFO - training batch 601, loss: 0.256, 19232/60000 datapoints
2025-03-06 19:41:55,965 - INFO - training batch 651, loss: 0.273, 20832/60000 datapoints
2025-03-06 19:41:56,161 - INFO - training batch 701, loss: 0.211, 22432/60000 datapoints
2025-03-06 19:41:56,363 - INFO - training batch 751, loss: 0.417, 24032/60000 datapoints
2025-03-06 19:41:56,569 - INFO - training batch 801, loss: 0.295, 25632/60000 datapoints
2025-03-06 19:41:56,789 - INFO - training batch 851, loss: 0.139, 27232/60000 datapoints
2025-03-06 19:41:57,002 - INFO - training batch 901, loss: 0.193, 28832/60000 datapoints
2025-03-06 19:41:57,201 - INFO - training batch 951, loss: 0.175, 30432/60000 datapoints
2025-03-06 19:41:57,396 - INFO - training batch 1001, loss: 0.440, 32032/60000 datapoints
2025-03-06 19:41:57,590 - INFO - training batch 1051, loss: 0.145, 33632/60000 datapoints
2025-03-06 19:41:57,789 - INFO - training batch 1101, loss: 0.600, 35232/60000 datapoints
2025-03-06 19:41:57,983 - INFO - training batch 1151, loss: 0.390, 36832/60000 datapoints
2025-03-06 19:41:58,177 - INFO - training batch 1201, loss: 0.185, 38432/60000 datapoints
2025-03-06 19:41:58,373 - INFO - training batch 1251, loss: 0.261, 40032/60000 datapoints
2025-03-06 19:41:58,568 - INFO - training batch 1301, loss: 0.167, 41632/60000 datapoints
2025-03-06 19:41:58,765 - INFO - training batch 1351, loss: 0.132, 43232/60000 datapoints
2025-03-06 19:41:58,970 - INFO - training batch 1401, loss: 0.266, 44832/60000 datapoints
2025-03-06 19:41:59,209 - INFO - training batch 1451, loss: 0.261, 46432/60000 datapoints
2025-03-06 19:41:59,406 - INFO - training batch 1501, loss: 0.113, 48032/60000 datapoints
2025-03-06 19:41:59,601 - INFO - training batch 1551, loss: 0.471, 49632/60000 datapoints
2025-03-06 19:41:59,799 - INFO - training batch 1601, loss: 0.114, 51232/60000 datapoints
2025-03-06 19:41:59,992 - INFO - training batch 1651, loss: 0.190, 52832/60000 datapoints
2025-03-06 19:42:00,189 - INFO - training batch 1701, loss: 0.273, 54432/60000 datapoints
2025-03-06 19:42:00,382 - INFO - training batch 1751, loss: 0.112, 56032/60000 datapoints
2025-03-06 19:42:00,580 - INFO - training batch 1801, loss: 0.404, 57632/60000 datapoints
2025-03-06 19:42:00,779 - INFO - training batch 1851, loss: 0.340, 59232/60000 datapoints
2025-03-06 19:42:00,882 - INFO - validation batch 1, loss: 0.202, 32/10016 datapoints
2025-03-06 19:42:01,039 - INFO - validation batch 51, loss: 0.086, 1632/10016 datapoints
2025-03-06 19:42:01,196 - INFO - validation batch 101, loss: 0.148, 3232/10016 datapoints
2025-03-06 19:42:01,350 - INFO - validation batch 151, loss: 0.120, 4832/10016 datapoints
2025-03-06 19:42:01,504 - INFO - validation batch 201, loss: 0.458, 6432/10016 datapoints
2025-03-06 19:42:01,663 - INFO - validation batch 251, loss: 0.415, 8032/10016 datapoints
2025-03-06 19:42:01,817 - INFO - validation batch 301, loss: 0.474, 9632/10016 datapoints
2025-03-06 19:42:01,854 - INFO - Epoch 436/800 done.
2025-03-06 19:42:01,854 - INFO - Final validation performance:
Loss: 0.272, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:42:01,855 - INFO - Beginning epoch 437/800
2025-03-06 19:42:01,862 - INFO - training batch 1, loss: 0.221, 32/60000 datapoints
2025-03-06 19:42:02,066 - INFO - training batch 51, loss: 0.101, 1632/60000 datapoints
2025-03-06 19:42:02,282 - INFO - training batch 101, loss: 0.251, 3232/60000 datapoints
2025-03-06 19:42:02,487 - INFO - training batch 151, loss: 0.475, 4832/60000 datapoints
2025-03-06 19:42:02,683 - INFO - training batch 201, loss: 0.276, 6432/60000 datapoints
2025-03-06 19:42:02,885 - INFO - training batch 251, loss: 0.233, 8032/60000 datapoints
2025-03-06 19:42:03,081 - INFO - training batch 301, loss: 0.294, 9632/60000 datapoints
2025-03-06 19:42:03,287 - INFO - training batch 351, loss: 0.290, 11232/60000 datapoints
2025-03-06 19:42:03,482 - INFO - training batch 401, loss: 0.176, 12832/60000 datapoints
2025-03-06 19:42:03,678 - INFO - training batch 451, loss: 0.163, 14432/60000 datapoints
2025-03-06 19:42:03,873 - INFO - training batch 501, loss: 0.323, 16032/60000 datapoints
2025-03-06 19:42:04,070 - INFO - training batch 551, loss: 0.125, 17632/60000 datapoints
2025-03-06 19:42:04,268 - INFO - training batch 601, loss: 0.070, 19232/60000 datapoints
2025-03-06 19:42:04,464 - INFO - training batch 651, loss: 0.175, 20832/60000 datapoints
2025-03-06 19:42:04,659 - INFO - training batch 701, loss: 0.072, 22432/60000 datapoints
2025-03-06 19:42:04,856 - INFO - training batch 751, loss: 0.275, 24032/60000 datapoints
2025-03-06 19:42:05,057 - INFO - training batch 801, loss: 0.145, 25632/60000 datapoints
2025-03-06 19:42:05,255 - INFO - training batch 851, loss: 0.277, 27232/60000 datapoints
2025-03-06 19:42:05,448 - INFO - training batch 901, loss: 0.576, 28832/60000 datapoints
2025-03-06 19:42:05,646 - INFO - training batch 951, loss: 0.390, 30432/60000 datapoints
2025-03-06 19:42:05,845 - INFO - training batch 1001, loss: 0.162, 32032/60000 datapoints
2025-03-06 19:42:06,043 - INFO - training batch 1051, loss: 0.428, 33632/60000 datapoints
2025-03-06 19:42:06,241 - INFO - training batch 1101, loss: 0.271, 35232/60000 datapoints
2025-03-06 19:42:06,446 - INFO - training batch 1151, loss: 0.506, 36832/60000 datapoints
2025-03-06 19:42:06,642 - INFO - training batch 1201, loss: 0.184, 38432/60000 datapoints
2025-03-06 19:42:06,835 - INFO - training batch 1251, loss: 0.255, 40032/60000 datapoints
2025-03-06 19:42:07,030 - INFO - training batch 1301, loss: 0.259, 41632/60000 datapoints
2025-03-06 19:42:07,228 - INFO - training batch 1351, loss: 0.080, 43232/60000 datapoints
2025-03-06 19:42:07,422 - INFO - training batch 1401, loss: 0.178, 44832/60000 datapoints
2025-03-06 19:42:07,615 - INFO - training batch 1451, loss: 0.359, 46432/60000 datapoints
2025-03-06 19:42:07,811 - INFO - training batch 1501, loss: 0.174, 48032/60000 datapoints
2025-03-06 19:42:08,007 - INFO - training batch 1551, loss: 0.717, 49632/60000 datapoints
2025-03-06 19:42:08,203 - INFO - training batch 1601, loss: 0.482, 51232/60000 datapoints
2025-03-06 19:42:08,404 - INFO - training batch 1651, loss: 0.101, 52832/60000 datapoints
2025-03-06 19:42:08,598 - INFO - training batch 1701, loss: 0.231, 54432/60000 datapoints
2025-03-06 19:42:08,790 - INFO - training batch 1751, loss: 0.283, 56032/60000 datapoints
2025-03-06 19:42:08,984 - INFO - training batch 1801, loss: 0.211, 57632/60000 datapoints
2025-03-06 19:42:09,192 - INFO - training batch 1851, loss: 0.442, 59232/60000 datapoints
2025-03-06 19:42:09,298 - INFO - validation batch 1, loss: 0.262, 32/10016 datapoints
2025-03-06 19:42:09,459 - INFO - validation batch 51, loss: 0.230, 1632/10016 datapoints
2025-03-06 19:42:09,612 - INFO - validation batch 101, loss: 0.053, 3232/10016 datapoints
2025-03-06 19:42:09,768 - INFO - validation batch 151, loss: 0.243, 4832/10016 datapoints
2025-03-06 19:42:09,919 - INFO - validation batch 201, loss: 0.385, 6432/10016 datapoints
2025-03-06 19:42:10,073 - INFO - validation batch 251, loss: 0.178, 8032/10016 datapoints
2025-03-06 19:42:10,226 - INFO - validation batch 301, loss: 0.216, 9632/10016 datapoints
2025-03-06 19:42:10,261 - INFO - Epoch 437/800 done.
2025-03-06 19:42:10,262 - INFO - Final validation performance:
Loss: 0.224, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:42:10,262 - INFO - Beginning epoch 438/800
2025-03-06 19:42:10,271 - INFO - training batch 1, loss: 0.308, 32/60000 datapoints
2025-03-06 19:42:10,516 - INFO - training batch 51, loss: 0.232, 1632/60000 datapoints
2025-03-06 19:42:10,717 - INFO - training batch 101, loss: 0.257, 3232/60000 datapoints
2025-03-06 19:42:10,923 - INFO - training batch 151, loss: 0.521, 4832/60000 datapoints
2025-03-06 19:42:11,120 - INFO - training batch 201, loss: 0.246, 6432/60000 datapoints
2025-03-06 19:42:11,322 - INFO - training batch 251, loss: 0.578, 8032/60000 datapoints
2025-03-06 19:42:11,516 - INFO - training batch 301, loss: 0.240, 9632/60000 datapoints
2025-03-06 19:42:11,714 - INFO - training batch 351, loss: 0.253, 11232/60000 datapoints
2025-03-06 19:42:11,910 - INFO - training batch 401, loss: 0.211, 12832/60000 datapoints
2025-03-06 19:42:12,104 - INFO - training batch 451, loss: 0.200, 14432/60000 datapoints
2025-03-06 19:42:12,317 - INFO - training batch 501, loss: 0.185, 16032/60000 datapoints
2025-03-06 19:42:12,514 - INFO - training batch 551, loss: 0.267, 17632/60000 datapoints
2025-03-06 19:42:12,711 - INFO - training batch 601, loss: 0.256, 19232/60000 datapoints
2025-03-06 19:42:12,906 - INFO - training batch 651, loss: 0.165, 20832/60000 datapoints
2025-03-06 19:42:13,099 - INFO - training batch 701, loss: 0.239, 22432/60000 datapoints
2025-03-06 19:42:13,297 - INFO - training batch 751, loss: 0.219, 24032/60000 datapoints
2025-03-06 19:42:13,493 - INFO - training batch 801, loss: 0.158, 25632/60000 datapoints
2025-03-06 19:42:13,690 - INFO - training batch 851, loss: 0.078, 27232/60000 datapoints
2025-03-06 19:42:13,886 - INFO - training batch 901, loss: 0.166, 28832/60000 datapoints
2025-03-06 19:42:14,083 - INFO - training batch 951, loss: 0.294, 30432/60000 datapoints
2025-03-06 19:42:14,279 - INFO - training batch 1001, loss: 0.204, 32032/60000 datapoints
2025-03-06 19:42:14,475 - INFO - training batch 1051, loss: 0.173, 33632/60000 datapoints
2025-03-06 19:42:14,672 - INFO - training batch 1101, loss: 0.237, 35232/60000 datapoints
2025-03-06 19:42:14,869 - INFO - training batch 1151, loss: 0.495, 36832/60000 datapoints
2025-03-06 19:42:15,067 - INFO - training batch 1201, loss: 0.152, 38432/60000 datapoints
2025-03-06 19:42:15,265 - INFO - training batch 1251, loss: 0.250, 40032/60000 datapoints
2025-03-06 19:42:15,462 - INFO - training batch 1301, loss: 0.394, 41632/60000 datapoints
2025-03-06 19:42:15,658 - INFO - training batch 1351, loss: 0.467, 43232/60000 datapoints
2025-03-06 19:42:15,850 - INFO - training batch 1401, loss: 0.277, 44832/60000 datapoints
2025-03-06 19:42:16,046 - INFO - training batch 1451, loss: 0.247, 46432/60000 datapoints
2025-03-06 19:42:16,241 - INFO - training batch 1501, loss: 0.109, 48032/60000 datapoints
2025-03-06 19:42:16,444 - INFO - training batch 1551, loss: 0.205, 49632/60000 datapoints
2025-03-06 19:42:16,641 - INFO - training batch 1601, loss: 0.137, 51232/60000 datapoints
2025-03-06 19:42:16,835 - INFO - training batch 1651, loss: 0.083, 52832/60000 datapoints
2025-03-06 19:42:17,030 - INFO - training batch 1701, loss: 0.248, 54432/60000 datapoints
2025-03-06 19:42:17,226 - INFO - training batch 1751, loss: 0.376, 56032/60000 datapoints
2025-03-06 19:42:17,422 - INFO - training batch 1801, loss: 0.295, 57632/60000 datapoints
2025-03-06 19:42:17,620 - INFO - training batch 1851, loss: 0.364, 59232/60000 datapoints
2025-03-06 19:42:17,720 - INFO - validation batch 1, loss: 0.207, 32/10016 datapoints
2025-03-06 19:42:17,874 - INFO - validation batch 51, loss: 0.549, 1632/10016 datapoints
2025-03-06 19:42:18,030 - INFO - validation batch 101, loss: 0.613, 3232/10016 datapoints
2025-03-06 19:42:18,184 - INFO - validation batch 151, loss: 0.260, 4832/10016 datapoints
2025-03-06 19:42:18,338 - INFO - validation batch 201, loss: 0.205, 6432/10016 datapoints
2025-03-06 19:42:18,492 - INFO - validation batch 251, loss: 0.306, 8032/10016 datapoints
2025-03-06 19:42:18,646 - INFO - validation batch 301, loss: 0.161, 9632/10016 datapoints
2025-03-06 19:42:18,684 - INFO - Epoch 438/800 done.
2025-03-06 19:42:18,684 - INFO - Final validation performance:
Loss: 0.329, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:42:18,685 - INFO - Beginning epoch 439/800
2025-03-06 19:42:18,691 - INFO - training batch 1, loss: 0.228, 32/60000 datapoints
2025-03-06 19:42:18,889 - INFO - training batch 51, loss: 0.156, 1632/60000 datapoints
2025-03-06 19:42:19,100 - INFO - training batch 101, loss: 0.073, 3232/60000 datapoints
2025-03-06 19:42:19,298 - INFO - training batch 151, loss: 0.276, 4832/60000 datapoints
2025-03-06 19:42:19,503 - INFO - training batch 201, loss: 0.229, 6432/60000 datapoints
2025-03-06 19:42:19,704 - INFO - training batch 251, loss: 0.094, 8032/60000 datapoints
2025-03-06 19:42:19,902 - INFO - training batch 301, loss: 0.273, 9632/60000 datapoints
2025-03-06 19:42:20,099 - INFO - training batch 351, loss: 0.832, 11232/60000 datapoints
2025-03-06 19:42:20,292 - INFO - training batch 401, loss: 0.312, 12832/60000 datapoints
2025-03-06 19:42:20,486 - INFO - training batch 451, loss: 0.416, 14432/60000 datapoints
2025-03-06 19:42:20,682 - INFO - training batch 501, loss: 0.276, 16032/60000 datapoints
2025-03-06 19:42:20,873 - INFO - training batch 551, loss: 0.177, 17632/60000 datapoints
2025-03-06 19:42:21,068 - INFO - training batch 601, loss: 0.476, 19232/60000 datapoints
2025-03-06 19:42:21,266 - INFO - training batch 651, loss: 0.255, 20832/60000 datapoints
2025-03-06 19:42:21,461 - INFO - training batch 701, loss: 0.145, 22432/60000 datapoints
2025-03-06 19:42:21,657 - INFO - training batch 751, loss: 0.213, 24032/60000 datapoints
2025-03-06 19:42:21,851 - INFO - training batch 801, loss: 0.386, 25632/60000 datapoints
2025-03-06 19:42:22,045 - INFO - training batch 851, loss: 0.334, 27232/60000 datapoints
2025-03-06 19:42:22,243 - INFO - training batch 901, loss: 0.370, 28832/60000 datapoints
2025-03-06 19:42:22,451 - INFO - training batch 951, loss: 0.517, 30432/60000 datapoints
2025-03-06 19:42:22,648 - INFO - training batch 1001, loss: 0.271, 32032/60000 datapoints
2025-03-06 19:42:22,842 - INFO - training batch 1051, loss: 0.202, 33632/60000 datapoints
2025-03-06 19:42:23,040 - INFO - training batch 1101, loss: 0.156, 35232/60000 datapoints
2025-03-06 19:42:23,236 - INFO - training batch 1151, loss: 0.280, 36832/60000 datapoints
2025-03-06 19:42:23,430 - INFO - training batch 1201, loss: 0.376, 38432/60000 datapoints
2025-03-06 19:42:23,629 - INFO - training batch 1251, loss: 0.344, 40032/60000 datapoints
2025-03-06 19:42:23,824 - INFO - training batch 1301, loss: 0.085, 41632/60000 datapoints
2025-03-06 19:42:24,018 - INFO - training batch 1351, loss: 0.338, 43232/60000 datapoints
2025-03-06 19:42:24,212 - INFO - training batch 1401, loss: 0.261, 44832/60000 datapoints
2025-03-06 19:42:24,406 - INFO - training batch 1451, loss: 0.237, 46432/60000 datapoints
2025-03-06 19:42:24,604 - INFO - training batch 1501, loss: 0.289, 48032/60000 datapoints
2025-03-06 19:42:24,798 - INFO - training batch 1551, loss: 0.513, 49632/60000 datapoints
2025-03-06 19:42:24,995 - INFO - training batch 1601, loss: 0.180, 51232/60000 datapoints
2025-03-06 19:42:25,191 - INFO - training batch 1651, loss: 0.092, 52832/60000 datapoints
2025-03-06 19:42:25,387 - INFO - training batch 1701, loss: 0.076, 54432/60000 datapoints
2025-03-06 19:42:25,583 - INFO - training batch 1751, loss: 0.163, 56032/60000 datapoints
2025-03-06 19:42:25,778 - INFO - training batch 1801, loss: 0.330, 57632/60000 datapoints
2025-03-06 19:42:25,972 - INFO - training batch 1851, loss: 0.346, 59232/60000 datapoints
2025-03-06 19:42:26,075 - INFO - validation batch 1, loss: 0.190, 32/10016 datapoints
2025-03-06 19:42:26,227 - INFO - validation batch 51, loss: 0.362, 1632/10016 datapoints
2025-03-06 19:42:26,380 - INFO - validation batch 101, loss: 0.423, 3232/10016 datapoints
2025-03-06 19:42:26,535 - INFO - validation batch 151, loss: 0.257, 4832/10016 datapoints
2025-03-06 19:42:26,692 - INFO - validation batch 201, loss: 0.266, 6432/10016 datapoints
2025-03-06 19:42:26,844 - INFO - validation batch 251, loss: 0.223, 8032/10016 datapoints
2025-03-06 19:42:26,996 - INFO - validation batch 301, loss: 0.191, 9632/10016 datapoints
2025-03-06 19:42:27,034 - INFO - Epoch 439/800 done.
2025-03-06 19:42:27,034 - INFO - Final validation performance:
Loss: 0.273, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:42:27,035 - INFO - Beginning epoch 440/800
2025-03-06 19:42:27,041 - INFO - training batch 1, loss: 0.159, 32/60000 datapoints
2025-03-06 19:42:27,242 - INFO - training batch 51, loss: 0.158, 1632/60000 datapoints
2025-03-06 19:42:27,446 - INFO - training batch 101, loss: 0.310, 3232/60000 datapoints
2025-03-06 19:42:27,644 - INFO - training batch 151, loss: 0.435, 4832/60000 datapoints
2025-03-06 19:42:27,844 - INFO - training batch 201, loss: 0.124, 6432/60000 datapoints
2025-03-06 19:42:28,044 - INFO - training batch 251, loss: 0.259, 8032/60000 datapoints
2025-03-06 19:42:28,243 - INFO - training batch 301, loss: 0.054, 9632/60000 datapoints
2025-03-06 19:42:28,438 - INFO - training batch 351, loss: 0.065, 11232/60000 datapoints
2025-03-06 19:42:28,637 - INFO - training batch 401, loss: 0.135, 12832/60000 datapoints
2025-03-06 19:42:28,831 - INFO - training batch 451, loss: 0.435, 14432/60000 datapoints
2025-03-06 19:42:29,026 - INFO - training batch 501, loss: 0.720, 16032/60000 datapoints
2025-03-06 19:42:29,224 - INFO - training batch 551, loss: 0.511, 17632/60000 datapoints
2025-03-06 19:42:29,416 - INFO - training batch 601, loss: 0.235, 19232/60000 datapoints
2025-03-06 19:42:29,620 - INFO - training batch 651, loss: 0.291, 20832/60000 datapoints
2025-03-06 19:42:29,816 - INFO - training batch 701, loss: 0.210, 22432/60000 datapoints
2025-03-06 19:42:30,012 - INFO - training batch 751, loss: 0.155, 24032/60000 datapoints
2025-03-06 19:42:30,209 - INFO - training batch 801, loss: 0.107, 25632/60000 datapoints
2025-03-06 19:42:30,403 - INFO - training batch 851, loss: 0.208, 27232/60000 datapoints
2025-03-06 19:42:30,597 - INFO - training batch 901, loss: 0.355, 28832/60000 datapoints
2025-03-06 19:42:30,792 - INFO - training batch 951, loss: 0.266, 30432/60000 datapoints
2025-03-06 19:42:30,987 - INFO - training batch 1001, loss: 0.182, 32032/60000 datapoints
2025-03-06 19:42:31,179 - INFO - training batch 1051, loss: 0.220, 33632/60000 datapoints
2025-03-06 19:42:31,376 - INFO - training batch 1101, loss: 0.103, 35232/60000 datapoints
2025-03-06 19:42:31,570 - INFO - training batch 1151, loss: 0.090, 36832/60000 datapoints
2025-03-06 19:42:31,767 - INFO - training batch 1201, loss: 0.097, 38432/60000 datapoints
2025-03-06 19:42:31,963 - INFO - training batch 1251, loss: 0.150, 40032/60000 datapoints
2025-03-06 19:42:32,161 - INFO - training batch 1301, loss: 0.351, 41632/60000 datapoints
2025-03-06 19:42:32,364 - INFO - training batch 1351, loss: 0.128, 43232/60000 datapoints
2025-03-06 19:42:32,564 - INFO - training batch 1401, loss: 0.528, 44832/60000 datapoints
2025-03-06 19:42:32,766 - INFO - training batch 1451, loss: 0.339, 46432/60000 datapoints
2025-03-06 19:42:32,962 - INFO - training batch 1501, loss: 0.207, 48032/60000 datapoints
2025-03-06 19:42:33,158 - INFO - training batch 1551, loss: 0.239, 49632/60000 datapoints
2025-03-06 19:42:33,357 - INFO - training batch 1601, loss: 0.116, 51232/60000 datapoints
2025-03-06 19:42:33,558 - INFO - training batch 1651, loss: 0.151, 52832/60000 datapoints
2025-03-06 19:42:33,770 - INFO - training batch 1701, loss: 0.188, 54432/60000 datapoints
2025-03-06 19:42:33,966 - INFO - training batch 1751, loss: 0.102, 56032/60000 datapoints
2025-03-06 19:42:34,161 - INFO - training batch 1801, loss: 0.180, 57632/60000 datapoints
2025-03-06 19:42:34,357 - INFO - training batch 1851, loss: 0.180, 59232/60000 datapoints
2025-03-06 19:42:34,458 - INFO - validation batch 1, loss: 0.133, 32/10016 datapoints
2025-03-06 19:42:34,614 - INFO - validation batch 51, loss: 0.206, 1632/10016 datapoints
2025-03-06 19:42:34,799 - INFO - validation batch 101, loss: 0.154, 3232/10016 datapoints
2025-03-06 19:42:34,956 - INFO - validation batch 151, loss: 0.261, 4832/10016 datapoints
2025-03-06 19:42:35,115 - INFO - validation batch 201, loss: 0.357, 6432/10016 datapoints
2025-03-06 19:42:35,273 - INFO - validation batch 251, loss: 0.145, 8032/10016 datapoints
2025-03-06 19:42:35,426 - INFO - validation batch 301, loss: 0.510, 9632/10016 datapoints
2025-03-06 19:42:35,463 - INFO - Epoch 440/800 done.
2025-03-06 19:42:35,463 - INFO - Final validation performance:
Loss: 0.252, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:42:35,464 - INFO - Beginning epoch 441/800
2025-03-06 19:42:35,470 - INFO - training batch 1, loss: 0.267, 32/60000 datapoints
2025-03-06 19:42:35,684 - INFO - training batch 51, loss: 0.424, 1632/60000 datapoints
2025-03-06 19:42:35,877 - INFO - training batch 101, loss: 0.230, 3232/60000 datapoints
2025-03-06 19:42:36,076 - INFO - training batch 151, loss: 0.220, 4832/60000 datapoints
2025-03-06 19:42:36,274 - INFO - training batch 201, loss: 0.122, 6432/60000 datapoints
2025-03-06 19:42:36,473 - INFO - training batch 251, loss: 0.073, 8032/60000 datapoints
2025-03-06 19:42:36,677 - INFO - training batch 301, loss: 0.586, 9632/60000 datapoints
2025-03-06 19:42:36,869 - INFO - training batch 351, loss: 0.187, 11232/60000 datapoints
2025-03-06 19:42:37,059 - INFO - training batch 401, loss: 0.266, 12832/60000 datapoints
2025-03-06 19:42:37,254 - INFO - training batch 451, loss: 0.474, 14432/60000 datapoints
2025-03-06 19:42:37,443 - INFO - training batch 501, loss: 0.065, 16032/60000 datapoints
2025-03-06 19:42:37,654 - INFO - training batch 551, loss: 0.144, 17632/60000 datapoints
2025-03-06 19:42:37,850 - INFO - training batch 601, loss: 0.769, 19232/60000 datapoints
2025-03-06 19:42:38,041 - INFO - training batch 651, loss: 0.281, 20832/60000 datapoints
2025-03-06 19:42:38,239 - INFO - training batch 701, loss: 0.551, 22432/60000 datapoints
2025-03-06 19:42:38,431 - INFO - training batch 751, loss: 0.159, 24032/60000 datapoints
2025-03-06 19:42:38,629 - INFO - training batch 801, loss: 0.133, 25632/60000 datapoints
2025-03-06 19:42:38,821 - INFO - training batch 851, loss: 0.192, 27232/60000 datapoints
2025-03-06 19:42:39,015 - INFO - training batch 901, loss: 0.204, 28832/60000 datapoints
2025-03-06 19:42:39,208 - INFO - training batch 951, loss: 0.234, 30432/60000 datapoints
2025-03-06 19:42:39,404 - INFO - training batch 1001, loss: 0.172, 32032/60000 datapoints
2025-03-06 19:42:39,593 - INFO - training batch 1051, loss: 0.548, 33632/60000 datapoints
2025-03-06 19:42:39,787 - INFO - training batch 1101, loss: 0.217, 35232/60000 datapoints
2025-03-06 19:42:39,979 - INFO - training batch 1151, loss: 0.145, 36832/60000 datapoints
2025-03-06 19:42:40,169 - INFO - training batch 1201, loss: 0.236, 38432/60000 datapoints
2025-03-06 19:42:40,357 - INFO - training batch 1251, loss: 0.274, 40032/60000 datapoints
2025-03-06 19:42:40,552 - INFO - training batch 1301, loss: 0.568, 41632/60000 datapoints
2025-03-06 19:42:40,750 - INFO - training batch 1351, loss: 0.514, 43232/60000 datapoints
2025-03-06 19:42:40,941 - INFO - training batch 1401, loss: 0.299, 44832/60000 datapoints
2025-03-06 19:42:41,130 - INFO - training batch 1451, loss: 0.228, 46432/60000 datapoints
2025-03-06 19:42:41,326 - INFO - training batch 1501, loss: 0.256, 48032/60000 datapoints
2025-03-06 19:42:41,516 - INFO - training batch 1551, loss: 0.095, 49632/60000 datapoints
2025-03-06 19:42:41,711 - INFO - training batch 1601, loss: 0.234, 51232/60000 datapoints
2025-03-06 19:42:41,905 - INFO - training batch 1651, loss: 0.352, 52832/60000 datapoints
2025-03-06 19:42:42,100 - INFO - training batch 1701, loss: 0.139, 54432/60000 datapoints
2025-03-06 19:42:42,319 - INFO - training batch 1751, loss: 0.070, 56032/60000 datapoints
2025-03-06 19:42:42,556 - INFO - training batch 1801, loss: 0.095, 57632/60000 datapoints
2025-03-06 19:42:42,767 - INFO - training batch 1851, loss: 0.461, 59232/60000 datapoints
2025-03-06 19:42:42,875 - INFO - validation batch 1, loss: 0.395, 32/10016 datapoints
2025-03-06 19:42:43,026 - INFO - validation batch 51, loss: 0.137, 1632/10016 datapoints
2025-03-06 19:42:43,178 - INFO - validation batch 101, loss: 0.268, 3232/10016 datapoints
2025-03-06 19:42:43,331 - INFO - validation batch 151, loss: 0.389, 4832/10016 datapoints
2025-03-06 19:42:43,482 - INFO - validation batch 201, loss: 0.275, 6432/10016 datapoints
2025-03-06 19:42:43,636 - INFO - validation batch 251, loss: 0.170, 8032/10016 datapoints
2025-03-06 19:42:43,791 - INFO - validation batch 301, loss: 0.501, 9632/10016 datapoints
2025-03-06 19:42:43,827 - INFO - Epoch 441/800 done.
2025-03-06 19:42:43,827 - INFO - Final validation performance:
Loss: 0.305, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:42:43,827 - INFO - Beginning epoch 442/800
2025-03-06 19:42:43,834 - INFO - training batch 1, loss: 0.163, 32/60000 datapoints
2025-03-06 19:42:44,038 - INFO - training batch 51, loss: 0.110, 1632/60000 datapoints
2025-03-06 19:42:44,235 - INFO - training batch 101, loss: 0.139, 3232/60000 datapoints
2025-03-06 19:42:44,434 - INFO - training batch 151, loss: 0.206, 4832/60000 datapoints
2025-03-06 19:42:44,633 - INFO - training batch 201, loss: 0.172, 6432/60000 datapoints
2025-03-06 19:42:44,831 - INFO - training batch 251, loss: 0.076, 8032/60000 datapoints
2025-03-06 19:42:45,027 - INFO - training batch 301, loss: 0.258, 9632/60000 datapoints
2025-03-06 19:42:45,223 - INFO - training batch 351, loss: 0.329, 11232/60000 datapoints
2025-03-06 19:42:45,420 - INFO - training batch 401, loss: 0.172, 12832/60000 datapoints
2025-03-06 19:42:45,614 - INFO - training batch 451, loss: 0.344, 14432/60000 datapoints
2025-03-06 19:42:45,810 - INFO - training batch 501, loss: 0.481, 16032/60000 datapoints
2025-03-06 19:42:46,003 - INFO - training batch 551, loss: 0.461, 17632/60000 datapoints
2025-03-06 19:42:46,197 - INFO - training batch 601, loss: 0.180, 19232/60000 datapoints
2025-03-06 19:42:46,390 - INFO - training batch 651, loss: 0.362, 20832/60000 datapoints
2025-03-06 19:42:46,585 - INFO - training batch 701, loss: 0.154, 22432/60000 datapoints
2025-03-06 19:42:46,779 - INFO - training batch 751, loss: 0.340, 24032/60000 datapoints
2025-03-06 19:42:46,970 - INFO - training batch 801, loss: 0.213, 25632/60000 datapoints
2025-03-06 19:42:47,163 - INFO - training batch 851, loss: 0.415, 27232/60000 datapoints
2025-03-06 19:42:47,358 - INFO - training batch 901, loss: 0.434, 28832/60000 datapoints
2025-03-06 19:42:47,552 - INFO - training batch 951, loss: 0.275, 30432/60000 datapoints
2025-03-06 19:42:47,746 - INFO - training batch 1001, loss: 0.130, 32032/60000 datapoints
2025-03-06 19:42:47,939 - INFO - training batch 1051, loss: 0.266, 33632/60000 datapoints
2025-03-06 19:42:48,136 - INFO - training batch 1101, loss: 0.182, 35232/60000 datapoints
2025-03-06 19:42:48,333 - INFO - training batch 1151, loss: 0.147, 36832/60000 datapoints
2025-03-06 19:42:48,527 - INFO - training batch 1201, loss: 0.198, 38432/60000 datapoints
2025-03-06 19:42:48,721 - INFO - training batch 1251, loss: 0.202, 40032/60000 datapoints
2025-03-06 19:42:48,915 - INFO - training batch 1301, loss: 0.214, 41632/60000 datapoints
2025-03-06 19:42:49,109 - INFO - training batch 1351, loss: 0.470, 43232/60000 datapoints
2025-03-06 19:42:49,303 - INFO - training batch 1401, loss: 0.454, 44832/60000 datapoints
2025-03-06 19:42:49,495 - INFO - training batch 1451, loss: 0.226, 46432/60000 datapoints
2025-03-06 19:42:49,693 - INFO - training batch 1501, loss: 0.460, 48032/60000 datapoints
2025-03-06 19:42:49,896 - INFO - training batch 1551, loss: 0.356, 49632/60000 datapoints
2025-03-06 19:42:50,090 - INFO - training batch 1601, loss: 0.091, 51232/60000 datapoints
2025-03-06 19:42:50,283 - INFO - training batch 1651, loss: 0.356, 52832/60000 datapoints
2025-03-06 19:42:50,479 - INFO - training batch 1701, loss: 0.173, 54432/60000 datapoints
2025-03-06 19:42:50,673 - INFO - training batch 1751, loss: 0.179, 56032/60000 datapoints
2025-03-06 19:42:50,873 - INFO - training batch 1801, loss: 0.236, 57632/60000 datapoints
2025-03-06 19:42:51,065 - INFO - training batch 1851, loss: 0.451, 59232/60000 datapoints
2025-03-06 19:42:51,167 - INFO - validation batch 1, loss: 0.091, 32/10016 datapoints
2025-03-06 19:42:51,324 - INFO - validation batch 51, loss: 0.165, 1632/10016 datapoints
2025-03-06 19:42:51,476 - INFO - validation batch 101, loss: 0.283, 3232/10016 datapoints
2025-03-06 19:42:51,632 - INFO - validation batch 151, loss: 0.105, 4832/10016 datapoints
2025-03-06 19:42:51,785 - INFO - validation batch 201, loss: 0.272, 6432/10016 datapoints
2025-03-06 19:42:51,942 - INFO - validation batch 251, loss: 0.478, 8032/10016 datapoints
2025-03-06 19:42:52,092 - INFO - validation batch 301, loss: 0.196, 9632/10016 datapoints
2025-03-06 19:42:52,131 - INFO - Epoch 442/800 done.
2025-03-06 19:42:52,131 - INFO - Final validation performance:
Loss: 0.227, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:42:52,132 - INFO - Beginning epoch 443/800
2025-03-06 19:42:52,138 - INFO - training batch 1, loss: 0.285, 32/60000 datapoints
2025-03-06 19:42:52,353 - INFO - training batch 51, loss: 0.224, 1632/60000 datapoints
2025-03-06 19:42:52,566 - INFO - training batch 101, loss: 0.407, 3232/60000 datapoints
2025-03-06 19:42:52,768 - INFO - training batch 151, loss: 0.381, 4832/60000 datapoints
2025-03-06 19:42:52,966 - INFO - training batch 201, loss: 0.343, 6432/60000 datapoints
2025-03-06 19:42:53,166 - INFO - training batch 251, loss: 0.136, 8032/60000 datapoints
2025-03-06 19:42:53,368 - INFO - training batch 301, loss: 0.112, 9632/60000 datapoints
2025-03-06 19:42:53,566 - INFO - training batch 351, loss: 0.155, 11232/60000 datapoints
2025-03-06 19:42:53,763 - INFO - training batch 401, loss: 0.417, 12832/60000 datapoints
2025-03-06 19:42:53,958 - INFO - training batch 451, loss: 0.206, 14432/60000 datapoints
2025-03-06 19:42:54,154 - INFO - training batch 501, loss: 0.216, 16032/60000 datapoints
2025-03-06 19:42:54,348 - INFO - training batch 551, loss: 0.210, 17632/60000 datapoints
2025-03-06 19:42:54,543 - INFO - training batch 601, loss: 0.338, 19232/60000 datapoints
2025-03-06 19:42:54,743 - INFO - training batch 651, loss: 0.396, 20832/60000 datapoints
2025-03-06 19:42:54,944 - INFO - training batch 701, loss: 0.353, 22432/60000 datapoints
2025-03-06 19:42:55,141 - INFO - training batch 751, loss: 0.070, 24032/60000 datapoints
2025-03-06 19:42:55,336 - INFO - training batch 801, loss: 0.382, 25632/60000 datapoints
2025-03-06 19:42:55,532 - INFO - training batch 851, loss: 0.342, 27232/60000 datapoints
2025-03-06 19:42:55,727 - INFO - training batch 901, loss: 0.254, 28832/60000 datapoints
2025-03-06 19:42:55,924 - INFO - training batch 951, loss: 0.230, 30432/60000 datapoints
2025-03-06 19:42:56,117 - INFO - training batch 1001, loss: 0.196, 32032/60000 datapoints
2025-03-06 19:42:56,313 - INFO - training batch 1051, loss: 0.220, 33632/60000 datapoints
2025-03-06 19:42:56,514 - INFO - training batch 1101, loss: 0.201, 35232/60000 datapoints
2025-03-06 19:42:56,714 - INFO - training batch 1151, loss: 0.135, 36832/60000 datapoints
2025-03-06 19:42:56,910 - INFO - training batch 1201, loss: 0.251, 38432/60000 datapoints
2025-03-06 19:42:57,102 - INFO - training batch 1251, loss: 0.505, 40032/60000 datapoints
2025-03-06 19:42:57,299 - INFO - training batch 1301, loss: 0.611, 41632/60000 datapoints
2025-03-06 19:42:57,491 - INFO - training batch 1351, loss: 0.173, 43232/60000 datapoints
2025-03-06 19:42:57,690 - INFO - training batch 1401, loss: 0.197, 44832/60000 datapoints
2025-03-06 19:42:57,886 - INFO - training batch 1451, loss: 0.310, 46432/60000 datapoints
2025-03-06 19:42:58,078 - INFO - training batch 1501, loss: 0.080, 48032/60000 datapoints
2025-03-06 19:42:58,271 - INFO - training batch 1551, loss: 0.195, 49632/60000 datapoints
2025-03-06 19:42:58,468 - INFO - training batch 1601, loss: 0.096, 51232/60000 datapoints
2025-03-06 19:42:58,665 - INFO - training batch 1651, loss: 0.268, 52832/60000 datapoints
2025-03-06 19:42:58,859 - INFO - training batch 1701, loss: 0.158, 54432/60000 datapoints
2025-03-06 19:42:59,058 - INFO - training batch 1751, loss: 0.262, 56032/60000 datapoints
2025-03-06 19:42:59,253 - INFO - training batch 1801, loss: 0.227, 57632/60000 datapoints
2025-03-06 19:42:59,448 - INFO - training batch 1851, loss: 0.147, 59232/60000 datapoints
2025-03-06 19:42:59,550 - INFO - validation batch 1, loss: 0.399, 32/10016 datapoints
2025-03-06 19:42:59,706 - INFO - validation batch 51, loss: 0.271, 1632/10016 datapoints
2025-03-06 19:42:59,858 - INFO - validation batch 101, loss: 0.160, 3232/10016 datapoints
2025-03-06 19:43:00,018 - INFO - validation batch 151, loss: 0.172, 4832/10016 datapoints
2025-03-06 19:43:00,174 - INFO - validation batch 201, loss: 0.432, 6432/10016 datapoints
2025-03-06 19:43:00,330 - INFO - validation batch 251, loss: 0.269, 8032/10016 datapoints
2025-03-06 19:43:00,484 - INFO - validation batch 301, loss: 0.378, 9632/10016 datapoints
2025-03-06 19:43:00,521 - INFO - Epoch 443/800 done.
2025-03-06 19:43:00,521 - INFO - Final validation performance:
Loss: 0.297, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:43:00,522 - INFO - Beginning epoch 444/800
2025-03-06 19:43:00,529 - INFO - training batch 1, loss: 0.115, 32/60000 datapoints
2025-03-06 19:43:00,731 - INFO - training batch 51, loss: 0.369, 1632/60000 datapoints
2025-03-06 19:43:00,927 - INFO - training batch 101, loss: 0.123, 3232/60000 datapoints
2025-03-06 19:43:01,137 - INFO - training batch 151, loss: 0.366, 4832/60000 datapoints
2025-03-06 19:43:01,333 - INFO - training batch 201, loss: 0.167, 6432/60000 datapoints
2025-03-06 19:43:01,537 - INFO - training batch 251, loss: 0.552, 8032/60000 datapoints
2025-03-06 19:43:01,739 - INFO - training batch 301, loss: 0.190, 9632/60000 datapoints
2025-03-06 19:43:01,939 - INFO - training batch 351, loss: 0.215, 11232/60000 datapoints
2025-03-06 19:43:02,134 - INFO - training batch 401, loss: 0.226, 12832/60000 datapoints
2025-03-06 19:43:02,330 - INFO - training batch 451, loss: 0.313, 14432/60000 datapoints
2025-03-06 19:43:02,527 - INFO - training batch 501, loss: 0.290, 16032/60000 datapoints
2025-03-06 19:43:02,746 - INFO - training batch 551, loss: 0.117, 17632/60000 datapoints
2025-03-06 19:43:02,941 - INFO - training batch 601, loss: 0.331, 19232/60000 datapoints
2025-03-06 19:43:03,135 - INFO - training batch 651, loss: 0.237, 20832/60000 datapoints
2025-03-06 19:43:03,334 - INFO - training batch 701, loss: 0.145, 22432/60000 datapoints
2025-03-06 19:43:03,529 - INFO - training batch 751, loss: 0.295, 24032/60000 datapoints
2025-03-06 19:43:03,725 - INFO - training batch 801, loss: 0.599, 25632/60000 datapoints
2025-03-06 19:43:03,920 - INFO - training batch 851, loss: 0.102, 27232/60000 datapoints
2025-03-06 19:43:04,114 - INFO - training batch 901, loss: 0.165, 28832/60000 datapoints
2025-03-06 19:43:04,308 - INFO - training batch 951, loss: 0.160, 30432/60000 datapoints
2025-03-06 19:43:04,505 - INFO - training batch 1001, loss: 0.249, 32032/60000 datapoints
2025-03-06 19:43:04,703 - INFO - training batch 1051, loss: 0.244, 33632/60000 datapoints
2025-03-06 19:43:04,902 - INFO - training batch 1101, loss: 0.135, 35232/60000 datapoints
2025-03-06 19:43:05,101 - INFO - training batch 1151, loss: 0.334, 36832/60000 datapoints
2025-03-06 19:43:05,299 - INFO - training batch 1201, loss: 0.270, 38432/60000 datapoints
2025-03-06 19:43:05,497 - INFO - training batch 1251, loss: 0.109, 40032/60000 datapoints
2025-03-06 19:43:05,698 - INFO - training batch 1301, loss: 0.718, 41632/60000 datapoints
2025-03-06 19:43:05,896 - INFO - training batch 1351, loss: 0.203, 43232/60000 datapoints
2025-03-06 19:43:06,095 - INFO - training batch 1401, loss: 0.268, 44832/60000 datapoints
2025-03-06 19:43:06,288 - INFO - training batch 1451, loss: 0.540, 46432/60000 datapoints
2025-03-06 19:43:06,485 - INFO - training batch 1501, loss: 0.206, 48032/60000 datapoints
2025-03-06 19:43:06,687 - INFO - training batch 1551, loss: 0.335, 49632/60000 datapoints
2025-03-06 19:43:06,882 - INFO - training batch 1601, loss: 0.121, 51232/60000 datapoints
2025-03-06 19:43:07,076 - INFO - training batch 1651, loss: 0.409, 52832/60000 datapoints
2025-03-06 19:43:07,274 - INFO - training batch 1701, loss: 0.623, 54432/60000 datapoints
2025-03-06 19:43:07,471 - INFO - training batch 1751, loss: 0.077, 56032/60000 datapoints
2025-03-06 19:43:07,671 - INFO - training batch 1801, loss: 0.281, 57632/60000 datapoints
2025-03-06 19:43:07,864 - INFO - training batch 1851, loss: 0.319, 59232/60000 datapoints
2025-03-06 19:43:07,966 - INFO - validation batch 1, loss: 0.542, 32/10016 datapoints
2025-03-06 19:43:08,120 - INFO - validation batch 51, loss: 0.146, 1632/10016 datapoints
2025-03-06 19:43:08,273 - INFO - validation batch 101, loss: 0.255, 3232/10016 datapoints
2025-03-06 19:43:08,430 - INFO - validation batch 151, loss: 0.385, 4832/10016 datapoints
2025-03-06 19:43:08,584 - INFO - validation batch 201, loss: 0.172, 6432/10016 datapoints
2025-03-06 19:43:08,738 - INFO - validation batch 251, loss: 0.149, 8032/10016 datapoints
2025-03-06 19:43:08,905 - INFO - validation batch 301, loss: 0.191, 9632/10016 datapoints
2025-03-06 19:43:08,947 - INFO - Epoch 444/800 done.
2025-03-06 19:43:08,947 - INFO - Final validation performance:
Loss: 0.263, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:43:08,948 - INFO - Beginning epoch 445/800
2025-03-06 19:43:08,956 - INFO - training batch 1, loss: 0.678, 32/60000 datapoints
2025-03-06 19:43:09,164 - INFO - training batch 51, loss: 0.173, 1632/60000 datapoints
2025-03-06 19:43:09,359 - INFO - training batch 101, loss: 0.249, 3232/60000 datapoints
2025-03-06 19:43:09,564 - INFO - training batch 151, loss: 0.363, 4832/60000 datapoints
2025-03-06 19:43:09,763 - INFO - training batch 201, loss: 0.395, 6432/60000 datapoints
2025-03-06 19:43:09,963 - INFO - training batch 251, loss: 0.102, 8032/60000 datapoints
2025-03-06 19:43:10,162 - INFO - training batch 301, loss: 0.597, 9632/60000 datapoints
2025-03-06 19:43:10,359 - INFO - training batch 351, loss: 0.495, 11232/60000 datapoints
2025-03-06 19:43:10,556 - INFO - training batch 401, loss: 0.412, 12832/60000 datapoints
2025-03-06 19:43:10,754 - INFO - training batch 451, loss: 0.334, 14432/60000 datapoints
2025-03-06 19:43:10,947 - INFO - training batch 501, loss: 0.267, 16032/60000 datapoints
2025-03-06 19:43:11,143 - INFO - training batch 551, loss: 0.267, 17632/60000 datapoints
2025-03-06 19:43:11,338 - INFO - training batch 601, loss: 0.218, 19232/60000 datapoints
2025-03-06 19:43:11,535 - INFO - training batch 651, loss: 0.194, 20832/60000 datapoints
2025-03-06 19:43:11,737 - INFO - training batch 701, loss: 0.254, 22432/60000 datapoints
2025-03-06 19:43:11,933 - INFO - training batch 751, loss: 0.319, 24032/60000 datapoints
2025-03-06 19:43:12,126 - INFO - training batch 801, loss: 0.081, 25632/60000 datapoints
2025-03-06 19:43:12,320 - INFO - training batch 851, loss: 0.359, 27232/60000 datapoints
2025-03-06 19:43:12,520 - INFO - training batch 901, loss: 0.192, 28832/60000 datapoints
2025-03-06 19:43:12,735 - INFO - training batch 951, loss: 0.561, 30432/60000 datapoints
2025-03-06 19:43:12,929 - INFO - training batch 1001, loss: 0.349, 32032/60000 datapoints
2025-03-06 19:43:13,126 - INFO - training batch 1051, loss: 0.795, 33632/60000 datapoints
2025-03-06 19:43:13,326 - INFO - training batch 1101, loss: 0.251, 35232/60000 datapoints
2025-03-06 19:43:13,523 - INFO - training batch 1151, loss: 0.160, 36832/60000 datapoints
2025-03-06 19:43:13,725 - INFO - training batch 1201, loss: 0.150, 38432/60000 datapoints
2025-03-06 19:43:13,929 - INFO - training batch 1251, loss: 0.438, 40032/60000 datapoints
2025-03-06 19:43:14,125 - INFO - training batch 1301, loss: 0.448, 41632/60000 datapoints
2025-03-06 19:43:14,322 - INFO - training batch 1351, loss: 0.241, 43232/60000 datapoints
2025-03-06 19:43:14,516 - INFO - training batch 1401, loss: 0.167, 44832/60000 datapoints
2025-03-06 19:43:14,727 - INFO - training batch 1451, loss: 0.442, 46432/60000 datapoints
2025-03-06 19:43:14,924 - INFO - training batch 1501, loss: 0.201, 48032/60000 datapoints
2025-03-06 19:43:15,119 - INFO - training batch 1551, loss: 0.208, 49632/60000 datapoints
2025-03-06 19:43:15,318 - INFO - training batch 1601, loss: 0.355, 51232/60000 datapoints
2025-03-06 19:43:15,514 - INFO - training batch 1651, loss: 0.264, 52832/60000 datapoints
2025-03-06 19:43:15,709 - INFO - training batch 1701, loss: 0.089, 54432/60000 datapoints
2025-03-06 19:43:15,903 - INFO - training batch 1751, loss: 0.365, 56032/60000 datapoints
2025-03-06 19:43:16,099 - INFO - training batch 1801, loss: 0.477, 57632/60000 datapoints
2025-03-06 19:43:16,292 - INFO - training batch 1851, loss: 0.135, 59232/60000 datapoints
2025-03-06 19:43:16,393 - INFO - validation batch 1, loss: 0.199, 32/10016 datapoints
2025-03-06 19:43:16,547 - INFO - validation batch 51, loss: 0.193, 1632/10016 datapoints
2025-03-06 19:43:16,708 - INFO - validation batch 101, loss: 0.234, 3232/10016 datapoints
2025-03-06 19:43:16,859 - INFO - validation batch 151, loss: 0.536, 4832/10016 datapoints
2025-03-06 19:43:17,013 - INFO - validation batch 201, loss: 0.116, 6432/10016 datapoints
2025-03-06 19:43:17,164 - INFO - validation batch 251, loss: 0.294, 8032/10016 datapoints
2025-03-06 19:43:17,316 - INFO - validation batch 301, loss: 0.413, 9632/10016 datapoints
2025-03-06 19:43:17,352 - INFO - Epoch 445/800 done.
2025-03-06 19:43:17,353 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.923top-5 acc: 0.923
2025-03-06 19:43:17,353 - INFO - Beginning epoch 446/800
2025-03-06 19:43:17,360 - INFO - training batch 1, loss: 0.232, 32/60000 datapoints
2025-03-06 19:43:17,570 - INFO - training batch 51, loss: 0.125, 1632/60000 datapoints
2025-03-06 19:43:17,779 - INFO - training batch 101, loss: 0.228, 3232/60000 datapoints
2025-03-06 19:43:17,977 - INFO - training batch 151, loss: 0.165, 4832/60000 datapoints
2025-03-06 19:43:18,172 - INFO - training batch 201, loss: 0.497, 6432/60000 datapoints
2025-03-06 19:43:18,372 - INFO - training batch 251, loss: 0.111, 8032/60000 datapoints
2025-03-06 19:43:18,569 - INFO - training batch 301, loss: 0.441, 9632/60000 datapoints
2025-03-06 19:43:18,767 - INFO - training batch 351, loss: 0.237, 11232/60000 datapoints
2025-03-06 19:43:18,961 - INFO - training batch 401, loss: 0.195, 12832/60000 datapoints
2025-03-06 19:43:19,156 - INFO - training batch 451, loss: 0.136, 14432/60000 datapoints
2025-03-06 19:43:19,350 - INFO - training batch 501, loss: 0.245, 16032/60000 datapoints
2025-03-06 19:43:19,545 - INFO - training batch 551, loss: 0.166, 17632/60000 datapoints
2025-03-06 19:43:19,744 - INFO - training batch 601, loss: 0.085, 19232/60000 datapoints
2025-03-06 19:43:19,938 - INFO - training batch 651, loss: 0.189, 20832/60000 datapoints
2025-03-06 19:43:20,132 - INFO - training batch 701, loss: 0.186, 22432/60000 datapoints
2025-03-06 19:43:20,329 - INFO - training batch 751, loss: 0.333, 24032/60000 datapoints
2025-03-06 19:43:20,523 - INFO - training batch 801, loss: 0.252, 25632/60000 datapoints
2025-03-06 19:43:20,719 - INFO - training batch 851, loss: 0.356, 27232/60000 datapoints
2025-03-06 19:43:20,918 - INFO - training batch 901, loss: 0.226, 28832/60000 datapoints
2025-03-06 19:43:21,114 - INFO - training batch 951, loss: 0.176, 30432/60000 datapoints
2025-03-06 19:43:21,310 - INFO - training batch 1001, loss: 0.113, 32032/60000 datapoints
2025-03-06 19:43:21,505 - INFO - training batch 1051, loss: 0.761, 33632/60000 datapoints
2025-03-06 19:43:21,705 - INFO - training batch 1101, loss: 0.546, 35232/60000 datapoints
2025-03-06 19:43:21,899 - INFO - training batch 1151, loss: 0.106, 36832/60000 datapoints
2025-03-06 19:43:22,095 - INFO - training batch 1201, loss: 0.131, 38432/60000 datapoints
2025-03-06 19:43:22,291 - INFO - training batch 1251, loss: 0.104, 40032/60000 datapoints
2025-03-06 19:43:22,485 - INFO - training batch 1301, loss: 0.197, 41632/60000 datapoints
2025-03-06 19:43:22,682 - INFO - training batch 1351, loss: 0.106, 43232/60000 datapoints
2025-03-06 19:43:22,895 - INFO - training batch 1401, loss: 0.559, 44832/60000 datapoints
2025-03-06 19:43:23,087 - INFO - training batch 1451, loss: 0.332, 46432/60000 datapoints
2025-03-06 19:43:23,284 - INFO - training batch 1501, loss: 0.149, 48032/60000 datapoints
2025-03-06 19:43:23,480 - INFO - training batch 1551, loss: 0.275, 49632/60000 datapoints
2025-03-06 19:43:23,678 - INFO - training batch 1601, loss: 0.247, 51232/60000 datapoints
2025-03-06 19:43:23,872 - INFO - training batch 1651, loss: 0.138, 52832/60000 datapoints
2025-03-06 19:43:24,070 - INFO - training batch 1701, loss: 0.573, 54432/60000 datapoints
2025-03-06 19:43:24,265 - INFO - training batch 1751, loss: 0.198, 56032/60000 datapoints
2025-03-06 19:43:24,458 - INFO - training batch 1801, loss: 0.302, 57632/60000 datapoints
2025-03-06 19:43:24,654 - INFO - training batch 1851, loss: 0.118, 59232/60000 datapoints
2025-03-06 19:43:24,754 - INFO - validation batch 1, loss: 0.348, 32/10016 datapoints
2025-03-06 19:43:24,912 - INFO - validation batch 51, loss: 0.260, 1632/10016 datapoints
2025-03-06 19:43:25,066 - INFO - validation batch 101, loss: 0.128, 3232/10016 datapoints
2025-03-06 19:43:25,219 - INFO - validation batch 151, loss: 0.329, 4832/10016 datapoints
2025-03-06 19:43:25,371 - INFO - validation batch 201, loss: 0.313, 6432/10016 datapoints
2025-03-06 19:43:25,527 - INFO - validation batch 251, loss: 0.243, 8032/10016 datapoints
2025-03-06 19:43:25,683 - INFO - validation batch 301, loss: 0.154, 9632/10016 datapoints
2025-03-06 19:43:25,719 - INFO - Epoch 446/800 done.
2025-03-06 19:43:25,719 - INFO - Final validation performance:
Loss: 0.253, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:43:25,720 - INFO - Beginning epoch 447/800
2025-03-06 19:43:25,727 - INFO - training batch 1, loss: 0.277, 32/60000 datapoints
2025-03-06 19:43:25,935 - INFO - training batch 51, loss: 0.221, 1632/60000 datapoints
2025-03-06 19:43:26,141 - INFO - training batch 101, loss: 0.093, 3232/60000 datapoints
2025-03-06 19:43:26,338 - INFO - training batch 151, loss: 0.318, 4832/60000 datapoints
2025-03-06 19:43:26,541 - INFO - training batch 201, loss: 0.203, 6432/60000 datapoints
2025-03-06 19:43:26,745 - INFO - training batch 251, loss: 0.239, 8032/60000 datapoints
2025-03-06 19:43:26,943 - INFO - training batch 301, loss: 0.252, 9632/60000 datapoints
2025-03-06 19:43:27,137 - INFO - training batch 351, loss: 0.705, 11232/60000 datapoints
2025-03-06 19:43:27,332 - INFO - training batch 401, loss: 0.273, 12832/60000 datapoints
2025-03-06 19:43:27,529 - INFO - training batch 451, loss: 0.287, 14432/60000 datapoints
2025-03-06 19:43:27,724 - INFO - training batch 501, loss: 0.200, 16032/60000 datapoints
2025-03-06 19:43:27,917 - INFO - training batch 551, loss: 0.137, 17632/60000 datapoints
2025-03-06 19:43:28,110 - INFO - training batch 601, loss: 0.106, 19232/60000 datapoints
2025-03-06 19:43:28,306 - INFO - training batch 651, loss: 0.230, 20832/60000 datapoints
2025-03-06 19:43:28,501 - INFO - training batch 701, loss: 0.300, 22432/60000 datapoints
2025-03-06 19:43:28,699 - INFO - training batch 751, loss: 0.495, 24032/60000 datapoints
2025-03-06 19:43:28,893 - INFO - training batch 801, loss: 0.445, 25632/60000 datapoints
2025-03-06 19:43:29,086 - INFO - training batch 851, loss: 0.334, 27232/60000 datapoints
2025-03-06 19:43:29,279 - INFO - training batch 901, loss: 0.097, 28832/60000 datapoints
2025-03-06 19:43:29,474 - INFO - training batch 951, loss: 0.223, 30432/60000 datapoints
2025-03-06 19:43:29,670 - INFO - training batch 1001, loss: 0.095, 32032/60000 datapoints
2025-03-06 19:43:29,865 - INFO - training batch 1051, loss: 0.166, 33632/60000 datapoints
2025-03-06 19:43:30,059 - INFO - training batch 1101, loss: 0.139, 35232/60000 datapoints
2025-03-06 19:43:30,253 - INFO - training batch 1151, loss: 0.141, 36832/60000 datapoints
2025-03-06 19:43:30,451 - INFO - training batch 1201, loss: 0.130, 38432/60000 datapoints
2025-03-06 19:43:30,644 - INFO - training batch 1251, loss: 0.398, 40032/60000 datapoints
2025-03-06 19:43:30,840 - INFO - training batch 1301, loss: 0.161, 41632/60000 datapoints
2025-03-06 19:43:31,034 - INFO - training batch 1351, loss: 0.112, 43232/60000 datapoints
2025-03-06 19:43:31,227 - INFO - training batch 1401, loss: 0.301, 44832/60000 datapoints
2025-03-06 19:43:31,422 - INFO - training batch 1451, loss: 0.493, 46432/60000 datapoints
2025-03-06 19:43:31,615 - INFO - training batch 1501, loss: 0.107, 48032/60000 datapoints
2025-03-06 19:43:31,819 - INFO - training batch 1551, loss: 0.333, 49632/60000 datapoints
2025-03-06 19:43:32,013 - INFO - training batch 1601, loss: 0.339, 51232/60000 datapoints
2025-03-06 19:43:32,207 - INFO - training batch 1651, loss: 0.142, 52832/60000 datapoints
2025-03-06 19:43:32,401 - INFO - training batch 1701, loss: 0.234, 54432/60000 datapoints
2025-03-06 19:43:32,594 - INFO - training batch 1751, loss: 0.254, 56032/60000 datapoints
2025-03-06 19:43:32,793 - INFO - training batch 1801, loss: 0.548, 57632/60000 datapoints
2025-03-06 19:43:33,005 - INFO - training batch 1851, loss: 0.249, 59232/60000 datapoints
2025-03-06 19:43:33,108 - INFO - validation batch 1, loss: 0.223, 32/10016 datapoints
2025-03-06 19:43:33,261 - INFO - validation batch 51, loss: 0.251, 1632/10016 datapoints
2025-03-06 19:43:33,415 - INFO - validation batch 101, loss: 0.190, 3232/10016 datapoints
2025-03-06 19:43:33,571 - INFO - validation batch 151, loss: 0.179, 4832/10016 datapoints
2025-03-06 19:43:33,727 - INFO - validation batch 201, loss: 0.264, 6432/10016 datapoints
2025-03-06 19:43:33,880 - INFO - validation batch 251, loss: 0.141, 8032/10016 datapoints
2025-03-06 19:43:34,033 - INFO - validation batch 301, loss: 0.189, 9632/10016 datapoints
2025-03-06 19:43:34,075 - INFO - Epoch 447/800 done.
2025-03-06 19:43:34,075 - INFO - Final validation performance:
Loss: 0.205, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:43:34,076 - INFO - Beginning epoch 448/800
2025-03-06 19:43:34,083 - INFO - training batch 1, loss: 0.088, 32/60000 datapoints
2025-03-06 19:43:34,286 - INFO - training batch 51, loss: 0.189, 1632/60000 datapoints
2025-03-06 19:43:34,488 - INFO - training batch 101, loss: 0.172, 3232/60000 datapoints
2025-03-06 19:43:34,699 - INFO - training batch 151, loss: 0.168, 4832/60000 datapoints
2025-03-06 19:43:34,915 - INFO - training batch 201, loss: 0.141, 6432/60000 datapoints
2025-03-06 19:43:35,119 - INFO - training batch 251, loss: 0.349, 8032/60000 datapoints
2025-03-06 19:43:35,332 - INFO - training batch 301, loss: 0.219, 9632/60000 datapoints
2025-03-06 19:43:35,529 - INFO - training batch 351, loss: 0.190, 11232/60000 datapoints
2025-03-06 19:43:35,730 - INFO - training batch 401, loss: 0.209, 12832/60000 datapoints
2025-03-06 19:43:35,927 - INFO - training batch 451, loss: 0.318, 14432/60000 datapoints
2025-03-06 19:43:36,120 - INFO - training batch 501, loss: 0.071, 16032/60000 datapoints
2025-03-06 19:43:36,314 - INFO - training batch 551, loss: 0.122, 17632/60000 datapoints
2025-03-06 19:43:36,508 - INFO - training batch 601, loss: 0.210, 19232/60000 datapoints
2025-03-06 19:43:36,712 - INFO - training batch 651, loss: 0.358, 20832/60000 datapoints
2025-03-06 19:43:36,912 - INFO - training batch 701, loss: 0.176, 22432/60000 datapoints
2025-03-06 19:43:37,106 - INFO - training batch 751, loss: 0.143, 24032/60000 datapoints
2025-03-06 19:43:37,300 - INFO - training batch 801, loss: 0.223, 25632/60000 datapoints
2025-03-06 19:43:37,496 - INFO - training batch 851, loss: 0.238, 27232/60000 datapoints
2025-03-06 19:43:37,705 - INFO - training batch 901, loss: 0.592, 28832/60000 datapoints
2025-03-06 19:43:37,901 - INFO - training batch 951, loss: 0.174, 30432/60000 datapoints
2025-03-06 19:43:38,099 - INFO - training batch 1001, loss: 0.193, 32032/60000 datapoints
2025-03-06 19:43:38,293 - INFO - training batch 1051, loss: 0.247, 33632/60000 datapoints
2025-03-06 19:43:38,488 - INFO - training batch 1101, loss: 0.296, 35232/60000 datapoints
2025-03-06 19:43:38,683 - INFO - training batch 1151, loss: 0.353, 36832/60000 datapoints
2025-03-06 19:43:38,879 - INFO - training batch 1201, loss: 0.322, 38432/60000 datapoints
2025-03-06 19:43:39,077 - INFO - training batch 1251, loss: 0.123, 40032/60000 datapoints
2025-03-06 19:43:39,273 - INFO - training batch 1301, loss: 0.530, 41632/60000 datapoints
2025-03-06 19:43:39,473 - INFO - training batch 1351, loss: 0.214, 43232/60000 datapoints
2025-03-06 19:43:39,669 - INFO - training batch 1401, loss: 0.672, 44832/60000 datapoints
2025-03-06 19:43:39,864 - INFO - training batch 1451, loss: 0.319, 46432/60000 datapoints
2025-03-06 19:43:40,058 - INFO - training batch 1501, loss: 0.078, 48032/60000 datapoints
2025-03-06 19:43:40,251 - INFO - training batch 1551, loss: 0.179, 49632/60000 datapoints
2025-03-06 19:43:40,447 - INFO - training batch 1601, loss: 0.439, 51232/60000 datapoints
2025-03-06 19:43:40,642 - INFO - training batch 1651, loss: 0.182, 52832/60000 datapoints
2025-03-06 19:43:40,835 - INFO - training batch 1701, loss: 0.130, 54432/60000 datapoints
2025-03-06 19:43:41,029 - INFO - training batch 1751, loss: 0.343, 56032/60000 datapoints
2025-03-06 19:43:41,222 - INFO - training batch 1801, loss: 0.455, 57632/60000 datapoints
2025-03-06 19:43:41,417 - INFO - training batch 1851, loss: 0.379, 59232/60000 datapoints
2025-03-06 19:43:41,521 - INFO - validation batch 1, loss: 0.108, 32/10016 datapoints
2025-03-06 19:43:41,677 - INFO - validation batch 51, loss: 0.160, 1632/10016 datapoints
2025-03-06 19:43:41,830 - INFO - validation batch 101, loss: 0.089, 3232/10016 datapoints
2025-03-06 19:43:41,983 - INFO - validation batch 151, loss: 0.130, 4832/10016 datapoints
2025-03-06 19:43:42,134 - INFO - validation batch 201, loss: 0.211, 6432/10016 datapoints
2025-03-06 19:43:42,285 - INFO - validation batch 251, loss: 0.576, 8032/10016 datapoints
2025-03-06 19:43:42,438 - INFO - validation batch 301, loss: 0.506, 9632/10016 datapoints
2025-03-06 19:43:42,476 - INFO - Epoch 448/800 done.
2025-03-06 19:43:42,477 - INFO - Final validation performance:
Loss: 0.254, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:43:42,477 - INFO - Beginning epoch 449/800
2025-03-06 19:43:42,483 - INFO - training batch 1, loss: 0.098, 32/60000 datapoints
2025-03-06 19:43:42,690 - INFO - training batch 51, loss: 0.407, 1632/60000 datapoints
2025-03-06 19:43:42,888 - INFO - training batch 101, loss: 0.172, 3232/60000 datapoints
2025-03-06 19:43:43,105 - INFO - training batch 151, loss: 0.475, 4832/60000 datapoints
2025-03-06 19:43:43,302 - INFO - training batch 201, loss: 0.113, 6432/60000 datapoints
2025-03-06 19:43:43,504 - INFO - training batch 251, loss: 0.201, 8032/60000 datapoints
2025-03-06 19:43:43,701 - INFO - training batch 301, loss: 0.299, 9632/60000 datapoints
2025-03-06 19:43:43,894 - INFO - training batch 351, loss: 0.189, 11232/60000 datapoints
2025-03-06 19:43:44,086 - INFO - training batch 401, loss: 0.233, 12832/60000 datapoints
2025-03-06 19:43:44,281 - INFO - training batch 451, loss: 0.220, 14432/60000 datapoints
2025-03-06 19:43:44,476 - INFO - training batch 501, loss: 0.297, 16032/60000 datapoints
2025-03-06 19:43:44,675 - INFO - training batch 551, loss: 0.168, 17632/60000 datapoints
2025-03-06 19:43:44,870 - INFO - training batch 601, loss: 0.228, 19232/60000 datapoints
2025-03-06 19:43:45,070 - INFO - training batch 651, loss: 0.251, 20832/60000 datapoints
2025-03-06 19:43:45,264 - INFO - training batch 701, loss: 0.173, 22432/60000 datapoints
2025-03-06 19:43:45,457 - INFO - training batch 751, loss: 0.411, 24032/60000 datapoints
2025-03-06 19:43:45,654 - INFO - training batch 801, loss: 0.199, 25632/60000 datapoints
2025-03-06 19:43:45,852 - INFO - training batch 851, loss: 0.188, 27232/60000 datapoints
2025-03-06 19:43:46,048 - INFO - training batch 901, loss: 0.094, 28832/60000 datapoints
2025-03-06 19:43:46,243 - INFO - training batch 951, loss: 0.156, 30432/60000 datapoints
2025-03-06 19:43:46,439 - INFO - training batch 1001, loss: 0.244, 32032/60000 datapoints
2025-03-06 19:43:46,637 - INFO - training batch 1051, loss: 0.201, 33632/60000 datapoints
2025-03-06 19:43:46,830 - INFO - training batch 1101, loss: 0.217, 35232/60000 datapoints
2025-03-06 19:43:47,025 - INFO - training batch 1151, loss: 0.135, 36832/60000 datapoints
2025-03-06 19:43:47,218 - INFO - training batch 1201, loss: 0.338, 38432/60000 datapoints
2025-03-06 19:43:47,412 - INFO - training batch 1251, loss: 0.184, 40032/60000 datapoints
2025-03-06 19:43:47,608 - INFO - training batch 1301, loss: 0.413, 41632/60000 datapoints
2025-03-06 19:43:47,806 - INFO - training batch 1351, loss: 0.295, 43232/60000 datapoints
2025-03-06 19:43:48,002 - INFO - training batch 1401, loss: 0.284, 44832/60000 datapoints
2025-03-06 19:43:48,203 - INFO - training batch 1451, loss: 0.177, 46432/60000 datapoints
2025-03-06 19:43:48,396 - INFO - training batch 1501, loss: 0.200, 48032/60000 datapoints
2025-03-06 19:43:48,591 - INFO - training batch 1551, loss: 0.136, 49632/60000 datapoints
2025-03-06 19:43:48,789 - INFO - training batch 1601, loss: 0.212, 51232/60000 datapoints
2025-03-06 19:43:48,983 - INFO - training batch 1651, loss: 0.304, 52832/60000 datapoints
2025-03-06 19:43:49,178 - INFO - training batch 1701, loss: 0.418, 54432/60000 datapoints
2025-03-06 19:43:49,372 - INFO - training batch 1751, loss: 0.133, 56032/60000 datapoints
2025-03-06 19:43:49,569 - INFO - training batch 1801, loss: 0.561, 57632/60000 datapoints
2025-03-06 19:43:49,766 - INFO - training batch 1851, loss: 0.179, 59232/60000 datapoints
2025-03-06 19:43:49,869 - INFO - validation batch 1, loss: 0.311, 32/10016 datapoints
2025-03-06 19:43:50,023 - INFO - validation batch 51, loss: 0.103, 1632/10016 datapoints
2025-03-06 19:43:50,176 - INFO - validation batch 101, loss: 0.386, 3232/10016 datapoints
2025-03-06 19:43:50,330 - INFO - validation batch 151, loss: 0.569, 4832/10016 datapoints
2025-03-06 19:43:50,484 - INFO - validation batch 201, loss: 0.248, 6432/10016 datapoints
2025-03-06 19:43:50,639 - INFO - validation batch 251, loss: 0.200, 8032/10016 datapoints
2025-03-06 19:43:50,793 - INFO - validation batch 301, loss: 0.333, 9632/10016 datapoints
2025-03-06 19:43:50,832 - INFO - Epoch 449/800 done.
2025-03-06 19:43:50,832 - INFO - Final validation performance:
Loss: 0.307, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:43:50,832 - INFO - Beginning epoch 450/800
2025-03-06 19:43:50,839 - INFO - training batch 1, loss: 0.276, 32/60000 datapoints
2025-03-06 19:43:51,050 - INFO - training batch 51, loss: 0.470, 1632/60000 datapoints
2025-03-06 19:43:51,246 - INFO - training batch 101, loss: 0.056, 3232/60000 datapoints
2025-03-06 19:43:51,446 - INFO - training batch 151, loss: 0.255, 4832/60000 datapoints
2025-03-06 19:43:51,652 - INFO - training batch 201, loss: 0.047, 6432/60000 datapoints
2025-03-06 19:43:51,866 - INFO - training batch 251, loss: 0.250, 8032/60000 datapoints
2025-03-06 19:43:52,061 - INFO - training batch 301, loss: 0.155, 9632/60000 datapoints
2025-03-06 19:43:52,254 - INFO - training batch 351, loss: 0.537, 11232/60000 datapoints
2025-03-06 19:43:52,447 - INFO - training batch 401, loss: 0.134, 12832/60000 datapoints
2025-03-06 19:43:52,642 - INFO - training batch 451, loss: 0.169, 14432/60000 datapoints
2025-03-06 19:43:52,835 - INFO - training batch 501, loss: 0.296, 16032/60000 datapoints
2025-03-06 19:43:53,046 - INFO - training batch 551, loss: 0.146, 17632/60000 datapoints
2025-03-06 19:43:53,241 - INFO - training batch 601, loss: 0.241, 19232/60000 datapoints
2025-03-06 19:43:53,438 - INFO - training batch 651, loss: 0.137, 20832/60000 datapoints
2025-03-06 19:43:53,639 - INFO - training batch 701, loss: 0.144, 22432/60000 datapoints
2025-03-06 19:43:53,832 - INFO - training batch 751, loss: 0.243, 24032/60000 datapoints
2025-03-06 19:43:54,027 - INFO - training batch 801, loss: 0.465, 25632/60000 datapoints
2025-03-06 19:43:54,220 - INFO - training batch 851, loss: 0.116, 27232/60000 datapoints
2025-03-06 19:43:54,413 - INFO - training batch 901, loss: 0.341, 28832/60000 datapoints
2025-03-06 19:43:54,612 - INFO - training batch 951, loss: 0.070, 30432/60000 datapoints
2025-03-06 19:43:54,805 - INFO - training batch 1001, loss: 1.277, 32032/60000 datapoints
2025-03-06 19:43:55,001 - INFO - training batch 1051, loss: 0.238, 33632/60000 datapoints
2025-03-06 19:43:55,197 - INFO - training batch 1101, loss: 0.230, 35232/60000 datapoints
2025-03-06 19:43:55,390 - INFO - training batch 1151, loss: 0.111, 36832/60000 datapoints
2025-03-06 19:43:55,586 - INFO - training batch 1201, loss: 0.119, 38432/60000 datapoints
2025-03-06 19:43:55,780 - INFO - training batch 1251, loss: 0.386, 40032/60000 datapoints
2025-03-06 19:43:55,973 - INFO - training batch 1301, loss: 0.089, 41632/60000 datapoints
2025-03-06 19:43:56,170 - INFO - training batch 1351, loss: 0.165, 43232/60000 datapoints
2025-03-06 19:43:56,364 - INFO - training batch 1401, loss: 0.203, 44832/60000 datapoints
2025-03-06 19:43:56,558 - INFO - training batch 1451, loss: 0.113, 46432/60000 datapoints
2025-03-06 19:43:56,765 - INFO - training batch 1501, loss: 0.180, 48032/60000 datapoints
2025-03-06 19:43:56,959 - INFO - training batch 1551, loss: 0.197, 49632/60000 datapoints
2025-03-06 19:43:57,152 - INFO - training batch 1601, loss: 0.108, 51232/60000 datapoints
2025-03-06 19:43:57,346 - INFO - training batch 1651, loss: 0.294, 52832/60000 datapoints
2025-03-06 19:43:57,541 - INFO - training batch 1701, loss: 0.232, 54432/60000 datapoints
2025-03-06 19:43:57,737 - INFO - training batch 1751, loss: 0.121, 56032/60000 datapoints
2025-03-06 19:43:57,932 - INFO - training batch 1801, loss: 0.221, 57632/60000 datapoints
2025-03-06 19:43:58,126 - INFO - training batch 1851, loss: 0.150, 59232/60000 datapoints
2025-03-06 19:43:58,227 - INFO - validation batch 1, loss: 0.260, 32/10016 datapoints
2025-03-06 19:43:58,377 - INFO - validation batch 51, loss: 0.276, 1632/10016 datapoints
2025-03-06 19:43:58,530 - INFO - validation batch 101, loss: 0.338, 3232/10016 datapoints
2025-03-06 19:43:58,687 - INFO - validation batch 151, loss: 0.269, 4832/10016 datapoints
2025-03-06 19:43:58,839 - INFO - validation batch 201, loss: 0.313, 6432/10016 datapoints
2025-03-06 19:43:58,991 - INFO - validation batch 251, loss: 0.420, 8032/10016 datapoints
2025-03-06 19:43:59,143 - INFO - validation batch 301, loss: 0.046, 9632/10016 datapoints
2025-03-06 19:43:59,180 - INFO - Epoch 450/800 done.
2025-03-06 19:43:59,180 - INFO - Final validation performance:
Loss: 0.275, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:43:59,181 - INFO - Beginning epoch 451/800
2025-03-06 19:43:59,188 - INFO - training batch 1, loss: 0.129, 32/60000 datapoints
2025-03-06 19:43:59,382 - INFO - training batch 51, loss: 0.210, 1632/60000 datapoints
2025-03-06 19:43:59,580 - INFO - training batch 101, loss: 0.340, 3232/60000 datapoints
2025-03-06 19:43:59,788 - INFO - training batch 151, loss: 0.137, 4832/60000 datapoints
2025-03-06 19:43:59,980 - INFO - training batch 201, loss: 0.262, 6432/60000 datapoints
2025-03-06 19:44:00,184 - INFO - training batch 251, loss: 0.195, 8032/60000 datapoints
2025-03-06 19:44:00,380 - INFO - training batch 301, loss: 0.466, 9632/60000 datapoints
2025-03-06 19:44:00,576 - INFO - training batch 351, loss: 0.118, 11232/60000 datapoints
2025-03-06 19:44:00,773 - INFO - training batch 401, loss: 0.095, 12832/60000 datapoints
2025-03-06 19:44:00,970 - INFO - training batch 451, loss: 0.352, 14432/60000 datapoints
2025-03-06 19:44:01,169 - INFO - training batch 501, loss: 0.223, 16032/60000 datapoints
2025-03-06 19:44:01,365 - INFO - training batch 551, loss: 0.206, 17632/60000 datapoints
2025-03-06 19:44:01,562 - INFO - training batch 601, loss: 0.093, 19232/60000 datapoints
2025-03-06 19:44:01,761 - INFO - training batch 651, loss: 0.315, 20832/60000 datapoints
2025-03-06 19:44:01,957 - INFO - training batch 701, loss: 0.300, 22432/60000 datapoints
2025-03-06 19:44:02,150 - INFO - training batch 751, loss: 0.375, 24032/60000 datapoints
2025-03-06 19:44:02,346 - INFO - training batch 801, loss: 0.226, 25632/60000 datapoints
2025-03-06 19:44:02,539 - INFO - training batch 851, loss: 0.319, 27232/60000 datapoints
2025-03-06 19:44:02,734 - INFO - training batch 901, loss: 0.193, 28832/60000 datapoints
2025-03-06 19:44:02,930 - INFO - training batch 951, loss: 0.176, 30432/60000 datapoints
2025-03-06 19:44:03,140 - INFO - training batch 1001, loss: 0.099, 32032/60000 datapoints
2025-03-06 19:44:03,335 - INFO - training batch 1051, loss: 0.258, 33632/60000 datapoints
2025-03-06 19:44:03,529 - INFO - training batch 1101, loss: 0.176, 35232/60000 datapoints
2025-03-06 19:44:03,726 - INFO - training batch 1151, loss: 0.186, 36832/60000 datapoints
2025-03-06 19:44:03,918 - INFO - training batch 1201, loss: 0.254, 38432/60000 datapoints
2025-03-06 19:44:04,110 - INFO - training batch 1251, loss: 0.158, 40032/60000 datapoints
2025-03-06 19:44:04,305 - INFO - training batch 1301, loss: 0.086, 41632/60000 datapoints
2025-03-06 19:44:04,496 - INFO - training batch 1351, loss: 0.140, 43232/60000 datapoints
2025-03-06 19:44:04,694 - INFO - training batch 1401, loss: 0.302, 44832/60000 datapoints
2025-03-06 19:44:04,892 - INFO - training batch 1451, loss: 0.198, 46432/60000 datapoints
2025-03-06 19:44:05,084 - INFO - training batch 1501, loss: 0.243, 48032/60000 datapoints
2025-03-06 19:44:05,278 - INFO - training batch 1551, loss: 0.048, 49632/60000 datapoints
2025-03-06 19:44:05,470 - INFO - training batch 1601, loss: 0.332, 51232/60000 datapoints
2025-03-06 19:44:05,669 - INFO - training batch 1651, loss: 0.286, 52832/60000 datapoints
2025-03-06 19:44:05,863 - INFO - training batch 1701, loss: 0.272, 54432/60000 datapoints
2025-03-06 19:44:06,055 - INFO - training batch 1751, loss: 0.242, 56032/60000 datapoints
2025-03-06 19:44:06,250 - INFO - training batch 1801, loss: 0.569, 57632/60000 datapoints
2025-03-06 19:44:06,445 - INFO - training batch 1851, loss: 0.125, 59232/60000 datapoints
2025-03-06 19:44:06,547 - INFO - validation batch 1, loss: 0.272, 32/10016 datapoints
2025-03-06 19:44:06,709 - INFO - validation batch 51, loss: 0.204, 1632/10016 datapoints
2025-03-06 19:44:06,861 - INFO - validation batch 101, loss: 0.241, 3232/10016 datapoints
2025-03-06 19:44:07,013 - INFO - validation batch 151, loss: 0.735, 4832/10016 datapoints
2025-03-06 19:44:07,166 - INFO - validation batch 201, loss: 0.104, 6432/10016 datapoints
2025-03-06 19:44:07,323 - INFO - validation batch 251, loss: 0.290, 8032/10016 datapoints
2025-03-06 19:44:07,473 - INFO - validation batch 301, loss: 0.248, 9632/10016 datapoints
2025-03-06 19:44:07,509 - INFO - Epoch 451/800 done.
2025-03-06 19:44:07,509 - INFO - Final validation performance:
Loss: 0.299, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:44:07,510 - INFO - Beginning epoch 452/800
2025-03-06 19:44:07,518 - INFO - training batch 1, loss: 0.143, 32/60000 datapoints
2025-03-06 19:44:07,744 - INFO - training batch 51, loss: 0.279, 1632/60000 datapoints
2025-03-06 19:44:07,940 - INFO - training batch 101, loss: 0.098, 3232/60000 datapoints
2025-03-06 19:44:08,140 - INFO - training batch 151, loss: 0.290, 4832/60000 datapoints
2025-03-06 19:44:08,341 - INFO - training batch 201, loss: 0.256, 6432/60000 datapoints
2025-03-06 19:44:08,536 - INFO - training batch 251, loss: 0.260, 8032/60000 datapoints
2025-03-06 19:44:08,736 - INFO - training batch 301, loss: 0.102, 9632/60000 datapoints
2025-03-06 19:44:08,930 - INFO - training batch 351, loss: 0.140, 11232/60000 datapoints
2025-03-06 19:44:09,127 - INFO - training batch 401, loss: 0.255, 12832/60000 datapoints
2025-03-06 19:44:09,322 - INFO - training batch 451, loss: 0.298, 14432/60000 datapoints
2025-03-06 19:44:09,514 - INFO - training batch 501, loss: 0.449, 16032/60000 datapoints
2025-03-06 19:44:09,712 - INFO - training batch 551, loss: 0.275, 17632/60000 datapoints
2025-03-06 19:44:09,906 - INFO - training batch 601, loss: 0.282, 19232/60000 datapoints
2025-03-06 19:44:10,102 - INFO - training batch 651, loss: 0.436, 20832/60000 datapoints
2025-03-06 19:44:10,298 - INFO - training batch 701, loss: 0.558, 22432/60000 datapoints
2025-03-06 19:44:10,491 - INFO - training batch 751, loss: 0.167, 24032/60000 datapoints
2025-03-06 19:44:10,687 - INFO - training batch 801, loss: 0.219, 25632/60000 datapoints
2025-03-06 19:44:10,882 - INFO - training batch 851, loss: 0.204, 27232/60000 datapoints
2025-03-06 19:44:11,077 - INFO - training batch 901, loss: 0.290, 28832/60000 datapoints
2025-03-06 19:44:11,273 - INFO - training batch 951, loss: 0.344, 30432/60000 datapoints
2025-03-06 19:44:11,467 - INFO - training batch 1001, loss: 0.070, 32032/60000 datapoints
2025-03-06 19:44:11,665 - INFO - training batch 1051, loss: 0.044, 33632/60000 datapoints
2025-03-06 19:44:11,862 - INFO - training batch 1101, loss: 0.234, 35232/60000 datapoints
2025-03-06 19:44:12,055 - INFO - training batch 1151, loss: 0.577, 36832/60000 datapoints
2025-03-06 19:44:12,251 - INFO - training batch 1201, loss: 0.335, 38432/60000 datapoints
2025-03-06 19:44:12,444 - INFO - training batch 1251, loss: 0.202, 40032/60000 datapoints
2025-03-06 19:44:12,639 - INFO - training batch 1301, loss: 0.106, 41632/60000 datapoints
2025-03-06 19:44:12,834 - INFO - training batch 1351, loss: 0.124, 43232/60000 datapoints
2025-03-06 19:44:13,027 - INFO - training batch 1401, loss: 0.633, 44832/60000 datapoints
2025-03-06 19:44:13,240 - INFO - training batch 1451, loss: 0.131, 46432/60000 datapoints
2025-03-06 19:44:13,436 - INFO - training batch 1501, loss: 0.221, 48032/60000 datapoints
2025-03-06 19:44:13,635 - INFO - training batch 1551, loss: 0.673, 49632/60000 datapoints
2025-03-06 19:44:13,829 - INFO - training batch 1601, loss: 0.303, 51232/60000 datapoints
2025-03-06 19:44:14,024 - INFO - training batch 1651, loss: 0.573, 52832/60000 datapoints
2025-03-06 19:44:14,216 - INFO - training batch 1701, loss: 0.557, 54432/60000 datapoints
2025-03-06 19:44:14,412 - INFO - training batch 1751, loss: 0.379, 56032/60000 datapoints
2025-03-06 19:44:14,606 - INFO - training batch 1801, loss: 0.062, 57632/60000 datapoints
2025-03-06 19:44:14,804 - INFO - training batch 1851, loss: 0.177, 59232/60000 datapoints
2025-03-06 19:44:14,908 - INFO - validation batch 1, loss: 0.495, 32/10016 datapoints
2025-03-06 19:44:15,060 - INFO - validation batch 51, loss: 0.236, 1632/10016 datapoints
2025-03-06 19:44:15,212 - INFO - validation batch 101, loss: 0.489, 3232/10016 datapoints
2025-03-06 19:44:15,362 - INFO - validation batch 151, loss: 0.270, 4832/10016 datapoints
2025-03-06 19:44:15,516 - INFO - validation batch 201, loss: 0.211, 6432/10016 datapoints
2025-03-06 19:44:15,673 - INFO - validation batch 251, loss: 0.176, 8032/10016 datapoints
2025-03-06 19:44:15,825 - INFO - validation batch 301, loss: 0.453, 9632/10016 datapoints
2025-03-06 19:44:15,861 - INFO - Epoch 452/800 done.
2025-03-06 19:44:15,862 - INFO - Final validation performance:
Loss: 0.333, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:44:15,863 - INFO - Beginning epoch 453/800
2025-03-06 19:44:15,869 - INFO - training batch 1, loss: 0.438, 32/60000 datapoints
2025-03-06 19:44:16,087 - INFO - training batch 51, loss: 0.235, 1632/60000 datapoints
2025-03-06 19:44:16,290 - INFO - training batch 101, loss: 0.319, 3232/60000 datapoints
2025-03-06 19:44:16,492 - INFO - training batch 151, loss: 0.112, 4832/60000 datapoints
2025-03-06 19:44:16,692 - INFO - training batch 201, loss: 0.598, 6432/60000 datapoints
2025-03-06 19:44:16,889 - INFO - training batch 251, loss: 0.550, 8032/60000 datapoints
2025-03-06 19:44:17,083 - INFO - training batch 301, loss: 0.202, 9632/60000 datapoints
2025-03-06 19:44:17,278 - INFO - training batch 351, loss: 0.134, 11232/60000 datapoints
2025-03-06 19:44:17,472 - INFO - training batch 401, loss: 0.250, 12832/60000 datapoints
2025-03-06 19:44:17,672 - INFO - training batch 451, loss: 0.077, 14432/60000 datapoints
2025-03-06 19:44:17,867 - INFO - training batch 501, loss: 0.219, 16032/60000 datapoints
2025-03-06 19:44:18,063 - INFO - training batch 551, loss: 0.109, 17632/60000 datapoints
2025-03-06 19:44:18,257 - INFO - training batch 601, loss: 0.552, 19232/60000 datapoints
2025-03-06 19:44:18,449 - INFO - training batch 651, loss: 0.291, 20832/60000 datapoints
2025-03-06 19:44:18,645 - INFO - training batch 701, loss: 0.159, 22432/60000 datapoints
2025-03-06 19:44:18,839 - INFO - training batch 751, loss: 0.207, 24032/60000 datapoints
2025-03-06 19:44:19,030 - INFO - training batch 801, loss: 0.210, 25632/60000 datapoints
2025-03-06 19:44:19,225 - INFO - training batch 851, loss: 0.289, 27232/60000 datapoints
2025-03-06 19:44:19,430 - INFO - training batch 901, loss: 0.339, 28832/60000 datapoints
2025-03-06 19:44:19,629 - INFO - training batch 951, loss: 0.144, 30432/60000 datapoints
2025-03-06 19:44:19,824 - INFO - training batch 1001, loss: 0.491, 32032/60000 datapoints
2025-03-06 19:44:20,018 - INFO - training batch 1051, loss: 0.108, 33632/60000 datapoints
2025-03-06 19:44:20,212 - INFO - training batch 1101, loss: 0.126, 35232/60000 datapoints
2025-03-06 19:44:20,406 - INFO - training batch 1151, loss: 0.315, 36832/60000 datapoints
2025-03-06 19:44:20,601 - INFO - training batch 1201, loss: 0.474, 38432/60000 datapoints
2025-03-06 19:44:20,796 - INFO - training batch 1251, loss: 0.264, 40032/60000 datapoints
2025-03-06 19:44:20,990 - INFO - training batch 1301, loss: 0.175, 41632/60000 datapoints
2025-03-06 19:44:21,186 - INFO - training batch 1351, loss: 0.467, 43232/60000 datapoints
2025-03-06 19:44:21,379 - INFO - training batch 1401, loss: 0.461, 44832/60000 datapoints
2025-03-06 19:44:21,575 - INFO - training batch 1451, loss: 0.113, 46432/60000 datapoints
2025-03-06 19:44:21,774 - INFO - training batch 1501, loss: 0.276, 48032/60000 datapoints
2025-03-06 19:44:21,973 - INFO - training batch 1551, loss: 0.142, 49632/60000 datapoints
2025-03-06 19:44:22,167 - INFO - training batch 1601, loss: 0.147, 51232/60000 datapoints
2025-03-06 19:44:22,360 - INFO - training batch 1651, loss: 0.377, 52832/60000 datapoints
2025-03-06 19:44:22,552 - INFO - training batch 1701, loss: 0.185, 54432/60000 datapoints
2025-03-06 19:44:22,748 - INFO - training batch 1751, loss: 0.528, 56032/60000 datapoints
2025-03-06 19:44:22,943 - INFO - training batch 1801, loss: 0.180, 57632/60000 datapoints
2025-03-06 19:44:23,138 - INFO - training batch 1851, loss: 0.494, 59232/60000 datapoints
2025-03-06 19:44:23,245 - INFO - validation batch 1, loss: 0.164, 32/10016 datapoints
2025-03-06 19:44:23,414 - INFO - validation batch 51, loss: 0.216, 1632/10016 datapoints
2025-03-06 19:44:23,566 - INFO - validation batch 101, loss: 0.123, 3232/10016 datapoints
2025-03-06 19:44:23,724 - INFO - validation batch 151, loss: 0.191, 4832/10016 datapoints
2025-03-06 19:44:23,880 - INFO - validation batch 201, loss: 0.364, 6432/10016 datapoints
2025-03-06 19:44:24,033 - INFO - validation batch 251, loss: 0.075, 8032/10016 datapoints
2025-03-06 19:44:24,187 - INFO - validation batch 301, loss: 0.214, 9632/10016 datapoints
2025-03-06 19:44:24,225 - INFO - Epoch 453/800 done.
2025-03-06 19:44:24,225 - INFO - Final validation performance:
Loss: 0.192, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:44:24,226 - INFO - Beginning epoch 454/800
2025-03-06 19:44:24,232 - INFO - training batch 1, loss: 0.230, 32/60000 datapoints
2025-03-06 19:44:24,442 - INFO - training batch 51, loss: 0.050, 1632/60000 datapoints
2025-03-06 19:44:24,638 - INFO - training batch 101, loss: 0.118, 3232/60000 datapoints
2025-03-06 19:44:24,837 - INFO - training batch 151, loss: 0.469, 4832/60000 datapoints
2025-03-06 19:44:25,040 - INFO - training batch 201, loss: 0.365, 6432/60000 datapoints
2025-03-06 19:44:25,237 - INFO - training batch 251, loss: 0.708, 8032/60000 datapoints
2025-03-06 19:44:25,432 - INFO - training batch 301, loss: 0.293, 9632/60000 datapoints
2025-03-06 19:44:25,632 - INFO - training batch 351, loss: 0.203, 11232/60000 datapoints
2025-03-06 19:44:25,827 - INFO - training batch 401, loss: 0.096, 12832/60000 datapoints
2025-03-06 19:44:26,022 - INFO - training batch 451, loss: 0.343, 14432/60000 datapoints
2025-03-06 19:44:26,216 - INFO - training batch 501, loss: 0.268, 16032/60000 datapoints
2025-03-06 19:44:26,409 - INFO - training batch 551, loss: 0.257, 17632/60000 datapoints
2025-03-06 19:44:26,606 - INFO - training batch 601, loss: 0.193, 19232/60000 datapoints
2025-03-06 19:44:26,806 - INFO - training batch 651, loss: 0.372, 20832/60000 datapoints
2025-03-06 19:44:27,001 - INFO - training batch 701, loss: 0.298, 22432/60000 datapoints
2025-03-06 19:44:27,192 - INFO - training batch 751, loss: 0.237, 24032/60000 datapoints
2025-03-06 19:44:27,389 - INFO - training batch 801, loss: 0.442, 25632/60000 datapoints
2025-03-06 19:44:27,582 - INFO - training batch 851, loss: 0.268, 27232/60000 datapoints
2025-03-06 19:44:27,780 - INFO - training batch 901, loss: 0.329, 28832/60000 datapoints
2025-03-06 19:44:27,974 - INFO - training batch 951, loss: 0.135, 30432/60000 datapoints
2025-03-06 19:44:28,166 - INFO - training batch 1001, loss: 0.179, 32032/60000 datapoints
2025-03-06 19:44:28,358 - INFO - training batch 1051, loss: 0.217, 33632/60000 datapoints
2025-03-06 19:44:28,552 - INFO - training batch 1101, loss: 0.356, 35232/60000 datapoints
2025-03-06 19:44:28,750 - INFO - training batch 1151, loss: 0.322, 36832/60000 datapoints
2025-03-06 19:44:28,951 - INFO - training batch 1201, loss: 0.399, 38432/60000 datapoints
2025-03-06 19:44:29,145 - INFO - training batch 1251, loss: 0.292, 40032/60000 datapoints
2025-03-06 19:44:29,338 - INFO - training batch 1301, loss: 0.708, 41632/60000 datapoints
2025-03-06 19:44:29,532 - INFO - training batch 1351, loss: 0.147, 43232/60000 datapoints
2025-03-06 19:44:29,731 - INFO - training batch 1401, loss: 0.304, 44832/60000 datapoints
2025-03-06 19:44:29,926 - INFO - training batch 1451, loss: 0.383, 46432/60000 datapoints
2025-03-06 19:44:30,121 - INFO - training batch 1501, loss: 0.271, 48032/60000 datapoints
2025-03-06 19:44:30,314 - INFO - training batch 1551, loss: 0.236, 49632/60000 datapoints
2025-03-06 19:44:30,510 - INFO - training batch 1601, loss: 0.269, 51232/60000 datapoints
2025-03-06 19:44:30,704 - INFO - training batch 1651, loss: 0.047, 52832/60000 datapoints
2025-03-06 19:44:30,896 - INFO - training batch 1701, loss: 0.079, 54432/60000 datapoints
2025-03-06 19:44:31,090 - INFO - training batch 1751, loss: 0.226, 56032/60000 datapoints
2025-03-06 19:44:31,283 - INFO - training batch 1801, loss: 0.110, 57632/60000 datapoints
2025-03-06 19:44:31,481 - INFO - training batch 1851, loss: 0.264, 59232/60000 datapoints
2025-03-06 19:44:31,582 - INFO - validation batch 1, loss: 0.298, 32/10016 datapoints
2025-03-06 19:44:31,739 - INFO - validation batch 51, loss: 0.230, 1632/10016 datapoints
2025-03-06 19:44:31,894 - INFO - validation batch 101, loss: 0.269, 3232/10016 datapoints
2025-03-06 19:44:32,046 - INFO - validation batch 151, loss: 0.319, 4832/10016 datapoints
2025-03-06 19:44:32,199 - INFO - validation batch 201, loss: 0.407, 6432/10016 datapoints
2025-03-06 19:44:32,351 - INFO - validation batch 251, loss: 0.646, 8032/10016 datapoints
2025-03-06 19:44:32,505 - INFO - validation batch 301, loss: 0.471, 9632/10016 datapoints
2025-03-06 19:44:32,543 - INFO - Epoch 454/800 done.
2025-03-06 19:44:32,543 - INFO - Final validation performance:
Loss: 0.377, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:44:32,544 - INFO - Beginning epoch 455/800
2025-03-06 19:44:32,550 - INFO - training batch 1, loss: 0.179, 32/60000 datapoints
2025-03-06 19:44:32,746 - INFO - training batch 51, loss: 0.201, 1632/60000 datapoints
2025-03-06 19:44:32,940 - INFO - training batch 101, loss: 0.173, 3232/60000 datapoints
2025-03-06 19:44:33,141 - INFO - training batch 151, loss: 0.245, 4832/60000 datapoints
2025-03-06 19:44:33,344 - INFO - training batch 201, loss: 0.337, 6432/60000 datapoints
2025-03-06 19:44:33,552 - INFO - training batch 251, loss: 0.171, 8032/60000 datapoints
2025-03-06 19:44:33,757 - INFO - training batch 301, loss: 0.072, 9632/60000 datapoints
2025-03-06 19:44:33,954 - INFO - training batch 351, loss: 0.197, 11232/60000 datapoints
2025-03-06 19:44:34,151 - INFO - training batch 401, loss: 0.320, 12832/60000 datapoints
2025-03-06 19:44:34,347 - INFO - training batch 451, loss: 0.219, 14432/60000 datapoints
2025-03-06 19:44:34,549 - INFO - training batch 501, loss: 0.239, 16032/60000 datapoints
2025-03-06 19:44:34,744 - INFO - training batch 551, loss: 0.213, 17632/60000 datapoints
2025-03-06 19:44:34,944 - INFO - training batch 601, loss: 0.186, 19232/60000 datapoints
2025-03-06 19:44:35,140 - INFO - training batch 651, loss: 0.361, 20832/60000 datapoints
2025-03-06 19:44:35,335 - INFO - training batch 701, loss: 0.269, 22432/60000 datapoints
2025-03-06 19:44:35,553 - INFO - training batch 751, loss: 0.163, 24032/60000 datapoints
2025-03-06 19:44:35,756 - INFO - training batch 801, loss: 0.173, 25632/60000 datapoints
2025-03-06 19:44:35,949 - INFO - training batch 851, loss: 0.349, 27232/60000 datapoints
2025-03-06 19:44:36,145 - INFO - training batch 901, loss: 0.394, 28832/60000 datapoints
2025-03-06 19:44:36,338 - INFO - training batch 951, loss: 0.131, 30432/60000 datapoints
2025-03-06 19:44:36,533 - INFO - training batch 1001, loss: 0.629, 32032/60000 datapoints
2025-03-06 19:44:36,734 - INFO - training batch 1051, loss: 0.461, 33632/60000 datapoints
2025-03-06 19:44:36,933 - INFO - training batch 1101, loss: 0.219, 35232/60000 datapoints
2025-03-06 19:44:37,125 - INFO - training batch 1151, loss: 0.202, 36832/60000 datapoints
2025-03-06 19:44:37,315 - INFO - training batch 1201, loss: 0.226, 38432/60000 datapoints
2025-03-06 19:44:37,510 - INFO - training batch 1251, loss: 0.302, 40032/60000 datapoints
2025-03-06 19:44:37,725 - INFO - training batch 1301, loss: 0.138, 41632/60000 datapoints
2025-03-06 19:44:37,918 - INFO - training batch 1351, loss: 0.262, 43232/60000 datapoints
2025-03-06 19:44:38,112 - INFO - training batch 1401, loss: 0.212, 44832/60000 datapoints
2025-03-06 19:44:38,307 - INFO - training batch 1451, loss: 0.191, 46432/60000 datapoints
2025-03-06 19:44:38,500 - INFO - training batch 1501, loss: 0.621, 48032/60000 datapoints
2025-03-06 19:44:38,700 - INFO - training batch 1551, loss: 0.205, 49632/60000 datapoints
2025-03-06 19:44:38,896 - INFO - training batch 1601, loss: 0.142, 51232/60000 datapoints
2025-03-06 19:44:39,095 - INFO - training batch 1651, loss: 0.402, 52832/60000 datapoints
2025-03-06 19:44:39,288 - INFO - training batch 1701, loss: 0.254, 54432/60000 datapoints
2025-03-06 19:44:39,482 - INFO - training batch 1751, loss: 0.300, 56032/60000 datapoints
2025-03-06 19:44:39,682 - INFO - training batch 1801, loss: 0.240, 57632/60000 datapoints
2025-03-06 19:44:39,879 - INFO - training batch 1851, loss: 0.370, 59232/60000 datapoints
2025-03-06 19:44:39,980 - INFO - validation batch 1, loss: 0.202, 32/10016 datapoints
2025-03-06 19:44:40,134 - INFO - validation batch 51, loss: 0.389, 1632/10016 datapoints
2025-03-06 19:44:40,286 - INFO - validation batch 101, loss: 0.230, 3232/10016 datapoints
2025-03-06 19:44:40,439 - INFO - validation batch 151, loss: 0.221, 4832/10016 datapoints
2025-03-06 19:44:40,593 - INFO - validation batch 201, loss: 0.167, 6432/10016 datapoints
2025-03-06 19:44:40,748 - INFO - validation batch 251, loss: 0.314, 8032/10016 datapoints
2025-03-06 19:44:40,900 - INFO - validation batch 301, loss: 0.233, 9632/10016 datapoints
2025-03-06 19:44:40,938 - INFO - Epoch 455/800 done.
2025-03-06 19:44:40,938 - INFO - Final validation performance:
Loss: 0.251, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:44:40,939 - INFO - Beginning epoch 456/800
2025-03-06 19:44:40,945 - INFO - training batch 1, loss: 0.158, 32/60000 datapoints
2025-03-06 19:44:41,144 - INFO - training batch 51, loss: 0.377, 1632/60000 datapoints
2025-03-06 19:44:41,336 - INFO - training batch 101, loss: 0.140, 3232/60000 datapoints
2025-03-06 19:44:41,544 - INFO - training batch 151, loss: 0.347, 4832/60000 datapoints
2025-03-06 19:44:41,746 - INFO - training batch 201, loss: 0.373, 6432/60000 datapoints
2025-03-06 19:44:41,940 - INFO - training batch 251, loss: 0.282, 8032/60000 datapoints
2025-03-06 19:44:42,144 - INFO - training batch 301, loss: 0.215, 9632/60000 datapoints
2025-03-06 19:44:42,343 - INFO - training batch 351, loss: 0.295, 11232/60000 datapoints
2025-03-06 19:44:42,540 - INFO - training batch 401, loss: 0.291, 12832/60000 datapoints
2025-03-06 19:44:42,736 - INFO - training batch 451, loss: 0.160, 14432/60000 datapoints
2025-03-06 19:44:42,929 - INFO - training batch 501, loss: 0.139, 16032/60000 datapoints
2025-03-06 19:44:43,123 - INFO - training batch 551, loss: 0.492, 17632/60000 datapoints
2025-03-06 19:44:43,319 - INFO - training batch 601, loss: 0.208, 19232/60000 datapoints
2025-03-06 19:44:43,531 - INFO - training batch 651, loss: 0.421, 20832/60000 datapoints
2025-03-06 19:44:43,731 - INFO - training batch 701, loss: 0.190, 22432/60000 datapoints
2025-03-06 19:44:43,926 - INFO - training batch 751, loss: 0.718, 24032/60000 datapoints
2025-03-06 19:44:44,120 - INFO - training batch 801, loss: 0.490, 25632/60000 datapoints
2025-03-06 19:44:44,315 - INFO - training batch 851, loss: 0.237, 27232/60000 datapoints
2025-03-06 19:44:44,508 - INFO - training batch 901, loss: 0.250, 28832/60000 datapoints
2025-03-06 19:44:44,704 - INFO - training batch 951, loss: 0.152, 30432/60000 datapoints
2025-03-06 19:44:44,904 - INFO - training batch 1001, loss: 0.338, 32032/60000 datapoints
2025-03-06 19:44:45,098 - INFO - training batch 1051, loss: 0.477, 33632/60000 datapoints
2025-03-06 19:44:45,290 - INFO - training batch 1101, loss: 0.320, 35232/60000 datapoints
2025-03-06 19:44:45,484 - INFO - training batch 1151, loss: 0.666, 36832/60000 datapoints
2025-03-06 19:44:45,679 - INFO - training batch 1201, loss: 0.267, 38432/60000 datapoints
2025-03-06 19:44:45,876 - INFO - training batch 1251, loss: 0.252, 40032/60000 datapoints
2025-03-06 19:44:46,069 - INFO - training batch 1301, loss: 0.340, 41632/60000 datapoints
2025-03-06 19:44:46,266 - INFO - training batch 1351, loss: 0.209, 43232/60000 datapoints
2025-03-06 19:44:46,460 - INFO - training batch 1401, loss: 0.299, 44832/60000 datapoints
2025-03-06 19:44:46,656 - INFO - training batch 1451, loss: 0.101, 46432/60000 datapoints
2025-03-06 19:44:46,853 - INFO - training batch 1501, loss: 0.173, 48032/60000 datapoints
2025-03-06 19:44:47,049 - INFO - training batch 1551, loss: 0.137, 49632/60000 datapoints
2025-03-06 19:44:47,245 - INFO - training batch 1601, loss: 0.179, 51232/60000 datapoints
2025-03-06 19:44:47,437 - INFO - training batch 1651, loss: 0.166, 52832/60000 datapoints
2025-03-06 19:44:47,632 - INFO - training batch 1701, loss: 0.297, 54432/60000 datapoints
2025-03-06 19:44:47,828 - INFO - training batch 1751, loss: 0.238, 56032/60000 datapoints
2025-03-06 19:44:48,044 - INFO - training batch 1801, loss: 0.125, 57632/60000 datapoints
2025-03-06 19:44:48,241 - INFO - training batch 1851, loss: 0.390, 59232/60000 datapoints
2025-03-06 19:44:48,343 - INFO - validation batch 1, loss: 0.248, 32/10016 datapoints
2025-03-06 19:44:48,494 - INFO - validation batch 51, loss: 0.094, 1632/10016 datapoints
2025-03-06 19:44:48,647 - INFO - validation batch 101, loss: 0.403, 3232/10016 datapoints
2025-03-06 19:44:48,799 - INFO - validation batch 151, loss: 0.285, 4832/10016 datapoints
2025-03-06 19:44:48,952 - INFO - validation batch 201, loss: 0.339, 6432/10016 datapoints
2025-03-06 19:44:49,103 - INFO - validation batch 251, loss: 0.183, 8032/10016 datapoints
2025-03-06 19:44:49,258 - INFO - validation batch 301, loss: 0.215, 9632/10016 datapoints
2025-03-06 19:44:49,295 - INFO - Epoch 456/800 done.
2025-03-06 19:44:49,295 - INFO - Final validation performance:
Loss: 0.252, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:44:49,296 - INFO - Beginning epoch 457/800
2025-03-06 19:44:49,302 - INFO - training batch 1, loss: 0.283, 32/60000 datapoints
2025-03-06 19:44:49,502 - INFO - training batch 51, loss: 0.207, 1632/60000 datapoints
2025-03-06 19:44:49,707 - INFO - training batch 101, loss: 0.241, 3232/60000 datapoints
2025-03-06 19:44:49,913 - INFO - training batch 151, loss: 0.403, 4832/60000 datapoints
2025-03-06 19:44:50,116 - INFO - training batch 201, loss: 0.205, 6432/60000 datapoints
2025-03-06 19:44:50,314 - INFO - training batch 251, loss: 0.208, 8032/60000 datapoints
2025-03-06 19:44:50,510 - INFO - training batch 301, loss: 0.396, 9632/60000 datapoints
2025-03-06 19:44:50,707 - INFO - training batch 351, loss: 0.141, 11232/60000 datapoints
2025-03-06 19:44:50,901 - INFO - training batch 401, loss: 0.247, 12832/60000 datapoints
2025-03-06 19:44:51,098 - INFO - training batch 451, loss: 0.230, 14432/60000 datapoints
2025-03-06 19:44:51,293 - INFO - training batch 501, loss: 0.356, 16032/60000 datapoints
2025-03-06 19:44:51,489 - INFO - training batch 551, loss: 0.176, 17632/60000 datapoints
2025-03-06 19:44:51,685 - INFO - training batch 601, loss: 0.221, 19232/60000 datapoints
2025-03-06 19:44:51,884 - INFO - training batch 651, loss: 0.371, 20832/60000 datapoints
2025-03-06 19:44:52,080 - INFO - training batch 701, loss: 0.208, 22432/60000 datapoints
2025-03-06 19:44:52,275 - INFO - training batch 751, loss: 0.754, 24032/60000 datapoints
2025-03-06 19:44:52,467 - INFO - training batch 801, loss: 0.115, 25632/60000 datapoints
2025-03-06 19:44:52,668 - INFO - training batch 851, loss: 0.299, 27232/60000 datapoints
2025-03-06 19:44:52,858 - INFO - training batch 901, loss: 0.147, 28832/60000 datapoints
2025-03-06 19:44:53,050 - INFO - training batch 951, loss: 0.179, 30432/60000 datapoints
2025-03-06 19:44:53,245 - INFO - training batch 1001, loss: 0.203, 32032/60000 datapoints
2025-03-06 19:44:53,440 - INFO - training batch 1051, loss: 0.218, 33632/60000 datapoints
2025-03-06 19:44:53,654 - INFO - training batch 1101, loss: 0.482, 35232/60000 datapoints
2025-03-06 19:44:53,852 - INFO - training batch 1151, loss: 0.250, 36832/60000 datapoints
2025-03-06 19:44:54,047 - INFO - training batch 1201, loss: 0.383, 38432/60000 datapoints
2025-03-06 19:44:54,244 - INFO - training batch 1251, loss: 0.190, 40032/60000 datapoints
2025-03-06 19:44:54,438 - INFO - training batch 1301, loss: 0.311, 41632/60000 datapoints
2025-03-06 19:44:54,632 - INFO - training batch 1351, loss: 0.479, 43232/60000 datapoints
2025-03-06 19:44:54,827 - INFO - training batch 1401, loss: 0.187, 44832/60000 datapoints
2025-03-06 19:44:55,025 - INFO - training batch 1451, loss: 0.258, 46432/60000 datapoints
2025-03-06 19:44:55,221 - INFO - training batch 1501, loss: 0.432, 48032/60000 datapoints
2025-03-06 19:44:55,417 - INFO - training batch 1551, loss: 0.353, 49632/60000 datapoints
2025-03-06 19:44:55,612 - INFO - training batch 1601, loss: 0.120, 51232/60000 datapoints
2025-03-06 19:44:55,811 - INFO - training batch 1651, loss: 0.296, 52832/60000 datapoints
2025-03-06 19:44:56,006 - INFO - training batch 1701, loss: 0.403, 54432/60000 datapoints
2025-03-06 19:44:56,201 - INFO - training batch 1751, loss: 0.266, 56032/60000 datapoints
2025-03-06 19:44:56,395 - INFO - training batch 1801, loss: 0.155, 57632/60000 datapoints
2025-03-06 19:44:56,588 - INFO - training batch 1851, loss: 0.430, 59232/60000 datapoints
2025-03-06 19:44:56,690 - INFO - validation batch 1, loss: 0.115, 32/10016 datapoints
2025-03-06 19:44:56,849 - INFO - validation batch 51, loss: 0.290, 1632/10016 datapoints
2025-03-06 19:44:57,004 - INFO - validation batch 101, loss: 0.324, 3232/10016 datapoints
2025-03-06 19:44:57,157 - INFO - validation batch 151, loss: 0.341, 4832/10016 datapoints
2025-03-06 19:44:57,309 - INFO - validation batch 201, loss: 0.119, 6432/10016 datapoints
2025-03-06 19:44:57,461 - INFO - validation batch 251, loss: 0.233, 8032/10016 datapoints
2025-03-06 19:44:57,613 - INFO - validation batch 301, loss: 0.210, 9632/10016 datapoints
2025-03-06 19:44:57,653 - INFO - Epoch 457/800 done.
2025-03-06 19:44:57,654 - INFO - Final validation performance:
Loss: 0.233, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:44:57,654 - INFO - Beginning epoch 458/800
2025-03-06 19:44:57,661 - INFO - training batch 1, loss: 0.163, 32/60000 datapoints
2025-03-06 19:44:57,879 - INFO - training batch 51, loss: 0.072, 1632/60000 datapoints
2025-03-06 19:44:58,074 - INFO - training batch 101, loss: 0.184, 3232/60000 datapoints
2025-03-06 19:44:58,275 - INFO - training batch 151, loss: 0.406, 4832/60000 datapoints
2025-03-06 19:44:58,473 - INFO - training batch 201, loss: 0.656, 6432/60000 datapoints
2025-03-06 19:44:58,676 - INFO - training batch 251, loss: 0.183, 8032/60000 datapoints
2025-03-06 19:44:58,871 - INFO - training batch 301, loss: 0.065, 9632/60000 datapoints
2025-03-06 19:44:59,067 - INFO - training batch 351, loss: 0.431, 11232/60000 datapoints
2025-03-06 19:44:59,264 - INFO - training batch 401, loss: 0.142, 12832/60000 datapoints
2025-03-06 19:44:59,460 - INFO - training batch 451, loss: 0.137, 14432/60000 datapoints
2025-03-06 19:44:59,656 - INFO - training batch 501, loss: 0.256, 16032/60000 datapoints
2025-03-06 19:44:59,856 - INFO - training batch 551, loss: 0.268, 17632/60000 datapoints
2025-03-06 19:45:00,053 - INFO - training batch 601, loss: 0.262, 19232/60000 datapoints
2025-03-06 19:45:00,247 - INFO - training batch 651, loss: 0.302, 20832/60000 datapoints
2025-03-06 19:45:00,442 - INFO - training batch 701, loss: 0.227, 22432/60000 datapoints
2025-03-06 19:45:00,639 - INFO - training batch 751, loss: 0.243, 24032/60000 datapoints
2025-03-06 19:45:00,832 - INFO - training batch 801, loss: 0.194, 25632/60000 datapoints
2025-03-06 19:45:01,027 - INFO - training batch 851, loss: 0.166, 27232/60000 datapoints
2025-03-06 19:45:01,223 - INFO - training batch 901, loss: 0.482, 28832/60000 datapoints
2025-03-06 19:45:01,417 - INFO - training batch 951, loss: 0.238, 30432/60000 datapoints
2025-03-06 19:45:01,612 - INFO - training batch 1001, loss: 0.547, 32032/60000 datapoints
2025-03-06 19:45:01,813 - INFO - training batch 1051, loss: 0.181, 33632/60000 datapoints
2025-03-06 19:45:02,009 - INFO - training batch 1101, loss: 0.181, 35232/60000 datapoints
2025-03-06 19:45:02,207 - INFO - training batch 1151, loss: 0.254, 36832/60000 datapoints
2025-03-06 19:45:02,405 - INFO - training batch 1201, loss: 0.140, 38432/60000 datapoints
2025-03-06 19:45:02,599 - INFO - training batch 1251, loss: 0.244, 40032/60000 datapoints
2025-03-06 19:45:02,798 - INFO - training batch 1301, loss: 0.312, 41632/60000 datapoints
2025-03-06 19:45:02,991 - INFO - training batch 1351, loss: 0.135, 43232/60000 datapoints
2025-03-06 19:45:03,184 - INFO - training batch 1401, loss: 0.162, 44832/60000 datapoints
2025-03-06 19:45:03,380 - INFO - training batch 1451, loss: 0.507, 46432/60000 datapoints
2025-03-06 19:45:03,580 - INFO - training batch 1501, loss: 0.228, 48032/60000 datapoints
2025-03-06 19:45:03,794 - INFO - training batch 1551, loss: 0.230, 49632/60000 datapoints
2025-03-06 19:45:03,989 - INFO - training batch 1601, loss: 0.464, 51232/60000 datapoints
2025-03-06 19:45:04,183 - INFO - training batch 1651, loss: 0.309, 52832/60000 datapoints
2025-03-06 19:45:04,379 - INFO - training batch 1701, loss: 0.488, 54432/60000 datapoints
2025-03-06 19:45:04,571 - INFO - training batch 1751, loss: 0.188, 56032/60000 datapoints
2025-03-06 19:45:04,769 - INFO - training batch 1801, loss: 0.299, 57632/60000 datapoints
2025-03-06 19:45:04,967 - INFO - training batch 1851, loss: 0.283, 59232/60000 datapoints
2025-03-06 19:45:05,071 - INFO - validation batch 1, loss: 0.093, 32/10016 datapoints
2025-03-06 19:45:05,223 - INFO - validation batch 51, loss: 0.561, 1632/10016 datapoints
2025-03-06 19:45:05,380 - INFO - validation batch 101, loss: 0.310, 3232/10016 datapoints
2025-03-06 19:45:05,531 - INFO - validation batch 151, loss: 0.400, 4832/10016 datapoints
2025-03-06 19:45:05,686 - INFO - validation batch 201, loss: 0.268, 6432/10016 datapoints
2025-03-06 19:45:05,842 - INFO - validation batch 251, loss: 0.165, 8032/10016 datapoints
2025-03-06 19:45:05,994 - INFO - validation batch 301, loss: 0.126, 9632/10016 datapoints
2025-03-06 19:45:06,030 - INFO - Epoch 458/800 done.
2025-03-06 19:45:06,031 - INFO - Final validation performance:
Loss: 0.275, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:45:06,031 - INFO - Beginning epoch 459/800
2025-03-06 19:45:06,038 - INFO - training batch 1, loss: 0.131, 32/60000 datapoints
2025-03-06 19:45:06,243 - INFO - training batch 51, loss: 0.291, 1632/60000 datapoints
2025-03-06 19:45:06,442 - INFO - training batch 101, loss: 0.308, 3232/60000 datapoints
2025-03-06 19:45:06,645 - INFO - training batch 151, loss: 0.836, 4832/60000 datapoints
2025-03-06 19:45:06,845 - INFO - training batch 201, loss: 0.318, 6432/60000 datapoints
2025-03-06 19:45:07,042 - INFO - training batch 251, loss: 0.219, 8032/60000 datapoints
2025-03-06 19:45:07,237 - INFO - training batch 301, loss: 0.350, 9632/60000 datapoints
2025-03-06 19:45:07,434 - INFO - training batch 351, loss: 0.213, 11232/60000 datapoints
2025-03-06 19:45:07,634 - INFO - training batch 401, loss: 0.193, 12832/60000 datapoints
2025-03-06 19:45:07,832 - INFO - training batch 451, loss: 0.247, 14432/60000 datapoints
2025-03-06 19:45:08,027 - INFO - training batch 501, loss: 0.371, 16032/60000 datapoints
2025-03-06 19:45:08,220 - INFO - training batch 551, loss: 0.306, 17632/60000 datapoints
2025-03-06 19:45:08,416 - INFO - training batch 601, loss: 0.140, 19232/60000 datapoints
2025-03-06 19:45:08,609 - INFO - training batch 651, loss: 0.074, 20832/60000 datapoints
2025-03-06 19:45:08,804 - INFO - training batch 701, loss: 0.165, 22432/60000 datapoints
2025-03-06 19:45:09,000 - INFO - training batch 751, loss: 0.385, 24032/60000 datapoints
2025-03-06 19:45:09,195 - INFO - training batch 801, loss: 0.168, 25632/60000 datapoints
2025-03-06 19:45:09,393 - INFO - training batch 851, loss: 0.284, 27232/60000 datapoints
2025-03-06 19:45:09,589 - INFO - training batch 901, loss: 0.132, 28832/60000 datapoints
2025-03-06 19:45:09,789 - INFO - training batch 951, loss: 0.132, 30432/60000 datapoints
2025-03-06 19:45:09,988 - INFO - training batch 1001, loss: 0.275, 32032/60000 datapoints
2025-03-06 19:45:10,184 - INFO - training batch 1051, loss: 0.342, 33632/60000 datapoints
2025-03-06 19:45:10,380 - INFO - training batch 1101, loss: 0.138, 35232/60000 datapoints
2025-03-06 19:45:10,574 - INFO - training batch 1151, loss: 0.144, 36832/60000 datapoints
2025-03-06 19:45:10,771 - INFO - training batch 1201, loss: 0.358, 38432/60000 datapoints
2025-03-06 19:45:10,965 - INFO - training batch 1251, loss: 0.661, 40032/60000 datapoints
2025-03-06 19:45:11,160 - INFO - training batch 1301, loss: 0.332, 41632/60000 datapoints
2025-03-06 19:45:11,352 - INFO - training batch 1351, loss: 0.333, 43232/60000 datapoints
2025-03-06 19:45:11,546 - INFO - training batch 1401, loss: 0.222, 44832/60000 datapoints
2025-03-06 19:45:11,742 - INFO - training batch 1451, loss: 0.214, 46432/60000 datapoints
2025-03-06 19:45:11,939 - INFO - training batch 1501, loss: 0.374, 48032/60000 datapoints
2025-03-06 19:45:12,132 - INFO - training batch 1551, loss: 0.396, 49632/60000 datapoints
2025-03-06 19:45:12,327 - INFO - training batch 1601, loss: 0.220, 51232/60000 datapoints
2025-03-06 19:45:12,521 - INFO - training batch 1651, loss: 0.186, 52832/60000 datapoints
2025-03-06 19:45:12,715 - INFO - training batch 1701, loss: 0.330, 54432/60000 datapoints
2025-03-06 19:45:12,908 - INFO - training batch 1751, loss: 0.588, 56032/60000 datapoints
2025-03-06 19:45:13,098 - INFO - training batch 1801, loss: 0.464, 57632/60000 datapoints
2025-03-06 19:45:13,293 - INFO - training batch 1851, loss: 0.315, 59232/60000 datapoints
2025-03-06 19:45:13,397 - INFO - validation batch 1, loss: 0.217, 32/10016 datapoints
2025-03-06 19:45:13,550 - INFO - validation batch 51, loss: 0.272, 1632/10016 datapoints
2025-03-06 19:45:13,721 - INFO - validation batch 101, loss: 0.207, 3232/10016 datapoints
2025-03-06 19:45:13,879 - INFO - validation batch 151, loss: 0.341, 4832/10016 datapoints
2025-03-06 19:45:14,035 - INFO - validation batch 201, loss: 0.337, 6432/10016 datapoints
2025-03-06 19:45:14,186 - INFO - validation batch 251, loss: 0.296, 8032/10016 datapoints
2025-03-06 19:45:14,340 - INFO - validation batch 301, loss: 0.084, 9632/10016 datapoints
2025-03-06 19:45:14,379 - INFO - Epoch 459/800 done.
2025-03-06 19:45:14,379 - INFO - Final validation performance:
Loss: 0.251, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:45:14,379 - INFO - Beginning epoch 460/800
2025-03-06 19:45:14,386 - INFO - training batch 1, loss: 0.235, 32/60000 datapoints
2025-03-06 19:45:14,598 - INFO - training batch 51, loss: 0.276, 1632/60000 datapoints
2025-03-06 19:45:14,797 - INFO - training batch 101, loss: 0.262, 3232/60000 datapoints
2025-03-06 19:45:15,001 - INFO - training batch 151, loss: 0.152, 4832/60000 datapoints
2025-03-06 19:45:15,201 - INFO - training batch 201, loss: 0.164, 6432/60000 datapoints
2025-03-06 19:45:15,399 - INFO - training batch 251, loss: 0.226, 8032/60000 datapoints
2025-03-06 19:45:15,596 - INFO - training batch 301, loss: 0.264, 9632/60000 datapoints
2025-03-06 19:45:15,794 - INFO - training batch 351, loss: 0.394, 11232/60000 datapoints
2025-03-06 19:45:15,993 - INFO - training batch 401, loss: 0.096, 12832/60000 datapoints
2025-03-06 19:45:16,190 - INFO - training batch 451, loss: 0.159, 14432/60000 datapoints
2025-03-06 19:45:16,385 - INFO - training batch 501, loss: 0.100, 16032/60000 datapoints
2025-03-06 19:45:16,578 - INFO - training batch 551, loss: 0.381, 17632/60000 datapoints
2025-03-06 19:45:16,780 - INFO - training batch 601, loss: 0.204, 19232/60000 datapoints
2025-03-06 19:45:16,985 - INFO - training batch 651, loss: 0.275, 20832/60000 datapoints
2025-03-06 19:45:17,182 - INFO - training batch 701, loss: 0.087, 22432/60000 datapoints
2025-03-06 19:45:17,380 - INFO - training batch 751, loss: 0.383, 24032/60000 datapoints
2025-03-06 19:45:17,576 - INFO - training batch 801, loss: 0.350, 25632/60000 datapoints
2025-03-06 19:45:17,773 - INFO - training batch 851, loss: 0.308, 27232/60000 datapoints
2025-03-06 19:45:17,969 - INFO - training batch 901, loss: 0.210, 28832/60000 datapoints
2025-03-06 19:45:18,162 - INFO - training batch 951, loss: 0.351, 30432/60000 datapoints
2025-03-06 19:45:18,357 - INFO - training batch 1001, loss: 0.312, 32032/60000 datapoints
2025-03-06 19:45:18,551 - INFO - training batch 1051, loss: 0.225, 33632/60000 datapoints
2025-03-06 19:45:18,748 - INFO - training batch 1101, loss: 0.087, 35232/60000 datapoints
2025-03-06 19:45:18,942 - INFO - training batch 1151, loss: 0.372, 36832/60000 datapoints
2025-03-06 19:45:19,139 - INFO - training batch 1201, loss: 0.225, 38432/60000 datapoints
2025-03-06 19:45:19,335 - INFO - training batch 1251, loss: 0.060, 40032/60000 datapoints
2025-03-06 19:45:19,533 - INFO - training batch 1301, loss: 0.140, 41632/60000 datapoints
2025-03-06 19:45:19,731 - INFO - training batch 1351, loss: 0.146, 43232/60000 datapoints
2025-03-06 19:45:19,928 - INFO - training batch 1401, loss: 0.146, 44832/60000 datapoints
2025-03-06 19:45:20,123 - INFO - training batch 1451, loss: 0.290, 46432/60000 datapoints
2025-03-06 19:45:20,319 - INFO - training batch 1501, loss: 0.213, 48032/60000 datapoints
2025-03-06 19:45:20,511 - INFO - training batch 1551, loss: 0.291, 49632/60000 datapoints
2025-03-06 19:45:20,707 - INFO - training batch 1601, loss: 0.205, 51232/60000 datapoints
2025-03-06 19:45:20,902 - INFO - training batch 1651, loss: 0.155, 52832/60000 datapoints
2025-03-06 19:45:21,156 - INFO - training batch 1701, loss: 0.467, 54432/60000 datapoints
2025-03-06 19:45:21,351 - INFO - training batch 1751, loss: 0.305, 56032/60000 datapoints
2025-03-06 19:45:21,549 - INFO - training batch 1801, loss: 0.273, 57632/60000 datapoints
2025-03-06 19:45:21,762 - INFO - training batch 1851, loss: 0.124, 59232/60000 datapoints
2025-03-06 19:45:21,877 - INFO - validation batch 1, loss: 0.310, 32/10016 datapoints
2025-03-06 19:45:22,051 - INFO - validation batch 51, loss: 0.176, 1632/10016 datapoints
2025-03-06 19:45:22,204 - INFO - validation batch 101, loss: 0.169, 3232/10016 datapoints
2025-03-06 19:45:22,358 - INFO - validation batch 151, loss: 0.547, 4832/10016 datapoints
2025-03-06 19:45:22,510 - INFO - validation batch 201, loss: 0.233, 6432/10016 datapoints
2025-03-06 19:45:22,667 - INFO - validation batch 251, loss: 0.126, 8032/10016 datapoints
2025-03-06 19:45:22,836 - INFO - validation batch 301, loss: 0.214, 9632/10016 datapoints
2025-03-06 19:45:22,874 - INFO - Epoch 460/800 done.
2025-03-06 19:45:22,874 - INFO - Final validation performance:
Loss: 0.254, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:45:22,875 - INFO - Beginning epoch 461/800
2025-03-06 19:45:22,882 - INFO - training batch 1, loss: 0.190, 32/60000 datapoints
2025-03-06 19:45:23,091 - INFO - training batch 51, loss: 0.774, 1632/60000 datapoints
2025-03-06 19:45:23,285 - INFO - training batch 101, loss: 0.212, 3232/60000 datapoints
2025-03-06 19:45:23,482 - INFO - training batch 151, loss: 0.232, 4832/60000 datapoints
2025-03-06 19:45:23,687 - INFO - training batch 201, loss: 0.380, 6432/60000 datapoints
2025-03-06 19:45:23,909 - INFO - training batch 251, loss: 0.258, 8032/60000 datapoints
2025-03-06 19:45:24,107 - INFO - training batch 301, loss: 0.071, 9632/60000 datapoints
2025-03-06 19:45:24,303 - INFO - training batch 351, loss: 0.390, 11232/60000 datapoints
2025-03-06 19:45:24,500 - INFO - training batch 401, loss: 0.431, 12832/60000 datapoints
2025-03-06 19:45:24,700 - INFO - training batch 451, loss: 0.259, 14432/60000 datapoints
2025-03-06 19:45:24,899 - INFO - training batch 501, loss: 0.602, 16032/60000 datapoints
2025-03-06 19:45:25,095 - INFO - training batch 551, loss: 0.068, 17632/60000 datapoints
2025-03-06 19:45:25,288 - INFO - training batch 601, loss: 0.150, 19232/60000 datapoints
2025-03-06 19:45:25,480 - INFO - training batch 651, loss: 0.186, 20832/60000 datapoints
2025-03-06 19:45:25,680 - INFO - training batch 701, loss: 0.109, 22432/60000 datapoints
2025-03-06 19:45:25,876 - INFO - training batch 751, loss: 0.207, 24032/60000 datapoints
2025-03-06 19:45:26,072 - INFO - training batch 801, loss: 0.271, 25632/60000 datapoints
2025-03-06 19:45:26,266 - INFO - training batch 851, loss: 0.252, 27232/60000 datapoints
2025-03-06 19:45:26,461 - INFO - training batch 901, loss: 0.282, 28832/60000 datapoints
2025-03-06 19:45:26,656 - INFO - training batch 951, loss: 0.339, 30432/60000 datapoints
2025-03-06 19:45:26,853 - INFO - training batch 1001, loss: 0.193, 32032/60000 datapoints
2025-03-06 19:45:27,049 - INFO - training batch 1051, loss: 0.193, 33632/60000 datapoints
2025-03-06 19:45:27,246 - INFO - training batch 1101, loss: 0.488, 35232/60000 datapoints
2025-03-06 19:45:27,443 - INFO - training batch 1151, loss: 0.500, 36832/60000 datapoints
2025-03-06 19:45:27,642 - INFO - training batch 1201, loss: 0.271, 38432/60000 datapoints
2025-03-06 19:45:27,838 - INFO - training batch 1251, loss: 0.168, 40032/60000 datapoints
2025-03-06 19:45:28,034 - INFO - training batch 1301, loss: 0.318, 41632/60000 datapoints
2025-03-06 19:45:28,228 - INFO - training batch 1351, loss: 0.188, 43232/60000 datapoints
2025-03-06 19:45:28,422 - INFO - training batch 1401, loss: 0.373, 44832/60000 datapoints
2025-03-06 19:45:28,617 - INFO - training batch 1451, loss: 0.139, 46432/60000 datapoints
2025-03-06 19:45:28,815 - INFO - training batch 1501, loss: 0.159, 48032/60000 datapoints
2025-03-06 19:45:29,013 - INFO - training batch 1551, loss: 0.110, 49632/60000 datapoints
2025-03-06 19:45:29,207 - INFO - training batch 1601, loss: 0.216, 51232/60000 datapoints
2025-03-06 19:45:29,404 - INFO - training batch 1651, loss: 0.298, 52832/60000 datapoints
2025-03-06 19:45:29,597 - INFO - training batch 1701, loss: 0.127, 54432/60000 datapoints
2025-03-06 19:45:29,794 - INFO - training batch 1751, loss: 0.529, 56032/60000 datapoints
2025-03-06 19:45:29,994 - INFO - training batch 1801, loss: 0.545, 57632/60000 datapoints
2025-03-06 19:45:30,191 - INFO - training batch 1851, loss: 0.243, 59232/60000 datapoints
2025-03-06 19:45:30,292 - INFO - validation batch 1, loss: 0.116, 32/10016 datapoints
2025-03-06 19:45:30,448 - INFO - validation batch 51, loss: 0.781, 1632/10016 datapoints
2025-03-06 19:45:30,602 - INFO - validation batch 101, loss: 0.056, 3232/10016 datapoints
2025-03-06 19:45:30,758 - INFO - validation batch 151, loss: 0.402, 4832/10016 datapoints
2025-03-06 19:45:30,911 - INFO - validation batch 201, loss: 0.230, 6432/10016 datapoints
2025-03-06 19:45:31,064 - INFO - validation batch 251, loss: 0.182, 8032/10016 datapoints
2025-03-06 19:45:31,217 - INFO - validation batch 301, loss: 0.225, 9632/10016 datapoints
2025-03-06 19:45:31,254 - INFO - Epoch 461/800 done.
2025-03-06 19:45:31,254 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:45:31,255 - INFO - Beginning epoch 462/800
2025-03-06 19:45:31,262 - INFO - training batch 1, loss: 0.245, 32/60000 datapoints
2025-03-06 19:45:31,468 - INFO - training batch 51, loss: 0.343, 1632/60000 datapoints
2025-03-06 19:45:31,664 - INFO - training batch 101, loss: 0.156, 3232/60000 datapoints
2025-03-06 19:45:31,872 - INFO - training batch 151, loss: 0.145, 4832/60000 datapoints
2025-03-06 19:45:32,068 - INFO - training batch 201, loss: 0.151, 6432/60000 datapoints
2025-03-06 19:45:32,269 - INFO - training batch 251, loss: 0.404, 8032/60000 datapoints
2025-03-06 19:45:32,466 - INFO - training batch 301, loss: 0.203, 9632/60000 datapoints
2025-03-06 19:45:32,670 - INFO - training batch 351, loss: 0.241, 11232/60000 datapoints
2025-03-06 19:45:32,867 - INFO - training batch 401, loss: 0.481, 12832/60000 datapoints
2025-03-06 19:45:33,062 - INFO - training batch 451, loss: 0.318, 14432/60000 datapoints
2025-03-06 19:45:33,256 - INFO - training batch 501, loss: 0.211, 16032/60000 datapoints
2025-03-06 19:45:33,447 - INFO - training batch 551, loss: 0.369, 17632/60000 datapoints
2025-03-06 19:45:33,643 - INFO - training batch 601, loss: 0.153, 19232/60000 datapoints
2025-03-06 19:45:33,846 - INFO - training batch 651, loss: 0.383, 20832/60000 datapoints
2025-03-06 19:45:34,054 - INFO - training batch 701, loss: 0.167, 22432/60000 datapoints
2025-03-06 19:45:34,252 - INFO - training batch 751, loss: 0.284, 24032/60000 datapoints
2025-03-06 19:45:34,447 - INFO - training batch 801, loss: 0.147, 25632/60000 datapoints
2025-03-06 19:45:34,641 - INFO - training batch 851, loss: 0.232, 27232/60000 datapoints
2025-03-06 19:45:34,839 - INFO - training batch 901, loss: 0.554, 28832/60000 datapoints
2025-03-06 19:45:35,042 - INFO - training batch 951, loss: 0.332, 30432/60000 datapoints
2025-03-06 19:45:35,238 - INFO - training batch 1001, loss: 0.215, 32032/60000 datapoints
2025-03-06 19:45:35,432 - INFO - training batch 1051, loss: 0.212, 33632/60000 datapoints
2025-03-06 19:45:35,631 - INFO - training batch 1101, loss: 0.694, 35232/60000 datapoints
2025-03-06 19:45:35,826 - INFO - training batch 1151, loss: 0.075, 36832/60000 datapoints
2025-03-06 19:45:36,047 - INFO - training batch 1201, loss: 0.643, 38432/60000 datapoints
2025-03-06 19:45:36,240 - INFO - training batch 1251, loss: 0.069, 40032/60000 datapoints
2025-03-06 19:45:36,437 - INFO - training batch 1301, loss: 0.072, 41632/60000 datapoints
2025-03-06 19:45:36,634 - INFO - training batch 1351, loss: 0.261, 43232/60000 datapoints
2025-03-06 19:45:36,835 - INFO - training batch 1401, loss: 0.080, 44832/60000 datapoints
2025-03-06 19:45:37,027 - INFO - training batch 1451, loss: 0.199, 46432/60000 datapoints
2025-03-06 19:45:37,221 - INFO - training batch 1501, loss: 0.193, 48032/60000 datapoints
2025-03-06 19:45:37,417 - INFO - training batch 1551, loss: 0.330, 49632/60000 datapoints
2025-03-06 19:45:37,612 - INFO - training batch 1601, loss: 0.300, 51232/60000 datapoints
2025-03-06 19:45:37,828 - INFO - training batch 1651, loss: 0.166, 52832/60000 datapoints
2025-03-06 19:45:38,024 - INFO - training batch 1701, loss: 0.257, 54432/60000 datapoints
2025-03-06 19:45:38,219 - INFO - training batch 1751, loss: 0.172, 56032/60000 datapoints
2025-03-06 19:45:38,415 - INFO - training batch 1801, loss: 0.129, 57632/60000 datapoints
2025-03-06 19:45:38,611 - INFO - training batch 1851, loss: 0.197, 59232/60000 datapoints
2025-03-06 19:45:38,712 - INFO - validation batch 1, loss: 0.282, 32/10016 datapoints
2025-03-06 19:45:38,865 - INFO - validation batch 51, loss: 0.479, 1632/10016 datapoints
2025-03-06 19:45:39,022 - INFO - validation batch 101, loss: 0.500, 3232/10016 datapoints
2025-03-06 19:45:39,179 - INFO - validation batch 151, loss: 0.209, 4832/10016 datapoints
2025-03-06 19:45:39,335 - INFO - validation batch 201, loss: 0.210, 6432/10016 datapoints
2025-03-06 19:45:39,486 - INFO - validation batch 251, loss: 0.154, 8032/10016 datapoints
2025-03-06 19:45:39,642 - INFO - validation batch 301, loss: 0.269, 9632/10016 datapoints
2025-03-06 19:45:39,681 - INFO - Epoch 462/800 done.
2025-03-06 19:45:39,682 - INFO - Final validation performance:
Loss: 0.300, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:45:39,682 - INFO - Beginning epoch 463/800
2025-03-06 19:45:39,689 - INFO - training batch 1, loss: 0.302, 32/60000 datapoints
2025-03-06 19:45:39,888 - INFO - training batch 51, loss: 0.169, 1632/60000 datapoints
2025-03-06 19:45:40,098 - INFO - training batch 101, loss: 0.262, 3232/60000 datapoints
2025-03-06 19:45:40,292 - INFO - training batch 151, loss: 0.488, 4832/60000 datapoints
2025-03-06 19:45:40,498 - INFO - training batch 201, loss: 0.088, 6432/60000 datapoints
2025-03-06 19:45:40,698 - INFO - training batch 251, loss: 0.278, 8032/60000 datapoints
2025-03-06 19:45:40,895 - INFO - training batch 301, loss: 0.222, 9632/60000 datapoints
2025-03-06 19:45:41,090 - INFO - training batch 351, loss: 0.175, 11232/60000 datapoints
2025-03-06 19:45:41,284 - INFO - training batch 401, loss: 0.336, 12832/60000 datapoints
2025-03-06 19:45:41,479 - INFO - training batch 451, loss: 0.145, 14432/60000 datapoints
2025-03-06 19:45:41,674 - INFO - training batch 501, loss: 0.219, 16032/60000 datapoints
2025-03-06 19:45:41,870 - INFO - training batch 551, loss: 0.175, 17632/60000 datapoints
2025-03-06 19:45:42,066 - INFO - training batch 601, loss: 0.305, 19232/60000 datapoints
2025-03-06 19:45:42,260 - INFO - training batch 651, loss: 0.127, 20832/60000 datapoints
2025-03-06 19:45:42,456 - INFO - training batch 701, loss: 0.366, 22432/60000 datapoints
2025-03-06 19:45:42,651 - INFO - training batch 751, loss: 0.356, 24032/60000 datapoints
2025-03-06 19:45:42,847 - INFO - training batch 801, loss: 0.326, 25632/60000 datapoints
2025-03-06 19:45:43,041 - INFO - training batch 851, loss: 0.181, 27232/60000 datapoints
2025-03-06 19:45:43,236 - INFO - training batch 901, loss: 0.247, 28832/60000 datapoints
2025-03-06 19:45:43,434 - INFO - training batch 951, loss: 0.162, 30432/60000 datapoints
2025-03-06 19:45:43,631 - INFO - training batch 1001, loss: 0.087, 32032/60000 datapoints
2025-03-06 19:45:43,825 - INFO - training batch 1051, loss: 0.486, 33632/60000 datapoints
2025-03-06 19:45:44,042 - INFO - training batch 1101, loss: 0.452, 35232/60000 datapoints
2025-03-06 19:45:44,238 - INFO - training batch 1151, loss: 0.448, 36832/60000 datapoints
2025-03-06 19:45:44,433 - INFO - training batch 1201, loss: 0.124, 38432/60000 datapoints
2025-03-06 19:45:44,628 - INFO - training batch 1251, loss: 0.345, 40032/60000 datapoints
2025-03-06 19:45:44,823 - INFO - training batch 1301, loss: 0.190, 41632/60000 datapoints
2025-03-06 19:45:45,025 - INFO - training batch 1351, loss: 0.089, 43232/60000 datapoints
2025-03-06 19:45:45,220 - INFO - training batch 1401, loss: 0.325, 44832/60000 datapoints
2025-03-06 19:45:45,415 - INFO - training batch 1451, loss: 0.318, 46432/60000 datapoints
2025-03-06 19:45:45,609 - INFO - training batch 1501, loss: 0.169, 48032/60000 datapoints
2025-03-06 19:45:45,808 - INFO - training batch 1551, loss: 0.269, 49632/60000 datapoints
2025-03-06 19:45:46,007 - INFO - training batch 1601, loss: 0.407, 51232/60000 datapoints
2025-03-06 19:45:46,201 - INFO - training batch 1651, loss: 0.333, 52832/60000 datapoints
2025-03-06 19:45:46,398 - INFO - training batch 1701, loss: 0.406, 54432/60000 datapoints
2025-03-06 19:45:46,594 - INFO - training batch 1751, loss: 0.089, 56032/60000 datapoints
2025-03-06 19:45:46,791 - INFO - training batch 1801, loss: 0.072, 57632/60000 datapoints
2025-03-06 19:45:46,989 - INFO - training batch 1851, loss: 0.402, 59232/60000 datapoints
2025-03-06 19:45:47,091 - INFO - validation batch 1, loss: 0.545, 32/10016 datapoints
2025-03-06 19:45:47,245 - INFO - validation batch 51, loss: 0.089, 1632/10016 datapoints
2025-03-06 19:45:47,398 - INFO - validation batch 101, loss: 0.176, 3232/10016 datapoints
2025-03-06 19:45:47,550 - INFO - validation batch 151, loss: 0.286, 4832/10016 datapoints
2025-03-06 19:45:47,703 - INFO - validation batch 201, loss: 0.587, 6432/10016 datapoints
2025-03-06 19:45:47,861 - INFO - validation batch 251, loss: 0.353, 8032/10016 datapoints
2025-03-06 19:45:48,020 - INFO - validation batch 301, loss: 0.119, 9632/10016 datapoints
2025-03-06 19:45:48,058 - INFO - Epoch 463/800 done.
2025-03-06 19:45:48,059 - INFO - Final validation performance:
Loss: 0.308, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:45:48,059 - INFO - Beginning epoch 464/800
2025-03-06 19:45:48,068 - INFO - training batch 1, loss: 0.129, 32/60000 datapoints
2025-03-06 19:45:48,281 - INFO - training batch 51, loss: 0.377, 1632/60000 datapoints
2025-03-06 19:45:48,477 - INFO - training batch 101, loss: 0.242, 3232/60000 datapoints
2025-03-06 19:45:48,678 - INFO - training batch 151, loss: 0.165, 4832/60000 datapoints
2025-03-06 19:45:48,876 - INFO - training batch 201, loss: 0.295, 6432/60000 datapoints
2025-03-06 19:45:49,076 - INFO - training batch 251, loss: 0.318, 8032/60000 datapoints
2025-03-06 19:45:49,270 - INFO - training batch 301, loss: 0.488, 9632/60000 datapoints
2025-03-06 19:45:49,465 - INFO - training batch 351, loss: 0.197, 11232/60000 datapoints
2025-03-06 19:45:49,658 - INFO - training batch 401, loss: 0.687, 12832/60000 datapoints
2025-03-06 19:45:49,852 - INFO - training batch 451, loss: 0.556, 14432/60000 datapoints
2025-03-06 19:45:50,049 - INFO - training batch 501, loss: 0.366, 16032/60000 datapoints
2025-03-06 19:45:50,243 - INFO - training batch 551, loss: 0.326, 17632/60000 datapoints
2025-03-06 19:45:50,436 - INFO - training batch 601, loss: 0.382, 19232/60000 datapoints
2025-03-06 19:45:50,633 - INFO - training batch 651, loss: 0.330, 20832/60000 datapoints
2025-03-06 19:45:50,826 - INFO - training batch 701, loss: 0.075, 22432/60000 datapoints
2025-03-06 19:45:51,024 - INFO - training batch 751, loss: 0.114, 24032/60000 datapoints
2025-03-06 19:45:51,217 - INFO - training batch 801, loss: 0.156, 25632/60000 datapoints
2025-03-06 19:45:51,413 - INFO - training batch 851, loss: 0.281, 27232/60000 datapoints
2025-03-06 19:45:51,608 - INFO - training batch 901, loss: 0.372, 28832/60000 datapoints
2025-03-06 19:45:51,804 - INFO - training batch 951, loss: 0.157, 30432/60000 datapoints
2025-03-06 19:45:52,003 - INFO - training batch 1001, loss: 0.495, 32032/60000 datapoints
2025-03-06 19:45:52,198 - INFO - training batch 1051, loss: 0.512, 33632/60000 datapoints
2025-03-06 19:45:52,392 - INFO - training batch 1101, loss: 0.173, 35232/60000 datapoints
2025-03-06 19:45:52,586 - INFO - training batch 1151, loss: 0.349, 36832/60000 datapoints
2025-03-06 19:45:52,782 - INFO - training batch 1201, loss: 0.093, 38432/60000 datapoints
2025-03-06 19:45:52,979 - INFO - training batch 1251, loss: 0.179, 40032/60000 datapoints
2025-03-06 19:45:53,175 - INFO - training batch 1301, loss: 0.401, 41632/60000 datapoints
2025-03-06 19:45:53,370 - INFO - training batch 1351, loss: 0.444, 43232/60000 datapoints
2025-03-06 19:45:53,565 - INFO - training batch 1401, loss: 0.127, 44832/60000 datapoints
2025-03-06 19:45:53,765 - INFO - training batch 1451, loss: 0.441, 46432/60000 datapoints
2025-03-06 19:45:53,960 - INFO - training batch 1501, loss: 0.247, 48032/60000 datapoints
2025-03-06 19:45:54,180 - INFO - training batch 1551, loss: 0.352, 49632/60000 datapoints
2025-03-06 19:45:54,373 - INFO - training batch 1601, loss: 0.225, 51232/60000 datapoints
2025-03-06 19:45:54,565 - INFO - training batch 1651, loss: 0.204, 52832/60000 datapoints
2025-03-06 19:45:54,761 - INFO - training batch 1701, loss: 0.210, 54432/60000 datapoints
2025-03-06 19:45:54,960 - INFO - training batch 1751, loss: 0.116, 56032/60000 datapoints
2025-03-06 19:45:55,156 - INFO - training batch 1801, loss: 0.248, 57632/60000 datapoints
2025-03-06 19:45:55,350 - INFO - training batch 1851, loss: 0.196, 59232/60000 datapoints
2025-03-06 19:45:55,453 - INFO - validation batch 1, loss: 0.463, 32/10016 datapoints
2025-03-06 19:45:55,607 - INFO - validation batch 51, loss: 0.308, 1632/10016 datapoints
2025-03-06 19:45:55,762 - INFO - validation batch 101, loss: 0.199, 3232/10016 datapoints
2025-03-06 19:45:55,915 - INFO - validation batch 151, loss: 0.257, 4832/10016 datapoints
2025-03-06 19:45:56,072 - INFO - validation batch 201, loss: 0.100, 6432/10016 datapoints
2025-03-06 19:45:56,225 - INFO - validation batch 251, loss: 0.131, 8032/10016 datapoints
2025-03-06 19:45:56,379 - INFO - validation batch 301, loss: 0.228, 9632/10016 datapoints
2025-03-06 19:45:56,415 - INFO - Epoch 464/800 done.
2025-03-06 19:45:56,416 - INFO - Final validation performance:
Loss: 0.241, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:45:56,416 - INFO - Beginning epoch 465/800
2025-03-06 19:45:56,423 - INFO - training batch 1, loss: 0.330, 32/60000 datapoints
2025-03-06 19:45:56,631 - INFO - training batch 51, loss: 0.318, 1632/60000 datapoints
2025-03-06 19:45:56,823 - INFO - training batch 101, loss: 0.407, 3232/60000 datapoints
2025-03-06 19:45:57,029 - INFO - training batch 151, loss: 0.330, 4832/60000 datapoints
2025-03-06 19:45:57,222 - INFO - training batch 201, loss: 0.090, 6432/60000 datapoints
2025-03-06 19:45:57,419 - INFO - training batch 251, loss: 0.308, 8032/60000 datapoints
2025-03-06 19:45:57,611 - INFO - training batch 301, loss: 0.150, 9632/60000 datapoints
2025-03-06 19:45:57,806 - INFO - training batch 351, loss: 0.239, 11232/60000 datapoints
2025-03-06 19:45:58,002 - INFO - training batch 401, loss: 0.308, 12832/60000 datapoints
2025-03-06 19:45:58,194 - INFO - training batch 451, loss: 0.503, 14432/60000 datapoints
2025-03-06 19:45:58,386 - INFO - training batch 501, loss: 0.287, 16032/60000 datapoints
2025-03-06 19:45:58,580 - INFO - training batch 551, loss: 0.630, 17632/60000 datapoints
2025-03-06 19:45:58,772 - INFO - training batch 601, loss: 0.176, 19232/60000 datapoints
2025-03-06 19:45:58,966 - INFO - training batch 651, loss: 0.213, 20832/60000 datapoints
2025-03-06 19:45:59,160 - INFO - training batch 701, loss: 0.035, 22432/60000 datapoints
2025-03-06 19:45:59,351 - INFO - training batch 751, loss: 0.159, 24032/60000 datapoints
2025-03-06 19:45:59,544 - INFO - training batch 801, loss: 0.541, 25632/60000 datapoints
2025-03-06 19:45:59,739 - INFO - training batch 851, loss: 0.386, 27232/60000 datapoints
2025-03-06 19:45:59,931 - INFO - training batch 901, loss: 0.436, 28832/60000 datapoints
2025-03-06 19:46:00,126 - INFO - training batch 951, loss: 0.368, 30432/60000 datapoints
2025-03-06 19:46:00,316 - INFO - training batch 1001, loss: 0.129, 32032/60000 datapoints
2025-03-06 19:46:00,508 - INFO - training batch 1051, loss: 0.162, 33632/60000 datapoints
2025-03-06 19:46:00,700 - INFO - training batch 1101, loss: 0.177, 35232/60000 datapoints
2025-03-06 19:46:00,892 - INFO - training batch 1151, loss: 0.235, 36832/60000 datapoints
2025-03-06 19:46:01,083 - INFO - training batch 1201, loss: 0.113, 38432/60000 datapoints
2025-03-06 19:46:01,273 - INFO - training batch 1251, loss: 0.173, 40032/60000 datapoints
2025-03-06 19:46:01,464 - INFO - training batch 1301, loss: 0.106, 41632/60000 datapoints
2025-03-06 19:46:01,659 - INFO - training batch 1351, loss: 0.472, 43232/60000 datapoints
2025-03-06 19:46:01,851 - INFO - training batch 1401, loss: 0.316, 44832/60000 datapoints
2025-03-06 19:46:02,049 - INFO - training batch 1451, loss: 0.364, 46432/60000 datapoints
2025-03-06 19:46:02,239 - INFO - training batch 1501, loss: 0.270, 48032/60000 datapoints
2025-03-06 19:46:02,432 - INFO - training batch 1551, loss: 0.519, 49632/60000 datapoints
2025-03-06 19:46:02,625 - INFO - training batch 1601, loss: 0.310, 51232/60000 datapoints
2025-03-06 19:46:02,817 - INFO - training batch 1651, loss: 0.273, 52832/60000 datapoints
2025-03-06 19:46:03,012 - INFO - training batch 1701, loss: 0.138, 54432/60000 datapoints
2025-03-06 19:46:03,208 - INFO - training batch 1751, loss: 0.284, 56032/60000 datapoints
2025-03-06 19:46:03,399 - INFO - training batch 1801, loss: 0.366, 57632/60000 datapoints
2025-03-06 19:46:03,590 - INFO - training batch 1851, loss: 0.234, 59232/60000 datapoints
2025-03-06 19:46:03,691 - INFO - validation batch 1, loss: 0.226, 32/10016 datapoints
2025-03-06 19:46:03,840 - INFO - validation batch 51, loss: 0.343, 1632/10016 datapoints
2025-03-06 19:46:03,989 - INFO - validation batch 101, loss: 0.385, 3232/10016 datapoints
2025-03-06 19:46:04,160 - INFO - validation batch 151, loss: 0.424, 4832/10016 datapoints
2025-03-06 19:46:04,310 - INFO - validation batch 201, loss: 0.447, 6432/10016 datapoints
2025-03-06 19:46:04,460 - INFO - validation batch 251, loss: 0.082, 8032/10016 datapoints
2025-03-06 19:46:04,613 - INFO - validation batch 301, loss: 0.375, 9632/10016 datapoints
2025-03-06 19:46:04,650 - INFO - Epoch 465/800 done.
2025-03-06 19:46:04,651 - INFO - Final validation performance:
Loss: 0.326, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:46:04,651 - INFO - Beginning epoch 466/800
2025-03-06 19:46:04,657 - INFO - training batch 1, loss: 0.079, 32/60000 datapoints
2025-03-06 19:46:04,859 - INFO - training batch 51, loss: 0.256, 1632/60000 datapoints
2025-03-06 19:46:05,058 - INFO - training batch 101, loss: 0.179, 3232/60000 datapoints
2025-03-06 19:46:05,259 - INFO - training batch 151, loss: 0.237, 4832/60000 datapoints
2025-03-06 19:46:05,452 - INFO - training batch 201, loss: 0.113, 6432/60000 datapoints
2025-03-06 19:46:05,649 - INFO - training batch 251, loss: 0.175, 8032/60000 datapoints
2025-03-06 19:46:05,842 - INFO - training batch 301, loss: 0.282, 9632/60000 datapoints
2025-03-06 19:46:06,039 - INFO - training batch 351, loss: 0.282, 11232/60000 datapoints
2025-03-06 19:46:06,232 - INFO - training batch 401, loss: 0.227, 12832/60000 datapoints
2025-03-06 19:46:06,426 - INFO - training batch 451, loss: 0.235, 14432/60000 datapoints
2025-03-06 19:46:06,620 - INFO - training batch 501, loss: 0.197, 16032/60000 datapoints
2025-03-06 19:46:06,814 - INFO - training batch 551, loss: 0.165, 17632/60000 datapoints
2025-03-06 19:46:07,013 - INFO - training batch 601, loss: 0.077, 19232/60000 datapoints
2025-03-06 19:46:07,207 - INFO - training batch 651, loss: 0.381, 20832/60000 datapoints
2025-03-06 19:46:07,400 - INFO - training batch 701, loss: 0.167, 22432/60000 datapoints
2025-03-06 19:46:07,598 - INFO - training batch 751, loss: 0.197, 24032/60000 datapoints
2025-03-06 19:46:07,793 - INFO - training batch 801, loss: 0.279, 25632/60000 datapoints
2025-03-06 19:46:07,986 - INFO - training batch 851, loss: 0.228, 27232/60000 datapoints
2025-03-06 19:46:08,182 - INFO - training batch 901, loss: 0.120, 28832/60000 datapoints
2025-03-06 19:46:08,374 - INFO - training batch 951, loss: 0.433, 30432/60000 datapoints
2025-03-06 19:46:08,566 - INFO - training batch 1001, loss: 0.228, 32032/60000 datapoints
2025-03-06 19:46:08,760 - INFO - training batch 1051, loss: 0.513, 33632/60000 datapoints
2025-03-06 19:46:08,953 - INFO - training batch 1101, loss: 0.241, 35232/60000 datapoints
2025-03-06 19:46:09,145 - INFO - training batch 1151, loss: 0.460, 36832/60000 datapoints
2025-03-06 19:46:09,339 - INFO - training batch 1201, loss: 0.467, 38432/60000 datapoints
2025-03-06 19:46:09,530 - INFO - training batch 1251, loss: 0.286, 40032/60000 datapoints
2025-03-06 19:46:09,725 - INFO - training batch 1301, loss: 0.529, 41632/60000 datapoints
2025-03-06 19:46:09,913 - INFO - training batch 1351, loss: 0.534, 43232/60000 datapoints
2025-03-06 19:46:10,107 - INFO - training batch 1401, loss: 0.209, 44832/60000 datapoints
2025-03-06 19:46:10,301 - INFO - training batch 1451, loss: 0.087, 46432/60000 datapoints
2025-03-06 19:46:10,493 - INFO - training batch 1501, loss: 0.194, 48032/60000 datapoints
2025-03-06 19:46:10,688 - INFO - training batch 1551, loss: 0.261, 49632/60000 datapoints
2025-03-06 19:46:10,880 - INFO - training batch 1601, loss: 0.128, 51232/60000 datapoints
2025-03-06 19:46:11,072 - INFO - training batch 1651, loss: 0.244, 52832/60000 datapoints
2025-03-06 19:46:11,268 - INFO - training batch 1701, loss: 0.218, 54432/60000 datapoints
2025-03-06 19:46:11,460 - INFO - training batch 1751, loss: 0.307, 56032/60000 datapoints
2025-03-06 19:46:11,653 - INFO - training batch 1801, loss: 0.139, 57632/60000 datapoints
2025-03-06 19:46:11,844 - INFO - training batch 1851, loss: 0.164, 59232/60000 datapoints
2025-03-06 19:46:11,943 - INFO - validation batch 1, loss: 0.120, 32/10016 datapoints
2025-03-06 19:46:12,094 - INFO - validation batch 51, loss: 0.110, 1632/10016 datapoints
2025-03-06 19:46:12,245 - INFO - validation batch 101, loss: 0.256, 3232/10016 datapoints
2025-03-06 19:46:12,395 - INFO - validation batch 151, loss: 0.335, 4832/10016 datapoints
2025-03-06 19:46:12,545 - INFO - validation batch 201, loss: 0.389, 6432/10016 datapoints
2025-03-06 19:46:12,699 - INFO - validation batch 251, loss: 0.124, 8032/10016 datapoints
2025-03-06 19:46:12,852 - INFO - validation batch 301, loss: 0.254, 9632/10016 datapoints
2025-03-06 19:46:12,893 - INFO - Epoch 466/800 done.
2025-03-06 19:46:12,893 - INFO - Final validation performance:
Loss: 0.227, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:46:12,893 - INFO - Beginning epoch 467/800
2025-03-06 19:46:12,900 - INFO - training batch 1, loss: 0.280, 32/60000 datapoints
2025-03-06 19:46:13,108 - INFO - training batch 51, loss: 0.201, 1632/60000 datapoints
2025-03-06 19:46:13,306 - INFO - training batch 101, loss: 0.221, 3232/60000 datapoints
2025-03-06 19:46:13,508 - INFO - training batch 151, loss: 0.369, 4832/60000 datapoints
2025-03-06 19:46:13,710 - INFO - training batch 201, loss: 0.390, 6432/60000 datapoints
2025-03-06 19:46:13,908 - INFO - training batch 251, loss: 0.439, 8032/60000 datapoints
2025-03-06 19:46:14,104 - INFO - training batch 301, loss: 0.287, 9632/60000 datapoints
2025-03-06 19:46:14,317 - INFO - training batch 351, loss: 0.281, 11232/60000 datapoints
2025-03-06 19:46:14,512 - INFO - training batch 401, loss: 0.343, 12832/60000 datapoints
2025-03-06 19:46:14,713 - INFO - training batch 451, loss: 0.206, 14432/60000 datapoints
2025-03-06 19:46:14,913 - INFO - training batch 501, loss: 0.157, 16032/60000 datapoints
2025-03-06 19:46:15,108 - INFO - training batch 551, loss: 0.114, 17632/60000 datapoints
2025-03-06 19:46:15,305 - INFO - training batch 601, loss: 0.142, 19232/60000 datapoints
2025-03-06 19:46:15,501 - INFO - training batch 651, loss: 0.095, 20832/60000 datapoints
2025-03-06 19:46:15,698 - INFO - training batch 701, loss: 0.092, 22432/60000 datapoints
2025-03-06 19:46:15,895 - INFO - training batch 751, loss: 0.063, 24032/60000 datapoints
2025-03-06 19:46:16,091 - INFO - training batch 801, loss: 0.238, 25632/60000 datapoints
2025-03-06 19:46:16,285 - INFO - training batch 851, loss: 0.143, 27232/60000 datapoints
2025-03-06 19:46:16,481 - INFO - training batch 901, loss: 0.336, 28832/60000 datapoints
2025-03-06 19:46:16,678 - INFO - training batch 951, loss: 0.309, 30432/60000 datapoints
2025-03-06 19:46:16,874 - INFO - training batch 1001, loss: 0.471, 32032/60000 datapoints
2025-03-06 19:46:17,071 - INFO - training batch 1051, loss: 0.164, 33632/60000 datapoints
2025-03-06 19:46:17,266 - INFO - training batch 1101, loss: 0.447, 35232/60000 datapoints
2025-03-06 19:46:17,462 - INFO - training batch 1151, loss: 0.454, 36832/60000 datapoints
2025-03-06 19:46:17,660 - INFO - training batch 1201, loss: 0.203, 38432/60000 datapoints
2025-03-06 19:46:17,855 - INFO - training batch 1251, loss: 0.179, 40032/60000 datapoints
2025-03-06 19:46:18,053 - INFO - training batch 1301, loss: 0.258, 41632/60000 datapoints
2025-03-06 19:46:18,253 - INFO - training batch 1351, loss: 0.356, 43232/60000 datapoints
2025-03-06 19:46:18,447 - INFO - training batch 1401, loss: 0.294, 44832/60000 datapoints
2025-03-06 19:46:18,642 - INFO - training batch 1451, loss: 0.142, 46432/60000 datapoints
2025-03-06 19:46:18,835 - INFO - training batch 1501, loss: 0.343, 48032/60000 datapoints
2025-03-06 19:46:19,029 - INFO - training batch 1551, loss: 0.194, 49632/60000 datapoints
2025-03-06 19:46:19,224 - INFO - training batch 1601, loss: 0.571, 51232/60000 datapoints
2025-03-06 19:46:19,420 - INFO - training batch 1651, loss: 0.236, 52832/60000 datapoints
2025-03-06 19:46:19,616 - INFO - training batch 1701, loss: 0.155, 54432/60000 datapoints
2025-03-06 19:46:19,815 - INFO - training batch 1751, loss: 0.433, 56032/60000 datapoints
2025-03-06 19:46:20,010 - INFO - training batch 1801, loss: 0.114, 57632/60000 datapoints
2025-03-06 19:46:20,206 - INFO - training batch 1851, loss: 0.204, 59232/60000 datapoints
2025-03-06 19:46:20,309 - INFO - validation batch 1, loss: 0.409, 32/10016 datapoints
2025-03-06 19:46:20,461 - INFO - validation batch 51, loss: 0.392, 1632/10016 datapoints
2025-03-06 19:46:20,613 - INFO - validation batch 101, loss: 0.161, 3232/10016 datapoints
2025-03-06 19:46:20,766 - INFO - validation batch 151, loss: 0.183, 4832/10016 datapoints
2025-03-06 19:46:20,920 - INFO - validation batch 201, loss: 0.283, 6432/10016 datapoints
2025-03-06 19:46:21,072 - INFO - validation batch 251, loss: 0.210, 8032/10016 datapoints
2025-03-06 19:46:21,223 - INFO - validation batch 301, loss: 0.203, 9632/10016 datapoints
2025-03-06 19:46:21,259 - INFO - Epoch 467/800 done.
2025-03-06 19:46:21,259 - INFO - Final validation performance:
Loss: 0.263, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:46:21,260 - INFO - Beginning epoch 468/800
2025-03-06 19:46:21,267 - INFO - training batch 1, loss: 0.171, 32/60000 datapoints
2025-03-06 19:46:21,484 - INFO - training batch 51, loss: 0.093, 1632/60000 datapoints
2025-03-06 19:46:21,681 - INFO - training batch 101, loss: 0.224, 3232/60000 datapoints
2025-03-06 19:46:21,887 - INFO - training batch 151, loss: 0.318, 4832/60000 datapoints
2025-03-06 19:46:22,090 - INFO - training batch 201, loss: 0.191, 6432/60000 datapoints
2025-03-06 19:46:22,288 - INFO - training batch 251, loss: 0.322, 8032/60000 datapoints
2025-03-06 19:46:22,482 - INFO - training batch 301, loss: 0.339, 9632/60000 datapoints
2025-03-06 19:46:22,678 - INFO - training batch 351, loss: 0.686, 11232/60000 datapoints
2025-03-06 19:46:22,870 - INFO - training batch 401, loss: 0.132, 12832/60000 datapoints
2025-03-06 19:46:23,064 - INFO - training batch 451, loss: 0.259, 14432/60000 datapoints
2025-03-06 19:46:23,257 - INFO - training batch 501, loss: 0.394, 16032/60000 datapoints
2025-03-06 19:46:23,456 - INFO - training batch 551, loss: 0.319, 17632/60000 datapoints
2025-03-06 19:46:23,653 - INFO - training batch 601, loss: 0.197, 19232/60000 datapoints
2025-03-06 19:46:23,847 - INFO - training batch 651, loss: 0.335, 20832/60000 datapoints
2025-03-06 19:46:24,040 - INFO - training batch 701, loss: 0.222, 22432/60000 datapoints
2025-03-06 19:46:24,250 - INFO - training batch 751, loss: 0.331, 24032/60000 datapoints
2025-03-06 19:46:24,465 - INFO - training batch 801, loss: 0.184, 25632/60000 datapoints
2025-03-06 19:46:24,663 - INFO - training batch 851, loss: 0.181, 27232/60000 datapoints
2025-03-06 19:46:24,859 - INFO - training batch 901, loss: 0.218, 28832/60000 datapoints
2025-03-06 19:46:25,059 - INFO - training batch 951, loss: 0.150, 30432/60000 datapoints
2025-03-06 19:46:25,255 - INFO - training batch 1001, loss: 0.186, 32032/60000 datapoints
2025-03-06 19:46:25,452 - INFO - training batch 1051, loss: 0.081, 33632/60000 datapoints
2025-03-06 19:46:25,650 - INFO - training batch 1101, loss: 0.268, 35232/60000 datapoints
2025-03-06 19:46:25,845 - INFO - training batch 1151, loss: 0.266, 36832/60000 datapoints
2025-03-06 19:46:26,038 - INFO - training batch 1201, loss: 0.487, 38432/60000 datapoints
2025-03-06 19:46:26,233 - INFO - training batch 1251, loss: 0.234, 40032/60000 datapoints
2025-03-06 19:46:26,429 - INFO - training batch 1301, loss: 0.151, 41632/60000 datapoints
2025-03-06 19:46:26,623 - INFO - training batch 1351, loss: 0.090, 43232/60000 datapoints
2025-03-06 19:46:26,819 - INFO - training batch 1401, loss: 0.253, 44832/60000 datapoints
2025-03-06 19:46:27,019 - INFO - training batch 1451, loss: 0.268, 46432/60000 datapoints
2025-03-06 19:46:27,212 - INFO - training batch 1501, loss: 0.258, 48032/60000 datapoints
2025-03-06 19:46:27,406 - INFO - training batch 1551, loss: 0.413, 49632/60000 datapoints
2025-03-06 19:46:27,600 - INFO - training batch 1601, loss: 0.233, 51232/60000 datapoints
2025-03-06 19:46:27,795 - INFO - training batch 1651, loss: 0.173, 52832/60000 datapoints
2025-03-06 19:46:27,992 - INFO - training batch 1701, loss: 0.310, 54432/60000 datapoints
2025-03-06 19:46:28,188 - INFO - training batch 1751, loss: 0.052, 56032/60000 datapoints
2025-03-06 19:46:28,384 - INFO - training batch 1801, loss: 0.198, 57632/60000 datapoints
2025-03-06 19:46:28,576 - INFO - training batch 1851, loss: 0.186, 59232/60000 datapoints
2025-03-06 19:46:28,681 - INFO - validation batch 1, loss: 0.226, 32/10016 datapoints
2025-03-06 19:46:28,834 - INFO - validation batch 51, loss: 0.380, 1632/10016 datapoints
2025-03-06 19:46:28,985 - INFO - validation batch 101, loss: 0.367, 3232/10016 datapoints
2025-03-06 19:46:29,136 - INFO - validation batch 151, loss: 0.311, 4832/10016 datapoints
2025-03-06 19:46:29,290 - INFO - validation batch 201, loss: 0.192, 6432/10016 datapoints
2025-03-06 19:46:29,445 - INFO - validation batch 251, loss: 0.101, 8032/10016 datapoints
2025-03-06 19:46:29,598 - INFO - validation batch 301, loss: 0.155, 9632/10016 datapoints
2025-03-06 19:46:29,640 - INFO - Epoch 468/800 done.
2025-03-06 19:46:29,640 - INFO - Final validation performance:
Loss: 0.247, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:46:29,641 - INFO - Beginning epoch 469/800
2025-03-06 19:46:29,647 - INFO - training batch 1, loss: 0.119, 32/60000 datapoints
2025-03-06 19:46:29,842 - INFO - training batch 51, loss: 0.258, 1632/60000 datapoints
2025-03-06 19:46:30,038 - INFO - training batch 101, loss: 0.195, 3232/60000 datapoints
2025-03-06 19:46:30,247 - INFO - training batch 151, loss: 0.282, 4832/60000 datapoints
2025-03-06 19:46:30,443 - INFO - training batch 201, loss: 0.285, 6432/60000 datapoints
2025-03-06 19:46:30,643 - INFO - training batch 251, loss: 0.373, 8032/60000 datapoints
2025-03-06 19:46:30,839 - INFO - training batch 301, loss: 0.230, 9632/60000 datapoints
2025-03-06 19:46:31,040 - INFO - training batch 351, loss: 0.058, 11232/60000 datapoints
2025-03-06 19:46:31,235 - INFO - training batch 401, loss: 0.311, 12832/60000 datapoints
2025-03-06 19:46:31,434 - INFO - training batch 451, loss: 0.398, 14432/60000 datapoints
2025-03-06 19:46:31,631 - INFO - training batch 501, loss: 0.374, 16032/60000 datapoints
2025-03-06 19:46:31,826 - INFO - training batch 551, loss: 0.603, 17632/60000 datapoints
2025-03-06 19:46:32,021 - INFO - training batch 601, loss: 0.307, 19232/60000 datapoints
2025-03-06 19:46:32,217 - INFO - training batch 651, loss: 0.202, 20832/60000 datapoints
2025-03-06 19:46:32,411 - INFO - training batch 701, loss: 0.225, 22432/60000 datapoints
2025-03-06 19:46:32,604 - INFO - training batch 751, loss: 0.217, 24032/60000 datapoints
2025-03-06 19:46:32,800 - INFO - training batch 801, loss: 0.494, 25632/60000 datapoints
2025-03-06 19:46:32,995 - INFO - training batch 851, loss: 0.234, 27232/60000 datapoints
2025-03-06 19:46:33,192 - INFO - training batch 901, loss: 0.142, 28832/60000 datapoints
2025-03-06 19:46:33,388 - INFO - training batch 951, loss: 0.298, 30432/60000 datapoints
2025-03-06 19:46:33,590 - INFO - training batch 1001, loss: 0.189, 32032/60000 datapoints
2025-03-06 19:46:33,786 - INFO - training batch 1051, loss: 0.135, 33632/60000 datapoints
2025-03-06 19:46:33,981 - INFO - training batch 1101, loss: 0.084, 35232/60000 datapoints
2025-03-06 19:46:34,177 - INFO - training batch 1151, loss: 0.377, 36832/60000 datapoints
2025-03-06 19:46:34,395 - INFO - training batch 1201, loss: 0.295, 38432/60000 datapoints
2025-03-06 19:46:34,590 - INFO - training batch 1251, loss: 0.361, 40032/60000 datapoints
2025-03-06 19:46:34,786 - INFO - training batch 1301, loss: 0.192, 41632/60000 datapoints
2025-03-06 19:46:34,985 - INFO - training batch 1351, loss: 0.276, 43232/60000 datapoints
2025-03-06 19:46:35,179 - INFO - training batch 1401, loss: 0.048, 44832/60000 datapoints
2025-03-06 19:46:35,382 - INFO - training batch 1451, loss: 0.273, 46432/60000 datapoints
2025-03-06 19:46:35,576 - INFO - training batch 1501, loss: 0.256, 48032/60000 datapoints
2025-03-06 19:46:35,800 - INFO - training batch 1551, loss: 0.165, 49632/60000 datapoints
2025-03-06 19:46:35,995 - INFO - training batch 1601, loss: 0.156, 51232/60000 datapoints
2025-03-06 19:46:36,191 - INFO - training batch 1651, loss: 0.124, 52832/60000 datapoints
2025-03-06 19:46:36,418 - INFO - training batch 1701, loss: 0.169, 54432/60000 datapoints
2025-03-06 19:46:36,611 - INFO - training batch 1751, loss: 0.236, 56032/60000 datapoints
2025-03-06 19:46:36,808 - INFO - training batch 1801, loss: 0.546, 57632/60000 datapoints
2025-03-06 19:46:37,007 - INFO - training batch 1851, loss: 0.472, 59232/60000 datapoints
2025-03-06 19:46:37,108 - INFO - validation batch 1, loss: 0.068, 32/10016 datapoints
2025-03-06 19:46:37,268 - INFO - validation batch 51, loss: 0.212, 1632/10016 datapoints
2025-03-06 19:46:37,421 - INFO - validation batch 101, loss: 0.239, 3232/10016 datapoints
2025-03-06 19:46:37,575 - INFO - validation batch 151, loss: 0.305, 4832/10016 datapoints
2025-03-06 19:46:37,741 - INFO - validation batch 201, loss: 0.204, 6432/10016 datapoints
2025-03-06 19:46:37,905 - INFO - validation batch 251, loss: 0.198, 8032/10016 datapoints
2025-03-06 19:46:38,061 - INFO - validation batch 301, loss: 0.262, 9632/10016 datapoints
2025-03-06 19:46:38,098 - INFO - Epoch 469/800 done.
2025-03-06 19:46:38,099 - INFO - Final validation performance:
Loss: 0.213, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:46:38,099 - INFO - Beginning epoch 470/800
2025-03-06 19:46:38,106 - INFO - training batch 1, loss: 0.217, 32/60000 datapoints
2025-03-06 19:46:38,305 - INFO - training batch 51, loss: 0.222, 1632/60000 datapoints
2025-03-06 19:46:38,505 - INFO - training batch 101, loss: 0.192, 3232/60000 datapoints
2025-03-06 19:46:38,711 - INFO - training batch 151, loss: 0.176, 4832/60000 datapoints
2025-03-06 19:46:38,907 - INFO - training batch 201, loss: 0.113, 6432/60000 datapoints
2025-03-06 19:46:39,115 - INFO - training batch 251, loss: 0.065, 8032/60000 datapoints
2025-03-06 19:46:39,312 - INFO - training batch 301, loss: 0.280, 9632/60000 datapoints
2025-03-06 19:46:39,513 - INFO - training batch 351, loss: 0.173, 11232/60000 datapoints
2025-03-06 19:46:39,711 - INFO - training batch 401, loss: 0.119, 12832/60000 datapoints
2025-03-06 19:46:39,905 - INFO - training batch 451, loss: 0.278, 14432/60000 datapoints
2025-03-06 19:46:40,103 - INFO - training batch 501, loss: 0.343, 16032/60000 datapoints
2025-03-06 19:46:40,303 - INFO - training batch 551, loss: 0.078, 17632/60000 datapoints
2025-03-06 19:46:40,499 - INFO - training batch 601, loss: 0.215, 19232/60000 datapoints
2025-03-06 19:46:40,694 - INFO - training batch 651, loss: 0.511, 20832/60000 datapoints
2025-03-06 19:46:40,886 - INFO - training batch 701, loss: 0.251, 22432/60000 datapoints
2025-03-06 19:46:41,083 - INFO - training batch 751, loss: 0.549, 24032/60000 datapoints
2025-03-06 19:46:41,283 - INFO - training batch 801, loss: 0.218, 25632/60000 datapoints
2025-03-06 19:46:41,479 - INFO - training batch 851, loss: 0.290, 27232/60000 datapoints
2025-03-06 19:46:41,679 - INFO - training batch 901, loss: 0.431, 28832/60000 datapoints
2025-03-06 19:46:41,876 - INFO - training batch 951, loss: 0.369, 30432/60000 datapoints
2025-03-06 19:46:42,071 - INFO - training batch 1001, loss: 0.158, 32032/60000 datapoints
2025-03-06 19:46:42,273 - INFO - training batch 1051, loss: 0.154, 33632/60000 datapoints
2025-03-06 19:46:42,467 - INFO - training batch 1101, loss: 0.139, 35232/60000 datapoints
2025-03-06 19:46:42,664 - INFO - training batch 1151, loss: 0.297, 36832/60000 datapoints
2025-03-06 19:46:42,860 - INFO - training batch 1201, loss: 0.040, 38432/60000 datapoints
2025-03-06 19:46:43,051 - INFO - training batch 1251, loss: 0.719, 40032/60000 datapoints
2025-03-06 19:46:43,248 - INFO - training batch 1301, loss: 0.268, 41632/60000 datapoints
2025-03-06 19:46:43,443 - INFO - training batch 1351, loss: 0.292, 43232/60000 datapoints
2025-03-06 19:46:43,643 - INFO - training batch 1401, loss: 0.309, 44832/60000 datapoints
2025-03-06 19:46:43,840 - INFO - training batch 1451, loss: 0.401, 46432/60000 datapoints
2025-03-06 19:46:44,038 - INFO - training batch 1501, loss: 0.117, 48032/60000 datapoints
2025-03-06 19:46:44,234 - INFO - training batch 1551, loss: 0.406, 49632/60000 datapoints
2025-03-06 19:46:44,451 - INFO - training batch 1601, loss: 0.180, 51232/60000 datapoints
2025-03-06 19:46:44,649 - INFO - training batch 1651, loss: 0.456, 52832/60000 datapoints
2025-03-06 19:46:44,842 - INFO - training batch 1701, loss: 0.124, 54432/60000 datapoints
2025-03-06 19:46:45,040 - INFO - training batch 1751, loss: 0.319, 56032/60000 datapoints
2025-03-06 19:46:45,235 - INFO - training batch 1801, loss: 0.330, 57632/60000 datapoints
2025-03-06 19:46:45,430 - INFO - training batch 1851, loss: 0.148, 59232/60000 datapoints
2025-03-06 19:46:45,531 - INFO - validation batch 1, loss: 0.256, 32/10016 datapoints
2025-03-06 19:46:45,689 - INFO - validation batch 51, loss: 0.171, 1632/10016 datapoints
2025-03-06 19:46:45,839 - INFO - validation batch 101, loss: 0.219, 3232/10016 datapoints
2025-03-06 19:46:45,990 - INFO - validation batch 151, loss: 0.232, 4832/10016 datapoints
2025-03-06 19:46:46,146 - INFO - validation batch 201, loss: 0.327, 6432/10016 datapoints
2025-03-06 19:46:46,299 - INFO - validation batch 251, loss: 0.224, 8032/10016 datapoints
2025-03-06 19:46:46,451 - INFO - validation batch 301, loss: 0.169, 9632/10016 datapoints
2025-03-06 19:46:46,490 - INFO - Epoch 470/800 done.
2025-03-06 19:46:46,490 - INFO - Final validation performance:
Loss: 0.228, top-1 acc: 0.924top-5 acc: 0.924
2025-03-06 19:46:46,491 - INFO - Beginning epoch 471/800
2025-03-06 19:46:46,497 - INFO - training batch 1, loss: 0.330, 32/60000 datapoints
2025-03-06 19:46:46,700 - INFO - training batch 51, loss: 0.502, 1632/60000 datapoints
2025-03-06 19:46:46,896 - INFO - training batch 101, loss: 0.231, 3232/60000 datapoints
2025-03-06 19:46:47,116 - INFO - training batch 151, loss: 0.245, 4832/60000 datapoints
2025-03-06 19:46:47,309 - INFO - training batch 201, loss: 0.452, 6432/60000 datapoints
2025-03-06 19:46:47,502 - INFO - training batch 251, loss: 0.374, 8032/60000 datapoints
2025-03-06 19:46:47,706 - INFO - training batch 301, loss: 0.372, 9632/60000 datapoints
2025-03-06 19:46:47,919 - INFO - training batch 351, loss: 0.080, 11232/60000 datapoints
2025-03-06 19:46:48,126 - INFO - training batch 401, loss: 0.145, 12832/60000 datapoints
2025-03-06 19:46:48,323 - INFO - training batch 451, loss: 0.368, 14432/60000 datapoints
2025-03-06 19:46:48,517 - INFO - training batch 501, loss: 0.160, 16032/60000 datapoints
2025-03-06 19:46:48,720 - INFO - training batch 551, loss: 0.556, 17632/60000 datapoints
2025-03-06 19:46:48,916 - INFO - training batch 601, loss: 0.143, 19232/60000 datapoints
2025-03-06 19:46:49,113 - INFO - training batch 651, loss: 0.096, 20832/60000 datapoints
2025-03-06 19:46:49,309 - INFO - training batch 701, loss: 0.218, 22432/60000 datapoints
2025-03-06 19:46:49,507 - INFO - training batch 751, loss: 0.112, 24032/60000 datapoints
2025-03-06 19:46:49,707 - INFO - training batch 801, loss: 0.111, 25632/60000 datapoints
2025-03-06 19:46:49,905 - INFO - training batch 851, loss: 0.547, 27232/60000 datapoints
2025-03-06 19:46:50,099 - INFO - training batch 901, loss: 0.219, 28832/60000 datapoints
2025-03-06 19:46:50,296 - INFO - training batch 951, loss: 0.164, 30432/60000 datapoints
2025-03-06 19:46:50,492 - INFO - training batch 1001, loss: 0.359, 32032/60000 datapoints
2025-03-06 19:46:50,692 - INFO - training batch 1051, loss: 0.260, 33632/60000 datapoints
2025-03-06 19:46:50,887 - INFO - training batch 1101, loss: 0.196, 35232/60000 datapoints
2025-03-06 19:46:51,082 - INFO - training batch 1151, loss: 0.229, 36832/60000 datapoints
2025-03-06 19:46:51,280 - INFO - training batch 1201, loss: 0.601, 38432/60000 datapoints
2025-03-06 19:46:51,475 - INFO - training batch 1251, loss: 0.485, 40032/60000 datapoints
2025-03-06 19:46:51,673 - INFO - training batch 1301, loss: 0.281, 41632/60000 datapoints
2025-03-06 19:46:51,872 - INFO - training batch 1351, loss: 0.397, 43232/60000 datapoints
2025-03-06 19:46:52,063 - INFO - training batch 1401, loss: 0.206, 44832/60000 datapoints
2025-03-06 19:46:52,262 - INFO - training batch 1451, loss: 0.387, 46432/60000 datapoints
2025-03-06 19:46:52,456 - INFO - training batch 1501, loss: 0.218, 48032/60000 datapoints
2025-03-06 19:46:52,654 - INFO - training batch 1551, loss: 0.652, 49632/60000 datapoints
2025-03-06 19:46:52,849 - INFO - training batch 1601, loss: 0.372, 51232/60000 datapoints
2025-03-06 19:46:53,045 - INFO - training batch 1651, loss: 0.178, 52832/60000 datapoints
2025-03-06 19:46:53,239 - INFO - training batch 1701, loss: 0.338, 54432/60000 datapoints
2025-03-06 19:46:53,434 - INFO - training batch 1751, loss: 0.287, 56032/60000 datapoints
2025-03-06 19:46:53,633 - INFO - training batch 1801, loss: 0.353, 57632/60000 datapoints
2025-03-06 19:46:53,828 - INFO - training batch 1851, loss: 0.211, 59232/60000 datapoints
2025-03-06 19:46:53,929 - INFO - validation batch 1, loss: 0.258, 32/10016 datapoints
2025-03-06 19:46:54,082 - INFO - validation batch 51, loss: 0.239, 1632/10016 datapoints
2025-03-06 19:46:54,241 - INFO - validation batch 101, loss: 0.206, 3232/10016 datapoints
2025-03-06 19:46:54,398 - INFO - validation batch 151, loss: 0.118, 4832/10016 datapoints
2025-03-06 19:46:54,567 - INFO - validation batch 201, loss: 0.144, 6432/10016 datapoints
2025-03-06 19:46:54,723 - INFO - validation batch 251, loss: 0.132, 8032/10016 datapoints
2025-03-06 19:46:54,879 - INFO - validation batch 301, loss: 0.383, 9632/10016 datapoints
2025-03-06 19:46:54,920 - INFO - Epoch 471/800 done.
2025-03-06 19:46:54,920 - INFO - Final validation performance:
Loss: 0.211, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:46:54,921 - INFO - Beginning epoch 472/800
2025-03-06 19:46:54,927 - INFO - training batch 1, loss: 0.184, 32/60000 datapoints
2025-03-06 19:46:55,131 - INFO - training batch 51, loss: 0.365, 1632/60000 datapoints
2025-03-06 19:46:55,325 - INFO - training batch 101, loss: 0.057, 3232/60000 datapoints
2025-03-06 19:46:55,521 - INFO - training batch 151, loss: 0.317, 4832/60000 datapoints
2025-03-06 19:46:55,722 - INFO - training batch 201, loss: 0.384, 6432/60000 datapoints
2025-03-06 19:46:55,920 - INFO - training batch 251, loss: 0.398, 8032/60000 datapoints
2025-03-06 19:46:56,109 - INFO - training batch 301, loss: 0.440, 9632/60000 datapoints
2025-03-06 19:46:56,307 - INFO - training batch 351, loss: 0.784, 11232/60000 datapoints
2025-03-06 19:46:56,497 - INFO - training batch 401, loss: 0.224, 12832/60000 datapoints
2025-03-06 19:46:56,691 - INFO - training batch 451, loss: 0.190, 14432/60000 datapoints
2025-03-06 19:46:56,883 - INFO - training batch 501, loss: 0.395, 16032/60000 datapoints
2025-03-06 19:46:57,079 - INFO - training batch 551, loss: 0.113, 17632/60000 datapoints
2025-03-06 19:46:57,273 - INFO - training batch 601, loss: 0.245, 19232/60000 datapoints
2025-03-06 19:46:57,464 - INFO - training batch 651, loss: 0.380, 20832/60000 datapoints
2025-03-06 19:46:57,657 - INFO - training batch 701, loss: 0.309, 22432/60000 datapoints
2025-03-06 19:46:57,850 - INFO - training batch 751, loss: 0.217, 24032/60000 datapoints
2025-03-06 19:46:58,042 - INFO - training batch 801, loss: 0.320, 25632/60000 datapoints
2025-03-06 19:46:58,233 - INFO - training batch 851, loss: 0.142, 27232/60000 datapoints
2025-03-06 19:46:58,423 - INFO - training batch 901, loss: 0.254, 28832/60000 datapoints
2025-03-06 19:46:58,618 - INFO - training batch 951, loss: 0.230, 30432/60000 datapoints
2025-03-06 19:46:58,814 - INFO - training batch 1001, loss: 0.251, 32032/60000 datapoints
2025-03-06 19:46:59,011 - INFO - training batch 1051, loss: 0.077, 33632/60000 datapoints
2025-03-06 19:46:59,206 - INFO - training batch 1101, loss: 0.568, 35232/60000 datapoints
2025-03-06 19:46:59,399 - INFO - training batch 1151, loss: 0.198, 36832/60000 datapoints
2025-03-06 19:46:59,597 - INFO - training batch 1201, loss: 0.260, 38432/60000 datapoints
2025-03-06 19:46:59,792 - INFO - training batch 1251, loss: 0.234, 40032/60000 datapoints
2025-03-06 19:46:59,987 - INFO - training batch 1301, loss: 0.178, 41632/60000 datapoints
2025-03-06 19:47:00,183 - INFO - training batch 1351, loss: 0.368, 43232/60000 datapoints
2025-03-06 19:47:00,377 - INFO - training batch 1401, loss: 0.063, 44832/60000 datapoints
2025-03-06 19:47:00,570 - INFO - training batch 1451, loss: 0.358, 46432/60000 datapoints
2025-03-06 19:47:00,765 - INFO - training batch 1501, loss: 0.150, 48032/60000 datapoints
2025-03-06 19:47:00,960 - INFO - training batch 1551, loss: 0.279, 49632/60000 datapoints
2025-03-06 19:47:01,154 - INFO - training batch 1601, loss: 0.599, 51232/60000 datapoints
2025-03-06 19:47:01,349 - INFO - training batch 1651, loss: 0.216, 52832/60000 datapoints
2025-03-06 19:47:01,545 - INFO - training batch 1701, loss: 0.271, 54432/60000 datapoints
2025-03-06 19:47:01,741 - INFO - training batch 1751, loss: 0.497, 56032/60000 datapoints
2025-03-06 19:47:01,936 - INFO - training batch 1801, loss: 0.544, 57632/60000 datapoints
2025-03-06 19:47:02,126 - INFO - training batch 1851, loss: 0.091, 59232/60000 datapoints
2025-03-06 19:47:02,227 - INFO - validation batch 1, loss: 0.285, 32/10016 datapoints
2025-03-06 19:47:02,377 - INFO - validation batch 51, loss: 0.353, 1632/10016 datapoints
2025-03-06 19:47:02,528 - INFO - validation batch 101, loss: 0.072, 3232/10016 datapoints
2025-03-06 19:47:02,678 - INFO - validation batch 151, loss: 0.305, 4832/10016 datapoints
2025-03-06 19:47:02,833 - INFO - validation batch 201, loss: 0.250, 6432/10016 datapoints
2025-03-06 19:47:02,985 - INFO - validation batch 251, loss: 0.130, 8032/10016 datapoints
2025-03-06 19:47:03,136 - INFO - validation batch 301, loss: 0.371, 9632/10016 datapoints
2025-03-06 19:47:03,171 - INFO - Epoch 472/800 done.
2025-03-06 19:47:03,171 - INFO - Final validation performance:
Loss: 0.252, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:47:03,172 - INFO - Beginning epoch 473/800
2025-03-06 19:47:03,178 - INFO - training batch 1, loss: 0.179, 32/60000 datapoints
2025-03-06 19:47:03,372 - INFO - training batch 51, loss: 0.309, 1632/60000 datapoints
2025-03-06 19:47:03,567 - INFO - training batch 101, loss: 0.296, 3232/60000 datapoints
2025-03-06 19:47:03,773 - INFO - training batch 151, loss: 0.220, 4832/60000 datapoints
2025-03-06 19:47:03,972 - INFO - training batch 201, loss: 0.191, 6432/60000 datapoints
2025-03-06 19:47:04,167 - INFO - training batch 251, loss: 0.089, 8032/60000 datapoints
2025-03-06 19:47:04,370 - INFO - training batch 301, loss: 0.207, 9632/60000 datapoints
2025-03-06 19:47:04,572 - INFO - training batch 351, loss: 0.246, 11232/60000 datapoints
2025-03-06 19:47:04,783 - INFO - training batch 401, loss: 0.262, 12832/60000 datapoints
2025-03-06 19:47:04,978 - INFO - training batch 451, loss: 0.277, 14432/60000 datapoints
2025-03-06 19:47:05,172 - INFO - training batch 501, loss: 0.047, 16032/60000 datapoints
2025-03-06 19:47:05,364 - INFO - training batch 551, loss: 0.704, 17632/60000 datapoints
2025-03-06 19:47:05,554 - INFO - training batch 601, loss: 0.153, 19232/60000 datapoints
2025-03-06 19:47:05,751 - INFO - training batch 651, loss: 0.542, 20832/60000 datapoints
2025-03-06 19:47:05,942 - INFO - training batch 701, loss: 0.260, 22432/60000 datapoints
2025-03-06 19:47:06,134 - INFO - training batch 751, loss: 0.154, 24032/60000 datapoints
2025-03-06 19:47:06,329 - INFO - training batch 801, loss: 0.122, 25632/60000 datapoints
2025-03-06 19:47:06,523 - INFO - training batch 851, loss: 0.228, 27232/60000 datapoints
2025-03-06 19:47:06,718 - INFO - training batch 901, loss: 0.108, 28832/60000 datapoints
2025-03-06 19:47:06,910 - INFO - training batch 951, loss: 0.296, 30432/60000 datapoints
2025-03-06 19:47:07,106 - INFO - training batch 1001, loss: 0.431, 32032/60000 datapoints
2025-03-06 19:47:07,299 - INFO - training batch 1051, loss: 0.266, 33632/60000 datapoints
2025-03-06 19:47:07,490 - INFO - training batch 1101, loss: 0.547, 35232/60000 datapoints
2025-03-06 19:47:07,687 - INFO - training batch 1151, loss: 0.568, 36832/60000 datapoints
2025-03-06 19:47:07,879 - INFO - training batch 1201, loss: 0.471, 38432/60000 datapoints
2025-03-06 19:47:08,072 - INFO - training batch 1251, loss: 0.157, 40032/60000 datapoints
2025-03-06 19:47:08,272 - INFO - training batch 1301, loss: 0.264, 41632/60000 datapoints
2025-03-06 19:47:08,464 - INFO - training batch 1351, loss: 0.390, 43232/60000 datapoints
2025-03-06 19:47:08,660 - INFO - training batch 1401, loss: 0.302, 44832/60000 datapoints
2025-03-06 19:47:08,854 - INFO - training batch 1451, loss: 0.359, 46432/60000 datapoints
2025-03-06 19:47:09,047 - INFO - training batch 1501, loss: 0.118, 48032/60000 datapoints
2025-03-06 19:47:09,268 - INFO - training batch 1551, loss: 0.242, 49632/60000 datapoints
2025-03-06 19:47:09,464 - INFO - training batch 1601, loss: 0.159, 51232/60000 datapoints
2025-03-06 19:47:09,658 - INFO - training batch 1651, loss: 0.191, 52832/60000 datapoints
2025-03-06 19:47:09,851 - INFO - training batch 1701, loss: 0.240, 54432/60000 datapoints
2025-03-06 19:47:10,043 - INFO - training batch 1751, loss: 0.475, 56032/60000 datapoints
2025-03-06 19:47:10,238 - INFO - training batch 1801, loss: 0.265, 57632/60000 datapoints
2025-03-06 19:47:10,431 - INFO - training batch 1851, loss: 0.236, 59232/60000 datapoints
2025-03-06 19:47:10,527 - INFO - validation batch 1, loss: 0.231, 32/10016 datapoints
2025-03-06 19:47:10,680 - INFO - validation batch 51, loss: 0.519, 1632/10016 datapoints
2025-03-06 19:47:10,832 - INFO - validation batch 101, loss: 0.303, 3232/10016 datapoints
2025-03-06 19:47:10,984 - INFO - validation batch 151, loss: 0.173, 4832/10016 datapoints
2025-03-06 19:47:11,134 - INFO - validation batch 201, loss: 0.143, 6432/10016 datapoints
2025-03-06 19:47:11,284 - INFO - validation batch 251, loss: 0.141, 8032/10016 datapoints
2025-03-06 19:47:11,436 - INFO - validation batch 301, loss: 0.174, 9632/10016 datapoints
2025-03-06 19:47:11,472 - INFO - Epoch 473/800 done.
2025-03-06 19:47:11,473 - INFO - Final validation performance:
Loss: 0.240, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:47:11,473 - INFO - Beginning epoch 474/800
2025-03-06 19:47:11,479 - INFO - training batch 1, loss: 0.257, 32/60000 datapoints
2025-03-06 19:47:11,676 - INFO - training batch 51, loss: 0.310, 1632/60000 datapoints
2025-03-06 19:47:11,870 - INFO - training batch 101, loss: 0.112, 3232/60000 datapoints
2025-03-06 19:47:12,078 - INFO - training batch 151, loss: 0.244, 4832/60000 datapoints
2025-03-06 19:47:12,275 - INFO - training batch 201, loss: 0.159, 6432/60000 datapoints
2025-03-06 19:47:12,469 - INFO - training batch 251, loss: 0.194, 8032/60000 datapoints
2025-03-06 19:47:12,674 - INFO - training batch 301, loss: 0.113, 9632/60000 datapoints
2025-03-06 19:47:12,869 - INFO - training batch 351, loss: 0.272, 11232/60000 datapoints
2025-03-06 19:47:13,066 - INFO - training batch 401, loss: 0.386, 12832/60000 datapoints
2025-03-06 19:47:13,259 - INFO - training batch 451, loss: 0.193, 14432/60000 datapoints
2025-03-06 19:47:13,459 - INFO - training batch 501, loss: 0.109, 16032/60000 datapoints
2025-03-06 19:47:13,654 - INFO - training batch 551, loss: 0.212, 17632/60000 datapoints
2025-03-06 19:47:13,848 - INFO - training batch 601, loss: 0.107, 19232/60000 datapoints
2025-03-06 19:47:14,045 - INFO - training batch 651, loss: 0.765, 20832/60000 datapoints
2025-03-06 19:47:14,241 - INFO - training batch 701, loss: 0.156, 22432/60000 datapoints
2025-03-06 19:47:14,439 - INFO - training batch 751, loss: 0.155, 24032/60000 datapoints
2025-03-06 19:47:14,646 - INFO - training batch 801, loss: 0.154, 25632/60000 datapoints
2025-03-06 19:47:14,854 - INFO - training batch 851, loss: 0.410, 27232/60000 datapoints
2025-03-06 19:47:15,054 - INFO - training batch 901, loss: 0.336, 28832/60000 datapoints
2025-03-06 19:47:15,248 - INFO - training batch 951, loss: 0.191, 30432/60000 datapoints
2025-03-06 19:47:15,445 - INFO - training batch 1001, loss: 0.397, 32032/60000 datapoints
2025-03-06 19:47:15,642 - INFO - training batch 1051, loss: 0.164, 33632/60000 datapoints
2025-03-06 19:47:15,839 - INFO - training batch 1101, loss: 0.297, 35232/60000 datapoints
2025-03-06 19:47:16,037 - INFO - training batch 1151, loss: 0.255, 36832/60000 datapoints
2025-03-06 19:47:16,230 - INFO - training batch 1201, loss: 0.223, 38432/60000 datapoints
2025-03-06 19:47:16,431 - INFO - training batch 1251, loss: 0.329, 40032/60000 datapoints
2025-03-06 19:47:16,628 - INFO - training batch 1301, loss: 0.182, 41632/60000 datapoints
2025-03-06 19:47:16,825 - INFO - training batch 1351, loss: 0.199, 43232/60000 datapoints
2025-03-06 19:47:17,021 - INFO - training batch 1401, loss: 0.239, 44832/60000 datapoints
2025-03-06 19:47:17,217 - INFO - training batch 1451, loss: 0.122, 46432/60000 datapoints
2025-03-06 19:47:17,412 - INFO - training batch 1501, loss: 0.448, 48032/60000 datapoints
2025-03-06 19:47:17,604 - INFO - training batch 1551, loss: 0.191, 49632/60000 datapoints
2025-03-06 19:47:17,799 - INFO - training batch 1601, loss: 0.272, 51232/60000 datapoints
2025-03-06 19:47:17,994 - INFO - training batch 1651, loss: 0.490, 52832/60000 datapoints
2025-03-06 19:47:18,188 - INFO - training batch 1701, loss: 0.226, 54432/60000 datapoints
2025-03-06 19:47:18,385 - INFO - training batch 1751, loss: 0.230, 56032/60000 datapoints
2025-03-06 19:47:18,579 - INFO - training batch 1801, loss: 0.301, 57632/60000 datapoints
2025-03-06 19:47:18,774 - INFO - training batch 1851, loss: 0.388, 59232/60000 datapoints
2025-03-06 19:47:18,873 - INFO - validation batch 1, loss: 0.344, 32/10016 datapoints
2025-03-06 19:47:19,026 - INFO - validation batch 51, loss: 0.305, 1632/10016 datapoints
2025-03-06 19:47:19,183 - INFO - validation batch 101, loss: 0.156, 3232/10016 datapoints
2025-03-06 19:47:19,336 - INFO - validation batch 151, loss: 0.438, 4832/10016 datapoints
2025-03-06 19:47:19,495 - INFO - validation batch 201, loss: 0.347, 6432/10016 datapoints
2025-03-06 19:47:19,652 - INFO - validation batch 251, loss: 0.189, 8032/10016 datapoints
2025-03-06 19:47:19,806 - INFO - validation batch 301, loss: 0.276, 9632/10016 datapoints
2025-03-06 19:47:19,845 - INFO - Epoch 474/800 done.
2025-03-06 19:47:19,845 - INFO - Final validation performance:
Loss: 0.294, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:47:19,845 - INFO - Beginning epoch 475/800
2025-03-06 19:47:19,852 - INFO - training batch 1, loss: 0.284, 32/60000 datapoints
2025-03-06 19:47:20,070 - INFO - training batch 51, loss: 0.302, 1632/60000 datapoints
2025-03-06 19:47:20,268 - INFO - training batch 101, loss: 0.255, 3232/60000 datapoints
2025-03-06 19:47:20,468 - INFO - training batch 151, loss: 0.187, 4832/60000 datapoints
2025-03-06 19:47:20,667 - INFO - training batch 201, loss: 0.069, 6432/60000 datapoints
2025-03-06 19:47:20,866 - INFO - training batch 251, loss: 0.282, 8032/60000 datapoints
2025-03-06 19:47:21,061 - INFO - training batch 301, loss: 0.141, 9632/60000 datapoints
2025-03-06 19:47:21,255 - INFO - training batch 351, loss: 0.470, 11232/60000 datapoints
2025-03-06 19:47:21,449 - INFO - training batch 401, loss: 0.446, 12832/60000 datapoints
2025-03-06 19:47:21,646 - INFO - training batch 451, loss: 0.478, 14432/60000 datapoints
2025-03-06 19:47:21,842 - INFO - training batch 501, loss: 0.467, 16032/60000 datapoints
2025-03-06 19:47:22,039 - INFO - training batch 551, loss: 0.106, 17632/60000 datapoints
2025-03-06 19:47:22,231 - INFO - training batch 601, loss: 0.238, 19232/60000 datapoints
2025-03-06 19:47:22,428 - INFO - training batch 651, loss: 0.109, 20832/60000 datapoints
2025-03-06 19:47:22,631 - INFO - training batch 701, loss: 0.163, 22432/60000 datapoints
2025-03-06 19:47:22,825 - INFO - training batch 751, loss: 0.165, 24032/60000 datapoints
2025-03-06 19:47:23,020 - INFO - training batch 801, loss: 0.238, 25632/60000 datapoints
2025-03-06 19:47:23,216 - INFO - training batch 851, loss: 0.128, 27232/60000 datapoints
2025-03-06 19:47:23,413 - INFO - training batch 901, loss: 0.224, 28832/60000 datapoints
2025-03-06 19:47:23,610 - INFO - training batch 951, loss: 0.375, 30432/60000 datapoints
2025-03-06 19:47:23,806 - INFO - training batch 1001, loss: 0.305, 32032/60000 datapoints
2025-03-06 19:47:24,002 - INFO - training batch 1051, loss: 0.133, 33632/60000 datapoints
2025-03-06 19:47:24,197 - INFO - training batch 1101, loss: 0.141, 35232/60000 datapoints
2025-03-06 19:47:24,396 - INFO - training batch 1151, loss: 0.173, 36832/60000 datapoints
2025-03-06 19:47:24,595 - INFO - training batch 1201, loss: 0.436, 38432/60000 datapoints
2025-03-06 19:47:24,811 - INFO - training batch 1251, loss: 0.630, 40032/60000 datapoints
2025-03-06 19:47:25,013 - INFO - training batch 1301, loss: 0.178, 41632/60000 datapoints
2025-03-06 19:47:25,209 - INFO - training batch 1351, loss: 0.152, 43232/60000 datapoints
2025-03-06 19:47:25,405 - INFO - training batch 1401, loss: 0.135, 44832/60000 datapoints
2025-03-06 19:47:25,601 - INFO - training batch 1451, loss: 0.201, 46432/60000 datapoints
2025-03-06 19:47:25,798 - INFO - training batch 1501, loss: 0.250, 48032/60000 datapoints
2025-03-06 19:47:25,994 - INFO - training batch 1551, loss: 0.764, 49632/60000 datapoints
2025-03-06 19:47:26,190 - INFO - training batch 1601, loss: 0.379, 51232/60000 datapoints
2025-03-06 19:47:26,388 - INFO - training batch 1651, loss: 0.258, 52832/60000 datapoints
2025-03-06 19:47:26,582 - INFO - training batch 1701, loss: 0.222, 54432/60000 datapoints
2025-03-06 19:47:26,781 - INFO - training batch 1751, loss: 0.548, 56032/60000 datapoints
2025-03-06 19:47:26,975 - INFO - training batch 1801, loss: 0.249, 57632/60000 datapoints
2025-03-06 19:47:27,173 - INFO - training batch 1851, loss: 0.092, 59232/60000 datapoints
2025-03-06 19:47:27,275 - INFO - validation batch 1, loss: 0.303, 32/10016 datapoints
2025-03-06 19:47:27,429 - INFO - validation batch 51, loss: 0.258, 1632/10016 datapoints
2025-03-06 19:47:27,583 - INFO - validation batch 101, loss: 0.096, 3232/10016 datapoints
2025-03-06 19:47:27,737 - INFO - validation batch 151, loss: 0.324, 4832/10016 datapoints
2025-03-06 19:47:27,891 - INFO - validation batch 201, loss: 0.377, 6432/10016 datapoints
2025-03-06 19:47:28,040 - INFO - validation batch 251, loss: 0.366, 8032/10016 datapoints
2025-03-06 19:47:28,192 - INFO - validation batch 301, loss: 0.245, 9632/10016 datapoints
2025-03-06 19:47:28,229 - INFO - Epoch 475/800 done.
2025-03-06 19:47:28,229 - INFO - Final validation performance:
Loss: 0.281, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:47:28,230 - INFO - Beginning epoch 476/800
2025-03-06 19:47:28,236 - INFO - training batch 1, loss: 0.629, 32/60000 datapoints
2025-03-06 19:47:28,434 - INFO - training batch 51, loss: 0.261, 1632/60000 datapoints
2025-03-06 19:47:28,633 - INFO - training batch 101, loss: 0.219, 3232/60000 datapoints
2025-03-06 19:47:28,842 - INFO - training batch 151, loss: 0.356, 4832/60000 datapoints
2025-03-06 19:47:29,040 - INFO - training batch 201, loss: 0.469, 6432/60000 datapoints
2025-03-06 19:47:29,238 - INFO - training batch 251, loss: 0.263, 8032/60000 datapoints
2025-03-06 19:47:29,445 - INFO - training batch 301, loss: 0.529, 9632/60000 datapoints
2025-03-06 19:47:29,644 - INFO - training batch 351, loss: 0.183, 11232/60000 datapoints
2025-03-06 19:47:29,843 - INFO - training batch 401, loss: 0.441, 12832/60000 datapoints
2025-03-06 19:47:30,041 - INFO - training batch 451, loss: 0.274, 14432/60000 datapoints
2025-03-06 19:47:30,235 - INFO - training batch 501, loss: 0.135, 16032/60000 datapoints
2025-03-06 19:47:30,434 - INFO - training batch 551, loss: 0.096, 17632/60000 datapoints
2025-03-06 19:47:30,633 - INFO - training batch 601, loss: 0.351, 19232/60000 datapoints
2025-03-06 19:47:30,829 - INFO - training batch 651, loss: 0.218, 20832/60000 datapoints
2025-03-06 19:47:31,023 - INFO - training batch 701, loss: 0.348, 22432/60000 datapoints
2025-03-06 19:47:31,215 - INFO - training batch 751, loss: 0.198, 24032/60000 datapoints
2025-03-06 19:47:31,413 - INFO - training batch 801, loss: 0.365, 25632/60000 datapoints
2025-03-06 19:47:31,609 - INFO - training batch 851, loss: 0.180, 27232/60000 datapoints
2025-03-06 19:47:31,808 - INFO - training batch 901, loss: 0.236, 28832/60000 datapoints
2025-03-06 19:47:32,008 - INFO - training batch 951, loss: 0.215, 30432/60000 datapoints
2025-03-06 19:47:32,203 - INFO - training batch 1001, loss: 0.245, 32032/60000 datapoints
2025-03-06 19:47:32,399 - INFO - training batch 1051, loss: 0.333, 33632/60000 datapoints
2025-03-06 19:47:32,595 - INFO - training batch 1101, loss: 0.249, 35232/60000 datapoints
2025-03-06 19:47:32,792 - INFO - training batch 1151, loss: 0.208, 36832/60000 datapoints
2025-03-06 19:47:32,985 - INFO - training batch 1201, loss: 0.105, 38432/60000 datapoints
2025-03-06 19:47:33,181 - INFO - training batch 1251, loss: 0.202, 40032/60000 datapoints
2025-03-06 19:47:33,375 - INFO - training batch 1301, loss: 0.334, 41632/60000 datapoints
2025-03-06 19:47:33,570 - INFO - training batch 1351, loss: 0.223, 43232/60000 datapoints
2025-03-06 19:47:33,767 - INFO - training batch 1401, loss: 0.114, 44832/60000 datapoints
2025-03-06 19:47:33,962 - INFO - training batch 1451, loss: 0.240, 46432/60000 datapoints
2025-03-06 19:47:34,161 - INFO - training batch 1501, loss: 0.249, 48032/60000 datapoints
2025-03-06 19:47:34,359 - INFO - training batch 1551, loss: 0.351, 49632/60000 datapoints
2025-03-06 19:47:34,558 - INFO - training batch 1601, loss: 0.178, 51232/60000 datapoints
2025-03-06 19:47:34,758 - INFO - training batch 1651, loss: 0.153, 52832/60000 datapoints
2025-03-06 19:47:34,977 - INFO - training batch 1701, loss: 0.724, 54432/60000 datapoints
2025-03-06 19:47:35,173 - INFO - training batch 1751, loss: 0.222, 56032/60000 datapoints
2025-03-06 19:47:35,367 - INFO - training batch 1801, loss: 0.270, 57632/60000 datapoints
2025-03-06 19:47:35,561 - INFO - training batch 1851, loss: 0.046, 59232/60000 datapoints
2025-03-06 19:47:35,673 - INFO - validation batch 1, loss: 0.157, 32/10016 datapoints
2025-03-06 19:47:35,823 - INFO - validation batch 51, loss: 0.327, 1632/10016 datapoints
2025-03-06 19:47:35,975 - INFO - validation batch 101, loss: 0.196, 3232/10016 datapoints
2025-03-06 19:47:36,131 - INFO - validation batch 151, loss: 0.171, 4832/10016 datapoints
2025-03-06 19:47:36,284 - INFO - validation batch 201, loss: 0.173, 6432/10016 datapoints
2025-03-06 19:47:36,438 - INFO - validation batch 251, loss: 0.224, 8032/10016 datapoints
2025-03-06 19:47:36,590 - INFO - validation batch 301, loss: 0.198, 9632/10016 datapoints
2025-03-06 19:47:36,636 - INFO - Epoch 476/800 done.
2025-03-06 19:47:36,636 - INFO - Final validation performance:
Loss: 0.207, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:47:36,637 - INFO - Beginning epoch 477/800
2025-03-06 19:47:36,646 - INFO - training batch 1, loss: 0.414, 32/60000 datapoints
2025-03-06 19:47:36,873 - INFO - training batch 51, loss: 0.291, 1632/60000 datapoints
2025-03-06 19:47:37,067 - INFO - training batch 101, loss: 0.178, 3232/60000 datapoints
2025-03-06 19:47:37,266 - INFO - training batch 151, loss: 0.354, 4832/60000 datapoints
2025-03-06 19:47:37,466 - INFO - training batch 201, loss: 0.380, 6432/60000 datapoints
2025-03-06 19:47:37,690 - INFO - training batch 251, loss: 0.172, 8032/60000 datapoints
2025-03-06 19:47:37,887 - INFO - training batch 301, loss: 0.273, 9632/60000 datapoints
2025-03-06 19:47:38,081 - INFO - training batch 351, loss: 0.381, 11232/60000 datapoints
2025-03-06 19:47:38,279 - INFO - training batch 401, loss: 0.245, 12832/60000 datapoints
2025-03-06 19:47:38,477 - INFO - training batch 451, loss: 0.126, 14432/60000 datapoints
2025-03-06 19:47:38,678 - INFO - training batch 501, loss: 0.298, 16032/60000 datapoints
2025-03-06 19:47:38,872 - INFO - training batch 551, loss: 0.188, 17632/60000 datapoints
2025-03-06 19:47:39,069 - INFO - training batch 601, loss: 0.228, 19232/60000 datapoints
2025-03-06 19:47:39,266 - INFO - training batch 651, loss: 0.163, 20832/60000 datapoints
2025-03-06 19:47:39,464 - INFO - training batch 701, loss: 0.141, 22432/60000 datapoints
2025-03-06 19:47:39,662 - INFO - training batch 751, loss: 0.274, 24032/60000 datapoints
2025-03-06 19:47:39,857 - INFO - training batch 801, loss: 0.139, 25632/60000 datapoints
2025-03-06 19:47:40,052 - INFO - training batch 851, loss: 0.114, 27232/60000 datapoints
2025-03-06 19:47:40,244 - INFO - training batch 901, loss: 0.401, 28832/60000 datapoints
2025-03-06 19:47:40,441 - INFO - training batch 951, loss: 0.281, 30432/60000 datapoints
2025-03-06 19:47:40,640 - INFO - training batch 1001, loss: 0.240, 32032/60000 datapoints
2025-03-06 19:47:40,836 - INFO - training batch 1051, loss: 0.520, 33632/60000 datapoints
2025-03-06 19:47:41,041 - INFO - training batch 1101, loss: 0.143, 35232/60000 datapoints
2025-03-06 19:47:41,235 - INFO - training batch 1151, loss: 0.196, 36832/60000 datapoints
2025-03-06 19:47:41,430 - INFO - training batch 1201, loss: 0.133, 38432/60000 datapoints
2025-03-06 19:47:41,627 - INFO - training batch 1251, loss: 0.163, 40032/60000 datapoints
2025-03-06 19:47:41,821 - INFO - training batch 1301, loss: 0.381, 41632/60000 datapoints
2025-03-06 19:47:42,018 - INFO - training batch 1351, loss: 0.136, 43232/60000 datapoints
2025-03-06 19:47:42,216 - INFO - training batch 1401, loss: 0.106, 44832/60000 datapoints
2025-03-06 19:47:42,412 - INFO - training batch 1451, loss: 0.329, 46432/60000 datapoints
2025-03-06 19:47:42,607 - INFO - training batch 1501, loss: 0.115, 48032/60000 datapoints
2025-03-06 19:47:42,804 - INFO - training batch 1551, loss: 0.246, 49632/60000 datapoints
2025-03-06 19:47:42,999 - INFO - training batch 1601, loss: 0.186, 51232/60000 datapoints
2025-03-06 19:47:43,195 - INFO - training batch 1651, loss: 0.257, 52832/60000 datapoints
2025-03-06 19:47:43,388 - INFO - training batch 1701, loss: 0.384, 54432/60000 datapoints
2025-03-06 19:47:43,582 - INFO - training batch 1751, loss: 0.831, 56032/60000 datapoints
2025-03-06 19:47:43,780 - INFO - training batch 1801, loss: 0.337, 57632/60000 datapoints
2025-03-06 19:47:43,976 - INFO - training batch 1851, loss: 0.349, 59232/60000 datapoints
2025-03-06 19:47:44,078 - INFO - validation batch 1, loss: 0.106, 32/10016 datapoints
2025-03-06 19:47:44,244 - INFO - validation batch 51, loss: 0.467, 1632/10016 datapoints
2025-03-06 19:47:44,421 - INFO - validation batch 101, loss: 0.166, 3232/10016 datapoints
2025-03-06 19:47:44,598 - INFO - validation batch 151, loss: 0.126, 4832/10016 datapoints
2025-03-06 19:47:44,779 - INFO - validation batch 201, loss: 0.242, 6432/10016 datapoints
2025-03-06 19:47:44,958 - INFO - validation batch 251, loss: 0.165, 8032/10016 datapoints
2025-03-06 19:47:45,152 - INFO - validation batch 301, loss: 0.773, 9632/10016 datapoints
2025-03-06 19:47:45,199 - INFO - Epoch 477/800 done.
2025-03-06 19:47:45,200 - INFO - Final validation performance:
Loss: 0.292, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:47:45,205 - INFO - Beginning epoch 478/800
2025-03-06 19:47:45,234 - INFO - training batch 1, loss: 0.231, 32/60000 datapoints
2025-03-06 19:47:45,552 - INFO - training batch 51, loss: 0.095, 1632/60000 datapoints
2025-03-06 19:47:45,841 - INFO - training batch 101, loss: 0.400, 3232/60000 datapoints
2025-03-06 19:47:46,118 - INFO - training batch 151, loss: 0.173, 4832/60000 datapoints
2025-03-06 19:47:46,416 - INFO - training batch 201, loss: 0.113, 6432/60000 datapoints
2025-03-06 19:47:46,616 - INFO - training batch 251, loss: 0.628, 8032/60000 datapoints
2025-03-06 19:47:46,816 - INFO - training batch 301, loss: 0.124, 9632/60000 datapoints
2025-03-06 19:47:47,012 - INFO - training batch 351, loss: 0.181, 11232/60000 datapoints
2025-03-06 19:47:47,208 - INFO - training batch 401, loss: 0.145, 12832/60000 datapoints
2025-03-06 19:47:47,406 - INFO - training batch 451, loss: 0.080, 14432/60000 datapoints
2025-03-06 19:47:47,601 - INFO - training batch 501, loss: 0.332, 16032/60000 datapoints
2025-03-06 19:47:47,801 - INFO - training batch 551, loss: 0.190, 17632/60000 datapoints
2025-03-06 19:47:48,051 - INFO - training batch 601, loss: 0.475, 19232/60000 datapoints
2025-03-06 19:47:48,350 - INFO - training batch 651, loss: 0.345, 20832/60000 datapoints
2025-03-06 19:47:48,621 - INFO - training batch 701, loss: 0.170, 22432/60000 datapoints
2025-03-06 19:47:48,899 - INFO - training batch 751, loss: 0.316, 24032/60000 datapoints
2025-03-06 19:47:49,226 - INFO - training batch 801, loss: 0.511, 25632/60000 datapoints
2025-03-06 19:47:49,537 - INFO - training batch 851, loss: 0.182, 27232/60000 datapoints
2025-03-06 19:47:49,971 - INFO - training batch 901, loss: 0.179, 28832/60000 datapoints
2025-03-06 19:47:50,410 - INFO - training batch 951, loss: 0.146, 30432/60000 datapoints
2025-03-06 19:47:50,970 - INFO - training batch 1001, loss: 0.054, 32032/60000 datapoints
2025-03-06 19:47:51,563 - INFO - training batch 1051, loss: 0.432, 33632/60000 datapoints
2025-03-06 19:47:51,890 - INFO - training batch 1101, loss: 0.332, 35232/60000 datapoints
2025-03-06 19:47:52,163 - INFO - training batch 1151, loss: 0.185, 36832/60000 datapoints
2025-03-06 19:47:52,412 - INFO - training batch 1201, loss: 0.091, 38432/60000 datapoints
2025-03-06 19:47:52,706 - INFO - training batch 1251, loss: 0.153, 40032/60000 datapoints
2025-03-06 19:47:52,968 - INFO - training batch 1301, loss: 0.255, 41632/60000 datapoints
2025-03-06 19:47:53,230 - INFO - training batch 1351, loss: 0.278, 43232/60000 datapoints
2025-03-06 19:47:53,514 - INFO - training batch 1401, loss: 0.202, 44832/60000 datapoints
2025-03-06 19:47:53,820 - INFO - training batch 1451, loss: 0.281, 46432/60000 datapoints
2025-03-06 19:47:54,078 - INFO - training batch 1501, loss: 0.361, 48032/60000 datapoints
2025-03-06 19:47:54,294 - INFO - training batch 1551, loss: 0.213, 49632/60000 datapoints
2025-03-06 19:47:54,522 - INFO - training batch 1601, loss: 0.123, 51232/60000 datapoints
2025-03-06 19:47:54,732 - INFO - training batch 1651, loss: 0.214, 52832/60000 datapoints
2025-03-06 19:47:54,938 - INFO - training batch 1701, loss: 0.212, 54432/60000 datapoints
2025-03-06 19:47:55,164 - INFO - training batch 1751, loss: 0.137, 56032/60000 datapoints
2025-03-06 19:47:55,364 - INFO - training batch 1801, loss: 0.256, 57632/60000 datapoints
2025-03-06 19:47:55,562 - INFO - training batch 1851, loss: 0.192, 59232/60000 datapoints
2025-03-06 19:47:55,676 - INFO - validation batch 1, loss: 0.422, 32/10016 datapoints
2025-03-06 19:47:55,873 - INFO - validation batch 51, loss: 0.089, 1632/10016 datapoints
2025-03-06 19:47:56,035 - INFO - validation batch 101, loss: 0.164, 3232/10016 datapoints
2025-03-06 19:47:56,194 - INFO - validation batch 151, loss: 0.090, 4832/10016 datapoints
2025-03-06 19:47:56,348 - INFO - validation batch 201, loss: 0.172, 6432/10016 datapoints
2025-03-06 19:47:56,505 - INFO - validation batch 251, loss: 0.080, 8032/10016 datapoints
2025-03-06 19:47:56,658 - INFO - validation batch 301, loss: 0.448, 9632/10016 datapoints
2025-03-06 19:47:56,696 - INFO - Epoch 478/800 done.
2025-03-06 19:47:56,697 - INFO - Final validation performance:
Loss: 0.209, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:47:56,698 - INFO - Beginning epoch 479/800
2025-03-06 19:47:56,705 - INFO - training batch 1, loss: 0.204, 32/60000 datapoints
2025-03-06 19:47:56,931 - INFO - training batch 51, loss: 0.233, 1632/60000 datapoints
2025-03-06 19:47:57,124 - INFO - training batch 101, loss: 0.178, 3232/60000 datapoints
2025-03-06 19:47:57,333 - INFO - training batch 151, loss: 0.218, 4832/60000 datapoints
2025-03-06 19:47:57,533 - INFO - training batch 201, loss: 0.262, 6432/60000 datapoints
2025-03-06 19:47:57,739 - INFO - training batch 251, loss: 0.310, 8032/60000 datapoints
2025-03-06 19:47:57,940 - INFO - training batch 301, loss: 0.129, 9632/60000 datapoints
2025-03-06 19:47:58,138 - INFO - training batch 351, loss: 0.119, 11232/60000 datapoints
2025-03-06 19:47:58,334 - INFO - training batch 401, loss: 0.373, 12832/60000 datapoints
2025-03-06 19:47:58,534 - INFO - training batch 451, loss: 0.149, 14432/60000 datapoints
2025-03-06 19:47:58,733 - INFO - training batch 501, loss: 0.292, 16032/60000 datapoints
2025-03-06 19:47:58,928 - INFO - training batch 551, loss: 0.418, 17632/60000 datapoints
2025-03-06 19:47:59,126 - INFO - training batch 601, loss: 0.637, 19232/60000 datapoints
2025-03-06 19:47:59,323 - INFO - training batch 651, loss: 0.214, 20832/60000 datapoints
2025-03-06 19:47:59,518 - INFO - training batch 701, loss: 0.214, 22432/60000 datapoints
2025-03-06 19:47:59,716 - INFO - training batch 751, loss: 0.127, 24032/60000 datapoints
2025-03-06 19:47:59,912 - INFO - training batch 801, loss: 0.246, 25632/60000 datapoints
2025-03-06 19:48:00,107 - INFO - training batch 851, loss: 0.326, 27232/60000 datapoints
2025-03-06 19:48:00,304 - INFO - training batch 901, loss: 0.233, 28832/60000 datapoints
2025-03-06 19:48:00,502 - INFO - training batch 951, loss: 0.105, 30432/60000 datapoints
2025-03-06 19:48:00,697 - INFO - training batch 1001, loss: 0.330, 32032/60000 datapoints
2025-03-06 19:48:00,892 - INFO - training batch 1051, loss: 0.142, 33632/60000 datapoints
2025-03-06 19:48:01,085 - INFO - training batch 1101, loss: 0.181, 35232/60000 datapoints
2025-03-06 19:48:01,300 - INFO - training batch 1151, loss: 0.242, 36832/60000 datapoints
2025-03-06 19:48:01,497 - INFO - training batch 1201, loss: 0.442, 38432/60000 datapoints
2025-03-06 19:48:01,693 - INFO - training batch 1251, loss: 0.145, 40032/60000 datapoints
2025-03-06 19:48:01,887 - INFO - training batch 1301, loss: 0.395, 41632/60000 datapoints
2025-03-06 19:48:02,085 - INFO - training batch 1351, loss: 0.360, 43232/60000 datapoints
2025-03-06 19:48:02,278 - INFO - training batch 1401, loss: 0.339, 44832/60000 datapoints
2025-03-06 19:48:02,474 - INFO - training batch 1451, loss: 0.594, 46432/60000 datapoints
2025-03-06 19:48:02,671 - INFO - training batch 1501, loss: 0.167, 48032/60000 datapoints
2025-03-06 19:48:02,867 - INFO - training batch 1551, loss: 0.338, 49632/60000 datapoints
2025-03-06 19:48:03,062 - INFO - training batch 1601, loss: 0.233, 51232/60000 datapoints
2025-03-06 19:48:03,256 - INFO - training batch 1651, loss: 0.166, 52832/60000 datapoints
2025-03-06 19:48:03,449 - INFO - training batch 1701, loss: 0.397, 54432/60000 datapoints
2025-03-06 19:48:03,649 - INFO - training batch 1751, loss: 0.224, 56032/60000 datapoints
2025-03-06 19:48:03,864 - INFO - training batch 1801, loss: 0.329, 57632/60000 datapoints
2025-03-06 19:48:04,080 - INFO - training batch 1851, loss: 0.111, 59232/60000 datapoints
2025-03-06 19:48:04,219 - INFO - validation batch 1, loss: 0.214, 32/10016 datapoints
2025-03-06 19:48:04,374 - INFO - validation batch 51, loss: 0.155, 1632/10016 datapoints
2025-03-06 19:48:04,532 - INFO - validation batch 101, loss: 0.395, 3232/10016 datapoints
2025-03-06 19:48:04,689 - INFO - validation batch 151, loss: 0.196, 4832/10016 datapoints
2025-03-06 19:48:04,850 - INFO - validation batch 201, loss: 0.383, 6432/10016 datapoints
2025-03-06 19:48:05,004 - INFO - validation batch 251, loss: 0.099, 8032/10016 datapoints
2025-03-06 19:48:05,181 - INFO - validation batch 301, loss: 0.108, 9632/10016 datapoints
2025-03-06 19:48:05,218 - INFO - Epoch 479/800 done.
2025-03-06 19:48:05,218 - INFO - Final validation performance:
Loss: 0.221, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:48:05,219 - INFO - Beginning epoch 480/800
2025-03-06 19:48:05,227 - INFO - training batch 1, loss: 0.267, 32/60000 datapoints
2025-03-06 19:48:05,434 - INFO - training batch 51, loss: 0.321, 1632/60000 datapoints
2025-03-06 19:48:05,630 - INFO - training batch 101, loss: 0.318, 3232/60000 datapoints
2025-03-06 19:48:05,820 - INFO - training batch 151, loss: 0.100, 4832/60000 datapoints
2025-03-06 19:48:06,020 - INFO - training batch 201, loss: 0.233, 6432/60000 datapoints
2025-03-06 19:48:06,216 - INFO - training batch 251, loss: 0.303, 8032/60000 datapoints
2025-03-06 19:48:06,432 - INFO - training batch 301, loss: 0.276, 9632/60000 datapoints
2025-03-06 19:48:06,624 - INFO - training batch 351, loss: 0.182, 11232/60000 datapoints
2025-03-06 19:48:06,817 - INFO - training batch 401, loss: 0.195, 12832/60000 datapoints
2025-03-06 19:48:07,011 - INFO - training batch 451, loss: 0.221, 14432/60000 datapoints
2025-03-06 19:48:07,204 - INFO - training batch 501, loss: 0.210, 16032/60000 datapoints
2025-03-06 19:48:07,402 - INFO - training batch 551, loss: 0.332, 17632/60000 datapoints
2025-03-06 19:48:07,603 - INFO - training batch 601, loss: 0.133, 19232/60000 datapoints
2025-03-06 19:48:07,801 - INFO - training batch 651, loss: 0.373, 20832/60000 datapoints
2025-03-06 19:48:07,995 - INFO - training batch 701, loss: 0.116, 22432/60000 datapoints
2025-03-06 19:48:08,202 - INFO - training batch 751, loss: 0.200, 24032/60000 datapoints
2025-03-06 19:48:08,395 - INFO - training batch 801, loss: 0.542, 25632/60000 datapoints
2025-03-06 19:48:08,588 - INFO - training batch 851, loss: 0.150, 27232/60000 datapoints
2025-03-06 19:48:08,796 - INFO - training batch 901, loss: 0.323, 28832/60000 datapoints
2025-03-06 19:48:09,001 - INFO - training batch 951, loss: 0.092, 30432/60000 datapoints
2025-03-06 19:48:09,193 - INFO - training batch 1001, loss: 0.337, 32032/60000 datapoints
2025-03-06 19:48:09,385 - INFO - training batch 1051, loss: 0.148, 33632/60000 datapoints
2025-03-06 19:48:09,578 - INFO - training batch 1101, loss: 0.253, 35232/60000 datapoints
2025-03-06 19:48:09,773 - INFO - training batch 1151, loss: 0.350, 36832/60000 datapoints
2025-03-06 19:48:09,992 - INFO - training batch 1201, loss: 0.223, 38432/60000 datapoints
2025-03-06 19:48:10,189 - INFO - training batch 1251, loss: 0.203, 40032/60000 datapoints
2025-03-06 19:48:10,380 - INFO - training batch 1301, loss: 0.247, 41632/60000 datapoints
2025-03-06 19:48:10,577 - INFO - training batch 1351, loss: 0.510, 43232/60000 datapoints
2025-03-06 19:48:10,771 - INFO - training batch 1401, loss: 0.413, 44832/60000 datapoints
2025-03-06 19:48:10,961 - INFO - training batch 1451, loss: 0.410, 46432/60000 datapoints
2025-03-06 19:48:11,155 - INFO - training batch 1501, loss: 0.077, 48032/60000 datapoints
2025-03-06 19:48:11,358 - INFO - training batch 1551, loss: 0.229, 49632/60000 datapoints
2025-03-06 19:48:11,555 - INFO - training batch 1601, loss: 0.381, 51232/60000 datapoints
2025-03-06 19:48:11,749 - INFO - training batch 1651, loss: 0.105, 52832/60000 datapoints
2025-03-06 19:48:11,941 - INFO - training batch 1701, loss: 0.498, 54432/60000 datapoints
2025-03-06 19:48:12,139 - INFO - training batch 1751, loss: 0.126, 56032/60000 datapoints
2025-03-06 19:48:12,336 - INFO - training batch 1801, loss: 0.561, 57632/60000 datapoints
2025-03-06 19:48:12,533 - INFO - training batch 1851, loss: 0.062, 59232/60000 datapoints
2025-03-06 19:48:12,636 - INFO - validation batch 1, loss: 0.210, 32/10016 datapoints
2025-03-06 19:48:12,790 - INFO - validation batch 51, loss: 0.185, 1632/10016 datapoints
2025-03-06 19:48:12,943 - INFO - validation batch 101, loss: 0.193, 3232/10016 datapoints
2025-03-06 19:48:13,095 - INFO - validation batch 151, loss: 0.397, 4832/10016 datapoints
2025-03-06 19:48:13,248 - INFO - validation batch 201, loss: 0.354, 6432/10016 datapoints
2025-03-06 19:48:13,401 - INFO - validation batch 251, loss: 0.124, 8032/10016 datapoints
2025-03-06 19:48:13,554 - INFO - validation batch 301, loss: 0.207, 9632/10016 datapoints
2025-03-06 19:48:13,592 - INFO - Epoch 480/800 done.
2025-03-06 19:48:13,592 - INFO - Final validation performance:
Loss: 0.239, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:48:13,592 - INFO - Beginning epoch 481/800
2025-03-06 19:48:13,599 - INFO - training batch 1, loss: 0.449, 32/60000 datapoints
2025-03-06 19:48:13,814 - INFO - training batch 51, loss: 0.272, 1632/60000 datapoints
2025-03-06 19:48:14,011 - INFO - training batch 101, loss: 0.077, 3232/60000 datapoints
2025-03-06 19:48:14,211 - INFO - training batch 151, loss: 0.251, 4832/60000 datapoints
2025-03-06 19:48:14,411 - INFO - training batch 201, loss: 0.245, 6432/60000 datapoints
2025-03-06 19:48:14,612 - INFO - training batch 251, loss: 0.425, 8032/60000 datapoints
2025-03-06 19:48:14,811 - INFO - training batch 301, loss: 0.139, 9632/60000 datapoints
2025-03-06 19:48:15,007 - INFO - training batch 351, loss: 0.152, 11232/60000 datapoints
2025-03-06 19:48:15,222 - INFO - training batch 401, loss: 0.261, 12832/60000 datapoints
2025-03-06 19:48:15,417 - INFO - training batch 451, loss: 0.241, 14432/60000 datapoints
2025-03-06 19:48:15,610 - INFO - training batch 501, loss: 0.251, 16032/60000 datapoints
2025-03-06 19:48:15,806 - INFO - training batch 551, loss: 0.262, 17632/60000 datapoints
2025-03-06 19:48:16,001 - INFO - training batch 601, loss: 0.420, 19232/60000 datapoints
2025-03-06 19:48:16,197 - INFO - training batch 651, loss: 0.127, 20832/60000 datapoints
2025-03-06 19:48:16,392 - INFO - training batch 701, loss: 0.156, 22432/60000 datapoints
2025-03-06 19:48:16,587 - INFO - training batch 751, loss: 0.161, 24032/60000 datapoints
2025-03-06 19:48:16,782 - INFO - training batch 801, loss: 0.205, 25632/60000 datapoints
2025-03-06 19:48:16,975 - INFO - training batch 851, loss: 0.353, 27232/60000 datapoints
2025-03-06 19:48:17,172 - INFO - training batch 901, loss: 0.331, 28832/60000 datapoints
2025-03-06 19:48:17,371 - INFO - training batch 951, loss: 0.348, 30432/60000 datapoints
2025-03-06 19:48:17,567 - INFO - training batch 1001, loss: 0.409, 32032/60000 datapoints
2025-03-06 19:48:17,762 - INFO - training batch 1051, loss: 0.674, 33632/60000 datapoints
2025-03-06 19:48:17,956 - INFO - training batch 1101, loss: 0.394, 35232/60000 datapoints
2025-03-06 19:48:18,170 - INFO - training batch 1151, loss: 0.098, 36832/60000 datapoints
2025-03-06 19:48:18,368 - INFO - training batch 1201, loss: 0.395, 38432/60000 datapoints
2025-03-06 19:48:18,584 - INFO - training batch 1251, loss: 0.327, 40032/60000 datapoints
2025-03-06 19:48:18,782 - INFO - training batch 1301, loss: 0.128, 41632/60000 datapoints
2025-03-06 19:48:18,978 - INFO - training batch 1351, loss: 0.186, 43232/60000 datapoints
2025-03-06 19:48:19,176 - INFO - training batch 1401, loss: 0.321, 44832/60000 datapoints
2025-03-06 19:48:19,370 - INFO - training batch 1451, loss: 0.173, 46432/60000 datapoints
2025-03-06 19:48:19,568 - INFO - training batch 1501, loss: 0.111, 48032/60000 datapoints
2025-03-06 19:48:19,766 - INFO - training batch 1551, loss: 0.243, 49632/60000 datapoints
2025-03-06 19:48:19,960 - INFO - training batch 1601, loss: 0.289, 51232/60000 datapoints
2025-03-06 19:48:20,157 - INFO - training batch 1651, loss: 0.275, 52832/60000 datapoints
2025-03-06 19:48:20,355 - INFO - training batch 1701, loss: 0.446, 54432/60000 datapoints
2025-03-06 19:48:20,552 - INFO - training batch 1751, loss: 0.364, 56032/60000 datapoints
2025-03-06 19:48:20,750 - INFO - training batch 1801, loss: 0.192, 57632/60000 datapoints
2025-03-06 19:48:20,945 - INFO - training batch 1851, loss: 0.300, 59232/60000 datapoints
2025-03-06 19:48:21,047 - INFO - validation batch 1, loss: 0.350, 32/10016 datapoints
2025-03-06 19:48:21,199 - INFO - validation batch 51, loss: 0.253, 1632/10016 datapoints
2025-03-06 19:48:21,351 - INFO - validation batch 101, loss: 0.526, 3232/10016 datapoints
2025-03-06 19:48:21,503 - INFO - validation batch 151, loss: 0.287, 4832/10016 datapoints
2025-03-06 19:48:21,659 - INFO - validation batch 201, loss: 0.205, 6432/10016 datapoints
2025-03-06 19:48:21,811 - INFO - validation batch 251, loss: 0.337, 8032/10016 datapoints
2025-03-06 19:48:21,964 - INFO - validation batch 301, loss: 0.158, 9632/10016 datapoints
2025-03-06 19:48:22,003 - INFO - Epoch 481/800 done.
2025-03-06 19:48:22,003 - INFO - Final validation performance:
Loss: 0.302, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:48:22,004 - INFO - Beginning epoch 482/800
2025-03-06 19:48:22,011 - INFO - training batch 1, loss: 0.581, 32/60000 datapoints
2025-03-06 19:48:22,228 - INFO - training batch 51, loss: 0.636, 1632/60000 datapoints
2025-03-06 19:48:22,428 - INFO - training batch 101, loss: 0.093, 3232/60000 datapoints
2025-03-06 19:48:22,646 - INFO - training batch 151, loss: 0.180, 4832/60000 datapoints
2025-03-06 19:48:22,842 - INFO - training batch 201, loss: 0.121, 6432/60000 datapoints
2025-03-06 19:48:23,046 - INFO - training batch 251, loss: 0.070, 8032/60000 datapoints
2025-03-06 19:48:23,245 - INFO - training batch 301, loss: 0.279, 9632/60000 datapoints
2025-03-06 19:48:23,451 - INFO - training batch 351, loss: 0.355, 11232/60000 datapoints
2025-03-06 19:48:23,652 - INFO - training batch 401, loss: 0.183, 12832/60000 datapoints
2025-03-06 19:48:23,848 - INFO - training batch 451, loss: 0.177, 14432/60000 datapoints
2025-03-06 19:48:24,054 - INFO - training batch 501, loss: 0.288, 16032/60000 datapoints
2025-03-06 19:48:24,249 - INFO - training batch 551, loss: 0.119, 17632/60000 datapoints
2025-03-06 19:48:24,444 - INFO - training batch 601, loss: 0.221, 19232/60000 datapoints
2025-03-06 19:48:24,651 - INFO - training batch 651, loss: 0.224, 20832/60000 datapoints
2025-03-06 19:48:24,871 - INFO - training batch 701, loss: 0.062, 22432/60000 datapoints
2025-03-06 19:48:25,077 - INFO - training batch 751, loss: 0.428, 24032/60000 datapoints
2025-03-06 19:48:25,283 - INFO - training batch 801, loss: 0.250, 25632/60000 datapoints
2025-03-06 19:48:25,484 - INFO - training batch 851, loss: 0.129, 27232/60000 datapoints
2025-03-06 19:48:25,692 - INFO - training batch 901, loss: 0.448, 28832/60000 datapoints
2025-03-06 19:48:25,899 - INFO - training batch 951, loss: 0.242, 30432/60000 datapoints
2025-03-06 19:48:26,122 - INFO - training batch 1001, loss: 0.601, 32032/60000 datapoints
2025-03-06 19:48:26,335 - INFO - training batch 1051, loss: 0.315, 33632/60000 datapoints
2025-03-06 19:48:26,545 - INFO - training batch 1101, loss: 0.577, 35232/60000 datapoints
2025-03-06 19:48:26,762 - INFO - training batch 1151, loss: 0.155, 36832/60000 datapoints
2025-03-06 19:48:26,963 - INFO - training batch 1201, loss: 0.347, 38432/60000 datapoints
2025-03-06 19:48:27,165 - INFO - training batch 1251, loss: 0.152, 40032/60000 datapoints
2025-03-06 19:48:27,378 - INFO - training batch 1301, loss: 0.253, 41632/60000 datapoints
2025-03-06 19:48:27,588 - INFO - training batch 1351, loss: 0.123, 43232/60000 datapoints
2025-03-06 19:48:27,801 - INFO - training batch 1401, loss: 0.273, 44832/60000 datapoints
2025-03-06 19:48:28,016 - INFO - training batch 1451, loss: 0.173, 46432/60000 datapoints
2025-03-06 19:48:28,253 - INFO - training batch 1501, loss: 0.236, 48032/60000 datapoints
2025-03-06 19:48:28,489 - INFO - training batch 1551, loss: 0.186, 49632/60000 datapoints
2025-03-06 19:48:28,714 - INFO - training batch 1601, loss: 0.328, 51232/60000 datapoints
2025-03-06 19:48:28,931 - INFO - training batch 1651, loss: 0.217, 52832/60000 datapoints
2025-03-06 19:48:29,167 - INFO - training batch 1701, loss: 0.150, 54432/60000 datapoints
2025-03-06 19:48:29,389 - INFO - training batch 1751, loss: 0.099, 56032/60000 datapoints
2025-03-06 19:48:29,601 - INFO - training batch 1801, loss: 0.482, 57632/60000 datapoints
2025-03-06 19:48:29,816 - INFO - training batch 1851, loss: 0.133, 59232/60000 datapoints
2025-03-06 19:48:29,920 - INFO - validation batch 1, loss: 0.457, 32/10016 datapoints
2025-03-06 19:48:30,076 - INFO - validation batch 51, loss: 0.116, 1632/10016 datapoints
2025-03-06 19:48:30,241 - INFO - validation batch 101, loss: 0.284, 3232/10016 datapoints
2025-03-06 19:48:30,422 - INFO - validation batch 151, loss: 0.109, 4832/10016 datapoints
2025-03-06 19:48:30,604 - INFO - validation batch 201, loss: 0.313, 6432/10016 datapoints
2025-03-06 19:48:30,779 - INFO - validation batch 251, loss: 0.242, 8032/10016 datapoints
2025-03-06 19:48:30,966 - INFO - validation batch 301, loss: 0.188, 9632/10016 datapoints
2025-03-06 19:48:31,008 - INFO - Epoch 482/800 done.
2025-03-06 19:48:31,009 - INFO - Final validation performance:
Loss: 0.244, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:48:31,009 - INFO - Beginning epoch 483/800
2025-03-06 19:48:31,017 - INFO - training batch 1, loss: 0.215, 32/60000 datapoints
2025-03-06 19:48:31,252 - INFO - training batch 51, loss: 0.160, 1632/60000 datapoints
2025-03-06 19:48:31,482 - INFO - training batch 101, loss: 0.132, 3232/60000 datapoints
2025-03-06 19:48:31,727 - INFO - training batch 151, loss: 0.497, 4832/60000 datapoints
2025-03-06 19:48:31,943 - INFO - training batch 201, loss: 0.219, 6432/60000 datapoints
2025-03-06 19:48:32,163 - INFO - training batch 251, loss: 0.445, 8032/60000 datapoints
2025-03-06 19:48:32,378 - INFO - training batch 301, loss: 0.451, 9632/60000 datapoints
2025-03-06 19:48:32,599 - INFO - training batch 351, loss: 0.169, 11232/60000 datapoints
2025-03-06 19:48:32,824 - INFO - training batch 401, loss: 0.591, 12832/60000 datapoints
2025-03-06 19:48:33,064 - INFO - training batch 451, loss: 0.186, 14432/60000 datapoints
2025-03-06 19:48:33,297 - INFO - training batch 501, loss: 0.161, 16032/60000 datapoints
2025-03-06 19:48:33,552 - INFO - training batch 551, loss: 0.139, 17632/60000 datapoints
2025-03-06 19:48:33,782 - INFO - training batch 601, loss: 0.228, 19232/60000 datapoints
2025-03-06 19:48:34,001 - INFO - training batch 651, loss: 0.170, 20832/60000 datapoints
2025-03-06 19:48:34,238 - INFO - training batch 701, loss: 0.209, 22432/60000 datapoints
2025-03-06 19:48:34,448 - INFO - training batch 751, loss: 0.372, 24032/60000 datapoints
2025-03-06 19:48:34,674 - INFO - training batch 801, loss: 0.225, 25632/60000 datapoints
2025-03-06 19:48:34,919 - INFO - training batch 851, loss: 0.193, 27232/60000 datapoints
2025-03-06 19:48:35,169 - INFO - training batch 901, loss: 0.223, 28832/60000 datapoints
2025-03-06 19:48:35,395 - INFO - training batch 951, loss: 0.172, 30432/60000 datapoints
2025-03-06 19:48:35,594 - INFO - training batch 1001, loss: 0.252, 32032/60000 datapoints
2025-03-06 19:48:35,954 - INFO - training batch 1051, loss: 0.424, 33632/60000 datapoints
2025-03-06 19:48:36,164 - INFO - training batch 1101, loss: 0.110, 35232/60000 datapoints
2025-03-06 19:48:36,366 - INFO - training batch 1151, loss: 0.152, 36832/60000 datapoints
2025-03-06 19:48:36,585 - INFO - training batch 1201, loss: 0.192, 38432/60000 datapoints
2025-03-06 19:48:36,802 - INFO - training batch 1251, loss: 0.396, 40032/60000 datapoints
2025-03-06 19:48:37,099 - INFO - training batch 1301, loss: 0.104, 41632/60000 datapoints
2025-03-06 19:48:37,350 - INFO - training batch 1351, loss: 0.432, 43232/60000 datapoints
2025-03-06 19:48:37,751 - INFO - training batch 1401, loss: 0.431, 44832/60000 datapoints
2025-03-06 19:48:38,126 - INFO - training batch 1451, loss: 0.199, 46432/60000 datapoints
2025-03-06 19:48:38,503 - INFO - training batch 1501, loss: 0.117, 48032/60000 datapoints
2025-03-06 19:48:38,827 - INFO - training batch 1551, loss: 0.598, 49632/60000 datapoints
2025-03-06 19:48:39,281 - INFO - training batch 1601, loss: 0.465, 51232/60000 datapoints
2025-03-06 19:48:39,722 - INFO - training batch 1651, loss: 0.598, 52832/60000 datapoints
2025-03-06 19:48:40,036 - INFO - training batch 1701, loss: 0.134, 54432/60000 datapoints
2025-03-06 19:48:40,367 - INFO - training batch 1751, loss: 0.136, 56032/60000 datapoints
2025-03-06 19:48:40,683 - INFO - training batch 1801, loss: 0.085, 57632/60000 datapoints
2025-03-06 19:48:41,026 - INFO - training batch 1851, loss: 0.194, 59232/60000 datapoints
2025-03-06 19:48:41,188 - INFO - validation batch 1, loss: 0.074, 32/10016 datapoints
2025-03-06 19:48:41,430 - INFO - validation batch 51, loss: 0.122, 1632/10016 datapoints
2025-03-06 19:48:41,644 - INFO - validation batch 101, loss: 0.149, 3232/10016 datapoints
2025-03-06 19:48:41,852 - INFO - validation batch 151, loss: 0.220, 4832/10016 datapoints
2025-03-06 19:48:42,034 - INFO - validation batch 201, loss: 0.148, 6432/10016 datapoints
2025-03-06 19:48:42,203 - INFO - validation batch 251, loss: 0.272, 8032/10016 datapoints
2025-03-06 19:48:42,366 - INFO - validation batch 301, loss: 0.192, 9632/10016 datapoints
2025-03-06 19:48:42,411 - INFO - Epoch 483/800 done.
2025-03-06 19:48:42,412 - INFO - Final validation performance:
Loss: 0.168, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:48:42,412 - INFO - Beginning epoch 484/800
2025-03-06 19:48:42,420 - INFO - training batch 1, loss: 0.336, 32/60000 datapoints
2025-03-06 19:48:42,627 - INFO - training batch 51, loss: 0.125, 1632/60000 datapoints
2025-03-06 19:48:42,842 - INFO - training batch 101, loss: 0.170, 3232/60000 datapoints
2025-03-06 19:48:43,056 - INFO - training batch 151, loss: 0.232, 4832/60000 datapoints
2025-03-06 19:48:43,264 - INFO - training batch 201, loss: 0.091, 6432/60000 datapoints
2025-03-06 19:48:43,480 - INFO - training batch 251, loss: 0.227, 8032/60000 datapoints
2025-03-06 19:48:43,708 - INFO - training batch 301, loss: 0.119, 9632/60000 datapoints
2025-03-06 19:48:43,915 - INFO - training batch 351, loss: 0.201, 11232/60000 datapoints
2025-03-06 19:48:44,115 - INFO - training batch 401, loss: 0.156, 12832/60000 datapoints
2025-03-06 19:48:44,318 - INFO - training batch 451, loss: 0.216, 14432/60000 datapoints
2025-03-06 19:48:44,522 - INFO - training batch 501, loss: 0.135, 16032/60000 datapoints
2025-03-06 19:48:44,728 - INFO - training batch 551, loss: 0.347, 17632/60000 datapoints
2025-03-06 19:48:44,944 - INFO - training batch 601, loss: 0.538, 19232/60000 datapoints
2025-03-06 19:48:45,141 - INFO - training batch 651, loss: 0.410, 20832/60000 datapoints
2025-03-06 19:48:45,337 - INFO - training batch 701, loss: 0.360, 22432/60000 datapoints
2025-03-06 19:48:45,557 - INFO - training batch 751, loss: 0.337, 24032/60000 datapoints
2025-03-06 19:48:45,758 - INFO - training batch 801, loss: 0.109, 25632/60000 datapoints
2025-03-06 19:48:45,961 - INFO - training batch 851, loss: 0.398, 27232/60000 datapoints
2025-03-06 19:48:46,158 - INFO - training batch 901, loss: 0.273, 28832/60000 datapoints
2025-03-06 19:48:46,361 - INFO - training batch 951, loss: 0.332, 30432/60000 datapoints
2025-03-06 19:48:46,576 - INFO - training batch 1001, loss: 0.520, 32032/60000 datapoints
2025-03-06 19:48:46,783 - INFO - training batch 1051, loss: 0.296, 33632/60000 datapoints
2025-03-06 19:48:46,988 - INFO - training batch 1101, loss: 0.325, 35232/60000 datapoints
2025-03-06 19:48:47,189 - INFO - training batch 1151, loss: 0.192, 36832/60000 datapoints
2025-03-06 19:48:47,394 - INFO - training batch 1201, loss: 0.260, 38432/60000 datapoints
2025-03-06 19:48:47,594 - INFO - training batch 1251, loss: 0.185, 40032/60000 datapoints
2025-03-06 19:48:47,793 - INFO - training batch 1301, loss: 0.355, 41632/60000 datapoints
2025-03-06 19:48:47,993 - INFO - training batch 1351, loss: 0.248, 43232/60000 datapoints
2025-03-06 19:48:48,211 - INFO - training batch 1401, loss: 0.086, 44832/60000 datapoints
2025-03-06 19:48:48,410 - INFO - training batch 1451, loss: 0.320, 46432/60000 datapoints
2025-03-06 19:48:48,612 - INFO - training batch 1501, loss: 0.221, 48032/60000 datapoints
2025-03-06 19:48:48,815 - INFO - training batch 1551, loss: 0.271, 49632/60000 datapoints
2025-03-06 19:48:49,017 - INFO - training batch 1601, loss: 0.242, 51232/60000 datapoints
2025-03-06 19:48:49,220 - INFO - training batch 1651, loss: 0.179, 52832/60000 datapoints
2025-03-06 19:48:49,428 - INFO - training batch 1701, loss: 0.242, 54432/60000 datapoints
2025-03-06 19:48:49,634 - INFO - training batch 1751, loss: 0.654, 56032/60000 datapoints
2025-03-06 19:48:49,837 - INFO - training batch 1801, loss: 0.454, 57632/60000 datapoints
2025-03-06 19:48:50,036 - INFO - training batch 1851, loss: 0.492, 59232/60000 datapoints
2025-03-06 19:48:50,138 - INFO - validation batch 1, loss: 0.401, 32/10016 datapoints
2025-03-06 19:48:50,293 - INFO - validation batch 51, loss: 0.251, 1632/10016 datapoints
2025-03-06 19:48:50,452 - INFO - validation batch 101, loss: 0.279, 3232/10016 datapoints
2025-03-06 19:48:50,612 - INFO - validation batch 151, loss: 0.334, 4832/10016 datapoints
2025-03-06 19:48:50,769 - INFO - validation batch 201, loss: 0.144, 6432/10016 datapoints
2025-03-06 19:48:50,928 - INFO - validation batch 251, loss: 0.080, 8032/10016 datapoints
2025-03-06 19:48:51,086 - INFO - validation batch 301, loss: 0.250, 9632/10016 datapoints
2025-03-06 19:48:51,124 - INFO - Epoch 484/800 done.
2025-03-06 19:48:51,124 - INFO - Final validation performance:
Loss: 0.248, top-1 acc: 0.925top-5 acc: 0.925
2025-03-06 19:48:51,125 - INFO - Beginning epoch 485/800
2025-03-06 19:48:51,132 - INFO - training batch 1, loss: 0.237, 32/60000 datapoints
2025-03-06 19:48:51,341 - INFO - training batch 51, loss: 0.221, 1632/60000 datapoints
2025-03-06 19:48:51,551 - INFO - training batch 101, loss: 0.188, 3232/60000 datapoints
2025-03-06 19:48:51,779 - INFO - training batch 151, loss: 0.239, 4832/60000 datapoints
2025-03-06 19:48:52,003 - INFO - training batch 201, loss: 0.470, 6432/60000 datapoints
2025-03-06 19:48:52,247 - INFO - training batch 251, loss: 0.814, 8032/60000 datapoints
2025-03-06 19:48:52,461 - INFO - training batch 301, loss: 0.642, 9632/60000 datapoints
2025-03-06 19:48:52,676 - INFO - training batch 351, loss: 0.405, 11232/60000 datapoints
2025-03-06 19:48:52,879 - INFO - training batch 401, loss: 0.625, 12832/60000 datapoints
2025-03-06 19:48:53,083 - INFO - training batch 451, loss: 0.561, 14432/60000 datapoints
2025-03-06 19:48:53,291 - INFO - training batch 501, loss: 0.156, 16032/60000 datapoints
2025-03-06 19:48:53,492 - INFO - training batch 551, loss: 0.210, 17632/60000 datapoints
2025-03-06 19:48:53,708 - INFO - training batch 601, loss: 0.141, 19232/60000 datapoints
2025-03-06 19:48:53,912 - INFO - training batch 651, loss: 0.283, 20832/60000 datapoints
2025-03-06 19:48:54,121 - INFO - training batch 701, loss: 0.089, 22432/60000 datapoints
2025-03-06 19:48:54,329 - INFO - training batch 751, loss: 0.114, 24032/60000 datapoints
2025-03-06 19:48:54,536 - INFO - training batch 801, loss: 0.411, 25632/60000 datapoints
2025-03-06 19:48:54,735 - INFO - training batch 851, loss: 0.134, 27232/60000 datapoints
2025-03-06 19:48:54,938 - INFO - training batch 901, loss: 0.247, 28832/60000 datapoints
2025-03-06 19:48:55,134 - INFO - training batch 951, loss: 0.372, 30432/60000 datapoints
2025-03-06 19:48:55,335 - INFO - training batch 1001, loss: 0.074, 32032/60000 datapoints
2025-03-06 19:48:55,551 - INFO - training batch 1051, loss: 0.662, 33632/60000 datapoints
2025-03-06 19:48:55,761 - INFO - training batch 1101, loss: 0.388, 35232/60000 datapoints
2025-03-06 19:48:55,959 - INFO - training batch 1151, loss: 0.341, 36832/60000 datapoints
2025-03-06 19:48:56,162 - INFO - training batch 1201, loss: 0.221, 38432/60000 datapoints
2025-03-06 19:48:56,360 - INFO - training batch 1251, loss: 0.117, 40032/60000 datapoints
2025-03-06 19:48:56,557 - INFO - training batch 1301, loss: 0.153, 41632/60000 datapoints
2025-03-06 19:48:56,763 - INFO - training batch 1351, loss: 0.220, 43232/60000 datapoints
2025-03-06 19:48:56,969 - INFO - training batch 1401, loss: 0.360, 44832/60000 datapoints
2025-03-06 19:48:57,179 - INFO - training batch 1451, loss: 0.518, 46432/60000 datapoints
2025-03-06 19:48:57,386 - INFO - training batch 1501, loss: 0.241, 48032/60000 datapoints
2025-03-06 19:48:57,590 - INFO - training batch 1551, loss: 0.145, 49632/60000 datapoints
2025-03-06 19:48:57,799 - INFO - training batch 1601, loss: 0.211, 51232/60000 datapoints
2025-03-06 19:48:58,001 - INFO - training batch 1651, loss: 0.277, 52832/60000 datapoints
2025-03-06 19:48:58,200 - INFO - training batch 1701, loss: 0.208, 54432/60000 datapoints
2025-03-06 19:48:58,409 - INFO - training batch 1751, loss: 0.405, 56032/60000 datapoints
2025-03-06 19:48:58,616 - INFO - training batch 1801, loss: 0.466, 57632/60000 datapoints
2025-03-06 19:48:58,837 - INFO - training batch 1851, loss: 0.076, 59232/60000 datapoints
2025-03-06 19:48:58,949 - INFO - validation batch 1, loss: 0.643, 32/10016 datapoints
2025-03-06 19:48:59,123 - INFO - validation batch 51, loss: 0.087, 1632/10016 datapoints
2025-03-06 19:48:59,281 - INFO - validation batch 101, loss: 0.264, 3232/10016 datapoints
2025-03-06 19:48:59,452 - INFO - validation batch 151, loss: 0.125, 4832/10016 datapoints
2025-03-06 19:48:59,615 - INFO - validation batch 201, loss: 0.791, 6432/10016 datapoints
2025-03-06 19:48:59,787 - INFO - validation batch 251, loss: 0.201, 8032/10016 datapoints
2025-03-06 19:48:59,950 - INFO - validation batch 301, loss: 0.186, 9632/10016 datapoints
2025-03-06 19:48:59,997 - INFO - Epoch 485/800 done.
2025-03-06 19:48:59,997 - INFO - Final validation performance:
Loss: 0.328, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:48:59,998 - INFO - Beginning epoch 486/800
2025-03-06 19:49:00,005 - INFO - training batch 1, loss: 0.446, 32/60000 datapoints
2025-03-06 19:49:00,261 - INFO - training batch 51, loss: 0.180, 1632/60000 datapoints
2025-03-06 19:49:00,526 - INFO - training batch 101, loss: 0.243, 3232/60000 datapoints
2025-03-06 19:49:00,757 - INFO - training batch 151, loss: 0.232, 4832/60000 datapoints
2025-03-06 19:49:00,979 - INFO - training batch 201, loss: 0.251, 6432/60000 datapoints
2025-03-06 19:49:01,208 - INFO - training batch 251, loss: 0.302, 8032/60000 datapoints
2025-03-06 19:49:01,424 - INFO - training batch 301, loss: 0.223, 9632/60000 datapoints
2025-03-06 19:49:01,675 - INFO - training batch 351, loss: 0.145, 11232/60000 datapoints
2025-03-06 19:49:01,876 - INFO - training batch 401, loss: 0.321, 12832/60000 datapoints
2025-03-06 19:49:02,094 - INFO - training batch 451, loss: 0.323, 14432/60000 datapoints
2025-03-06 19:49:02,329 - INFO - training batch 501, loss: 0.250, 16032/60000 datapoints
2025-03-06 19:49:02,550 - INFO - training batch 551, loss: 0.059, 17632/60000 datapoints
2025-03-06 19:49:02,771 - INFO - training batch 601, loss: 0.319, 19232/60000 datapoints
2025-03-06 19:49:02,983 - INFO - training batch 651, loss: 0.353, 20832/60000 datapoints
2025-03-06 19:49:03,220 - INFO - training batch 701, loss: 0.436, 22432/60000 datapoints
2025-03-06 19:49:03,431 - INFO - training batch 751, loss: 0.187, 24032/60000 datapoints
2025-03-06 19:49:03,648 - INFO - training batch 801, loss: 0.041, 25632/60000 datapoints
2025-03-06 19:49:03,857 - INFO - training batch 851, loss: 0.332, 27232/60000 datapoints
2025-03-06 19:49:04,058 - INFO - training batch 901, loss: 0.295, 28832/60000 datapoints
2025-03-06 19:49:04,262 - INFO - training batch 951, loss: 0.203, 30432/60000 datapoints
2025-03-06 19:49:04,474 - INFO - training batch 1001, loss: 0.273, 32032/60000 datapoints
2025-03-06 19:49:04,698 - INFO - training batch 1051, loss: 0.203, 33632/60000 datapoints
2025-03-06 19:49:04,917 - INFO - training batch 1101, loss: 0.181, 35232/60000 datapoints
2025-03-06 19:49:05,121 - INFO - training batch 1151, loss: 0.237, 36832/60000 datapoints
2025-03-06 19:49:05,328 - INFO - training batch 1201, loss: 0.320, 38432/60000 datapoints
2025-03-06 19:49:05,534 - INFO - training batch 1251, loss: 0.227, 40032/60000 datapoints
2025-03-06 19:49:05,824 - INFO - training batch 1301, loss: 0.300, 41632/60000 datapoints
2025-03-06 19:49:06,106 - INFO - training batch 1351, loss: 0.216, 43232/60000 datapoints
2025-03-06 19:49:06,369 - INFO - training batch 1401, loss: 0.174, 44832/60000 datapoints
2025-03-06 19:49:06,602 - INFO - training batch 1451, loss: 0.226, 46432/60000 datapoints
2025-03-06 19:49:06,846 - INFO - training batch 1501, loss: 0.187, 48032/60000 datapoints
2025-03-06 19:49:07,066 - INFO - training batch 1551, loss: 0.127, 49632/60000 datapoints
2025-03-06 19:49:07,283 - INFO - training batch 1601, loss: 0.416, 51232/60000 datapoints
2025-03-06 19:49:07,512 - INFO - training batch 1651, loss: 0.214, 52832/60000 datapoints
2025-03-06 19:49:07,737 - INFO - training batch 1701, loss: 0.184, 54432/60000 datapoints
2025-03-06 19:49:08,024 - INFO - training batch 1751, loss: 0.546, 56032/60000 datapoints
2025-03-06 19:49:08,364 - INFO - training batch 1801, loss: 0.175, 57632/60000 datapoints
2025-03-06 19:49:08,712 - INFO - training batch 1851, loss: 0.543, 59232/60000 datapoints
2025-03-06 19:49:08,853 - INFO - validation batch 1, loss: 0.208, 32/10016 datapoints
2025-03-06 19:49:09,042 - INFO - validation batch 51, loss: 0.177, 1632/10016 datapoints
2025-03-06 19:49:09,235 - INFO - validation batch 101, loss: 0.496, 3232/10016 datapoints
2025-03-06 19:49:09,416 - INFO - validation batch 151, loss: 0.227, 4832/10016 datapoints
2025-03-06 19:49:09,613 - INFO - validation batch 201, loss: 0.183, 6432/10016 datapoints
2025-03-06 19:49:09,949 - INFO - validation batch 251, loss: 0.345, 8032/10016 datapoints
2025-03-06 19:49:10,162 - INFO - validation batch 301, loss: 0.184, 9632/10016 datapoints
2025-03-06 19:49:10,215 - INFO - Epoch 486/800 done.
2025-03-06 19:49:10,215 - INFO - Final validation performance:
Loss: 0.260, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:49:10,217 - INFO - Beginning epoch 487/800
2025-03-06 19:49:10,226 - INFO - training batch 1, loss: 0.140, 32/60000 datapoints
2025-03-06 19:49:10,637 - INFO - training batch 51, loss: 0.483, 1632/60000 datapoints
2025-03-06 19:49:10,931 - INFO - training batch 101, loss: 0.280, 3232/60000 datapoints
2025-03-06 19:49:11,185 - INFO - training batch 151, loss: 0.154, 4832/60000 datapoints
2025-03-06 19:49:11,441 - INFO - training batch 201, loss: 0.256, 6432/60000 datapoints
2025-03-06 19:49:11,695 - INFO - training batch 251, loss: 0.101, 8032/60000 datapoints
2025-03-06 19:49:11,937 - INFO - training batch 301, loss: 0.106, 9632/60000 datapoints
2025-03-06 19:49:12,221 - INFO - training batch 351, loss: 0.128, 11232/60000 datapoints
2025-03-06 19:49:12,575 - INFO - training batch 401, loss: 0.064, 12832/60000 datapoints
2025-03-06 19:49:12,795 - INFO - training batch 451, loss: 0.210, 14432/60000 datapoints
2025-03-06 19:49:13,030 - INFO - training batch 501, loss: 0.264, 16032/60000 datapoints
2025-03-06 19:49:13,261 - INFO - training batch 551, loss: 0.206, 17632/60000 datapoints
2025-03-06 19:49:13,490 - INFO - training batch 601, loss: 0.229, 19232/60000 datapoints
2025-03-06 19:49:13,707 - INFO - training batch 651, loss: 0.275, 20832/60000 datapoints
2025-03-06 19:49:13,917 - INFO - training batch 701, loss: 0.237, 22432/60000 datapoints
2025-03-06 19:49:14,127 - INFO - training batch 751, loss: 0.290, 24032/60000 datapoints
2025-03-06 19:49:14,352 - INFO - training batch 801, loss: 0.373, 25632/60000 datapoints
2025-03-06 19:49:14,643 - INFO - training batch 851, loss: 0.539, 27232/60000 datapoints
2025-03-06 19:49:14,894 - INFO - training batch 901, loss: 0.248, 28832/60000 datapoints
2025-03-06 19:49:15,121 - INFO - training batch 951, loss: 0.093, 30432/60000 datapoints
2025-03-06 19:49:15,339 - INFO - training batch 1001, loss: 0.103, 32032/60000 datapoints
2025-03-06 19:49:15,561 - INFO - training batch 1051, loss: 0.322, 33632/60000 datapoints
2025-03-06 19:49:15,793 - INFO - training batch 1101, loss: 0.344, 35232/60000 datapoints
2025-03-06 19:49:16,016 - INFO - training batch 1151, loss: 0.099, 36832/60000 datapoints
2025-03-06 19:49:16,236 - INFO - training batch 1201, loss: 0.243, 38432/60000 datapoints
2025-03-06 19:49:16,459 - INFO - training batch 1251, loss: 0.480, 40032/60000 datapoints
2025-03-06 19:49:16,682 - INFO - training batch 1301, loss: 0.377, 41632/60000 datapoints
2025-03-06 19:49:16,902 - INFO - training batch 1351, loss: 0.227, 43232/60000 datapoints
2025-03-06 19:49:17,249 - INFO - training batch 1401, loss: 0.279, 44832/60000 datapoints
2025-03-06 19:49:17,472 - INFO - training batch 1451, loss: 0.125, 46432/60000 datapoints
2025-03-06 19:49:17,704 - INFO - training batch 1501, loss: 0.288, 48032/60000 datapoints
2025-03-06 19:49:17,927 - INFO - training batch 1551, loss: 0.233, 49632/60000 datapoints
2025-03-06 19:49:18,149 - INFO - training batch 1601, loss: 0.166, 51232/60000 datapoints
2025-03-06 19:49:18,379 - INFO - training batch 1651, loss: 0.271, 52832/60000 datapoints
2025-03-06 19:49:18,609 - INFO - training batch 1701, loss: 0.207, 54432/60000 datapoints
2025-03-06 19:49:18,877 - INFO - training batch 1751, loss: 0.432, 56032/60000 datapoints
2025-03-06 19:49:19,116 - INFO - training batch 1801, loss: 0.285, 57632/60000 datapoints
2025-03-06 19:49:19,342 - INFO - training batch 1851, loss: 0.129, 59232/60000 datapoints
2025-03-06 19:49:19,454 - INFO - validation batch 1, loss: 0.127, 32/10016 datapoints
2025-03-06 19:49:19,634 - INFO - validation batch 51, loss: 0.158, 1632/10016 datapoints
2025-03-06 19:49:19,819 - INFO - validation batch 101, loss: 0.230, 3232/10016 datapoints
2025-03-06 19:49:19,983 - INFO - validation batch 151, loss: 0.269, 4832/10016 datapoints
2025-03-06 19:49:20,167 - INFO - validation batch 201, loss: 0.101, 6432/10016 datapoints
2025-03-06 19:49:20,355 - INFO - validation batch 251, loss: 0.110, 8032/10016 datapoints
2025-03-06 19:49:20,530 - INFO - validation batch 301, loss: 0.320, 9632/10016 datapoints
2025-03-06 19:49:20,572 - INFO - Epoch 487/800 done.
2025-03-06 19:49:20,572 - INFO - Final validation performance:
Loss: 0.188, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:49:20,573 - INFO - Beginning epoch 488/800
2025-03-06 19:49:20,581 - INFO - training batch 1, loss: 0.330, 32/60000 datapoints
2025-03-06 19:49:20,816 - INFO - training batch 51, loss: 0.261, 1632/60000 datapoints
2025-03-06 19:49:21,042 - INFO - training batch 101, loss: 0.113, 3232/60000 datapoints
2025-03-06 19:49:21,297 - INFO - training batch 151, loss: 0.241, 4832/60000 datapoints
2025-03-06 19:49:21,543 - INFO - training batch 201, loss: 0.138, 6432/60000 datapoints
2025-03-06 19:49:21,811 - INFO - training batch 251, loss: 0.107, 8032/60000 datapoints
2025-03-06 19:49:22,041 - INFO - training batch 301, loss: 0.143, 9632/60000 datapoints
2025-03-06 19:49:22,252 - INFO - training batch 351, loss: 0.324, 11232/60000 datapoints
2025-03-06 19:49:22,457 - INFO - training batch 401, loss: 0.335, 12832/60000 datapoints
2025-03-06 19:49:22,680 - INFO - training batch 451, loss: 0.156, 14432/60000 datapoints
2025-03-06 19:49:22,885 - INFO - training batch 501, loss: 0.143, 16032/60000 datapoints
2025-03-06 19:49:23,095 - INFO - training batch 551, loss: 0.289, 17632/60000 datapoints
2025-03-06 19:49:23,298 - INFO - training batch 601, loss: 0.231, 19232/60000 datapoints
2025-03-06 19:49:23,502 - INFO - training batch 651, loss: 0.284, 20832/60000 datapoints
2025-03-06 19:49:23,715 - INFO - training batch 701, loss: 0.467, 22432/60000 datapoints
2025-03-06 19:49:23,947 - INFO - training batch 751, loss: 0.130, 24032/60000 datapoints
2025-03-06 19:49:24,192 - INFO - training batch 801, loss: 0.137, 25632/60000 datapoints
2025-03-06 19:49:24,416 - INFO - training batch 851, loss: 0.499, 27232/60000 datapoints
2025-03-06 19:49:24,698 - INFO - training batch 901, loss: 0.363, 28832/60000 datapoints
2025-03-06 19:49:24,912 - INFO - training batch 951, loss: 0.352, 30432/60000 datapoints
2025-03-06 19:49:25,146 - INFO - training batch 1001, loss: 0.080, 32032/60000 datapoints
2025-03-06 19:49:25,363 - INFO - training batch 1051, loss: 0.313, 33632/60000 datapoints
2025-03-06 19:49:25,637 - INFO - training batch 1101, loss: 0.228, 35232/60000 datapoints
2025-03-06 19:49:25,884 - INFO - training batch 1151, loss: 0.169, 36832/60000 datapoints
2025-03-06 19:49:26,097 - INFO - training batch 1201, loss: 0.108, 38432/60000 datapoints
2025-03-06 19:49:26,305 - INFO - training batch 1251, loss: 0.391, 40032/60000 datapoints
2025-03-06 19:49:26,517 - INFO - training batch 1301, loss: 0.321, 41632/60000 datapoints
2025-03-06 19:49:26,785 - INFO - training batch 1351, loss: 0.368, 43232/60000 datapoints
2025-03-06 19:49:27,006 - INFO - training batch 1401, loss: 0.224, 44832/60000 datapoints
2025-03-06 19:49:27,304 - INFO - training batch 1451, loss: 0.379, 46432/60000 datapoints
2025-03-06 19:49:28,075 - INFO - training batch 1501, loss: 0.203, 48032/60000 datapoints
2025-03-06 19:49:28,320 - INFO - training batch 1551, loss: 0.267, 49632/60000 datapoints
2025-03-06 19:49:28,549 - INFO - training batch 1601, loss: 0.379, 51232/60000 datapoints
2025-03-06 19:49:28,811 - INFO - training batch 1651, loss: 0.258, 52832/60000 datapoints
2025-03-06 19:49:29,169 - INFO - training batch 1701, loss: 0.257, 54432/60000 datapoints
2025-03-06 19:49:29,423 - INFO - training batch 1751, loss: 0.104, 56032/60000 datapoints
2025-03-06 19:49:29,881 - INFO - training batch 1801, loss: 0.261, 57632/60000 datapoints
2025-03-06 19:49:30,136 - INFO - training batch 1851, loss: 0.218, 59232/60000 datapoints
2025-03-06 19:49:30,246 - INFO - validation batch 1, loss: 0.346, 32/10016 datapoints
2025-03-06 19:49:30,411 - INFO - validation batch 51, loss: 0.246, 1632/10016 datapoints
2025-03-06 19:49:30,580 - INFO - validation batch 101, loss: 0.235, 3232/10016 datapoints
2025-03-06 19:49:30,761 - INFO - validation batch 151, loss: 0.164, 4832/10016 datapoints
2025-03-06 19:49:30,968 - INFO - validation batch 201, loss: 0.102, 6432/10016 datapoints
2025-03-06 19:49:31,170 - INFO - validation batch 251, loss: 0.172, 8032/10016 datapoints
2025-03-06 19:49:31,347 - INFO - validation batch 301, loss: 0.250, 9632/10016 datapoints
2025-03-06 19:49:31,387 - INFO - Epoch 488/800 done.
2025-03-06 19:49:31,388 - INFO - Final validation performance:
Loss: 0.216, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:49:31,389 - INFO - Beginning epoch 489/800
2025-03-06 19:49:31,396 - INFO - training batch 1, loss: 0.288, 32/60000 datapoints
2025-03-06 19:49:31,640 - INFO - training batch 51, loss: 0.315, 1632/60000 datapoints
2025-03-06 19:49:31,884 - INFO - training batch 101, loss: 0.309, 3232/60000 datapoints
2025-03-06 19:49:32,102 - INFO - training batch 151, loss: 0.305, 4832/60000 datapoints
2025-03-06 19:49:32,324 - INFO - training batch 201, loss: 0.165, 6432/60000 datapoints
2025-03-06 19:49:32,562 - INFO - training batch 251, loss: 0.307, 8032/60000 datapoints
2025-03-06 19:49:32,796 - INFO - training batch 301, loss: 0.480, 9632/60000 datapoints
2025-03-06 19:49:33,031 - INFO - training batch 351, loss: 0.274, 11232/60000 datapoints
2025-03-06 19:49:33,261 - INFO - training batch 401, loss: 0.584, 12832/60000 datapoints
2025-03-06 19:49:33,532 - INFO - training batch 451, loss: 0.239, 14432/60000 datapoints
2025-03-06 19:49:33,740 - INFO - training batch 501, loss: 0.339, 16032/60000 datapoints
2025-03-06 19:49:33,943 - INFO - training batch 551, loss: 0.166, 17632/60000 datapoints
2025-03-06 19:49:34,388 - INFO - training batch 601, loss: 0.214, 19232/60000 datapoints
2025-03-06 19:49:34,612 - INFO - training batch 651, loss: 0.414, 20832/60000 datapoints
2025-03-06 19:49:34,837 - INFO - training batch 701, loss: 0.198, 22432/60000 datapoints
2025-03-06 19:49:35,099 - INFO - training batch 751, loss: 0.358, 24032/60000 datapoints
2025-03-06 19:49:35,303 - INFO - training batch 801, loss: 0.244, 25632/60000 datapoints
2025-03-06 19:49:35,505 - INFO - training batch 851, loss: 0.148, 27232/60000 datapoints
2025-03-06 19:49:35,739 - INFO - training batch 901, loss: 0.113, 28832/60000 datapoints
2025-03-06 19:49:35,993 - INFO - training batch 951, loss: 0.513, 30432/60000 datapoints
2025-03-06 19:49:36,235 - INFO - training batch 1001, loss: 0.029, 32032/60000 datapoints
2025-03-06 19:49:36,457 - INFO - training batch 1051, loss: 0.099, 33632/60000 datapoints
2025-03-06 19:49:36,669 - INFO - training batch 1101, loss: 0.116, 35232/60000 datapoints
2025-03-06 19:49:36,872 - INFO - training batch 1151, loss: 0.190, 36832/60000 datapoints
2025-03-06 19:49:37,070 - INFO - training batch 1201, loss: 0.198, 38432/60000 datapoints
2025-03-06 19:49:37,304 - INFO - training batch 1251, loss: 0.387, 40032/60000 datapoints
2025-03-06 19:49:37,551 - INFO - training batch 1301, loss: 0.230, 41632/60000 datapoints
2025-03-06 19:49:37,784 - INFO - training batch 1351, loss: 0.253, 43232/60000 datapoints
2025-03-06 19:49:38,082 - INFO - training batch 1401, loss: 0.225, 44832/60000 datapoints
2025-03-06 19:49:38,349 - INFO - training batch 1451, loss: 0.096, 46432/60000 datapoints
2025-03-06 19:49:38,561 - INFO - training batch 1501, loss: 0.366, 48032/60000 datapoints
2025-03-06 19:49:38,771 - INFO - training batch 1551, loss: 0.496, 49632/60000 datapoints
2025-03-06 19:49:39,014 - INFO - training batch 1601, loss: 0.325, 51232/60000 datapoints
2025-03-06 19:49:39,273 - INFO - training batch 1651, loss: 0.234, 52832/60000 datapoints
2025-03-06 19:49:39,476 - INFO - training batch 1701, loss: 0.245, 54432/60000 datapoints
2025-03-06 19:49:39,691 - INFO - training batch 1751, loss: 0.097, 56032/60000 datapoints
2025-03-06 19:49:39,893 - INFO - training batch 1801, loss: 0.091, 57632/60000 datapoints
2025-03-06 19:49:40,102 - INFO - training batch 1851, loss: 0.171, 59232/60000 datapoints
2025-03-06 19:49:40,210 - INFO - validation batch 1, loss: 0.217, 32/10016 datapoints
2025-03-06 19:49:40,371 - INFO - validation batch 51, loss: 0.086, 1632/10016 datapoints
2025-03-06 19:49:40,541 - INFO - validation batch 101, loss: 0.159, 3232/10016 datapoints
2025-03-06 19:49:40,731 - INFO - validation batch 151, loss: 0.110, 4832/10016 datapoints
2025-03-06 19:49:40,900 - INFO - validation batch 201, loss: 0.464, 6432/10016 datapoints
2025-03-06 19:49:41,065 - INFO - validation batch 251, loss: 0.227, 8032/10016 datapoints
2025-03-06 19:49:41,227 - INFO - validation batch 301, loss: 0.312, 9632/10016 datapoints
2025-03-06 19:49:41,269 - INFO - Epoch 489/800 done.
2025-03-06 19:49:41,269 - INFO - Final validation performance:
Loss: 0.225, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:49:41,270 - INFO - Beginning epoch 490/800
2025-03-06 19:49:41,277 - INFO - training batch 1, loss: 0.152, 32/60000 datapoints
2025-03-06 19:49:41,490 - INFO - training batch 51, loss: 0.398, 1632/60000 datapoints
2025-03-06 19:49:41,697 - INFO - training batch 101, loss: 0.230, 3232/60000 datapoints
2025-03-06 19:49:41,907 - INFO - training batch 151, loss: 0.357, 4832/60000 datapoints
2025-03-06 19:49:42,107 - INFO - training batch 201, loss: 0.255, 6432/60000 datapoints
2025-03-06 19:49:42,355 - INFO - training batch 251, loss: 0.095, 8032/60000 datapoints
2025-03-06 19:49:42,588 - INFO - training batch 301, loss: 0.227, 9632/60000 datapoints
2025-03-06 19:49:42,808 - INFO - training batch 351, loss: 0.202, 11232/60000 datapoints
2025-03-06 19:49:43,023 - INFO - training batch 401, loss: 0.296, 12832/60000 datapoints
2025-03-06 19:49:43,247 - INFO - training batch 451, loss: 0.197, 14432/60000 datapoints
2025-03-06 19:49:43,453 - INFO - training batch 501, loss: 0.218, 16032/60000 datapoints
2025-03-06 19:49:43,665 - INFO - training batch 551, loss: 0.186, 17632/60000 datapoints
2025-03-06 19:49:43,863 - INFO - training batch 601, loss: 0.211, 19232/60000 datapoints
2025-03-06 19:49:44,061 - INFO - training batch 651, loss: 0.240, 20832/60000 datapoints
2025-03-06 19:49:44,260 - INFO - training batch 701, loss: 0.214, 22432/60000 datapoints
2025-03-06 19:49:44,512 - INFO - training batch 751, loss: 0.229, 24032/60000 datapoints
2025-03-06 19:49:44,732 - INFO - training batch 801, loss: 0.307, 25632/60000 datapoints
2025-03-06 19:49:44,987 - INFO - training batch 851, loss: 0.215, 27232/60000 datapoints
2025-03-06 19:49:45,205 - INFO - training batch 901, loss: 0.136, 28832/60000 datapoints
2025-03-06 19:49:45,450 - INFO - training batch 951, loss: 0.463, 30432/60000 datapoints
2025-03-06 19:49:45,689 - INFO - training batch 1001, loss: 0.400, 32032/60000 datapoints
2025-03-06 19:49:45,927 - INFO - training batch 1051, loss: 0.252, 33632/60000 datapoints
2025-03-06 19:49:46,156 - INFO - training batch 1101, loss: 0.108, 35232/60000 datapoints
2025-03-06 19:49:46,400 - INFO - training batch 1151, loss: 0.365, 36832/60000 datapoints
2025-03-06 19:49:46,642 - INFO - training batch 1201, loss: 0.068, 38432/60000 datapoints
2025-03-06 19:49:46,895 - INFO - training batch 1251, loss: 0.311, 40032/60000 datapoints
2025-03-06 19:49:47,155 - INFO - training batch 1301, loss: 0.079, 41632/60000 datapoints
2025-03-06 19:49:47,400 - INFO - training batch 1351, loss: 0.441, 43232/60000 datapoints
2025-03-06 19:49:47,647 - INFO - training batch 1401, loss: 0.525, 44832/60000 datapoints
2025-03-06 19:49:48,002 - INFO - training batch 1451, loss: 0.379, 46432/60000 datapoints
2025-03-06 19:49:48,218 - INFO - training batch 1501, loss: 0.396, 48032/60000 datapoints
2025-03-06 19:49:48,441 - INFO - training batch 1551, loss: 0.171, 49632/60000 datapoints
2025-03-06 19:49:48,682 - INFO - training batch 1601, loss: 0.284, 51232/60000 datapoints
2025-03-06 19:49:48,912 - INFO - training batch 1651, loss: 0.129, 52832/60000 datapoints
2025-03-06 19:49:49,143 - INFO - training batch 1701, loss: 0.239, 54432/60000 datapoints
2025-03-06 19:49:49,380 - INFO - training batch 1751, loss: 0.650, 56032/60000 datapoints
2025-03-06 19:49:49,610 - INFO - training batch 1801, loss: 0.322, 57632/60000 datapoints
2025-03-06 19:49:49,822 - INFO - training batch 1851, loss: 0.262, 59232/60000 datapoints
2025-03-06 19:49:49,941 - INFO - validation batch 1, loss: 0.182, 32/10016 datapoints
2025-03-06 19:49:50,115 - INFO - validation batch 51, loss: 0.314, 1632/10016 datapoints
2025-03-06 19:49:50,298 - INFO - validation batch 101, loss: 0.104, 3232/10016 datapoints
2025-03-06 19:49:50,469 - INFO - validation batch 151, loss: 0.216, 4832/10016 datapoints
2025-03-06 19:49:50,645 - INFO - validation batch 201, loss: 0.284, 6432/10016 datapoints
2025-03-06 19:49:50,854 - INFO - validation batch 251, loss: 0.186, 8032/10016 datapoints
2025-03-06 19:49:51,026 - INFO - validation batch 301, loss: 0.483, 9632/10016 datapoints
2025-03-06 19:49:51,065 - INFO - Epoch 490/800 done.
2025-03-06 19:49:51,065 - INFO - Final validation performance:
Loss: 0.253, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:49:51,066 - INFO - Beginning epoch 491/800
2025-03-06 19:49:51,074 - INFO - training batch 1, loss: 0.242, 32/60000 datapoints
2025-03-06 19:49:51,309 - INFO - training batch 51, loss: 0.141, 1632/60000 datapoints
2025-03-06 19:49:51,525 - INFO - training batch 101, loss: 0.275, 3232/60000 datapoints
2025-03-06 19:49:51,759 - INFO - training batch 151, loss: 0.279, 4832/60000 datapoints
2025-03-06 19:49:51,976 - INFO - training batch 201, loss: 0.208, 6432/60000 datapoints
2025-03-06 19:49:52,201 - INFO - training batch 251, loss: 0.151, 8032/60000 datapoints
2025-03-06 19:49:52,412 - INFO - training batch 301, loss: 0.400, 9632/60000 datapoints
2025-03-06 19:49:52,615 - INFO - training batch 351, loss: 0.143, 11232/60000 datapoints
2025-03-06 19:49:52,829 - INFO - training batch 401, loss: 0.322, 12832/60000 datapoints
2025-03-06 19:49:53,052 - INFO - training batch 451, loss: 0.303, 14432/60000 datapoints
2025-03-06 19:49:53,270 - INFO - training batch 501, loss: 0.121, 16032/60000 datapoints
2025-03-06 19:49:53,484 - INFO - training batch 551, loss: 0.261, 17632/60000 datapoints
2025-03-06 19:49:53,725 - INFO - training batch 601, loss: 0.252, 19232/60000 datapoints
2025-03-06 19:49:53,967 - INFO - training batch 651, loss: 0.171, 20832/60000 datapoints
2025-03-06 19:49:54,207 - INFO - training batch 701, loss: 0.247, 22432/60000 datapoints
2025-03-06 19:49:54,437 - INFO - training batch 751, loss: 0.137, 24032/60000 datapoints
2025-03-06 19:49:54,644 - INFO - training batch 801, loss: 0.623, 25632/60000 datapoints
2025-03-06 19:49:54,888 - INFO - training batch 851, loss: 0.211, 27232/60000 datapoints
2025-03-06 19:49:55,116 - INFO - training batch 901, loss: 0.174, 28832/60000 datapoints
2025-03-06 19:49:55,356 - INFO - training batch 951, loss: 0.365, 30432/60000 datapoints
2025-03-06 19:49:55,574 - INFO - training batch 1001, loss: 0.232, 32032/60000 datapoints
2025-03-06 19:49:55,780 - INFO - training batch 1051, loss: 0.501, 33632/60000 datapoints
2025-03-06 19:49:55,990 - INFO - training batch 1101, loss: 0.263, 35232/60000 datapoints
2025-03-06 19:49:56,342 - INFO - training batch 1151, loss: 0.145, 36832/60000 datapoints
2025-03-06 19:49:56,560 - INFO - training batch 1201, loss: 0.250, 38432/60000 datapoints
2025-03-06 19:49:56,801 - INFO - training batch 1251, loss: 0.409, 40032/60000 datapoints
2025-03-06 19:49:57,047 - INFO - training batch 1301, loss: 0.227, 41632/60000 datapoints
2025-03-06 19:49:57,289 - INFO - training batch 1351, loss: 0.300, 43232/60000 datapoints
2025-03-06 19:49:57,518 - INFO - training batch 1401, loss: 0.212, 44832/60000 datapoints
2025-03-06 19:49:57,744 - INFO - training batch 1451, loss: 0.330, 46432/60000 datapoints
2025-03-06 19:49:57,973 - INFO - training batch 1501, loss: 0.097, 48032/60000 datapoints
2025-03-06 19:49:58,184 - INFO - training batch 1551, loss: 0.646, 49632/60000 datapoints
2025-03-06 19:49:58,388 - INFO - training batch 1601, loss: 0.120, 51232/60000 datapoints
2025-03-06 19:49:58,628 - INFO - training batch 1651, loss: 0.130, 52832/60000 datapoints
2025-03-06 19:49:58,869 - INFO - training batch 1701, loss: 0.150, 54432/60000 datapoints
2025-03-06 19:49:59,096 - INFO - training batch 1751, loss: 0.390, 56032/60000 datapoints
2025-03-06 19:49:59,302 - INFO - training batch 1801, loss: 0.469, 57632/60000 datapoints
2025-03-06 19:49:59,531 - INFO - training batch 1851, loss: 0.213, 59232/60000 datapoints
2025-03-06 19:49:59,647 - INFO - validation batch 1, loss: 0.312, 32/10016 datapoints
2025-03-06 19:49:59,817 - INFO - validation batch 51, loss: 0.273, 1632/10016 datapoints
2025-03-06 19:49:59,998 - INFO - validation batch 101, loss: 0.132, 3232/10016 datapoints
2025-03-06 19:50:00,169 - INFO - validation batch 151, loss: 0.243, 4832/10016 datapoints
2025-03-06 19:50:00,347 - INFO - validation batch 201, loss: 0.091, 6432/10016 datapoints
2025-03-06 19:50:00,515 - INFO - validation batch 251, loss: 0.226, 8032/10016 datapoints
2025-03-06 19:50:00,682 - INFO - validation batch 301, loss: 0.193, 9632/10016 datapoints
2025-03-06 19:50:00,724 - INFO - Epoch 491/800 done.
2025-03-06 19:50:00,724 - INFO - Final validation performance:
Loss: 0.210, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:50:00,725 - INFO - Beginning epoch 492/800
2025-03-06 19:50:00,732 - INFO - training batch 1, loss: 0.139, 32/60000 datapoints
2025-03-06 19:50:00,998 - INFO - training batch 51, loss: 0.249, 1632/60000 datapoints
2025-03-06 19:50:01,233 - INFO - training batch 101, loss: 0.113, 3232/60000 datapoints
2025-03-06 19:50:01,501 - INFO - training batch 151, loss: 0.061, 4832/60000 datapoints
2025-03-06 19:50:01,745 - INFO - training batch 201, loss: 0.170, 6432/60000 datapoints
2025-03-06 19:50:01,957 - INFO - training batch 251, loss: 0.137, 8032/60000 datapoints
2025-03-06 19:50:02,170 - INFO - training batch 301, loss: 0.435, 9632/60000 datapoints
2025-03-06 19:50:02,392 - INFO - training batch 351, loss: 0.223, 11232/60000 datapoints
2025-03-06 19:50:02,593 - INFO - training batch 401, loss: 0.355, 12832/60000 datapoints
2025-03-06 19:50:02,813 - INFO - training batch 451, loss: 0.236, 14432/60000 datapoints
2025-03-06 19:50:03,017 - INFO - training batch 501, loss: 0.330, 16032/60000 datapoints
2025-03-06 19:50:03,226 - INFO - training batch 551, loss: 0.127, 17632/60000 datapoints
2025-03-06 19:50:03,429 - INFO - training batch 601, loss: 0.234, 19232/60000 datapoints
2025-03-06 19:50:03,631 - INFO - training batch 651, loss: 0.203, 20832/60000 datapoints
2025-03-06 19:50:03,829 - INFO - training batch 701, loss: 0.102, 22432/60000 datapoints
2025-03-06 19:50:04,026 - INFO - training batch 751, loss: 0.412, 24032/60000 datapoints
2025-03-06 19:50:04,218 - INFO - training batch 801, loss: 0.641, 25632/60000 datapoints
2025-03-06 19:50:04,411 - INFO - training batch 851, loss: 0.283, 27232/60000 datapoints
2025-03-06 19:50:04,602 - INFO - training batch 901, loss: 0.169, 28832/60000 datapoints
2025-03-06 19:50:04,801 - INFO - training batch 951, loss: 0.127, 30432/60000 datapoints
2025-03-06 19:50:04,995 - INFO - training batch 1001, loss: 0.372, 32032/60000 datapoints
2025-03-06 19:50:05,187 - INFO - training batch 1051, loss: 0.246, 33632/60000 datapoints
2025-03-06 19:50:05,381 - INFO - training batch 1101, loss: 0.142, 35232/60000 datapoints
2025-03-06 19:50:05,573 - INFO - training batch 1151, loss: 0.470, 36832/60000 datapoints
2025-03-06 19:50:05,769 - INFO - training batch 1201, loss: 0.408, 38432/60000 datapoints
2025-03-06 19:50:05,962 - INFO - training batch 1251, loss: 0.295, 40032/60000 datapoints
2025-03-06 19:50:06,152 - INFO - training batch 1301, loss: 0.093, 41632/60000 datapoints
2025-03-06 19:50:06,361 - INFO - training batch 1351, loss: 0.112, 43232/60000 datapoints
2025-03-06 19:50:06,556 - INFO - training batch 1401, loss: 0.102, 44832/60000 datapoints
2025-03-06 19:50:06,764 - INFO - training batch 1451, loss: 0.234, 46432/60000 datapoints
2025-03-06 19:50:06,966 - INFO - training batch 1501, loss: 0.305, 48032/60000 datapoints
2025-03-06 19:50:07,178 - INFO - training batch 1551, loss: 0.082, 49632/60000 datapoints
2025-03-06 19:50:07,382 - INFO - training batch 1601, loss: 0.409, 51232/60000 datapoints
2025-03-06 19:50:07,592 - INFO - training batch 1651, loss: 0.556, 52832/60000 datapoints
2025-03-06 19:50:07,802 - INFO - training batch 1701, loss: 0.591, 54432/60000 datapoints
2025-03-06 19:50:08,003 - INFO - training batch 1751, loss: 0.216, 56032/60000 datapoints
2025-03-06 19:50:08,212 - INFO - training batch 1801, loss: 0.460, 57632/60000 datapoints
2025-03-06 19:50:08,411 - INFO - training batch 1851, loss: 0.442, 59232/60000 datapoints
2025-03-06 19:50:08,514 - INFO - validation batch 1, loss: 0.200, 32/10016 datapoints
2025-03-06 19:50:08,672 - INFO - validation batch 51, loss: 0.224, 1632/10016 datapoints
2025-03-06 19:50:08,885 - INFO - validation batch 101, loss: 0.113, 3232/10016 datapoints
2025-03-06 19:50:09,054 - INFO - validation batch 151, loss: 0.294, 4832/10016 datapoints
2025-03-06 19:50:09,218 - INFO - validation batch 201, loss: 0.218, 6432/10016 datapoints
2025-03-06 19:50:09,446 - INFO - validation batch 251, loss: 0.210, 8032/10016 datapoints
2025-03-06 19:50:09,629 - INFO - validation batch 301, loss: 0.224, 9632/10016 datapoints
2025-03-06 19:50:09,674 - INFO - Epoch 492/800 done.
2025-03-06 19:50:09,674 - INFO - Final validation performance:
Loss: 0.212, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:50:09,675 - INFO - Beginning epoch 493/800
2025-03-06 19:50:09,682 - INFO - training batch 1, loss: 0.089, 32/60000 datapoints
2025-03-06 19:50:09,915 - INFO - training batch 51, loss: 0.323, 1632/60000 datapoints
2025-03-06 19:50:10,128 - INFO - training batch 101, loss: 0.057, 3232/60000 datapoints
2025-03-06 19:50:10,349 - INFO - training batch 151, loss: 0.314, 4832/60000 datapoints
2025-03-06 19:50:10,700 - INFO - training batch 201, loss: 0.217, 6432/60000 datapoints
2025-03-06 19:50:10,940 - INFO - training batch 251, loss: 0.106, 8032/60000 datapoints
2025-03-06 19:50:11,174 - INFO - training batch 301, loss: 0.432, 9632/60000 datapoints
2025-03-06 19:50:11,395 - INFO - training batch 351, loss: 0.073, 11232/60000 datapoints
2025-03-06 19:50:11,620 - INFO - training batch 401, loss: 0.157, 12832/60000 datapoints
2025-03-06 19:50:11,841 - INFO - training batch 451, loss: 0.183, 14432/60000 datapoints
2025-03-06 19:50:12,067 - INFO - training batch 501, loss: 0.084, 16032/60000 datapoints
2025-03-06 19:50:12,303 - INFO - training batch 551, loss: 0.324, 17632/60000 datapoints
2025-03-06 19:50:12,530 - INFO - training batch 601, loss: 0.365, 19232/60000 datapoints
2025-03-06 19:50:12,741 - INFO - training batch 651, loss: 0.188, 20832/60000 datapoints
2025-03-06 19:50:12,973 - INFO - training batch 701, loss: 0.569, 22432/60000 datapoints
2025-03-06 19:50:13,199 - INFO - training batch 751, loss: 0.096, 24032/60000 datapoints
2025-03-06 19:50:13,432 - INFO - training batch 801, loss: 0.324, 25632/60000 datapoints
2025-03-06 19:50:13,655 - INFO - training batch 851, loss: 0.195, 27232/60000 datapoints
2025-03-06 19:50:13,875 - INFO - training batch 901, loss: 0.074, 28832/60000 datapoints
2025-03-06 19:50:14,086 - INFO - training batch 951, loss: 0.303, 30432/60000 datapoints
2025-03-06 19:50:14,300 - INFO - training batch 1001, loss: 0.178, 32032/60000 datapoints
2025-03-06 19:50:14,518 - INFO - training batch 1051, loss: 0.353, 33632/60000 datapoints
2025-03-06 19:50:14,737 - INFO - training batch 1101, loss: 0.165, 35232/60000 datapoints
2025-03-06 19:50:14,955 - INFO - training batch 1151, loss: 0.294, 36832/60000 datapoints
2025-03-06 19:50:15,166 - INFO - training batch 1201, loss: 0.164, 38432/60000 datapoints
2025-03-06 19:50:15,385 - INFO - training batch 1251, loss: 0.315, 40032/60000 datapoints
2025-03-06 19:50:15,596 - INFO - training batch 1301, loss: 0.418, 41632/60000 datapoints
2025-03-06 19:50:15,820 - INFO - training batch 1351, loss: 0.155, 43232/60000 datapoints
2025-03-06 19:50:16,028 - INFO - training batch 1401, loss: 0.221, 44832/60000 datapoints
2025-03-06 19:50:16,230 - INFO - training batch 1451, loss: 0.312, 46432/60000 datapoints
2025-03-06 19:50:16,449 - INFO - training batch 1501, loss: 0.223, 48032/60000 datapoints
2025-03-06 19:50:16,683 - INFO - training batch 1551, loss: 0.411, 49632/60000 datapoints
2025-03-06 19:50:16,901 - INFO - training batch 1601, loss: 0.160, 51232/60000 datapoints
2025-03-06 19:50:17,100 - INFO - training batch 1651, loss: 0.185, 52832/60000 datapoints
2025-03-06 19:50:17,305 - INFO - training batch 1701, loss: 0.235, 54432/60000 datapoints
2025-03-06 19:50:17,534 - INFO - training batch 1751, loss: 0.265, 56032/60000 datapoints
2025-03-06 19:50:17,754 - INFO - training batch 1801, loss: 0.182, 57632/60000 datapoints
2025-03-06 19:50:17,969 - INFO - training batch 1851, loss: 0.076, 59232/60000 datapoints
2025-03-06 19:50:18,076 - INFO - validation batch 1, loss: 0.306, 32/10016 datapoints
2025-03-06 19:50:18,249 - INFO - validation batch 51, loss: 0.417, 1632/10016 datapoints
2025-03-06 19:50:18,409 - INFO - validation batch 101, loss: 0.253, 3232/10016 datapoints
2025-03-06 19:50:18,580 - INFO - validation batch 151, loss: 0.458, 4832/10016 datapoints
2025-03-06 19:50:18,762 - INFO - validation batch 201, loss: 0.363, 6432/10016 datapoints
2025-03-06 19:50:18,929 - INFO - validation batch 251, loss: 0.095, 8032/10016 datapoints
2025-03-06 19:50:19,089 - INFO - validation batch 301, loss: 0.612, 9632/10016 datapoints
2025-03-06 19:50:19,130 - INFO - Epoch 493/800 done.
2025-03-06 19:50:19,130 - INFO - Final validation performance:
Loss: 0.358, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:50:19,130 - INFO - Beginning epoch 494/800
2025-03-06 19:50:19,137 - INFO - training batch 1, loss: 0.146, 32/60000 datapoints
2025-03-06 19:50:19,375 - INFO - training batch 51, loss: 0.295, 1632/60000 datapoints
2025-03-06 19:50:19,577 - INFO - training batch 101, loss: 0.243, 3232/60000 datapoints
2025-03-06 19:50:19,814 - INFO - training batch 151, loss: 0.121, 4832/60000 datapoints
2025-03-06 19:50:20,093 - INFO - training batch 201, loss: 0.285, 6432/60000 datapoints
2025-03-06 19:50:20,343 - INFO - training batch 251, loss: 0.339, 8032/60000 datapoints
2025-03-06 19:50:20,630 - INFO - training batch 301, loss: 0.241, 9632/60000 datapoints
2025-03-06 19:50:20,866 - INFO - training batch 351, loss: 0.186, 11232/60000 datapoints
2025-03-06 19:50:21,087 - INFO - training batch 401, loss: 0.432, 12832/60000 datapoints
2025-03-06 19:50:21,318 - INFO - training batch 451, loss: 0.129, 14432/60000 datapoints
2025-03-06 19:50:21,567 - INFO - training batch 501, loss: 0.383, 16032/60000 datapoints
2025-03-06 19:50:21,820 - INFO - training batch 551, loss: 0.462, 17632/60000 datapoints
2025-03-06 19:50:22,053 - INFO - training batch 601, loss: 0.391, 19232/60000 datapoints
2025-03-06 19:50:22,255 - INFO - training batch 651, loss: 0.202, 20832/60000 datapoints
2025-03-06 19:50:22,459 - INFO - training batch 701, loss: 0.445, 22432/60000 datapoints
2025-03-06 19:50:22,665 - INFO - training batch 751, loss: 0.308, 24032/60000 datapoints
2025-03-06 19:50:22,866 - INFO - training batch 801, loss: 0.440, 25632/60000 datapoints
2025-03-06 19:50:23,063 - INFO - training batch 851, loss: 0.277, 27232/60000 datapoints
2025-03-06 19:50:23,259 - INFO - training batch 901, loss: 0.110, 28832/60000 datapoints
2025-03-06 19:50:23,502 - INFO - training batch 951, loss: 0.255, 30432/60000 datapoints
2025-03-06 19:50:23,716 - INFO - training batch 1001, loss: 0.605, 32032/60000 datapoints
2025-03-06 19:50:23,947 - INFO - training batch 1051, loss: 0.596, 33632/60000 datapoints
2025-03-06 19:50:24,155 - INFO - training batch 1101, loss: 0.235, 35232/60000 datapoints
2025-03-06 19:50:24,389 - INFO - training batch 1151, loss: 0.320, 36832/60000 datapoints
2025-03-06 19:50:24,608 - INFO - training batch 1201, loss: 0.263, 38432/60000 datapoints
2025-03-06 19:50:24,827 - INFO - training batch 1251, loss: 0.313, 40032/60000 datapoints
2025-03-06 19:50:25,035 - INFO - training batch 1301, loss: 0.097, 41632/60000 datapoints
2025-03-06 19:50:25,239 - INFO - training batch 1351, loss: 0.168, 43232/60000 datapoints
2025-03-06 19:50:25,466 - INFO - training batch 1401, loss: 0.601, 44832/60000 datapoints
2025-03-06 19:50:25,700 - INFO - training batch 1451, loss: 0.268, 46432/60000 datapoints
2025-03-06 19:50:25,922 - INFO - training batch 1501, loss: 0.254, 48032/60000 datapoints
2025-03-06 19:50:26,119 - INFO - training batch 1551, loss: 0.288, 49632/60000 datapoints
2025-03-06 19:50:26,314 - INFO - training batch 1601, loss: 0.137, 51232/60000 datapoints
2025-03-06 19:50:26,548 - INFO - training batch 1651, loss: 0.240, 52832/60000 datapoints
2025-03-06 19:50:26,933 - INFO - training batch 1701, loss: 0.255, 54432/60000 datapoints
2025-03-06 19:50:27,210 - INFO - training batch 1751, loss: 0.233, 56032/60000 datapoints
2025-03-06 19:50:27,417 - INFO - training batch 1801, loss: 0.163, 57632/60000 datapoints
2025-03-06 19:50:27,662 - INFO - training batch 1851, loss: 0.231, 59232/60000 datapoints
2025-03-06 19:50:27,782 - INFO - validation batch 1, loss: 0.330, 32/10016 datapoints
2025-03-06 19:50:27,945 - INFO - validation batch 51, loss: 0.232, 1632/10016 datapoints
2025-03-06 19:50:28,109 - INFO - validation batch 101, loss: 0.324, 3232/10016 datapoints
2025-03-06 19:50:28,273 - INFO - validation batch 151, loss: 0.494, 4832/10016 datapoints
2025-03-06 19:50:28,438 - INFO - validation batch 201, loss: 0.129, 6432/10016 datapoints
2025-03-06 19:50:28,612 - INFO - validation batch 251, loss: 0.352, 8032/10016 datapoints
2025-03-06 19:50:28,797 - INFO - validation batch 301, loss: 0.454, 9632/10016 datapoints
2025-03-06 19:50:28,836 - INFO - Epoch 494/800 done.
2025-03-06 19:50:28,836 - INFO - Final validation performance:
Loss: 0.331, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:50:28,837 - INFO - Beginning epoch 495/800
2025-03-06 19:50:28,843 - INFO - training batch 1, loss: 0.545, 32/60000 datapoints
2025-03-06 19:50:29,075 - INFO - training batch 51, loss: 0.419, 1632/60000 datapoints
2025-03-06 19:50:29,288 - INFO - training batch 101, loss: 0.102, 3232/60000 datapoints
2025-03-06 19:50:29,504 - INFO - training batch 151, loss: 0.304, 4832/60000 datapoints
2025-03-06 19:50:29,722 - INFO - training batch 201, loss: 0.227, 6432/60000 datapoints
2025-03-06 19:50:29,940 - INFO - training batch 251, loss: 0.414, 8032/60000 datapoints
2025-03-06 19:50:30,152 - INFO - training batch 301, loss: 0.304, 9632/60000 datapoints
2025-03-06 19:50:30,368 - INFO - training batch 351, loss: 0.327, 11232/60000 datapoints
2025-03-06 19:50:30,584 - INFO - training batch 401, loss: 0.383, 12832/60000 datapoints
2025-03-06 19:50:30,798 - INFO - training batch 451, loss: 0.138, 14432/60000 datapoints
2025-03-06 19:50:31,004 - INFO - training batch 501, loss: 0.262, 16032/60000 datapoints
2025-03-06 19:50:31,203 - INFO - training batch 551, loss: 0.286, 17632/60000 datapoints
2025-03-06 19:50:31,404 - INFO - training batch 601, loss: 0.353, 19232/60000 datapoints
2025-03-06 19:50:31,632 - INFO - training batch 651, loss: 0.212, 20832/60000 datapoints
2025-03-06 19:50:31,831 - INFO - training batch 701, loss: 0.091, 22432/60000 datapoints
2025-03-06 19:50:32,028 - INFO - training batch 751, loss: 0.079, 24032/60000 datapoints
2025-03-06 19:50:32,236 - INFO - training batch 801, loss: 0.199, 25632/60000 datapoints
2025-03-06 19:50:32,440 - INFO - training batch 851, loss: 0.277, 27232/60000 datapoints
2025-03-06 19:50:32,651 - INFO - training batch 901, loss: 0.358, 28832/60000 datapoints
2025-03-06 19:50:32,868 - INFO - training batch 951, loss: 0.370, 30432/60000 datapoints
2025-03-06 19:50:33,112 - INFO - training batch 1001, loss: 0.224, 32032/60000 datapoints
2025-03-06 19:50:33,313 - INFO - training batch 1051, loss: 0.325, 33632/60000 datapoints
2025-03-06 19:50:33,531 - INFO - training batch 1101, loss: 0.316, 35232/60000 datapoints
2025-03-06 19:50:33,763 - INFO - training batch 1151, loss: 0.535, 36832/60000 datapoints
2025-03-06 19:50:33,962 - INFO - training batch 1201, loss: 0.364, 38432/60000 datapoints
2025-03-06 19:50:34,207 - INFO - training batch 1251, loss: 0.490, 40032/60000 datapoints
2025-03-06 19:50:34,426 - INFO - training batch 1301, loss: 0.268, 41632/60000 datapoints
2025-03-06 19:50:34,667 - INFO - training batch 1351, loss: 0.177, 43232/60000 datapoints
2025-03-06 19:50:34,891 - INFO - training batch 1401, loss: 0.231, 44832/60000 datapoints
2025-03-06 19:50:35,089 - INFO - training batch 1451, loss: 0.322, 46432/60000 datapoints
2025-03-06 19:50:35,302 - INFO - training batch 1501, loss: 0.148, 48032/60000 datapoints
2025-03-06 19:50:35,574 - INFO - training batch 1551, loss: 0.285, 49632/60000 datapoints
2025-03-06 19:50:35,846 - INFO - training batch 1601, loss: 0.191, 51232/60000 datapoints
2025-03-06 19:50:36,102 - INFO - training batch 1651, loss: 0.171, 52832/60000 datapoints
2025-03-06 19:50:36,357 - INFO - training batch 1701, loss: 0.136, 54432/60000 datapoints
2025-03-06 19:50:36,604 - INFO - training batch 1751, loss: 0.108, 56032/60000 datapoints
2025-03-06 19:50:36,887 - INFO - training batch 1801, loss: 0.373, 57632/60000 datapoints
2025-03-06 19:50:37,156 - INFO - training batch 1851, loss: 0.214, 59232/60000 datapoints
2025-03-06 19:50:37,267 - INFO - validation batch 1, loss: 0.134, 32/10016 datapoints
2025-03-06 19:50:37,490 - INFO - validation batch 51, loss: 0.428, 1632/10016 datapoints
2025-03-06 19:50:37,771 - INFO - validation batch 101, loss: 0.091, 3232/10016 datapoints
2025-03-06 19:50:37,992 - INFO - validation batch 151, loss: 0.107, 4832/10016 datapoints
2025-03-06 19:50:38,214 - INFO - validation batch 201, loss: 0.180, 6432/10016 datapoints
2025-03-06 19:50:38,440 - INFO - validation batch 251, loss: 0.259, 8032/10016 datapoints
2025-03-06 19:50:38,654 - INFO - validation batch 301, loss: 0.380, 9632/10016 datapoints
2025-03-06 19:50:38,708 - INFO - Epoch 495/800 done.
2025-03-06 19:50:38,709 - INFO - Final validation performance:
Loss: 0.225, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:50:38,709 - INFO - Beginning epoch 496/800
2025-03-06 19:50:38,719 - INFO - training batch 1, loss: 0.614, 32/60000 datapoints
2025-03-06 19:50:39,013 - INFO - training batch 51, loss: 0.119, 1632/60000 datapoints
2025-03-06 19:50:39,245 - INFO - training batch 101, loss: 0.295, 3232/60000 datapoints
2025-03-06 19:50:39,467 - INFO - training batch 151, loss: 0.082, 4832/60000 datapoints
2025-03-06 19:50:39,727 - INFO - training batch 201, loss: 0.114, 6432/60000 datapoints
2025-03-06 19:50:39,940 - INFO - training batch 251, loss: 0.152, 8032/60000 datapoints
2025-03-06 19:50:40,186 - INFO - training batch 301, loss: 0.327, 9632/60000 datapoints
2025-03-06 19:50:40,391 - INFO - training batch 351, loss: 0.307, 11232/60000 datapoints
2025-03-06 19:50:40,592 - INFO - training batch 401, loss: 0.113, 12832/60000 datapoints
2025-03-06 19:50:40,796 - INFO - training batch 451, loss: 0.230, 14432/60000 datapoints
2025-03-06 19:50:41,010 - INFO - training batch 501, loss: 0.293, 16032/60000 datapoints
2025-03-06 19:50:41,225 - INFO - training batch 551, loss: 0.162, 17632/60000 datapoints
2025-03-06 19:50:41,452 - INFO - training batch 601, loss: 0.277, 19232/60000 datapoints
2025-03-06 19:50:41,678 - INFO - training batch 651, loss: 0.262, 20832/60000 datapoints
2025-03-06 19:50:41,879 - INFO - training batch 701, loss: 0.141, 22432/60000 datapoints
2025-03-06 19:50:42,078 - INFO - training batch 751, loss: 0.223, 24032/60000 datapoints
2025-03-06 19:50:42,288 - INFO - training batch 801, loss: 0.262, 25632/60000 datapoints
2025-03-06 19:50:42,494 - INFO - training batch 851, loss: 0.302, 27232/60000 datapoints
2025-03-06 19:50:42,703 - INFO - training batch 901, loss: 0.346, 28832/60000 datapoints
2025-03-06 19:50:42,909 - INFO - training batch 951, loss: 0.267, 30432/60000 datapoints
2025-03-06 19:50:43,116 - INFO - training batch 1001, loss: 0.312, 32032/60000 datapoints
2025-03-06 19:50:43,312 - INFO - training batch 1051, loss: 0.175, 33632/60000 datapoints
2025-03-06 19:50:43,521 - INFO - training batch 1101, loss: 0.224, 35232/60000 datapoints
2025-03-06 19:50:43,733 - INFO - training batch 1151, loss: 0.399, 36832/60000 datapoints
2025-03-06 19:50:43,946 - INFO - training batch 1201, loss: 0.540, 38432/60000 datapoints
2025-03-06 19:50:44,146 - INFO - training batch 1251, loss: 0.295, 40032/60000 datapoints
2025-03-06 19:50:44,351 - INFO - training batch 1301, loss: 0.235, 41632/60000 datapoints
2025-03-06 19:50:44,557 - INFO - training batch 1351, loss: 0.172, 43232/60000 datapoints
2025-03-06 19:50:44,764 - INFO - training batch 1401, loss: 0.175, 44832/60000 datapoints
2025-03-06 19:50:45,009 - INFO - training batch 1451, loss: 0.181, 46432/60000 datapoints
2025-03-06 19:50:45,207 - INFO - training batch 1501, loss: 0.112, 48032/60000 datapoints
2025-03-06 19:50:45,407 - INFO - training batch 1551, loss: 0.361, 49632/60000 datapoints
2025-03-06 19:50:45,606 - INFO - training batch 1601, loss: 0.152, 51232/60000 datapoints
2025-03-06 19:50:45,804 - INFO - training batch 1651, loss: 0.519, 52832/60000 datapoints
2025-03-06 19:50:46,007 - INFO - training batch 1701, loss: 0.104, 54432/60000 datapoints
2025-03-06 19:50:46,204 - INFO - training batch 1751, loss: 0.120, 56032/60000 datapoints
2025-03-06 19:50:46,401 - INFO - training batch 1801, loss: 0.567, 57632/60000 datapoints
2025-03-06 19:50:46,608 - INFO - training batch 1851, loss: 0.139, 59232/60000 datapoints
2025-03-06 19:50:46,736 - INFO - validation batch 1, loss: 0.206, 32/10016 datapoints
2025-03-06 19:50:46,893 - INFO - validation batch 51, loss: 0.171, 1632/10016 datapoints
2025-03-06 19:50:47,046 - INFO - validation batch 101, loss: 0.121, 3232/10016 datapoints
2025-03-06 19:50:47,200 - INFO - validation batch 151, loss: 0.248, 4832/10016 datapoints
2025-03-06 19:50:47,352 - INFO - validation batch 201, loss: 0.461, 6432/10016 datapoints
2025-03-06 19:50:47,506 - INFO - validation batch 251, loss: 0.147, 8032/10016 datapoints
2025-03-06 19:50:47,663 - INFO - validation batch 301, loss: 0.425, 9632/10016 datapoints
2025-03-06 19:50:47,701 - INFO - Epoch 496/800 done.
2025-03-06 19:50:47,701 - INFO - Final validation performance:
Loss: 0.254, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:50:47,702 - INFO - Beginning epoch 497/800
2025-03-06 19:50:47,709 - INFO - training batch 1, loss: 0.257, 32/60000 datapoints
2025-03-06 19:50:47,927 - INFO - training batch 51, loss: 0.880, 1632/60000 datapoints
2025-03-06 19:50:48,148 - INFO - training batch 101, loss: 0.315, 3232/60000 datapoints
2025-03-06 19:50:48,368 - INFO - training batch 151, loss: 0.228, 4832/60000 datapoints
2025-03-06 19:50:48,580 - INFO - training batch 201, loss: 0.165, 6432/60000 datapoints
2025-03-06 19:50:48,809 - INFO - training batch 251, loss: 0.114, 8032/60000 datapoints
2025-03-06 19:50:49,062 - INFO - training batch 301, loss: 0.508, 9632/60000 datapoints
2025-03-06 19:50:49,404 - INFO - training batch 351, loss: 0.213, 11232/60000 datapoints
2025-03-06 19:50:49,684 - INFO - training batch 401, loss: 0.287, 12832/60000 datapoints
2025-03-06 19:50:49,898 - INFO - training batch 451, loss: 0.144, 14432/60000 datapoints
2025-03-06 19:50:50,114 - INFO - training batch 501, loss: 0.229, 16032/60000 datapoints
2025-03-06 19:50:50,317 - INFO - training batch 551, loss: 0.479, 17632/60000 datapoints
2025-03-06 19:50:50,557 - INFO - training batch 601, loss: 0.404, 19232/60000 datapoints
2025-03-06 19:50:50,796 - INFO - training batch 651, loss: 0.121, 20832/60000 datapoints
2025-03-06 19:50:51,038 - INFO - training batch 701, loss: 0.226, 22432/60000 datapoints
2025-03-06 19:50:51,277 - INFO - training batch 751, loss: 0.227, 24032/60000 datapoints
2025-03-06 19:50:51,514 - INFO - training batch 801, loss: 0.446, 25632/60000 datapoints
2025-03-06 19:50:51,736 - INFO - training batch 851, loss: 0.313, 27232/60000 datapoints
2025-03-06 19:50:51,944 - INFO - training batch 901, loss: 0.184, 28832/60000 datapoints
2025-03-06 19:50:52,156 - INFO - training batch 951, loss: 0.363, 30432/60000 datapoints
2025-03-06 19:50:52,378 - INFO - training batch 1001, loss: 0.198, 32032/60000 datapoints
2025-03-06 19:50:52,605 - INFO - training batch 1051, loss: 0.297, 33632/60000 datapoints
2025-03-06 19:50:52,845 - INFO - training batch 1101, loss: 0.189, 35232/60000 datapoints
2025-03-06 19:50:53,068 - INFO - training batch 1151, loss: 0.277, 36832/60000 datapoints
2025-03-06 19:50:53,278 - INFO - training batch 1201, loss: 0.252, 38432/60000 datapoints
2025-03-06 19:50:53,494 - INFO - training batch 1251, loss: 0.273, 40032/60000 datapoints
2025-03-06 19:50:53,724 - INFO - training batch 1301, loss: 0.106, 41632/60000 datapoints
2025-03-06 19:50:53,929 - INFO - training batch 1351, loss: 0.154, 43232/60000 datapoints
2025-03-06 19:50:54,145 - INFO - training batch 1401, loss: 0.329, 44832/60000 datapoints
2025-03-06 19:50:54,344 - INFO - training batch 1451, loss: 0.309, 46432/60000 datapoints
2025-03-06 19:50:54,544 - INFO - training batch 1501, loss: 0.310, 48032/60000 datapoints
2025-03-06 19:50:54,745 - INFO - training batch 1551, loss: 0.160, 49632/60000 datapoints
2025-03-06 19:50:54,953 - INFO - training batch 1601, loss: 0.275, 51232/60000 datapoints
2025-03-06 19:50:55,154 - INFO - training batch 1651, loss: 0.140, 52832/60000 datapoints
2025-03-06 19:50:55,353 - INFO - training batch 1701, loss: 0.501, 54432/60000 datapoints
2025-03-06 19:50:55,560 - INFO - training batch 1751, loss: 0.045, 56032/60000 datapoints
2025-03-06 19:50:55,773 - INFO - training batch 1801, loss: 0.277, 57632/60000 datapoints
2025-03-06 19:50:55,970 - INFO - training batch 1851, loss: 0.086, 59232/60000 datapoints
2025-03-06 19:50:56,085 - INFO - validation batch 1, loss: 0.224, 32/10016 datapoints
2025-03-06 19:50:56,244 - INFO - validation batch 51, loss: 0.267, 1632/10016 datapoints
2025-03-06 19:50:56,398 - INFO - validation batch 101, loss: 0.200, 3232/10016 datapoints
2025-03-06 19:50:56,575 - INFO - validation batch 151, loss: 0.339, 4832/10016 datapoints
2025-03-06 19:50:56,772 - INFO - validation batch 201, loss: 0.313, 6432/10016 datapoints
2025-03-06 19:50:56,951 - INFO - validation batch 251, loss: 0.478, 8032/10016 datapoints
2025-03-06 19:50:57,122 - INFO - validation batch 301, loss: 0.134, 9632/10016 datapoints
2025-03-06 19:50:57,170 - INFO - Epoch 497/800 done.
2025-03-06 19:50:57,170 - INFO - Final validation performance:
Loss: 0.279, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:50:57,171 - INFO - Beginning epoch 498/800
2025-03-06 19:50:57,178 - INFO - training batch 1, loss: 0.231, 32/60000 datapoints
2025-03-06 19:50:57,388 - INFO - training batch 51, loss: 0.153, 1632/60000 datapoints
2025-03-06 19:50:57,622 - INFO - training batch 101, loss: 0.090, 3232/60000 datapoints
2025-03-06 19:50:57,844 - INFO - training batch 151, loss: 0.334, 4832/60000 datapoints
2025-03-06 19:50:58,046 - INFO - training batch 201, loss: 0.198, 6432/60000 datapoints
2025-03-06 19:50:58,277 - INFO - training batch 251, loss: 0.269, 8032/60000 datapoints
2025-03-06 19:50:58,491 - INFO - training batch 301, loss: 0.387, 9632/60000 datapoints
2025-03-06 19:50:58,708 - INFO - training batch 351, loss: 0.147, 11232/60000 datapoints
2025-03-06 19:50:59,032 - INFO - training batch 401, loss: 0.128, 12832/60000 datapoints
2025-03-06 19:50:59,246 - INFO - training batch 451, loss: 0.395, 14432/60000 datapoints
2025-03-06 19:50:59,460 - INFO - training batch 501, loss: 0.292, 16032/60000 datapoints
2025-03-06 19:50:59,690 - INFO - training batch 551, loss: 0.276, 17632/60000 datapoints
2025-03-06 19:50:59,920 - INFO - training batch 601, loss: 0.451, 19232/60000 datapoints
2025-03-06 19:51:00,201 - INFO - training batch 651, loss: 0.163, 20832/60000 datapoints
2025-03-06 19:51:00,445 - INFO - training batch 701, loss: 0.254, 22432/60000 datapoints
2025-03-06 19:51:00,726 - INFO - training batch 751, loss: 0.247, 24032/60000 datapoints
2025-03-06 19:51:00,958 - INFO - training batch 801, loss: 0.302, 25632/60000 datapoints
2025-03-06 19:51:01,193 - INFO - training batch 851, loss: 0.224, 27232/60000 datapoints
2025-03-06 19:51:01,419 - INFO - training batch 901, loss: 0.282, 28832/60000 datapoints
2025-03-06 19:51:01,710 - INFO - training batch 951, loss: 0.219, 30432/60000 datapoints
2025-03-06 19:51:01,936 - INFO - training batch 1001, loss: 0.347, 32032/60000 datapoints
2025-03-06 19:51:02,154 - INFO - training batch 1051, loss: 0.078, 33632/60000 datapoints
2025-03-06 19:51:02,400 - INFO - training batch 1101, loss: 0.517, 35232/60000 datapoints
2025-03-06 19:51:02,612 - INFO - training batch 1151, loss: 0.328, 36832/60000 datapoints
2025-03-06 19:51:02,838 - INFO - training batch 1201, loss: 0.436, 38432/60000 datapoints
2025-03-06 19:51:03,043 - INFO - training batch 1251, loss: 0.195, 40032/60000 datapoints
2025-03-06 19:51:03,260 - INFO - training batch 1301, loss: 0.069, 41632/60000 datapoints
2025-03-06 19:51:03,476 - INFO - training batch 1351, loss: 0.222, 43232/60000 datapoints
2025-03-06 19:51:03,691 - INFO - training batch 1401, loss: 0.084, 44832/60000 datapoints
2025-03-06 19:51:03,917 - INFO - training batch 1451, loss: 0.382, 46432/60000 datapoints
2025-03-06 19:51:04,160 - INFO - training batch 1501, loss: 0.098, 48032/60000 datapoints
2025-03-06 19:51:04,375 - INFO - training batch 1551, loss: 0.417, 49632/60000 datapoints
2025-03-06 19:51:04,613 - INFO - training batch 1601, loss: 0.281, 51232/60000 datapoints
2025-03-06 19:51:04,842 - INFO - training batch 1651, loss: 0.161, 52832/60000 datapoints
2025-03-06 19:51:05,120 - INFO - training batch 1701, loss: 0.221, 54432/60000 datapoints
2025-03-06 19:51:05,358 - INFO - training batch 1751, loss: 0.138, 56032/60000 datapoints
2025-03-06 19:51:05,600 - INFO - training batch 1801, loss: 0.356, 57632/60000 datapoints
2025-03-06 19:51:05,858 - INFO - training batch 1851, loss: 0.078, 59232/60000 datapoints
2025-03-06 19:51:05,989 - INFO - validation batch 1, loss: 0.278, 32/10016 datapoints
2025-03-06 19:51:06,174 - INFO - validation batch 51, loss: 0.551, 1632/10016 datapoints
2025-03-06 19:51:06,369 - INFO - validation batch 101, loss: 0.062, 3232/10016 datapoints
2025-03-06 19:51:06,544 - INFO - validation batch 151, loss: 0.480, 4832/10016 datapoints
2025-03-06 19:51:06,766 - INFO - validation batch 201, loss: 0.184, 6432/10016 datapoints
2025-03-06 19:51:06,964 - INFO - validation batch 251, loss: 0.504, 8032/10016 datapoints
2025-03-06 19:51:07,197 - INFO - validation batch 301, loss: 0.089, 9632/10016 datapoints
2025-03-06 19:51:07,243 - INFO - Epoch 498/800 done.
2025-03-06 19:51:07,243 - INFO - Final validation performance:
Loss: 0.307, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:51:07,244 - INFO - Beginning epoch 499/800
2025-03-06 19:51:07,251 - INFO - training batch 1, loss: 0.165, 32/60000 datapoints
2025-03-06 19:51:07,477 - INFO - training batch 51, loss: 0.467, 1632/60000 datapoints
2025-03-06 19:51:07,713 - INFO - training batch 101, loss: 0.300, 3232/60000 datapoints
2025-03-06 19:51:07,951 - INFO - training batch 151, loss: 0.409, 4832/60000 datapoints
2025-03-06 19:51:08,213 - INFO - training batch 201, loss: 0.268, 6432/60000 datapoints
2025-03-06 19:51:08,448 - INFO - training batch 251, loss: 0.374, 8032/60000 datapoints
2025-03-06 19:51:08,669 - INFO - training batch 301, loss: 0.363, 9632/60000 datapoints
2025-03-06 19:51:08,917 - INFO - training batch 351, loss: 0.267, 11232/60000 datapoints
2025-03-06 19:51:09,195 - INFO - training batch 401, loss: 0.280, 12832/60000 datapoints
2025-03-06 19:51:09,416 - INFO - training batch 451, loss: 0.354, 14432/60000 datapoints
2025-03-06 19:51:09,648 - INFO - training batch 501, loss: 0.599, 16032/60000 datapoints
2025-03-06 19:51:09,866 - INFO - training batch 551, loss: 0.309, 17632/60000 datapoints
2025-03-06 19:51:10,089 - INFO - training batch 601, loss: 0.241, 19232/60000 datapoints
2025-03-06 19:51:10,314 - INFO - training batch 651, loss: 0.855, 20832/60000 datapoints
2025-03-06 19:51:10,534 - INFO - training batch 701, loss: 0.210, 22432/60000 datapoints
2025-03-06 19:51:10,752 - INFO - training batch 751, loss: 0.080, 24032/60000 datapoints
2025-03-06 19:51:10,987 - INFO - training batch 801, loss: 0.114, 25632/60000 datapoints
2025-03-06 19:51:11,220 - INFO - training batch 851, loss: 0.561, 27232/60000 datapoints
2025-03-06 19:51:11,457 - INFO - training batch 901, loss: 0.365, 28832/60000 datapoints
2025-03-06 19:51:11,701 - INFO - training batch 951, loss: 0.191, 30432/60000 datapoints
2025-03-06 19:51:11,929 - INFO - training batch 1001, loss: 0.187, 32032/60000 datapoints
2025-03-06 19:51:12,154 - INFO - training batch 1051, loss: 0.271, 33632/60000 datapoints
2025-03-06 19:51:12,355 - INFO - training batch 1101, loss: 0.348, 35232/60000 datapoints
2025-03-06 19:51:12,551 - INFO - training batch 1151, loss: 0.272, 36832/60000 datapoints
2025-03-06 19:51:12,761 - INFO - training batch 1201, loss: 0.370, 38432/60000 datapoints
2025-03-06 19:51:12,956 - INFO - training batch 1251, loss: 0.152, 40032/60000 datapoints
2025-03-06 19:51:13,168 - INFO - training batch 1301, loss: 0.080, 41632/60000 datapoints
2025-03-06 19:51:13,380 - INFO - training batch 1351, loss: 0.358, 43232/60000 datapoints
2025-03-06 19:51:13,611 - INFO - training batch 1401, loss: 0.261, 44832/60000 datapoints
2025-03-06 19:51:13,822 - INFO - training batch 1451, loss: 0.278, 46432/60000 datapoints
2025-03-06 19:51:14,020 - INFO - training batch 1501, loss: 0.255, 48032/60000 datapoints
2025-03-06 19:51:14,215 - INFO - training batch 1551, loss: 0.157, 49632/60000 datapoints
2025-03-06 19:51:14,416 - INFO - training batch 1601, loss: 0.274, 51232/60000 datapoints
2025-03-06 19:51:14,615 - INFO - training batch 1651, loss: 0.244, 52832/60000 datapoints
2025-03-06 19:51:14,848 - INFO - training batch 1701, loss: 0.140, 54432/60000 datapoints
2025-03-06 19:51:15,067 - INFO - training batch 1751, loss: 0.323, 56032/60000 datapoints
2025-03-06 19:51:15,284 - INFO - training batch 1801, loss: 0.197, 57632/60000 datapoints
2025-03-06 19:51:15,484 - INFO - training batch 1851, loss: 0.188, 59232/60000 datapoints
2025-03-06 19:51:15,590 - INFO - validation batch 1, loss: 0.119, 32/10016 datapoints
2025-03-06 19:51:15,770 - INFO - validation batch 51, loss: 0.259, 1632/10016 datapoints
2025-03-06 19:51:15,944 - INFO - validation batch 101, loss: 0.616, 3232/10016 datapoints
2025-03-06 19:51:16,121 - INFO - validation batch 151, loss: 0.263, 4832/10016 datapoints
2025-03-06 19:51:16,299 - INFO - validation batch 201, loss: 0.281, 6432/10016 datapoints
2025-03-06 19:51:16,478 - INFO - validation batch 251, loss: 0.096, 8032/10016 datapoints
2025-03-06 19:51:16,664 - INFO - validation batch 301, loss: 0.175, 9632/10016 datapoints
2025-03-06 19:51:16,711 - INFO - Epoch 499/800 done.
2025-03-06 19:51:16,711 - INFO - Final validation performance:
Loss: 0.258, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:51:16,712 - INFO - Beginning epoch 500/800
2025-03-06 19:51:16,719 - INFO - training batch 1, loss: 0.135, 32/60000 datapoints
2025-03-06 19:51:16,978 - INFO - training batch 51, loss: 0.250, 1632/60000 datapoints
2025-03-06 19:51:17,211 - INFO - training batch 101, loss: 0.315, 3232/60000 datapoints
2025-03-06 19:51:17,464 - INFO - training batch 151, loss: 0.237, 4832/60000 datapoints
2025-03-06 19:51:17,678 - INFO - training batch 201, loss: 0.476, 6432/60000 datapoints
2025-03-06 19:51:17,922 - INFO - training batch 251, loss: 0.416, 8032/60000 datapoints
2025-03-06 19:51:18,158 - INFO - training batch 301, loss: 0.123, 9632/60000 datapoints
2025-03-06 19:51:18,365 - INFO - training batch 351, loss: 0.375, 11232/60000 datapoints
2025-03-06 19:51:18,619 - INFO - training batch 401, loss: 0.171, 12832/60000 datapoints
2025-03-06 19:51:18,884 - INFO - training batch 451, loss: 0.306, 14432/60000 datapoints
2025-03-06 19:51:19,160 - INFO - training batch 501, loss: 0.204, 16032/60000 datapoints
2025-03-06 19:51:19,407 - INFO - training batch 551, loss: 0.123, 17632/60000 datapoints
2025-03-06 19:51:19,655 - INFO - training batch 601, loss: 0.339, 19232/60000 datapoints
2025-03-06 19:51:19,873 - INFO - training batch 651, loss: 0.095, 20832/60000 datapoints
2025-03-06 19:51:20,109 - INFO - training batch 701, loss: 0.479, 22432/60000 datapoints
2025-03-06 19:51:20,339 - INFO - training batch 751, loss: 0.176, 24032/60000 datapoints
2025-03-06 19:51:20,570 - INFO - training batch 801, loss: 0.244, 25632/60000 datapoints
2025-03-06 19:51:20,805 - INFO - training batch 851, loss: 0.306, 27232/60000 datapoints
2025-03-06 19:51:21,015 - INFO - training batch 901, loss: 0.183, 28832/60000 datapoints
2025-03-06 19:51:21,216 - INFO - training batch 951, loss: 0.327, 30432/60000 datapoints
2025-03-06 19:51:21,418 - INFO - training batch 1001, loss: 0.137, 32032/60000 datapoints
2025-03-06 19:51:21,616 - INFO - training batch 1051, loss: 0.280, 33632/60000 datapoints
2025-03-06 19:51:21,818 - INFO - training batch 1101, loss: 0.135, 35232/60000 datapoints
2025-03-06 19:51:22,022 - INFO - training batch 1151, loss: 0.349, 36832/60000 datapoints
2025-03-06 19:51:22,229 - INFO - training batch 1201, loss: 0.310, 38432/60000 datapoints
2025-03-06 19:51:22,444 - INFO - training batch 1251, loss: 0.198, 40032/60000 datapoints
2025-03-06 19:51:22,676 - INFO - training batch 1301, loss: 0.207, 41632/60000 datapoints
2025-03-06 19:51:23,068 - INFO - training batch 1351, loss: 0.377, 43232/60000 datapoints
2025-03-06 19:51:23,483 - INFO - training batch 1401, loss: 0.143, 44832/60000 datapoints
2025-03-06 19:51:23,866 - INFO - training batch 1451, loss: 0.373, 46432/60000 datapoints
2025-03-06 19:51:24,214 - INFO - training batch 1501, loss: 0.279, 48032/60000 datapoints
2025-03-06 19:51:24,522 - INFO - training batch 1551, loss: 0.214, 49632/60000 datapoints
2025-03-06 19:51:24,840 - INFO - training batch 1601, loss: 0.781, 51232/60000 datapoints
2025-03-06 19:51:25,134 - INFO - training batch 1651, loss: 0.449, 52832/60000 datapoints
2025-03-06 19:51:25,420 - INFO - training batch 1701, loss: 0.140, 54432/60000 datapoints
2025-03-06 19:51:25,708 - INFO - training batch 1751, loss: 0.191, 56032/60000 datapoints
2025-03-06 19:51:25,986 - INFO - training batch 1801, loss: 0.173, 57632/60000 datapoints
2025-03-06 19:51:26,221 - INFO - training batch 1851, loss: 0.252, 59232/60000 datapoints
2025-03-06 19:51:26,339 - INFO - validation batch 1, loss: 0.328, 32/10016 datapoints
2025-03-06 19:51:26,517 - INFO - validation batch 51, loss: 0.237, 1632/10016 datapoints
2025-03-06 19:51:26,681 - INFO - validation batch 101, loss: 0.304, 3232/10016 datapoints
2025-03-06 19:51:26,846 - INFO - validation batch 151, loss: 0.314, 4832/10016 datapoints
2025-03-06 19:51:27,024 - INFO - validation batch 201, loss: 0.198, 6432/10016 datapoints
2025-03-06 19:51:27,205 - INFO - validation batch 251, loss: 0.075, 8032/10016 datapoints
2025-03-06 19:51:27,369 - INFO - validation batch 301, loss: 0.222, 9632/10016 datapoints
2025-03-06 19:51:27,409 - INFO - Epoch 500/800 done.
2025-03-06 19:51:27,409 - INFO - Final validation performance:
Loss: 0.240, top-1 acc: 0.926top-5 acc: 0.926
2025-03-06 19:51:27,410 - INFO - Beginning epoch 501/800
2025-03-06 19:51:27,417 - INFO - training batch 1, loss: 0.093, 32/60000 datapoints
2025-03-06 19:51:27,625 - INFO - training batch 51, loss: 0.174, 1632/60000 datapoints
2025-03-06 19:51:27,851 - INFO - training batch 101, loss: 0.222, 3232/60000 datapoints
2025-03-06 19:51:28,065 - INFO - training batch 151, loss: 0.217, 4832/60000 datapoints
2025-03-06 19:51:28,268 - INFO - training batch 201, loss: 0.105, 6432/60000 datapoints
2025-03-06 19:51:28,478 - INFO - training batch 251, loss: 0.290, 8032/60000 datapoints
2025-03-06 19:51:28,681 - INFO - training batch 301, loss: 0.118, 9632/60000 datapoints
2025-03-06 19:51:28,881 - INFO - training batch 351, loss: 0.143, 11232/60000 datapoints
2025-03-06 19:51:29,081 - INFO - training batch 401, loss: 0.174, 12832/60000 datapoints
2025-03-06 19:51:29,282 - INFO - training batch 451, loss: 0.297, 14432/60000 datapoints
2025-03-06 19:51:29,477 - INFO - training batch 501, loss: 0.144, 16032/60000 datapoints
2025-03-06 19:51:29,674 - INFO - training batch 551, loss: 0.294, 17632/60000 datapoints
2025-03-06 19:51:29,875 - INFO - training batch 601, loss: 0.161, 19232/60000 datapoints
2025-03-06 19:51:30,076 - INFO - training batch 651, loss: 0.233, 20832/60000 datapoints
2025-03-06 19:51:30,273 - INFO - training batch 701, loss: 0.271, 22432/60000 datapoints
2025-03-06 19:51:30,470 - INFO - training batch 751, loss: 0.091, 24032/60000 datapoints
2025-03-06 19:51:30,675 - INFO - training batch 801, loss: 0.195, 25632/60000 datapoints
2025-03-06 19:51:30,872 - INFO - training batch 851, loss: 0.267, 27232/60000 datapoints
2025-03-06 19:51:31,072 - INFO - training batch 901, loss: 0.264, 28832/60000 datapoints
2025-03-06 19:51:31,272 - INFO - training batch 951, loss: 0.246, 30432/60000 datapoints
2025-03-06 19:51:31,473 - INFO - training batch 1001, loss: 0.373, 32032/60000 datapoints
2025-03-06 19:51:31,679 - INFO - training batch 1051, loss: 0.226, 33632/60000 datapoints
2025-03-06 19:51:31,882 - INFO - training batch 1101, loss: 0.250, 35232/60000 datapoints
2025-03-06 19:51:32,080 - INFO - training batch 1151, loss: 0.219, 36832/60000 datapoints
2025-03-06 19:51:32,281 - INFO - training batch 1201, loss: 0.342, 38432/60000 datapoints
2025-03-06 19:51:32,478 - INFO - training batch 1251, loss: 0.376, 40032/60000 datapoints
2025-03-06 19:51:32,679 - INFO - training batch 1301, loss: 0.227, 41632/60000 datapoints
2025-03-06 19:51:32,876 - INFO - training batch 1351, loss: 0.243, 43232/60000 datapoints
2025-03-06 19:51:33,094 - INFO - training batch 1401, loss: 0.204, 44832/60000 datapoints
2025-03-06 19:51:33,328 - INFO - training batch 1451, loss: 0.172, 46432/60000 datapoints
2025-03-06 19:51:33,546 - INFO - training batch 1501, loss: 0.132, 48032/60000 datapoints
2025-03-06 19:51:33,768 - INFO - training batch 1551, loss: 0.186, 49632/60000 datapoints
2025-03-06 19:51:33,993 - INFO - training batch 1601, loss: 0.190, 51232/60000 datapoints
2025-03-06 19:51:34,193 - INFO - training batch 1651, loss: 0.234, 52832/60000 datapoints
2025-03-06 19:51:34,391 - INFO - training batch 1701, loss: 0.354, 54432/60000 datapoints
2025-03-06 19:51:34,594 - INFO - training batch 1751, loss: 0.086, 56032/60000 datapoints
2025-03-06 19:51:34,809 - INFO - training batch 1801, loss: 0.121, 57632/60000 datapoints
2025-03-06 19:51:35,038 - INFO - training batch 1851, loss: 0.409, 59232/60000 datapoints
2025-03-06 19:51:35,149 - INFO - validation batch 1, loss: 0.317, 32/10016 datapoints
2025-03-06 19:51:35,309 - INFO - validation batch 51, loss: 0.320, 1632/10016 datapoints
2025-03-06 19:51:35,464 - INFO - validation batch 101, loss: 0.139, 3232/10016 datapoints
2025-03-06 19:51:35,627 - INFO - validation batch 151, loss: 0.240, 4832/10016 datapoints
2025-03-06 19:51:35,780 - INFO - validation batch 201, loss: 0.319, 6432/10016 datapoints
2025-03-06 19:51:35,934 - INFO - validation batch 251, loss: 0.305, 8032/10016 datapoints
2025-03-06 19:51:36,089 - INFO - validation batch 301, loss: 0.108, 9632/10016 datapoints
2025-03-06 19:51:36,127 - INFO - Epoch 501/800 done.
2025-03-06 19:51:36,127 - INFO - Final validation performance:
Loss: 0.250, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:51:36,128 - INFO - Beginning epoch 502/800
2025-03-06 19:51:36,134 - INFO - training batch 1, loss: 0.279, 32/60000 datapoints
2025-03-06 19:51:36,338 - INFO - training batch 51, loss: 0.157, 1632/60000 datapoints
2025-03-06 19:51:36,537 - INFO - training batch 101, loss: 0.221, 3232/60000 datapoints
2025-03-06 19:51:36,769 - INFO - training batch 151, loss: 0.189, 4832/60000 datapoints
2025-03-06 19:51:37,002 - INFO - training batch 201, loss: 0.097, 6432/60000 datapoints
2025-03-06 19:51:37,274 - INFO - training batch 251, loss: 0.463, 8032/60000 datapoints
2025-03-06 19:51:37,521 - INFO - training batch 301, loss: 0.241, 9632/60000 datapoints
2025-03-06 19:51:37,755 - INFO - training batch 351, loss: 0.381, 11232/60000 datapoints
2025-03-06 19:51:37,987 - INFO - training batch 401, loss: 0.180, 12832/60000 datapoints
2025-03-06 19:51:38,223 - INFO - training batch 451, loss: 0.328, 14432/60000 datapoints
2025-03-06 19:51:38,456 - INFO - training batch 501, loss: 0.093, 16032/60000 datapoints
2025-03-06 19:51:38,660 - INFO - training batch 551, loss: 0.326, 17632/60000 datapoints
2025-03-06 19:51:38,885 - INFO - training batch 601, loss: 0.139, 19232/60000 datapoints
2025-03-06 19:51:39,112 - INFO - training batch 651, loss: 0.303, 20832/60000 datapoints
2025-03-06 19:51:39,332 - INFO - training batch 701, loss: 0.170, 22432/60000 datapoints
2025-03-06 19:51:39,560 - INFO - training batch 751, loss: 0.424, 24032/60000 datapoints
2025-03-06 19:51:39,789 - INFO - training batch 801, loss: 0.206, 25632/60000 datapoints
2025-03-06 19:51:40,017 - INFO - training batch 851, loss: 0.192, 27232/60000 datapoints
2025-03-06 19:51:40,250 - INFO - training batch 901, loss: 0.318, 28832/60000 datapoints
2025-03-06 19:51:40,467 - INFO - training batch 951, loss: 0.473, 30432/60000 datapoints
2025-03-06 19:51:40,688 - INFO - training batch 1001, loss: 0.208, 32032/60000 datapoints
2025-03-06 19:51:40,919 - INFO - training batch 1051, loss: 0.209, 33632/60000 datapoints
2025-03-06 19:51:41,142 - INFO - training batch 1101, loss: 0.317, 35232/60000 datapoints
2025-03-06 19:51:41,350 - INFO - training batch 1151, loss: 0.354, 36832/60000 datapoints
2025-03-06 19:51:41,556 - INFO - training batch 1201, loss: 0.180, 38432/60000 datapoints
2025-03-06 19:51:41,767 - INFO - training batch 1251, loss: 0.305, 40032/60000 datapoints
2025-03-06 19:51:41,988 - INFO - training batch 1301, loss: 0.165, 41632/60000 datapoints
2025-03-06 19:51:42,265 - INFO - training batch 1351, loss: 0.113, 43232/60000 datapoints
2025-03-06 19:51:42,510 - INFO - training batch 1401, loss: 0.198, 44832/60000 datapoints
2025-03-06 19:51:42,790 - INFO - training batch 1451, loss: 0.197, 46432/60000 datapoints
2025-03-06 19:51:43,056 - INFO - training batch 1501, loss: 0.268, 48032/60000 datapoints
2025-03-06 19:51:43,348 - INFO - training batch 1551, loss: 0.211, 49632/60000 datapoints
2025-03-06 19:51:43,644 - INFO - training batch 1601, loss: 0.147, 51232/60000 datapoints
2025-03-06 19:51:43,905 - INFO - training batch 1651, loss: 0.149, 52832/60000 datapoints
2025-03-06 19:51:44,134 - INFO - training batch 1701, loss: 0.255, 54432/60000 datapoints
2025-03-06 19:51:44,347 - INFO - training batch 1751, loss: 0.387, 56032/60000 datapoints
2025-03-06 19:51:44,569 - INFO - training batch 1801, loss: 0.292, 57632/60000 datapoints
2025-03-06 19:51:44,788 - INFO - training batch 1851, loss: 0.097, 59232/60000 datapoints
2025-03-06 19:51:44,933 - INFO - validation batch 1, loss: 0.312, 32/10016 datapoints
2025-03-06 19:51:45,129 - INFO - validation batch 51, loss: 0.100, 1632/10016 datapoints
2025-03-06 19:51:45,319 - INFO - validation batch 101, loss: 0.075, 3232/10016 datapoints
2025-03-06 19:51:45,563 - INFO - validation batch 151, loss: 0.276, 4832/10016 datapoints
2025-03-06 19:51:45,770 - INFO - validation batch 201, loss: 0.352, 6432/10016 datapoints
2025-03-06 19:51:45,960 - INFO - validation batch 251, loss: 0.252, 8032/10016 datapoints
2025-03-06 19:51:46,127 - INFO - validation batch 301, loss: 0.473, 9632/10016 datapoints
2025-03-06 19:51:46,167 - INFO - Epoch 502/800 done.
2025-03-06 19:51:46,167 - INFO - Final validation performance:
Loss: 0.263, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:51:46,168 - INFO - Beginning epoch 503/800
2025-03-06 19:51:46,175 - INFO - training batch 1, loss: 0.339, 32/60000 datapoints
2025-03-06 19:51:46,405 - INFO - training batch 51, loss: 0.172, 1632/60000 datapoints
2025-03-06 19:51:46,679 - INFO - training batch 101, loss: 0.242, 3232/60000 datapoints
2025-03-06 19:51:46,911 - INFO - training batch 151, loss: 0.133, 4832/60000 datapoints
2025-03-06 19:51:47,136 - INFO - training batch 201, loss: 0.173, 6432/60000 datapoints
2025-03-06 19:51:47,362 - INFO - training batch 251, loss: 0.190, 8032/60000 datapoints
2025-03-06 19:51:47,590 - INFO - training batch 301, loss: 0.231, 9632/60000 datapoints
2025-03-06 19:51:47,812 - INFO - training batch 351, loss: 0.247, 11232/60000 datapoints
2025-03-06 19:51:48,061 - INFO - training batch 401, loss: 0.143, 12832/60000 datapoints
2025-03-06 19:51:48,261 - INFO - training batch 451, loss: 0.175, 14432/60000 datapoints
2025-03-06 19:51:48,491 - INFO - training batch 501, loss: 0.401, 16032/60000 datapoints
2025-03-06 19:51:48,716 - INFO - training batch 551, loss: 0.230, 17632/60000 datapoints
2025-03-06 19:51:48,936 - INFO - training batch 601, loss: 0.145, 19232/60000 datapoints
2025-03-06 19:51:49,168 - INFO - training batch 651, loss: 0.352, 20832/60000 datapoints
2025-03-06 19:51:49,395 - INFO - training batch 701, loss: 0.294, 22432/60000 datapoints
2025-03-06 19:51:49,613 - INFO - training batch 751, loss: 0.229, 24032/60000 datapoints
2025-03-06 19:51:49,838 - INFO - training batch 801, loss: 0.453, 25632/60000 datapoints
2025-03-06 19:51:50,158 - INFO - training batch 851, loss: 0.137, 27232/60000 datapoints
2025-03-06 19:51:50,364 - INFO - training batch 901, loss: 0.118, 28832/60000 datapoints
2025-03-06 19:51:50,581 - INFO - training batch 951, loss: 0.064, 30432/60000 datapoints
2025-03-06 19:51:50,804 - INFO - training batch 1001, loss: 0.579, 32032/60000 datapoints
2025-03-06 19:51:51,022 - INFO - training batch 1051, loss: 0.185, 33632/60000 datapoints
2025-03-06 19:51:51,235 - INFO - training batch 1101, loss: 0.146, 35232/60000 datapoints
2025-03-06 19:51:51,438 - INFO - training batch 1151, loss: 0.132, 36832/60000 datapoints
2025-03-06 19:51:51,683 - INFO - training batch 1201, loss: 0.206, 38432/60000 datapoints
2025-03-06 19:51:51,913 - INFO - training batch 1251, loss: 0.075, 40032/60000 datapoints
2025-03-06 19:51:52,189 - INFO - training batch 1301, loss: 0.195, 41632/60000 datapoints
2025-03-06 19:51:52,413 - INFO - training batch 1351, loss: 0.128, 43232/60000 datapoints
2025-03-06 19:51:52,628 - INFO - training batch 1401, loss: 0.351, 44832/60000 datapoints
2025-03-06 19:51:52,830 - INFO - training batch 1451, loss: 0.433, 46432/60000 datapoints
2025-03-06 19:51:53,039 - INFO - training batch 1501, loss: 0.123, 48032/60000 datapoints
2025-03-06 19:51:53,246 - INFO - training batch 1551, loss: 0.184, 49632/60000 datapoints
2025-03-06 19:51:53,451 - INFO - training batch 1601, loss: 0.434, 51232/60000 datapoints
2025-03-06 19:51:53,671 - INFO - training batch 1651, loss: 0.266, 52832/60000 datapoints
2025-03-06 19:51:53,891 - INFO - training batch 1701, loss: 0.165, 54432/60000 datapoints
2025-03-06 19:51:54,098 - INFO - training batch 1751, loss: 0.168, 56032/60000 datapoints
2025-03-06 19:51:54,295 - INFO - training batch 1801, loss: 0.185, 57632/60000 datapoints
2025-03-06 19:51:54,491 - INFO - training batch 1851, loss: 0.356, 59232/60000 datapoints
2025-03-06 19:51:54,592 - INFO - validation batch 1, loss: 0.112, 32/10016 datapoints
2025-03-06 19:51:54,756 - INFO - validation batch 51, loss: 0.292, 1632/10016 datapoints
2025-03-06 19:51:54,920 - INFO - validation batch 101, loss: 0.224, 3232/10016 datapoints
2025-03-06 19:51:55,093 - INFO - validation batch 151, loss: 0.318, 4832/10016 datapoints
2025-03-06 19:51:55,258 - INFO - validation batch 201, loss: 0.326, 6432/10016 datapoints
2025-03-06 19:51:55,427 - INFO - validation batch 251, loss: 0.172, 8032/10016 datapoints
2025-03-06 19:51:55,584 - INFO - validation batch 301, loss: 0.512, 9632/10016 datapoints
2025-03-06 19:51:55,631 - INFO - Epoch 503/800 done.
2025-03-06 19:51:55,631 - INFO - Final validation performance:
Loss: 0.279, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:51:55,632 - INFO - Beginning epoch 504/800
2025-03-06 19:51:55,640 - INFO - training batch 1, loss: 0.132, 32/60000 datapoints
2025-03-06 19:51:55,861 - INFO - training batch 51, loss: 0.079, 1632/60000 datapoints
2025-03-06 19:51:56,060 - INFO - training batch 101, loss: 0.324, 3232/60000 datapoints
2025-03-06 19:51:56,262 - INFO - training batch 151, loss: 0.169, 4832/60000 datapoints
2025-03-06 19:51:56,456 - INFO - training batch 201, loss: 0.291, 6432/60000 datapoints
2025-03-06 19:51:56,653 - INFO - training batch 251, loss: 0.231, 8032/60000 datapoints
2025-03-06 19:51:56,845 - INFO - training batch 301, loss: 0.179, 9632/60000 datapoints
2025-03-06 19:51:57,043 - INFO - training batch 351, loss: 0.431, 11232/60000 datapoints
2025-03-06 19:51:57,241 - INFO - training batch 401, loss: 0.462, 12832/60000 datapoints
2025-03-06 19:51:57,447 - INFO - training batch 451, loss: 0.177, 14432/60000 datapoints
2025-03-06 19:51:57,646 - INFO - training batch 501, loss: 0.058, 16032/60000 datapoints
2025-03-06 19:51:57,846 - INFO - training batch 551, loss: 0.218, 17632/60000 datapoints
2025-03-06 19:51:58,038 - INFO - training batch 601, loss: 0.350, 19232/60000 datapoints
2025-03-06 19:51:58,247 - INFO - training batch 651, loss: 0.329, 20832/60000 datapoints
2025-03-06 19:51:58,452 - INFO - training batch 701, loss: 0.455, 22432/60000 datapoints
2025-03-06 19:51:58,666 - INFO - training batch 751, loss: 0.308, 24032/60000 datapoints
2025-03-06 19:51:58,877 - INFO - training batch 801, loss: 0.295, 25632/60000 datapoints
2025-03-06 19:51:59,079 - INFO - training batch 851, loss: 0.375, 27232/60000 datapoints
2025-03-06 19:51:59,322 - INFO - training batch 901, loss: 0.320, 28832/60000 datapoints
2025-03-06 19:51:59,540 - INFO - training batch 951, loss: 0.135, 30432/60000 datapoints
2025-03-06 19:51:59,813 - INFO - training batch 1001, loss: 0.229, 32032/60000 datapoints
2025-03-06 19:52:00,028 - INFO - training batch 1051, loss: 0.115, 33632/60000 datapoints
2025-03-06 19:52:00,240 - INFO - training batch 1101, loss: 0.116, 35232/60000 datapoints
2025-03-06 19:52:00,450 - INFO - training batch 1151, loss: 0.231, 36832/60000 datapoints
2025-03-06 19:52:00,665 - INFO - training batch 1201, loss: 0.262, 38432/60000 datapoints
2025-03-06 19:52:00,901 - INFO - training batch 1251, loss: 0.332, 40032/60000 datapoints
2025-03-06 19:52:01,112 - INFO - training batch 1301, loss: 0.241, 41632/60000 datapoints
2025-03-06 19:52:01,334 - INFO - training batch 1351, loss: 0.281, 43232/60000 datapoints
2025-03-06 19:52:01,542 - INFO - training batch 1401, loss: 0.214, 44832/60000 datapoints
2025-03-06 19:52:01,762 - INFO - training batch 1451, loss: 0.086, 46432/60000 datapoints
2025-03-06 19:52:01,991 - INFO - training batch 1501, loss: 0.186, 48032/60000 datapoints
2025-03-06 19:52:02,199 - INFO - training batch 1551, loss: 0.147, 49632/60000 datapoints
2025-03-06 19:52:02,404 - INFO - training batch 1601, loss: 0.146, 51232/60000 datapoints
2025-03-06 19:52:02,623 - INFO - training batch 1651, loss: 0.130, 52832/60000 datapoints
2025-03-06 19:52:02,834 - INFO - training batch 1701, loss: 0.320, 54432/60000 datapoints
2025-03-06 19:52:03,044 - INFO - training batch 1751, loss: 0.384, 56032/60000 datapoints
2025-03-06 19:52:03,246 - INFO - training batch 1801, loss: 0.145, 57632/60000 datapoints
2025-03-06 19:52:03,460 - INFO - training batch 1851, loss: 0.239, 59232/60000 datapoints
2025-03-06 19:52:03,572 - INFO - validation batch 1, loss: 0.369, 32/10016 datapoints
2025-03-06 19:52:03,735 - INFO - validation batch 51, loss: 0.110, 1632/10016 datapoints
2025-03-06 19:52:03,903 - INFO - validation batch 101, loss: 0.283, 3232/10016 datapoints
2025-03-06 19:52:04,069 - INFO - validation batch 151, loss: 0.099, 4832/10016 datapoints
2025-03-06 19:52:04,229 - INFO - validation batch 201, loss: 0.054, 6432/10016 datapoints
2025-03-06 19:52:04,404 - INFO - validation batch 251, loss: 0.272, 8032/10016 datapoints
2025-03-06 19:52:04,585 - INFO - validation batch 301, loss: 0.133, 9632/10016 datapoints
2025-03-06 19:52:04,626 - INFO - Epoch 504/800 done.
2025-03-06 19:52:04,626 - INFO - Final validation performance:
Loss: 0.189, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:52:04,627 - INFO - Beginning epoch 505/800
2025-03-06 19:52:04,634 - INFO - training batch 1, loss: 0.151, 32/60000 datapoints
2025-03-06 19:52:04,857 - INFO - training batch 51, loss: 0.140, 1632/60000 datapoints
2025-03-06 19:52:05,079 - INFO - training batch 101, loss: 0.629, 3232/60000 datapoints
2025-03-06 19:52:05,317 - INFO - training batch 151, loss: 0.277, 4832/60000 datapoints
2025-03-06 19:52:05,538 - INFO - training batch 201, loss: 0.118, 6432/60000 datapoints
2025-03-06 19:52:05,771 - INFO - training batch 251, loss: 0.148, 8032/60000 datapoints
2025-03-06 19:52:06,000 - INFO - training batch 301, loss: 0.370, 9632/60000 datapoints
2025-03-06 19:52:06,212 - INFO - training batch 351, loss: 0.165, 11232/60000 datapoints
2025-03-06 19:52:06,436 - INFO - training batch 401, loss: 0.127, 12832/60000 datapoints
2025-03-06 19:52:06,654 - INFO - training batch 451, loss: 0.641, 14432/60000 datapoints
2025-03-06 19:52:06,863 - INFO - training batch 501, loss: 0.434, 16032/60000 datapoints
2025-03-06 19:52:07,087 - INFO - training batch 551, loss: 0.352, 17632/60000 datapoints
2025-03-06 19:52:07,307 - INFO - training batch 601, loss: 0.198, 19232/60000 datapoints
2025-03-06 19:52:07,556 - INFO - training batch 651, loss: 0.057, 20832/60000 datapoints
2025-03-06 19:52:07,773 - INFO - training batch 701, loss: 0.161, 22432/60000 datapoints
2025-03-06 19:52:08,002 - INFO - training batch 751, loss: 0.092, 24032/60000 datapoints
2025-03-06 19:52:08,228 - INFO - training batch 801, loss: 0.400, 25632/60000 datapoints
2025-03-06 19:52:08,578 - INFO - training batch 851, loss: 0.431, 27232/60000 datapoints
2025-03-06 19:52:08,823 - INFO - training batch 901, loss: 0.202, 28832/60000 datapoints
2025-03-06 19:52:09,056 - INFO - training batch 951, loss: 0.319, 30432/60000 datapoints
2025-03-06 19:52:09,278 - INFO - training batch 1001, loss: 0.279, 32032/60000 datapoints
2025-03-06 19:52:09,494 - INFO - training batch 1051, loss: 0.165, 33632/60000 datapoints
2025-03-06 19:52:09,816 - INFO - training batch 1101, loss: 0.326, 35232/60000 datapoints
2025-03-06 19:52:10,063 - INFO - training batch 1151, loss: 0.081, 36832/60000 datapoints
2025-03-06 19:52:10,287 - INFO - training batch 1201, loss: 0.434, 38432/60000 datapoints
2025-03-06 19:52:10,516 - INFO - training batch 1251, loss: 0.341, 40032/60000 datapoints
2025-03-06 19:52:10,736 - INFO - training batch 1301, loss: 0.511, 41632/60000 datapoints
2025-03-06 19:52:10,935 - INFO - training batch 1351, loss: 0.641, 43232/60000 datapoints
2025-03-06 19:52:11,149 - INFO - training batch 1401, loss: 0.398, 44832/60000 datapoints
2025-03-06 19:52:11,359 - INFO - training batch 1451, loss: 0.181, 46432/60000 datapoints
2025-03-06 19:52:11,573 - INFO - training batch 1501, loss: 0.370, 48032/60000 datapoints
2025-03-06 19:52:11,787 - INFO - training batch 1551, loss: 0.491, 49632/60000 datapoints
2025-03-06 19:52:12,002 - INFO - training batch 1601, loss: 0.254, 51232/60000 datapoints
2025-03-06 19:52:12,216 - INFO - training batch 1651, loss: 0.088, 52832/60000 datapoints
2025-03-06 19:52:12,421 - INFO - training batch 1701, loss: 0.127, 54432/60000 datapoints
2025-03-06 19:52:12,629 - INFO - training batch 1751, loss: 0.191, 56032/60000 datapoints
2025-03-06 19:52:12,837 - INFO - training batch 1801, loss: 0.417, 57632/60000 datapoints
2025-03-06 19:52:13,078 - INFO - training batch 1851, loss: 0.293, 59232/60000 datapoints
2025-03-06 19:52:13,199 - INFO - validation batch 1, loss: 0.141, 32/10016 datapoints
2025-03-06 19:52:13,370 - INFO - validation batch 51, loss: 0.164, 1632/10016 datapoints
2025-03-06 19:52:13,587 - INFO - validation batch 101, loss: 0.096, 3232/10016 datapoints
2025-03-06 19:52:13,775 - INFO - validation batch 151, loss: 0.126, 4832/10016 datapoints
2025-03-06 19:52:13,936 - INFO - validation batch 201, loss: 0.161, 6432/10016 datapoints
2025-03-06 19:52:14,099 - INFO - validation batch 251, loss: 0.238, 8032/10016 datapoints
2025-03-06 19:52:14,260 - INFO - validation batch 301, loss: 0.173, 9632/10016 datapoints
2025-03-06 19:52:14,299 - INFO - Epoch 505/800 done.
2025-03-06 19:52:14,300 - INFO - Final validation performance:
Loss: 0.157, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:52:14,300 - INFO - Beginning epoch 506/800
2025-03-06 19:52:14,307 - INFO - training batch 1, loss: 0.238, 32/60000 datapoints
2025-03-06 19:52:14,507 - INFO - training batch 51, loss: 0.171, 1632/60000 datapoints
2025-03-06 19:52:14,722 - INFO - training batch 101, loss: 0.117, 3232/60000 datapoints
2025-03-06 19:52:14,928 - INFO - training batch 151, loss: 0.170, 4832/60000 datapoints
2025-03-06 19:52:15,140 - INFO - training batch 201, loss: 0.194, 6432/60000 datapoints
2025-03-06 19:52:15,343 - INFO - training batch 251, loss: 0.298, 8032/60000 datapoints
2025-03-06 19:52:15,554 - INFO - training batch 301, loss: 0.077, 9632/60000 datapoints
2025-03-06 19:52:15,754 - INFO - training batch 351, loss: 0.176, 11232/60000 datapoints
2025-03-06 19:52:15,949 - INFO - training batch 401, loss: 0.132, 12832/60000 datapoints
2025-03-06 19:52:16,144 - INFO - training batch 451, loss: 0.321, 14432/60000 datapoints
2025-03-06 19:52:16,338 - INFO - training batch 501, loss: 0.193, 16032/60000 datapoints
2025-03-06 19:52:16,531 - INFO - training batch 551, loss: 0.258, 17632/60000 datapoints
2025-03-06 19:52:16,728 - INFO - training batch 601, loss: 0.111, 19232/60000 datapoints
2025-03-06 19:52:16,921 - INFO - training batch 651, loss: 0.166, 20832/60000 datapoints
2025-03-06 19:52:17,124 - INFO - training batch 701, loss: 0.103, 22432/60000 datapoints
2025-03-06 19:52:17,320 - INFO - training batch 751, loss: 0.109, 24032/60000 datapoints
2025-03-06 19:52:17,531 - INFO - training batch 801, loss: 0.151, 25632/60000 datapoints
2025-03-06 19:52:17,727 - INFO - training batch 851, loss: 0.224, 27232/60000 datapoints
2025-03-06 19:52:17,922 - INFO - training batch 901, loss: 0.221, 28832/60000 datapoints
2025-03-06 19:52:18,116 - INFO - training batch 951, loss: 0.239, 30432/60000 datapoints
2025-03-06 19:52:18,313 - INFO - training batch 1001, loss: 0.353, 32032/60000 datapoints
2025-03-06 19:52:18,513 - INFO - training batch 1051, loss: 0.363, 33632/60000 datapoints
2025-03-06 19:52:18,718 - INFO - training batch 1101, loss: 0.197, 35232/60000 datapoints
2025-03-06 19:52:18,914 - INFO - training batch 1151, loss: 0.255, 36832/60000 datapoints
2025-03-06 19:52:19,110 - INFO - training batch 1201, loss: 0.431, 38432/60000 datapoints
2025-03-06 19:52:19,306 - INFO - training batch 1251, loss: 0.236, 40032/60000 datapoints
2025-03-06 19:52:19,501 - INFO - training batch 1301, loss: 0.150, 41632/60000 datapoints
2025-03-06 19:52:19,702 - INFO - training batch 1351, loss: 0.243, 43232/60000 datapoints
2025-03-06 19:52:19,910 - INFO - training batch 1401, loss: 0.273, 44832/60000 datapoints
2025-03-06 19:52:20,107 - INFO - training batch 1451, loss: 0.165, 46432/60000 datapoints
2025-03-06 19:52:20,313 - INFO - training batch 1501, loss: 0.088, 48032/60000 datapoints
2025-03-06 19:52:20,517 - INFO - training batch 1551, loss: 0.287, 49632/60000 datapoints
2025-03-06 19:52:20,717 - INFO - training batch 1601, loss: 0.187, 51232/60000 datapoints
2025-03-06 19:52:20,913 - INFO - training batch 1651, loss: 0.271, 52832/60000 datapoints
2025-03-06 19:52:21,118 - INFO - training batch 1701, loss: 0.208, 54432/60000 datapoints
2025-03-06 19:52:21,313 - INFO - training batch 1751, loss: 0.177, 56032/60000 datapoints
2025-03-06 19:52:21,510 - INFO - training batch 1801, loss: 0.367, 57632/60000 datapoints
2025-03-06 19:52:21,722 - INFO - training batch 1851, loss: 0.108, 59232/60000 datapoints
2025-03-06 19:52:21,837 - INFO - validation batch 1, loss: 0.303, 32/10016 datapoints
2025-03-06 19:52:21,997 - INFO - validation batch 51, loss: 0.183, 1632/10016 datapoints
2025-03-06 19:52:22,153 - INFO - validation batch 101, loss: 0.216, 3232/10016 datapoints
2025-03-06 19:52:22,310 - INFO - validation batch 151, loss: 0.183, 4832/10016 datapoints
2025-03-06 19:52:22,465 - INFO - validation batch 201, loss: 0.438, 6432/10016 datapoints
2025-03-06 19:52:22,621 - INFO - validation batch 251, loss: 0.236, 8032/10016 datapoints
2025-03-06 19:52:22,781 - INFO - validation batch 301, loss: 0.315, 9632/10016 datapoints
2025-03-06 19:52:22,818 - INFO - Epoch 506/800 done.
2025-03-06 19:52:22,818 - INFO - Final validation performance:
Loss: 0.268, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:52:22,819 - INFO - Beginning epoch 507/800
2025-03-06 19:52:22,826 - INFO - training batch 1, loss: 0.137, 32/60000 datapoints
2025-03-06 19:52:23,033 - INFO - training batch 51, loss: 0.271, 1632/60000 datapoints
2025-03-06 19:52:23,231 - INFO - training batch 101, loss: 0.216, 3232/60000 datapoints
2025-03-06 19:52:23,431 - INFO - training batch 151, loss: 0.140, 4832/60000 datapoints
2025-03-06 19:52:23,630 - INFO - training batch 201, loss: 0.458, 6432/60000 datapoints
2025-03-06 19:52:23,831 - INFO - training batch 251, loss: 0.435, 8032/60000 datapoints
2025-03-06 19:52:24,022 - INFO - training batch 301, loss: 0.288, 9632/60000 datapoints
2025-03-06 19:52:24,223 - INFO - training batch 351, loss: 0.201, 11232/60000 datapoints
2025-03-06 19:52:24,417 - INFO - training batch 401, loss: 0.480, 12832/60000 datapoints
2025-03-06 19:52:24,613 - INFO - training batch 451, loss: 0.295, 14432/60000 datapoints
2025-03-06 19:52:24,815 - INFO - training batch 501, loss: 0.210, 16032/60000 datapoints
2025-03-06 19:52:25,014 - INFO - training batch 551, loss: 0.196, 17632/60000 datapoints
2025-03-06 19:52:25,217 - INFO - training batch 601, loss: 0.182, 19232/60000 datapoints
2025-03-06 19:52:25,413 - INFO - training batch 651, loss: 0.116, 20832/60000 datapoints
2025-03-06 19:52:25,609 - INFO - training batch 701, loss: 0.252, 22432/60000 datapoints
2025-03-06 19:52:25,825 - INFO - training batch 751, loss: 0.342, 24032/60000 datapoints
2025-03-06 19:52:26,026 - INFO - training batch 801, loss: 0.131, 25632/60000 datapoints
2025-03-06 19:52:26,226 - INFO - training batch 851, loss: 0.191, 27232/60000 datapoints
2025-03-06 19:52:26,438 - INFO - training batch 901, loss: 0.280, 28832/60000 datapoints
2025-03-06 19:52:26,638 - INFO - training batch 951, loss: 0.167, 30432/60000 datapoints
2025-03-06 19:52:26,853 - INFO - training batch 1001, loss: 0.186, 32032/60000 datapoints
2025-03-06 19:52:27,054 - INFO - training batch 1051, loss: 0.258, 33632/60000 datapoints
2025-03-06 19:52:27,257 - INFO - training batch 1101, loss: 0.166, 35232/60000 datapoints
2025-03-06 19:52:27,453 - INFO - training batch 1151, loss: 0.331, 36832/60000 datapoints
2025-03-06 19:52:27,683 - INFO - training batch 1201, loss: 0.116, 38432/60000 datapoints
2025-03-06 19:52:27,886 - INFO - training batch 1251, loss: 0.433, 40032/60000 datapoints
2025-03-06 19:52:28,093 - INFO - training batch 1301, loss: 0.135, 41632/60000 datapoints
2025-03-06 19:52:28,299 - INFO - training batch 1351, loss: 0.289, 43232/60000 datapoints
2025-03-06 19:52:28,504 - INFO - training batch 1401, loss: 0.371, 44832/60000 datapoints
2025-03-06 19:52:28,716 - INFO - training batch 1451, loss: 0.481, 46432/60000 datapoints
2025-03-06 19:52:28,933 - INFO - training batch 1501, loss: 0.267, 48032/60000 datapoints
2025-03-06 19:52:29,131 - INFO - training batch 1551, loss: 0.142, 49632/60000 datapoints
2025-03-06 19:52:29,329 - INFO - training batch 1601, loss: 0.164, 51232/60000 datapoints
2025-03-06 19:52:29,525 - INFO - training batch 1651, loss: 0.401, 52832/60000 datapoints
2025-03-06 19:52:29,722 - INFO - training batch 1701, loss: 0.084, 54432/60000 datapoints
2025-03-06 19:52:29,917 - INFO - training batch 1751, loss: 0.267, 56032/60000 datapoints
2025-03-06 19:52:30,116 - INFO - training batch 1801, loss: 0.190, 57632/60000 datapoints
2025-03-06 19:52:30,310 - INFO - training batch 1851, loss: 0.280, 59232/60000 datapoints
2025-03-06 19:52:30,428 - INFO - validation batch 1, loss: 0.714, 32/10016 datapoints
2025-03-06 19:52:30,605 - INFO - validation batch 51, loss: 0.277, 1632/10016 datapoints
2025-03-06 19:52:30,760 - INFO - validation batch 101, loss: 0.205, 3232/10016 datapoints
2025-03-06 19:52:30,915 - INFO - validation batch 151, loss: 0.248, 4832/10016 datapoints
2025-03-06 19:52:31,068 - INFO - validation batch 201, loss: 0.178, 6432/10016 datapoints
2025-03-06 19:52:31,221 - INFO - validation batch 251, loss: 0.149, 8032/10016 datapoints
2025-03-06 19:52:31,374 - INFO - validation batch 301, loss: 0.253, 9632/10016 datapoints
2025-03-06 19:52:31,409 - INFO - Epoch 507/800 done.
2025-03-06 19:52:31,409 - INFO - Final validation performance:
Loss: 0.289, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:52:31,410 - INFO - Beginning epoch 508/800
2025-03-06 19:52:31,418 - INFO - training batch 1, loss: 0.330, 32/60000 datapoints
2025-03-06 19:52:31,642 - INFO - training batch 51, loss: 0.186, 1632/60000 datapoints
2025-03-06 19:52:31,841 - INFO - training batch 101, loss: 0.193, 3232/60000 datapoints
2025-03-06 19:52:32,043 - INFO - training batch 151, loss: 0.676, 4832/60000 datapoints
2025-03-06 19:52:32,241 - INFO - training batch 201, loss: 0.217, 6432/60000 datapoints
2025-03-06 19:52:32,440 - INFO - training batch 251, loss: 0.083, 8032/60000 datapoints
2025-03-06 19:52:32,657 - INFO - training batch 301, loss: 0.382, 9632/60000 datapoints
2025-03-06 19:52:32,912 - INFO - training batch 351, loss: 0.430, 11232/60000 datapoints
2025-03-06 19:52:33,133 - INFO - training batch 401, loss: 0.182, 12832/60000 datapoints
2025-03-06 19:52:33,389 - INFO - training batch 451, loss: 0.180, 14432/60000 datapoints
2025-03-06 19:52:33,607 - INFO - training batch 501, loss: 0.434, 16032/60000 datapoints
2025-03-06 19:52:33,823 - INFO - training batch 551, loss: 0.172, 17632/60000 datapoints
2025-03-06 19:52:34,037 - INFO - training batch 601, loss: 0.095, 19232/60000 datapoints
2025-03-06 19:52:34,233 - INFO - training batch 651, loss: 0.370, 20832/60000 datapoints
2025-03-06 19:52:34,429 - INFO - training batch 701, loss: 0.282, 22432/60000 datapoints
2025-03-06 19:52:34,640 - INFO - training batch 751, loss: 0.195, 24032/60000 datapoints
2025-03-06 19:52:34,862 - INFO - training batch 801, loss: 0.414, 25632/60000 datapoints
2025-03-06 19:52:35,072 - INFO - training batch 851, loss: 0.294, 27232/60000 datapoints
2025-03-06 19:52:35,290 - INFO - training batch 901, loss: 0.698, 28832/60000 datapoints
2025-03-06 19:52:35,493 - INFO - training batch 951, loss: 0.513, 30432/60000 datapoints
2025-03-06 19:52:35,697 - INFO - training batch 1001, loss: 0.176, 32032/60000 datapoints
2025-03-06 19:52:35,898 - INFO - training batch 1051, loss: 0.214, 33632/60000 datapoints
2025-03-06 19:52:36,093 - INFO - training batch 1101, loss: 0.324, 35232/60000 datapoints
2025-03-06 19:52:36,291 - INFO - training batch 1151, loss: 0.349, 36832/60000 datapoints
2025-03-06 19:52:36,504 - INFO - training batch 1201, loss: 0.579, 38432/60000 datapoints
2025-03-06 19:52:36,712 - INFO - training batch 1251, loss: 0.271, 40032/60000 datapoints
2025-03-06 19:52:36,909 - INFO - training batch 1301, loss: 0.329, 41632/60000 datapoints
2025-03-06 19:52:37,112 - INFO - training batch 1351, loss: 0.476, 43232/60000 datapoints
2025-03-06 19:52:37,317 - INFO - training batch 1401, loss: 0.367, 44832/60000 datapoints
2025-03-06 19:52:37,525 - INFO - training batch 1451, loss: 0.204, 46432/60000 datapoints
2025-03-06 19:52:37,755 - INFO - training batch 1501, loss: 0.234, 48032/60000 datapoints
2025-03-06 19:52:37,954 - INFO - training batch 1551, loss: 0.837, 49632/60000 datapoints
2025-03-06 19:52:38,148 - INFO - training batch 1601, loss: 0.347, 51232/60000 datapoints
2025-03-06 19:52:38,377 - INFO - training batch 1651, loss: 0.200, 52832/60000 datapoints
2025-03-06 19:52:38,575 - INFO - training batch 1701, loss: 0.110, 54432/60000 datapoints
2025-03-06 19:52:38,775 - INFO - training batch 1751, loss: 0.145, 56032/60000 datapoints
2025-03-06 19:52:38,977 - INFO - training batch 1801, loss: 0.126, 57632/60000 datapoints
2025-03-06 19:52:39,189 - INFO - training batch 1851, loss: 0.071, 59232/60000 datapoints
2025-03-06 19:52:39,291 - INFO - validation batch 1, loss: 0.099, 32/10016 datapoints
2025-03-06 19:52:39,448 - INFO - validation batch 51, loss: 0.143, 1632/10016 datapoints
2025-03-06 19:52:39,609 - INFO - validation batch 101, loss: 0.279, 3232/10016 datapoints
2025-03-06 19:52:39,769 - INFO - validation batch 151, loss: 0.316, 4832/10016 datapoints
2025-03-06 19:52:39,937 - INFO - validation batch 201, loss: 0.545, 6432/10016 datapoints
2025-03-06 19:52:40,113 - INFO - validation batch 251, loss: 0.280, 8032/10016 datapoints
2025-03-06 19:52:40,276 - INFO - validation batch 301, loss: 0.198, 9632/10016 datapoints
2025-03-06 19:52:40,318 - INFO - Epoch 508/800 done.
2025-03-06 19:52:40,318 - INFO - Final validation performance:
Loss: 0.266, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:52:40,318 - INFO - Beginning epoch 509/800
2025-03-06 19:52:40,327 - INFO - training batch 1, loss: 0.483, 32/60000 datapoints
2025-03-06 19:52:40,543 - INFO - training batch 51, loss: 0.296, 1632/60000 datapoints
2025-03-06 19:52:40,765 - INFO - training batch 101, loss: 0.187, 3232/60000 datapoints
2025-03-06 19:52:40,972 - INFO - training batch 151, loss: 0.347, 4832/60000 datapoints
2025-03-06 19:52:41,178 - INFO - training batch 201, loss: 0.180, 6432/60000 datapoints
2025-03-06 19:52:41,388 - INFO - training batch 251, loss: 0.268, 8032/60000 datapoints
2025-03-06 19:52:41,584 - INFO - training batch 301, loss: 0.227, 9632/60000 datapoints
2025-03-06 19:52:41,787 - INFO - training batch 351, loss: 0.242, 11232/60000 datapoints
2025-03-06 19:52:41,984 - INFO - training batch 401, loss: 0.227, 12832/60000 datapoints
2025-03-06 19:52:42,180 - INFO - training batch 451, loss: 0.244, 14432/60000 datapoints
2025-03-06 19:52:42,380 - INFO - training batch 501, loss: 0.222, 16032/60000 datapoints
2025-03-06 19:52:42,576 - INFO - training batch 551, loss: 0.463, 17632/60000 datapoints
2025-03-06 19:52:42,779 - INFO - training batch 601, loss: 0.416, 19232/60000 datapoints
2025-03-06 19:52:42,978 - INFO - training batch 651, loss: 0.247, 20832/60000 datapoints
2025-03-06 19:52:43,174 - INFO - training batch 701, loss: 0.109, 22432/60000 datapoints
2025-03-06 19:52:43,372 - INFO - training batch 751, loss: 0.401, 24032/60000 datapoints
2025-03-06 19:52:43,567 - INFO - training batch 801, loss: 0.457, 25632/60000 datapoints
2025-03-06 19:52:43,764 - INFO - training batch 851, loss: 0.173, 27232/60000 datapoints
2025-03-06 19:52:43,991 - INFO - training batch 901, loss: 0.275, 28832/60000 datapoints
2025-03-06 19:52:44,334 - INFO - training batch 951, loss: 0.203, 30432/60000 datapoints
2025-03-06 19:52:44,567 - INFO - training batch 1001, loss: 0.405, 32032/60000 datapoints
2025-03-06 19:52:44,799 - INFO - training batch 1051, loss: 0.569, 33632/60000 datapoints
2025-03-06 19:52:45,010 - INFO - training batch 1101, loss: 0.081, 35232/60000 datapoints
2025-03-06 19:52:45,231 - INFO - training batch 1151, loss: 0.264, 36832/60000 datapoints
2025-03-06 19:52:45,457 - INFO - training batch 1201, loss: 0.460, 38432/60000 datapoints
2025-03-06 19:52:45,713 - INFO - training batch 1251, loss: 0.178, 40032/60000 datapoints
2025-03-06 19:52:45,982 - INFO - training batch 1301, loss: 0.196, 41632/60000 datapoints
2025-03-06 19:52:46,221 - INFO - training batch 1351, loss: 0.206, 43232/60000 datapoints
2025-03-06 19:52:46,445 - INFO - training batch 1401, loss: 0.190, 44832/60000 datapoints
2025-03-06 19:52:46,679 - INFO - training batch 1451, loss: 0.147, 46432/60000 datapoints
2025-03-06 19:52:46,896 - INFO - training batch 1501, loss: 0.361, 48032/60000 datapoints
2025-03-06 19:52:47,116 - INFO - training batch 1551, loss: 0.111, 49632/60000 datapoints
2025-03-06 19:52:47,345 - INFO - training batch 1601, loss: 0.192, 51232/60000 datapoints
2025-03-06 19:52:47,562 - INFO - training batch 1651, loss: 0.321, 52832/60000 datapoints
2025-03-06 19:52:47,811 - INFO - training batch 1701, loss: 0.344, 54432/60000 datapoints
2025-03-06 19:52:48,027 - INFO - training batch 1751, loss: 0.132, 56032/60000 datapoints
2025-03-06 19:52:48,265 - INFO - training batch 1801, loss: 0.269, 57632/60000 datapoints
2025-03-06 19:52:48,485 - INFO - training batch 1851, loss: 0.227, 59232/60000 datapoints
2025-03-06 19:52:48,592 - INFO - validation batch 1, loss: 0.441, 32/10016 datapoints
2025-03-06 19:52:48,773 - INFO - validation batch 51, loss: 0.131, 1632/10016 datapoints
2025-03-06 19:52:48,944 - INFO - validation batch 101, loss: 0.182, 3232/10016 datapoints
2025-03-06 19:52:49,115 - INFO - validation batch 151, loss: 0.507, 4832/10016 datapoints
2025-03-06 19:52:49,288 - INFO - validation batch 201, loss: 0.106, 6432/10016 datapoints
2025-03-06 19:52:49,460 - INFO - validation batch 251, loss: 0.245, 8032/10016 datapoints
2025-03-06 19:52:49,638 - INFO - validation batch 301, loss: 0.259, 9632/10016 datapoints
2025-03-06 19:52:49,680 - INFO - Epoch 509/800 done.
2025-03-06 19:52:49,680 - INFO - Final validation performance:
Loss: 0.267, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:52:49,681 - INFO - Beginning epoch 510/800
2025-03-06 19:52:49,687 - INFO - training batch 1, loss: 0.198, 32/60000 datapoints
2025-03-06 19:52:49,908 - INFO - training batch 51, loss: 0.085, 1632/60000 datapoints
2025-03-06 19:52:50,145 - INFO - training batch 101, loss: 0.236, 3232/60000 datapoints
2025-03-06 19:52:50,371 - INFO - training batch 151, loss: 0.228, 4832/60000 datapoints
2025-03-06 19:52:50,593 - INFO - training batch 201, loss: 0.491, 6432/60000 datapoints
2025-03-06 19:52:50,820 - INFO - training batch 251, loss: 0.318, 8032/60000 datapoints
2025-03-06 19:52:51,041 - INFO - training batch 301, loss: 0.236, 9632/60000 datapoints
2025-03-06 19:52:51,265 - INFO - training batch 351, loss: 0.261, 11232/60000 datapoints
2025-03-06 19:52:51,490 - INFO - training batch 401, loss: 0.303, 12832/60000 datapoints
2025-03-06 19:52:51,812 - INFO - training batch 451, loss: 0.524, 14432/60000 datapoints
2025-03-06 19:52:52,037 - INFO - training batch 501, loss: 0.180, 16032/60000 datapoints
2025-03-06 19:52:52,256 - INFO - training batch 551, loss: 0.415, 17632/60000 datapoints
2025-03-06 19:52:52,466 - INFO - training batch 601, loss: 0.289, 19232/60000 datapoints
2025-03-06 19:52:52,687 - INFO - training batch 651, loss: 0.325, 20832/60000 datapoints
2025-03-06 19:52:52,892 - INFO - training batch 701, loss: 0.269, 22432/60000 datapoints
2025-03-06 19:52:53,100 - INFO - training batch 751, loss: 0.236, 24032/60000 datapoints
2025-03-06 19:52:53,311 - INFO - training batch 801, loss: 0.267, 25632/60000 datapoints
2025-03-06 19:52:53,510 - INFO - training batch 851, loss: 0.104, 27232/60000 datapoints
2025-03-06 19:52:53,720 - INFO - training batch 901, loss: 0.241, 28832/60000 datapoints
2025-03-06 19:52:53,927 - INFO - training batch 951, loss: 0.274, 30432/60000 datapoints
2025-03-06 19:52:54,128 - INFO - training batch 1001, loss: 0.139, 32032/60000 datapoints
2025-03-06 19:52:54,330 - INFO - training batch 1051, loss: 0.267, 33632/60000 datapoints
2025-03-06 19:52:54,526 - INFO - training batch 1101, loss: 0.121, 35232/60000 datapoints
2025-03-06 19:52:54,732 - INFO - training batch 1151, loss: 0.078, 36832/60000 datapoints
2025-03-06 19:52:54,958 - INFO - training batch 1201, loss: 0.189, 38432/60000 datapoints
2025-03-06 19:52:55,180 - INFO - training batch 1251, loss: 0.314, 40032/60000 datapoints
2025-03-06 19:52:55,393 - INFO - training batch 1301, loss: 0.180, 41632/60000 datapoints
2025-03-06 19:52:55,592 - INFO - training batch 1351, loss: 0.262, 43232/60000 datapoints
2025-03-06 19:52:55,801 - INFO - training batch 1401, loss: 0.123, 44832/60000 datapoints
2025-03-06 19:52:56,026 - INFO - training batch 1451, loss: 0.133, 46432/60000 datapoints
2025-03-06 19:52:56,248 - INFO - training batch 1501, loss: 0.366, 48032/60000 datapoints
2025-03-06 19:52:56,460 - INFO - training batch 1551, loss: 0.120, 49632/60000 datapoints
2025-03-06 19:52:56,682 - INFO - training batch 1601, loss: 0.312, 51232/60000 datapoints
2025-03-06 19:52:56,904 - INFO - training batch 1651, loss: 0.366, 52832/60000 datapoints
2025-03-06 19:52:57,103 - INFO - training batch 1701, loss: 0.188, 54432/60000 datapoints
2025-03-06 19:52:57,324 - INFO - training batch 1751, loss: 0.119, 56032/60000 datapoints
2025-03-06 19:52:57,538 - INFO - training batch 1801, loss: 0.221, 57632/60000 datapoints
2025-03-06 19:52:57,751 - INFO - training batch 1851, loss: 0.233, 59232/60000 datapoints
2025-03-06 19:52:57,873 - INFO - validation batch 1, loss: 0.317, 32/10016 datapoints
2025-03-06 19:52:58,044 - INFO - validation batch 51, loss: 0.334, 1632/10016 datapoints
2025-03-06 19:52:58,212 - INFO - validation batch 101, loss: 0.190, 3232/10016 datapoints
2025-03-06 19:52:58,386 - INFO - validation batch 151, loss: 0.219, 4832/10016 datapoints
2025-03-06 19:52:58,548 - INFO - validation batch 201, loss: 0.247, 6432/10016 datapoints
2025-03-06 19:52:58,720 - INFO - validation batch 251, loss: 0.133, 8032/10016 datapoints
2025-03-06 19:52:58,901 - INFO - validation batch 301, loss: 0.177, 9632/10016 datapoints
2025-03-06 19:52:58,940 - INFO - Epoch 510/800 done.
2025-03-06 19:52:58,940 - INFO - Final validation performance:
Loss: 0.231, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:52:58,941 - INFO - Beginning epoch 511/800
2025-03-06 19:52:58,948 - INFO - training batch 1, loss: 0.262, 32/60000 datapoints
2025-03-06 19:52:59,167 - INFO - training batch 51, loss: 0.520, 1632/60000 datapoints
2025-03-06 19:52:59,390 - INFO - training batch 101, loss: 0.201, 3232/60000 datapoints
2025-03-06 19:52:59,606 - INFO - training batch 151, loss: 0.193, 4832/60000 datapoints
2025-03-06 19:52:59,823 - INFO - training batch 201, loss: 0.124, 6432/60000 datapoints
2025-03-06 19:53:00,046 - INFO - training batch 251, loss: 0.229, 8032/60000 datapoints
2025-03-06 19:53:00,276 - INFO - training batch 301, loss: 0.217, 9632/60000 datapoints
2025-03-06 19:53:00,479 - INFO - training batch 351, loss: 0.243, 11232/60000 datapoints
2025-03-06 19:53:00,689 - INFO - training batch 401, loss: 0.230, 12832/60000 datapoints
2025-03-06 19:53:00,891 - INFO - training batch 451, loss: 0.516, 14432/60000 datapoints
2025-03-06 19:53:01,093 - INFO - training batch 501, loss: 0.229, 16032/60000 datapoints
2025-03-06 19:53:01,297 - INFO - training batch 551, loss: 0.556, 17632/60000 datapoints
2025-03-06 19:53:01,501 - INFO - training batch 601, loss: 0.093, 19232/60000 datapoints
2025-03-06 19:53:01,743 - INFO - training batch 651, loss: 0.134, 20832/60000 datapoints
2025-03-06 19:53:02,067 - INFO - training batch 701, loss: 0.191, 22432/60000 datapoints
2025-03-06 19:53:02,309 - INFO - training batch 751, loss: 0.215, 24032/60000 datapoints
2025-03-06 19:53:02,526 - INFO - training batch 801, loss: 0.167, 25632/60000 datapoints
2025-03-06 19:53:02,754 - INFO - training batch 851, loss: 0.193, 27232/60000 datapoints
2025-03-06 19:53:02,955 - INFO - training batch 901, loss: 0.100, 28832/60000 datapoints
2025-03-06 19:53:03,165 - INFO - training batch 951, loss: 0.173, 30432/60000 datapoints
2025-03-06 19:53:03,382 - INFO - training batch 1001, loss: 0.550, 32032/60000 datapoints
2025-03-06 19:53:03,606 - INFO - training batch 1051, loss: 0.102, 33632/60000 datapoints
2025-03-06 19:53:03,817 - INFO - training batch 1101, loss: 0.202, 35232/60000 datapoints
2025-03-06 19:53:04,044 - INFO - training batch 1151, loss: 0.141, 36832/60000 datapoints
2025-03-06 19:53:04,257 - INFO - training batch 1201, loss: 0.168, 38432/60000 datapoints
2025-03-06 19:53:04,460 - INFO - training batch 1251, loss: 0.483, 40032/60000 datapoints
2025-03-06 19:53:04,675 - INFO - training batch 1301, loss: 0.506, 41632/60000 datapoints
2025-03-06 19:53:04,882 - INFO - training batch 1351, loss: 0.343, 43232/60000 datapoints
2025-03-06 19:53:05,086 - INFO - training batch 1401, loss: 0.528, 44832/60000 datapoints
2025-03-06 19:53:05,296 - INFO - training batch 1451, loss: 0.311, 46432/60000 datapoints
2025-03-06 19:53:05,492 - INFO - training batch 1501, loss: 0.121, 48032/60000 datapoints
2025-03-06 19:53:05,692 - INFO - training batch 1551, loss: 0.336, 49632/60000 datapoints
2025-03-06 19:53:05,895 - INFO - training batch 1601, loss: 0.462, 51232/60000 datapoints
2025-03-06 19:53:06,104 - INFO - training batch 1651, loss: 0.157, 52832/60000 datapoints
2025-03-06 19:53:06,309 - INFO - training batch 1701, loss: 0.389, 54432/60000 datapoints
2025-03-06 19:53:06,549 - INFO - training batch 1751, loss: 0.244, 56032/60000 datapoints
2025-03-06 19:53:06,755 - INFO - training batch 1801, loss: 0.282, 57632/60000 datapoints
2025-03-06 19:53:06,962 - INFO - training batch 1851, loss: 0.185, 59232/60000 datapoints
2025-03-06 19:53:07,076 - INFO - validation batch 1, loss: 0.347, 32/10016 datapoints
2025-03-06 19:53:07,243 - INFO - validation batch 51, loss: 0.230, 1632/10016 datapoints
2025-03-06 19:53:07,405 - INFO - validation batch 101, loss: 0.279, 3232/10016 datapoints
2025-03-06 19:53:07,562 - INFO - validation batch 151, loss: 0.222, 4832/10016 datapoints
2025-03-06 19:53:07,726 - INFO - validation batch 201, loss: 0.197, 6432/10016 datapoints
2025-03-06 19:53:07,904 - INFO - validation batch 251, loss: 0.140, 8032/10016 datapoints
2025-03-06 19:53:08,062 - INFO - validation batch 301, loss: 0.575, 9632/10016 datapoints
2025-03-06 19:53:08,100 - INFO - Epoch 511/800 done.
2025-03-06 19:53:08,100 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:53:08,101 - INFO - Beginning epoch 512/800
2025-03-06 19:53:08,108 - INFO - training batch 1, loss: 0.223, 32/60000 datapoints
2025-03-06 19:53:08,324 - INFO - training batch 51, loss: 0.536, 1632/60000 datapoints
2025-03-06 19:53:08,525 - INFO - training batch 101, loss: 0.139, 3232/60000 datapoints
2025-03-06 19:53:08,732 - INFO - training batch 151, loss: 0.757, 4832/60000 datapoints
2025-03-06 19:53:08,946 - INFO - training batch 201, loss: 0.125, 6432/60000 datapoints
2025-03-06 19:53:09,146 - INFO - training batch 251, loss: 0.154, 8032/60000 datapoints
2025-03-06 19:53:09,348 - INFO - training batch 301, loss: 0.434, 9632/60000 datapoints
2025-03-06 19:53:09,547 - INFO - training batch 351, loss: 0.399, 11232/60000 datapoints
2025-03-06 19:53:09,744 - INFO - training batch 401, loss: 0.267, 12832/60000 datapoints
2025-03-06 19:53:09,947 - INFO - training batch 451, loss: 0.254, 14432/60000 datapoints
2025-03-06 19:53:10,143 - INFO - training batch 501, loss: 0.313, 16032/60000 datapoints
2025-03-06 19:53:10,365 - INFO - training batch 551, loss: 0.265, 17632/60000 datapoints
2025-03-06 19:53:10,563 - INFO - training batch 601, loss: 0.412, 19232/60000 datapoints
2025-03-06 19:53:10,761 - INFO - training batch 651, loss: 0.492, 20832/60000 datapoints
2025-03-06 19:53:10,958 - INFO - training batch 701, loss: 0.192, 22432/60000 datapoints
2025-03-06 19:53:11,154 - INFO - training batch 751, loss: 0.053, 24032/60000 datapoints
2025-03-06 19:53:11,357 - INFO - training batch 801, loss: 0.153, 25632/60000 datapoints
2025-03-06 19:53:11,556 - INFO - training batch 851, loss: 0.132, 27232/60000 datapoints
2025-03-06 19:53:11,754 - INFO - training batch 901, loss: 0.112, 28832/60000 datapoints
2025-03-06 19:53:11,950 - INFO - training batch 951, loss: 0.155, 30432/60000 datapoints
2025-03-06 19:53:12,147 - INFO - training batch 1001, loss: 0.285, 32032/60000 datapoints
2025-03-06 19:53:12,343 - INFO - training batch 1051, loss: 0.204, 33632/60000 datapoints
2025-03-06 19:53:12,544 - INFO - training batch 1101, loss: 0.194, 35232/60000 datapoints
2025-03-06 19:53:12,743 - INFO - training batch 1151, loss: 0.106, 36832/60000 datapoints
2025-03-06 19:53:12,942 - INFO - training batch 1201, loss: 0.175, 38432/60000 datapoints
2025-03-06 19:53:13,138 - INFO - training batch 1251, loss: 0.151, 40032/60000 datapoints
2025-03-06 19:53:13,334 - INFO - training batch 1301, loss: 0.384, 41632/60000 datapoints
2025-03-06 19:53:13,531 - INFO - training batch 1351, loss: 0.209, 43232/60000 datapoints
2025-03-06 19:53:13,729 - INFO - training batch 1401, loss: 0.155, 44832/60000 datapoints
2025-03-06 19:53:13,927 - INFO - training batch 1451, loss: 0.227, 46432/60000 datapoints
2025-03-06 19:53:14,125 - INFO - training batch 1501, loss: 0.186, 48032/60000 datapoints
2025-03-06 19:53:14,319 - INFO - training batch 1551, loss: 0.362, 49632/60000 datapoints
2025-03-06 19:53:14,521 - INFO - training batch 1601, loss: 0.392, 51232/60000 datapoints
2025-03-06 19:53:14,721 - INFO - training batch 1651, loss: 0.095, 52832/60000 datapoints
2025-03-06 19:53:14,923 - INFO - training batch 1701, loss: 0.124, 54432/60000 datapoints
2025-03-06 19:53:15,120 - INFO - training batch 1751, loss: 0.384, 56032/60000 datapoints
2025-03-06 19:53:15,318 - INFO - training batch 1801, loss: 0.219, 57632/60000 datapoints
2025-03-06 19:53:15,515 - INFO - training batch 1851, loss: 0.270, 59232/60000 datapoints
2025-03-06 19:53:15,617 - INFO - validation batch 1, loss: 0.308, 32/10016 datapoints
2025-03-06 19:53:15,773 - INFO - validation batch 51, loss: 0.050, 1632/10016 datapoints
2025-03-06 19:53:15,930 - INFO - validation batch 101, loss: 0.246, 3232/10016 datapoints
2025-03-06 19:53:16,083 - INFO - validation batch 151, loss: 0.105, 4832/10016 datapoints
2025-03-06 19:53:16,237 - INFO - validation batch 201, loss: 0.151, 6432/10016 datapoints
2025-03-06 19:53:16,391 - INFO - validation batch 251, loss: 0.253, 8032/10016 datapoints
2025-03-06 19:53:16,546 - INFO - validation batch 301, loss: 0.206, 9632/10016 datapoints
2025-03-06 19:53:16,586 - INFO - Epoch 512/800 done.
2025-03-06 19:53:16,586 - INFO - Final validation performance:
Loss: 0.188, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:53:16,586 - INFO - Beginning epoch 513/800
2025-03-06 19:53:16,593 - INFO - training batch 1, loss: 0.515, 32/60000 datapoints
2025-03-06 19:53:16,793 - INFO - training batch 51, loss: 0.106, 1632/60000 datapoints
2025-03-06 19:53:16,990 - INFO - training batch 101, loss: 0.145, 3232/60000 datapoints
2025-03-06 19:53:17,200 - INFO - training batch 151, loss: 0.201, 4832/60000 datapoints
2025-03-06 19:53:17,401 - INFO - training batch 201, loss: 0.480, 6432/60000 datapoints
2025-03-06 19:53:17,602 - INFO - training batch 251, loss: 0.239, 8032/60000 datapoints
2025-03-06 19:53:17,808 - INFO - training batch 301, loss: 0.249, 9632/60000 datapoints
2025-03-06 19:53:18,027 - INFO - training batch 351, loss: 0.102, 11232/60000 datapoints
2025-03-06 19:53:18,223 - INFO - training batch 401, loss: 0.186, 12832/60000 datapoints
2025-03-06 19:53:18,420 - INFO - training batch 451, loss: 0.120, 14432/60000 datapoints
2025-03-06 19:53:18,617 - INFO - training batch 501, loss: 0.360, 16032/60000 datapoints
2025-03-06 19:53:18,814 - INFO - training batch 551, loss: 0.110, 17632/60000 datapoints
2025-03-06 19:53:19,028 - INFO - training batch 601, loss: 0.438, 19232/60000 datapoints
2025-03-06 19:53:19,225 - INFO - training batch 651, loss: 0.479, 20832/60000 datapoints
2025-03-06 19:53:19,422 - INFO - training batch 701, loss: 0.081, 22432/60000 datapoints
2025-03-06 19:53:19,622 - INFO - training batch 751, loss: 0.155, 24032/60000 datapoints
2025-03-06 19:53:19,818 - INFO - training batch 801, loss: 0.327, 25632/60000 datapoints
2025-03-06 19:53:20,014 - INFO - training batch 851, loss: 0.349, 27232/60000 datapoints
2025-03-06 19:53:20,212 - INFO - training batch 901, loss: 0.260, 28832/60000 datapoints
2025-03-06 19:53:20,409 - INFO - training batch 951, loss: 0.214, 30432/60000 datapoints
2025-03-06 19:53:20,606 - INFO - training batch 1001, loss: 0.089, 32032/60000 datapoints
2025-03-06 19:53:20,806 - INFO - training batch 1051, loss: 0.131, 33632/60000 datapoints
2025-03-06 19:53:21,002 - INFO - training batch 1101, loss: 0.081, 35232/60000 datapoints
2025-03-06 19:53:21,196 - INFO - training batch 1151, loss: 0.448, 36832/60000 datapoints
2025-03-06 19:53:21,395 - INFO - training batch 1201, loss: 0.378, 38432/60000 datapoints
2025-03-06 19:53:21,591 - INFO - training batch 1251, loss: 0.653, 40032/60000 datapoints
2025-03-06 19:53:21,788 - INFO - training batch 1301, loss: 0.644, 41632/60000 datapoints
2025-03-06 19:53:21,986 - INFO - training batch 1351, loss: 0.396, 43232/60000 datapoints
2025-03-06 19:53:22,183 - INFO - training batch 1401, loss: 0.119, 44832/60000 datapoints
2025-03-06 19:53:22,380 - INFO - training batch 1451, loss: 0.245, 46432/60000 datapoints
2025-03-06 19:53:22,576 - INFO - training batch 1501, loss: 0.336, 48032/60000 datapoints
2025-03-06 19:53:22,776 - INFO - training batch 1551, loss: 0.229, 49632/60000 datapoints
2025-03-06 19:53:22,974 - INFO - training batch 1601, loss: 0.205, 51232/60000 datapoints
2025-03-06 19:53:23,172 - INFO - training batch 1651, loss: 0.131, 52832/60000 datapoints
2025-03-06 19:53:23,372 - INFO - training batch 1701, loss: 0.484, 54432/60000 datapoints
2025-03-06 19:53:23,568 - INFO - training batch 1751, loss: 0.211, 56032/60000 datapoints
2025-03-06 19:53:23,767 - INFO - training batch 1801, loss: 0.063, 57632/60000 datapoints
2025-03-06 19:53:23,963 - INFO - training batch 1851, loss: 0.286, 59232/60000 datapoints
2025-03-06 19:53:24,066 - INFO - validation batch 1, loss: 0.419, 32/10016 datapoints
2025-03-06 19:53:24,221 - INFO - validation batch 51, loss: 0.169, 1632/10016 datapoints
2025-03-06 19:53:24,372 - INFO - validation batch 101, loss: 0.088, 3232/10016 datapoints
2025-03-06 19:53:24,525 - INFO - validation batch 151, loss: 0.372, 4832/10016 datapoints
2025-03-06 19:53:24,681 - INFO - validation batch 201, loss: 0.353, 6432/10016 datapoints
2025-03-06 19:53:24,837 - INFO - validation batch 251, loss: 0.072, 8032/10016 datapoints
2025-03-06 19:53:24,997 - INFO - validation batch 301, loss: 0.459, 9632/10016 datapoints
2025-03-06 19:53:25,035 - INFO - Epoch 513/800 done.
2025-03-06 19:53:25,035 - INFO - Final validation performance:
Loss: 0.276, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:53:25,036 - INFO - Beginning epoch 514/800
2025-03-06 19:53:25,043 - INFO - training batch 1, loss: 0.343, 32/60000 datapoints
2025-03-06 19:53:25,264 - INFO - training batch 51, loss: 0.169, 1632/60000 datapoints
2025-03-06 19:53:25,466 - INFO - training batch 101, loss: 0.213, 3232/60000 datapoints
2025-03-06 19:53:25,675 - INFO - training batch 151, loss: 0.378, 4832/60000 datapoints
2025-03-06 19:53:25,874 - INFO - training batch 201, loss: 0.339, 6432/60000 datapoints
2025-03-06 19:53:26,076 - INFO - training batch 251, loss: 0.451, 8032/60000 datapoints
2025-03-06 19:53:26,272 - INFO - training batch 301, loss: 0.188, 9632/60000 datapoints
2025-03-06 19:53:26,469 - INFO - training batch 351, loss: 0.267, 11232/60000 datapoints
2025-03-06 19:53:26,667 - INFO - training batch 401, loss: 0.225, 12832/60000 datapoints
2025-03-06 19:53:27,018 - INFO - training batch 451, loss: 0.167, 14432/60000 datapoints
2025-03-06 19:53:27,294 - INFO - training batch 501, loss: 0.401, 16032/60000 datapoints
2025-03-06 19:53:27,535 - INFO - training batch 551, loss: 0.068, 17632/60000 datapoints
2025-03-06 19:53:27,816 - INFO - training batch 601, loss: 0.078, 19232/60000 datapoints
2025-03-06 19:53:28,066 - INFO - training batch 651, loss: 0.097, 20832/60000 datapoints
2025-03-06 19:53:28,278 - INFO - training batch 701, loss: 0.291, 22432/60000 datapoints
2025-03-06 19:53:28,489 - INFO - training batch 751, loss: 0.174, 24032/60000 datapoints
2025-03-06 19:53:28,703 - INFO - training batch 801, loss: 0.295, 25632/60000 datapoints
2025-03-06 19:53:28,926 - INFO - training batch 851, loss: 0.111, 27232/60000 datapoints
2025-03-06 19:53:29,162 - INFO - training batch 901, loss: 0.183, 28832/60000 datapoints
2025-03-06 19:53:29,395 - INFO - training batch 951, loss: 0.114, 30432/60000 datapoints
2025-03-06 19:53:29,597 - INFO - training batch 1001, loss: 0.508, 32032/60000 datapoints
2025-03-06 19:53:29,803 - INFO - training batch 1051, loss: 0.288, 33632/60000 datapoints
2025-03-06 19:53:30,026 - INFO - training batch 1101, loss: 0.173, 35232/60000 datapoints
2025-03-06 19:53:30,232 - INFO - training batch 1151, loss: 0.218, 36832/60000 datapoints
2025-03-06 19:53:30,452 - INFO - training batch 1201, loss: 0.303, 38432/60000 datapoints
2025-03-06 19:53:30,666 - INFO - training batch 1251, loss: 0.199, 40032/60000 datapoints
2025-03-06 19:53:30,875 - INFO - training batch 1301, loss: 0.070, 41632/60000 datapoints
2025-03-06 19:53:31,104 - INFO - training batch 1351, loss: 0.174, 43232/60000 datapoints
2025-03-06 19:53:31,314 - INFO - training batch 1401, loss: 0.305, 44832/60000 datapoints
2025-03-06 19:53:31,533 - INFO - training batch 1451, loss: 0.306, 46432/60000 datapoints
2025-03-06 19:53:31,745 - INFO - training batch 1501, loss: 0.511, 48032/60000 datapoints
2025-03-06 19:53:32,006 - INFO - training batch 1551, loss: 0.313, 49632/60000 datapoints
2025-03-06 19:53:32,212 - INFO - training batch 1601, loss: 0.335, 51232/60000 datapoints
2025-03-06 19:53:32,431 - INFO - training batch 1651, loss: 0.163, 52832/60000 datapoints
2025-03-06 19:53:32,660 - INFO - training batch 1701, loss: 0.209, 54432/60000 datapoints
2025-03-06 19:53:32,865 - INFO - training batch 1751, loss: 0.525, 56032/60000 datapoints
2025-03-06 19:53:33,071 - INFO - training batch 1801, loss: 0.303, 57632/60000 datapoints
2025-03-06 19:53:33,274 - INFO - training batch 1851, loss: 0.476, 59232/60000 datapoints
2025-03-06 19:53:33,390 - INFO - validation batch 1, loss: 0.177, 32/10016 datapoints
2025-03-06 19:53:33,569 - INFO - validation batch 51, loss: 0.418, 1632/10016 datapoints
2025-03-06 19:53:33,754 - INFO - validation batch 101, loss: 0.300, 3232/10016 datapoints
2025-03-06 19:53:33,934 - INFO - validation batch 151, loss: 0.327, 4832/10016 datapoints
2025-03-06 19:53:34,124 - INFO - validation batch 201, loss: 0.216, 6432/10016 datapoints
2025-03-06 19:53:34,303 - INFO - validation batch 251, loss: 0.096, 8032/10016 datapoints
2025-03-06 19:53:34,491 - INFO - validation batch 301, loss: 0.310, 9632/10016 datapoints
2025-03-06 19:53:34,535 - INFO - Epoch 514/800 done.
2025-03-06 19:53:34,535 - INFO - Final validation performance:
Loss: 0.263, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:53:34,536 - INFO - Beginning epoch 515/800
2025-03-06 19:53:34,543 - INFO - training batch 1, loss: 0.177, 32/60000 datapoints
2025-03-06 19:53:34,794 - INFO - training batch 51, loss: 0.294, 1632/60000 datapoints
2025-03-06 19:53:35,029 - INFO - training batch 101, loss: 0.420, 3232/60000 datapoints
2025-03-06 19:53:35,295 - INFO - training batch 151, loss: 0.350, 4832/60000 datapoints
2025-03-06 19:53:35,535 - INFO - training batch 201, loss: 0.448, 6432/60000 datapoints
2025-03-06 19:53:35,781 - INFO - training batch 251, loss: 0.311, 8032/60000 datapoints
2025-03-06 19:53:36,038 - INFO - training batch 301, loss: 0.408, 9632/60000 datapoints
2025-03-06 19:53:36,294 - INFO - training batch 351, loss: 0.057, 11232/60000 datapoints
2025-03-06 19:53:36,512 - INFO - training batch 401, loss: 0.196, 12832/60000 datapoints
2025-03-06 19:53:36,721 - INFO - training batch 451, loss: 0.189, 14432/60000 datapoints
2025-03-06 19:53:36,956 - INFO - training batch 501, loss: 0.179, 16032/60000 datapoints
2025-03-06 19:53:37,186 - INFO - training batch 551, loss: 0.281, 17632/60000 datapoints
2025-03-06 19:53:37,391 - INFO - training batch 601, loss: 0.189, 19232/60000 datapoints
2025-03-06 19:53:37,616 - INFO - training batch 651, loss: 0.093, 20832/60000 datapoints
2025-03-06 19:53:37,902 - INFO - training batch 701, loss: 0.262, 22432/60000 datapoints
2025-03-06 19:53:38,128 - INFO - training batch 751, loss: 0.163, 24032/60000 datapoints
2025-03-06 19:53:38,363 - INFO - training batch 801, loss: 0.428, 25632/60000 datapoints
2025-03-06 19:53:38,620 - INFO - training batch 851, loss: 0.416, 27232/60000 datapoints
2025-03-06 19:53:38,832 - INFO - training batch 901, loss: 0.448, 28832/60000 datapoints
2025-03-06 19:53:39,078 - INFO - training batch 951, loss: 0.233, 30432/60000 datapoints
2025-03-06 19:53:39,310 - INFO - training batch 1001, loss: 0.423, 32032/60000 datapoints
2025-03-06 19:53:39,542 - INFO - training batch 1051, loss: 0.243, 33632/60000 datapoints
2025-03-06 19:53:39,777 - INFO - training batch 1101, loss: 0.292, 35232/60000 datapoints
2025-03-06 19:53:40,004 - INFO - training batch 1151, loss: 0.134, 36832/60000 datapoints
2025-03-06 19:53:40,233 - INFO - training batch 1201, loss: 0.329, 38432/60000 datapoints
2025-03-06 19:53:40,444 - INFO - training batch 1251, loss: 0.255, 40032/60000 datapoints
2025-03-06 19:53:40,664 - INFO - training batch 1301, loss: 0.219, 41632/60000 datapoints
2025-03-06 19:53:40,894 - INFO - training batch 1351, loss: 0.142, 43232/60000 datapoints
2025-03-06 19:53:41,146 - INFO - training batch 1401, loss: 0.090, 44832/60000 datapoints
2025-03-06 19:53:41,401 - INFO - training batch 1451, loss: 0.199, 46432/60000 datapoints
2025-03-06 19:53:41,640 - INFO - training batch 1501, loss: 0.604, 48032/60000 datapoints
2025-03-06 19:53:41,935 - INFO - training batch 1551, loss: 0.481, 49632/60000 datapoints
2025-03-06 19:53:42,158 - INFO - training batch 1601, loss: 0.137, 51232/60000 datapoints
2025-03-06 19:53:42,369 - INFO - training batch 1651, loss: 0.687, 52832/60000 datapoints
2025-03-06 19:53:42,662 - INFO - training batch 1701, loss: 0.284, 54432/60000 datapoints
2025-03-06 19:53:42,910 - INFO - training batch 1751, loss: 0.193, 56032/60000 datapoints
2025-03-06 19:53:43,203 - INFO - training batch 1801, loss: 0.552, 57632/60000 datapoints
2025-03-06 19:53:43,486 - INFO - training batch 1851, loss: 0.272, 59232/60000 datapoints
2025-03-06 19:53:43,626 - INFO - validation batch 1, loss: 0.449, 32/10016 datapoints
2025-03-06 19:53:43,791 - INFO - validation batch 51, loss: 0.126, 1632/10016 datapoints
2025-03-06 19:53:43,953 - INFO - validation batch 101, loss: 0.236, 3232/10016 datapoints
2025-03-06 19:53:44,152 - INFO - validation batch 151, loss: 0.089, 4832/10016 datapoints
2025-03-06 19:53:44,378 - INFO - validation batch 201, loss: 0.124, 6432/10016 datapoints
2025-03-06 19:53:44,543 - INFO - validation batch 251, loss: 0.138, 8032/10016 datapoints
2025-03-06 19:53:44,705 - INFO - validation batch 301, loss: 0.163, 9632/10016 datapoints
2025-03-06 19:53:44,743 - INFO - Epoch 515/800 done.
2025-03-06 19:53:44,744 - INFO - Final validation performance:
Loss: 0.190, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:53:44,744 - INFO - Beginning epoch 516/800
2025-03-06 19:53:44,751 - INFO - training batch 1, loss: 0.369, 32/60000 datapoints
2025-03-06 19:53:44,958 - INFO - training batch 51, loss: 0.241, 1632/60000 datapoints
2025-03-06 19:53:45,159 - INFO - training batch 101, loss: 0.167, 3232/60000 datapoints
2025-03-06 19:53:45,382 - INFO - training batch 151, loss: 0.070, 4832/60000 datapoints
2025-03-06 19:53:45,636 - INFO - training batch 201, loss: 0.216, 6432/60000 datapoints
2025-03-06 19:53:45,934 - INFO - training batch 251, loss: 0.123, 8032/60000 datapoints
2025-03-06 19:53:46,152 - INFO - training batch 301, loss: 0.100, 9632/60000 datapoints
2025-03-06 19:53:46,413 - INFO - training batch 351, loss: 0.403, 11232/60000 datapoints
2025-03-06 19:53:46,661 - INFO - training batch 401, loss: 0.178, 12832/60000 datapoints
2025-03-06 19:53:46,908 - INFO - training batch 451, loss: 0.166, 14432/60000 datapoints
2025-03-06 19:53:47,144 - INFO - training batch 501, loss: 0.314, 16032/60000 datapoints
2025-03-06 19:53:47,360 - INFO - training batch 551, loss: 0.192, 17632/60000 datapoints
2025-03-06 19:53:47,561 - INFO - training batch 601, loss: 0.179, 19232/60000 datapoints
2025-03-06 19:53:47,786 - INFO - training batch 651, loss: 0.142, 20832/60000 datapoints
2025-03-06 19:53:47,995 - INFO - training batch 701, loss: 0.416, 22432/60000 datapoints
2025-03-06 19:53:48,257 - INFO - training batch 751, loss: 0.204, 24032/60000 datapoints
2025-03-06 19:53:48,481 - INFO - training batch 801, loss: 0.322, 25632/60000 datapoints
2025-03-06 19:53:48,692 - INFO - training batch 851, loss: 0.223, 27232/60000 datapoints
2025-03-06 19:53:48,903 - INFO - training batch 901, loss: 0.110, 28832/60000 datapoints
2025-03-06 19:53:49,102 - INFO - training batch 951, loss: 0.395, 30432/60000 datapoints
2025-03-06 19:53:49,309 - INFO - training batch 1001, loss: 0.547, 32032/60000 datapoints
2025-03-06 19:53:49,529 - INFO - training batch 1051, loss: 0.130, 33632/60000 datapoints
2025-03-06 19:53:49,731 - INFO - training batch 1101, loss: 0.171, 35232/60000 datapoints
2025-03-06 19:53:49,937 - INFO - training batch 1151, loss: 0.257, 36832/60000 datapoints
2025-03-06 19:53:50,133 - INFO - training batch 1201, loss: 0.559, 38432/60000 datapoints
2025-03-06 19:53:50,328 - INFO - training batch 1251, loss: 0.357, 40032/60000 datapoints
2025-03-06 19:53:50,523 - INFO - training batch 1301, loss: 0.253, 41632/60000 datapoints
2025-03-06 19:53:50,728 - INFO - training batch 1351, loss: 0.270, 43232/60000 datapoints
2025-03-06 19:53:51,179 - INFO - training batch 1401, loss: 0.120, 44832/60000 datapoints
2025-03-06 19:53:51,404 - INFO - training batch 1451, loss: 0.244, 46432/60000 datapoints
2025-03-06 19:53:51,600 - INFO - training batch 1501, loss: 0.195, 48032/60000 datapoints
2025-03-06 19:53:51,799 - INFO - training batch 1551, loss: 0.231, 49632/60000 datapoints
2025-03-06 19:53:51,996 - INFO - training batch 1601, loss: 0.313, 51232/60000 datapoints
2025-03-06 19:53:52,204 - INFO - training batch 1651, loss: 0.245, 52832/60000 datapoints
2025-03-06 19:53:52,399 - INFO - training batch 1701, loss: 0.584, 54432/60000 datapoints
2025-03-06 19:53:52,592 - INFO - training batch 1751, loss: 0.300, 56032/60000 datapoints
2025-03-06 19:53:52,790 - INFO - training batch 1801, loss: 0.206, 57632/60000 datapoints
2025-03-06 19:53:52,985 - INFO - training batch 1851, loss: 0.173, 59232/60000 datapoints
2025-03-06 19:53:53,089 - INFO - validation batch 1, loss: 0.048, 32/10016 datapoints
2025-03-06 19:53:53,243 - INFO - validation batch 51, loss: 0.131, 1632/10016 datapoints
2025-03-06 19:53:53,399 - INFO - validation batch 101, loss: 0.126, 3232/10016 datapoints
2025-03-06 19:53:53,553 - INFO - validation batch 151, loss: 0.097, 4832/10016 datapoints
2025-03-06 19:53:53,707 - INFO - validation batch 201, loss: 0.322, 6432/10016 datapoints
2025-03-06 19:53:53,864 - INFO - validation batch 251, loss: 0.383, 8032/10016 datapoints
2025-03-06 19:53:54,016 - INFO - validation batch 301, loss: 0.181, 9632/10016 datapoints
2025-03-06 19:53:54,055 - INFO - Epoch 516/800 done.
2025-03-06 19:53:54,055 - INFO - Final validation performance:
Loss: 0.184, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:53:54,056 - INFO - Beginning epoch 517/800
2025-03-06 19:53:54,063 - INFO - training batch 1, loss: 0.424, 32/60000 datapoints
2025-03-06 19:53:54,266 - INFO - training batch 51, loss: 0.148, 1632/60000 datapoints
2025-03-06 19:53:54,466 - INFO - training batch 101, loss: 0.149, 3232/60000 datapoints
2025-03-06 19:53:54,662 - INFO - training batch 151, loss: 0.170, 4832/60000 datapoints
2025-03-06 19:53:54,866 - INFO - training batch 201, loss: 0.251, 6432/60000 datapoints
2025-03-06 19:53:55,068 - INFO - training batch 251, loss: 0.060, 8032/60000 datapoints
2025-03-06 19:53:55,263 - INFO - training batch 301, loss: 0.112, 9632/60000 datapoints
2025-03-06 19:53:55,461 - INFO - training batch 351, loss: 0.251, 11232/60000 datapoints
2025-03-06 19:53:55,662 - INFO - training batch 401, loss: 0.308, 12832/60000 datapoints
2025-03-06 19:53:55,867 - INFO - training batch 451, loss: 0.371, 14432/60000 datapoints
2025-03-06 19:53:56,068 - INFO - training batch 501, loss: 0.439, 16032/60000 datapoints
2025-03-06 19:53:56,264 - INFO - training batch 551, loss: 0.288, 17632/60000 datapoints
2025-03-06 19:53:56,456 - INFO - training batch 601, loss: 0.112, 19232/60000 datapoints
2025-03-06 19:53:56,654 - INFO - training batch 651, loss: 0.324, 20832/60000 datapoints
2025-03-06 19:53:56,847 - INFO - training batch 701, loss: 0.147, 22432/60000 datapoints
2025-03-06 19:53:57,042 - INFO - training batch 751, loss: 0.131, 24032/60000 datapoints
2025-03-06 19:53:57,239 - INFO - training batch 801, loss: 0.293, 25632/60000 datapoints
2025-03-06 19:53:57,450 - INFO - training batch 851, loss: 0.506, 27232/60000 datapoints
2025-03-06 19:53:57,650 - INFO - training batch 901, loss: 0.297, 28832/60000 datapoints
2025-03-06 19:53:57,850 - INFO - training batch 951, loss: 0.620, 30432/60000 datapoints
2025-03-06 19:53:58,046 - INFO - training batch 1001, loss: 0.315, 32032/60000 datapoints
2025-03-06 19:53:58,244 - INFO - training batch 1051, loss: 0.403, 33632/60000 datapoints
2025-03-06 19:53:58,455 - INFO - training batch 1101, loss: 0.218, 35232/60000 datapoints
2025-03-06 19:53:58,653 - INFO - training batch 1151, loss: 0.658, 36832/60000 datapoints
2025-03-06 19:53:58,852 - INFO - training batch 1201, loss: 0.181, 38432/60000 datapoints
2025-03-06 19:53:59,046 - INFO - training batch 1251, loss: 0.100, 40032/60000 datapoints
2025-03-06 19:53:59,246 - INFO - training batch 1301, loss: 0.115, 41632/60000 datapoints
2025-03-06 19:53:59,444 - INFO - training batch 1351, loss: 0.653, 43232/60000 datapoints
2025-03-06 19:53:59,642 - INFO - training batch 1401, loss: 0.364, 44832/60000 datapoints
2025-03-06 19:53:59,836 - INFO - training batch 1451, loss: 0.139, 46432/60000 datapoints
2025-03-06 19:54:00,036 - INFO - training batch 1501, loss: 0.101, 48032/60000 datapoints
2025-03-06 19:54:00,232 - INFO - training batch 1551, loss: 0.142, 49632/60000 datapoints
2025-03-06 19:54:00,426 - INFO - training batch 1601, loss: 0.506, 51232/60000 datapoints
2025-03-06 19:54:00,622 - INFO - training batch 1651, loss: 0.738, 52832/60000 datapoints
2025-03-06 19:54:00,816 - INFO - training batch 1701, loss: 0.222, 54432/60000 datapoints
2025-03-06 19:54:01,012 - INFO - training batch 1751, loss: 0.198, 56032/60000 datapoints
2025-03-06 19:54:01,210 - INFO - training batch 1801, loss: 0.487, 57632/60000 datapoints
2025-03-06 19:54:01,408 - INFO - training batch 1851, loss: 0.190, 59232/60000 datapoints
2025-03-06 19:54:01,510 - INFO - validation batch 1, loss: 0.225, 32/10016 datapoints
2025-03-06 19:54:01,668 - INFO - validation batch 51, loss: 0.337, 1632/10016 datapoints
2025-03-06 19:54:01,818 - INFO - validation batch 101, loss: 0.220, 3232/10016 datapoints
2025-03-06 19:54:01,972 - INFO - validation batch 151, loss: 0.096, 4832/10016 datapoints
2025-03-06 19:54:02,125 - INFO - validation batch 201, loss: 0.331, 6432/10016 datapoints
2025-03-06 19:54:02,277 - INFO - validation batch 251, loss: 0.430, 8032/10016 datapoints
2025-03-06 19:54:02,431 - INFO - validation batch 301, loss: 0.233, 9632/10016 datapoints
2025-03-06 19:54:02,468 - INFO - Epoch 517/800 done.
2025-03-06 19:54:02,469 - INFO - Final validation performance:
Loss: 0.267, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:54:02,469 - INFO - Beginning epoch 518/800
2025-03-06 19:54:02,476 - INFO - training batch 1, loss: 0.311, 32/60000 datapoints
2025-03-06 19:54:02,693 - INFO - training batch 51, loss: 0.280, 1632/60000 datapoints
2025-03-06 19:54:02,885 - INFO - training batch 101, loss: 0.107, 3232/60000 datapoints
2025-03-06 19:54:03,094 - INFO - training batch 151, loss: 0.292, 4832/60000 datapoints
2025-03-06 19:54:03,350 - INFO - training batch 201, loss: 0.077, 6432/60000 datapoints
2025-03-06 19:54:03,608 - INFO - training batch 251, loss: 0.178, 8032/60000 datapoints
2025-03-06 19:54:03,874 - INFO - training batch 301, loss: 0.212, 9632/60000 datapoints
2025-03-06 19:54:04,129 - INFO - training batch 351, loss: 0.163, 11232/60000 datapoints
2025-03-06 19:54:04,360 - INFO - training batch 401, loss: 0.289, 12832/60000 datapoints
2025-03-06 19:54:04,582 - INFO - training batch 451, loss: 0.400, 14432/60000 datapoints
2025-03-06 19:54:04,814 - INFO - training batch 501, loss: 0.212, 16032/60000 datapoints
2025-03-06 19:54:05,040 - INFO - training batch 551, loss: 0.181, 17632/60000 datapoints
2025-03-06 19:54:05,253 - INFO - training batch 601, loss: 0.692, 19232/60000 datapoints
2025-03-06 19:54:05,452 - INFO - training batch 651, loss: 0.167, 20832/60000 datapoints
2025-03-06 19:54:05,653 - INFO - training batch 701, loss: 0.119, 22432/60000 datapoints
2025-03-06 19:54:05,848 - INFO - training batch 751, loss: 0.286, 24032/60000 datapoints
2025-03-06 19:54:06,043 - INFO - training batch 801, loss: 0.262, 25632/60000 datapoints
2025-03-06 19:54:06,261 - INFO - training batch 851, loss: 0.765, 27232/60000 datapoints
2025-03-06 19:54:06,468 - INFO - training batch 901, loss: 0.193, 28832/60000 datapoints
2025-03-06 19:54:06,674 - INFO - training batch 951, loss: 0.397, 30432/60000 datapoints
2025-03-06 19:54:06,873 - INFO - training batch 1001, loss: 0.497, 32032/60000 datapoints
2025-03-06 19:54:07,074 - INFO - training batch 1051, loss: 0.612, 33632/60000 datapoints
2025-03-06 19:54:07,276 - INFO - training batch 1101, loss: 0.100, 35232/60000 datapoints
2025-03-06 19:54:07,480 - INFO - training batch 1151, loss: 0.208, 36832/60000 datapoints
2025-03-06 19:54:07,683 - INFO - training batch 1201, loss: 0.144, 38432/60000 datapoints
2025-03-06 19:54:07,877 - INFO - training batch 1251, loss: 0.150, 40032/60000 datapoints
2025-03-06 19:54:08,071 - INFO - training batch 1301, loss: 0.178, 41632/60000 datapoints
2025-03-06 19:54:08,273 - INFO - training batch 1351, loss: 0.255, 43232/60000 datapoints
2025-03-06 19:54:08,486 - INFO - training batch 1401, loss: 0.288, 44832/60000 datapoints
2025-03-06 19:54:08,685 - INFO - training batch 1451, loss: 0.227, 46432/60000 datapoints
2025-03-06 19:54:08,885 - INFO - training batch 1501, loss: 0.127, 48032/60000 datapoints
2025-03-06 19:54:09,080 - INFO - training batch 1551, loss: 0.450, 49632/60000 datapoints
2025-03-06 19:54:09,275 - INFO - training batch 1601, loss: 0.334, 51232/60000 datapoints
2025-03-06 19:54:09,471 - INFO - training batch 1651, loss: 0.108, 52832/60000 datapoints
2025-03-06 19:54:09,668 - INFO - training batch 1701, loss: 0.226, 54432/60000 datapoints
2025-03-06 19:54:09,862 - INFO - training batch 1751, loss: 0.108, 56032/60000 datapoints
2025-03-06 19:54:10,059 - INFO - training batch 1801, loss: 0.565, 57632/60000 datapoints
2025-03-06 19:54:10,256 - INFO - training batch 1851, loss: 0.194, 59232/60000 datapoints
2025-03-06 19:54:10,356 - INFO - validation batch 1, loss: 0.172, 32/10016 datapoints
2025-03-06 19:54:10,508 - INFO - validation batch 51, loss: 0.231, 1632/10016 datapoints
2025-03-06 19:54:10,667 - INFO - validation batch 101, loss: 0.255, 3232/10016 datapoints
2025-03-06 19:54:10,820 - INFO - validation batch 151, loss: 0.582, 4832/10016 datapoints
2025-03-06 19:54:10,974 - INFO - validation batch 201, loss: 0.094, 6432/10016 datapoints
2025-03-06 19:54:11,129 - INFO - validation batch 251, loss: 0.419, 8032/10016 datapoints
2025-03-06 19:54:11,287 - INFO - validation batch 301, loss: 0.351, 9632/10016 datapoints
2025-03-06 19:54:11,324 - INFO - Epoch 518/800 done.
2025-03-06 19:54:11,324 - INFO - Final validation performance:
Loss: 0.300, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:54:11,325 - INFO - Beginning epoch 519/800
2025-03-06 19:54:11,332 - INFO - training batch 1, loss: 0.366, 32/60000 datapoints
2025-03-06 19:54:11,547 - INFO - training batch 51, loss: 0.178, 1632/60000 datapoints
2025-03-06 19:54:11,746 - INFO - training batch 101, loss: 0.588, 3232/60000 datapoints
2025-03-06 19:54:11,947 - INFO - training batch 151, loss: 0.261, 4832/60000 datapoints
2025-03-06 19:54:12,148 - INFO - training batch 201, loss: 0.199, 6432/60000 datapoints
2025-03-06 19:54:12,346 - INFO - training batch 251, loss: 0.169, 8032/60000 datapoints
2025-03-06 19:54:12,544 - INFO - training batch 301, loss: 0.145, 9632/60000 datapoints
2025-03-06 19:54:12,739 - INFO - training batch 351, loss: 0.222, 11232/60000 datapoints
2025-03-06 19:54:12,944 - INFO - training batch 401, loss: 0.072, 12832/60000 datapoints
2025-03-06 19:54:13,142 - INFO - training batch 451, loss: 0.233, 14432/60000 datapoints
2025-03-06 19:54:13,384 - INFO - training batch 501, loss: 0.074, 16032/60000 datapoints
2025-03-06 19:54:13,638 - INFO - training batch 551, loss: 0.075, 17632/60000 datapoints
2025-03-06 19:54:13,867 - INFO - training batch 601, loss: 0.107, 19232/60000 datapoints
2025-03-06 19:54:14,079 - INFO - training batch 651, loss: 0.173, 20832/60000 datapoints
2025-03-06 19:54:14,295 - INFO - training batch 701, loss: 0.390, 22432/60000 datapoints
2025-03-06 19:54:14,510 - INFO - training batch 751, loss: 0.159, 24032/60000 datapoints
2025-03-06 19:54:14,732 - INFO - training batch 801, loss: 0.367, 25632/60000 datapoints
2025-03-06 19:54:14,951 - INFO - training batch 851, loss: 0.338, 27232/60000 datapoints
2025-03-06 19:54:15,159 - INFO - training batch 901, loss: 0.095, 28832/60000 datapoints
2025-03-06 19:54:15,377 - INFO - training batch 951, loss: 0.428, 30432/60000 datapoints
2025-03-06 19:54:15,612 - INFO - training batch 1001, loss: 0.435, 32032/60000 datapoints
2025-03-06 19:54:15,998 - INFO - training batch 1051, loss: 0.086, 33632/60000 datapoints
2025-03-06 19:54:16,426 - INFO - training batch 1101, loss: 0.305, 35232/60000 datapoints
2025-03-06 19:54:16,767 - INFO - training batch 1151, loss: 0.312, 36832/60000 datapoints
2025-03-06 19:54:17,056 - INFO - training batch 1201, loss: 0.312, 38432/60000 datapoints
2025-03-06 19:54:17,351 - INFO - training batch 1251, loss: 0.533, 40032/60000 datapoints
2025-03-06 19:54:17,645 - INFO - training batch 1301, loss: 0.442, 41632/60000 datapoints
2025-03-06 19:54:17,918 - INFO - training batch 1351, loss: 0.232, 43232/60000 datapoints
2025-03-06 19:54:18,182 - INFO - training batch 1401, loss: 0.167, 44832/60000 datapoints
2025-03-06 19:54:18,442 - INFO - training batch 1451, loss: 0.099, 46432/60000 datapoints
2025-03-06 19:54:18,705 - INFO - training batch 1501, loss: 0.062, 48032/60000 datapoints
2025-03-06 19:54:18,917 - INFO - training batch 1551, loss: 0.203, 49632/60000 datapoints
2025-03-06 19:54:19,122 - INFO - training batch 1601, loss: 0.146, 51232/60000 datapoints
2025-03-06 19:54:19,329 - INFO - training batch 1651, loss: 0.191, 52832/60000 datapoints
2025-03-06 19:54:19,537 - INFO - training batch 1701, loss: 0.304, 54432/60000 datapoints
2025-03-06 19:54:19,745 - INFO - training batch 1751, loss: 0.285, 56032/60000 datapoints
2025-03-06 19:54:19,955 - INFO - training batch 1801, loss: 0.048, 57632/60000 datapoints
2025-03-06 19:54:20,162 - INFO - training batch 1851, loss: 0.117, 59232/60000 datapoints
2025-03-06 19:54:20,275 - INFO - validation batch 1, loss: 0.121, 32/10016 datapoints
2025-03-06 19:54:20,431 - INFO - validation batch 51, loss: 0.161, 1632/10016 datapoints
2025-03-06 19:54:20,586 - INFO - validation batch 101, loss: 0.084, 3232/10016 datapoints
2025-03-06 19:54:20,746 - INFO - validation batch 151, loss: 0.180, 4832/10016 datapoints
2025-03-06 19:54:20,902 - INFO - validation batch 201, loss: 0.085, 6432/10016 datapoints
2025-03-06 19:54:21,057 - INFO - validation batch 251, loss: 0.407, 8032/10016 datapoints
2025-03-06 19:54:21,213 - INFO - validation batch 301, loss: 0.163, 9632/10016 datapoints
2025-03-06 19:54:21,251 - INFO - Epoch 519/800 done.
2025-03-06 19:54:21,252 - INFO - Final validation performance:
Loss: 0.172, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:54:21,252 - INFO - Beginning epoch 520/800
2025-03-06 19:54:21,259 - INFO - training batch 1, loss: 0.141, 32/60000 datapoints
2025-03-06 19:54:21,474 - INFO - training batch 51, loss: 0.106, 1632/60000 datapoints
2025-03-06 19:54:21,686 - INFO - training batch 101, loss: 0.159, 3232/60000 datapoints
2025-03-06 19:54:21,889 - INFO - training batch 151, loss: 0.200, 4832/60000 datapoints
2025-03-06 19:54:22,109 - INFO - training batch 201, loss: 0.302, 6432/60000 datapoints
2025-03-06 19:54:22,329 - INFO - training batch 251, loss: 0.358, 8032/60000 datapoints
2025-03-06 19:54:22,551 - INFO - training batch 301, loss: 0.152, 9632/60000 datapoints
2025-03-06 19:54:22,794 - INFO - training batch 351, loss: 0.557, 11232/60000 datapoints
2025-03-06 19:54:23,043 - INFO - training batch 401, loss: 0.424, 12832/60000 datapoints
2025-03-06 19:54:23,261 - INFO - training batch 451, loss: 0.090, 14432/60000 datapoints
2025-03-06 19:54:23,469 - INFO - training batch 501, loss: 0.173, 16032/60000 datapoints
2025-03-06 19:54:23,706 - INFO - training batch 551, loss: 0.147, 17632/60000 datapoints
2025-03-06 19:54:23,913 - INFO - training batch 601, loss: 0.174, 19232/60000 datapoints
2025-03-06 19:54:24,134 - INFO - training batch 651, loss: 0.187, 20832/60000 datapoints
2025-03-06 19:54:24,361 - INFO - training batch 701, loss: 0.386, 22432/60000 datapoints
2025-03-06 19:54:24,559 - INFO - training batch 751, loss: 0.235, 24032/60000 datapoints
2025-03-06 19:54:24,789 - INFO - training batch 801, loss: 0.265, 25632/60000 datapoints
2025-03-06 19:54:25,002 - INFO - training batch 851, loss: 0.233, 27232/60000 datapoints
2025-03-06 19:54:25,213 - INFO - training batch 901, loss: 0.257, 28832/60000 datapoints
2025-03-06 19:54:25,424 - INFO - training batch 951, loss: 0.467, 30432/60000 datapoints
2025-03-06 19:54:25,651 - INFO - training batch 1001, loss: 0.073, 32032/60000 datapoints
2025-03-06 19:54:25,875 - INFO - training batch 1051, loss: 0.205, 33632/60000 datapoints
2025-03-06 19:54:26,099 - INFO - training batch 1101, loss: 0.076, 35232/60000 datapoints
2025-03-06 19:54:26,325 - INFO - training batch 1151, loss: 0.172, 36832/60000 datapoints
2025-03-06 19:54:26,553 - INFO - training batch 1201, loss: 0.289, 38432/60000 datapoints
2025-03-06 19:54:26,778 - INFO - training batch 1251, loss: 0.158, 40032/60000 datapoints
2025-03-06 19:54:27,065 - INFO - training batch 1301, loss: 0.445, 41632/60000 datapoints
2025-03-06 19:54:27,298 - INFO - training batch 1351, loss: 0.369, 43232/60000 datapoints
2025-03-06 19:54:27,552 - INFO - training batch 1401, loss: 0.062, 44832/60000 datapoints
2025-03-06 19:54:27,817 - INFO - training batch 1451, loss: 0.102, 46432/60000 datapoints
2025-03-06 19:54:28,040 - INFO - training batch 1501, loss: 0.118, 48032/60000 datapoints
2025-03-06 19:54:28,251 - INFO - training batch 1551, loss: 0.414, 49632/60000 datapoints
2025-03-06 19:54:28,455 - INFO - training batch 1601, loss: 0.165, 51232/60000 datapoints
2025-03-06 19:54:28,683 - INFO - training batch 1651, loss: 0.326, 52832/60000 datapoints
2025-03-06 19:54:28,914 - INFO - training batch 1701, loss: 0.179, 54432/60000 datapoints
2025-03-06 19:54:29,126 - INFO - training batch 1751, loss: 0.052, 56032/60000 datapoints
2025-03-06 19:54:29,345 - INFO - training batch 1801, loss: 0.116, 57632/60000 datapoints
2025-03-06 19:54:29,555 - INFO - training batch 1851, loss: 0.367, 59232/60000 datapoints
2025-03-06 19:54:29,669 - INFO - validation batch 1, loss: 0.215, 32/10016 datapoints
2025-03-06 19:54:29,855 - INFO - validation batch 51, loss: 0.410, 1632/10016 datapoints
2025-03-06 19:54:30,030 - INFO - validation batch 101, loss: 0.124, 3232/10016 datapoints
2025-03-06 19:54:30,196 - INFO - validation batch 151, loss: 0.317, 4832/10016 datapoints
2025-03-06 19:54:30,359 - INFO - validation batch 201, loss: 0.173, 6432/10016 datapoints
2025-03-06 19:54:30,517 - INFO - validation batch 251, loss: 0.163, 8032/10016 datapoints
2025-03-06 19:54:30,679 - INFO - validation batch 301, loss: 0.275, 9632/10016 datapoints
2025-03-06 19:54:30,722 - INFO - Epoch 520/800 done.
2025-03-06 19:54:30,722 - INFO - Final validation performance:
Loss: 0.240, top-1 acc: 0.927top-5 acc: 0.927
2025-03-06 19:54:30,723 - INFO - Beginning epoch 521/800
2025-03-06 19:54:30,730 - INFO - training batch 1, loss: 0.296, 32/60000 datapoints
2025-03-06 19:54:30,937 - INFO - training batch 51, loss: 0.599, 1632/60000 datapoints
2025-03-06 19:54:31,137 - INFO - training batch 101, loss: 0.091, 3232/60000 datapoints
2025-03-06 19:54:31,345 - INFO - training batch 151, loss: 0.364, 4832/60000 datapoints
2025-03-06 19:54:31,548 - INFO - training batch 201, loss: 0.197, 6432/60000 datapoints
2025-03-06 19:54:31,757 - INFO - training batch 251, loss: 0.230, 8032/60000 datapoints
2025-03-06 19:54:31,960 - INFO - training batch 301, loss: 0.536, 9632/60000 datapoints
2025-03-06 19:54:32,158 - INFO - training batch 351, loss: 0.188, 11232/60000 datapoints
2025-03-06 19:54:32,351 - INFO - training batch 401, loss: 0.397, 12832/60000 datapoints
2025-03-06 19:54:32,546 - INFO - training batch 451, loss: 0.183, 14432/60000 datapoints
2025-03-06 19:54:32,743 - INFO - training batch 501, loss: 0.482, 16032/60000 datapoints
2025-03-06 19:54:32,936 - INFO - training batch 551, loss: 0.294, 17632/60000 datapoints
2025-03-06 19:54:33,130 - INFO - training batch 601, loss: 0.359, 19232/60000 datapoints
2025-03-06 19:54:33,322 - INFO - training batch 651, loss: 0.239, 20832/60000 datapoints
2025-03-06 19:54:33,517 - INFO - training batch 701, loss: 0.360, 22432/60000 datapoints
2025-03-06 19:54:33,712 - INFO - training batch 751, loss: 0.245, 24032/60000 datapoints
2025-03-06 19:54:33,905 - INFO - training batch 801, loss: 0.532, 25632/60000 datapoints
2025-03-06 19:54:34,096 - INFO - training batch 851, loss: 0.122, 27232/60000 datapoints
2025-03-06 19:54:34,290 - INFO - training batch 901, loss: 0.161, 28832/60000 datapoints
2025-03-06 19:54:34,484 - INFO - training batch 951, loss: 0.392, 30432/60000 datapoints
2025-03-06 19:54:34,679 - INFO - training batch 1001, loss: 0.350, 32032/60000 datapoints
2025-03-06 19:54:34,878 - INFO - training batch 1051, loss: 0.202, 33632/60000 datapoints
2025-03-06 19:54:35,074 - INFO - training batch 1101, loss: 0.158, 35232/60000 datapoints
2025-03-06 19:54:35,265 - INFO - training batch 1151, loss: 0.286, 36832/60000 datapoints
2025-03-06 19:54:35,461 - INFO - training batch 1201, loss: 0.410, 38432/60000 datapoints
2025-03-06 19:54:35,659 - INFO - training batch 1251, loss: 0.209, 40032/60000 datapoints
2025-03-06 19:54:35,854 - INFO - training batch 1301, loss: 0.227, 41632/60000 datapoints
2025-03-06 19:54:36,051 - INFO - training batch 1351, loss: 0.223, 43232/60000 datapoints
2025-03-06 19:54:36,244 - INFO - training batch 1401, loss: 0.171, 44832/60000 datapoints
2025-03-06 19:54:36,436 - INFO - training batch 1451, loss: 0.296, 46432/60000 datapoints
2025-03-06 19:54:36,631 - INFO - training batch 1501, loss: 0.190, 48032/60000 datapoints
2025-03-06 19:54:36,822 - INFO - training batch 1551, loss: 0.163, 49632/60000 datapoints
2025-03-06 19:54:37,016 - INFO - training batch 1601, loss: 0.599, 51232/60000 datapoints
2025-03-06 19:54:37,207 - INFO - training batch 1651, loss: 0.101, 52832/60000 datapoints
2025-03-06 19:54:37,407 - INFO - training batch 1701, loss: 0.222, 54432/60000 datapoints
2025-03-06 19:54:37,603 - INFO - training batch 1751, loss: 0.429, 56032/60000 datapoints
2025-03-06 19:54:37,823 - INFO - training batch 1801, loss: 0.306, 57632/60000 datapoints
2025-03-06 19:54:38,019 - INFO - training batch 1851, loss: 0.440, 59232/60000 datapoints
2025-03-06 19:54:38,122 - INFO - validation batch 1, loss: 0.278, 32/10016 datapoints
2025-03-06 19:54:38,279 - INFO - validation batch 51, loss: 0.315, 1632/10016 datapoints
2025-03-06 19:54:38,443 - INFO - validation batch 101, loss: 0.216, 3232/10016 datapoints
2025-03-06 19:54:38,641 - INFO - validation batch 151, loss: 0.410, 4832/10016 datapoints
2025-03-06 19:54:38,860 - INFO - validation batch 201, loss: 0.076, 6432/10016 datapoints
2025-03-06 19:54:39,051 - INFO - validation batch 251, loss: 0.431, 8032/10016 datapoints
2025-03-06 19:54:39,232 - INFO - validation batch 301, loss: 0.115, 9632/10016 datapoints
2025-03-06 19:54:39,274 - INFO - Epoch 521/800 done.
2025-03-06 19:54:39,274 - INFO - Final validation performance:
Loss: 0.263, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:54:39,275 - INFO - Beginning epoch 522/800
2025-03-06 19:54:39,283 - INFO - training batch 1, loss: 0.496, 32/60000 datapoints
2025-03-06 19:54:39,503 - INFO - training batch 51, loss: 0.185, 1632/60000 datapoints
2025-03-06 19:54:39,715 - INFO - training batch 101, loss: 0.291, 3232/60000 datapoints
2025-03-06 19:54:39,934 - INFO - training batch 151, loss: 0.125, 4832/60000 datapoints
2025-03-06 19:54:40,142 - INFO - training batch 201, loss: 0.226, 6432/60000 datapoints
2025-03-06 19:54:40,357 - INFO - training batch 251, loss: 0.364, 8032/60000 datapoints
2025-03-06 19:54:40,568 - INFO - training batch 301, loss: 0.189, 9632/60000 datapoints
2025-03-06 19:54:40,771 - INFO - training batch 351, loss: 0.327, 11232/60000 datapoints
2025-03-06 19:54:40,973 - INFO - training batch 401, loss: 0.453, 12832/60000 datapoints
2025-03-06 19:54:41,172 - INFO - training batch 451, loss: 0.196, 14432/60000 datapoints
2025-03-06 19:54:41,373 - INFO - training batch 501, loss: 0.339, 16032/60000 datapoints
2025-03-06 19:54:41,604 - INFO - training batch 551, loss: 0.223, 17632/60000 datapoints
2025-03-06 19:54:41,811 - INFO - training batch 601, loss: 0.200, 19232/60000 datapoints
2025-03-06 19:54:42,017 - INFO - training batch 651, loss: 0.213, 20832/60000 datapoints
2025-03-06 19:54:42,225 - INFO - training batch 701, loss: 0.149, 22432/60000 datapoints
2025-03-06 19:54:42,425 - INFO - training batch 751, loss: 0.204, 24032/60000 datapoints
2025-03-06 19:54:42,630 - INFO - training batch 801, loss: 0.192, 25632/60000 datapoints
2025-03-06 19:54:42,834 - INFO - training batch 851, loss: 0.392, 27232/60000 datapoints
2025-03-06 19:54:43,036 - INFO - training batch 901, loss: 0.667, 28832/60000 datapoints
2025-03-06 19:54:43,235 - INFO - training batch 951, loss: 0.264, 30432/60000 datapoints
2025-03-06 19:54:43,440 - INFO - training batch 1001, loss: 0.283, 32032/60000 datapoints
2025-03-06 19:54:43,642 - INFO - training batch 1051, loss: 0.098, 33632/60000 datapoints
2025-03-06 19:54:43,841 - INFO - training batch 1101, loss: 0.160, 35232/60000 datapoints
2025-03-06 19:54:44,042 - INFO - training batch 1151, loss: 0.379, 36832/60000 datapoints
2025-03-06 19:54:44,239 - INFO - training batch 1201, loss: 0.379, 38432/60000 datapoints
2025-03-06 19:54:44,436 - INFO - training batch 1251, loss: 0.484, 40032/60000 datapoints
2025-03-06 19:54:44,633 - INFO - training batch 1301, loss: 0.216, 41632/60000 datapoints
2025-03-06 19:54:44,834 - INFO - training batch 1351, loss: 0.269, 43232/60000 datapoints
2025-03-06 19:54:45,029 - INFO - training batch 1401, loss: 0.109, 44832/60000 datapoints
2025-03-06 19:54:45,225 - INFO - training batch 1451, loss: 0.277, 46432/60000 datapoints
2025-03-06 19:54:45,429 - INFO - training batch 1501, loss: 0.175, 48032/60000 datapoints
2025-03-06 19:54:45,628 - INFO - training batch 1551, loss: 0.576, 49632/60000 datapoints
2025-03-06 19:54:45,825 - INFO - training batch 1601, loss: 0.358, 51232/60000 datapoints
2025-03-06 19:54:46,029 - INFO - training batch 1651, loss: 0.507, 52832/60000 datapoints
2025-03-06 19:54:46,223 - INFO - training batch 1701, loss: 0.156, 54432/60000 datapoints
2025-03-06 19:54:46,426 - INFO - training batch 1751, loss: 0.255, 56032/60000 datapoints
2025-03-06 19:54:46,621 - INFO - training batch 1801, loss: 0.299, 57632/60000 datapoints
2025-03-06 19:54:46,818 - INFO - training batch 1851, loss: 0.200, 59232/60000 datapoints
2025-03-06 19:54:46,921 - INFO - validation batch 1, loss: 0.130, 32/10016 datapoints
2025-03-06 19:54:47,075 - INFO - validation batch 51, loss: 0.152, 1632/10016 datapoints
2025-03-06 19:54:47,230 - INFO - validation batch 101, loss: 0.441, 3232/10016 datapoints
2025-03-06 19:54:47,382 - INFO - validation batch 151, loss: 0.178, 4832/10016 datapoints
2025-03-06 19:54:47,541 - INFO - validation batch 201, loss: 0.362, 6432/10016 datapoints
2025-03-06 19:54:47,699 - INFO - validation batch 251, loss: 0.130, 8032/10016 datapoints
2025-03-06 19:54:47,856 - INFO - validation batch 301, loss: 0.334, 9632/10016 datapoints
2025-03-06 19:54:47,895 - INFO - Epoch 522/800 done.
2025-03-06 19:54:47,895 - INFO - Final validation performance:
Loss: 0.247, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:54:47,896 - INFO - Beginning epoch 523/800
2025-03-06 19:54:47,903 - INFO - training batch 1, loss: 0.151, 32/60000 datapoints
2025-03-06 19:54:48,103 - INFO - training batch 51, loss: 0.303, 1632/60000 datapoints
2025-03-06 19:54:48,295 - INFO - training batch 101, loss: 0.197, 3232/60000 datapoints
2025-03-06 19:54:48,502 - INFO - training batch 151, loss: 0.144, 4832/60000 datapoints
2025-03-06 19:54:48,716 - INFO - training batch 201, loss: 0.200, 6432/60000 datapoints
2025-03-06 19:54:48,932 - INFO - training batch 251, loss: 0.197, 8032/60000 datapoints
2025-03-06 19:54:49,129 - INFO - training batch 301, loss: 0.280, 9632/60000 datapoints
2025-03-06 19:54:49,328 - INFO - training batch 351, loss: 0.358, 11232/60000 datapoints
2025-03-06 19:54:49,525 - INFO - training batch 401, loss: 0.418, 12832/60000 datapoints
2025-03-06 19:54:49,723 - INFO - training batch 451, loss: 0.278, 14432/60000 datapoints
2025-03-06 19:54:49,919 - INFO - training batch 501, loss: 0.405, 16032/60000 datapoints
2025-03-06 19:54:50,114 - INFO - training batch 551, loss: 0.542, 17632/60000 datapoints
2025-03-06 19:54:50,307 - INFO - training batch 601, loss: 0.316, 19232/60000 datapoints
2025-03-06 19:54:50,502 - INFO - training batch 651, loss: 0.228, 20832/60000 datapoints
2025-03-06 19:54:50,701 - INFO - training batch 701, loss: 0.237, 22432/60000 datapoints
2025-03-06 19:54:50,896 - INFO - training batch 751, loss: 0.146, 24032/60000 datapoints
2025-03-06 19:54:51,094 - INFO - training batch 801, loss: 0.499, 25632/60000 datapoints
2025-03-06 19:54:51,289 - INFO - training batch 851, loss: 0.525, 27232/60000 datapoints
2025-03-06 19:54:51,487 - INFO - training batch 901, loss: 0.229, 28832/60000 datapoints
2025-03-06 19:54:51,684 - INFO - training batch 951, loss: 0.187, 30432/60000 datapoints
2025-03-06 19:54:51,880 - INFO - training batch 1001, loss: 0.234, 32032/60000 datapoints
2025-03-06 19:54:52,078 - INFO - training batch 1051, loss: 0.159, 33632/60000 datapoints
2025-03-06 19:54:52,274 - INFO - training batch 1101, loss: 0.302, 35232/60000 datapoints
2025-03-06 19:54:52,475 - INFO - training batch 1151, loss: 0.120, 36832/60000 datapoints
2025-03-06 19:54:52,672 - INFO - training batch 1201, loss: 0.103, 38432/60000 datapoints
2025-03-06 19:54:52,864 - INFO - training batch 1251, loss: 0.245, 40032/60000 datapoints
2025-03-06 19:54:53,058 - INFO - training batch 1301, loss: 0.355, 41632/60000 datapoints
2025-03-06 19:54:53,252 - INFO - training batch 1351, loss: 0.196, 43232/60000 datapoints
2025-03-06 19:54:53,447 - INFO - training batch 1401, loss: 0.249, 44832/60000 datapoints
2025-03-06 19:54:53,645 - INFO - training batch 1451, loss: 0.562, 46432/60000 datapoints
2025-03-06 19:54:53,840 - INFO - training batch 1501, loss: 0.197, 48032/60000 datapoints
2025-03-06 19:54:54,036 - INFO - training batch 1551, loss: 0.271, 49632/60000 datapoints
2025-03-06 19:54:54,230 - INFO - training batch 1601, loss: 0.194, 51232/60000 datapoints
2025-03-06 19:54:54,429 - INFO - training batch 1651, loss: 0.273, 52832/60000 datapoints
2025-03-06 19:54:54,626 - INFO - training batch 1701, loss: 0.385, 54432/60000 datapoints
2025-03-06 19:54:54,826 - INFO - training batch 1751, loss: 0.357, 56032/60000 datapoints
2025-03-06 19:54:55,022 - INFO - training batch 1801, loss: 0.180, 57632/60000 datapoints
2025-03-06 19:54:55,219 - INFO - training batch 1851, loss: 0.339, 59232/60000 datapoints
2025-03-06 19:54:55,319 - INFO - validation batch 1, loss: 0.265, 32/10016 datapoints
2025-03-06 19:54:55,478 - INFO - validation batch 51, loss: 0.092, 1632/10016 datapoints
2025-03-06 19:54:55,635 - INFO - validation batch 101, loss: 0.260, 3232/10016 datapoints
2025-03-06 19:54:55,787 - INFO - validation batch 151, loss: 0.093, 4832/10016 datapoints
2025-03-06 19:54:55,939 - INFO - validation batch 201, loss: 0.173, 6432/10016 datapoints
2025-03-06 19:54:56,093 - INFO - validation batch 251, loss: 0.052, 8032/10016 datapoints
2025-03-06 19:54:56,246 - INFO - validation batch 301, loss: 0.075, 9632/10016 datapoints
2025-03-06 19:54:56,284 - INFO - Epoch 523/800 done.
2025-03-06 19:54:56,285 - INFO - Final validation performance:
Loss: 0.144, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:54:56,285 - INFO - Beginning epoch 524/800
2025-03-06 19:54:56,291 - INFO - training batch 1, loss: 0.093, 32/60000 datapoints
2025-03-06 19:54:56,492 - INFO - training batch 51, loss: 0.105, 1632/60000 datapoints
2025-03-06 19:54:56,701 - INFO - training batch 101, loss: 0.213, 3232/60000 datapoints
2025-03-06 19:54:56,894 - INFO - training batch 151, loss: 0.269, 4832/60000 datapoints
2025-03-06 19:54:57,099 - INFO - training batch 201, loss: 0.165, 6432/60000 datapoints
2025-03-06 19:54:57,297 - INFO - training batch 251, loss: 0.145, 8032/60000 datapoints
2025-03-06 19:54:57,513 - INFO - training batch 301, loss: 0.318, 9632/60000 datapoints
2025-03-06 19:54:57,772 - INFO - training batch 351, loss: 0.244, 11232/60000 datapoints
2025-03-06 19:54:57,981 - INFO - training batch 401, loss: 0.118, 12832/60000 datapoints
2025-03-06 19:54:58,215 - INFO - training batch 451, loss: 0.381, 14432/60000 datapoints
2025-03-06 19:54:58,436 - INFO - training batch 501, loss: 0.154, 16032/60000 datapoints
2025-03-06 19:54:58,690 - INFO - training batch 551, loss: 0.216, 17632/60000 datapoints
2025-03-06 19:54:58,980 - INFO - training batch 601, loss: 0.109, 19232/60000 datapoints
2025-03-06 19:54:59,201 - INFO - training batch 651, loss: 0.312, 20832/60000 datapoints
2025-03-06 19:54:59,404 - INFO - training batch 701, loss: 0.230, 22432/60000 datapoints
2025-03-06 19:54:59,636 - INFO - training batch 751, loss: 0.156, 24032/60000 datapoints
2025-03-06 19:54:59,879 - INFO - training batch 801, loss: 0.269, 25632/60000 datapoints
2025-03-06 19:55:00,101 - INFO - training batch 851, loss: 0.281, 27232/60000 datapoints
2025-03-06 19:55:00,390 - INFO - training batch 901, loss: 0.900, 28832/60000 datapoints
2025-03-06 19:55:00,621 - INFO - training batch 951, loss: 0.375, 30432/60000 datapoints
2025-03-06 19:55:00,828 - INFO - training batch 1001, loss: 0.218, 32032/60000 datapoints
2025-03-06 19:55:01,038 - INFO - training batch 1051, loss: 0.059, 33632/60000 datapoints
2025-03-06 19:55:01,254 - INFO - training batch 1101, loss: 0.319, 35232/60000 datapoints
2025-03-06 19:55:01,470 - INFO - training batch 1151, loss: 0.294, 36832/60000 datapoints
2025-03-06 19:55:01,702 - INFO - training batch 1201, loss: 0.312, 38432/60000 datapoints
2025-03-06 19:55:01,927 - INFO - training batch 1251, loss: 0.239, 40032/60000 datapoints
2025-03-06 19:55:02,184 - INFO - training batch 1301, loss: 0.111, 41632/60000 datapoints
2025-03-06 19:55:02,443 - INFO - training batch 1351, loss: 0.363, 43232/60000 datapoints
2025-03-06 19:55:02,716 - INFO - training batch 1401, loss: 0.187, 44832/60000 datapoints
2025-03-06 19:55:02,955 - INFO - training batch 1451, loss: 0.374, 46432/60000 datapoints
2025-03-06 19:55:03,206 - INFO - training batch 1501, loss: 0.241, 48032/60000 datapoints
2025-03-06 19:55:03,443 - INFO - training batch 1551, loss: 0.199, 49632/60000 datapoints
2025-03-06 19:55:03,678 - INFO - training batch 1601, loss: 0.537, 51232/60000 datapoints
2025-03-06 19:55:03,911 - INFO - training batch 1651, loss: 0.132, 52832/60000 datapoints
2025-03-06 19:55:04,133 - INFO - training batch 1701, loss: 0.119, 54432/60000 datapoints
2025-03-06 19:55:04,354 - INFO - training batch 1751, loss: 0.388, 56032/60000 datapoints
2025-03-06 19:55:04,570 - INFO - training batch 1801, loss: 0.085, 57632/60000 datapoints
2025-03-06 19:55:04,793 - INFO - training batch 1851, loss: 0.303, 59232/60000 datapoints
2025-03-06 19:55:04,910 - INFO - validation batch 1, loss: 0.461, 32/10016 datapoints
2025-03-06 19:55:05,076 - INFO - validation batch 51, loss: 0.379, 1632/10016 datapoints
2025-03-06 19:55:05,241 - INFO - validation batch 101, loss: 0.329, 3232/10016 datapoints
2025-03-06 19:55:05,407 - INFO - validation batch 151, loss: 0.276, 4832/10016 datapoints
2025-03-06 19:55:05,572 - INFO - validation batch 201, loss: 0.126, 6432/10016 datapoints
2025-03-06 19:55:05,732 - INFO - validation batch 251, loss: 0.107, 8032/10016 datapoints
2025-03-06 19:55:05,897 - INFO - validation batch 301, loss: 0.067, 9632/10016 datapoints
2025-03-06 19:55:05,936 - INFO - Epoch 524/800 done.
2025-03-06 19:55:05,936 - INFO - Final validation performance:
Loss: 0.250, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:55:05,937 - INFO - Beginning epoch 525/800
2025-03-06 19:55:05,944 - INFO - training batch 1, loss: 0.380, 32/60000 datapoints
2025-03-06 19:55:06,146 - INFO - training batch 51, loss: 0.108, 1632/60000 datapoints
2025-03-06 19:55:06,344 - INFO - training batch 101, loss: 0.522, 3232/60000 datapoints
2025-03-06 19:55:06,559 - INFO - training batch 151, loss: 0.389, 4832/60000 datapoints
2025-03-06 19:55:06,759 - INFO - training batch 201, loss: 0.366, 6432/60000 datapoints
2025-03-06 19:55:06,964 - INFO - training batch 251, loss: 0.092, 8032/60000 datapoints
2025-03-06 19:55:07,299 - INFO - training batch 301, loss: 0.113, 9632/60000 datapoints
2025-03-06 19:55:07,505 - INFO - training batch 351, loss: 0.290, 11232/60000 datapoints
2025-03-06 19:55:07,708 - INFO - training batch 401, loss: 0.236, 12832/60000 datapoints
2025-03-06 19:55:07,922 - INFO - training batch 451, loss: 0.181, 14432/60000 datapoints
2025-03-06 19:55:08,137 - INFO - training batch 501, loss: 0.346, 16032/60000 datapoints
2025-03-06 19:55:08,356 - INFO - training batch 551, loss: 0.080, 17632/60000 datapoints
2025-03-06 19:55:08,588 - INFO - training batch 601, loss: 0.116, 19232/60000 datapoints
2025-03-06 19:55:08,819 - INFO - training batch 651, loss: 0.138, 20832/60000 datapoints
2025-03-06 19:55:09,065 - INFO - training batch 701, loss: 0.141, 22432/60000 datapoints
2025-03-06 19:55:09,265 - INFO - training batch 751, loss: 0.226, 24032/60000 datapoints
2025-03-06 19:55:09,464 - INFO - training batch 801, loss: 0.208, 25632/60000 datapoints
2025-03-06 19:55:09,673 - INFO - training batch 851, loss: 0.255, 27232/60000 datapoints
2025-03-06 19:55:09,869 - INFO - training batch 901, loss: 0.223, 28832/60000 datapoints
2025-03-06 19:55:10,075 - INFO - training batch 951, loss: 0.348, 30432/60000 datapoints
2025-03-06 19:55:10,284 - INFO - training batch 1001, loss: 0.200, 32032/60000 datapoints
2025-03-06 19:55:10,505 - INFO - training batch 1051, loss: 0.102, 33632/60000 datapoints
2025-03-06 19:55:10,729 - INFO - training batch 1101, loss: 0.141, 35232/60000 datapoints
2025-03-06 19:55:10,952 - INFO - training batch 1151, loss: 0.158, 36832/60000 datapoints
2025-03-06 19:55:11,153 - INFO - training batch 1201, loss: 0.198, 38432/60000 datapoints
2025-03-06 19:55:11,372 - INFO - training batch 1251, loss: 0.217, 40032/60000 datapoints
2025-03-06 19:55:11,592 - INFO - training batch 1301, loss: 0.160, 41632/60000 datapoints
2025-03-06 19:55:11,817 - INFO - training batch 1351, loss: 0.346, 43232/60000 datapoints
2025-03-06 19:55:12,034 - INFO - training batch 1401, loss: 0.119, 44832/60000 datapoints
2025-03-06 19:55:12,260 - INFO - training batch 1451, loss: 0.275, 46432/60000 datapoints
2025-03-06 19:55:12,481 - INFO - training batch 1501, loss: 0.222, 48032/60000 datapoints
2025-03-06 19:55:12,702 - INFO - training batch 1551, loss: 0.450, 49632/60000 datapoints
2025-03-06 19:55:12,917 - INFO - training batch 1601, loss: 0.362, 51232/60000 datapoints
2025-03-06 19:55:13,147 - INFO - training batch 1651, loss: 0.206, 52832/60000 datapoints
2025-03-06 19:55:13,360 - INFO - training batch 1701, loss: 0.466, 54432/60000 datapoints
2025-03-06 19:55:13,587 - INFO - training batch 1751, loss: 0.082, 56032/60000 datapoints
2025-03-06 19:55:13,821 - INFO - training batch 1801, loss: 0.359, 57632/60000 datapoints
2025-03-06 19:55:14,037 - INFO - training batch 1851, loss: 0.403, 59232/60000 datapoints
2025-03-06 19:55:14,155 - INFO - validation batch 1, loss: 0.497, 32/10016 datapoints
2025-03-06 19:55:14,333 - INFO - validation batch 51, loss: 0.177, 1632/10016 datapoints
2025-03-06 19:55:14,510 - INFO - validation batch 101, loss: 0.060, 3232/10016 datapoints
2025-03-06 19:55:14,699 - INFO - validation batch 151, loss: 0.319, 4832/10016 datapoints
2025-03-06 19:55:14,876 - INFO - validation batch 201, loss: 0.244, 6432/10016 datapoints
2025-03-06 19:55:15,078 - INFO - validation batch 251, loss: 0.254, 8032/10016 datapoints
2025-03-06 19:55:15,257 - INFO - validation batch 301, loss: 0.124, 9632/10016 datapoints
2025-03-06 19:55:15,302 - INFO - Epoch 525/800 done.
2025-03-06 19:55:15,302 - INFO - Final validation performance:
Loss: 0.239, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:55:15,303 - INFO - Beginning epoch 526/800
2025-03-06 19:55:15,310 - INFO - training batch 1, loss: 0.132, 32/60000 datapoints
2025-03-06 19:55:15,526 - INFO - training batch 51, loss: 0.487, 1632/60000 datapoints
2025-03-06 19:55:15,756 - INFO - training batch 101, loss: 0.139, 3232/60000 datapoints
2025-03-06 19:55:16,002 - INFO - training batch 151, loss: 0.474, 4832/60000 datapoints
2025-03-06 19:55:16,213 - INFO - training batch 201, loss: 0.195, 6432/60000 datapoints
2025-03-06 19:55:16,435 - INFO - training batch 251, loss: 0.237, 8032/60000 datapoints
2025-03-06 19:55:16,650 - INFO - training batch 301, loss: 0.181, 9632/60000 datapoints
2025-03-06 19:55:16,866 - INFO - training batch 351, loss: 0.645, 11232/60000 datapoints
2025-03-06 19:55:17,069 - INFO - training batch 401, loss: 0.245, 12832/60000 datapoints
2025-03-06 19:55:17,308 - INFO - training batch 451, loss: 0.210, 14432/60000 datapoints
2025-03-06 19:55:17,531 - INFO - training batch 501, loss: 0.323, 16032/60000 datapoints
2025-03-06 19:55:17,742 - INFO - training batch 551, loss: 0.318, 17632/60000 datapoints
2025-03-06 19:55:17,958 - INFO - training batch 601, loss: 0.380, 19232/60000 datapoints
2025-03-06 19:55:18,169 - INFO - training batch 651, loss: 0.102, 20832/60000 datapoints
2025-03-06 19:55:18,403 - INFO - training batch 701, loss: 0.161, 22432/60000 datapoints
2025-03-06 19:55:18,631 - INFO - training batch 751, loss: 0.323, 24032/60000 datapoints
2025-03-06 19:55:18,869 - INFO - training batch 801, loss: 0.150, 25632/60000 datapoints
2025-03-06 19:55:19,146 - INFO - training batch 851, loss: 0.185, 27232/60000 datapoints
2025-03-06 19:55:19,348 - INFO - training batch 901, loss: 0.276, 28832/60000 datapoints
2025-03-06 19:55:19,554 - INFO - training batch 951, loss: 0.271, 30432/60000 datapoints
2025-03-06 19:55:19,763 - INFO - training batch 1001, loss: 0.073, 32032/60000 datapoints
2025-03-06 19:55:19,966 - INFO - training batch 1051, loss: 0.315, 33632/60000 datapoints
2025-03-06 19:55:20,166 - INFO - training batch 1101, loss: 0.270, 35232/60000 datapoints
2025-03-06 19:55:20,364 - INFO - training batch 1151, loss: 0.197, 36832/60000 datapoints
2025-03-06 19:55:20,570 - INFO - training batch 1201, loss: 0.332, 38432/60000 datapoints
2025-03-06 19:55:20,771 - INFO - training batch 1251, loss: 0.265, 40032/60000 datapoints
2025-03-06 19:55:20,978 - INFO - training batch 1301, loss: 0.095, 41632/60000 datapoints
2025-03-06 19:55:21,339 - INFO - training batch 1351, loss: 0.311, 43232/60000 datapoints
2025-03-06 19:55:21,585 - INFO - training batch 1401, loss: 0.174, 44832/60000 datapoints
2025-03-06 19:55:21,795 - INFO - training batch 1451, loss: 0.218, 46432/60000 datapoints
2025-03-06 19:55:22,016 - INFO - training batch 1501, loss: 0.287, 48032/60000 datapoints
2025-03-06 19:55:22,224 - INFO - training batch 1551, loss: 0.149, 49632/60000 datapoints
2025-03-06 19:55:22,429 - INFO - training batch 1601, loss: 0.237, 51232/60000 datapoints
2025-03-06 19:55:22,633 - INFO - training batch 1651, loss: 0.205, 52832/60000 datapoints
2025-03-06 19:55:22,833 - INFO - training batch 1701, loss: 0.190, 54432/60000 datapoints
2025-03-06 19:55:23,029 - INFO - training batch 1751, loss: 0.047, 56032/60000 datapoints
2025-03-06 19:55:23,251 - INFO - training batch 1801, loss: 0.383, 57632/60000 datapoints
2025-03-06 19:55:23,459 - INFO - training batch 1851, loss: 0.210, 59232/60000 datapoints
2025-03-06 19:55:23,576 - INFO - validation batch 1, loss: 0.453, 32/10016 datapoints
2025-03-06 19:55:23,757 - INFO - validation batch 51, loss: 0.193, 1632/10016 datapoints
2025-03-06 19:55:23,922 - INFO - validation batch 101, loss: 0.418, 3232/10016 datapoints
2025-03-06 19:55:24,082 - INFO - validation batch 151, loss: 0.429, 4832/10016 datapoints
2025-03-06 19:55:24,247 - INFO - validation batch 201, loss: 0.198, 6432/10016 datapoints
2025-03-06 19:55:24,412 - INFO - validation batch 251, loss: 0.148, 8032/10016 datapoints
2025-03-06 19:55:24,579 - INFO - validation batch 301, loss: 0.230, 9632/10016 datapoints
2025-03-06 19:55:24,617 - INFO - Epoch 526/800 done.
2025-03-06 19:55:24,617 - INFO - Final validation performance:
Loss: 0.296, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:55:24,618 - INFO - Beginning epoch 527/800
2025-03-06 19:55:24,627 - INFO - training batch 1, loss: 0.522, 32/60000 datapoints
2025-03-06 19:55:24,850 - INFO - training batch 51, loss: 0.140, 1632/60000 datapoints
2025-03-06 19:55:25,058 - INFO - training batch 101, loss: 0.451, 3232/60000 datapoints
2025-03-06 19:55:25,292 - INFO - training batch 151, loss: 0.243, 4832/60000 datapoints
2025-03-06 19:55:25,500 - INFO - training batch 201, loss: 0.199, 6432/60000 datapoints
2025-03-06 19:55:25,734 - INFO - training batch 251, loss: 0.464, 8032/60000 datapoints
2025-03-06 19:55:25,941 - INFO - training batch 301, loss: 0.340, 9632/60000 datapoints
2025-03-06 19:55:26,161 - INFO - training batch 351, loss: 0.109, 11232/60000 datapoints
2025-03-06 19:55:26,365 - INFO - training batch 401, loss: 0.176, 12832/60000 datapoints
2025-03-06 19:55:26,581 - INFO - training batch 451, loss: 0.244, 14432/60000 datapoints
2025-03-06 19:55:26,795 - INFO - training batch 501, loss: 0.364, 16032/60000 datapoints
2025-03-06 19:55:27,029 - INFO - training batch 551, loss: 0.072, 17632/60000 datapoints
2025-03-06 19:55:27,240 - INFO - training batch 601, loss: 0.492, 19232/60000 datapoints
2025-03-06 19:55:27,478 - INFO - training batch 651, loss: 0.259, 20832/60000 datapoints
2025-03-06 19:55:27,683 - INFO - training batch 701, loss: 0.314, 22432/60000 datapoints
2025-03-06 19:55:27,901 - INFO - training batch 751, loss: 0.427, 24032/60000 datapoints
2025-03-06 19:55:28,106 - INFO - training batch 801, loss: 0.316, 25632/60000 datapoints
2025-03-06 19:55:28,306 - INFO - training batch 851, loss: 0.134, 27232/60000 datapoints
2025-03-06 19:55:28,504 - INFO - training batch 901, loss: 0.134, 28832/60000 datapoints
2025-03-06 19:55:28,720 - INFO - training batch 951, loss: 0.273, 30432/60000 datapoints
2025-03-06 19:55:28,930 - INFO - training batch 1001, loss: 0.269, 32032/60000 datapoints
2025-03-06 19:55:29,147 - INFO - training batch 1051, loss: 0.119, 33632/60000 datapoints
2025-03-06 19:55:29,358 - INFO - training batch 1101, loss: 0.284, 35232/60000 datapoints
2025-03-06 19:55:29,563 - INFO - training batch 1151, loss: 0.450, 36832/60000 datapoints
2025-03-06 19:55:29,774 - INFO - training batch 1201, loss: 0.382, 38432/60000 datapoints
2025-03-06 19:55:30,006 - INFO - training batch 1251, loss: 0.252, 40032/60000 datapoints
2025-03-06 19:55:30,245 - INFO - training batch 1301, loss: 0.155, 41632/60000 datapoints
2025-03-06 19:55:30,459 - INFO - training batch 1351, loss: 0.512, 43232/60000 datapoints
2025-03-06 19:55:30,732 - INFO - training batch 1401, loss: 0.351, 44832/60000 datapoints
2025-03-06 19:55:30,933 - INFO - training batch 1451, loss: 0.159, 46432/60000 datapoints
2025-03-06 19:55:31,133 - INFO - training batch 1501, loss: 0.185, 48032/60000 datapoints
2025-03-06 19:55:31,332 - INFO - training batch 1551, loss: 0.236, 49632/60000 datapoints
2025-03-06 19:55:31,531 - INFO - training batch 1601, loss: 0.548, 51232/60000 datapoints
2025-03-06 19:55:31,755 - INFO - training batch 1651, loss: 0.229, 52832/60000 datapoints
2025-03-06 19:55:31,953 - INFO - training batch 1701, loss: 0.222, 54432/60000 datapoints
2025-03-06 19:55:32,155 - INFO - training batch 1751, loss: 0.150, 56032/60000 datapoints
2025-03-06 19:55:32,349 - INFO - training batch 1801, loss: 0.143, 57632/60000 datapoints
2025-03-06 19:55:32,546 - INFO - training batch 1851, loss: 0.151, 59232/60000 datapoints
2025-03-06 19:55:32,652 - INFO - validation batch 1, loss: 0.301, 32/10016 datapoints
2025-03-06 19:55:32,808 - INFO - validation batch 51, loss: 0.229, 1632/10016 datapoints
2025-03-06 19:55:32,963 - INFO - validation batch 101, loss: 0.656, 3232/10016 datapoints
2025-03-06 19:55:33,117 - INFO - validation batch 151, loss: 0.162, 4832/10016 datapoints
2025-03-06 19:55:33,273 - INFO - validation batch 201, loss: 0.088, 6432/10016 datapoints
2025-03-06 19:55:33,460 - INFO - validation batch 251, loss: 0.382, 8032/10016 datapoints
2025-03-06 19:55:33,639 - INFO - validation batch 301, loss: 0.459, 9632/10016 datapoints
2025-03-06 19:55:33,678 - INFO - Epoch 527/800 done.
2025-03-06 19:55:33,679 - INFO - Final validation performance:
Loss: 0.325, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:55:33,679 - INFO - Beginning epoch 528/800
2025-03-06 19:55:33,689 - INFO - training batch 1, loss: 0.202, 32/60000 datapoints
2025-03-06 19:55:33,900 - INFO - training batch 51, loss: 0.211, 1632/60000 datapoints
2025-03-06 19:55:34,152 - INFO - training batch 101, loss: 0.220, 3232/60000 datapoints
2025-03-06 19:55:34,365 - INFO - training batch 151, loss: 0.182, 4832/60000 datapoints
2025-03-06 19:55:34,592 - INFO - training batch 201, loss: 0.321, 6432/60000 datapoints
2025-03-06 19:55:34,829 - INFO - training batch 251, loss: 0.246, 8032/60000 datapoints
2025-03-06 19:55:35,045 - INFO - training batch 301, loss: 0.327, 9632/60000 datapoints
2025-03-06 19:55:35,267 - INFO - training batch 351, loss: 0.144, 11232/60000 datapoints
2025-03-06 19:55:35,484 - INFO - training batch 401, loss: 0.284, 12832/60000 datapoints
2025-03-06 19:55:35,710 - INFO - training batch 451, loss: 0.428, 14432/60000 datapoints
2025-03-06 19:55:35,930 - INFO - training batch 501, loss: 0.182, 16032/60000 datapoints
2025-03-06 19:55:36,156 - INFO - training batch 551, loss: 0.444, 17632/60000 datapoints
2025-03-06 19:55:36,369 - INFO - training batch 601, loss: 0.145, 19232/60000 datapoints
2025-03-06 19:55:36,582 - INFO - training batch 651, loss: 0.270, 20832/60000 datapoints
2025-03-06 19:55:36,791 - INFO - training batch 701, loss: 0.148, 22432/60000 datapoints
2025-03-06 19:55:37,144 - INFO - training batch 751, loss: 0.352, 24032/60000 datapoints
2025-03-06 19:55:37,363 - INFO - training batch 801, loss: 0.141, 25632/60000 datapoints
2025-03-06 19:55:37,600 - INFO - training batch 851, loss: 0.094, 27232/60000 datapoints
2025-03-06 19:55:37,838 - INFO - training batch 901, loss: 0.148, 28832/60000 datapoints
2025-03-06 19:55:38,078 - INFO - training batch 951, loss: 0.540, 30432/60000 datapoints
2025-03-06 19:55:38,296 - INFO - training batch 1001, loss: 0.377, 32032/60000 datapoints
2025-03-06 19:55:38,516 - INFO - training batch 1051, loss: 0.155, 33632/60000 datapoints
2025-03-06 19:55:38,739 - INFO - training batch 1101, loss: 0.194, 35232/60000 datapoints
2025-03-06 19:55:38,969 - INFO - training batch 1151, loss: 0.188, 36832/60000 datapoints
2025-03-06 19:55:39,229 - INFO - training batch 1201, loss: 0.169, 38432/60000 datapoints
2025-03-06 19:55:39,456 - INFO - training batch 1251, loss: 0.179, 40032/60000 datapoints
2025-03-06 19:55:39,678 - INFO - training batch 1301, loss: 0.195, 41632/60000 datapoints
2025-03-06 19:55:39,904 - INFO - training batch 1351, loss: 0.217, 43232/60000 datapoints
2025-03-06 19:55:40,126 - INFO - training batch 1401, loss: 0.245, 44832/60000 datapoints
2025-03-06 19:55:40,349 - INFO - training batch 1451, loss: 0.199, 46432/60000 datapoints
2025-03-06 19:55:40,567 - INFO - training batch 1501, loss: 0.147, 48032/60000 datapoints
2025-03-06 19:55:40,786 - INFO - training batch 1551, loss: 0.323, 49632/60000 datapoints
2025-03-06 19:55:41,008 - INFO - training batch 1601, loss: 0.207, 51232/60000 datapoints
2025-03-06 19:55:41,222 - INFO - training batch 1651, loss: 0.138, 52832/60000 datapoints
2025-03-06 19:55:41,430 - INFO - training batch 1701, loss: 0.207, 54432/60000 datapoints
2025-03-06 19:55:41,642 - INFO - training batch 1751, loss: 0.308, 56032/60000 datapoints
2025-03-06 19:55:41,858 - INFO - training batch 1801, loss: 0.550, 57632/60000 datapoints
2025-03-06 19:55:42,078 - INFO - training batch 1851, loss: 0.104, 59232/60000 datapoints
2025-03-06 19:55:42,190 - INFO - validation batch 1, loss: 0.277, 32/10016 datapoints
2025-03-06 19:55:42,357 - INFO - validation batch 51, loss: 0.099, 1632/10016 datapoints
2025-03-06 19:55:42,532 - INFO - validation batch 101, loss: 0.122, 3232/10016 datapoints
2025-03-06 19:55:42,708 - INFO - validation batch 151, loss: 0.324, 4832/10016 datapoints
2025-03-06 19:55:42,885 - INFO - validation batch 201, loss: 0.153, 6432/10016 datapoints
2025-03-06 19:55:43,063 - INFO - validation batch 251, loss: 0.225, 8032/10016 datapoints
2025-03-06 19:55:43,221 - INFO - validation batch 301, loss: 0.252, 9632/10016 datapoints
2025-03-06 19:55:43,267 - INFO - Epoch 528/800 done.
2025-03-06 19:55:43,267 - INFO - Final validation performance:
Loss: 0.207, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:55:43,268 - INFO - Beginning epoch 529/800
2025-03-06 19:55:43,275 - INFO - training batch 1, loss: 0.178, 32/60000 datapoints
2025-03-06 19:55:43,488 - INFO - training batch 51, loss: 0.349, 1632/60000 datapoints
2025-03-06 19:55:43,698 - INFO - training batch 101, loss: 0.328, 3232/60000 datapoints
2025-03-06 19:55:43,919 - INFO - training batch 151, loss: 0.177, 4832/60000 datapoints
2025-03-06 19:55:44,131 - INFO - training batch 201, loss: 0.272, 6432/60000 datapoints
2025-03-06 19:55:44,363 - INFO - training batch 251, loss: 0.083, 8032/60000 datapoints
2025-03-06 19:55:44,588 - INFO - training batch 301, loss: 0.137, 9632/60000 datapoints
2025-03-06 19:55:44,820 - INFO - training batch 351, loss: 0.252, 11232/60000 datapoints
2025-03-06 19:55:45,025 - INFO - training batch 401, loss: 0.340, 12832/60000 datapoints
2025-03-06 19:55:45,247 - INFO - training batch 451, loss: 0.368, 14432/60000 datapoints
2025-03-06 19:55:45,464 - INFO - training batch 501, loss: 0.216, 16032/60000 datapoints
2025-03-06 19:55:45,692 - INFO - training batch 551, loss: 0.158, 17632/60000 datapoints
2025-03-06 19:55:45,905 - INFO - training batch 601, loss: 0.227, 19232/60000 datapoints
2025-03-06 19:55:46,122 - INFO - training batch 651, loss: 0.276, 20832/60000 datapoints
2025-03-06 19:55:46,470 - INFO - training batch 701, loss: 0.144, 22432/60000 datapoints
2025-03-06 19:55:46,692 - INFO - training batch 751, loss: 0.181, 24032/60000 datapoints
2025-03-06 19:55:46,904 - INFO - training batch 801, loss: 0.226, 25632/60000 datapoints
2025-03-06 19:55:47,110 - INFO - training batch 851, loss: 0.502, 27232/60000 datapoints
2025-03-06 19:55:47,357 - INFO - training batch 901, loss: 0.142, 28832/60000 datapoints
2025-03-06 19:55:47,578 - INFO - training batch 951, loss: 0.351, 30432/60000 datapoints
2025-03-06 19:55:47,833 - INFO - training batch 1001, loss: 0.166, 32032/60000 datapoints
2025-03-06 19:55:48,104 - INFO - training batch 1051, loss: 0.212, 33632/60000 datapoints
2025-03-06 19:55:48,342 - INFO - training batch 1101, loss: 0.136, 35232/60000 datapoints
2025-03-06 19:55:48,561 - INFO - training batch 1151, loss: 0.360, 36832/60000 datapoints
2025-03-06 19:55:48,765 - INFO - training batch 1201, loss: 0.199, 38432/60000 datapoints
2025-03-06 19:55:48,967 - INFO - training batch 1251, loss: 0.353, 40032/60000 datapoints
2025-03-06 19:55:49,171 - INFO - training batch 1301, loss: 0.171, 41632/60000 datapoints
2025-03-06 19:55:49,403 - INFO - training batch 1351, loss: 0.200, 43232/60000 datapoints
2025-03-06 19:55:49,609 - INFO - training batch 1401, loss: 0.382, 44832/60000 datapoints
2025-03-06 19:55:49,812 - INFO - training batch 1451, loss: 0.258, 46432/60000 datapoints
2025-03-06 19:55:50,009 - INFO - training batch 1501, loss: 0.370, 48032/60000 datapoints
2025-03-06 19:55:50,202 - INFO - training batch 1551, loss: 0.386, 49632/60000 datapoints
2025-03-06 19:55:50,401 - INFO - training batch 1601, loss: 0.247, 51232/60000 datapoints
2025-03-06 19:55:50,625 - INFO - training batch 1651, loss: 0.202, 52832/60000 datapoints
2025-03-06 19:55:50,861 - INFO - training batch 1701, loss: 0.369, 54432/60000 datapoints
2025-03-06 19:55:51,058 - INFO - training batch 1751, loss: 0.269, 56032/60000 datapoints
2025-03-06 19:55:51,254 - INFO - training batch 1801, loss: 0.258, 57632/60000 datapoints
2025-03-06 19:55:51,452 - INFO - training batch 1851, loss: 0.131, 59232/60000 datapoints
2025-03-06 19:55:51,554 - INFO - validation batch 1, loss: 0.301, 32/10016 datapoints
2025-03-06 19:55:51,714 - INFO - validation batch 51, loss: 0.089, 1632/10016 datapoints
2025-03-06 19:55:51,874 - INFO - validation batch 101, loss: 0.379, 3232/10016 datapoints
2025-03-06 19:55:52,028 - INFO - validation batch 151, loss: 0.237, 4832/10016 datapoints
2025-03-06 19:55:52,180 - INFO - validation batch 201, loss: 0.156, 6432/10016 datapoints
2025-03-06 19:55:52,335 - INFO - validation batch 251, loss: 0.124, 8032/10016 datapoints
2025-03-06 19:55:52,497 - INFO - validation batch 301, loss: 0.190, 9632/10016 datapoints
2025-03-06 19:55:52,536 - INFO - Epoch 529/800 done.
2025-03-06 19:55:52,536 - INFO - Final validation performance:
Loss: 0.211, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:55:52,536 - INFO - Beginning epoch 530/800
2025-03-06 19:55:52,543 - INFO - training batch 1, loss: 0.189, 32/60000 datapoints
2025-03-06 19:55:52,743 - INFO - training batch 51, loss: 0.229, 1632/60000 datapoints
2025-03-06 19:55:52,954 - INFO - training batch 101, loss: 0.299, 3232/60000 datapoints
2025-03-06 19:55:53,178 - INFO - training batch 151, loss: 0.116, 4832/60000 datapoints
2025-03-06 19:55:53,376 - INFO - training batch 201, loss: 0.353, 6432/60000 datapoints
2025-03-06 19:55:53,575 - INFO - training batch 251, loss: 0.129, 8032/60000 datapoints
2025-03-06 19:55:53,777 - INFO - training batch 301, loss: 0.221, 9632/60000 datapoints
2025-03-06 19:55:53,974 - INFO - training batch 351, loss: 0.134, 11232/60000 datapoints
2025-03-06 19:55:54,166 - INFO - training batch 401, loss: 0.138, 12832/60000 datapoints
2025-03-06 19:55:54,361 - INFO - training batch 451, loss: 0.262, 14432/60000 datapoints
2025-03-06 19:55:54,558 - INFO - training batch 501, loss: 0.588, 16032/60000 datapoints
2025-03-06 19:55:54,753 - INFO - training batch 551, loss: 0.363, 17632/60000 datapoints
2025-03-06 19:55:54,952 - INFO - training batch 601, loss: 0.161, 19232/60000 datapoints
2025-03-06 19:55:55,149 - INFO - training batch 651, loss: 0.248, 20832/60000 datapoints
2025-03-06 19:55:55,342 - INFO - training batch 701, loss: 0.211, 22432/60000 datapoints
2025-03-06 19:55:55,540 - INFO - training batch 751, loss: 0.192, 24032/60000 datapoints
2025-03-06 19:55:55,740 - INFO - training batch 801, loss: 0.349, 25632/60000 datapoints
2025-03-06 19:55:55,938 - INFO - training batch 851, loss: 0.094, 27232/60000 datapoints
2025-03-06 19:55:56,132 - INFO - training batch 901, loss: 0.382, 28832/60000 datapoints
2025-03-06 19:55:56,325 - INFO - training batch 951, loss: 0.123, 30432/60000 datapoints
2025-03-06 19:55:56,518 - INFO - training batch 1001, loss: 0.135, 32032/60000 datapoints
2025-03-06 19:55:56,714 - INFO - training batch 1051, loss: 0.220, 33632/60000 datapoints
2025-03-06 19:55:56,908 - INFO - training batch 1101, loss: 0.465, 35232/60000 datapoints
2025-03-06 19:55:57,104 - INFO - training batch 1151, loss: 0.253, 36832/60000 datapoints
2025-03-06 19:55:57,295 - INFO - training batch 1201, loss: 0.278, 38432/60000 datapoints
2025-03-06 19:55:57,489 - INFO - training batch 1251, loss: 0.074, 40032/60000 datapoints
2025-03-06 19:55:57,687 - INFO - training batch 1301, loss: 0.067, 41632/60000 datapoints
2025-03-06 19:55:57,881 - INFO - training batch 1351, loss: 0.270, 43232/60000 datapoints
2025-03-06 19:55:58,081 - INFO - training batch 1401, loss: 0.546, 44832/60000 datapoints
2025-03-06 19:55:58,280 - INFO - training batch 1451, loss: 0.224, 46432/60000 datapoints
2025-03-06 19:55:58,475 - INFO - training batch 1501, loss: 0.307, 48032/60000 datapoints
2025-03-06 19:55:58,672 - INFO - training batch 1551, loss: 0.351, 49632/60000 datapoints
2025-03-06 19:55:58,869 - INFO - training batch 1601, loss: 0.280, 51232/60000 datapoints
2025-03-06 19:55:59,068 - INFO - training batch 1651, loss: 0.156, 52832/60000 datapoints
2025-03-06 19:55:59,262 - INFO - training batch 1701, loss: 0.173, 54432/60000 datapoints
2025-03-06 19:55:59,474 - INFO - training batch 1751, loss: 0.778, 56032/60000 datapoints
2025-03-06 19:55:59,673 - INFO - training batch 1801, loss: 0.429, 57632/60000 datapoints
2025-03-06 19:55:59,865 - INFO - training batch 1851, loss: 0.574, 59232/60000 datapoints
2025-03-06 19:55:59,966 - INFO - validation batch 1, loss: 0.164, 32/10016 datapoints
2025-03-06 19:56:00,119 - INFO - validation batch 51, loss: 0.188, 1632/10016 datapoints
2025-03-06 19:56:00,272 - INFO - validation batch 101, loss: 0.391, 3232/10016 datapoints
2025-03-06 19:56:00,426 - INFO - validation batch 151, loss: 0.177, 4832/10016 datapoints
2025-03-06 19:56:00,584 - INFO - validation batch 201, loss: 0.310, 6432/10016 datapoints
2025-03-06 19:56:00,738 - INFO - validation batch 251, loss: 0.245, 8032/10016 datapoints
2025-03-06 19:56:00,893 - INFO - validation batch 301, loss: 0.196, 9632/10016 datapoints
2025-03-06 19:56:00,934 - INFO - Epoch 530/800 done.
2025-03-06 19:56:00,934 - INFO - Final validation performance:
Loss: 0.239, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:56:00,935 - INFO - Beginning epoch 531/800
2025-03-06 19:56:00,941 - INFO - training batch 1, loss: 0.204, 32/60000 datapoints
2025-03-06 19:56:01,136 - INFO - training batch 51, loss: 0.511, 1632/60000 datapoints
2025-03-06 19:56:01,332 - INFO - training batch 101, loss: 0.356, 3232/60000 datapoints
2025-03-06 19:56:01,541 - INFO - training batch 151, loss: 0.438, 4832/60000 datapoints
2025-03-06 19:56:01,806 - INFO - training batch 201, loss: 0.095, 6432/60000 datapoints
2025-03-06 19:56:02,032 - INFO - training batch 251, loss: 0.674, 8032/60000 datapoints
2025-03-06 19:56:02,305 - INFO - training batch 301, loss: 0.168, 9632/60000 datapoints
2025-03-06 19:56:02,521 - INFO - training batch 351, loss: 0.124, 11232/60000 datapoints
2025-03-06 19:56:02,765 - INFO - training batch 401, loss: 0.171, 12832/60000 datapoints
2025-03-06 19:56:03,011 - INFO - training batch 451, loss: 0.165, 14432/60000 datapoints
2025-03-06 19:56:03,246 - INFO - training batch 501, loss: 0.232, 16032/60000 datapoints
2025-03-06 19:56:03,465 - INFO - training batch 551, loss: 0.256, 17632/60000 datapoints
2025-03-06 19:56:03,693 - INFO - training batch 601, loss: 0.313, 19232/60000 datapoints
2025-03-06 19:56:03,910 - INFO - training batch 651, loss: 0.232, 20832/60000 datapoints
2025-03-06 19:56:04,130 - INFO - training batch 701, loss: 0.344, 22432/60000 datapoints
2025-03-06 19:56:04,346 - INFO - training batch 751, loss: 0.351, 24032/60000 datapoints
2025-03-06 19:56:04,566 - INFO - training batch 801, loss: 0.156, 25632/60000 datapoints
2025-03-06 19:56:04,795 - INFO - training batch 851, loss: 0.208, 27232/60000 datapoints
2025-03-06 19:56:05,016 - INFO - training batch 901, loss: 0.237, 28832/60000 datapoints
2025-03-06 19:56:05,223 - INFO - training batch 951, loss: 0.247, 30432/60000 datapoints
2025-03-06 19:56:05,443 - INFO - training batch 1001, loss: 0.292, 32032/60000 datapoints
2025-03-06 19:56:05,667 - INFO - training batch 1051, loss: 0.302, 33632/60000 datapoints
2025-03-06 19:56:05,910 - INFO - training batch 1101, loss: 0.091, 35232/60000 datapoints
2025-03-06 19:56:06,153 - INFO - training batch 1151, loss: 0.152, 36832/60000 datapoints
2025-03-06 19:56:06,382 - INFO - training batch 1201, loss: 0.450, 38432/60000 datapoints
2025-03-06 19:56:06,595 - INFO - training batch 1251, loss: 0.212, 40032/60000 datapoints
2025-03-06 19:56:06,805 - INFO - training batch 1301, loss: 0.123, 41632/60000 datapoints
2025-03-06 19:56:07,014 - INFO - training batch 1351, loss: 0.279, 43232/60000 datapoints
2025-03-06 19:56:07,272 - INFO - training batch 1401, loss: 0.207, 44832/60000 datapoints
2025-03-06 19:56:07,517 - INFO - training batch 1451, loss: 0.282, 46432/60000 datapoints
2025-03-06 19:56:07,778 - INFO - training batch 1501, loss: 0.298, 48032/60000 datapoints
2025-03-06 19:56:08,027 - INFO - training batch 1551, loss: 0.632, 49632/60000 datapoints
2025-03-06 19:56:08,242 - INFO - training batch 1601, loss: 0.296, 51232/60000 datapoints
2025-03-06 19:56:08,583 - INFO - training batch 1651, loss: 0.229, 52832/60000 datapoints
2025-03-06 19:56:08,800 - INFO - training batch 1701, loss: 0.286, 54432/60000 datapoints
2025-03-06 19:56:09,054 - INFO - training batch 1751, loss: 0.284, 56032/60000 datapoints
2025-03-06 19:56:09,263 - INFO - training batch 1801, loss: 0.244, 57632/60000 datapoints
2025-03-06 19:56:09,498 - INFO - training batch 1851, loss: 0.226, 59232/60000 datapoints
2025-03-06 19:56:09,609 - INFO - validation batch 1, loss: 0.268, 32/10016 datapoints
2025-03-06 19:56:09,783 - INFO - validation batch 51, loss: 0.367, 1632/10016 datapoints
2025-03-06 19:56:09,967 - INFO - validation batch 101, loss: 0.364, 3232/10016 datapoints
2025-03-06 19:56:10,139 - INFO - validation batch 151, loss: 0.070, 4832/10016 datapoints
2025-03-06 19:56:10,312 - INFO - validation batch 201, loss: 0.074, 6432/10016 datapoints
2025-03-06 19:56:10,474 - INFO - validation batch 251, loss: 0.331, 8032/10016 datapoints
2025-03-06 19:56:10,654 - INFO - validation batch 301, loss: 0.312, 9632/10016 datapoints
2025-03-06 19:56:10,695 - INFO - Epoch 531/800 done.
2025-03-06 19:56:10,696 - INFO - Final validation performance:
Loss: 0.255, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:56:10,696 - INFO - Beginning epoch 532/800
2025-03-06 19:56:10,704 - INFO - training batch 1, loss: 0.380, 32/60000 datapoints
2025-03-06 19:56:10,933 - INFO - training batch 51, loss: 0.291, 1632/60000 datapoints
2025-03-06 19:56:11,148 - INFO - training batch 101, loss: 0.370, 3232/60000 datapoints
2025-03-06 19:56:11,369 - INFO - training batch 151, loss: 0.216, 4832/60000 datapoints
2025-03-06 19:56:11,588 - INFO - training batch 201, loss: 0.263, 6432/60000 datapoints
2025-03-06 19:56:11,808 - INFO - training batch 251, loss: 0.346, 8032/60000 datapoints
2025-03-06 19:56:12,020 - INFO - training batch 301, loss: 0.280, 9632/60000 datapoints
2025-03-06 19:56:12,238 - INFO - training batch 351, loss: 0.200, 11232/60000 datapoints
2025-03-06 19:56:12,448 - INFO - training batch 401, loss: 0.103, 12832/60000 datapoints
2025-03-06 19:56:12,661 - INFO - training batch 451, loss: 0.185, 14432/60000 datapoints
2025-03-06 19:56:12,890 - INFO - training batch 501, loss: 0.093, 16032/60000 datapoints
2025-03-06 19:56:13,131 - INFO - training batch 551, loss: 0.164, 17632/60000 datapoints
2025-03-06 19:56:13,356 - INFO - training batch 601, loss: 0.172, 19232/60000 datapoints
2025-03-06 19:56:13,610 - INFO - training batch 651, loss: 0.258, 20832/60000 datapoints
2025-03-06 19:56:13,830 - INFO - training batch 701, loss: 0.088, 22432/60000 datapoints
2025-03-06 19:56:14,047 - INFO - training batch 751, loss: 0.075, 24032/60000 datapoints
2025-03-06 19:56:14,276 - INFO - training batch 801, loss: 0.243, 25632/60000 datapoints
2025-03-06 19:56:14,511 - INFO - training batch 851, loss: 0.070, 27232/60000 datapoints
2025-03-06 19:56:14,725 - INFO - training batch 901, loss: 0.200, 28832/60000 datapoints
2025-03-06 19:56:14,934 - INFO - training batch 951, loss: 0.086, 30432/60000 datapoints
2025-03-06 19:56:15,180 - INFO - training batch 1001, loss: 0.128, 32032/60000 datapoints
2025-03-06 19:56:15,407 - INFO - training batch 1051, loss: 0.451, 33632/60000 datapoints
2025-03-06 19:56:15,627 - INFO - training batch 1101, loss: 0.161, 35232/60000 datapoints
2025-03-06 19:56:15,847 - INFO - training batch 1151, loss: 0.296, 36832/60000 datapoints
2025-03-06 19:56:16,064 - INFO - training batch 1201, loss: 0.424, 38432/60000 datapoints
2025-03-06 19:56:16,304 - INFO - training batch 1251, loss: 0.137, 40032/60000 datapoints
2025-03-06 19:56:16,532 - INFO - training batch 1301, loss: 0.337, 41632/60000 datapoints
2025-03-06 19:56:16,752 - INFO - training batch 1351, loss: 0.268, 43232/60000 datapoints
2025-03-06 19:56:17,007 - INFO - training batch 1401, loss: 0.091, 44832/60000 datapoints
2025-03-06 19:56:17,241 - INFO - training batch 1451, loss: 0.225, 46432/60000 datapoints
2025-03-06 19:56:17,496 - INFO - training batch 1501, loss: 0.441, 48032/60000 datapoints
2025-03-06 19:56:17,715 - INFO - training batch 1551, loss: 0.236, 49632/60000 datapoints
2025-03-06 19:56:17,966 - INFO - training batch 1601, loss: 0.143, 51232/60000 datapoints
2025-03-06 19:56:18,187 - INFO - training batch 1651, loss: 0.179, 52832/60000 datapoints
2025-03-06 19:56:18,398 - INFO - training batch 1701, loss: 0.084, 54432/60000 datapoints
2025-03-06 19:56:18,701 - INFO - training batch 1751, loss: 0.089, 56032/60000 datapoints
2025-03-06 19:56:18,982 - INFO - training batch 1801, loss: 0.427, 57632/60000 datapoints
2025-03-06 19:56:19,193 - INFO - training batch 1851, loss: 0.343, 59232/60000 datapoints
2025-03-06 19:56:19,308 - INFO - validation batch 1, loss: 0.216, 32/10016 datapoints
2025-03-06 19:56:19,591 - INFO - validation batch 51, loss: 0.275, 1632/10016 datapoints
2025-03-06 19:56:19,778 - INFO - validation batch 101, loss: 0.581, 3232/10016 datapoints
2025-03-06 19:56:19,957 - INFO - validation batch 151, loss: 0.328, 4832/10016 datapoints
2025-03-06 19:56:20,127 - INFO - validation batch 201, loss: 0.195, 6432/10016 datapoints
2025-03-06 19:56:20,320 - INFO - validation batch 251, loss: 0.248, 8032/10016 datapoints
2025-03-06 19:56:20,501 - INFO - validation batch 301, loss: 0.147, 9632/10016 datapoints
2025-03-06 19:56:20,543 - INFO - Epoch 532/800 done.
2025-03-06 19:56:20,543 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:56:20,544 - INFO - Beginning epoch 533/800
2025-03-06 19:56:20,551 - INFO - training batch 1, loss: 0.130, 32/60000 datapoints
2025-03-06 19:56:20,781 - INFO - training batch 51, loss: 0.582, 1632/60000 datapoints
2025-03-06 19:56:20,983 - INFO - training batch 101, loss: 0.155, 3232/60000 datapoints
2025-03-06 19:56:21,225 - INFO - training batch 151, loss: 0.303, 4832/60000 datapoints
2025-03-06 19:56:21,443 - INFO - training batch 201, loss: 0.132, 6432/60000 datapoints
2025-03-06 19:56:21,671 - INFO - training batch 251, loss: 0.381, 8032/60000 datapoints
2025-03-06 19:56:21,888 - INFO - training batch 301, loss: 0.402, 9632/60000 datapoints
2025-03-06 19:56:22,114 - INFO - training batch 351, loss: 0.182, 11232/60000 datapoints
2025-03-06 19:56:22,333 - INFO - training batch 401, loss: 0.416, 12832/60000 datapoints
2025-03-06 19:56:22,565 - INFO - training batch 451, loss: 0.151, 14432/60000 datapoints
2025-03-06 19:56:22,770 - INFO - training batch 501, loss: 0.182, 16032/60000 datapoints
2025-03-06 19:56:22,968 - INFO - training batch 551, loss: 0.213, 17632/60000 datapoints
2025-03-06 19:56:23,177 - INFO - training batch 601, loss: 0.177, 19232/60000 datapoints
2025-03-06 19:56:23,383 - INFO - training batch 651, loss: 0.228, 20832/60000 datapoints
2025-03-06 19:56:23,591 - INFO - training batch 701, loss: 0.292, 22432/60000 datapoints
2025-03-06 19:56:23,792 - INFO - training batch 751, loss: 0.060, 24032/60000 datapoints
2025-03-06 19:56:23,994 - INFO - training batch 801, loss: 0.229, 25632/60000 datapoints
2025-03-06 19:56:24,213 - INFO - training batch 851, loss: 0.330, 27232/60000 datapoints
2025-03-06 19:56:24,411 - INFO - training batch 901, loss: 0.279, 28832/60000 datapoints
2025-03-06 19:56:24,607 - INFO - training batch 951, loss: 0.288, 30432/60000 datapoints
2025-03-06 19:56:24,805 - INFO - training batch 1001, loss: 0.164, 32032/60000 datapoints
2025-03-06 19:56:25,004 - INFO - training batch 1051, loss: 0.203, 33632/60000 datapoints
2025-03-06 19:56:25,204 - INFO - training batch 1101, loss: 0.125, 35232/60000 datapoints
2025-03-06 19:56:25,399 - INFO - training batch 1151, loss: 0.061, 36832/60000 datapoints
2025-03-06 19:56:25,594 - INFO - training batch 1201, loss: 0.146, 38432/60000 datapoints
2025-03-06 19:56:25,794 - INFO - training batch 1251, loss: 0.403, 40032/60000 datapoints
2025-03-06 19:56:25,989 - INFO - training batch 1301, loss: 0.408, 41632/60000 datapoints
2025-03-06 19:56:26,185 - INFO - training batch 1351, loss: 0.204, 43232/60000 datapoints
2025-03-06 19:56:26,378 - INFO - training batch 1401, loss: 0.141, 44832/60000 datapoints
2025-03-06 19:56:26,571 - INFO - training batch 1451, loss: 0.470, 46432/60000 datapoints
2025-03-06 19:56:26,770 - INFO - training batch 1501, loss: 0.695, 48032/60000 datapoints
2025-03-06 19:56:26,963 - INFO - training batch 1551, loss: 0.314, 49632/60000 datapoints
2025-03-06 19:56:27,159 - INFO - training batch 1601, loss: 0.080, 51232/60000 datapoints
2025-03-06 19:56:27,353 - INFO - training batch 1651, loss: 0.083, 52832/60000 datapoints
2025-03-06 19:56:27,547 - INFO - training batch 1701, loss: 0.105, 54432/60000 datapoints
2025-03-06 19:56:27,748 - INFO - training batch 1751, loss: 0.223, 56032/60000 datapoints
2025-03-06 19:56:27,940 - INFO - training batch 1801, loss: 0.233, 57632/60000 datapoints
2025-03-06 19:56:28,136 - INFO - training batch 1851, loss: 0.038, 59232/60000 datapoints
2025-03-06 19:56:28,239 - INFO - validation batch 1, loss: 0.114, 32/10016 datapoints
2025-03-06 19:56:28,392 - INFO - validation batch 51, loss: 0.135, 1632/10016 datapoints
2025-03-06 19:56:28,548 - INFO - validation batch 101, loss: 0.239, 3232/10016 datapoints
2025-03-06 19:56:28,703 - INFO - validation batch 151, loss: 0.240, 4832/10016 datapoints
2025-03-06 19:56:28,858 - INFO - validation batch 201, loss: 0.131, 6432/10016 datapoints
2025-03-06 19:56:29,009 - INFO - validation batch 251, loss: 0.123, 8032/10016 datapoints
2025-03-06 19:56:29,161 - INFO - validation batch 301, loss: 0.257, 9632/10016 datapoints
2025-03-06 19:56:29,198 - INFO - Epoch 533/800 done.
2025-03-06 19:56:29,198 - INFO - Final validation performance:
Loss: 0.177, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:56:29,199 - INFO - Beginning epoch 534/800
2025-03-06 19:56:29,205 - INFO - training batch 1, loss: 0.308, 32/60000 datapoints
2025-03-06 19:56:29,416 - INFO - training batch 51, loss: 0.087, 1632/60000 datapoints
2025-03-06 19:56:29,611 - INFO - training batch 101, loss: 0.093, 3232/60000 datapoints
2025-03-06 19:56:29,835 - INFO - training batch 151, loss: 0.377, 4832/60000 datapoints
2025-03-06 19:56:30,034 - INFO - training batch 201, loss: 0.080, 6432/60000 datapoints
2025-03-06 19:56:30,232 - INFO - training batch 251, loss: 0.108, 8032/60000 datapoints
2025-03-06 19:56:30,426 - INFO - training batch 301, loss: 0.411, 9632/60000 datapoints
2025-03-06 19:56:30,622 - INFO - training batch 351, loss: 0.248, 11232/60000 datapoints
2025-03-06 19:56:30,816 - INFO - training batch 401, loss: 0.240, 12832/60000 datapoints
2025-03-06 19:56:31,011 - INFO - training batch 451, loss: 0.412, 14432/60000 datapoints
2025-03-06 19:56:31,206 - INFO - training batch 501, loss: 0.260, 16032/60000 datapoints
2025-03-06 19:56:31,401 - INFO - training batch 551, loss: 0.371, 17632/60000 datapoints
2025-03-06 19:56:31,599 - INFO - training batch 601, loss: 0.264, 19232/60000 datapoints
2025-03-06 19:56:31,800 - INFO - training batch 651, loss: 0.394, 20832/60000 datapoints
2025-03-06 19:56:31,997 - INFO - training batch 701, loss: 0.522, 22432/60000 datapoints
2025-03-06 19:56:32,194 - INFO - training batch 751, loss: 0.264, 24032/60000 datapoints
2025-03-06 19:56:32,395 - INFO - training batch 801, loss: 0.160, 25632/60000 datapoints
2025-03-06 19:56:32,593 - INFO - training batch 851, loss: 0.092, 27232/60000 datapoints
2025-03-06 19:56:32,799 - INFO - training batch 901, loss: 0.131, 28832/60000 datapoints
2025-03-06 19:56:32,991 - INFO - training batch 951, loss: 0.176, 30432/60000 datapoints
2025-03-06 19:56:33,184 - INFO - training batch 1001, loss: 0.135, 32032/60000 datapoints
2025-03-06 19:56:33,378 - INFO - training batch 1051, loss: 0.093, 33632/60000 datapoints
2025-03-06 19:56:33,572 - INFO - training batch 1101, loss: 0.197, 35232/60000 datapoints
2025-03-06 19:56:33,771 - INFO - training batch 1151, loss: 0.250, 36832/60000 datapoints
2025-03-06 19:56:33,968 - INFO - training batch 1201, loss: 0.461, 38432/60000 datapoints
2025-03-06 19:56:34,162 - INFO - training batch 1251, loss: 0.147, 40032/60000 datapoints
2025-03-06 19:56:34,358 - INFO - training batch 1301, loss: 0.306, 41632/60000 datapoints
2025-03-06 19:56:34,553 - INFO - training batch 1351, loss: 0.046, 43232/60000 datapoints
2025-03-06 19:56:34,751 - INFO - training batch 1401, loss: 0.095, 44832/60000 datapoints
2025-03-06 19:56:34,950 - INFO - training batch 1451, loss: 0.274, 46432/60000 datapoints
2025-03-06 19:56:35,147 - INFO - training batch 1501, loss: 0.142, 48032/60000 datapoints
2025-03-06 19:56:35,342 - INFO - training batch 1551, loss: 0.146, 49632/60000 datapoints
2025-03-06 19:56:35,537 - INFO - training batch 1601, loss: 0.314, 51232/60000 datapoints
2025-03-06 19:56:35,734 - INFO - training batch 1651, loss: 0.385, 52832/60000 datapoints
2025-03-06 19:56:35,930 - INFO - training batch 1701, loss: 0.398, 54432/60000 datapoints
2025-03-06 19:56:36,124 - INFO - training batch 1751, loss: 0.219, 56032/60000 datapoints
2025-03-06 19:56:36,319 - INFO - training batch 1801, loss: 0.184, 57632/60000 datapoints
2025-03-06 19:56:36,514 - INFO - training batch 1851, loss: 0.221, 59232/60000 datapoints
2025-03-06 19:56:36,618 - INFO - validation batch 1, loss: 0.287, 32/10016 datapoints
2025-03-06 19:56:36,773 - INFO - validation batch 51, loss: 0.172, 1632/10016 datapoints
2025-03-06 19:56:36,925 - INFO - validation batch 101, loss: 0.100, 3232/10016 datapoints
2025-03-06 19:56:37,078 - INFO - validation batch 151, loss: 0.335, 4832/10016 datapoints
2025-03-06 19:56:37,233 - INFO - validation batch 201, loss: 0.351, 6432/10016 datapoints
2025-03-06 19:56:37,390 - INFO - validation batch 251, loss: 0.212, 8032/10016 datapoints
2025-03-06 19:56:37,545 - INFO - validation batch 301, loss: 0.157, 9632/10016 datapoints
2025-03-06 19:56:37,583 - INFO - Epoch 534/800 done.
2025-03-06 19:56:37,583 - INFO - Final validation performance:
Loss: 0.231, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:56:37,584 - INFO - Beginning epoch 535/800
2025-03-06 19:56:37,591 - INFO - training batch 1, loss: 0.363, 32/60000 datapoints
2025-03-06 19:56:37,814 - INFO - training batch 51, loss: 0.214, 1632/60000 datapoints
2025-03-06 19:56:38,012 - INFO - training batch 101, loss: 0.322, 3232/60000 datapoints
2025-03-06 19:56:38,220 - INFO - training batch 151, loss: 0.273, 4832/60000 datapoints
2025-03-06 19:56:38,418 - INFO - training batch 201, loss: 0.279, 6432/60000 datapoints
2025-03-06 19:56:38,616 - INFO - training batch 251, loss: 0.363, 8032/60000 datapoints
2025-03-06 19:56:38,816 - INFO - training batch 301, loss: 0.135, 9632/60000 datapoints
2025-03-06 19:56:39,012 - INFO - training batch 351, loss: 0.164, 11232/60000 datapoints
2025-03-06 19:56:39,207 - INFO - training batch 401, loss: 0.091, 12832/60000 datapoints
2025-03-06 19:56:39,404 - INFO - training batch 451, loss: 0.156, 14432/60000 datapoints
2025-03-06 19:56:39,599 - INFO - training batch 501, loss: 0.146, 16032/60000 datapoints
2025-03-06 19:56:39,817 - INFO - training batch 551, loss: 0.374, 17632/60000 datapoints
2025-03-06 19:56:40,014 - INFO - training batch 601, loss: 0.279, 19232/60000 datapoints
2025-03-06 19:56:40,209 - INFO - training batch 651, loss: 0.202, 20832/60000 datapoints
2025-03-06 19:56:40,404 - INFO - training batch 701, loss: 0.087, 22432/60000 datapoints
2025-03-06 19:56:40,597 - INFO - training batch 751, loss: 0.219, 24032/60000 datapoints
2025-03-06 19:56:40,793 - INFO - training batch 801, loss: 0.200, 25632/60000 datapoints
2025-03-06 19:56:40,987 - INFO - training batch 851, loss: 0.327, 27232/60000 datapoints
2025-03-06 19:56:41,182 - INFO - training batch 901, loss: 0.165, 28832/60000 datapoints
2025-03-06 19:56:41,377 - INFO - training batch 951, loss: 0.407, 30432/60000 datapoints
2025-03-06 19:56:41,570 - INFO - training batch 1001, loss: 0.333, 32032/60000 datapoints
2025-03-06 19:56:41,771 - INFO - training batch 1051, loss: 0.230, 33632/60000 datapoints
2025-03-06 19:56:41,964 - INFO - training batch 1101, loss: 0.203, 35232/60000 datapoints
2025-03-06 19:56:42,160 - INFO - training batch 1151, loss: 0.173, 36832/60000 datapoints
2025-03-06 19:56:42,356 - INFO - training batch 1201, loss: 0.395, 38432/60000 datapoints
2025-03-06 19:56:42,551 - INFO - training batch 1251, loss: 0.375, 40032/60000 datapoints
2025-03-06 19:56:42,747 - INFO - training batch 1301, loss: 0.191, 41632/60000 datapoints
2025-03-06 19:56:42,939 - INFO - training batch 1351, loss: 0.110, 43232/60000 datapoints
2025-03-06 19:56:43,130 - INFO - training batch 1401, loss: 0.227, 44832/60000 datapoints
2025-03-06 19:56:43,324 - INFO - training batch 1451, loss: 0.264, 46432/60000 datapoints
2025-03-06 19:56:43,517 - INFO - training batch 1501, loss: 0.100, 48032/60000 datapoints
2025-03-06 19:56:43,711 - INFO - training batch 1551, loss: 0.234, 49632/60000 datapoints
2025-03-06 19:56:43,912 - INFO - training batch 1601, loss: 0.433, 51232/60000 datapoints
2025-03-06 19:56:44,107 - INFO - training batch 1651, loss: 0.397, 52832/60000 datapoints
2025-03-06 19:56:44,308 - INFO - training batch 1701, loss: 0.219, 54432/60000 datapoints
2025-03-06 19:56:44,510 - INFO - training batch 1751, loss: 0.230, 56032/60000 datapoints
2025-03-06 19:56:44,709 - INFO - training batch 1801, loss: 0.217, 57632/60000 datapoints
2025-03-06 19:56:44,911 - INFO - training batch 1851, loss: 0.188, 59232/60000 datapoints
2025-03-06 19:56:45,013 - INFO - validation batch 1, loss: 0.048, 32/10016 datapoints
2025-03-06 19:56:45,166 - INFO - validation batch 51, loss: 0.553, 1632/10016 datapoints
2025-03-06 19:56:45,319 - INFO - validation batch 101, loss: 0.489, 3232/10016 datapoints
2025-03-06 19:56:45,471 - INFO - validation batch 151, loss: 0.288, 4832/10016 datapoints
2025-03-06 19:56:45,627 - INFO - validation batch 201, loss: 0.254, 6432/10016 datapoints
2025-03-06 19:56:45,783 - INFO - validation batch 251, loss: 0.171, 8032/10016 datapoints
2025-03-06 19:56:45,935 - INFO - validation batch 301, loss: 0.305, 9632/10016 datapoints
2025-03-06 19:56:45,973 - INFO - Epoch 535/800 done.
2025-03-06 19:56:45,973 - INFO - Final validation performance:
Loss: 0.301, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:56:45,974 - INFO - Beginning epoch 536/800
2025-03-06 19:56:45,981 - INFO - training batch 1, loss: 0.246, 32/60000 datapoints
2025-03-06 19:56:46,187 - INFO - training batch 51, loss: 0.490, 1632/60000 datapoints
2025-03-06 19:56:46,387 - INFO - training batch 101, loss: 0.207, 3232/60000 datapoints
2025-03-06 19:56:46,581 - INFO - training batch 151, loss: 0.240, 4832/60000 datapoints
2025-03-06 19:56:46,787 - INFO - training batch 201, loss: 0.122, 6432/60000 datapoints
2025-03-06 19:56:46,984 - INFO - training batch 251, loss: 0.233, 8032/60000 datapoints
2025-03-06 19:56:47,186 - INFO - training batch 301, loss: 0.258, 9632/60000 datapoints
2025-03-06 19:56:47,380 - INFO - training batch 351, loss: 0.230, 11232/60000 datapoints
2025-03-06 19:56:47,574 - INFO - training batch 401, loss: 0.232, 12832/60000 datapoints
2025-03-06 19:56:47,795 - INFO - training batch 451, loss: 0.201, 14432/60000 datapoints
2025-03-06 19:56:48,036 - INFO - training batch 501, loss: 0.338, 16032/60000 datapoints
2025-03-06 19:56:48,231 - INFO - training batch 551, loss: 0.363, 17632/60000 datapoints
2025-03-06 19:56:48,426 - INFO - training batch 601, loss: 0.063, 19232/60000 datapoints
2025-03-06 19:56:48,624 - INFO - training batch 651, loss: 0.282, 20832/60000 datapoints
2025-03-06 19:56:48,820 - INFO - training batch 701, loss: 0.215, 22432/60000 datapoints
2025-03-06 19:56:49,014 - INFO - training batch 751, loss: 0.446, 24032/60000 datapoints
2025-03-06 19:56:49,206 - INFO - training batch 801, loss: 0.420, 25632/60000 datapoints
2025-03-06 19:56:49,406 - INFO - training batch 851, loss: 0.190, 27232/60000 datapoints
2025-03-06 19:56:49,601 - INFO - training batch 901, loss: 0.186, 28832/60000 datapoints
2025-03-06 19:56:49,806 - INFO - training batch 951, loss: 0.074, 30432/60000 datapoints
2025-03-06 19:56:50,012 - INFO - training batch 1001, loss: 0.149, 32032/60000 datapoints
2025-03-06 19:56:50,209 - INFO - training batch 1051, loss: 0.264, 33632/60000 datapoints
2025-03-06 19:56:50,404 - INFO - training batch 1101, loss: 0.384, 35232/60000 datapoints
2025-03-06 19:56:50,598 - INFO - training batch 1151, loss: 0.367, 36832/60000 datapoints
2025-03-06 19:56:50,793 - INFO - training batch 1201, loss: 0.117, 38432/60000 datapoints
2025-03-06 19:56:50,986 - INFO - training batch 1251, loss: 0.306, 40032/60000 datapoints
2025-03-06 19:56:51,177 - INFO - training batch 1301, loss: 0.291, 41632/60000 datapoints
2025-03-06 19:56:51,379 - INFO - training batch 1351, loss: 0.180, 43232/60000 datapoints
2025-03-06 19:56:51,573 - INFO - training batch 1401, loss: 0.227, 44832/60000 datapoints
2025-03-06 19:56:51,772 - INFO - training batch 1451, loss: 0.301, 46432/60000 datapoints
2025-03-06 19:56:51,967 - INFO - training batch 1501, loss: 0.274, 48032/60000 datapoints
2025-03-06 19:56:52,168 - INFO - training batch 1551, loss: 0.154, 49632/60000 datapoints
2025-03-06 19:56:52,365 - INFO - training batch 1601, loss: 0.069, 51232/60000 datapoints
2025-03-06 19:56:52,558 - INFO - training batch 1651, loss: 0.400, 52832/60000 datapoints
2025-03-06 19:56:52,756 - INFO - training batch 1701, loss: 0.399, 54432/60000 datapoints
2025-03-06 19:56:52,951 - INFO - training batch 1751, loss: 0.159, 56032/60000 datapoints
2025-03-06 19:56:53,145 - INFO - training batch 1801, loss: 0.288, 57632/60000 datapoints
2025-03-06 19:56:53,340 - INFO - training batch 1851, loss: 0.119, 59232/60000 datapoints
2025-03-06 19:56:53,442 - INFO - validation batch 1, loss: 0.210, 32/10016 datapoints
2025-03-06 19:56:53,595 - INFO - validation batch 51, loss: 0.177, 1632/10016 datapoints
2025-03-06 19:56:53,750 - INFO - validation batch 101, loss: 0.131, 3232/10016 datapoints
2025-03-06 19:56:53,907 - INFO - validation batch 151, loss: 0.191, 4832/10016 datapoints
2025-03-06 19:56:54,061 - INFO - validation batch 201, loss: 0.304, 6432/10016 datapoints
2025-03-06 19:56:54,214 - INFO - validation batch 251, loss: 0.571, 8032/10016 datapoints
2025-03-06 19:56:54,367 - INFO - validation batch 301, loss: 0.125, 9632/10016 datapoints
2025-03-06 19:56:54,405 - INFO - Epoch 536/800 done.
2025-03-06 19:56:54,405 - INFO - Final validation performance:
Loss: 0.244, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:56:54,406 - INFO - Beginning epoch 537/800
2025-03-06 19:56:54,413 - INFO - training batch 1, loss: 0.206, 32/60000 datapoints
2025-03-06 19:56:54,606 - INFO - training batch 51, loss: 0.315, 1632/60000 datapoints
2025-03-06 19:56:54,815 - INFO - training batch 101, loss: 0.378, 3232/60000 datapoints
2025-03-06 19:56:55,013 - INFO - training batch 151, loss: 0.131, 4832/60000 datapoints
2025-03-06 19:56:55,208 - INFO - training batch 201, loss: 0.124, 6432/60000 datapoints
2025-03-06 19:56:55,410 - INFO - training batch 251, loss: 0.357, 8032/60000 datapoints
2025-03-06 19:56:55,608 - INFO - training batch 301, loss: 0.436, 9632/60000 datapoints
2025-03-06 19:56:55,811 - INFO - training batch 351, loss: 0.396, 11232/60000 datapoints
2025-03-06 19:56:56,006 - INFO - training batch 401, loss: 0.376, 12832/60000 datapoints
2025-03-06 19:56:56,200 - INFO - training batch 451, loss: 0.194, 14432/60000 datapoints
2025-03-06 19:56:56,397 - INFO - training batch 501, loss: 0.084, 16032/60000 datapoints
2025-03-06 19:56:56,589 - INFO - training batch 551, loss: 0.125, 17632/60000 datapoints
2025-03-06 19:56:56,784 - INFO - training batch 601, loss: 0.214, 19232/60000 datapoints
2025-03-06 19:56:56,980 - INFO - training batch 651, loss: 0.088, 20832/60000 datapoints
2025-03-06 19:56:57,175 - INFO - training batch 701, loss: 0.073, 22432/60000 datapoints
2025-03-06 19:56:57,371 - INFO - training batch 751, loss: 0.085, 24032/60000 datapoints
2025-03-06 19:56:57,569 - INFO - training batch 801, loss: 0.239, 25632/60000 datapoints
2025-03-06 19:56:57,764 - INFO - training batch 851, loss: 0.402, 27232/60000 datapoints
2025-03-06 19:56:57,961 - INFO - training batch 901, loss: 0.253, 28832/60000 datapoints
2025-03-06 19:56:58,153 - INFO - training batch 951, loss: 0.071, 30432/60000 datapoints
2025-03-06 19:56:58,348 - INFO - training batch 1001, loss: 0.193, 32032/60000 datapoints
2025-03-06 19:56:58,542 - INFO - training batch 1051, loss: 0.109, 33632/60000 datapoints
2025-03-06 19:56:58,738 - INFO - training batch 1101, loss: 0.165, 35232/60000 datapoints
2025-03-06 19:56:58,935 - INFO - training batch 1151, loss: 0.163, 36832/60000 datapoints
2025-03-06 19:56:59,134 - INFO - training batch 1201, loss: 0.204, 38432/60000 datapoints
2025-03-06 19:56:59,332 - INFO - training batch 1251, loss: 0.244, 40032/60000 datapoints
2025-03-06 19:56:59,527 - INFO - training batch 1301, loss: 0.244, 41632/60000 datapoints
2025-03-06 19:56:59,722 - INFO - training batch 1351, loss: 0.148, 43232/60000 datapoints
2025-03-06 19:56:59,938 - INFO - training batch 1401, loss: 0.217, 44832/60000 datapoints
2025-03-06 19:57:00,135 - INFO - training batch 1451, loss: 0.347, 46432/60000 datapoints
2025-03-06 19:57:00,329 - INFO - training batch 1501, loss: 0.185, 48032/60000 datapoints
2025-03-06 19:57:00,527 - INFO - training batch 1551, loss: 0.191, 49632/60000 datapoints
2025-03-06 19:57:00,723 - INFO - training batch 1601, loss: 0.199, 51232/60000 datapoints
2025-03-06 19:57:00,919 - INFO - training batch 1651, loss: 0.244, 52832/60000 datapoints
2025-03-06 19:57:01,115 - INFO - training batch 1701, loss: 0.080, 54432/60000 datapoints
2025-03-06 19:57:01,309 - INFO - training batch 1751, loss: 0.153, 56032/60000 datapoints
2025-03-06 19:57:01,505 - INFO - training batch 1801, loss: 0.492, 57632/60000 datapoints
2025-03-06 19:57:01,702 - INFO - training batch 1851, loss: 0.273, 59232/60000 datapoints
2025-03-06 19:57:01,805 - INFO - validation batch 1, loss: 0.264, 32/10016 datapoints
2025-03-06 19:57:01,962 - INFO - validation batch 51, loss: 0.289, 1632/10016 datapoints
2025-03-06 19:57:02,121 - INFO - validation batch 101, loss: 0.202, 3232/10016 datapoints
2025-03-06 19:57:02,282 - INFO - validation batch 151, loss: 0.116, 4832/10016 datapoints
2025-03-06 19:57:02,441 - INFO - validation batch 201, loss: 0.573, 6432/10016 datapoints
2025-03-06 19:57:02,604 - INFO - validation batch 251, loss: 0.226, 8032/10016 datapoints
2025-03-06 19:57:02,765 - INFO - validation batch 301, loss: 0.133, 9632/10016 datapoints
2025-03-06 19:57:02,805 - INFO - Epoch 537/800 done.
2025-03-06 19:57:02,805 - INFO - Final validation performance:
Loss: 0.257, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:57:02,806 - INFO - Beginning epoch 538/800
2025-03-06 19:57:02,814 - INFO - training batch 1, loss: 0.309, 32/60000 datapoints
2025-03-06 19:57:03,027 - INFO - training batch 51, loss: 0.319, 1632/60000 datapoints
2025-03-06 19:57:03,220 - INFO - training batch 101, loss: 0.213, 3232/60000 datapoints
2025-03-06 19:57:03,428 - INFO - training batch 151, loss: 0.184, 4832/60000 datapoints
2025-03-06 19:57:03,627 - INFO - training batch 201, loss: 0.218, 6432/60000 datapoints
2025-03-06 19:57:03,823 - INFO - training batch 251, loss: 0.300, 8032/60000 datapoints
2025-03-06 19:57:04,031 - INFO - training batch 301, loss: 0.121, 9632/60000 datapoints
2025-03-06 19:57:04,226 - INFO - training batch 351, loss: 0.271, 11232/60000 datapoints
2025-03-06 19:57:04,427 - INFO - training batch 401, loss: 0.176, 12832/60000 datapoints
2025-03-06 19:57:04,623 - INFO - training batch 451, loss: 0.313, 14432/60000 datapoints
2025-03-06 19:57:04,817 - INFO - training batch 501, loss: 0.195, 16032/60000 datapoints
2025-03-06 19:57:05,018 - INFO - training batch 551, loss: 0.110, 17632/60000 datapoints
2025-03-06 19:57:05,213 - INFO - training batch 601, loss: 0.053, 19232/60000 datapoints
2025-03-06 19:57:05,410 - INFO - training batch 651, loss: 0.147, 20832/60000 datapoints
2025-03-06 19:57:05,603 - INFO - training batch 701, loss: 0.196, 22432/60000 datapoints
2025-03-06 19:57:05,802 - INFO - training batch 751, loss: 0.130, 24032/60000 datapoints
2025-03-06 19:57:05,999 - INFO - training batch 801, loss: 0.158, 25632/60000 datapoints
2025-03-06 19:57:06,194 - INFO - training batch 851, loss: 0.162, 27232/60000 datapoints
2025-03-06 19:57:06,389 - INFO - training batch 901, loss: 0.534, 28832/60000 datapoints
2025-03-06 19:57:06,585 - INFO - training batch 951, loss: 0.205, 30432/60000 datapoints
2025-03-06 19:57:06,782 - INFO - training batch 1001, loss: 0.368, 32032/60000 datapoints
2025-03-06 19:57:06,978 - INFO - training batch 1051, loss: 0.200, 33632/60000 datapoints
2025-03-06 19:57:07,175 - INFO - training batch 1101, loss: 0.300, 35232/60000 datapoints
2025-03-06 19:57:07,369 - INFO - training batch 1151, loss: 0.414, 36832/60000 datapoints
2025-03-06 19:57:07,565 - INFO - training batch 1201, loss: 0.208, 38432/60000 datapoints
2025-03-06 19:57:07,761 - INFO - training batch 1251, loss: 0.126, 40032/60000 datapoints
2025-03-06 19:57:07,956 - INFO - training batch 1301, loss: 0.210, 41632/60000 datapoints
2025-03-06 19:57:08,151 - INFO - training batch 1351, loss: 0.048, 43232/60000 datapoints
2025-03-06 19:57:08,347 - INFO - training batch 1401, loss: 0.233, 44832/60000 datapoints
2025-03-06 19:57:08,541 - INFO - training batch 1451, loss: 0.369, 46432/60000 datapoints
2025-03-06 19:57:08,735 - INFO - training batch 1501, loss: 0.310, 48032/60000 datapoints
2025-03-06 19:57:08,940 - INFO - training batch 1551, loss: 0.317, 49632/60000 datapoints
2025-03-06 19:57:09,134 - INFO - training batch 1601, loss: 0.117, 51232/60000 datapoints
2025-03-06 19:57:09,333 - INFO - training batch 1651, loss: 0.174, 52832/60000 datapoints
2025-03-06 19:57:09,530 - INFO - training batch 1701, loss: 0.498, 54432/60000 datapoints
2025-03-06 19:57:09,727 - INFO - training batch 1751, loss: 0.214, 56032/60000 datapoints
2025-03-06 19:57:09,921 - INFO - training batch 1801, loss: 0.124, 57632/60000 datapoints
2025-03-06 19:57:10,138 - INFO - training batch 1851, loss: 0.217, 59232/60000 datapoints
2025-03-06 19:57:10,237 - INFO - validation batch 1, loss: 0.342, 32/10016 datapoints
2025-03-06 19:57:10,391 - INFO - validation batch 51, loss: 0.178, 1632/10016 datapoints
2025-03-06 19:57:10,545 - INFO - validation batch 101, loss: 0.204, 3232/10016 datapoints
2025-03-06 19:57:10,702 - INFO - validation batch 151, loss: 0.379, 4832/10016 datapoints
2025-03-06 19:57:10,857 - INFO - validation batch 201, loss: 0.180, 6432/10016 datapoints
2025-03-06 19:57:11,011 - INFO - validation batch 251, loss: 0.470, 8032/10016 datapoints
2025-03-06 19:57:11,166 - INFO - validation batch 301, loss: 0.374, 9632/10016 datapoints
2025-03-06 19:57:11,212 - INFO - Epoch 538/800 done.
2025-03-06 19:57:11,212 - INFO - Final validation performance:
Loss: 0.304, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:57:11,213 - INFO - Beginning epoch 539/800
2025-03-06 19:57:11,220 - INFO - training batch 1, loss: 0.454, 32/60000 datapoints
2025-03-06 19:57:11,415 - INFO - training batch 51, loss: 0.176, 1632/60000 datapoints
2025-03-06 19:57:11,609 - INFO - training batch 101, loss: 0.123, 3232/60000 datapoints
2025-03-06 19:57:11,818 - INFO - training batch 151, loss: 0.132, 4832/60000 datapoints
2025-03-06 19:57:12,010 - INFO - training batch 201, loss: 0.208, 6432/60000 datapoints
2025-03-06 19:57:12,203 - INFO - training batch 251, loss: 0.284, 8032/60000 datapoints
2025-03-06 19:57:12,413 - INFO - training batch 301, loss: 0.221, 9632/60000 datapoints
2025-03-06 19:57:12,616 - INFO - training batch 351, loss: 0.280, 11232/60000 datapoints
2025-03-06 19:57:12,815 - INFO - training batch 401, loss: 0.337, 12832/60000 datapoints
2025-03-06 19:57:13,019 - INFO - training batch 451, loss: 0.487, 14432/60000 datapoints
2025-03-06 19:57:13,297 - INFO - training batch 501, loss: 0.207, 16032/60000 datapoints
2025-03-06 19:57:13,500 - INFO - training batch 551, loss: 0.081, 17632/60000 datapoints
2025-03-06 19:57:13,698 - INFO - training batch 601, loss: 0.626, 19232/60000 datapoints
2025-03-06 19:57:13,892 - INFO - training batch 651, loss: 0.302, 20832/60000 datapoints
2025-03-06 19:57:14,086 - INFO - training batch 701, loss: 0.211, 22432/60000 datapoints
2025-03-06 19:57:14,292 - INFO - training batch 751, loss: 0.074, 24032/60000 datapoints
2025-03-06 19:57:14,487 - INFO - training batch 801, loss: 0.417, 25632/60000 datapoints
2025-03-06 19:57:14,684 - INFO - training batch 851, loss: 0.294, 27232/60000 datapoints
2025-03-06 19:57:14,882 - INFO - training batch 901, loss: 0.542, 28832/60000 datapoints
2025-03-06 19:57:15,077 - INFO - training batch 951, loss: 0.321, 30432/60000 datapoints
2025-03-06 19:57:15,300 - INFO - training batch 1001, loss: 0.262, 32032/60000 datapoints
2025-03-06 19:57:15,499 - INFO - training batch 1051, loss: 0.185, 33632/60000 datapoints
2025-03-06 19:57:15,697 - INFO - training batch 1101, loss: 0.409, 35232/60000 datapoints
2025-03-06 19:57:15,893 - INFO - training batch 1151, loss: 0.212, 36832/60000 datapoints
2025-03-06 19:57:16,088 - INFO - training batch 1201, loss: 0.160, 38432/60000 datapoints
2025-03-06 19:57:16,282 - INFO - training batch 1251, loss: 0.218, 40032/60000 datapoints
2025-03-06 19:57:16,477 - INFO - training batch 1301, loss: 0.205, 41632/60000 datapoints
2025-03-06 19:57:16,675 - INFO - training batch 1351, loss: 0.412, 43232/60000 datapoints
2025-03-06 19:57:16,872 - INFO - training batch 1401, loss: 0.296, 44832/60000 datapoints
2025-03-06 19:57:17,065 - INFO - training batch 1451, loss: 0.184, 46432/60000 datapoints
2025-03-06 19:57:17,266 - INFO - training batch 1501, loss: 0.411, 48032/60000 datapoints
2025-03-06 19:57:17,462 - INFO - training batch 1551, loss: 0.348, 49632/60000 datapoints
2025-03-06 19:57:17,660 - INFO - training batch 1601, loss: 0.147, 51232/60000 datapoints
2025-03-06 19:57:17,856 - INFO - training batch 1651, loss: 0.126, 52832/60000 datapoints
2025-03-06 19:57:18,052 - INFO - training batch 1701, loss: 0.220, 54432/60000 datapoints
2025-03-06 19:57:18,250 - INFO - training batch 1751, loss: 0.327, 56032/60000 datapoints
2025-03-06 19:57:18,449 - INFO - training batch 1801, loss: 0.191, 57632/60000 datapoints
2025-03-06 19:57:18,648 - INFO - training batch 1851, loss: 0.331, 59232/60000 datapoints
2025-03-06 19:57:18,749 - INFO - validation batch 1, loss: 0.112, 32/10016 datapoints
2025-03-06 19:57:18,903 - INFO - validation batch 51, loss: 0.282, 1632/10016 datapoints
2025-03-06 19:57:19,056 - INFO - validation batch 101, loss: 0.309, 3232/10016 datapoints
2025-03-06 19:57:19,210 - INFO - validation batch 151, loss: 0.338, 4832/10016 datapoints
2025-03-06 19:57:19,366 - INFO - validation batch 201, loss: 0.348, 6432/10016 datapoints
2025-03-06 19:57:19,521 - INFO - validation batch 251, loss: 0.310, 8032/10016 datapoints
2025-03-06 19:57:19,677 - INFO - validation batch 301, loss: 0.147, 9632/10016 datapoints
2025-03-06 19:57:19,713 - INFO - Epoch 539/800 done.
2025-03-06 19:57:19,713 - INFO - Final validation performance:
Loss: 0.264, top-1 acc: 0.928top-5 acc: 0.928
2025-03-06 19:57:19,714 - INFO - Beginning epoch 540/800
2025-03-06 19:57:19,723 - INFO - training batch 1, loss: 0.676, 32/60000 datapoints
2025-03-06 19:57:19,920 - INFO - training batch 51, loss: 0.263, 1632/60000 datapoints
2025-03-06 19:57:20,136 - INFO - training batch 101, loss: 0.222, 3232/60000 datapoints
2025-03-06 19:57:20,342 - INFO - training batch 151, loss: 0.199, 4832/60000 datapoints
2025-03-06 19:57:20,537 - INFO - training batch 201, loss: 0.341, 6432/60000 datapoints
2025-03-06 19:57:20,734 - INFO - training batch 251, loss: 0.410, 8032/60000 datapoints
2025-03-06 19:57:20,934 - INFO - training batch 301, loss: 0.354, 9632/60000 datapoints
2025-03-06 19:57:21,129 - INFO - training batch 351, loss: 0.513, 11232/60000 datapoints
2025-03-06 19:57:21,323 - INFO - training batch 401, loss: 0.294, 12832/60000 datapoints
2025-03-06 19:57:21,517 - INFO - training batch 451, loss: 0.067, 14432/60000 datapoints
2025-03-06 19:57:21,712 - INFO - training batch 501, loss: 0.131, 16032/60000 datapoints
2025-03-06 19:57:21,910 - INFO - training batch 551, loss: 0.254, 17632/60000 datapoints
2025-03-06 19:57:22,105 - INFO - training batch 601, loss: 0.382, 19232/60000 datapoints
2025-03-06 19:57:22,300 - INFO - training batch 651, loss: 0.226, 20832/60000 datapoints
2025-03-06 19:57:22,510 - INFO - training batch 701, loss: 0.229, 22432/60000 datapoints
2025-03-06 19:57:22,706 - INFO - training batch 751, loss: 0.281, 24032/60000 datapoints
2025-03-06 19:57:22,901 - INFO - training batch 801, loss: 0.234, 25632/60000 datapoints
2025-03-06 19:57:23,092 - INFO - training batch 851, loss: 0.148, 27232/60000 datapoints
2025-03-06 19:57:23,280 - INFO - training batch 901, loss: 0.214, 28832/60000 datapoints
2025-03-06 19:57:23,471 - INFO - training batch 951, loss: 0.414, 30432/60000 datapoints
2025-03-06 19:57:23,667 - INFO - training batch 1001, loss: 0.240, 32032/60000 datapoints
2025-03-06 19:57:23,860 - INFO - training batch 1051, loss: 0.213, 33632/60000 datapoints
2025-03-06 19:57:24,052 - INFO - training batch 1101, loss: 0.280, 35232/60000 datapoints
2025-03-06 19:57:24,244 - INFO - training batch 1151, loss: 0.111, 36832/60000 datapoints
2025-03-06 19:57:24,437 - INFO - training batch 1201, loss: 0.314, 38432/60000 datapoints
2025-03-06 19:57:24,634 - INFO - training batch 1251, loss: 0.249, 40032/60000 datapoints
2025-03-06 19:57:24,828 - INFO - training batch 1301, loss: 0.150, 41632/60000 datapoints
2025-03-06 19:57:25,022 - INFO - training batch 1351, loss: 0.114, 43232/60000 datapoints
2025-03-06 19:57:25,213 - INFO - training batch 1401, loss: 0.334, 44832/60000 datapoints
2025-03-06 19:57:25,410 - INFO - training batch 1451, loss: 0.148, 46432/60000 datapoints
2025-03-06 19:57:25,604 - INFO - training batch 1501, loss: 0.229, 48032/60000 datapoints
2025-03-06 19:57:25,801 - INFO - training batch 1551, loss: 0.502, 49632/60000 datapoints
2025-03-06 19:57:25,996 - INFO - training batch 1601, loss: 0.289, 51232/60000 datapoints
2025-03-06 19:57:26,192 - INFO - training batch 1651, loss: 0.283, 52832/60000 datapoints
2025-03-06 19:57:26,386 - INFO - training batch 1701, loss: 0.086, 54432/60000 datapoints
2025-03-06 19:57:26,580 - INFO - training batch 1751, loss: 0.186, 56032/60000 datapoints
2025-03-06 19:57:26,775 - INFO - training batch 1801, loss: 0.193, 57632/60000 datapoints
2025-03-06 19:57:26,967 - INFO - training batch 1851, loss: 0.091, 59232/60000 datapoints
2025-03-06 19:57:27,067 - INFO - validation batch 1, loss: 0.227, 32/10016 datapoints
2025-03-06 19:57:27,221 - INFO - validation batch 51, loss: 0.134, 1632/10016 datapoints
2025-03-06 19:57:27,373 - INFO - validation batch 101, loss: 0.422, 3232/10016 datapoints
2025-03-06 19:57:27,526 - INFO - validation batch 151, loss: 0.378, 4832/10016 datapoints
2025-03-06 19:57:27,682 - INFO - validation batch 201, loss: 0.141, 6432/10016 datapoints
2025-03-06 19:57:27,835 - INFO - validation batch 251, loss: 0.552, 8032/10016 datapoints
2025-03-06 19:57:27,992 - INFO - validation batch 301, loss: 0.331, 9632/10016 datapoints
2025-03-06 19:57:28,030 - INFO - Epoch 540/800 done.
2025-03-06 19:57:28,030 - INFO - Final validation performance:
Loss: 0.312, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:57:28,031 - INFO - Beginning epoch 541/800
2025-03-06 19:57:28,037 - INFO - training batch 1, loss: 0.083, 32/60000 datapoints
2025-03-06 19:57:28,234 - INFO - training batch 51, loss: 0.261, 1632/60000 datapoints
2025-03-06 19:57:28,449 - INFO - training batch 101, loss: 0.195, 3232/60000 datapoints
2025-03-06 19:57:28,652 - INFO - training batch 151, loss: 0.216, 4832/60000 datapoints
2025-03-06 19:57:28,852 - INFO - training batch 201, loss: 0.087, 6432/60000 datapoints
2025-03-06 19:57:29,048 - INFO - training batch 251, loss: 0.275, 8032/60000 datapoints
2025-03-06 19:57:29,247 - INFO - training batch 301, loss: 0.081, 9632/60000 datapoints
2025-03-06 19:57:29,447 - INFO - training batch 351, loss: 0.560, 11232/60000 datapoints
2025-03-06 19:57:29,651 - INFO - training batch 401, loss: 0.415, 12832/60000 datapoints
2025-03-06 19:57:29,876 - INFO - training batch 451, loss: 0.184, 14432/60000 datapoints
2025-03-06 19:57:30,070 - INFO - training batch 501, loss: 0.117, 16032/60000 datapoints
2025-03-06 19:57:30,287 - INFO - training batch 551, loss: 0.280, 17632/60000 datapoints
2025-03-06 19:57:30,482 - INFO - training batch 601, loss: 0.250, 19232/60000 datapoints
2025-03-06 19:57:30,676 - INFO - training batch 651, loss: 0.229, 20832/60000 datapoints
2025-03-06 19:57:30,871 - INFO - training batch 701, loss: 0.224, 22432/60000 datapoints
2025-03-06 19:57:31,064 - INFO - training batch 751, loss: 0.251, 24032/60000 datapoints
2025-03-06 19:57:31,260 - INFO - training batch 801, loss: 0.346, 25632/60000 datapoints
2025-03-06 19:57:31,454 - INFO - training batch 851, loss: 0.157, 27232/60000 datapoints
2025-03-06 19:57:31,650 - INFO - training batch 901, loss: 0.056, 28832/60000 datapoints
2025-03-06 19:57:31,844 - INFO - training batch 951, loss: 0.400, 30432/60000 datapoints
2025-03-06 19:57:32,041 - INFO - training batch 1001, loss: 0.104, 32032/60000 datapoints
2025-03-06 19:57:32,238 - INFO - training batch 1051, loss: 0.246, 33632/60000 datapoints
2025-03-06 19:57:32,432 - INFO - training batch 1101, loss: 0.202, 35232/60000 datapoints
2025-03-06 19:57:32,628 - INFO - training batch 1151, loss: 0.160, 36832/60000 datapoints
2025-03-06 19:57:32,825 - INFO - training batch 1201, loss: 0.157, 38432/60000 datapoints
2025-03-06 19:57:33,017 - INFO - training batch 1251, loss: 0.144, 40032/60000 datapoints
2025-03-06 19:57:33,213 - INFO - training batch 1301, loss: 0.144, 41632/60000 datapoints
2025-03-06 19:57:33,407 - INFO - training batch 1351, loss: 0.147, 43232/60000 datapoints
2025-03-06 19:57:33,602 - INFO - training batch 1401, loss: 0.389, 44832/60000 datapoints
2025-03-06 19:57:33,826 - INFO - training batch 1451, loss: 0.264, 46432/60000 datapoints
2025-03-06 19:57:34,049 - INFO - training batch 1501, loss: 0.157, 48032/60000 datapoints
2025-03-06 19:57:34,270 - INFO - training batch 1551, loss: 0.247, 49632/60000 datapoints
2025-03-06 19:57:34,462 - INFO - training batch 1601, loss: 0.205, 51232/60000 datapoints
2025-03-06 19:57:34,661 - INFO - training batch 1651, loss: 0.176, 52832/60000 datapoints
2025-03-06 19:57:34,859 - INFO - training batch 1701, loss: 0.108, 54432/60000 datapoints
2025-03-06 19:57:35,050 - INFO - training batch 1751, loss: 0.197, 56032/60000 datapoints
2025-03-06 19:57:35,247 - INFO - training batch 1801, loss: 0.242, 57632/60000 datapoints
2025-03-06 19:57:35,442 - INFO - training batch 1851, loss: 0.276, 59232/60000 datapoints
2025-03-06 19:57:35,545 - INFO - validation batch 1, loss: 0.219, 32/10016 datapoints
2025-03-06 19:57:35,700 - INFO - validation batch 51, loss: 0.240, 1632/10016 datapoints
2025-03-06 19:57:35,854 - INFO - validation batch 101, loss: 0.111, 3232/10016 datapoints
2025-03-06 19:57:36,008 - INFO - validation batch 151, loss: 0.638, 4832/10016 datapoints
2025-03-06 19:57:36,161 - INFO - validation batch 201, loss: 0.131, 6432/10016 datapoints
2025-03-06 19:57:36,314 - INFO - validation batch 251, loss: 0.370, 8032/10016 datapoints
2025-03-06 19:57:36,466 - INFO - validation batch 301, loss: 0.318, 9632/10016 datapoints
2025-03-06 19:57:36,504 - INFO - Epoch 541/800 done.
2025-03-06 19:57:36,504 - INFO - Final validation performance:
Loss: 0.290, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:57:36,505 - INFO - Beginning epoch 542/800
2025-03-06 19:57:36,512 - INFO - training batch 1, loss: 0.332, 32/60000 datapoints
2025-03-06 19:57:36,725 - INFO - training batch 51, loss: 0.131, 1632/60000 datapoints
2025-03-06 19:57:36,921 - INFO - training batch 101, loss: 0.105, 3232/60000 datapoints
2025-03-06 19:57:37,123 - INFO - training batch 151, loss: 0.229, 4832/60000 datapoints
2025-03-06 19:57:37,321 - INFO - training batch 201, loss: 0.075, 6432/60000 datapoints
2025-03-06 19:57:37,518 - INFO - training batch 251, loss: 0.356, 8032/60000 datapoints
2025-03-06 19:57:37,735 - INFO - training batch 301, loss: 0.221, 9632/60000 datapoints
2025-03-06 19:57:37,934 - INFO - training batch 351, loss: 0.466, 11232/60000 datapoints
2025-03-06 19:57:38,126 - INFO - training batch 401, loss: 0.702, 12832/60000 datapoints
2025-03-06 19:57:38,321 - INFO - training batch 451, loss: 0.266, 14432/60000 datapoints
2025-03-06 19:57:38,523 - INFO - training batch 501, loss: 0.120, 16032/60000 datapoints
2025-03-06 19:57:38,718 - INFO - training batch 551, loss: 0.125, 17632/60000 datapoints
2025-03-06 19:57:38,913 - INFO - training batch 601, loss: 0.320, 19232/60000 datapoints
2025-03-06 19:57:39,107 - INFO - training batch 651, loss: 0.104, 20832/60000 datapoints
2025-03-06 19:57:39,319 - INFO - training batch 701, loss: 0.380, 22432/60000 datapoints
2025-03-06 19:57:39,523 - INFO - training batch 751, loss: 0.335, 24032/60000 datapoints
2025-03-06 19:57:39,748 - INFO - training batch 801, loss: 0.117, 25632/60000 datapoints
2025-03-06 19:57:39,992 - INFO - training batch 851, loss: 0.413, 27232/60000 datapoints
2025-03-06 19:57:40,196 - INFO - training batch 901, loss: 0.293, 28832/60000 datapoints
2025-03-06 19:57:40,426 - INFO - training batch 951, loss: 0.122, 30432/60000 datapoints
2025-03-06 19:57:40,674 - INFO - training batch 1001, loss: 0.131, 32032/60000 datapoints
2025-03-06 19:57:40,872 - INFO - training batch 1051, loss: 0.499, 33632/60000 datapoints
2025-03-06 19:57:41,093 - INFO - training batch 1101, loss: 0.349, 35232/60000 datapoints
2025-03-06 19:57:41,300 - INFO - training batch 1151, loss: 0.242, 36832/60000 datapoints
2025-03-06 19:57:41,504 - INFO - training batch 1201, loss: 0.322, 38432/60000 datapoints
2025-03-06 19:57:41,710 - INFO - training batch 1251, loss: 0.226, 40032/60000 datapoints
2025-03-06 19:57:41,915 - INFO - training batch 1301, loss: 0.179, 41632/60000 datapoints
2025-03-06 19:57:42,114 - INFO - training batch 1351, loss: 0.092, 43232/60000 datapoints
2025-03-06 19:57:42,332 - INFO - training batch 1401, loss: 0.514, 44832/60000 datapoints
2025-03-06 19:57:42,538 - INFO - training batch 1451, loss: 0.686, 46432/60000 datapoints
2025-03-06 19:57:42,749 - INFO - training batch 1501, loss: 0.296, 48032/60000 datapoints
2025-03-06 19:57:42,944 - INFO - training batch 1551, loss: 0.906, 49632/60000 datapoints
2025-03-06 19:57:43,143 - INFO - training batch 1601, loss: 0.257, 51232/60000 datapoints
2025-03-06 19:57:43,339 - INFO - training batch 1651, loss: 0.085, 52832/60000 datapoints
2025-03-06 19:57:43,537 - INFO - training batch 1701, loss: 0.234, 54432/60000 datapoints
2025-03-06 19:57:43,734 - INFO - training batch 1751, loss: 0.141, 56032/60000 datapoints
2025-03-06 19:57:43,929 - INFO - training batch 1801, loss: 0.339, 57632/60000 datapoints
2025-03-06 19:57:44,124 - INFO - training batch 1851, loss: 0.166, 59232/60000 datapoints
2025-03-06 19:57:44,238 - INFO - validation batch 1, loss: 0.093, 32/10016 datapoints
2025-03-06 19:57:44,437 - INFO - validation batch 51, loss: 0.401, 1632/10016 datapoints
2025-03-06 19:57:44,615 - INFO - validation batch 101, loss: 0.327, 3232/10016 datapoints
2025-03-06 19:57:44,838 - INFO - validation batch 151, loss: 0.255, 4832/10016 datapoints
2025-03-06 19:57:45,017 - INFO - validation batch 201, loss: 0.333, 6432/10016 datapoints
2025-03-06 19:57:45,179 - INFO - validation batch 251, loss: 0.267, 8032/10016 datapoints
2025-03-06 19:57:45,336 - INFO - validation batch 301, loss: 0.195, 9632/10016 datapoints
2025-03-06 19:57:45,380 - INFO - Epoch 542/800 done.
2025-03-06 19:57:45,381 - INFO - Final validation performance:
Loss: 0.267, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:57:45,381 - INFO - Beginning epoch 543/800
2025-03-06 19:57:45,390 - INFO - training batch 1, loss: 0.207, 32/60000 datapoints
2025-03-06 19:57:45,648 - INFO - training batch 51, loss: 0.239, 1632/60000 datapoints
2025-03-06 19:57:45,866 - INFO - training batch 101, loss: 0.496, 3232/60000 datapoints
2025-03-06 19:57:46,091 - INFO - training batch 151, loss: 0.208, 4832/60000 datapoints
2025-03-06 19:57:46,343 - INFO - training batch 201, loss: 0.302, 6432/60000 datapoints
2025-03-06 19:57:46,569 - INFO - training batch 251, loss: 0.511, 8032/60000 datapoints
2025-03-06 19:57:46,799 - INFO - training batch 301, loss: 0.134, 9632/60000 datapoints
2025-03-06 19:57:47,003 - INFO - training batch 351, loss: 0.173, 11232/60000 datapoints
2025-03-06 19:57:47,203 - INFO - training batch 401, loss: 0.400, 12832/60000 datapoints
2025-03-06 19:57:47,413 - INFO - training batch 451, loss: 0.090, 14432/60000 datapoints
2025-03-06 19:57:47,612 - INFO - training batch 501, loss: 0.314, 16032/60000 datapoints
2025-03-06 19:57:47,824 - INFO - training batch 551, loss: 0.437, 17632/60000 datapoints
2025-03-06 19:57:48,074 - INFO - training batch 601, loss: 0.160, 19232/60000 datapoints
2025-03-06 19:57:48,298 - INFO - training batch 651, loss: 0.166, 20832/60000 datapoints
2025-03-06 19:57:48,529 - INFO - training batch 701, loss: 0.180, 22432/60000 datapoints
2025-03-06 19:57:48,744 - INFO - training batch 751, loss: 0.315, 24032/60000 datapoints
2025-03-06 19:57:48,952 - INFO - training batch 801, loss: 0.137, 25632/60000 datapoints
2025-03-06 19:57:49,162 - INFO - training batch 851, loss: 0.622, 27232/60000 datapoints
2025-03-06 19:57:49,369 - INFO - training batch 901, loss: 0.509, 28832/60000 datapoints
2025-03-06 19:57:49,581 - INFO - training batch 951, loss: 0.253, 30432/60000 datapoints
2025-03-06 19:57:49,793 - INFO - training batch 1001, loss: 0.092, 32032/60000 datapoints
2025-03-06 19:57:50,001 - INFO - training batch 1051, loss: 0.146, 33632/60000 datapoints
2025-03-06 19:57:50,200 - INFO - training batch 1101, loss: 0.168, 35232/60000 datapoints
2025-03-06 19:57:50,415 - INFO - training batch 1151, loss: 0.128, 36832/60000 datapoints
2025-03-06 19:57:50,612 - INFO - training batch 1201, loss: 0.519, 38432/60000 datapoints
2025-03-06 19:57:50,806 - INFO - training batch 1251, loss: 0.323, 40032/60000 datapoints
2025-03-06 19:57:51,003 - INFO - training batch 1301, loss: 0.446, 41632/60000 datapoints
2025-03-06 19:57:51,193 - INFO - training batch 1351, loss: 0.139, 43232/60000 datapoints
2025-03-06 19:57:51,386 - INFO - training batch 1401, loss: 0.419, 44832/60000 datapoints
2025-03-06 19:57:51,577 - INFO - training batch 1451, loss: 0.326, 46432/60000 datapoints
2025-03-06 19:57:51,772 - INFO - training batch 1501, loss: 0.528, 48032/60000 datapoints
2025-03-06 19:57:51,966 - INFO - training batch 1551, loss: 0.336, 49632/60000 datapoints
2025-03-06 19:57:52,160 - INFO - training batch 1601, loss: 0.154, 51232/60000 datapoints
2025-03-06 19:57:52,354 - INFO - training batch 1651, loss: 0.100, 52832/60000 datapoints
2025-03-06 19:57:52,546 - INFO - training batch 1701, loss: 0.207, 54432/60000 datapoints
2025-03-06 19:57:52,741 - INFO - training batch 1751, loss: 0.206, 56032/60000 datapoints
2025-03-06 19:57:52,932 - INFO - training batch 1801, loss: 0.144, 57632/60000 datapoints
2025-03-06 19:57:53,123 - INFO - training batch 1851, loss: 0.142, 59232/60000 datapoints
2025-03-06 19:57:53,221 - INFO - validation batch 1, loss: 0.326, 32/10016 datapoints
2025-03-06 19:57:53,435 - INFO - validation batch 51, loss: 0.707, 1632/10016 datapoints
2025-03-06 19:57:53,586 - INFO - validation batch 101, loss: 0.149, 3232/10016 datapoints
2025-03-06 19:57:53,739 - INFO - validation batch 151, loss: 0.193, 4832/10016 datapoints
2025-03-06 19:57:53,890 - INFO - validation batch 201, loss: 0.389, 6432/10016 datapoints
2025-03-06 19:57:54,042 - INFO - validation batch 251, loss: 0.174, 8032/10016 datapoints
2025-03-06 19:57:54,192 - INFO - validation batch 301, loss: 0.309, 9632/10016 datapoints
2025-03-06 19:57:54,228 - INFO - Epoch 543/800 done.
2025-03-06 19:57:54,228 - INFO - Final validation performance:
Loss: 0.321, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:57:54,229 - INFO - Beginning epoch 544/800
2025-03-06 19:57:54,235 - INFO - training batch 1, loss: 0.441, 32/60000 datapoints
2025-03-06 19:57:54,441 - INFO - training batch 51, loss: 0.319, 1632/60000 datapoints
2025-03-06 19:57:54,634 - INFO - training batch 101, loss: 0.164, 3232/60000 datapoints
2025-03-06 19:57:54,835 - INFO - training batch 151, loss: 0.301, 4832/60000 datapoints
2025-03-06 19:57:55,033 - INFO - training batch 201, loss: 0.341, 6432/60000 datapoints
2025-03-06 19:57:55,230 - INFO - training batch 251, loss: 0.066, 8032/60000 datapoints
2025-03-06 19:57:55,420 - INFO - training batch 301, loss: 0.466, 9632/60000 datapoints
2025-03-06 19:57:55,613 - INFO - training batch 351, loss: 0.185, 11232/60000 datapoints
2025-03-06 19:57:55,808 - INFO - training batch 401, loss: 0.245, 12832/60000 datapoints
2025-03-06 19:57:56,004 - INFO - training batch 451, loss: 0.086, 14432/60000 datapoints
2025-03-06 19:57:56,196 - INFO - training batch 501, loss: 0.218, 16032/60000 datapoints
2025-03-06 19:57:56,389 - INFO - training batch 551, loss: 0.114, 17632/60000 datapoints
2025-03-06 19:57:56,582 - INFO - training batch 601, loss: 0.135, 19232/60000 datapoints
2025-03-06 19:57:56,776 - INFO - training batch 651, loss: 0.278, 20832/60000 datapoints
2025-03-06 19:57:56,970 - INFO - training batch 701, loss: 0.387, 22432/60000 datapoints
2025-03-06 19:57:57,170 - INFO - training batch 751, loss: 0.193, 24032/60000 datapoints
2025-03-06 19:57:57,364 - INFO - training batch 801, loss: 0.111, 25632/60000 datapoints
2025-03-06 19:57:57,557 - INFO - training batch 851, loss: 0.046, 27232/60000 datapoints
2025-03-06 19:57:57,755 - INFO - training batch 901, loss: 0.120, 28832/60000 datapoints
2025-03-06 19:57:57,950 - INFO - training batch 951, loss: 0.329, 30432/60000 datapoints
2025-03-06 19:57:58,142 - INFO - training batch 1001, loss: 0.095, 32032/60000 datapoints
2025-03-06 19:57:58,341 - INFO - training batch 1051, loss: 0.282, 33632/60000 datapoints
2025-03-06 19:57:58,542 - INFO - training batch 1101, loss: 0.167, 35232/60000 datapoints
2025-03-06 19:57:58,737 - INFO - training batch 1151, loss: 0.268, 36832/60000 datapoints
2025-03-06 19:57:58,927 - INFO - training batch 1201, loss: 0.319, 38432/60000 datapoints
2025-03-06 19:57:59,126 - INFO - training batch 1251, loss: 0.162, 40032/60000 datapoints
2025-03-06 19:57:59,321 - INFO - training batch 1301, loss: 0.486, 41632/60000 datapoints
2025-03-06 19:57:59,519 - INFO - training batch 1351, loss: 0.163, 43232/60000 datapoints
2025-03-06 19:57:59,716 - INFO - training batch 1401, loss: 0.208, 44832/60000 datapoints
2025-03-06 19:57:59,910 - INFO - training batch 1451, loss: 0.289, 46432/60000 datapoints
2025-03-06 19:58:00,119 - INFO - training batch 1501, loss: 0.091, 48032/60000 datapoints
2025-03-06 19:58:00,320 - INFO - training batch 1551, loss: 0.263, 49632/60000 datapoints
2025-03-06 19:58:00,548 - INFO - training batch 1601, loss: 0.140, 51232/60000 datapoints
2025-03-06 19:58:00,748 - INFO - training batch 1651, loss: 0.240, 52832/60000 datapoints
2025-03-06 19:58:00,947 - INFO - training batch 1701, loss: 0.118, 54432/60000 datapoints
2025-03-06 19:58:01,144 - INFO - training batch 1751, loss: 0.112, 56032/60000 datapoints
2025-03-06 19:58:01,343 - INFO - training batch 1801, loss: 0.456, 57632/60000 datapoints
2025-03-06 19:58:01,553 - INFO - training batch 1851, loss: 0.515, 59232/60000 datapoints
2025-03-06 19:58:01,670 - INFO - validation batch 1, loss: 0.159, 32/10016 datapoints
2025-03-06 19:58:01,840 - INFO - validation batch 51, loss: 0.421, 1632/10016 datapoints
2025-03-06 19:58:02,005 - INFO - validation batch 101, loss: 0.268, 3232/10016 datapoints
2025-03-06 19:58:02,174 - INFO - validation batch 151, loss: 0.219, 4832/10016 datapoints
2025-03-06 19:58:02,330 - INFO - validation batch 201, loss: 0.328, 6432/10016 datapoints
2025-03-06 19:58:02,486 - INFO - validation batch 251, loss: 0.171, 8032/10016 datapoints
2025-03-06 19:58:02,645 - INFO - validation batch 301, loss: 0.143, 9632/10016 datapoints
2025-03-06 19:58:02,683 - INFO - Epoch 544/800 done.
2025-03-06 19:58:02,683 - INFO - Final validation performance:
Loss: 0.244, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:58:02,684 - INFO - Beginning epoch 545/800
2025-03-06 19:58:02,690 - INFO - training batch 1, loss: 0.312, 32/60000 datapoints
2025-03-06 19:58:02,904 - INFO - training batch 51, loss: 0.323, 1632/60000 datapoints
2025-03-06 19:58:03,123 - INFO - training batch 101, loss: 0.273, 3232/60000 datapoints
2025-03-06 19:58:03,321 - INFO - training batch 151, loss: 0.250, 4832/60000 datapoints
2025-03-06 19:58:03,525 - INFO - training batch 201, loss: 0.237, 6432/60000 datapoints
2025-03-06 19:58:03,727 - INFO - training batch 251, loss: 0.448, 8032/60000 datapoints
2025-03-06 19:58:03,948 - INFO - training batch 301, loss: 0.274, 9632/60000 datapoints
2025-03-06 19:58:04,146 - INFO - training batch 351, loss: 0.325, 11232/60000 datapoints
2025-03-06 19:58:04,340 - INFO - training batch 401, loss: 0.107, 12832/60000 datapoints
2025-03-06 19:58:04,535 - INFO - training batch 451, loss: 0.362, 14432/60000 datapoints
2025-03-06 19:58:04,733 - INFO - training batch 501, loss: 0.321, 16032/60000 datapoints
2025-03-06 19:58:04,936 - INFO - training batch 551, loss: 0.628, 17632/60000 datapoints
2025-03-06 19:58:05,132 - INFO - training batch 601, loss: 0.363, 19232/60000 datapoints
2025-03-06 19:58:05,326 - INFO - training batch 651, loss: 0.169, 20832/60000 datapoints
2025-03-06 19:58:05,525 - INFO - training batch 701, loss: 0.315, 22432/60000 datapoints
2025-03-06 19:58:05,722 - INFO - training batch 751, loss: 0.176, 24032/60000 datapoints
2025-03-06 19:58:05,917 - INFO - training batch 801, loss: 0.211, 25632/60000 datapoints
2025-03-06 19:58:06,113 - INFO - training batch 851, loss: 0.128, 27232/60000 datapoints
2025-03-06 19:58:06,304 - INFO - training batch 901, loss: 0.246, 28832/60000 datapoints
2025-03-06 19:58:06,499 - INFO - training batch 951, loss: 0.211, 30432/60000 datapoints
2025-03-06 19:58:06,697 - INFO - training batch 1001, loss: 0.360, 32032/60000 datapoints
2025-03-06 19:58:06,891 - INFO - training batch 1051, loss: 0.120, 33632/60000 datapoints
2025-03-06 19:58:07,085 - INFO - training batch 1101, loss: 0.235, 35232/60000 datapoints
2025-03-06 19:58:07,280 - INFO - training batch 1151, loss: 0.111, 36832/60000 datapoints
2025-03-06 19:58:07,474 - INFO - training batch 1201, loss: 0.176, 38432/60000 datapoints
2025-03-06 19:58:07,676 - INFO - training batch 1251, loss: 0.157, 40032/60000 datapoints
2025-03-06 19:58:07,872 - INFO - training batch 1301, loss: 0.294, 41632/60000 datapoints
2025-03-06 19:58:08,072 - INFO - training batch 1351, loss: 0.130, 43232/60000 datapoints
2025-03-06 19:58:08,269 - INFO - training batch 1401, loss: 0.265, 44832/60000 datapoints
2025-03-06 19:58:08,464 - INFO - training batch 1451, loss: 0.145, 46432/60000 datapoints
2025-03-06 19:58:08,659 - INFO - training batch 1501, loss: 0.943, 48032/60000 datapoints
2025-03-06 19:58:08,858 - INFO - training batch 1551, loss: 0.178, 49632/60000 datapoints
2025-03-06 19:58:09,064 - INFO - training batch 1601, loss: 0.330, 51232/60000 datapoints
2025-03-06 19:58:09,259 - INFO - training batch 1651, loss: 0.720, 52832/60000 datapoints
2025-03-06 19:58:09,454 - INFO - training batch 1701, loss: 0.286, 54432/60000 datapoints
2025-03-06 19:58:09,666 - INFO - training batch 1751, loss: 0.184, 56032/60000 datapoints
2025-03-06 19:58:09,858 - INFO - training batch 1801, loss: 0.282, 57632/60000 datapoints
2025-03-06 19:58:10,058 - INFO - training batch 1851, loss: 0.177, 59232/60000 datapoints
2025-03-06 19:58:10,160 - INFO - validation batch 1, loss: 0.188, 32/10016 datapoints
2025-03-06 19:58:10,314 - INFO - validation batch 51, loss: 0.461, 1632/10016 datapoints
2025-03-06 19:58:10,478 - INFO - validation batch 101, loss: 0.922, 3232/10016 datapoints
2025-03-06 19:58:10,642 - INFO - validation batch 151, loss: 0.119, 4832/10016 datapoints
2025-03-06 19:58:10,797 - INFO - validation batch 201, loss: 0.599, 6432/10016 datapoints
2025-03-06 19:58:10,951 - INFO - validation batch 251, loss: 0.401, 8032/10016 datapoints
2025-03-06 19:58:11,108 - INFO - validation batch 301, loss: 0.617, 9632/10016 datapoints
2025-03-06 19:58:11,144 - INFO - Epoch 545/800 done.
2025-03-06 19:58:11,144 - INFO - Final validation performance:
Loss: 0.473, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:58:11,145 - INFO - Beginning epoch 546/800
2025-03-06 19:58:11,151 - INFO - training batch 1, loss: 0.202, 32/60000 datapoints
2025-03-06 19:58:11,374 - INFO - training batch 51, loss: 0.312, 1632/60000 datapoints
2025-03-06 19:58:11,574 - INFO - training batch 101, loss: 0.318, 3232/60000 datapoints
2025-03-06 19:58:11,783 - INFO - training batch 151, loss: 0.095, 4832/60000 datapoints
2025-03-06 19:58:11,983 - INFO - training batch 201, loss: 0.140, 6432/60000 datapoints
2025-03-06 19:58:12,184 - INFO - training batch 251, loss: 0.351, 8032/60000 datapoints
2025-03-06 19:58:12,383 - INFO - training batch 301, loss: 0.392, 9632/60000 datapoints
2025-03-06 19:58:12,584 - INFO - training batch 351, loss: 0.189, 11232/60000 datapoints
2025-03-06 19:58:12,782 - INFO - training batch 401, loss: 0.151, 12832/60000 datapoints
2025-03-06 19:58:12,978 - INFO - training batch 451, loss: 0.215, 14432/60000 datapoints
2025-03-06 19:58:13,177 - INFO - training batch 501, loss: 0.070, 16032/60000 datapoints
2025-03-06 19:58:13,371 - INFO - training batch 551, loss: 0.316, 17632/60000 datapoints
2025-03-06 19:58:13,566 - INFO - training batch 601, loss: 0.646, 19232/60000 datapoints
2025-03-06 19:58:13,764 - INFO - training batch 651, loss: 0.484, 20832/60000 datapoints
2025-03-06 19:58:13,962 - INFO - training batch 701, loss: 0.115, 22432/60000 datapoints
2025-03-06 19:58:14,156 - INFO - training batch 751, loss: 0.098, 24032/60000 datapoints
2025-03-06 19:58:14,354 - INFO - training batch 801, loss: 0.384, 25632/60000 datapoints
2025-03-06 19:58:14,549 - INFO - training batch 851, loss: 0.239, 27232/60000 datapoints
2025-03-06 19:58:14,743 - INFO - training batch 901, loss: 0.417, 28832/60000 datapoints
2025-03-06 19:58:14,941 - INFO - training batch 951, loss: 0.339, 30432/60000 datapoints
2025-03-06 19:58:15,156 - INFO - training batch 1001, loss: 0.158, 32032/60000 datapoints
2025-03-06 19:58:15,382 - INFO - training batch 1051, loss: 0.325, 33632/60000 datapoints
2025-03-06 19:58:15,607 - INFO - training batch 1101, loss: 0.167, 35232/60000 datapoints
2025-03-06 19:58:15,824 - INFO - training batch 1151, loss: 0.231, 36832/60000 datapoints
2025-03-06 19:58:16,103 - INFO - training batch 1201, loss: 0.170, 38432/60000 datapoints
2025-03-06 19:58:16,338 - INFO - training batch 1251, loss: 0.110, 40032/60000 datapoints
2025-03-06 19:58:16,553 - INFO - training batch 1301, loss: 0.262, 41632/60000 datapoints
2025-03-06 19:58:16,797 - INFO - training batch 1351, loss: 0.155, 43232/60000 datapoints
2025-03-06 19:58:17,033 - INFO - training batch 1401, loss: 0.250, 44832/60000 datapoints
2025-03-06 19:58:17,238 - INFO - training batch 1451, loss: 0.097, 46432/60000 datapoints
2025-03-06 19:58:17,453 - INFO - training batch 1501, loss: 0.087, 48032/60000 datapoints
2025-03-06 19:58:17,677 - INFO - training batch 1551, loss: 0.094, 49632/60000 datapoints
2025-03-06 19:58:17,898 - INFO - training batch 1601, loss: 0.248, 51232/60000 datapoints
2025-03-06 19:58:18,100 - INFO - training batch 1651, loss: 0.285, 52832/60000 datapoints
2025-03-06 19:58:18,303 - INFO - training batch 1701, loss: 0.246, 54432/60000 datapoints
2025-03-06 19:58:18,498 - INFO - training batch 1751, loss: 0.314, 56032/60000 datapoints
2025-03-06 19:58:18,695 - INFO - training batch 1801, loss: 0.204, 57632/60000 datapoints
2025-03-06 19:58:18,895 - INFO - training batch 1851, loss: 0.300, 59232/60000 datapoints
2025-03-06 19:58:19,006 - INFO - validation batch 1, loss: 0.180, 32/10016 datapoints
2025-03-06 19:58:19,167 - INFO - validation batch 51, loss: 0.181, 1632/10016 datapoints
2025-03-06 19:58:19,332 - INFO - validation batch 101, loss: 0.441, 3232/10016 datapoints
2025-03-06 19:58:19,509 - INFO - validation batch 151, loss: 0.281, 4832/10016 datapoints
2025-03-06 19:58:19,708 - INFO - validation batch 201, loss: 0.664, 6432/10016 datapoints
2025-03-06 19:58:19,897 - INFO - validation batch 251, loss: 0.104, 8032/10016 datapoints
2025-03-06 19:58:20,077 - INFO - validation batch 301, loss: 0.201, 9632/10016 datapoints
2025-03-06 19:58:20,124 - INFO - Epoch 546/800 done.
2025-03-06 19:58:20,124 - INFO - Final validation performance:
Loss: 0.293, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:58:20,125 - INFO - Beginning epoch 547/800
2025-03-06 19:58:20,132 - INFO - training batch 1, loss: 0.306, 32/60000 datapoints
2025-03-06 19:58:20,368 - INFO - training batch 51, loss: 0.167, 1632/60000 datapoints
2025-03-06 19:58:20,633 - INFO - training batch 101, loss: 0.215, 3232/60000 datapoints
2025-03-06 19:58:20,864 - INFO - training batch 151, loss: 0.355, 4832/60000 datapoints
2025-03-06 19:58:21,098 - INFO - training batch 201, loss: 0.482, 6432/60000 datapoints
2025-03-06 19:58:21,331 - INFO - training batch 251, loss: 0.093, 8032/60000 datapoints
2025-03-06 19:58:21,549 - INFO - training batch 301, loss: 0.162, 9632/60000 datapoints
2025-03-06 19:58:21,778 - INFO - training batch 351, loss: 0.314, 11232/60000 datapoints
2025-03-06 19:58:22,007 - INFO - training batch 401, loss: 0.234, 12832/60000 datapoints
2025-03-06 19:58:22,229 - INFO - training batch 451, loss: 0.359, 14432/60000 datapoints
2025-03-06 19:58:22,465 - INFO - training batch 501, loss: 0.056, 16032/60000 datapoints
2025-03-06 19:58:22,695 - INFO - training batch 551, loss: 0.381, 17632/60000 datapoints
2025-03-06 19:58:22,917 - INFO - training batch 601, loss: 0.219, 19232/60000 datapoints
2025-03-06 19:58:23,147 - INFO - training batch 651, loss: 0.395, 20832/60000 datapoints
2025-03-06 19:58:23,376 - INFO - training batch 701, loss: 0.221, 22432/60000 datapoints
2025-03-06 19:58:23,604 - INFO - training batch 751, loss: 0.177, 24032/60000 datapoints
2025-03-06 19:58:23,836 - INFO - training batch 801, loss: 0.659, 25632/60000 datapoints
2025-03-06 19:58:24,068 - INFO - training batch 851, loss: 0.548, 27232/60000 datapoints
2025-03-06 19:58:24,296 - INFO - training batch 901, loss: 0.227, 28832/60000 datapoints
2025-03-06 19:58:24,520 - INFO - training batch 951, loss: 0.116, 30432/60000 datapoints
2025-03-06 19:58:24,733 - INFO - training batch 1001, loss: 0.283, 32032/60000 datapoints
2025-03-06 19:58:24,979 - INFO - training batch 1051, loss: 0.075, 33632/60000 datapoints
2025-03-06 19:58:25,303 - INFO - training batch 1101, loss: 0.107, 35232/60000 datapoints
2025-03-06 19:58:25,705 - INFO - training batch 1151, loss: 0.197, 36832/60000 datapoints
2025-03-06 19:58:26,051 - INFO - training batch 1201, loss: 0.181, 38432/60000 datapoints
2025-03-06 19:58:26,595 - INFO - training batch 1251, loss: 0.246, 40032/60000 datapoints
2025-03-06 19:58:26,854 - INFO - training batch 1301, loss: 0.260, 41632/60000 datapoints
2025-03-06 19:58:27,109 - INFO - training batch 1351, loss: 0.221, 43232/60000 datapoints
2025-03-06 19:58:27,453 - INFO - training batch 1401, loss: 0.100, 44832/60000 datapoints
2025-03-06 19:58:27,736 - INFO - training batch 1451, loss: 0.305, 46432/60000 datapoints
2025-03-06 19:58:28,026 - INFO - training batch 1501, loss: 0.637, 48032/60000 datapoints
2025-03-06 19:58:28,399 - INFO - training batch 1551, loss: 0.134, 49632/60000 datapoints
2025-03-06 19:58:28,674 - INFO - training batch 1601, loss: 0.215, 51232/60000 datapoints
2025-03-06 19:58:28,929 - INFO - training batch 1651, loss: 0.199, 52832/60000 datapoints
2025-03-06 19:58:29,143 - INFO - training batch 1701, loss: 0.369, 54432/60000 datapoints
2025-03-06 19:58:29,380 - INFO - training batch 1751, loss: 0.047, 56032/60000 datapoints
2025-03-06 19:58:29,622 - INFO - training batch 1801, loss: 0.110, 57632/60000 datapoints
2025-03-06 19:58:29,831 - INFO - training batch 1851, loss: 0.257, 59232/60000 datapoints
2025-03-06 19:58:29,941 - INFO - validation batch 1, loss: 0.388, 32/10016 datapoints
2025-03-06 19:58:30,106 - INFO - validation batch 51, loss: 0.410, 1632/10016 datapoints
2025-03-06 19:58:30,268 - INFO - validation batch 101, loss: 0.293, 3232/10016 datapoints
2025-03-06 19:58:30,427 - INFO - validation batch 151, loss: 0.271, 4832/10016 datapoints
2025-03-06 19:58:30,597 - INFO - validation batch 201, loss: 0.377, 6432/10016 datapoints
2025-03-06 19:58:30,785 - INFO - validation batch 251, loss: 0.232, 8032/10016 datapoints
2025-03-06 19:58:30,949 - INFO - validation batch 301, loss: 0.428, 9632/10016 datapoints
2025-03-06 19:58:30,988 - INFO - Epoch 547/800 done.
2025-03-06 19:58:30,988 - INFO - Final validation performance:
Loss: 0.343, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:58:30,988 - INFO - Beginning epoch 548/800
2025-03-06 19:58:30,995 - INFO - training batch 1, loss: 0.093, 32/60000 datapoints
2025-03-06 19:58:31,215 - INFO - training batch 51, loss: 0.427, 1632/60000 datapoints
2025-03-06 19:58:31,419 - INFO - training batch 101, loss: 0.276, 3232/60000 datapoints
2025-03-06 19:58:31,637 - INFO - training batch 151, loss: 0.208, 4832/60000 datapoints
2025-03-06 19:58:31,848 - INFO - training batch 201, loss: 0.185, 6432/60000 datapoints
2025-03-06 19:58:32,052 - INFO - training batch 251, loss: 0.248, 8032/60000 datapoints
2025-03-06 19:58:32,255 - INFO - training batch 301, loss: 0.266, 9632/60000 datapoints
2025-03-06 19:58:32,459 - INFO - training batch 351, loss: 0.109, 11232/60000 datapoints
2025-03-06 19:58:32,664 - INFO - training batch 401, loss: 0.211, 12832/60000 datapoints
2025-03-06 19:58:32,872 - INFO - training batch 451, loss: 0.349, 14432/60000 datapoints
2025-03-06 19:58:33,087 - INFO - training batch 501, loss: 0.225, 16032/60000 datapoints
2025-03-06 19:58:33,288 - INFO - training batch 551, loss: 0.125, 17632/60000 datapoints
2025-03-06 19:58:33,502 - INFO - training batch 601, loss: 0.664, 19232/60000 datapoints
2025-03-06 19:58:33,719 - INFO - training batch 651, loss: 0.112, 20832/60000 datapoints
2025-03-06 19:58:33,960 - INFO - training batch 701, loss: 0.143, 22432/60000 datapoints
2025-03-06 19:58:34,182 - INFO - training batch 751, loss: 0.238, 24032/60000 datapoints
2025-03-06 19:58:34,397 - INFO - training batch 801, loss: 0.290, 25632/60000 datapoints
2025-03-06 19:58:34,607 - INFO - training batch 851, loss: 0.119, 27232/60000 datapoints
2025-03-06 19:58:34,856 - INFO - training batch 901, loss: 0.240, 28832/60000 datapoints
2025-03-06 19:58:35,105 - INFO - training batch 951, loss: 0.135, 30432/60000 datapoints
2025-03-06 19:58:35,314 - INFO - training batch 1001, loss: 0.259, 32032/60000 datapoints
2025-03-06 19:58:35,528 - INFO - training batch 1051, loss: 0.122, 33632/60000 datapoints
2025-03-06 19:58:35,747 - INFO - training batch 1101, loss: 0.186, 35232/60000 datapoints
2025-03-06 19:58:35,959 - INFO - training batch 1151, loss: 0.420, 36832/60000 datapoints
2025-03-06 19:58:36,178 - INFO - training batch 1201, loss: 0.280, 38432/60000 datapoints
2025-03-06 19:58:36,385 - INFO - training batch 1251, loss: 0.121, 40032/60000 datapoints
2025-03-06 19:58:36,601 - INFO - training batch 1301, loss: 0.173, 41632/60000 datapoints
2025-03-06 19:58:36,818 - INFO - training batch 1351, loss: 0.244, 43232/60000 datapoints
2025-03-06 19:58:37,044 - INFO - training batch 1401, loss: 0.285, 44832/60000 datapoints
2025-03-06 19:58:37,276 - INFO - training batch 1451, loss: 0.294, 46432/60000 datapoints
2025-03-06 19:58:37,509 - INFO - training batch 1501, loss: 0.272, 48032/60000 datapoints
2025-03-06 19:58:37,758 - INFO - training batch 1551, loss: 0.070, 49632/60000 datapoints
2025-03-06 19:58:38,159 - INFO - training batch 1601, loss: 0.099, 51232/60000 datapoints
2025-03-06 19:58:38,374 - INFO - training batch 1651, loss: 0.191, 52832/60000 datapoints
2025-03-06 19:58:38,596 - INFO - training batch 1701, loss: 0.253, 54432/60000 datapoints
2025-03-06 19:58:38,824 - INFO - training batch 1751, loss: 0.420, 56032/60000 datapoints
2025-03-06 19:58:39,057 - INFO - training batch 1801, loss: 0.224, 57632/60000 datapoints
2025-03-06 19:58:39,287 - INFO - training batch 1851, loss: 0.124, 59232/60000 datapoints
2025-03-06 19:58:39,399 - INFO - validation batch 1, loss: 0.130, 32/10016 datapoints
2025-03-06 19:58:39,575 - INFO - validation batch 51, loss: 0.366, 1632/10016 datapoints
2025-03-06 19:58:39,782 - INFO - validation batch 101, loss: 0.173, 3232/10016 datapoints
2025-03-06 19:58:40,042 - INFO - validation batch 151, loss: 0.343, 4832/10016 datapoints
2025-03-06 19:58:40,222 - INFO - validation batch 201, loss: 0.463, 6432/10016 datapoints
2025-03-06 19:58:40,391 - INFO - validation batch 251, loss: 0.219, 8032/10016 datapoints
2025-03-06 19:58:40,562 - INFO - validation batch 301, loss: 0.354, 9632/10016 datapoints
2025-03-06 19:58:40,605 - INFO - Epoch 548/800 done.
2025-03-06 19:58:40,606 - INFO - Final validation performance:
Loss: 0.293, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:58:40,606 - INFO - Beginning epoch 549/800
2025-03-06 19:58:40,615 - INFO - training batch 1, loss: 0.266, 32/60000 datapoints
2025-03-06 19:58:40,864 - INFO - training batch 51, loss: 0.120, 1632/60000 datapoints
2025-03-06 19:58:41,101 - INFO - training batch 101, loss: 0.165, 3232/60000 datapoints
2025-03-06 19:58:41,345 - INFO - training batch 151, loss: 0.235, 4832/60000 datapoints
2025-03-06 19:58:41,598 - INFO - training batch 201, loss: 0.285, 6432/60000 datapoints
2025-03-06 19:58:41,842 - INFO - training batch 251, loss: 0.234, 8032/60000 datapoints
2025-03-06 19:58:42,089 - INFO - training batch 301, loss: 0.154, 9632/60000 datapoints
2025-03-06 19:58:42,326 - INFO - training batch 351, loss: 0.171, 11232/60000 datapoints
2025-03-06 19:58:42,567 - INFO - training batch 401, loss: 0.366, 12832/60000 datapoints
2025-03-06 19:58:42,805 - INFO - training batch 451, loss: 0.397, 14432/60000 datapoints
2025-03-06 19:58:43,064 - INFO - training batch 501, loss: 0.194, 16032/60000 datapoints
2025-03-06 19:58:43,313 - INFO - training batch 551, loss: 0.336, 17632/60000 datapoints
2025-03-06 19:58:43,540 - INFO - training batch 601, loss: 0.253, 19232/60000 datapoints
2025-03-06 19:58:43,774 - INFO - training batch 651, loss: 0.454, 20832/60000 datapoints
2025-03-06 19:58:43,995 - INFO - training batch 701, loss: 0.108, 22432/60000 datapoints
2025-03-06 19:58:44,222 - INFO - training batch 751, loss: 0.173, 24032/60000 datapoints
2025-03-06 19:58:44,445 - INFO - training batch 801, loss: 0.192, 25632/60000 datapoints
2025-03-06 19:58:44,675 - INFO - training batch 851, loss: 0.103, 27232/60000 datapoints
2025-03-06 19:58:44,896 - INFO - training batch 901, loss: 0.310, 28832/60000 datapoints
2025-03-06 19:58:45,117 - INFO - training batch 951, loss: 0.107, 30432/60000 datapoints
2025-03-06 19:58:45,357 - INFO - training batch 1001, loss: 0.325, 32032/60000 datapoints
2025-03-06 19:58:45,596 - INFO - training batch 1051, loss: 0.255, 33632/60000 datapoints
2025-03-06 19:58:45,825 - INFO - training batch 1101, loss: 0.072, 35232/60000 datapoints
2025-03-06 19:58:46,070 - INFO - training batch 1151, loss: 0.183, 36832/60000 datapoints
2025-03-06 19:58:46,283 - INFO - training batch 1201, loss: 0.456, 38432/60000 datapoints
2025-03-06 19:58:46,498 - INFO - training batch 1251, loss: 0.197, 40032/60000 datapoints
2025-03-06 19:58:46,729 - INFO - training batch 1301, loss: 0.166, 41632/60000 datapoints
2025-03-06 19:58:46,957 - INFO - training batch 1351, loss: 0.082, 43232/60000 datapoints
2025-03-06 19:58:47,184 - INFO - training batch 1401, loss: 0.227, 44832/60000 datapoints
2025-03-06 19:58:47,408 - INFO - training batch 1451, loss: 0.141, 46432/60000 datapoints
2025-03-06 19:58:47,630 - INFO - training batch 1501, loss: 0.129, 48032/60000 datapoints
2025-03-06 19:58:47,861 - INFO - training batch 1551, loss: 0.053, 49632/60000 datapoints
2025-03-06 19:58:48,098 - INFO - training batch 1601, loss: 0.129, 51232/60000 datapoints
2025-03-06 19:58:48,294 - INFO - training batch 1651, loss: 0.058, 52832/60000 datapoints
2025-03-06 19:58:48,505 - INFO - training batch 1701, loss: 0.330, 54432/60000 datapoints
2025-03-06 19:58:48,722 - INFO - training batch 1751, loss: 0.248, 56032/60000 datapoints
2025-03-06 19:58:48,953 - INFO - training batch 1801, loss: 0.170, 57632/60000 datapoints
2025-03-06 19:58:49,176 - INFO - training batch 1851, loss: 0.268, 59232/60000 datapoints
2025-03-06 19:58:49,289 - INFO - validation batch 1, loss: 0.321, 32/10016 datapoints
2025-03-06 19:58:49,464 - INFO - validation batch 51, loss: 0.184, 1632/10016 datapoints
2025-03-06 19:58:49,731 - INFO - validation batch 101, loss: 0.126, 3232/10016 datapoints
2025-03-06 19:58:49,894 - INFO - validation batch 151, loss: 0.179, 4832/10016 datapoints
2025-03-06 19:58:50,071 - INFO - validation batch 201, loss: 0.211, 6432/10016 datapoints
2025-03-06 19:58:50,271 - INFO - validation batch 251, loss: 0.563, 8032/10016 datapoints
2025-03-06 19:58:50,457 - INFO - validation batch 301, loss: 0.145, 9632/10016 datapoints
2025-03-06 19:58:50,499 - INFO - Epoch 549/800 done.
2025-03-06 19:58:50,499 - INFO - Final validation performance:
Loss: 0.247, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:58:50,500 - INFO - Beginning epoch 550/800
2025-03-06 19:58:50,508 - INFO - training batch 1, loss: 0.389, 32/60000 datapoints
2025-03-06 19:58:50,809 - INFO - training batch 51, loss: 0.109, 1632/60000 datapoints
2025-03-06 19:58:51,180 - INFO - training batch 101, loss: 0.123, 3232/60000 datapoints
2025-03-06 19:58:51,535 - INFO - training batch 151, loss: 0.148, 4832/60000 datapoints
2025-03-06 19:58:51,916 - INFO - training batch 201, loss: 0.189, 6432/60000 datapoints
2025-03-06 19:58:52,351 - INFO - training batch 251, loss: 0.253, 8032/60000 datapoints
2025-03-06 19:58:52,577 - INFO - training batch 301, loss: 0.149, 9632/60000 datapoints
2025-03-06 19:58:52,819 - INFO - training batch 351, loss: 0.175, 11232/60000 datapoints
2025-03-06 19:58:53,063 - INFO - training batch 401, loss: 0.126, 12832/60000 datapoints
2025-03-06 19:58:53,294 - INFO - training batch 451, loss: 0.411, 14432/60000 datapoints
2025-03-06 19:58:53,541 - INFO - training batch 501, loss: 0.354, 16032/60000 datapoints
2025-03-06 19:58:53,848 - INFO - training batch 551, loss: 0.350, 17632/60000 datapoints
2025-03-06 19:58:54,101 - INFO - training batch 601, loss: 0.278, 19232/60000 datapoints
2025-03-06 19:58:54,379 - INFO - training batch 651, loss: 0.175, 20832/60000 datapoints
2025-03-06 19:58:55,231 - INFO - training batch 701, loss: 0.313, 22432/60000 datapoints
2025-03-06 19:58:56,144 - INFO - training batch 751, loss: 0.173, 24032/60000 datapoints
2025-03-06 19:58:56,847 - INFO - training batch 801, loss: 0.141, 25632/60000 datapoints
2025-03-06 19:58:57,698 - INFO - training batch 851, loss: 0.265, 27232/60000 datapoints
2025-03-06 19:58:58,186 - INFO - training batch 901, loss: 0.384, 28832/60000 datapoints
2025-03-06 19:58:58,692 - INFO - training batch 951, loss: 0.209, 30432/60000 datapoints
2025-03-06 19:58:59,147 - INFO - training batch 1001, loss: 0.206, 32032/60000 datapoints
2025-03-06 19:58:59,545 - INFO - training batch 1051, loss: 0.290, 33632/60000 datapoints
2025-03-06 19:58:59,968 - INFO - training batch 1101, loss: 0.104, 35232/60000 datapoints
2025-03-06 19:59:00,377 - INFO - training batch 1151, loss: 0.332, 36832/60000 datapoints
2025-03-06 19:59:00,829 - INFO - training batch 1201, loss: 0.324, 38432/60000 datapoints
2025-03-06 19:59:01,290 - INFO - training batch 1251, loss: 0.205, 40032/60000 datapoints
2025-03-06 19:59:01,713 - INFO - training batch 1301, loss: 0.138, 41632/60000 datapoints
2025-03-06 19:59:02,228 - INFO - training batch 1351, loss: 0.178, 43232/60000 datapoints
2025-03-06 19:59:02,681 - INFO - training batch 1401, loss: 0.254, 44832/60000 datapoints
2025-03-06 19:59:03,113 - INFO - training batch 1451, loss: 0.201, 46432/60000 datapoints
2025-03-06 19:59:03,515 - INFO - training batch 1501, loss: 0.374, 48032/60000 datapoints
2025-03-06 19:59:03,935 - INFO - training batch 1551, loss: 0.188, 49632/60000 datapoints
2025-03-06 19:59:04,368 - INFO - training batch 1601, loss: 0.190, 51232/60000 datapoints
2025-03-06 19:59:04,769 - INFO - training batch 1651, loss: 0.079, 52832/60000 datapoints
2025-03-06 19:59:05,227 - INFO - training batch 1701, loss: 0.096, 54432/60000 datapoints
2025-03-06 19:59:05,634 - INFO - training batch 1751, loss: 0.295, 56032/60000 datapoints
2025-03-06 19:59:06,040 - INFO - training batch 1801, loss: 0.257, 57632/60000 datapoints
2025-03-06 19:59:06,473 - INFO - training batch 1851, loss: 0.129, 59232/60000 datapoints
2025-03-06 19:59:06,687 - INFO - validation batch 1, loss: 0.306, 32/10016 datapoints
2025-03-06 19:59:07,029 - INFO - validation batch 51, loss: 0.097, 1632/10016 datapoints
2025-03-06 19:59:07,371 - INFO - validation batch 101, loss: 0.220, 3232/10016 datapoints
2025-03-06 19:59:07,692 - INFO - validation batch 151, loss: 0.167, 4832/10016 datapoints
2025-03-06 19:59:08,009 - INFO - validation batch 201, loss: 0.313, 6432/10016 datapoints
2025-03-06 19:59:08,325 - INFO - validation batch 251, loss: 0.538, 8032/10016 datapoints
2025-03-06 19:59:08,649 - INFO - validation batch 301, loss: 0.160, 9632/10016 datapoints
2025-03-06 19:59:08,719 - INFO - Epoch 550/800 done.
2025-03-06 19:59:08,720 - INFO - Final validation performance:
Loss: 0.257, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:59:08,724 - INFO - Beginning epoch 551/800
2025-03-06 19:59:08,737 - INFO - training batch 1, loss: 0.140, 32/60000 datapoints
2025-03-06 19:59:09,162 - INFO - training batch 51, loss: 0.433, 1632/60000 datapoints
2025-03-06 19:59:09,606 - INFO - training batch 101, loss: 0.422, 3232/60000 datapoints
2025-03-06 19:59:10,035 - INFO - training batch 151, loss: 0.195, 4832/60000 datapoints
2025-03-06 19:59:10,477 - INFO - training batch 201, loss: 0.149, 6432/60000 datapoints
2025-03-06 19:59:10,886 - INFO - training batch 251, loss: 0.217, 8032/60000 datapoints
2025-03-06 19:59:11,310 - INFO - training batch 301, loss: 0.236, 9632/60000 datapoints
2025-03-06 19:59:11,727 - INFO - training batch 351, loss: 0.151, 11232/60000 datapoints
2025-03-06 19:59:12,126 - INFO - training batch 401, loss: 0.245, 12832/60000 datapoints
2025-03-06 19:59:12,523 - INFO - training batch 451, loss: 0.336, 14432/60000 datapoints
2025-03-06 19:59:12,910 - INFO - training batch 501, loss: 0.241, 16032/60000 datapoints
2025-03-06 19:59:13,347 - INFO - training batch 551, loss: 0.478, 17632/60000 datapoints
2025-03-06 19:59:13,764 - INFO - training batch 601, loss: 0.093, 19232/60000 datapoints
2025-03-06 19:59:14,155 - INFO - training batch 651, loss: 0.083, 20832/60000 datapoints
2025-03-06 19:59:14,543 - INFO - training batch 701, loss: 0.111, 22432/60000 datapoints
2025-03-06 19:59:14,935 - INFO - training batch 751, loss: 0.148, 24032/60000 datapoints
2025-03-06 19:59:15,357 - INFO - training batch 801, loss: 0.097, 25632/60000 datapoints
2025-03-06 19:59:15,808 - INFO - training batch 851, loss: 0.512, 27232/60000 datapoints
2025-03-06 19:59:16,197 - INFO - training batch 901, loss: 0.163, 28832/60000 datapoints
2025-03-06 19:59:16,599 - INFO - training batch 951, loss: 0.203, 30432/60000 datapoints
2025-03-06 19:59:16,984 - INFO - training batch 1001, loss: 0.127, 32032/60000 datapoints
2025-03-06 19:59:17,366 - INFO - training batch 1051, loss: 0.255, 33632/60000 datapoints
2025-03-06 19:59:17,903 - INFO - training batch 1101, loss: 0.112, 35232/60000 datapoints
2025-03-06 19:59:18,296 - INFO - training batch 1151, loss: 0.328, 36832/60000 datapoints
2025-03-06 19:59:18,702 - INFO - training batch 1201, loss: 0.196, 38432/60000 datapoints
2025-03-06 19:59:19,094 - INFO - training batch 1251, loss: 0.181, 40032/60000 datapoints
2025-03-06 19:59:19,473 - INFO - training batch 1301, loss: 0.354, 41632/60000 datapoints
2025-03-06 19:59:19,948 - INFO - training batch 1351, loss: 0.085, 43232/60000 datapoints
2025-03-06 19:59:20,334 - INFO - training batch 1401, loss: 0.244, 44832/60000 datapoints
2025-03-06 19:59:20,714 - INFO - training batch 1451, loss: 0.208, 46432/60000 datapoints
2025-03-06 19:59:21,108 - INFO - training batch 1501, loss: 0.227, 48032/60000 datapoints
2025-03-06 19:59:21,531 - INFO - training batch 1551, loss: 0.068, 49632/60000 datapoints
2025-03-06 19:59:21,941 - INFO - training batch 1601, loss: 0.234, 51232/60000 datapoints
2025-03-06 19:59:22,375 - INFO - training batch 1651, loss: 0.213, 52832/60000 datapoints
2025-03-06 19:59:22,783 - INFO - training batch 1701, loss: 0.116, 54432/60000 datapoints
2025-03-06 19:59:23,181 - INFO - training batch 1751, loss: 0.172, 56032/60000 datapoints
2025-03-06 19:59:23,572 - INFO - training batch 1801, loss: 0.129, 57632/60000 datapoints
2025-03-06 19:59:23,983 - INFO - training batch 1851, loss: 0.160, 59232/60000 datapoints
2025-03-06 19:59:24,195 - INFO - validation batch 1, loss: 0.120, 32/10016 datapoints
2025-03-06 19:59:24,512 - INFO - validation batch 51, loss: 0.091, 1632/10016 datapoints
2025-03-06 19:59:24,836 - INFO - validation batch 101, loss: 0.378, 3232/10016 datapoints
2025-03-06 19:59:25,148 - INFO - validation batch 151, loss: 0.378, 4832/10016 datapoints
2025-03-06 19:59:25,457 - INFO - validation batch 201, loss: 0.239, 6432/10016 datapoints
2025-03-06 19:59:25,776 - INFO - validation batch 251, loss: 0.182, 8032/10016 datapoints
2025-03-06 19:59:26,089 - INFO - validation batch 301, loss: 0.101, 9632/10016 datapoints
2025-03-06 19:59:26,170 - INFO - Epoch 551/800 done.
2025-03-06 19:59:26,170 - INFO - Final validation performance:
Loss: 0.213, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:59:26,171 - INFO - Beginning epoch 552/800
2025-03-06 19:59:26,184 - INFO - training batch 1, loss: 0.609, 32/60000 datapoints
2025-03-06 19:59:26,564 - INFO - training batch 51, loss: 0.298, 1632/60000 datapoints
2025-03-06 19:59:27,104 - INFO - training batch 101, loss: 0.343, 3232/60000 datapoints
2025-03-06 19:59:27,508 - INFO - training batch 151, loss: 0.183, 4832/60000 datapoints
2025-03-06 19:59:27,909 - INFO - training batch 201, loss: 0.100, 6432/60000 datapoints
2025-03-06 19:59:28,303 - INFO - training batch 251, loss: 0.081, 8032/60000 datapoints
2025-03-06 19:59:28,690 - INFO - training batch 301, loss: 0.482, 9632/60000 datapoints
2025-03-06 19:59:29,154 - INFO - training batch 351, loss: 0.114, 11232/60000 datapoints
2025-03-06 19:59:29,547 - INFO - training batch 401, loss: 0.081, 12832/60000 datapoints
2025-03-06 19:59:29,964 - INFO - training batch 451, loss: 0.065, 14432/60000 datapoints
2025-03-06 19:59:30,379 - INFO - training batch 501, loss: 0.192, 16032/60000 datapoints
2025-03-06 19:59:30,812 - INFO - training batch 551, loss: 0.254, 17632/60000 datapoints
2025-03-06 19:59:31,255 - INFO - training batch 601, loss: 0.061, 19232/60000 datapoints
2025-03-06 19:59:31,767 - INFO - training batch 651, loss: 0.384, 20832/60000 datapoints
2025-03-06 19:59:32,176 - INFO - training batch 701, loss: 0.262, 22432/60000 datapoints
2025-03-06 19:59:32,442 - INFO - training batch 751, loss: 0.097, 24032/60000 datapoints
2025-03-06 19:59:32,650 - INFO - training batch 801, loss: 0.302, 25632/60000 datapoints
2025-03-06 19:59:32,855 - INFO - training batch 851, loss: 0.164, 27232/60000 datapoints
2025-03-06 19:59:33,052 - INFO - training batch 901, loss: 0.308, 28832/60000 datapoints
2025-03-06 19:59:33,265 - INFO - training batch 951, loss: 0.099, 30432/60000 datapoints
2025-03-06 19:59:33,469 - INFO - training batch 1001, loss: 0.328, 32032/60000 datapoints
2025-03-06 19:59:33,677 - INFO - training batch 1051, loss: 0.228, 33632/60000 datapoints
2025-03-06 19:59:33,880 - INFO - training batch 1101, loss: 0.111, 35232/60000 datapoints
2025-03-06 19:59:34,087 - INFO - training batch 1151, loss: 0.291, 36832/60000 datapoints
2025-03-06 19:59:34,288 - INFO - training batch 1201, loss: 0.224, 38432/60000 datapoints
2025-03-06 19:59:34,486 - INFO - training batch 1251, loss: 0.183, 40032/60000 datapoints
2025-03-06 19:59:34,686 - INFO - training batch 1301, loss: 0.106, 41632/60000 datapoints
2025-03-06 19:59:34,888 - INFO - training batch 1351, loss: 0.440, 43232/60000 datapoints
2025-03-06 19:59:35,084 - INFO - training batch 1401, loss: 0.383, 44832/60000 datapoints
2025-03-06 19:59:35,292 - INFO - training batch 1451, loss: 0.410, 46432/60000 datapoints
2025-03-06 19:59:35,494 - INFO - training batch 1501, loss: 0.134, 48032/60000 datapoints
2025-03-06 19:59:35,696 - INFO - training batch 1551, loss: 0.241, 49632/60000 datapoints
2025-03-06 19:59:35,890 - INFO - training batch 1601, loss: 0.182, 51232/60000 datapoints
2025-03-06 19:59:36,086 - INFO - training batch 1651, loss: 0.059, 52832/60000 datapoints
2025-03-06 19:59:36,285 - INFO - training batch 1701, loss: 0.381, 54432/60000 datapoints
2025-03-06 19:59:36,499 - INFO - training batch 1751, loss: 0.200, 56032/60000 datapoints
2025-03-06 19:59:36,725 - INFO - training batch 1801, loss: 0.150, 57632/60000 datapoints
2025-03-06 19:59:36,923 - INFO - training batch 1851, loss: 0.483, 59232/60000 datapoints
2025-03-06 19:59:37,029 - INFO - validation batch 1, loss: 0.142, 32/10016 datapoints
2025-03-06 19:59:37,204 - INFO - validation batch 51, loss: 0.165, 1632/10016 datapoints
2025-03-06 19:59:37,367 - INFO - validation batch 101, loss: 0.422, 3232/10016 datapoints
2025-03-06 19:59:37,522 - INFO - validation batch 151, loss: 0.342, 4832/10016 datapoints
2025-03-06 19:59:37,704 - INFO - validation batch 201, loss: 0.376, 6432/10016 datapoints
2025-03-06 19:59:37,862 - INFO - validation batch 251, loss: 0.243, 8032/10016 datapoints
2025-03-06 19:59:38,051 - INFO - validation batch 301, loss: 0.182, 9632/10016 datapoints
2025-03-06 19:59:38,092 - INFO - Epoch 552/800 done.
2025-03-06 19:59:38,092 - INFO - Final validation performance:
Loss: 0.267, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:59:38,093 - INFO - Beginning epoch 553/800
2025-03-06 19:59:38,100 - INFO - training batch 1, loss: 0.329, 32/60000 datapoints
2025-03-06 19:59:38,329 - INFO - training batch 51, loss: 0.209, 1632/60000 datapoints
2025-03-06 19:59:38,533 - INFO - training batch 101, loss: 0.273, 3232/60000 datapoints
2025-03-06 19:59:38,749 - INFO - training batch 151, loss: 0.252, 4832/60000 datapoints
2025-03-06 19:59:38,963 - INFO - training batch 201, loss: 0.199, 6432/60000 datapoints
2025-03-06 19:59:39,175 - INFO - training batch 251, loss: 0.426, 8032/60000 datapoints
2025-03-06 19:59:39,378 - INFO - training batch 301, loss: 0.206, 9632/60000 datapoints
2025-03-06 19:59:39,598 - INFO - training batch 351, loss: 0.514, 11232/60000 datapoints
2025-03-06 19:59:39,812 - INFO - training batch 401, loss: 0.269, 12832/60000 datapoints
2025-03-06 19:59:40,013 - INFO - training batch 451, loss: 0.184, 14432/60000 datapoints
2025-03-06 19:59:40,250 - INFO - training batch 501, loss: 0.110, 16032/60000 datapoints
2025-03-06 19:59:40,463 - INFO - training batch 551, loss: 0.472, 17632/60000 datapoints
2025-03-06 19:59:40,680 - INFO - training batch 601, loss: 0.287, 19232/60000 datapoints
2025-03-06 19:59:40,883 - INFO - training batch 651, loss: 0.165, 20832/60000 datapoints
2025-03-06 19:59:41,101 - INFO - training batch 701, loss: 0.303, 22432/60000 datapoints
2025-03-06 19:59:41,317 - INFO - training batch 751, loss: 0.393, 24032/60000 datapoints
2025-03-06 19:59:41,529 - INFO - training batch 801, loss: 0.263, 25632/60000 datapoints
2025-03-06 19:59:41,762 - INFO - training batch 851, loss: 0.190, 27232/60000 datapoints
2025-03-06 19:59:41,963 - INFO - training batch 901, loss: 0.203, 28832/60000 datapoints
2025-03-06 19:59:42,158 - INFO - training batch 951, loss: 0.250, 30432/60000 datapoints
2025-03-06 19:59:42,395 - INFO - training batch 1001, loss: 0.201, 32032/60000 datapoints
2025-03-06 19:59:42,782 - INFO - training batch 1051, loss: 0.143, 33632/60000 datapoints
2025-03-06 19:59:43,159 - INFO - training batch 1101, loss: 0.349, 35232/60000 datapoints
2025-03-06 19:59:43,609 - INFO - training batch 1151, loss: 0.203, 36832/60000 datapoints
2025-03-06 19:59:44,073 - INFO - training batch 1201, loss: 0.281, 38432/60000 datapoints
2025-03-06 19:59:44,430 - INFO - training batch 1251, loss: 0.087, 40032/60000 datapoints
2025-03-06 19:59:44,659 - INFO - training batch 1301, loss: 0.340, 41632/60000 datapoints
2025-03-06 19:59:44,884 - INFO - training batch 1351, loss: 0.253, 43232/60000 datapoints
2025-03-06 19:59:45,092 - INFO - training batch 1401, loss: 0.122, 44832/60000 datapoints
2025-03-06 19:59:45,295 - INFO - training batch 1451, loss: 0.220, 46432/60000 datapoints
2025-03-06 19:59:45,500 - INFO - training batch 1501, loss: 0.240, 48032/60000 datapoints
2025-03-06 19:59:45,711 - INFO - training batch 1551, loss: 0.210, 49632/60000 datapoints
2025-03-06 19:59:46,145 - INFO - training batch 1601, loss: 0.462, 51232/60000 datapoints
2025-03-06 19:59:46,415 - INFO - training batch 1651, loss: 0.259, 52832/60000 datapoints
2025-03-06 19:59:46,632 - INFO - training batch 1701, loss: 0.441, 54432/60000 datapoints
2025-03-06 19:59:46,863 - INFO - training batch 1751, loss: 0.190, 56032/60000 datapoints
2025-03-06 19:59:47,086 - INFO - training batch 1801, loss: 0.178, 57632/60000 datapoints
2025-03-06 19:59:47,337 - INFO - training batch 1851, loss: 0.184, 59232/60000 datapoints
2025-03-06 19:59:47,460 - INFO - validation batch 1, loss: 0.337, 32/10016 datapoints
2025-03-06 19:59:47,638 - INFO - validation batch 51, loss: 0.201, 1632/10016 datapoints
2025-03-06 19:59:47,798 - INFO - validation batch 101, loss: 0.124, 3232/10016 datapoints
2025-03-06 19:59:47,975 - INFO - validation batch 151, loss: 0.270, 4832/10016 datapoints
2025-03-06 19:59:48,170 - INFO - validation batch 201, loss: 0.134, 6432/10016 datapoints
2025-03-06 19:59:48,394 - INFO - validation batch 251, loss: 0.190, 8032/10016 datapoints
2025-03-06 19:59:48,570 - INFO - validation batch 301, loss: 0.264, 9632/10016 datapoints
2025-03-06 19:59:48,613 - INFO - Epoch 553/800 done.
2025-03-06 19:59:48,613 - INFO - Final validation performance:
Loss: 0.217, top-1 acc: 0.929top-5 acc: 0.929
2025-03-06 19:59:48,614 - INFO - Beginning epoch 554/800
2025-03-06 19:59:48,621 - INFO - training batch 1, loss: 0.253, 32/60000 datapoints
2025-03-06 19:59:48,846 - INFO - training batch 51, loss: 0.252, 1632/60000 datapoints
2025-03-06 19:59:49,167 - INFO - training batch 101, loss: 0.100, 3232/60000 datapoints
2025-03-06 19:59:49,561 - INFO - training batch 151, loss: 0.326, 4832/60000 datapoints
2025-03-06 19:59:49,893 - INFO - training batch 201, loss: 0.391, 6432/60000 datapoints
2025-03-06 19:59:50,138 - INFO - training batch 251, loss: 0.374, 8032/60000 datapoints
2025-03-06 19:59:50,481 - INFO - training batch 301, loss: 0.174, 9632/60000 datapoints
2025-03-06 19:59:50,695 - INFO - training batch 351, loss: 0.275, 11232/60000 datapoints
2025-03-06 19:59:50,899 - INFO - training batch 401, loss: 0.085, 12832/60000 datapoints
2025-03-06 19:59:51,103 - INFO - training batch 451, loss: 0.354, 14432/60000 datapoints
2025-03-06 19:59:51,505 - INFO - training batch 501, loss: 0.361, 16032/60000 datapoints
2025-03-06 19:59:51,987 - INFO - training batch 551, loss: 0.230, 17632/60000 datapoints
2025-03-06 19:59:52,464 - INFO - training batch 601, loss: 0.074, 19232/60000 datapoints
2025-03-06 19:59:53,080 - INFO - training batch 651, loss: 0.288, 20832/60000 datapoints
2025-03-06 19:59:53,805 - INFO - training batch 701, loss: 0.311, 22432/60000 datapoints
2025-03-06 19:59:54,353 - INFO - training batch 751, loss: 0.208, 24032/60000 datapoints
2025-03-06 19:59:54,595 - INFO - training batch 801, loss: 0.119, 25632/60000 datapoints
2025-03-06 19:59:54,827 - INFO - training batch 851, loss: 0.272, 27232/60000 datapoints
2025-03-06 19:59:55,025 - INFO - training batch 901, loss: 0.188, 28832/60000 datapoints
2025-03-06 19:59:55,247 - INFO - training batch 951, loss: 0.340, 30432/60000 datapoints
2025-03-06 19:59:55,486 - INFO - training batch 1001, loss: 0.172, 32032/60000 datapoints
2025-03-06 19:59:55,841 - INFO - training batch 1051, loss: 0.299, 33632/60000 datapoints
2025-03-06 19:59:56,054 - INFO - training batch 1101, loss: 0.318, 35232/60000 datapoints
2025-03-06 19:59:56,264 - INFO - training batch 1151, loss: 0.306, 36832/60000 datapoints
2025-03-06 19:59:56,470 - INFO - training batch 1201, loss: 0.102, 38432/60000 datapoints
2025-03-06 19:59:56,668 - INFO - training batch 1251, loss: 0.084, 40032/60000 datapoints
2025-03-06 19:59:56,865 - INFO - training batch 1301, loss: 0.407, 41632/60000 datapoints
2025-03-06 19:59:57,058 - INFO - training batch 1351, loss: 0.140, 43232/60000 datapoints
2025-03-06 19:59:57,257 - INFO - training batch 1401, loss: 0.421, 44832/60000 datapoints
2025-03-06 19:59:57,452 - INFO - training batch 1451, loss: 0.090, 46432/60000 datapoints
2025-03-06 19:59:57,676 - INFO - training batch 1501, loss: 0.171, 48032/60000 datapoints
2025-03-06 19:59:57,874 - INFO - training batch 1551, loss: 0.324, 49632/60000 datapoints
2025-03-06 19:59:58,071 - INFO - training batch 1601, loss: 0.177, 51232/60000 datapoints
2025-03-06 19:59:58,281 - INFO - training batch 1651, loss: 0.166, 52832/60000 datapoints
2025-03-06 19:59:58,478 - INFO - training batch 1701, loss: 0.729, 54432/60000 datapoints
2025-03-06 19:59:58,681 - INFO - training batch 1751, loss: 0.262, 56032/60000 datapoints
2025-03-06 19:59:58,879 - INFO - training batch 1801, loss: 0.513, 57632/60000 datapoints
2025-03-06 19:59:59,075 - INFO - training batch 1851, loss: 0.275, 59232/60000 datapoints
2025-03-06 19:59:59,178 - INFO - validation batch 1, loss: 0.221, 32/10016 datapoints
2025-03-06 19:59:59,336 - INFO - validation batch 51, loss: 0.585, 1632/10016 datapoints
2025-03-06 19:59:59,489 - INFO - validation batch 101, loss: 0.070, 3232/10016 datapoints
2025-03-06 19:59:59,654 - INFO - validation batch 151, loss: 0.126, 4832/10016 datapoints
2025-03-06 19:59:59,813 - INFO - validation batch 201, loss: 0.169, 6432/10016 datapoints
2025-03-06 19:59:59,965 - INFO - validation batch 251, loss: 0.145, 8032/10016 datapoints
2025-03-06 20:00:00,132 - INFO - validation batch 301, loss: 0.108, 9632/10016 datapoints
2025-03-06 20:00:00,172 - INFO - Epoch 554/800 done.
2025-03-06 20:00:00,172 - INFO - Final validation performance:
Loss: 0.203, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:00:00,173 - INFO - Beginning epoch 555/800
2025-03-06 20:00:00,183 - INFO - training batch 1, loss: 0.152, 32/60000 datapoints
2025-03-06 20:00:00,402 - INFO - training batch 51, loss: 0.367, 1632/60000 datapoints
2025-03-06 20:00:00,604 - INFO - training batch 101, loss: 0.299, 3232/60000 datapoints
2025-03-06 20:00:00,813 - INFO - training batch 151, loss: 0.299, 4832/60000 datapoints
2025-03-06 20:00:01,009 - INFO - training batch 201, loss: 0.225, 6432/60000 datapoints
2025-03-06 20:00:01,217 - INFO - training batch 251, loss: 0.310, 8032/60000 datapoints
2025-03-06 20:00:01,412 - INFO - training batch 301, loss: 0.187, 9632/60000 datapoints
2025-03-06 20:00:01,614 - INFO - training batch 351, loss: 0.223, 11232/60000 datapoints
2025-03-06 20:00:01,816 - INFO - training batch 401, loss: 0.260, 12832/60000 datapoints
2025-03-06 20:00:02,032 - INFO - training batch 451, loss: 0.247, 14432/60000 datapoints
2025-03-06 20:00:02,229 - INFO - training batch 501, loss: 0.299, 16032/60000 datapoints
2025-03-06 20:00:02,431 - INFO - training batch 551, loss: 0.431, 17632/60000 datapoints
2025-03-06 20:00:02,629 - INFO - training batch 601, loss: 0.510, 19232/60000 datapoints
2025-03-06 20:00:02,828 - INFO - training batch 651, loss: 0.120, 20832/60000 datapoints
2025-03-06 20:00:03,033 - INFO - training batch 701, loss: 0.182, 22432/60000 datapoints
2025-03-06 20:00:03,230 - INFO - training batch 751, loss: 0.821, 24032/60000 datapoints
2025-03-06 20:00:03,443 - INFO - training batch 801, loss: 0.308, 25632/60000 datapoints
2025-03-06 20:00:03,651 - INFO - training batch 851, loss: 0.270, 27232/60000 datapoints
2025-03-06 20:00:03,855 - INFO - training batch 901, loss: 0.272, 28832/60000 datapoints
2025-03-06 20:00:04,053 - INFO - training batch 951, loss: 0.308, 30432/60000 datapoints
2025-03-06 20:00:04,271 - INFO - training batch 1001, loss: 0.393, 32032/60000 datapoints
2025-03-06 20:00:04,472 - INFO - training batch 1051, loss: 0.370, 33632/60000 datapoints
2025-03-06 20:00:04,674 - INFO - training batch 1101, loss: 0.437, 35232/60000 datapoints
2025-03-06 20:00:04,880 - INFO - training batch 1151, loss: 0.117, 36832/60000 datapoints
2025-03-06 20:00:05,075 - INFO - training batch 1201, loss: 0.238, 38432/60000 datapoints
2025-03-06 20:00:05,273 - INFO - training batch 1251, loss: 0.289, 40032/60000 datapoints
2025-03-06 20:00:05,634 - INFO - training batch 1301, loss: 0.105, 41632/60000 datapoints
2025-03-06 20:00:06,051 - INFO - training batch 1351, loss: 0.452, 43232/60000 datapoints
2025-03-06 20:00:06,468 - INFO - training batch 1401, loss: 0.096, 44832/60000 datapoints
2025-03-06 20:00:06,871 - INFO - training batch 1451, loss: 0.106, 46432/60000 datapoints
2025-03-06 20:00:07,266 - INFO - training batch 1501, loss: 0.467, 48032/60000 datapoints
2025-03-06 20:00:07,501 - INFO - training batch 1551, loss: 0.195, 49632/60000 datapoints
2025-03-06 20:00:07,706 - INFO - training batch 1601, loss: 0.199, 51232/60000 datapoints
2025-03-06 20:00:07,908 - INFO - training batch 1651, loss: 0.462, 52832/60000 datapoints
2025-03-06 20:00:08,104 - INFO - training batch 1701, loss: 0.154, 54432/60000 datapoints
2025-03-06 20:00:08,302 - INFO - training batch 1751, loss: 0.104, 56032/60000 datapoints
2025-03-06 20:00:08,499 - INFO - training batch 1801, loss: 0.266, 57632/60000 datapoints
2025-03-06 20:00:08,703 - INFO - training batch 1851, loss: 0.359, 59232/60000 datapoints
2025-03-06 20:00:08,808 - INFO - validation batch 1, loss: 0.195, 32/10016 datapoints
2025-03-06 20:00:08,968 - INFO - validation batch 51, loss: 0.186, 1632/10016 datapoints
2025-03-06 20:00:09,130 - INFO - validation batch 101, loss: 0.078, 3232/10016 datapoints
2025-03-06 20:00:09,296 - INFO - validation batch 151, loss: 0.046, 4832/10016 datapoints
2025-03-06 20:00:09,455 - INFO - validation batch 201, loss: 0.358, 6432/10016 datapoints
2025-03-06 20:00:09,616 - INFO - validation batch 251, loss: 0.188, 8032/10016 datapoints
2025-03-06 20:00:09,790 - INFO - validation batch 301, loss: 0.116, 9632/10016 datapoints
2025-03-06 20:00:09,831 - INFO - Epoch 555/800 done.
2025-03-06 20:00:09,831 - INFO - Final validation performance:
Loss: 0.167, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:00:09,831 - INFO - Beginning epoch 556/800
2025-03-06 20:00:09,838 - INFO - training batch 1, loss: 0.194, 32/60000 datapoints
2025-03-06 20:00:10,056 - INFO - training batch 51, loss: 0.315, 1632/60000 datapoints
2025-03-06 20:00:10,266 - INFO - training batch 101, loss: 0.373, 3232/60000 datapoints
2025-03-06 20:00:10,474 - INFO - training batch 151, loss: 0.223, 4832/60000 datapoints
2025-03-06 20:00:10,685 - INFO - training batch 201, loss: 0.289, 6432/60000 datapoints
2025-03-06 20:00:10,891 - INFO - training batch 251, loss: 0.136, 8032/60000 datapoints
2025-03-06 20:00:11,089 - INFO - training batch 301, loss: 0.623, 9632/60000 datapoints
2025-03-06 20:00:11,306 - INFO - training batch 351, loss: 0.190, 11232/60000 datapoints
2025-03-06 20:00:11,503 - INFO - training batch 401, loss: 0.198, 12832/60000 datapoints
2025-03-06 20:00:11,705 - INFO - training batch 451, loss: 0.220, 14432/60000 datapoints
2025-03-06 20:00:11,903 - INFO - training batch 501, loss: 0.178, 16032/60000 datapoints
2025-03-06 20:00:12,123 - INFO - training batch 551, loss: 0.338, 17632/60000 datapoints
2025-03-06 20:00:12,327 - INFO - training batch 601, loss: 0.139, 19232/60000 datapoints
2025-03-06 20:00:12,525 - INFO - training batch 651, loss: 0.566, 20832/60000 datapoints
2025-03-06 20:00:12,725 - INFO - training batch 701, loss: 0.072, 22432/60000 datapoints
2025-03-06 20:00:12,923 - INFO - training batch 751, loss: 0.099, 24032/60000 datapoints
2025-03-06 20:00:13,118 - INFO - training batch 801, loss: 0.410, 25632/60000 datapoints
2025-03-06 20:00:13,319 - INFO - training batch 851, loss: 0.151, 27232/60000 datapoints
2025-03-06 20:00:13,521 - INFO - training batch 901, loss: 0.148, 28832/60000 datapoints
2025-03-06 20:00:13,722 - INFO - training batch 951, loss: 0.131, 30432/60000 datapoints
2025-03-06 20:00:13,918 - INFO - training batch 1001, loss: 0.348, 32032/60000 datapoints
2025-03-06 20:00:14,113 - INFO - training batch 1051, loss: 0.221, 33632/60000 datapoints
2025-03-06 20:00:14,311 - INFO - training batch 1101, loss: 0.358, 35232/60000 datapoints
2025-03-06 20:00:14,506 - INFO - training batch 1151, loss: 0.330, 36832/60000 datapoints
2025-03-06 20:00:14,703 - INFO - training batch 1201, loss: 0.440, 38432/60000 datapoints
2025-03-06 20:00:14,908 - INFO - training batch 1251, loss: 0.337, 40032/60000 datapoints
2025-03-06 20:00:15,102 - INFO - training batch 1301, loss: 0.059, 41632/60000 datapoints
2025-03-06 20:00:15,300 - INFO - training batch 1351, loss: 0.402, 43232/60000 datapoints
2025-03-06 20:00:15,499 - INFO - training batch 1401, loss: 0.193, 44832/60000 datapoints
2025-03-06 20:00:15,697 - INFO - training batch 1451, loss: 0.199, 46432/60000 datapoints
2025-03-06 20:00:15,895 - INFO - training batch 1501, loss: 0.136, 48032/60000 datapoints
2025-03-06 20:00:16,090 - INFO - training batch 1551, loss: 0.267, 49632/60000 datapoints
2025-03-06 20:00:16,290 - INFO - training batch 1601, loss: 0.210, 51232/60000 datapoints
2025-03-06 20:00:16,489 - INFO - training batch 1651, loss: 0.151, 52832/60000 datapoints
2025-03-06 20:00:16,689 - INFO - training batch 1701, loss: 0.149, 54432/60000 datapoints
2025-03-06 20:00:16,885 - INFO - training batch 1751, loss: 0.025, 56032/60000 datapoints
2025-03-06 20:00:17,080 - INFO - training batch 1801, loss: 0.149, 57632/60000 datapoints
2025-03-06 20:00:17,275 - INFO - training batch 1851, loss: 0.536, 59232/60000 datapoints
2025-03-06 20:00:17,387 - INFO - validation batch 1, loss: 0.189, 32/10016 datapoints
2025-03-06 20:00:17,545 - INFO - validation batch 51, loss: 0.080, 1632/10016 datapoints
2025-03-06 20:00:17,703 - INFO - validation batch 101, loss: 0.140, 3232/10016 datapoints
2025-03-06 20:00:17,863 - INFO - validation batch 151, loss: 0.261, 4832/10016 datapoints
2025-03-06 20:00:18,021 - INFO - validation batch 201, loss: 0.291, 6432/10016 datapoints
2025-03-06 20:00:18,182 - INFO - validation batch 251, loss: 0.444, 8032/10016 datapoints
2025-03-06 20:00:18,340 - INFO - validation batch 301, loss: 0.169, 9632/10016 datapoints
2025-03-06 20:00:18,380 - INFO - Epoch 556/800 done.
2025-03-06 20:00:18,381 - INFO - Final validation performance:
Loss: 0.225, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:00:18,381 - INFO - Beginning epoch 557/800
2025-03-06 20:00:18,388 - INFO - training batch 1, loss: 0.267, 32/60000 datapoints
2025-03-06 20:00:18,604 - INFO - training batch 51, loss: 0.240, 1632/60000 datapoints
2025-03-06 20:00:18,806 - INFO - training batch 101, loss: 0.075, 3232/60000 datapoints
2025-03-06 20:00:19,003 - INFO - training batch 151, loss: 0.145, 4832/60000 datapoints
2025-03-06 20:00:19,206 - INFO - training batch 201, loss: 0.247, 6432/60000 datapoints
2025-03-06 20:00:19,406 - INFO - training batch 251, loss: 0.118, 8032/60000 datapoints
2025-03-06 20:00:19,606 - INFO - training batch 301, loss: 0.350, 9632/60000 datapoints
2025-03-06 20:00:19,808 - INFO - training batch 351, loss: 0.239, 11232/60000 datapoints
2025-03-06 20:00:20,002 - INFO - training batch 401, loss: 0.160, 12832/60000 datapoints
2025-03-06 20:00:20,202 - INFO - training batch 451, loss: 0.339, 14432/60000 datapoints
2025-03-06 20:00:20,404 - INFO - training batch 501, loss: 0.170, 16032/60000 datapoints
2025-03-06 20:00:20,600 - INFO - training batch 551, loss: 0.141, 17632/60000 datapoints
2025-03-06 20:00:20,797 - INFO - training batch 601, loss: 0.278, 19232/60000 datapoints
2025-03-06 20:00:20,993 - INFO - training batch 651, loss: 0.211, 20832/60000 datapoints
2025-03-06 20:00:21,187 - INFO - training batch 701, loss: 0.381, 22432/60000 datapoints
2025-03-06 20:00:21,386 - INFO - training batch 751, loss: 0.532, 24032/60000 datapoints
2025-03-06 20:00:21,583 - INFO - training batch 801, loss: 0.079, 25632/60000 datapoints
2025-03-06 20:00:21,781 - INFO - training batch 851, loss: 0.107, 27232/60000 datapoints
2025-03-06 20:00:21,981 - INFO - training batch 901, loss: 0.124, 28832/60000 datapoints
2025-03-06 20:00:22,206 - INFO - training batch 951, loss: 0.470, 30432/60000 datapoints
2025-03-06 20:00:22,408 - INFO - training batch 1001, loss: 0.316, 32032/60000 datapoints
2025-03-06 20:00:22,608 - INFO - training batch 1051, loss: 0.167, 33632/60000 datapoints
2025-03-06 20:00:22,811 - INFO - training batch 1101, loss: 0.091, 35232/60000 datapoints
2025-03-06 20:00:23,010 - INFO - training batch 1151, loss: 0.035, 36832/60000 datapoints
2025-03-06 20:00:23,204 - INFO - training batch 1201, loss: 0.132, 38432/60000 datapoints
2025-03-06 20:00:23,403 - INFO - training batch 1251, loss: 0.308, 40032/60000 datapoints
2025-03-06 20:00:23,599 - INFO - training batch 1301, loss: 0.190, 41632/60000 datapoints
2025-03-06 20:00:23,801 - INFO - training batch 1351, loss: 0.449, 43232/60000 datapoints
2025-03-06 20:00:23,997 - INFO - training batch 1401, loss: 0.599, 44832/60000 datapoints
2025-03-06 20:00:24,193 - INFO - training batch 1451, loss: 0.315, 46432/60000 datapoints
2025-03-06 20:00:24,392 - INFO - training batch 1501, loss: 0.304, 48032/60000 datapoints
2025-03-06 20:00:24,588 - INFO - training batch 1551, loss: 0.234, 49632/60000 datapoints
2025-03-06 20:00:24,786 - INFO - training batch 1601, loss: 0.259, 51232/60000 datapoints
2025-03-06 20:00:24,987 - INFO - training batch 1651, loss: 0.082, 52832/60000 datapoints
2025-03-06 20:00:25,185 - INFO - training batch 1701, loss: 0.366, 54432/60000 datapoints
2025-03-06 20:00:25,386 - INFO - training batch 1751, loss: 0.366, 56032/60000 datapoints
2025-03-06 20:00:25,583 - INFO - training batch 1801, loss: 0.133, 57632/60000 datapoints
2025-03-06 20:00:25,783 - INFO - training batch 1851, loss: 0.484, 59232/60000 datapoints
2025-03-06 20:00:25,889 - INFO - validation batch 1, loss: 0.334, 32/10016 datapoints
2025-03-06 20:00:26,046 - INFO - validation batch 51, loss: 0.311, 1632/10016 datapoints
2025-03-06 20:00:26,203 - INFO - validation batch 101, loss: 0.303, 3232/10016 datapoints
2025-03-06 20:00:26,364 - INFO - validation batch 151, loss: 0.360, 4832/10016 datapoints
2025-03-06 20:00:26,525 - INFO - validation batch 201, loss: 0.459, 6432/10016 datapoints
2025-03-06 20:00:26,684 - INFO - validation batch 251, loss: 0.064, 8032/10016 datapoints
2025-03-06 20:00:26,841 - INFO - validation batch 301, loss: 0.102, 9632/10016 datapoints
2025-03-06 20:00:26,883 - INFO - Epoch 557/800 done.
2025-03-06 20:00:26,883 - INFO - Final validation performance:
Loss: 0.276, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:00:26,884 - INFO - Beginning epoch 558/800
2025-03-06 20:00:26,890 - INFO - training batch 1, loss: 0.148, 32/60000 datapoints
2025-03-06 20:00:27,098 - INFO - training batch 51, loss: 0.173, 1632/60000 datapoints
2025-03-06 20:00:27,299 - INFO - training batch 101, loss: 0.133, 3232/60000 datapoints
2025-03-06 20:00:27,506 - INFO - training batch 151, loss: 0.090, 4832/60000 datapoints
2025-03-06 20:00:27,709 - INFO - training batch 201, loss: 0.088, 6432/60000 datapoints
2025-03-06 20:00:27,918 - INFO - training batch 251, loss: 0.227, 8032/60000 datapoints
2025-03-06 20:00:28,121 - INFO - training batch 301, loss: 0.213, 9632/60000 datapoints
2025-03-06 20:00:28,323 - INFO - training batch 351, loss: 0.486, 11232/60000 datapoints
2025-03-06 20:00:28,524 - INFO - training batch 401, loss: 0.431, 12832/60000 datapoints
2025-03-06 20:00:28,722 - INFO - training batch 451, loss: 0.147, 14432/60000 datapoints
2025-03-06 20:00:28,924 - INFO - training batch 501, loss: 0.316, 16032/60000 datapoints
2025-03-06 20:00:29,120 - INFO - training batch 551, loss: 0.378, 17632/60000 datapoints
2025-03-06 20:00:29,321 - INFO - training batch 601, loss: 0.258, 19232/60000 datapoints
2025-03-06 20:00:29,520 - INFO - training batch 651, loss: 0.250, 20832/60000 datapoints
2025-03-06 20:00:29,721 - INFO - training batch 701, loss: 0.165, 22432/60000 datapoints
2025-03-06 20:00:29,939 - INFO - training batch 751, loss: 0.403, 24032/60000 datapoints
2025-03-06 20:00:30,137 - INFO - training batch 801, loss: 0.147, 25632/60000 datapoints
2025-03-06 20:00:30,341 - INFO - training batch 851, loss: 0.426, 27232/60000 datapoints
2025-03-06 20:00:30,552 - INFO - training batch 901, loss: 0.115, 28832/60000 datapoints
2025-03-06 20:00:30,749 - INFO - training batch 951, loss: 0.297, 30432/60000 datapoints
2025-03-06 20:00:30,948 - INFO - training batch 1001, loss: 0.060, 32032/60000 datapoints
2025-03-06 20:00:31,142 - INFO - training batch 1051, loss: 0.486, 33632/60000 datapoints
2025-03-06 20:00:31,336 - INFO - training batch 1101, loss: 0.273, 35232/60000 datapoints
2025-03-06 20:00:31,532 - INFO - training batch 1151, loss: 0.184, 36832/60000 datapoints
2025-03-06 20:00:31,731 - INFO - training batch 1201, loss: 0.258, 38432/60000 datapoints
2025-03-06 20:00:31,931 - INFO - training batch 1251, loss: 0.081, 40032/60000 datapoints
2025-03-06 20:00:32,131 - INFO - training batch 1301, loss: 0.457, 41632/60000 datapoints
2025-03-06 20:00:32,344 - INFO - training batch 1351, loss: 0.174, 43232/60000 datapoints
2025-03-06 20:00:32,540 - INFO - training batch 1401, loss: 0.153, 44832/60000 datapoints
2025-03-06 20:00:32,739 - INFO - training batch 1451, loss: 0.278, 46432/60000 datapoints
2025-03-06 20:00:32,938 - INFO - training batch 1501, loss: 0.209, 48032/60000 datapoints
2025-03-06 20:00:33,134 - INFO - training batch 1551, loss: 0.172, 49632/60000 datapoints
2025-03-06 20:00:33,327 - INFO - training batch 1601, loss: 0.143, 51232/60000 datapoints
2025-03-06 20:00:33,523 - INFO - training batch 1651, loss: 0.242, 52832/60000 datapoints
2025-03-06 20:00:33,721 - INFO - training batch 1701, loss: 0.200, 54432/60000 datapoints
2025-03-06 20:00:33,921 - INFO - training batch 1751, loss: 0.126, 56032/60000 datapoints
2025-03-06 20:00:34,118 - INFO - training batch 1801, loss: 0.370, 57632/60000 datapoints
2025-03-06 20:00:34,312 - INFO - training batch 1851, loss: 0.213, 59232/60000 datapoints
2025-03-06 20:00:34,415 - INFO - validation batch 1, loss: 0.498, 32/10016 datapoints
2025-03-06 20:00:34,570 - INFO - validation batch 51, loss: 0.217, 1632/10016 datapoints
2025-03-06 20:00:34,725 - INFO - validation batch 101, loss: 0.170, 3232/10016 datapoints
2025-03-06 20:00:34,884 - INFO - validation batch 151, loss: 0.572, 4832/10016 datapoints
2025-03-06 20:00:35,039 - INFO - validation batch 201, loss: 0.070, 6432/10016 datapoints
2025-03-06 20:00:35,191 - INFO - validation batch 251, loss: 0.233, 8032/10016 datapoints
2025-03-06 20:00:35,344 - INFO - validation batch 301, loss: 0.231, 9632/10016 datapoints
2025-03-06 20:00:35,383 - INFO - Epoch 558/800 done.
2025-03-06 20:00:35,383 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:00:35,384 - INFO - Beginning epoch 559/800
2025-03-06 20:00:35,390 - INFO - training batch 1, loss: 0.175, 32/60000 datapoints
2025-03-06 20:00:35,603 - INFO - training batch 51, loss: 0.326, 1632/60000 datapoints
2025-03-06 20:00:35,799 - INFO - training batch 101, loss: 0.379, 3232/60000 datapoints
2025-03-06 20:00:36,006 - INFO - training batch 151, loss: 0.234, 4832/60000 datapoints
2025-03-06 20:00:36,202 - INFO - training batch 201, loss: 0.287, 6432/60000 datapoints
2025-03-06 20:00:36,408 - INFO - training batch 251, loss: 0.133, 8032/60000 datapoints
2025-03-06 20:00:36,604 - INFO - training batch 301, loss: 0.097, 9632/60000 datapoints
2025-03-06 20:00:36,801 - INFO - training batch 351, loss: 0.083, 11232/60000 datapoints
2025-03-06 20:00:36,998 - INFO - training batch 401, loss: 0.240, 12832/60000 datapoints
2025-03-06 20:00:37,192 - INFO - training batch 451, loss: 0.140, 14432/60000 datapoints
2025-03-06 20:00:37,385 - INFO - training batch 501, loss: 0.161, 16032/60000 datapoints
2025-03-06 20:00:37,585 - INFO - training batch 551, loss: 0.631, 17632/60000 datapoints
2025-03-06 20:00:37,827 - INFO - training batch 601, loss: 0.152, 19232/60000 datapoints
2025-03-06 20:00:38,029 - INFO - training batch 651, loss: 0.251, 20832/60000 datapoints
2025-03-06 20:00:38,227 - INFO - training batch 701, loss: 0.159, 22432/60000 datapoints
2025-03-06 20:00:38,425 - INFO - training batch 751, loss: 0.217, 24032/60000 datapoints
2025-03-06 20:00:38,622 - INFO - training batch 801, loss: 0.266, 25632/60000 datapoints
2025-03-06 20:00:38,820 - INFO - training batch 851, loss: 0.167, 27232/60000 datapoints
2025-03-06 20:00:39,021 - INFO - training batch 901, loss: 0.447, 28832/60000 datapoints
2025-03-06 20:00:39,216 - INFO - training batch 951, loss: 0.225, 30432/60000 datapoints
2025-03-06 20:00:39,410 - INFO - training batch 1001, loss: 0.233, 32032/60000 datapoints
2025-03-06 20:00:39,619 - INFO - training batch 1051, loss: 0.099, 33632/60000 datapoints
2025-03-06 20:00:39,815 - INFO - training batch 1101, loss: 0.147, 35232/60000 datapoints
2025-03-06 20:00:40,012 - INFO - training batch 1151, loss: 0.218, 36832/60000 datapoints
2025-03-06 20:00:40,213 - INFO - training batch 1201, loss: 0.108, 38432/60000 datapoints
2025-03-06 20:00:40,414 - INFO - training batch 1251, loss: 0.317, 40032/60000 datapoints
2025-03-06 20:00:40,640 - INFO - training batch 1301, loss: 0.121, 41632/60000 datapoints
2025-03-06 20:00:40,843 - INFO - training batch 1351, loss: 0.116, 43232/60000 datapoints
2025-03-06 20:00:41,039 - INFO - training batch 1401, loss: 0.148, 44832/60000 datapoints
2025-03-06 20:00:41,234 - INFO - training batch 1451, loss: 0.293, 46432/60000 datapoints
2025-03-06 20:00:41,429 - INFO - training batch 1501, loss: 0.249, 48032/60000 datapoints
2025-03-06 20:00:41,629 - INFO - training batch 1551, loss: 0.144, 49632/60000 datapoints
2025-03-06 20:00:41,825 - INFO - training batch 1601, loss: 0.177, 51232/60000 datapoints
2025-03-06 20:00:42,021 - INFO - training batch 1651, loss: 0.174, 52832/60000 datapoints
2025-03-06 20:00:42,217 - INFO - training batch 1701, loss: 0.251, 54432/60000 datapoints
2025-03-06 20:00:42,435 - INFO - training batch 1751, loss: 0.215, 56032/60000 datapoints
2025-03-06 20:00:42,633 - INFO - training batch 1801, loss: 0.255, 57632/60000 datapoints
2025-03-06 20:00:42,828 - INFO - training batch 1851, loss: 0.095, 59232/60000 datapoints
2025-03-06 20:00:42,932 - INFO - validation batch 1, loss: 0.086, 32/10016 datapoints
2025-03-06 20:00:43,097 - INFO - validation batch 51, loss: 0.296, 1632/10016 datapoints
2025-03-06 20:00:43,251 - INFO - validation batch 101, loss: 0.302, 3232/10016 datapoints
2025-03-06 20:00:43,405 - INFO - validation batch 151, loss: 0.284, 4832/10016 datapoints
2025-03-06 20:00:43,559 - INFO - validation batch 201, loss: 0.097, 6432/10016 datapoints
2025-03-06 20:00:43,717 - INFO - validation batch 251, loss: 0.122, 8032/10016 datapoints
2025-03-06 20:00:43,871 - INFO - validation batch 301, loss: 0.410, 9632/10016 datapoints
2025-03-06 20:00:43,907 - INFO - Epoch 559/800 done.
2025-03-06 20:00:43,907 - INFO - Final validation performance:
Loss: 0.228, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:00:43,908 - INFO - Beginning epoch 560/800
2025-03-06 20:00:43,915 - INFO - training batch 1, loss: 0.279, 32/60000 datapoints
2025-03-06 20:00:44,117 - INFO - training batch 51, loss: 0.105, 1632/60000 datapoints
2025-03-06 20:00:44,309 - INFO - training batch 101, loss: 0.438, 3232/60000 datapoints
2025-03-06 20:00:44,515 - INFO - training batch 151, loss: 0.281, 4832/60000 datapoints
2025-03-06 20:00:44,712 - INFO - training batch 201, loss: 0.173, 6432/60000 datapoints
2025-03-06 20:00:44,910 - INFO - training batch 251, loss: 0.174, 8032/60000 datapoints
2025-03-06 20:00:45,110 - INFO - training batch 301, loss: 0.525, 9632/60000 datapoints
2025-03-06 20:00:45,309 - INFO - training batch 351, loss: 0.142, 11232/60000 datapoints
2025-03-06 20:00:45,506 - INFO - training batch 401, loss: 0.282, 12832/60000 datapoints
2025-03-06 20:00:45,734 - INFO - training batch 451, loss: 0.377, 14432/60000 datapoints
2025-03-06 20:00:45,969 - INFO - training batch 501, loss: 0.121, 16032/60000 datapoints
2025-03-06 20:00:46,166 - INFO - training batch 551, loss: 0.122, 17632/60000 datapoints
2025-03-06 20:00:46,363 - INFO - training batch 601, loss: 0.191, 19232/60000 datapoints
2025-03-06 20:00:46,561 - INFO - training batch 651, loss: 0.455, 20832/60000 datapoints
2025-03-06 20:00:46,761 - INFO - training batch 701, loss: 0.449, 22432/60000 datapoints
2025-03-06 20:00:46,956 - INFO - training batch 751, loss: 0.304, 24032/60000 datapoints
2025-03-06 20:00:47,154 - INFO - training batch 801, loss: 0.101, 25632/60000 datapoints
2025-03-06 20:00:47,349 - INFO - training batch 851, loss: 0.498, 27232/60000 datapoints
2025-03-06 20:00:47,545 - INFO - training batch 901, loss: 0.540, 28832/60000 datapoints
2025-03-06 20:00:47,745 - INFO - training batch 951, loss: 0.182, 30432/60000 datapoints
2025-03-06 20:00:47,944 - INFO - training batch 1001, loss: 0.552, 32032/60000 datapoints
2025-03-06 20:00:48,143 - INFO - training batch 1051, loss: 0.407, 33632/60000 datapoints
2025-03-06 20:00:48,339 - INFO - training batch 1101, loss: 0.437, 35232/60000 datapoints
2025-03-06 20:00:48,538 - INFO - training batch 1151, loss: 0.128, 36832/60000 datapoints
2025-03-06 20:00:48,735 - INFO - training batch 1201, loss: 0.293, 38432/60000 datapoints
2025-03-06 20:00:48,936 - INFO - training batch 1251, loss: 0.288, 40032/60000 datapoints
2025-03-06 20:00:49,151 - INFO - training batch 1301, loss: 0.142, 41632/60000 datapoints
2025-03-06 20:00:49,361 - INFO - training batch 1351, loss: 0.127, 43232/60000 datapoints
2025-03-06 20:00:49,575 - INFO - training batch 1401, loss: 0.644, 44832/60000 datapoints
2025-03-06 20:00:49,808 - INFO - training batch 1451, loss: 0.233, 46432/60000 datapoints
2025-03-06 20:00:50,038 - INFO - training batch 1501, loss: 0.633, 48032/60000 datapoints
2025-03-06 20:00:50,272 - INFO - training batch 1551, loss: 0.285, 49632/60000 datapoints
2025-03-06 20:00:50,484 - INFO - training batch 1601, loss: 0.283, 51232/60000 datapoints
2025-03-06 20:00:50,696 - INFO - training batch 1651, loss: 0.242, 52832/60000 datapoints
2025-03-06 20:00:50,893 - INFO - training batch 1701, loss: 0.345, 54432/60000 datapoints
2025-03-06 20:00:51,093 - INFO - training batch 1751, loss: 0.110, 56032/60000 datapoints
2025-03-06 20:00:51,301 - INFO - training batch 1801, loss: 0.134, 57632/60000 datapoints
2025-03-06 20:00:51,546 - INFO - training batch 1851, loss: 0.079, 59232/60000 datapoints
2025-03-06 20:00:51,654 - INFO - validation batch 1, loss: 0.163, 32/10016 datapoints
2025-03-06 20:00:51,811 - INFO - validation batch 51, loss: 0.226, 1632/10016 datapoints
2025-03-06 20:00:51,980 - INFO - validation batch 101, loss: 0.086, 3232/10016 datapoints
2025-03-06 20:00:52,139 - INFO - validation batch 151, loss: 0.272, 4832/10016 datapoints
2025-03-06 20:00:52,296 - INFO - validation batch 201, loss: 0.184, 6432/10016 datapoints
2025-03-06 20:00:52,475 - INFO - validation batch 251, loss: 0.300, 8032/10016 datapoints
2025-03-06 20:00:52,635 - INFO - validation batch 301, loss: 0.275, 9632/10016 datapoints
2025-03-06 20:00:52,673 - INFO - Epoch 560/800 done.
2025-03-06 20:00:52,674 - INFO - Final validation performance:
Loss: 0.215, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:00:52,674 - INFO - Beginning epoch 561/800
2025-03-06 20:00:52,682 - INFO - training batch 1, loss: 0.098, 32/60000 datapoints
2025-03-06 20:00:52,895 - INFO - training batch 51, loss: 0.156, 1632/60000 datapoints
2025-03-06 20:00:53,096 - INFO - training batch 101, loss: 0.149, 3232/60000 datapoints
2025-03-06 20:00:53,307 - INFO - training batch 151, loss: 0.242, 4832/60000 datapoints
2025-03-06 20:00:53,511 - INFO - training batch 201, loss: 0.237, 6432/60000 datapoints
2025-03-06 20:00:53,731 - INFO - training batch 251, loss: 0.192, 8032/60000 datapoints
2025-03-06 20:00:53,936 - INFO - training batch 301, loss: 0.287, 9632/60000 datapoints
2025-03-06 20:00:54,139 - INFO - training batch 351, loss: 0.325, 11232/60000 datapoints
2025-03-06 20:00:54,343 - INFO - training batch 401, loss: 0.150, 12832/60000 datapoints
2025-03-06 20:00:54,547 - INFO - training batch 451, loss: 0.404, 14432/60000 datapoints
2025-03-06 20:00:54,754 - INFO - training batch 501, loss: 0.097, 16032/60000 datapoints
2025-03-06 20:00:54,961 - INFO - training batch 551, loss: 0.166, 17632/60000 datapoints
2025-03-06 20:00:55,163 - INFO - training batch 601, loss: 0.504, 19232/60000 datapoints
2025-03-06 20:00:55,364 - INFO - training batch 651, loss: 0.150, 20832/60000 datapoints
2025-03-06 20:00:55,564 - INFO - training batch 701, loss: 0.138, 22432/60000 datapoints
2025-03-06 20:00:55,770 - INFO - training batch 751, loss: 0.186, 24032/60000 datapoints
2025-03-06 20:00:55,973 - INFO - training batch 801, loss: 0.381, 25632/60000 datapoints
2025-03-06 20:00:56,184 - INFO - training batch 851, loss: 0.100, 27232/60000 datapoints
2025-03-06 20:00:56,397 - INFO - training batch 901, loss: 0.186, 28832/60000 datapoints
2025-03-06 20:00:56,607 - INFO - training batch 951, loss: 0.436, 30432/60000 datapoints
2025-03-06 20:00:56,814 - INFO - training batch 1001, loss: 0.275, 32032/60000 datapoints
2025-03-06 20:00:57,017 - INFO - training batch 1051, loss: 0.283, 33632/60000 datapoints
2025-03-06 20:00:57,222 - INFO - training batch 1101, loss: 0.159, 35232/60000 datapoints
2025-03-06 20:00:57,430 - INFO - training batch 1151, loss: 0.257, 36832/60000 datapoints
2025-03-06 20:00:57,639 - INFO - training batch 1201, loss: 0.597, 38432/60000 datapoints
2025-03-06 20:00:57,850 - INFO - training batch 1251, loss: 0.133, 40032/60000 datapoints
2025-03-06 20:00:58,061 - INFO - training batch 1301, loss: 0.341, 41632/60000 datapoints
2025-03-06 20:00:58,282 - INFO - training batch 1351, loss: 0.131, 43232/60000 datapoints
2025-03-06 20:00:58,502 - INFO - training batch 1401, loss: 0.259, 44832/60000 datapoints
2025-03-06 20:00:58,787 - INFO - training batch 1451, loss: 0.247, 46432/60000 datapoints
2025-03-06 20:00:59,026 - INFO - training batch 1501, loss: 0.115, 48032/60000 datapoints
2025-03-06 20:00:59,247 - INFO - training batch 1551, loss: 0.092, 49632/60000 datapoints
2025-03-06 20:00:59,468 - INFO - training batch 1601, loss: 0.065, 51232/60000 datapoints
2025-03-06 20:00:59,702 - INFO - training batch 1651, loss: 0.239, 52832/60000 datapoints
2025-03-06 20:00:59,933 - INFO - training batch 1701, loss: 0.164, 54432/60000 datapoints
2025-03-06 20:01:00,158 - INFO - training batch 1751, loss: 0.246, 56032/60000 datapoints
2025-03-06 20:01:00,414 - INFO - training batch 1801, loss: 0.302, 57632/60000 datapoints
2025-03-06 20:01:00,659 - INFO - training batch 1851, loss: 0.309, 59232/60000 datapoints
2025-03-06 20:01:00,775 - INFO - validation batch 1, loss: 0.206, 32/10016 datapoints
2025-03-06 20:01:00,953 - INFO - validation batch 51, loss: 0.112, 1632/10016 datapoints
2025-03-06 20:01:01,131 - INFO - validation batch 101, loss: 0.137, 3232/10016 datapoints
2025-03-06 20:01:01,321 - INFO - validation batch 151, loss: 0.257, 4832/10016 datapoints
2025-03-06 20:01:01,504 - INFO - validation batch 201, loss: 0.206, 6432/10016 datapoints
2025-03-06 20:01:01,703 - INFO - validation batch 251, loss: 0.187, 8032/10016 datapoints
2025-03-06 20:01:01,885 - INFO - validation batch 301, loss: 0.309, 9632/10016 datapoints
2025-03-06 20:01:01,934 - INFO - Epoch 561/800 done.
2025-03-06 20:01:01,935 - INFO - Final validation performance:
Loss: 0.202, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:01:01,935 - INFO - Beginning epoch 562/800
2025-03-06 20:01:01,944 - INFO - training batch 1, loss: 0.137, 32/60000 datapoints
2025-03-06 20:01:02,170 - INFO - training batch 51, loss: 0.573, 1632/60000 datapoints
2025-03-06 20:01:02,445 - INFO - training batch 101, loss: 0.178, 3232/60000 datapoints
2025-03-06 20:01:02,754 - INFO - training batch 151, loss: 0.182, 4832/60000 datapoints
2025-03-06 20:01:03,023 - INFO - training batch 201, loss: 0.107, 6432/60000 datapoints
2025-03-06 20:01:03,261 - INFO - training batch 251, loss: 0.168, 8032/60000 datapoints
2025-03-06 20:01:03,484 - INFO - training batch 301, loss: 0.185, 9632/60000 datapoints
2025-03-06 20:01:03,702 - INFO - training batch 351, loss: 0.455, 11232/60000 datapoints
2025-03-06 20:01:03,916 - INFO - training batch 401, loss: 0.367, 12832/60000 datapoints
2025-03-06 20:01:04,131 - INFO - training batch 451, loss: 0.466, 14432/60000 datapoints
2025-03-06 20:01:04,347 - INFO - training batch 501, loss: 0.524, 16032/60000 datapoints
2025-03-06 20:01:04,564 - INFO - training batch 551, loss: 0.134, 17632/60000 datapoints
2025-03-06 20:01:04,783 - INFO - training batch 601, loss: 0.187, 19232/60000 datapoints
2025-03-06 20:01:04,988 - INFO - training batch 651, loss: 0.105, 20832/60000 datapoints
2025-03-06 20:01:05,196 - INFO - training batch 701, loss: 0.362, 22432/60000 datapoints
2025-03-06 20:01:05,402 - INFO - training batch 751, loss: 0.190, 24032/60000 datapoints
2025-03-06 20:01:05,610 - INFO - training batch 801, loss: 0.255, 25632/60000 datapoints
2025-03-06 20:01:05,817 - INFO - training batch 851, loss: 0.185, 27232/60000 datapoints
2025-03-06 20:01:06,016 - INFO - training batch 901, loss: 0.234, 28832/60000 datapoints
2025-03-06 20:01:06,217 - INFO - training batch 951, loss: 0.366, 30432/60000 datapoints
2025-03-06 20:01:06,421 - INFO - training batch 1001, loss: 0.193, 32032/60000 datapoints
2025-03-06 20:01:06,630 - INFO - training batch 1051, loss: 0.579, 33632/60000 datapoints
2025-03-06 20:01:06,829 - INFO - training batch 1101, loss: 0.122, 35232/60000 datapoints
2025-03-06 20:01:07,032 - INFO - training batch 1151, loss: 0.235, 36832/60000 datapoints
2025-03-06 20:01:07,234 - INFO - training batch 1201, loss: 0.459, 38432/60000 datapoints
2025-03-06 20:01:07,438 - INFO - training batch 1251, loss: 0.358, 40032/60000 datapoints
2025-03-06 20:01:07,646 - INFO - training batch 1301, loss: 0.212, 41632/60000 datapoints
2025-03-06 20:01:07,859 - INFO - training batch 1351, loss: 0.427, 43232/60000 datapoints
2025-03-06 20:01:08,058 - INFO - training batch 1401, loss: 0.110, 44832/60000 datapoints
2025-03-06 20:01:08,271 - INFO - training batch 1451, loss: 0.282, 46432/60000 datapoints
2025-03-06 20:01:08,493 - INFO - training batch 1501, loss: 0.163, 48032/60000 datapoints
2025-03-06 20:01:08,728 - INFO - training batch 1551, loss: 0.480, 49632/60000 datapoints
2025-03-06 20:01:08,955 - INFO - training batch 1601, loss: 0.059, 51232/60000 datapoints
2025-03-06 20:01:09,204 - INFO - training batch 1651, loss: 0.716, 52832/60000 datapoints
2025-03-06 20:01:09,452 - INFO - training batch 1701, loss: 0.428, 54432/60000 datapoints
2025-03-06 20:01:09,667 - INFO - training batch 1751, loss: 0.205, 56032/60000 datapoints
2025-03-06 20:01:09,892 - INFO - training batch 1801, loss: 0.119, 57632/60000 datapoints
2025-03-06 20:01:10,104 - INFO - training batch 1851, loss: 0.193, 59232/60000 datapoints
2025-03-06 20:01:10,226 - INFO - validation batch 1, loss: 0.285, 32/10016 datapoints
2025-03-06 20:01:10,485 - INFO - validation batch 51, loss: 0.190, 1632/10016 datapoints
2025-03-06 20:01:10,919 - INFO - validation batch 101, loss: 0.038, 3232/10016 datapoints
2025-03-06 20:01:11,241 - INFO - validation batch 151, loss: 0.230, 4832/10016 datapoints
2025-03-06 20:01:11,601 - INFO - validation batch 201, loss: 0.327, 6432/10016 datapoints
2025-03-06 20:01:11,926 - INFO - validation batch 251, loss: 0.279, 8032/10016 datapoints
2025-03-06 20:01:12,238 - INFO - validation batch 301, loss: 0.296, 9632/10016 datapoints
2025-03-06 20:01:12,315 - INFO - Epoch 562/800 done.
2025-03-06 20:01:12,315 - INFO - Final validation performance:
Loss: 0.235, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:01:12,316 - INFO - Beginning epoch 563/800
2025-03-06 20:01:12,332 - INFO - training batch 1, loss: 0.095, 32/60000 datapoints
2025-03-06 20:01:12,773 - INFO - training batch 51, loss: 0.160, 1632/60000 datapoints
2025-03-06 20:01:13,183 - INFO - training batch 101, loss: 0.328, 3232/60000 datapoints
2025-03-06 20:01:13,459 - INFO - training batch 151, loss: 0.195, 4832/60000 datapoints
2025-03-06 20:01:13,681 - INFO - training batch 201, loss: 0.194, 6432/60000 datapoints
2025-03-06 20:01:13,914 - INFO - training batch 251, loss: 0.178, 8032/60000 datapoints
2025-03-06 20:01:14,143 - INFO - training batch 301, loss: 0.191, 9632/60000 datapoints
2025-03-06 20:01:14,337 - INFO - training batch 351, loss: 0.104, 11232/60000 datapoints
2025-03-06 20:01:14,533 - INFO - training batch 401, loss: 0.072, 12832/60000 datapoints
2025-03-06 20:01:14,733 - INFO - training batch 451, loss: 0.347, 14432/60000 datapoints
2025-03-06 20:01:14,935 - INFO - training batch 501, loss: 0.109, 16032/60000 datapoints
2025-03-06 20:01:15,130 - INFO - training batch 551, loss: 0.097, 17632/60000 datapoints
2025-03-06 20:01:15,327 - INFO - training batch 601, loss: 0.326, 19232/60000 datapoints
2025-03-06 20:01:15,521 - INFO - training batch 651, loss: 0.443, 20832/60000 datapoints
2025-03-06 20:01:15,719 - INFO - training batch 701, loss: 0.371, 22432/60000 datapoints
2025-03-06 20:01:15,913 - INFO - training batch 751, loss: 0.141, 24032/60000 datapoints
2025-03-06 20:01:16,108 - INFO - training batch 801, loss: 0.076, 25632/60000 datapoints
2025-03-06 20:01:16,308 - INFO - training batch 851, loss: 0.129, 27232/60000 datapoints
2025-03-06 20:01:16,506 - INFO - training batch 901, loss: 0.296, 28832/60000 datapoints
2025-03-06 20:01:16,702 - INFO - training batch 951, loss: 0.224, 30432/60000 datapoints
2025-03-06 20:01:16,896 - INFO - training batch 1001, loss: 0.353, 32032/60000 datapoints
2025-03-06 20:01:17,091 - INFO - training batch 1051, loss: 0.149, 33632/60000 datapoints
2025-03-06 20:01:17,284 - INFO - training batch 1101, loss: 0.238, 35232/60000 datapoints
2025-03-06 20:01:17,492 - INFO - training batch 1151, loss: 0.254, 36832/60000 datapoints
2025-03-06 20:01:17,691 - INFO - training batch 1201, loss: 0.278, 38432/60000 datapoints
2025-03-06 20:01:17,912 - INFO - training batch 1251, loss: 0.428, 40032/60000 datapoints
2025-03-06 20:01:18,105 - INFO - training batch 1301, loss: 0.392, 41632/60000 datapoints
2025-03-06 20:01:18,300 - INFO - training batch 1351, loss: 0.089, 43232/60000 datapoints
2025-03-06 20:01:18,499 - INFO - training batch 1401, loss: 0.160, 44832/60000 datapoints
2025-03-06 20:01:18,695 - INFO - training batch 1451, loss: 0.371, 46432/60000 datapoints
2025-03-06 20:01:18,892 - INFO - training batch 1501, loss: 0.352, 48032/60000 datapoints
2025-03-06 20:01:19,086 - INFO - training batch 1551, loss: 0.302, 49632/60000 datapoints
2025-03-06 20:01:19,280 - INFO - training batch 1601, loss: 0.565, 51232/60000 datapoints
2025-03-06 20:01:19,475 - INFO - training batch 1651, loss: 0.265, 52832/60000 datapoints
2025-03-06 20:01:19,676 - INFO - training batch 1701, loss: 0.173, 54432/60000 datapoints
2025-03-06 20:01:19,871 - INFO - training batch 1751, loss: 0.332, 56032/60000 datapoints
2025-03-06 20:01:20,068 - INFO - training batch 1801, loss: 0.192, 57632/60000 datapoints
2025-03-06 20:01:20,262 - INFO - training batch 1851, loss: 0.296, 59232/60000 datapoints
2025-03-06 20:01:20,367 - INFO - validation batch 1, loss: 0.186, 32/10016 datapoints
2025-03-06 20:01:20,523 - INFO - validation batch 51, loss: 0.306, 1632/10016 datapoints
2025-03-06 20:01:20,679 - INFO - validation batch 101, loss: 0.197, 3232/10016 datapoints
2025-03-06 20:01:20,831 - INFO - validation batch 151, loss: 0.253, 4832/10016 datapoints
2025-03-06 20:01:20,984 - INFO - validation batch 201, loss: 0.311, 6432/10016 datapoints
2025-03-06 20:01:21,138 - INFO - validation batch 251, loss: 0.174, 8032/10016 datapoints
2025-03-06 20:01:21,292 - INFO - validation batch 301, loss: 0.370, 9632/10016 datapoints
2025-03-06 20:01:21,328 - INFO - Epoch 563/800 done.
2025-03-06 20:01:21,329 - INFO - Final validation performance:
Loss: 0.257, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:01:21,329 - INFO - Beginning epoch 564/800
2025-03-06 20:01:21,336 - INFO - training batch 1, loss: 0.082, 32/60000 datapoints
2025-03-06 20:01:21,534 - INFO - training batch 51, loss: 0.234, 1632/60000 datapoints
2025-03-06 20:01:21,733 - INFO - training batch 101, loss: 0.090, 3232/60000 datapoints
2025-03-06 20:01:21,947 - INFO - training batch 151, loss: 0.181, 4832/60000 datapoints
2025-03-06 20:01:22,140 - INFO - training batch 201, loss: 0.351, 6432/60000 datapoints
2025-03-06 20:01:22,339 - INFO - training batch 251, loss: 0.127, 8032/60000 datapoints
2025-03-06 20:01:22,542 - INFO - training batch 301, loss: 0.091, 9632/60000 datapoints
2025-03-06 20:01:22,755 - INFO - training batch 351, loss: 0.255, 11232/60000 datapoints
2025-03-06 20:01:22,958 - INFO - training batch 401, loss: 0.248, 12832/60000 datapoints
2025-03-06 20:01:23,154 - INFO - training batch 451, loss: 0.240, 14432/60000 datapoints
2025-03-06 20:01:23,348 - INFO - training batch 501, loss: 0.359, 16032/60000 datapoints
2025-03-06 20:01:23,548 - INFO - training batch 551, loss: 0.354, 17632/60000 datapoints
2025-03-06 20:01:23,746 - INFO - training batch 601, loss: 0.248, 19232/60000 datapoints
2025-03-06 20:01:23,943 - INFO - training batch 651, loss: 0.063, 20832/60000 datapoints
2025-03-06 20:01:24,137 - INFO - training batch 701, loss: 0.243, 22432/60000 datapoints
2025-03-06 20:01:24,331 - INFO - training batch 751, loss: 0.342, 24032/60000 datapoints
2025-03-06 20:01:24,529 - INFO - training batch 801, loss: 0.117, 25632/60000 datapoints
2025-03-06 20:01:24,725 - INFO - training batch 851, loss: 0.267, 27232/60000 datapoints
2025-03-06 20:01:24,926 - INFO - training batch 901, loss: 0.188, 28832/60000 datapoints
2025-03-06 20:01:25,121 - INFO - training batch 951, loss: 0.375, 30432/60000 datapoints
2025-03-06 20:01:25,315 - INFO - training batch 1001, loss: 0.306, 32032/60000 datapoints
2025-03-06 20:01:25,509 - INFO - training batch 1051, loss: 0.354, 33632/60000 datapoints
2025-03-06 20:01:25,704 - INFO - training batch 1101, loss: 0.241, 35232/60000 datapoints
2025-03-06 20:01:25,905 - INFO - training batch 1151, loss: 0.098, 36832/60000 datapoints
2025-03-06 20:01:26,097 - INFO - training batch 1201, loss: 0.230, 38432/60000 datapoints
2025-03-06 20:01:26,289 - INFO - training batch 1251, loss: 0.576, 40032/60000 datapoints
2025-03-06 20:01:26,508 - INFO - training batch 1301, loss: 0.726, 41632/60000 datapoints
2025-03-06 20:01:26,707 - INFO - training batch 1351, loss: 0.152, 43232/60000 datapoints
2025-03-06 20:01:26,914 - INFO - training batch 1401, loss: 0.260, 44832/60000 datapoints
2025-03-06 20:01:27,119 - INFO - training batch 1451, loss: 0.197, 46432/60000 datapoints
2025-03-06 20:01:27,325 - INFO - training batch 1501, loss: 0.197, 48032/60000 datapoints
2025-03-06 20:01:27,535 - INFO - training batch 1551, loss: 0.137, 49632/60000 datapoints
2025-03-06 20:01:27,749 - INFO - training batch 1601, loss: 0.104, 51232/60000 datapoints
2025-03-06 20:01:27,953 - INFO - training batch 1651, loss: 0.100, 52832/60000 datapoints
2025-03-06 20:01:28,158 - INFO - training batch 1701, loss: 0.252, 54432/60000 datapoints
2025-03-06 20:01:28,415 - INFO - training batch 1751, loss: 0.196, 56032/60000 datapoints
2025-03-06 20:01:28,870 - INFO - training batch 1801, loss: 0.299, 57632/60000 datapoints
2025-03-06 20:01:29,287 - INFO - training batch 1851, loss: 0.142, 59232/60000 datapoints
2025-03-06 20:01:29,413 - INFO - validation batch 1, loss: 0.099, 32/10016 datapoints
2025-03-06 20:01:29,582 - INFO - validation batch 51, loss: 0.157, 1632/10016 datapoints
2025-03-06 20:01:29,752 - INFO - validation batch 101, loss: 0.203, 3232/10016 datapoints
2025-03-06 20:01:29,923 - INFO - validation batch 151, loss: 0.090, 4832/10016 datapoints
2025-03-06 20:01:30,103 - INFO - validation batch 201, loss: 0.111, 6432/10016 datapoints
2025-03-06 20:01:30,276 - INFO - validation batch 251, loss: 0.154, 8032/10016 datapoints
2025-03-06 20:01:30,452 - INFO - validation batch 301, loss: 0.470, 9632/10016 datapoints
2025-03-06 20:01:30,503 - INFO - Epoch 564/800 done.
2025-03-06 20:01:30,503 - INFO - Final validation performance:
Loss: 0.183, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:01:30,504 - INFO - Beginning epoch 565/800
2025-03-06 20:01:30,512 - INFO - training batch 1, loss: 0.084, 32/60000 datapoints
2025-03-06 20:01:30,747 - INFO - training batch 51, loss: 0.203, 1632/60000 datapoints
2025-03-06 20:01:30,960 - INFO - training batch 101, loss: 0.180, 3232/60000 datapoints
2025-03-06 20:01:31,180 - INFO - training batch 151, loss: 0.302, 4832/60000 datapoints
2025-03-06 20:01:31,402 - INFO - training batch 201, loss: 0.352, 6432/60000 datapoints
2025-03-06 20:01:31,613 - INFO - training batch 251, loss: 0.144, 8032/60000 datapoints
2025-03-06 20:01:31,818 - INFO - training batch 301, loss: 0.136, 9632/60000 datapoints
2025-03-06 20:01:32,036 - INFO - training batch 351, loss: 0.232, 11232/60000 datapoints
2025-03-06 20:01:32,256 - INFO - training batch 401, loss: 0.174, 12832/60000 datapoints
2025-03-06 20:01:32,467 - INFO - training batch 451, loss: 0.453, 14432/60000 datapoints
2025-03-06 20:01:32,676 - INFO - training batch 501, loss: 0.282, 16032/60000 datapoints
2025-03-06 20:01:32,903 - INFO - training batch 551, loss: 0.143, 17632/60000 datapoints
2025-03-06 20:01:33,106 - INFO - training batch 601, loss: 0.222, 19232/60000 datapoints
2025-03-06 20:01:33,310 - INFO - training batch 651, loss: 0.184, 20832/60000 datapoints
2025-03-06 20:01:33,515 - INFO - training batch 701, loss: 0.770, 22432/60000 datapoints
2025-03-06 20:01:33,731 - INFO - training batch 751, loss: 0.079, 24032/60000 datapoints
2025-03-06 20:01:33,941 - INFO - training batch 801, loss: 0.222, 25632/60000 datapoints
2025-03-06 20:01:34,147 - INFO - training batch 851, loss: 0.459, 27232/60000 datapoints
2025-03-06 20:01:34,368 - INFO - training batch 901, loss: 0.134, 28832/60000 datapoints
2025-03-06 20:01:34,582 - INFO - training batch 951, loss: 0.206, 30432/60000 datapoints
2025-03-06 20:01:34,814 - INFO - training batch 1001, loss: 0.280, 32032/60000 datapoints
2025-03-06 20:01:35,029 - INFO - training batch 1051, loss: 0.184, 33632/60000 datapoints
2025-03-06 20:01:35,254 - INFO - training batch 1101, loss: 0.243, 35232/60000 datapoints
2025-03-06 20:01:35,492 - INFO - training batch 1151, loss: 0.196, 36832/60000 datapoints
2025-03-06 20:01:35,706 - INFO - training batch 1201, loss: 0.118, 38432/60000 datapoints
2025-03-06 20:01:35,915 - INFO - training batch 1251, loss: 0.541, 40032/60000 datapoints
2025-03-06 20:01:36,125 - INFO - training batch 1301, loss: 0.143, 41632/60000 datapoints
2025-03-06 20:01:36,333 - INFO - training batch 1351, loss: 0.150, 43232/60000 datapoints
2025-03-06 20:01:36,579 - INFO - training batch 1401, loss: 0.139, 44832/60000 datapoints
2025-03-06 20:01:36,842 - INFO - training batch 1451, loss: 0.241, 46432/60000 datapoints
2025-03-06 20:01:37,074 - INFO - training batch 1501, loss: 0.357, 48032/60000 datapoints
2025-03-06 20:01:37,304 - INFO - training batch 1551, loss: 0.223, 49632/60000 datapoints
2025-03-06 20:01:37,587 - INFO - training batch 1601, loss: 0.151, 51232/60000 datapoints
2025-03-06 20:01:37,841 - INFO - training batch 1651, loss: 0.246, 52832/60000 datapoints
2025-03-06 20:01:38,106 - INFO - training batch 1701, loss: 0.122, 54432/60000 datapoints
2025-03-06 20:01:38,349 - INFO - training batch 1751, loss: 0.492, 56032/60000 datapoints
2025-03-06 20:01:38,599 - INFO - training batch 1801, loss: 0.194, 57632/60000 datapoints
2025-03-06 20:01:38,839 - INFO - training batch 1851, loss: 0.258, 59232/60000 datapoints
2025-03-06 20:01:38,961 - INFO - validation batch 1, loss: 0.411, 32/10016 datapoints
2025-03-06 20:01:39,139 - INFO - validation batch 51, loss: 0.356, 1632/10016 datapoints
2025-03-06 20:01:39,316 - INFO - validation batch 101, loss: 0.076, 3232/10016 datapoints
2025-03-06 20:01:39,512 - INFO - validation batch 151, loss: 0.166, 4832/10016 datapoints
2025-03-06 20:01:39,723 - INFO - validation batch 201, loss: 0.363, 6432/10016 datapoints
2025-03-06 20:01:39,905 - INFO - validation batch 251, loss: 0.602, 8032/10016 datapoints
2025-03-06 20:01:40,082 - INFO - validation batch 301, loss: 0.312, 9632/10016 datapoints
2025-03-06 20:01:40,128 - INFO - Epoch 565/800 done.
2025-03-06 20:01:40,129 - INFO - Final validation performance:
Loss: 0.327, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:01:40,129 - INFO - Beginning epoch 566/800
2025-03-06 20:01:40,136 - INFO - training batch 1, loss: 0.149, 32/60000 datapoints
2025-03-06 20:01:40,362 - INFO - training batch 51, loss: 0.199, 1632/60000 datapoints
2025-03-06 20:01:40,604 - INFO - training batch 101, loss: 0.098, 3232/60000 datapoints
2025-03-06 20:01:40,879 - INFO - training batch 151, loss: 0.032, 4832/60000 datapoints
2025-03-06 20:01:41,111 - INFO - training batch 201, loss: 0.076, 6432/60000 datapoints
2025-03-06 20:01:41,339 - INFO - training batch 251, loss: 0.455, 8032/60000 datapoints
2025-03-06 20:01:41,566 - INFO - training batch 301, loss: 0.407, 9632/60000 datapoints
2025-03-06 20:01:41,796 - INFO - training batch 351, loss: 0.195, 11232/60000 datapoints
2025-03-06 20:01:42,014 - INFO - training batch 401, loss: 0.115, 12832/60000 datapoints
2025-03-06 20:01:42,230 - INFO - training batch 451, loss: 0.344, 14432/60000 datapoints
2025-03-06 20:01:42,443 - INFO - training batch 501, loss: 0.180, 16032/60000 datapoints
2025-03-06 20:01:42,663 - INFO - training batch 551, loss: 0.173, 17632/60000 datapoints
2025-03-06 20:01:42,884 - INFO - training batch 601, loss: 0.506, 19232/60000 datapoints
2025-03-06 20:01:43,111 - INFO - training batch 651, loss: 0.175, 20832/60000 datapoints
2025-03-06 20:01:43,321 - INFO - training batch 701, loss: 0.239, 22432/60000 datapoints
2025-03-06 20:01:43,529 - INFO - training batch 751, loss: 0.137, 24032/60000 datapoints
2025-03-06 20:01:43,742 - INFO - training batch 801, loss: 0.365, 25632/60000 datapoints
2025-03-06 20:01:43,952 - INFO - training batch 851, loss: 0.128, 27232/60000 datapoints
2025-03-06 20:01:44,158 - INFO - training batch 901, loss: 0.255, 28832/60000 datapoints
2025-03-06 20:01:44,362 - INFO - training batch 951, loss: 0.171, 30432/60000 datapoints
2025-03-06 20:01:44,570 - INFO - training batch 1001, loss: 0.183, 32032/60000 datapoints
2025-03-06 20:01:44,776 - INFO - training batch 1051, loss: 0.180, 33632/60000 datapoints
2025-03-06 20:01:44,987 - INFO - training batch 1101, loss: 0.158, 35232/60000 datapoints
2025-03-06 20:01:45,189 - INFO - training batch 1151, loss: 0.539, 36832/60000 datapoints
2025-03-06 20:01:45,392 - INFO - training batch 1201, loss: 0.431, 38432/60000 datapoints
2025-03-06 20:01:45,597 - INFO - training batch 1251, loss: 0.453, 40032/60000 datapoints
2025-03-06 20:01:45,803 - INFO - training batch 1301, loss: 0.080, 41632/60000 datapoints
2025-03-06 20:01:46,004 - INFO - training batch 1351, loss: 0.123, 43232/60000 datapoints
2025-03-06 20:01:46,206 - INFO - training batch 1401, loss: 0.137, 44832/60000 datapoints
2025-03-06 20:01:46,409 - INFO - training batch 1451, loss: 0.310, 46432/60000 datapoints
2025-03-06 20:01:46,614 - INFO - training batch 1501, loss: 0.172, 48032/60000 datapoints
2025-03-06 20:01:46,818 - INFO - training batch 1551, loss: 0.146, 49632/60000 datapoints
2025-03-06 20:01:47,021 - INFO - training batch 1601, loss: 0.440, 51232/60000 datapoints
2025-03-06 20:01:47,226 - INFO - training batch 1651, loss: 0.414, 52832/60000 datapoints
2025-03-06 20:01:47,426 - INFO - training batch 1701, loss: 0.076, 54432/60000 datapoints
2025-03-06 20:01:47,631 - INFO - training batch 1751, loss: 0.159, 56032/60000 datapoints
2025-03-06 20:01:47,831 - INFO - training batch 1801, loss: 0.384, 57632/60000 datapoints
2025-03-06 20:01:48,034 - INFO - training batch 1851, loss: 0.073, 59232/60000 datapoints
2025-03-06 20:01:48,148 - INFO - validation batch 1, loss: 0.113, 32/10016 datapoints
2025-03-06 20:01:48,318 - INFO - validation batch 51, loss: 0.212, 1632/10016 datapoints
2025-03-06 20:01:48,485 - INFO - validation batch 101, loss: 0.264, 3232/10016 datapoints
2025-03-06 20:01:48,656 - INFO - validation batch 151, loss: 0.227, 4832/10016 datapoints
2025-03-06 20:01:48,820 - INFO - validation batch 201, loss: 0.282, 6432/10016 datapoints
2025-03-06 20:01:48,992 - INFO - validation batch 251, loss: 0.108, 8032/10016 datapoints
2025-03-06 20:01:49,155 - INFO - validation batch 301, loss: 0.443, 9632/10016 datapoints
2025-03-06 20:01:49,199 - INFO - Epoch 566/800 done.
2025-03-06 20:01:49,200 - INFO - Final validation performance:
Loss: 0.236, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:01:49,200 - INFO - Beginning epoch 567/800
2025-03-06 20:01:49,208 - INFO - training batch 1, loss: 0.482, 32/60000 datapoints
2025-03-06 20:01:49,412 - INFO - training batch 51, loss: 0.270, 1632/60000 datapoints
2025-03-06 20:01:49,625 - INFO - training batch 101, loss: 0.156, 3232/60000 datapoints
2025-03-06 20:01:49,838 - INFO - training batch 151, loss: 0.359, 4832/60000 datapoints
2025-03-06 20:01:50,047 - INFO - training batch 201, loss: 0.102, 6432/60000 datapoints
2025-03-06 20:01:50,261 - INFO - training batch 251, loss: 0.103, 8032/60000 datapoints
2025-03-06 20:01:50,472 - INFO - training batch 301, loss: 0.079, 9632/60000 datapoints
2025-03-06 20:01:50,684 - INFO - training batch 351, loss: 0.307, 11232/60000 datapoints
2025-03-06 20:01:50,885 - INFO - training batch 401, loss: 0.380, 12832/60000 datapoints
2025-03-06 20:01:51,087 - INFO - training batch 451, loss: 0.266, 14432/60000 datapoints
2025-03-06 20:01:51,286 - INFO - training batch 501, loss: 0.377, 16032/60000 datapoints
2025-03-06 20:01:51,488 - INFO - training batch 551, loss: 0.096, 17632/60000 datapoints
2025-03-06 20:01:51,694 - INFO - training batch 601, loss: 0.482, 19232/60000 datapoints
2025-03-06 20:01:51,899 - INFO - training batch 651, loss: 0.194, 20832/60000 datapoints
2025-03-06 20:01:52,102 - INFO - training batch 701, loss: 0.174, 22432/60000 datapoints
2025-03-06 20:01:52,304 - INFO - training batch 751, loss: 0.260, 24032/60000 datapoints
2025-03-06 20:01:52,506 - INFO - training batch 801, loss: 0.191, 25632/60000 datapoints
2025-03-06 20:01:52,721 - INFO - training batch 851, loss: 0.087, 27232/60000 datapoints
2025-03-06 20:01:52,965 - INFO - training batch 901, loss: 0.311, 28832/60000 datapoints
2025-03-06 20:01:53,183 - INFO - training batch 951, loss: 0.138, 30432/60000 datapoints
2025-03-06 20:01:53,387 - INFO - training batch 1001, loss: 0.154, 32032/60000 datapoints
2025-03-06 20:01:53,593 - INFO - training batch 1051, loss: 0.145, 33632/60000 datapoints
2025-03-06 20:01:53,801 - INFO - training batch 1101, loss: 0.122, 35232/60000 datapoints
2025-03-06 20:01:54,005 - INFO - training batch 1151, loss: 0.386, 36832/60000 datapoints
2025-03-06 20:01:54,211 - INFO - training batch 1201, loss: 0.117, 38432/60000 datapoints
2025-03-06 20:01:54,414 - INFO - training batch 1251, loss: 0.273, 40032/60000 datapoints
2025-03-06 20:01:54,615 - INFO - training batch 1301, loss: 0.244, 41632/60000 datapoints
2025-03-06 20:01:54,827 - INFO - training batch 1351, loss: 0.130, 43232/60000 datapoints
2025-03-06 20:01:55,045 - INFO - training batch 1401, loss: 0.277, 44832/60000 datapoints
2025-03-06 20:01:55,248 - INFO - training batch 1451, loss: 0.323, 46432/60000 datapoints
2025-03-06 20:01:55,449 - INFO - training batch 1501, loss: 0.292, 48032/60000 datapoints
2025-03-06 20:01:55,651 - INFO - training batch 1551, loss: 0.052, 49632/60000 datapoints
2025-03-06 20:01:55,849 - INFO - training batch 1601, loss: 0.413, 51232/60000 datapoints
2025-03-06 20:01:56,053 - INFO - training batch 1651, loss: 0.205, 52832/60000 datapoints
2025-03-06 20:01:56,254 - INFO - training batch 1701, loss: 0.162, 54432/60000 datapoints
2025-03-06 20:01:56,452 - INFO - training batch 1751, loss: 0.158, 56032/60000 datapoints
2025-03-06 20:01:56,658 - INFO - training batch 1801, loss: 0.160, 57632/60000 datapoints
2025-03-06 20:01:56,857 - INFO - training batch 1851, loss: 0.170, 59232/60000 datapoints
2025-03-06 20:01:56,961 - INFO - validation batch 1, loss: 0.188, 32/10016 datapoints
2025-03-06 20:01:57,126 - INFO - validation batch 51, loss: 0.216, 1632/10016 datapoints
2025-03-06 20:01:57,286 - INFO - validation batch 101, loss: 0.077, 3232/10016 datapoints
2025-03-06 20:01:57,452 - INFO - validation batch 151, loss: 0.139, 4832/10016 datapoints
2025-03-06 20:01:57,612 - INFO - validation batch 201, loss: 0.226, 6432/10016 datapoints
2025-03-06 20:01:57,774 - INFO - validation batch 251, loss: 0.097, 8032/10016 datapoints
2025-03-06 20:01:57,940 - INFO - validation batch 301, loss: 0.345, 9632/10016 datapoints
2025-03-06 20:01:57,981 - INFO - Epoch 567/800 done.
2025-03-06 20:01:57,981 - INFO - Final validation performance:
Loss: 0.184, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:01:57,982 - INFO - Beginning epoch 568/800
2025-03-06 20:01:57,989 - INFO - training batch 1, loss: 0.320, 32/60000 datapoints
2025-03-06 20:01:58,196 - INFO - training batch 51, loss: 0.260, 1632/60000 datapoints
2025-03-06 20:01:58,410 - INFO - training batch 101, loss: 0.176, 3232/60000 datapoints
2025-03-06 20:01:58,623 - INFO - training batch 151, loss: 0.242, 4832/60000 datapoints
2025-03-06 20:01:58,837 - INFO - training batch 201, loss: 0.185, 6432/60000 datapoints
2025-03-06 20:01:59,049 - INFO - training batch 251, loss: 0.179, 8032/60000 datapoints
2025-03-06 20:01:59,263 - INFO - training batch 301, loss: 0.479, 9632/60000 datapoints
2025-03-06 20:01:59,476 - INFO - training batch 351, loss: 0.175, 11232/60000 datapoints
2025-03-06 20:01:59,695 - INFO - training batch 401, loss: 0.162, 12832/60000 datapoints
2025-03-06 20:01:59,909 - INFO - training batch 451, loss: 0.246, 14432/60000 datapoints
2025-03-06 20:02:00,125 - INFO - training batch 501, loss: 0.294, 16032/60000 datapoints
2025-03-06 20:02:00,340 - INFO - training batch 551, loss: 0.142, 17632/60000 datapoints
2025-03-06 20:02:00,564 - INFO - training batch 601, loss: 0.086, 19232/60000 datapoints
2025-03-06 20:02:00,781 - INFO - training batch 651, loss: 0.114, 20832/60000 datapoints
2025-03-06 20:02:00,995 - INFO - training batch 701, loss: 0.359, 22432/60000 datapoints
2025-03-06 20:02:01,209 - INFO - training batch 751, loss: 0.138, 24032/60000 datapoints
2025-03-06 20:02:01,421 - INFO - training batch 801, loss: 0.090, 25632/60000 datapoints
2025-03-06 20:02:01,641 - INFO - training batch 851, loss: 0.226, 27232/60000 datapoints
2025-03-06 20:02:01,854 - INFO - training batch 901, loss: 0.359, 28832/60000 datapoints
2025-03-06 20:02:02,083 - INFO - training batch 951, loss: 0.224, 30432/60000 datapoints
2025-03-06 20:02:02,294 - INFO - training batch 1001, loss: 0.396, 32032/60000 datapoints
2025-03-06 20:02:02,516 - INFO - training batch 1051, loss: 0.153, 33632/60000 datapoints
2025-03-06 20:02:02,734 - INFO - training batch 1101, loss: 0.331, 35232/60000 datapoints
2025-03-06 20:02:02,950 - INFO - training batch 1151, loss: 0.184, 36832/60000 datapoints
2025-03-06 20:02:03,191 - INFO - training batch 1201, loss: 0.155, 38432/60000 datapoints
2025-03-06 20:02:03,405 - INFO - training batch 1251, loss: 0.215, 40032/60000 datapoints
2025-03-06 20:02:03,617 - INFO - training batch 1301, loss: 0.288, 41632/60000 datapoints
2025-03-06 20:02:03,836 - INFO - training batch 1351, loss: 0.180, 43232/60000 datapoints
2025-03-06 20:02:04,055 - INFO - training batch 1401, loss: 0.098, 44832/60000 datapoints
2025-03-06 20:02:04,270 - INFO - training batch 1451, loss: 0.135, 46432/60000 datapoints
2025-03-06 20:02:04,487 - INFO - training batch 1501, loss: 0.121, 48032/60000 datapoints
2025-03-06 20:02:04,706 - INFO - training batch 1551, loss: 0.242, 49632/60000 datapoints
2025-03-06 20:02:04,928 - INFO - training batch 1601, loss: 0.212, 51232/60000 datapoints
2025-03-06 20:02:05,143 - INFO - training batch 1651, loss: 0.236, 52832/60000 datapoints
2025-03-06 20:02:05,350 - INFO - training batch 1701, loss: 0.517, 54432/60000 datapoints
2025-03-06 20:02:05,564 - INFO - training batch 1751, loss: 0.127, 56032/60000 datapoints
2025-03-06 20:02:05,780 - INFO - training batch 1801, loss: 0.422, 57632/60000 datapoints
2025-03-06 20:02:05,995 - INFO - training batch 1851, loss: 0.247, 59232/60000 datapoints
2025-03-06 20:02:06,109 - INFO - validation batch 1, loss: 0.365, 32/10016 datapoints
2025-03-06 20:02:06,284 - INFO - validation batch 51, loss: 0.074, 1632/10016 datapoints
2025-03-06 20:02:06,447 - INFO - validation batch 101, loss: 0.231, 3232/10016 datapoints
2025-03-06 20:02:06,616 - INFO - validation batch 151, loss: 0.210, 4832/10016 datapoints
2025-03-06 20:02:06,785 - INFO - validation batch 201, loss: 0.187, 6432/10016 datapoints
2025-03-06 20:02:06,954 - INFO - validation batch 251, loss: 0.213, 8032/10016 datapoints
2025-03-06 20:02:07,124 - INFO - validation batch 301, loss: 0.173, 9632/10016 datapoints
2025-03-06 20:02:07,164 - INFO - Epoch 568/800 done.
2025-03-06 20:02:07,164 - INFO - Final validation performance:
Loss: 0.208, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:02:07,165 - INFO - Beginning epoch 569/800
2025-03-06 20:02:07,178 - INFO - training batch 1, loss: 0.316, 32/60000 datapoints
2025-03-06 20:02:07,431 - INFO - training batch 51, loss: 0.153, 1632/60000 datapoints
2025-03-06 20:02:07,694 - INFO - training batch 101, loss: 0.229, 3232/60000 datapoints
2025-03-06 20:02:07,916 - INFO - training batch 151, loss: 0.218, 4832/60000 datapoints
2025-03-06 20:02:08,136 - INFO - training batch 201, loss: 0.103, 6432/60000 datapoints
2025-03-06 20:02:08,362 - INFO - training batch 251, loss: 0.259, 8032/60000 datapoints
2025-03-06 20:02:08,606 - INFO - training batch 301, loss: 0.502, 9632/60000 datapoints
2025-03-06 20:02:08,819 - INFO - training batch 351, loss: 0.111, 11232/60000 datapoints
2025-03-06 20:02:09,035 - INFO - training batch 401, loss: 0.251, 12832/60000 datapoints
2025-03-06 20:02:09,239 - INFO - training batch 451, loss: 0.417, 14432/60000 datapoints
2025-03-06 20:02:09,441 - INFO - training batch 501, loss: 0.200, 16032/60000 datapoints
2025-03-06 20:02:09,679 - INFO - training batch 551, loss: 0.088, 17632/60000 datapoints
2025-03-06 20:02:09,919 - INFO - training batch 601, loss: 0.341, 19232/60000 datapoints
2025-03-06 20:02:10,156 - INFO - training batch 651, loss: 0.291, 20832/60000 datapoints
2025-03-06 20:02:10,395 - INFO - training batch 701, loss: 0.127, 22432/60000 datapoints
2025-03-06 20:02:10,612 - INFO - training batch 751, loss: 0.240, 24032/60000 datapoints
2025-03-06 20:02:10,821 - INFO - training batch 801, loss: 0.347, 25632/60000 datapoints
2025-03-06 20:02:11,041 - INFO - training batch 851, loss: 0.284, 27232/60000 datapoints
2025-03-06 20:02:11,252 - INFO - training batch 901, loss: 0.264, 28832/60000 datapoints
2025-03-06 20:02:11,469 - INFO - training batch 951, loss: 0.157, 30432/60000 datapoints
2025-03-06 20:02:11,692 - INFO - training batch 1001, loss: 0.402, 32032/60000 datapoints
2025-03-06 20:02:11,905 - INFO - training batch 1051, loss: 0.202, 33632/60000 datapoints
2025-03-06 20:02:12,117 - INFO - training batch 1101, loss: 0.296, 35232/60000 datapoints
2025-03-06 20:02:12,335 - INFO - training batch 1151, loss: 0.092, 36832/60000 datapoints
2025-03-06 20:02:12,550 - INFO - training batch 1201, loss: 0.118, 38432/60000 datapoints
2025-03-06 20:02:12,772 - INFO - training batch 1251, loss: 0.157, 40032/60000 datapoints
2025-03-06 20:02:13,004 - INFO - training batch 1301, loss: 0.267, 41632/60000 datapoints
2025-03-06 20:02:13,238 - INFO - training batch 1351, loss: 0.145, 43232/60000 datapoints
2025-03-06 20:02:13,453 - INFO - training batch 1401, loss: 0.315, 44832/60000 datapoints
2025-03-06 20:02:13,677 - INFO - training batch 1451, loss: 0.069, 46432/60000 datapoints
2025-03-06 20:02:13,893 - INFO - training batch 1501, loss: 0.134, 48032/60000 datapoints
2025-03-06 20:02:14,108 - INFO - training batch 1551, loss: 0.330, 49632/60000 datapoints
2025-03-06 20:02:14,328 - INFO - training batch 1601, loss: 0.368, 51232/60000 datapoints
2025-03-06 20:02:14,545 - INFO - training batch 1651, loss: 0.402, 52832/60000 datapoints
2025-03-06 20:02:14,766 - INFO - training batch 1701, loss: 0.140, 54432/60000 datapoints
2025-03-06 20:02:14,990 - INFO - training batch 1751, loss: 0.280, 56032/60000 datapoints
2025-03-06 20:02:15,211 - INFO - training batch 1801, loss: 0.154, 57632/60000 datapoints
2025-03-06 20:02:15,430 - INFO - training batch 1851, loss: 0.069, 59232/60000 datapoints
2025-03-06 20:02:15,544 - INFO - validation batch 1, loss: 0.361, 32/10016 datapoints
2025-03-06 20:02:15,749 - INFO - validation batch 51, loss: 0.293, 1632/10016 datapoints
2025-03-06 20:02:15,940 - INFO - validation batch 101, loss: 0.283, 3232/10016 datapoints
2025-03-06 20:02:16,115 - INFO - validation batch 151, loss: 0.187, 4832/10016 datapoints
2025-03-06 20:02:16,288 - INFO - validation batch 201, loss: 0.232, 6432/10016 datapoints
2025-03-06 20:02:16,463 - INFO - validation batch 251, loss: 0.113, 8032/10016 datapoints
2025-03-06 20:02:16,649 - INFO - validation batch 301, loss: 0.289, 9632/10016 datapoints
2025-03-06 20:02:16,697 - INFO - Epoch 569/800 done.
2025-03-06 20:02:16,698 - INFO - Final validation performance:
Loss: 0.251, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:02:16,698 - INFO - Beginning epoch 570/800
2025-03-06 20:02:16,707 - INFO - training batch 1, loss: 0.163, 32/60000 datapoints
2025-03-06 20:02:16,943 - INFO - training batch 51, loss: 0.115, 1632/60000 datapoints
2025-03-06 20:02:17,164 - INFO - training batch 101, loss: 0.063, 3232/60000 datapoints
2025-03-06 20:02:17,395 - INFO - training batch 151, loss: 0.099, 4832/60000 datapoints
2025-03-06 20:02:17,617 - INFO - training batch 201, loss: 0.218, 6432/60000 datapoints
2025-03-06 20:02:17,861 - INFO - training batch 251, loss: 0.300, 8032/60000 datapoints
2025-03-06 20:02:18,102 - INFO - training batch 301, loss: 0.139, 9632/60000 datapoints
2025-03-06 20:02:18,324 - INFO - training batch 351, loss: 0.149, 11232/60000 datapoints
2025-03-06 20:02:18,549 - INFO - training batch 401, loss: 0.189, 12832/60000 datapoints
2025-03-06 20:02:18,810 - INFO - training batch 451, loss: 0.319, 14432/60000 datapoints
2025-03-06 20:02:19,036 - INFO - training batch 501, loss: 0.208, 16032/60000 datapoints
2025-03-06 20:02:19,280 - INFO - training batch 551, loss: 0.192, 17632/60000 datapoints
2025-03-06 20:02:19,503 - INFO - training batch 601, loss: 0.251, 19232/60000 datapoints
2025-03-06 20:02:19,723 - INFO - training batch 651, loss: 0.103, 20832/60000 datapoints
2025-03-06 20:02:19,945 - INFO - training batch 701, loss: 0.070, 22432/60000 datapoints
2025-03-06 20:02:20,163 - INFO - training batch 751, loss: 0.284, 24032/60000 datapoints
2025-03-06 20:02:20,390 - INFO - training batch 801, loss: 0.154, 25632/60000 datapoints
2025-03-06 20:02:20,619 - INFO - training batch 851, loss: 0.272, 27232/60000 datapoints
2025-03-06 20:02:20,840 - INFO - training batch 901, loss: 0.246, 28832/60000 datapoints
2025-03-06 20:02:21,060 - INFO - training batch 951, loss: 0.084, 30432/60000 datapoints
2025-03-06 20:02:21,280 - INFO - training batch 1001, loss: 0.219, 32032/60000 datapoints
2025-03-06 20:02:21,496 - INFO - training batch 1051, loss: 0.145, 33632/60000 datapoints
2025-03-06 20:02:21,715 - INFO - training batch 1101, loss: 0.152, 35232/60000 datapoints
2025-03-06 20:02:21,938 - INFO - training batch 1151, loss: 0.218, 36832/60000 datapoints
2025-03-06 20:02:22,158 - INFO - training batch 1201, loss: 0.265, 38432/60000 datapoints
2025-03-06 20:02:22,369 - INFO - training batch 1251, loss: 0.158, 40032/60000 datapoints
2025-03-06 20:02:22,581 - INFO - training batch 1301, loss: 0.081, 41632/60000 datapoints
2025-03-06 20:02:22,797 - INFO - training batch 1351, loss: 0.297, 43232/60000 datapoints
2025-03-06 20:02:23,012 - INFO - training batch 1401, loss: 0.118, 44832/60000 datapoints
2025-03-06 20:02:23,237 - INFO - training batch 1451, loss: 0.315, 46432/60000 datapoints
2025-03-06 20:02:23,464 - INFO - training batch 1501, loss: 0.224, 48032/60000 datapoints
2025-03-06 20:02:23,683 - INFO - training batch 1551, loss: 0.180, 49632/60000 datapoints
2025-03-06 20:02:23,895 - INFO - training batch 1601, loss: 0.237, 51232/60000 datapoints
2025-03-06 20:02:24,106 - INFO - training batch 1651, loss: 0.496, 52832/60000 datapoints
2025-03-06 20:02:24,321 - INFO - training batch 1701, loss: 0.481, 54432/60000 datapoints
2025-03-06 20:02:24,526 - INFO - training batch 1751, loss: 0.037, 56032/60000 datapoints
2025-03-06 20:02:24,739 - INFO - training batch 1801, loss: 0.262, 57632/60000 datapoints
2025-03-06 20:02:24,954 - INFO - training batch 1851, loss: 0.108, 59232/60000 datapoints
2025-03-06 20:02:25,065 - INFO - validation batch 1, loss: 0.063, 32/10016 datapoints
2025-03-06 20:02:25,234 - INFO - validation batch 51, loss: 0.229, 1632/10016 datapoints
2025-03-06 20:02:25,402 - INFO - validation batch 101, loss: 0.115, 3232/10016 datapoints
2025-03-06 20:02:25,571 - INFO - validation batch 151, loss: 0.249, 4832/10016 datapoints
2025-03-06 20:02:25,761 - INFO - validation batch 201, loss: 0.513, 6432/10016 datapoints
2025-03-06 20:02:25,932 - INFO - validation batch 251, loss: 0.128, 8032/10016 datapoints
2025-03-06 20:02:26,101 - INFO - validation batch 301, loss: 0.292, 9632/10016 datapoints
2025-03-06 20:02:26,149 - INFO - Epoch 570/800 done.
2025-03-06 20:02:26,149 - INFO - Final validation performance:
Loss: 0.227, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:02:26,149 - INFO - Beginning epoch 571/800
2025-03-06 20:02:26,157 - INFO - training batch 1, loss: 0.190, 32/60000 datapoints
2025-03-06 20:02:26,379 - INFO - training batch 51, loss: 0.294, 1632/60000 datapoints
2025-03-06 20:02:26,583 - INFO - training batch 101, loss: 0.096, 3232/60000 datapoints
2025-03-06 20:02:26,798 - INFO - training batch 151, loss: 0.189, 4832/60000 datapoints
2025-03-06 20:02:27,009 - INFO - training batch 201, loss: 0.180, 6432/60000 datapoints
2025-03-06 20:02:27,254 - INFO - training batch 251, loss: 0.144, 8032/60000 datapoints
2025-03-06 20:02:27,489 - INFO - training batch 301, loss: 0.163, 9632/60000 datapoints
2025-03-06 20:02:27,712 - INFO - training batch 351, loss: 0.261, 11232/60000 datapoints
2025-03-06 20:02:27,916 - INFO - training batch 401, loss: 0.290, 12832/60000 datapoints
2025-03-06 20:02:28,120 - INFO - training batch 451, loss: 0.159, 14432/60000 datapoints
2025-03-06 20:02:28,334 - INFO - training batch 501, loss: 0.759, 16032/60000 datapoints
2025-03-06 20:02:28,550 - INFO - training batch 551, loss: 0.342, 17632/60000 datapoints
2025-03-06 20:02:28,762 - INFO - training batch 601, loss: 0.165, 19232/60000 datapoints
2025-03-06 20:02:28,963 - INFO - training batch 651, loss: 0.082, 20832/60000 datapoints
2025-03-06 20:02:29,161 - INFO - training batch 701, loss: 0.128, 22432/60000 datapoints
2025-03-06 20:02:29,362 - INFO - training batch 751, loss: 0.642, 24032/60000 datapoints
2025-03-06 20:02:29,567 - INFO - training batch 801, loss: 0.170, 25632/60000 datapoints
2025-03-06 20:02:29,802 - INFO - training batch 851, loss: 0.186, 27232/60000 datapoints
2025-03-06 20:02:30,009 - INFO - training batch 901, loss: 0.210, 28832/60000 datapoints
2025-03-06 20:02:30,216 - INFO - training batch 951, loss: 0.086, 30432/60000 datapoints
2025-03-06 20:02:30,423 - INFO - training batch 1001, loss: 0.162, 32032/60000 datapoints
2025-03-06 20:02:30,635 - INFO - training batch 1051, loss: 0.242, 33632/60000 datapoints
2025-03-06 20:02:30,847 - INFO - training batch 1101, loss: 0.145, 35232/60000 datapoints
2025-03-06 20:02:31,058 - INFO - training batch 1151, loss: 0.360, 36832/60000 datapoints
2025-03-06 20:02:31,276 - INFO - training batch 1201, loss: 0.130, 38432/60000 datapoints
2025-03-06 20:02:31,488 - INFO - training batch 1251, loss: 0.196, 40032/60000 datapoints
2025-03-06 20:02:31,707 - INFO - training batch 1301, loss: 0.216, 41632/60000 datapoints
2025-03-06 20:02:31,918 - INFO - training batch 1351, loss: 0.155, 43232/60000 datapoints
2025-03-06 20:02:32,132 - INFO - training batch 1401, loss: 0.187, 44832/60000 datapoints
2025-03-06 20:02:32,354 - INFO - training batch 1451, loss: 0.066, 46432/60000 datapoints
2025-03-06 20:02:32,568 - INFO - training batch 1501, loss: 0.342, 48032/60000 datapoints
2025-03-06 20:02:32,789 - INFO - training batch 1551, loss: 0.073, 49632/60000 datapoints
2025-03-06 20:02:33,008 - INFO - training batch 1601, loss: 0.092, 51232/60000 datapoints
2025-03-06 20:02:33,222 - INFO - training batch 1651, loss: 0.368, 52832/60000 datapoints
2025-03-06 20:02:33,460 - INFO - training batch 1701, loss: 0.225, 54432/60000 datapoints
2025-03-06 20:02:33,677 - INFO - training batch 1751, loss: 0.380, 56032/60000 datapoints
2025-03-06 20:02:33,891 - INFO - training batch 1801, loss: 0.224, 57632/60000 datapoints
2025-03-06 20:02:34,103 - INFO - training batch 1851, loss: 0.474, 59232/60000 datapoints
2025-03-06 20:02:34,217 - INFO - validation batch 1, loss: 0.099, 32/10016 datapoints
2025-03-06 20:02:34,388 - INFO - validation batch 51, loss: 0.247, 1632/10016 datapoints
2025-03-06 20:02:34,562 - INFO - validation batch 101, loss: 0.123, 3232/10016 datapoints
2025-03-06 20:02:34,743 - INFO - validation batch 151, loss: 0.155, 4832/10016 datapoints
2025-03-06 20:02:34,927 - INFO - validation batch 201, loss: 0.222, 6432/10016 datapoints
2025-03-06 20:02:35,100 - INFO - validation batch 251, loss: 0.173, 8032/10016 datapoints
2025-03-06 20:02:35,272 - INFO - validation batch 301, loss: 0.327, 9632/10016 datapoints
2025-03-06 20:02:35,316 - INFO - Epoch 571/800 done.
2025-03-06 20:02:35,316 - INFO - Final validation performance:
Loss: 0.192, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:02:35,320 - INFO - Beginning epoch 572/800
2025-03-06 20:02:35,329 - INFO - training batch 1, loss: 0.107, 32/60000 datapoints
2025-03-06 20:02:35,556 - INFO - training batch 51, loss: 0.198, 1632/60000 datapoints
2025-03-06 20:02:35,765 - INFO - training batch 101, loss: 0.240, 3232/60000 datapoints
2025-03-06 20:02:35,980 - INFO - training batch 151, loss: 0.223, 4832/60000 datapoints
2025-03-06 20:02:36,189 - INFO - training batch 201, loss: 0.429, 6432/60000 datapoints
2025-03-06 20:02:36,400 - INFO - training batch 251, loss: 0.289, 8032/60000 datapoints
2025-03-06 20:02:36,608 - INFO - training batch 301, loss: 0.255, 9632/60000 datapoints
2025-03-06 20:02:36,821 - INFO - training batch 351, loss: 0.142, 11232/60000 datapoints
2025-03-06 20:02:37,035 - INFO - training batch 401, loss: 0.143, 12832/60000 datapoints
2025-03-06 20:02:37,239 - INFO - training batch 451, loss: 0.229, 14432/60000 datapoints
2025-03-06 20:02:37,444 - INFO - training batch 501, loss: 0.091, 16032/60000 datapoints
2025-03-06 20:02:37,681 - INFO - training batch 551, loss: 0.158, 17632/60000 datapoints
2025-03-06 20:02:37,888 - INFO - training batch 601, loss: 0.091, 19232/60000 datapoints
2025-03-06 20:02:38,099 - INFO - training batch 651, loss: 0.145, 20832/60000 datapoints
2025-03-06 20:02:38,308 - INFO - training batch 701, loss: 0.158, 22432/60000 datapoints
2025-03-06 20:02:38,514 - INFO - training batch 751, loss: 0.156, 24032/60000 datapoints
2025-03-06 20:02:38,723 - INFO - training batch 801, loss: 0.406, 25632/60000 datapoints
2025-03-06 20:02:38,944 - INFO - training batch 851, loss: 0.287, 27232/60000 datapoints
2025-03-06 20:02:39,154 - INFO - training batch 901, loss: 0.330, 28832/60000 datapoints
2025-03-06 20:02:39,362 - INFO - training batch 951, loss: 0.116, 30432/60000 datapoints
2025-03-06 20:02:39,572 - INFO - training batch 1001, loss: 0.363, 32032/60000 datapoints
2025-03-06 20:02:39,783 - INFO - training batch 1051, loss: 0.182, 33632/60000 datapoints
2025-03-06 20:02:39,988 - INFO - training batch 1101, loss: 0.278, 35232/60000 datapoints
2025-03-06 20:02:40,199 - INFO - training batch 1151, loss: 0.540, 36832/60000 datapoints
2025-03-06 20:02:40,405 - INFO - training batch 1201, loss: 0.180, 38432/60000 datapoints
2025-03-06 20:02:40,613 - INFO - training batch 1251, loss: 0.412, 40032/60000 datapoints
2025-03-06 20:02:40,818 - INFO - training batch 1301, loss: 0.143, 41632/60000 datapoints
2025-03-06 20:02:41,023 - INFO - training batch 1351, loss: 0.485, 43232/60000 datapoints
2025-03-06 20:02:41,267 - INFO - training batch 1401, loss: 0.303, 44832/60000 datapoints
2025-03-06 20:02:41,468 - INFO - training batch 1451, loss: 0.187, 46432/60000 datapoints
2025-03-06 20:02:41,671 - INFO - training batch 1501, loss: 0.368, 48032/60000 datapoints
2025-03-06 20:02:41,895 - INFO - training batch 1551, loss: 0.188, 49632/60000 datapoints
2025-03-06 20:02:42,115 - INFO - training batch 1601, loss: 0.170, 51232/60000 datapoints
2025-03-06 20:02:42,331 - INFO - training batch 1651, loss: 0.335, 52832/60000 datapoints
2025-03-06 20:02:42,545 - INFO - training batch 1701, loss: 0.164, 54432/60000 datapoints
2025-03-06 20:02:42,767 - INFO - training batch 1751, loss: 0.151, 56032/60000 datapoints
2025-03-06 20:02:42,997 - INFO - training batch 1801, loss: 0.252, 57632/60000 datapoints
2025-03-06 20:02:43,216 - INFO - training batch 1851, loss: 0.403, 59232/60000 datapoints
2025-03-06 20:02:43,337 - INFO - validation batch 1, loss: 0.091, 32/10016 datapoints
2025-03-06 20:02:43,533 - INFO - validation batch 51, loss: 0.067, 1632/10016 datapoints
2025-03-06 20:02:43,712 - INFO - validation batch 101, loss: 0.264, 3232/10016 datapoints
2025-03-06 20:02:43,892 - INFO - validation batch 151, loss: 0.386, 4832/10016 datapoints
2025-03-06 20:02:44,289 - INFO - validation batch 201, loss: 0.274, 6432/10016 datapoints
2025-03-06 20:02:44,471 - INFO - validation batch 251, loss: 0.378, 8032/10016 datapoints
2025-03-06 20:02:44,635 - INFO - validation batch 301, loss: 0.086, 9632/10016 datapoints
2025-03-06 20:02:44,677 - INFO - Epoch 572/800 done.
2025-03-06 20:02:44,677 - INFO - Final validation performance:
Loss: 0.221, top-1 acc: 0.930top-5 acc: 0.930
2025-03-06 20:02:44,677 - INFO - Beginning epoch 573/800
2025-03-06 20:02:44,684 - INFO - training batch 1, loss: 0.208, 32/60000 datapoints
2025-03-06 20:02:44,909 - INFO - training batch 51, loss: 0.343, 1632/60000 datapoints
2025-03-06 20:02:45,129 - INFO - training batch 101, loss: 0.388, 3232/60000 datapoints
2025-03-06 20:02:45,344 - INFO - training batch 151, loss: 0.140, 4832/60000 datapoints
2025-03-06 20:02:45,560 - INFO - training batch 201, loss: 0.519, 6432/60000 datapoints
2025-03-06 20:02:45,774 - INFO - training batch 251, loss: 0.049, 8032/60000 datapoints
2025-03-06 20:02:45,985 - INFO - training batch 301, loss: 0.362, 9632/60000 datapoints
2025-03-06 20:02:46,221 - INFO - training batch 351, loss: 0.270, 11232/60000 datapoints
2025-03-06 20:02:46,427 - INFO - training batch 401, loss: 0.105, 12832/60000 datapoints
2025-03-06 20:02:46,636 - INFO - training batch 451, loss: 0.201, 14432/60000 datapoints
2025-03-06 20:02:46,855 - INFO - training batch 501, loss: 0.159, 16032/60000 datapoints
2025-03-06 20:02:47,063 - INFO - training batch 551, loss: 0.072, 17632/60000 datapoints
2025-03-06 20:02:47,301 - INFO - training batch 601, loss: 0.218, 19232/60000 datapoints
2025-03-06 20:02:47,541 - INFO - training batch 651, loss: 0.231, 20832/60000 datapoints
2025-03-06 20:02:47,782 - INFO - training batch 701, loss: 0.108, 22432/60000 datapoints
2025-03-06 20:02:48,021 - INFO - training batch 751, loss: 0.224, 24032/60000 datapoints
2025-03-06 20:02:48,243 - INFO - training batch 801, loss: 0.110, 25632/60000 datapoints
2025-03-06 20:02:48,458 - INFO - training batch 851, loss: 0.292, 27232/60000 datapoints
2025-03-06 20:02:48,680 - INFO - training batch 901, loss: 0.255, 28832/60000 datapoints
2025-03-06 20:02:48,892 - INFO - training batch 951, loss: 0.077, 30432/60000 datapoints
2025-03-06 20:02:49,113 - INFO - training batch 1001, loss: 0.364, 32032/60000 datapoints
2025-03-06 20:02:49,332 - INFO - training batch 1051, loss: 0.116, 33632/60000 datapoints
2025-03-06 20:02:49,568 - INFO - training batch 1101, loss: 0.420, 35232/60000 datapoints
2025-03-06 20:02:49,794 - INFO - training batch 1151, loss: 0.107, 36832/60000 datapoints
2025-03-06 20:02:50,028 - INFO - training batch 1201, loss: 0.220, 38432/60000 datapoints
2025-03-06 20:02:50,253 - INFO - training batch 1251, loss: 0.136, 40032/60000 datapoints
2025-03-06 20:02:50,485 - INFO - training batch 1301, loss: 0.393, 41632/60000 datapoints
2025-03-06 20:02:50,718 - INFO - training batch 1351, loss: 0.284, 43232/60000 datapoints
2025-03-06 20:02:50,948 - INFO - training batch 1401, loss: 0.279, 44832/60000 datapoints
2025-03-06 20:02:51,175 - INFO - training batch 1451, loss: 0.478, 46432/60000 datapoints
2025-03-06 20:02:51,399 - INFO - training batch 1501, loss: 0.277, 48032/60000 datapoints
2025-03-06 20:02:51,627 - INFO - training batch 1551, loss: 0.394, 49632/60000 datapoints
2025-03-06 20:02:51,853 - INFO - training batch 1601, loss: 0.201, 51232/60000 datapoints
2025-03-06 20:02:52,080 - INFO - training batch 1651, loss: 0.092, 52832/60000 datapoints
2025-03-06 20:02:52,307 - INFO - training batch 1701, loss: 0.273, 54432/60000 datapoints
2025-03-06 20:02:52,535 - INFO - training batch 1751, loss: 0.079, 56032/60000 datapoints
2025-03-06 20:02:52,762 - INFO - training batch 1801, loss: 0.128, 57632/60000 datapoints
2025-03-06 20:02:53,025 - INFO - training batch 1851, loss: 0.509, 59232/60000 datapoints
2025-03-06 20:02:53,142 - INFO - validation batch 1, loss: 0.109, 32/10016 datapoints
2025-03-06 20:02:53,316 - INFO - validation batch 51, loss: 0.186, 1632/10016 datapoints
2025-03-06 20:02:53,509 - INFO - validation batch 101, loss: 0.101, 3232/10016 datapoints
2025-03-06 20:02:53,706 - INFO - validation batch 151, loss: 0.614, 4832/10016 datapoints
2025-03-06 20:02:53,883 - INFO - validation batch 201, loss: 0.153, 6432/10016 datapoints
2025-03-06 20:02:54,061 - INFO - validation batch 251, loss: 0.345, 8032/10016 datapoints
2025-03-06 20:02:54,242 - INFO - validation batch 301, loss: 0.168, 9632/10016 datapoints
2025-03-06 20:02:54,288 - INFO - Epoch 573/800 done.
2025-03-06 20:02:54,288 - INFO - Final validation performance:
Loss: 0.240, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:02:54,288 - INFO - Beginning epoch 574/800
2025-03-06 20:02:54,296 - INFO - training batch 1, loss: 0.318, 32/60000 datapoints
2025-03-06 20:02:54,519 - INFO - training batch 51, loss: 0.165, 1632/60000 datapoints
2025-03-06 20:02:54,772 - INFO - training batch 101, loss: 0.057, 3232/60000 datapoints
2025-03-06 20:02:54,997 - INFO - training batch 151, loss: 0.476, 4832/60000 datapoints
2025-03-06 20:02:55,221 - INFO - training batch 201, loss: 0.289, 6432/60000 datapoints
2025-03-06 20:02:55,442 - INFO - training batch 251, loss: 0.723, 8032/60000 datapoints
2025-03-06 20:02:55,665 - INFO - training batch 301, loss: 0.283, 9632/60000 datapoints
2025-03-06 20:02:55,881 - INFO - training batch 351, loss: 0.154, 11232/60000 datapoints
2025-03-06 20:02:56,118 - INFO - training batch 401, loss: 0.337, 12832/60000 datapoints
2025-03-06 20:02:56,349 - INFO - training batch 451, loss: 0.099, 14432/60000 datapoints
2025-03-06 20:02:56,572 - INFO - training batch 501, loss: 0.248, 16032/60000 datapoints
2025-03-06 20:02:56,796 - INFO - training batch 551, loss: 0.187, 17632/60000 datapoints
2025-03-06 20:02:57,027 - INFO - training batch 601, loss: 0.370, 19232/60000 datapoints
2025-03-06 20:02:57,273 - INFO - training batch 651, loss: 0.188, 20832/60000 datapoints
2025-03-06 20:02:57,504 - INFO - training batch 701, loss: 0.395, 22432/60000 datapoints
2025-03-06 20:02:57,721 - INFO - training batch 751, loss: 0.514, 24032/60000 datapoints
2025-03-06 20:02:57,934 - INFO - training batch 801, loss: 0.209, 25632/60000 datapoints
2025-03-06 20:02:58,143 - INFO - training batch 851, loss: 0.255, 27232/60000 datapoints
2025-03-06 20:02:58,351 - INFO - training batch 901, loss: 0.089, 28832/60000 datapoints
2025-03-06 20:02:58,558 - INFO - training batch 951, loss: 0.118, 30432/60000 datapoints
2025-03-06 20:02:58,763 - INFO - training batch 1001, loss: 0.252, 32032/60000 datapoints
2025-03-06 20:02:58,965 - INFO - training batch 1051, loss: 0.333, 33632/60000 datapoints
2025-03-06 20:02:59,173 - INFO - training batch 1101, loss: 0.130, 35232/60000 datapoints
2025-03-06 20:02:59,373 - INFO - training batch 1151, loss: 0.152, 36832/60000 datapoints
2025-03-06 20:02:59,576 - INFO - training batch 1201, loss: 0.190, 38432/60000 datapoints
2025-03-06 20:02:59,798 - INFO - training batch 1251, loss: 0.097, 40032/60000 datapoints
2025-03-06 20:03:00,004 - INFO - training batch 1301, loss: 0.152, 41632/60000 datapoints
2025-03-06 20:03:00,209 - INFO - training batch 1351, loss: 0.143, 43232/60000 datapoints
2025-03-06 20:03:00,412 - INFO - training batch 1401, loss: 0.177, 44832/60000 datapoints
2025-03-06 20:03:00,618 - INFO - training batch 1451, loss: 0.104, 46432/60000 datapoints
2025-03-06 20:03:00,829 - INFO - training batch 1501, loss: 0.324, 48032/60000 datapoints
2025-03-06 20:03:01,035 - INFO - training batch 1551, loss: 0.297, 49632/60000 datapoints
2025-03-06 20:03:01,239 - INFO - training batch 1601, loss: 0.259, 51232/60000 datapoints
2025-03-06 20:03:01,442 - INFO - training batch 1651, loss: 0.223, 52832/60000 datapoints
2025-03-06 20:03:01,652 - INFO - training batch 1701, loss: 0.190, 54432/60000 datapoints
2025-03-06 20:03:01,855 - INFO - training batch 1751, loss: 0.523, 56032/60000 datapoints
2025-03-06 20:03:02,071 - INFO - training batch 1801, loss: 0.295, 57632/60000 datapoints
2025-03-06 20:03:02,277 - INFO - training batch 1851, loss: 0.525, 59232/60000 datapoints
2025-03-06 20:03:02,383 - INFO - validation batch 1, loss: 0.317, 32/10016 datapoints
2025-03-06 20:03:02,554 - INFO - validation batch 51, loss: 0.237, 1632/10016 datapoints
2025-03-06 20:03:02,727 - INFO - validation batch 101, loss: 0.761, 3232/10016 datapoints
2025-03-06 20:03:02,892 - INFO - validation batch 151, loss: 0.292, 4832/10016 datapoints
2025-03-06 20:03:03,059 - INFO - validation batch 201, loss: 0.280, 6432/10016 datapoints
2025-03-06 20:03:03,222 - INFO - validation batch 251, loss: 0.132, 8032/10016 datapoints
2025-03-06 20:03:03,384 - INFO - validation batch 301, loss: 0.178, 9632/10016 datapoints
2025-03-06 20:03:03,425 - INFO - Epoch 574/800 done.
2025-03-06 20:03:03,426 - INFO - Final validation performance:
Loss: 0.314, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:03:03,427 - INFO - Beginning epoch 575/800
2025-03-06 20:03:03,434 - INFO - training batch 1, loss: 0.382, 32/60000 datapoints
2025-03-06 20:03:03,679 - INFO - training batch 51, loss: 0.104, 1632/60000 datapoints
2025-03-06 20:03:03,920 - INFO - training batch 101, loss: 0.275, 3232/60000 datapoints
2025-03-06 20:03:04,145 - INFO - training batch 151, loss: 0.035, 4832/60000 datapoints
2025-03-06 20:03:04,364 - INFO - training batch 201, loss: 0.124, 6432/60000 datapoints
2025-03-06 20:03:04,587 - INFO - training batch 251, loss: 0.218, 8032/60000 datapoints
2025-03-06 20:03:04,816 - INFO - training batch 301, loss: 0.216, 9632/60000 datapoints
2025-03-06 20:03:05,039 - INFO - training batch 351, loss: 0.453, 11232/60000 datapoints
2025-03-06 20:03:05,249 - INFO - training batch 401, loss: 0.361, 12832/60000 datapoints
2025-03-06 20:03:05,458 - INFO - training batch 451, loss: 0.160, 14432/60000 datapoints
2025-03-06 20:03:05,678 - INFO - training batch 501, loss: 0.229, 16032/60000 datapoints
2025-03-06 20:03:05,887 - INFO - training batch 551, loss: 0.149, 17632/60000 datapoints
2025-03-06 20:03:06,118 - INFO - training batch 601, loss: 0.227, 19232/60000 datapoints
2025-03-06 20:03:06,329 - INFO - training batch 651, loss: 0.365, 20832/60000 datapoints
2025-03-06 20:03:06,535 - INFO - training batch 701, loss: 0.377, 22432/60000 datapoints
2025-03-06 20:03:06,745 - INFO - training batch 751, loss: 0.103, 24032/60000 datapoints
2025-03-06 20:03:06,953 - INFO - training batch 801, loss: 0.105, 25632/60000 datapoints
2025-03-06 20:03:07,157 - INFO - training batch 851, loss: 0.315, 27232/60000 datapoints
2025-03-06 20:03:07,364 - INFO - training batch 901, loss: 0.535, 28832/60000 datapoints
2025-03-06 20:03:07,575 - INFO - training batch 951, loss: 0.367, 30432/60000 datapoints
2025-03-06 20:03:07,784 - INFO - training batch 1001, loss: 0.377, 32032/60000 datapoints
2025-03-06 20:03:07,993 - INFO - training batch 1051, loss: 0.495, 33632/60000 datapoints
2025-03-06 20:03:08,208 - INFO - training batch 1101, loss: 0.554, 35232/60000 datapoints
2025-03-06 20:03:08,416 - INFO - training batch 1151, loss: 0.369, 36832/60000 datapoints
2025-03-06 20:03:08,636 - INFO - training batch 1201, loss: 0.289, 38432/60000 datapoints
2025-03-06 20:03:08,854 - INFO - training batch 1251, loss: 0.222, 40032/60000 datapoints
2025-03-06 20:03:09,072 - INFO - training batch 1301, loss: 0.365, 41632/60000 datapoints
2025-03-06 20:03:09,286 - INFO - training batch 1351, loss: 0.345, 43232/60000 datapoints
2025-03-06 20:03:09,500 - INFO - training batch 1401, loss: 0.309, 44832/60000 datapoints
2025-03-06 20:03:09,721 - INFO - training batch 1451, loss: 0.867, 46432/60000 datapoints
2025-03-06 20:03:09,938 - INFO - training batch 1501, loss: 0.425, 48032/60000 datapoints
2025-03-06 20:03:10,157 - INFO - training batch 1551, loss: 0.084, 49632/60000 datapoints
2025-03-06 20:03:10,370 - INFO - training batch 1601, loss: 0.353, 51232/60000 datapoints
2025-03-06 20:03:10,587 - INFO - training batch 1651, loss: 0.391, 52832/60000 datapoints
2025-03-06 20:03:10,823 - INFO - training batch 1701, loss: 0.566, 54432/60000 datapoints
2025-03-06 20:03:11,037 - INFO - training batch 1751, loss: 0.262, 56032/60000 datapoints
2025-03-06 20:03:11,260 - INFO - training batch 1801, loss: 0.361, 57632/60000 datapoints
2025-03-06 20:03:11,533 - INFO - training batch 1851, loss: 0.056, 59232/60000 datapoints
2025-03-06 20:03:11,655 - INFO - validation batch 1, loss: 0.214, 32/10016 datapoints
2025-03-06 20:03:11,831 - INFO - validation batch 51, loss: 0.157, 1632/10016 datapoints
2025-03-06 20:03:12,013 - INFO - validation batch 101, loss: 0.172, 3232/10016 datapoints
2025-03-06 20:03:12,207 - INFO - validation batch 151, loss: 0.161, 4832/10016 datapoints
2025-03-06 20:03:12,417 - INFO - validation batch 201, loss: 0.284, 6432/10016 datapoints
2025-03-06 20:03:12,627 - INFO - validation batch 251, loss: 0.164, 8032/10016 datapoints
2025-03-06 20:03:12,882 - INFO - validation batch 301, loss: 0.328, 9632/10016 datapoints
2025-03-06 20:03:12,929 - INFO - Epoch 575/800 done.
2025-03-06 20:03:12,929 - INFO - Final validation performance:
Loss: 0.211, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:03:12,930 - INFO - Beginning epoch 576/800
2025-03-06 20:03:12,938 - INFO - training batch 1, loss: 0.582, 32/60000 datapoints
2025-03-06 20:03:13,170 - INFO - training batch 51, loss: 0.061, 1632/60000 datapoints
2025-03-06 20:03:13,423 - INFO - training batch 101, loss: 0.279, 3232/60000 datapoints
2025-03-06 20:03:13,690 - INFO - training batch 151, loss: 0.251, 4832/60000 datapoints
2025-03-06 20:03:14,025 - INFO - training batch 201, loss: 0.135, 6432/60000 datapoints
2025-03-06 20:03:14,353 - INFO - training batch 251, loss: 0.552, 8032/60000 datapoints
2025-03-06 20:03:14,578 - INFO - training batch 301, loss: 0.192, 9632/60000 datapoints
2025-03-06 20:03:14,953 - INFO - training batch 351, loss: 0.383, 11232/60000 datapoints
2025-03-06 20:03:15,248 - INFO - training batch 401, loss: 0.248, 12832/60000 datapoints
2025-03-06 20:03:15,541 - INFO - training batch 451, loss: 0.335, 14432/60000 datapoints
2025-03-06 20:03:15,777 - INFO - training batch 501, loss: 0.226, 16032/60000 datapoints
2025-03-06 20:03:16,065 - INFO - training batch 551, loss: 0.065, 17632/60000 datapoints
2025-03-06 20:03:16,303 - INFO - training batch 601, loss: 0.151, 19232/60000 datapoints
2025-03-06 20:03:16,531 - INFO - training batch 651, loss: 0.102, 20832/60000 datapoints
2025-03-06 20:03:16,766 - INFO - training batch 701, loss: 0.191, 22432/60000 datapoints
2025-03-06 20:03:16,996 - INFO - training batch 751, loss: 0.364, 24032/60000 datapoints
2025-03-06 20:03:17,246 - INFO - training batch 801, loss: 0.243, 25632/60000 datapoints
2025-03-06 20:03:17,486 - INFO - training batch 851, loss: 0.307, 27232/60000 datapoints
2025-03-06 20:03:17,724 - INFO - training batch 901, loss: 0.197, 28832/60000 datapoints
2025-03-06 20:03:17,954 - INFO - training batch 951, loss: 0.509, 30432/60000 datapoints
2025-03-06 20:03:18,242 - INFO - training batch 1001, loss: 0.094, 32032/60000 datapoints
2025-03-06 20:03:18,473 - INFO - training batch 1051, loss: 0.169, 33632/60000 datapoints
2025-03-06 20:03:18,703 - INFO - training batch 1101, loss: 0.367, 35232/60000 datapoints
2025-03-06 20:03:18,938 - INFO - training batch 1151, loss: 0.175, 36832/60000 datapoints
2025-03-06 20:03:19,167 - INFO - training batch 1201, loss: 0.290, 38432/60000 datapoints
2025-03-06 20:03:19,427 - INFO - training batch 1251, loss: 0.286, 40032/60000 datapoints
2025-03-06 20:03:19,683 - INFO - training batch 1301, loss: 0.528, 41632/60000 datapoints
2025-03-06 20:03:19,915 - INFO - training batch 1351, loss: 0.260, 43232/60000 datapoints
2025-03-06 20:03:20,153 - INFO - training batch 1401, loss: 0.196, 44832/60000 datapoints
2025-03-06 20:03:20,388 - INFO - training batch 1451, loss: 0.554, 46432/60000 datapoints
2025-03-06 20:03:20,648 - INFO - training batch 1501, loss: 0.088, 48032/60000 datapoints
2025-03-06 20:03:20,897 - INFO - training batch 1551, loss: 0.102, 49632/60000 datapoints
2025-03-06 20:03:21,128 - INFO - training batch 1601, loss: 0.190, 51232/60000 datapoints
2025-03-06 20:03:21,367 - INFO - training batch 1651, loss: 0.136, 52832/60000 datapoints
2025-03-06 20:03:21,598 - INFO - training batch 1701, loss: 0.403, 54432/60000 datapoints
2025-03-06 20:03:21,828 - INFO - training batch 1751, loss: 0.265, 56032/60000 datapoints
2025-03-06 20:03:22,058 - INFO - training batch 1801, loss: 0.291, 57632/60000 datapoints
2025-03-06 20:03:22,290 - INFO - training batch 1851, loss: 0.300, 59232/60000 datapoints
2025-03-06 20:03:22,430 - INFO - validation batch 1, loss: 0.221, 32/10016 datapoints
2025-03-06 20:03:22,619 - INFO - validation batch 51, loss: 0.142, 1632/10016 datapoints
2025-03-06 20:03:22,798 - INFO - validation batch 101, loss: 0.326, 3232/10016 datapoints
2025-03-06 20:03:23,030 - INFO - validation batch 151, loss: 0.257, 4832/10016 datapoints
2025-03-06 20:03:23,245 - INFO - validation batch 201, loss: 0.165, 6432/10016 datapoints
2025-03-06 20:03:23,453 - INFO - validation batch 251, loss: 0.259, 8032/10016 datapoints
2025-03-06 20:03:23,657 - INFO - validation batch 301, loss: 0.166, 9632/10016 datapoints
2025-03-06 20:03:23,713 - INFO - Epoch 576/800 done.
2025-03-06 20:03:23,713 - INFO - Final validation performance:
Loss: 0.219, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:03:23,715 - INFO - Beginning epoch 577/800
2025-03-06 20:03:23,726 - INFO - training batch 1, loss: 0.111, 32/60000 datapoints
2025-03-06 20:03:24,127 - INFO - training batch 51, loss: 0.146, 1632/60000 datapoints
2025-03-06 20:03:24,529 - INFO - training batch 101, loss: 0.414, 3232/60000 datapoints
2025-03-06 20:03:24,824 - INFO - training batch 151, loss: 0.478, 4832/60000 datapoints
2025-03-06 20:03:25,119 - INFO - training batch 201, loss: 0.439, 6432/60000 datapoints
2025-03-06 20:03:25,411 - INFO - training batch 251, loss: 0.254, 8032/60000 datapoints
2025-03-06 20:03:25,680 - INFO - training batch 301, loss: 0.725, 9632/60000 datapoints
2025-03-06 20:03:25,976 - INFO - training batch 351, loss: 0.249, 11232/60000 datapoints
2025-03-06 20:03:26,261 - INFO - training batch 401, loss: 0.171, 12832/60000 datapoints
2025-03-06 20:03:26,540 - INFO - training batch 451, loss: 0.138, 14432/60000 datapoints
2025-03-06 20:03:26,805 - INFO - training batch 501, loss: 0.618, 16032/60000 datapoints
2025-03-06 20:03:27,195 - INFO - training batch 551, loss: 0.142, 17632/60000 datapoints
2025-03-06 20:03:27,450 - INFO - training batch 601, loss: 0.204, 19232/60000 datapoints
2025-03-06 20:03:27,722 - INFO - training batch 651, loss: 0.454, 20832/60000 datapoints
2025-03-06 20:03:28,000 - INFO - training batch 701, loss: 0.348, 22432/60000 datapoints
2025-03-06 20:03:28,280 - INFO - training batch 751, loss: 0.209, 24032/60000 datapoints
2025-03-06 20:03:28,536 - INFO - training batch 801, loss: 0.094, 25632/60000 datapoints
2025-03-06 20:03:28,822 - INFO - training batch 851, loss: 0.769, 27232/60000 datapoints
2025-03-06 20:03:29,131 - INFO - training batch 901, loss: 0.399, 28832/60000 datapoints
2025-03-06 20:03:29,398 - INFO - training batch 951, loss: 0.198, 30432/60000 datapoints
2025-03-06 20:03:29,670 - INFO - training batch 1001, loss: 0.113, 32032/60000 datapoints
2025-03-06 20:03:29,912 - INFO - training batch 1051, loss: 0.162, 33632/60000 datapoints
2025-03-06 20:03:30,155 - INFO - training batch 1101, loss: 0.114, 35232/60000 datapoints
2025-03-06 20:03:30,401 - INFO - training batch 1151, loss: 0.438, 36832/60000 datapoints
2025-03-06 20:03:30,643 - INFO - training batch 1201, loss: 0.158, 38432/60000 datapoints
2025-03-06 20:03:30,929 - INFO - training batch 1251, loss: 0.539, 40032/60000 datapoints
2025-03-06 20:03:31,251 - INFO - training batch 1301, loss: 0.350, 41632/60000 datapoints
2025-03-06 20:03:31,539 - INFO - training batch 1351, loss: 0.178, 43232/60000 datapoints
2025-03-06 20:03:31,787 - INFO - training batch 1401, loss: 0.149, 44832/60000 datapoints
2025-03-06 20:03:32,046 - INFO - training batch 1451, loss: 0.170, 46432/60000 datapoints
2025-03-06 20:03:32,304 - INFO - training batch 1501, loss: 0.198, 48032/60000 datapoints
2025-03-06 20:03:32,551 - INFO - training batch 1551, loss: 0.467, 49632/60000 datapoints
2025-03-06 20:03:32,806 - INFO - training batch 1601, loss: 0.178, 51232/60000 datapoints
2025-03-06 20:03:33,080 - INFO - training batch 1651, loss: 0.267, 52832/60000 datapoints
2025-03-06 20:03:33,343 - INFO - training batch 1701, loss: 0.069, 54432/60000 datapoints
2025-03-06 20:03:33,607 - INFO - training batch 1751, loss: 0.240, 56032/60000 datapoints
2025-03-06 20:03:33,864 - INFO - training batch 1801, loss: 0.439, 57632/60000 datapoints
2025-03-06 20:03:34,139 - INFO - training batch 1851, loss: 0.156, 59232/60000 datapoints
2025-03-06 20:03:34,273 - INFO - validation batch 1, loss: 0.128, 32/10016 datapoints
2025-03-06 20:03:34,509 - INFO - validation batch 51, loss: 0.209, 1632/10016 datapoints
2025-03-06 20:03:34,878 - INFO - validation batch 101, loss: 0.047, 3232/10016 datapoints
2025-03-06 20:03:35,202 - INFO - validation batch 151, loss: 0.139, 4832/10016 datapoints
2025-03-06 20:03:35,576 - INFO - validation batch 201, loss: 0.376, 6432/10016 datapoints
2025-03-06 20:03:35,828 - INFO - validation batch 251, loss: 0.183, 8032/10016 datapoints
2025-03-06 20:03:36,084 - INFO - validation batch 301, loss: 0.123, 9632/10016 datapoints
2025-03-06 20:03:36,139 - INFO - Epoch 577/800 done.
2025-03-06 20:03:36,140 - INFO - Final validation performance:
Loss: 0.172, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:03:36,140 - INFO - Beginning epoch 578/800
2025-03-06 20:03:36,150 - INFO - training batch 1, loss: 0.116, 32/60000 datapoints
2025-03-06 20:03:36,454 - INFO - training batch 51, loss: 0.238, 1632/60000 datapoints
2025-03-06 20:03:36,757 - INFO - training batch 101, loss: 0.112, 3232/60000 datapoints
2025-03-06 20:03:37,029 - INFO - training batch 151, loss: 0.167, 4832/60000 datapoints
2025-03-06 20:03:37,287 - INFO - training batch 201, loss: 0.167, 6432/60000 datapoints
2025-03-06 20:03:37,573 - INFO - training batch 251, loss: 0.124, 8032/60000 datapoints
2025-03-06 20:03:37,897 - INFO - training batch 301, loss: 0.198, 9632/60000 datapoints
2025-03-06 20:03:38,168 - INFO - training batch 351, loss: 0.242, 11232/60000 datapoints
2025-03-06 20:03:38,443 - INFO - training batch 401, loss: 0.604, 12832/60000 datapoints
2025-03-06 20:03:38,728 - INFO - training batch 451, loss: 0.236, 14432/60000 datapoints
2025-03-06 20:03:39,004 - INFO - training batch 501, loss: 0.167, 16032/60000 datapoints
2025-03-06 20:03:39,243 - INFO - training batch 551, loss: 0.370, 17632/60000 datapoints
2025-03-06 20:03:39,484 - INFO - training batch 601, loss: 0.175, 19232/60000 datapoints
2025-03-06 20:03:39,733 - INFO - training batch 651, loss: 0.247, 20832/60000 datapoints
2025-03-06 20:03:39,968 - INFO - training batch 701, loss: 0.079, 22432/60000 datapoints
2025-03-06 20:03:40,197 - INFO - training batch 751, loss: 0.160, 24032/60000 datapoints
2025-03-06 20:03:40,446 - INFO - training batch 801, loss: 0.515, 25632/60000 datapoints
2025-03-06 20:03:40,700 - INFO - training batch 851, loss: 0.323, 27232/60000 datapoints
2025-03-06 20:03:40,951 - INFO - training batch 901, loss: 0.412, 28832/60000 datapoints
2025-03-06 20:03:41,182 - INFO - training batch 951, loss: 0.172, 30432/60000 datapoints
2025-03-06 20:03:41,451 - INFO - training batch 1001, loss: 0.150, 32032/60000 datapoints
2025-03-06 20:03:41,725 - INFO - training batch 1051, loss: 0.113, 33632/60000 datapoints
2025-03-06 20:03:41,989 - INFO - training batch 1101, loss: 0.325, 35232/60000 datapoints
2025-03-06 20:03:42,252 - INFO - training batch 1151, loss: 0.457, 36832/60000 datapoints
2025-03-06 20:03:42,555 - INFO - training batch 1201, loss: 0.188, 38432/60000 datapoints
2025-03-06 20:03:42,856 - INFO - training batch 1251, loss: 0.627, 40032/60000 datapoints
2025-03-06 20:03:43,116 - INFO - training batch 1301, loss: 0.332, 41632/60000 datapoints
2025-03-06 20:03:43,368 - INFO - training batch 1351, loss: 0.194, 43232/60000 datapoints
2025-03-06 20:03:43,632 - INFO - training batch 1401, loss: 0.293, 44832/60000 datapoints
2025-03-06 20:03:43,866 - INFO - training batch 1451, loss: 0.484, 46432/60000 datapoints
2025-03-06 20:03:44,120 - INFO - training batch 1501, loss: 0.129, 48032/60000 datapoints
2025-03-06 20:03:44,360 - INFO - training batch 1551, loss: 0.439, 49632/60000 datapoints
2025-03-06 20:03:44,628 - INFO - training batch 1601, loss: 0.227, 51232/60000 datapoints
2025-03-06 20:03:44,857 - INFO - training batch 1651, loss: 0.116, 52832/60000 datapoints
2025-03-06 20:03:45,122 - INFO - training batch 1701, loss: 0.211, 54432/60000 datapoints
2025-03-06 20:03:45,375 - INFO - training batch 1751, loss: 0.371, 56032/60000 datapoints
2025-03-06 20:03:45,613 - INFO - training batch 1801, loss: 0.244, 57632/60000 datapoints
2025-03-06 20:03:45,868 - INFO - training batch 1851, loss: 0.353, 59232/60000 datapoints
2025-03-06 20:03:45,997 - INFO - validation batch 1, loss: 0.773, 32/10016 datapoints
2025-03-06 20:03:46,192 - INFO - validation batch 51, loss: 0.121, 1632/10016 datapoints
2025-03-06 20:03:46,431 - INFO - validation batch 101, loss: 0.163, 3232/10016 datapoints
2025-03-06 20:03:46,639 - INFO - validation batch 151, loss: 0.202, 4832/10016 datapoints
2025-03-06 20:03:46,847 - INFO - validation batch 201, loss: 0.247, 6432/10016 datapoints
2025-03-06 20:03:47,102 - INFO - validation batch 251, loss: 0.398, 8032/10016 datapoints
2025-03-06 20:03:47,309 - INFO - validation batch 301, loss: 0.121, 9632/10016 datapoints
2025-03-06 20:03:47,354 - INFO - Epoch 578/800 done.
2025-03-06 20:03:47,354 - INFO - Final validation performance:
Loss: 0.289, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:03:47,355 - INFO - Beginning epoch 579/800
2025-03-06 20:03:47,363 - INFO - training batch 1, loss: 0.175, 32/60000 datapoints
2025-03-06 20:03:47,611 - INFO - training batch 51, loss: 0.477, 1632/60000 datapoints
2025-03-06 20:03:47,869 - INFO - training batch 101, loss: 0.148, 3232/60000 datapoints
2025-03-06 20:03:48,203 - INFO - training batch 151, loss: 0.272, 4832/60000 datapoints
2025-03-06 20:03:48,477 - INFO - training batch 201, loss: 0.137, 6432/60000 datapoints
2025-03-06 20:03:48,728 - INFO - training batch 251, loss: 0.074, 8032/60000 datapoints
2025-03-06 20:03:49,093 - INFO - training batch 301, loss: 0.279, 9632/60000 datapoints
2025-03-06 20:03:49,395 - INFO - training batch 351, loss: 0.222, 11232/60000 datapoints
2025-03-06 20:03:49,677 - INFO - training batch 401, loss: 0.058, 12832/60000 datapoints
2025-03-06 20:03:49,949 - INFO - training batch 451, loss: 0.256, 14432/60000 datapoints
2025-03-06 20:03:50,193 - INFO - training batch 501, loss: 0.245, 16032/60000 datapoints
2025-03-06 20:03:50,480 - INFO - training batch 551, loss: 0.186, 17632/60000 datapoints
2025-03-06 20:03:50,796 - INFO - training batch 601, loss: 0.361, 19232/60000 datapoints
2025-03-06 20:03:51,072 - INFO - training batch 651, loss: 0.317, 20832/60000 datapoints
2025-03-06 20:03:51,316 - INFO - training batch 701, loss: 0.116, 22432/60000 datapoints
2025-03-06 20:03:51,598 - INFO - training batch 751, loss: 0.120, 24032/60000 datapoints
2025-03-06 20:03:51,865 - INFO - training batch 801, loss: 0.274, 25632/60000 datapoints
2025-03-06 20:03:52,123 - INFO - training batch 851, loss: 0.175, 27232/60000 datapoints
2025-03-06 20:03:52,627 - INFO - training batch 901, loss: 0.219, 28832/60000 datapoints
2025-03-06 20:03:52,903 - INFO - training batch 951, loss: 0.377, 30432/60000 datapoints
2025-03-06 20:03:53,166 - INFO - training batch 1001, loss: 0.434, 32032/60000 datapoints
2025-03-06 20:03:53,423 - INFO - training batch 1051, loss: 0.370, 33632/60000 datapoints
2025-03-06 20:03:53,679 - INFO - training batch 1101, loss: 0.149, 35232/60000 datapoints
2025-03-06 20:03:53,941 - INFO - training batch 1151, loss: 0.280, 36832/60000 datapoints
2025-03-06 20:03:54,222 - INFO - training batch 1201, loss: 0.208, 38432/60000 datapoints
2025-03-06 20:03:54,530 - INFO - training batch 1251, loss: 0.335, 40032/60000 datapoints
2025-03-06 20:03:54,787 - INFO - training batch 1301, loss: 0.076, 41632/60000 datapoints
2025-03-06 20:03:55,122 - INFO - training batch 1351, loss: 0.304, 43232/60000 datapoints
2025-03-06 20:03:55,362 - INFO - training batch 1401, loss: 0.142, 44832/60000 datapoints
2025-03-06 20:03:55,630 - INFO - training batch 1451, loss: 0.171, 46432/60000 datapoints
2025-03-06 20:03:55,882 - INFO - training batch 1501, loss: 0.246, 48032/60000 datapoints
2025-03-06 20:03:56,124 - INFO - training batch 1551, loss: 0.067, 49632/60000 datapoints
2025-03-06 20:03:56,365 - INFO - training batch 1601, loss: 0.229, 51232/60000 datapoints
2025-03-06 20:03:56,609 - INFO - training batch 1651, loss: 0.527, 52832/60000 datapoints
2025-03-06 20:03:56,869 - INFO - training batch 1701, loss: 0.187, 54432/60000 datapoints
2025-03-06 20:03:57,127 - INFO - training batch 1751, loss: 0.252, 56032/60000 datapoints
2025-03-06 20:03:57,405 - INFO - training batch 1801, loss: 0.619, 57632/60000 datapoints
2025-03-06 20:03:57,638 - INFO - training batch 1851, loss: 0.137, 59232/60000 datapoints
2025-03-06 20:03:57,760 - INFO - validation batch 1, loss: 0.095, 32/10016 datapoints
2025-03-06 20:03:57,952 - INFO - validation batch 51, loss: 0.511, 1632/10016 datapoints
2025-03-06 20:03:58,132 - INFO - validation batch 101, loss: 0.423, 3232/10016 datapoints
2025-03-06 20:03:58,310 - INFO - validation batch 151, loss: 0.463, 4832/10016 datapoints
2025-03-06 20:03:58,503 - INFO - validation batch 201, loss: 0.134, 6432/10016 datapoints
2025-03-06 20:03:58,686 - INFO - validation batch 251, loss: 0.241, 8032/10016 datapoints
2025-03-06 20:03:58,866 - INFO - validation batch 301, loss: 0.381, 9632/10016 datapoints
2025-03-06 20:03:58,912 - INFO - Epoch 579/800 done.
2025-03-06 20:03:58,912 - INFO - Final validation performance:
Loss: 0.321, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:03:58,913 - INFO - Beginning epoch 580/800
2025-03-06 20:03:58,922 - INFO - training batch 1, loss: 0.184, 32/60000 datapoints
2025-03-06 20:03:59,143 - INFO - training batch 51, loss: 0.122, 1632/60000 datapoints
2025-03-06 20:03:59,403 - INFO - training batch 101, loss: 0.354, 3232/60000 datapoints
2025-03-06 20:03:59,635 - INFO - training batch 151, loss: 0.380, 4832/60000 datapoints
2025-03-06 20:03:59,856 - INFO - training batch 201, loss: 0.246, 6432/60000 datapoints
2025-03-06 20:04:00,086 - INFO - training batch 251, loss: 0.093, 8032/60000 datapoints
2025-03-06 20:04:00,300 - INFO - training batch 301, loss: 0.420, 9632/60000 datapoints
2025-03-06 20:04:00,515 - INFO - training batch 351, loss: 0.204, 11232/60000 datapoints
2025-03-06 20:04:00,725 - INFO - training batch 401, loss: 0.155, 12832/60000 datapoints
2025-03-06 20:04:00,940 - INFO - training batch 451, loss: 0.177, 14432/60000 datapoints
2025-03-06 20:04:01,152 - INFO - training batch 501, loss: 0.116, 16032/60000 datapoints
2025-03-06 20:04:01,361 - INFO - training batch 551, loss: 0.260, 17632/60000 datapoints
2025-03-06 20:04:01,586 - INFO - training batch 601, loss: 0.258, 19232/60000 datapoints
2025-03-06 20:04:01,794 - INFO - training batch 651, loss: 0.223, 20832/60000 datapoints
2025-03-06 20:04:02,009 - INFO - training batch 701, loss: 0.309, 22432/60000 datapoints
2025-03-06 20:04:02,234 - INFO - training batch 751, loss: 0.157, 24032/60000 datapoints
2025-03-06 20:04:02,438 - INFO - training batch 801, loss: 0.099, 25632/60000 datapoints
2025-03-06 20:04:02,652 - INFO - training batch 851, loss: 0.129, 27232/60000 datapoints
2025-03-06 20:04:02,855 - INFO - training batch 901, loss: 0.171, 28832/60000 datapoints
2025-03-06 20:04:03,065 - INFO - training batch 951, loss: 0.090, 30432/60000 datapoints
2025-03-06 20:04:03,279 - INFO - training batch 1001, loss: 0.161, 32032/60000 datapoints
2025-03-06 20:04:03,527 - INFO - training batch 1051, loss: 0.181, 33632/60000 datapoints
2025-03-06 20:04:03,763 - INFO - training batch 1101, loss: 0.141, 35232/60000 datapoints
2025-03-06 20:04:03,999 - INFO - training batch 1151, loss: 0.303, 36832/60000 datapoints
2025-03-06 20:04:04,217 - INFO - training batch 1201, loss: 0.218, 38432/60000 datapoints
2025-03-06 20:04:04,511 - INFO - training batch 1251, loss: 0.392, 40032/60000 datapoints
2025-03-06 20:04:04,750 - INFO - training batch 1301, loss: 0.362, 41632/60000 datapoints
2025-03-06 20:04:05,006 - INFO - training batch 1351, loss: 0.340, 43232/60000 datapoints
2025-03-06 20:04:05,274 - INFO - training batch 1401, loss: 0.240, 44832/60000 datapoints
2025-03-06 20:04:05,568 - INFO - training batch 1451, loss: 0.131, 46432/60000 datapoints
2025-03-06 20:04:05,821 - INFO - training batch 1501, loss: 0.259, 48032/60000 datapoints
2025-03-06 20:04:06,064 - INFO - training batch 1551, loss: 0.096, 49632/60000 datapoints
2025-03-06 20:04:06,383 - INFO - training batch 1601, loss: 0.185, 51232/60000 datapoints
2025-03-06 20:04:06,683 - INFO - training batch 1651, loss: 0.214, 52832/60000 datapoints
2025-03-06 20:04:06,933 - INFO - training batch 1701, loss: 0.223, 54432/60000 datapoints
2025-03-06 20:04:07,172 - INFO - training batch 1751, loss: 0.088, 56032/60000 datapoints
2025-03-06 20:04:07,435 - INFO - training batch 1801, loss: 0.171, 57632/60000 datapoints
2025-03-06 20:04:07,683 - INFO - training batch 1851, loss: 0.101, 59232/60000 datapoints
2025-03-06 20:04:07,847 - INFO - validation batch 1, loss: 0.217, 32/10016 datapoints
2025-03-06 20:04:08,052 - INFO - validation batch 51, loss: 0.189, 1632/10016 datapoints
2025-03-06 20:04:08,268 - INFO - validation batch 101, loss: 0.268, 3232/10016 datapoints
2025-03-06 20:04:08,463 - INFO - validation batch 151, loss: 0.149, 4832/10016 datapoints
2025-03-06 20:04:08,710 - INFO - validation batch 201, loss: 0.286, 6432/10016 datapoints
2025-03-06 20:04:08,927 - INFO - validation batch 251, loss: 0.238, 8032/10016 datapoints
2025-03-06 20:04:09,180 - INFO - validation batch 301, loss: 0.253, 9632/10016 datapoints
2025-03-06 20:04:09,237 - INFO - Epoch 580/800 done.
2025-03-06 20:04:09,238 - INFO - Final validation performance:
Loss: 0.229, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:04:09,238 - INFO - Beginning epoch 581/800
2025-03-06 20:04:09,247 - INFO - training batch 1, loss: 0.178, 32/60000 datapoints
2025-03-06 20:04:09,519 - INFO - training batch 51, loss: 0.292, 1632/60000 datapoints
2025-03-06 20:04:09,762 - INFO - training batch 101, loss: 0.198, 3232/60000 datapoints
2025-03-06 20:04:10,020 - INFO - training batch 151, loss: 0.462, 4832/60000 datapoints
2025-03-06 20:04:10,262 - INFO - training batch 201, loss: 0.289, 6432/60000 datapoints
2025-03-06 20:04:10,498 - INFO - training batch 251, loss: 0.137, 8032/60000 datapoints
2025-03-06 20:04:10,743 - INFO - training batch 301, loss: 0.177, 9632/60000 datapoints
2025-03-06 20:04:10,984 - INFO - training batch 351, loss: 0.366, 11232/60000 datapoints
2025-03-06 20:04:11,236 - INFO - training batch 401, loss: 0.375, 12832/60000 datapoints
2025-03-06 20:04:11,470 - INFO - training batch 451, loss: 0.137, 14432/60000 datapoints
2025-03-06 20:04:11,708 - INFO - training batch 501, loss: 0.232, 16032/60000 datapoints
2025-03-06 20:04:11,944 - INFO - training batch 551, loss: 0.131, 17632/60000 datapoints
2025-03-06 20:04:12,176 - INFO - training batch 601, loss: 0.303, 19232/60000 datapoints
2025-03-06 20:04:12,410 - INFO - training batch 651, loss: 0.294, 20832/60000 datapoints
2025-03-06 20:04:12,649 - INFO - training batch 701, loss: 0.323, 22432/60000 datapoints
2025-03-06 20:04:12,884 - INFO - training batch 751, loss: 0.323, 24032/60000 datapoints
2025-03-06 20:04:13,115 - INFO - training batch 801, loss: 0.100, 25632/60000 datapoints
2025-03-06 20:04:13,342 - INFO - training batch 851, loss: 0.148, 27232/60000 datapoints
2025-03-06 20:04:13,583 - INFO - training batch 901, loss: 0.046, 28832/60000 datapoints
2025-03-06 20:04:13,814 - INFO - training batch 951, loss: 0.105, 30432/60000 datapoints
2025-03-06 20:04:14,036 - INFO - training batch 1001, loss: 0.335, 32032/60000 datapoints
2025-03-06 20:04:14,267 - INFO - training batch 1051, loss: 0.291, 33632/60000 datapoints
2025-03-06 20:04:14,517 - INFO - training batch 1101, loss: 0.238, 35232/60000 datapoints
2025-03-06 20:04:14,752 - INFO - training batch 1151, loss: 0.287, 36832/60000 datapoints
2025-03-06 20:04:14,975 - INFO - training batch 1201, loss: 0.334, 38432/60000 datapoints
2025-03-06 20:04:15,188 - INFO - training batch 1251, loss: 0.130, 40032/60000 datapoints
2025-03-06 20:04:15,401 - INFO - training batch 1301, loss: 0.166, 41632/60000 datapoints
2025-03-06 20:04:15,615 - INFO - training batch 1351, loss: 0.206, 43232/60000 datapoints
2025-03-06 20:04:15,827 - INFO - training batch 1401, loss: 0.313, 44832/60000 datapoints
2025-03-06 20:04:16,042 - INFO - training batch 1451, loss: 0.152, 46432/60000 datapoints
2025-03-06 20:04:16,256 - INFO - training batch 1501, loss: 0.376, 48032/60000 datapoints
2025-03-06 20:04:16,461 - INFO - training batch 1551, loss: 0.163, 49632/60000 datapoints
2025-03-06 20:04:16,674 - INFO - training batch 1601, loss: 0.196, 51232/60000 datapoints
2025-03-06 20:04:16,881 - INFO - training batch 1651, loss: 0.743, 52832/60000 datapoints
2025-03-06 20:04:17,090 - INFO - training batch 1701, loss: 0.242, 54432/60000 datapoints
2025-03-06 20:04:17,292 - INFO - training batch 1751, loss: 0.165, 56032/60000 datapoints
2025-03-06 20:04:17,493 - INFO - training batch 1801, loss: 0.495, 57632/60000 datapoints
2025-03-06 20:04:17,700 - INFO - training batch 1851, loss: 0.168, 59232/60000 datapoints
2025-03-06 20:04:17,807 - INFO - validation batch 1, loss: 0.360, 32/10016 datapoints
2025-03-06 20:04:17,970 - INFO - validation batch 51, loss: 0.570, 1632/10016 datapoints
2025-03-06 20:04:18,136 - INFO - validation batch 101, loss: 0.156, 3232/10016 datapoints
2025-03-06 20:04:18,299 - INFO - validation batch 151, loss: 0.133, 4832/10016 datapoints
2025-03-06 20:04:18,463 - INFO - validation batch 201, loss: 0.114, 6432/10016 datapoints
2025-03-06 20:04:18,632 - INFO - validation batch 251, loss: 0.239, 8032/10016 datapoints
2025-03-06 20:04:18,797 - INFO - validation batch 301, loss: 0.189, 9632/10016 datapoints
2025-03-06 20:04:18,845 - INFO - Epoch 581/800 done.
2025-03-06 20:04:18,845 - INFO - Final validation performance:
Loss: 0.252, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:04:18,846 - INFO - Beginning epoch 582/800
2025-03-06 20:04:18,853 - INFO - training batch 1, loss: 0.250, 32/60000 datapoints
2025-03-06 20:04:19,059 - INFO - training batch 51, loss: 0.188, 1632/60000 datapoints
2025-03-06 20:04:19,259 - INFO - training batch 101, loss: 0.461, 3232/60000 datapoints
2025-03-06 20:04:19,474 - INFO - training batch 151, loss: 0.187, 4832/60000 datapoints
2025-03-06 20:04:19,691 - INFO - training batch 201, loss: 0.321, 6432/60000 datapoints
2025-03-06 20:04:19,901 - INFO - training batch 251, loss: 0.132, 8032/60000 datapoints
2025-03-06 20:04:20,115 - INFO - training batch 301, loss: 0.344, 9632/60000 datapoints
2025-03-06 20:04:20,323 - INFO - training batch 351, loss: 0.155, 11232/60000 datapoints
2025-03-06 20:04:20,530 - INFO - training batch 401, loss: 0.098, 12832/60000 datapoints
2025-03-06 20:04:20,771 - INFO - training batch 451, loss: 0.126, 14432/60000 datapoints
2025-03-06 20:04:20,989 - INFO - training batch 501, loss: 0.075, 16032/60000 datapoints
2025-03-06 20:04:21,199 - INFO - training batch 551, loss: 0.220, 17632/60000 datapoints
2025-03-06 20:04:21,399 - INFO - training batch 601, loss: 0.233, 19232/60000 datapoints
2025-03-06 20:04:21,608 - INFO - training batch 651, loss: 0.206, 20832/60000 datapoints
2025-03-06 20:04:21,813 - INFO - training batch 701, loss: 0.140, 22432/60000 datapoints
2025-03-06 20:04:22,015 - INFO - training batch 751, loss: 0.081, 24032/60000 datapoints
2025-03-06 20:04:22,221 - INFO - training batch 801, loss: 0.115, 25632/60000 datapoints
2025-03-06 20:04:22,422 - INFO - training batch 851, loss: 0.324, 27232/60000 datapoints
2025-03-06 20:04:22,630 - INFO - training batch 901, loss: 0.223, 28832/60000 datapoints
2025-03-06 20:04:22,831 - INFO - training batch 951, loss: 0.167, 30432/60000 datapoints
2025-03-06 20:04:23,035 - INFO - training batch 1001, loss: 0.048, 32032/60000 datapoints
2025-03-06 20:04:23,238 - INFO - training batch 1051, loss: 0.216, 33632/60000 datapoints
2025-03-06 20:04:23,440 - INFO - training batch 1101, loss: 0.218, 35232/60000 datapoints
2025-03-06 20:04:23,649 - INFO - training batch 1151, loss: 0.396, 36832/60000 datapoints
2025-03-06 20:04:23,854 - INFO - training batch 1201, loss: 0.454, 38432/60000 datapoints
2025-03-06 20:04:24,057 - INFO - training batch 1251, loss: 0.124, 40032/60000 datapoints
2025-03-06 20:04:24,259 - INFO - training batch 1301, loss: 0.181, 41632/60000 datapoints
2025-03-06 20:04:24,466 - INFO - training batch 1351, loss: 0.116, 43232/60000 datapoints
2025-03-06 20:04:24,686 - INFO - training batch 1401, loss: 0.073, 44832/60000 datapoints
2025-03-06 20:04:24,899 - INFO - training batch 1451, loss: 0.201, 46432/60000 datapoints
2025-03-06 20:04:25,104 - INFO - training batch 1501, loss: 0.184, 48032/60000 datapoints
2025-03-06 20:04:25,303 - INFO - training batch 1551, loss: 0.160, 49632/60000 datapoints
2025-03-06 20:04:25,504 - INFO - training batch 1601, loss: 0.459, 51232/60000 datapoints
2025-03-06 20:04:25,712 - INFO - training batch 1651, loss: 0.120, 52832/60000 datapoints
2025-03-06 20:04:25,921 - INFO - training batch 1701, loss: 0.473, 54432/60000 datapoints
2025-03-06 20:04:26,134 - INFO - training batch 1751, loss: 0.097, 56032/60000 datapoints
2025-03-06 20:04:26,337 - INFO - training batch 1801, loss: 0.250, 57632/60000 datapoints
2025-03-06 20:04:26,543 - INFO - training batch 1851, loss: 0.533, 59232/60000 datapoints
2025-03-06 20:04:26,657 - INFO - validation batch 1, loss: 0.214, 32/10016 datapoints
2025-03-06 20:04:26,825 - INFO - validation batch 51, loss: 0.199, 1632/10016 datapoints
2025-03-06 20:04:26,996 - INFO - validation batch 101, loss: 0.245, 3232/10016 datapoints
2025-03-06 20:04:27,163 - INFO - validation batch 151, loss: 0.326, 4832/10016 datapoints
2025-03-06 20:04:27,331 - INFO - validation batch 201, loss: 0.354, 6432/10016 datapoints
2025-03-06 20:04:27,499 - INFO - validation batch 251, loss: 0.161, 8032/10016 datapoints
2025-03-06 20:04:27,671 - INFO - validation batch 301, loss: 0.233, 9632/10016 datapoints
2025-03-06 20:04:27,712 - INFO - Epoch 582/800 done.
2025-03-06 20:04:27,712 - INFO - Final validation performance:
Loss: 0.247, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:04:27,713 - INFO - Beginning epoch 583/800
2025-03-06 20:04:27,725 - INFO - training batch 1, loss: 0.187, 32/60000 datapoints
2025-03-06 20:04:27,947 - INFO - training batch 51, loss: 0.249, 1632/60000 datapoints
2025-03-06 20:04:28,162 - INFO - training batch 101, loss: 0.110, 3232/60000 datapoints
2025-03-06 20:04:28,392 - INFO - training batch 151, loss: 0.236, 4832/60000 datapoints
2025-03-06 20:04:28,605 - INFO - training batch 201, loss: 0.242, 6432/60000 datapoints
2025-03-06 20:04:28,833 - INFO - training batch 251, loss: 0.088, 8032/60000 datapoints
2025-03-06 20:04:29,056 - INFO - training batch 301, loss: 0.232, 9632/60000 datapoints
2025-03-06 20:04:29,277 - INFO - training batch 351, loss: 0.606, 11232/60000 datapoints
2025-03-06 20:04:29,486 - INFO - training batch 401, loss: 0.186, 12832/60000 datapoints
2025-03-06 20:04:29,722 - INFO - training batch 451, loss: 0.230, 14432/60000 datapoints
2025-03-06 20:04:29,936 - INFO - training batch 501, loss: 0.244, 16032/60000 datapoints
2025-03-06 20:04:30,148 - INFO - training batch 551, loss: 0.364, 17632/60000 datapoints
2025-03-06 20:04:30,361 - INFO - training batch 601, loss: 0.281, 19232/60000 datapoints
2025-03-06 20:04:30,584 - INFO - training batch 651, loss: 0.417, 20832/60000 datapoints
2025-03-06 20:04:30,824 - INFO - training batch 701, loss: 0.246, 22432/60000 datapoints
2025-03-06 20:04:31,068 - INFO - training batch 751, loss: 0.076, 24032/60000 datapoints
2025-03-06 20:04:31,286 - INFO - training batch 801, loss: 0.252, 25632/60000 datapoints
2025-03-06 20:04:31,504 - INFO - training batch 851, loss: 0.346, 27232/60000 datapoints
2025-03-06 20:04:31,728 - INFO - training batch 901, loss: 0.565, 28832/60000 datapoints
2025-03-06 20:04:31,945 - INFO - training batch 951, loss: 0.334, 30432/60000 datapoints
2025-03-06 20:04:32,158 - INFO - training batch 1001, loss: 0.091, 32032/60000 datapoints
2025-03-06 20:04:32,372 - INFO - training batch 1051, loss: 0.138, 33632/60000 datapoints
2025-03-06 20:04:32,587 - INFO - training batch 1101, loss: 0.174, 35232/60000 datapoints
2025-03-06 20:04:32,806 - INFO - training batch 1151, loss: 0.401, 36832/60000 datapoints
2025-03-06 20:04:33,018 - INFO - training batch 1201, loss: 0.193, 38432/60000 datapoints
2025-03-06 20:04:33,232 - INFO - training batch 1251, loss: 0.061, 40032/60000 datapoints
2025-03-06 20:04:33,443 - INFO - training batch 1301, loss: 0.299, 41632/60000 datapoints
2025-03-06 20:04:33,658 - INFO - training batch 1351, loss: 0.235, 43232/60000 datapoints
2025-03-06 20:04:33,874 - INFO - training batch 1401, loss: 0.275, 44832/60000 datapoints
2025-03-06 20:04:34,089 - INFO - training batch 1451, loss: 0.109, 46432/60000 datapoints
2025-03-06 20:04:34,301 - INFO - training batch 1501, loss: 0.244, 48032/60000 datapoints
2025-03-06 20:04:34,515 - INFO - training batch 1551, loss: 0.153, 49632/60000 datapoints
2025-03-06 20:04:34,755 - INFO - training batch 1601, loss: 0.295, 51232/60000 datapoints
2025-03-06 20:04:34,974 - INFO - training batch 1651, loss: 0.136, 52832/60000 datapoints
2025-03-06 20:04:35,187 - INFO - training batch 1701, loss: 0.234, 54432/60000 datapoints
2025-03-06 20:04:35,398 - INFO - training batch 1751, loss: 0.662, 56032/60000 datapoints
2025-03-06 20:04:35,614 - INFO - training batch 1801, loss: 0.185, 57632/60000 datapoints
2025-03-06 20:04:35,832 - INFO - training batch 1851, loss: 0.376, 59232/60000 datapoints
2025-03-06 20:04:35,946 - INFO - validation batch 1, loss: 0.066, 32/10016 datapoints
2025-03-06 20:04:36,122 - INFO - validation batch 51, loss: 0.774, 1632/10016 datapoints
2025-03-06 20:04:36,297 - INFO - validation batch 101, loss: 0.196, 3232/10016 datapoints
2025-03-06 20:04:36,468 - INFO - validation batch 151, loss: 0.136, 4832/10016 datapoints
2025-03-06 20:04:36,644 - INFO - validation batch 201, loss: 0.151, 6432/10016 datapoints
2025-03-06 20:04:36,820 - INFO - validation batch 251, loss: 0.222, 8032/10016 datapoints
2025-03-06 20:04:36,998 - INFO - validation batch 301, loss: 0.159, 9632/10016 datapoints
2025-03-06 20:04:37,045 - INFO - Epoch 583/800 done.
2025-03-06 20:04:37,045 - INFO - Final validation performance:
Loss: 0.243, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:04:37,046 - INFO - Beginning epoch 584/800
2025-03-06 20:04:37,053 - INFO - training batch 1, loss: 0.360, 32/60000 datapoints
2025-03-06 20:04:37,270 - INFO - training batch 51, loss: 0.114, 1632/60000 datapoints
2025-03-06 20:04:37,486 - INFO - training batch 101, loss: 0.382, 3232/60000 datapoints
2025-03-06 20:04:37,733 - INFO - training batch 151, loss: 0.260, 4832/60000 datapoints
2025-03-06 20:04:37,950 - INFO - training batch 201, loss: 0.147, 6432/60000 datapoints
2025-03-06 20:04:38,171 - INFO - training batch 251, loss: 0.380, 8032/60000 datapoints
2025-03-06 20:04:38,392 - INFO - training batch 301, loss: 0.392, 9632/60000 datapoints
2025-03-06 20:04:38,605 - INFO - training batch 351, loss: 0.093, 11232/60000 datapoints
2025-03-06 20:04:38,825 - INFO - training batch 401, loss: 0.191, 12832/60000 datapoints
2025-03-06 20:04:39,060 - INFO - training batch 451, loss: 0.247, 14432/60000 datapoints
2025-03-06 20:04:39,270 - INFO - training batch 501, loss: 0.222, 16032/60000 datapoints
2025-03-06 20:04:39,486 - INFO - training batch 551, loss: 0.507, 17632/60000 datapoints
2025-03-06 20:04:39,701 - INFO - training batch 601, loss: 0.302, 19232/60000 datapoints
2025-03-06 20:04:39,909 - INFO - training batch 651, loss: 0.227, 20832/60000 datapoints
2025-03-06 20:04:40,115 - INFO - training batch 701, loss: 0.171, 22432/60000 datapoints
2025-03-06 20:04:40,324 - INFO - training batch 751, loss: 0.428, 24032/60000 datapoints
2025-03-06 20:04:40,530 - INFO - training batch 801, loss: 0.176, 25632/60000 datapoints
2025-03-06 20:04:40,749 - INFO - training batch 851, loss: 0.401, 27232/60000 datapoints
2025-03-06 20:04:40,958 - INFO - training batch 901, loss: 0.222, 28832/60000 datapoints
2025-03-06 20:04:41,170 - INFO - training batch 951, loss: 0.052, 30432/60000 datapoints
2025-03-06 20:04:41,383 - INFO - training batch 1001, loss: 0.299, 32032/60000 datapoints
2025-03-06 20:04:41,589 - INFO - training batch 1051, loss: 0.160, 33632/60000 datapoints
2025-03-06 20:04:41,868 - INFO - training batch 1101, loss: 0.143, 35232/60000 datapoints
2025-03-06 20:04:42,097 - INFO - training batch 1151, loss: 0.342, 36832/60000 datapoints
2025-03-06 20:04:42,302 - INFO - training batch 1201, loss: 0.116, 38432/60000 datapoints
2025-03-06 20:04:42,507 - INFO - training batch 1251, loss: 0.210, 40032/60000 datapoints
2025-03-06 20:04:42,723 - INFO - training batch 1301, loss: 0.260, 41632/60000 datapoints
2025-03-06 20:04:42,931 - INFO - training batch 1351, loss: 0.219, 43232/60000 datapoints
2025-03-06 20:04:43,139 - INFO - training batch 1401, loss: 0.123, 44832/60000 datapoints
2025-03-06 20:04:43,345 - INFO - training batch 1451, loss: 0.133, 46432/60000 datapoints
2025-03-06 20:04:43,552 - INFO - training batch 1501, loss: 0.256, 48032/60000 datapoints
2025-03-06 20:04:43,766 - INFO - training batch 1551, loss: 0.338, 49632/60000 datapoints
2025-03-06 20:04:43,976 - INFO - training batch 1601, loss: 0.518, 51232/60000 datapoints
2025-03-06 20:04:44,182 - INFO - training batch 1651, loss: 0.126, 52832/60000 datapoints
2025-03-06 20:04:44,388 - INFO - training batch 1701, loss: 0.496, 54432/60000 datapoints
2025-03-06 20:04:44,619 - INFO - training batch 1751, loss: 0.142, 56032/60000 datapoints
2025-03-06 20:04:44,853 - INFO - training batch 1801, loss: 0.077, 57632/60000 datapoints
2025-03-06 20:04:45,063 - INFO - training batch 1851, loss: 0.335, 59232/60000 datapoints
2025-03-06 20:04:45,171 - INFO - validation batch 1, loss: 0.277, 32/10016 datapoints
2025-03-06 20:04:45,336 - INFO - validation batch 51, loss: 0.443, 1632/10016 datapoints
2025-03-06 20:04:45,505 - INFO - validation batch 101, loss: 0.342, 3232/10016 datapoints
2025-03-06 20:04:45,675 - INFO - validation batch 151, loss: 0.495, 4832/10016 datapoints
2025-03-06 20:04:45,845 - INFO - validation batch 201, loss: 0.115, 6432/10016 datapoints
2025-03-06 20:04:46,012 - INFO - validation batch 251, loss: 0.372, 8032/10016 datapoints
2025-03-06 20:04:46,180 - INFO - validation batch 301, loss: 0.252, 9632/10016 datapoints
2025-03-06 20:04:46,221 - INFO - Epoch 584/800 done.
2025-03-06 20:04:46,222 - INFO - Final validation performance:
Loss: 0.328, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:04:46,222 - INFO - Beginning epoch 585/800
2025-03-06 20:04:46,233 - INFO - training batch 1, loss: 0.453, 32/60000 datapoints
2025-03-06 20:04:46,438 - INFO - training batch 51, loss: 0.121, 1632/60000 datapoints
2025-03-06 20:04:46,656 - INFO - training batch 101, loss: 0.229, 3232/60000 datapoints
2025-03-06 20:04:46,860 - INFO - training batch 151, loss: 0.134, 4832/60000 datapoints
2025-03-06 20:04:47,070 - INFO - training batch 201, loss: 0.184, 6432/60000 datapoints
2025-03-06 20:04:47,276 - INFO - training batch 251, loss: 0.165, 8032/60000 datapoints
2025-03-06 20:04:47,480 - INFO - training batch 301, loss: 0.121, 9632/60000 datapoints
2025-03-06 20:04:47,700 - INFO - training batch 351, loss: 0.245, 11232/60000 datapoints
2025-03-06 20:04:47,914 - INFO - training batch 401, loss: 0.569, 12832/60000 datapoints
2025-03-06 20:04:48,137 - INFO - training batch 451, loss: 0.087, 14432/60000 datapoints
2025-03-06 20:04:48,351 - INFO - training batch 501, loss: 0.248, 16032/60000 datapoints
2025-03-06 20:04:48,602 - INFO - training batch 551, loss: 0.142, 17632/60000 datapoints
2025-03-06 20:04:48,826 - INFO - training batch 601, loss: 0.287, 19232/60000 datapoints
2025-03-06 20:04:49,045 - INFO - training batch 651, loss: 0.500, 20832/60000 datapoints
2025-03-06 20:04:49,297 - INFO - training batch 701, loss: 0.176, 22432/60000 datapoints
2025-03-06 20:04:49,517 - INFO - training batch 751, loss: 0.184, 24032/60000 datapoints
2025-03-06 20:04:49,808 - INFO - training batch 801, loss: 0.129, 25632/60000 datapoints
2025-03-06 20:04:50,033 - INFO - training batch 851, loss: 0.184, 27232/60000 datapoints
2025-03-06 20:04:50,257 - INFO - training batch 901, loss: 0.159, 28832/60000 datapoints
2025-03-06 20:04:50,472 - INFO - training batch 951, loss: 0.299, 30432/60000 datapoints
2025-03-06 20:04:50,686 - INFO - training batch 1001, loss: 0.204, 32032/60000 datapoints
2025-03-06 20:04:50,923 - INFO - training batch 1051, loss: 0.259, 33632/60000 datapoints
2025-03-06 20:04:51,166 - INFO - training batch 1101, loss: 0.253, 35232/60000 datapoints
2025-03-06 20:04:51,400 - INFO - training batch 1151, loss: 0.489, 36832/60000 datapoints
2025-03-06 20:04:51,634 - INFO - training batch 1201, loss: 0.145, 38432/60000 datapoints
2025-03-06 20:04:51,876 - INFO - training batch 1251, loss: 0.212, 40032/60000 datapoints
2025-03-06 20:04:52,114 - INFO - training batch 1301, loss: 0.401, 41632/60000 datapoints
2025-03-06 20:04:52,356 - INFO - training batch 1351, loss: 0.205, 43232/60000 datapoints
2025-03-06 20:04:52,607 - INFO - training batch 1401, loss: 0.039, 44832/60000 datapoints
2025-03-06 20:04:52,985 - INFO - training batch 1451, loss: 0.533, 46432/60000 datapoints
2025-03-06 20:04:53,218 - INFO - training batch 1501, loss: 0.279, 48032/60000 datapoints
2025-03-06 20:04:53,448 - INFO - training batch 1551, loss: 0.147, 49632/60000 datapoints
2025-03-06 20:04:53,670 - INFO - training batch 1601, loss: 0.223, 51232/60000 datapoints
2025-03-06 20:04:53,924 - INFO - training batch 1651, loss: 0.334, 52832/60000 datapoints
2025-03-06 20:04:54,173 - INFO - training batch 1701, loss: 0.162, 54432/60000 datapoints
2025-03-06 20:04:54,436 - INFO - training batch 1751, loss: 0.351, 56032/60000 datapoints
2025-03-06 20:04:54,672 - INFO - training batch 1801, loss: 0.130, 57632/60000 datapoints
2025-03-06 20:04:54,962 - INFO - training batch 1851, loss: 0.495, 59232/60000 datapoints
2025-03-06 20:04:55,102 - INFO - validation batch 1, loss: 0.138, 32/10016 datapoints
2025-03-06 20:04:55,317 - INFO - validation batch 51, loss: 0.261, 1632/10016 datapoints
2025-03-06 20:04:55,524 - INFO - validation batch 101, loss: 0.290, 3232/10016 datapoints
2025-03-06 20:04:55,748 - INFO - validation batch 151, loss: 0.094, 4832/10016 datapoints
2025-03-06 20:04:55,942 - INFO - validation batch 201, loss: 0.250, 6432/10016 datapoints
2025-03-06 20:04:56,134 - INFO - validation batch 251, loss: 0.277, 8032/10016 datapoints
2025-03-06 20:04:56,320 - INFO - validation batch 301, loss: 0.094, 9632/10016 datapoints
2025-03-06 20:04:56,369 - INFO - Epoch 585/800 done.
2025-03-06 20:04:56,370 - INFO - Final validation performance:
Loss: 0.201, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:04:56,371 - INFO - Beginning epoch 586/800
2025-03-06 20:04:56,380 - INFO - training batch 1, loss: 0.284, 32/60000 datapoints
2025-03-06 20:04:56,609 - INFO - training batch 51, loss: 0.165, 1632/60000 datapoints
2025-03-06 20:04:56,860 - INFO - training batch 101, loss: 0.111, 3232/60000 datapoints
2025-03-06 20:04:57,149 - INFO - training batch 151, loss: 0.246, 4832/60000 datapoints
2025-03-06 20:04:57,414 - INFO - training batch 201, loss: 0.471, 6432/60000 datapoints
2025-03-06 20:04:57,674 - INFO - training batch 251, loss: 0.132, 8032/60000 datapoints
2025-03-06 20:04:57,936 - INFO - training batch 301, loss: 0.277, 9632/60000 datapoints
2025-03-06 20:04:58,168 - INFO - training batch 351, loss: 0.103, 11232/60000 datapoints
2025-03-06 20:04:58,417 - INFO - training batch 401, loss: 0.210, 12832/60000 datapoints
2025-03-06 20:04:58,690 - INFO - training batch 451, loss: 0.055, 14432/60000 datapoints
2025-03-06 20:04:58,979 - INFO - training batch 501, loss: 0.123, 16032/60000 datapoints
2025-03-06 20:04:59,232 - INFO - training batch 551, loss: 0.220, 17632/60000 datapoints
2025-03-06 20:04:59,483 - INFO - training batch 601, loss: 0.102, 19232/60000 datapoints
2025-03-06 20:04:59,718 - INFO - training batch 651, loss: 0.185, 20832/60000 datapoints
2025-03-06 20:04:59,959 - INFO - training batch 701, loss: 0.269, 22432/60000 datapoints
2025-03-06 20:05:00,183 - INFO - training batch 751, loss: 0.171, 24032/60000 datapoints
2025-03-06 20:05:00,415 - INFO - training batch 801, loss: 0.232, 25632/60000 datapoints
2025-03-06 20:05:00,666 - INFO - training batch 851, loss: 0.146, 27232/60000 datapoints
2025-03-06 20:05:00,930 - INFO - training batch 901, loss: 0.159, 28832/60000 datapoints
2025-03-06 20:05:01,198 - INFO - training batch 951, loss: 0.388, 30432/60000 datapoints
2025-03-06 20:05:01,465 - INFO - training batch 1001, loss: 0.091, 32032/60000 datapoints
2025-03-06 20:05:01,717 - INFO - training batch 1051, loss: 0.408, 33632/60000 datapoints
2025-03-06 20:05:01,965 - INFO - training batch 1101, loss: 0.159, 35232/60000 datapoints
2025-03-06 20:05:02,281 - INFO - training batch 1151, loss: 0.318, 36832/60000 datapoints
2025-03-06 20:05:02,524 - INFO - training batch 1201, loss: 0.336, 38432/60000 datapoints
2025-03-06 20:05:02,868 - INFO - training batch 1251, loss: 0.170, 40032/60000 datapoints
2025-03-06 20:05:03,180 - INFO - training batch 1301, loss: 0.256, 41632/60000 datapoints
2025-03-06 20:05:03,492 - INFO - training batch 1351, loss: 0.812, 43232/60000 datapoints
2025-03-06 20:05:03,745 - INFO - training batch 1401, loss: 0.407, 44832/60000 datapoints
2025-03-06 20:05:04,016 - INFO - training batch 1451, loss: 0.205, 46432/60000 datapoints
2025-03-06 20:05:04,263 - INFO - training batch 1501, loss: 0.137, 48032/60000 datapoints
2025-03-06 20:05:04,508 - INFO - training batch 1551, loss: 0.114, 49632/60000 datapoints
2025-03-06 20:05:04,777 - INFO - training batch 1601, loss: 0.305, 51232/60000 datapoints
2025-03-06 20:05:05,052 - INFO - training batch 1651, loss: 0.224, 52832/60000 datapoints
2025-03-06 20:05:05,311 - INFO - training batch 1701, loss: 0.198, 54432/60000 datapoints
2025-03-06 20:05:05,546 - INFO - training batch 1751, loss: 0.336, 56032/60000 datapoints
2025-03-06 20:05:05,804 - INFO - training batch 1801, loss: 0.263, 57632/60000 datapoints
2025-03-06 20:05:06,092 - INFO - training batch 1851, loss: 0.357, 59232/60000 datapoints
2025-03-06 20:05:06,227 - INFO - validation batch 1, loss: 0.439, 32/10016 datapoints
2025-03-06 20:05:06,425 - INFO - validation batch 51, loss: 0.084, 1632/10016 datapoints
2025-03-06 20:05:06,740 - INFO - validation batch 101, loss: 0.171, 3232/10016 datapoints
2025-03-06 20:05:07,022 - INFO - validation batch 151, loss: 0.090, 4832/10016 datapoints
2025-03-06 20:05:07,232 - INFO - validation batch 201, loss: 0.147, 6432/10016 datapoints
2025-03-06 20:05:07,430 - INFO - validation batch 251, loss: 0.461, 8032/10016 datapoints
2025-03-06 20:05:07,638 - INFO - validation batch 301, loss: 0.189, 9632/10016 datapoints
2025-03-06 20:05:07,691 - INFO - Epoch 586/800 done.
2025-03-06 20:05:07,691 - INFO - Final validation performance:
Loss: 0.226, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:05:07,693 - INFO - Beginning epoch 587/800
2025-03-06 20:05:07,703 - INFO - training batch 1, loss: 0.208, 32/60000 datapoints
2025-03-06 20:05:08,047 - INFO - training batch 51, loss: 0.335, 1632/60000 datapoints
2025-03-06 20:05:08,324 - INFO - training batch 101, loss: 0.207, 3232/60000 datapoints
2025-03-06 20:05:08,586 - INFO - training batch 151, loss: 0.259, 4832/60000 datapoints
2025-03-06 20:05:08,932 - INFO - training batch 201, loss: 0.440, 6432/60000 datapoints
2025-03-06 20:05:09,248 - INFO - training batch 251, loss: 0.319, 8032/60000 datapoints
2025-03-06 20:05:09,525 - INFO - training batch 301, loss: 0.348, 9632/60000 datapoints
2025-03-06 20:05:09,886 - INFO - training batch 351, loss: 0.229, 11232/60000 datapoints
2025-03-06 20:05:10,171 - INFO - training batch 401, loss: 0.167, 12832/60000 datapoints
2025-03-06 20:05:10,435 - INFO - training batch 451, loss: 0.080, 14432/60000 datapoints
2025-03-06 20:05:10,714 - INFO - training batch 501, loss: 0.387, 16032/60000 datapoints
2025-03-06 20:05:10,965 - INFO - training batch 551, loss: 0.038, 17632/60000 datapoints
2025-03-06 20:05:11,283 - INFO - training batch 601, loss: 0.186, 19232/60000 datapoints
2025-03-06 20:05:11,570 - INFO - training batch 651, loss: 0.205, 20832/60000 datapoints
2025-03-06 20:05:11,949 - INFO - training batch 701, loss: 0.321, 22432/60000 datapoints
2025-03-06 20:05:12,308 - INFO - training batch 751, loss: 0.404, 24032/60000 datapoints
2025-03-06 20:05:12,581 - INFO - training batch 801, loss: 0.169, 25632/60000 datapoints
2025-03-06 20:05:12,858 - INFO - training batch 851, loss: 0.283, 27232/60000 datapoints
2025-03-06 20:05:13,245 - INFO - training batch 901, loss: 0.259, 28832/60000 datapoints
2025-03-06 20:05:13,580 - INFO - training batch 951, loss: 0.312, 30432/60000 datapoints
2025-03-06 20:05:13,967 - INFO - training batch 1001, loss: 0.263, 32032/60000 datapoints
2025-03-06 20:05:14,282 - INFO - training batch 1051, loss: 0.200, 33632/60000 datapoints
2025-03-06 20:05:14,574 - INFO - training batch 1101, loss: 0.178, 35232/60000 datapoints
2025-03-06 20:05:14,887 - INFO - training batch 1151, loss: 0.210, 36832/60000 datapoints
2025-03-06 20:05:15,200 - INFO - training batch 1201, loss: 0.169, 38432/60000 datapoints
2025-03-06 20:05:15,524 - INFO - training batch 1251, loss: 0.264, 40032/60000 datapoints
2025-03-06 20:05:15,942 - INFO - training batch 1301, loss: 0.076, 41632/60000 datapoints
2025-03-06 20:05:16,252 - INFO - training batch 1351, loss: 0.197, 43232/60000 datapoints
2025-03-06 20:05:16,564 - INFO - training batch 1401, loss: 0.452, 44832/60000 datapoints
2025-03-06 20:05:17,005 - INFO - training batch 1451, loss: 0.151, 46432/60000 datapoints
2025-03-06 20:05:17,552 - INFO - training batch 1501, loss: 0.156, 48032/60000 datapoints
2025-03-06 20:05:17,917 - INFO - training batch 1551, loss: 0.252, 49632/60000 datapoints
2025-03-06 20:05:18,325 - INFO - training batch 1601, loss: 0.182, 51232/60000 datapoints
2025-03-06 20:05:18,665 - INFO - training batch 1651, loss: 0.200, 52832/60000 datapoints
2025-03-06 20:05:19,154 - INFO - training batch 1701, loss: 0.265, 54432/60000 datapoints
2025-03-06 20:05:19,481 - INFO - training batch 1751, loss: 0.558, 56032/60000 datapoints
2025-03-06 20:05:19,825 - INFO - training batch 1801, loss: 0.153, 57632/60000 datapoints
2025-03-06 20:05:20,193 - INFO - training batch 1851, loss: 0.296, 59232/60000 datapoints
2025-03-06 20:05:20,333 - INFO - validation batch 1, loss: 0.780, 32/10016 datapoints
2025-03-06 20:05:20,545 - INFO - validation batch 51, loss: 0.077, 1632/10016 datapoints
2025-03-06 20:05:20,765 - INFO - validation batch 101, loss: 0.148, 3232/10016 datapoints
2025-03-06 20:05:20,997 - INFO - validation batch 151, loss: 0.163, 4832/10016 datapoints
2025-03-06 20:05:21,206 - INFO - validation batch 201, loss: 0.098, 6432/10016 datapoints
2025-03-06 20:05:21,426 - INFO - validation batch 251, loss: 0.309, 8032/10016 datapoints
2025-03-06 20:05:21,627 - INFO - validation batch 301, loss: 0.419, 9632/10016 datapoints
2025-03-06 20:05:21,679 - INFO - Epoch 587/800 done.
2025-03-06 20:05:21,680 - INFO - Final validation performance:
Loss: 0.285, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:05:21,681 - INFO - Beginning epoch 588/800
2025-03-06 20:05:21,689 - INFO - training batch 1, loss: 0.298, 32/60000 datapoints
2025-03-06 20:05:21,979 - INFO - training batch 51, loss: 0.281, 1632/60000 datapoints
2025-03-06 20:05:22,249 - INFO - training batch 101, loss: 0.283, 3232/60000 datapoints
2025-03-06 20:05:22,508 - INFO - training batch 151, loss: 0.182, 4832/60000 datapoints
2025-03-06 20:05:22,754 - INFO - training batch 201, loss: 0.097, 6432/60000 datapoints
2025-03-06 20:05:23,005 - INFO - training batch 251, loss: 0.251, 8032/60000 datapoints
2025-03-06 20:05:23,251 - INFO - training batch 301, loss: 0.138, 9632/60000 datapoints
2025-03-06 20:05:23,475 - INFO - training batch 351, loss: 0.330, 11232/60000 datapoints
2025-03-06 20:05:23,705 - INFO - training batch 401, loss: 0.187, 12832/60000 datapoints
2025-03-06 20:05:23,934 - INFO - training batch 451, loss: 0.234, 14432/60000 datapoints
2025-03-06 20:05:24,163 - INFO - training batch 501, loss: 0.149, 16032/60000 datapoints
2025-03-06 20:05:24,437 - INFO - training batch 551, loss: 0.201, 17632/60000 datapoints
2025-03-06 20:05:24,666 - INFO - training batch 601, loss: 0.124, 19232/60000 datapoints
2025-03-06 20:05:24,892 - INFO - training batch 651, loss: 0.215, 20832/60000 datapoints
2025-03-06 20:05:25,134 - INFO - training batch 701, loss: 0.133, 22432/60000 datapoints
2025-03-06 20:05:25,354 - INFO - training batch 751, loss: 0.109, 24032/60000 datapoints
2025-03-06 20:05:25,572 - INFO - training batch 801, loss: 0.144, 25632/60000 datapoints
2025-03-06 20:05:25,791 - INFO - training batch 851, loss: 0.169, 27232/60000 datapoints
2025-03-06 20:05:26,005 - INFO - training batch 901, loss: 0.160, 28832/60000 datapoints
2025-03-06 20:05:26,224 - INFO - training batch 951, loss: 0.230, 30432/60000 datapoints
2025-03-06 20:05:26,441 - INFO - training batch 1001, loss: 0.528, 32032/60000 datapoints
2025-03-06 20:05:26,654 - INFO - training batch 1051, loss: 0.134, 33632/60000 datapoints
2025-03-06 20:05:26,867 - INFO - training batch 1101, loss: 0.470, 35232/60000 datapoints
2025-03-06 20:05:27,081 - INFO - training batch 1151, loss: 0.293, 36832/60000 datapoints
2025-03-06 20:05:27,315 - INFO - training batch 1201, loss: 0.249, 38432/60000 datapoints
2025-03-06 20:05:27,522 - INFO - training batch 1251, loss: 0.259, 40032/60000 datapoints
2025-03-06 20:05:27,733 - INFO - training batch 1301, loss: 0.169, 41632/60000 datapoints
2025-03-06 20:05:27,938 - INFO - training batch 1351, loss: 0.415, 43232/60000 datapoints
2025-03-06 20:05:28,149 - INFO - training batch 1401, loss: 0.292, 44832/60000 datapoints
2025-03-06 20:05:28,356 - INFO - training batch 1451, loss: 0.101, 46432/60000 datapoints
2025-03-06 20:05:28,560 - INFO - training batch 1501, loss: 0.092, 48032/60000 datapoints
2025-03-06 20:05:28,869 - INFO - training batch 1551, loss: 0.298, 49632/60000 datapoints
2025-03-06 20:05:29,122 - INFO - training batch 1601, loss: 0.247, 51232/60000 datapoints
2025-03-06 20:05:29,337 - INFO - training batch 1651, loss: 0.238, 52832/60000 datapoints
2025-03-06 20:05:29,567 - INFO - training batch 1701, loss: 0.223, 54432/60000 datapoints
2025-03-06 20:05:29,785 - INFO - training batch 1751, loss: 0.144, 56032/60000 datapoints
2025-03-06 20:05:30,006 - INFO - training batch 1801, loss: 0.170, 57632/60000 datapoints
2025-03-06 20:05:30,208 - INFO - training batch 1851, loss: 0.314, 59232/60000 datapoints
2025-03-06 20:05:30,314 - INFO - validation batch 1, loss: 0.551, 32/10016 datapoints
2025-03-06 20:05:30,493 - INFO - validation batch 51, loss: 0.240, 1632/10016 datapoints
2025-03-06 20:05:30,663 - INFO - validation batch 101, loss: 0.366, 3232/10016 datapoints
2025-03-06 20:05:30,886 - INFO - validation batch 151, loss: 0.087, 4832/10016 datapoints
2025-03-06 20:05:31,086 - INFO - validation batch 201, loss: 0.443, 6432/10016 datapoints
2025-03-06 20:05:31,314 - INFO - validation batch 251, loss: 0.121, 8032/10016 datapoints
2025-03-06 20:05:31,492 - INFO - validation batch 301, loss: 0.476, 9632/10016 datapoints
2025-03-06 20:05:31,542 - INFO - Epoch 588/800 done.
2025-03-06 20:05:31,543 - INFO - Final validation performance:
Loss: 0.326, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:05:31,543 - INFO - Beginning epoch 589/800
2025-03-06 20:05:31,557 - INFO - training batch 1, loss: 0.445, 32/60000 datapoints
2025-03-06 20:05:31,826 - INFO - training batch 51, loss: 0.183, 1632/60000 datapoints
2025-03-06 20:05:32,086 - INFO - training batch 101, loss: 0.078, 3232/60000 datapoints
2025-03-06 20:05:32,382 - INFO - training batch 151, loss: 0.221, 4832/60000 datapoints
2025-03-06 20:05:32,637 - INFO - training batch 201, loss: 0.196, 6432/60000 datapoints
2025-03-06 20:05:32,940 - INFO - training batch 251, loss: 0.117, 8032/60000 datapoints
2025-03-06 20:05:33,238 - INFO - training batch 301, loss: 0.210, 9632/60000 datapoints
2025-03-06 20:05:33,502 - INFO - training batch 351, loss: 0.252, 11232/60000 datapoints
2025-03-06 20:05:33,806 - INFO - training batch 401, loss: 0.161, 12832/60000 datapoints
2025-03-06 20:05:34,110 - INFO - training batch 451, loss: 0.434, 14432/60000 datapoints
2025-03-06 20:05:34,389 - INFO - training batch 501, loss: 0.114, 16032/60000 datapoints
2025-03-06 20:05:34,663 - INFO - training batch 551, loss: 0.156, 17632/60000 datapoints
2025-03-06 20:05:34,965 - INFO - training batch 601, loss: 0.285, 19232/60000 datapoints
2025-03-06 20:05:35,305 - INFO - training batch 651, loss: 0.233, 20832/60000 datapoints
2025-03-06 20:05:35,626 - INFO - training batch 701, loss: 0.112, 22432/60000 datapoints
2025-03-06 20:05:35,916 - INFO - training batch 751, loss: 0.397, 24032/60000 datapoints
2025-03-06 20:05:36,223 - INFO - training batch 801, loss: 0.320, 25632/60000 datapoints
2025-03-06 20:05:36,488 - INFO - training batch 851, loss: 0.076, 27232/60000 datapoints
2025-03-06 20:05:36,842 - INFO - training batch 901, loss: 0.435, 28832/60000 datapoints
2025-03-06 20:05:37,095 - INFO - training batch 951, loss: 0.249, 30432/60000 datapoints
2025-03-06 20:05:37,444 - INFO - training batch 1001, loss: 0.349, 32032/60000 datapoints
2025-03-06 20:05:37,802 - INFO - training batch 1051, loss: 0.255, 33632/60000 datapoints
2025-03-06 20:05:38,157 - INFO - training batch 1101, loss: 0.140, 35232/60000 datapoints
2025-03-06 20:05:38,454 - INFO - training batch 1151, loss: 0.427, 36832/60000 datapoints
2025-03-06 20:05:38,808 - INFO - training batch 1201, loss: 0.189, 38432/60000 datapoints
2025-03-06 20:05:39,155 - INFO - training batch 1251, loss: 0.343, 40032/60000 datapoints
2025-03-06 20:05:39,452 - INFO - training batch 1301, loss: 0.141, 41632/60000 datapoints
2025-03-06 20:05:39,793 - INFO - training batch 1351, loss: 0.168, 43232/60000 datapoints
2025-03-06 20:05:40,082 - INFO - training batch 1401, loss: 0.237, 44832/60000 datapoints
2025-03-06 20:05:40,380 - INFO - training batch 1451, loss: 0.314, 46432/60000 datapoints
2025-03-06 20:05:40,657 - INFO - training batch 1501, loss: 0.066, 48032/60000 datapoints
2025-03-06 20:05:40,952 - INFO - training batch 1551, loss: 0.497, 49632/60000 datapoints
2025-03-06 20:05:41,273 - INFO - training batch 1601, loss: 0.334, 51232/60000 datapoints
2025-03-06 20:05:41,561 - INFO - training batch 1651, loss: 0.066, 52832/60000 datapoints
2025-03-06 20:05:41,869 - INFO - training batch 1701, loss: 0.258, 54432/60000 datapoints
2025-03-06 20:05:42,187 - INFO - training batch 1751, loss: 0.275, 56032/60000 datapoints
2025-03-06 20:05:42,483 - INFO - training batch 1801, loss: 0.491, 57632/60000 datapoints
2025-03-06 20:05:42,778 - INFO - training batch 1851, loss: 0.197, 59232/60000 datapoints
2025-03-06 20:05:42,927 - INFO - validation batch 1, loss: 0.213, 32/10016 datapoints
2025-03-06 20:05:43,185 - INFO - validation batch 51, loss: 0.112, 1632/10016 datapoints
2025-03-06 20:05:43,424 - INFO - validation batch 101, loss: 0.129, 3232/10016 datapoints
2025-03-06 20:05:43,682 - INFO - validation batch 151, loss: 0.193, 4832/10016 datapoints
2025-03-06 20:05:43,872 - INFO - validation batch 201, loss: 0.233, 6432/10016 datapoints
2025-03-06 20:05:44,075 - INFO - validation batch 251, loss: 0.242, 8032/10016 datapoints
2025-03-06 20:05:44,345 - INFO - validation batch 301, loss: 0.162, 9632/10016 datapoints
2025-03-06 20:05:44,408 - INFO - Epoch 589/800 done.
2025-03-06 20:05:44,408 - INFO - Final validation performance:
Loss: 0.183, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:05:44,409 - INFO - Beginning epoch 590/800
2025-03-06 20:05:44,420 - INFO - training batch 1, loss: 0.161, 32/60000 datapoints
2025-03-06 20:05:44,766 - INFO - training batch 51, loss: 0.356, 1632/60000 datapoints
2025-03-06 20:05:45,052 - INFO - training batch 101, loss: 0.445, 3232/60000 datapoints
2025-03-06 20:05:45,356 - INFO - training batch 151, loss: 0.462, 4832/60000 datapoints
2025-03-06 20:05:45,630 - INFO - training batch 201, loss: 0.389, 6432/60000 datapoints
2025-03-06 20:05:45,900 - INFO - training batch 251, loss: 0.272, 8032/60000 datapoints
2025-03-06 20:05:46,164 - INFO - training batch 301, loss: 0.318, 9632/60000 datapoints
2025-03-06 20:05:46,415 - INFO - training batch 351, loss: 0.128, 11232/60000 datapoints
2025-03-06 20:05:46,658 - INFO - training batch 401, loss: 0.293, 12832/60000 datapoints
2025-03-06 20:05:46,898 - INFO - training batch 451, loss: 0.115, 14432/60000 datapoints
2025-03-06 20:05:47,154 - INFO - training batch 501, loss: 0.136, 16032/60000 datapoints
2025-03-06 20:05:47,377 - INFO - training batch 551, loss: 0.368, 17632/60000 datapoints
2025-03-06 20:05:47,667 - INFO - training batch 601, loss: 0.353, 19232/60000 datapoints
2025-03-06 20:05:47,924 - INFO - training batch 651, loss: 0.264, 20832/60000 datapoints
2025-03-06 20:05:48,176 - INFO - training batch 701, loss: 0.410, 22432/60000 datapoints
2025-03-06 20:05:48,394 - INFO - training batch 751, loss: 0.128, 24032/60000 datapoints
2025-03-06 20:05:48,657 - INFO - training batch 801, loss: 0.350, 25632/60000 datapoints
2025-03-06 20:05:48,941 - INFO - training batch 851, loss: 0.175, 27232/60000 datapoints
2025-03-06 20:05:49,201 - INFO - training batch 901, loss: 0.109, 28832/60000 datapoints
2025-03-06 20:05:49,440 - INFO - training batch 951, loss: 0.125, 30432/60000 datapoints
2025-03-06 20:05:49,693 - INFO - training batch 1001, loss: 0.167, 32032/60000 datapoints
2025-03-06 20:05:49,919 - INFO - training batch 1051, loss: 0.147, 33632/60000 datapoints
2025-03-06 20:05:50,154 - INFO - training batch 1101, loss: 0.133, 35232/60000 datapoints
2025-03-06 20:05:50,423 - INFO - training batch 1151, loss: 0.214, 36832/60000 datapoints
2025-03-06 20:05:50,665 - INFO - training batch 1201, loss: 0.160, 38432/60000 datapoints
2025-03-06 20:05:50,957 - INFO - training batch 1251, loss: 0.429, 40032/60000 datapoints
2025-03-06 20:05:51,242 - INFO - training batch 1301, loss: 0.287, 41632/60000 datapoints
2025-03-06 20:05:51,541 - INFO - training batch 1351, loss: 0.084, 43232/60000 datapoints
2025-03-06 20:05:51,787 - INFO - training batch 1401, loss: 0.192, 44832/60000 datapoints
2025-03-06 20:05:52,022 - INFO - training batch 1451, loss: 0.263, 46432/60000 datapoints
2025-03-06 20:05:52,270 - INFO - training batch 1501, loss: 0.622, 48032/60000 datapoints
2025-03-06 20:05:52,513 - INFO - training batch 1551, loss: 0.405, 49632/60000 datapoints
2025-03-06 20:05:52,787 - INFO - training batch 1601, loss: 0.113, 51232/60000 datapoints
2025-03-06 20:05:53,063 - INFO - training batch 1651, loss: 0.264, 52832/60000 datapoints
2025-03-06 20:05:53,330 - INFO - training batch 1701, loss: 0.229, 54432/60000 datapoints
2025-03-06 20:05:53,603 - INFO - training batch 1751, loss: 0.308, 56032/60000 datapoints
2025-03-06 20:05:53,885 - INFO - training batch 1801, loss: 0.558, 57632/60000 datapoints
2025-03-06 20:05:54,163 - INFO - training batch 1851, loss: 0.169, 59232/60000 datapoints
2025-03-06 20:05:54,297 - INFO - validation batch 1, loss: 0.316, 32/10016 datapoints
2025-03-06 20:05:54,503 - INFO - validation batch 51, loss: 0.387, 1632/10016 datapoints
2025-03-06 20:05:54,704 - INFO - validation batch 101, loss: 0.156, 3232/10016 datapoints
2025-03-06 20:05:54,953 - INFO - validation batch 151, loss: 0.168, 4832/10016 datapoints
2025-03-06 20:05:55,157 - INFO - validation batch 201, loss: 0.455, 6432/10016 datapoints
2025-03-06 20:05:55,370 - INFO - validation batch 251, loss: 0.184, 8032/10016 datapoints
2025-03-06 20:05:55,594 - INFO - validation batch 301, loss: 0.104, 9632/10016 datapoints
2025-03-06 20:05:55,658 - INFO - Epoch 590/800 done.
2025-03-06 20:05:55,658 - INFO - Final validation performance:
Loss: 0.253, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:05:55,659 - INFO - Beginning epoch 591/800
2025-03-06 20:05:55,669 - INFO - training batch 1, loss: 0.166, 32/60000 datapoints
2025-03-06 20:05:55,946 - INFO - training batch 51, loss: 0.251, 1632/60000 datapoints
2025-03-06 20:05:56,239 - INFO - training batch 101, loss: 0.055, 3232/60000 datapoints
2025-03-06 20:05:56,535 - INFO - training batch 151, loss: 0.130, 4832/60000 datapoints
2025-03-06 20:05:56,844 - INFO - training batch 201, loss: 0.576, 6432/60000 datapoints
2025-03-06 20:05:57,077 - INFO - training batch 251, loss: 0.088, 8032/60000 datapoints
2025-03-06 20:05:57,318 - INFO - training batch 301, loss: 0.147, 9632/60000 datapoints
2025-03-06 20:05:57,578 - INFO - training batch 351, loss: 0.148, 11232/60000 datapoints
2025-03-06 20:05:57,865 - INFO - training batch 401, loss: 0.107, 12832/60000 datapoints
2025-03-06 20:05:58,096 - INFO - training batch 451, loss: 0.150, 14432/60000 datapoints
2025-03-06 20:05:58,385 - INFO - training batch 501, loss: 0.475, 16032/60000 datapoints
2025-03-06 20:05:58,659 - INFO - training batch 551, loss: 0.267, 17632/60000 datapoints
2025-03-06 20:05:58,898 - INFO - training batch 601, loss: 0.462, 19232/60000 datapoints
2025-03-06 20:05:59,228 - INFO - training batch 651, loss: 0.302, 20832/60000 datapoints
2025-03-06 20:05:59,472 - INFO - training batch 701, loss: 0.321, 22432/60000 datapoints
2025-03-06 20:05:59,745 - INFO - training batch 751, loss: 0.201, 24032/60000 datapoints
2025-03-06 20:06:00,042 - INFO - training batch 801, loss: 0.234, 25632/60000 datapoints
2025-03-06 20:06:00,310 - INFO - training batch 851, loss: 0.146, 27232/60000 datapoints
2025-03-06 20:06:00,664 - INFO - training batch 901, loss: 0.218, 28832/60000 datapoints
2025-03-06 20:06:00,925 - INFO - training batch 951, loss: 0.329, 30432/60000 datapoints
2025-03-06 20:06:01,199 - INFO - training batch 1001, loss: 0.154, 32032/60000 datapoints
2025-03-06 20:06:01,558 - INFO - training batch 1051, loss: 0.257, 33632/60000 datapoints
2025-03-06 20:06:01,949 - INFO - training batch 1101, loss: 0.175, 35232/60000 datapoints
2025-03-06 20:06:02,238 - INFO - training batch 1151, loss: 0.171, 36832/60000 datapoints
2025-03-06 20:06:02,597 - INFO - training batch 1201, loss: 0.516, 38432/60000 datapoints
2025-03-06 20:06:02,907 - INFO - training batch 1251, loss: 0.221, 40032/60000 datapoints
2025-03-06 20:06:03,186 - INFO - training batch 1301, loss: 0.186, 41632/60000 datapoints
2025-03-06 20:06:03,520 - INFO - training batch 1351, loss: 0.331, 43232/60000 datapoints
2025-03-06 20:06:03,858 - INFO - training batch 1401, loss: 0.433, 44832/60000 datapoints
2025-03-06 20:06:04,153 - INFO - training batch 1451, loss: 0.209, 46432/60000 datapoints
2025-03-06 20:06:04,437 - INFO - training batch 1501, loss: 0.349, 48032/60000 datapoints
2025-03-06 20:06:04,759 - INFO - training batch 1551, loss: 0.264, 49632/60000 datapoints
2025-03-06 20:06:05,027 - INFO - training batch 1601, loss: 0.180, 51232/60000 datapoints
2025-03-06 20:06:05,282 - INFO - training batch 1651, loss: 0.109, 52832/60000 datapoints
2025-03-06 20:06:05,661 - INFO - training batch 1701, loss: 0.216, 54432/60000 datapoints
2025-03-06 20:06:05,936 - INFO - training batch 1751, loss: 0.127, 56032/60000 datapoints
2025-03-06 20:06:06,195 - INFO - training batch 1801, loss: 0.923, 57632/60000 datapoints
2025-03-06 20:06:06,469 - INFO - training batch 1851, loss: 0.121, 59232/60000 datapoints
2025-03-06 20:06:06,625 - INFO - validation batch 1, loss: 0.492, 32/10016 datapoints
2025-03-06 20:06:06,873 - INFO - validation batch 51, loss: 0.061, 1632/10016 datapoints
2025-03-06 20:06:07,088 - INFO - validation batch 101, loss: 0.209, 3232/10016 datapoints
2025-03-06 20:06:07,305 - INFO - validation batch 151, loss: 0.033, 4832/10016 datapoints
2025-03-06 20:06:07,519 - INFO - validation batch 201, loss: 0.218, 6432/10016 datapoints
2025-03-06 20:06:07,973 - INFO - validation batch 251, loss: 0.169, 8032/10016 datapoints
2025-03-06 20:06:08,427 - INFO - validation batch 301, loss: 0.066, 9632/10016 datapoints
2025-03-06 20:06:08,530 - INFO - Epoch 591/800 done.
2025-03-06 20:06:08,533 - INFO - Final validation performance:
Loss: 0.178, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:06:08,536 - INFO - Beginning epoch 592/800
2025-03-06 20:06:08,558 - INFO - training batch 1, loss: 0.200, 32/60000 datapoints
2025-03-06 20:06:09,073 - INFO - training batch 51, loss: 0.318, 1632/60000 datapoints
2025-03-06 20:06:09,451 - INFO - training batch 101, loss: 0.251, 3232/60000 datapoints
2025-03-06 20:06:09,830 - INFO - training batch 151, loss: 0.145, 4832/60000 datapoints
2025-03-06 20:06:10,330 - INFO - training batch 201, loss: 0.253, 6432/60000 datapoints
2025-03-06 20:06:10,670 - INFO - training batch 251, loss: 0.135, 8032/60000 datapoints
2025-03-06 20:06:11,032 - INFO - training batch 301, loss: 0.078, 9632/60000 datapoints
2025-03-06 20:06:11,336 - INFO - training batch 351, loss: 0.107, 11232/60000 datapoints
2025-03-06 20:06:11,643 - INFO - training batch 401, loss: 0.430, 12832/60000 datapoints
2025-03-06 20:06:11,939 - INFO - training batch 451, loss: 0.177, 14432/60000 datapoints
2025-03-06 20:06:12,199 - INFO - training batch 501, loss: 0.289, 16032/60000 datapoints
2025-03-06 20:06:12,477 - INFO - training batch 551, loss: 0.165, 17632/60000 datapoints
2025-03-06 20:06:12,738 - INFO - training batch 601, loss: 0.088, 19232/60000 datapoints
2025-03-06 20:06:13,008 - INFO - training batch 651, loss: 0.179, 20832/60000 datapoints
2025-03-06 20:06:13,318 - INFO - training batch 701, loss: 0.291, 22432/60000 datapoints
2025-03-06 20:06:13,687 - INFO - training batch 751, loss: 0.087, 24032/60000 datapoints
2025-03-06 20:06:13,993 - INFO - training batch 801, loss: 0.260, 25632/60000 datapoints
2025-03-06 20:06:14,267 - INFO - training batch 851, loss: 0.184, 27232/60000 datapoints
2025-03-06 20:06:14,598 - INFO - training batch 901, loss: 0.218, 28832/60000 datapoints
2025-03-06 20:06:14,875 - INFO - training batch 951, loss: 0.208, 30432/60000 datapoints
2025-03-06 20:06:15,189 - INFO - training batch 1001, loss: 0.172, 32032/60000 datapoints
2025-03-06 20:06:15,497 - INFO - training batch 1051, loss: 0.123, 33632/60000 datapoints
2025-03-06 20:06:15,879 - INFO - training batch 1101, loss: 0.203, 35232/60000 datapoints
2025-03-06 20:06:16,151 - INFO - training batch 1151, loss: 0.103, 36832/60000 datapoints
2025-03-06 20:06:16,409 - INFO - training batch 1201, loss: 0.202, 38432/60000 datapoints
2025-03-06 20:06:16,669 - INFO - training batch 1251, loss: 0.141, 40032/60000 datapoints
2025-03-06 20:06:16,937 - INFO - training batch 1301, loss: 0.110, 41632/60000 datapoints
2025-03-06 20:06:17,188 - INFO - training batch 1351, loss: 0.092, 43232/60000 datapoints
2025-03-06 20:06:17,622 - INFO - training batch 1401, loss: 0.339, 44832/60000 datapoints
2025-03-06 20:06:18,105 - INFO - training batch 1451, loss: 0.132, 46432/60000 datapoints
2025-03-06 20:06:18,471 - INFO - training batch 1501, loss: 0.145, 48032/60000 datapoints
2025-03-06 20:06:18,829 - INFO - training batch 1551, loss: 0.152, 49632/60000 datapoints
2025-03-06 20:06:19,163 - INFO - training batch 1601, loss: 0.151, 51232/60000 datapoints
2025-03-06 20:06:19,503 - INFO - training batch 1651, loss: 0.564, 52832/60000 datapoints
2025-03-06 20:06:19,825 - INFO - training batch 1701, loss: 0.364, 54432/60000 datapoints
2025-03-06 20:06:20,131 - INFO - training batch 1751, loss: 0.112, 56032/60000 datapoints
2025-03-06 20:06:20,396 - INFO - training batch 1801, loss: 0.291, 57632/60000 datapoints
2025-03-06 20:06:20,648 - INFO - training batch 1851, loss: 0.189, 59232/60000 datapoints
2025-03-06 20:06:20,780 - INFO - validation batch 1, loss: 0.298, 32/10016 datapoints
2025-03-06 20:06:20,992 - INFO - validation batch 51, loss: 0.220, 1632/10016 datapoints
2025-03-06 20:06:21,180 - INFO - validation batch 101, loss: 0.057, 3232/10016 datapoints
2025-03-06 20:06:21,374 - INFO - validation batch 151, loss: 0.231, 4832/10016 datapoints
2025-03-06 20:06:21,568 - INFO - validation batch 201, loss: 0.217, 6432/10016 datapoints
2025-03-06 20:06:21,761 - INFO - validation batch 251, loss: 0.357, 8032/10016 datapoints
2025-03-06 20:06:21,954 - INFO - validation batch 301, loss: 0.267, 9632/10016 datapoints
2025-03-06 20:06:22,004 - INFO - Epoch 592/800 done.
2025-03-06 20:06:22,004 - INFO - Final validation performance:
Loss: 0.235, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:06:22,005 - INFO - Beginning epoch 593/800
2025-03-06 20:06:22,014 - INFO - training batch 1, loss: 0.599, 32/60000 datapoints
2025-03-06 20:06:22,257 - INFO - training batch 51, loss: 0.292, 1632/60000 datapoints
2025-03-06 20:06:22,499 - INFO - training batch 101, loss: 0.043, 3232/60000 datapoints
2025-03-06 20:06:22,765 - INFO - training batch 151, loss: 0.249, 4832/60000 datapoints
2025-03-06 20:06:22,996 - INFO - training batch 201, loss: 0.235, 6432/60000 datapoints
2025-03-06 20:06:23,261 - INFO - training batch 251, loss: 0.157, 8032/60000 datapoints
2025-03-06 20:06:23,503 - INFO - training batch 301, loss: 0.218, 9632/60000 datapoints
2025-03-06 20:06:23,754 - INFO - training batch 351, loss: 0.069, 11232/60000 datapoints
2025-03-06 20:06:23,983 - INFO - training batch 401, loss: 0.101, 12832/60000 datapoints
2025-03-06 20:06:24,225 - INFO - training batch 451, loss: 0.125, 14432/60000 datapoints
2025-03-06 20:06:24,458 - INFO - training batch 501, loss: 0.124, 16032/60000 datapoints
2025-03-06 20:06:24,681 - INFO - training batch 551, loss: 0.207, 17632/60000 datapoints
2025-03-06 20:06:24,906 - INFO - training batch 601, loss: 0.489, 19232/60000 datapoints
2025-03-06 20:06:25,127 - INFO - training batch 651, loss: 0.927, 20832/60000 datapoints
2025-03-06 20:06:25,349 - INFO - training batch 701, loss: 0.239, 22432/60000 datapoints
2025-03-06 20:06:25,566 - INFO - training batch 751, loss: 0.251, 24032/60000 datapoints
2025-03-06 20:06:25,807 - INFO - training batch 801, loss: 0.573, 25632/60000 datapoints
2025-03-06 20:06:26,038 - INFO - training batch 851, loss: 0.394, 27232/60000 datapoints
2025-03-06 20:06:26,285 - INFO - training batch 901, loss: 0.044, 28832/60000 datapoints
2025-03-06 20:06:26,532 - INFO - training batch 951, loss: 0.284, 30432/60000 datapoints
2025-03-06 20:06:26,785 - INFO - training batch 1001, loss: 0.080, 32032/60000 datapoints
2025-03-06 20:06:27,073 - INFO - training batch 1051, loss: 0.183, 33632/60000 datapoints
2025-03-06 20:06:27,342 - INFO - training batch 1101, loss: 0.205, 35232/60000 datapoints
2025-03-06 20:06:27,555 - INFO - training batch 1151, loss: 0.349, 36832/60000 datapoints
2025-03-06 20:06:27,768 - INFO - training batch 1201, loss: 0.032, 38432/60000 datapoints
2025-03-06 20:06:27,978 - INFO - training batch 1251, loss: 0.512, 40032/60000 datapoints
2025-03-06 20:06:28,188 - INFO - training batch 1301, loss: 0.176, 41632/60000 datapoints
2025-03-06 20:06:28,407 - INFO - training batch 1351, loss: 0.046, 43232/60000 datapoints
2025-03-06 20:06:28,609 - INFO - training batch 1401, loss: 0.379, 44832/60000 datapoints
2025-03-06 20:06:28,817 - INFO - training batch 1451, loss: 0.202, 46432/60000 datapoints
2025-03-06 20:06:29,040 - INFO - training batch 1501, loss: 0.065, 48032/60000 datapoints
2025-03-06 20:06:29,270 - INFO - training batch 1551, loss: 0.415, 49632/60000 datapoints
2025-03-06 20:06:29,482 - INFO - training batch 1601, loss: 0.451, 51232/60000 datapoints
2025-03-06 20:06:29,717 - INFO - training batch 1651, loss: 0.615, 52832/60000 datapoints
2025-03-06 20:06:29,963 - INFO - training batch 1701, loss: 0.133, 54432/60000 datapoints
2025-03-06 20:06:30,180 - INFO - training batch 1751, loss: 0.249, 56032/60000 datapoints
2025-03-06 20:06:30,422 - INFO - training batch 1801, loss: 0.442, 57632/60000 datapoints
2025-03-06 20:06:30,654 - INFO - training batch 1851, loss: 0.069, 59232/60000 datapoints
2025-03-06 20:06:30,795 - INFO - validation batch 1, loss: 0.076, 32/10016 datapoints
2025-03-06 20:06:31,012 - INFO - validation batch 51, loss: 0.376, 1632/10016 datapoints
2025-03-06 20:06:31,271 - INFO - validation batch 101, loss: 0.359, 3232/10016 datapoints
2025-03-06 20:06:31,469 - INFO - validation batch 151, loss: 0.160, 4832/10016 datapoints
2025-03-06 20:06:31,677 - INFO - validation batch 201, loss: 0.232, 6432/10016 datapoints
2025-03-06 20:06:31,876 - INFO - validation batch 251, loss: 0.406, 8032/10016 datapoints
2025-03-06 20:06:32,059 - INFO - validation batch 301, loss: 0.212, 9632/10016 datapoints
2025-03-06 20:06:32,110 - INFO - Epoch 593/800 done.
2025-03-06 20:06:32,111 - INFO - Final validation performance:
Loss: 0.260, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:06:32,111 - INFO - Beginning epoch 594/800
2025-03-06 20:06:32,122 - INFO - training batch 1, loss: 0.211, 32/60000 datapoints
2025-03-06 20:06:32,421 - INFO - training batch 51, loss: 0.193, 1632/60000 datapoints
2025-03-06 20:06:32,672 - INFO - training batch 101, loss: 0.068, 3232/60000 datapoints
2025-03-06 20:06:32,924 - INFO - training batch 151, loss: 0.102, 4832/60000 datapoints
2025-03-06 20:06:33,160 - INFO - training batch 201, loss: 0.170, 6432/60000 datapoints
2025-03-06 20:06:33,411 - INFO - training batch 251, loss: 0.177, 8032/60000 datapoints
2025-03-06 20:06:33,645 - INFO - training batch 301, loss: 0.393, 9632/60000 datapoints
2025-03-06 20:06:33,881 - INFO - training batch 351, loss: 0.540, 11232/60000 datapoints
2025-03-06 20:06:34,119 - INFO - training batch 401, loss: 0.105, 12832/60000 datapoints
2025-03-06 20:06:34,360 - INFO - training batch 451, loss: 0.116, 14432/60000 datapoints
2025-03-06 20:06:34,602 - INFO - training batch 501, loss: 0.363, 16032/60000 datapoints
2025-03-06 20:06:34,849 - INFO - training batch 551, loss: 0.270, 17632/60000 datapoints
2025-03-06 20:06:35,090 - INFO - training batch 601, loss: 0.334, 19232/60000 datapoints
2025-03-06 20:06:35,337 - INFO - training batch 651, loss: 0.169, 20832/60000 datapoints
2025-03-06 20:06:35,578 - INFO - training batch 701, loss: 0.104, 22432/60000 datapoints
2025-03-06 20:06:35,820 - INFO - training batch 751, loss: 0.383, 24032/60000 datapoints
2025-03-06 20:06:36,078 - INFO - training batch 801, loss: 0.166, 25632/60000 datapoints
2025-03-06 20:06:36,318 - INFO - training batch 851, loss: 0.275, 27232/60000 datapoints
2025-03-06 20:06:36,553 - INFO - training batch 901, loss: 0.101, 28832/60000 datapoints
2025-03-06 20:06:36,787 - INFO - training batch 951, loss: 0.084, 30432/60000 datapoints
2025-03-06 20:06:37,020 - INFO - training batch 1001, loss: 0.094, 32032/60000 datapoints
2025-03-06 20:06:37,249 - INFO - training batch 1051, loss: 0.168, 33632/60000 datapoints
2025-03-06 20:06:37,480 - INFO - training batch 1101, loss: 0.630, 35232/60000 datapoints
2025-03-06 20:06:37,747 - INFO - training batch 1151, loss: 0.342, 36832/60000 datapoints
2025-03-06 20:06:37,981 - INFO - training batch 1201, loss: 0.076, 38432/60000 datapoints
2025-03-06 20:06:38,226 - INFO - training batch 1251, loss: 0.137, 40032/60000 datapoints
2025-03-06 20:06:38,454 - INFO - training batch 1301, loss: 0.079, 41632/60000 datapoints
2025-03-06 20:06:38,689 - INFO - training batch 1351, loss: 0.246, 43232/60000 datapoints
2025-03-06 20:06:38,933 - INFO - training batch 1401, loss: 0.306, 44832/60000 datapoints
2025-03-06 20:06:39,173 - INFO - training batch 1451, loss: 0.268, 46432/60000 datapoints
2025-03-06 20:06:39,420 - INFO - training batch 1501, loss: 0.324, 48032/60000 datapoints
2025-03-06 20:06:39,651 - INFO - training batch 1551, loss: 0.268, 49632/60000 datapoints
2025-03-06 20:06:39,879 - INFO - training batch 1601, loss: 0.305, 51232/60000 datapoints
2025-03-06 20:06:40,104 - INFO - training batch 1651, loss: 0.210, 52832/60000 datapoints
2025-03-06 20:06:40,334 - INFO - training batch 1701, loss: 0.248, 54432/60000 datapoints
2025-03-06 20:06:40,557 - INFO - training batch 1751, loss: 0.387, 56032/60000 datapoints
2025-03-06 20:06:40,781 - INFO - training batch 1801, loss: 0.170, 57632/60000 datapoints
2025-03-06 20:06:41,001 - INFO - training batch 1851, loss: 0.119, 59232/60000 datapoints
2025-03-06 20:06:41,121 - INFO - validation batch 1, loss: 0.195, 32/10016 datapoints
2025-03-06 20:06:41,300 - INFO - validation batch 51, loss: 0.221, 1632/10016 datapoints
2025-03-06 20:06:41,480 - INFO - validation batch 101, loss: 0.249, 3232/10016 datapoints
2025-03-06 20:06:41,665 - INFO - validation batch 151, loss: 0.111, 4832/10016 datapoints
2025-03-06 20:06:41,844 - INFO - validation batch 201, loss: 0.090, 6432/10016 datapoints
2025-03-06 20:06:42,028 - INFO - validation batch 251, loss: 0.213, 8032/10016 datapoints
2025-03-06 20:06:42,205 - INFO - validation batch 301, loss: 0.101, 9632/10016 datapoints
2025-03-06 20:06:42,250 - INFO - Epoch 594/800 done.
2025-03-06 20:06:42,251 - INFO - Final validation performance:
Loss: 0.169, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:06:42,251 - INFO - Beginning epoch 595/800
2025-03-06 20:06:42,259 - INFO - training batch 1, loss: 0.119, 32/60000 datapoints
2025-03-06 20:06:42,475 - INFO - training batch 51, loss: 0.087, 1632/60000 datapoints
2025-03-06 20:06:42,789 - INFO - training batch 101, loss: 0.266, 3232/60000 datapoints
2025-03-06 20:06:43,053 - INFO - training batch 151, loss: 0.186, 4832/60000 datapoints
2025-03-06 20:06:43,312 - INFO - training batch 201, loss: 0.546, 6432/60000 datapoints
2025-03-06 20:06:43,557 - INFO - training batch 251, loss: 0.348, 8032/60000 datapoints
2025-03-06 20:06:43,775 - INFO - training batch 301, loss: 0.110, 9632/60000 datapoints
2025-03-06 20:06:43,992 - INFO - training batch 351, loss: 0.279, 11232/60000 datapoints
2025-03-06 20:06:44,206 - INFO - training batch 401, loss: 0.152, 12832/60000 datapoints
2025-03-06 20:06:44,421 - INFO - training batch 451, loss: 0.212, 14432/60000 datapoints
2025-03-06 20:06:44,662 - INFO - training batch 501, loss: 0.316, 16032/60000 datapoints
2025-03-06 20:06:44,902 - INFO - training batch 551, loss: 0.122, 17632/60000 datapoints
2025-03-06 20:06:45,138 - INFO - training batch 601, loss: 0.216, 19232/60000 datapoints
2025-03-06 20:06:45,360 - INFO - training batch 651, loss: 0.535, 20832/60000 datapoints
2025-03-06 20:06:45,577 - INFO - training batch 701, loss: 0.306, 22432/60000 datapoints
2025-03-06 20:06:45,797 - INFO - training batch 751, loss: 0.225, 24032/60000 datapoints
2025-03-06 20:06:46,031 - INFO - training batch 801, loss: 0.169, 25632/60000 datapoints
2025-03-06 20:06:46,262 - INFO - training batch 851, loss: 0.356, 27232/60000 datapoints
2025-03-06 20:06:46,475 - INFO - training batch 901, loss: 0.417, 28832/60000 datapoints
2025-03-06 20:06:46,698 - INFO - training batch 951, loss: 0.170, 30432/60000 datapoints
2025-03-06 20:06:46,918 - INFO - training batch 1001, loss: 0.166, 32032/60000 datapoints
2025-03-06 20:06:47,136 - INFO - training batch 1051, loss: 0.187, 33632/60000 datapoints
2025-03-06 20:06:47,359 - INFO - training batch 1101, loss: 0.432, 35232/60000 datapoints
2025-03-06 20:06:47,586 - INFO - training batch 1151, loss: 0.156, 36832/60000 datapoints
2025-03-06 20:06:47,807 - INFO - training batch 1201, loss: 0.435, 38432/60000 datapoints
2025-03-06 20:06:48,028 - INFO - training batch 1251, loss: 0.244, 40032/60000 datapoints
2025-03-06 20:06:48,250 - INFO - training batch 1301, loss: 0.118, 41632/60000 datapoints
2025-03-06 20:06:48,471 - INFO - training batch 1351, loss: 0.270, 43232/60000 datapoints
2025-03-06 20:06:48,695 - INFO - training batch 1401, loss: 0.264, 44832/60000 datapoints
2025-03-06 20:06:48,917 - INFO - training batch 1451, loss: 0.509, 46432/60000 datapoints
2025-03-06 20:06:49,147 - INFO - training batch 1501, loss: 0.243, 48032/60000 datapoints
2025-03-06 20:06:49,377 - INFO - training batch 1551, loss: 0.223, 49632/60000 datapoints
2025-03-06 20:06:49,598 - INFO - training batch 1601, loss: 0.260, 51232/60000 datapoints
2025-03-06 20:06:49,823 - INFO - training batch 1651, loss: 0.314, 52832/60000 datapoints
2025-03-06 20:06:50,044 - INFO - training batch 1701, loss: 0.410, 54432/60000 datapoints
2025-03-06 20:06:50,266 - INFO - training batch 1751, loss: 0.291, 56032/60000 datapoints
2025-03-06 20:06:50,486 - INFO - training batch 1801, loss: 0.124, 57632/60000 datapoints
2025-03-06 20:06:50,718 - INFO - training batch 1851, loss: 0.428, 59232/60000 datapoints
2025-03-06 20:06:50,835 - INFO - validation batch 1, loss: 0.213, 32/10016 datapoints
2025-03-06 20:06:51,014 - INFO - validation batch 51, loss: 0.340, 1632/10016 datapoints
2025-03-06 20:06:51,194 - INFO - validation batch 101, loss: 0.160, 3232/10016 datapoints
2025-03-06 20:06:51,375 - INFO - validation batch 151, loss: 0.137, 4832/10016 datapoints
2025-03-06 20:06:51,552 - INFO - validation batch 201, loss: 0.134, 6432/10016 datapoints
2025-03-06 20:06:51,741 - INFO - validation batch 251, loss: 0.288, 8032/10016 datapoints
2025-03-06 20:06:51,919 - INFO - validation batch 301, loss: 0.277, 9632/10016 datapoints
2025-03-06 20:06:51,967 - INFO - Epoch 595/800 done.
2025-03-06 20:06:51,967 - INFO - Final validation performance:
Loss: 0.221, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:06:51,968 - INFO - Beginning epoch 596/800
2025-03-06 20:06:51,978 - INFO - training batch 1, loss: 0.185, 32/60000 datapoints
2025-03-06 20:06:52,219 - INFO - training batch 51, loss: 0.262, 1632/60000 datapoints
2025-03-06 20:06:52,445 - INFO - training batch 101, loss: 0.317, 3232/60000 datapoints
2025-03-06 20:06:52,663 - INFO - training batch 151, loss: 0.914, 4832/60000 datapoints
2025-03-06 20:06:52,885 - INFO - training batch 201, loss: 0.174, 6432/60000 datapoints
2025-03-06 20:06:53,095 - INFO - training batch 251, loss: 0.174, 8032/60000 datapoints
2025-03-06 20:06:53,307 - INFO - training batch 301, loss: 0.189, 9632/60000 datapoints
2025-03-06 20:06:53,514 - INFO - training batch 351, loss: 0.108, 11232/60000 datapoints
2025-03-06 20:06:53,723 - INFO - training batch 401, loss: 0.283, 12832/60000 datapoints
2025-03-06 20:06:53,930 - INFO - training batch 451, loss: 0.094, 14432/60000 datapoints
2025-03-06 20:06:54,139 - INFO - training batch 501, loss: 0.337, 16032/60000 datapoints
2025-03-06 20:06:54,345 - INFO - training batch 551, loss: 0.106, 17632/60000 datapoints
2025-03-06 20:06:54,547 - INFO - training batch 601, loss: 0.374, 19232/60000 datapoints
2025-03-06 20:06:54,753 - INFO - training batch 651, loss: 0.294, 20832/60000 datapoints
2025-03-06 20:06:54,960 - INFO - training batch 701, loss: 0.328, 22432/60000 datapoints
2025-03-06 20:06:55,164 - INFO - training batch 751, loss: 0.298, 24032/60000 datapoints
2025-03-06 20:06:55,384 - INFO - training batch 801, loss: 0.458, 25632/60000 datapoints
2025-03-06 20:06:55,591 - INFO - training batch 851, loss: 0.053, 27232/60000 datapoints
2025-03-06 20:06:55,805 - INFO - training batch 901, loss: 0.112, 28832/60000 datapoints
2025-03-06 20:06:56,014 - INFO - training batch 951, loss: 0.146, 30432/60000 datapoints
2025-03-06 20:06:56,260 - INFO - training batch 1001, loss: 0.319, 32032/60000 datapoints
2025-03-06 20:06:56,467 - INFO - training batch 1051, loss: 0.174, 33632/60000 datapoints
2025-03-06 20:06:56,680 - INFO - training batch 1101, loss: 0.455, 35232/60000 datapoints
2025-03-06 20:06:56,885 - INFO - training batch 1151, loss: 0.308, 36832/60000 datapoints
2025-03-06 20:06:57,097 - INFO - training batch 1201, loss: 0.521, 38432/60000 datapoints
2025-03-06 20:06:57,305 - INFO - training batch 1251, loss: 0.342, 40032/60000 datapoints
2025-03-06 20:06:57,519 - INFO - training batch 1301, loss: 0.498, 41632/60000 datapoints
2025-03-06 20:06:57,735 - INFO - training batch 1351, loss: 0.068, 43232/60000 datapoints
2025-03-06 20:06:57,943 - INFO - training batch 1401, loss: 0.151, 44832/60000 datapoints
2025-03-06 20:06:58,154 - INFO - training batch 1451, loss: 0.159, 46432/60000 datapoints
2025-03-06 20:06:58,357 - INFO - training batch 1501, loss: 0.403, 48032/60000 datapoints
2025-03-06 20:06:58,559 - INFO - training batch 1551, loss: 0.252, 49632/60000 datapoints
2025-03-06 20:06:58,770 - INFO - training batch 1601, loss: 0.064, 51232/60000 datapoints
2025-03-06 20:06:58,973 - INFO - training batch 1651, loss: 0.357, 52832/60000 datapoints
2025-03-06 20:06:59,175 - INFO - training batch 1701, loss: 0.187, 54432/60000 datapoints
2025-03-06 20:06:59,381 - INFO - training batch 1751, loss: 0.129, 56032/60000 datapoints
2025-03-06 20:06:59,586 - INFO - training batch 1801, loss: 0.068, 57632/60000 datapoints
2025-03-06 20:06:59,799 - INFO - training batch 1851, loss: 0.184, 59232/60000 datapoints
2025-03-06 20:06:59,906 - INFO - validation batch 1, loss: 0.458, 32/10016 datapoints
2025-03-06 20:07:00,072 - INFO - validation batch 51, loss: 0.331, 1632/10016 datapoints
2025-03-06 20:07:00,237 - INFO - validation batch 101, loss: 0.184, 3232/10016 datapoints
2025-03-06 20:07:00,403 - INFO - validation batch 151, loss: 0.241, 4832/10016 datapoints
2025-03-06 20:07:00,566 - INFO - validation batch 201, loss: 0.178, 6432/10016 datapoints
2025-03-06 20:07:00,735 - INFO - validation batch 251, loss: 0.263, 8032/10016 datapoints
2025-03-06 20:07:00,897 - INFO - validation batch 301, loss: 0.297, 9632/10016 datapoints
2025-03-06 20:07:00,941 - INFO - Epoch 596/800 done.
2025-03-06 20:07:00,941 - INFO - Final validation performance:
Loss: 0.279, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:07:00,941 - INFO - Beginning epoch 597/800
2025-03-06 20:07:00,948 - INFO - training batch 1, loss: 0.243, 32/60000 datapoints
2025-03-06 20:07:01,151 - INFO - training batch 51, loss: 0.196, 1632/60000 datapoints
2025-03-06 20:07:01,375 - INFO - training batch 101, loss: 0.366, 3232/60000 datapoints
2025-03-06 20:07:01,576 - INFO - training batch 151, loss: 0.335, 4832/60000 datapoints
2025-03-06 20:07:01,799 - INFO - training batch 201, loss: 0.232, 6432/60000 datapoints
2025-03-06 20:07:02,007 - INFO - training batch 251, loss: 0.252, 8032/60000 datapoints
2025-03-06 20:07:02,228 - INFO - training batch 301, loss: 0.261, 9632/60000 datapoints
2025-03-06 20:07:02,435 - INFO - training batch 351, loss: 0.436, 11232/60000 datapoints
2025-03-06 20:07:02,648 - INFO - training batch 401, loss: 0.238, 12832/60000 datapoints
2025-03-06 20:07:02,857 - INFO - training batch 451, loss: 0.384, 14432/60000 datapoints
2025-03-06 20:07:03,062 - INFO - training batch 501, loss: 0.344, 16032/60000 datapoints
2025-03-06 20:07:03,273 - INFO - training batch 551, loss: 0.668, 17632/60000 datapoints
2025-03-06 20:07:03,486 - INFO - training batch 601, loss: 0.160, 19232/60000 datapoints
2025-03-06 20:07:03,698 - INFO - training batch 651, loss: 0.157, 20832/60000 datapoints
2025-03-06 20:07:03,909 - INFO - training batch 701, loss: 0.115, 22432/60000 datapoints
2025-03-06 20:07:04,116 - INFO - training batch 751, loss: 0.049, 24032/60000 datapoints
2025-03-06 20:07:04,335 - INFO - training batch 801, loss: 0.348, 25632/60000 datapoints
2025-03-06 20:07:04,550 - INFO - training batch 851, loss: 0.145, 27232/60000 datapoints
2025-03-06 20:07:04,769 - INFO - training batch 901, loss: 0.517, 28832/60000 datapoints
2025-03-06 20:07:04,992 - INFO - training batch 951, loss: 0.459, 30432/60000 datapoints
2025-03-06 20:07:05,209 - INFO - training batch 1001, loss: 0.362, 32032/60000 datapoints
2025-03-06 20:07:05,426 - INFO - training batch 1051, loss: 0.141, 33632/60000 datapoints
2025-03-06 20:07:05,644 - INFO - training batch 1101, loss: 0.220, 35232/60000 datapoints
2025-03-06 20:07:05,858 - INFO - training batch 1151, loss: 0.271, 36832/60000 datapoints
2025-03-06 20:07:06,071 - INFO - training batch 1201, loss: 0.205, 38432/60000 datapoints
2025-03-06 20:07:06,314 - INFO - training batch 1251, loss: 0.193, 40032/60000 datapoints
2025-03-06 20:07:06,527 - INFO - training batch 1301, loss: 0.252, 41632/60000 datapoints
2025-03-06 20:07:06,743 - INFO - training batch 1351, loss: 0.238, 43232/60000 datapoints
2025-03-06 20:07:06,954 - INFO - training batch 1401, loss: 0.210, 44832/60000 datapoints
2025-03-06 20:07:07,167 - INFO - training batch 1451, loss: 0.255, 46432/60000 datapoints
2025-03-06 20:07:07,384 - INFO - training batch 1501, loss: 0.154, 48032/60000 datapoints
2025-03-06 20:07:07,597 - INFO - training batch 1551, loss: 0.153, 49632/60000 datapoints
2025-03-06 20:07:07,818 - INFO - training batch 1601, loss: 0.296, 51232/60000 datapoints
2025-03-06 20:07:08,035 - INFO - training batch 1651, loss: 0.218, 52832/60000 datapoints
2025-03-06 20:07:08,254 - INFO - training batch 1701, loss: 0.128, 54432/60000 datapoints
2025-03-06 20:07:08,468 - INFO - training batch 1751, loss: 0.160, 56032/60000 datapoints
2025-03-06 20:07:08,692 - INFO - training batch 1801, loss: 0.356, 57632/60000 datapoints
2025-03-06 20:07:08,922 - INFO - training batch 1851, loss: 0.095, 59232/60000 datapoints
2025-03-06 20:07:09,040 - INFO - validation batch 1, loss: 0.131, 32/10016 datapoints
2025-03-06 20:07:09,218 - INFO - validation batch 51, loss: 0.289, 1632/10016 datapoints
2025-03-06 20:07:09,405 - INFO - validation batch 101, loss: 0.245, 3232/10016 datapoints
2025-03-06 20:07:09,583 - INFO - validation batch 151, loss: 0.197, 4832/10016 datapoints
2025-03-06 20:07:09,765 - INFO - validation batch 201, loss: 0.282, 6432/10016 datapoints
2025-03-06 20:07:09,950 - INFO - validation batch 251, loss: 0.421, 8032/10016 datapoints
2025-03-06 20:07:10,147 - INFO - validation batch 301, loss: 0.360, 9632/10016 datapoints
2025-03-06 20:07:10,196 - INFO - Epoch 597/800 done.
2025-03-06 20:07:10,196 - INFO - Final validation performance:
Loss: 0.275, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:07:10,197 - INFO - Beginning epoch 598/800
2025-03-06 20:07:10,206 - INFO - training batch 1, loss: 0.350, 32/60000 datapoints
2025-03-06 20:07:10,462 - INFO - training batch 51, loss: 0.288, 1632/60000 datapoints
2025-03-06 20:07:10,698 - INFO - training batch 101, loss: 0.304, 3232/60000 datapoints
2025-03-06 20:07:10,932 - INFO - training batch 151, loss: 0.184, 4832/60000 datapoints
2025-03-06 20:07:11,220 - INFO - training batch 201, loss: 0.480, 6432/60000 datapoints
2025-03-06 20:07:11,468 - INFO - training batch 251, loss: 0.324, 8032/60000 datapoints
2025-03-06 20:07:11,712 - INFO - training batch 301, loss: 0.170, 9632/60000 datapoints
2025-03-06 20:07:12,020 - INFO - training batch 351, loss: 0.148, 11232/60000 datapoints
2025-03-06 20:07:12,362 - INFO - training batch 401, loss: 0.498, 12832/60000 datapoints
2025-03-06 20:07:12,606 - INFO - training batch 451, loss: 0.147, 14432/60000 datapoints
2025-03-06 20:07:12,847 - INFO - training batch 501, loss: 0.339, 16032/60000 datapoints
2025-03-06 20:07:13,085 - INFO - training batch 551, loss: 0.268, 17632/60000 datapoints
2025-03-06 20:07:13,347 - INFO - training batch 601, loss: 0.129, 19232/60000 datapoints
2025-03-06 20:07:13,649 - INFO - training batch 651, loss: 0.273, 20832/60000 datapoints
2025-03-06 20:07:13,984 - INFO - training batch 701, loss: 0.274, 22432/60000 datapoints
2025-03-06 20:07:14,265 - INFO - training batch 751, loss: 0.130, 24032/60000 datapoints
2025-03-06 20:07:14,533 - INFO - training batch 801, loss: 0.276, 25632/60000 datapoints
2025-03-06 20:07:14,796 - INFO - training batch 851, loss: 0.203, 27232/60000 datapoints
2025-03-06 20:07:15,068 - INFO - training batch 901, loss: 0.418, 28832/60000 datapoints
2025-03-06 20:07:15,346 - INFO - training batch 951, loss: 0.386, 30432/60000 datapoints
2025-03-06 20:07:15,613 - INFO - training batch 1001, loss: 0.148, 32032/60000 datapoints
2025-03-06 20:07:15,873 - INFO - training batch 1051, loss: 0.178, 33632/60000 datapoints
2025-03-06 20:07:16,136 - INFO - training batch 1101, loss: 0.269, 35232/60000 datapoints
2025-03-06 20:07:16,423 - INFO - training batch 1151, loss: 0.131, 36832/60000 datapoints
2025-03-06 20:07:16,680 - INFO - training batch 1201, loss: 0.114, 38432/60000 datapoints
2025-03-06 20:07:16,932 - INFO - training batch 1251, loss: 0.214, 40032/60000 datapoints
2025-03-06 20:07:17,178 - INFO - training batch 1301, loss: 0.223, 41632/60000 datapoints
2025-03-06 20:07:17,480 - INFO - training batch 1351, loss: 0.359, 43232/60000 datapoints
2025-03-06 20:07:17,776 - INFO - training batch 1401, loss: 0.196, 44832/60000 datapoints
2025-03-06 20:07:18,061 - INFO - training batch 1451, loss: 0.364, 46432/60000 datapoints
2025-03-06 20:07:18,300 - INFO - training batch 1501, loss: 0.371, 48032/60000 datapoints
2025-03-06 20:07:18,529 - INFO - training batch 1551, loss: 0.126, 49632/60000 datapoints
2025-03-06 20:07:18,849 - INFO - training batch 1601, loss: 0.456, 51232/60000 datapoints
2025-03-06 20:07:19,079 - INFO - training batch 1651, loss: 0.231, 52832/60000 datapoints
2025-03-06 20:07:19,309 - INFO - training batch 1701, loss: 0.268, 54432/60000 datapoints
2025-03-06 20:07:19,608 - INFO - training batch 1751, loss: 0.332, 56032/60000 datapoints
2025-03-06 20:07:19,868 - INFO - training batch 1801, loss: 0.248, 57632/60000 datapoints
2025-03-06 20:07:20,099 - INFO - training batch 1851, loss: 0.125, 59232/60000 datapoints
2025-03-06 20:07:20,228 - INFO - validation batch 1, loss: 0.163, 32/10016 datapoints
2025-03-06 20:07:20,462 - INFO - validation batch 51, loss: 0.457, 1632/10016 datapoints
2025-03-06 20:07:20,678 - INFO - validation batch 101, loss: 0.247, 3232/10016 datapoints
2025-03-06 20:07:20,943 - INFO - validation batch 151, loss: 0.398, 4832/10016 datapoints
2025-03-06 20:07:21,150 - INFO - validation batch 201, loss: 0.090, 6432/10016 datapoints
2025-03-06 20:07:21,343 - INFO - validation batch 251, loss: 0.352, 8032/10016 datapoints
2025-03-06 20:07:21,534 - INFO - validation batch 301, loss: 0.083, 9632/10016 datapoints
2025-03-06 20:07:21,583 - INFO - Epoch 598/800 done.
2025-03-06 20:07:21,583 - INFO - Final validation performance:
Loss: 0.256, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:07:21,584 - INFO - Beginning epoch 599/800
2025-03-06 20:07:21,591 - INFO - training batch 1, loss: 0.293, 32/60000 datapoints
2025-03-06 20:07:21,886 - INFO - training batch 51, loss: 0.152, 1632/60000 datapoints
2025-03-06 20:07:22,141 - INFO - training batch 101, loss: 0.094, 3232/60000 datapoints
2025-03-06 20:07:22,382 - INFO - training batch 151, loss: 0.465, 4832/60000 datapoints
2025-03-06 20:07:22,620 - INFO - training batch 201, loss: 0.168, 6432/60000 datapoints
2025-03-06 20:07:22,856 - INFO - training batch 251, loss: 0.409, 8032/60000 datapoints
2025-03-06 20:07:23,079 - INFO - training batch 301, loss: 0.323, 9632/60000 datapoints
2025-03-06 20:07:23,305 - INFO - training batch 351, loss: 0.372, 11232/60000 datapoints
2025-03-06 20:07:23,529 - INFO - training batch 401, loss: 0.187, 12832/60000 datapoints
2025-03-06 20:07:23,749 - INFO - training batch 451, loss: 0.455, 14432/60000 datapoints
2025-03-06 20:07:23,967 - INFO - training batch 501, loss: 0.064, 16032/60000 datapoints
2025-03-06 20:07:24,185 - INFO - training batch 551, loss: 0.532, 17632/60000 datapoints
2025-03-06 20:07:24,424 - INFO - training batch 601, loss: 0.377, 19232/60000 datapoints
2025-03-06 20:07:24,644 - INFO - training batch 651, loss: 0.097, 20832/60000 datapoints
2025-03-06 20:07:24,877 - INFO - training batch 701, loss: 0.257, 22432/60000 datapoints
2025-03-06 20:07:25,095 - INFO - training batch 751, loss: 0.133, 24032/60000 datapoints
2025-03-06 20:07:25,310 - INFO - training batch 801, loss: 0.327, 25632/60000 datapoints
2025-03-06 20:07:25,526 - INFO - training batch 851, loss: 0.085, 27232/60000 datapoints
2025-03-06 20:07:25,746 - INFO - training batch 901, loss: 0.142, 28832/60000 datapoints
2025-03-06 20:07:25,958 - INFO - training batch 951, loss: 0.422, 30432/60000 datapoints
2025-03-06 20:07:26,170 - INFO - training batch 1001, loss: 0.315, 32032/60000 datapoints
2025-03-06 20:07:26,393 - INFO - training batch 1051, loss: 0.253, 33632/60000 datapoints
2025-03-06 20:07:26,619 - INFO - training batch 1101, loss: 0.068, 35232/60000 datapoints
2025-03-06 20:07:26,832 - INFO - training batch 1151, loss: 0.129, 36832/60000 datapoints
2025-03-06 20:07:27,043 - INFO - training batch 1201, loss: 0.189, 38432/60000 datapoints
2025-03-06 20:07:27,249 - INFO - training batch 1251, loss: 0.183, 40032/60000 datapoints
2025-03-06 20:07:27,466 - INFO - training batch 1301, loss: 0.202, 41632/60000 datapoints
2025-03-06 20:07:27,686 - INFO - training batch 1351, loss: 0.170, 43232/60000 datapoints
2025-03-06 20:07:27,895 - INFO - training batch 1401, loss: 0.103, 44832/60000 datapoints
2025-03-06 20:07:28,104 - INFO - training batch 1451, loss: 0.075, 46432/60000 datapoints
2025-03-06 20:07:28,315 - INFO - training batch 1501, loss: 0.171, 48032/60000 datapoints
2025-03-06 20:07:28,523 - INFO - training batch 1551, loss: 0.113, 49632/60000 datapoints
2025-03-06 20:07:28,735 - INFO - training batch 1601, loss: 0.314, 51232/60000 datapoints
2025-03-06 20:07:28,963 - INFO - training batch 1651, loss: 0.418, 52832/60000 datapoints
2025-03-06 20:07:29,169 - INFO - training batch 1701, loss: 0.146, 54432/60000 datapoints
2025-03-06 20:07:29,378 - INFO - training batch 1751, loss: 0.207, 56032/60000 datapoints
2025-03-06 20:07:29,589 - INFO - training batch 1801, loss: 1.187, 57632/60000 datapoints
2025-03-06 20:07:29,802 - INFO - training batch 1851, loss: 0.166, 59232/60000 datapoints
2025-03-06 20:07:29,915 - INFO - validation batch 1, loss: 0.111, 32/10016 datapoints
2025-03-06 20:07:30,085 - INFO - validation batch 51, loss: 0.510, 1632/10016 datapoints
2025-03-06 20:07:30,259 - INFO - validation batch 101, loss: 0.131, 3232/10016 datapoints
2025-03-06 20:07:30,436 - INFO - validation batch 151, loss: 0.235, 4832/10016 datapoints
2025-03-06 20:07:30,603 - INFO - validation batch 201, loss: 0.167, 6432/10016 datapoints
2025-03-06 20:07:30,776 - INFO - validation batch 251, loss: 0.428, 8032/10016 datapoints
2025-03-06 20:07:30,946 - INFO - validation batch 301, loss: 0.151, 9632/10016 datapoints
2025-03-06 20:07:30,991 - INFO - Epoch 599/800 done.
2025-03-06 20:07:30,991 - INFO - Final validation performance:
Loss: 0.248, top-1 acc: 0.931top-5 acc: 0.931
2025-03-06 20:07:30,992 - INFO - Beginning epoch 600/800
2025-03-06 20:07:30,999 - INFO - training batch 1, loss: 0.132, 32/60000 datapoints
2025-03-06 20:07:31,208 - INFO - training batch 51, loss: 0.079, 1632/60000 datapoints
2025-03-06 20:07:31,419 - INFO - training batch 101, loss: 0.123, 3232/60000 datapoints
2025-03-06 20:07:31,643 - INFO - training batch 151, loss: 0.408, 4832/60000 datapoints
2025-03-06 20:07:31,858 - INFO - training batch 201, loss: 0.544, 6432/60000 datapoints
2025-03-06 20:07:32,071 - INFO - training batch 251, loss: 0.296, 8032/60000 datapoints
2025-03-06 20:07:32,284 - INFO - training batch 301, loss: 0.233, 9632/60000 datapoints
2025-03-06 20:07:32,498 - INFO - training batch 351, loss: 0.283, 11232/60000 datapoints
2025-03-06 20:07:32,723 - INFO - training batch 401, loss: 0.150, 12832/60000 datapoints
2025-03-06 20:07:33,210 - INFO - training batch 451, loss: 0.354, 14432/60000 datapoints
2025-03-06 20:07:33,411 - INFO - training batch 501, loss: 0.113, 16032/60000 datapoints
2025-03-06 20:07:33,617 - INFO - training batch 551, loss: 0.609, 17632/60000 datapoints
2025-03-06 20:07:33,822 - INFO - training batch 601, loss: 0.222, 19232/60000 datapoints
2025-03-06 20:07:34,023 - INFO - training batch 651, loss: 0.372, 20832/60000 datapoints
2025-03-06 20:07:34,224 - INFO - training batch 701, loss: 0.092, 22432/60000 datapoints
2025-03-06 20:07:34,432 - INFO - training batch 751, loss: 0.169, 24032/60000 datapoints
2025-03-06 20:07:34,639 - INFO - training batch 801, loss: 0.469, 25632/60000 datapoints
2025-03-06 20:07:34,848 - INFO - training batch 851, loss: 0.157, 27232/60000 datapoints
2025-03-06 20:07:35,055 - INFO - training batch 901, loss: 0.190, 28832/60000 datapoints
2025-03-06 20:07:35,260 - INFO - training batch 951, loss: 0.376, 30432/60000 datapoints
2025-03-06 20:07:35,480 - INFO - training batch 1001, loss: 0.131, 32032/60000 datapoints
2025-03-06 20:07:35,702 - INFO - training batch 1051, loss: 0.107, 33632/60000 datapoints
2025-03-06 20:07:35,915 - INFO - training batch 1101, loss: 0.213, 35232/60000 datapoints
2025-03-06 20:07:36,125 - INFO - training batch 1151, loss: 0.291, 36832/60000 datapoints
2025-03-06 20:07:36,341 - INFO - training batch 1201, loss: 0.451, 38432/60000 datapoints
2025-03-06 20:07:36,585 - INFO - training batch 1251, loss: 0.159, 40032/60000 datapoints
2025-03-06 20:07:36,809 - INFO - training batch 1301, loss: 0.289, 41632/60000 datapoints
2025-03-06 20:07:37,028 - INFO - training batch 1351, loss: 0.230, 43232/60000 datapoints
2025-03-06 20:07:37,249 - INFO - training batch 1401, loss: 0.165, 44832/60000 datapoints
2025-03-06 20:07:37,472 - INFO - training batch 1451, loss: 0.261, 46432/60000 datapoints
2025-03-06 20:07:37,731 - INFO - training batch 1501, loss: 0.174, 48032/60000 datapoints
2025-03-06 20:07:37,954 - INFO - training batch 1551, loss: 0.205, 49632/60000 datapoints
2025-03-06 20:07:38,173 - INFO - training batch 1601, loss: 0.227, 51232/60000 datapoints
2025-03-06 20:07:38,394 - INFO - training batch 1651, loss: 0.213, 52832/60000 datapoints
2025-03-06 20:07:38,667 - INFO - training batch 1701, loss: 0.198, 54432/60000 datapoints
2025-03-06 20:07:38,969 - INFO - training batch 1751, loss: 0.216, 56032/60000 datapoints
2025-03-06 20:07:39,191 - INFO - training batch 1801, loss: 0.173, 57632/60000 datapoints
2025-03-06 20:07:39,413 - INFO - training batch 1851, loss: 0.109, 59232/60000 datapoints
2025-03-06 20:07:39,536 - INFO - validation batch 1, loss: 0.194, 32/10016 datapoints
2025-03-06 20:07:39,720 - INFO - validation batch 51, loss: 0.200, 1632/10016 datapoints
2025-03-06 20:07:39,901 - INFO - validation batch 101, loss: 0.113, 3232/10016 datapoints
2025-03-06 20:07:40,083 - INFO - validation batch 151, loss: 0.517, 4832/10016 datapoints
2025-03-06 20:07:40,263 - INFO - validation batch 201, loss: 0.297, 6432/10016 datapoints
2025-03-06 20:07:40,443 - INFO - validation batch 251, loss: 0.123, 8032/10016 datapoints
2025-03-06 20:07:40,621 - INFO - validation batch 301, loss: 0.171, 9632/10016 datapoints
2025-03-06 20:07:40,668 - INFO - Epoch 600/800 done.
2025-03-06 20:07:40,669 - INFO - Final validation performance:
Loss: 0.231, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:07:40,670 - INFO - Beginning epoch 601/800
2025-03-06 20:07:40,682 - INFO - training batch 1, loss: 0.207, 32/60000 datapoints
2025-03-06 20:07:40,896 - INFO - training batch 51, loss: 0.213, 1632/60000 datapoints
2025-03-06 20:07:41,115 - INFO - training batch 101, loss: 0.196, 3232/60000 datapoints
2025-03-06 20:07:41,343 - INFO - training batch 151, loss: 0.123, 4832/60000 datapoints
2025-03-06 20:07:41,562 - INFO - training batch 201, loss: 0.219, 6432/60000 datapoints
2025-03-06 20:07:41,782 - INFO - training batch 251, loss: 0.578, 8032/60000 datapoints
2025-03-06 20:07:42,006 - INFO - training batch 301, loss: 0.351, 9632/60000 datapoints
2025-03-06 20:07:42,221 - INFO - training batch 351, loss: 0.220, 11232/60000 datapoints
2025-03-06 20:07:42,443 - INFO - training batch 401, loss: 0.459, 12832/60000 datapoints
2025-03-06 20:07:42,674 - INFO - training batch 451, loss: 0.244, 14432/60000 datapoints
2025-03-06 20:07:42,895 - INFO - training batch 501, loss: 0.268, 16032/60000 datapoints
2025-03-06 20:07:43,115 - INFO - training batch 551, loss: 0.485, 17632/60000 datapoints
2025-03-06 20:07:43,340 - INFO - training batch 601, loss: 0.262, 19232/60000 datapoints
2025-03-06 20:07:43,563 - INFO - training batch 651, loss: 0.054, 20832/60000 datapoints
2025-03-06 20:07:43,788 - INFO - training batch 701, loss: 0.224, 22432/60000 datapoints
2025-03-06 20:07:44,013 - INFO - training batch 751, loss: 0.206, 24032/60000 datapoints
2025-03-06 20:07:44,229 - INFO - training batch 801, loss: 0.077, 25632/60000 datapoints
2025-03-06 20:07:44,448 - INFO - training batch 851, loss: 0.183, 27232/60000 datapoints
2025-03-06 20:07:44,670 - INFO - training batch 901, loss: 0.124, 28832/60000 datapoints
2025-03-06 20:07:44,898 - INFO - training batch 951, loss: 0.132, 30432/60000 datapoints
2025-03-06 20:07:45,122 - INFO - training batch 1001, loss: 0.422, 32032/60000 datapoints
2025-03-06 20:07:45,348 - INFO - training batch 1051, loss: 0.176, 33632/60000 datapoints
2025-03-06 20:07:45,570 - INFO - training batch 1101, loss: 0.850, 35232/60000 datapoints
2025-03-06 20:07:45,788 - INFO - training batch 1151, loss: 0.358, 36832/60000 datapoints
2025-03-06 20:07:46,005 - INFO - training batch 1201, loss: 0.174, 38432/60000 datapoints
2025-03-06 20:07:46,220 - INFO - training batch 1251, loss: 0.213, 40032/60000 datapoints
2025-03-06 20:07:46,436 - INFO - training batch 1301, loss: 0.281, 41632/60000 datapoints
2025-03-06 20:07:46,680 - INFO - training batch 1351, loss: 0.130, 43232/60000 datapoints
2025-03-06 20:07:46,897 - INFO - training batch 1401, loss: 0.357, 44832/60000 datapoints
2025-03-06 20:07:47,105 - INFO - training batch 1451, loss: 0.410, 46432/60000 datapoints
2025-03-06 20:07:47,313 - INFO - training batch 1501, loss: 0.073, 48032/60000 datapoints
2025-03-06 20:07:47,523 - INFO - training batch 1551, loss: 0.144, 49632/60000 datapoints
2025-03-06 20:07:47,733 - INFO - training batch 1601, loss: 0.155, 51232/60000 datapoints
2025-03-06 20:07:47,946 - INFO - training batch 1651, loss: 0.273, 52832/60000 datapoints
2025-03-06 20:07:48,155 - INFO - training batch 1701, loss: 0.190, 54432/60000 datapoints
2025-03-06 20:07:48,401 - INFO - training batch 1751, loss: 0.176, 56032/60000 datapoints
2025-03-06 20:07:48,638 - INFO - training batch 1801, loss: 0.136, 57632/60000 datapoints
2025-03-06 20:07:48,868 - INFO - training batch 1851, loss: 0.239, 59232/60000 datapoints
2025-03-06 20:07:48,993 - INFO - validation batch 1, loss: 0.252, 32/10016 datapoints
2025-03-06 20:07:49,167 - INFO - validation batch 51, loss: 0.133, 1632/10016 datapoints
2025-03-06 20:07:49,346 - INFO - validation batch 101, loss: 0.394, 3232/10016 datapoints
2025-03-06 20:07:49,530 - INFO - validation batch 151, loss: 0.170, 4832/10016 datapoints
2025-03-06 20:07:49,712 - INFO - validation batch 201, loss: 0.222, 6432/10016 datapoints
2025-03-06 20:07:49,916 - INFO - validation batch 251, loss: 0.279, 8032/10016 datapoints
2025-03-06 20:07:50,117 - INFO - validation batch 301, loss: 0.552, 9632/10016 datapoints
2025-03-06 20:07:50,164 - INFO - Epoch 601/800 done.
2025-03-06 20:07:50,166 - INFO - Final validation performance:
Loss: 0.286, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:07:50,167 - INFO - Beginning epoch 602/800
2025-03-06 20:07:50,176 - INFO - training batch 1, loss: 0.069, 32/60000 datapoints
2025-03-06 20:07:50,419 - INFO - training batch 51, loss: 0.318, 1632/60000 datapoints
2025-03-06 20:07:50,635 - INFO - training batch 101, loss: 0.308, 3232/60000 datapoints
2025-03-06 20:07:50,843 - INFO - training batch 151, loss: 0.147, 4832/60000 datapoints
2025-03-06 20:07:51,066 - INFO - training batch 201, loss: 0.173, 6432/60000 datapoints
2025-03-06 20:07:51,274 - INFO - training batch 251, loss: 0.098, 8032/60000 datapoints
2025-03-06 20:07:51,487 - INFO - training batch 301, loss: 0.422, 9632/60000 datapoints
2025-03-06 20:07:51,704 - INFO - training batch 351, loss: 0.129, 11232/60000 datapoints
2025-03-06 20:07:51,913 - INFO - training batch 401, loss: 0.116, 12832/60000 datapoints
2025-03-06 20:07:52,130 - INFO - training batch 451, loss: 0.200, 14432/60000 datapoints
2025-03-06 20:07:52,341 - INFO - training batch 501, loss: 0.141, 16032/60000 datapoints
2025-03-06 20:07:52,550 - INFO - training batch 551, loss: 0.032, 17632/60000 datapoints
2025-03-06 20:07:52,763 - INFO - training batch 601, loss: 0.239, 19232/60000 datapoints
2025-03-06 20:07:52,971 - INFO - training batch 651, loss: 0.148, 20832/60000 datapoints
2025-03-06 20:07:53,180 - INFO - training batch 701, loss: 0.309, 22432/60000 datapoints
2025-03-06 20:07:53,391 - INFO - training batch 751, loss: 0.224, 24032/60000 datapoints
2025-03-06 20:07:53,603 - INFO - training batch 801, loss: 0.118, 25632/60000 datapoints
2025-03-06 20:07:53,815 - INFO - training batch 851, loss: 0.234, 27232/60000 datapoints
2025-03-06 20:07:54,026 - INFO - training batch 901, loss: 0.373, 28832/60000 datapoints
2025-03-06 20:07:54,232 - INFO - training batch 951, loss: 0.222, 30432/60000 datapoints
2025-03-06 20:07:54,439 - INFO - training batch 1001, loss: 0.199, 32032/60000 datapoints
2025-03-06 20:07:54,655 - INFO - training batch 1051, loss: 0.218, 33632/60000 datapoints
2025-03-06 20:07:54,868 - INFO - training batch 1101, loss: 0.192, 35232/60000 datapoints
2025-03-06 20:07:55,077 - INFO - training batch 1151, loss: 0.085, 36832/60000 datapoints
2025-03-06 20:07:55,290 - INFO - training batch 1201, loss: 0.301, 38432/60000 datapoints
2025-03-06 20:07:55,504 - INFO - training batch 1251, loss: 0.128, 40032/60000 datapoints
2025-03-06 20:07:55,722 - INFO - training batch 1301, loss: 0.408, 41632/60000 datapoints
2025-03-06 20:07:55,938 - INFO - training batch 1351, loss: 0.314, 43232/60000 datapoints
2025-03-06 20:07:56,155 - INFO - training batch 1401, loss: 0.226, 44832/60000 datapoints
2025-03-06 20:07:56,370 - INFO - training batch 1451, loss: 0.190, 46432/60000 datapoints
2025-03-06 20:07:56,586 - INFO - training batch 1501, loss: 0.113, 48032/60000 datapoints
2025-03-06 20:07:56,826 - INFO - training batch 1551, loss: 0.307, 49632/60000 datapoints
2025-03-06 20:07:57,042 - INFO - training batch 1601, loss: 0.199, 51232/60000 datapoints
2025-03-06 20:07:57,264 - INFO - training batch 1651, loss: 0.179, 52832/60000 datapoints
2025-03-06 20:07:57,484 - INFO - training batch 1701, loss: 0.100, 54432/60000 datapoints
2025-03-06 20:07:57,706 - INFO - training batch 1751, loss: 0.560, 56032/60000 datapoints
2025-03-06 20:07:57,920 - INFO - training batch 1801, loss: 0.152, 57632/60000 datapoints
2025-03-06 20:07:58,139 - INFO - training batch 1851, loss: 0.224, 59232/60000 datapoints
2025-03-06 20:07:58,252 - INFO - validation batch 1, loss: 0.170, 32/10016 datapoints
2025-03-06 20:07:58,425 - INFO - validation batch 51, loss: 0.285, 1632/10016 datapoints
2025-03-06 20:07:58,603 - INFO - validation batch 101, loss: 0.101, 3232/10016 datapoints
2025-03-06 20:07:58,779 - INFO - validation batch 151, loss: 0.269, 4832/10016 datapoints
2025-03-06 20:07:58,954 - INFO - validation batch 201, loss: 0.311, 6432/10016 datapoints
2025-03-06 20:07:59,129 - INFO - validation batch 251, loss: 0.062, 8032/10016 datapoints
2025-03-06 20:07:59,304 - INFO - validation batch 301, loss: 0.152, 9632/10016 datapoints
2025-03-06 20:07:59,349 - INFO - Epoch 602/800 done.
2025-03-06 20:07:59,349 - INFO - Final validation performance:
Loss: 0.193, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:07:59,350 - INFO - Beginning epoch 603/800
2025-03-06 20:07:59,357 - INFO - training batch 1, loss: 0.229, 32/60000 datapoints
2025-03-06 20:07:59,576 - INFO - training batch 51, loss: 0.071, 1632/60000 datapoints
2025-03-06 20:07:59,792 - INFO - training batch 101, loss: 0.181, 3232/60000 datapoints
2025-03-06 20:08:00,007 - INFO - training batch 151, loss: 0.369, 4832/60000 datapoints
2025-03-06 20:08:00,222 - INFO - training batch 201, loss: 0.165, 6432/60000 datapoints
2025-03-06 20:08:00,445 - INFO - training batch 251, loss: 0.218, 8032/60000 datapoints
2025-03-06 20:08:00,664 - INFO - training batch 301, loss: 0.252, 9632/60000 datapoints
2025-03-06 20:08:00,890 - INFO - training batch 351, loss: 0.147, 11232/60000 datapoints
2025-03-06 20:08:01,105 - INFO - training batch 401, loss: 0.202, 12832/60000 datapoints
2025-03-06 20:08:01,324 - INFO - training batch 451, loss: 0.381, 14432/60000 datapoints
2025-03-06 20:08:01,546 - INFO - training batch 501, loss: 0.252, 16032/60000 datapoints
2025-03-06 20:08:01,819 - INFO - training batch 551, loss: 0.182, 17632/60000 datapoints
2025-03-06 20:08:02,052 - INFO - training batch 601, loss: 0.255, 19232/60000 datapoints
2025-03-06 20:08:02,300 - INFO - training batch 651, loss: 0.228, 20832/60000 datapoints
2025-03-06 20:08:02,544 - INFO - training batch 701, loss: 0.177, 22432/60000 datapoints
2025-03-06 20:08:02,827 - INFO - training batch 751, loss: 0.095, 24032/60000 datapoints
2025-03-06 20:08:03,094 - INFO - training batch 801, loss: 0.145, 25632/60000 datapoints
2025-03-06 20:08:03,322 - INFO - training batch 851, loss: 0.115, 27232/60000 datapoints
2025-03-06 20:08:03,592 - INFO - training batch 901, loss: 0.184, 28832/60000 datapoints
2025-03-06 20:08:03,845 - INFO - training batch 951, loss: 0.334, 30432/60000 datapoints
2025-03-06 20:08:04,068 - INFO - training batch 1001, loss: 0.236, 32032/60000 datapoints
2025-03-06 20:08:04,297 - INFO - training batch 1051, loss: 0.174, 33632/60000 datapoints
2025-03-06 20:08:04,562 - INFO - training batch 1101, loss: 0.346, 35232/60000 datapoints
2025-03-06 20:08:04,812 - INFO - training batch 1151, loss: 0.197, 36832/60000 datapoints
2025-03-06 20:08:05,084 - INFO - training batch 1201, loss: 0.149, 38432/60000 datapoints
2025-03-06 20:08:05,301 - INFO - training batch 1251, loss: 0.440, 40032/60000 datapoints
2025-03-06 20:08:05,540 - INFO - training batch 1301, loss: 0.257, 41632/60000 datapoints
2025-03-06 20:08:05,854 - INFO - training batch 1351, loss: 0.114, 43232/60000 datapoints
2025-03-06 20:08:06,135 - INFO - training batch 1401, loss: 0.240, 44832/60000 datapoints
2025-03-06 20:08:06,417 - INFO - training batch 1451, loss: 0.162, 46432/60000 datapoints
2025-03-06 20:08:06,714 - INFO - training batch 1501, loss: 0.542, 48032/60000 datapoints
2025-03-06 20:08:06,994 - INFO - training batch 1551, loss: 0.100, 49632/60000 datapoints
2025-03-06 20:08:07,269 - INFO - training batch 1601, loss: 0.050, 51232/60000 datapoints
2025-03-06 20:08:07,539 - INFO - training batch 1651, loss: 0.141, 52832/60000 datapoints
2025-03-06 20:08:07,806 - INFO - training batch 1701, loss: 0.105, 54432/60000 datapoints
2025-03-06 20:08:08,104 - INFO - training batch 1751, loss: 0.133, 56032/60000 datapoints
2025-03-06 20:08:08,386 - INFO - training batch 1801, loss: 0.242, 57632/60000 datapoints
2025-03-06 20:08:08,729 - INFO - training batch 1851, loss: 0.231, 59232/60000 datapoints
2025-03-06 20:08:08,903 - INFO - validation batch 1, loss: 0.390, 32/10016 datapoints
2025-03-06 20:08:09,215 - INFO - validation batch 51, loss: 0.187, 1632/10016 datapoints
2025-03-06 20:08:09,448 - INFO - validation batch 101, loss: 0.091, 3232/10016 datapoints
2025-03-06 20:08:09,894 - INFO - validation batch 151, loss: 0.156, 4832/10016 datapoints
2025-03-06 20:08:10,490 - INFO - validation batch 201, loss: 0.190, 6432/10016 datapoints
2025-03-06 20:08:10,811 - INFO - validation batch 251, loss: 0.646, 8032/10016 datapoints
2025-03-06 20:08:11,087 - INFO - validation batch 301, loss: 0.589, 9632/10016 datapoints
2025-03-06 20:08:11,168 - INFO - Epoch 603/800 done.
2025-03-06 20:08:11,169 - INFO - Final validation performance:
Loss: 0.321, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:08:11,170 - INFO - Beginning epoch 604/800
2025-03-06 20:08:11,181 - INFO - training batch 1, loss: 0.108, 32/60000 datapoints
2025-03-06 20:08:11,524 - INFO - training batch 51, loss: 0.059, 1632/60000 datapoints
2025-03-06 20:08:11,794 - INFO - training batch 101, loss: 0.383, 3232/60000 datapoints
2025-03-06 20:08:12,096 - INFO - training batch 151, loss: 0.140, 4832/60000 datapoints
2025-03-06 20:08:12,356 - INFO - training batch 201, loss: 0.069, 6432/60000 datapoints
2025-03-06 20:08:12,617 - INFO - training batch 251, loss: 0.231, 8032/60000 datapoints
2025-03-06 20:08:12,899 - INFO - training batch 301, loss: 0.212, 9632/60000 datapoints
2025-03-06 20:08:13,220 - INFO - training batch 351, loss: 0.116, 11232/60000 datapoints
2025-03-06 20:08:13,572 - INFO - training batch 401, loss: 0.425, 12832/60000 datapoints
2025-03-06 20:08:13,921 - INFO - training batch 451, loss: 0.200, 14432/60000 datapoints
2025-03-06 20:08:14,209 - INFO - training batch 501, loss: 0.378, 16032/60000 datapoints
2025-03-06 20:08:14,637 - INFO - training batch 551, loss: 0.392, 17632/60000 datapoints
2025-03-06 20:08:15,027 - INFO - training batch 601, loss: 0.257, 19232/60000 datapoints
2025-03-06 20:08:15,334 - INFO - training batch 651, loss: 0.411, 20832/60000 datapoints
2025-03-06 20:08:15,608 - INFO - training batch 701, loss: 0.205, 22432/60000 datapoints
2025-03-06 20:08:15,863 - INFO - training batch 751, loss: 0.125, 24032/60000 datapoints
2025-03-06 20:08:16,131 - INFO - training batch 801, loss: 0.229, 25632/60000 datapoints
2025-03-06 20:08:16,427 - INFO - training batch 851, loss: 0.167, 27232/60000 datapoints
2025-03-06 20:08:16,748 - INFO - training batch 901, loss: 0.070, 28832/60000 datapoints
2025-03-06 20:08:17,041 - INFO - training batch 951, loss: 0.127, 30432/60000 datapoints
2025-03-06 20:08:17,328 - INFO - training batch 1001, loss: 0.355, 32032/60000 datapoints
2025-03-06 20:08:17,582 - INFO - training batch 1051, loss: 0.120, 33632/60000 datapoints
2025-03-06 20:08:17,831 - INFO - training batch 1101, loss: 0.208, 35232/60000 datapoints
2025-03-06 20:08:18,080 - INFO - training batch 1151, loss: 0.330, 36832/60000 datapoints
2025-03-06 20:08:18,320 - INFO - training batch 1201, loss: 0.161, 38432/60000 datapoints
2025-03-06 20:08:18,569 - INFO - training batch 1251, loss: 0.273, 40032/60000 datapoints
2025-03-06 20:08:18,837 - INFO - training batch 1301, loss: 0.167, 41632/60000 datapoints
2025-03-06 20:08:19,275 - INFO - training batch 1351, loss: 0.393, 43232/60000 datapoints
2025-03-06 20:08:19,667 - INFO - training batch 1401, loss: 0.323, 44832/60000 datapoints
2025-03-06 20:08:20,438 - INFO - training batch 1451, loss: 0.164, 46432/60000 datapoints
2025-03-06 20:08:20,750 - INFO - training batch 1501, loss: 0.692, 48032/60000 datapoints
2025-03-06 20:08:21,083 - INFO - training batch 1551, loss: 0.104, 49632/60000 datapoints
2025-03-06 20:08:21,337 - INFO - training batch 1601, loss: 0.344, 51232/60000 datapoints
2025-03-06 20:08:21,698 - INFO - training batch 1651, loss: 0.228, 52832/60000 datapoints
2025-03-06 20:08:21,950 - INFO - training batch 1701, loss: 0.134, 54432/60000 datapoints
2025-03-06 20:08:22,251 - INFO - training batch 1751, loss: 0.415, 56032/60000 datapoints
2025-03-06 20:08:22,513 - INFO - training batch 1801, loss: 0.115, 57632/60000 datapoints
2025-03-06 20:08:22,777 - INFO - training batch 1851, loss: 0.134, 59232/60000 datapoints
2025-03-06 20:08:22,919 - INFO - validation batch 1, loss: 0.379, 32/10016 datapoints
2025-03-06 20:08:23,233 - INFO - validation batch 51, loss: 0.446, 1632/10016 datapoints
2025-03-06 20:08:23,495 - INFO - validation batch 101, loss: 0.184, 3232/10016 datapoints
2025-03-06 20:08:23,705 - INFO - validation batch 151, loss: 0.082, 4832/10016 datapoints
2025-03-06 20:08:23,907 - INFO - validation batch 201, loss: 0.103, 6432/10016 datapoints
2025-03-06 20:08:24,211 - INFO - validation batch 251, loss: 0.263, 8032/10016 datapoints
2025-03-06 20:08:24,419 - INFO - validation batch 301, loss: 0.237, 9632/10016 datapoints
2025-03-06 20:08:24,469 - INFO - Epoch 604/800 done.
2025-03-06 20:08:24,470 - INFO - Final validation performance:
Loss: 0.242, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:08:24,472 - INFO - Beginning epoch 605/800
2025-03-06 20:08:24,481 - INFO - training batch 1, loss: 0.098, 32/60000 datapoints
2025-03-06 20:08:24,820 - INFO - training batch 51, loss: 0.179, 1632/60000 datapoints
2025-03-06 20:08:25,237 - INFO - training batch 101, loss: 0.277, 3232/60000 datapoints
2025-03-06 20:08:25,498 - INFO - training batch 151, loss: 0.147, 4832/60000 datapoints
2025-03-06 20:08:25,755 - INFO - training batch 201, loss: 0.089, 6432/60000 datapoints
2025-03-06 20:08:26,011 - INFO - training batch 251, loss: 0.421, 8032/60000 datapoints
2025-03-06 20:08:26,265 - INFO - training batch 301, loss: 0.157, 9632/60000 datapoints
2025-03-06 20:08:26,511 - INFO - training batch 351, loss: 0.395, 11232/60000 datapoints
2025-03-06 20:08:26,764 - INFO - training batch 401, loss: 0.141, 12832/60000 datapoints
2025-03-06 20:08:27,071 - INFO - training batch 451, loss: 0.222, 14432/60000 datapoints
2025-03-06 20:08:27,370 - INFO - training batch 501, loss: 0.125, 16032/60000 datapoints
2025-03-06 20:08:27,652 - INFO - training batch 551, loss: 0.118, 17632/60000 datapoints
2025-03-06 20:08:27,897 - INFO - training batch 601, loss: 0.300, 19232/60000 datapoints
2025-03-06 20:08:28,196 - INFO - training batch 651, loss: 0.117, 20832/60000 datapoints
2025-03-06 20:08:28,465 - INFO - training batch 701, loss: 0.264, 22432/60000 datapoints
2025-03-06 20:08:28,749 - INFO - training batch 751, loss: 0.096, 24032/60000 datapoints
2025-03-06 20:08:29,010 - INFO - training batch 801, loss: 0.221, 25632/60000 datapoints
2025-03-06 20:08:29,256 - INFO - training batch 851, loss: 0.336, 27232/60000 datapoints
2025-03-06 20:08:29,496 - INFO - training batch 901, loss: 0.243, 28832/60000 datapoints
2025-03-06 20:08:29,868 - INFO - training batch 951, loss: 0.337, 30432/60000 datapoints
2025-03-06 20:08:30,154 - INFO - training batch 1001, loss: 0.520, 32032/60000 datapoints
2025-03-06 20:08:30,402 - INFO - training batch 1051, loss: 0.318, 33632/60000 datapoints
2025-03-06 20:08:30,696 - INFO - training batch 1101, loss: 0.549, 35232/60000 datapoints
2025-03-06 20:08:31,037 - INFO - training batch 1151, loss: 0.266, 36832/60000 datapoints
2025-03-06 20:08:31,315 - INFO - training batch 1201, loss: 0.275, 38432/60000 datapoints
2025-03-06 20:08:31,599 - INFO - training batch 1251, loss: 0.144, 40032/60000 datapoints
2025-03-06 20:08:31,867 - INFO - training batch 1301, loss: 0.280, 41632/60000 datapoints
2025-03-06 20:08:32,126 - INFO - training batch 1351, loss: 0.663, 43232/60000 datapoints
2025-03-06 20:08:32,376 - INFO - training batch 1401, loss: 0.189, 44832/60000 datapoints
2025-03-06 20:08:32,667 - INFO - training batch 1451, loss: 0.370, 46432/60000 datapoints
2025-03-06 20:08:32,893 - INFO - training batch 1501, loss: 0.386, 48032/60000 datapoints
2025-03-06 20:08:33,156 - INFO - training batch 1551, loss: 0.295, 49632/60000 datapoints
2025-03-06 20:08:33,394 - INFO - training batch 1601, loss: 0.432, 51232/60000 datapoints
2025-03-06 20:08:33,634 - INFO - training batch 1651, loss: 0.170, 52832/60000 datapoints
2025-03-06 20:08:33,870 - INFO - training batch 1701, loss: 0.091, 54432/60000 datapoints
2025-03-06 20:08:34,097 - INFO - training batch 1751, loss: 0.410, 56032/60000 datapoints
2025-03-06 20:08:34,325 - INFO - training batch 1801, loss: 0.111, 57632/60000 datapoints
2025-03-06 20:08:34,569 - INFO - training batch 1851, loss: 0.105, 59232/60000 datapoints
2025-03-06 20:08:34,716 - INFO - validation batch 1, loss: 0.109, 32/10016 datapoints
2025-03-06 20:08:34,906 - INFO - validation batch 51, loss: 0.145, 1632/10016 datapoints
2025-03-06 20:08:35,089 - INFO - validation batch 101, loss: 0.407, 3232/10016 datapoints
2025-03-06 20:08:35,259 - INFO - validation batch 151, loss: 0.084, 4832/10016 datapoints
2025-03-06 20:08:35,442 - INFO - validation batch 201, loss: 0.293, 6432/10016 datapoints
2025-03-06 20:08:35,616 - INFO - validation batch 251, loss: 0.246, 8032/10016 datapoints
2025-03-06 20:08:35,799 - INFO - validation batch 301, loss: 0.506, 9632/10016 datapoints
2025-03-06 20:08:35,845 - INFO - Epoch 605/800 done.
2025-03-06 20:08:35,845 - INFO - Final validation performance:
Loss: 0.256, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:08:35,846 - INFO - Beginning epoch 606/800
2025-03-06 20:08:35,853 - INFO - training batch 1, loss: 0.108, 32/60000 datapoints
2025-03-06 20:08:36,083 - INFO - training batch 51, loss: 0.320, 1632/60000 datapoints
2025-03-06 20:08:36,300 - INFO - training batch 101, loss: 0.194, 3232/60000 datapoints
2025-03-06 20:08:36,524 - INFO - training batch 151, loss: 0.296, 4832/60000 datapoints
2025-03-06 20:08:36,755 - INFO - training batch 201, loss: 0.221, 6432/60000 datapoints
2025-03-06 20:08:36,975 - INFO - training batch 251, loss: 0.178, 8032/60000 datapoints
2025-03-06 20:08:37,206 - INFO - training batch 301, loss: 0.165, 9632/60000 datapoints
2025-03-06 20:08:37,415 - INFO - training batch 351, loss: 0.144, 11232/60000 datapoints
2025-03-06 20:08:37,647 - INFO - training batch 401, loss: 0.144, 12832/60000 datapoints
2025-03-06 20:08:37,863 - INFO - training batch 451, loss: 0.237, 14432/60000 datapoints
2025-03-06 20:08:38,079 - INFO - training batch 501, loss: 0.172, 16032/60000 datapoints
2025-03-06 20:08:38,285 - INFO - training batch 551, loss: 0.498, 17632/60000 datapoints
2025-03-06 20:08:38,496 - INFO - training batch 601, loss: 0.313, 19232/60000 datapoints
2025-03-06 20:08:38,712 - INFO - training batch 651, loss: 0.315, 20832/60000 datapoints
2025-03-06 20:08:38,926 - INFO - training batch 701, loss: 0.280, 22432/60000 datapoints
2025-03-06 20:08:39,132 - INFO - training batch 751, loss: 0.092, 24032/60000 datapoints
2025-03-06 20:08:39,358 - INFO - training batch 801, loss: 0.251, 25632/60000 datapoints
2025-03-06 20:08:39,564 - INFO - training batch 851, loss: 0.097, 27232/60000 datapoints
2025-03-06 20:08:39,807 - INFO - training batch 901, loss: 0.264, 28832/60000 datapoints
2025-03-06 20:08:40,192 - INFO - training batch 951, loss: 0.219, 30432/60000 datapoints
2025-03-06 20:08:40,397 - INFO - training batch 1001, loss: 0.538, 32032/60000 datapoints
2025-03-06 20:08:40,601 - INFO - training batch 1051, loss: 0.467, 33632/60000 datapoints
2025-03-06 20:08:40,811 - INFO - training batch 1101, loss: 0.137, 35232/60000 datapoints
2025-03-06 20:08:41,017 - INFO - training batch 1151, loss: 0.135, 36832/60000 datapoints
2025-03-06 20:08:41,226 - INFO - training batch 1201, loss: 0.118, 38432/60000 datapoints
2025-03-06 20:08:41,458 - INFO - training batch 1251, loss: 0.249, 40032/60000 datapoints
2025-03-06 20:08:41,715 - INFO - training batch 1301, loss: 0.154, 41632/60000 datapoints
2025-03-06 20:08:41,944 - INFO - training batch 1351, loss: 0.145, 43232/60000 datapoints
2025-03-06 20:08:42,154 - INFO - training batch 1401, loss: 0.135, 44832/60000 datapoints
2025-03-06 20:08:42,401 - INFO - training batch 1451, loss: 0.062, 46432/60000 datapoints
2025-03-06 20:08:42,614 - INFO - training batch 1501, loss: 0.256, 48032/60000 datapoints
2025-03-06 20:08:42,822 - INFO - training batch 1551, loss: 0.237, 49632/60000 datapoints
2025-03-06 20:08:43,029 - INFO - training batch 1601, loss: 0.186, 51232/60000 datapoints
2025-03-06 20:08:43,234 - INFO - training batch 1651, loss: 0.207, 52832/60000 datapoints
2025-03-06 20:08:43,462 - INFO - training batch 1701, loss: 0.132, 54432/60000 datapoints
2025-03-06 20:08:43,687 - INFO - training batch 1751, loss: 0.495, 56032/60000 datapoints
2025-03-06 20:08:43,903 - INFO - training batch 1801, loss: 0.158, 57632/60000 datapoints
2025-03-06 20:08:44,121 - INFO - training batch 1851, loss: 0.125, 59232/60000 datapoints
2025-03-06 20:08:44,235 - INFO - validation batch 1, loss: 0.285, 32/10016 datapoints
2025-03-06 20:08:44,428 - INFO - validation batch 51, loss: 0.407, 1632/10016 datapoints
2025-03-06 20:08:44,631 - INFO - validation batch 101, loss: 0.210, 3232/10016 datapoints
2025-03-06 20:08:44,808 - INFO - validation batch 151, loss: 0.103, 4832/10016 datapoints
2025-03-06 20:08:44,993 - INFO - validation batch 201, loss: 0.353, 6432/10016 datapoints
2025-03-06 20:08:45,174 - INFO - validation batch 251, loss: 0.240, 8032/10016 datapoints
2025-03-06 20:08:45,356 - INFO - validation batch 301, loss: 0.186, 9632/10016 datapoints
2025-03-06 20:08:45,402 - INFO - Epoch 606/800 done.
2025-03-06 20:08:45,403 - INFO - Final validation performance:
Loss: 0.255, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:08:45,403 - INFO - Beginning epoch 607/800
2025-03-06 20:08:45,412 - INFO - training batch 1, loss: 0.198, 32/60000 datapoints
2025-03-06 20:08:45,646 - INFO - training batch 51, loss: 0.148, 1632/60000 datapoints
2025-03-06 20:08:45,892 - INFO - training batch 101, loss: 0.197, 3232/60000 datapoints
2025-03-06 20:08:46,115 - INFO - training batch 151, loss: 0.093, 4832/60000 datapoints
2025-03-06 20:08:46,339 - INFO - training batch 201, loss: 0.230, 6432/60000 datapoints
2025-03-06 20:08:46,569 - INFO - training batch 251, loss: 0.092, 8032/60000 datapoints
2025-03-06 20:08:46,798 - INFO - training batch 301, loss: 0.215, 9632/60000 datapoints
2025-03-06 20:08:47,027 - INFO - training batch 351, loss: 0.213, 11232/60000 datapoints
2025-03-06 20:08:47,282 - INFO - training batch 401, loss: 0.477, 12832/60000 datapoints
2025-03-06 20:08:47,503 - INFO - training batch 451, loss: 0.284, 14432/60000 datapoints
2025-03-06 20:08:47,736 - INFO - training batch 501, loss: 0.141, 16032/60000 datapoints
2025-03-06 20:08:47,963 - INFO - training batch 551, loss: 0.280, 17632/60000 datapoints
2025-03-06 20:08:48,185 - INFO - training batch 601, loss: 0.179, 19232/60000 datapoints
2025-03-06 20:08:48,407 - INFO - training batch 651, loss: 0.141, 20832/60000 datapoints
2025-03-06 20:08:48,631 - INFO - training batch 701, loss: 0.118, 22432/60000 datapoints
2025-03-06 20:08:48,858 - INFO - training batch 751, loss: 0.123, 24032/60000 datapoints
2025-03-06 20:08:49,082 - INFO - training batch 801, loss: 0.298, 25632/60000 datapoints
2025-03-06 20:08:49,303 - INFO - training batch 851, loss: 0.283, 27232/60000 datapoints
2025-03-06 20:08:49,523 - INFO - training batch 901, loss: 0.411, 28832/60000 datapoints
2025-03-06 20:08:49,750 - INFO - training batch 951, loss: 0.379, 30432/60000 datapoints
2025-03-06 20:08:49,972 - INFO - training batch 1001, loss: 0.177, 32032/60000 datapoints
2025-03-06 20:08:50,196 - INFO - training batch 1051, loss: 0.125, 33632/60000 datapoints
2025-03-06 20:08:50,414 - INFO - training batch 1101, loss: 0.102, 35232/60000 datapoints
2025-03-06 20:08:50,641 - INFO - training batch 1151, loss: 0.151, 36832/60000 datapoints
2025-03-06 20:08:50,862 - INFO - training batch 1201, loss: 0.535, 38432/60000 datapoints
2025-03-06 20:08:51,083 - INFO - training batch 1251, loss: 0.219, 40032/60000 datapoints
2025-03-06 20:08:51,305 - INFO - training batch 1301, loss: 0.234, 41632/60000 datapoints
2025-03-06 20:08:51,525 - INFO - training batch 1351, loss: 0.219, 43232/60000 datapoints
2025-03-06 20:08:51,763 - INFO - training batch 1401, loss: 0.350, 44832/60000 datapoints
2025-03-06 20:08:51,979 - INFO - training batch 1451, loss: 0.089, 46432/60000 datapoints
2025-03-06 20:08:52,197 - INFO - training batch 1501, loss: 0.473, 48032/60000 datapoints
2025-03-06 20:08:52,419 - INFO - training batch 1551, loss: 0.234, 49632/60000 datapoints
2025-03-06 20:08:52,644 - INFO - training batch 1601, loss: 0.244, 51232/60000 datapoints
2025-03-06 20:08:52,861 - INFO - training batch 1651, loss: 0.185, 52832/60000 datapoints
2025-03-06 20:08:53,077 - INFO - training batch 1701, loss: 0.293, 54432/60000 datapoints
2025-03-06 20:08:53,293 - INFO - training batch 1751, loss: 0.096, 56032/60000 datapoints
2025-03-06 20:08:53,509 - INFO - training batch 1801, loss: 0.430, 57632/60000 datapoints
2025-03-06 20:08:53,770 - INFO - training batch 1851, loss: 0.275, 59232/60000 datapoints
2025-03-06 20:08:53,902 - INFO - validation batch 1, loss: 0.112, 32/10016 datapoints
2025-03-06 20:08:54,088 - INFO - validation batch 51, loss: 0.313, 1632/10016 datapoints
2025-03-06 20:08:54,257 - INFO - validation batch 101, loss: 0.321, 3232/10016 datapoints
2025-03-06 20:08:54,424 - INFO - validation batch 151, loss: 0.083, 4832/10016 datapoints
2025-03-06 20:08:54,598 - INFO - validation batch 201, loss: 0.177, 6432/10016 datapoints
2025-03-06 20:08:54,771 - INFO - validation batch 251, loss: 0.124, 8032/10016 datapoints
2025-03-06 20:08:54,947 - INFO - validation batch 301, loss: 0.278, 9632/10016 datapoints
2025-03-06 20:08:54,998 - INFO - Epoch 607/800 done.
2025-03-06 20:08:54,999 - INFO - Final validation performance:
Loss: 0.201, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:08:54,999 - INFO - Beginning epoch 608/800
2025-03-06 20:08:55,008 - INFO - training batch 1, loss: 0.329, 32/60000 datapoints
2025-03-06 20:08:55,256 - INFO - training batch 51, loss: 0.180, 1632/60000 datapoints
2025-03-06 20:08:55,517 - INFO - training batch 101, loss: 0.533, 3232/60000 datapoints
2025-03-06 20:08:55,756 - INFO - training batch 151, loss: 0.190, 4832/60000 datapoints
2025-03-06 20:08:55,968 - INFO - training batch 201, loss: 0.118, 6432/60000 datapoints
2025-03-06 20:08:56,202 - INFO - training batch 251, loss: 0.058, 8032/60000 datapoints
2025-03-06 20:08:56,523 - INFO - training batch 301, loss: 0.117, 9632/60000 datapoints
2025-03-06 20:08:56,739 - INFO - training batch 351, loss: 0.178, 11232/60000 datapoints
2025-03-06 20:08:56,953 - INFO - training batch 401, loss: 0.256, 12832/60000 datapoints
2025-03-06 20:08:57,173 - INFO - training batch 451, loss: 0.092, 14432/60000 datapoints
2025-03-06 20:08:57,407 - INFO - training batch 501, loss: 0.152, 16032/60000 datapoints
2025-03-06 20:08:57,621 - INFO - training batch 551, loss: 0.230, 17632/60000 datapoints
2025-03-06 20:08:57,846 - INFO - training batch 601, loss: 0.206, 19232/60000 datapoints
2025-03-06 20:08:58,066 - INFO - training batch 651, loss: 0.230, 20832/60000 datapoints
2025-03-06 20:08:58,288 - INFO - training batch 701, loss: 0.119, 22432/60000 datapoints
2025-03-06 20:08:58,513 - INFO - training batch 751, loss: 0.097, 24032/60000 datapoints
2025-03-06 20:08:58,757 - INFO - training batch 801, loss: 0.372, 25632/60000 datapoints
2025-03-06 20:08:58,979 - INFO - training batch 851, loss: 0.243, 27232/60000 datapoints
2025-03-06 20:08:59,202 - INFO - training batch 901, loss: 0.440, 28832/60000 datapoints
2025-03-06 20:08:59,419 - INFO - training batch 951, loss: 0.257, 30432/60000 datapoints
2025-03-06 20:08:59,651 - INFO - training batch 1001, loss: 0.197, 32032/60000 datapoints
2025-03-06 20:08:59,876 - INFO - training batch 1051, loss: 0.057, 33632/60000 datapoints
2025-03-06 20:09:00,122 - INFO - training batch 1101, loss: 0.108, 35232/60000 datapoints
2025-03-06 20:09:00,342 - INFO - training batch 1151, loss: 0.301, 36832/60000 datapoints
2025-03-06 20:09:00,645 - INFO - training batch 1201, loss: 0.422, 38432/60000 datapoints
2025-03-06 20:09:00,900 - INFO - training batch 1251, loss: 0.162, 40032/60000 datapoints
2025-03-06 20:09:01,160 - INFO - training batch 1301, loss: 0.257, 41632/60000 datapoints
2025-03-06 20:09:01,466 - INFO - training batch 1351, loss: 0.117, 43232/60000 datapoints
2025-03-06 20:09:01,709 - INFO - training batch 1401, loss: 0.098, 44832/60000 datapoints
2025-03-06 20:09:01,936 - INFO - training batch 1451, loss: 0.172, 46432/60000 datapoints
2025-03-06 20:09:02,201 - INFO - training batch 1501, loss: 0.241, 48032/60000 datapoints
2025-03-06 20:09:02,445 - INFO - training batch 1551, loss: 0.123, 49632/60000 datapoints
2025-03-06 20:09:02,832 - INFO - training batch 1601, loss: 0.465, 51232/60000 datapoints
2025-03-06 20:09:03,089 - INFO - training batch 1651, loss: 0.042, 52832/60000 datapoints
2025-03-06 20:09:03,401 - INFO - training batch 1701, loss: 0.079, 54432/60000 datapoints
2025-03-06 20:09:03,681 - INFO - training batch 1751, loss: 0.276, 56032/60000 datapoints
2025-03-06 20:09:03,970 - INFO - training batch 1801, loss: 0.435, 57632/60000 datapoints
2025-03-06 20:09:04,207 - INFO - training batch 1851, loss: 0.163, 59232/60000 datapoints
2025-03-06 20:09:04,341 - INFO - validation batch 1, loss: 0.068, 32/10016 datapoints
2025-03-06 20:09:04,534 - INFO - validation batch 51, loss: 0.324, 1632/10016 datapoints
2025-03-06 20:09:04,723 - INFO - validation batch 101, loss: 0.227, 3232/10016 datapoints
2025-03-06 20:09:04,946 - INFO - validation batch 151, loss: 0.143, 4832/10016 datapoints
2025-03-06 20:09:05,143 - INFO - validation batch 201, loss: 0.263, 6432/10016 datapoints
2025-03-06 20:09:05,331 - INFO - validation batch 251, loss: 0.239, 8032/10016 datapoints
2025-03-06 20:09:05,536 - INFO - validation batch 301, loss: 0.146, 9632/10016 datapoints
2025-03-06 20:09:05,587 - INFO - Epoch 608/800 done.
2025-03-06 20:09:05,587 - INFO - Final validation performance:
Loss: 0.201, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:09:05,588 - INFO - Beginning epoch 609/800
2025-03-06 20:09:05,596 - INFO - training batch 1, loss: 0.302, 32/60000 datapoints
2025-03-06 20:09:05,919 - INFO - training batch 51, loss: 0.457, 1632/60000 datapoints
2025-03-06 20:09:06,197 - INFO - training batch 101, loss: 0.269, 3232/60000 datapoints
2025-03-06 20:09:06,443 - INFO - training batch 151, loss: 0.106, 4832/60000 datapoints
2025-03-06 20:09:06,690 - INFO - training batch 201, loss: 0.220, 6432/60000 datapoints
2025-03-06 20:09:07,005 - INFO - training batch 251, loss: 0.111, 8032/60000 datapoints
2025-03-06 20:09:07,258 - INFO - training batch 301, loss: 0.091, 9632/60000 datapoints
2025-03-06 20:09:07,560 - INFO - training batch 351, loss: 0.233, 11232/60000 datapoints
2025-03-06 20:09:07,840 - INFO - training batch 401, loss: 0.297, 12832/60000 datapoints
2025-03-06 20:09:08,110 - INFO - training batch 451, loss: 0.173, 14432/60000 datapoints
2025-03-06 20:09:08,374 - INFO - training batch 501, loss: 0.301, 16032/60000 datapoints
2025-03-06 20:09:08,654 - INFO - training batch 551, loss: 0.207, 17632/60000 datapoints
2025-03-06 20:09:08,948 - INFO - training batch 601, loss: 0.380, 19232/60000 datapoints
2025-03-06 20:09:09,219 - INFO - training batch 651, loss: 0.513, 20832/60000 datapoints
2025-03-06 20:09:09,459 - INFO - training batch 701, loss: 0.223, 22432/60000 datapoints
2025-03-06 20:09:09,764 - INFO - training batch 751, loss: 0.151, 24032/60000 datapoints
2025-03-06 20:09:10,010 - INFO - training batch 801, loss: 0.241, 25632/60000 datapoints
2025-03-06 20:09:10,243 - INFO - training batch 851, loss: 0.162, 27232/60000 datapoints
2025-03-06 20:09:10,468 - INFO - training batch 901, loss: 0.106, 28832/60000 datapoints
2025-03-06 20:09:10,711 - INFO - training batch 951, loss: 0.229, 30432/60000 datapoints
2025-03-06 20:09:10,944 - INFO - training batch 1001, loss: 0.284, 32032/60000 datapoints
2025-03-06 20:09:11,172 - INFO - training batch 1051, loss: 0.234, 33632/60000 datapoints
2025-03-06 20:09:11,395 - INFO - training batch 1101, loss: 0.388, 35232/60000 datapoints
2025-03-06 20:09:11,616 - INFO - training batch 1151, loss: 0.117, 36832/60000 datapoints
2025-03-06 20:09:11,887 - INFO - training batch 1201, loss: 0.291, 38432/60000 datapoints
2025-03-06 20:09:12,109 - INFO - training batch 1251, loss: 0.157, 40032/60000 datapoints
2025-03-06 20:09:12,332 - INFO - training batch 1301, loss: 0.391, 41632/60000 datapoints
2025-03-06 20:09:12,549 - INFO - training batch 1351, loss: 0.597, 43232/60000 datapoints
2025-03-06 20:09:12,769 - INFO - training batch 1401, loss: 0.541, 44832/60000 datapoints
2025-03-06 20:09:12,987 - INFO - training batch 1451, loss: 0.126, 46432/60000 datapoints
2025-03-06 20:09:13,209 - INFO - training batch 1501, loss: 0.211, 48032/60000 datapoints
2025-03-06 20:09:13,421 - INFO - training batch 1551, loss: 0.209, 49632/60000 datapoints
2025-03-06 20:09:13,643 - INFO - training batch 1601, loss: 0.227, 51232/60000 datapoints
2025-03-06 20:09:13,864 - INFO - training batch 1651, loss: 0.347, 52832/60000 datapoints
2025-03-06 20:09:14,076 - INFO - training batch 1701, loss: 0.165, 54432/60000 datapoints
2025-03-06 20:09:14,285 - INFO - training batch 1751, loss: 0.341, 56032/60000 datapoints
2025-03-06 20:09:14,494 - INFO - training batch 1801, loss: 0.084, 57632/60000 datapoints
2025-03-06 20:09:14,707 - INFO - training batch 1851, loss: 0.341, 59232/60000 datapoints
2025-03-06 20:09:14,827 - INFO - validation batch 1, loss: 0.231, 32/10016 datapoints
2025-03-06 20:09:15,002 - INFO - validation batch 51, loss: 0.353, 1632/10016 datapoints
2025-03-06 20:09:15,172 - INFO - validation batch 101, loss: 0.197, 3232/10016 datapoints
2025-03-06 20:09:15,342 - INFO - validation batch 151, loss: 0.380, 4832/10016 datapoints
2025-03-06 20:09:15,509 - INFO - validation batch 201, loss: 0.356, 6432/10016 datapoints
2025-03-06 20:09:15,683 - INFO - validation batch 251, loss: 0.230, 8032/10016 datapoints
2025-03-06 20:09:15,851 - INFO - validation batch 301, loss: 0.144, 9632/10016 datapoints
2025-03-06 20:09:15,907 - INFO - Epoch 609/800 done.
2025-03-06 20:09:15,907 - INFO - Final validation performance:
Loss: 0.270, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:09:15,908 - INFO - Beginning epoch 610/800
2025-03-06 20:09:15,915 - INFO - training batch 1, loss: 0.157, 32/60000 datapoints
2025-03-06 20:09:16,123 - INFO - training batch 51, loss: 0.076, 1632/60000 datapoints
2025-03-06 20:09:16,345 - INFO - training batch 101, loss: 0.324, 3232/60000 datapoints
2025-03-06 20:09:16,548 - INFO - training batch 151, loss: 0.446, 4832/60000 datapoints
2025-03-06 20:09:16,762 - INFO - training batch 201, loss: 0.306, 6432/60000 datapoints
2025-03-06 20:09:16,970 - INFO - training batch 251, loss: 0.264, 8032/60000 datapoints
2025-03-06 20:09:17,188 - INFO - training batch 301, loss: 0.172, 9632/60000 datapoints
2025-03-06 20:09:17,392 - INFO - training batch 351, loss: 0.411, 11232/60000 datapoints
2025-03-06 20:09:17,627 - INFO - training batch 401, loss: 0.161, 12832/60000 datapoints
2025-03-06 20:09:17,836 - INFO - training batch 451, loss: 0.124, 14432/60000 datapoints
2025-03-06 20:09:18,035 - INFO - training batch 501, loss: 0.411, 16032/60000 datapoints
2025-03-06 20:09:18,239 - INFO - training batch 551, loss: 0.122, 17632/60000 datapoints
2025-03-06 20:09:18,440 - INFO - training batch 601, loss: 0.239, 19232/60000 datapoints
2025-03-06 20:09:18,645 - INFO - training batch 651, loss: 0.262, 20832/60000 datapoints
2025-03-06 20:09:18,865 - INFO - training batch 701, loss: 0.355, 22432/60000 datapoints
2025-03-06 20:09:19,069 - INFO - training batch 751, loss: 0.107, 24032/60000 datapoints
2025-03-06 20:09:19,273 - INFO - training batch 801, loss: 0.298, 25632/60000 datapoints
2025-03-06 20:09:19,476 - INFO - training batch 851, loss: 0.672, 27232/60000 datapoints
2025-03-06 20:09:19,681 - INFO - training batch 901, loss: 0.390, 28832/60000 datapoints
2025-03-06 20:09:19,895 - INFO - training batch 951, loss: 0.227, 30432/60000 datapoints
2025-03-06 20:09:20,096 - INFO - training batch 1001, loss: 0.193, 32032/60000 datapoints
2025-03-06 20:09:20,303 - INFO - training batch 1051, loss: 0.160, 33632/60000 datapoints
2025-03-06 20:09:20,508 - INFO - training batch 1101, loss: 0.533, 35232/60000 datapoints
2025-03-06 20:09:20,720 - INFO - training batch 1151, loss: 0.430, 36832/60000 datapoints
2025-03-06 20:09:20,932 - INFO - training batch 1201, loss: 0.667, 38432/60000 datapoints
2025-03-06 20:09:21,143 - INFO - training batch 1251, loss: 0.245, 40032/60000 datapoints
2025-03-06 20:09:21,350 - INFO - training batch 1301, loss: 0.086, 41632/60000 datapoints
2025-03-06 20:09:21,571 - INFO - training batch 1351, loss: 0.157, 43232/60000 datapoints
2025-03-06 20:09:21,785 - INFO - training batch 1401, loss: 0.169, 44832/60000 datapoints
2025-03-06 20:09:21,993 - INFO - training batch 1451, loss: 0.194, 46432/60000 datapoints
2025-03-06 20:09:22,208 - INFO - training batch 1501, loss: 0.214, 48032/60000 datapoints
2025-03-06 20:09:22,442 - INFO - training batch 1551, loss: 0.147, 49632/60000 datapoints
2025-03-06 20:09:22,681 - INFO - training batch 1601, loss: 0.429, 51232/60000 datapoints
2025-03-06 20:09:22,941 - INFO - training batch 1651, loss: 0.396, 52832/60000 datapoints
2025-03-06 20:09:23,160 - INFO - training batch 1701, loss: 0.200, 54432/60000 datapoints
2025-03-06 20:09:23,379 - INFO - training batch 1751, loss: 0.325, 56032/60000 datapoints
2025-03-06 20:09:23,605 - INFO - training batch 1801, loss: 0.615, 57632/60000 datapoints
2025-03-06 20:09:23,888 - INFO - training batch 1851, loss: 0.093, 59232/60000 datapoints
2025-03-06 20:09:24,006 - INFO - validation batch 1, loss: 0.086, 32/10016 datapoints
2025-03-06 20:09:24,214 - INFO - validation batch 51, loss: 0.120, 1632/10016 datapoints
2025-03-06 20:09:24,408 - INFO - validation batch 101, loss: 0.299, 3232/10016 datapoints
2025-03-06 20:09:24,592 - INFO - validation batch 151, loss: 0.202, 4832/10016 datapoints
2025-03-06 20:09:24,777 - INFO - validation batch 201, loss: 0.408, 6432/10016 datapoints
2025-03-06 20:09:25,005 - INFO - validation batch 251, loss: 0.223, 8032/10016 datapoints
2025-03-06 20:09:25,190 - INFO - validation batch 301, loss: 0.422, 9632/10016 datapoints
2025-03-06 20:09:25,236 - INFO - Epoch 610/800 done.
2025-03-06 20:09:25,236 - INFO - Final validation performance:
Loss: 0.252, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:09:25,237 - INFO - Beginning epoch 611/800
2025-03-06 20:09:25,245 - INFO - training batch 1, loss: 0.068, 32/60000 datapoints
2025-03-06 20:09:25,466 - INFO - training batch 51, loss: 0.111, 1632/60000 datapoints
2025-03-06 20:09:25,696 - INFO - training batch 101, loss: 0.254, 3232/60000 datapoints
2025-03-06 20:09:25,942 - INFO - training batch 151, loss: 0.162, 4832/60000 datapoints
2025-03-06 20:09:26,174 - INFO - training batch 201, loss: 0.180, 6432/60000 datapoints
2025-03-06 20:09:26,402 - INFO - training batch 251, loss: 0.126, 8032/60000 datapoints
2025-03-06 20:09:26,625 - INFO - training batch 301, loss: 0.407, 9632/60000 datapoints
2025-03-06 20:09:26,862 - INFO - training batch 351, loss: 0.045, 11232/60000 datapoints
2025-03-06 20:09:27,116 - INFO - training batch 401, loss: 0.117, 12832/60000 datapoints
2025-03-06 20:09:27,453 - INFO - training batch 451, loss: 0.071, 14432/60000 datapoints
2025-03-06 20:09:27,759 - INFO - training batch 501, loss: 0.059, 16032/60000 datapoints
2025-03-06 20:09:28,075 - INFO - training batch 551, loss: 0.278, 17632/60000 datapoints
2025-03-06 20:09:28,325 - INFO - training batch 601, loss: 0.102, 19232/60000 datapoints
2025-03-06 20:09:28,590 - INFO - training batch 651, loss: 0.147, 20832/60000 datapoints
2025-03-06 20:09:28,852 - INFO - training batch 701, loss: 0.407, 22432/60000 datapoints
2025-03-06 20:09:29,119 - INFO - training batch 751, loss: 0.185, 24032/60000 datapoints
2025-03-06 20:09:29,356 - INFO - training batch 801, loss: 0.130, 25632/60000 datapoints
2025-03-06 20:09:29,588 - INFO - training batch 851, loss: 0.139, 27232/60000 datapoints
2025-03-06 20:09:29,829 - INFO - training batch 901, loss: 0.108, 28832/60000 datapoints
2025-03-06 20:09:30,076 - INFO - training batch 951, loss: 0.256, 30432/60000 datapoints
2025-03-06 20:09:30,310 - INFO - training batch 1001, loss: 0.237, 32032/60000 datapoints
2025-03-06 20:09:30,544 - INFO - training batch 1051, loss: 0.163, 33632/60000 datapoints
2025-03-06 20:09:30,784 - INFO - training batch 1101, loss: 0.081, 35232/60000 datapoints
2025-03-06 20:09:31,069 - INFO - training batch 1151, loss: 0.323, 36832/60000 datapoints
2025-03-06 20:09:31,310 - INFO - training batch 1201, loss: 0.281, 38432/60000 datapoints
2025-03-06 20:09:31,543 - INFO - training batch 1251, loss: 0.147, 40032/60000 datapoints
2025-03-06 20:09:31,794 - INFO - training batch 1301, loss: 0.277, 41632/60000 datapoints
2025-03-06 20:09:32,027 - INFO - training batch 1351, loss: 0.120, 43232/60000 datapoints
2025-03-06 20:09:32,254 - INFO - training batch 1401, loss: 0.402, 44832/60000 datapoints
2025-03-06 20:09:32,486 - INFO - training batch 1451, loss: 0.206, 46432/60000 datapoints
2025-03-06 20:09:32,720 - INFO - training batch 1501, loss: 0.171, 48032/60000 datapoints
2025-03-06 20:09:32,962 - INFO - training batch 1551, loss: 0.187, 49632/60000 datapoints
2025-03-06 20:09:33,188 - INFO - training batch 1601, loss: 0.341, 51232/60000 datapoints
2025-03-06 20:09:33,410 - INFO - training batch 1651, loss: 0.063, 52832/60000 datapoints
2025-03-06 20:09:33,632 - INFO - training batch 1701, loss: 0.299, 54432/60000 datapoints
2025-03-06 20:09:33,856 - INFO - training batch 1751, loss: 0.320, 56032/60000 datapoints
2025-03-06 20:09:34,084 - INFO - training batch 1801, loss: 0.145, 57632/60000 datapoints
2025-03-06 20:09:34,300 - INFO - training batch 1851, loss: 0.217, 59232/60000 datapoints
2025-03-06 20:09:34,418 - INFO - validation batch 1, loss: 0.243, 32/10016 datapoints
2025-03-06 20:09:34,593 - INFO - validation batch 51, loss: 0.828, 1632/10016 datapoints
2025-03-06 20:09:34,770 - INFO - validation batch 101, loss: 0.213, 3232/10016 datapoints
2025-03-06 20:09:34,952 - INFO - validation batch 151, loss: 0.240, 4832/10016 datapoints
2025-03-06 20:09:35,125 - INFO - validation batch 201, loss: 0.229, 6432/10016 datapoints
2025-03-06 20:09:35,295 - INFO - validation batch 251, loss: 0.165, 8032/10016 datapoints
2025-03-06 20:09:35,463 - INFO - validation batch 301, loss: 0.130, 9632/10016 datapoints
2025-03-06 20:09:35,506 - INFO - Epoch 611/800 done.
2025-03-06 20:09:35,506 - INFO - Final validation performance:
Loss: 0.293, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:09:35,507 - INFO - Beginning epoch 612/800
2025-03-06 20:09:35,515 - INFO - training batch 1, loss: 0.184, 32/60000 datapoints
2025-03-06 20:09:35,749 - INFO - training batch 51, loss: 0.293, 1632/60000 datapoints
2025-03-06 20:09:35,956 - INFO - training batch 101, loss: 0.210, 3232/60000 datapoints
2025-03-06 20:09:36,163 - INFO - training batch 151, loss: 0.330, 4832/60000 datapoints
2025-03-06 20:09:36,387 - INFO - training batch 201, loss: 0.204, 6432/60000 datapoints
2025-03-06 20:09:36,607 - INFO - training batch 251, loss: 0.179, 8032/60000 datapoints
2025-03-06 20:09:36,818 - INFO - training batch 301, loss: 0.203, 9632/60000 datapoints
2025-03-06 20:09:37,031 - INFO - training batch 351, loss: 0.302, 11232/60000 datapoints
2025-03-06 20:09:37,238 - INFO - training batch 401, loss: 0.213, 12832/60000 datapoints
2025-03-06 20:09:37,446 - INFO - training batch 451, loss: 0.160, 14432/60000 datapoints
2025-03-06 20:09:37,662 - INFO - training batch 501, loss: 0.347, 16032/60000 datapoints
2025-03-06 20:09:37,894 - INFO - training batch 551, loss: 0.253, 17632/60000 datapoints
2025-03-06 20:09:38,100 - INFO - training batch 601, loss: 0.251, 19232/60000 datapoints
2025-03-06 20:09:38,304 - INFO - training batch 651, loss: 0.389, 20832/60000 datapoints
2025-03-06 20:09:38,508 - INFO - training batch 701, loss: 0.078, 22432/60000 datapoints
2025-03-06 20:09:38,710 - INFO - training batch 751, loss: 0.182, 24032/60000 datapoints
2025-03-06 20:09:38,922 - INFO - training batch 801, loss: 0.149, 25632/60000 datapoints
2025-03-06 20:09:39,125 - INFO - training batch 851, loss: 0.375, 27232/60000 datapoints
2025-03-06 20:09:39,328 - INFO - training batch 901, loss: 0.295, 28832/60000 datapoints
2025-03-06 20:09:39,526 - INFO - training batch 951, loss: 0.262, 30432/60000 datapoints
2025-03-06 20:09:39,729 - INFO - training batch 1001, loss: 0.255, 32032/60000 datapoints
2025-03-06 20:09:39,935 - INFO - training batch 1051, loss: 0.803, 33632/60000 datapoints
2025-03-06 20:09:40,142 - INFO - training batch 1101, loss: 0.274, 35232/60000 datapoints
2025-03-06 20:09:40,344 - INFO - training batch 1151, loss: 0.439, 36832/60000 datapoints
2025-03-06 20:09:40,729 - INFO - training batch 1201, loss: 0.107, 38432/60000 datapoints
2025-03-06 20:09:40,995 - INFO - training batch 1251, loss: 0.195, 40032/60000 datapoints
2025-03-06 20:09:41,223 - INFO - training batch 1301, loss: 0.080, 41632/60000 datapoints
2025-03-06 20:09:41,420 - INFO - training batch 1351, loss: 0.120, 43232/60000 datapoints
2025-03-06 20:09:41,619 - INFO - training batch 1401, loss: 0.304, 44832/60000 datapoints
2025-03-06 20:09:41,829 - INFO - training batch 1451, loss: 0.044, 46432/60000 datapoints
2025-03-06 20:09:42,041 - INFO - training batch 1501, loss: 0.098, 48032/60000 datapoints
2025-03-06 20:09:42,244 - INFO - training batch 1551, loss: 0.473, 49632/60000 datapoints
2025-03-06 20:09:42,455 - INFO - training batch 1601, loss: 0.099, 51232/60000 datapoints
2025-03-06 20:09:42,675 - INFO - training batch 1651, loss: 0.398, 52832/60000 datapoints
2025-03-06 20:09:42,938 - INFO - training batch 1701, loss: 0.312, 54432/60000 datapoints
2025-03-06 20:09:43,184 - INFO - training batch 1751, loss: 0.104, 56032/60000 datapoints
2025-03-06 20:09:43,450 - INFO - training batch 1801, loss: 0.376, 57632/60000 datapoints
2025-03-06 20:09:43,659 - INFO - training batch 1851, loss: 0.099, 59232/60000 datapoints
2025-03-06 20:09:43,777 - INFO - validation batch 1, loss: 0.110, 32/10016 datapoints
2025-03-06 20:09:43,962 - INFO - validation batch 51, loss: 0.547, 1632/10016 datapoints
2025-03-06 20:09:44,142 - INFO - validation batch 101, loss: 0.125, 3232/10016 datapoints
2025-03-06 20:09:44,321 - INFO - validation batch 151, loss: 0.514, 4832/10016 datapoints
2025-03-06 20:09:44,497 - INFO - validation batch 201, loss: 0.134, 6432/10016 datapoints
2025-03-06 20:09:44,684 - INFO - validation batch 251, loss: 0.066, 8032/10016 datapoints
2025-03-06 20:09:44,867 - INFO - validation batch 301, loss: 0.620, 9632/10016 datapoints
2025-03-06 20:09:44,912 - INFO - Epoch 612/800 done.
2025-03-06 20:09:44,913 - INFO - Final validation performance:
Loss: 0.302, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:09:44,913 - INFO - Beginning epoch 613/800
2025-03-06 20:09:44,922 - INFO - training batch 1, loss: 0.278, 32/60000 datapoints
2025-03-06 20:09:45,168 - INFO - training batch 51, loss: 0.129, 1632/60000 datapoints
2025-03-06 20:09:45,391 - INFO - training batch 101, loss: 0.310, 3232/60000 datapoints
2025-03-06 20:09:45,619 - INFO - training batch 151, loss: 0.291, 4832/60000 datapoints
2025-03-06 20:09:45,858 - INFO - training batch 201, loss: 0.316, 6432/60000 datapoints
2025-03-06 20:09:46,098 - INFO - training batch 251, loss: 0.355, 8032/60000 datapoints
2025-03-06 20:09:46,341 - INFO - training batch 301, loss: 0.122, 9632/60000 datapoints
2025-03-06 20:09:46,567 - INFO - training batch 351, loss: 0.314, 11232/60000 datapoints
2025-03-06 20:09:46,797 - INFO - training batch 401, loss: 0.201, 12832/60000 datapoints
2025-03-06 20:09:47,020 - INFO - training batch 451, loss: 0.220, 14432/60000 datapoints
2025-03-06 20:09:47,243 - INFO - training batch 501, loss: 0.300, 16032/60000 datapoints
2025-03-06 20:09:47,464 - INFO - training batch 551, loss: 0.282, 17632/60000 datapoints
2025-03-06 20:09:47,687 - INFO - training batch 601, loss: 0.186, 19232/60000 datapoints
2025-03-06 20:09:47,942 - INFO - training batch 651, loss: 0.302, 20832/60000 datapoints
2025-03-06 20:09:48,165 - INFO - training batch 701, loss: 0.183, 22432/60000 datapoints
2025-03-06 20:09:48,391 - INFO - training batch 751, loss: 0.126, 24032/60000 datapoints
2025-03-06 20:09:48,615 - INFO - training batch 801, loss: 0.316, 25632/60000 datapoints
2025-03-06 20:09:48,843 - INFO - training batch 851, loss: 0.288, 27232/60000 datapoints
2025-03-06 20:09:49,063 - INFO - training batch 901, loss: 0.330, 28832/60000 datapoints
2025-03-06 20:09:49,287 - INFO - training batch 951, loss: 0.354, 30432/60000 datapoints
2025-03-06 20:09:49,511 - INFO - training batch 1001, loss: 0.196, 32032/60000 datapoints
2025-03-06 20:09:49,733 - INFO - training batch 1051, loss: 0.688, 33632/60000 datapoints
2025-03-06 20:09:49,959 - INFO - training batch 1101, loss: 0.120, 35232/60000 datapoints
2025-03-06 20:09:50,184 - INFO - training batch 1151, loss: 0.170, 36832/60000 datapoints
2025-03-06 20:09:50,408 - INFO - training batch 1201, loss: 0.230, 38432/60000 datapoints
2025-03-06 20:09:50,631 - INFO - training batch 1251, loss: 0.382, 40032/60000 datapoints
2025-03-06 20:09:50,853 - INFO - training batch 1301, loss: 0.417, 41632/60000 datapoints
2025-03-06 20:09:51,076 - INFO - training batch 1351, loss: 0.190, 43232/60000 datapoints
2025-03-06 20:09:51,299 - INFO - training batch 1401, loss: 0.409, 44832/60000 datapoints
2025-03-06 20:09:51,520 - INFO - training batch 1451, loss: 0.071, 46432/60000 datapoints
2025-03-06 20:09:51,739 - INFO - training batch 1501, loss: 0.186, 48032/60000 datapoints
2025-03-06 20:09:51,954 - INFO - training batch 1551, loss: 0.092, 49632/60000 datapoints
2025-03-06 20:09:52,168 - INFO - training batch 1601, loss: 0.135, 51232/60000 datapoints
2025-03-06 20:09:52,386 - INFO - training batch 1651, loss: 0.553, 52832/60000 datapoints
2025-03-06 20:09:52,603 - INFO - training batch 1701, loss: 0.788, 54432/60000 datapoints
2025-03-06 20:09:52,895 - INFO - training batch 1751, loss: 0.081, 56032/60000 datapoints
2025-03-06 20:09:53,124 - INFO - training batch 1801, loss: 0.217, 57632/60000 datapoints
2025-03-06 20:09:53,338 - INFO - training batch 1851, loss: 0.129, 59232/60000 datapoints
2025-03-06 20:09:53,454 - INFO - validation batch 1, loss: 0.218, 32/10016 datapoints
2025-03-06 20:09:53,631 - INFO - validation batch 51, loss: 0.135, 1632/10016 datapoints
2025-03-06 20:09:53,807 - INFO - validation batch 101, loss: 0.460, 3232/10016 datapoints
2025-03-06 20:09:53,984 - INFO - validation batch 151, loss: 0.287, 4832/10016 datapoints
2025-03-06 20:09:54,161 - INFO - validation batch 201, loss: 0.145, 6432/10016 datapoints
2025-03-06 20:09:54,332 - INFO - validation batch 251, loss: 0.187, 8032/10016 datapoints
2025-03-06 20:09:54,503 - INFO - validation batch 301, loss: 0.284, 9632/10016 datapoints
2025-03-06 20:09:54,546 - INFO - Epoch 613/800 done.
2025-03-06 20:09:54,546 - INFO - Final validation performance:
Loss: 0.245, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:09:54,546 - INFO - Beginning epoch 614/800
2025-03-06 20:09:54,554 - INFO - training batch 1, loss: 0.191, 32/60000 datapoints
2025-03-06 20:09:54,784 - INFO - training batch 51, loss: 0.125, 1632/60000 datapoints
2025-03-06 20:09:55,000 - INFO - training batch 101, loss: 0.244, 3232/60000 datapoints
2025-03-06 20:09:55,210 - INFO - training batch 151, loss: 0.405, 4832/60000 datapoints
2025-03-06 20:09:55,423 - INFO - training batch 201, loss: 0.217, 6432/60000 datapoints
2025-03-06 20:09:55,634 - INFO - training batch 251, loss: 0.069, 8032/60000 datapoints
2025-03-06 20:09:55,844 - INFO - training batch 301, loss: 0.486, 9632/60000 datapoints
2025-03-06 20:09:56,046 - INFO - training batch 351, loss: 0.341, 11232/60000 datapoints
2025-03-06 20:09:56,241 - INFO - training batch 401, loss: 0.088, 12832/60000 datapoints
2025-03-06 20:09:56,729 - INFO - training batch 451, loss: 0.690, 14432/60000 datapoints
2025-03-06 20:09:57,145 - INFO - training batch 501, loss: 0.147, 16032/60000 datapoints
2025-03-06 20:09:57,523 - INFO - training batch 551, loss: 0.298, 17632/60000 datapoints
2025-03-06 20:09:57,827 - INFO - training batch 601, loss: 0.478, 19232/60000 datapoints
2025-03-06 20:09:58,153 - INFO - training batch 651, loss: 0.209, 20832/60000 datapoints
2025-03-06 20:09:58,451 - INFO - training batch 701, loss: 0.471, 22432/60000 datapoints
2025-03-06 20:09:58,750 - INFO - training batch 751, loss: 0.163, 24032/60000 datapoints
2025-03-06 20:09:59,054 - INFO - training batch 801, loss: 0.201, 25632/60000 datapoints
2025-03-06 20:09:59,350 - INFO - training batch 851, loss: 0.258, 27232/60000 datapoints
2025-03-06 20:09:59,666 - INFO - training batch 901, loss: 0.213, 28832/60000 datapoints
2025-03-06 20:10:00,072 - INFO - training batch 951, loss: 0.234, 30432/60000 datapoints
2025-03-06 20:10:00,601 - INFO - training batch 1001, loss: 0.125, 32032/60000 datapoints
2025-03-06 20:10:00,960 - INFO - training batch 1051, loss: 0.350, 33632/60000 datapoints
2025-03-06 20:10:01,272 - INFO - training batch 1101, loss: 0.195, 35232/60000 datapoints
2025-03-06 20:10:01,554 - INFO - training batch 1151, loss: 0.295, 36832/60000 datapoints
2025-03-06 20:10:01,835 - INFO - training batch 1201, loss: 0.361, 38432/60000 datapoints
2025-03-06 20:10:02,138 - INFO - training batch 1251, loss: 0.060, 40032/60000 datapoints
2025-03-06 20:10:02,415 - INFO - training batch 1301, loss: 0.091, 41632/60000 datapoints
2025-03-06 20:10:02,702 - INFO - training batch 1351, loss: 0.311, 43232/60000 datapoints
2025-03-06 20:10:03,006 - INFO - training batch 1401, loss: 0.245, 44832/60000 datapoints
2025-03-06 20:10:03,320 - INFO - training batch 1451, loss: 0.199, 46432/60000 datapoints
2025-03-06 20:10:03,607 - INFO - training batch 1501, loss: 0.388, 48032/60000 datapoints
2025-03-06 20:10:03,904 - INFO - training batch 1551, loss: 0.317, 49632/60000 datapoints
2025-03-06 20:10:04,197 - INFO - training batch 1601, loss: 0.060, 51232/60000 datapoints
2025-03-06 20:10:04,485 - INFO - training batch 1651, loss: 0.299, 52832/60000 datapoints
2025-03-06 20:10:04,801 - INFO - training batch 1701, loss: 0.246, 54432/60000 datapoints
2025-03-06 20:10:05,101 - INFO - training batch 1751, loss: 0.413, 56032/60000 datapoints
2025-03-06 20:10:05,395 - INFO - training batch 1801, loss: 0.247, 57632/60000 datapoints
2025-03-06 20:10:05,695 - INFO - training batch 1851, loss: 0.168, 59232/60000 datapoints
2025-03-06 20:10:05,854 - INFO - validation batch 1, loss: 0.077, 32/10016 datapoints
2025-03-06 20:10:06,092 - INFO - validation batch 51, loss: 0.076, 1632/10016 datapoints
2025-03-06 20:10:06,327 - INFO - validation batch 101, loss: 0.269, 3232/10016 datapoints
2025-03-06 20:10:06,560 - INFO - validation batch 151, loss: 0.068, 4832/10016 datapoints
2025-03-06 20:10:06,808 - INFO - validation batch 201, loss: 0.396, 6432/10016 datapoints
2025-03-06 20:10:07,046 - INFO - validation batch 251, loss: 0.168, 8032/10016 datapoints
2025-03-06 20:10:07,290 - INFO - validation batch 301, loss: 0.182, 9632/10016 datapoints
2025-03-06 20:10:07,346 - INFO - Epoch 614/800 done.
2025-03-06 20:10:07,346 - INFO - Final validation performance:
Loss: 0.176, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:10:07,347 - INFO - Beginning epoch 615/800
2025-03-06 20:10:07,356 - INFO - training batch 1, loss: 0.228, 32/60000 datapoints
2025-03-06 20:10:07,663 - INFO - training batch 51, loss: 0.155, 1632/60000 datapoints
2025-03-06 20:10:07,964 - INFO - training batch 101, loss: 0.245, 3232/60000 datapoints
2025-03-06 20:10:08,284 - INFO - training batch 151, loss: 0.242, 4832/60000 datapoints
2025-03-06 20:10:08,574 - INFO - training batch 201, loss: 0.153, 6432/60000 datapoints
2025-03-06 20:10:08,865 - INFO - training batch 251, loss: 0.359, 8032/60000 datapoints
2025-03-06 20:10:09,150 - INFO - training batch 301, loss: 0.430, 9632/60000 datapoints
2025-03-06 20:10:09,433 - INFO - training batch 351, loss: 0.166, 11232/60000 datapoints
2025-03-06 20:10:09,726 - INFO - training batch 401, loss: 0.128, 12832/60000 datapoints
2025-03-06 20:10:10,006 - INFO - training batch 451, loss: 0.239, 14432/60000 datapoints
2025-03-06 20:10:10,290 - INFO - training batch 501, loss: 0.098, 16032/60000 datapoints
2025-03-06 20:10:10,576 - INFO - training batch 551, loss: 0.179, 17632/60000 datapoints
2025-03-06 20:10:10,861 - INFO - training batch 601, loss: 0.147, 19232/60000 datapoints
2025-03-06 20:10:11,141 - INFO - training batch 651, loss: 0.300, 20832/60000 datapoints
2025-03-06 20:10:11,417 - INFO - training batch 701, loss: 0.160, 22432/60000 datapoints
2025-03-06 20:10:11,699 - INFO - training batch 751, loss: 0.201, 24032/60000 datapoints
2025-03-06 20:10:11,976 - INFO - training batch 801, loss: 0.471, 25632/60000 datapoints
2025-03-06 20:10:12,250 - INFO - training batch 851, loss: 0.323, 27232/60000 datapoints
2025-03-06 20:10:12,533 - INFO - training batch 901, loss: 0.367, 28832/60000 datapoints
2025-03-06 20:10:12,810 - INFO - training batch 951, loss: 0.160, 30432/60000 datapoints
2025-03-06 20:10:13,088 - INFO - training batch 1001, loss: 0.322, 32032/60000 datapoints
2025-03-06 20:10:13,364 - INFO - training batch 1051, loss: 0.416, 33632/60000 datapoints
2025-03-06 20:10:13,649 - INFO - training batch 1101, loss: 0.187, 35232/60000 datapoints
2025-03-06 20:10:13,927 - INFO - training batch 1151, loss: 0.536, 36832/60000 datapoints
2025-03-06 20:10:14,212 - INFO - training batch 1201, loss: 0.277, 38432/60000 datapoints
2025-03-06 20:10:14,491 - INFO - training batch 1251, loss: 0.279, 40032/60000 datapoints
2025-03-06 20:10:14,766 - INFO - training batch 1301, loss: 0.285, 41632/60000 datapoints
2025-03-06 20:10:15,049 - INFO - training batch 1351, loss: 0.348, 43232/60000 datapoints
2025-03-06 20:10:15,322 - INFO - training batch 1401, loss: 0.203, 44832/60000 datapoints
2025-03-06 20:10:15,601 - INFO - training batch 1451, loss: 0.270, 46432/60000 datapoints
2025-03-06 20:10:15,883 - INFO - training batch 1501, loss: 0.270, 48032/60000 datapoints
2025-03-06 20:10:16,170 - INFO - training batch 1551, loss: 0.264, 49632/60000 datapoints
2025-03-06 20:10:16,446 - INFO - training batch 1601, loss: 0.071, 51232/60000 datapoints
2025-03-06 20:10:16,727 - INFO - training batch 1651, loss: 0.330, 52832/60000 datapoints
2025-03-06 20:10:17,005 - INFO - training batch 1701, loss: 0.305, 54432/60000 datapoints
2025-03-06 20:10:17,286 - INFO - training batch 1751, loss: 0.166, 56032/60000 datapoints
2025-03-06 20:10:17,567 - INFO - training batch 1801, loss: 0.115, 57632/60000 datapoints
2025-03-06 20:10:17,851 - INFO - training batch 1851, loss: 0.046, 59232/60000 datapoints
2025-03-06 20:10:17,997 - INFO - validation batch 1, loss: 0.182, 32/10016 datapoints
2025-03-06 20:10:18,247 - INFO - validation batch 51, loss: 0.425, 1632/10016 datapoints
2025-03-06 20:10:18,471 - INFO - validation batch 101, loss: 0.169, 3232/10016 datapoints
2025-03-06 20:10:18,701 - INFO - validation batch 151, loss: 0.074, 4832/10016 datapoints
2025-03-06 20:10:18,924 - INFO - validation batch 201, loss: 0.143, 6432/10016 datapoints
2025-03-06 20:10:19,152 - INFO - validation batch 251, loss: 0.213, 8032/10016 datapoints
2025-03-06 20:10:19,390 - INFO - validation batch 301, loss: 0.147, 9632/10016 datapoints
2025-03-06 20:10:19,443 - INFO - Epoch 615/800 done.
2025-03-06 20:10:19,444 - INFO - Final validation performance:
Loss: 0.193, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:10:19,444 - INFO - Beginning epoch 616/800
2025-03-06 20:10:19,454 - INFO - training batch 1, loss: 0.703, 32/60000 datapoints
2025-03-06 20:10:19,733 - INFO - training batch 51, loss: 0.180, 1632/60000 datapoints
2025-03-06 20:10:20,032 - INFO - training batch 101, loss: 0.357, 3232/60000 datapoints
2025-03-06 20:10:20,350 - INFO - training batch 151, loss: 0.175, 4832/60000 datapoints
2025-03-06 20:10:20,655 - INFO - training batch 201, loss: 0.144, 6432/60000 datapoints
2025-03-06 20:10:20,940 - INFO - training batch 251, loss: 0.125, 8032/60000 datapoints
2025-03-06 20:10:21,214 - INFO - training batch 301, loss: 0.184, 9632/60000 datapoints
2025-03-06 20:10:21,489 - INFO - training batch 351, loss: 0.313, 11232/60000 datapoints
2025-03-06 20:10:21,764 - INFO - training batch 401, loss: 0.256, 12832/60000 datapoints
2025-03-06 20:10:22,043 - INFO - training batch 451, loss: 0.163, 14432/60000 datapoints
2025-03-06 20:10:22,320 - INFO - training batch 501, loss: 0.342, 16032/60000 datapoints
2025-03-06 20:10:22,606 - INFO - training batch 551, loss: 0.127, 17632/60000 datapoints
2025-03-06 20:10:22,888 - INFO - training batch 601, loss: 0.224, 19232/60000 datapoints
2025-03-06 20:10:23,161 - INFO - training batch 651, loss: 0.097, 20832/60000 datapoints
2025-03-06 20:10:23,439 - INFO - training batch 701, loss: 0.175, 22432/60000 datapoints
2025-03-06 20:10:23,721 - INFO - training batch 751, loss: 0.128, 24032/60000 datapoints
2025-03-06 20:10:24,003 - INFO - training batch 801, loss: 0.140, 25632/60000 datapoints
2025-03-06 20:10:24,283 - INFO - training batch 851, loss: 0.224, 27232/60000 datapoints
2025-03-06 20:10:24,561 - INFO - training batch 901, loss: 0.121, 28832/60000 datapoints
2025-03-06 20:10:24,843 - INFO - training batch 951, loss: 0.218, 30432/60000 datapoints
2025-03-06 20:10:25,122 - INFO - training batch 1001, loss: 0.216, 32032/60000 datapoints
2025-03-06 20:10:25,395 - INFO - training batch 1051, loss: 0.394, 33632/60000 datapoints
2025-03-06 20:10:25,686 - INFO - training batch 1101, loss: 0.131, 35232/60000 datapoints
2025-03-06 20:10:26,010 - INFO - training batch 1151, loss: 0.108, 36832/60000 datapoints
2025-03-06 20:10:26,324 - INFO - training batch 1201, loss: 0.205, 38432/60000 datapoints
2025-03-06 20:10:26,599 - INFO - training batch 1251, loss: 0.245, 40032/60000 datapoints
2025-03-06 20:10:26,881 - INFO - training batch 1301, loss: 0.307, 41632/60000 datapoints
2025-03-06 20:10:27,157 - INFO - training batch 1351, loss: 0.233, 43232/60000 datapoints
2025-03-06 20:10:27,434 - INFO - training batch 1401, loss: 0.218, 44832/60000 datapoints
2025-03-06 20:10:27,732 - INFO - training batch 1451, loss: 0.778, 46432/60000 datapoints
2025-03-06 20:10:28,010 - INFO - training batch 1501, loss: 0.189, 48032/60000 datapoints
2025-03-06 20:10:28,321 - INFO - training batch 1551, loss: 0.136, 49632/60000 datapoints
2025-03-06 20:10:28,609 - INFO - training batch 1601, loss: 0.220, 51232/60000 datapoints
2025-03-06 20:10:28,890 - INFO - training batch 1651, loss: 0.146, 52832/60000 datapoints
2025-03-06 20:10:29,174 - INFO - training batch 1701, loss: 0.168, 54432/60000 datapoints
2025-03-06 20:10:29,455 - INFO - training batch 1751, loss: 0.296, 56032/60000 datapoints
2025-03-06 20:10:29,893 - INFO - training batch 1801, loss: 0.409, 57632/60000 datapoints
2025-03-06 20:10:30,182 - INFO - training batch 1851, loss: 0.135, 59232/60000 datapoints
2025-03-06 20:10:30,327 - INFO - validation batch 1, loss: 0.135, 32/10016 datapoints
2025-03-06 20:10:30,568 - INFO - validation batch 51, loss: 0.222, 1632/10016 datapoints
2025-03-06 20:10:30,869 - INFO - validation batch 101, loss: 0.294, 3232/10016 datapoints
2025-03-06 20:10:31,105 - INFO - validation batch 151, loss: 0.146, 4832/10016 datapoints
2025-03-06 20:10:31,328 - INFO - validation batch 201, loss: 0.286, 6432/10016 datapoints
2025-03-06 20:10:31,553 - INFO - validation batch 251, loss: 0.301, 8032/10016 datapoints
2025-03-06 20:10:31,789 - INFO - validation batch 301, loss: 0.427, 9632/10016 datapoints
2025-03-06 20:10:31,846 - INFO - Epoch 616/800 done.
2025-03-06 20:10:31,847 - INFO - Final validation performance:
Loss: 0.259, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:10:31,848 - INFO - Beginning epoch 617/800
2025-03-06 20:10:31,858 - INFO - training batch 1, loss: 0.405, 32/60000 datapoints
2025-03-06 20:10:32,150 - INFO - training batch 51, loss: 0.127, 1632/60000 datapoints
2025-03-06 20:10:32,453 - INFO - training batch 101, loss: 0.438, 3232/60000 datapoints
2025-03-06 20:10:32,748 - INFO - training batch 151, loss: 0.084, 4832/60000 datapoints
2025-03-06 20:10:33,182 - INFO - training batch 201, loss: 0.196, 6432/60000 datapoints
2025-03-06 20:10:33,510 - INFO - training batch 251, loss: 0.208, 8032/60000 datapoints
2025-03-06 20:10:33,841 - INFO - training batch 301, loss: 0.098, 9632/60000 datapoints
2025-03-06 20:10:34,241 - INFO - training batch 351, loss: 0.264, 11232/60000 datapoints
2025-03-06 20:10:34,717 - INFO - training batch 401, loss: 0.223, 12832/60000 datapoints
2025-03-06 20:10:35,276 - INFO - training batch 451, loss: 0.216, 14432/60000 datapoints
2025-03-06 20:10:35,728 - INFO - training batch 501, loss: 0.202, 16032/60000 datapoints
2025-03-06 20:10:36,117 - INFO - training batch 551, loss: 0.296, 17632/60000 datapoints
2025-03-06 20:10:36,440 - INFO - training batch 601, loss: 0.175, 19232/60000 datapoints
2025-03-06 20:10:36,760 - INFO - training batch 651, loss: 0.238, 20832/60000 datapoints
2025-03-06 20:10:37,052 - INFO - training batch 701, loss: 0.122, 22432/60000 datapoints
2025-03-06 20:10:37,338 - INFO - training batch 751, loss: 0.206, 24032/60000 datapoints
2025-03-06 20:10:37,620 - INFO - training batch 801, loss: 0.507, 25632/60000 datapoints
2025-03-06 20:10:37,978 - INFO - training batch 851, loss: 0.064, 27232/60000 datapoints
2025-03-06 20:10:38,260 - INFO - training batch 901, loss: 0.129, 28832/60000 datapoints
2025-03-06 20:10:38,624 - INFO - training batch 951, loss: 0.134, 30432/60000 datapoints
2025-03-06 20:10:38,949 - INFO - training batch 1001, loss: 0.301, 32032/60000 datapoints
2025-03-06 20:10:39,283 - INFO - training batch 1051, loss: 0.197, 33632/60000 datapoints
2025-03-06 20:10:39,568 - INFO - training batch 1101, loss: 0.111, 35232/60000 datapoints
2025-03-06 20:10:39,852 - INFO - training batch 1151, loss: 0.338, 36832/60000 datapoints
2025-03-06 20:10:40,221 - INFO - training batch 1201, loss: 0.194, 38432/60000 datapoints
2025-03-06 20:10:40,532 - INFO - training batch 1251, loss: 0.125, 40032/60000 datapoints
2025-03-06 20:10:40,825 - INFO - training batch 1301, loss: 0.181, 41632/60000 datapoints
2025-03-06 20:10:41,192 - INFO - training batch 1351, loss: 0.273, 43232/60000 datapoints
2025-03-06 20:10:41,475 - INFO - training batch 1401, loss: 0.506, 44832/60000 datapoints
2025-03-06 20:10:41,835 - INFO - training batch 1451, loss: 0.124, 46432/60000 datapoints
2025-03-06 20:10:42,121 - INFO - training batch 1501, loss: 0.156, 48032/60000 datapoints
2025-03-06 20:10:42,403 - INFO - training batch 1551, loss: 0.256, 49632/60000 datapoints
2025-03-06 20:10:42,718 - INFO - training batch 1601, loss: 0.218, 51232/60000 datapoints
2025-03-06 20:10:43,002 - INFO - training batch 1651, loss: 0.116, 52832/60000 datapoints
2025-03-06 20:10:43,299 - INFO - training batch 1701, loss: 0.078, 54432/60000 datapoints
2025-03-06 20:10:43,588 - INFO - training batch 1751, loss: 0.073, 56032/60000 datapoints
2025-03-06 20:10:43,886 - INFO - training batch 1801, loss: 0.167, 57632/60000 datapoints
2025-03-06 20:10:44,170 - INFO - training batch 1851, loss: 0.193, 59232/60000 datapoints
2025-03-06 20:10:44,322 - INFO - validation batch 1, loss: 0.181, 32/10016 datapoints
2025-03-06 20:10:44,552 - INFO - validation batch 51, loss: 0.046, 1632/10016 datapoints
2025-03-06 20:10:44,832 - INFO - validation batch 101, loss: 0.063, 3232/10016 datapoints
2025-03-06 20:10:45,109 - INFO - validation batch 151, loss: 0.343, 4832/10016 datapoints
2025-03-06 20:10:45,356 - INFO - validation batch 201, loss: 0.434, 6432/10016 datapoints
2025-03-06 20:10:45,657 - INFO - validation batch 251, loss: 0.434, 8032/10016 datapoints
2025-03-06 20:10:45,888 - INFO - validation batch 301, loss: 0.123, 9632/10016 datapoints
2025-03-06 20:10:45,948 - INFO - Epoch 617/800 done.
2025-03-06 20:10:45,948 - INFO - Final validation performance:
Loss: 0.232, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:10:45,949 - INFO - Beginning epoch 618/800
2025-03-06 20:10:45,961 - INFO - training batch 1, loss: 0.155, 32/60000 datapoints
2025-03-06 20:10:46,247 - INFO - training batch 51, loss: 0.245, 1632/60000 datapoints
2025-03-06 20:10:46,545 - INFO - training batch 101, loss: 0.371, 3232/60000 datapoints
2025-03-06 20:10:46,848 - INFO - training batch 151, loss: 0.216, 4832/60000 datapoints
2025-03-06 20:10:47,137 - INFO - training batch 201, loss: 0.218, 6432/60000 datapoints
2025-03-06 20:10:47,435 - INFO - training batch 251, loss: 0.253, 8032/60000 datapoints
2025-03-06 20:10:47,730 - INFO - training batch 301, loss: 0.389, 9632/60000 datapoints
2025-03-06 20:10:48,011 - INFO - training batch 351, loss: 0.326, 11232/60000 datapoints
2025-03-06 20:10:48,295 - INFO - training batch 401, loss: 0.294, 12832/60000 datapoints
2025-03-06 20:10:48,630 - INFO - training batch 451, loss: 0.256, 14432/60000 datapoints
2025-03-06 20:10:48,947 - INFO - training batch 501, loss: 0.424, 16032/60000 datapoints
2025-03-06 20:10:49,236 - INFO - training batch 551, loss: 0.099, 17632/60000 datapoints
2025-03-06 20:10:49,527 - INFO - training batch 601, loss: 0.141, 19232/60000 datapoints
2025-03-06 20:10:49,825 - INFO - training batch 651, loss: 0.358, 20832/60000 datapoints
2025-03-06 20:10:50,120 - INFO - training batch 701, loss: 0.100, 22432/60000 datapoints
2025-03-06 20:10:50,407 - INFO - training batch 751, loss: 0.054, 24032/60000 datapoints
2025-03-06 20:10:50,692 - INFO - training batch 801, loss: 0.473, 25632/60000 datapoints
2025-03-06 20:10:50,978 - INFO - training batch 851, loss: 0.084, 27232/60000 datapoints
2025-03-06 20:10:51,263 - INFO - training batch 901, loss: 0.105, 28832/60000 datapoints
2025-03-06 20:10:51,549 - INFO - training batch 951, loss: 0.394, 30432/60000 datapoints
2025-03-06 20:10:51,839 - INFO - training batch 1001, loss: 0.163, 32032/60000 datapoints
2025-03-06 20:10:52,125 - INFO - training batch 1051, loss: 0.277, 33632/60000 datapoints
2025-03-06 20:10:52,412 - INFO - training batch 1101, loss: 0.078, 35232/60000 datapoints
2025-03-06 20:10:52,713 - INFO - training batch 1151, loss: 0.181, 36832/60000 datapoints
2025-03-06 20:10:53,000 - INFO - training batch 1201, loss: 0.334, 38432/60000 datapoints
2025-03-06 20:10:53,283 - INFO - training batch 1251, loss: 0.259, 40032/60000 datapoints
2025-03-06 20:10:53,568 - INFO - training batch 1301, loss: 0.229, 41632/60000 datapoints
2025-03-06 20:10:53,853 - INFO - training batch 1351, loss: 0.213, 43232/60000 datapoints
2025-03-06 20:10:54,142 - INFO - training batch 1401, loss: 0.319, 44832/60000 datapoints
2025-03-06 20:10:54,431 - INFO - training batch 1451, loss: 0.113, 46432/60000 datapoints
2025-03-06 20:10:54,746 - INFO - training batch 1501, loss: 0.232, 48032/60000 datapoints
2025-03-06 20:10:55,048 - INFO - training batch 1551, loss: 0.252, 49632/60000 datapoints
2025-03-06 20:10:55,338 - INFO - training batch 1601, loss: 0.204, 51232/60000 datapoints
2025-03-06 20:10:55,630 - INFO - training batch 1651, loss: 0.638, 52832/60000 datapoints
2025-03-06 20:10:55,911 - INFO - training batch 1701, loss: 0.141, 54432/60000 datapoints
2025-03-06 20:10:56,200 - INFO - training batch 1751, loss: 0.463, 56032/60000 datapoints
2025-03-06 20:10:56,485 - INFO - training batch 1801, loss: 0.218, 57632/60000 datapoints
2025-03-06 20:10:56,769 - INFO - training batch 1851, loss: 0.288, 59232/60000 datapoints
2025-03-06 20:10:56,918 - INFO - validation batch 1, loss: 0.162, 32/10016 datapoints
2025-03-06 20:10:57,146 - INFO - validation batch 51, loss: 0.386, 1632/10016 datapoints
2025-03-06 20:10:57,374 - INFO - validation batch 101, loss: 0.130, 3232/10016 datapoints
2025-03-06 20:10:57,604 - INFO - validation batch 151, loss: 0.164, 4832/10016 datapoints
2025-03-06 20:10:57,834 - INFO - validation batch 201, loss: 0.246, 6432/10016 datapoints
2025-03-06 20:10:58,063 - INFO - validation batch 251, loss: 0.276, 8032/10016 datapoints
2025-03-06 20:10:58,296 - INFO - validation batch 301, loss: 0.082, 9632/10016 datapoints
2025-03-06 20:10:58,360 - INFO - Epoch 618/800 done.
2025-03-06 20:10:58,361 - INFO - Final validation performance:
Loss: 0.207, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:10:58,362 - INFO - Beginning epoch 619/800
2025-03-06 20:10:58,373 - INFO - training batch 1, loss: 0.081, 32/60000 datapoints
2025-03-06 20:10:58,682 - INFO - training batch 51, loss: 0.123, 1632/60000 datapoints
2025-03-06 20:10:58,993 - INFO - training batch 101, loss: 0.233, 3232/60000 datapoints
2025-03-06 20:10:59,331 - INFO - training batch 151, loss: 0.271, 4832/60000 datapoints
2025-03-06 20:10:59,629 - INFO - training batch 201, loss: 0.168, 6432/60000 datapoints
2025-03-06 20:10:59,922 - INFO - training batch 251, loss: 0.148, 8032/60000 datapoints
2025-03-06 20:11:00,220 - INFO - training batch 301, loss: 0.260, 9632/60000 datapoints
2025-03-06 20:11:00,508 - INFO - training batch 351, loss: 0.183, 11232/60000 datapoints
2025-03-06 20:11:00,802 - INFO - training batch 401, loss: 0.592, 12832/60000 datapoints
2025-03-06 20:11:01,107 - INFO - training batch 451, loss: 0.361, 14432/60000 datapoints
2025-03-06 20:11:01,419 - INFO - training batch 501, loss: 0.096, 16032/60000 datapoints
2025-03-06 20:11:01,707 - INFO - training batch 551, loss: 0.098, 17632/60000 datapoints
2025-03-06 20:11:02,001 - INFO - training batch 601, loss: 0.409, 19232/60000 datapoints
2025-03-06 20:11:02,283 - INFO - training batch 651, loss: 0.127, 20832/60000 datapoints
2025-03-06 20:11:02,575 - INFO - training batch 701, loss: 0.135, 22432/60000 datapoints
2025-03-06 20:11:02,960 - INFO - training batch 751, loss: 0.106, 24032/60000 datapoints
2025-03-06 20:11:03,239 - INFO - training batch 801, loss: 0.211, 25632/60000 datapoints
2025-03-06 20:11:03,516 - INFO - training batch 851, loss: 0.446, 27232/60000 datapoints
2025-03-06 20:11:03,794 - INFO - training batch 901, loss: 0.151, 28832/60000 datapoints
2025-03-06 20:11:04,078 - INFO - training batch 951, loss: 0.351, 30432/60000 datapoints
2025-03-06 20:11:04,351 - INFO - training batch 1001, loss: 0.145, 32032/60000 datapoints
2025-03-06 20:11:04,638 - INFO - training batch 1051, loss: 0.284, 33632/60000 datapoints
2025-03-06 20:11:04,935 - INFO - training batch 1101, loss: 0.120, 35232/60000 datapoints
2025-03-06 20:11:05,217 - INFO - training batch 1151, loss: 0.193, 36832/60000 datapoints
2025-03-06 20:11:05,495 - INFO - training batch 1201, loss: 0.197, 38432/60000 datapoints
2025-03-06 20:11:05,771 - INFO - training batch 1251, loss: 0.063, 40032/60000 datapoints
2025-03-06 20:11:06,055 - INFO - training batch 1301, loss: 0.210, 41632/60000 datapoints
2025-03-06 20:11:06,335 - INFO - training batch 1351, loss: 0.287, 43232/60000 datapoints
2025-03-06 20:11:06,619 - INFO - training batch 1401, loss: 0.245, 44832/60000 datapoints
2025-03-06 20:11:06,908 - INFO - training batch 1451, loss: 0.154, 46432/60000 datapoints
2025-03-06 20:11:07,191 - INFO - training batch 1501, loss: 0.530, 48032/60000 datapoints
2025-03-06 20:11:07,474 - INFO - training batch 1551, loss: 0.574, 49632/60000 datapoints
2025-03-06 20:11:07,763 - INFO - training batch 1601, loss: 0.241, 51232/60000 datapoints
2025-03-06 20:11:08,051 - INFO - training batch 1651, loss: 0.259, 52832/60000 datapoints
2025-03-06 20:11:08,342 - INFO - training batch 1701, loss: 0.066, 54432/60000 datapoints
2025-03-06 20:11:08,628 - INFO - training batch 1751, loss: 0.210, 56032/60000 datapoints
2025-03-06 20:11:08,942 - INFO - training batch 1801, loss: 0.225, 57632/60000 datapoints
2025-03-06 20:11:09,219 - INFO - training batch 1851, loss: 0.281, 59232/60000 datapoints
2025-03-06 20:11:09,366 - INFO - validation batch 1, loss: 0.195, 32/10016 datapoints
2025-03-06 20:11:09,629 - INFO - validation batch 51, loss: 0.190, 1632/10016 datapoints
2025-03-06 20:11:09,885 - INFO - validation batch 101, loss: 0.332, 3232/10016 datapoints
2025-03-06 20:11:10,119 - INFO - validation batch 151, loss: 0.255, 4832/10016 datapoints
2025-03-06 20:11:10,347 - INFO - validation batch 201, loss: 0.150, 6432/10016 datapoints
2025-03-06 20:11:10,583 - INFO - validation batch 251, loss: 0.056, 8032/10016 datapoints
2025-03-06 20:11:10,813 - INFO - validation batch 301, loss: 0.256, 9632/10016 datapoints
2025-03-06 20:11:10,873 - INFO - Epoch 619/800 done.
2025-03-06 20:11:10,873 - INFO - Final validation performance:
Loss: 0.205, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:11:10,874 - INFO - Beginning epoch 620/800
2025-03-06 20:11:10,883 - INFO - training batch 1, loss: 0.231, 32/60000 datapoints
2025-03-06 20:11:11,159 - INFO - training batch 51, loss: 0.124, 1632/60000 datapoints
2025-03-06 20:11:11,446 - INFO - training batch 101, loss: 0.405, 3232/60000 datapoints
2025-03-06 20:11:11,729 - INFO - training batch 151, loss: 0.122, 4832/60000 datapoints
2025-03-06 20:11:12,012 - INFO - training batch 201, loss: 0.293, 6432/60000 datapoints
2025-03-06 20:11:12,299 - INFO - training batch 251, loss: 0.242, 8032/60000 datapoints
2025-03-06 20:11:12,591 - INFO - training batch 301, loss: 0.125, 9632/60000 datapoints
2025-03-06 20:11:12,873 - INFO - training batch 351, loss: 0.147, 11232/60000 datapoints
2025-03-06 20:11:13,146 - INFO - training batch 401, loss: 0.091, 12832/60000 datapoints
2025-03-06 20:11:13,429 - INFO - training batch 451, loss: 0.231, 14432/60000 datapoints
2025-03-06 20:11:13,785 - INFO - training batch 501, loss: 0.247, 16032/60000 datapoints
2025-03-06 20:11:14,072 - INFO - training batch 551, loss: 0.183, 17632/60000 datapoints
2025-03-06 20:11:14,342 - INFO - training batch 601, loss: 0.179, 19232/60000 datapoints
2025-03-06 20:11:14,628 - INFO - training batch 651, loss: 0.247, 20832/60000 datapoints
2025-03-06 20:11:14,921 - INFO - training batch 701, loss: 0.310, 22432/60000 datapoints
2025-03-06 20:11:15,208 - INFO - training batch 751, loss: 0.174, 24032/60000 datapoints
2025-03-06 20:11:15,489 - INFO - training batch 801, loss: 0.128, 25632/60000 datapoints
2025-03-06 20:11:15,775 - INFO - training batch 851, loss: 0.163, 27232/60000 datapoints
2025-03-06 20:11:16,062 - INFO - training batch 901, loss: 0.180, 28832/60000 datapoints
2025-03-06 20:11:16,340 - INFO - training batch 951, loss: 0.190, 30432/60000 datapoints
2025-03-06 20:11:16,614 - INFO - training batch 1001, loss: 0.433, 32032/60000 datapoints
2025-03-06 20:11:16,885 - INFO - training batch 1051, loss: 0.125, 33632/60000 datapoints
2025-03-06 20:11:17,167 - INFO - training batch 1101, loss: 0.179, 35232/60000 datapoints
2025-03-06 20:11:17,439 - INFO - training batch 1151, loss: 0.205, 36832/60000 datapoints
2025-03-06 20:11:17,780 - INFO - training batch 1201, loss: 0.165, 38432/60000 datapoints
2025-03-06 20:11:18,214 - INFO - training batch 1251, loss: 0.305, 40032/60000 datapoints
2025-03-06 20:11:18,533 - INFO - training batch 1301, loss: 0.148, 41632/60000 datapoints
2025-03-06 20:11:18,803 - INFO - training batch 1351, loss: 0.134, 43232/60000 datapoints
2025-03-06 20:11:19,106 - INFO - training batch 1401, loss: 0.239, 44832/60000 datapoints
2025-03-06 20:11:19,381 - INFO - training batch 1451, loss: 0.281, 46432/60000 datapoints
2025-03-06 20:11:19,674 - INFO - training batch 1501, loss: 0.206, 48032/60000 datapoints
2025-03-06 20:11:19,958 - INFO - training batch 1551, loss: 0.359, 49632/60000 datapoints
2025-03-06 20:11:20,243 - INFO - training batch 1601, loss: 0.065, 51232/60000 datapoints
2025-03-06 20:11:20,552 - INFO - training batch 1651, loss: 0.392, 52832/60000 datapoints
2025-03-06 20:11:21,117 - INFO - training batch 1701, loss: 0.309, 54432/60000 datapoints
2025-03-06 20:11:21,472 - INFO - training batch 1751, loss: 0.162, 56032/60000 datapoints
2025-03-06 20:11:21,790 - INFO - training batch 1801, loss: 0.147, 57632/60000 datapoints
2025-03-06 20:11:22,143 - INFO - training batch 1851, loss: 0.156, 59232/60000 datapoints
2025-03-06 20:11:22,336 - INFO - validation batch 1, loss: 0.321, 32/10016 datapoints
2025-03-06 20:11:22,620 - INFO - validation batch 51, loss: 0.245, 1632/10016 datapoints
2025-03-06 20:11:22,891 - INFO - validation batch 101, loss: 0.111, 3232/10016 datapoints
2025-03-06 20:11:23,201 - INFO - validation batch 151, loss: 0.348, 4832/10016 datapoints
2025-03-06 20:11:23,549 - INFO - validation batch 201, loss: 0.382, 6432/10016 datapoints
2025-03-06 20:11:23,848 - INFO - validation batch 251, loss: 0.198, 8032/10016 datapoints
2025-03-06 20:11:24,135 - INFO - validation batch 301, loss: 0.086, 9632/10016 datapoints
2025-03-06 20:11:24,214 - INFO - Epoch 620/800 done.
2025-03-06 20:11:24,214 - INFO - Final validation performance:
Loss: 0.242, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:11:24,215 - INFO - Beginning epoch 621/800
2025-03-06 20:11:24,231 - INFO - training batch 1, loss: 0.199, 32/60000 datapoints
2025-03-06 20:11:24,674 - INFO - training batch 51, loss: 0.207, 1632/60000 datapoints
2025-03-06 20:11:24,985 - INFO - training batch 101, loss: 0.321, 3232/60000 datapoints
2025-03-06 20:11:25,385 - INFO - training batch 151, loss: 0.140, 4832/60000 datapoints
2025-03-06 20:11:25,721 - INFO - training batch 201, loss: 0.698, 6432/60000 datapoints
2025-03-06 20:11:26,223 - INFO - training batch 251, loss: 0.136, 8032/60000 datapoints
2025-03-06 20:11:26,602 - INFO - training batch 301, loss: 0.371, 9632/60000 datapoints
2025-03-06 20:11:27,032 - INFO - training batch 351, loss: 0.134, 11232/60000 datapoints
2025-03-06 20:11:27,472 - INFO - training batch 401, loss: 0.232, 12832/60000 datapoints
2025-03-06 20:11:27,963 - INFO - training batch 451, loss: 0.286, 14432/60000 datapoints
2025-03-06 20:11:28,392 - INFO - training batch 501, loss: 0.206, 16032/60000 datapoints
2025-03-06 20:11:29,125 - INFO - training batch 551, loss: 0.387, 17632/60000 datapoints
2025-03-06 20:11:29,817 - INFO - training batch 601, loss: 0.128, 19232/60000 datapoints
2025-03-06 20:11:30,431 - INFO - training batch 651, loss: 0.520, 20832/60000 datapoints
2025-03-06 20:11:30,973 - INFO - training batch 701, loss: 0.088, 22432/60000 datapoints
2025-03-06 20:11:31,304 - INFO - training batch 751, loss: 0.374, 24032/60000 datapoints
2025-03-06 20:11:31,645 - INFO - training batch 801, loss: 0.157, 25632/60000 datapoints
2025-03-06 20:11:31,982 - INFO - training batch 851, loss: 0.393, 27232/60000 datapoints
2025-03-06 20:11:32,498 - INFO - training batch 901, loss: 0.107, 28832/60000 datapoints
2025-03-06 20:11:33,456 - INFO - training batch 951, loss: 0.191, 30432/60000 datapoints
2025-03-06 20:11:33,917 - INFO - training batch 1001, loss: 0.359, 32032/60000 datapoints
2025-03-06 20:11:34,755 - INFO - training batch 1051, loss: 0.161, 33632/60000 datapoints
2025-03-06 20:11:35,190 - INFO - training batch 1101, loss: 0.274, 35232/60000 datapoints
2025-03-06 20:11:35,684 - INFO - training batch 1151, loss: 0.100, 36832/60000 datapoints
2025-03-06 20:11:35,952 - INFO - training batch 1201, loss: 0.132, 38432/60000 datapoints
2025-03-06 20:11:36,272 - INFO - training batch 1251, loss: 0.199, 40032/60000 datapoints
2025-03-06 20:11:36,598 - INFO - training batch 1301, loss: 0.120, 41632/60000 datapoints
2025-03-06 20:11:36,844 - INFO - training batch 1351, loss: 0.128, 43232/60000 datapoints
2025-03-06 20:11:37,074 - INFO - training batch 1401, loss: 0.110, 44832/60000 datapoints
2025-03-06 20:11:37,283 - INFO - training batch 1451, loss: 0.256, 46432/60000 datapoints
2025-03-06 20:11:37,496 - INFO - training batch 1501, loss: 0.137, 48032/60000 datapoints
2025-03-06 20:11:37,939 - INFO - training batch 1551, loss: 0.220, 49632/60000 datapoints
2025-03-06 20:11:38,213 - INFO - training batch 1601, loss: 0.307, 51232/60000 datapoints
2025-03-06 20:11:38,588 - INFO - training batch 1651, loss: 0.210, 52832/60000 datapoints
2025-03-06 20:11:38,834 - INFO - training batch 1701, loss: 0.322, 54432/60000 datapoints
2025-03-06 20:11:39,041 - INFO - training batch 1751, loss: 0.591, 56032/60000 datapoints
2025-03-06 20:11:39,285 - INFO - training batch 1801, loss: 0.114, 57632/60000 datapoints
2025-03-06 20:11:39,487 - INFO - training batch 1851, loss: 0.406, 59232/60000 datapoints
2025-03-06 20:11:39,602 - INFO - validation batch 1, loss: 0.204, 32/10016 datapoints
2025-03-06 20:11:39,777 - INFO - validation batch 51, loss: 0.330, 1632/10016 datapoints
2025-03-06 20:11:39,956 - INFO - validation batch 101, loss: 0.265, 3232/10016 datapoints
2025-03-06 20:11:40,131 - INFO - validation batch 151, loss: 0.196, 4832/10016 datapoints
2025-03-06 20:11:40,285 - INFO - validation batch 201, loss: 0.465, 6432/10016 datapoints
2025-03-06 20:11:40,452 - INFO - validation batch 251, loss: 0.097, 8032/10016 datapoints
2025-03-06 20:11:40,612 - INFO - validation batch 301, loss: 0.239, 9632/10016 datapoints
2025-03-06 20:11:40,656 - INFO - Epoch 621/800 done.
2025-03-06 20:11:40,657 - INFO - Final validation performance:
Loss: 0.257, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:11:40,659 - INFO - Beginning epoch 622/800
2025-03-06 20:11:40,668 - INFO - training batch 1, loss: 0.246, 32/60000 datapoints
2025-03-06 20:11:40,943 - INFO - training batch 51, loss: 0.054, 1632/60000 datapoints
2025-03-06 20:11:41,168 - INFO - training batch 101, loss: 0.159, 3232/60000 datapoints
2025-03-06 20:11:41,367 - INFO - training batch 151, loss: 0.182, 4832/60000 datapoints
2025-03-06 20:11:41,594 - INFO - training batch 201, loss: 0.256, 6432/60000 datapoints
2025-03-06 20:11:41,800 - INFO - training batch 251, loss: 0.126, 8032/60000 datapoints
2025-03-06 20:11:42,008 - INFO - training batch 301, loss: 0.465, 9632/60000 datapoints
2025-03-06 20:11:42,210 - INFO - training batch 351, loss: 0.300, 11232/60000 datapoints
2025-03-06 20:11:42,413 - INFO - training batch 401, loss: 0.187, 12832/60000 datapoints
2025-03-06 20:11:42,614 - INFO - training batch 451, loss: 0.219, 14432/60000 datapoints
2025-03-06 20:11:42,825 - INFO - training batch 501, loss: 0.163, 16032/60000 datapoints
2025-03-06 20:11:43,033 - INFO - training batch 551, loss: 0.213, 17632/60000 datapoints
2025-03-06 20:11:43,252 - INFO - training batch 601, loss: 0.443, 19232/60000 datapoints
2025-03-06 20:11:43,463 - INFO - training batch 651, loss: 0.296, 20832/60000 datapoints
2025-03-06 20:11:43,670 - INFO - training batch 701, loss: 0.304, 22432/60000 datapoints
2025-03-06 20:11:43,894 - INFO - training batch 751, loss: 0.071, 24032/60000 datapoints
2025-03-06 20:11:44,142 - INFO - training batch 801, loss: 0.119, 25632/60000 datapoints
2025-03-06 20:11:44,376 - INFO - training batch 851, loss: 0.110, 27232/60000 datapoints
2025-03-06 20:11:44,613 - INFO - training batch 901, loss: 0.323, 28832/60000 datapoints
2025-03-06 20:11:44,830 - INFO - training batch 951, loss: 0.171, 30432/60000 datapoints
2025-03-06 20:11:45,100 - INFO - training batch 1001, loss: 0.179, 32032/60000 datapoints
2025-03-06 20:11:45,334 - INFO - training batch 1051, loss: 0.215, 33632/60000 datapoints
2025-03-06 20:11:45,539 - INFO - training batch 1101, loss: 0.158, 35232/60000 datapoints
2025-03-06 20:11:45,750 - INFO - training batch 1151, loss: 0.101, 36832/60000 datapoints
2025-03-06 20:11:46,038 - INFO - training batch 1201, loss: 0.120, 38432/60000 datapoints
2025-03-06 20:11:46,290 - INFO - training batch 1251, loss: 0.172, 40032/60000 datapoints
2025-03-06 20:11:46,497 - INFO - training batch 1301, loss: 0.162, 41632/60000 datapoints
2025-03-06 20:11:46,716 - INFO - training batch 1351, loss: 0.337, 43232/60000 datapoints
2025-03-06 20:11:46,936 - INFO - training batch 1401, loss: 0.297, 44832/60000 datapoints
2025-03-06 20:11:47,193 - INFO - training batch 1451, loss: 0.261, 46432/60000 datapoints
2025-03-06 20:11:47,405 - INFO - training batch 1501, loss: 0.335, 48032/60000 datapoints
2025-03-06 20:11:47,608 - INFO - training batch 1551, loss: 0.209, 49632/60000 datapoints
2025-03-06 20:11:47,824 - INFO - training batch 1601, loss: 0.109, 51232/60000 datapoints
2025-03-06 20:11:48,036 - INFO - training batch 1651, loss: 0.363, 52832/60000 datapoints
2025-03-06 20:11:48,249 - INFO - training batch 1701, loss: 0.338, 54432/60000 datapoints
2025-03-06 20:11:48,451 - INFO - training batch 1751, loss: 0.272, 56032/60000 datapoints
2025-03-06 20:11:48,693 - INFO - training batch 1801, loss: 0.157, 57632/60000 datapoints
2025-03-06 20:11:48,896 - INFO - training batch 1851, loss: 0.333, 59232/60000 datapoints
2025-03-06 20:11:49,002 - INFO - validation batch 1, loss: 0.176, 32/10016 datapoints
2025-03-06 20:11:49,180 - INFO - validation batch 51, loss: 0.358, 1632/10016 datapoints
2025-03-06 20:11:49,364 - INFO - validation batch 101, loss: 0.366, 3232/10016 datapoints
2025-03-06 20:11:49,524 - INFO - validation batch 151, loss: 0.258, 4832/10016 datapoints
2025-03-06 20:11:49,686 - INFO - validation batch 201, loss: 0.090, 6432/10016 datapoints
2025-03-06 20:11:49,851 - INFO - validation batch 251, loss: 0.048, 8032/10016 datapoints
2025-03-06 20:11:50,015 - INFO - validation batch 301, loss: 0.187, 9632/10016 datapoints
2025-03-06 20:11:50,054 - INFO - Epoch 622/800 done.
2025-03-06 20:11:50,054 - INFO - Final validation performance:
Loss: 0.212, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:11:50,055 - INFO - Beginning epoch 623/800
2025-03-06 20:11:50,062 - INFO - training batch 1, loss: 0.278, 32/60000 datapoints
2025-03-06 20:11:50,287 - INFO - training batch 51, loss: 0.308, 1632/60000 datapoints
2025-03-06 20:11:50,487 - INFO - training batch 101, loss: 0.204, 3232/60000 datapoints
2025-03-06 20:11:50,693 - INFO - training batch 151, loss: 0.503, 4832/60000 datapoints
2025-03-06 20:11:50,896 - INFO - training batch 201, loss: 0.165, 6432/60000 datapoints
2025-03-06 20:11:51,103 - INFO - training batch 251, loss: 0.434, 8032/60000 datapoints
2025-03-06 20:11:51,301 - INFO - training batch 301, loss: 0.287, 9632/60000 datapoints
2025-03-06 20:11:51,494 - INFO - training batch 351, loss: 0.370, 11232/60000 datapoints
2025-03-06 20:11:51,693 - INFO - training batch 401, loss: 0.100, 12832/60000 datapoints
2025-03-06 20:11:51,910 - INFO - training batch 451, loss: 0.343, 14432/60000 datapoints
2025-03-06 20:11:52,122 - INFO - training batch 501, loss: 0.545, 16032/60000 datapoints
2025-03-06 20:11:52,339 - INFO - training batch 551, loss: 0.220, 17632/60000 datapoints
2025-03-06 20:11:52,616 - INFO - training batch 601, loss: 0.257, 19232/60000 datapoints
2025-03-06 20:11:52,923 - INFO - training batch 651, loss: 0.402, 20832/60000 datapoints
2025-03-06 20:11:53,234 - INFO - training batch 701, loss: 0.176, 22432/60000 datapoints
2025-03-06 20:11:53,487 - INFO - training batch 751, loss: 0.300, 24032/60000 datapoints
2025-03-06 20:11:53,724 - INFO - training batch 801, loss: 0.651, 25632/60000 datapoints
2025-03-06 20:11:53,959 - INFO - training batch 851, loss: 0.205, 27232/60000 datapoints
2025-03-06 20:11:54,177 - INFO - training batch 901, loss: 0.294, 28832/60000 datapoints
2025-03-06 20:11:54,401 - INFO - training batch 951, loss: 0.097, 30432/60000 datapoints
2025-03-06 20:11:54,618 - INFO - training batch 1001, loss: 0.222, 32032/60000 datapoints
2025-03-06 20:11:54,938 - INFO - training batch 1051, loss: 0.117, 33632/60000 datapoints
2025-03-06 20:11:55,242 - INFO - training batch 1101, loss: 0.208, 35232/60000 datapoints
2025-03-06 20:11:55,502 - INFO - training batch 1151, loss: 0.111, 36832/60000 datapoints
2025-03-06 20:11:55,777 - INFO - training batch 1201, loss: 0.245, 38432/60000 datapoints
2025-03-06 20:11:56,018 - INFO - training batch 1251, loss: 0.198, 40032/60000 datapoints
2025-03-06 20:11:56,272 - INFO - training batch 1301, loss: 0.380, 41632/60000 datapoints
2025-03-06 20:11:56,539 - INFO - training batch 1351, loss: 0.453, 43232/60000 datapoints
2025-03-06 20:11:56,825 - INFO - training batch 1401, loss: 0.105, 44832/60000 datapoints
2025-03-06 20:11:57,055 - INFO - training batch 1451, loss: 0.052, 46432/60000 datapoints
2025-03-06 20:11:57,298 - INFO - training batch 1501, loss: 0.264, 48032/60000 datapoints
2025-03-06 20:11:57,530 - INFO - training batch 1551, loss: 0.187, 49632/60000 datapoints
2025-03-06 20:11:57,759 - INFO - training batch 1601, loss: 0.572, 51232/60000 datapoints
2025-03-06 20:11:57,974 - INFO - training batch 1651, loss: 0.370, 52832/60000 datapoints
2025-03-06 20:11:58,194 - INFO - training batch 1701, loss: 0.143, 54432/60000 datapoints
2025-03-06 20:11:58,430 - INFO - training batch 1751, loss: 0.205, 56032/60000 datapoints
2025-03-06 20:11:58,662 - INFO - training batch 1801, loss: 0.226, 57632/60000 datapoints
2025-03-06 20:11:58,873 - INFO - training batch 1851, loss: 0.276, 59232/60000 datapoints
2025-03-06 20:11:58,988 - INFO - validation batch 1, loss: 0.219, 32/10016 datapoints
2025-03-06 20:11:59,162 - INFO - validation batch 51, loss: 0.245, 1632/10016 datapoints
2025-03-06 20:11:59,360 - INFO - validation batch 101, loss: 0.089, 3232/10016 datapoints
2025-03-06 20:11:59,534 - INFO - validation batch 151, loss: 0.444, 4832/10016 datapoints
2025-03-06 20:11:59,716 - INFO - validation batch 201, loss: 0.368, 6432/10016 datapoints
2025-03-06 20:11:59,886 - INFO - validation batch 251, loss: 0.494, 8032/10016 datapoints
2025-03-06 20:12:00,053 - INFO - validation batch 301, loss: 0.087, 9632/10016 datapoints
2025-03-06 20:12:00,100 - INFO - Epoch 623/800 done.
2025-03-06 20:12:00,101 - INFO - Final validation performance:
Loss: 0.278, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:12:00,101 - INFO - Beginning epoch 624/800
2025-03-06 20:12:00,109 - INFO - training batch 1, loss: 0.135, 32/60000 datapoints
2025-03-06 20:12:00,339 - INFO - training batch 51, loss: 0.159, 1632/60000 datapoints
2025-03-06 20:12:00,554 - INFO - training batch 101, loss: 0.295, 3232/60000 datapoints
2025-03-06 20:12:00,763 - INFO - training batch 151, loss: 0.134, 4832/60000 datapoints
2025-03-06 20:12:00,973 - INFO - training batch 201, loss: 0.180, 6432/60000 datapoints
2025-03-06 20:12:01,183 - INFO - training batch 251, loss: 0.173, 8032/60000 datapoints
2025-03-06 20:12:01,401 - INFO - training batch 301, loss: 0.129, 9632/60000 datapoints
2025-03-06 20:12:01,606 - INFO - training batch 351, loss: 0.276, 11232/60000 datapoints
2025-03-06 20:12:01,813 - INFO - training batch 401, loss: 0.109, 12832/60000 datapoints
2025-03-06 20:12:02,023 - INFO - training batch 451, loss: 0.184, 14432/60000 datapoints
2025-03-06 20:12:02,231 - INFO - training batch 501, loss: 0.142, 16032/60000 datapoints
2025-03-06 20:12:02,431 - INFO - training batch 551, loss: 0.371, 17632/60000 datapoints
2025-03-06 20:12:02,643 - INFO - training batch 601, loss: 0.236, 19232/60000 datapoints
2025-03-06 20:12:02,843 - INFO - training batch 651, loss: 0.218, 20832/60000 datapoints
2025-03-06 20:12:03,060 - INFO - training batch 701, loss: 0.183, 22432/60000 datapoints
2025-03-06 20:12:03,277 - INFO - training batch 751, loss: 0.342, 24032/60000 datapoints
2025-03-06 20:12:03,481 - INFO - training batch 801, loss: 0.129, 25632/60000 datapoints
2025-03-06 20:12:03,686 - INFO - training batch 851, loss: 0.203, 27232/60000 datapoints
2025-03-06 20:12:03,880 - INFO - training batch 901, loss: 0.201, 28832/60000 datapoints
2025-03-06 20:12:04,072 - INFO - training batch 951, loss: 0.180, 30432/60000 datapoints
2025-03-06 20:12:04,271 - INFO - training batch 1001, loss: 0.204, 32032/60000 datapoints
2025-03-06 20:12:04,469 - INFO - training batch 1051, loss: 0.075, 33632/60000 datapoints
2025-03-06 20:12:04,670 - INFO - training batch 1101, loss: 0.357, 35232/60000 datapoints
2025-03-06 20:12:04,893 - INFO - training batch 1151, loss: 0.476, 36832/60000 datapoints
2025-03-06 20:12:05,097 - INFO - training batch 1201, loss: 0.127, 38432/60000 datapoints
2025-03-06 20:12:05,311 - INFO - training batch 1251, loss: 0.096, 40032/60000 datapoints
2025-03-06 20:12:05,515 - INFO - training batch 1301, loss: 0.133, 41632/60000 datapoints
2025-03-06 20:12:05,720 - INFO - training batch 1351, loss: 0.166, 43232/60000 datapoints
2025-03-06 20:12:05,950 - INFO - training batch 1401, loss: 0.302, 44832/60000 datapoints
2025-03-06 20:12:06,208 - INFO - training batch 1451, loss: 0.195, 46432/60000 datapoints
2025-03-06 20:12:06,417 - INFO - training batch 1501, loss: 0.222, 48032/60000 datapoints
2025-03-06 20:12:06,627 - INFO - training batch 1551, loss: 0.357, 49632/60000 datapoints
2025-03-06 20:12:06,855 - INFO - training batch 1601, loss: 0.088, 51232/60000 datapoints
2025-03-06 20:12:07,082 - INFO - training batch 1651, loss: 0.402, 52832/60000 datapoints
2025-03-06 20:12:07,315 - INFO - training batch 1701, loss: 0.087, 54432/60000 datapoints
2025-03-06 20:12:07,541 - INFO - training batch 1751, loss: 0.496, 56032/60000 datapoints
2025-03-06 20:12:07,897 - INFO - training batch 1801, loss: 0.257, 57632/60000 datapoints
2025-03-06 20:12:08,512 - INFO - training batch 1851, loss: 0.186, 59232/60000 datapoints
2025-03-06 20:12:08,821 - INFO - validation batch 1, loss: 0.211, 32/10016 datapoints
2025-03-06 20:12:09,136 - INFO - validation batch 51, loss: 0.249, 1632/10016 datapoints
2025-03-06 20:12:09,422 - INFO - validation batch 101, loss: 0.213, 3232/10016 datapoints
2025-03-06 20:12:09,738 - INFO - validation batch 151, loss: 0.172, 4832/10016 datapoints
2025-03-06 20:12:09,960 - INFO - validation batch 201, loss: 0.137, 6432/10016 datapoints
2025-03-06 20:12:10,167 - INFO - validation batch 251, loss: 0.259, 8032/10016 datapoints
2025-03-06 20:12:10,476 - INFO - validation batch 301, loss: 0.275, 9632/10016 datapoints
2025-03-06 20:12:10,540 - INFO - Epoch 624/800 done.
2025-03-06 20:12:10,542 - INFO - Final validation performance:
Loss: 0.217, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:12:10,545 - INFO - Beginning epoch 625/800
2025-03-06 20:12:10,559 - INFO - training batch 1, loss: 0.184, 32/60000 datapoints
2025-03-06 20:12:11,048 - INFO - training batch 51, loss: 0.161, 1632/60000 datapoints
2025-03-06 20:12:11,370 - INFO - training batch 101, loss: 0.217, 3232/60000 datapoints
2025-03-06 20:12:11,680 - INFO - training batch 151, loss: 0.346, 4832/60000 datapoints
2025-03-06 20:12:11,943 - INFO - training batch 201, loss: 0.045, 6432/60000 datapoints
2025-03-06 20:12:12,168 - INFO - training batch 251, loss: 0.435, 8032/60000 datapoints
2025-03-06 20:12:12,386 - INFO - training batch 301, loss: 0.214, 9632/60000 datapoints
2025-03-06 20:12:12,609 - INFO - training batch 351, loss: 0.102, 11232/60000 datapoints
2025-03-06 20:12:12,832 - INFO - training batch 401, loss: 0.296, 12832/60000 datapoints
2025-03-06 20:12:13,047 - INFO - training batch 451, loss: 0.163, 14432/60000 datapoints
2025-03-06 20:12:13,327 - INFO - training batch 501, loss: 0.501, 16032/60000 datapoints
2025-03-06 20:12:13,554 - INFO - training batch 551, loss: 0.137, 17632/60000 datapoints
2025-03-06 20:12:13,787 - INFO - training batch 601, loss: 0.118, 19232/60000 datapoints
2025-03-06 20:12:14,016 - INFO - training batch 651, loss: 0.562, 20832/60000 datapoints
2025-03-06 20:12:14,258 - INFO - training batch 701, loss: 0.168, 22432/60000 datapoints
2025-03-06 20:12:14,480 - INFO - training batch 751, loss: 0.149, 24032/60000 datapoints
2025-03-06 20:12:14,707 - INFO - training batch 801, loss: 0.110, 25632/60000 datapoints
2025-03-06 20:12:14,988 - INFO - training batch 851, loss: 0.187, 27232/60000 datapoints
2025-03-06 20:12:15,320 - INFO - training batch 901, loss: 0.108, 28832/60000 datapoints
2025-03-06 20:12:15,739 - INFO - training batch 951, loss: 0.374, 30432/60000 datapoints
2025-03-06 20:12:16,077 - INFO - training batch 1001, loss: 0.101, 32032/60000 datapoints
2025-03-06 20:12:16,378 - INFO - training batch 1051, loss: 0.177, 33632/60000 datapoints
2025-03-06 20:12:16,711 - INFO - training batch 1101, loss: 0.076, 35232/60000 datapoints
2025-03-06 20:12:17,074 - INFO - training batch 1151, loss: 0.328, 36832/60000 datapoints
2025-03-06 20:12:17,445 - INFO - training batch 1201, loss: 0.318, 38432/60000 datapoints
2025-03-06 20:12:17,795 - INFO - training batch 1251, loss: 0.131, 40032/60000 datapoints
2025-03-06 20:12:18,082 - INFO - training batch 1301, loss: 0.212, 41632/60000 datapoints
2025-03-06 20:12:18,340 - INFO - training batch 1351, loss: 0.069, 43232/60000 datapoints
2025-03-06 20:12:18,582 - INFO - training batch 1401, loss: 0.178, 44832/60000 datapoints
2025-03-06 20:12:18,825 - INFO - training batch 1451, loss: 0.311, 46432/60000 datapoints
2025-03-06 20:12:19,056 - INFO - training batch 1501, loss: 0.291, 48032/60000 datapoints
2025-03-06 20:12:19,304 - INFO - training batch 1551, loss: 0.227, 49632/60000 datapoints
2025-03-06 20:12:19,545 - INFO - training batch 1601, loss: 0.146, 51232/60000 datapoints
2025-03-06 20:12:19,790 - INFO - training batch 1651, loss: 0.109, 52832/60000 datapoints
2025-03-06 20:12:20,022 - INFO - training batch 1701, loss: 0.092, 54432/60000 datapoints
2025-03-06 20:12:20,248 - INFO - training batch 1751, loss: 0.211, 56032/60000 datapoints
2025-03-06 20:12:20,468 - INFO - training batch 1801, loss: 0.662, 57632/60000 datapoints
2025-03-06 20:12:20,692 - INFO - training batch 1851, loss: 0.115, 59232/60000 datapoints
2025-03-06 20:12:20,802 - INFO - validation batch 1, loss: 0.540, 32/10016 datapoints
2025-03-06 20:12:20,981 - INFO - validation batch 51, loss: 0.353, 1632/10016 datapoints
2025-03-06 20:12:21,154 - INFO - validation batch 101, loss: 0.153, 3232/10016 datapoints
2025-03-06 20:12:21,321 - INFO - validation batch 151, loss: 0.063, 4832/10016 datapoints
2025-03-06 20:12:21,492 - INFO - validation batch 201, loss: 0.403, 6432/10016 datapoints
2025-03-06 20:12:21,661 - INFO - validation batch 251, loss: 0.103, 8032/10016 datapoints
2025-03-06 20:12:21,828 - INFO - validation batch 301, loss: 0.435, 9632/10016 datapoints
2025-03-06 20:12:21,872 - INFO - Epoch 625/800 done.
2025-03-06 20:12:21,873 - INFO - Final validation performance:
Loss: 0.293, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:12:21,873 - INFO - Beginning epoch 626/800
2025-03-06 20:12:21,882 - INFO - training batch 1, loss: 0.248, 32/60000 datapoints
2025-03-06 20:12:22,103 - INFO - training batch 51, loss: 0.166, 1632/60000 datapoints
2025-03-06 20:12:22,335 - INFO - training batch 101, loss: 0.475, 3232/60000 datapoints
2025-03-06 20:12:22,594 - INFO - training batch 151, loss: 0.067, 4832/60000 datapoints
2025-03-06 20:12:22,807 - INFO - training batch 201, loss: 0.166, 6432/60000 datapoints
2025-03-06 20:12:23,031 - INFO - training batch 251, loss: 0.323, 8032/60000 datapoints
2025-03-06 20:12:23,250 - INFO - training batch 301, loss: 0.281, 9632/60000 datapoints
2025-03-06 20:12:23,475 - INFO - training batch 351, loss: 0.293, 11232/60000 datapoints
2025-03-06 20:12:23,695 - INFO - training batch 401, loss: 0.338, 12832/60000 datapoints
2025-03-06 20:12:23,909 - INFO - training batch 451, loss: 0.264, 14432/60000 datapoints
2025-03-06 20:12:24,122 - INFO - training batch 501, loss: 0.507, 16032/60000 datapoints
2025-03-06 20:12:24,338 - INFO - training batch 551, loss: 0.379, 17632/60000 datapoints
2025-03-06 20:12:24,552 - INFO - training batch 601, loss: 0.432, 19232/60000 datapoints
2025-03-06 20:12:24,765 - INFO - training batch 651, loss: 0.149, 20832/60000 datapoints
2025-03-06 20:12:24,984 - INFO - training batch 701, loss: 0.250, 22432/60000 datapoints
2025-03-06 20:12:25,200 - INFO - training batch 751, loss: 0.114, 24032/60000 datapoints
2025-03-06 20:12:25,416 - INFO - training batch 801, loss: 0.132, 25632/60000 datapoints
2025-03-06 20:12:25,632 - INFO - training batch 851, loss: 0.256, 27232/60000 datapoints
2025-03-06 20:12:25,845 - INFO - training batch 901, loss: 0.184, 28832/60000 datapoints
2025-03-06 20:12:26,061 - INFO - training batch 951, loss: 0.272, 30432/60000 datapoints
2025-03-06 20:12:26,279 - INFO - training batch 1001, loss: 0.183, 32032/60000 datapoints
2025-03-06 20:12:26,496 - INFO - training batch 1051, loss: 0.329, 33632/60000 datapoints
2025-03-06 20:12:26,734 - INFO - training batch 1101, loss: 0.090, 35232/60000 datapoints
2025-03-06 20:12:26,943 - INFO - training batch 1151, loss: 0.245, 36832/60000 datapoints
2025-03-06 20:12:27,156 - INFO - training batch 1201, loss: 0.143, 38432/60000 datapoints
2025-03-06 20:12:27,376 - INFO - training batch 1251, loss: 0.260, 40032/60000 datapoints
2025-03-06 20:12:27,588 - INFO - training batch 1301, loss: 0.309, 41632/60000 datapoints
2025-03-06 20:12:27,800 - INFO - training batch 1351, loss: 0.164, 43232/60000 datapoints
2025-03-06 20:12:28,017 - INFO - training batch 1401, loss: 0.101, 44832/60000 datapoints
2025-03-06 20:12:28,264 - INFO - training batch 1451, loss: 0.177, 46432/60000 datapoints
2025-03-06 20:12:28,518 - INFO - training batch 1501, loss: 0.235, 48032/60000 datapoints
2025-03-06 20:12:28,757 - INFO - training batch 1551, loss: 0.297, 49632/60000 datapoints
2025-03-06 20:12:28,987 - INFO - training batch 1601, loss: 0.137, 51232/60000 datapoints
2025-03-06 20:12:29,211 - INFO - training batch 1651, loss: 0.387, 52832/60000 datapoints
2025-03-06 20:12:29,426 - INFO - training batch 1701, loss: 0.086, 54432/60000 datapoints
2025-03-06 20:12:29,646 - INFO - training batch 1751, loss: 0.351, 56032/60000 datapoints
2025-03-06 20:12:29,883 - INFO - training batch 1801, loss: 0.175, 57632/60000 datapoints
2025-03-06 20:12:30,117 - INFO - training batch 1851, loss: 0.162, 59232/60000 datapoints
2025-03-06 20:12:30,242 - INFO - validation batch 1, loss: 0.102, 32/10016 datapoints
2025-03-06 20:12:30,418 - INFO - validation batch 51, loss: 0.398, 1632/10016 datapoints
2025-03-06 20:12:30,600 - INFO - validation batch 101, loss: 0.192, 3232/10016 datapoints
2025-03-06 20:12:30,800 - INFO - validation batch 151, loss: 0.165, 4832/10016 datapoints
2025-03-06 20:12:30,987 - INFO - validation batch 201, loss: 0.090, 6432/10016 datapoints
2025-03-06 20:12:31,167 - INFO - validation batch 251, loss: 0.298, 8032/10016 datapoints
2025-03-06 20:12:31,340 - INFO - validation batch 301, loss: 0.058, 9632/10016 datapoints
2025-03-06 20:12:31,388 - INFO - Epoch 626/800 done.
2025-03-06 20:12:31,388 - INFO - Final validation performance:
Loss: 0.186, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:12:31,389 - INFO - Beginning epoch 627/800
2025-03-06 20:12:31,398 - INFO - training batch 1, loss: 0.188, 32/60000 datapoints
2025-03-06 20:12:31,632 - INFO - training batch 51, loss: 0.172, 1632/60000 datapoints
2025-03-06 20:12:31,851 - INFO - training batch 101, loss: 0.298, 3232/60000 datapoints
2025-03-06 20:12:32,090 - INFO - training batch 151, loss: 0.226, 4832/60000 datapoints
2025-03-06 20:12:32,310 - INFO - training batch 201, loss: 0.392, 6432/60000 datapoints
2025-03-06 20:12:32,534 - INFO - training batch 251, loss: 0.361, 8032/60000 datapoints
2025-03-06 20:12:32,752 - INFO - training batch 301, loss: 0.107, 9632/60000 datapoints
2025-03-06 20:12:32,972 - INFO - training batch 351, loss: 0.307, 11232/60000 datapoints
2025-03-06 20:12:33,198 - INFO - training batch 401, loss: 0.369, 12832/60000 datapoints
2025-03-06 20:12:33,419 - INFO - training batch 451, loss: 0.105, 14432/60000 datapoints
2025-03-06 20:12:33,640 - INFO - training batch 501, loss: 0.211, 16032/60000 datapoints
2025-03-06 20:12:33,858 - INFO - training batch 551, loss: 0.160, 17632/60000 datapoints
2025-03-06 20:12:34,073 - INFO - training batch 601, loss: 0.270, 19232/60000 datapoints
2025-03-06 20:12:34,289 - INFO - training batch 651, loss: 0.145, 20832/60000 datapoints
2025-03-06 20:12:34,502 - INFO - training batch 701, loss: 0.226, 22432/60000 datapoints
2025-03-06 20:12:34,729 - INFO - training batch 751, loss: 0.155, 24032/60000 datapoints
2025-03-06 20:12:34,944 - INFO - training batch 801, loss: 0.141, 25632/60000 datapoints
2025-03-06 20:12:35,162 - INFO - training batch 851, loss: 0.114, 27232/60000 datapoints
2025-03-06 20:12:35,373 - INFO - training batch 901, loss: 0.079, 28832/60000 datapoints
2025-03-06 20:12:35,593 - INFO - training batch 951, loss: 0.630, 30432/60000 datapoints
2025-03-06 20:12:35,834 - INFO - training batch 1001, loss: 0.152, 32032/60000 datapoints
2025-03-06 20:12:36,085 - INFO - training batch 1051, loss: 0.275, 33632/60000 datapoints
2025-03-06 20:12:36,304 - INFO - training batch 1101, loss: 0.112, 35232/60000 datapoints
2025-03-06 20:12:36,516 - INFO - training batch 1151, loss: 0.075, 36832/60000 datapoints
2025-03-06 20:12:36,733 - INFO - training batch 1201, loss: 0.181, 38432/60000 datapoints
2025-03-06 20:12:36,963 - INFO - training batch 1251, loss: 0.368, 40032/60000 datapoints
2025-03-06 20:12:37,223 - INFO - training batch 1301, loss: 0.181, 41632/60000 datapoints
2025-03-06 20:12:37,506 - INFO - training batch 1351, loss: 0.093, 43232/60000 datapoints
2025-03-06 20:12:37,844 - INFO - training batch 1401, loss: 0.133, 44832/60000 datapoints
2025-03-06 20:12:38,331 - INFO - training batch 1451, loss: 0.219, 46432/60000 datapoints
2025-03-06 20:12:38,555 - INFO - training batch 1501, loss: 0.319, 48032/60000 datapoints
2025-03-06 20:12:38,780 - INFO - training batch 1551, loss: 0.136, 49632/60000 datapoints
2025-03-06 20:12:39,010 - INFO - training batch 1601, loss: 0.216, 51232/60000 datapoints
2025-03-06 20:12:39,234 - INFO - training batch 1651, loss: 0.147, 52832/60000 datapoints
2025-03-06 20:12:39,456 - INFO - training batch 1701, loss: 0.676, 54432/60000 datapoints
2025-03-06 20:12:39,675 - INFO - training batch 1751, loss: 0.063, 56032/60000 datapoints
2025-03-06 20:12:39,903 - INFO - training batch 1801, loss: 0.290, 57632/60000 datapoints
2025-03-06 20:12:40,125 - INFO - training batch 1851, loss: 0.160, 59232/60000 datapoints
2025-03-06 20:12:40,240 - INFO - validation batch 1, loss: 0.056, 32/10016 datapoints
2025-03-06 20:12:40,410 - INFO - validation batch 51, loss: 0.149, 1632/10016 datapoints
2025-03-06 20:12:40,580 - INFO - validation batch 101, loss: 0.392, 3232/10016 datapoints
2025-03-06 20:12:40,755 - INFO - validation batch 151, loss: 0.220, 4832/10016 datapoints
2025-03-06 20:12:40,930 - INFO - validation batch 201, loss: 0.157, 6432/10016 datapoints
2025-03-06 20:12:41,113 - INFO - validation batch 251, loss: 0.231, 8032/10016 datapoints
2025-03-06 20:12:41,312 - INFO - validation batch 301, loss: 0.380, 9632/10016 datapoints
2025-03-06 20:12:41,352 - INFO - Epoch 627/800 done.
2025-03-06 20:12:41,353 - INFO - Final validation performance:
Loss: 0.226, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:12:41,353 - INFO - Beginning epoch 628/800
2025-03-06 20:12:41,361 - INFO - training batch 1, loss: 0.128, 32/60000 datapoints
2025-03-06 20:12:41,582 - INFO - training batch 51, loss: 0.209, 1632/60000 datapoints
2025-03-06 20:12:41,813 - INFO - training batch 101, loss: 0.247, 3232/60000 datapoints
2025-03-06 20:12:42,030 - INFO - training batch 151, loss: 0.146, 4832/60000 datapoints
2025-03-06 20:12:42,257 - INFO - training batch 201, loss: 0.122, 6432/60000 datapoints
2025-03-06 20:12:42,475 - INFO - training batch 251, loss: 0.058, 8032/60000 datapoints
2025-03-06 20:12:42,697 - INFO - training batch 301, loss: 0.227, 9632/60000 datapoints
2025-03-06 20:12:42,925 - INFO - training batch 351, loss: 0.232, 11232/60000 datapoints
2025-03-06 20:12:43,144 - INFO - training batch 401, loss: 0.162, 12832/60000 datapoints
2025-03-06 20:12:43,370 - INFO - training batch 451, loss: 0.406, 14432/60000 datapoints
2025-03-06 20:12:43,592 - INFO - training batch 501, loss: 0.077, 16032/60000 datapoints
2025-03-06 20:12:43,820 - INFO - training batch 551, loss: 0.392, 17632/60000 datapoints
2025-03-06 20:12:44,040 - INFO - training batch 601, loss: 0.259, 19232/60000 datapoints
2025-03-06 20:12:44,266 - INFO - training batch 651, loss: 0.096, 20832/60000 datapoints
2025-03-06 20:12:44,487 - INFO - training batch 701, loss: 0.355, 22432/60000 datapoints
2025-03-06 20:12:44,720 - INFO - training batch 751, loss: 0.206, 24032/60000 datapoints
2025-03-06 20:12:44,996 - INFO - training batch 801, loss: 0.140, 25632/60000 datapoints
2025-03-06 20:12:45,223 - INFO - training batch 851, loss: 0.100, 27232/60000 datapoints
2025-03-06 20:12:45,548 - INFO - training batch 901, loss: 0.316, 28832/60000 datapoints
2025-03-06 20:12:45,874 - INFO - training batch 951, loss: 0.394, 30432/60000 datapoints
2025-03-06 20:12:46,148 - INFO - training batch 1001, loss: 0.323, 32032/60000 datapoints
2025-03-06 20:12:46,511 - INFO - training batch 1051, loss: 0.153, 33632/60000 datapoints
2025-03-06 20:12:46,798 - INFO - training batch 1101, loss: 0.148, 35232/60000 datapoints
2025-03-06 20:12:47,059 - INFO - training batch 1151, loss: 0.158, 36832/60000 datapoints
2025-03-06 20:12:47,348 - INFO - training batch 1201, loss: 0.226, 38432/60000 datapoints
2025-03-06 20:12:47,600 - INFO - training batch 1251, loss: 0.612, 40032/60000 datapoints
2025-03-06 20:12:47,839 - INFO - training batch 1301, loss: 0.219, 41632/60000 datapoints
2025-03-06 20:12:48,078 - INFO - training batch 1351, loss: 0.225, 43232/60000 datapoints
2025-03-06 20:12:48,295 - INFO - training batch 1401, loss: 0.103, 44832/60000 datapoints
2025-03-06 20:12:48,522 - INFO - training batch 1451, loss: 0.406, 46432/60000 datapoints
2025-03-06 20:12:48,760 - INFO - training batch 1501, loss: 0.325, 48032/60000 datapoints
2025-03-06 20:12:49,008 - INFO - training batch 1551, loss: 0.294, 49632/60000 datapoints
2025-03-06 20:12:49,263 - INFO - training batch 1601, loss: 0.237, 51232/60000 datapoints
2025-03-06 20:12:49,516 - INFO - training batch 1651, loss: 0.245, 52832/60000 datapoints
2025-03-06 20:12:49,749 - INFO - training batch 1701, loss: 0.320, 54432/60000 datapoints
2025-03-06 20:12:50,007 - INFO - training batch 1751, loss: 0.208, 56032/60000 datapoints
2025-03-06 20:12:50,249 - INFO - training batch 1801, loss: 0.132, 57632/60000 datapoints
2025-03-06 20:12:50,465 - INFO - training batch 1851, loss: 0.464, 59232/60000 datapoints
2025-03-06 20:12:50,581 - INFO - validation batch 1, loss: 0.184, 32/10016 datapoints
2025-03-06 20:12:50,754 - INFO - validation batch 51, loss: 0.233, 1632/10016 datapoints
2025-03-06 20:12:50,921 - INFO - validation batch 101, loss: 0.296, 3232/10016 datapoints
2025-03-06 20:12:51,089 - INFO - validation batch 151, loss: 0.183, 4832/10016 datapoints
2025-03-06 20:12:51,263 - INFO - validation batch 201, loss: 0.189, 6432/10016 datapoints
2025-03-06 20:12:51,432 - INFO - validation batch 251, loss: 0.409, 8032/10016 datapoints
2025-03-06 20:12:51,596 - INFO - validation batch 301, loss: 0.344, 9632/10016 datapoints
2025-03-06 20:12:51,640 - INFO - Epoch 628/800 done.
2025-03-06 20:12:51,641 - INFO - Final validation performance:
Loss: 0.263, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:12:51,641 - INFO - Beginning epoch 629/800
2025-03-06 20:12:51,649 - INFO - training batch 1, loss: 0.302, 32/60000 datapoints
2025-03-06 20:12:51,879 - INFO - training batch 51, loss: 0.195, 1632/60000 datapoints
2025-03-06 20:12:52,120 - INFO - training batch 101, loss: 0.062, 3232/60000 datapoints
2025-03-06 20:12:52,345 - INFO - training batch 151, loss: 0.205, 4832/60000 datapoints
2025-03-06 20:12:52,566 - INFO - training batch 201, loss: 0.318, 6432/60000 datapoints
2025-03-06 20:12:52,821 - INFO - training batch 251, loss: 0.319, 8032/60000 datapoints
2025-03-06 20:12:53,053 - INFO - training batch 301, loss: 0.126, 9632/60000 datapoints
2025-03-06 20:12:53,283 - INFO - training batch 351, loss: 0.092, 11232/60000 datapoints
2025-03-06 20:12:53,600 - INFO - training batch 401, loss: 0.182, 12832/60000 datapoints
2025-03-06 20:12:53,919 - INFO - training batch 451, loss: 0.135, 14432/60000 datapoints
2025-03-06 20:12:54,219 - INFO - training batch 501, loss: 0.210, 16032/60000 datapoints
2025-03-06 20:12:54,570 - INFO - training batch 551, loss: 0.160, 17632/60000 datapoints
2025-03-06 20:12:54,895 - INFO - training batch 601, loss: 0.214, 19232/60000 datapoints
2025-03-06 20:12:55,244 - INFO - training batch 651, loss: 0.047, 20832/60000 datapoints
2025-03-06 20:12:55,675 - INFO - training batch 701, loss: 0.358, 22432/60000 datapoints
2025-03-06 20:12:55,946 - INFO - training batch 751, loss: 0.347, 24032/60000 datapoints
2025-03-06 20:12:56,206 - INFO - training batch 801, loss: 0.055, 25632/60000 datapoints
2025-03-06 20:12:56,520 - INFO - training batch 851, loss: 0.185, 27232/60000 datapoints
2025-03-06 20:12:56,820 - INFO - training batch 901, loss: 0.084, 28832/60000 datapoints
2025-03-06 20:12:57,322 - INFO - training batch 951, loss: 0.476, 30432/60000 datapoints
2025-03-06 20:12:57,567 - INFO - training batch 1001, loss: 0.077, 32032/60000 datapoints
2025-03-06 20:12:57,805 - INFO - training batch 1051, loss: 0.186, 33632/60000 datapoints
2025-03-06 20:12:58,086 - INFO - training batch 1101, loss: 0.203, 35232/60000 datapoints
2025-03-06 20:12:58,361 - INFO - training batch 1151, loss: 0.407, 36832/60000 datapoints
2025-03-06 20:12:58,691 - INFO - training batch 1201, loss: 0.202, 38432/60000 datapoints
2025-03-06 20:12:59,080 - INFO - training batch 1251, loss: 0.084, 40032/60000 datapoints
2025-03-06 20:12:59,352 - INFO - training batch 1301, loss: 0.136, 41632/60000 datapoints
2025-03-06 20:12:59,573 - INFO - training batch 1351, loss: 0.446, 43232/60000 datapoints
2025-03-06 20:12:59,857 - INFO - training batch 1401, loss: 0.498, 44832/60000 datapoints
2025-03-06 20:13:00,158 - INFO - training batch 1451, loss: 0.250, 46432/60000 datapoints
2025-03-06 20:13:00,405 - INFO - training batch 1501, loss: 0.233, 48032/60000 datapoints
2025-03-06 20:13:00,638 - INFO - training batch 1551, loss: 0.377, 49632/60000 datapoints
2025-03-06 20:13:00,876 - INFO - training batch 1601, loss: 0.699, 51232/60000 datapoints
2025-03-06 20:13:01,161 - INFO - training batch 1651, loss: 0.373, 52832/60000 datapoints
2025-03-06 20:13:01,406 - INFO - training batch 1701, loss: 0.299, 54432/60000 datapoints
2025-03-06 20:13:01,704 - INFO - training batch 1751, loss: 0.180, 56032/60000 datapoints
2025-03-06 20:13:02,064 - INFO - training batch 1801, loss: 0.195, 57632/60000 datapoints
2025-03-06 20:13:02,308 - INFO - training batch 1851, loss: 0.330, 59232/60000 datapoints
2025-03-06 20:13:02,473 - INFO - validation batch 1, loss: 0.311, 32/10016 datapoints
2025-03-06 20:13:02,688 - INFO - validation batch 51, loss: 0.262, 1632/10016 datapoints
2025-03-06 20:13:02,875 - INFO - validation batch 101, loss: 0.391, 3232/10016 datapoints
2025-03-06 20:13:03,070 - INFO - validation batch 151, loss: 0.382, 4832/10016 datapoints
2025-03-06 20:13:03,243 - INFO - validation batch 201, loss: 0.315, 6432/10016 datapoints
2025-03-06 20:13:03,441 - INFO - validation batch 251, loss: 0.271, 8032/10016 datapoints
2025-03-06 20:13:03,645 - INFO - validation batch 301, loss: 0.618, 9632/10016 datapoints
2025-03-06 20:13:03,692 - INFO - Epoch 629/800 done.
2025-03-06 20:13:03,693 - INFO - Final validation performance:
Loss: 0.364, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:13:03,695 - INFO - Beginning epoch 630/800
2025-03-06 20:13:03,706 - INFO - training batch 1, loss: 0.179, 32/60000 datapoints
2025-03-06 20:13:03,958 - INFO - training batch 51, loss: 0.155, 1632/60000 datapoints
2025-03-06 20:13:04,194 - INFO - training batch 101, loss: 0.216, 3232/60000 datapoints
2025-03-06 20:13:04,468 - INFO - training batch 151, loss: 0.256, 4832/60000 datapoints
2025-03-06 20:13:04,713 - INFO - training batch 201, loss: 0.190, 6432/60000 datapoints
2025-03-06 20:13:05,110 - INFO - training batch 251, loss: 0.226, 8032/60000 datapoints
2025-03-06 20:13:05,343 - INFO - training batch 301, loss: 0.110, 9632/60000 datapoints
2025-03-06 20:13:05,553 - INFO - training batch 351, loss: 0.117, 11232/60000 datapoints
2025-03-06 20:13:05,779 - INFO - training batch 401, loss: 0.246, 12832/60000 datapoints
2025-03-06 20:13:06,010 - INFO - training batch 451, loss: 0.116, 14432/60000 datapoints
2025-03-06 20:13:06,292 - INFO - training batch 501, loss: 0.294, 16032/60000 datapoints
2025-03-06 20:13:06,527 - INFO - training batch 551, loss: 0.201, 17632/60000 datapoints
2025-03-06 20:13:06,741 - INFO - training batch 601, loss: 0.332, 19232/60000 datapoints
2025-03-06 20:13:06,957 - INFO - training batch 651, loss: 0.234, 20832/60000 datapoints
2025-03-06 20:13:07,173 - INFO - training batch 701, loss: 0.144, 22432/60000 datapoints
2025-03-06 20:13:07,387 - INFO - training batch 751, loss: 0.043, 24032/60000 datapoints
2025-03-06 20:13:07,601 - INFO - training batch 801, loss: 0.121, 25632/60000 datapoints
2025-03-06 20:13:07,826 - INFO - training batch 851, loss: 0.398, 27232/60000 datapoints
2025-03-06 20:13:08,035 - INFO - training batch 901, loss: 0.209, 28832/60000 datapoints
2025-03-06 20:13:08,251 - INFO - training batch 951, loss: 0.328, 30432/60000 datapoints
2025-03-06 20:13:08,494 - INFO - training batch 1001, loss: 0.332, 32032/60000 datapoints
2025-03-06 20:13:08,723 - INFO - training batch 1051, loss: 0.264, 33632/60000 datapoints
2025-03-06 20:13:08,943 - INFO - training batch 1101, loss: 0.464, 35232/60000 datapoints
2025-03-06 20:13:09,178 - INFO - training batch 1151, loss: 0.268, 36832/60000 datapoints
2025-03-06 20:13:09,386 - INFO - training batch 1201, loss: 0.101, 38432/60000 datapoints
2025-03-06 20:13:09,596 - INFO - training batch 1251, loss: 0.383, 40032/60000 datapoints
2025-03-06 20:13:09,807 - INFO - training batch 1301, loss: 0.105, 41632/60000 datapoints
2025-03-06 20:13:10,013 - INFO - training batch 1351, loss: 0.134, 43232/60000 datapoints
2025-03-06 20:13:10,247 - INFO - training batch 1401, loss: 0.177, 44832/60000 datapoints
2025-03-06 20:13:10,459 - INFO - training batch 1451, loss: 0.245, 46432/60000 datapoints
2025-03-06 20:13:10,668 - INFO - training batch 1501, loss: 0.385, 48032/60000 datapoints
2025-03-06 20:13:10,885 - INFO - training batch 1551, loss: 0.154, 49632/60000 datapoints
2025-03-06 20:13:11,116 - INFO - training batch 1601, loss: 0.095, 51232/60000 datapoints
2025-03-06 20:13:11,327 - INFO - training batch 1651, loss: 0.311, 52832/60000 datapoints
2025-03-06 20:13:11,536 - INFO - training batch 1701, loss: 0.159, 54432/60000 datapoints
2025-03-06 20:13:11,750 - INFO - training batch 1751, loss: 0.054, 56032/60000 datapoints
2025-03-06 20:13:11,965 - INFO - training batch 1801, loss: 0.186, 57632/60000 datapoints
2025-03-06 20:13:12,181 - INFO - training batch 1851, loss: 0.157, 59232/60000 datapoints
2025-03-06 20:13:12,293 - INFO - validation batch 1, loss: 0.226, 32/10016 datapoints
2025-03-06 20:13:12,458 - INFO - validation batch 51, loss: 0.354, 1632/10016 datapoints
2025-03-06 20:13:12,634 - INFO - validation batch 101, loss: 0.192, 3232/10016 datapoints
2025-03-06 20:13:12,807 - INFO - validation batch 151, loss: 0.206, 4832/10016 datapoints
2025-03-06 20:13:12,969 - INFO - validation batch 201, loss: 0.193, 6432/10016 datapoints
2025-03-06 20:13:13,170 - INFO - validation batch 251, loss: 0.383, 8032/10016 datapoints
2025-03-06 20:13:13,356 - INFO - validation batch 301, loss: 0.381, 9632/10016 datapoints
2025-03-06 20:13:13,401 - INFO - Epoch 630/800 done.
2025-03-06 20:13:13,402 - INFO - Final validation performance:
Loss: 0.276, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:13:13,402 - INFO - Beginning epoch 631/800
2025-03-06 20:13:13,412 - INFO - training batch 1, loss: 0.279, 32/60000 datapoints
2025-03-06 20:13:13,645 - INFO - training batch 51, loss: 0.326, 1632/60000 datapoints
2025-03-06 20:13:13,859 - INFO - training batch 101, loss: 0.243, 3232/60000 datapoints
2025-03-06 20:13:14,077 - INFO - training batch 151, loss: 0.276, 4832/60000 datapoints
2025-03-06 20:13:14,317 - INFO - training batch 201, loss: 0.584, 6432/60000 datapoints
2025-03-06 20:13:14,548 - INFO - training batch 251, loss: 0.330, 8032/60000 datapoints
2025-03-06 20:13:14,783 - INFO - training batch 301, loss: 0.152, 9632/60000 datapoints
2025-03-06 20:13:15,012 - INFO - training batch 351, loss: 0.229, 11232/60000 datapoints
2025-03-06 20:13:15,259 - INFO - training batch 401, loss: 0.265, 12832/60000 datapoints
2025-03-06 20:13:15,514 - INFO - training batch 451, loss: 0.128, 14432/60000 datapoints
2025-03-06 20:13:15,759 - INFO - training batch 501, loss: 0.246, 16032/60000 datapoints
2025-03-06 20:13:15,978 - INFO - training batch 551, loss: 0.482, 17632/60000 datapoints
2025-03-06 20:13:16,210 - INFO - training batch 601, loss: 0.311, 19232/60000 datapoints
2025-03-06 20:13:16,433 - INFO - training batch 651, loss: 0.182, 20832/60000 datapoints
2025-03-06 20:13:16,664 - INFO - training batch 701, loss: 0.116, 22432/60000 datapoints
2025-03-06 20:13:16,888 - INFO - training batch 751, loss: 0.129, 24032/60000 datapoints
2025-03-06 20:13:17,106 - INFO - training batch 801, loss: 0.357, 25632/60000 datapoints
2025-03-06 20:13:17,323 - INFO - training batch 851, loss: 0.129, 27232/60000 datapoints
2025-03-06 20:13:17,532 - INFO - training batch 901, loss: 0.163, 28832/60000 datapoints
2025-03-06 20:13:17,747 - INFO - training batch 951, loss: 0.201, 30432/60000 datapoints
2025-03-06 20:13:17,962 - INFO - training batch 1001, loss: 0.487, 32032/60000 datapoints
2025-03-06 20:13:18,208 - INFO - training batch 1051, loss: 0.316, 33632/60000 datapoints
2025-03-06 20:13:18,453 - INFO - training batch 1101, loss: 0.194, 35232/60000 datapoints
2025-03-06 20:13:18,674 - INFO - training batch 1151, loss: 0.224, 36832/60000 datapoints
2025-03-06 20:13:18,910 - INFO - training batch 1201, loss: 0.135, 38432/60000 datapoints
2025-03-06 20:13:19,129 - INFO - training batch 1251, loss: 0.321, 40032/60000 datapoints
2025-03-06 20:13:19,357 - INFO - training batch 1301, loss: 0.177, 41632/60000 datapoints
2025-03-06 20:13:19,636 - INFO - training batch 1351, loss: 0.305, 43232/60000 datapoints
2025-03-06 20:13:19,920 - INFO - training batch 1401, loss: 0.075, 44832/60000 datapoints
2025-03-06 20:13:20,170 - INFO - training batch 1451, loss: 0.384, 46432/60000 datapoints
2025-03-06 20:13:20,435 - INFO - training batch 1501, loss: 0.296, 48032/60000 datapoints
2025-03-06 20:13:20,686 - INFO - training batch 1551, loss: 0.429, 49632/60000 datapoints
2025-03-06 20:13:20,943 - INFO - training batch 1601, loss: 0.146, 51232/60000 datapoints
2025-03-06 20:13:21,212 - INFO - training batch 1651, loss: 0.491, 52832/60000 datapoints
2025-03-06 20:13:21,459 - INFO - training batch 1701, loss: 0.298, 54432/60000 datapoints
2025-03-06 20:13:21,708 - INFO - training batch 1751, loss: 0.161, 56032/60000 datapoints
2025-03-06 20:13:21,941 - INFO - training batch 1801, loss: 0.179, 57632/60000 datapoints
2025-03-06 20:13:22,176 - INFO - training batch 1851, loss: 0.240, 59232/60000 datapoints
2025-03-06 20:13:22,307 - INFO - validation batch 1, loss: 0.309, 32/10016 datapoints
2025-03-06 20:13:22,493 - INFO - validation batch 51, loss: 0.365, 1632/10016 datapoints
2025-03-06 20:13:22,680 - INFO - validation batch 101, loss: 0.155, 3232/10016 datapoints
2025-03-06 20:13:22,864 - INFO - validation batch 151, loss: 0.325, 4832/10016 datapoints
2025-03-06 20:13:23,062 - INFO - validation batch 201, loss: 0.356, 6432/10016 datapoints
2025-03-06 20:13:23,272 - INFO - validation batch 251, loss: 0.369, 8032/10016 datapoints
2025-03-06 20:13:23,479 - INFO - validation batch 301, loss: 0.222, 9632/10016 datapoints
2025-03-06 20:13:23,535 - INFO - Epoch 631/800 done.
2025-03-06 20:13:23,535 - INFO - Final validation performance:
Loss: 0.300, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:13:23,536 - INFO - Beginning epoch 632/800
2025-03-06 20:13:23,544 - INFO - training batch 1, loss: 0.387, 32/60000 datapoints
2025-03-06 20:13:23,832 - INFO - training batch 51, loss: 0.168, 1632/60000 datapoints
2025-03-06 20:13:24,095 - INFO - training batch 101, loss: 0.125, 3232/60000 datapoints
2025-03-06 20:13:24,369 - INFO - training batch 151, loss: 0.208, 4832/60000 datapoints
2025-03-06 20:13:24,754 - INFO - training batch 201, loss: 0.225, 6432/60000 datapoints
2025-03-06 20:13:24,996 - INFO - training batch 251, loss: 0.284, 8032/60000 datapoints
2025-03-06 20:13:25,239 - INFO - training batch 301, loss: 0.103, 9632/60000 datapoints
2025-03-06 20:13:25,504 - INFO - training batch 351, loss: 0.401, 11232/60000 datapoints
2025-03-06 20:13:25,749 - INFO - training batch 401, loss: 0.130, 12832/60000 datapoints
2025-03-06 20:13:25,983 - INFO - training batch 451, loss: 0.053, 14432/60000 datapoints
2025-03-06 20:13:26,221 - INFO - training batch 501, loss: 0.303, 16032/60000 datapoints
2025-03-06 20:13:26,477 - INFO - training batch 551, loss: 0.186, 17632/60000 datapoints
2025-03-06 20:13:26,718 - INFO - training batch 601, loss: 0.048, 19232/60000 datapoints
2025-03-06 20:13:26,949 - INFO - training batch 651, loss: 0.107, 20832/60000 datapoints
2025-03-06 20:13:27,177 - INFO - training batch 701, loss: 0.333, 22432/60000 datapoints
2025-03-06 20:13:27,395 - INFO - training batch 751, loss: 0.464, 24032/60000 datapoints
2025-03-06 20:13:27,624 - INFO - training batch 801, loss: 0.160, 25632/60000 datapoints
2025-03-06 20:13:27,867 - INFO - training batch 851, loss: 0.293, 27232/60000 datapoints
2025-03-06 20:13:28,093 - INFO - training batch 901, loss: 0.477, 28832/60000 datapoints
2025-03-06 20:13:28,303 - INFO - training batch 951, loss: 0.158, 30432/60000 datapoints
2025-03-06 20:13:28,517 - INFO - training batch 1001, loss: 0.267, 32032/60000 datapoints
2025-03-06 20:13:28,727 - INFO - training batch 1051, loss: 0.194, 33632/60000 datapoints
2025-03-06 20:13:28,928 - INFO - training batch 1101, loss: 0.318, 35232/60000 datapoints
2025-03-06 20:13:29,133 - INFO - training batch 1151, loss: 0.162, 36832/60000 datapoints
2025-03-06 20:13:29,351 - INFO - training batch 1201, loss: 0.299, 38432/60000 datapoints
2025-03-06 20:13:29,557 - INFO - training batch 1251, loss: 0.152, 40032/60000 datapoints
2025-03-06 20:13:29,775 - INFO - training batch 1301, loss: 0.282, 41632/60000 datapoints
2025-03-06 20:13:29,981 - INFO - training batch 1351, loss: 0.205, 43232/60000 datapoints
2025-03-06 20:13:30,264 - INFO - training batch 1401, loss: 0.143, 44832/60000 datapoints
2025-03-06 20:13:30,515 - INFO - training batch 1451, loss: 0.077, 46432/60000 datapoints
2025-03-06 20:13:30,747 - INFO - training batch 1501, loss: 0.141, 48032/60000 datapoints
2025-03-06 20:13:30,983 - INFO - training batch 1551, loss: 0.176, 49632/60000 datapoints
2025-03-06 20:13:31,219 - INFO - training batch 1601, loss: 0.483, 51232/60000 datapoints
2025-03-06 20:13:31,437 - INFO - training batch 1651, loss: 0.116, 52832/60000 datapoints
2025-03-06 20:13:31,658 - INFO - training batch 1701, loss: 0.139, 54432/60000 datapoints
2025-03-06 20:13:31,885 - INFO - training batch 1751, loss: 0.114, 56032/60000 datapoints
2025-03-06 20:13:32,098 - INFO - training batch 1801, loss: 0.572, 57632/60000 datapoints
2025-03-06 20:13:32,324 - INFO - training batch 1851, loss: 0.195, 59232/60000 datapoints
2025-03-06 20:13:32,438 - INFO - validation batch 1, loss: 0.214, 32/10016 datapoints
2025-03-06 20:13:32,639 - INFO - validation batch 51, loss: 0.129, 1632/10016 datapoints
2025-03-06 20:13:32,815 - INFO - validation batch 101, loss: 0.461, 3232/10016 datapoints
2025-03-06 20:13:32,989 - INFO - validation batch 151, loss: 0.110, 4832/10016 datapoints
2025-03-06 20:13:33,161 - INFO - validation batch 201, loss: 0.254, 6432/10016 datapoints
2025-03-06 20:13:33,341 - INFO - validation batch 251, loss: 0.157, 8032/10016 datapoints
2025-03-06 20:13:33,509 - INFO - validation batch 301, loss: 0.237, 9632/10016 datapoints
2025-03-06 20:13:33,551 - INFO - Epoch 632/800 done.
2025-03-06 20:13:33,551 - INFO - Final validation performance:
Loss: 0.223, top-1 acc: 0.932top-5 acc: 0.932
2025-03-06 20:13:33,552 - INFO - Beginning epoch 633/800
2025-03-06 20:13:33,559 - INFO - training batch 1, loss: 0.065, 32/60000 datapoints
2025-03-06 20:13:33,930 - INFO - training batch 51, loss: 0.222, 1632/60000 datapoints
2025-03-06 20:13:34,277 - INFO - training batch 101, loss: 0.299, 3232/60000 datapoints
2025-03-06 20:13:34,580 - INFO - training batch 151, loss: 0.247, 4832/60000 datapoints
2025-03-06 20:13:34,855 - INFO - training batch 201, loss: 0.220, 6432/60000 datapoints
2025-03-06 20:13:35,088 - INFO - training batch 251, loss: 0.084, 8032/60000 datapoints
2025-03-06 20:13:35,316 - INFO - training batch 301, loss: 0.208, 9632/60000 datapoints
2025-03-06 20:13:35,536 - INFO - training batch 351, loss: 0.377, 11232/60000 datapoints
2025-03-06 20:13:35,772 - INFO - training batch 401, loss: 0.133, 12832/60000 datapoints
2025-03-06 20:13:36,033 - INFO - training batch 451, loss: 0.305, 14432/60000 datapoints
2025-03-06 20:13:36,281 - INFO - training batch 501, loss: 0.247, 16032/60000 datapoints
2025-03-06 20:13:36,497 - INFO - training batch 551, loss: 0.347, 17632/60000 datapoints
2025-03-06 20:13:36,709 - INFO - training batch 601, loss: 0.213, 19232/60000 datapoints
2025-03-06 20:13:36,925 - INFO - training batch 651, loss: 0.289, 20832/60000 datapoints
2025-03-06 20:13:37,150 - INFO - training batch 701, loss: 0.186, 22432/60000 datapoints
2025-03-06 20:13:37,361 - INFO - training batch 751, loss: 0.080, 24032/60000 datapoints
2025-03-06 20:13:37,588 - INFO - training batch 801, loss: 0.324, 25632/60000 datapoints
2025-03-06 20:13:37,837 - INFO - training batch 851, loss: 0.173, 27232/60000 datapoints
2025-03-06 20:13:38,063 - INFO - training batch 901, loss: 0.436, 28832/60000 datapoints
2025-03-06 20:13:38,291 - INFO - training batch 951, loss: 0.102, 30432/60000 datapoints
2025-03-06 20:13:38,518 - INFO - training batch 1001, loss: 0.277, 32032/60000 datapoints
2025-03-06 20:13:38,753 - INFO - training batch 1051, loss: 0.457, 33632/60000 datapoints
2025-03-06 20:13:39,029 - INFO - training batch 1101, loss: 0.378, 35232/60000 datapoints
2025-03-06 20:13:39,369 - INFO - training batch 1151, loss: 0.470, 36832/60000 datapoints
2025-03-06 20:13:39,594 - INFO - training batch 1201, loss: 0.297, 38432/60000 datapoints
2025-03-06 20:13:39,823 - INFO - training batch 1251, loss: 0.165, 40032/60000 datapoints
2025-03-06 20:13:40,049 - INFO - training batch 1301, loss: 0.205, 41632/60000 datapoints
2025-03-06 20:13:40,289 - INFO - training batch 1351, loss: 0.350, 43232/60000 datapoints
2025-03-06 20:13:40,539 - INFO - training batch 1401, loss: 0.163, 44832/60000 datapoints
2025-03-06 20:13:40,783 - INFO - training batch 1451, loss: 0.359, 46432/60000 datapoints
2025-03-06 20:13:41,051 - INFO - training batch 1501, loss: 0.194, 48032/60000 datapoints
2025-03-06 20:13:41,280 - INFO - training batch 1551, loss: 0.296, 49632/60000 datapoints
2025-03-06 20:13:41,531 - INFO - training batch 1601, loss: 0.217, 51232/60000 datapoints
2025-03-06 20:13:41,753 - INFO - training batch 1651, loss: 0.251, 52832/60000 datapoints
2025-03-06 20:13:41,977 - INFO - training batch 1701, loss: 0.344, 54432/60000 datapoints
2025-03-06 20:13:42,191 - INFO - training batch 1751, loss: 0.281, 56032/60000 datapoints
2025-03-06 20:13:42,413 - INFO - training batch 1801, loss: 0.358, 57632/60000 datapoints
2025-03-06 20:13:42,675 - INFO - training batch 1851, loss: 0.245, 59232/60000 datapoints
2025-03-06 20:13:42,804 - INFO - validation batch 1, loss: 0.149, 32/10016 datapoints
2025-03-06 20:13:42,993 - INFO - validation batch 51, loss: 0.594, 1632/10016 datapoints
2025-03-06 20:13:43,189 - INFO - validation batch 101, loss: 0.305, 3232/10016 datapoints
2025-03-06 20:13:43,375 - INFO - validation batch 151, loss: 0.395, 4832/10016 datapoints
2025-03-06 20:13:43,548 - INFO - validation batch 201, loss: 0.529, 6432/10016 datapoints
2025-03-06 20:13:43,724 - INFO - validation batch 251, loss: 0.259, 8032/10016 datapoints
2025-03-06 20:13:43,911 - INFO - validation batch 301, loss: 0.107, 9632/10016 datapoints
2025-03-06 20:13:43,976 - INFO - Epoch 633/800 done.
2025-03-06 20:13:43,976 - INFO - Final validation performance:
Loss: 0.334, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:13:43,977 - INFO - Beginning epoch 634/800
2025-03-06 20:13:43,988 - INFO - training batch 1, loss: 0.087, 32/60000 datapoints
2025-03-06 20:13:44,245 - INFO - training batch 51, loss: 0.080, 1632/60000 datapoints
2025-03-06 20:13:44,456 - INFO - training batch 101, loss: 0.187, 3232/60000 datapoints
2025-03-06 20:13:44,693 - INFO - training batch 151, loss: 0.098, 4832/60000 datapoints
2025-03-06 20:13:44,927 - INFO - training batch 201, loss: 0.146, 6432/60000 datapoints
2025-03-06 20:13:45,134 - INFO - training batch 251, loss: 0.065, 8032/60000 datapoints
2025-03-06 20:13:45,343 - INFO - training batch 301, loss: 0.384, 9632/60000 datapoints
2025-03-06 20:13:45,543 - INFO - training batch 351, loss: 0.129, 11232/60000 datapoints
2025-03-06 20:13:45,747 - INFO - training batch 401, loss: 0.217, 12832/60000 datapoints
2025-03-06 20:13:45,948 - INFO - training batch 451, loss: 0.388, 14432/60000 datapoints
2025-03-06 20:13:46,148 - INFO - training batch 501, loss: 0.254, 16032/60000 datapoints
2025-03-06 20:13:46,352 - INFO - training batch 551, loss: 0.120, 17632/60000 datapoints
2025-03-06 20:13:46,551 - INFO - training batch 601, loss: 0.293, 19232/60000 datapoints
2025-03-06 20:13:46,753 - INFO - training batch 651, loss: 0.485, 20832/60000 datapoints
2025-03-06 20:13:46,975 - INFO - training batch 701, loss: 0.119, 22432/60000 datapoints
2025-03-06 20:13:47,184 - INFO - training batch 751, loss: 0.328, 24032/60000 datapoints
2025-03-06 20:13:47,381 - INFO - training batch 801, loss: 0.097, 25632/60000 datapoints
2025-03-06 20:13:47,580 - INFO - training batch 851, loss: 0.256, 27232/60000 datapoints
2025-03-06 20:13:47,782 - INFO - training batch 901, loss: 0.204, 28832/60000 datapoints
2025-03-06 20:13:47,981 - INFO - training batch 951, loss: 0.204, 30432/60000 datapoints
2025-03-06 20:13:48,180 - INFO - training batch 1001, loss: 0.205, 32032/60000 datapoints
2025-03-06 20:13:48,383 - INFO - training batch 1051, loss: 0.176, 33632/60000 datapoints
2025-03-06 20:13:48,581 - INFO - training batch 1101, loss: 0.161, 35232/60000 datapoints
2025-03-06 20:13:48,785 - INFO - training batch 1151, loss: 0.084, 36832/60000 datapoints
2025-03-06 20:13:48,986 - INFO - training batch 1201, loss: 0.209, 38432/60000 datapoints
2025-03-06 20:13:49,185 - INFO - training batch 1251, loss: 0.232, 40032/60000 datapoints
2025-03-06 20:13:49,383 - INFO - training batch 1301, loss: 0.137, 41632/60000 datapoints
2025-03-06 20:13:49,581 - INFO - training batch 1351, loss: 0.476, 43232/60000 datapoints
2025-03-06 20:13:49,786 - INFO - training batch 1401, loss: 0.181, 44832/60000 datapoints
2025-03-06 20:13:50,005 - INFO - training batch 1451, loss: 0.058, 46432/60000 datapoints
2025-03-06 20:13:50,209 - INFO - training batch 1501, loss: 0.260, 48032/60000 datapoints
2025-03-06 20:13:50,410 - INFO - training batch 1551, loss: 0.186, 49632/60000 datapoints
2025-03-06 20:13:50,621 - INFO - training batch 1601, loss: 0.226, 51232/60000 datapoints
2025-03-06 20:13:50,829 - INFO - training batch 1651, loss: 0.384, 52832/60000 datapoints
2025-03-06 20:13:51,029 - INFO - training batch 1701, loss: 0.118, 54432/60000 datapoints
2025-03-06 20:13:51,248 - INFO - training batch 1751, loss: 0.124, 56032/60000 datapoints
2025-03-06 20:13:51,463 - INFO - training batch 1801, loss: 0.295, 57632/60000 datapoints
2025-03-06 20:13:51,663 - INFO - training batch 1851, loss: 0.302, 59232/60000 datapoints
2025-03-06 20:13:51,768 - INFO - validation batch 1, loss: 0.121, 32/10016 datapoints
2025-03-06 20:13:51,929 - INFO - validation batch 51, loss: 0.117, 1632/10016 datapoints
2025-03-06 20:13:52,091 - INFO - validation batch 101, loss: 0.179, 3232/10016 datapoints
2025-03-06 20:13:52,255 - INFO - validation batch 151, loss: 0.141, 4832/10016 datapoints
2025-03-06 20:13:52,418 - INFO - validation batch 201, loss: 0.243, 6432/10016 datapoints
2025-03-06 20:13:52,576 - INFO - validation batch 251, loss: 0.139, 8032/10016 datapoints
2025-03-06 20:13:52,746 - INFO - validation batch 301, loss: 0.351, 9632/10016 datapoints
2025-03-06 20:13:52,786 - INFO - Epoch 634/800 done.
2025-03-06 20:13:52,786 - INFO - Final validation performance:
Loss: 0.184, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:13:52,786 - INFO - Beginning epoch 635/800
2025-03-06 20:13:52,793 - INFO - training batch 1, loss: 0.156, 32/60000 datapoints
2025-03-06 20:13:53,009 - INFO - training batch 51, loss: 0.164, 1632/60000 datapoints
2025-03-06 20:13:53,207 - INFO - training batch 101, loss: 0.189, 3232/60000 datapoints
2025-03-06 20:13:53,419 - INFO - training batch 151, loss: 0.491, 4832/60000 datapoints
2025-03-06 20:13:53,629 - INFO - training batch 201, loss: 0.241, 6432/60000 datapoints
2025-03-06 20:13:53,833 - INFO - training batch 251, loss: 0.312, 8032/60000 datapoints
2025-03-06 20:13:54,034 - INFO - training batch 301, loss: 0.051, 9632/60000 datapoints
2025-03-06 20:13:54,234 - INFO - training batch 351, loss: 0.250, 11232/60000 datapoints
2025-03-06 20:13:54,438 - INFO - training batch 401, loss: 0.274, 12832/60000 datapoints
2025-03-06 20:13:54,638 - INFO - training batch 451, loss: 0.246, 14432/60000 datapoints
2025-03-06 20:13:54,839 - INFO - training batch 501, loss: 0.587, 16032/60000 datapoints
2025-03-06 20:13:55,044 - INFO - training batch 551, loss: 0.161, 17632/60000 datapoints
2025-03-06 20:13:55,244 - INFO - training batch 601, loss: 0.038, 19232/60000 datapoints
2025-03-06 20:13:55,443 - INFO - training batch 651, loss: 0.285, 20832/60000 datapoints
2025-03-06 20:13:55,644 - INFO - training batch 701, loss: 0.115, 22432/60000 datapoints
2025-03-06 20:13:55,842 - INFO - training batch 751, loss: 0.049, 24032/60000 datapoints
2025-03-06 20:13:56,041 - INFO - training batch 801, loss: 0.193, 25632/60000 datapoints
2025-03-06 20:13:56,239 - INFO - training batch 851, loss: 0.050, 27232/60000 datapoints
2025-03-06 20:13:56,443 - INFO - training batch 901, loss: 0.274, 28832/60000 datapoints
2025-03-06 20:13:56,644 - INFO - training batch 951, loss: 0.233, 30432/60000 datapoints
2025-03-06 20:13:56,844 - INFO - training batch 1001, loss: 0.246, 32032/60000 datapoints
2025-03-06 20:13:57,043 - INFO - training batch 1051, loss: 0.318, 33632/60000 datapoints
2025-03-06 20:13:57,238 - INFO - training batch 1101, loss: 0.373, 35232/60000 datapoints
2025-03-06 20:13:57,438 - INFO - training batch 1151, loss: 0.218, 36832/60000 datapoints
2025-03-06 20:13:57,638 - INFO - training batch 1201, loss: 0.128, 38432/60000 datapoints
2025-03-06 20:13:57,837 - INFO - training batch 1251, loss: 0.133, 40032/60000 datapoints
2025-03-06 20:13:58,039 - INFO - training batch 1301, loss: 0.198, 41632/60000 datapoints
2025-03-06 20:13:58,238 - INFO - training batch 1351, loss: 0.061, 43232/60000 datapoints
2025-03-06 20:13:58,443 - INFO - training batch 1401, loss: 0.091, 44832/60000 datapoints
2025-03-06 20:13:58,643 - INFO - training batch 1451, loss: 0.097, 46432/60000 datapoints
2025-03-06 20:13:58,845 - INFO - training batch 1501, loss: 0.163, 48032/60000 datapoints
2025-03-06 20:13:59,046 - INFO - training batch 1551, loss: 0.036, 49632/60000 datapoints
2025-03-06 20:13:59,244 - INFO - training batch 1601, loss: 0.335, 51232/60000 datapoints
2025-03-06 20:13:59,444 - INFO - training batch 1651, loss: 0.037, 52832/60000 datapoints
2025-03-06 20:13:59,646 - INFO - training batch 1701, loss: 0.185, 54432/60000 datapoints
2025-03-06 20:13:59,848 - INFO - training batch 1751, loss: 0.482, 56032/60000 datapoints
2025-03-06 20:14:00,053 - INFO - training batch 1801, loss: 0.256, 57632/60000 datapoints
2025-03-06 20:14:00,256 - INFO - training batch 1851, loss: 0.203, 59232/60000 datapoints
2025-03-06 20:14:00,362 - INFO - validation batch 1, loss: 0.216, 32/10016 datapoints
2025-03-06 20:14:00,524 - INFO - validation batch 51, loss: 0.169, 1632/10016 datapoints
2025-03-06 20:14:00,693 - INFO - validation batch 101, loss: 0.402, 3232/10016 datapoints
2025-03-06 20:14:00,871 - INFO - validation batch 151, loss: 0.171, 4832/10016 datapoints
2025-03-06 20:14:01,032 - INFO - validation batch 201, loss: 0.337, 6432/10016 datapoints
2025-03-06 20:14:01,190 - INFO - validation batch 251, loss: 0.438, 8032/10016 datapoints
2025-03-06 20:14:01,354 - INFO - validation batch 301, loss: 0.085, 9632/10016 datapoints
2025-03-06 20:14:01,394 - INFO - Epoch 635/800 done.
2025-03-06 20:14:01,394 - INFO - Final validation performance:
Loss: 0.260, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:14:01,395 - INFO - Beginning epoch 636/800
2025-03-06 20:14:01,401 - INFO - training batch 1, loss: 0.138, 32/60000 datapoints
2025-03-06 20:14:01,603 - INFO - training batch 51, loss: 0.235, 1632/60000 datapoints
2025-03-06 20:14:01,807 - INFO - training batch 101, loss: 0.236, 3232/60000 datapoints
2025-03-06 20:14:02,029 - INFO - training batch 151, loss: 0.217, 4832/60000 datapoints
2025-03-06 20:14:02,234 - INFO - training batch 201, loss: 0.395, 6432/60000 datapoints
2025-03-06 20:14:02,447 - INFO - training batch 251, loss: 0.433, 8032/60000 datapoints
2025-03-06 20:14:02,661 - INFO - training batch 301, loss: 0.583, 9632/60000 datapoints
2025-03-06 20:14:02,874 - INFO - training batch 351, loss: 0.234, 11232/60000 datapoints
2025-03-06 20:14:03,081 - INFO - training batch 401, loss: 0.101, 12832/60000 datapoints
2025-03-06 20:14:03,290 - INFO - training batch 451, loss: 0.168, 14432/60000 datapoints
2025-03-06 20:14:03,499 - INFO - training batch 501, loss: 0.453, 16032/60000 datapoints
2025-03-06 20:14:03,715 - INFO - training batch 551, loss: 0.058, 17632/60000 datapoints
2025-03-06 20:14:03,932 - INFO - training batch 601, loss: 0.220, 19232/60000 datapoints
2025-03-06 20:14:04,142 - INFO - training batch 651, loss: 0.232, 20832/60000 datapoints
2025-03-06 20:14:04,349 - INFO - training batch 701, loss: 0.060, 22432/60000 datapoints
2025-03-06 20:14:04,562 - INFO - training batch 751, loss: 0.222, 24032/60000 datapoints
2025-03-06 20:14:04,773 - INFO - training batch 801, loss: 0.134, 25632/60000 datapoints
2025-03-06 20:14:04,990 - INFO - training batch 851, loss: 0.434, 27232/60000 datapoints
2025-03-06 20:14:05,201 - INFO - training batch 901, loss: 0.276, 28832/60000 datapoints
2025-03-06 20:14:05,410 - INFO - training batch 951, loss: 0.125, 30432/60000 datapoints
2025-03-06 20:14:05,616 - INFO - training batch 1001, loss: 0.248, 32032/60000 datapoints
2025-03-06 20:14:05,828 - INFO - training batch 1051, loss: 0.258, 33632/60000 datapoints
2025-03-06 20:14:06,036 - INFO - training batch 1101, loss: 0.241, 35232/60000 datapoints
2025-03-06 20:14:06,246 - INFO - training batch 1151, loss: 0.128, 36832/60000 datapoints
2025-03-06 20:14:06,458 - INFO - training batch 1201, loss: 0.123, 38432/60000 datapoints
2025-03-06 20:14:06,667 - INFO - training batch 1251, loss: 0.412, 40032/60000 datapoints
2025-03-06 20:14:06,875 - INFO - training batch 1301, loss: 0.359, 41632/60000 datapoints
2025-03-06 20:14:07,083 - INFO - training batch 1351, loss: 0.308, 43232/60000 datapoints
2025-03-06 20:14:07,288 - INFO - training batch 1401, loss: 0.182, 44832/60000 datapoints
2025-03-06 20:14:07,503 - INFO - training batch 1451, loss: 0.103, 46432/60000 datapoints
2025-03-06 20:14:07,722 - INFO - training batch 1501, loss: 0.209, 48032/60000 datapoints
2025-03-06 20:14:07,929 - INFO - training batch 1551, loss: 0.321, 49632/60000 datapoints
2025-03-06 20:14:08,133 - INFO - training batch 1601, loss: 0.094, 51232/60000 datapoints
2025-03-06 20:14:08,334 - INFO - training batch 1651, loss: 0.224, 52832/60000 datapoints
2025-03-06 20:14:08,540 - INFO - training batch 1701, loss: 0.128, 54432/60000 datapoints
2025-03-06 20:14:08,747 - INFO - training batch 1751, loss: 0.437, 56032/60000 datapoints
2025-03-06 20:14:08,953 - INFO - training batch 1801, loss: 0.284, 57632/60000 datapoints
2025-03-06 20:14:09,158 - INFO - training batch 1851, loss: 0.318, 59232/60000 datapoints
2025-03-06 20:14:09,269 - INFO - validation batch 1, loss: 0.511, 32/10016 datapoints
2025-03-06 20:14:09,435 - INFO - validation batch 51, loss: 0.212, 1632/10016 datapoints
2025-03-06 20:14:09,597 - INFO - validation batch 101, loss: 0.380, 3232/10016 datapoints
2025-03-06 20:14:09,761 - INFO - validation batch 151, loss: 0.232, 4832/10016 datapoints
2025-03-06 20:14:09,925 - INFO - validation batch 201, loss: 0.304, 6432/10016 datapoints
2025-03-06 20:14:10,089 - INFO - validation batch 251, loss: 0.097, 8032/10016 datapoints
2025-03-06 20:14:10,251 - INFO - validation batch 301, loss: 0.105, 9632/10016 datapoints
2025-03-06 20:14:10,293 - INFO - Epoch 636/800 done.
2025-03-06 20:14:10,293 - INFO - Final validation performance:
Loss: 0.263, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:14:10,294 - INFO - Beginning epoch 637/800
2025-03-06 20:14:10,301 - INFO - training batch 1, loss: 0.148, 32/60000 datapoints
2025-03-06 20:14:10,511 - INFO - training batch 51, loss: 0.201, 1632/60000 datapoints
2025-03-06 20:14:10,716 - INFO - training batch 101, loss: 0.340, 3232/60000 datapoints
2025-03-06 20:14:10,962 - INFO - training batch 151, loss: 0.308, 4832/60000 datapoints
2025-03-06 20:14:11,163 - INFO - training batch 201, loss: 0.183, 6432/60000 datapoints
2025-03-06 20:14:11,378 - INFO - training batch 251, loss: 0.349, 8032/60000 datapoints
2025-03-06 20:14:11,592 - INFO - training batch 301, loss: 0.153, 9632/60000 datapoints
2025-03-06 20:14:11,802 - INFO - training batch 351, loss: 0.227, 11232/60000 datapoints
2025-03-06 20:14:12,010 - INFO - training batch 401, loss: 0.308, 12832/60000 datapoints
2025-03-06 20:14:12,209 - INFO - training batch 451, loss: 0.360, 14432/60000 datapoints
2025-03-06 20:14:12,409 - INFO - training batch 501, loss: 0.349, 16032/60000 datapoints
2025-03-06 20:14:12,612 - INFO - training batch 551, loss: 0.091, 17632/60000 datapoints
2025-03-06 20:14:12,819 - INFO - training batch 601, loss: 0.178, 19232/60000 datapoints
2025-03-06 20:14:13,022 - INFO - training batch 651, loss: 0.149, 20832/60000 datapoints
2025-03-06 20:14:13,223 - INFO - training batch 701, loss: 0.213, 22432/60000 datapoints
2025-03-06 20:14:13,428 - INFO - training batch 751, loss: 0.236, 24032/60000 datapoints
2025-03-06 20:14:13,636 - INFO - training batch 801, loss: 0.299, 25632/60000 datapoints
2025-03-06 20:14:13,841 - INFO - training batch 851, loss: 0.248, 27232/60000 datapoints
2025-03-06 20:14:14,039 - INFO - training batch 901, loss: 0.298, 28832/60000 datapoints
2025-03-06 20:14:14,233 - INFO - training batch 951, loss: 0.170, 30432/60000 datapoints
2025-03-06 20:14:14,428 - INFO - training batch 1001, loss: 0.183, 32032/60000 datapoints
2025-03-06 20:14:14,638 - INFO - training batch 1051, loss: 0.330, 33632/60000 datapoints
2025-03-06 20:14:14,834 - INFO - training batch 1101, loss: 0.189, 35232/60000 datapoints
2025-03-06 20:14:15,040 - INFO - training batch 1151, loss: 0.202, 36832/60000 datapoints
2025-03-06 20:14:15,240 - INFO - training batch 1201, loss: 0.311, 38432/60000 datapoints
2025-03-06 20:14:15,437 - INFO - training batch 1251, loss: 0.155, 40032/60000 datapoints
2025-03-06 20:14:15,635 - INFO - training batch 1301, loss: 0.196, 41632/60000 datapoints
2025-03-06 20:14:15,831 - INFO - training batch 1351, loss: 0.277, 43232/60000 datapoints
2025-03-06 20:14:16,030 - INFO - training batch 1401, loss: 0.135, 44832/60000 datapoints
2025-03-06 20:14:16,226 - INFO - training batch 1451, loss: 0.064, 46432/60000 datapoints
2025-03-06 20:14:16,424 - INFO - training batch 1501, loss: 0.162, 48032/60000 datapoints
2025-03-06 20:14:16,628 - INFO - training batch 1551, loss: 0.090, 49632/60000 datapoints
2025-03-06 20:14:16,825 - INFO - training batch 1601, loss: 0.299, 51232/60000 datapoints
2025-03-06 20:14:17,022 - INFO - training batch 1651, loss: 0.063, 52832/60000 datapoints
2025-03-06 20:14:17,221 - INFO - training batch 1701, loss: 0.173, 54432/60000 datapoints
2025-03-06 20:14:17,416 - INFO - training batch 1751, loss: 0.393, 56032/60000 datapoints
2025-03-06 20:14:17,634 - INFO - training batch 1801, loss: 0.326, 57632/60000 datapoints
2025-03-06 20:14:17,828 - INFO - training batch 1851, loss: 0.195, 59232/60000 datapoints
2025-03-06 20:14:17,935 - INFO - validation batch 1, loss: 0.295, 32/10016 datapoints
2025-03-06 20:14:18,096 - INFO - validation batch 51, loss: 0.169, 1632/10016 datapoints
2025-03-06 20:14:18,257 - INFO - validation batch 101, loss: 0.193, 3232/10016 datapoints
2025-03-06 20:14:18,414 - INFO - validation batch 151, loss: 0.463, 4832/10016 datapoints
2025-03-06 20:14:18,576 - INFO - validation batch 201, loss: 0.074, 6432/10016 datapoints
2025-03-06 20:14:18,737 - INFO - validation batch 251, loss: 0.375, 8032/10016 datapoints
2025-03-06 20:14:18,900 - INFO - validation batch 301, loss: 0.265, 9632/10016 datapoints
2025-03-06 20:14:18,942 - INFO - Epoch 637/800 done.
2025-03-06 20:14:18,942 - INFO - Final validation performance:
Loss: 0.262, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:14:18,943 - INFO - Beginning epoch 638/800
2025-03-06 20:14:18,950 - INFO - training batch 1, loss: 0.121, 32/60000 datapoints
2025-03-06 20:14:19,150 - INFO - training batch 51, loss: 0.075, 1632/60000 datapoints
2025-03-06 20:14:19,346 - INFO - training batch 101, loss: 0.108, 3232/60000 datapoints
2025-03-06 20:14:19,561 - INFO - training batch 151, loss: 0.147, 4832/60000 datapoints
2025-03-06 20:14:19,772 - INFO - training batch 201, loss: 0.174, 6432/60000 datapoints
2025-03-06 20:14:19,978 - INFO - training batch 251, loss: 0.213, 8032/60000 datapoints
2025-03-06 20:14:20,188 - INFO - training batch 301, loss: 0.510, 9632/60000 datapoints
2025-03-06 20:14:20,395 - INFO - training batch 351, loss: 0.231, 11232/60000 datapoints
2025-03-06 20:14:20,605 - INFO - training batch 401, loss: 0.249, 12832/60000 datapoints
2025-03-06 20:14:20,813 - INFO - training batch 451, loss: 0.175, 14432/60000 datapoints
2025-03-06 20:14:21,062 - INFO - training batch 501, loss: 0.266, 16032/60000 datapoints
2025-03-06 20:14:21,288 - INFO - training batch 551, loss: 0.232, 17632/60000 datapoints
2025-03-06 20:14:21,492 - INFO - training batch 601, loss: 0.174, 19232/60000 datapoints
2025-03-06 20:14:21,695 - INFO - training batch 651, loss: 0.428, 20832/60000 datapoints
2025-03-06 20:14:21,897 - INFO - training batch 701, loss: 0.152, 22432/60000 datapoints
2025-03-06 20:14:22,098 - INFO - training batch 751, loss: 0.414, 24032/60000 datapoints
2025-03-06 20:14:22,297 - INFO - training batch 801, loss: 0.159, 25632/60000 datapoints
2025-03-06 20:14:22,498 - INFO - training batch 851, loss: 0.114, 27232/60000 datapoints
2025-03-06 20:14:22,703 - INFO - training batch 901, loss: 0.211, 28832/60000 datapoints
2025-03-06 20:14:22,901 - INFO - training batch 951, loss: 0.164, 30432/60000 datapoints
2025-03-06 20:14:23,099 - INFO - training batch 1001, loss: 0.201, 32032/60000 datapoints
2025-03-06 20:14:23,299 - INFO - training batch 1051, loss: 0.220, 33632/60000 datapoints
2025-03-06 20:14:23,498 - INFO - training batch 1101, loss: 0.057, 35232/60000 datapoints
2025-03-06 20:14:23,711 - INFO - training batch 1151, loss: 0.128, 36832/60000 datapoints
2025-03-06 20:14:23,918 - INFO - training batch 1201, loss: 0.154, 38432/60000 datapoints
2025-03-06 20:14:24,127 - INFO - training batch 1251, loss: 0.062, 40032/60000 datapoints
2025-03-06 20:14:24,333 - INFO - training batch 1301, loss: 0.265, 41632/60000 datapoints
2025-03-06 20:14:24,544 - INFO - training batch 1351, loss: 0.298, 43232/60000 datapoints
2025-03-06 20:14:24,751 - INFO - training batch 1401, loss: 0.218, 44832/60000 datapoints
2025-03-06 20:14:24,966 - INFO - training batch 1451, loss: 0.270, 46432/60000 datapoints
2025-03-06 20:14:25,176 - INFO - training batch 1501, loss: 0.122, 48032/60000 datapoints
2025-03-06 20:14:25,382 - INFO - training batch 1551, loss: 0.061, 49632/60000 datapoints
2025-03-06 20:14:25,589 - INFO - training batch 1601, loss: 0.303, 51232/60000 datapoints
2025-03-06 20:14:25,798 - INFO - training batch 1651, loss: 0.252, 52832/60000 datapoints
2025-03-06 20:14:26,006 - INFO - training batch 1701, loss: 0.298, 54432/60000 datapoints
2025-03-06 20:14:26,213 - INFO - training batch 1751, loss: 0.206, 56032/60000 datapoints
2025-03-06 20:14:26,424 - INFO - training batch 1801, loss: 0.572, 57632/60000 datapoints
2025-03-06 20:14:26,638 - INFO - training batch 1851, loss: 0.377, 59232/60000 datapoints
2025-03-06 20:14:26,749 - INFO - validation batch 1, loss: 0.123, 32/10016 datapoints
2025-03-06 20:14:26,920 - INFO - validation batch 51, loss: 0.113, 1632/10016 datapoints
2025-03-06 20:14:27,087 - INFO - validation batch 101, loss: 0.278, 3232/10016 datapoints
2025-03-06 20:14:27,256 - INFO - validation batch 151, loss: 0.108, 4832/10016 datapoints
2025-03-06 20:14:27,424 - INFO - validation batch 201, loss: 0.653, 6432/10016 datapoints
2025-03-06 20:14:27,590 - INFO - validation batch 251, loss: 0.483, 8032/10016 datapoints
2025-03-06 20:14:27,761 - INFO - validation batch 301, loss: 0.096, 9632/10016 datapoints
2025-03-06 20:14:27,806 - INFO - Epoch 638/800 done.
2025-03-06 20:14:27,806 - INFO - Final validation performance:
Loss: 0.265, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:14:27,807 - INFO - Beginning epoch 639/800
2025-03-06 20:14:27,816 - INFO - training batch 1, loss: 0.050, 32/60000 datapoints
2025-03-06 20:14:28,029 - INFO - training batch 51, loss: 0.440, 1632/60000 datapoints
2025-03-06 20:14:28,240 - INFO - training batch 101, loss: 0.117, 3232/60000 datapoints
2025-03-06 20:14:28,465 - INFO - training batch 151, loss: 0.156, 4832/60000 datapoints
2025-03-06 20:14:28,680 - INFO - training batch 201, loss: 0.367, 6432/60000 datapoints
2025-03-06 20:14:28,892 - INFO - training batch 251, loss: 0.484, 8032/60000 datapoints
2025-03-06 20:14:29,103 - INFO - training batch 301, loss: 0.138, 9632/60000 datapoints
2025-03-06 20:14:29,315 - INFO - training batch 351, loss: 0.386, 11232/60000 datapoints
2025-03-06 20:14:29,526 - INFO - training batch 401, loss: 0.087, 12832/60000 datapoints
2025-03-06 20:14:29,739 - INFO - training batch 451, loss: 0.170, 14432/60000 datapoints
2025-03-06 20:14:29,947 - INFO - training batch 501, loss: 0.149, 16032/60000 datapoints
2025-03-06 20:14:30,155 - INFO - training batch 551, loss: 0.150, 17632/60000 datapoints
2025-03-06 20:14:30,364 - INFO - training batch 601, loss: 0.155, 19232/60000 datapoints
2025-03-06 20:14:30,570 - INFO - training batch 651, loss: 0.276, 20832/60000 datapoints
2025-03-06 20:14:30,775 - INFO - training batch 701, loss: 0.139, 22432/60000 datapoints
2025-03-06 20:14:30,979 - INFO - training batch 751, loss: 0.140, 24032/60000 datapoints
2025-03-06 20:14:31,200 - INFO - training batch 801, loss: 0.104, 25632/60000 datapoints
2025-03-06 20:14:31,402 - INFO - training batch 851, loss: 0.474, 27232/60000 datapoints
2025-03-06 20:14:31,604 - INFO - training batch 901, loss: 0.174, 28832/60000 datapoints
2025-03-06 20:14:31,811 - INFO - training batch 951, loss: 0.158, 30432/60000 datapoints
2025-03-06 20:14:32,013 - INFO - training batch 1001, loss: 0.104, 32032/60000 datapoints
2025-03-06 20:14:32,216 - INFO - training batch 1051, loss: 0.088, 33632/60000 datapoints
2025-03-06 20:14:32,415 - INFO - training batch 1101, loss: 0.297, 35232/60000 datapoints
2025-03-06 20:14:32,627 - INFO - training batch 1151, loss: 0.199, 36832/60000 datapoints
2025-03-06 20:14:32,829 - INFO - training batch 1201, loss: 0.145, 38432/60000 datapoints
2025-03-06 20:14:33,031 - INFO - training batch 1251, loss: 0.324, 40032/60000 datapoints
2025-03-06 20:14:33,236 - INFO - training batch 1301, loss: 0.362, 41632/60000 datapoints
2025-03-06 20:14:33,439 - INFO - training batch 1351, loss: 0.174, 43232/60000 datapoints
2025-03-06 20:14:33,645 - INFO - training batch 1401, loss: 0.076, 44832/60000 datapoints
2025-03-06 20:14:33,857 - INFO - training batch 1451, loss: 0.207, 46432/60000 datapoints
2025-03-06 20:14:34,062 - INFO - training batch 1501, loss: 0.201, 48032/60000 datapoints
2025-03-06 20:14:34,265 - INFO - training batch 1551, loss: 0.386, 49632/60000 datapoints
2025-03-06 20:14:34,462 - INFO - training batch 1601, loss: 0.264, 51232/60000 datapoints
2025-03-06 20:14:34,668 - INFO - training batch 1651, loss: 0.328, 52832/60000 datapoints
2025-03-06 20:14:34,870 - INFO - training batch 1701, loss: 0.079, 54432/60000 datapoints
2025-03-06 20:14:35,076 - INFO - training batch 1751, loss: 0.321, 56032/60000 datapoints
2025-03-06 20:14:35,287 - INFO - training batch 1801, loss: 0.147, 57632/60000 datapoints
2025-03-06 20:14:35,490 - INFO - training batch 1851, loss: 0.426, 59232/60000 datapoints
2025-03-06 20:14:35,597 - INFO - validation batch 1, loss: 0.122, 32/10016 datapoints
2025-03-06 20:14:35,763 - INFO - validation batch 51, loss: 0.181, 1632/10016 datapoints
2025-03-06 20:14:35,925 - INFO - validation batch 101, loss: 0.318, 3232/10016 datapoints
2025-03-06 20:14:36,089 - INFO - validation batch 151, loss: 0.093, 4832/10016 datapoints
2025-03-06 20:14:36,250 - INFO - validation batch 201, loss: 0.302, 6432/10016 datapoints
2025-03-06 20:14:36,446 - INFO - validation batch 251, loss: 0.079, 8032/10016 datapoints
2025-03-06 20:14:36,620 - INFO - validation batch 301, loss: 0.191, 9632/10016 datapoints
2025-03-06 20:14:36,661 - INFO - Epoch 639/800 done.
2025-03-06 20:14:36,661 - INFO - Final validation performance:
Loss: 0.184, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:14:36,662 - INFO - Beginning epoch 640/800
2025-03-06 20:14:36,670 - INFO - training batch 1, loss: 0.449, 32/60000 datapoints
2025-03-06 20:14:36,919 - INFO - training batch 51, loss: 0.174, 1632/60000 datapoints
2025-03-06 20:14:37,122 - INFO - training batch 101, loss: 0.414, 3232/60000 datapoints
2025-03-06 20:14:37,331 - INFO - training batch 151, loss: 0.250, 4832/60000 datapoints
2025-03-06 20:14:37,534 - INFO - training batch 201, loss: 0.403, 6432/60000 datapoints
2025-03-06 20:14:37,759 - INFO - training batch 251, loss: 0.316, 8032/60000 datapoints
2025-03-06 20:14:37,966 - INFO - training batch 301, loss: 0.544, 9632/60000 datapoints
2025-03-06 20:14:38,166 - INFO - training batch 351, loss: 0.179, 11232/60000 datapoints
2025-03-06 20:14:38,368 - INFO - training batch 401, loss: 0.215, 12832/60000 datapoints
2025-03-06 20:14:38,573 - INFO - training batch 451, loss: 0.187, 14432/60000 datapoints
2025-03-06 20:14:38,776 - INFO - training batch 501, loss: 0.130, 16032/60000 datapoints
2025-03-06 20:14:38,975 - INFO - training batch 551, loss: 0.213, 17632/60000 datapoints
2025-03-06 20:14:39,180 - INFO - training batch 601, loss: 0.164, 19232/60000 datapoints
2025-03-06 20:14:39,379 - INFO - training batch 651, loss: 0.226, 20832/60000 datapoints
2025-03-06 20:14:39,582 - INFO - training batch 701, loss: 0.201, 22432/60000 datapoints
2025-03-06 20:14:39,797 - INFO - training batch 751, loss: 0.233, 24032/60000 datapoints
2025-03-06 20:14:39,996 - INFO - training batch 801, loss: 0.147, 25632/60000 datapoints
2025-03-06 20:14:40,197 - INFO - training batch 851, loss: 0.193, 27232/60000 datapoints
2025-03-06 20:14:40,399 - INFO - training batch 901, loss: 0.307, 28832/60000 datapoints
2025-03-06 20:14:40,603 - INFO - training batch 951, loss: 0.163, 30432/60000 datapoints
2025-03-06 20:14:40,806 - INFO - training batch 1001, loss: 0.322, 32032/60000 datapoints
2025-03-06 20:14:41,006 - INFO - training batch 1051, loss: 0.299, 33632/60000 datapoints
2025-03-06 20:14:41,236 - INFO - training batch 1101, loss: 0.525, 35232/60000 datapoints
2025-03-06 20:14:41,442 - INFO - training batch 1151, loss: 0.269, 36832/60000 datapoints
2025-03-06 20:14:41,646 - INFO - training batch 1201, loss: 0.071, 38432/60000 datapoints
2025-03-06 20:14:41,844 - INFO - training batch 1251, loss: 0.255, 40032/60000 datapoints
2025-03-06 20:14:42,045 - INFO - training batch 1301, loss: 0.203, 41632/60000 datapoints
2025-03-06 20:14:42,247 - INFO - training batch 1351, loss: 0.707, 43232/60000 datapoints
2025-03-06 20:14:42,449 - INFO - training batch 1401, loss: 0.362, 44832/60000 datapoints
2025-03-06 20:14:42,655 - INFO - training batch 1451, loss: 0.477, 46432/60000 datapoints
2025-03-06 20:14:42,856 - INFO - training batch 1501, loss: 0.154, 48032/60000 datapoints
2025-03-06 20:14:43,058 - INFO - training batch 1551, loss: 0.480, 49632/60000 datapoints
2025-03-06 20:14:43,269 - INFO - training batch 1601, loss: 0.129, 51232/60000 datapoints
2025-03-06 20:14:43,473 - INFO - training batch 1651, loss: 0.298, 52832/60000 datapoints
2025-03-06 20:14:43,676 - INFO - training batch 1701, loss: 0.123, 54432/60000 datapoints
2025-03-06 20:14:43,883 - INFO - training batch 1751, loss: 0.187, 56032/60000 datapoints
2025-03-06 20:14:44,082 - INFO - training batch 1801, loss: 0.248, 57632/60000 datapoints
2025-03-06 20:14:44,281 - INFO - training batch 1851, loss: 0.150, 59232/60000 datapoints
2025-03-06 20:14:44,391 - INFO - validation batch 1, loss: 0.692, 32/10016 datapoints
2025-03-06 20:14:44,554 - INFO - validation batch 51, loss: 0.068, 1632/10016 datapoints
2025-03-06 20:14:44,719 - INFO - validation batch 101, loss: 0.098, 3232/10016 datapoints
2025-03-06 20:14:44,882 - INFO - validation batch 151, loss: 0.192, 4832/10016 datapoints
2025-03-06 20:14:45,048 - INFO - validation batch 201, loss: 0.430, 6432/10016 datapoints
2025-03-06 20:14:45,210 - INFO - validation batch 251, loss: 0.138, 8032/10016 datapoints
2025-03-06 20:14:45,374 - INFO - validation batch 301, loss: 0.037, 9632/10016 datapoints
2025-03-06 20:14:45,411 - INFO - Epoch 640/800 done.
2025-03-06 20:14:45,411 - INFO - Final validation performance:
Loss: 0.236, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:14:45,413 - INFO - Beginning epoch 641/800
2025-03-06 20:14:45,420 - INFO - training batch 1, loss: 0.238, 32/60000 datapoints
2025-03-06 20:14:45,635 - INFO - training batch 51, loss: 0.245, 1632/60000 datapoints
2025-03-06 20:14:45,854 - INFO - training batch 101, loss: 0.439, 3232/60000 datapoints
2025-03-06 20:14:46,056 - INFO - training batch 151, loss: 0.537, 4832/60000 datapoints
2025-03-06 20:14:46,261 - INFO - training batch 201, loss: 0.068, 6432/60000 datapoints
2025-03-06 20:14:46,471 - INFO - training batch 251, loss: 0.266, 8032/60000 datapoints
2025-03-06 20:14:46,685 - INFO - training batch 301, loss: 0.395, 9632/60000 datapoints
2025-03-06 20:14:46,891 - INFO - training batch 351, loss: 0.324, 11232/60000 datapoints
2025-03-06 20:14:47,091 - INFO - training batch 401, loss: 0.528, 12832/60000 datapoints
2025-03-06 20:14:47,290 - INFO - training batch 451, loss: 0.236, 14432/60000 datapoints
2025-03-06 20:14:47,493 - INFO - training batch 501, loss: 0.208, 16032/60000 datapoints
2025-03-06 20:14:47,802 - INFO - training batch 551, loss: 0.150, 17632/60000 datapoints
2025-03-06 20:14:48,029 - INFO - training batch 601, loss: 0.183, 19232/60000 datapoints
2025-03-06 20:14:48,253 - INFO - training batch 651, loss: 0.129, 20832/60000 datapoints
2025-03-06 20:14:48,454 - INFO - training batch 701, loss: 0.359, 22432/60000 datapoints
2025-03-06 20:14:48,660 - INFO - training batch 751, loss: 0.095, 24032/60000 datapoints
2025-03-06 20:14:48,858 - INFO - training batch 801, loss: 0.084, 25632/60000 datapoints
2025-03-06 20:14:49,059 - INFO - training batch 851, loss: 0.229, 27232/60000 datapoints
2025-03-06 20:14:49,259 - INFO - training batch 901, loss: 0.112, 28832/60000 datapoints
2025-03-06 20:14:49,460 - INFO - training batch 951, loss: 0.330, 30432/60000 datapoints
2025-03-06 20:14:49,664 - INFO - training batch 1001, loss: 0.117, 32032/60000 datapoints
2025-03-06 20:14:49,861 - INFO - training batch 1051, loss: 0.359, 33632/60000 datapoints
2025-03-06 20:14:50,064 - INFO - training batch 1101, loss: 0.066, 35232/60000 datapoints
2025-03-06 20:14:50,272 - INFO - training batch 1151, loss: 0.308, 36832/60000 datapoints
2025-03-06 20:14:50,487 - INFO - training batch 1201, loss: 0.244, 38432/60000 datapoints
2025-03-06 20:14:50,698 - INFO - training batch 1251, loss: 0.234, 40032/60000 datapoints
2025-03-06 20:14:50,906 - INFO - training batch 1301, loss: 0.066, 41632/60000 datapoints
2025-03-06 20:14:51,111 - INFO - training batch 1351, loss: 0.640, 43232/60000 datapoints
2025-03-06 20:14:51,344 - INFO - training batch 1401, loss: 0.081, 44832/60000 datapoints
2025-03-06 20:14:51,552 - INFO - training batch 1451, loss: 0.148, 46432/60000 datapoints
2025-03-06 20:14:51,763 - INFO - training batch 1501, loss: 0.275, 48032/60000 datapoints
2025-03-06 20:14:51,972 - INFO - training batch 1551, loss: 0.167, 49632/60000 datapoints
2025-03-06 20:14:52,207 - INFO - training batch 1601, loss: 0.197, 51232/60000 datapoints
2025-03-06 20:14:52,440 - INFO - training batch 1651, loss: 0.205, 52832/60000 datapoints
2025-03-06 20:14:52,651 - INFO - training batch 1701, loss: 0.238, 54432/60000 datapoints
2025-03-06 20:14:52,859 - INFO - training batch 1751, loss: 0.183, 56032/60000 datapoints
2025-03-06 20:14:53,066 - INFO - training batch 1801, loss: 0.107, 57632/60000 datapoints
2025-03-06 20:14:53,280 - INFO - training batch 1851, loss: 0.400, 59232/60000 datapoints
2025-03-06 20:14:53,389 - INFO - validation batch 1, loss: 0.286, 32/10016 datapoints
2025-03-06 20:14:53,559 - INFO - validation batch 51, loss: 0.238, 1632/10016 datapoints
2025-03-06 20:14:53,731 - INFO - validation batch 101, loss: 0.160, 3232/10016 datapoints
2025-03-06 20:14:53,905 - INFO - validation batch 151, loss: 0.335, 4832/10016 datapoints
2025-03-06 20:14:54,073 - INFO - validation batch 201, loss: 0.624, 6432/10016 datapoints
2025-03-06 20:14:54,238 - INFO - validation batch 251, loss: 0.158, 8032/10016 datapoints
2025-03-06 20:14:54,405 - INFO - validation batch 301, loss: 0.149, 9632/10016 datapoints
2025-03-06 20:14:54,450 - INFO - Epoch 641/800 done.
2025-03-06 20:14:54,450 - INFO - Final validation performance:
Loss: 0.279, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:14:54,451 - INFO - Beginning epoch 642/800
2025-03-06 20:14:54,459 - INFO - training batch 1, loss: 0.112, 32/60000 datapoints
2025-03-06 20:14:54,692 - INFO - training batch 51, loss: 0.061, 1632/60000 datapoints
2025-03-06 20:14:54,906 - INFO - training batch 101, loss: 0.330, 3232/60000 datapoints
2025-03-06 20:14:55,122 - INFO - training batch 151, loss: 0.085, 4832/60000 datapoints
2025-03-06 20:14:55,333 - INFO - training batch 201, loss: 0.514, 6432/60000 datapoints
2025-03-06 20:14:55,545 - INFO - training batch 251, loss: 0.131, 8032/60000 datapoints
2025-03-06 20:14:55,766 - INFO - training batch 301, loss: 0.194, 9632/60000 datapoints
2025-03-06 20:14:55,977 - INFO - training batch 351, loss: 0.070, 11232/60000 datapoints
2025-03-06 20:14:56,187 - INFO - training batch 401, loss: 0.106, 12832/60000 datapoints
2025-03-06 20:14:56,398 - INFO - training batch 451, loss: 0.121, 14432/60000 datapoints
2025-03-06 20:14:56,646 - INFO - training batch 501, loss: 0.060, 16032/60000 datapoints
2025-03-06 20:14:56,864 - INFO - training batch 551, loss: 0.071, 17632/60000 datapoints
2025-03-06 20:14:57,071 - INFO - training batch 601, loss: 0.273, 19232/60000 datapoints
2025-03-06 20:14:57,274 - INFO - training batch 651, loss: 0.291, 20832/60000 datapoints
2025-03-06 20:14:57,480 - INFO - training batch 701, loss: 0.325, 22432/60000 datapoints
2025-03-06 20:14:57,687 - INFO - training batch 751, loss: 0.248, 24032/60000 datapoints
2025-03-06 20:14:57,893 - INFO - training batch 801, loss: 0.148, 25632/60000 datapoints
2025-03-06 20:14:58,104 - INFO - training batch 851, loss: 0.454, 27232/60000 datapoints
2025-03-06 20:14:58,314 - INFO - training batch 901, loss: 0.327, 28832/60000 datapoints
2025-03-06 20:14:58,524 - INFO - training batch 951, loss: 0.158, 30432/60000 datapoints
2025-03-06 20:14:58,740 - INFO - training batch 1001, loss: 0.067, 32032/60000 datapoints
2025-03-06 20:14:58,947 - INFO - training batch 1051, loss: 0.198, 33632/60000 datapoints
2025-03-06 20:14:59,151 - INFO - training batch 1101, loss: 0.392, 35232/60000 datapoints
2025-03-06 20:14:59,350 - INFO - training batch 1151, loss: 0.203, 36832/60000 datapoints
2025-03-06 20:14:59,552 - INFO - training batch 1201, loss: 0.261, 38432/60000 datapoints
2025-03-06 20:14:59,757 - INFO - training batch 1251, loss: 0.210, 40032/60000 datapoints
2025-03-06 20:14:59,955 - INFO - training batch 1301, loss: 0.153, 41632/60000 datapoints
2025-03-06 20:15:00,163 - INFO - training batch 1351, loss: 0.284, 43232/60000 datapoints
2025-03-06 20:15:00,366 - INFO - training batch 1401, loss: 0.105, 44832/60000 datapoints
2025-03-06 20:15:00,568 - INFO - training batch 1451, loss: 0.315, 46432/60000 datapoints
2025-03-06 20:15:00,775 - INFO - training batch 1501, loss: 0.109, 48032/60000 datapoints
2025-03-06 20:15:00,979 - INFO - training batch 1551, loss: 0.240, 49632/60000 datapoints
2025-03-06 20:15:01,179 - INFO - training batch 1601, loss: 0.444, 51232/60000 datapoints
2025-03-06 20:15:01,403 - INFO - training batch 1651, loss: 0.087, 52832/60000 datapoints
2025-03-06 20:15:01,604 - INFO - training batch 1701, loss: 0.108, 54432/60000 datapoints
2025-03-06 20:15:01,809 - INFO - training batch 1751, loss: 0.126, 56032/60000 datapoints
2025-03-06 20:15:02,010 - INFO - training batch 1801, loss: 0.217, 57632/60000 datapoints
2025-03-06 20:15:02,212 - INFO - training batch 1851, loss: 0.296, 59232/60000 datapoints
2025-03-06 20:15:02,318 - INFO - validation batch 1, loss: 0.160, 32/10016 datapoints
2025-03-06 20:15:02,479 - INFO - validation batch 51, loss: 0.303, 1632/10016 datapoints
2025-03-06 20:15:02,650 - INFO - validation batch 101, loss: 0.209, 3232/10016 datapoints
2025-03-06 20:15:02,812 - INFO - validation batch 151, loss: 0.100, 4832/10016 datapoints
2025-03-06 20:15:02,973 - INFO - validation batch 201, loss: 0.161, 6432/10016 datapoints
2025-03-06 20:15:03,135 - INFO - validation batch 251, loss: 0.309, 8032/10016 datapoints
2025-03-06 20:15:03,298 - INFO - validation batch 301, loss: 0.285, 9632/10016 datapoints
2025-03-06 20:15:03,339 - INFO - Epoch 642/800 done.
2025-03-06 20:15:03,339 - INFO - Final validation performance:
Loss: 0.218, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:15:03,340 - INFO - Beginning epoch 643/800
2025-03-06 20:15:03,347 - INFO - training batch 1, loss: 0.544, 32/60000 datapoints
2025-03-06 20:15:03,553 - INFO - training batch 51, loss: 0.234, 1632/60000 datapoints
2025-03-06 20:15:03,760 - INFO - training batch 101, loss: 0.545, 3232/60000 datapoints
2025-03-06 20:15:03,976 - INFO - training batch 151, loss: 0.181, 4832/60000 datapoints
2025-03-06 20:15:04,181 - INFO - training batch 201, loss: 0.251, 6432/60000 datapoints
2025-03-06 20:15:04,393 - INFO - training batch 251, loss: 0.134, 8032/60000 datapoints
2025-03-06 20:15:04,603 - INFO - training batch 301, loss: 0.275, 9632/60000 datapoints
2025-03-06 20:15:04,816 - INFO - training batch 351, loss: 0.189, 11232/60000 datapoints
2025-03-06 20:15:05,019 - INFO - training batch 401, loss: 0.429, 12832/60000 datapoints
2025-03-06 20:15:05,224 - INFO - training batch 451, loss: 0.187, 14432/60000 datapoints
2025-03-06 20:15:05,425 - INFO - training batch 501, loss: 0.197, 16032/60000 datapoints
2025-03-06 20:15:05,629 - INFO - training batch 551, loss: 0.350, 17632/60000 datapoints
2025-03-06 20:15:05,829 - INFO - training batch 601, loss: 0.098, 19232/60000 datapoints
2025-03-06 20:15:06,044 - INFO - training batch 651, loss: 0.154, 20832/60000 datapoints
2025-03-06 20:15:06,254 - INFO - training batch 701, loss: 0.242, 22432/60000 datapoints
2025-03-06 20:15:06,458 - INFO - training batch 751, loss: 0.135, 24032/60000 datapoints
2025-03-06 20:15:06,670 - INFO - training batch 801, loss: 0.295, 25632/60000 datapoints
2025-03-06 20:15:06,873 - INFO - training batch 851, loss: 0.326, 27232/60000 datapoints
2025-03-06 20:15:07,078 - INFO - training batch 901, loss: 0.225, 28832/60000 datapoints
2025-03-06 20:15:07,281 - INFO - training batch 951, loss: 0.253, 30432/60000 datapoints
2025-03-06 20:15:07,501 - INFO - training batch 1001, loss: 0.263, 32032/60000 datapoints
2025-03-06 20:15:07,727 - INFO - training batch 1051, loss: 0.112, 33632/60000 datapoints
2025-03-06 20:15:07,939 - INFO - training batch 1101, loss: 0.086, 35232/60000 datapoints
2025-03-06 20:15:08,145 - INFO - training batch 1151, loss: 0.336, 36832/60000 datapoints
2025-03-06 20:15:08,349 - INFO - training batch 1201, loss: 0.214, 38432/60000 datapoints
2025-03-06 20:15:08,555 - INFO - training batch 1251, loss: 0.183, 40032/60000 datapoints
2025-03-06 20:15:08,761 - INFO - training batch 1301, loss: 0.168, 41632/60000 datapoints
2025-03-06 20:15:08,965 - INFO - training batch 1351, loss: 0.240, 43232/60000 datapoints
2025-03-06 20:15:09,173 - INFO - training batch 1401, loss: 0.132, 44832/60000 datapoints
2025-03-06 20:15:09,376 - INFO - training batch 1451, loss: 0.219, 46432/60000 datapoints
2025-03-06 20:15:09,577 - INFO - training batch 1501, loss: 0.310, 48032/60000 datapoints
2025-03-06 20:15:09,785 - INFO - training batch 1551, loss: 0.043, 49632/60000 datapoints
2025-03-06 20:15:09,986 - INFO - training batch 1601, loss: 0.121, 51232/60000 datapoints
2025-03-06 20:15:10,189 - INFO - training batch 1651, loss: 0.319, 52832/60000 datapoints
2025-03-06 20:15:10,390 - INFO - training batch 1701, loss: 0.349, 54432/60000 datapoints
2025-03-06 20:15:10,589 - INFO - training batch 1751, loss: 0.328, 56032/60000 datapoints
2025-03-06 20:15:10,798 - INFO - training batch 1801, loss: 0.465, 57632/60000 datapoints
2025-03-06 20:15:11,021 - INFO - training batch 1851, loss: 0.043, 59232/60000 datapoints
2025-03-06 20:15:11,137 - INFO - validation batch 1, loss: 0.111, 32/10016 datapoints
2025-03-06 20:15:11,303 - INFO - validation batch 51, loss: 0.114, 1632/10016 datapoints
2025-03-06 20:15:11,485 - INFO - validation batch 101, loss: 0.263, 3232/10016 datapoints
2025-03-06 20:15:11,652 - INFO - validation batch 151, loss: 0.409, 4832/10016 datapoints
2025-03-06 20:15:11,820 - INFO - validation batch 201, loss: 0.320, 6432/10016 datapoints
2025-03-06 20:15:11,983 - INFO - validation batch 251, loss: 0.147, 8032/10016 datapoints
2025-03-06 20:15:12,145 - INFO - validation batch 301, loss: 0.167, 9632/10016 datapoints
2025-03-06 20:15:12,188 - INFO - Epoch 643/800 done.
2025-03-06 20:15:12,188 - INFO - Final validation performance:
Loss: 0.219, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:15:12,189 - INFO - Beginning epoch 644/800
2025-03-06 20:15:12,196 - INFO - training batch 1, loss: 0.258, 32/60000 datapoints
2025-03-06 20:15:12,418 - INFO - training batch 51, loss: 0.373, 1632/60000 datapoints
2025-03-06 20:15:12,621 - INFO - training batch 101, loss: 0.346, 3232/60000 datapoints
2025-03-06 20:15:12,829 - INFO - training batch 151, loss: 0.239, 4832/60000 datapoints
2025-03-06 20:15:13,035 - INFO - training batch 201, loss: 0.155, 6432/60000 datapoints
2025-03-06 20:15:13,246 - INFO - training batch 251, loss: 0.206, 8032/60000 datapoints
2025-03-06 20:15:13,449 - INFO - training batch 301, loss: 0.136, 9632/60000 datapoints
2025-03-06 20:15:13,653 - INFO - training batch 351, loss: 0.358, 11232/60000 datapoints
2025-03-06 20:15:13,862 - INFO - training batch 401, loss: 0.153, 12832/60000 datapoints
2025-03-06 20:15:14,062 - INFO - training batch 451, loss: 0.390, 14432/60000 datapoints
2025-03-06 20:15:14,264 - INFO - training batch 501, loss: 0.281, 16032/60000 datapoints
2025-03-06 20:15:14,470 - INFO - training batch 551, loss: 0.262, 17632/60000 datapoints
2025-03-06 20:15:14,674 - INFO - training batch 601, loss: 0.166, 19232/60000 datapoints
2025-03-06 20:15:14,881 - INFO - training batch 651, loss: 0.121, 20832/60000 datapoints
2025-03-06 20:15:15,085 - INFO - training batch 701, loss: 0.190, 22432/60000 datapoints
2025-03-06 20:15:15,290 - INFO - training batch 751, loss: 0.043, 24032/60000 datapoints
2025-03-06 20:15:15,490 - INFO - training batch 801, loss: 0.362, 25632/60000 datapoints
2025-03-06 20:15:15,692 - INFO - training batch 851, loss: 0.343, 27232/60000 datapoints
2025-03-06 20:15:15,900 - INFO - training batch 901, loss: 0.256, 28832/60000 datapoints
2025-03-06 20:15:16,107 - INFO - training batch 951, loss: 0.270, 30432/60000 datapoints
2025-03-06 20:15:16,317 - INFO - training batch 1001, loss: 0.254, 32032/60000 datapoints
2025-03-06 20:15:16,523 - INFO - training batch 1051, loss: 0.160, 33632/60000 datapoints
2025-03-06 20:15:16,739 - INFO - training batch 1101, loss: 0.160, 35232/60000 datapoints
2025-03-06 20:15:16,946 - INFO - training batch 1151, loss: 0.100, 36832/60000 datapoints
2025-03-06 20:15:17,157 - INFO - training batch 1201, loss: 0.182, 38432/60000 datapoints
2025-03-06 20:15:17,363 - INFO - training batch 1251, loss: 0.463, 40032/60000 datapoints
2025-03-06 20:15:17,568 - INFO - training batch 1301, loss: 0.313, 41632/60000 datapoints
2025-03-06 20:15:17,778 - INFO - training batch 1351, loss: 0.211, 43232/60000 datapoints
2025-03-06 20:15:17,984 - INFO - training batch 1401, loss: 0.085, 44832/60000 datapoints
2025-03-06 20:15:18,189 - INFO - training batch 1451, loss: 0.419, 46432/60000 datapoints
2025-03-06 20:15:18,397 - INFO - training batch 1501, loss: 0.353, 48032/60000 datapoints
2025-03-06 20:15:18,602 - INFO - training batch 1551, loss: 0.065, 49632/60000 datapoints
2025-03-06 20:15:18,816 - INFO - training batch 1601, loss: 0.165, 51232/60000 datapoints
2025-03-06 20:15:19,025 - INFO - training batch 1651, loss: 0.344, 52832/60000 datapoints
2025-03-06 20:15:19,230 - INFO - training batch 1701, loss: 0.176, 54432/60000 datapoints
2025-03-06 20:15:19,434 - INFO - training batch 1751, loss: 0.387, 56032/60000 datapoints
2025-03-06 20:15:19,639 - INFO - training batch 1801, loss: 0.091, 57632/60000 datapoints
2025-03-06 20:15:19,845 - INFO - training batch 1851, loss: 0.202, 59232/60000 datapoints
2025-03-06 20:15:19,955 - INFO - validation batch 1, loss: 0.117, 32/10016 datapoints
2025-03-06 20:15:20,126 - INFO - validation batch 51, loss: 0.120, 1632/10016 datapoints
2025-03-06 20:15:20,295 - INFO - validation batch 101, loss: 0.337, 3232/10016 datapoints
2025-03-06 20:15:20,463 - INFO - validation batch 151, loss: 0.748, 4832/10016 datapoints
2025-03-06 20:15:20,633 - INFO - validation batch 201, loss: 0.171, 6432/10016 datapoints
2025-03-06 20:15:20,804 - INFO - validation batch 251, loss: 0.323, 8032/10016 datapoints
2025-03-06 20:15:20,973 - INFO - validation batch 301, loss: 0.317, 9632/10016 datapoints
2025-03-06 20:15:21,013 - INFO - Epoch 644/800 done.
2025-03-06 20:15:21,013 - INFO - Final validation performance:
Loss: 0.305, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:15:21,014 - INFO - Beginning epoch 645/800
2025-03-06 20:15:21,022 - INFO - training batch 1, loss: 0.203, 32/60000 datapoints
2025-03-06 20:15:21,260 - INFO - training batch 51, loss: 0.109, 1632/60000 datapoints
2025-03-06 20:15:21,488 - INFO - training batch 101, loss: 0.166, 3232/60000 datapoints
2025-03-06 20:15:21,714 - INFO - training batch 151, loss: 0.201, 4832/60000 datapoints
2025-03-06 20:15:21,925 - INFO - training batch 201, loss: 0.169, 6432/60000 datapoints
2025-03-06 20:15:22,136 - INFO - training batch 251, loss: 0.340, 8032/60000 datapoints
2025-03-06 20:15:22,344 - INFO - training batch 301, loss: 0.094, 9632/60000 datapoints
2025-03-06 20:15:22,553 - INFO - training batch 351, loss: 0.170, 11232/60000 datapoints
2025-03-06 20:15:22,760 - INFO - training batch 401, loss: 0.373, 12832/60000 datapoints
2025-03-06 20:15:22,967 - INFO - training batch 451, loss: 0.138, 14432/60000 datapoints
2025-03-06 20:15:23,168 - INFO - training batch 501, loss: 0.447, 16032/60000 datapoints
2025-03-06 20:15:23,375 - INFO - training batch 551, loss: 0.261, 17632/60000 datapoints
2025-03-06 20:15:23,574 - INFO - training batch 601, loss: 0.170, 19232/60000 datapoints
2025-03-06 20:15:23,783 - INFO - training batch 651, loss: 0.188, 20832/60000 datapoints
2025-03-06 20:15:23,990 - INFO - training batch 701, loss: 0.273, 22432/60000 datapoints
2025-03-06 20:15:24,190 - INFO - training batch 751, loss: 0.596, 24032/60000 datapoints
2025-03-06 20:15:24,391 - INFO - training batch 801, loss: 0.114, 25632/60000 datapoints
2025-03-06 20:15:24,591 - INFO - training batch 851, loss: 0.186, 27232/60000 datapoints
2025-03-06 20:15:24,797 - INFO - training batch 901, loss: 0.101, 28832/60000 datapoints
2025-03-06 20:15:25,006 - INFO - training batch 951, loss: 0.100, 30432/60000 datapoints
2025-03-06 20:15:25,207 - INFO - training batch 1001, loss: 0.355, 32032/60000 datapoints
2025-03-06 20:15:25,409 - INFO - training batch 1051, loss: 0.273, 33632/60000 datapoints
2025-03-06 20:15:25,611 - INFO - training batch 1101, loss: 0.069, 35232/60000 datapoints
2025-03-06 20:15:25,815 - INFO - training batch 1151, loss: 0.372, 36832/60000 datapoints
2025-03-06 20:15:26,018 - INFO - training batch 1201, loss: 0.177, 38432/60000 datapoints
2025-03-06 20:15:26,220 - INFO - training batch 1251, loss: 0.285, 40032/60000 datapoints
2025-03-06 20:15:26,423 - INFO - training batch 1301, loss: 0.181, 41632/60000 datapoints
2025-03-06 20:15:26,624 - INFO - training batch 1351, loss: 0.398, 43232/60000 datapoints
2025-03-06 20:15:26,828 - INFO - training batch 1401, loss: 0.147, 44832/60000 datapoints
2025-03-06 20:15:27,030 - INFO - training batch 1451, loss: 0.161, 46432/60000 datapoints
2025-03-06 20:15:27,229 - INFO - training batch 1501, loss: 0.083, 48032/60000 datapoints
2025-03-06 20:15:27,429 - INFO - training batch 1551, loss: 0.332, 49632/60000 datapoints
2025-03-06 20:15:27,630 - INFO - training batch 1601, loss: 0.179, 51232/60000 datapoints
2025-03-06 20:15:27,831 - INFO - training batch 1651, loss: 0.129, 52832/60000 datapoints
2025-03-06 20:15:28,032 - INFO - training batch 1701, loss: 0.309, 54432/60000 datapoints
2025-03-06 20:15:28,230 - INFO - training batch 1751, loss: 0.297, 56032/60000 datapoints
2025-03-06 20:15:28,436 - INFO - training batch 1801, loss: 0.231, 57632/60000 datapoints
2025-03-06 20:15:28,639 - INFO - training batch 1851, loss: 0.200, 59232/60000 datapoints
2025-03-06 20:15:28,746 - INFO - validation batch 1, loss: 0.250, 32/10016 datapoints
2025-03-06 20:15:28,915 - INFO - validation batch 51, loss: 0.200, 1632/10016 datapoints
2025-03-06 20:15:29,076 - INFO - validation batch 101, loss: 0.211, 3232/10016 datapoints
2025-03-06 20:15:29,238 - INFO - validation batch 151, loss: 0.161, 4832/10016 datapoints
2025-03-06 20:15:29,403 - INFO - validation batch 201, loss: 0.120, 6432/10016 datapoints
2025-03-06 20:15:29,568 - INFO - validation batch 251, loss: 0.112, 8032/10016 datapoints
2025-03-06 20:15:29,748 - INFO - validation batch 301, loss: 0.163, 9632/10016 datapoints
2025-03-06 20:15:29,801 - INFO - Epoch 645/800 done.
2025-03-06 20:15:29,801 - INFO - Final validation performance:
Loss: 0.174, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:15:29,802 - INFO - Beginning epoch 646/800
2025-03-06 20:15:29,809 - INFO - training batch 1, loss: 0.261, 32/60000 datapoints
2025-03-06 20:15:30,053 - INFO - training batch 51, loss: 0.312, 1632/60000 datapoints
2025-03-06 20:15:30,282 - INFO - training batch 101, loss: 0.197, 3232/60000 datapoints
2025-03-06 20:15:30,486 - INFO - training batch 151, loss: 0.161, 4832/60000 datapoints
2025-03-06 20:15:30,692 - INFO - training batch 201, loss: 0.102, 6432/60000 datapoints
2025-03-06 20:15:30,898 - INFO - training batch 251, loss: 0.103, 8032/60000 datapoints
2025-03-06 20:15:31,095 - INFO - training batch 301, loss: 0.248, 9632/60000 datapoints
2025-03-06 20:15:31,294 - INFO - training batch 351, loss: 0.213, 11232/60000 datapoints
2025-03-06 20:15:31,495 - INFO - training batch 401, loss: 0.145, 12832/60000 datapoints
2025-03-06 20:15:31,719 - INFO - training batch 451, loss: 0.316, 14432/60000 datapoints
2025-03-06 20:15:31,918 - INFO - training batch 501, loss: 0.355, 16032/60000 datapoints
2025-03-06 20:15:32,119 - INFO - training batch 551, loss: 0.231, 17632/60000 datapoints
2025-03-06 20:15:32,318 - INFO - training batch 601, loss: 0.227, 19232/60000 datapoints
2025-03-06 20:15:32,518 - INFO - training batch 651, loss: 0.361, 20832/60000 datapoints
2025-03-06 20:15:32,720 - INFO - training batch 701, loss: 0.280, 22432/60000 datapoints
2025-03-06 20:15:32,925 - INFO - training batch 751, loss: 0.259, 24032/60000 datapoints
2025-03-06 20:15:33,125 - INFO - training batch 801, loss: 0.356, 25632/60000 datapoints
2025-03-06 20:15:33,328 - INFO - training batch 851, loss: 0.305, 27232/60000 datapoints
2025-03-06 20:15:33,528 - INFO - training batch 901, loss: 0.093, 28832/60000 datapoints
2025-03-06 20:15:33,726 - INFO - training batch 951, loss: 0.165, 30432/60000 datapoints
2025-03-06 20:15:33,932 - INFO - training batch 1001, loss: 0.164, 32032/60000 datapoints
2025-03-06 20:15:34,130 - INFO - training batch 1051, loss: 0.157, 33632/60000 datapoints
2025-03-06 20:15:34,329 - INFO - training batch 1101, loss: 0.137, 35232/60000 datapoints
2025-03-06 20:15:34,526 - INFO - training batch 1151, loss: 0.126, 36832/60000 datapoints
2025-03-06 20:15:34,729 - INFO - training batch 1201, loss: 0.100, 38432/60000 datapoints
2025-03-06 20:15:34,939 - INFO - training batch 1251, loss: 0.166, 40032/60000 datapoints
2025-03-06 20:15:35,144 - INFO - training batch 1301, loss: 0.118, 41632/60000 datapoints
2025-03-06 20:15:35,349 - INFO - training batch 1351, loss: 0.375, 43232/60000 datapoints
2025-03-06 20:15:35,569 - INFO - training batch 1401, loss: 0.101, 44832/60000 datapoints
2025-03-06 20:15:35,779 - INFO - training batch 1451, loss: 0.193, 46432/60000 datapoints
2025-03-06 20:15:35,984 - INFO - training batch 1501, loss: 0.134, 48032/60000 datapoints
2025-03-06 20:15:36,194 - INFO - training batch 1551, loss: 0.186, 49632/60000 datapoints
2025-03-06 20:15:36,401 - INFO - training batch 1601, loss: 0.120, 51232/60000 datapoints
2025-03-06 20:15:36,632 - INFO - training batch 1651, loss: 0.412, 52832/60000 datapoints
2025-03-06 20:15:36,875 - INFO - training batch 1701, loss: 0.082, 54432/60000 datapoints
2025-03-06 20:15:37,081 - INFO - training batch 1751, loss: 0.394, 56032/60000 datapoints
2025-03-06 20:15:37,287 - INFO - training batch 1801, loss: 0.128, 57632/60000 datapoints
2025-03-06 20:15:37,492 - INFO - training batch 1851, loss: 0.084, 59232/60000 datapoints
2025-03-06 20:15:37,602 - INFO - validation batch 1, loss: 0.150, 32/10016 datapoints
2025-03-06 20:15:37,798 - INFO - validation batch 51, loss: 0.109, 1632/10016 datapoints
2025-03-06 20:15:37,977 - INFO - validation batch 101, loss: 0.115, 3232/10016 datapoints
2025-03-06 20:15:38,147 - INFO - validation batch 151, loss: 0.390, 4832/10016 datapoints
2025-03-06 20:15:38,317 - INFO - validation batch 201, loss: 0.083, 6432/10016 datapoints
2025-03-06 20:15:38,486 - INFO - validation batch 251, loss: 0.111, 8032/10016 datapoints
2025-03-06 20:15:38,656 - INFO - validation batch 301, loss: 0.224, 9632/10016 datapoints
2025-03-06 20:15:38,699 - INFO - Epoch 646/800 done.
2025-03-06 20:15:38,700 - INFO - Final validation performance:
Loss: 0.169, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:15:38,700 - INFO - Beginning epoch 647/800
2025-03-06 20:15:38,707 - INFO - training batch 1, loss: 0.134, 32/60000 datapoints
2025-03-06 20:15:38,948 - INFO - training batch 51, loss: 0.307, 1632/60000 datapoints
2025-03-06 20:15:39,169 - INFO - training batch 101, loss: 0.358, 3232/60000 datapoints
2025-03-06 20:15:39,388 - INFO - training batch 151, loss: 0.137, 4832/60000 datapoints
2025-03-06 20:15:39,597 - INFO - training batch 201, loss: 0.190, 6432/60000 datapoints
2025-03-06 20:15:39,810 - INFO - training batch 251, loss: 0.281, 8032/60000 datapoints
2025-03-06 20:15:40,019 - INFO - training batch 301, loss: 0.045, 9632/60000 datapoints
2025-03-06 20:15:40,227 - INFO - training batch 351, loss: 0.190, 11232/60000 datapoints
2025-03-06 20:15:40,435 - INFO - training batch 401, loss: 0.341, 12832/60000 datapoints
2025-03-06 20:15:40,647 - INFO - training batch 451, loss: 0.168, 14432/60000 datapoints
2025-03-06 20:15:40,859 - INFO - training batch 501, loss: 0.066, 16032/60000 datapoints
2025-03-06 20:15:41,071 - INFO - training batch 551, loss: 0.065, 17632/60000 datapoints
2025-03-06 20:15:41,277 - INFO - training batch 601, loss: 0.062, 19232/60000 datapoints
2025-03-06 20:15:41,485 - INFO - training batch 651, loss: 0.332, 20832/60000 datapoints
2025-03-06 20:15:41,722 - INFO - training batch 701, loss: 0.157, 22432/60000 datapoints
2025-03-06 20:15:41,933 - INFO - training batch 751, loss: 0.263, 24032/60000 datapoints
2025-03-06 20:15:42,140 - INFO - training batch 801, loss: 0.130, 25632/60000 datapoints
2025-03-06 20:15:42,350 - INFO - training batch 851, loss: 0.261, 27232/60000 datapoints
2025-03-06 20:15:42,558 - INFO - training batch 901, loss: 0.263, 28832/60000 datapoints
2025-03-06 20:15:42,765 - INFO - training batch 951, loss: 0.050, 30432/60000 datapoints
2025-03-06 20:15:42,977 - INFO - training batch 1001, loss: 0.303, 32032/60000 datapoints
2025-03-06 20:15:43,183 - INFO - training batch 1051, loss: 0.103, 33632/60000 datapoints
2025-03-06 20:15:43,393 - INFO - training batch 1101, loss: 0.315, 35232/60000 datapoints
2025-03-06 20:15:43,602 - INFO - training batch 1151, loss: 0.133, 36832/60000 datapoints
2025-03-06 20:15:43,811 - INFO - training batch 1201, loss: 0.275, 38432/60000 datapoints
2025-03-06 20:15:44,024 - INFO - training batch 1251, loss: 0.256, 40032/60000 datapoints
2025-03-06 20:15:44,230 - INFO - training batch 1301, loss: 0.134, 41632/60000 datapoints
2025-03-06 20:15:44,438 - INFO - training batch 1351, loss: 0.265, 43232/60000 datapoints
2025-03-06 20:15:44,649 - INFO - training batch 1401, loss: 0.233, 44832/60000 datapoints
2025-03-06 20:15:44,857 - INFO - training batch 1451, loss: 0.232, 46432/60000 datapoints
2025-03-06 20:15:45,071 - INFO - training batch 1501, loss: 0.145, 48032/60000 datapoints
2025-03-06 20:15:45,281 - INFO - training batch 1551, loss: 0.197, 49632/60000 datapoints
2025-03-06 20:15:45,487 - INFO - training batch 1601, loss: 0.240, 51232/60000 datapoints
2025-03-06 20:15:45,700 - INFO - training batch 1651, loss: 0.264, 52832/60000 datapoints
2025-03-06 20:15:45,903 - INFO - training batch 1701, loss: 0.142, 54432/60000 datapoints
2025-03-06 20:15:46,111 - INFO - training batch 1751, loss: 0.076, 56032/60000 datapoints
2025-03-06 20:15:46,312 - INFO - training batch 1801, loss: 0.111, 57632/60000 datapoints
2025-03-06 20:15:46,514 - INFO - training batch 1851, loss: 0.111, 59232/60000 datapoints
2025-03-06 20:15:46,622 - INFO - validation batch 1, loss: 0.143, 32/10016 datapoints
2025-03-06 20:15:46,787 - INFO - validation batch 51, loss: 0.366, 1632/10016 datapoints
2025-03-06 20:15:46,955 - INFO - validation batch 101, loss: 0.237, 3232/10016 datapoints
2025-03-06 20:15:47,117 - INFO - validation batch 151, loss: 0.223, 4832/10016 datapoints
2025-03-06 20:15:47,280 - INFO - validation batch 201, loss: 0.216, 6432/10016 datapoints
2025-03-06 20:15:47,443 - INFO - validation batch 251, loss: 0.135, 8032/10016 datapoints
2025-03-06 20:15:47,607 - INFO - validation batch 301, loss: 0.096, 9632/10016 datapoints
2025-03-06 20:15:47,652 - INFO - Epoch 647/800 done.
2025-03-06 20:15:47,652 - INFO - Final validation performance:
Loss: 0.202, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:15:47,653 - INFO - Beginning epoch 648/800
2025-03-06 20:15:47,661 - INFO - training batch 1, loss: 0.288, 32/60000 datapoints
2025-03-06 20:15:47,880 - INFO - training batch 51, loss: 0.076, 1632/60000 datapoints
2025-03-06 20:15:48,083 - INFO - training batch 101, loss: 0.447, 3232/60000 datapoints
2025-03-06 20:15:48,292 - INFO - training batch 151, loss: 0.287, 4832/60000 datapoints
2025-03-06 20:15:48,501 - INFO - training batch 201, loss: 0.311, 6432/60000 datapoints
2025-03-06 20:15:48,710 - INFO - training batch 251, loss: 0.349, 8032/60000 datapoints
2025-03-06 20:15:48,912 - INFO - training batch 301, loss: 0.132, 9632/60000 datapoints
2025-03-06 20:15:49,109 - INFO - training batch 351, loss: 0.095, 11232/60000 datapoints
2025-03-06 20:15:49,318 - INFO - training batch 401, loss: 0.248, 12832/60000 datapoints
2025-03-06 20:15:49,524 - INFO - training batch 451, loss: 0.261, 14432/60000 datapoints
2025-03-06 20:15:49,729 - INFO - training batch 501, loss: 0.224, 16032/60000 datapoints
2025-03-06 20:15:49,929 - INFO - training batch 551, loss: 0.175, 17632/60000 datapoints
2025-03-06 20:15:50,133 - INFO - training batch 601, loss: 0.296, 19232/60000 datapoints
2025-03-06 20:15:50,336 - INFO - training batch 651, loss: 0.110, 20832/60000 datapoints
2025-03-06 20:15:50,536 - INFO - training batch 701, loss: 0.161, 22432/60000 datapoints
2025-03-06 20:15:50,745 - INFO - training batch 751, loss: 0.607, 24032/60000 datapoints
2025-03-06 20:15:50,948 - INFO - training batch 801, loss: 0.406, 25632/60000 datapoints
2025-03-06 20:15:51,151 - INFO - training batch 851, loss: 0.182, 27232/60000 datapoints
2025-03-06 20:15:51,352 - INFO - training batch 901, loss: 0.079, 28832/60000 datapoints
2025-03-06 20:15:51,555 - INFO - training batch 951, loss: 0.180, 30432/60000 datapoints
2025-03-06 20:15:51,777 - INFO - training batch 1001, loss: 0.359, 32032/60000 datapoints
2025-03-06 20:15:51,989 - INFO - training batch 1051, loss: 0.275, 33632/60000 datapoints
2025-03-06 20:15:52,190 - INFO - training batch 1101, loss: 0.229, 35232/60000 datapoints
2025-03-06 20:15:52,394 - INFO - training batch 1151, loss: 0.339, 36832/60000 datapoints
2025-03-06 20:15:52,596 - INFO - training batch 1201, loss: 0.235, 38432/60000 datapoints
2025-03-06 20:15:52,798 - INFO - training batch 1251, loss: 0.323, 40032/60000 datapoints
2025-03-06 20:15:53,000 - INFO - training batch 1301, loss: 0.107, 41632/60000 datapoints
2025-03-06 20:15:53,202 - INFO - training batch 1351, loss: 0.242, 43232/60000 datapoints
2025-03-06 20:15:53,404 - INFO - training batch 1401, loss: 0.168, 44832/60000 datapoints
2025-03-06 20:15:53,603 - INFO - training batch 1451, loss: 0.190, 46432/60000 datapoints
2025-03-06 20:15:53,807 - INFO - training batch 1501, loss: 0.175, 48032/60000 datapoints
2025-03-06 20:15:54,013 - INFO - training batch 1551, loss: 0.330, 49632/60000 datapoints
2025-03-06 20:15:54,212 - INFO - training batch 1601, loss: 0.118, 51232/60000 datapoints
2025-03-06 20:15:54,420 - INFO - training batch 1651, loss: 0.113, 52832/60000 datapoints
2025-03-06 20:15:54,621 - INFO - training batch 1701, loss: 0.189, 54432/60000 datapoints
2025-03-06 20:15:54,824 - INFO - training batch 1751, loss: 0.383, 56032/60000 datapoints
2025-03-06 20:15:55,032 - INFO - training batch 1801, loss: 0.111, 57632/60000 datapoints
2025-03-06 20:15:55,238 - INFO - training batch 1851, loss: 0.195, 59232/60000 datapoints
2025-03-06 20:15:55,344 - INFO - validation batch 1, loss: 0.342, 32/10016 datapoints
2025-03-06 20:15:55,506 - INFO - validation batch 51, loss: 0.282, 1632/10016 datapoints
2025-03-06 20:15:55,676 - INFO - validation batch 101, loss: 0.151, 3232/10016 datapoints
2025-03-06 20:15:55,838 - INFO - validation batch 151, loss: 0.181, 4832/10016 datapoints
2025-03-06 20:15:56,000 - INFO - validation batch 201, loss: 0.320, 6432/10016 datapoints
2025-03-06 20:15:56,161 - INFO - validation batch 251, loss: 0.361, 8032/10016 datapoints
2025-03-06 20:15:56,326 - INFO - validation batch 301, loss: 0.199, 9632/10016 datapoints
2025-03-06 20:15:56,365 - INFO - Epoch 648/800 done.
2025-03-06 20:15:56,366 - INFO - Final validation performance:
Loss: 0.262, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:15:56,366 - INFO - Beginning epoch 649/800
2025-03-06 20:15:56,377 - INFO - training batch 1, loss: 0.260, 32/60000 datapoints
2025-03-06 20:15:56,601 - INFO - training batch 51, loss: 0.094, 1632/60000 datapoints
2025-03-06 20:15:56,817 - INFO - training batch 101, loss: 0.129, 3232/60000 datapoints
2025-03-06 20:15:57,042 - INFO - training batch 151, loss: 0.635, 4832/60000 datapoints
2025-03-06 20:15:57,246 - INFO - training batch 201, loss: 0.254, 6432/60000 datapoints
2025-03-06 20:15:57,449 - INFO - training batch 251, loss: 0.205, 8032/60000 datapoints
2025-03-06 20:15:57,650 - INFO - training batch 301, loss: 0.253, 9632/60000 datapoints
2025-03-06 20:15:57,849 - INFO - training batch 351, loss: 0.445, 11232/60000 datapoints
2025-03-06 20:15:58,052 - INFO - training batch 401, loss: 0.656, 12832/60000 datapoints
2025-03-06 20:15:58,252 - INFO - training batch 451, loss: 0.091, 14432/60000 datapoints
2025-03-06 20:15:58,454 - INFO - training batch 501, loss: 0.378, 16032/60000 datapoints
2025-03-06 20:15:58,657 - INFO - training batch 551, loss: 0.252, 17632/60000 datapoints
2025-03-06 20:15:58,857 - INFO - training batch 601, loss: 0.250, 19232/60000 datapoints
2025-03-06 20:15:59,056 - INFO - training batch 651, loss: 0.241, 20832/60000 datapoints
2025-03-06 20:15:59,254 - INFO - training batch 701, loss: 0.194, 22432/60000 datapoints
2025-03-06 20:15:59,453 - INFO - training batch 751, loss: 0.166, 24032/60000 datapoints
2025-03-06 20:15:59,656 - INFO - training batch 801, loss: 0.185, 25632/60000 datapoints
2025-03-06 20:15:59,856 - INFO - training batch 851, loss: 0.214, 27232/60000 datapoints
2025-03-06 20:16:00,055 - INFO - training batch 901, loss: 0.148, 28832/60000 datapoints
2025-03-06 20:16:00,253 - INFO - training batch 951, loss: 0.187, 30432/60000 datapoints
2025-03-06 20:16:00,460 - INFO - training batch 1001, loss: 0.200, 32032/60000 datapoints
2025-03-06 20:16:00,666 - INFO - training batch 1051, loss: 0.146, 33632/60000 datapoints
2025-03-06 20:16:00,872 - INFO - training batch 1101, loss: 0.214, 35232/60000 datapoints
2025-03-06 20:16:01,080 - INFO - training batch 1151, loss: 0.107, 36832/60000 datapoints
2025-03-06 20:16:01,288 - INFO - training batch 1201, loss: 0.176, 38432/60000 datapoints
2025-03-06 20:16:01,527 - INFO - training batch 1251, loss: 0.153, 40032/60000 datapoints
2025-03-06 20:16:01,754 - INFO - training batch 1301, loss: 0.195, 41632/60000 datapoints
2025-03-06 20:16:01,982 - INFO - training batch 1351, loss: 0.645, 43232/60000 datapoints
2025-03-06 20:16:02,190 - INFO - training batch 1401, loss: 0.109, 44832/60000 datapoints
2025-03-06 20:16:02,396 - INFO - training batch 1451, loss: 0.100, 46432/60000 datapoints
2025-03-06 20:16:02,605 - INFO - training batch 1501, loss: 0.086, 48032/60000 datapoints
2025-03-06 20:16:02,815 - INFO - training batch 1551, loss: 0.194, 49632/60000 datapoints
2025-03-06 20:16:03,029 - INFO - training batch 1601, loss: 0.115, 51232/60000 datapoints
2025-03-06 20:16:03,235 - INFO - training batch 1651, loss: 0.450, 52832/60000 datapoints
2025-03-06 20:16:03,444 - INFO - training batch 1701, loss: 0.118, 54432/60000 datapoints
2025-03-06 20:16:03,656 - INFO - training batch 1751, loss: 0.214, 56032/60000 datapoints
2025-03-06 20:16:03,864 - INFO - training batch 1801, loss: 0.225, 57632/60000 datapoints
2025-03-06 20:16:04,076 - INFO - training batch 1851, loss: 0.133, 59232/60000 datapoints
2025-03-06 20:16:04,187 - INFO - validation batch 1, loss: 0.225, 32/10016 datapoints
2025-03-06 20:16:04,358 - INFO - validation batch 51, loss: 0.161, 1632/10016 datapoints
2025-03-06 20:16:04,529 - INFO - validation batch 101, loss: 0.343, 3232/10016 datapoints
2025-03-06 20:16:04,709 - INFO - validation batch 151, loss: 0.355, 4832/10016 datapoints
2025-03-06 20:16:04,888 - INFO - validation batch 201, loss: 0.502, 6432/10016 datapoints
2025-03-06 20:16:05,059 - INFO - validation batch 251, loss: 0.079, 8032/10016 datapoints
2025-03-06 20:16:05,228 - INFO - validation batch 301, loss: 0.070, 9632/10016 datapoints
2025-03-06 20:16:05,269 - INFO - Epoch 649/800 done.
2025-03-06 20:16:05,269 - INFO - Final validation performance:
Loss: 0.248, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:16:05,270 - INFO - Beginning epoch 650/800
2025-03-06 20:16:05,281 - INFO - training batch 1, loss: 0.109, 32/60000 datapoints
2025-03-06 20:16:05,488 - INFO - training batch 51, loss: 0.148, 1632/60000 datapoints
2025-03-06 20:16:05,697 - INFO - training batch 101, loss: 0.256, 3232/60000 datapoints
2025-03-06 20:16:05,917 - INFO - training batch 151, loss: 0.119, 4832/60000 datapoints
2025-03-06 20:16:06,117 - INFO - training batch 201, loss: 0.386, 6432/60000 datapoints
2025-03-06 20:16:06,328 - INFO - training batch 251, loss: 0.133, 8032/60000 datapoints
2025-03-06 20:16:06,534 - INFO - training batch 301, loss: 0.254, 9632/60000 datapoints
2025-03-06 20:16:06,743 - INFO - training batch 351, loss: 0.151, 11232/60000 datapoints
2025-03-06 20:16:06,952 - INFO - training batch 401, loss: 0.206, 12832/60000 datapoints
2025-03-06 20:16:07,154 - INFO - training batch 451, loss: 0.149, 14432/60000 datapoints
2025-03-06 20:16:07,354 - INFO - training batch 501, loss: 0.374, 16032/60000 datapoints
2025-03-06 20:16:07,553 - INFO - training batch 551, loss: 0.024, 17632/60000 datapoints
2025-03-06 20:16:07,758 - INFO - training batch 601, loss: 0.193, 19232/60000 datapoints
2025-03-06 20:16:07,959 - INFO - training batch 651, loss: 0.335, 20832/60000 datapoints
2025-03-06 20:16:08,158 - INFO - training batch 701, loss: 0.202, 22432/60000 datapoints
2025-03-06 20:16:08,360 - INFO - training batch 751, loss: 0.528, 24032/60000 datapoints
2025-03-06 20:16:08,563 - INFO - training batch 801, loss: 0.165, 25632/60000 datapoints
2025-03-06 20:16:08,763 - INFO - training batch 851, loss: 0.242, 27232/60000 datapoints
2025-03-06 20:16:08,969 - INFO - training batch 901, loss: 0.125, 28832/60000 datapoints
2025-03-06 20:16:09,172 - INFO - training batch 951, loss: 0.387, 30432/60000 datapoints
2025-03-06 20:16:09,372 - INFO - training batch 1001, loss: 0.113, 32032/60000 datapoints
2025-03-06 20:16:09,578 - INFO - training batch 1051, loss: 0.289, 33632/60000 datapoints
2025-03-06 20:16:09,779 - INFO - training batch 1101, loss: 0.278, 35232/60000 datapoints
2025-03-06 20:16:09,986 - INFO - training batch 1151, loss: 0.120, 36832/60000 datapoints
2025-03-06 20:16:10,187 - INFO - training batch 1201, loss: 0.274, 38432/60000 datapoints
2025-03-06 20:16:10,391 - INFO - training batch 1251, loss: 0.366, 40032/60000 datapoints
2025-03-06 20:16:10,591 - INFO - training batch 1301, loss: 0.282, 41632/60000 datapoints
2025-03-06 20:16:10,795 - INFO - training batch 1351, loss: 0.087, 43232/60000 datapoints
2025-03-06 20:16:11,001 - INFO - training batch 1401, loss: 0.163, 44832/60000 datapoints
2025-03-06 20:16:11,198 - INFO - training batch 1451, loss: 0.324, 46432/60000 datapoints
2025-03-06 20:16:11,394 - INFO - training batch 1501, loss: 0.079, 48032/60000 datapoints
2025-03-06 20:16:11,590 - INFO - training batch 1551, loss: 0.129, 49632/60000 datapoints
2025-03-06 20:16:11,789 - INFO - training batch 1601, loss: 0.191, 51232/60000 datapoints
2025-03-06 20:16:12,012 - INFO - training batch 1651, loss: 0.211, 52832/60000 datapoints
2025-03-06 20:16:12,216 - INFO - training batch 1701, loss: 0.116, 54432/60000 datapoints
2025-03-06 20:16:12,411 - INFO - training batch 1751, loss: 0.208, 56032/60000 datapoints
2025-03-06 20:16:12,606 - INFO - training batch 1801, loss: 0.119, 57632/60000 datapoints
2025-03-06 20:16:12,802 - INFO - training batch 1851, loss: 0.122, 59232/60000 datapoints
2025-03-06 20:16:12,903 - INFO - validation batch 1, loss: 0.244, 32/10016 datapoints
2025-03-06 20:16:13,064 - INFO - validation batch 51, loss: 0.109, 1632/10016 datapoints
2025-03-06 20:16:13,219 - INFO - validation batch 101, loss: 0.123, 3232/10016 datapoints
2025-03-06 20:16:13,380 - INFO - validation batch 151, loss: 0.298, 4832/10016 datapoints
2025-03-06 20:16:13,536 - INFO - validation batch 201, loss: 0.270, 6432/10016 datapoints
2025-03-06 20:16:13,696 - INFO - validation batch 251, loss: 0.463, 8032/10016 datapoints
2025-03-06 20:16:13,852 - INFO - validation batch 301, loss: 0.445, 9632/10016 datapoints
2025-03-06 20:16:13,889 - INFO - Epoch 650/800 done.
2025-03-06 20:16:13,889 - INFO - Final validation performance:
Loss: 0.279, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:16:13,890 - INFO - Beginning epoch 651/800
2025-03-06 20:16:13,896 - INFO - training batch 1, loss: 0.261, 32/60000 datapoints
2025-03-06 20:16:14,100 - INFO - training batch 51, loss: 0.071, 1632/60000 datapoints
2025-03-06 20:16:14,305 - INFO - training batch 101, loss: 0.207, 3232/60000 datapoints
2025-03-06 20:16:14,503 - INFO - training batch 151, loss: 0.422, 4832/60000 datapoints
2025-03-06 20:16:14,700 - INFO - training batch 201, loss: 0.092, 6432/60000 datapoints
2025-03-06 20:16:14,905 - INFO - training batch 251, loss: 0.225, 8032/60000 datapoints
2025-03-06 20:16:15,115 - INFO - training batch 301, loss: 0.305, 9632/60000 datapoints
2025-03-06 20:16:15,311 - INFO - training batch 351, loss: 0.264, 11232/60000 datapoints
2025-03-06 20:16:15,507 - INFO - training batch 401, loss: 0.153, 12832/60000 datapoints
2025-03-06 20:16:15,703 - INFO - training batch 451, loss: 0.240, 14432/60000 datapoints
2025-03-06 20:16:15,895 - INFO - training batch 501, loss: 0.122, 16032/60000 datapoints
2025-03-06 20:16:16,087 - INFO - training batch 551, loss: 0.237, 17632/60000 datapoints
2025-03-06 20:16:16,280 - INFO - training batch 601, loss: 0.160, 19232/60000 datapoints
2025-03-06 20:16:16,476 - INFO - training batch 651, loss: 0.219, 20832/60000 datapoints
2025-03-06 20:16:16,670 - INFO - training batch 701, loss: 0.307, 22432/60000 datapoints
2025-03-06 20:16:16,864 - INFO - training batch 751, loss: 0.181, 24032/60000 datapoints
2025-03-06 20:16:17,060 - INFO - training batch 801, loss: 0.302, 25632/60000 datapoints
2025-03-06 20:16:17,254 - INFO - training batch 851, loss: 0.189, 27232/60000 datapoints
2025-03-06 20:16:17,448 - INFO - training batch 901, loss: 0.113, 28832/60000 datapoints
2025-03-06 20:16:17,648 - INFO - training batch 951, loss: 0.185, 30432/60000 datapoints
2025-03-06 20:16:17,842 - INFO - training batch 1001, loss: 0.230, 32032/60000 datapoints
2025-03-06 20:16:18,036 - INFO - training batch 1051, loss: 0.236, 33632/60000 datapoints
2025-03-06 20:16:18,232 - INFO - training batch 1101, loss: 0.030, 35232/60000 datapoints
2025-03-06 20:16:18,425 - INFO - training batch 1151, loss: 0.395, 36832/60000 datapoints
2025-03-06 20:16:18,621 - INFO - training batch 1201, loss: 0.136, 38432/60000 datapoints
2025-03-06 20:16:18,817 - INFO - training batch 1251, loss: 0.076, 40032/60000 datapoints
2025-03-06 20:16:19,015 - INFO - training batch 1301, loss: 0.435, 41632/60000 datapoints
2025-03-06 20:16:19,206 - INFO - training batch 1351, loss: 0.695, 43232/60000 datapoints
2025-03-06 20:16:19,400 - INFO - training batch 1401, loss: 0.778, 44832/60000 datapoints
2025-03-06 20:16:19,595 - INFO - training batch 1451, loss: 0.363, 46432/60000 datapoints
2025-03-06 20:16:19,794 - INFO - training batch 1501, loss: 0.502, 48032/60000 datapoints
2025-03-06 20:16:19,990 - INFO - training batch 1551, loss: 0.177, 49632/60000 datapoints
2025-03-06 20:16:20,187 - INFO - training batch 1601, loss: 0.273, 51232/60000 datapoints
2025-03-06 20:16:20,381 - INFO - training batch 1651, loss: 0.148, 52832/60000 datapoints
2025-03-06 20:16:20,578 - INFO - training batch 1701, loss: 0.310, 54432/60000 datapoints
2025-03-06 20:16:20,777 - INFO - training batch 1751, loss: 0.157, 56032/60000 datapoints
2025-03-06 20:16:20,974 - INFO - training batch 1801, loss: 0.117, 57632/60000 datapoints
2025-03-06 20:16:21,179 - INFO - training batch 1851, loss: 0.646, 59232/60000 datapoints
2025-03-06 20:16:21,289 - INFO - validation batch 1, loss: 0.226, 32/10016 datapoints
2025-03-06 20:16:21,452 - INFO - validation batch 51, loss: 0.303, 1632/10016 datapoints
2025-03-06 20:16:21,618 - INFO - validation batch 101, loss: 0.405, 3232/10016 datapoints
2025-03-06 20:16:21,787 - INFO - validation batch 151, loss: 0.213, 4832/10016 datapoints
2025-03-06 20:16:21,951 - INFO - validation batch 201, loss: 0.400, 6432/10016 datapoints
2025-03-06 20:16:22,139 - INFO - validation batch 251, loss: 0.114, 8032/10016 datapoints
2025-03-06 20:16:22,301 - INFO - validation batch 301, loss: 0.130, 9632/10016 datapoints
2025-03-06 20:16:22,343 - INFO - Epoch 651/800 done.
2025-03-06 20:16:22,344 - INFO - Final validation performance:
Loss: 0.256, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:16:22,344 - INFO - Beginning epoch 652/800
2025-03-06 20:16:22,351 - INFO - training batch 1, loss: 0.237, 32/60000 datapoints
2025-03-06 20:16:22,573 - INFO - training batch 51, loss: 0.618, 1632/60000 datapoints
2025-03-06 20:16:22,778 - INFO - training batch 101, loss: 0.114, 3232/60000 datapoints
2025-03-06 20:16:22,989 - INFO - training batch 151, loss: 0.051, 4832/60000 datapoints
2025-03-06 20:16:23,194 - INFO - training batch 201, loss: 0.212, 6432/60000 datapoints
2025-03-06 20:16:23,401 - INFO - training batch 251, loss: 0.110, 8032/60000 datapoints
2025-03-06 20:16:23,601 - INFO - training batch 301, loss: 0.178, 9632/60000 datapoints
2025-03-06 20:16:23,803 - INFO - training batch 351, loss: 0.031, 11232/60000 datapoints
2025-03-06 20:16:24,004 - INFO - training batch 401, loss: 0.160, 12832/60000 datapoints
2025-03-06 20:16:24,207 - INFO - training batch 451, loss: 0.197, 14432/60000 datapoints
2025-03-06 20:16:24,409 - INFO - training batch 501, loss: 0.091, 16032/60000 datapoints
2025-03-06 20:16:24,609 - INFO - training batch 551, loss: 0.440, 17632/60000 datapoints
2025-03-06 20:16:24,818 - INFO - training batch 601, loss: 0.418, 19232/60000 datapoints
2025-03-06 20:16:25,028 - INFO - training batch 651, loss: 0.249, 20832/60000 datapoints
2025-03-06 20:16:25,238 - INFO - training batch 701, loss: 0.279, 22432/60000 datapoints
2025-03-06 20:16:25,439 - INFO - training batch 751, loss: 0.263, 24032/60000 datapoints
2025-03-06 20:16:25,646 - INFO - training batch 801, loss: 0.139, 25632/60000 datapoints
2025-03-06 20:16:25,845 - INFO - training batch 851, loss: 0.212, 27232/60000 datapoints
2025-03-06 20:16:26,045 - INFO - training batch 901, loss: 0.228, 28832/60000 datapoints
2025-03-06 20:16:26,248 - INFO - training batch 951, loss: 0.445, 30432/60000 datapoints
2025-03-06 20:16:26,449 - INFO - training batch 1001, loss: 0.099, 32032/60000 datapoints
2025-03-06 20:16:26,650 - INFO - training batch 1051, loss: 0.169, 33632/60000 datapoints
2025-03-06 20:16:26,851 - INFO - training batch 1101, loss: 0.170, 35232/60000 datapoints
2025-03-06 20:16:27,054 - INFO - training batch 1151, loss: 0.290, 36832/60000 datapoints
2025-03-06 20:16:27,255 - INFO - training batch 1201, loss: 0.506, 38432/60000 datapoints
2025-03-06 20:16:27,455 - INFO - training batch 1251, loss: 0.166, 40032/60000 datapoints
2025-03-06 20:16:27,658 - INFO - training batch 1301, loss: 0.206, 41632/60000 datapoints
2025-03-06 20:16:27,857 - INFO - training batch 1351, loss: 0.186, 43232/60000 datapoints
2025-03-06 20:16:28,057 - INFO - training batch 1401, loss: 0.144, 44832/60000 datapoints
2025-03-06 20:16:28,264 - INFO - training batch 1451, loss: 0.293, 46432/60000 datapoints
2025-03-06 20:16:28,466 - INFO - training batch 1501, loss: 0.194, 48032/60000 datapoints
2025-03-06 20:16:28,672 - INFO - training batch 1551, loss: 0.251, 49632/60000 datapoints
2025-03-06 20:16:28,873 - INFO - training batch 1601, loss: 0.126, 51232/60000 datapoints
2025-03-06 20:16:29,077 - INFO - training batch 1651, loss: 0.335, 52832/60000 datapoints
2025-03-06 20:16:29,278 - INFO - training batch 1701, loss: 0.064, 54432/60000 datapoints
2025-03-06 20:16:29,482 - INFO - training batch 1751, loss: 0.107, 56032/60000 datapoints
2025-03-06 20:16:29,690 - INFO - training batch 1801, loss: 0.256, 57632/60000 datapoints
2025-03-06 20:16:29,893 - INFO - training batch 1851, loss: 0.160, 59232/60000 datapoints
2025-03-06 20:16:30,001 - INFO - validation batch 1, loss: 0.201, 32/10016 datapoints
2025-03-06 20:16:30,168 - INFO - validation batch 51, loss: 0.210, 1632/10016 datapoints
2025-03-06 20:16:30,335 - INFO - validation batch 101, loss: 0.121, 3232/10016 datapoints
2025-03-06 20:16:30,506 - INFO - validation batch 151, loss: 0.082, 4832/10016 datapoints
2025-03-06 20:16:30,679 - INFO - validation batch 201, loss: 0.377, 6432/10016 datapoints
2025-03-06 20:16:30,847 - INFO - validation batch 251, loss: 0.203, 8032/10016 datapoints
2025-03-06 20:16:31,019 - INFO - validation batch 301, loss: 0.229, 9632/10016 datapoints
2025-03-06 20:16:31,061 - INFO - Epoch 652/800 done.
2025-03-06 20:16:31,062 - INFO - Final validation performance:
Loss: 0.203, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:16:31,063 - INFO - Beginning epoch 653/800
2025-03-06 20:16:31,073 - INFO - training batch 1, loss: 0.113, 32/60000 datapoints
2025-03-06 20:16:31,288 - INFO - training batch 51, loss: 0.201, 1632/60000 datapoints
2025-03-06 20:16:31,496 - INFO - training batch 101, loss: 0.416, 3232/60000 datapoints
2025-03-06 20:16:31,702 - INFO - training batch 151, loss: 0.213, 4832/60000 datapoints
2025-03-06 20:16:31,916 - INFO - training batch 201, loss: 0.350, 6432/60000 datapoints
2025-03-06 20:16:32,126 - INFO - training batch 251, loss: 0.210, 8032/60000 datapoints
2025-03-06 20:16:32,343 - INFO - training batch 301, loss: 0.202, 9632/60000 datapoints
2025-03-06 20:16:32,546 - INFO - training batch 351, loss: 0.350, 11232/60000 datapoints
2025-03-06 20:16:32,750 - INFO - training batch 401, loss: 0.290, 12832/60000 datapoints
2025-03-06 20:16:32,950 - INFO - training batch 451, loss: 0.347, 14432/60000 datapoints
2025-03-06 20:16:33,158 - INFO - training batch 501, loss: 0.140, 16032/60000 datapoints
2025-03-06 20:16:33,359 - INFO - training batch 551, loss: 0.196, 17632/60000 datapoints
2025-03-06 20:16:33,562 - INFO - training batch 601, loss: 0.267, 19232/60000 datapoints
2025-03-06 20:16:33,770 - INFO - training batch 651, loss: 0.206, 20832/60000 datapoints
2025-03-06 20:16:33,975 - INFO - training batch 701, loss: 0.177, 22432/60000 datapoints
2025-03-06 20:16:34,179 - INFO - training batch 751, loss: 0.048, 24032/60000 datapoints
2025-03-06 20:16:34,381 - INFO - training batch 801, loss: 0.223, 25632/60000 datapoints
2025-03-06 20:16:34,582 - INFO - training batch 851, loss: 0.327, 27232/60000 datapoints
2025-03-06 20:16:34,788 - INFO - training batch 901, loss: 0.102, 28832/60000 datapoints
2025-03-06 20:16:34,997 - INFO - training batch 951, loss: 0.186, 30432/60000 datapoints
2025-03-06 20:16:35,201 - INFO - training batch 1001, loss: 0.387, 32032/60000 datapoints
2025-03-06 20:16:35,403 - INFO - training batch 1051, loss: 0.225, 33632/60000 datapoints
2025-03-06 20:16:35,603 - INFO - training batch 1101, loss: 0.135, 35232/60000 datapoints
2025-03-06 20:16:35,807 - INFO - training batch 1151, loss: 0.147, 36832/60000 datapoints
2025-03-06 20:16:36,014 - INFO - training batch 1201, loss: 0.171, 38432/60000 datapoints
2025-03-06 20:16:36,216 - INFO - training batch 1251, loss: 0.265, 40032/60000 datapoints
2025-03-06 20:16:36,414 - INFO - training batch 1301, loss: 0.168, 41632/60000 datapoints
2025-03-06 20:16:36,616 - INFO - training batch 1351, loss: 0.131, 43232/60000 datapoints
2025-03-06 20:16:36,820 - INFO - training batch 1401, loss: 0.144, 44832/60000 datapoints
2025-03-06 20:16:37,024 - INFO - training batch 1451, loss: 0.162, 46432/60000 datapoints
2025-03-06 20:16:37,227 - INFO - training batch 1501, loss: 0.127, 48032/60000 datapoints
2025-03-06 20:16:37,425 - INFO - training batch 1551, loss: 0.154, 49632/60000 datapoints
2025-03-06 20:16:37,648 - INFO - training batch 1601, loss: 0.129, 51232/60000 datapoints
2025-03-06 20:16:37,851 - INFO - training batch 1651, loss: 0.482, 52832/60000 datapoints
2025-03-06 20:16:38,055 - INFO - training batch 1701, loss: 0.623, 54432/60000 datapoints
2025-03-06 20:16:38,263 - INFO - training batch 1751, loss: 0.216, 56032/60000 datapoints
2025-03-06 20:16:38,467 - INFO - training batch 1801, loss: 0.192, 57632/60000 datapoints
2025-03-06 20:16:38,672 - INFO - training batch 1851, loss: 0.179, 59232/60000 datapoints
2025-03-06 20:16:38,777 - INFO - validation batch 1, loss: 0.132, 32/10016 datapoints
2025-03-06 20:16:38,936 - INFO - validation batch 51, loss: 0.289, 1632/10016 datapoints
2025-03-06 20:16:39,100 - INFO - validation batch 101, loss: 0.176, 3232/10016 datapoints
2025-03-06 20:16:39,257 - INFO - validation batch 151, loss: 0.231, 4832/10016 datapoints
2025-03-06 20:16:39,415 - INFO - validation batch 201, loss: 0.235, 6432/10016 datapoints
2025-03-06 20:16:39,575 - INFO - validation batch 251, loss: 0.138, 8032/10016 datapoints
2025-03-06 20:16:39,735 - INFO - validation batch 301, loss: 0.411, 9632/10016 datapoints
2025-03-06 20:16:39,772 - INFO - Epoch 653/800 done.
2025-03-06 20:16:39,772 - INFO - Final validation performance:
Loss: 0.230, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:16:39,772 - INFO - Beginning epoch 654/800
2025-03-06 20:16:39,779 - INFO - training batch 1, loss: 0.217, 32/60000 datapoints
2025-03-06 20:16:39,992 - INFO - training batch 51, loss: 0.372, 1632/60000 datapoints
2025-03-06 20:16:40,199 - INFO - training batch 101, loss: 0.137, 3232/60000 datapoints
2025-03-06 20:16:40,405 - INFO - training batch 151, loss: 0.111, 4832/60000 datapoints
2025-03-06 20:16:40,603 - INFO - training batch 201, loss: 0.181, 6432/60000 datapoints
2025-03-06 20:16:40,804 - INFO - training batch 251, loss: 0.226, 8032/60000 datapoints
2025-03-06 20:16:40,997 - INFO - training batch 301, loss: 0.389, 9632/60000 datapoints
2025-03-06 20:16:41,238 - INFO - training batch 351, loss: 0.106, 11232/60000 datapoints
2025-03-06 20:16:41,448 - INFO - training batch 401, loss: 0.271, 12832/60000 datapoints
2025-03-06 20:16:41,650 - INFO - training batch 451, loss: 0.300, 14432/60000 datapoints
2025-03-06 20:16:41,852 - INFO - training batch 501, loss: 0.157, 16032/60000 datapoints
2025-03-06 20:16:42,054 - INFO - training batch 551, loss: 0.310, 17632/60000 datapoints
2025-03-06 20:16:42,280 - INFO - training batch 601, loss: 0.426, 19232/60000 datapoints
2025-03-06 20:16:42,489 - INFO - training batch 651, loss: 0.540, 20832/60000 datapoints
2025-03-06 20:16:42,693 - INFO - training batch 701, loss: 0.324, 22432/60000 datapoints
2025-03-06 20:16:42,895 - INFO - training batch 751, loss: 0.304, 24032/60000 datapoints
2025-03-06 20:16:43,098 - INFO - training batch 801, loss: 0.192, 25632/60000 datapoints
2025-03-06 20:16:43,305 - INFO - training batch 851, loss: 0.268, 27232/60000 datapoints
2025-03-06 20:16:43,504 - INFO - training batch 901, loss: 0.104, 28832/60000 datapoints
2025-03-06 20:16:43,709 - INFO - training batch 951, loss: 0.334, 30432/60000 datapoints
2025-03-06 20:16:43,911 - INFO - training batch 1001, loss: 0.213, 32032/60000 datapoints
2025-03-06 20:16:44,119 - INFO - training batch 1051, loss: 0.111, 33632/60000 datapoints
2025-03-06 20:16:44,322 - INFO - training batch 1101, loss: 0.176, 35232/60000 datapoints
2025-03-06 20:16:44,523 - INFO - training batch 1151, loss: 0.237, 36832/60000 datapoints
2025-03-06 20:16:44,724 - INFO - training batch 1201, loss: 0.231, 38432/60000 datapoints
2025-03-06 20:16:44,931 - INFO - training batch 1251, loss: 0.358, 40032/60000 datapoints
2025-03-06 20:16:45,138 - INFO - training batch 1301, loss: 0.220, 41632/60000 datapoints
2025-03-06 20:16:45,343 - INFO - training batch 1351, loss: 0.157, 43232/60000 datapoints
2025-03-06 20:16:45,546 - INFO - training batch 1401, loss: 0.204, 44832/60000 datapoints
2025-03-06 20:16:45,749 - INFO - training batch 1451, loss: 0.056, 46432/60000 datapoints
2025-03-06 20:16:45,949 - INFO - training batch 1501, loss: 0.145, 48032/60000 datapoints
2025-03-06 20:16:46,151 - INFO - training batch 1551, loss: 0.300, 49632/60000 datapoints
2025-03-06 20:16:46,351 - INFO - training batch 1601, loss: 0.139, 51232/60000 datapoints
2025-03-06 20:16:46,553 - INFO - training batch 1651, loss: 0.284, 52832/60000 datapoints
2025-03-06 20:16:46,754 - INFO - training batch 1701, loss: 0.354, 54432/60000 datapoints
2025-03-06 20:16:46,954 - INFO - training batch 1751, loss: 0.109, 56032/60000 datapoints
2025-03-06 20:16:47,159 - INFO - training batch 1801, loss: 0.175, 57632/60000 datapoints
2025-03-06 20:16:47,358 - INFO - training batch 1851, loss: 0.239, 59232/60000 datapoints
2025-03-06 20:16:47,466 - INFO - validation batch 1, loss: 0.151, 32/10016 datapoints
2025-03-06 20:16:47,630 - INFO - validation batch 51, loss: 0.157, 1632/10016 datapoints
2025-03-06 20:16:47,792 - INFO - validation batch 101, loss: 0.421, 3232/10016 datapoints
2025-03-06 20:16:47,956 - INFO - validation batch 151, loss: 0.416, 4832/10016 datapoints
2025-03-06 20:16:48,119 - INFO - validation batch 201, loss: 0.173, 6432/10016 datapoints
2025-03-06 20:16:48,281 - INFO - validation batch 251, loss: 0.377, 8032/10016 datapoints
2025-03-06 20:16:48,445 - INFO - validation batch 301, loss: 0.154, 9632/10016 datapoints
2025-03-06 20:16:48,488 - INFO - Epoch 654/800 done.
2025-03-06 20:16:48,488 - INFO - Final validation performance:
Loss: 0.264, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:16:48,489 - INFO - Beginning epoch 655/800
2025-03-06 20:16:48,496 - INFO - training batch 1, loss: 0.089, 32/60000 datapoints
2025-03-06 20:16:48,699 - INFO - training batch 51, loss: 0.291, 1632/60000 datapoints
2025-03-06 20:16:48,906 - INFO - training batch 101, loss: 0.165, 3232/60000 datapoints
2025-03-06 20:16:49,126 - INFO - training batch 151, loss: 0.453, 4832/60000 datapoints
2025-03-06 20:16:49,326 - INFO - training batch 201, loss: 0.079, 6432/60000 datapoints
2025-03-06 20:16:49,535 - INFO - training batch 251, loss: 0.097, 8032/60000 datapoints
2025-03-06 20:16:49,740 - INFO - training batch 301, loss: 0.083, 9632/60000 datapoints
2025-03-06 20:16:49,947 - INFO - training batch 351, loss: 0.072, 11232/60000 datapoints
2025-03-06 20:16:50,147 - INFO - training batch 401, loss: 0.338, 12832/60000 datapoints
2025-03-06 20:16:50,350 - INFO - training batch 451, loss: 0.178, 14432/60000 datapoints
2025-03-06 20:16:50,548 - INFO - training batch 501, loss: 0.222, 16032/60000 datapoints
2025-03-06 20:16:50,752 - INFO - training batch 551, loss: 0.384, 17632/60000 datapoints
2025-03-06 20:16:50,954 - INFO - training batch 601, loss: 0.127, 19232/60000 datapoints
2025-03-06 20:16:51,157 - INFO - training batch 651, loss: 0.282, 20832/60000 datapoints
2025-03-06 20:16:51,360 - INFO - training batch 701, loss: 0.166, 22432/60000 datapoints
2025-03-06 20:16:51,560 - INFO - training batch 751, loss: 0.435, 24032/60000 datapoints
2025-03-06 20:16:51,762 - INFO - training batch 801, loss: 0.128, 25632/60000 datapoints
2025-03-06 20:16:51,962 - INFO - training batch 851, loss: 0.103, 27232/60000 datapoints
2025-03-06 20:16:52,163 - INFO - training batch 901, loss: 0.195, 28832/60000 datapoints
2025-03-06 20:16:52,391 - INFO - training batch 951, loss: 0.250, 30432/60000 datapoints
2025-03-06 20:16:52,595 - INFO - training batch 1001, loss: 0.165, 32032/60000 datapoints
2025-03-06 20:16:52,803 - INFO - training batch 1051, loss: 0.298, 33632/60000 datapoints
2025-03-06 20:16:53,004 - INFO - training batch 1101, loss: 0.137, 35232/60000 datapoints
2025-03-06 20:16:53,208 - INFO - training batch 1151, loss: 0.516, 36832/60000 datapoints
2025-03-06 20:16:53,411 - INFO - training batch 1201, loss: 0.168, 38432/60000 datapoints
2025-03-06 20:16:53,616 - INFO - training batch 1251, loss: 0.141, 40032/60000 datapoints
2025-03-06 20:16:53,819 - INFO - training batch 1301, loss: 0.351, 41632/60000 datapoints
2025-03-06 20:16:54,030 - INFO - training batch 1351, loss: 0.259, 43232/60000 datapoints
2025-03-06 20:16:54,243 - INFO - training batch 1401, loss: 0.248, 44832/60000 datapoints
2025-03-06 20:16:54,535 - INFO - training batch 1451, loss: 0.161, 46432/60000 datapoints
2025-03-06 20:16:54,741 - INFO - training batch 1501, loss: 0.517, 48032/60000 datapoints
2025-03-06 20:16:54,951 - INFO - training batch 1551, loss: 0.348, 49632/60000 datapoints
2025-03-06 20:16:55,160 - INFO - training batch 1601, loss: 0.159, 51232/60000 datapoints
2025-03-06 20:16:55,374 - INFO - training batch 1651, loss: 0.237, 52832/60000 datapoints
2025-03-06 20:16:55,584 - INFO - training batch 1701, loss: 0.203, 54432/60000 datapoints
2025-03-06 20:16:55,794 - INFO - training batch 1751, loss: 0.220, 56032/60000 datapoints
2025-03-06 20:16:56,000 - INFO - training batch 1801, loss: 0.182, 57632/60000 datapoints
2025-03-06 20:16:56,205 - INFO - training batch 1851, loss: 0.477, 59232/60000 datapoints
2025-03-06 20:16:56,314 - INFO - validation batch 1, loss: 0.231, 32/10016 datapoints
2025-03-06 20:16:56,484 - INFO - validation batch 51, loss: 0.203, 1632/10016 datapoints
2025-03-06 20:16:56,653 - INFO - validation batch 101, loss: 0.380, 3232/10016 datapoints
2025-03-06 20:16:56,820 - INFO - validation batch 151, loss: 0.331, 4832/10016 datapoints
2025-03-06 20:16:56,989 - INFO - validation batch 201, loss: 0.508, 6432/10016 datapoints
2025-03-06 20:16:57,159 - INFO - validation batch 251, loss: 0.690, 8032/10016 datapoints
2025-03-06 20:16:57,325 - INFO - validation batch 301, loss: 0.163, 9632/10016 datapoints
2025-03-06 20:16:57,365 - INFO - Epoch 655/800 done.
2025-03-06 20:16:57,365 - INFO - Final validation performance:
Loss: 0.358, top-1 acc: 0.933top-5 acc: 0.933
2025-03-06 20:16:57,366 - INFO - Beginning epoch 656/800
2025-03-06 20:16:57,376 - INFO - training batch 1, loss: 0.238, 32/60000 datapoints
2025-03-06 20:16:57,584 - INFO - training batch 51, loss: 0.300, 1632/60000 datapoints
2025-03-06 20:16:57,791 - INFO - training batch 101, loss: 0.197, 3232/60000 datapoints
2025-03-06 20:16:58,017 - INFO - training batch 151, loss: 0.031, 4832/60000 datapoints
2025-03-06 20:16:58,224 - INFO - training batch 201, loss: 0.112, 6432/60000 datapoints
2025-03-06 20:16:58,433 - INFO - training batch 251, loss: 0.155, 8032/60000 datapoints
2025-03-06 20:16:58,655 - INFO - training batch 301, loss: 0.154, 9632/60000 datapoints
2025-03-06 20:16:58,865 - INFO - training batch 351, loss: 0.493, 11232/60000 datapoints
2025-03-06 20:16:59,072 - INFO - training batch 401, loss: 0.145, 12832/60000 datapoints
2025-03-06 20:16:59,291 - INFO - training batch 451, loss: 0.118, 14432/60000 datapoints
2025-03-06 20:16:59,520 - INFO - training batch 501, loss: 0.283, 16032/60000 datapoints
2025-03-06 20:16:59,726 - INFO - training batch 551, loss: 0.112, 17632/60000 datapoints
2025-03-06 20:16:59,929 - INFO - training batch 601, loss: 0.183, 19232/60000 datapoints
2025-03-06 20:17:00,136 - INFO - training batch 651, loss: 0.422, 20832/60000 datapoints
2025-03-06 20:17:00,340 - INFO - training batch 701, loss: 0.440, 22432/60000 datapoints
2025-03-06 20:17:00,550 - INFO - training batch 751, loss: 0.184, 24032/60000 datapoints
2025-03-06 20:17:00,761 - INFO - training batch 801, loss: 0.353, 25632/60000 datapoints
2025-03-06 20:17:00,969 - INFO - training batch 851, loss: 0.790, 27232/60000 datapoints
2025-03-06 20:17:01,179 - INFO - training batch 901, loss: 0.254, 28832/60000 datapoints
2025-03-06 20:17:01,384 - INFO - training batch 951, loss: 0.188, 30432/60000 datapoints
2025-03-06 20:17:01,593 - INFO - training batch 1001, loss: 0.187, 32032/60000 datapoints
2025-03-06 20:17:01,828 - INFO - training batch 1051, loss: 0.339, 33632/60000 datapoints
2025-03-06 20:17:02,058 - INFO - training batch 1101, loss: 0.092, 35232/60000 datapoints
2025-03-06 20:17:02,265 - INFO - training batch 1151, loss: 0.476, 36832/60000 datapoints
2025-03-06 20:17:02,496 - INFO - training batch 1201, loss: 0.055, 38432/60000 datapoints
2025-03-06 20:17:02,711 - INFO - training batch 1251, loss: 0.136, 40032/60000 datapoints
2025-03-06 20:17:02,916 - INFO - training batch 1301, loss: 0.477, 41632/60000 datapoints
2025-03-06 20:17:03,129 - INFO - training batch 1351, loss: 0.275, 43232/60000 datapoints
2025-03-06 20:17:03,335 - INFO - training batch 1401, loss: 0.144, 44832/60000 datapoints
2025-03-06 20:17:03,547 - INFO - training batch 1451, loss: 0.232, 46432/60000 datapoints
2025-03-06 20:17:03,756 - INFO - training batch 1501, loss: 0.203, 48032/60000 datapoints
2025-03-06 20:17:03,955 - INFO - training batch 1551, loss: 0.215, 49632/60000 datapoints
2025-03-06 20:17:04,159 - INFO - training batch 1601, loss: 0.156, 51232/60000 datapoints
2025-03-06 20:17:04,360 - INFO - training batch 1651, loss: 0.177, 52832/60000 datapoints
2025-03-06 20:17:04,563 - INFO - training batch 1701, loss: 0.423, 54432/60000 datapoints
2025-03-06 20:17:04,767 - INFO - training batch 1751, loss: 0.164, 56032/60000 datapoints
2025-03-06 20:17:04,972 - INFO - training batch 1801, loss: 0.098, 57632/60000 datapoints
2025-03-06 20:17:05,176 - INFO - training batch 1851, loss: 0.145, 59232/60000 datapoints
2025-03-06 20:17:05,284 - INFO - validation batch 1, loss: 0.767, 32/10016 datapoints
2025-03-06 20:17:05,448 - INFO - validation batch 51, loss: 0.222, 1632/10016 datapoints
2025-03-06 20:17:05,612 - INFO - validation batch 101, loss: 0.309, 3232/10016 datapoints
2025-03-06 20:17:05,778 - INFO - validation batch 151, loss: 0.347, 4832/10016 datapoints
2025-03-06 20:17:05,940 - INFO - validation batch 201, loss: 0.662, 6432/10016 datapoints
2025-03-06 20:17:06,110 - INFO - validation batch 251, loss: 0.104, 8032/10016 datapoints
2025-03-06 20:17:06,293 - INFO - validation batch 301, loss: 0.219, 9632/10016 datapoints
2025-03-06 20:17:06,338 - INFO - Epoch 656/800 done.
2025-03-06 20:17:06,338 - INFO - Final validation performance:
Loss: 0.376, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:17:06,339 - INFO - Beginning epoch 657/800
2025-03-06 20:17:06,346 - INFO - training batch 1, loss: 0.242, 32/60000 datapoints
2025-03-06 20:17:06,571 - INFO - training batch 51, loss: 0.133, 1632/60000 datapoints
2025-03-06 20:17:06,771 - INFO - training batch 101, loss: 0.114, 3232/60000 datapoints
2025-03-06 20:17:06,970 - INFO - training batch 151, loss: 0.279, 4832/60000 datapoints
2025-03-06 20:17:07,178 - INFO - training batch 201, loss: 0.278, 6432/60000 datapoints
2025-03-06 20:17:07,381 - INFO - training batch 251, loss: 0.160, 8032/60000 datapoints
2025-03-06 20:17:07,586 - INFO - training batch 301, loss: 0.267, 9632/60000 datapoints
2025-03-06 20:17:07,792 - INFO - training batch 351, loss: 0.236, 11232/60000 datapoints
2025-03-06 20:17:07,990 - INFO - training batch 401, loss: 0.583, 12832/60000 datapoints
2025-03-06 20:17:08,192 - INFO - training batch 451, loss: 0.116, 14432/60000 datapoints
2025-03-06 20:17:08,393 - INFO - training batch 501, loss: 0.155, 16032/60000 datapoints
2025-03-06 20:17:08,594 - INFO - training batch 551, loss: 0.411, 17632/60000 datapoints
2025-03-06 20:17:08,794 - INFO - training batch 601, loss: 0.145, 19232/60000 datapoints
2025-03-06 20:17:08,996 - INFO - training batch 651, loss: 0.148, 20832/60000 datapoints
2025-03-06 20:17:09,200 - INFO - training batch 701, loss: 0.305, 22432/60000 datapoints
2025-03-06 20:17:09,399 - INFO - training batch 751, loss: 0.496, 24032/60000 datapoints
2025-03-06 20:17:09,604 - INFO - training batch 801, loss: 0.221, 25632/60000 datapoints
2025-03-06 20:17:09,804 - INFO - training batch 851, loss: 0.109, 27232/60000 datapoints
2025-03-06 20:17:10,003 - INFO - training batch 901, loss: 0.177, 28832/60000 datapoints
2025-03-06 20:17:10,201 - INFO - training batch 951, loss: 0.164, 30432/60000 datapoints
2025-03-06 20:17:10,403 - INFO - training batch 1001, loss: 0.231, 32032/60000 datapoints
2025-03-06 20:17:10,608 - INFO - training batch 1051, loss: 0.158, 33632/60000 datapoints
2025-03-06 20:17:10,808 - INFO - training batch 1101, loss: 0.377, 35232/60000 datapoints
2025-03-06 20:17:11,006 - INFO - training batch 1151, loss: 0.147, 36832/60000 datapoints
2025-03-06 20:17:11,207 - INFO - training batch 1201, loss: 0.083, 38432/60000 datapoints
2025-03-06 20:17:11,405 - INFO - training batch 1251, loss: 0.150, 40032/60000 datapoints
2025-03-06 20:17:11,603 - INFO - training batch 1301, loss: 0.213, 41632/60000 datapoints
2025-03-06 20:17:11,809 - INFO - training batch 1351, loss: 0.069, 43232/60000 datapoints
2025-03-06 20:17:12,014 - INFO - training batch 1401, loss: 0.114, 44832/60000 datapoints
2025-03-06 20:17:12,218 - INFO - training batch 1451, loss: 0.213, 46432/60000 datapoints
2025-03-06 20:17:12,422 - INFO - training batch 1501, loss: 0.539, 48032/60000 datapoints
2025-03-06 20:17:12,659 - INFO - training batch 1551, loss: 0.235, 49632/60000 datapoints
2025-03-06 20:17:12,863 - INFO - training batch 1601, loss: 0.259, 51232/60000 datapoints
2025-03-06 20:17:13,066 - INFO - training batch 1651, loss: 0.153, 52832/60000 datapoints
2025-03-06 20:17:13,274 - INFO - training batch 1701, loss: 0.120, 54432/60000 datapoints
2025-03-06 20:17:13,478 - INFO - training batch 1751, loss: 0.093, 56032/60000 datapoints
2025-03-06 20:17:13,687 - INFO - training batch 1801, loss: 0.481, 57632/60000 datapoints
2025-03-06 20:17:13,895 - INFO - training batch 1851, loss: 0.183, 59232/60000 datapoints
2025-03-06 20:17:14,004 - INFO - validation batch 1, loss: 0.254, 32/10016 datapoints
2025-03-06 20:17:14,185 - INFO - validation batch 51, loss: 0.401, 1632/10016 datapoints
2025-03-06 20:17:14,355 - INFO - validation batch 101, loss: 0.087, 3232/10016 datapoints
2025-03-06 20:17:14,527 - INFO - validation batch 151, loss: 0.086, 4832/10016 datapoints
2025-03-06 20:17:14,704 - INFO - validation batch 201, loss: 0.459, 6432/10016 datapoints
2025-03-06 20:17:14,880 - INFO - validation batch 251, loss: 0.511, 8032/10016 datapoints
2025-03-06 20:17:15,060 - INFO - validation batch 301, loss: 0.292, 9632/10016 datapoints
2025-03-06 20:17:15,106 - INFO - Epoch 657/800 done.
2025-03-06 20:17:15,106 - INFO - Final validation performance:
Loss: 0.299, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:17:15,107 - INFO - Beginning epoch 658/800
2025-03-06 20:17:15,116 - INFO - training batch 1, loss: 0.217, 32/60000 datapoints
2025-03-06 20:17:15,361 - INFO - training batch 51, loss: 0.165, 1632/60000 datapoints
2025-03-06 20:17:15,580 - INFO - training batch 101, loss: 0.461, 3232/60000 datapoints
2025-03-06 20:17:15,808 - INFO - training batch 151, loss: 0.174, 4832/60000 datapoints
2025-03-06 20:17:16,026 - INFO - training batch 201, loss: 0.143, 6432/60000 datapoints
2025-03-06 20:17:16,239 - INFO - training batch 251, loss: 0.448, 8032/60000 datapoints
2025-03-06 20:17:16,451 - INFO - training batch 301, loss: 0.131, 9632/60000 datapoints
2025-03-06 20:17:16,668 - INFO - training batch 351, loss: 0.300, 11232/60000 datapoints
2025-03-06 20:17:16,881 - INFO - training batch 401, loss: 0.170, 12832/60000 datapoints
2025-03-06 20:17:17,094 - INFO - training batch 451, loss: 0.080, 14432/60000 datapoints
2025-03-06 20:17:17,314 - INFO - training batch 501, loss: 0.181, 16032/60000 datapoints
2025-03-06 20:17:17,527 - INFO - training batch 551, loss: 0.275, 17632/60000 datapoints
2025-03-06 20:17:17,743 - INFO - training batch 601, loss: 0.367, 19232/60000 datapoints
2025-03-06 20:17:17,953 - INFO - training batch 651, loss: 0.406, 20832/60000 datapoints
2025-03-06 20:17:18,164 - INFO - training batch 701, loss: 0.359, 22432/60000 datapoints
2025-03-06 20:17:18,379 - INFO - training batch 751, loss: 0.170, 24032/60000 datapoints
2025-03-06 20:17:18,587 - INFO - training batch 801, loss: 0.099, 25632/60000 datapoints
2025-03-06 20:17:18,799 - INFO - training batch 851, loss: 0.114, 27232/60000 datapoints
2025-03-06 20:17:19,009 - INFO - training batch 901, loss: 0.095, 28832/60000 datapoints
2025-03-06 20:17:19,219 - INFO - training batch 951, loss: 0.098, 30432/60000 datapoints
2025-03-06 20:17:19,425 - INFO - training batch 1001, loss: 0.198, 32032/60000 datapoints
2025-03-06 20:17:19,639 - INFO - training batch 1051, loss: 0.149, 33632/60000 datapoints
2025-03-06 20:17:19,846 - INFO - training batch 1101, loss: 0.481, 35232/60000 datapoints
2025-03-06 20:17:20,046 - INFO - training batch 1151, loss: 0.149, 36832/60000 datapoints
2025-03-06 20:17:20,246 - INFO - training batch 1201, loss: 0.205, 38432/60000 datapoints
2025-03-06 20:17:20,447 - INFO - training batch 1251, loss: 0.166, 40032/60000 datapoints
2025-03-06 20:17:20,650 - INFO - training batch 1301, loss: 0.187, 41632/60000 datapoints
2025-03-06 20:17:20,852 - INFO - training batch 1351, loss: 0.176, 43232/60000 datapoints
2025-03-06 20:17:21,051 - INFO - training batch 1401, loss: 0.313, 44832/60000 datapoints
2025-03-06 20:17:21,253 - INFO - training batch 1451, loss: 0.244, 46432/60000 datapoints
2025-03-06 20:17:21,452 - INFO - training batch 1501, loss: 0.380, 48032/60000 datapoints
2025-03-06 20:17:21,657 - INFO - training batch 1551, loss: 0.134, 49632/60000 datapoints
2025-03-06 20:17:21,861 - INFO - training batch 1601, loss: 0.216, 51232/60000 datapoints
2025-03-06 20:17:22,062 - INFO - training batch 1651, loss: 0.079, 52832/60000 datapoints
2025-03-06 20:17:22,264 - INFO - training batch 1701, loss: 0.085, 54432/60000 datapoints
2025-03-06 20:17:22,463 - INFO - training batch 1751, loss: 0.423, 56032/60000 datapoints
2025-03-06 20:17:22,690 - INFO - training batch 1801, loss: 0.232, 57632/60000 datapoints
2025-03-06 20:17:22,890 - INFO - training batch 1851, loss: 0.291, 59232/60000 datapoints
2025-03-06 20:17:22,995 - INFO - validation batch 1, loss: 0.865, 32/10016 datapoints
2025-03-06 20:17:23,155 - INFO - validation batch 51, loss: 0.194, 1632/10016 datapoints
2025-03-06 20:17:23,327 - INFO - validation batch 101, loss: 0.060, 3232/10016 datapoints
2025-03-06 20:17:23,489 - INFO - validation batch 151, loss: 0.096, 4832/10016 datapoints
2025-03-06 20:17:23,655 - INFO - validation batch 201, loss: 0.298, 6432/10016 datapoints
2025-03-06 20:17:23,836 - INFO - validation batch 251, loss: 0.384, 8032/10016 datapoints
2025-03-06 20:17:23,999 - INFO - validation batch 301, loss: 0.637, 9632/10016 datapoints
2025-03-06 20:17:24,041 - INFO - Epoch 658/800 done.
2025-03-06 20:17:24,041 - INFO - Final validation performance:
Loss: 0.362, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:17:24,042 - INFO - Beginning epoch 659/800
2025-03-06 20:17:24,049 - INFO - training batch 1, loss: 0.150, 32/60000 datapoints
2025-03-06 20:17:24,257 - INFO - training batch 51, loss: 0.163, 1632/60000 datapoints
2025-03-06 20:17:24,458 - INFO - training batch 101, loss: 0.260, 3232/60000 datapoints
2025-03-06 20:17:24,676 - INFO - training batch 151, loss: 0.230, 4832/60000 datapoints
2025-03-06 20:17:24,879 - INFO - training batch 201, loss: 0.142, 6432/60000 datapoints
2025-03-06 20:17:25,089 - INFO - training batch 251, loss: 0.202, 8032/60000 datapoints
2025-03-06 20:17:25,298 - INFO - training batch 301, loss: 0.191, 9632/60000 datapoints
2025-03-06 20:17:25,505 - INFO - training batch 351, loss: 0.277, 11232/60000 datapoints
2025-03-06 20:17:25,713 - INFO - training batch 401, loss: 0.311, 12832/60000 datapoints
2025-03-06 20:17:25,922 - INFO - training batch 451, loss: 0.247, 14432/60000 datapoints
2025-03-06 20:17:26,124 - INFO - training batch 501, loss: 0.370, 16032/60000 datapoints
2025-03-06 20:17:26,324 - INFO - training batch 551, loss: 0.165, 17632/60000 datapoints
2025-03-06 20:17:26,523 - INFO - training batch 601, loss: 0.110, 19232/60000 datapoints
2025-03-06 20:17:26,726 - INFO - training batch 651, loss: 0.437, 20832/60000 datapoints
2025-03-06 20:17:26,924 - INFO - training batch 701, loss: 0.459, 22432/60000 datapoints
2025-03-06 20:17:27,126 - INFO - training batch 751, loss: 0.221, 24032/60000 datapoints
2025-03-06 20:17:27,330 - INFO - training batch 801, loss: 0.319, 25632/60000 datapoints
2025-03-06 20:17:27,529 - INFO - training batch 851, loss: 0.106, 27232/60000 datapoints
2025-03-06 20:17:27,730 - INFO - training batch 901, loss: 0.235, 28832/60000 datapoints
2025-03-06 20:17:27,930 - INFO - training batch 951, loss: 0.194, 30432/60000 datapoints
2025-03-06 20:17:28,126 - INFO - training batch 1001, loss: 0.257, 32032/60000 datapoints
2025-03-06 20:17:28,328 - INFO - training batch 1051, loss: 0.052, 33632/60000 datapoints
2025-03-06 20:17:28,529 - INFO - training batch 1101, loss: 0.146, 35232/60000 datapoints
2025-03-06 20:17:28,734 - INFO - training batch 1151, loss: 0.154, 36832/60000 datapoints
2025-03-06 20:17:28,936 - INFO - training batch 1201, loss: 0.195, 38432/60000 datapoints
2025-03-06 20:17:29,135 - INFO - training batch 1251, loss: 0.174, 40032/60000 datapoints
2025-03-06 20:17:29,337 - INFO - training batch 1301, loss: 0.188, 41632/60000 datapoints
2025-03-06 20:17:29,538 - INFO - training batch 1351, loss: 0.243, 43232/60000 datapoints
2025-03-06 20:17:29,741 - INFO - training batch 1401, loss: 0.043, 44832/60000 datapoints
2025-03-06 20:17:29,941 - INFO - training batch 1451, loss: 0.216, 46432/60000 datapoints
2025-03-06 20:17:30,139 - INFO - training batch 1501, loss: 0.222, 48032/60000 datapoints
2025-03-06 20:17:30,338 - INFO - training batch 1551, loss: 0.164, 49632/60000 datapoints
2025-03-06 20:17:30,542 - INFO - training batch 1601, loss: 0.193, 51232/60000 datapoints
2025-03-06 20:17:30,923 - INFO - training batch 1651, loss: 0.453, 52832/60000 datapoints
2025-03-06 20:17:31,174 - INFO - training batch 1701, loss: 0.157, 54432/60000 datapoints
2025-03-06 20:17:31,375 - INFO - training batch 1751, loss: 0.162, 56032/60000 datapoints
2025-03-06 20:17:31,574 - INFO - training batch 1801, loss: 0.511, 57632/60000 datapoints
2025-03-06 20:17:31,775 - INFO - training batch 1851, loss: 0.214, 59232/60000 datapoints
2025-03-06 20:17:31,884 - INFO - validation batch 1, loss: 0.335, 32/10016 datapoints
2025-03-06 20:17:32,052 - INFO - validation batch 51, loss: 0.247, 1632/10016 datapoints
2025-03-06 20:17:32,237 - INFO - validation batch 101, loss: 0.104, 3232/10016 datapoints
2025-03-06 20:17:32,404 - INFO - validation batch 151, loss: 0.194, 4832/10016 datapoints
2025-03-06 20:17:32,569 - INFO - validation batch 201, loss: 0.069, 6432/10016 datapoints
2025-03-06 20:17:32,751 - INFO - validation batch 251, loss: 0.349, 8032/10016 datapoints
2025-03-06 20:17:32,929 - INFO - validation batch 301, loss: 0.118, 9632/10016 datapoints
2025-03-06 20:17:32,971 - INFO - Epoch 659/800 done.
2025-03-06 20:17:32,971 - INFO - Final validation performance:
Loss: 0.202, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:17:32,971 - INFO - Beginning epoch 660/800
2025-03-06 20:17:32,982 - INFO - training batch 1, loss: 0.340, 32/60000 datapoints
2025-03-06 20:17:33,184 - INFO - training batch 51, loss: 0.235, 1632/60000 datapoints
2025-03-06 20:17:33,405 - INFO - training batch 101, loss: 0.090, 3232/60000 datapoints
2025-03-06 20:17:33,607 - INFO - training batch 151, loss: 0.061, 4832/60000 datapoints
2025-03-06 20:17:33,823 - INFO - training batch 201, loss: 0.287, 6432/60000 datapoints
2025-03-06 20:17:34,034 - INFO - training batch 251, loss: 0.105, 8032/60000 datapoints
2025-03-06 20:17:34,251 - INFO - training batch 301, loss: 0.112, 9632/60000 datapoints
2025-03-06 20:17:34,459 - INFO - training batch 351, loss: 0.333, 11232/60000 datapoints
2025-03-06 20:17:34,667 - INFO - training batch 401, loss: 0.286, 12832/60000 datapoints
2025-03-06 20:17:34,872 - INFO - training batch 451, loss: 0.158, 14432/60000 datapoints
2025-03-06 20:17:35,083 - INFO - training batch 501, loss: 0.367, 16032/60000 datapoints
2025-03-06 20:17:35,293 - INFO - training batch 551, loss: 0.325, 17632/60000 datapoints
2025-03-06 20:17:35,501 - INFO - training batch 601, loss: 0.084, 19232/60000 datapoints
2025-03-06 20:17:35,709 - INFO - training batch 651, loss: 0.772, 20832/60000 datapoints
2025-03-06 20:17:35,920 - INFO - training batch 701, loss: 0.293, 22432/60000 datapoints
2025-03-06 20:17:36,128 - INFO - training batch 751, loss: 0.263, 24032/60000 datapoints
2025-03-06 20:17:36,336 - INFO - training batch 801, loss: 0.242, 25632/60000 datapoints
2025-03-06 20:17:36,561 - INFO - training batch 851, loss: 0.218, 27232/60000 datapoints
2025-03-06 20:17:36,770 - INFO - training batch 901, loss: 0.144, 28832/60000 datapoints
2025-03-06 20:17:36,980 - INFO - training batch 951, loss: 0.184, 30432/60000 datapoints
2025-03-06 20:17:37,189 - INFO - training batch 1001, loss: 0.296, 32032/60000 datapoints
2025-03-06 20:17:37,399 - INFO - training batch 1051, loss: 0.182, 33632/60000 datapoints
2025-03-06 20:17:37,685 - INFO - training batch 1101, loss: 0.043, 35232/60000 datapoints
2025-03-06 20:17:37,942 - INFO - training batch 1151, loss: 0.100, 36832/60000 datapoints
2025-03-06 20:17:38,171 - INFO - training batch 1201, loss: 0.086, 38432/60000 datapoints
2025-03-06 20:17:38,376 - INFO - training batch 1251, loss: 0.524, 40032/60000 datapoints
2025-03-06 20:17:38,584 - INFO - training batch 1301, loss: 0.247, 41632/60000 datapoints
2025-03-06 20:17:38,798 - INFO - training batch 1351, loss: 0.155, 43232/60000 datapoints
2025-03-06 20:17:39,011 - INFO - training batch 1401, loss: 0.153, 44832/60000 datapoints
2025-03-06 20:17:39,223 - INFO - training batch 1451, loss: 0.209, 46432/60000 datapoints
2025-03-06 20:17:39,425 - INFO - training batch 1501, loss: 0.281, 48032/60000 datapoints
2025-03-06 20:17:39,628 - INFO - training batch 1551, loss: 0.151, 49632/60000 datapoints
2025-03-06 20:17:39,828 - INFO - training batch 1601, loss: 0.187, 51232/60000 datapoints
2025-03-06 20:17:40,031 - INFO - training batch 1651, loss: 0.198, 52832/60000 datapoints
2025-03-06 20:17:40,230 - INFO - training batch 1701, loss: 0.135, 54432/60000 datapoints
2025-03-06 20:17:40,431 - INFO - training batch 1751, loss: 0.202, 56032/60000 datapoints
2025-03-06 20:17:40,636 - INFO - training batch 1801, loss: 0.191, 57632/60000 datapoints
2025-03-06 20:17:40,836 - INFO - training batch 1851, loss: 0.167, 59232/60000 datapoints
2025-03-06 20:17:40,938 - INFO - validation batch 1, loss: 0.394, 32/10016 datapoints
2025-03-06 20:17:41,106 - INFO - validation batch 51, loss: 0.416, 1632/10016 datapoints
2025-03-06 20:17:41,266 - INFO - validation batch 101, loss: 0.172, 3232/10016 datapoints
2025-03-06 20:17:41,425 - INFO - validation batch 151, loss: 0.138, 4832/10016 datapoints
2025-03-06 20:17:41,600 - INFO - validation batch 201, loss: 0.131, 6432/10016 datapoints
2025-03-06 20:17:41,780 - INFO - validation batch 251, loss: 0.189, 8032/10016 datapoints
2025-03-06 20:17:41,946 - INFO - validation batch 301, loss: 0.090, 9632/10016 datapoints
2025-03-06 20:17:41,989 - INFO - Epoch 660/800 done.
2025-03-06 20:17:41,989 - INFO - Final validation performance:
Loss: 0.218, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:17:41,990 - INFO - Beginning epoch 661/800
2025-03-06 20:17:41,997 - INFO - training batch 1, loss: 0.074, 32/60000 datapoints
2025-03-06 20:17:42,217 - INFO - training batch 51, loss: 0.688, 1632/60000 datapoints
2025-03-06 20:17:42,423 - INFO - training batch 101, loss: 0.447, 3232/60000 datapoints
2025-03-06 20:17:42,639 - INFO - training batch 151, loss: 0.325, 4832/60000 datapoints
2025-03-06 20:17:42,864 - INFO - training batch 201, loss: 0.128, 6432/60000 datapoints
2025-03-06 20:17:43,075 - INFO - training batch 251, loss: 0.154, 8032/60000 datapoints
2025-03-06 20:17:43,280 - INFO - training batch 301, loss: 0.065, 9632/60000 datapoints
2025-03-06 20:17:43,482 - INFO - training batch 351, loss: 0.247, 11232/60000 datapoints
2025-03-06 20:17:43,687 - INFO - training batch 401, loss: 0.157, 12832/60000 datapoints
2025-03-06 20:17:43,890 - INFO - training batch 451, loss: 0.200, 14432/60000 datapoints
2025-03-06 20:17:44,095 - INFO - training batch 501, loss: 0.170, 16032/60000 datapoints
2025-03-06 20:17:44,300 - INFO - training batch 551, loss: 0.243, 17632/60000 datapoints
2025-03-06 20:17:44,505 - INFO - training batch 601, loss: 0.344, 19232/60000 datapoints
2025-03-06 20:17:44,709 - INFO - training batch 651, loss: 0.227, 20832/60000 datapoints
2025-03-06 20:17:44,918 - INFO - training batch 701, loss: 0.290, 22432/60000 datapoints
2025-03-06 20:17:45,125 - INFO - training batch 751, loss: 0.207, 24032/60000 datapoints
2025-03-06 20:17:45,330 - INFO - training batch 801, loss: 0.278, 25632/60000 datapoints
2025-03-06 20:17:45,539 - INFO - training batch 851, loss: 0.287, 27232/60000 datapoints
2025-03-06 20:17:45,745 - INFO - training batch 901, loss: 0.155, 28832/60000 datapoints
2025-03-06 20:17:45,947 - INFO - training batch 951, loss: 0.392, 30432/60000 datapoints
2025-03-06 20:17:46,150 - INFO - training batch 1001, loss: 0.283, 32032/60000 datapoints
2025-03-06 20:17:46,349 - INFO - training batch 1051, loss: 0.377, 33632/60000 datapoints
2025-03-06 20:17:46,556 - INFO - training batch 1101, loss: 0.255, 35232/60000 datapoints
2025-03-06 20:17:46,763 - INFO - training batch 1151, loss: 0.190, 36832/60000 datapoints
2025-03-06 20:17:46,964 - INFO - training batch 1201, loss: 0.127, 38432/60000 datapoints
2025-03-06 20:17:47,163 - INFO - training batch 1251, loss: 0.150, 40032/60000 datapoints
2025-03-06 20:17:47,366 - INFO - training batch 1301, loss: 0.320, 41632/60000 datapoints
2025-03-06 20:17:47,569 - INFO - training batch 1351, loss: 0.156, 43232/60000 datapoints
2025-03-06 20:17:47,775 - INFO - training batch 1401, loss: 0.166, 44832/60000 datapoints
2025-03-06 20:17:47,974 - INFO - training batch 1451, loss: 0.071, 46432/60000 datapoints
2025-03-06 20:17:48,176 - INFO - training batch 1501, loss: 0.428, 48032/60000 datapoints
2025-03-06 20:17:48,374 - INFO - training batch 1551, loss: 0.133, 49632/60000 datapoints
2025-03-06 20:17:48,574 - INFO - training batch 1601, loss: 0.337, 51232/60000 datapoints
2025-03-06 20:17:48,775 - INFO - training batch 1651, loss: 0.124, 52832/60000 datapoints
2025-03-06 20:17:48,974 - INFO - training batch 1701, loss: 0.355, 54432/60000 datapoints
2025-03-06 20:17:49,174 - INFO - training batch 1751, loss: 0.561, 56032/60000 datapoints
2025-03-06 20:17:49,379 - INFO - training batch 1801, loss: 0.179, 57632/60000 datapoints
2025-03-06 20:17:49,581 - INFO - training batch 1851, loss: 0.099, 59232/60000 datapoints
2025-03-06 20:17:49,690 - INFO - validation batch 1, loss: 0.351, 32/10016 datapoints
2025-03-06 20:17:49,855 - INFO - validation batch 51, loss: 0.374, 1632/10016 datapoints
2025-03-06 20:17:50,019 - INFO - validation batch 101, loss: 0.132, 3232/10016 datapoints
2025-03-06 20:17:50,182 - INFO - validation batch 151, loss: 0.374, 4832/10016 datapoints
2025-03-06 20:17:50,344 - INFO - validation batch 201, loss: 0.101, 6432/10016 datapoints
2025-03-06 20:17:50,506 - INFO - validation batch 251, loss: 0.180, 8032/10016 datapoints
2025-03-06 20:17:50,669 - INFO - validation batch 301, loss: 0.107, 9632/10016 datapoints
2025-03-06 20:17:50,709 - INFO - Epoch 661/800 done.
2025-03-06 20:17:50,710 - INFO - Final validation performance:
Loss: 0.231, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:17:50,711 - INFO - Beginning epoch 662/800
2025-03-06 20:17:50,721 - INFO - training batch 1, loss: 0.392, 32/60000 datapoints
2025-03-06 20:17:50,937 - INFO - training batch 51, loss: 0.074, 1632/60000 datapoints
2025-03-06 20:17:51,135 - INFO - training batch 101, loss: 0.312, 3232/60000 datapoints
2025-03-06 20:17:51,362 - INFO - training batch 151, loss: 0.269, 4832/60000 datapoints
2025-03-06 20:17:51,613 - INFO - training batch 201, loss: 0.045, 6432/60000 datapoints
2025-03-06 20:17:51,843 - INFO - training batch 251, loss: 0.444, 8032/60000 datapoints
2025-03-06 20:17:52,062 - INFO - training batch 301, loss: 0.276, 9632/60000 datapoints
2025-03-06 20:17:52,263 - INFO - training batch 351, loss: 0.274, 11232/60000 datapoints
2025-03-06 20:17:52,458 - INFO - training batch 401, loss: 0.139, 12832/60000 datapoints
2025-03-06 20:17:52,657 - INFO - training batch 451, loss: 0.060, 14432/60000 datapoints
2025-03-06 20:17:52,853 - INFO - training batch 501, loss: 0.241, 16032/60000 datapoints
2025-03-06 20:17:53,079 - INFO - training batch 551, loss: 0.239, 17632/60000 datapoints
2025-03-06 20:17:53,286 - INFO - training batch 601, loss: 0.088, 19232/60000 datapoints
2025-03-06 20:17:53,488 - INFO - training batch 651, loss: 0.584, 20832/60000 datapoints
2025-03-06 20:17:53,693 - INFO - training batch 701, loss: 0.157, 22432/60000 datapoints
2025-03-06 20:17:53,896 - INFO - training batch 751, loss: 0.407, 24032/60000 datapoints
2025-03-06 20:17:54,098 - INFO - training batch 801, loss: 0.163, 25632/60000 datapoints
2025-03-06 20:17:54,299 - INFO - training batch 851, loss: 0.263, 27232/60000 datapoints
2025-03-06 20:17:54,501 - INFO - training batch 901, loss: 0.241, 28832/60000 datapoints
2025-03-06 20:17:54,706 - INFO - training batch 951, loss: 0.188, 30432/60000 datapoints
2025-03-06 20:17:54,915 - INFO - training batch 1001, loss: 0.131, 32032/60000 datapoints
2025-03-06 20:17:55,118 - INFO - training batch 1051, loss: 0.134, 33632/60000 datapoints
2025-03-06 20:17:55,322 - INFO - training batch 1101, loss: 0.376, 35232/60000 datapoints
2025-03-06 20:17:55,522 - INFO - training batch 1151, loss: 0.311, 36832/60000 datapoints
2025-03-06 20:17:55,725 - INFO - training batch 1201, loss: 0.373, 38432/60000 datapoints
2025-03-06 20:17:55,927 - INFO - training batch 1251, loss: 0.239, 40032/60000 datapoints
2025-03-06 20:17:56,128 - INFO - training batch 1301, loss: 0.129, 41632/60000 datapoints
2025-03-06 20:17:56,326 - INFO - training batch 1351, loss: 0.208, 43232/60000 datapoints
2025-03-06 20:17:56,536 - INFO - training batch 1401, loss: 0.395, 44832/60000 datapoints
2025-03-06 20:17:56,741 - INFO - training batch 1451, loss: 0.163, 46432/60000 datapoints
2025-03-06 20:17:56,941 - INFO - training batch 1501, loss: 0.169, 48032/60000 datapoints
2025-03-06 20:17:57,141 - INFO - training batch 1551, loss: 0.455, 49632/60000 datapoints
2025-03-06 20:17:57,341 - INFO - training batch 1601, loss: 0.492, 51232/60000 datapoints
2025-03-06 20:17:57,560 - INFO - training batch 1651, loss: 0.347, 52832/60000 datapoints
2025-03-06 20:17:57,779 - INFO - training batch 1701, loss: 0.280, 54432/60000 datapoints
2025-03-06 20:17:57,983 - INFO - training batch 1751, loss: 0.239, 56032/60000 datapoints
2025-03-06 20:17:58,186 - INFO - training batch 1801, loss: 0.132, 57632/60000 datapoints
2025-03-06 20:17:58,390 - INFO - training batch 1851, loss: 0.268, 59232/60000 datapoints
2025-03-06 20:17:58,497 - INFO - validation batch 1, loss: 0.109, 32/10016 datapoints
2025-03-06 20:17:58,665 - INFO - validation batch 51, loss: 0.130, 1632/10016 datapoints
2025-03-06 20:17:58,826 - INFO - validation batch 101, loss: 0.321, 3232/10016 datapoints
2025-03-06 20:17:58,991 - INFO - validation batch 151, loss: 0.306, 4832/10016 datapoints
2025-03-06 20:17:59,153 - INFO - validation batch 201, loss: 0.096, 6432/10016 datapoints
2025-03-06 20:17:59,312 - INFO - validation batch 251, loss: 0.118, 8032/10016 datapoints
2025-03-06 20:17:59,479 - INFO - validation batch 301, loss: 0.427, 9632/10016 datapoints
2025-03-06 20:17:59,522 - INFO - Epoch 662/800 done.
2025-03-06 20:17:59,522 - INFO - Final validation performance:
Loss: 0.215, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:17:59,522 - INFO - Beginning epoch 663/800
2025-03-06 20:17:59,531 - INFO - training batch 1, loss: 0.122, 32/60000 datapoints
2025-03-06 20:17:59,752 - INFO - training batch 51, loss: 0.057, 1632/60000 datapoints
2025-03-06 20:17:59,954 - INFO - training batch 101, loss: 0.220, 3232/60000 datapoints
2025-03-06 20:18:00,176 - INFO - training batch 151, loss: 0.180, 4832/60000 datapoints
2025-03-06 20:18:00,376 - INFO - training batch 201, loss: 0.113, 6432/60000 datapoints
2025-03-06 20:18:00,585 - INFO - training batch 251, loss: 0.186, 8032/60000 datapoints
2025-03-06 20:18:00,794 - INFO - training batch 301, loss: 0.167, 9632/60000 datapoints
2025-03-06 20:18:00,998 - INFO - training batch 351, loss: 0.296, 11232/60000 datapoints
2025-03-06 20:18:01,199 - INFO - training batch 401, loss: 0.167, 12832/60000 datapoints
2025-03-06 20:18:01,402 - INFO - training batch 451, loss: 0.167, 14432/60000 datapoints
2025-03-06 20:18:01,605 - INFO - training batch 501, loss: 0.185, 16032/60000 datapoints
2025-03-06 20:18:01,825 - INFO - training batch 551, loss: 0.134, 17632/60000 datapoints
2025-03-06 20:18:02,035 - INFO - training batch 601, loss: 0.311, 19232/60000 datapoints
2025-03-06 20:18:02,279 - INFO - training batch 651, loss: 0.655, 20832/60000 datapoints
2025-03-06 20:18:02,507 - INFO - training batch 701, loss: 0.116, 22432/60000 datapoints
2025-03-06 20:18:02,717 - INFO - training batch 751, loss: 0.334, 24032/60000 datapoints
2025-03-06 20:18:02,920 - INFO - training batch 801, loss: 0.319, 25632/60000 datapoints
2025-03-06 20:18:03,148 - INFO - training batch 851, loss: 0.296, 27232/60000 datapoints
2025-03-06 20:18:03,364 - INFO - training batch 901, loss: 0.129, 28832/60000 datapoints
2025-03-06 20:18:03,589 - INFO - training batch 951, loss: 0.324, 30432/60000 datapoints
2025-03-06 20:18:03,811 - INFO - training batch 1001, loss: 0.100, 32032/60000 datapoints
2025-03-06 20:18:04,016 - INFO - training batch 1051, loss: 0.191, 33632/60000 datapoints
2025-03-06 20:18:04,223 - INFO - training batch 1101, loss: 0.130, 35232/60000 datapoints
2025-03-06 20:18:04,430 - INFO - training batch 1151, loss: 0.146, 36832/60000 datapoints
2025-03-06 20:18:04,637 - INFO - training batch 1201, loss: 0.113, 38432/60000 datapoints
2025-03-06 20:18:04,846 - INFO - training batch 1251, loss: 0.268, 40032/60000 datapoints
2025-03-06 20:18:05,056 - INFO - training batch 1301, loss: 0.172, 41632/60000 datapoints
2025-03-06 20:18:05,260 - INFO - training batch 1351, loss: 0.143, 43232/60000 datapoints
2025-03-06 20:18:05,466 - INFO - training batch 1401, loss: 0.276, 44832/60000 datapoints
2025-03-06 20:18:05,673 - INFO - training batch 1451, loss: 0.308, 46432/60000 datapoints
2025-03-06 20:18:05,887 - INFO - training batch 1501, loss: 0.229, 48032/60000 datapoints
2025-03-06 20:18:06,094 - INFO - training batch 1551, loss: 0.484, 49632/60000 datapoints
2025-03-06 20:18:06,301 - INFO - training batch 1601, loss: 0.183, 51232/60000 datapoints
2025-03-06 20:18:06,521 - INFO - training batch 1651, loss: 0.204, 52832/60000 datapoints
2025-03-06 20:18:06,765 - INFO - training batch 1701, loss: 0.111, 54432/60000 datapoints
2025-03-06 20:18:06,975 - INFO - training batch 1751, loss: 0.350, 56032/60000 datapoints
2025-03-06 20:18:07,182 - INFO - training batch 1801, loss: 0.216, 57632/60000 datapoints
2025-03-06 20:18:07,389 - INFO - training batch 1851, loss: 0.298, 59232/60000 datapoints
2025-03-06 20:18:07,495 - INFO - validation batch 1, loss: 0.213, 32/10016 datapoints
2025-03-06 20:18:07,661 - INFO - validation batch 51, loss: 0.222, 1632/10016 datapoints
2025-03-06 20:18:07,827 - INFO - validation batch 101, loss: 0.206, 3232/10016 datapoints
2025-03-06 20:18:07,994 - INFO - validation batch 151, loss: 0.108, 4832/10016 datapoints
2025-03-06 20:18:08,159 - INFO - validation batch 201, loss: 0.122, 6432/10016 datapoints
2025-03-06 20:18:08,324 - INFO - validation batch 251, loss: 0.209, 8032/10016 datapoints
2025-03-06 20:18:08,493 - INFO - validation batch 301, loss: 0.429, 9632/10016 datapoints
2025-03-06 20:18:08,534 - INFO - Epoch 663/800 done.
2025-03-06 20:18:08,535 - INFO - Final validation performance:
Loss: 0.215, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:18:08,535 - INFO - Beginning epoch 664/800
2025-03-06 20:18:08,542 - INFO - training batch 1, loss: 0.297, 32/60000 datapoints
2025-03-06 20:18:08,770 - INFO - training batch 51, loss: 0.150, 1632/60000 datapoints
2025-03-06 20:18:08,979 - INFO - training batch 101, loss: 0.272, 3232/60000 datapoints
2025-03-06 20:18:09,210 - INFO - training batch 151, loss: 0.108, 4832/60000 datapoints
2025-03-06 20:18:09,423 - INFO - training batch 201, loss: 0.364, 6432/60000 datapoints
2025-03-06 20:18:09,641 - INFO - training batch 251, loss: 0.232, 8032/60000 datapoints
2025-03-06 20:18:09,851 - INFO - training batch 301, loss: 0.261, 9632/60000 datapoints
2025-03-06 20:18:10,088 - INFO - training batch 351, loss: 0.179, 11232/60000 datapoints
2025-03-06 20:18:10,296 - INFO - training batch 401, loss: 0.141, 12832/60000 datapoints
2025-03-06 20:18:10,501 - INFO - training batch 451, loss: 0.091, 14432/60000 datapoints
2025-03-06 20:18:10,713 - INFO - training batch 501, loss: 0.333, 16032/60000 datapoints
2025-03-06 20:18:10,921 - INFO - training batch 551, loss: 0.313, 17632/60000 datapoints
2025-03-06 20:18:11,128 - INFO - training batch 601, loss: 0.287, 19232/60000 datapoints
2025-03-06 20:18:11,337 - INFO - training batch 651, loss: 0.148, 20832/60000 datapoints
2025-03-06 20:18:11,547 - INFO - training batch 701, loss: 0.097, 22432/60000 datapoints
2025-03-06 20:18:11,763 - INFO - training batch 751, loss: 0.315, 24032/60000 datapoints
2025-03-06 20:18:11,971 - INFO - training batch 801, loss: 0.734, 25632/60000 datapoints
2025-03-06 20:18:12,176 - INFO - training batch 851, loss: 0.224, 27232/60000 datapoints
2025-03-06 20:18:12,384 - INFO - training batch 901, loss: 0.223, 28832/60000 datapoints
2025-03-06 20:18:12,589 - INFO - training batch 951, loss: 0.091, 30432/60000 datapoints
2025-03-06 20:18:12,802 - INFO - training batch 1001, loss: 0.299, 32032/60000 datapoints
2025-03-06 20:18:13,015 - INFO - training batch 1051, loss: 0.195, 33632/60000 datapoints
2025-03-06 20:18:13,250 - INFO - training batch 1101, loss: 0.298, 35232/60000 datapoints
2025-03-06 20:18:13,458 - INFO - training batch 1151, loss: 0.269, 36832/60000 datapoints
2025-03-06 20:18:13,668 - INFO - training batch 1201, loss: 0.203, 38432/60000 datapoints
2025-03-06 20:18:13,877 - INFO - training batch 1251, loss: 0.541, 40032/60000 datapoints
2025-03-06 20:18:14,090 - INFO - training batch 1301, loss: 0.363, 41632/60000 datapoints
2025-03-06 20:18:14,296 - INFO - training batch 1351, loss: 0.109, 43232/60000 datapoints
2025-03-06 20:18:14,503 - INFO - training batch 1401, loss: 0.182, 44832/60000 datapoints
2025-03-06 20:18:14,705 - INFO - training batch 1451, loss: 0.309, 46432/60000 datapoints
2025-03-06 20:18:14,909 - INFO - training batch 1501, loss: 0.224, 48032/60000 datapoints
2025-03-06 20:18:15,116 - INFO - training batch 1551, loss: 0.359, 49632/60000 datapoints
2025-03-06 20:18:15,316 - INFO - training batch 1601, loss: 0.646, 51232/60000 datapoints
2025-03-06 20:18:15,518 - INFO - training batch 1651, loss: 0.545, 52832/60000 datapoints
2025-03-06 20:18:15,723 - INFO - training batch 1701, loss: 0.207, 54432/60000 datapoints
2025-03-06 20:18:15,928 - INFO - training batch 1751, loss: 0.406, 56032/60000 datapoints
2025-03-06 20:18:16,130 - INFO - training batch 1801, loss: 0.057, 57632/60000 datapoints
2025-03-06 20:18:16,363 - INFO - training batch 1851, loss: 0.389, 59232/60000 datapoints
2025-03-06 20:18:16,472 - INFO - validation batch 1, loss: 0.159, 32/10016 datapoints
2025-03-06 20:18:16,639 - INFO - validation batch 51, loss: 0.178, 1632/10016 datapoints
2025-03-06 20:18:16,805 - INFO - validation batch 101, loss: 0.156, 3232/10016 datapoints
2025-03-06 20:18:16,968 - INFO - validation batch 151, loss: 0.090, 4832/10016 datapoints
2025-03-06 20:18:17,132 - INFO - validation batch 201, loss: 0.190, 6432/10016 datapoints
2025-03-06 20:18:17,296 - INFO - validation batch 251, loss: 0.168, 8032/10016 datapoints
2025-03-06 20:18:17,465 - INFO - validation batch 301, loss: 0.155, 9632/10016 datapoints
2025-03-06 20:18:17,509 - INFO - Epoch 664/800 done.
2025-03-06 20:18:17,509 - INFO - Final validation performance:
Loss: 0.157, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:18:17,509 - INFO - Beginning epoch 665/800
2025-03-06 20:18:17,518 - INFO - training batch 1, loss: 0.272, 32/60000 datapoints
2025-03-06 20:18:17,738 - INFO - training batch 51, loss: 0.062, 1632/60000 datapoints
2025-03-06 20:18:17,934 - INFO - training batch 101, loss: 0.331, 3232/60000 datapoints
2025-03-06 20:18:18,137 - INFO - training batch 151, loss: 0.078, 4832/60000 datapoints
2025-03-06 20:18:18,337 - INFO - training batch 201, loss: 0.502, 6432/60000 datapoints
2025-03-06 20:18:18,536 - INFO - training batch 251, loss: 0.251, 8032/60000 datapoints
2025-03-06 20:18:18,735 - INFO - training batch 301, loss: 0.288, 9632/60000 datapoints
2025-03-06 20:18:18,930 - INFO - training batch 351, loss: 0.303, 11232/60000 datapoints
2025-03-06 20:18:19,126 - INFO - training batch 401, loss: 0.166, 12832/60000 datapoints
2025-03-06 20:18:19,317 - INFO - training batch 451, loss: 0.356, 14432/60000 datapoints
2025-03-06 20:18:19,516 - INFO - training batch 501, loss: 0.106, 16032/60000 datapoints
2025-03-06 20:18:19,715 - INFO - training batch 551, loss: 0.135, 17632/60000 datapoints
2025-03-06 20:18:19,907 - INFO - training batch 601, loss: 0.451, 19232/60000 datapoints
2025-03-06 20:18:20,104 - INFO - training batch 651, loss: 0.188, 20832/60000 datapoints
2025-03-06 20:18:20,297 - INFO - training batch 701, loss: 0.129, 22432/60000 datapoints
2025-03-06 20:18:20,490 - INFO - training batch 751, loss: 0.181, 24032/60000 datapoints
2025-03-06 20:18:20,685 - INFO - training batch 801, loss: 0.096, 25632/60000 datapoints
2025-03-06 20:18:20,881 - INFO - training batch 851, loss: 0.075, 27232/60000 datapoints
2025-03-06 20:18:21,075 - INFO - training batch 901, loss: 0.139, 28832/60000 datapoints
2025-03-06 20:18:21,267 - INFO - training batch 951, loss: 0.398, 30432/60000 datapoints
2025-03-06 20:18:21,463 - INFO - training batch 1001, loss: 0.250, 32032/60000 datapoints
2025-03-06 20:18:21,659 - INFO - training batch 1051, loss: 0.127, 33632/60000 datapoints
2025-03-06 20:18:21,855 - INFO - training batch 1101, loss: 0.326, 35232/60000 datapoints
2025-03-06 20:18:22,063 - INFO - training batch 1151, loss: 0.154, 36832/60000 datapoints
2025-03-06 20:18:22,267 - INFO - training batch 1201, loss: 0.401, 38432/60000 datapoints
2025-03-06 20:18:22,469 - INFO - training batch 1251, loss: 0.154, 40032/60000 datapoints
2025-03-06 20:18:22,673 - INFO - training batch 1301, loss: 0.151, 41632/60000 datapoints
2025-03-06 20:18:22,874 - INFO - training batch 1351, loss: 0.291, 43232/60000 datapoints
2025-03-06 20:18:23,076 - INFO - training batch 1401, loss: 0.132, 44832/60000 datapoints
2025-03-06 20:18:23,293 - INFO - training batch 1451, loss: 0.429, 46432/60000 datapoints
2025-03-06 20:18:23,500 - INFO - training batch 1501, loss: 0.155, 48032/60000 datapoints
2025-03-06 20:18:23,703 - INFO - training batch 1551, loss: 0.166, 49632/60000 datapoints
2025-03-06 20:18:23,907 - INFO - training batch 1601, loss: 0.186, 51232/60000 datapoints
2025-03-06 20:18:24,109 - INFO - training batch 1651, loss: 0.190, 52832/60000 datapoints
2025-03-06 20:18:24,317 - INFO - training batch 1701, loss: 0.078, 54432/60000 datapoints
2025-03-06 20:18:24,524 - INFO - training batch 1751, loss: 0.172, 56032/60000 datapoints
2025-03-06 20:18:24,727 - INFO - training batch 1801, loss: 0.148, 57632/60000 datapoints
2025-03-06 20:18:24,935 - INFO - training batch 1851, loss: 0.453, 59232/60000 datapoints
2025-03-06 20:18:25,042 - INFO - validation batch 1, loss: 0.178, 32/10016 datapoints
2025-03-06 20:18:25,206 - INFO - validation batch 51, loss: 0.094, 1632/10016 datapoints
2025-03-06 20:18:25,369 - INFO - validation batch 101, loss: 0.131, 3232/10016 datapoints
2025-03-06 20:18:25,537 - INFO - validation batch 151, loss: 0.316, 4832/10016 datapoints
2025-03-06 20:18:25,702 - INFO - validation batch 201, loss: 0.604, 6432/10016 datapoints
2025-03-06 20:18:25,867 - INFO - validation batch 251, loss: 0.310, 8032/10016 datapoints
2025-03-06 20:18:26,034 - INFO - validation batch 301, loss: 0.087, 9632/10016 datapoints
2025-03-06 20:18:26,072 - INFO - Epoch 665/800 done.
2025-03-06 20:18:26,073 - INFO - Final validation performance:
Loss: 0.246, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:18:26,073 - INFO - Beginning epoch 666/800
2025-03-06 20:18:26,084 - INFO - training batch 1, loss: 0.159, 32/60000 datapoints
2025-03-06 20:18:26,281 - INFO - training batch 51, loss: 0.219, 1632/60000 datapoints
2025-03-06 20:18:26,480 - INFO - training batch 101, loss: 0.180, 3232/60000 datapoints
2025-03-06 20:18:26,699 - INFO - training batch 151, loss: 0.548, 4832/60000 datapoints
2025-03-06 20:18:26,896 - INFO - training batch 201, loss: 0.138, 6432/60000 datapoints
2025-03-06 20:18:27,097 - INFO - training batch 251, loss: 0.251, 8032/60000 datapoints
2025-03-06 20:18:27,302 - INFO - training batch 301, loss: 0.096, 9632/60000 datapoints
2025-03-06 20:18:27,504 - INFO - training batch 351, loss: 0.159, 11232/60000 datapoints
2025-03-06 20:18:27,705 - INFO - training batch 401, loss: 0.102, 12832/60000 datapoints
2025-03-06 20:18:27,899 - INFO - training batch 451, loss: 0.287, 14432/60000 datapoints
2025-03-06 20:18:28,094 - INFO - training batch 501, loss: 0.639, 16032/60000 datapoints
2025-03-06 20:18:28,295 - INFO - training batch 551, loss: 0.150, 17632/60000 datapoints
2025-03-06 20:18:28,496 - INFO - training batch 601, loss: 0.102, 19232/60000 datapoints
2025-03-06 20:18:28,695 - INFO - training batch 651, loss: 0.452, 20832/60000 datapoints
2025-03-06 20:18:28,893 - INFO - training batch 701, loss: 0.342, 22432/60000 datapoints
2025-03-06 20:18:29,098 - INFO - training batch 751, loss: 0.565, 24032/60000 datapoints
2025-03-06 20:18:29,303 - INFO - training batch 801, loss: 0.076, 25632/60000 datapoints
2025-03-06 20:18:29,505 - INFO - training batch 851, loss: 0.394, 27232/60000 datapoints
2025-03-06 20:18:29,709 - INFO - training batch 901, loss: 0.160, 28832/60000 datapoints
2025-03-06 20:18:29,911 - INFO - training batch 951, loss: 0.092, 30432/60000 datapoints
2025-03-06 20:18:30,114 - INFO - training batch 1001, loss: 0.359, 32032/60000 datapoints
2025-03-06 20:18:30,314 - INFO - training batch 1051, loss: 0.115, 33632/60000 datapoints
2025-03-06 20:18:30,513 - INFO - training batch 1101, loss: 0.136, 35232/60000 datapoints
2025-03-06 20:18:30,714 - INFO - training batch 1151, loss: 0.124, 36832/60000 datapoints
2025-03-06 20:18:30,915 - INFO - training batch 1201, loss: 0.055, 38432/60000 datapoints
2025-03-06 20:18:31,118 - INFO - training batch 1251, loss: 0.159, 40032/60000 datapoints
2025-03-06 20:18:31,320 - INFO - training batch 1301, loss: 0.245, 41632/60000 datapoints
2025-03-06 20:18:31,523 - INFO - training batch 1351, loss: 0.086, 43232/60000 datapoints
2025-03-06 20:18:31,729 - INFO - training batch 1401, loss: 0.186, 44832/60000 datapoints
2025-03-06 20:18:31,952 - INFO - training batch 1451, loss: 0.561, 46432/60000 datapoints
2025-03-06 20:18:32,161 - INFO - training batch 1501, loss: 0.166, 48032/60000 datapoints
2025-03-06 20:18:32,360 - INFO - training batch 1551, loss: 0.141, 49632/60000 datapoints
2025-03-06 20:18:32,563 - INFO - training batch 1601, loss: 0.182, 51232/60000 datapoints
2025-03-06 20:18:32,768 - INFO - training batch 1651, loss: 0.166, 52832/60000 datapoints
2025-03-06 20:18:32,971 - INFO - training batch 1701, loss: 0.377, 54432/60000 datapoints
2025-03-06 20:18:33,171 - INFO - training batch 1751, loss: 0.400, 56032/60000 datapoints
2025-03-06 20:18:33,418 - INFO - training batch 1801, loss: 0.218, 57632/60000 datapoints
2025-03-06 20:18:33,648 - INFO - training batch 1851, loss: 0.066, 59232/60000 datapoints
2025-03-06 20:18:33,769 - INFO - validation batch 1, loss: 0.163, 32/10016 datapoints
2025-03-06 20:18:33,960 - INFO - validation batch 51, loss: 0.414, 1632/10016 datapoints
2025-03-06 20:18:34,130 - INFO - validation batch 101, loss: 0.247, 3232/10016 datapoints
2025-03-06 20:18:34,293 - INFO - validation batch 151, loss: 0.110, 4832/10016 datapoints
2025-03-06 20:18:34,461 - INFO - validation batch 201, loss: 0.107, 6432/10016 datapoints
2025-03-06 20:18:34,622 - INFO - validation batch 251, loss: 0.383, 8032/10016 datapoints
2025-03-06 20:18:34,784 - INFO - validation batch 301, loss: 0.203, 9632/10016 datapoints
2025-03-06 20:18:34,822 - INFO - Epoch 666/800 done.
2025-03-06 20:18:34,822 - INFO - Final validation performance:
Loss: 0.232, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:18:34,823 - INFO - Beginning epoch 667/800
2025-03-06 20:18:34,830 - INFO - training batch 1, loss: 0.370, 32/60000 datapoints
2025-03-06 20:18:35,059 - INFO - training batch 51, loss: 0.258, 1632/60000 datapoints
2025-03-06 20:18:35,261 - INFO - training batch 101, loss: 0.251, 3232/60000 datapoints
2025-03-06 20:18:35,477 - INFO - training batch 151, loss: 0.126, 4832/60000 datapoints
2025-03-06 20:18:35,686 - INFO - training batch 201, loss: 0.545, 6432/60000 datapoints
2025-03-06 20:18:35,885 - INFO - training batch 251, loss: 0.086, 8032/60000 datapoints
2025-03-06 20:18:36,087 - INFO - training batch 301, loss: 0.146, 9632/60000 datapoints
2025-03-06 20:18:36,286 - INFO - training batch 351, loss: 0.617, 11232/60000 datapoints
2025-03-06 20:18:36,485 - INFO - training batch 401, loss: 0.251, 12832/60000 datapoints
2025-03-06 20:18:36,688 - INFO - training batch 451, loss: 0.419, 14432/60000 datapoints
2025-03-06 20:18:36,900 - INFO - training batch 501, loss: 0.107, 16032/60000 datapoints
2025-03-06 20:18:37,106 - INFO - training batch 551, loss: 0.193, 17632/60000 datapoints
2025-03-06 20:18:37,306 - INFO - training batch 601, loss: 0.123, 19232/60000 datapoints
2025-03-06 20:18:37,511 - INFO - training batch 651, loss: 0.312, 20832/60000 datapoints
2025-03-06 20:18:37,731 - INFO - training batch 701, loss: 0.115, 22432/60000 datapoints
2025-03-06 20:18:37,963 - INFO - training batch 751, loss: 0.114, 24032/60000 datapoints
2025-03-06 20:18:38,175 - INFO - training batch 801, loss: 0.372, 25632/60000 datapoints
2025-03-06 20:18:38,383 - INFO - training batch 851, loss: 0.139, 27232/60000 datapoints
2025-03-06 20:18:38,582 - INFO - training batch 901, loss: 0.142, 28832/60000 datapoints
2025-03-06 20:18:38,784 - INFO - training batch 951, loss: 0.132, 30432/60000 datapoints
2025-03-06 20:18:38,985 - INFO - training batch 1001, loss: 0.260, 32032/60000 datapoints
2025-03-06 20:18:39,192 - INFO - training batch 1051, loss: 0.140, 33632/60000 datapoints
2025-03-06 20:18:39,397 - INFO - training batch 1101, loss: 0.329, 35232/60000 datapoints
2025-03-06 20:18:39,600 - INFO - training batch 1151, loss: 0.092, 36832/60000 datapoints
2025-03-06 20:18:39,800 - INFO - training batch 1201, loss: 0.164, 38432/60000 datapoints
2025-03-06 20:18:40,005 - INFO - training batch 1251, loss: 0.203, 40032/60000 datapoints
2025-03-06 20:18:40,206 - INFO - training batch 1301, loss: 0.036, 41632/60000 datapoints
2025-03-06 20:18:40,408 - INFO - training batch 1351, loss: 0.225, 43232/60000 datapoints
2025-03-06 20:18:40,610 - INFO - training batch 1401, loss: 0.043, 44832/60000 datapoints
2025-03-06 20:18:40,814 - INFO - training batch 1451, loss: 0.201, 46432/60000 datapoints
2025-03-06 20:18:41,013 - INFO - training batch 1501, loss: 0.269, 48032/60000 datapoints
2025-03-06 20:18:41,217 - INFO - training batch 1551, loss: 0.360, 49632/60000 datapoints
2025-03-06 20:18:41,413 - INFO - training batch 1601, loss: 0.385, 51232/60000 datapoints
2025-03-06 20:18:41,613 - INFO - training batch 1651, loss: 0.168, 52832/60000 datapoints
2025-03-06 20:18:41,973 - INFO - training batch 1701, loss: 0.113, 54432/60000 datapoints
2025-03-06 20:18:42,177 - INFO - training batch 1751, loss: 0.439, 56032/60000 datapoints
2025-03-06 20:18:42,379 - INFO - training batch 1801, loss: 0.319, 57632/60000 datapoints
2025-03-06 20:18:42,589 - INFO - training batch 1851, loss: 0.285, 59232/60000 datapoints
2025-03-06 20:18:42,705 - INFO - validation batch 1, loss: 0.620, 32/10016 datapoints
2025-03-06 20:18:42,869 - INFO - validation batch 51, loss: 0.266, 1632/10016 datapoints
2025-03-06 20:18:43,034 - INFO - validation batch 101, loss: 0.144, 3232/10016 datapoints
2025-03-06 20:18:43,195 - INFO - validation batch 151, loss: 0.255, 4832/10016 datapoints
2025-03-06 20:18:43,355 - INFO - validation batch 201, loss: 0.212, 6432/10016 datapoints
2025-03-06 20:18:43,540 - INFO - validation batch 251, loss: 0.154, 8032/10016 datapoints
2025-03-06 20:18:43,705 - INFO - validation batch 301, loss: 0.322, 9632/10016 datapoints
2025-03-06 20:18:43,744 - INFO - Epoch 667/800 done.
2025-03-06 20:18:43,744 - INFO - Final validation performance:
Loss: 0.282, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:18:43,745 - INFO - Beginning epoch 668/800
2025-03-06 20:18:43,752 - INFO - training batch 1, loss: 0.180, 32/60000 datapoints
2025-03-06 20:18:43,952 - INFO - training batch 51, loss: 0.037, 1632/60000 datapoints
2025-03-06 20:18:44,169 - INFO - training batch 101, loss: 0.183, 3232/60000 datapoints
2025-03-06 20:18:44,368 - INFO - training batch 151, loss: 0.312, 4832/60000 datapoints
2025-03-06 20:18:44,572 - INFO - training batch 201, loss: 0.142, 6432/60000 datapoints
2025-03-06 20:18:44,800 - INFO - training batch 251, loss: 0.554, 8032/60000 datapoints
2025-03-06 20:18:45,009 - INFO - training batch 301, loss: 0.758, 9632/60000 datapoints
2025-03-06 20:18:45,215 - INFO - training batch 351, loss: 0.132, 11232/60000 datapoints
2025-03-06 20:18:45,415 - INFO - training batch 401, loss: 0.366, 12832/60000 datapoints
2025-03-06 20:18:45,618 - INFO - training batch 451, loss: 0.210, 14432/60000 datapoints
2025-03-06 20:18:45,821 - INFO - training batch 501, loss: 0.241, 16032/60000 datapoints
2025-03-06 20:18:46,031 - INFO - training batch 551, loss: 0.307, 17632/60000 datapoints
2025-03-06 20:18:46,238 - INFO - training batch 601, loss: 0.205, 19232/60000 datapoints
2025-03-06 20:18:46,443 - INFO - training batch 651, loss: 0.291, 20832/60000 datapoints
2025-03-06 20:18:46,650 - INFO - training batch 701, loss: 0.077, 22432/60000 datapoints
2025-03-06 20:18:46,854 - INFO - training batch 751, loss: 0.246, 24032/60000 datapoints
2025-03-06 20:18:47,061 - INFO - training batch 801, loss: 0.121, 25632/60000 datapoints
2025-03-06 20:18:47,266 - INFO - training batch 851, loss: 0.282, 27232/60000 datapoints
2025-03-06 20:18:47,475 - INFO - training batch 901, loss: 0.218, 28832/60000 datapoints
2025-03-06 20:18:47,690 - INFO - training batch 951, loss: 0.480, 30432/60000 datapoints
2025-03-06 20:18:47,896 - INFO - training batch 1001, loss: 0.426, 32032/60000 datapoints
2025-03-06 20:18:48,104 - INFO - training batch 1051, loss: 0.335, 33632/60000 datapoints
2025-03-06 20:18:48,310 - INFO - training batch 1101, loss: 0.425, 35232/60000 datapoints
2025-03-06 20:18:48,518 - INFO - training batch 1151, loss: 0.180, 36832/60000 datapoints
2025-03-06 20:18:48,727 - INFO - training batch 1201, loss: 0.283, 38432/60000 datapoints
2025-03-06 20:18:48,935 - INFO - training batch 1251, loss: 0.117, 40032/60000 datapoints
2025-03-06 20:18:49,146 - INFO - training batch 1301, loss: 0.090, 41632/60000 datapoints
2025-03-06 20:18:49,357 - INFO - training batch 1351, loss: 0.077, 43232/60000 datapoints
2025-03-06 20:18:49,563 - INFO - training batch 1401, loss: 0.193, 44832/60000 datapoints
2025-03-06 20:18:49,774 - INFO - training batch 1451, loss: 0.275, 46432/60000 datapoints
2025-03-06 20:18:49,981 - INFO - training batch 1501, loss: 0.132, 48032/60000 datapoints
2025-03-06 20:18:50,194 - INFO - training batch 1551, loss: 0.183, 49632/60000 datapoints
2025-03-06 20:18:50,400 - INFO - training batch 1601, loss: 0.309, 51232/60000 datapoints
2025-03-06 20:18:50,604 - INFO - training batch 1651, loss: 0.318, 52832/60000 datapoints
2025-03-06 20:18:50,814 - INFO - training batch 1701, loss: 0.382, 54432/60000 datapoints
2025-03-06 20:18:51,019 - INFO - training batch 1751, loss: 0.227, 56032/60000 datapoints
2025-03-06 20:18:51,226 - INFO - training batch 1801, loss: 0.192, 57632/60000 datapoints
2025-03-06 20:18:51,432 - INFO - training batch 1851, loss: 0.164, 59232/60000 datapoints
2025-03-06 20:18:51,546 - INFO - validation batch 1, loss: 0.122, 32/10016 datapoints
2025-03-06 20:18:51,718 - INFO - validation batch 51, loss: 0.221, 1632/10016 datapoints
2025-03-06 20:18:51,891 - INFO - validation batch 101, loss: 0.246, 3232/10016 datapoints
2025-03-06 20:18:52,057 - INFO - validation batch 151, loss: 0.242, 4832/10016 datapoints
2025-03-06 20:18:52,226 - INFO - validation batch 201, loss: 0.098, 6432/10016 datapoints
2025-03-06 20:18:52,390 - INFO - validation batch 251, loss: 0.165, 8032/10016 datapoints
2025-03-06 20:18:52,559 - INFO - validation batch 301, loss: 0.117, 9632/10016 datapoints
2025-03-06 20:18:52,601 - INFO - Epoch 668/800 done.
2025-03-06 20:18:52,601 - INFO - Final validation performance:
Loss: 0.173, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:18:52,601 - INFO - Beginning epoch 669/800
2025-03-06 20:18:52,609 - INFO - training batch 1, loss: 0.254, 32/60000 datapoints
2025-03-06 20:18:52,836 - INFO - training batch 51, loss: 0.184, 1632/60000 datapoints
2025-03-06 20:18:53,042 - INFO - training batch 101, loss: 0.103, 3232/60000 datapoints
2025-03-06 20:18:53,262 - INFO - training batch 151, loss: 0.364, 4832/60000 datapoints
2025-03-06 20:18:53,472 - INFO - training batch 201, loss: 0.106, 6432/60000 datapoints
2025-03-06 20:18:53,726 - INFO - training batch 251, loss: 0.078, 8032/60000 datapoints
2025-03-06 20:18:53,962 - INFO - training batch 301, loss: 0.163, 9632/60000 datapoints
2025-03-06 20:18:54,192 - INFO - training batch 351, loss: 0.249, 11232/60000 datapoints
2025-03-06 20:18:54,400 - INFO - training batch 401, loss: 0.155, 12832/60000 datapoints
2025-03-06 20:18:54,618 - INFO - training batch 451, loss: 0.423, 14432/60000 datapoints
2025-03-06 20:18:54,829 - INFO - training batch 501, loss: 0.150, 16032/60000 datapoints
2025-03-06 20:18:55,040 - INFO - training batch 551, loss: 0.154, 17632/60000 datapoints
2025-03-06 20:18:55,250 - INFO - training batch 601, loss: 0.285, 19232/60000 datapoints
2025-03-06 20:18:55,457 - INFO - training batch 651, loss: 0.378, 20832/60000 datapoints
2025-03-06 20:18:55,667 - INFO - training batch 701, loss: 0.244, 22432/60000 datapoints
2025-03-06 20:18:55,874 - INFO - training batch 751, loss: 0.352, 24032/60000 datapoints
2025-03-06 20:18:56,082 - INFO - training batch 801, loss: 0.217, 25632/60000 datapoints
2025-03-06 20:18:56,295 - INFO - training batch 851, loss: 0.211, 27232/60000 datapoints
2025-03-06 20:18:56,500 - INFO - training batch 901, loss: 0.396, 28832/60000 datapoints
2025-03-06 20:18:56,709 - INFO - training batch 951, loss: 0.240, 30432/60000 datapoints
2025-03-06 20:18:56,918 - INFO - training batch 1001, loss: 0.050, 32032/60000 datapoints
2025-03-06 20:18:57,123 - INFO - training batch 1051, loss: 0.331, 33632/60000 datapoints
2025-03-06 20:18:57,334 - INFO - training batch 1101, loss: 0.171, 35232/60000 datapoints
2025-03-06 20:18:57,545 - INFO - training batch 1151, loss: 0.238, 36832/60000 datapoints
2025-03-06 20:18:57,753 - INFO - training batch 1201, loss: 0.149, 38432/60000 datapoints
2025-03-06 20:18:57,958 - INFO - training batch 1251, loss: 0.274, 40032/60000 datapoints
2025-03-06 20:18:58,165 - INFO - training batch 1301, loss: 0.301, 41632/60000 datapoints
2025-03-06 20:18:58,370 - INFO - training batch 1351, loss: 0.232, 43232/60000 datapoints
2025-03-06 20:18:58,582 - INFO - training batch 1401, loss: 0.237, 44832/60000 datapoints
2025-03-06 20:18:58,794 - INFO - training batch 1451, loss: 0.151, 46432/60000 datapoints
2025-03-06 20:18:59,003 - INFO - training batch 1501, loss: 0.237, 48032/60000 datapoints
2025-03-06 20:18:59,210 - INFO - training batch 1551, loss: 0.323, 49632/60000 datapoints
2025-03-06 20:18:59,422 - INFO - training batch 1601, loss: 0.238, 51232/60000 datapoints
2025-03-06 20:18:59,636 - INFO - training batch 1651, loss: 0.049, 52832/60000 datapoints
2025-03-06 20:18:59,847 - INFO - training batch 1701, loss: 0.118, 54432/60000 datapoints
2025-03-06 20:19:00,056 - INFO - training batch 1751, loss: 0.411, 56032/60000 datapoints
2025-03-06 20:19:00,267 - INFO - training batch 1801, loss: 0.060, 57632/60000 datapoints
2025-03-06 20:19:00,473 - INFO - training batch 1851, loss: 0.216, 59232/60000 datapoints
2025-03-06 20:19:00,586 - INFO - validation batch 1, loss: 0.072, 32/10016 datapoints
2025-03-06 20:19:00,761 - INFO - validation batch 51, loss: 0.232, 1632/10016 datapoints
2025-03-06 20:19:00,933 - INFO - validation batch 101, loss: 0.199, 3232/10016 datapoints
2025-03-06 20:19:01,101 - INFO - validation batch 151, loss: 0.196, 4832/10016 datapoints
2025-03-06 20:19:01,269 - INFO - validation batch 201, loss: 0.563, 6432/10016 datapoints
2025-03-06 20:19:01,436 - INFO - validation batch 251, loss: 0.136, 8032/10016 datapoints
2025-03-06 20:19:01,601 - INFO - validation batch 301, loss: 0.377, 9632/10016 datapoints
2025-03-06 20:19:01,649 - INFO - Epoch 669/800 done.
2025-03-06 20:19:01,649 - INFO - Final validation performance:
Loss: 0.254, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:19:01,650 - INFO - Beginning epoch 670/800
2025-03-06 20:19:01,657 - INFO - training batch 1, loss: 0.243, 32/60000 datapoints
2025-03-06 20:19:01,861 - INFO - training batch 51, loss: 0.181, 1632/60000 datapoints
2025-03-06 20:19:02,061 - INFO - training batch 101, loss: 0.322, 3232/60000 datapoints
2025-03-06 20:19:02,277 - INFO - training batch 151, loss: 0.113, 4832/60000 datapoints
2025-03-06 20:19:02,485 - INFO - training batch 201, loss: 0.336, 6432/60000 datapoints
2025-03-06 20:19:02,696 - INFO - training batch 251, loss: 0.216, 8032/60000 datapoints
2025-03-06 20:19:02,904 - INFO - training batch 301, loss: 0.206, 9632/60000 datapoints
2025-03-06 20:19:03,112 - INFO - training batch 351, loss: 0.733, 11232/60000 datapoints
2025-03-06 20:19:03,318 - INFO - training batch 401, loss: 0.312, 12832/60000 datapoints
2025-03-06 20:19:03,518 - INFO - training batch 451, loss: 0.065, 14432/60000 datapoints
2025-03-06 20:19:03,755 - INFO - training batch 501, loss: 0.123, 16032/60000 datapoints
2025-03-06 20:19:03,956 - INFO - training batch 551, loss: 0.100, 17632/60000 datapoints
2025-03-06 20:19:04,157 - INFO - training batch 601, loss: 0.230, 19232/60000 datapoints
2025-03-06 20:19:04,360 - INFO - training batch 651, loss: 0.323, 20832/60000 datapoints
2025-03-06 20:19:04,572 - INFO - training batch 701, loss: 0.199, 22432/60000 datapoints
2025-03-06 20:19:04,774 - INFO - training batch 751, loss: 0.245, 24032/60000 datapoints
2025-03-06 20:19:04,986 - INFO - training batch 801, loss: 0.143, 25632/60000 datapoints
2025-03-06 20:19:05,186 - INFO - training batch 851, loss: 0.086, 27232/60000 datapoints
2025-03-06 20:19:05,387 - INFO - training batch 901, loss: 0.132, 28832/60000 datapoints
2025-03-06 20:19:05,589 - INFO - training batch 951, loss: 0.107, 30432/60000 datapoints
2025-03-06 20:19:05,793 - INFO - training batch 1001, loss: 0.092, 32032/60000 datapoints
2025-03-06 20:19:05,993 - INFO - training batch 1051, loss: 0.366, 33632/60000 datapoints
2025-03-06 20:19:06,202 - INFO - training batch 1101, loss: 0.199, 35232/60000 datapoints
2025-03-06 20:19:06,406 - INFO - training batch 1151, loss: 0.209, 36832/60000 datapoints
2025-03-06 20:19:06,606 - INFO - training batch 1201, loss: 0.143, 38432/60000 datapoints
2025-03-06 20:19:06,809 - INFO - training batch 1251, loss: 0.156, 40032/60000 datapoints
2025-03-06 20:19:07,012 - INFO - training batch 1301, loss: 0.094, 41632/60000 datapoints
2025-03-06 20:19:07,216 - INFO - training batch 1351, loss: 0.334, 43232/60000 datapoints
2025-03-06 20:19:07,419 - INFO - training batch 1401, loss: 0.432, 44832/60000 datapoints
2025-03-06 20:19:07,622 - INFO - training batch 1451, loss: 0.379, 46432/60000 datapoints
2025-03-06 20:19:07,829 - INFO - training batch 1501, loss: 0.223, 48032/60000 datapoints
2025-03-06 20:19:08,032 - INFO - training batch 1551, loss: 0.168, 49632/60000 datapoints
2025-03-06 20:19:08,237 - INFO - training batch 1601, loss: 0.337, 51232/60000 datapoints
2025-03-06 20:19:08,439 - INFO - training batch 1651, loss: 0.361, 52832/60000 datapoints
2025-03-06 20:19:08,641 - INFO - training batch 1701, loss: 0.106, 54432/60000 datapoints
2025-03-06 20:19:08,844 - INFO - training batch 1751, loss: 0.228, 56032/60000 datapoints
2025-03-06 20:19:09,050 - INFO - training batch 1801, loss: 0.243, 57632/60000 datapoints
2025-03-06 20:19:09,250 - INFO - training batch 1851, loss: 0.379, 59232/60000 datapoints
2025-03-06 20:19:09,355 - INFO - validation batch 1, loss: 0.247, 32/10016 datapoints
2025-03-06 20:19:09,521 - INFO - validation batch 51, loss: 0.059, 1632/10016 datapoints
2025-03-06 20:19:09,690 - INFO - validation batch 101, loss: 0.478, 3232/10016 datapoints
2025-03-06 20:19:09,850 - INFO - validation batch 151, loss: 0.185, 4832/10016 datapoints
2025-03-06 20:19:10,013 - INFO - validation batch 201, loss: 0.067, 6432/10016 datapoints
2025-03-06 20:19:10,181 - INFO - validation batch 251, loss: 0.074, 8032/10016 datapoints
2025-03-06 20:19:10,343 - INFO - validation batch 301, loss: 0.296, 9632/10016 datapoints
2025-03-06 20:19:10,386 - INFO - Epoch 670/800 done.
2025-03-06 20:19:10,386 - INFO - Final validation performance:
Loss: 0.201, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:19:10,387 - INFO - Beginning epoch 671/800
2025-03-06 20:19:10,394 - INFO - training batch 1, loss: 0.637, 32/60000 datapoints
2025-03-06 20:19:10,594 - INFO - training batch 51, loss: 0.572, 1632/60000 datapoints
2025-03-06 20:19:10,813 - INFO - training batch 101, loss: 0.189, 3232/60000 datapoints
2025-03-06 20:19:11,032 - INFO - training batch 151, loss: 0.374, 4832/60000 datapoints
2025-03-06 20:19:11,237 - INFO - training batch 201, loss: 0.128, 6432/60000 datapoints
2025-03-06 20:19:11,438 - INFO - training batch 251, loss: 0.296, 8032/60000 datapoints
2025-03-06 20:19:11,653 - INFO - training batch 301, loss: 0.146, 9632/60000 datapoints
2025-03-06 20:19:11,868 - INFO - training batch 351, loss: 0.273, 11232/60000 datapoints
2025-03-06 20:19:12,071 - INFO - training batch 401, loss: 0.182, 12832/60000 datapoints
2025-03-06 20:19:12,273 - INFO - training batch 451, loss: 0.388, 14432/60000 datapoints
2025-03-06 20:19:12,482 - INFO - training batch 501, loss: 0.245, 16032/60000 datapoints
2025-03-06 20:19:12,687 - INFO - training batch 551, loss: 0.196, 17632/60000 datapoints
2025-03-06 20:19:12,889 - INFO - training batch 601, loss: 0.118, 19232/60000 datapoints
2025-03-06 20:19:13,090 - INFO - training batch 651, loss: 0.268, 20832/60000 datapoints
2025-03-06 20:19:13,303 - INFO - training batch 701, loss: 0.148, 22432/60000 datapoints
2025-03-06 20:19:13,504 - INFO - training batch 751, loss: 0.101, 24032/60000 datapoints
2025-03-06 20:19:13,717 - INFO - training batch 801, loss: 0.226, 25632/60000 datapoints
2025-03-06 20:19:13,937 - INFO - training batch 851, loss: 0.129, 27232/60000 datapoints
2025-03-06 20:19:14,139 - INFO - training batch 901, loss: 0.320, 28832/60000 datapoints
2025-03-06 20:19:14,340 - INFO - training batch 951, loss: 0.070, 30432/60000 datapoints
2025-03-06 20:19:14,543 - INFO - training batch 1001, loss: 0.157, 32032/60000 datapoints
2025-03-06 20:19:14,749 - INFO - training batch 1051, loss: 0.072, 33632/60000 datapoints
2025-03-06 20:19:14,957 - INFO - training batch 1101, loss: 0.382, 35232/60000 datapoints
2025-03-06 20:19:15,164 - INFO - training batch 1151, loss: 0.279, 36832/60000 datapoints
2025-03-06 20:19:15,373 - INFO - training batch 1201, loss: 0.349, 38432/60000 datapoints
2025-03-06 20:19:15,580 - INFO - training batch 1251, loss: 0.350, 40032/60000 datapoints
2025-03-06 20:19:15,794 - INFO - training batch 1301, loss: 0.469, 41632/60000 datapoints
2025-03-06 20:19:16,008 - INFO - training batch 1351, loss: 0.149, 43232/60000 datapoints
2025-03-06 20:19:16,210 - INFO - training batch 1401, loss: 0.235, 44832/60000 datapoints
2025-03-06 20:19:16,420 - INFO - training batch 1451, loss: 0.193, 46432/60000 datapoints
2025-03-06 20:19:16,631 - INFO - training batch 1501, loss: 0.242, 48032/60000 datapoints
2025-03-06 20:19:16,838 - INFO - training batch 1551, loss: 0.317, 49632/60000 datapoints
2025-03-06 20:19:17,046 - INFO - training batch 1601, loss: 0.557, 51232/60000 datapoints
2025-03-06 20:19:17,254 - INFO - training batch 1651, loss: 0.134, 52832/60000 datapoints
2025-03-06 20:19:17,460 - INFO - training batch 1701, loss: 0.119, 54432/60000 datapoints
2025-03-06 20:19:17,673 - INFO - training batch 1751, loss: 0.157, 56032/60000 datapoints
2025-03-06 20:19:17,877 - INFO - training batch 1801, loss: 0.087, 57632/60000 datapoints
2025-03-06 20:19:18,083 - INFO - training batch 1851, loss: 0.507, 59232/60000 datapoints
2025-03-06 20:19:18,191 - INFO - validation batch 1, loss: 0.182, 32/10016 datapoints
2025-03-06 20:19:18,358 - INFO - validation batch 51, loss: 0.440, 1632/10016 datapoints
2025-03-06 20:19:18,526 - INFO - validation batch 101, loss: 0.182, 3232/10016 datapoints
2025-03-06 20:19:18,695 - INFO - validation batch 151, loss: 0.180, 4832/10016 datapoints
2025-03-06 20:19:18,863 - INFO - validation batch 201, loss: 0.106, 6432/10016 datapoints
2025-03-06 20:19:19,032 - INFO - validation batch 251, loss: 0.269, 8032/10016 datapoints
2025-03-06 20:19:19,199 - INFO - validation batch 301, loss: 0.445, 9632/10016 datapoints
2025-03-06 20:19:19,243 - INFO - Epoch 671/800 done.
2025-03-06 20:19:19,243 - INFO - Final validation performance:
Loss: 0.258, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:19:19,244 - INFO - Beginning epoch 672/800
2025-03-06 20:19:19,251 - INFO - training batch 1, loss: 0.107, 32/60000 datapoints
2025-03-06 20:19:19,458 - INFO - training batch 51, loss: 0.278, 1632/60000 datapoints
2025-03-06 20:19:19,674 - INFO - training batch 101, loss: 0.387, 3232/60000 datapoints
2025-03-06 20:19:19,899 - INFO - training batch 151, loss: 0.115, 4832/60000 datapoints
2025-03-06 20:19:20,107 - INFO - training batch 201, loss: 0.166, 6432/60000 datapoints
2025-03-06 20:19:20,317 - INFO - training batch 251, loss: 0.120, 8032/60000 datapoints
2025-03-06 20:19:20,530 - INFO - training batch 301, loss: 0.321, 9632/60000 datapoints
2025-03-06 20:19:20,742 - INFO - training batch 351, loss: 0.149, 11232/60000 datapoints
2025-03-06 20:19:20,946 - INFO - training batch 401, loss: 0.107, 12832/60000 datapoints
2025-03-06 20:19:21,151 - INFO - training batch 451, loss: 0.138, 14432/60000 datapoints
2025-03-06 20:19:21,361 - INFO - training batch 501, loss: 0.218, 16032/60000 datapoints
2025-03-06 20:19:21,568 - INFO - training batch 551, loss: 0.491, 17632/60000 datapoints
2025-03-06 20:19:21,781 - INFO - training batch 601, loss: 0.101, 19232/60000 datapoints
2025-03-06 20:19:21,988 - INFO - training batch 651, loss: 0.305, 20832/60000 datapoints
2025-03-06 20:19:22,193 - INFO - training batch 701, loss: 0.204, 22432/60000 datapoints
2025-03-06 20:19:22,403 - INFO - training batch 751, loss: 0.178, 24032/60000 datapoints
2025-03-06 20:19:22,611 - INFO - training batch 801, loss: 0.119, 25632/60000 datapoints
2025-03-06 20:19:22,821 - INFO - training batch 851, loss: 0.060, 27232/60000 datapoints
2025-03-06 20:19:23,029 - INFO - training batch 901, loss: 0.239, 28832/60000 datapoints
2025-03-06 20:19:23,235 - INFO - training batch 951, loss: 0.173, 30432/60000 datapoints
2025-03-06 20:19:23,442 - INFO - training batch 1001, loss: 0.191, 32032/60000 datapoints
2025-03-06 20:19:23,654 - INFO - training batch 1051, loss: 0.105, 33632/60000 datapoints
2025-03-06 20:19:23,885 - INFO - training batch 1101, loss: 0.096, 35232/60000 datapoints
2025-03-06 20:19:24,094 - INFO - training batch 1151, loss: 0.357, 36832/60000 datapoints
2025-03-06 20:19:24,305 - INFO - training batch 1201, loss: 0.181, 38432/60000 datapoints
2025-03-06 20:19:24,512 - INFO - training batch 1251, loss: 0.365, 40032/60000 datapoints
2025-03-06 20:19:24,727 - INFO - training batch 1301, loss: 0.082, 41632/60000 datapoints
2025-03-06 20:19:24,938 - INFO - training batch 1351, loss: 0.187, 43232/60000 datapoints
2025-03-06 20:19:25,148 - INFO - training batch 1401, loss: 0.186, 44832/60000 datapoints
2025-03-06 20:19:25,355 - INFO - training batch 1451, loss: 0.482, 46432/60000 datapoints
2025-03-06 20:19:25,562 - INFO - training batch 1501, loss: 0.331, 48032/60000 datapoints
2025-03-06 20:19:25,775 - INFO - training batch 1551, loss: 0.300, 49632/60000 datapoints
2025-03-06 20:19:25,983 - INFO - training batch 1601, loss: 0.148, 51232/60000 datapoints
2025-03-06 20:19:26,187 - INFO - training batch 1651, loss: 0.249, 52832/60000 datapoints
2025-03-06 20:19:26,391 - INFO - training batch 1701, loss: 0.456, 54432/60000 datapoints
2025-03-06 20:19:26,591 - INFO - training batch 1751, loss: 0.147, 56032/60000 datapoints
2025-03-06 20:19:26,794 - INFO - training batch 1801, loss: 0.228, 57632/60000 datapoints
2025-03-06 20:19:26,996 - INFO - training batch 1851, loss: 0.153, 59232/60000 datapoints
2025-03-06 20:19:27,106 - INFO - validation batch 1, loss: 0.088, 32/10016 datapoints
2025-03-06 20:19:27,269 - INFO - validation batch 51, loss: 0.254, 1632/10016 datapoints
2025-03-06 20:19:27,433 - INFO - validation batch 101, loss: 0.266, 3232/10016 datapoints
2025-03-06 20:19:27,597 - INFO - validation batch 151, loss: 0.097, 4832/10016 datapoints
2025-03-06 20:19:27,769 - INFO - validation batch 201, loss: 0.284, 6432/10016 datapoints
2025-03-06 20:19:27,934 - INFO - validation batch 251, loss: 0.473, 8032/10016 datapoints
2025-03-06 20:19:28,094 - INFO - validation batch 301, loss: 0.416, 9632/10016 datapoints
2025-03-06 20:19:28,138 - INFO - Epoch 672/800 done.
2025-03-06 20:19:28,139 - INFO - Final validation performance:
Loss: 0.268, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:19:28,139 - INFO - Beginning epoch 673/800
2025-03-06 20:19:28,147 - INFO - training batch 1, loss: 0.057, 32/60000 datapoints
2025-03-06 20:19:28,365 - INFO - training batch 51, loss: 0.266, 1632/60000 datapoints
2025-03-06 20:19:28,566 - INFO - training batch 101, loss: 0.464, 3232/60000 datapoints
2025-03-06 20:19:28,793 - INFO - training batch 151, loss: 0.083, 4832/60000 datapoints
2025-03-06 20:19:28,999 - INFO - training batch 201, loss: 0.249, 6432/60000 datapoints
2025-03-06 20:19:29,201 - INFO - training batch 251, loss: 0.194, 8032/60000 datapoints
2025-03-06 20:19:29,401 - INFO - training batch 301, loss: 0.104, 9632/60000 datapoints
2025-03-06 20:19:29,608 - INFO - training batch 351, loss: 0.121, 11232/60000 datapoints
2025-03-06 20:19:29,818 - INFO - training batch 401, loss: 0.319, 12832/60000 datapoints
2025-03-06 20:19:30,021 - INFO - training batch 451, loss: 0.205, 14432/60000 datapoints
2025-03-06 20:19:30,222 - INFO - training batch 501, loss: 0.256, 16032/60000 datapoints
2025-03-06 20:19:30,422 - INFO - training batch 551, loss: 0.114, 17632/60000 datapoints
2025-03-06 20:19:30,625 - INFO - training batch 601, loss: 0.204, 19232/60000 datapoints
2025-03-06 20:19:30,828 - INFO - training batch 651, loss: 0.389, 20832/60000 datapoints
2025-03-06 20:19:31,029 - INFO - training batch 701, loss: 0.119, 22432/60000 datapoints
2025-03-06 20:19:31,229 - INFO - training batch 751, loss: 0.285, 24032/60000 datapoints
2025-03-06 20:19:31,431 - INFO - training batch 801, loss: 0.371, 25632/60000 datapoints
2025-03-06 20:19:31,641 - INFO - training batch 851, loss: 0.140, 27232/60000 datapoints
2025-03-06 20:19:31,846 - INFO - training batch 901, loss: 0.171, 28832/60000 datapoints
2025-03-06 20:19:32,046 - INFO - training batch 951, loss: 0.111, 30432/60000 datapoints
2025-03-06 20:19:32,248 - INFO - training batch 1001, loss: 0.202, 32032/60000 datapoints
2025-03-06 20:19:32,451 - INFO - training batch 1051, loss: 0.198, 33632/60000 datapoints
2025-03-06 20:19:32,655 - INFO - training batch 1101, loss: 0.251, 35232/60000 datapoints
2025-03-06 20:19:32,857 - INFO - training batch 1151, loss: 0.203, 36832/60000 datapoints
2025-03-06 20:19:33,062 - INFO - training batch 1201, loss: 0.174, 38432/60000 datapoints
2025-03-06 20:19:33,262 - INFO - training batch 1251, loss: 0.469, 40032/60000 datapoints
2025-03-06 20:19:33,461 - INFO - training batch 1301, loss: 0.107, 41632/60000 datapoints
2025-03-06 20:19:33,667 - INFO - training batch 1351, loss: 0.089, 43232/60000 datapoints
2025-03-06 20:19:33,872 - INFO - training batch 1401, loss: 0.437, 44832/60000 datapoints
2025-03-06 20:19:34,102 - INFO - training batch 1451, loss: 0.102, 46432/60000 datapoints
2025-03-06 20:19:34,302 - INFO - training batch 1501, loss: 0.238, 48032/60000 datapoints
2025-03-06 20:19:34,505 - INFO - training batch 1551, loss: 0.138, 49632/60000 datapoints
2025-03-06 20:19:34,733 - INFO - training batch 1601, loss: 0.194, 51232/60000 datapoints
2025-03-06 20:19:34,987 - INFO - training batch 1651, loss: 0.131, 52832/60000 datapoints
2025-03-06 20:19:35,207 - INFO - training batch 1701, loss: 0.261, 54432/60000 datapoints
2025-03-06 20:19:35,406 - INFO - training batch 1751, loss: 0.369, 56032/60000 datapoints
2025-03-06 20:19:35,604 - INFO - training batch 1801, loss: 0.358, 57632/60000 datapoints
2025-03-06 20:19:35,804 - INFO - training batch 1851, loss: 0.099, 59232/60000 datapoints
2025-03-06 20:19:35,908 - INFO - validation batch 1, loss: 0.535, 32/10016 datapoints
2025-03-06 20:19:36,069 - INFO - validation batch 51, loss: 0.166, 1632/10016 datapoints
2025-03-06 20:19:36,231 - INFO - validation batch 101, loss: 0.212, 3232/10016 datapoints
2025-03-06 20:19:36,391 - INFO - validation batch 151, loss: 0.314, 4832/10016 datapoints
2025-03-06 20:19:36,551 - INFO - validation batch 201, loss: 0.267, 6432/10016 datapoints
2025-03-06 20:19:36,716 - INFO - validation batch 251, loss: 0.187, 8032/10016 datapoints
2025-03-06 20:19:36,874 - INFO - validation batch 301, loss: 0.252, 9632/10016 datapoints
2025-03-06 20:19:36,911 - INFO - Epoch 673/800 done.
2025-03-06 20:19:36,911 - INFO - Final validation performance:
Loss: 0.276, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:19:36,912 - INFO - Beginning epoch 674/800
2025-03-06 20:19:36,919 - INFO - training batch 1, loss: 0.184, 32/60000 datapoints
2025-03-06 20:19:37,125 - INFO - training batch 51, loss: 0.044, 1632/60000 datapoints
2025-03-06 20:19:37,326 - INFO - training batch 101, loss: 0.370, 3232/60000 datapoints
2025-03-06 20:19:37,546 - INFO - training batch 151, loss: 0.173, 4832/60000 datapoints
2025-03-06 20:19:37,774 - INFO - training batch 201, loss: 0.168, 6432/60000 datapoints
2025-03-06 20:19:37,987 - INFO - training batch 251, loss: 0.200, 8032/60000 datapoints
2025-03-06 20:19:38,208 - INFO - training batch 301, loss: 0.198, 9632/60000 datapoints
2025-03-06 20:19:38,439 - INFO - training batch 351, loss: 0.287, 11232/60000 datapoints
2025-03-06 20:19:38,649 - INFO - training batch 401, loss: 0.162, 12832/60000 datapoints
2025-03-06 20:19:38,861 - INFO - training batch 451, loss: 0.135, 14432/60000 datapoints
2025-03-06 20:19:39,074 - INFO - training batch 501, loss: 0.438, 16032/60000 datapoints
2025-03-06 20:19:39,290 - INFO - training batch 551, loss: 0.084, 17632/60000 datapoints
2025-03-06 20:19:39,513 - INFO - training batch 601, loss: 0.041, 19232/60000 datapoints
2025-03-06 20:19:39,734 - INFO - training batch 651, loss: 0.056, 20832/60000 datapoints
2025-03-06 20:19:39,947 - INFO - training batch 701, loss: 0.264, 22432/60000 datapoints
2025-03-06 20:19:40,160 - INFO - training batch 751, loss: 0.186, 24032/60000 datapoints
2025-03-06 20:19:40,372 - INFO - training batch 801, loss: 0.431, 25632/60000 datapoints
2025-03-06 20:19:40,584 - INFO - training batch 851, loss: 0.251, 27232/60000 datapoints
2025-03-06 20:19:40,799 - INFO - training batch 901, loss: 0.368, 28832/60000 datapoints
2025-03-06 20:19:41,006 - INFO - training batch 951, loss: 0.168, 30432/60000 datapoints
2025-03-06 20:19:41,223 - INFO - training batch 1001, loss: 0.211, 32032/60000 datapoints
2025-03-06 20:19:41,438 - INFO - training batch 1051, loss: 0.140, 33632/60000 datapoints
2025-03-06 20:19:41,652 - INFO - training batch 1101, loss: 0.103, 35232/60000 datapoints
2025-03-06 20:19:41,868 - INFO - training batch 1151, loss: 0.516, 36832/60000 datapoints
2025-03-06 20:19:42,081 - INFO - training batch 1201, loss: 0.397, 38432/60000 datapoints
2025-03-06 20:19:42,294 - INFO - training batch 1251, loss: 0.254, 40032/60000 datapoints
2025-03-06 20:19:42,508 - INFO - training batch 1301, loss: 0.168, 41632/60000 datapoints
2025-03-06 20:19:42,729 - INFO - training batch 1351, loss: 0.130, 43232/60000 datapoints
2025-03-06 20:19:42,946 - INFO - training batch 1401, loss: 0.102, 44832/60000 datapoints
2025-03-06 20:19:43,157 - INFO - training batch 1451, loss: 0.494, 46432/60000 datapoints
2025-03-06 20:19:43,370 - INFO - training batch 1501, loss: 0.200, 48032/60000 datapoints
2025-03-06 20:19:43,583 - INFO - training batch 1551, loss: 0.305, 49632/60000 datapoints
2025-03-06 20:19:43,801 - INFO - training batch 1601, loss: 0.224, 51232/60000 datapoints
2025-03-06 20:19:44,019 - INFO - training batch 1651, loss: 0.094, 52832/60000 datapoints
2025-03-06 20:19:44,250 - INFO - training batch 1701, loss: 0.097, 54432/60000 datapoints
2025-03-06 20:19:44,466 - INFO - training batch 1751, loss: 0.414, 56032/60000 datapoints
2025-03-06 20:19:44,681 - INFO - training batch 1801, loss: 0.233, 57632/60000 datapoints
2025-03-06 20:19:44,904 - INFO - training batch 1851, loss: 0.182, 59232/60000 datapoints
2025-03-06 20:19:45,018 - INFO - validation batch 1, loss: 0.075, 32/10016 datapoints
2025-03-06 20:19:45,192 - INFO - validation batch 51, loss: 0.366, 1632/10016 datapoints
2025-03-06 20:19:45,363 - INFO - validation batch 101, loss: 0.139, 3232/10016 datapoints
2025-03-06 20:19:45,532 - INFO - validation batch 151, loss: 0.273, 4832/10016 datapoints
2025-03-06 20:19:45,706 - INFO - validation batch 201, loss: 0.196, 6432/10016 datapoints
2025-03-06 20:19:45,875 - INFO - validation batch 251, loss: 0.191, 8032/10016 datapoints
2025-03-06 20:19:46,042 - INFO - validation batch 301, loss: 0.156, 9632/10016 datapoints
2025-03-06 20:19:46,087 - INFO - Epoch 674/800 done.
2025-03-06 20:19:46,088 - INFO - Final validation performance:
Loss: 0.199, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:19:46,088 - INFO - Beginning epoch 675/800
2025-03-06 20:19:46,095 - INFO - training batch 1, loss: 0.149, 32/60000 datapoints
2025-03-06 20:19:46,310 - INFO - training batch 51, loss: 0.264, 1632/60000 datapoints
2025-03-06 20:19:46,518 - INFO - training batch 101, loss: 0.165, 3232/60000 datapoints
2025-03-06 20:19:46,746 - INFO - training batch 151, loss: 0.075, 4832/60000 datapoints
2025-03-06 20:19:46,949 - INFO - training batch 201, loss: 0.100, 6432/60000 datapoints
2025-03-06 20:19:47,154 - INFO - training batch 251, loss: 0.338, 8032/60000 datapoints
2025-03-06 20:19:47,361 - INFO - training batch 301, loss: 0.536, 9632/60000 datapoints
2025-03-06 20:19:47,568 - INFO - training batch 351, loss: 0.100, 11232/60000 datapoints
2025-03-06 20:19:47,777 - INFO - training batch 401, loss: 0.147, 12832/60000 datapoints
2025-03-06 20:19:47,978 - INFO - training batch 451, loss: 0.387, 14432/60000 datapoints
2025-03-06 20:19:48,177 - INFO - training batch 501, loss: 0.403, 16032/60000 datapoints
2025-03-06 20:19:48,380 - INFO - training batch 551, loss: 0.107, 17632/60000 datapoints
2025-03-06 20:19:48,583 - INFO - training batch 601, loss: 0.145, 19232/60000 datapoints
2025-03-06 20:19:48,785 - INFO - training batch 651, loss: 0.195, 20832/60000 datapoints
2025-03-06 20:19:48,983 - INFO - training batch 701, loss: 0.283, 22432/60000 datapoints
2025-03-06 20:19:49,187 - INFO - training batch 751, loss: 0.335, 24032/60000 datapoints
2025-03-06 20:19:49,389 - INFO - training batch 801, loss: 0.106, 25632/60000 datapoints
2025-03-06 20:19:49,592 - INFO - training batch 851, loss: 0.175, 27232/60000 datapoints
2025-03-06 20:19:49,796 - INFO - training batch 901, loss: 0.380, 28832/60000 datapoints
2025-03-06 20:19:49,999 - INFO - training batch 951, loss: 0.164, 30432/60000 datapoints
2025-03-06 20:19:50,200 - INFO - training batch 1001, loss: 0.333, 32032/60000 datapoints
2025-03-06 20:19:50,401 - INFO - training batch 1051, loss: 0.350, 33632/60000 datapoints
2025-03-06 20:19:50,604 - INFO - training batch 1101, loss: 0.296, 35232/60000 datapoints
2025-03-06 20:19:50,807 - INFO - training batch 1151, loss: 0.283, 36832/60000 datapoints
2025-03-06 20:19:51,011 - INFO - training batch 1201, loss: 0.201, 38432/60000 datapoints
2025-03-06 20:19:51,211 - INFO - training batch 1251, loss: 0.107, 40032/60000 datapoints
2025-03-06 20:19:51,413 - INFO - training batch 1301, loss: 0.382, 41632/60000 datapoints
2025-03-06 20:19:51,616 - INFO - training batch 1351, loss: 0.164, 43232/60000 datapoints
2025-03-06 20:19:51,824 - INFO - training batch 1401, loss: 0.169, 44832/60000 datapoints
2025-03-06 20:19:52,027 - INFO - training batch 1451, loss: 0.506, 46432/60000 datapoints
2025-03-06 20:19:52,230 - INFO - training batch 1501, loss: 0.147, 48032/60000 datapoints
2025-03-06 20:19:52,433 - INFO - training batch 1551, loss: 0.097, 49632/60000 datapoints
2025-03-06 20:19:52,639 - INFO - training batch 1601, loss: 0.569, 51232/60000 datapoints
2025-03-06 20:19:52,841 - INFO - training batch 1651, loss: 0.475, 52832/60000 datapoints
2025-03-06 20:19:53,043 - INFO - training batch 1701, loss: 0.261, 54432/60000 datapoints
2025-03-06 20:19:53,245 - INFO - training batch 1751, loss: 0.300, 56032/60000 datapoints
2025-03-06 20:19:53,448 - INFO - training batch 1801, loss: 0.252, 57632/60000 datapoints
2025-03-06 20:19:53,656 - INFO - training batch 1851, loss: 0.534, 59232/60000 datapoints
2025-03-06 20:19:53,765 - INFO - validation batch 1, loss: 0.075, 32/10016 datapoints
2025-03-06 20:19:53,926 - INFO - validation batch 51, loss: 0.351, 1632/10016 datapoints
2025-03-06 20:19:54,088 - INFO - validation batch 101, loss: 0.250, 3232/10016 datapoints
2025-03-06 20:19:54,272 - INFO - validation batch 151, loss: 0.276, 4832/10016 datapoints
2025-03-06 20:19:54,437 - INFO - validation batch 201, loss: 0.229, 6432/10016 datapoints
2025-03-06 20:19:54,598 - INFO - validation batch 251, loss: 0.186, 8032/10016 datapoints
2025-03-06 20:19:54,765 - INFO - validation batch 301, loss: 0.183, 9632/10016 datapoints
2025-03-06 20:19:54,808 - INFO - Epoch 675/800 done.
2025-03-06 20:19:54,808 - INFO - Final validation performance:
Loss: 0.221, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:19:54,809 - INFO - Beginning epoch 676/800
2025-03-06 20:19:54,817 - INFO - training batch 1, loss: 0.119, 32/60000 datapoints
2025-03-06 20:19:55,029 - INFO - training batch 51, loss: 0.243, 1632/60000 datapoints
2025-03-06 20:19:55,231 - INFO - training batch 101, loss: 0.116, 3232/60000 datapoints
2025-03-06 20:19:55,446 - INFO - training batch 151, loss: 0.309, 4832/60000 datapoints
2025-03-06 20:19:55,649 - INFO - training batch 201, loss: 0.323, 6432/60000 datapoints
2025-03-06 20:19:55,860 - INFO - training batch 251, loss: 0.162, 8032/60000 datapoints
2025-03-06 20:19:56,068 - INFO - training batch 301, loss: 0.042, 9632/60000 datapoints
2025-03-06 20:19:56,272 - INFO - training batch 351, loss: 0.483, 11232/60000 datapoints
2025-03-06 20:19:56,474 - INFO - training batch 401, loss: 0.176, 12832/60000 datapoints
2025-03-06 20:19:56,685 - INFO - training batch 451, loss: 0.327, 14432/60000 datapoints
2025-03-06 20:19:56,894 - INFO - training batch 501, loss: 0.273, 16032/60000 datapoints
2025-03-06 20:19:57,096 - INFO - training batch 551, loss: 0.161, 17632/60000 datapoints
2025-03-06 20:19:57,295 - INFO - training batch 601, loss: 0.058, 19232/60000 datapoints
2025-03-06 20:19:57,498 - INFO - training batch 651, loss: 0.424, 20832/60000 datapoints
2025-03-06 20:19:57,701 - INFO - training batch 701, loss: 0.230, 22432/60000 datapoints
2025-03-06 20:19:57,903 - INFO - training batch 751, loss: 0.177, 24032/60000 datapoints
2025-03-06 20:19:58,104 - INFO - training batch 801, loss: 0.415, 25632/60000 datapoints
2025-03-06 20:19:58,308 - INFO - training batch 851, loss: 0.310, 27232/60000 datapoints
2025-03-06 20:19:58,510 - INFO - training batch 901, loss: 0.043, 28832/60000 datapoints
2025-03-06 20:19:58,720 - INFO - training batch 951, loss: 0.325, 30432/60000 datapoints
2025-03-06 20:19:58,924 - INFO - training batch 1001, loss: 0.222, 32032/60000 datapoints
2025-03-06 20:19:59,125 - INFO - training batch 1051, loss: 0.290, 33632/60000 datapoints
2025-03-06 20:19:59,329 - INFO - training batch 1101, loss: 0.322, 35232/60000 datapoints
2025-03-06 20:19:59,538 - INFO - training batch 1151, loss: 0.515, 36832/60000 datapoints
2025-03-06 20:19:59,748 - INFO - training batch 1201, loss: 0.178, 38432/60000 datapoints
2025-03-06 20:19:59,955 - INFO - training batch 1251, loss: 0.138, 40032/60000 datapoints
2025-03-06 20:20:00,166 - INFO - training batch 1301, loss: 0.251, 41632/60000 datapoints
2025-03-06 20:20:00,373 - INFO - training batch 1351, loss: 0.159, 43232/60000 datapoints
2025-03-06 20:20:00,583 - INFO - training batch 1401, loss: 0.340, 44832/60000 datapoints
2025-03-06 20:20:00,794 - INFO - training batch 1451, loss: 0.233, 46432/60000 datapoints
2025-03-06 20:20:01,002 - INFO - training batch 1501, loss: 0.071, 48032/60000 datapoints
2025-03-06 20:20:01,207 - INFO - training batch 1551, loss: 0.286, 49632/60000 datapoints
2025-03-06 20:20:01,415 - INFO - training batch 1601, loss: 0.143, 51232/60000 datapoints
2025-03-06 20:20:01,623 - INFO - training batch 1651, loss: 0.212, 52832/60000 datapoints
2025-03-06 20:20:01,838 - INFO - training batch 1701, loss: 0.266, 54432/60000 datapoints
2025-03-06 20:20:02,044 - INFO - training batch 1751, loss: 0.262, 56032/60000 datapoints
2025-03-06 20:20:02,251 - INFO - training batch 1801, loss: 0.238, 57632/60000 datapoints
2025-03-06 20:20:02,458 - INFO - training batch 1851, loss: 0.146, 59232/60000 datapoints
2025-03-06 20:20:02,569 - INFO - validation batch 1, loss: 0.339, 32/10016 datapoints
2025-03-06 20:20:02,740 - INFO - validation batch 51, loss: 0.311, 1632/10016 datapoints
2025-03-06 20:20:02,905 - INFO - validation batch 101, loss: 0.219, 3232/10016 datapoints
2025-03-06 20:20:03,075 - INFO - validation batch 151, loss: 0.370, 4832/10016 datapoints
2025-03-06 20:20:03,243 - INFO - validation batch 201, loss: 0.191, 6432/10016 datapoints
2025-03-06 20:20:03,410 - INFO - validation batch 251, loss: 0.205, 8032/10016 datapoints
2025-03-06 20:20:03,581 - INFO - validation batch 301, loss: 0.257, 9632/10016 datapoints
2025-03-06 20:20:03,621 - INFO - Epoch 676/800 done.
2025-03-06 20:20:03,621 - INFO - Final validation performance:
Loss: 0.270, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:20:03,622 - INFO - Beginning epoch 677/800
2025-03-06 20:20:03,635 - INFO - training batch 1, loss: 0.249, 32/60000 datapoints
2025-03-06 20:20:03,856 - INFO - training batch 51, loss: 0.143, 1632/60000 datapoints
2025-03-06 20:20:04,061 - INFO - training batch 101, loss: 0.076, 3232/60000 datapoints
2025-03-06 20:20:04,293 - INFO - training batch 151, loss: 0.218, 4832/60000 datapoints
2025-03-06 20:20:04,501 - INFO - training batch 201, loss: 0.086, 6432/60000 datapoints
2025-03-06 20:20:04,727 - INFO - training batch 251, loss: 0.195, 8032/60000 datapoints
2025-03-06 20:20:04,936 - INFO - training batch 301, loss: 0.292, 9632/60000 datapoints
2025-03-06 20:20:05,135 - INFO - training batch 351, loss: 0.165, 11232/60000 datapoints
2025-03-06 20:20:05,336 - INFO - training batch 401, loss: 0.099, 12832/60000 datapoints
2025-03-06 20:20:05,538 - INFO - training batch 451, loss: 0.462, 14432/60000 datapoints
2025-03-06 20:20:05,744 - INFO - training batch 501, loss: 0.190, 16032/60000 datapoints
2025-03-06 20:20:05,945 - INFO - training batch 551, loss: 0.751, 17632/60000 datapoints
2025-03-06 20:20:06,147 - INFO - training batch 601, loss: 0.075, 19232/60000 datapoints
2025-03-06 20:20:06,348 - INFO - training batch 651, loss: 0.259, 20832/60000 datapoints
2025-03-06 20:20:06,548 - INFO - training batch 701, loss: 0.272, 22432/60000 datapoints
2025-03-06 20:20:06,755 - INFO - training batch 751, loss: 0.190, 24032/60000 datapoints
2025-03-06 20:20:06,952 - INFO - training batch 801, loss: 0.119, 25632/60000 datapoints
2025-03-06 20:20:07,150 - INFO - training batch 851, loss: 0.225, 27232/60000 datapoints
2025-03-06 20:20:07,349 - INFO - training batch 901, loss: 0.064, 28832/60000 datapoints
2025-03-06 20:20:07,552 - INFO - training batch 951, loss: 0.096, 30432/60000 datapoints
2025-03-06 20:20:07,762 - INFO - training batch 1001, loss: 0.292, 32032/60000 datapoints
2025-03-06 20:20:07,962 - INFO - training batch 1051, loss: 0.169, 33632/60000 datapoints
2025-03-06 20:20:08,159 - INFO - training batch 1101, loss: 0.116, 35232/60000 datapoints
2025-03-06 20:20:08,360 - INFO - training batch 1151, loss: 0.254, 36832/60000 datapoints
2025-03-06 20:20:08,561 - INFO - training batch 1201, loss: 0.270, 38432/60000 datapoints
2025-03-06 20:20:08,769 - INFO - training batch 1251, loss: 0.170, 40032/60000 datapoints
2025-03-06 20:20:08,969 - INFO - training batch 1301, loss: 0.064, 41632/60000 datapoints
2025-03-06 20:20:09,171 - INFO - training batch 1351, loss: 0.160, 43232/60000 datapoints
2025-03-06 20:20:09,373 - INFO - training batch 1401, loss: 0.521, 44832/60000 datapoints
2025-03-06 20:20:09,576 - INFO - training batch 1451, loss: 0.613, 46432/60000 datapoints
2025-03-06 20:20:09,781 - INFO - training batch 1501, loss: 0.307, 48032/60000 datapoints
2025-03-06 20:20:09,982 - INFO - training batch 1551, loss: 0.135, 49632/60000 datapoints
2025-03-06 20:20:10,182 - INFO - training batch 1601, loss: 0.515, 51232/60000 datapoints
2025-03-06 20:20:10,380 - INFO - training batch 1651, loss: 0.457, 52832/60000 datapoints
2025-03-06 20:20:10,578 - INFO - training batch 1701, loss: 0.307, 54432/60000 datapoints
2025-03-06 20:20:10,781 - INFO - training batch 1751, loss: 0.144, 56032/60000 datapoints
2025-03-06 20:20:10,986 - INFO - training batch 1801, loss: 0.126, 57632/60000 datapoints
2025-03-06 20:20:11,180 - INFO - training batch 1851, loss: 0.655, 59232/60000 datapoints
2025-03-06 20:20:11,281 - INFO - validation batch 1, loss: 0.190, 32/10016 datapoints
2025-03-06 20:20:11,436 - INFO - validation batch 51, loss: 0.182, 1632/10016 datapoints
2025-03-06 20:20:11,592 - INFO - validation batch 101, loss: 0.267, 3232/10016 datapoints
2025-03-06 20:20:11,754 - INFO - validation batch 151, loss: 0.229, 4832/10016 datapoints
2025-03-06 20:20:11,916 - INFO - validation batch 201, loss: 0.265, 6432/10016 datapoints
2025-03-06 20:20:12,072 - INFO - validation batch 251, loss: 0.273, 8032/10016 datapoints
2025-03-06 20:20:12,240 - INFO - validation batch 301, loss: 0.257, 9632/10016 datapoints
2025-03-06 20:20:12,284 - INFO - Epoch 677/800 done.
2025-03-06 20:20:12,285 - INFO - Final validation performance:
Loss: 0.238, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:20:12,286 - INFO - Beginning epoch 678/800
2025-03-06 20:20:12,293 - INFO - training batch 1, loss: 0.156, 32/60000 datapoints
2025-03-06 20:20:12,496 - INFO - training batch 51, loss: 0.199, 1632/60000 datapoints
2025-03-06 20:20:12,705 - INFO - training batch 101, loss: 0.303, 3232/60000 datapoints
2025-03-06 20:20:12,913 - INFO - training batch 151, loss: 0.373, 4832/60000 datapoints
2025-03-06 20:20:13,109 - INFO - training batch 201, loss: 0.134, 6432/60000 datapoints
2025-03-06 20:20:13,305 - INFO - training batch 251, loss: 0.557, 8032/60000 datapoints
2025-03-06 20:20:13,522 - INFO - training batch 301, loss: 0.262, 9632/60000 datapoints
2025-03-06 20:20:13,737 - INFO - training batch 351, loss: 0.379, 11232/60000 datapoints
2025-03-06 20:20:13,938 - INFO - training batch 401, loss: 0.141, 12832/60000 datapoints
2025-03-06 20:20:14,133 - INFO - training batch 451, loss: 0.073, 14432/60000 datapoints
2025-03-06 20:20:14,348 - INFO - training batch 501, loss: 0.235, 16032/60000 datapoints
2025-03-06 20:20:14,565 - INFO - training batch 551, loss: 0.058, 17632/60000 datapoints
2025-03-06 20:20:14,765 - INFO - training batch 601, loss: 0.052, 19232/60000 datapoints
2025-03-06 20:20:14,972 - INFO - training batch 651, loss: 0.197, 20832/60000 datapoints
2025-03-06 20:20:15,168 - INFO - training batch 701, loss: 0.312, 22432/60000 datapoints
2025-03-06 20:20:15,362 - INFO - training batch 751, loss: 0.307, 24032/60000 datapoints
2025-03-06 20:20:15,558 - INFO - training batch 801, loss: 0.154, 25632/60000 datapoints
2025-03-06 20:20:15,756 - INFO - training batch 851, loss: 0.103, 27232/60000 datapoints
2025-03-06 20:20:15,953 - INFO - training batch 901, loss: 0.144, 28832/60000 datapoints
2025-03-06 20:20:16,148 - INFO - training batch 951, loss: 0.205, 30432/60000 datapoints
2025-03-06 20:20:16,348 - INFO - training batch 1001, loss: 0.080, 32032/60000 datapoints
2025-03-06 20:20:16,542 - INFO - training batch 1051, loss: 0.524, 33632/60000 datapoints
2025-03-06 20:20:16,741 - INFO - training batch 1101, loss: 0.243, 35232/60000 datapoints
2025-03-06 20:20:16,937 - INFO - training batch 1151, loss: 0.222, 36832/60000 datapoints
2025-03-06 20:20:17,128 - INFO - training batch 1201, loss: 0.135, 38432/60000 datapoints
2025-03-06 20:20:17,321 - INFO - training batch 1251, loss: 0.183, 40032/60000 datapoints
2025-03-06 20:20:17,515 - INFO - training batch 1301, loss: 0.482, 41632/60000 datapoints
2025-03-06 20:20:17,712 - INFO - training batch 1351, loss: 0.194, 43232/60000 datapoints
2025-03-06 20:20:17,909 - INFO - training batch 1401, loss: 0.115, 44832/60000 datapoints
2025-03-06 20:20:18,101 - INFO - training batch 1451, loss: 0.103, 46432/60000 datapoints
2025-03-06 20:20:18,295 - INFO - training batch 1501, loss: 0.177, 48032/60000 datapoints
2025-03-06 20:20:18,490 - INFO - training batch 1551, loss: 0.533, 49632/60000 datapoints
2025-03-06 20:20:18,688 - INFO - training batch 1601, loss: 0.227, 51232/60000 datapoints
2025-03-06 20:20:18,883 - INFO - training batch 1651, loss: 0.075, 52832/60000 datapoints
2025-03-06 20:20:19,077 - INFO - training batch 1701, loss: 0.137, 54432/60000 datapoints
2025-03-06 20:20:19,271 - INFO - training batch 1751, loss: 0.096, 56032/60000 datapoints
2025-03-06 20:20:19,466 - INFO - training batch 1801, loss: 0.080, 57632/60000 datapoints
2025-03-06 20:20:19,665 - INFO - training batch 1851, loss: 0.273, 59232/60000 datapoints
2025-03-06 20:20:19,769 - INFO - validation batch 1, loss: 0.150, 32/10016 datapoints
2025-03-06 20:20:19,929 - INFO - validation batch 51, loss: 0.397, 1632/10016 datapoints
2025-03-06 20:20:20,086 - INFO - validation batch 101, loss: 0.091, 3232/10016 datapoints
2025-03-06 20:20:20,244 - INFO - validation batch 151, loss: 0.302, 4832/10016 datapoints
2025-03-06 20:20:20,400 - INFO - validation batch 201, loss: 0.193, 6432/10016 datapoints
2025-03-06 20:20:20,558 - INFO - validation batch 251, loss: 0.070, 8032/10016 datapoints
2025-03-06 20:20:20,718 - INFO - validation batch 301, loss: 0.132, 9632/10016 datapoints
2025-03-06 20:20:20,755 - INFO - Epoch 678/800 done.
2025-03-06 20:20:20,756 - INFO - Final validation performance:
Loss: 0.191, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:20:20,756 - INFO - Beginning epoch 679/800
2025-03-06 20:20:20,764 - INFO - training batch 1, loss: 0.145, 32/60000 datapoints
2025-03-06 20:20:20,975 - INFO - training batch 51, loss: 0.262, 1632/60000 datapoints
2025-03-06 20:20:21,167 - INFO - training batch 101, loss: 0.413, 3232/60000 datapoints
2025-03-06 20:20:21,372 - INFO - training batch 151, loss: 0.231, 4832/60000 datapoints
2025-03-06 20:20:21,572 - INFO - training batch 201, loss: 0.133, 6432/60000 datapoints
2025-03-06 20:20:21,773 - INFO - training batch 251, loss: 0.119, 8032/60000 datapoints
2025-03-06 20:20:21,972 - INFO - training batch 301, loss: 0.391, 9632/60000 datapoints
2025-03-06 20:20:22,164 - INFO - training batch 351, loss: 0.102, 11232/60000 datapoints
2025-03-06 20:20:22,358 - INFO - training batch 401, loss: 0.203, 12832/60000 datapoints
2025-03-06 20:20:22,552 - INFO - training batch 451, loss: 0.214, 14432/60000 datapoints
2025-03-06 20:20:22,754 - INFO - training batch 501, loss: 0.215, 16032/60000 datapoints
2025-03-06 20:20:22,963 - INFO - training batch 551, loss: 0.162, 17632/60000 datapoints
2025-03-06 20:20:23,161 - INFO - training batch 601, loss: 0.214, 19232/60000 datapoints
2025-03-06 20:20:23,368 - INFO - training batch 651, loss: 0.087, 20832/60000 datapoints
2025-03-06 20:20:23,570 - INFO - training batch 701, loss: 0.174, 22432/60000 datapoints
2025-03-06 20:20:23,778 - INFO - training batch 751, loss: 0.107, 24032/60000 datapoints
2025-03-06 20:20:23,987 - INFO - training batch 801, loss: 0.231, 25632/60000 datapoints
2025-03-06 20:20:24,188 - INFO - training batch 851, loss: 0.121, 27232/60000 datapoints
2025-03-06 20:20:24,387 - INFO - training batch 901, loss: 0.156, 28832/60000 datapoints
2025-03-06 20:20:24,611 - INFO - training batch 951, loss: 0.169, 30432/60000 datapoints
2025-03-06 20:20:24,818 - INFO - training batch 1001, loss: 0.208, 32032/60000 datapoints
2025-03-06 20:20:25,026 - INFO - training batch 1051, loss: 0.105, 33632/60000 datapoints
2025-03-06 20:20:25,225 - INFO - training batch 1101, loss: 0.550, 35232/60000 datapoints
2025-03-06 20:20:25,429 - INFO - training batch 1151, loss: 0.137, 36832/60000 datapoints
2025-03-06 20:20:25,632 - INFO - training batch 1201, loss: 0.713, 38432/60000 datapoints
2025-03-06 20:20:25,836 - INFO - training batch 1251, loss: 0.383, 40032/60000 datapoints
2025-03-06 20:20:26,038 - INFO - training batch 1301, loss: 0.543, 41632/60000 datapoints
2025-03-06 20:20:26,237 - INFO - training batch 1351, loss: 0.473, 43232/60000 datapoints
2025-03-06 20:20:26,446 - INFO - training batch 1401, loss: 0.168, 44832/60000 datapoints
2025-03-06 20:20:26,648 - INFO - training batch 1451, loss: 0.161, 46432/60000 datapoints
2025-03-06 20:20:26,855 - INFO - training batch 1501, loss: 0.168, 48032/60000 datapoints
2025-03-06 20:20:27,057 - INFO - training batch 1551, loss: 0.341, 49632/60000 datapoints
2025-03-06 20:20:27,257 - INFO - training batch 1601, loss: 0.090, 51232/60000 datapoints
2025-03-06 20:20:27,465 - INFO - training batch 1651, loss: 0.315, 52832/60000 datapoints
2025-03-06 20:20:27,701 - INFO - training batch 1701, loss: 0.447, 54432/60000 datapoints
2025-03-06 20:20:27,913 - INFO - training batch 1751, loss: 0.184, 56032/60000 datapoints
2025-03-06 20:20:28,113 - INFO - training batch 1801, loss: 0.149, 57632/60000 datapoints
2025-03-06 20:20:28,313 - INFO - training batch 1851, loss: 0.149, 59232/60000 datapoints
2025-03-06 20:20:28,420 - INFO - validation batch 1, loss: 0.292, 32/10016 datapoints
2025-03-06 20:20:28,585 - INFO - validation batch 51, loss: 0.266, 1632/10016 datapoints
2025-03-06 20:20:28,753 - INFO - validation batch 101, loss: 0.218, 3232/10016 datapoints
2025-03-06 20:20:28,919 - INFO - validation batch 151, loss: 0.207, 4832/10016 datapoints
2025-03-06 20:20:29,084 - INFO - validation batch 201, loss: 0.101, 6432/10016 datapoints
2025-03-06 20:20:29,245 - INFO - validation batch 251, loss: 0.153, 8032/10016 datapoints
2025-03-06 20:20:29,410 - INFO - validation batch 301, loss: 0.278, 9632/10016 datapoints
2025-03-06 20:20:29,457 - INFO - Epoch 679/800 done.
2025-03-06 20:20:29,457 - INFO - Final validation performance:
Loss: 0.216, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:20:29,458 - INFO - Beginning epoch 680/800
2025-03-06 20:20:29,466 - INFO - training batch 1, loss: 0.257, 32/60000 datapoints
2025-03-06 20:20:29,691 - INFO - training batch 51, loss: 0.281, 1632/60000 datapoints
2025-03-06 20:20:29,901 - INFO - training batch 101, loss: 0.160, 3232/60000 datapoints
2025-03-06 20:20:30,105 - INFO - training batch 151, loss: 0.210, 4832/60000 datapoints
2025-03-06 20:20:30,307 - INFO - training batch 201, loss: 0.286, 6432/60000 datapoints
2025-03-06 20:20:30,511 - INFO - training batch 251, loss: 0.189, 8032/60000 datapoints
2025-03-06 20:20:30,716 - INFO - training batch 301, loss: 0.138, 9632/60000 datapoints
2025-03-06 20:20:30,916 - INFO - training batch 351, loss: 0.315, 11232/60000 datapoints
2025-03-06 20:20:31,114 - INFO - training batch 401, loss: 0.208, 12832/60000 datapoints
2025-03-06 20:20:31,312 - INFO - training batch 451, loss: 0.219, 14432/60000 datapoints
2025-03-06 20:20:31,514 - INFO - training batch 501, loss: 0.308, 16032/60000 datapoints
2025-03-06 20:20:31,719 - INFO - training batch 551, loss: 0.531, 17632/60000 datapoints
2025-03-06 20:20:31,928 - INFO - training batch 601, loss: 0.138, 19232/60000 datapoints
2025-03-06 20:20:32,128 - INFO - training batch 651, loss: 0.307, 20832/60000 datapoints
2025-03-06 20:20:32,330 - INFO - training batch 701, loss: 0.155, 22432/60000 datapoints
2025-03-06 20:20:32,530 - INFO - training batch 751, loss: 0.320, 24032/60000 datapoints
2025-03-06 20:20:32,734 - INFO - training batch 801, loss: 0.202, 25632/60000 datapoints
2025-03-06 20:20:32,935 - INFO - training batch 851, loss: 0.238, 27232/60000 datapoints
2025-03-06 20:20:33,132 - INFO - training batch 901, loss: 0.103, 28832/60000 datapoints
2025-03-06 20:20:33,335 - INFO - training batch 951, loss: 0.098, 30432/60000 datapoints
2025-03-06 20:20:33,538 - INFO - training batch 1001, loss: 0.121, 32032/60000 datapoints
2025-03-06 20:20:33,744 - INFO - training batch 1051, loss: 0.397, 33632/60000 datapoints
2025-03-06 20:20:33,949 - INFO - training batch 1101, loss: 0.195, 35232/60000 datapoints
2025-03-06 20:20:34,150 - INFO - training batch 1151, loss: 0.121, 36832/60000 datapoints
2025-03-06 20:20:34,353 - INFO - training batch 1201, loss: 0.435, 38432/60000 datapoints
2025-03-06 20:20:34,572 - INFO - training batch 1251, loss: 0.196, 40032/60000 datapoints
2025-03-06 20:20:34,790 - INFO - training batch 1301, loss: 0.172, 41632/60000 datapoints
2025-03-06 20:20:35,000 - INFO - training batch 1351, loss: 0.046, 43232/60000 datapoints
2025-03-06 20:20:35,199 - INFO - training batch 1401, loss: 0.128, 44832/60000 datapoints
2025-03-06 20:20:35,399 - INFO - training batch 1451, loss: 0.229, 46432/60000 datapoints
2025-03-06 20:20:35,599 - INFO - training batch 1501, loss: 0.235, 48032/60000 datapoints
2025-03-06 20:20:35,803 - INFO - training batch 1551, loss: 0.202, 49632/60000 datapoints
2025-03-06 20:20:36,007 - INFO - training batch 1601, loss: 0.214, 51232/60000 datapoints
2025-03-06 20:20:36,206 - INFO - training batch 1651, loss: 0.218, 52832/60000 datapoints
2025-03-06 20:20:36,408 - INFO - training batch 1701, loss: 0.580, 54432/60000 datapoints
2025-03-06 20:20:36,607 - INFO - training batch 1751, loss: 0.263, 56032/60000 datapoints
2025-03-06 20:20:36,815 - INFO - training batch 1801, loss: 0.193, 57632/60000 datapoints
2025-03-06 20:20:37,016 - INFO - training batch 1851, loss: 0.218, 59232/60000 datapoints
2025-03-06 20:20:37,121 - INFO - validation batch 1, loss: 0.198, 32/10016 datapoints
2025-03-06 20:20:37,280 - INFO - validation batch 51, loss: 0.445, 1632/10016 datapoints
2025-03-06 20:20:37,442 - INFO - validation batch 101, loss: 0.112, 3232/10016 datapoints
2025-03-06 20:20:37,614 - INFO - validation batch 151, loss: 0.206, 4832/10016 datapoints
2025-03-06 20:20:37,795 - INFO - validation batch 201, loss: 0.195, 6432/10016 datapoints
2025-03-06 20:20:37,961 - INFO - validation batch 251, loss: 0.153, 8032/10016 datapoints
2025-03-06 20:20:38,121 - INFO - validation batch 301, loss: 0.201, 9632/10016 datapoints
2025-03-06 20:20:38,159 - INFO - Epoch 680/800 done.
2025-03-06 20:20:38,160 - INFO - Final validation performance:
Loss: 0.216, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:20:38,160 - INFO - Beginning epoch 681/800
2025-03-06 20:20:38,168 - INFO - training batch 1, loss: 0.245, 32/60000 datapoints
2025-03-06 20:20:38,368 - INFO - training batch 51, loss: 0.233, 1632/60000 datapoints
2025-03-06 20:20:38,569 - INFO - training batch 101, loss: 0.099, 3232/60000 datapoints
2025-03-06 20:20:38,799 - INFO - training batch 151, loss: 0.128, 4832/60000 datapoints
2025-03-06 20:20:39,004 - INFO - training batch 201, loss: 0.135, 6432/60000 datapoints
2025-03-06 20:20:39,218 - INFO - training batch 251, loss: 0.392, 8032/60000 datapoints
2025-03-06 20:20:39,418 - INFO - training batch 301, loss: 0.069, 9632/60000 datapoints
2025-03-06 20:20:39,623 - INFO - training batch 351, loss: 0.194, 11232/60000 datapoints
2025-03-06 20:20:39,826 - INFO - training batch 401, loss: 0.257, 12832/60000 datapoints
2025-03-06 20:20:40,032 - INFO - training batch 451, loss: 0.181, 14432/60000 datapoints
2025-03-06 20:20:40,230 - INFO - training batch 501, loss: 0.119, 16032/60000 datapoints
2025-03-06 20:20:40,428 - INFO - training batch 551, loss: 0.242, 17632/60000 datapoints
2025-03-06 20:20:40,632 - INFO - training batch 601, loss: 0.132, 19232/60000 datapoints
2025-03-06 20:20:40,833 - INFO - training batch 651, loss: 0.190, 20832/60000 datapoints
2025-03-06 20:20:41,032 - INFO - training batch 701, loss: 0.573, 22432/60000 datapoints
2025-03-06 20:20:41,225 - INFO - training batch 751, loss: 0.421, 24032/60000 datapoints
2025-03-06 20:20:41,417 - INFO - training batch 801, loss: 0.081, 25632/60000 datapoints
2025-03-06 20:20:41,613 - INFO - training batch 851, loss: 0.105, 27232/60000 datapoints
2025-03-06 20:20:41,813 - INFO - training batch 901, loss: 0.334, 28832/60000 datapoints
2025-03-06 20:20:42,012 - INFO - training batch 951, loss: 0.267, 30432/60000 datapoints
2025-03-06 20:20:42,208 - INFO - training batch 1001, loss: 0.175, 32032/60000 datapoints
2025-03-06 20:20:42,398 - INFO - training batch 1051, loss: 0.130, 33632/60000 datapoints
2025-03-06 20:20:42,594 - INFO - training batch 1101, loss: 0.194, 35232/60000 datapoints
2025-03-06 20:20:42,792 - INFO - training batch 1151, loss: 0.351, 36832/60000 datapoints
2025-03-06 20:20:42,987 - INFO - training batch 1201, loss: 0.409, 38432/60000 datapoints
2025-03-06 20:20:43,195 - INFO - training batch 1251, loss: 0.275, 40032/60000 datapoints
2025-03-06 20:20:43,398 - INFO - training batch 1301, loss: 0.301, 41632/60000 datapoints
2025-03-06 20:20:43,601 - INFO - training batch 1351, loss: 0.275, 43232/60000 datapoints
2025-03-06 20:20:43,809 - INFO - training batch 1401, loss: 0.092, 44832/60000 datapoints
2025-03-06 20:20:44,015 - INFO - training batch 1451, loss: 0.251, 46432/60000 datapoints
2025-03-06 20:20:44,219 - INFO - training batch 1501, loss: 0.397, 48032/60000 datapoints
2025-03-06 20:20:44,426 - INFO - training batch 1551, loss: 0.090, 49632/60000 datapoints
2025-03-06 20:20:44,637 - INFO - training batch 1601, loss: 0.074, 51232/60000 datapoints
2025-03-06 20:20:44,856 - INFO - training batch 1651, loss: 0.127, 52832/60000 datapoints
2025-03-06 20:20:45,070 - INFO - training batch 1701, loss: 0.137, 54432/60000 datapoints
2025-03-06 20:20:45,273 - INFO - training batch 1751, loss: 0.470, 56032/60000 datapoints
2025-03-06 20:20:45,475 - INFO - training batch 1801, loss: 0.138, 57632/60000 datapoints
2025-03-06 20:20:45,681 - INFO - training batch 1851, loss: 0.040, 59232/60000 datapoints
2025-03-06 20:20:45,790 - INFO - validation batch 1, loss: 0.430, 32/10016 datapoints
2025-03-06 20:20:45,955 - INFO - validation batch 51, loss: 0.181, 1632/10016 datapoints
2025-03-06 20:20:46,117 - INFO - validation batch 101, loss: 0.203, 3232/10016 datapoints
2025-03-06 20:20:46,282 - INFO - validation batch 151, loss: 0.288, 4832/10016 datapoints
2025-03-06 20:20:46,445 - INFO - validation batch 201, loss: 0.232, 6432/10016 datapoints
2025-03-06 20:20:46,609 - INFO - validation batch 251, loss: 0.214, 8032/10016 datapoints
2025-03-06 20:20:46,783 - INFO - validation batch 301, loss: 0.399, 9632/10016 datapoints
2025-03-06 20:20:46,823 - INFO - Epoch 681/800 done.
2025-03-06 20:20:46,823 - INFO - Final validation performance:
Loss: 0.278, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:20:46,824 - INFO - Beginning epoch 682/800
2025-03-06 20:20:46,833 - INFO - training batch 1, loss: 0.135, 32/60000 datapoints
2025-03-06 20:20:47,031 - INFO - training batch 51, loss: 0.096, 1632/60000 datapoints
2025-03-06 20:20:47,244 - INFO - training batch 101, loss: 0.389, 3232/60000 datapoints
2025-03-06 20:20:47,436 - INFO - training batch 151, loss: 0.207, 4832/60000 datapoints
2025-03-06 20:20:47,643 - INFO - training batch 201, loss: 0.316, 6432/60000 datapoints
2025-03-06 20:20:47,842 - INFO - training batch 251, loss: 0.182, 8032/60000 datapoints
2025-03-06 20:20:48,049 - INFO - training batch 301, loss: 0.198, 9632/60000 datapoints
2025-03-06 20:20:48,244 - INFO - training batch 351, loss: 0.634, 11232/60000 datapoints
2025-03-06 20:20:48,438 - INFO - training batch 401, loss: 0.285, 12832/60000 datapoints
2025-03-06 20:20:48,634 - INFO - training batch 451, loss: 0.062, 14432/60000 datapoints
2025-03-06 20:20:48,829 - INFO - training batch 501, loss: 0.321, 16032/60000 datapoints
2025-03-06 20:20:49,023 - INFO - training batch 551, loss: 0.183, 17632/60000 datapoints
2025-03-06 20:20:49,218 - INFO - training batch 601, loss: 0.253, 19232/60000 datapoints
2025-03-06 20:20:49,415 - INFO - training batch 651, loss: 0.157, 20832/60000 datapoints
2025-03-06 20:20:49,611 - INFO - training batch 701, loss: 0.227, 22432/60000 datapoints
2025-03-06 20:20:49,811 - INFO - training batch 751, loss: 0.119, 24032/60000 datapoints
2025-03-06 20:20:50,010 - INFO - training batch 801, loss: 0.120, 25632/60000 datapoints
2025-03-06 20:20:50,203 - INFO - training batch 851, loss: 0.148, 27232/60000 datapoints
2025-03-06 20:20:50,397 - INFO - training batch 901, loss: 0.121, 28832/60000 datapoints
2025-03-06 20:20:50,591 - INFO - training batch 951, loss: 0.425, 30432/60000 datapoints
2025-03-06 20:20:50,789 - INFO - training batch 1001, loss: 0.114, 32032/60000 datapoints
2025-03-06 20:20:50,984 - INFO - training batch 1051, loss: 0.255, 33632/60000 datapoints
2025-03-06 20:20:51,181 - INFO - training batch 1101, loss: 0.128, 35232/60000 datapoints
2025-03-06 20:20:51,374 - INFO - training batch 1151, loss: 0.346, 36832/60000 datapoints
2025-03-06 20:20:51,567 - INFO - training batch 1201, loss: 0.163, 38432/60000 datapoints
2025-03-06 20:20:51,764 - INFO - training batch 1251, loss: 0.174, 40032/60000 datapoints
2025-03-06 20:20:51,957 - INFO - training batch 1301, loss: 0.063, 41632/60000 datapoints
2025-03-06 20:20:52,153 - INFO - training batch 1351, loss: 0.108, 43232/60000 datapoints
2025-03-06 20:20:52,348 - INFO - training batch 1401, loss: 0.442, 44832/60000 datapoints
2025-03-06 20:20:52,543 - INFO - training batch 1451, loss: 0.068, 46432/60000 datapoints
2025-03-06 20:20:52,744 - INFO - training batch 1501, loss: 0.239, 48032/60000 datapoints
2025-03-06 20:20:52,941 - INFO - training batch 1551, loss: 0.076, 49632/60000 datapoints
2025-03-06 20:20:53,136 - INFO - training batch 1601, loss: 0.165, 51232/60000 datapoints
2025-03-06 20:20:53,332 - INFO - training batch 1651, loss: 0.385, 52832/60000 datapoints
2025-03-06 20:20:53,523 - INFO - training batch 1701, loss: 0.191, 54432/60000 datapoints
2025-03-06 20:20:53,722 - INFO - training batch 1751, loss: 0.204, 56032/60000 datapoints
2025-03-06 20:20:53,916 - INFO - training batch 1801, loss: 0.328, 57632/60000 datapoints
2025-03-06 20:20:54,115 - INFO - training batch 1851, loss: 0.113, 59232/60000 datapoints
2025-03-06 20:20:54,218 - INFO - validation batch 1, loss: 0.139, 32/10016 datapoints
2025-03-06 20:20:54,375 - INFO - validation batch 51, loss: 0.264, 1632/10016 datapoints
2025-03-06 20:20:54,541 - INFO - validation batch 101, loss: 0.257, 3232/10016 datapoints
2025-03-06 20:20:54,705 - INFO - validation batch 151, loss: 0.135, 4832/10016 datapoints
2025-03-06 20:20:54,892 - INFO - validation batch 201, loss: 0.148, 6432/10016 datapoints
2025-03-06 20:20:55,058 - INFO - validation batch 251, loss: 0.280, 8032/10016 datapoints
2025-03-06 20:20:55,216 - INFO - validation batch 301, loss: 0.204, 9632/10016 datapoints
2025-03-06 20:20:55,253 - INFO - Epoch 682/800 done.
2025-03-06 20:20:55,253 - INFO - Final validation performance:
Loss: 0.204, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:20:55,254 - INFO - Beginning epoch 683/800
2025-03-06 20:20:55,261 - INFO - training batch 1, loss: 0.197, 32/60000 datapoints
2025-03-06 20:20:55,474 - INFO - training batch 51, loss: 0.231, 1632/60000 datapoints
2025-03-06 20:20:55,674 - INFO - training batch 101, loss: 0.474, 3232/60000 datapoints
2025-03-06 20:20:55,877 - INFO - training batch 151, loss: 0.691, 4832/60000 datapoints
2025-03-06 20:20:56,083 - INFO - training batch 201, loss: 0.519, 6432/60000 datapoints
2025-03-06 20:20:56,282 - INFO - training batch 251, loss: 0.182, 8032/60000 datapoints
2025-03-06 20:20:56,476 - INFO - training batch 301, loss: 0.239, 9632/60000 datapoints
2025-03-06 20:20:56,672 - INFO - training batch 351, loss: 0.285, 11232/60000 datapoints
2025-03-06 20:20:56,871 - INFO - training batch 401, loss: 0.043, 12832/60000 datapoints
2025-03-06 20:20:57,065 - INFO - training batch 451, loss: 0.200, 14432/60000 datapoints
2025-03-06 20:20:57,261 - INFO - training batch 501, loss: 0.284, 16032/60000 datapoints
2025-03-06 20:20:57,457 - INFO - training batch 551, loss: 0.418, 17632/60000 datapoints
2025-03-06 20:20:57,661 - INFO - training batch 601, loss: 0.399, 19232/60000 datapoints
2025-03-06 20:20:57,880 - INFO - training batch 651, loss: 0.144, 20832/60000 datapoints
2025-03-06 20:20:58,083 - INFO - training batch 701, loss: 0.133, 22432/60000 datapoints
2025-03-06 20:20:58,282 - INFO - training batch 751, loss: 0.089, 24032/60000 datapoints
2025-03-06 20:20:58,484 - INFO - training batch 801, loss: 0.216, 25632/60000 datapoints
2025-03-06 20:20:58,692 - INFO - training batch 851, loss: 0.076, 27232/60000 datapoints
2025-03-06 20:20:58,899 - INFO - training batch 901, loss: 0.049, 28832/60000 datapoints
2025-03-06 20:20:59,106 - INFO - training batch 951, loss: 0.105, 30432/60000 datapoints
2025-03-06 20:20:59,387 - INFO - training batch 1001, loss: 0.119, 32032/60000 datapoints
2025-03-06 20:20:59,583 - INFO - training batch 1051, loss: 0.190, 33632/60000 datapoints
2025-03-06 20:20:59,786 - INFO - training batch 1101, loss: 0.123, 35232/60000 datapoints
2025-03-06 20:20:59,985 - INFO - training batch 1151, loss: 0.052, 36832/60000 datapoints
2025-03-06 20:21:00,187 - INFO - training batch 1201, loss: 0.259, 38432/60000 datapoints
2025-03-06 20:21:00,386 - INFO - training batch 1251, loss: 0.117, 40032/60000 datapoints
2025-03-06 20:21:00,591 - INFO - training batch 1301, loss: 0.239, 41632/60000 datapoints
2025-03-06 20:21:00,793 - INFO - training batch 1351, loss: 0.397, 43232/60000 datapoints
2025-03-06 20:21:00,992 - INFO - training batch 1401, loss: 0.115, 44832/60000 datapoints
2025-03-06 20:21:01,191 - INFO - training batch 1451, loss: 0.201, 46432/60000 datapoints
2025-03-06 20:21:01,392 - INFO - training batch 1501, loss: 0.135, 48032/60000 datapoints
2025-03-06 20:21:01,591 - INFO - training batch 1551, loss: 0.232, 49632/60000 datapoints
2025-03-06 20:21:01,796 - INFO - training batch 1601, loss: 0.270, 51232/60000 datapoints
2025-03-06 20:21:02,001 - INFO - training batch 1651, loss: 0.238, 52832/60000 datapoints
2025-03-06 20:21:02,203 - INFO - training batch 1701, loss: 0.283, 54432/60000 datapoints
2025-03-06 20:21:02,400 - INFO - training batch 1751, loss: 0.122, 56032/60000 datapoints
2025-03-06 20:21:02,600 - INFO - training batch 1801, loss: 0.150, 57632/60000 datapoints
2025-03-06 20:21:02,813 - INFO - training batch 1851, loss: 0.306, 59232/60000 datapoints
2025-03-06 20:21:02,931 - INFO - validation batch 1, loss: 0.356, 32/10016 datapoints
2025-03-06 20:21:03,092 - INFO - validation batch 51, loss: 0.212, 1632/10016 datapoints
2025-03-06 20:21:03,268 - INFO - validation batch 101, loss: 0.115, 3232/10016 datapoints
2025-03-06 20:21:03,434 - INFO - validation batch 151, loss: 0.110, 4832/10016 datapoints
2025-03-06 20:21:03,602 - INFO - validation batch 201, loss: 0.146, 6432/10016 datapoints
2025-03-06 20:21:03,774 - INFO - validation batch 251, loss: 0.367, 8032/10016 datapoints
2025-03-06 20:21:03,941 - INFO - validation batch 301, loss: 0.178, 9632/10016 datapoints
2025-03-06 20:21:03,981 - INFO - Epoch 683/800 done.
2025-03-06 20:21:03,981 - INFO - Final validation performance:
Loss: 0.212, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:21:03,982 - INFO - Beginning epoch 684/800
2025-03-06 20:21:03,990 - INFO - training batch 1, loss: 0.160, 32/60000 datapoints
2025-03-06 20:21:04,211 - INFO - training batch 51, loss: 0.414, 1632/60000 datapoints
2025-03-06 20:21:04,419 - INFO - training batch 101, loss: 0.216, 3232/60000 datapoints
2025-03-06 20:21:04,646 - INFO - training batch 151, loss: 0.093, 4832/60000 datapoints
2025-03-06 20:21:04,868 - INFO - training batch 201, loss: 0.265, 6432/60000 datapoints
2025-03-06 20:21:05,109 - INFO - training batch 251, loss: 0.275, 8032/60000 datapoints
2025-03-06 20:21:05,321 - INFO - training batch 301, loss: 0.126, 9632/60000 datapoints
2025-03-06 20:21:05,531 - INFO - training batch 351, loss: 0.144, 11232/60000 datapoints
2025-03-06 20:21:05,742 - INFO - training batch 401, loss: 0.330, 12832/60000 datapoints
2025-03-06 20:21:05,952 - INFO - training batch 451, loss: 0.157, 14432/60000 datapoints
2025-03-06 20:21:06,162 - INFO - training batch 501, loss: 0.155, 16032/60000 datapoints
2025-03-06 20:21:06,369 - INFO - training batch 551, loss: 0.315, 17632/60000 datapoints
2025-03-06 20:21:06,580 - INFO - training batch 601, loss: 0.536, 19232/60000 datapoints
2025-03-06 20:21:06,790 - INFO - training batch 651, loss: 0.309, 20832/60000 datapoints
2025-03-06 20:21:07,001 - INFO - training batch 701, loss: 0.213, 22432/60000 datapoints
2025-03-06 20:21:07,208 - INFO - training batch 751, loss: 0.096, 24032/60000 datapoints
2025-03-06 20:21:07,416 - INFO - training batch 801, loss: 0.297, 25632/60000 datapoints
2025-03-06 20:21:07,620 - INFO - training batch 851, loss: 0.297, 27232/60000 datapoints
2025-03-06 20:21:07,835 - INFO - training batch 901, loss: 0.120, 28832/60000 datapoints
2025-03-06 20:21:08,037 - INFO - training batch 951, loss: 0.163, 30432/60000 datapoints
2025-03-06 20:21:08,237 - INFO - training batch 1001, loss: 0.238, 32032/60000 datapoints
2025-03-06 20:21:08,437 - INFO - training batch 1051, loss: 0.073, 33632/60000 datapoints
2025-03-06 20:21:08,644 - INFO - training batch 1101, loss: 0.274, 35232/60000 datapoints
2025-03-06 20:21:08,849 - INFO - training batch 1151, loss: 0.346, 36832/60000 datapoints
2025-03-06 20:21:09,054 - INFO - training batch 1201, loss: 0.231, 38432/60000 datapoints
2025-03-06 20:21:09,256 - INFO - training batch 1251, loss: 0.300, 40032/60000 datapoints
2025-03-06 20:21:09,457 - INFO - training batch 1301, loss: 0.290, 41632/60000 datapoints
2025-03-06 20:21:09,661 - INFO - training batch 1351, loss: 0.220, 43232/60000 datapoints
2025-03-06 20:21:09,864 - INFO - training batch 1401, loss: 0.267, 44832/60000 datapoints
2025-03-06 20:21:10,068 - INFO - training batch 1451, loss: 0.358, 46432/60000 datapoints
2025-03-06 20:21:10,267 - INFO - training batch 1501, loss: 0.177, 48032/60000 datapoints
2025-03-06 20:21:10,469 - INFO - training batch 1551, loss: 0.097, 49632/60000 datapoints
2025-03-06 20:21:10,670 - INFO - training batch 1601, loss: 0.237, 51232/60000 datapoints
2025-03-06 20:21:10,873 - INFO - training batch 1651, loss: 0.213, 52832/60000 datapoints
2025-03-06 20:21:11,073 - INFO - training batch 1701, loss: 0.361, 54432/60000 datapoints
2025-03-06 20:21:11,273 - INFO - training batch 1751, loss: 0.126, 56032/60000 datapoints
2025-03-06 20:21:11,475 - INFO - training batch 1801, loss: 0.311, 57632/60000 datapoints
2025-03-06 20:21:11,675 - INFO - training batch 1851, loss: 0.221, 59232/60000 datapoints
2025-03-06 20:21:11,782 - INFO - validation batch 1, loss: 0.163, 32/10016 datapoints
2025-03-06 20:21:11,949 - INFO - validation batch 51, loss: 0.182, 1632/10016 datapoints
2025-03-06 20:21:12,112 - INFO - validation batch 101, loss: 0.342, 3232/10016 datapoints
2025-03-06 20:21:12,277 - INFO - validation batch 151, loss: 0.513, 4832/10016 datapoints
2025-03-06 20:21:12,439 - INFO - validation batch 201, loss: 0.315, 6432/10016 datapoints
2025-03-06 20:21:12,602 - INFO - validation batch 251, loss: 0.171, 8032/10016 datapoints
2025-03-06 20:21:12,768 - INFO - validation batch 301, loss: 0.159, 9632/10016 datapoints
2025-03-06 20:21:12,810 - INFO - Epoch 684/800 done.
2025-03-06 20:21:12,810 - INFO - Final validation performance:
Loss: 0.264, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:21:12,810 - INFO - Beginning epoch 685/800
2025-03-06 20:21:12,823 - INFO - training batch 1, loss: 0.248, 32/60000 datapoints
2025-03-06 20:21:13,053 - INFO - training batch 51, loss: 0.054, 1632/60000 datapoints
2025-03-06 20:21:13,256 - INFO - training batch 101, loss: 0.369, 3232/60000 datapoints
2025-03-06 20:21:13,466 - INFO - training batch 151, loss: 0.182, 4832/60000 datapoints
2025-03-06 20:21:13,672 - INFO - training batch 201, loss: 0.288, 6432/60000 datapoints
2025-03-06 20:21:13,884 - INFO - training batch 251, loss: 0.190, 8032/60000 datapoints
2025-03-06 20:21:14,099 - INFO - training batch 301, loss: 0.127, 9632/60000 datapoints
2025-03-06 20:21:14,298 - INFO - training batch 351, loss: 0.169, 11232/60000 datapoints
2025-03-06 20:21:14,500 - INFO - training batch 401, loss: 0.169, 12832/60000 datapoints
2025-03-06 20:21:14,708 - INFO - training batch 451, loss: 0.760, 14432/60000 datapoints
2025-03-06 20:21:14,919 - INFO - training batch 501, loss: 0.360, 16032/60000 datapoints
2025-03-06 20:21:15,146 - INFO - training batch 551, loss: 0.160, 17632/60000 datapoints
2025-03-06 20:21:15,348 - INFO - training batch 601, loss: 0.282, 19232/60000 datapoints
2025-03-06 20:21:15,552 - INFO - training batch 651, loss: 0.077, 20832/60000 datapoints
2025-03-06 20:21:15,758 - INFO - training batch 701, loss: 0.073, 22432/60000 datapoints
2025-03-06 20:21:15,961 - INFO - training batch 751, loss: 0.257, 24032/60000 datapoints
2025-03-06 20:21:16,167 - INFO - training batch 801, loss: 0.149, 25632/60000 datapoints
2025-03-06 20:21:16,371 - INFO - training batch 851, loss: 0.336, 27232/60000 datapoints
2025-03-06 20:21:16,578 - INFO - training batch 901, loss: 0.333, 28832/60000 datapoints
2025-03-06 20:21:16,784 - INFO - training batch 951, loss: 0.227, 30432/60000 datapoints
2025-03-06 20:21:16,989 - INFO - training batch 1001, loss: 0.097, 32032/60000 datapoints
2025-03-06 20:21:17,188 - INFO - training batch 1051, loss: 0.091, 33632/60000 datapoints
2025-03-06 20:21:17,391 - INFO - training batch 1101, loss: 0.071, 35232/60000 datapoints
2025-03-06 20:21:17,600 - INFO - training batch 1151, loss: 0.189, 36832/60000 datapoints
2025-03-06 20:21:17,801 - INFO - training batch 1201, loss: 0.133, 38432/60000 datapoints
2025-03-06 20:21:18,035 - INFO - training batch 1251, loss: 0.161, 40032/60000 datapoints
2025-03-06 20:21:18,238 - INFO - training batch 1301, loss: 0.093, 41632/60000 datapoints
2025-03-06 20:21:18,440 - INFO - training batch 1351, loss: 0.074, 43232/60000 datapoints
2025-03-06 20:21:18,643 - INFO - training batch 1401, loss: 0.088, 44832/60000 datapoints
2025-03-06 20:21:18,847 - INFO - training batch 1451, loss: 0.155, 46432/60000 datapoints
2025-03-06 20:21:19,049 - INFO - training batch 1501, loss: 0.152, 48032/60000 datapoints
2025-03-06 20:21:19,254 - INFO - training batch 1551, loss: 0.240, 49632/60000 datapoints
2025-03-06 20:21:19,461 - INFO - training batch 1601, loss: 0.192, 51232/60000 datapoints
2025-03-06 20:21:19,670 - INFO - training batch 1651, loss: 0.138, 52832/60000 datapoints
2025-03-06 20:21:19,878 - INFO - training batch 1701, loss: 0.209, 54432/60000 datapoints
2025-03-06 20:21:20,092 - INFO - training batch 1751, loss: 0.196, 56032/60000 datapoints
2025-03-06 20:21:20,298 - INFO - training batch 1801, loss: 0.381, 57632/60000 datapoints
2025-03-06 20:21:20,505 - INFO - training batch 1851, loss: 0.120, 59232/60000 datapoints
2025-03-06 20:21:20,613 - INFO - validation batch 1, loss: 0.165, 32/10016 datapoints
2025-03-06 20:21:20,782 - INFO - validation batch 51, loss: 0.093, 1632/10016 datapoints
2025-03-06 20:21:20,950 - INFO - validation batch 101, loss: 0.160, 3232/10016 datapoints
2025-03-06 20:21:21,115 - INFO - validation batch 151, loss: 0.123, 4832/10016 datapoints
2025-03-06 20:21:21,283 - INFO - validation batch 201, loss: 0.140, 6432/10016 datapoints
2025-03-06 20:21:21,448 - INFO - validation batch 251, loss: 0.192, 8032/10016 datapoints
2025-03-06 20:21:21,613 - INFO - validation batch 301, loss: 0.350, 9632/10016 datapoints
2025-03-06 20:21:21,660 - INFO - Epoch 685/800 done.
2025-03-06 20:21:21,660 - INFO - Final validation performance:
Loss: 0.175, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:21:21,661 - INFO - Beginning epoch 686/800
2025-03-06 20:21:21,670 - INFO - training batch 1, loss: 0.282, 32/60000 datapoints
2025-03-06 20:21:21,886 - INFO - training batch 51, loss: 0.032, 1632/60000 datapoints
2025-03-06 20:21:22,118 - INFO - training batch 101, loss: 0.349, 3232/60000 datapoints
2025-03-06 20:21:22,327 - INFO - training batch 151, loss: 0.244, 4832/60000 datapoints
2025-03-06 20:21:22,541 - INFO - training batch 201, loss: 0.127, 6432/60000 datapoints
2025-03-06 20:21:22,753 - INFO - training batch 251, loss: 0.160, 8032/60000 datapoints
2025-03-06 20:21:22,968 - INFO - training batch 301, loss: 0.125, 9632/60000 datapoints
2025-03-06 20:21:23,176 - INFO - training batch 351, loss: 0.136, 11232/60000 datapoints
2025-03-06 20:21:23,387 - INFO - training batch 401, loss: 0.288, 12832/60000 datapoints
2025-03-06 20:21:23,598 - INFO - training batch 451, loss: 0.165, 14432/60000 datapoints
2025-03-06 20:21:23,809 - INFO - training batch 501, loss: 0.266, 16032/60000 datapoints
2025-03-06 20:21:24,018 - INFO - training batch 551, loss: 0.100, 17632/60000 datapoints
2025-03-06 20:21:24,226 - INFO - training batch 601, loss: 0.130, 19232/60000 datapoints
2025-03-06 20:21:24,432 - INFO - training batch 651, loss: 0.273, 20832/60000 datapoints
2025-03-06 20:21:24,644 - INFO - training batch 701, loss: 0.118, 22432/60000 datapoints
2025-03-06 20:21:24,854 - INFO - training batch 751, loss: 0.113, 24032/60000 datapoints
2025-03-06 20:21:25,088 - INFO - training batch 801, loss: 0.195, 25632/60000 datapoints
2025-03-06 20:21:25,301 - INFO - training batch 851, loss: 0.370, 27232/60000 datapoints
2025-03-06 20:21:25,506 - INFO - training batch 901, loss: 0.131, 28832/60000 datapoints
2025-03-06 20:21:25,716 - INFO - training batch 951, loss: 0.149, 30432/60000 datapoints
2025-03-06 20:21:25,924 - INFO - training batch 1001, loss: 0.276, 32032/60000 datapoints
2025-03-06 20:21:26,133 - INFO - training batch 1051, loss: 0.166, 33632/60000 datapoints
2025-03-06 20:21:26,338 - INFO - training batch 1101, loss: 0.254, 35232/60000 datapoints
2025-03-06 20:21:26,541 - INFO - training batch 1151, loss: 0.160, 36832/60000 datapoints
2025-03-06 20:21:26,747 - INFO - training batch 1201, loss: 0.378, 38432/60000 datapoints
2025-03-06 20:21:26,959 - INFO - training batch 1251, loss: 0.527, 40032/60000 datapoints
2025-03-06 20:21:27,170 - INFO - training batch 1301, loss: 0.381, 41632/60000 datapoints
2025-03-06 20:21:27,377 - INFO - training batch 1351, loss: 0.248, 43232/60000 datapoints
2025-03-06 20:21:27,585 - INFO - training batch 1401, loss: 0.203, 44832/60000 datapoints
2025-03-06 20:21:27,791 - INFO - training batch 1451, loss: 0.216, 46432/60000 datapoints
2025-03-06 20:21:27,999 - INFO - training batch 1501, loss: 0.104, 48032/60000 datapoints
2025-03-06 20:21:28,209 - INFO - training batch 1551, loss: 0.265, 49632/60000 datapoints
2025-03-06 20:21:28,416 - INFO - training batch 1601, loss: 0.134, 51232/60000 datapoints
2025-03-06 20:21:28,622 - INFO - training batch 1651, loss: 0.400, 52832/60000 datapoints
2025-03-06 20:21:28,841 - INFO - training batch 1701, loss: 0.160, 54432/60000 datapoints
2025-03-06 20:21:29,050 - INFO - training batch 1751, loss: 0.563, 56032/60000 datapoints
2025-03-06 20:21:29,260 - INFO - training batch 1801, loss: 0.112, 57632/60000 datapoints
2025-03-06 20:21:29,468 - INFO - training batch 1851, loss: 0.339, 59232/60000 datapoints
2025-03-06 20:21:29,578 - INFO - validation batch 1, loss: 0.398, 32/10016 datapoints
2025-03-06 20:21:29,748 - INFO - validation batch 51, loss: 0.255, 1632/10016 datapoints
2025-03-06 20:21:29,917 - INFO - validation batch 101, loss: 0.073, 3232/10016 datapoints
2025-03-06 20:21:30,087 - INFO - validation batch 151, loss: 0.291, 4832/10016 datapoints
2025-03-06 20:21:30,257 - INFO - validation batch 201, loss: 0.203, 6432/10016 datapoints
2025-03-06 20:21:30,425 - INFO - validation batch 251, loss: 0.220, 8032/10016 datapoints
2025-03-06 20:21:30,592 - INFO - validation batch 301, loss: 0.344, 9632/10016 datapoints
2025-03-06 20:21:30,638 - INFO - Epoch 686/800 done.
2025-03-06 20:21:30,638 - INFO - Final validation performance:
Loss: 0.255, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:21:30,639 - INFO - Beginning epoch 687/800
2025-03-06 20:21:30,646 - INFO - training batch 1, loss: 0.094, 32/60000 datapoints
2025-03-06 20:21:30,871 - INFO - training batch 51, loss: 0.289, 1632/60000 datapoints
2025-03-06 20:21:31,082 - INFO - training batch 101, loss: 0.317, 3232/60000 datapoints
2025-03-06 20:21:31,296 - INFO - training batch 151, loss: 0.216, 4832/60000 datapoints
2025-03-06 20:21:31,504 - INFO - training batch 201, loss: 0.239, 6432/60000 datapoints
2025-03-06 20:21:31,718 - INFO - training batch 251, loss: 0.322, 8032/60000 datapoints
2025-03-06 20:21:31,925 - INFO - training batch 301, loss: 0.191, 9632/60000 datapoints
2025-03-06 20:21:32,129 - INFO - training batch 351, loss: 0.202, 11232/60000 datapoints
2025-03-06 20:21:32,332 - INFO - training batch 401, loss: 0.178, 12832/60000 datapoints
2025-03-06 20:21:32,530 - INFO - training batch 451, loss: 0.235, 14432/60000 datapoints
2025-03-06 20:21:32,735 - INFO - training batch 501, loss: 0.282, 16032/60000 datapoints
2025-03-06 20:21:32,934 - INFO - training batch 551, loss: 0.509, 17632/60000 datapoints
2025-03-06 20:21:33,142 - INFO - training batch 601, loss: 0.494, 19232/60000 datapoints
2025-03-06 20:21:33,351 - INFO - training batch 651, loss: 0.348, 20832/60000 datapoints
2025-03-06 20:21:33,553 - INFO - training batch 701, loss: 0.429, 22432/60000 datapoints
2025-03-06 20:21:33,759 - INFO - training batch 751, loss: 0.227, 24032/60000 datapoints
2025-03-06 20:21:33,965 - INFO - training batch 801, loss: 0.407, 25632/60000 datapoints
2025-03-06 20:21:34,171 - INFO - training batch 851, loss: 0.616, 27232/60000 datapoints
2025-03-06 20:21:34,376 - INFO - training batch 901, loss: 0.096, 28832/60000 datapoints
2025-03-06 20:21:34,577 - INFO - training batch 951, loss: 0.203, 30432/60000 datapoints
2025-03-06 20:21:34,810 - INFO - training batch 1001, loss: 0.309, 32032/60000 datapoints
2025-03-06 20:21:35,070 - INFO - training batch 1051, loss: 0.196, 33632/60000 datapoints
2025-03-06 20:21:35,301 - INFO - training batch 1101, loss: 0.329, 35232/60000 datapoints
2025-03-06 20:21:35,499 - INFO - training batch 1151, loss: 0.113, 36832/60000 datapoints
2025-03-06 20:21:35,703 - INFO - training batch 1201, loss: 0.232, 38432/60000 datapoints
2025-03-06 20:21:35,901 - INFO - training batch 1251, loss: 0.181, 40032/60000 datapoints
2025-03-06 20:21:36,102 - INFO - training batch 1301, loss: 0.382, 41632/60000 datapoints
2025-03-06 20:21:36,306 - INFO - training batch 1351, loss: 0.205, 43232/60000 datapoints
2025-03-06 20:21:36,504 - INFO - training batch 1401, loss: 0.218, 44832/60000 datapoints
2025-03-06 20:21:36,707 - INFO - training batch 1451, loss: 0.215, 46432/60000 datapoints
2025-03-06 20:21:36,906 - INFO - training batch 1501, loss: 0.303, 48032/60000 datapoints
2025-03-06 20:21:37,108 - INFO - training batch 1551, loss: 0.242, 49632/60000 datapoints
2025-03-06 20:21:37,305 - INFO - training batch 1601, loss: 0.080, 51232/60000 datapoints
2025-03-06 20:21:37,503 - INFO - training batch 1651, loss: 0.131, 52832/60000 datapoints
2025-03-06 20:21:37,714 - INFO - training batch 1701, loss: 0.438, 54432/60000 datapoints
2025-03-06 20:21:37,922 - INFO - training batch 1751, loss: 0.118, 56032/60000 datapoints
2025-03-06 20:21:38,124 - INFO - training batch 1801, loss: 0.139, 57632/60000 datapoints
2025-03-06 20:21:38,325 - INFO - training batch 1851, loss: 0.060, 59232/60000 datapoints
2025-03-06 20:21:38,429 - INFO - validation batch 1, loss: 0.331, 32/10016 datapoints
2025-03-06 20:21:38,590 - INFO - validation batch 51, loss: 0.149, 1632/10016 datapoints
2025-03-06 20:21:38,756 - INFO - validation batch 101, loss: 0.173, 3232/10016 datapoints
2025-03-06 20:21:38,929 - INFO - validation batch 151, loss: 0.107, 4832/10016 datapoints
2025-03-06 20:21:39,127 - INFO - validation batch 201, loss: 0.241, 6432/10016 datapoints
2025-03-06 20:21:39,296 - INFO - validation batch 251, loss: 0.085, 8032/10016 datapoints
2025-03-06 20:21:39,460 - INFO - validation batch 301, loss: 0.278, 9632/10016 datapoints
2025-03-06 20:21:39,500 - INFO - Epoch 687/800 done.
2025-03-06 20:21:39,501 - INFO - Final validation performance:
Loss: 0.195, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:21:39,501 - INFO - Beginning epoch 688/800
2025-03-06 20:21:39,508 - INFO - training batch 1, loss: 0.205, 32/60000 datapoints
2025-03-06 20:21:39,729 - INFO - training batch 51, loss: 0.270, 1632/60000 datapoints
2025-03-06 20:21:39,936 - INFO - training batch 101, loss: 0.140, 3232/60000 datapoints
2025-03-06 20:21:40,155 - INFO - training batch 151, loss: 0.207, 4832/60000 datapoints
2025-03-06 20:21:40,375 - INFO - training batch 201, loss: 0.305, 6432/60000 datapoints
2025-03-06 20:21:40,596 - INFO - training batch 251, loss: 0.442, 8032/60000 datapoints
2025-03-06 20:21:40,809 - INFO - training batch 301, loss: 0.171, 9632/60000 datapoints
2025-03-06 20:21:41,025 - INFO - training batch 351, loss: 0.233, 11232/60000 datapoints
2025-03-06 20:21:41,268 - INFO - training batch 401, loss: 0.218, 12832/60000 datapoints
2025-03-06 20:21:41,483 - INFO - training batch 451, loss: 0.166, 14432/60000 datapoints
2025-03-06 20:21:41,699 - INFO - training batch 501, loss: 0.187, 16032/60000 datapoints
2025-03-06 20:21:41,912 - INFO - training batch 551, loss: 0.278, 17632/60000 datapoints
2025-03-06 20:21:42,124 - INFO - training batch 601, loss: 0.207, 19232/60000 datapoints
2025-03-06 20:21:42,343 - INFO - training batch 651, loss: 0.231, 20832/60000 datapoints
2025-03-06 20:21:42,556 - INFO - training batch 701, loss: 0.334, 22432/60000 datapoints
2025-03-06 20:21:42,772 - INFO - training batch 751, loss: 0.170, 24032/60000 datapoints
2025-03-06 20:21:42,988 - INFO - training batch 801, loss: 0.193, 25632/60000 datapoints
2025-03-06 20:21:43,201 - INFO - training batch 851, loss: 0.531, 27232/60000 datapoints
2025-03-06 20:21:43,419 - INFO - training batch 901, loss: 0.113, 28832/60000 datapoints
2025-03-06 20:21:43,636 - INFO - training batch 951, loss: 0.153, 30432/60000 datapoints
2025-03-06 20:21:43,850 - INFO - training batch 1001, loss: 0.403, 32032/60000 datapoints
2025-03-06 20:21:44,064 - INFO - training batch 1051, loss: 0.417, 33632/60000 datapoints
2025-03-06 20:21:44,283 - INFO - training batch 1101, loss: 0.170, 35232/60000 datapoints
2025-03-06 20:21:44,494 - INFO - training batch 1151, loss: 0.517, 36832/60000 datapoints
2025-03-06 20:21:44,712 - INFO - training batch 1201, loss: 0.110, 38432/60000 datapoints
2025-03-06 20:21:44,930 - INFO - training batch 1251, loss: 0.215, 40032/60000 datapoints
2025-03-06 20:21:45,153 - INFO - training batch 1301, loss: 0.175, 41632/60000 datapoints
2025-03-06 20:21:45,399 - INFO - training batch 1351, loss: 0.299, 43232/60000 datapoints
2025-03-06 20:21:45,614 - INFO - training batch 1401, loss: 0.300, 44832/60000 datapoints
2025-03-06 20:21:45,831 - INFO - training batch 1451, loss: 0.211, 46432/60000 datapoints
2025-03-06 20:21:46,043 - INFO - training batch 1501, loss: 0.180, 48032/60000 datapoints
2025-03-06 20:21:46,264 - INFO - training batch 1551, loss: 0.071, 49632/60000 datapoints
2025-03-06 20:21:46,479 - INFO - training batch 1601, loss: 0.195, 51232/60000 datapoints
2025-03-06 20:21:46,697 - INFO - training batch 1651, loss: 0.089, 52832/60000 datapoints
2025-03-06 20:21:46,910 - INFO - training batch 1701, loss: 0.211, 54432/60000 datapoints
2025-03-06 20:21:47,127 - INFO - training batch 1751, loss: 0.079, 56032/60000 datapoints
2025-03-06 20:21:47,334 - INFO - training batch 1801, loss: 0.124, 57632/60000 datapoints
2025-03-06 20:21:47,541 - INFO - training batch 1851, loss: 0.237, 59232/60000 datapoints
2025-03-06 20:21:47,655 - INFO - validation batch 1, loss: 0.060, 32/10016 datapoints
2025-03-06 20:21:47,822 - INFO - validation batch 51, loss: 0.128, 1632/10016 datapoints
2025-03-06 20:21:47,993 - INFO - validation batch 101, loss: 0.181, 3232/10016 datapoints
2025-03-06 20:21:48,159 - INFO - validation batch 151, loss: 0.145, 4832/10016 datapoints
2025-03-06 20:21:48,330 - INFO - validation batch 201, loss: 0.533, 6432/10016 datapoints
2025-03-06 20:21:48,497 - INFO - validation batch 251, loss: 0.190, 8032/10016 datapoints
2025-03-06 20:21:48,666 - INFO - validation batch 301, loss: 0.232, 9632/10016 datapoints
2025-03-06 20:21:48,709 - INFO - Epoch 688/800 done.
2025-03-06 20:21:48,709 - INFO - Final validation performance:
Loss: 0.210, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:21:48,710 - INFO - Beginning epoch 689/800
2025-03-06 20:21:48,717 - INFO - training batch 1, loss: 0.080, 32/60000 datapoints
2025-03-06 20:21:48,936 - INFO - training batch 51, loss: 0.097, 1632/60000 datapoints
2025-03-06 20:21:49,132 - INFO - training batch 101, loss: 0.249, 3232/60000 datapoints
2025-03-06 20:21:49,337 - INFO - training batch 151, loss: 0.150, 4832/60000 datapoints
2025-03-06 20:21:49,536 - INFO - training batch 201, loss: 0.076, 6432/60000 datapoints
2025-03-06 20:21:49,738 - INFO - training batch 251, loss: 0.340, 8032/60000 datapoints
2025-03-06 20:21:49,934 - INFO - training batch 301, loss: 0.265, 9632/60000 datapoints
2025-03-06 20:21:50,130 - INFO - training batch 351, loss: 0.201, 11232/60000 datapoints
2025-03-06 20:21:50,331 - INFO - training batch 401, loss: 0.221, 12832/60000 datapoints
2025-03-06 20:21:50,525 - INFO - training batch 451, loss: 0.298, 14432/60000 datapoints
2025-03-06 20:21:50,720 - INFO - training batch 501, loss: 0.139, 16032/60000 datapoints
2025-03-06 20:21:50,915 - INFO - training batch 551, loss: 0.251, 17632/60000 datapoints
2025-03-06 20:21:51,111 - INFO - training batch 601, loss: 0.176, 19232/60000 datapoints
2025-03-06 20:21:51,307 - INFO - training batch 651, loss: 0.231, 20832/60000 datapoints
2025-03-06 20:21:51,502 - INFO - training batch 701, loss: 0.191, 22432/60000 datapoints
2025-03-06 20:21:51,699 - INFO - training batch 751, loss: 0.183, 24032/60000 datapoints
2025-03-06 20:21:51,896 - INFO - training batch 801, loss: 0.181, 25632/60000 datapoints
2025-03-06 20:21:52,092 - INFO - training batch 851, loss: 0.247, 27232/60000 datapoints
2025-03-06 20:21:52,287 - INFO - training batch 901, loss: 0.111, 28832/60000 datapoints
2025-03-06 20:21:52,481 - INFO - training batch 951, loss: 0.267, 30432/60000 datapoints
2025-03-06 20:21:52,677 - INFO - training batch 1001, loss: 0.313, 32032/60000 datapoints
2025-03-06 20:21:52,868 - INFO - training batch 1051, loss: 0.102, 33632/60000 datapoints
2025-03-06 20:21:53,071 - INFO - training batch 1101, loss: 0.170, 35232/60000 datapoints
2025-03-06 20:21:53,270 - INFO - training batch 1151, loss: 0.331, 36832/60000 datapoints
2025-03-06 20:21:53,464 - INFO - training batch 1201, loss: 0.146, 38432/60000 datapoints
2025-03-06 20:21:53,662 - INFO - training batch 1251, loss: 0.310, 40032/60000 datapoints
2025-03-06 20:21:53,858 - INFO - training batch 1301, loss: 0.244, 41632/60000 datapoints
2025-03-06 20:21:54,052 - INFO - training batch 1351, loss: 0.246, 43232/60000 datapoints
2025-03-06 20:21:54,248 - INFO - training batch 1401, loss: 0.179, 44832/60000 datapoints
2025-03-06 20:21:54,443 - INFO - training batch 1451, loss: 0.171, 46432/60000 datapoints
2025-03-06 20:21:54,639 - INFO - training batch 1501, loss: 0.199, 48032/60000 datapoints
2025-03-06 20:21:54,834 - INFO - training batch 1551, loss: 0.175, 49632/60000 datapoints
2025-03-06 20:21:55,036 - INFO - training batch 1601, loss: 0.257, 51232/60000 datapoints
2025-03-06 20:21:55,238 - INFO - training batch 1651, loss: 0.162, 52832/60000 datapoints
2025-03-06 20:21:55,462 - INFO - training batch 1701, loss: 0.364, 54432/60000 datapoints
2025-03-06 20:21:55,660 - INFO - training batch 1751, loss: 0.076, 56032/60000 datapoints
2025-03-06 20:21:55,854 - INFO - training batch 1801, loss: 0.241, 57632/60000 datapoints
2025-03-06 20:21:56,047 - INFO - training batch 1851, loss: 0.164, 59232/60000 datapoints
2025-03-06 20:21:56,147 - INFO - validation batch 1, loss: 0.209, 32/10016 datapoints
2025-03-06 20:21:56,306 - INFO - validation batch 51, loss: 0.355, 1632/10016 datapoints
2025-03-06 20:21:56,464 - INFO - validation batch 101, loss: 0.212, 3232/10016 datapoints
2025-03-06 20:21:56,619 - INFO - validation batch 151, loss: 0.058, 4832/10016 datapoints
2025-03-06 20:21:56,776 - INFO - validation batch 201, loss: 0.542, 6432/10016 datapoints
2025-03-06 20:21:56,934 - INFO - validation batch 251, loss: 0.053, 8032/10016 datapoints
2025-03-06 20:21:57,092 - INFO - validation batch 301, loss: 0.259, 9632/10016 datapoints
2025-03-06 20:21:57,128 - INFO - Epoch 689/800 done.
2025-03-06 20:21:57,129 - INFO - Final validation performance:
Loss: 0.241, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:21:57,129 - INFO - Beginning epoch 690/800
2025-03-06 20:21:57,137 - INFO - training batch 1, loss: 0.118, 32/60000 datapoints
2025-03-06 20:21:57,350 - INFO - training batch 51, loss: 0.036, 1632/60000 datapoints
2025-03-06 20:21:57,546 - INFO - training batch 101, loss: 0.268, 3232/60000 datapoints
2025-03-06 20:21:57,751 - INFO - training batch 151, loss: 0.157, 4832/60000 datapoints
2025-03-06 20:21:57,950 - INFO - training batch 201, loss: 0.055, 6432/60000 datapoints
2025-03-06 20:21:58,149 - INFO - training batch 251, loss: 0.144, 8032/60000 datapoints
2025-03-06 20:21:58,348 - INFO - training batch 301, loss: 0.848, 9632/60000 datapoints
2025-03-06 20:21:58,543 - INFO - training batch 351, loss: 0.280, 11232/60000 datapoints
2025-03-06 20:21:58,739 - INFO - training batch 401, loss: 0.139, 12832/60000 datapoints
2025-03-06 20:21:58,936 - INFO - training batch 451, loss: 0.086, 14432/60000 datapoints
2025-03-06 20:21:59,131 - INFO - training batch 501, loss: 0.051, 16032/60000 datapoints
2025-03-06 20:21:59,327 - INFO - training batch 551, loss: 0.175, 17632/60000 datapoints
2025-03-06 20:21:59,532 - INFO - training batch 601, loss: 0.610, 19232/60000 datapoints
2025-03-06 20:21:59,728 - INFO - training batch 651, loss: 0.132, 20832/60000 datapoints
2025-03-06 20:21:59,922 - INFO - training batch 701, loss: 0.431, 22432/60000 datapoints
2025-03-06 20:22:00,120 - INFO - training batch 751, loss: 0.272, 24032/60000 datapoints
2025-03-06 20:22:00,327 - INFO - training batch 801, loss: 0.153, 25632/60000 datapoints
2025-03-06 20:22:00,524 - INFO - training batch 851, loss: 0.235, 27232/60000 datapoints
2025-03-06 20:22:00,720 - INFO - training batch 901, loss: 0.214, 28832/60000 datapoints
2025-03-06 20:22:00,913 - INFO - training batch 951, loss: 0.262, 30432/60000 datapoints
2025-03-06 20:22:01,107 - INFO - training batch 1001, loss: 0.040, 32032/60000 datapoints
2025-03-06 20:22:01,301 - INFO - training batch 1051, loss: 0.265, 33632/60000 datapoints
2025-03-06 20:22:01,496 - INFO - training batch 1101, loss: 0.299, 35232/60000 datapoints
2025-03-06 20:22:01,695 - INFO - training batch 1151, loss: 0.186, 36832/60000 datapoints
2025-03-06 20:22:01,893 - INFO - training batch 1201, loss: 0.094, 38432/60000 datapoints
2025-03-06 20:22:02,092 - INFO - training batch 1251, loss: 0.395, 40032/60000 datapoints
2025-03-06 20:22:02,297 - INFO - training batch 1301, loss: 0.211, 41632/60000 datapoints
2025-03-06 20:22:02,499 - INFO - training batch 1351, loss: 0.194, 43232/60000 datapoints
2025-03-06 20:22:02,701 - INFO - training batch 1401, loss: 0.065, 44832/60000 datapoints
2025-03-06 20:22:02,902 - INFO - training batch 1451, loss: 0.190, 46432/60000 datapoints
2025-03-06 20:22:03,105 - INFO - training batch 1501, loss: 0.195, 48032/60000 datapoints
2025-03-06 20:22:03,309 - INFO - training batch 1551, loss: 0.263, 49632/60000 datapoints
2025-03-06 20:22:03,510 - INFO - training batch 1601, loss: 0.212, 51232/60000 datapoints
2025-03-06 20:22:03,724 - INFO - training batch 1651, loss: 0.115, 52832/60000 datapoints
2025-03-06 20:22:03,933 - INFO - training batch 1701, loss: 0.269, 54432/60000 datapoints
2025-03-06 20:22:04,142 - INFO - training batch 1751, loss: 0.403, 56032/60000 datapoints
2025-03-06 20:22:04,360 - INFO - training batch 1801, loss: 0.149, 57632/60000 datapoints
2025-03-06 20:22:04,568 - INFO - training batch 1851, loss: 0.112, 59232/60000 datapoints
2025-03-06 20:22:04,682 - INFO - validation batch 1, loss: 0.420, 32/10016 datapoints
2025-03-06 20:22:04,849 - INFO - validation batch 51, loss: 0.254, 1632/10016 datapoints
2025-03-06 20:22:05,025 - INFO - validation batch 101, loss: 0.157, 3232/10016 datapoints
2025-03-06 20:22:05,194 - INFO - validation batch 151, loss: 0.234, 4832/10016 datapoints
2025-03-06 20:22:05,369 - INFO - validation batch 201, loss: 0.318, 6432/10016 datapoints
2025-03-06 20:22:05,563 - INFO - validation batch 251, loss: 0.601, 8032/10016 datapoints
2025-03-06 20:22:05,731 - INFO - validation batch 301, loss: 0.105, 9632/10016 datapoints
2025-03-06 20:22:05,771 - INFO - Epoch 690/800 done.
2025-03-06 20:22:05,771 - INFO - Final validation performance:
Loss: 0.298, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:22:05,772 - INFO - Beginning epoch 691/800
2025-03-06 20:22:05,781 - INFO - training batch 1, loss: 0.153, 32/60000 datapoints
2025-03-06 20:22:06,004 - INFO - training batch 51, loss: 0.339, 1632/60000 datapoints
2025-03-06 20:22:06,205 - INFO - training batch 101, loss: 0.232, 3232/60000 datapoints
2025-03-06 20:22:06,415 - INFO - training batch 151, loss: 0.150, 4832/60000 datapoints
2025-03-06 20:22:06,620 - INFO - training batch 201, loss: 0.114, 6432/60000 datapoints
2025-03-06 20:22:06,827 - INFO - training batch 251, loss: 0.280, 8032/60000 datapoints
2025-03-06 20:22:07,027 - INFO - training batch 301, loss: 0.161, 9632/60000 datapoints
2025-03-06 20:22:07,227 - INFO - training batch 351, loss: 0.244, 11232/60000 datapoints
2025-03-06 20:22:07,427 - INFO - training batch 401, loss: 0.266, 12832/60000 datapoints
2025-03-06 20:22:07,625 - INFO - training batch 451, loss: 0.349, 14432/60000 datapoints
2025-03-06 20:22:07,829 - INFO - training batch 501, loss: 0.207, 16032/60000 datapoints
2025-03-06 20:22:08,029 - INFO - training batch 551, loss: 0.286, 17632/60000 datapoints
2025-03-06 20:22:08,231 - INFO - training batch 601, loss: 0.154, 19232/60000 datapoints
2025-03-06 20:22:08,434 - INFO - training batch 651, loss: 0.275, 20832/60000 datapoints
2025-03-06 20:22:08,640 - INFO - training batch 701, loss: 0.217, 22432/60000 datapoints
2025-03-06 20:22:08,840 - INFO - training batch 751, loss: 0.268, 24032/60000 datapoints
2025-03-06 20:22:09,044 - INFO - training batch 801, loss: 0.173, 25632/60000 datapoints
2025-03-06 20:22:09,243 - INFO - training batch 851, loss: 0.299, 27232/60000 datapoints
2025-03-06 20:22:09,449 - INFO - training batch 901, loss: 0.154, 28832/60000 datapoints
2025-03-06 20:22:09,653 - INFO - training batch 951, loss: 0.291, 30432/60000 datapoints
2025-03-06 20:22:09,853 - INFO - training batch 1001, loss: 0.150, 32032/60000 datapoints
2025-03-06 20:22:10,055 - INFO - training batch 1051, loss: 0.064, 33632/60000 datapoints
2025-03-06 20:22:10,255 - INFO - training batch 1101, loss: 0.127, 35232/60000 datapoints
2025-03-06 20:22:10,459 - INFO - training batch 1151, loss: 0.309, 36832/60000 datapoints
2025-03-06 20:22:10,662 - INFO - training batch 1201, loss: 0.090, 38432/60000 datapoints
2025-03-06 20:22:10,859 - INFO - training batch 1251, loss: 0.190, 40032/60000 datapoints
2025-03-06 20:22:11,059 - INFO - training batch 1301, loss: 0.395, 41632/60000 datapoints
2025-03-06 20:22:11,255 - INFO - training batch 1351, loss: 0.330, 43232/60000 datapoints
2025-03-06 20:22:11,454 - INFO - training batch 1401, loss: 0.415, 44832/60000 datapoints
2025-03-06 20:22:11,660 - INFO - training batch 1451, loss: 0.154, 46432/60000 datapoints
2025-03-06 20:22:11,864 - INFO - training batch 1501, loss: 0.167, 48032/60000 datapoints
2025-03-06 20:22:12,067 - INFO - training batch 1551, loss: 0.239, 49632/60000 datapoints
2025-03-06 20:22:12,262 - INFO - training batch 1601, loss: 0.085, 51232/60000 datapoints
2025-03-06 20:22:12,467 - INFO - training batch 1651, loss: 0.107, 52832/60000 datapoints
2025-03-06 20:22:12,667 - INFO - training batch 1701, loss: 0.131, 54432/60000 datapoints
2025-03-06 20:22:12,866 - INFO - training batch 1751, loss: 0.172, 56032/60000 datapoints
2025-03-06 20:22:13,067 - INFO - training batch 1801, loss: 0.108, 57632/60000 datapoints
2025-03-06 20:22:13,287 - INFO - training batch 1851, loss: 0.479, 59232/60000 datapoints
2025-03-06 20:22:13,392 - INFO - validation batch 1, loss: 0.237, 32/10016 datapoints
2025-03-06 20:22:13,553 - INFO - validation batch 51, loss: 0.254, 1632/10016 datapoints
2025-03-06 20:22:13,719 - INFO - validation batch 101, loss: 0.156, 3232/10016 datapoints
2025-03-06 20:22:13,880 - INFO - validation batch 151, loss: 0.340, 4832/10016 datapoints
2025-03-06 20:22:14,043 - INFO - validation batch 201, loss: 0.103, 6432/10016 datapoints
2025-03-06 20:22:14,206 - INFO - validation batch 251, loss: 0.052, 8032/10016 datapoints
2025-03-06 20:22:14,366 - INFO - validation batch 301, loss: 0.108, 9632/10016 datapoints
2025-03-06 20:22:14,404 - INFO - Epoch 691/800 done.
2025-03-06 20:22:14,404 - INFO - Final validation performance:
Loss: 0.179, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:22:14,405 - INFO - Beginning epoch 692/800
2025-03-06 20:22:14,412 - INFO - training batch 1, loss: 0.159, 32/60000 datapoints
2025-03-06 20:22:14,644 - INFO - training batch 51, loss: 0.083, 1632/60000 datapoints
2025-03-06 20:22:14,840 - INFO - training batch 101, loss: 0.214, 3232/60000 datapoints
2025-03-06 20:22:15,053 - INFO - training batch 151, loss: 0.103, 4832/60000 datapoints
2025-03-06 20:22:15,258 - INFO - training batch 201, loss: 0.117, 6432/60000 datapoints
2025-03-06 20:22:15,465 - INFO - training batch 251, loss: 0.223, 8032/60000 datapoints
2025-03-06 20:22:15,689 - INFO - training batch 301, loss: 0.108, 9632/60000 datapoints
2025-03-06 20:22:15,885 - INFO - training batch 351, loss: 0.185, 11232/60000 datapoints
2025-03-06 20:22:16,080 - INFO - training batch 401, loss: 0.489, 12832/60000 datapoints
2025-03-06 20:22:16,272 - INFO - training batch 451, loss: 0.140, 14432/60000 datapoints
2025-03-06 20:22:16,473 - INFO - training batch 501, loss: 0.333, 16032/60000 datapoints
2025-03-06 20:22:16,672 - INFO - training batch 551, loss: 0.257, 17632/60000 datapoints
2025-03-06 20:22:16,868 - INFO - training batch 601, loss: 0.161, 19232/60000 datapoints
2025-03-06 20:22:17,066 - INFO - training batch 651, loss: 0.065, 20832/60000 datapoints
2025-03-06 20:22:17,262 - INFO - training batch 701, loss: 0.467, 22432/60000 datapoints
2025-03-06 20:22:17,458 - INFO - training batch 751, loss: 0.257, 24032/60000 datapoints
2025-03-06 20:22:17,658 - INFO - training batch 801, loss: 0.201, 25632/60000 datapoints
2025-03-06 20:22:17,851 - INFO - training batch 851, loss: 0.134, 27232/60000 datapoints
2025-03-06 20:22:18,043 - INFO - training batch 901, loss: 0.168, 28832/60000 datapoints
2025-03-06 20:22:18,243 - INFO - training batch 951, loss: 0.244, 30432/60000 datapoints
2025-03-06 20:22:18,439 - INFO - training batch 1001, loss: 0.194, 32032/60000 datapoints
2025-03-06 20:22:18,636 - INFO - training batch 1051, loss: 0.217, 33632/60000 datapoints
2025-03-06 20:22:18,828 - INFO - training batch 1101, loss: 0.407, 35232/60000 datapoints
2025-03-06 20:22:19,022 - INFO - training batch 1151, loss: 0.190, 36832/60000 datapoints
2025-03-06 20:22:19,219 - INFO - training batch 1201, loss: 0.484, 38432/60000 datapoints
2025-03-06 20:22:19,414 - INFO - training batch 1251, loss: 0.433, 40032/60000 datapoints
2025-03-06 20:22:19,606 - INFO - training batch 1301, loss: 0.087, 41632/60000 datapoints
2025-03-06 20:22:19,805 - INFO - training batch 1351, loss: 0.262, 43232/60000 datapoints
2025-03-06 20:22:19,998 - INFO - training batch 1401, loss: 0.188, 44832/60000 datapoints
2025-03-06 20:22:20,195 - INFO - training batch 1451, loss: 0.212, 46432/60000 datapoints
2025-03-06 20:22:20,392 - INFO - training batch 1501, loss: 0.078, 48032/60000 datapoints
2025-03-06 20:22:20,585 - INFO - training batch 1551, loss: 0.242, 49632/60000 datapoints
2025-03-06 20:22:20,781 - INFO - training batch 1601, loss: 0.130, 51232/60000 datapoints
2025-03-06 20:22:20,974 - INFO - training batch 1651, loss: 0.168, 52832/60000 datapoints
2025-03-06 20:22:21,170 - INFO - training batch 1701, loss: 0.251, 54432/60000 datapoints
2025-03-06 20:22:21,362 - INFO - training batch 1751, loss: 0.292, 56032/60000 datapoints
2025-03-06 20:22:21,559 - INFO - training batch 1801, loss: 0.092, 57632/60000 datapoints
2025-03-06 20:22:21,755 - INFO - training batch 1851, loss: 0.174, 59232/60000 datapoints
2025-03-06 20:22:21,857 - INFO - validation batch 1, loss: 0.177, 32/10016 datapoints
2025-03-06 20:22:22,013 - INFO - validation batch 51, loss: 0.151, 1632/10016 datapoints
2025-03-06 20:22:22,171 - INFO - validation batch 101, loss: 0.358, 3232/10016 datapoints
2025-03-06 20:22:22,331 - INFO - validation batch 151, loss: 0.225, 4832/10016 datapoints
2025-03-06 20:22:22,490 - INFO - validation batch 201, loss: 0.619, 6432/10016 datapoints
2025-03-06 20:22:22,650 - INFO - validation batch 251, loss: 0.322, 8032/10016 datapoints
2025-03-06 20:22:22,807 - INFO - validation batch 301, loss: 0.131, 9632/10016 datapoints
2025-03-06 20:22:22,846 - INFO - Epoch 692/800 done.
2025-03-06 20:22:22,847 - INFO - Final validation performance:
Loss: 0.284, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:22:22,847 - INFO - Beginning epoch 693/800
2025-03-06 20:22:22,854 - INFO - training batch 1, loss: 0.279, 32/60000 datapoints
2025-03-06 20:22:23,051 - INFO - training batch 51, loss: 0.190, 1632/60000 datapoints
2025-03-06 20:22:23,249 - INFO - training batch 101, loss: 0.174, 3232/60000 datapoints
2025-03-06 20:22:23,457 - INFO - training batch 151, loss: 0.186, 4832/60000 datapoints
2025-03-06 20:22:23,673 - INFO - training batch 201, loss: 0.274, 6432/60000 datapoints
2025-03-06 20:22:23,876 - INFO - training batch 251, loss: 0.244, 8032/60000 datapoints
2025-03-06 20:22:24,084 - INFO - training batch 301, loss: 0.251, 9632/60000 datapoints
2025-03-06 20:22:24,291 - INFO - training batch 351, loss: 0.119, 11232/60000 datapoints
2025-03-06 20:22:24,501 - INFO - training batch 401, loss: 0.128, 12832/60000 datapoints
2025-03-06 20:22:24,710 - INFO - training batch 451, loss: 0.086, 14432/60000 datapoints
2025-03-06 20:22:24,910 - INFO - training batch 501, loss: 0.304, 16032/60000 datapoints
2025-03-06 20:22:25,112 - INFO - training batch 551, loss: 0.123, 17632/60000 datapoints
2025-03-06 20:22:25,321 - INFO - training batch 601, loss: 0.109, 19232/60000 datapoints
2025-03-06 20:22:25,521 - INFO - training batch 651, loss: 0.069, 20832/60000 datapoints
2025-03-06 20:22:25,753 - INFO - training batch 701, loss: 0.101, 22432/60000 datapoints
2025-03-06 20:22:25,952 - INFO - training batch 751, loss: 0.441, 24032/60000 datapoints
2025-03-06 20:22:26,154 - INFO - training batch 801, loss: 0.205, 25632/60000 datapoints
2025-03-06 20:22:26,357 - INFO - training batch 851, loss: 0.346, 27232/60000 datapoints
2025-03-06 20:22:26,558 - INFO - training batch 901, loss: 0.139, 28832/60000 datapoints
2025-03-06 20:22:26,772 - INFO - training batch 951, loss: 0.313, 30432/60000 datapoints
2025-03-06 20:22:26,973 - INFO - training batch 1001, loss: 0.273, 32032/60000 datapoints
2025-03-06 20:22:27,174 - INFO - training batch 1051, loss: 0.199, 33632/60000 datapoints
2025-03-06 20:22:27,378 - INFO - training batch 1101, loss: 0.283, 35232/60000 datapoints
2025-03-06 20:22:27,582 - INFO - training batch 1151, loss: 0.313, 36832/60000 datapoints
2025-03-06 20:22:27,789 - INFO - training batch 1201, loss: 0.168, 38432/60000 datapoints
2025-03-06 20:22:27,989 - INFO - training batch 1251, loss: 0.322, 40032/60000 datapoints
2025-03-06 20:22:28,187 - INFO - training batch 1301, loss: 0.409, 41632/60000 datapoints
2025-03-06 20:22:28,391 - INFO - training batch 1351, loss: 0.511, 43232/60000 datapoints
2025-03-06 20:22:28,591 - INFO - training batch 1401, loss: 0.072, 44832/60000 datapoints
2025-03-06 20:22:28,795 - INFO - training batch 1451, loss: 0.579, 46432/60000 datapoints
2025-03-06 20:22:29,004 - INFO - training batch 1501, loss: 0.226, 48032/60000 datapoints
2025-03-06 20:22:29,212 - INFO - training batch 1551, loss: 0.326, 49632/60000 datapoints
2025-03-06 20:22:29,415 - INFO - training batch 1601, loss: 0.135, 51232/60000 datapoints
2025-03-06 20:22:29,633 - INFO - training batch 1651, loss: 0.268, 52832/60000 datapoints
2025-03-06 20:22:29,838 - INFO - training batch 1701, loss: 0.149, 54432/60000 datapoints
2025-03-06 20:22:30,038 - INFO - training batch 1751, loss: 0.108, 56032/60000 datapoints
2025-03-06 20:22:30,241 - INFO - training batch 1801, loss: 0.079, 57632/60000 datapoints
2025-03-06 20:22:30,444 - INFO - training batch 1851, loss: 0.137, 59232/60000 datapoints
2025-03-06 20:22:30,552 - INFO - validation batch 1, loss: 0.082, 32/10016 datapoints
2025-03-06 20:22:30,717 - INFO - validation batch 51, loss: 0.039, 1632/10016 datapoints
2025-03-06 20:22:30,886 - INFO - validation batch 101, loss: 0.250, 3232/10016 datapoints
2025-03-06 20:22:31,054 - INFO - validation batch 151, loss: 0.155, 4832/10016 datapoints
2025-03-06 20:22:31,223 - INFO - validation batch 201, loss: 0.484, 6432/10016 datapoints
2025-03-06 20:22:31,393 - INFO - validation batch 251, loss: 0.293, 8032/10016 datapoints
2025-03-06 20:22:31,560 - INFO - validation batch 301, loss: 0.089, 9632/10016 datapoints
2025-03-06 20:22:31,604 - INFO - Epoch 693/800 done.
2025-03-06 20:22:31,604 - INFO - Final validation performance:
Loss: 0.199, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:22:31,605 - INFO - Beginning epoch 694/800
2025-03-06 20:22:31,612 - INFO - training batch 1, loss: 0.322, 32/60000 datapoints
2025-03-06 20:22:31,838 - INFO - training batch 51, loss: 0.470, 1632/60000 datapoints
2025-03-06 20:22:32,045 - INFO - training batch 101, loss: 0.299, 3232/60000 datapoints
2025-03-06 20:22:32,264 - INFO - training batch 151, loss: 0.077, 4832/60000 datapoints
2025-03-06 20:22:32,479 - INFO - training batch 201, loss: 0.059, 6432/60000 datapoints
2025-03-06 20:22:32,692 - INFO - training batch 251, loss: 0.132, 8032/60000 datapoints
2025-03-06 20:22:32,898 - INFO - training batch 301, loss: 0.112, 9632/60000 datapoints
2025-03-06 20:22:33,107 - INFO - training batch 351, loss: 0.143, 11232/60000 datapoints
2025-03-06 20:22:33,317 - INFO - training batch 401, loss: 0.303, 12832/60000 datapoints
2025-03-06 20:22:33,523 - INFO - training batch 451, loss: 0.101, 14432/60000 datapoints
2025-03-06 20:22:33,734 - INFO - training batch 501, loss: 0.291, 16032/60000 datapoints
2025-03-06 20:22:33,942 - INFO - training batch 551, loss: 0.130, 17632/60000 datapoints
2025-03-06 20:22:34,151 - INFO - training batch 601, loss: 0.285, 19232/60000 datapoints
2025-03-06 20:22:34,359 - INFO - training batch 651, loss: 0.330, 20832/60000 datapoints
2025-03-06 20:22:34,572 - INFO - training batch 701, loss: 0.447, 22432/60000 datapoints
2025-03-06 20:22:34,779 - INFO - training batch 751, loss: 0.067, 24032/60000 datapoints
2025-03-06 20:22:34,990 - INFO - training batch 801, loss: 0.208, 25632/60000 datapoints
2025-03-06 20:22:35,198 - INFO - training batch 851, loss: 0.172, 27232/60000 datapoints
2025-03-06 20:22:35,413 - INFO - training batch 901, loss: 0.082, 28832/60000 datapoints
2025-03-06 20:22:35,617 - INFO - training batch 951, loss: 0.042, 30432/60000 datapoints
2025-03-06 20:22:35,851 - INFO - training batch 1001, loss: 0.190, 32032/60000 datapoints
2025-03-06 20:22:36,058 - INFO - training batch 1051, loss: 0.212, 33632/60000 datapoints
2025-03-06 20:22:36,263 - INFO - training batch 1101, loss: 0.233, 35232/60000 datapoints
2025-03-06 20:22:36,475 - INFO - training batch 1151, loss: 0.445, 36832/60000 datapoints
2025-03-06 20:22:36,685 - INFO - training batch 1201, loss: 0.318, 38432/60000 datapoints
2025-03-06 20:22:36,893 - INFO - training batch 1251, loss: 0.311, 40032/60000 datapoints
2025-03-06 20:22:37,100 - INFO - training batch 1301, loss: 0.431, 41632/60000 datapoints
2025-03-06 20:22:37,308 - INFO - training batch 1351, loss: 0.061, 43232/60000 datapoints
2025-03-06 20:22:37,515 - INFO - training batch 1401, loss: 0.275, 44832/60000 datapoints
2025-03-06 20:22:37,738 - INFO - training batch 1451, loss: 0.327, 46432/60000 datapoints
2025-03-06 20:22:37,945 - INFO - training batch 1501, loss: 0.125, 48032/60000 datapoints
2025-03-06 20:22:38,151 - INFO - training batch 1551, loss: 0.414, 49632/60000 datapoints
2025-03-06 20:22:38,365 - INFO - training batch 1601, loss: 0.250, 51232/60000 datapoints
2025-03-06 20:22:38,572 - INFO - training batch 1651, loss: 0.166, 52832/60000 datapoints
2025-03-06 20:22:38,778 - INFO - training batch 1701, loss: 0.179, 54432/60000 datapoints
2025-03-06 20:22:38,988 - INFO - training batch 1751, loss: 0.219, 56032/60000 datapoints
2025-03-06 20:22:39,199 - INFO - training batch 1801, loss: 0.356, 57632/60000 datapoints
2025-03-06 20:22:39,450 - INFO - training batch 1851, loss: 0.288, 59232/60000 datapoints
2025-03-06 20:22:39,565 - INFO - validation batch 1, loss: 0.079, 32/10016 datapoints
2025-03-06 20:22:39,734 - INFO - validation batch 51, loss: 0.403, 1632/10016 datapoints
2025-03-06 20:22:39,902 - INFO - validation batch 101, loss: 0.215, 3232/10016 datapoints
2025-03-06 20:22:40,073 - INFO - validation batch 151, loss: 0.106, 4832/10016 datapoints
2025-03-06 20:22:40,240 - INFO - validation batch 201, loss: 0.173, 6432/10016 datapoints
2025-03-06 20:22:40,411 - INFO - validation batch 251, loss: 0.199, 8032/10016 datapoints
2025-03-06 20:22:40,581 - INFO - validation batch 301, loss: 0.269, 9632/10016 datapoints
2025-03-06 20:22:40,621 - INFO - Epoch 694/800 done.
2025-03-06 20:22:40,621 - INFO - Final validation performance:
Loss: 0.206, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:22:40,622 - INFO - Beginning epoch 695/800
2025-03-06 20:22:40,635 - INFO - training batch 1, loss: 0.281, 32/60000 datapoints
2025-03-06 20:22:40,838 - INFO - training batch 51, loss: 0.239, 1632/60000 datapoints
2025-03-06 20:22:41,047 - INFO - training batch 101, loss: 0.212, 3232/60000 datapoints
2025-03-06 20:22:41,263 - INFO - training batch 151, loss: 0.194, 4832/60000 datapoints
2025-03-06 20:22:41,469 - INFO - training batch 201, loss: 0.090, 6432/60000 datapoints
2025-03-06 20:22:41,681 - INFO - training batch 251, loss: 0.216, 8032/60000 datapoints
2025-03-06 20:22:41,887 - INFO - training batch 301, loss: 0.189, 9632/60000 datapoints
2025-03-06 20:22:42,094 - INFO - training batch 351, loss: 0.083, 11232/60000 datapoints
2025-03-06 20:22:42,295 - INFO - training batch 401, loss: 0.089, 12832/60000 datapoints
2025-03-06 20:22:42,499 - INFO - training batch 451, loss: 0.428, 14432/60000 datapoints
2025-03-06 20:22:42,702 - INFO - training batch 501, loss: 0.260, 16032/60000 datapoints
2025-03-06 20:22:42,902 - INFO - training batch 551, loss: 0.175, 17632/60000 datapoints
2025-03-06 20:22:43,101 - INFO - training batch 601, loss: 0.215, 19232/60000 datapoints
2025-03-06 20:22:43,308 - INFO - training batch 651, loss: 0.139, 20832/60000 datapoints
2025-03-06 20:22:43,508 - INFO - training batch 701, loss: 0.118, 22432/60000 datapoints
2025-03-06 20:22:43,728 - INFO - training batch 751, loss: 0.304, 24032/60000 datapoints
2025-03-06 20:22:43,934 - INFO - training batch 801, loss: 0.209, 25632/60000 datapoints
2025-03-06 20:22:44,143 - INFO - training batch 851, loss: 0.432, 27232/60000 datapoints
2025-03-06 20:22:44,349 - INFO - training batch 901, loss: 0.181, 28832/60000 datapoints
2025-03-06 20:22:44,559 - INFO - training batch 951, loss: 0.206, 30432/60000 datapoints
2025-03-06 20:22:44,770 - INFO - training batch 1001, loss: 0.129, 32032/60000 datapoints
2025-03-06 20:22:44,987 - INFO - training batch 1051, loss: 0.115, 33632/60000 datapoints
2025-03-06 20:22:45,192 - INFO - training batch 1101, loss: 0.163, 35232/60000 datapoints
2025-03-06 20:22:45,405 - INFO - training batch 1151, loss: 0.310, 36832/60000 datapoints
2025-03-06 20:22:45,612 - INFO - training batch 1201, loss: 0.175, 38432/60000 datapoints
2025-03-06 20:22:45,839 - INFO - training batch 1251, loss: 0.257, 40032/60000 datapoints
2025-03-06 20:22:46,057 - INFO - training batch 1301, loss: 0.179, 41632/60000 datapoints
2025-03-06 20:22:46,259 - INFO - training batch 1351, loss: 0.159, 43232/60000 datapoints
2025-03-06 20:22:46,465 - INFO - training batch 1401, loss: 0.473, 44832/60000 datapoints
2025-03-06 20:22:46,676 - INFO - training batch 1451, loss: 0.043, 46432/60000 datapoints
2025-03-06 20:22:46,877 - INFO - training batch 1501, loss: 0.255, 48032/60000 datapoints
2025-03-06 20:22:47,078 - INFO - training batch 1551, loss: 0.291, 49632/60000 datapoints
2025-03-06 20:22:47,278 - INFO - training batch 1601, loss: 0.174, 51232/60000 datapoints
2025-03-06 20:22:47,485 - INFO - training batch 1651, loss: 0.128, 52832/60000 datapoints
2025-03-06 20:22:47,690 - INFO - training batch 1701, loss: 0.235, 54432/60000 datapoints
2025-03-06 20:22:47,889 - INFO - training batch 1751, loss: 0.135, 56032/60000 datapoints
2025-03-06 20:22:48,094 - INFO - training batch 1801, loss: 0.151, 57632/60000 datapoints
2025-03-06 20:22:48,295 - INFO - training batch 1851, loss: 0.299, 59232/60000 datapoints
2025-03-06 20:22:48,404 - INFO - validation batch 1, loss: 0.194, 32/10016 datapoints
2025-03-06 20:22:48,571 - INFO - validation batch 51, loss: 0.162, 1632/10016 datapoints
2025-03-06 20:22:48,737 - INFO - validation batch 101, loss: 0.236, 3232/10016 datapoints
2025-03-06 20:22:48,902 - INFO - validation batch 151, loss: 0.179, 4832/10016 datapoints
2025-03-06 20:22:49,063 - INFO - validation batch 201, loss: 0.138, 6432/10016 datapoints
2025-03-06 20:22:49,227 - INFO - validation batch 251, loss: 0.102, 8032/10016 datapoints
2025-03-06 20:22:49,392 - INFO - validation batch 301, loss: 0.408, 9632/10016 datapoints
2025-03-06 20:22:49,437 - INFO - Epoch 695/800 done.
2025-03-06 20:22:49,438 - INFO - Final validation performance:
Loss: 0.203, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:22:49,438 - INFO - Beginning epoch 696/800
2025-03-06 20:22:49,445 - INFO - training batch 1, loss: 0.099, 32/60000 datapoints
2025-03-06 20:22:49,664 - INFO - training batch 51, loss: 0.162, 1632/60000 datapoints
2025-03-06 20:22:49,865 - INFO - training batch 101, loss: 0.399, 3232/60000 datapoints
2025-03-06 20:22:50,079 - INFO - training batch 151, loss: 0.308, 4832/60000 datapoints
2025-03-06 20:22:50,281 - INFO - training batch 201, loss: 0.102, 6432/60000 datapoints
2025-03-06 20:22:50,492 - INFO - training batch 251, loss: 0.264, 8032/60000 datapoints
2025-03-06 20:22:50,693 - INFO - training batch 301, loss: 0.075, 9632/60000 datapoints
2025-03-06 20:22:50,894 - INFO - training batch 351, loss: 0.089, 11232/60000 datapoints
2025-03-06 20:22:51,095 - INFO - training batch 401, loss: 0.090, 12832/60000 datapoints
2025-03-06 20:22:51,297 - INFO - training batch 451, loss: 0.180, 14432/60000 datapoints
2025-03-06 20:22:51,500 - INFO - training batch 501, loss: 0.382, 16032/60000 datapoints
2025-03-06 20:22:51,702 - INFO - training batch 551, loss: 0.195, 17632/60000 datapoints
2025-03-06 20:22:51,905 - INFO - training batch 601, loss: 0.120, 19232/60000 datapoints
2025-03-06 20:22:52,109 - INFO - training batch 651, loss: 0.203, 20832/60000 datapoints
2025-03-06 20:22:52,311 - INFO - training batch 701, loss: 0.102, 22432/60000 datapoints
2025-03-06 20:22:52,515 - INFO - training batch 751, loss: 0.303, 24032/60000 datapoints
2025-03-06 20:22:52,717 - INFO - training batch 801, loss: 0.128, 25632/60000 datapoints
2025-03-06 20:22:52,935 - INFO - training batch 851, loss: 0.080, 27232/60000 datapoints
2025-03-06 20:22:53,178 - INFO - training batch 901, loss: 0.216, 28832/60000 datapoints
2025-03-06 20:22:53,414 - INFO - training batch 951, loss: 0.203, 30432/60000 datapoints
2025-03-06 20:22:53,639 - INFO - training batch 1001, loss: 0.135, 32032/60000 datapoints
2025-03-06 20:22:53,839 - INFO - training batch 1051, loss: 0.095, 33632/60000 datapoints
2025-03-06 20:22:54,040 - INFO - training batch 1101, loss: 0.358, 35232/60000 datapoints
2025-03-06 20:22:54,242 - INFO - training batch 1151, loss: 0.204, 36832/60000 datapoints
2025-03-06 20:22:54,447 - INFO - training batch 1201, loss: 0.270, 38432/60000 datapoints
2025-03-06 20:22:54,651 - INFO - training batch 1251, loss: 0.296, 40032/60000 datapoints
2025-03-06 20:22:54,857 - INFO - training batch 1301, loss: 0.056, 41632/60000 datapoints
2025-03-06 20:22:55,064 - INFO - training batch 1351, loss: 0.148, 43232/60000 datapoints
2025-03-06 20:22:55,265 - INFO - training batch 1401, loss: 0.267, 44832/60000 datapoints
2025-03-06 20:22:55,471 - INFO - training batch 1451, loss: 0.129, 46432/60000 datapoints
2025-03-06 20:22:55,676 - INFO - training batch 1501, loss: 0.211, 48032/60000 datapoints
2025-03-06 20:22:55,881 - INFO - training batch 1551, loss: 0.272, 49632/60000 datapoints
2025-03-06 20:22:56,106 - INFO - training batch 1601, loss: 0.122, 51232/60000 datapoints
2025-03-06 20:22:56,317 - INFO - training batch 1651, loss: 0.149, 52832/60000 datapoints
2025-03-06 20:22:56,531 - INFO - training batch 1701, loss: 0.333, 54432/60000 datapoints
2025-03-06 20:22:56,742 - INFO - training batch 1751, loss: 0.275, 56032/60000 datapoints
2025-03-06 20:22:56,951 - INFO - training batch 1801, loss: 0.149, 57632/60000 datapoints
2025-03-06 20:22:57,159 - INFO - training batch 1851, loss: 0.458, 59232/60000 datapoints
2025-03-06 20:22:57,269 - INFO - validation batch 1, loss: 0.132, 32/10016 datapoints
2025-03-06 20:22:57,437 - INFO - validation batch 51, loss: 0.199, 1632/10016 datapoints
2025-03-06 20:22:57,612 - INFO - validation batch 101, loss: 0.425, 3232/10016 datapoints
2025-03-06 20:22:57,786 - INFO - validation batch 151, loss: 0.085, 4832/10016 datapoints
2025-03-06 20:22:57,952 - INFO - validation batch 201, loss: 0.175, 6432/10016 datapoints
2025-03-06 20:22:58,119 - INFO - validation batch 251, loss: 0.290, 8032/10016 datapoints
2025-03-06 20:22:58,285 - INFO - validation batch 301, loss: 0.183, 9632/10016 datapoints
2025-03-06 20:22:58,330 - INFO - Epoch 696/800 done.
2025-03-06 20:22:58,331 - INFO - Final validation performance:
Loss: 0.213, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:22:58,332 - INFO - Beginning epoch 697/800
2025-03-06 20:22:58,339 - INFO - training batch 1, loss: 0.376, 32/60000 datapoints
2025-03-06 20:22:58,574 - INFO - training batch 51, loss: 0.331, 1632/60000 datapoints
2025-03-06 20:22:58,786 - INFO - training batch 101, loss: 0.365, 3232/60000 datapoints
2025-03-06 20:22:59,002 - INFO - training batch 151, loss: 0.195, 4832/60000 datapoints
2025-03-06 20:22:59,214 - INFO - training batch 201, loss: 0.135, 6432/60000 datapoints
2025-03-06 20:22:59,426 - INFO - training batch 251, loss: 0.203, 8032/60000 datapoints
2025-03-06 20:22:59,640 - INFO - training batch 301, loss: 0.627, 9632/60000 datapoints
2025-03-06 20:22:59,867 - INFO - training batch 351, loss: 0.090, 11232/60000 datapoints
2025-03-06 20:23:00,078 - INFO - training batch 401, loss: 0.102, 12832/60000 datapoints
2025-03-06 20:23:00,287 - INFO - training batch 451, loss: 0.081, 14432/60000 datapoints
2025-03-06 20:23:00,499 - INFO - training batch 501, loss: 0.234, 16032/60000 datapoints
2025-03-06 20:23:00,711 - INFO - training batch 551, loss: 0.149, 17632/60000 datapoints
2025-03-06 20:23:00,917 - INFO - training batch 601, loss: 0.234, 19232/60000 datapoints
2025-03-06 20:23:01,124 - INFO - training batch 651, loss: 0.060, 20832/60000 datapoints
2025-03-06 20:23:01,336 - INFO - training batch 701, loss: 0.291, 22432/60000 datapoints
2025-03-06 20:23:01,544 - INFO - training batch 751, loss: 0.158, 24032/60000 datapoints
2025-03-06 20:23:01,752 - INFO - training batch 801, loss: 0.370, 25632/60000 datapoints
2025-03-06 20:23:01,961 - INFO - training batch 851, loss: 0.195, 27232/60000 datapoints
2025-03-06 20:23:02,167 - INFO - training batch 901, loss: 0.055, 28832/60000 datapoints
2025-03-06 20:23:02,375 - INFO - training batch 951, loss: 0.647, 30432/60000 datapoints
2025-03-06 20:23:02,584 - INFO - training batch 1001, loss: 0.173, 32032/60000 datapoints
2025-03-06 20:23:02,795 - INFO - training batch 1051, loss: 0.297, 33632/60000 datapoints
2025-03-06 20:23:02,999 - INFO - training batch 1101, loss: 0.119, 35232/60000 datapoints
2025-03-06 20:23:03,205 - INFO - training batch 1151, loss: 0.074, 36832/60000 datapoints
2025-03-06 20:23:03,414 - INFO - training batch 1201, loss: 0.092, 38432/60000 datapoints
2025-03-06 20:23:03,620 - INFO - training batch 1251, loss: 0.338, 40032/60000 datapoints
2025-03-06 20:23:03,829 - INFO - training batch 1301, loss: 0.426, 41632/60000 datapoints
2025-03-06 20:23:04,036 - INFO - training batch 1351, loss: 0.092, 43232/60000 datapoints
2025-03-06 20:23:04,246 - INFO - training batch 1401, loss: 0.132, 44832/60000 datapoints
2025-03-06 20:23:04,452 - INFO - training batch 1451, loss: 0.097, 46432/60000 datapoints
2025-03-06 20:23:04,665 - INFO - training batch 1501, loss: 0.190, 48032/60000 datapoints
2025-03-06 20:23:04,870 - INFO - training batch 1551, loss: 0.542, 49632/60000 datapoints
2025-03-06 20:23:05,080 - INFO - training batch 1601, loss: 0.237, 51232/60000 datapoints
2025-03-06 20:23:05,283 - INFO - training batch 1651, loss: 0.162, 52832/60000 datapoints
2025-03-06 20:23:05,498 - INFO - training batch 1701, loss: 0.244, 54432/60000 datapoints
2025-03-06 20:23:05,710 - INFO - training batch 1751, loss: 0.193, 56032/60000 datapoints
2025-03-06 20:23:05,916 - INFO - training batch 1801, loss: 0.098, 57632/60000 datapoints
2025-03-06 20:23:06,149 - INFO - training batch 1851, loss: 0.120, 59232/60000 datapoints
2025-03-06 20:23:06,259 - INFO - validation batch 1, loss: 0.042, 32/10016 datapoints
2025-03-06 20:23:06,430 - INFO - validation batch 51, loss: 0.262, 1632/10016 datapoints
2025-03-06 20:23:06,601 - INFO - validation batch 101, loss: 0.398, 3232/10016 datapoints
2025-03-06 20:23:06,780 - INFO - validation batch 151, loss: 0.062, 4832/10016 datapoints
2025-03-06 20:23:06,947 - INFO - validation batch 201, loss: 0.168, 6432/10016 datapoints
2025-03-06 20:23:07,117 - INFO - validation batch 251, loss: 0.225, 8032/10016 datapoints
2025-03-06 20:23:07,283 - INFO - validation batch 301, loss: 0.437, 9632/10016 datapoints
2025-03-06 20:23:07,324 - INFO - Epoch 697/800 done.
2025-03-06 20:23:07,324 - INFO - Final validation performance:
Loss: 0.228, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:23:07,324 - INFO - Beginning epoch 698/800
2025-03-06 20:23:07,334 - INFO - training batch 1, loss: 0.091, 32/60000 datapoints
2025-03-06 20:23:07,563 - INFO - training batch 51, loss: 0.133, 1632/60000 datapoints
2025-03-06 20:23:07,779 - INFO - training batch 101, loss: 0.181, 3232/60000 datapoints
2025-03-06 20:23:07,992 - INFO - training batch 151, loss: 0.188, 4832/60000 datapoints
2025-03-06 20:23:08,202 - INFO - training batch 201, loss: 0.168, 6432/60000 datapoints
2025-03-06 20:23:08,417 - INFO - training batch 251, loss: 0.159, 8032/60000 datapoints
2025-03-06 20:23:08,635 - INFO - training batch 301, loss: 0.315, 9632/60000 datapoints
2025-03-06 20:23:08,840 - INFO - training batch 351, loss: 0.058, 11232/60000 datapoints
2025-03-06 20:23:09,050 - INFO - training batch 401, loss: 0.146, 12832/60000 datapoints
2025-03-06 20:23:09,259 - INFO - training batch 451, loss: 0.135, 14432/60000 datapoints
2025-03-06 20:23:09,467 - INFO - training batch 501, loss: 0.205, 16032/60000 datapoints
2025-03-06 20:23:09,681 - INFO - training batch 551, loss: 0.083, 17632/60000 datapoints
2025-03-06 20:23:09,887 - INFO - training batch 601, loss: 0.242, 19232/60000 datapoints
2025-03-06 20:23:10,093 - INFO - training batch 651, loss: 0.507, 20832/60000 datapoints
2025-03-06 20:23:10,299 - INFO - training batch 701, loss: 0.318, 22432/60000 datapoints
2025-03-06 20:23:10,512 - INFO - training batch 751, loss: 0.117, 24032/60000 datapoints
2025-03-06 20:23:10,723 - INFO - training batch 801, loss: 0.241, 25632/60000 datapoints
2025-03-06 20:23:10,930 - INFO - training batch 851, loss: 0.097, 27232/60000 datapoints
2025-03-06 20:23:11,140 - INFO - training batch 901, loss: 0.201, 28832/60000 datapoints
2025-03-06 20:23:11,346 - INFO - training batch 951, loss: 0.059, 30432/60000 datapoints
2025-03-06 20:23:11,549 - INFO - training batch 1001, loss: 0.376, 32032/60000 datapoints
2025-03-06 20:23:11,756 - INFO - training batch 1051, loss: 0.040, 33632/60000 datapoints
2025-03-06 20:23:11,958 - INFO - training batch 1101, loss: 0.104, 35232/60000 datapoints
2025-03-06 20:23:12,160 - INFO - training batch 1151, loss: 0.347, 36832/60000 datapoints
2025-03-06 20:23:12,357 - INFO - training batch 1201, loss: 0.370, 38432/60000 datapoints
2025-03-06 20:23:12,560 - INFO - training batch 1251, loss: 0.318, 40032/60000 datapoints
2025-03-06 20:23:12,768 - INFO - training batch 1301, loss: 0.127, 41632/60000 datapoints
2025-03-06 20:23:12,967 - INFO - training batch 1351, loss: 0.241, 43232/60000 datapoints
2025-03-06 20:23:13,168 - INFO - training batch 1401, loss: 0.206, 44832/60000 datapoints
2025-03-06 20:23:13,372 - INFO - training batch 1451, loss: 0.226, 46432/60000 datapoints
2025-03-06 20:23:13,573 - INFO - training batch 1501, loss: 0.381, 48032/60000 datapoints
2025-03-06 20:23:13,783 - INFO - training batch 1551, loss: 0.205, 49632/60000 datapoints
2025-03-06 20:23:13,984 - INFO - training batch 1601, loss: 0.124, 51232/60000 datapoints
2025-03-06 20:23:14,198 - INFO - training batch 1651, loss: 0.122, 52832/60000 datapoints
2025-03-06 20:23:14,399 - INFO - training batch 1701, loss: 0.253, 54432/60000 datapoints
2025-03-06 20:23:14,605 - INFO - training batch 1751, loss: 0.089, 56032/60000 datapoints
2025-03-06 20:23:14,820 - INFO - training batch 1801, loss: 0.104, 57632/60000 datapoints
2025-03-06 20:23:15,035 - INFO - training batch 1851, loss: 0.462, 59232/60000 datapoints
2025-03-06 20:23:15,143 - INFO - validation batch 1, loss: 0.408, 32/10016 datapoints
2025-03-06 20:23:15,307 - INFO - validation batch 51, loss: 0.401, 1632/10016 datapoints
2025-03-06 20:23:15,472 - INFO - validation batch 101, loss: 0.124, 3232/10016 datapoints
2025-03-06 20:23:15,641 - INFO - validation batch 151, loss: 0.235, 4832/10016 datapoints
2025-03-06 20:23:15,808 - INFO - validation batch 201, loss: 0.413, 6432/10016 datapoints
2025-03-06 20:23:15,974 - INFO - validation batch 251, loss: 0.211, 8032/10016 datapoints
2025-03-06 20:23:16,162 - INFO - validation batch 301, loss: 0.127, 9632/10016 datapoints
2025-03-06 20:23:16,206 - INFO - Epoch 698/800 done.
2025-03-06 20:23:16,206 - INFO - Final validation performance:
Loss: 0.274, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:23:16,207 - INFO - Beginning epoch 699/800
2025-03-06 20:23:16,215 - INFO - training batch 1, loss: 0.170, 32/60000 datapoints
2025-03-06 20:23:16,442 - INFO - training batch 51, loss: 0.390, 1632/60000 datapoints
2025-03-06 20:23:16,651 - INFO - training batch 101, loss: 0.199, 3232/60000 datapoints
2025-03-06 20:23:16,861 - INFO - training batch 151, loss: 0.265, 4832/60000 datapoints
2025-03-06 20:23:17,067 - INFO - training batch 201, loss: 0.107, 6432/60000 datapoints
2025-03-06 20:23:17,272 - INFO - training batch 251, loss: 0.161, 8032/60000 datapoints
2025-03-06 20:23:17,480 - INFO - training batch 301, loss: 0.241, 9632/60000 datapoints
2025-03-06 20:23:17,684 - INFO - training batch 351, loss: 0.264, 11232/60000 datapoints
2025-03-06 20:23:17,900 - INFO - training batch 401, loss: 0.143, 12832/60000 datapoints
2025-03-06 20:23:18,116 - INFO - training batch 451, loss: 0.103, 14432/60000 datapoints
2025-03-06 20:23:18,315 - INFO - training batch 501, loss: 0.133, 16032/60000 datapoints
2025-03-06 20:23:18,519 - INFO - training batch 551, loss: 0.246, 17632/60000 datapoints
2025-03-06 20:23:18,725 - INFO - training batch 601, loss: 0.121, 19232/60000 datapoints
2025-03-06 20:23:18,931 - INFO - training batch 651, loss: 0.173, 20832/60000 datapoints
2025-03-06 20:23:19,131 - INFO - training batch 701, loss: 0.220, 22432/60000 datapoints
2025-03-06 20:23:19,336 - INFO - training batch 751, loss: 0.162, 24032/60000 datapoints
2025-03-06 20:23:19,543 - INFO - training batch 801, loss: 0.206, 25632/60000 datapoints
2025-03-06 20:23:19,749 - INFO - training batch 851, loss: 0.149, 27232/60000 datapoints
2025-03-06 20:23:19,950 - INFO - training batch 901, loss: 0.066, 28832/60000 datapoints
2025-03-06 20:23:20,153 - INFO - training batch 951, loss: 0.191, 30432/60000 datapoints
2025-03-06 20:23:20,352 - INFO - training batch 1001, loss: 0.102, 32032/60000 datapoints
2025-03-06 20:23:20,558 - INFO - training batch 1051, loss: 0.088, 33632/60000 datapoints
2025-03-06 20:23:20,762 - INFO - training batch 1101, loss: 0.247, 35232/60000 datapoints
2025-03-06 20:23:20,963 - INFO - training batch 1151, loss: 0.083, 36832/60000 datapoints
2025-03-06 20:23:21,160 - INFO - training batch 1201, loss: 0.217, 38432/60000 datapoints
2025-03-06 20:23:21,362 - INFO - training batch 1251, loss: 0.141, 40032/60000 datapoints
2025-03-06 20:23:21,563 - INFO - training batch 1301, loss: 0.303, 41632/60000 datapoints
2025-03-06 20:23:21,769 - INFO - training batch 1351, loss: 0.391, 43232/60000 datapoints
2025-03-06 20:23:21,976 - INFO - training batch 1401, loss: 0.223, 44832/60000 datapoints
2025-03-06 20:23:22,178 - INFO - training batch 1451, loss: 0.140, 46432/60000 datapoints
2025-03-06 20:23:22,378 - INFO - training batch 1501, loss: 0.208, 48032/60000 datapoints
2025-03-06 20:23:22,586 - INFO - training batch 1551, loss: 0.213, 49632/60000 datapoints
2025-03-06 20:23:22,798 - INFO - training batch 1601, loss: 0.453, 51232/60000 datapoints
2025-03-06 20:23:23,004 - INFO - training batch 1651, loss: 0.274, 52832/60000 datapoints
2025-03-06 20:23:23,211 - INFO - training batch 1701, loss: 0.277, 54432/60000 datapoints
2025-03-06 20:23:23,421 - INFO - training batch 1751, loss: 0.494, 56032/60000 datapoints
2025-03-06 20:23:23,632 - INFO - training batch 1801, loss: 0.303, 57632/60000 datapoints
2025-03-06 20:23:23,839 - INFO - training batch 1851, loss: 0.132, 59232/60000 datapoints
2025-03-06 20:23:23,948 - INFO - validation batch 1, loss: 0.078, 32/10016 datapoints
2025-03-06 20:23:24,115 - INFO - validation batch 51, loss: 0.041, 1632/10016 datapoints
2025-03-06 20:23:24,283 - INFO - validation batch 101, loss: 0.237, 3232/10016 datapoints
2025-03-06 20:23:24,452 - INFO - validation batch 151, loss: 0.256, 4832/10016 datapoints
2025-03-06 20:23:24,624 - INFO - validation batch 201, loss: 0.122, 6432/10016 datapoints
2025-03-06 20:23:24,794 - INFO - validation batch 251, loss: 0.098, 8032/10016 datapoints
2025-03-06 20:23:24,971 - INFO - validation batch 301, loss: 0.085, 9632/10016 datapoints
2025-03-06 20:23:25,011 - INFO - Epoch 699/800 done.
2025-03-06 20:23:25,011 - INFO - Final validation performance:
Loss: 0.131, top-1 acc: 0.934top-5 acc: 0.934
2025-03-06 20:23:25,012 - INFO - Beginning epoch 700/800
2025-03-06 20:23:25,025 - INFO - training batch 1, loss: 0.124, 32/60000 datapoints
2025-03-06 20:23:25,253 - INFO - training batch 51, loss: 0.229, 1632/60000 datapoints
2025-03-06 20:23:25,465 - INFO - training batch 101, loss: 0.239, 3232/60000 datapoints
2025-03-06 20:23:25,686 - INFO - training batch 151, loss: 0.389, 4832/60000 datapoints
2025-03-06 20:23:25,898 - INFO - training batch 201, loss: 0.149, 6432/60000 datapoints
2025-03-06 20:23:26,108 - INFO - training batch 251, loss: 0.267, 8032/60000 datapoints
2025-03-06 20:23:26,341 - INFO - training batch 301, loss: 0.274, 9632/60000 datapoints
2025-03-06 20:23:26,552 - INFO - training batch 351, loss: 0.412, 11232/60000 datapoints
2025-03-06 20:23:26,769 - INFO - training batch 401, loss: 0.106, 12832/60000 datapoints
2025-03-06 20:23:26,983 - INFO - training batch 451, loss: 0.083, 14432/60000 datapoints
2025-03-06 20:23:27,190 - INFO - training batch 501, loss: 0.273, 16032/60000 datapoints
2025-03-06 20:23:27,398 - INFO - training batch 551, loss: 0.315, 17632/60000 datapoints
2025-03-06 20:23:27,608 - INFO - training batch 601, loss: 0.103, 19232/60000 datapoints
2025-03-06 20:23:27,822 - INFO - training batch 651, loss: 0.230, 20832/60000 datapoints
2025-03-06 20:23:28,033 - INFO - training batch 701, loss: 0.200, 22432/60000 datapoints
2025-03-06 20:23:28,240 - INFO - training batch 751, loss: 0.296, 24032/60000 datapoints
2025-03-06 20:23:28,447 - INFO - training batch 801, loss: 0.205, 25632/60000 datapoints
2025-03-06 20:23:28,659 - INFO - training batch 851, loss: 0.193, 27232/60000 datapoints
2025-03-06 20:23:28,866 - INFO - training batch 901, loss: 0.051, 28832/60000 datapoints
2025-03-06 20:23:29,075 - INFO - training batch 951, loss: 0.180, 30432/60000 datapoints
2025-03-06 20:23:29,285 - INFO - training batch 1001, loss: 0.102, 32032/60000 datapoints
2025-03-06 20:23:29,495 - INFO - training batch 1051, loss: 0.218, 33632/60000 datapoints
2025-03-06 20:23:29,706 - INFO - training batch 1101, loss: 0.296, 35232/60000 datapoints
2025-03-06 20:23:29,915 - INFO - training batch 1151, loss: 0.110, 36832/60000 datapoints
2025-03-06 20:23:30,125 - INFO - training batch 1201, loss: 0.240, 38432/60000 datapoints
2025-03-06 20:23:30,332 - INFO - training batch 1251, loss: 0.132, 40032/60000 datapoints
2025-03-06 20:23:30,544 - INFO - training batch 1301, loss: 0.209, 41632/60000 datapoints
2025-03-06 20:23:30,756 - INFO - training batch 1351, loss: 0.166, 43232/60000 datapoints
2025-03-06 20:23:30,965 - INFO - training batch 1401, loss: 0.418, 44832/60000 datapoints
2025-03-06 20:23:31,172 - INFO - training batch 1451, loss: 0.290, 46432/60000 datapoints
2025-03-06 20:23:31,381 - INFO - training batch 1501, loss: 0.358, 48032/60000 datapoints
2025-03-06 20:23:31,593 - INFO - training batch 1551, loss: 0.165, 49632/60000 datapoints
2025-03-06 20:23:31,809 - INFO - training batch 1601, loss: 0.145, 51232/60000 datapoints
2025-03-06 20:23:32,019 - INFO - training batch 1651, loss: 0.541, 52832/60000 datapoints
2025-03-06 20:23:32,227 - INFO - training batch 1701, loss: 0.294, 54432/60000 datapoints
2025-03-06 20:23:32,438 - INFO - training batch 1751, loss: 0.206, 56032/60000 datapoints
2025-03-06 20:23:32,652 - INFO - training batch 1801, loss: 0.277, 57632/60000 datapoints
2025-03-06 20:23:32,861 - INFO - training batch 1851, loss: 0.061, 59232/60000 datapoints
2025-03-06 20:23:32,973 - INFO - validation batch 1, loss: 0.192, 32/10016 datapoints
2025-03-06 20:23:33,141 - INFO - validation batch 51, loss: 0.143, 1632/10016 datapoints
2025-03-06 20:23:33,313 - INFO - validation batch 101, loss: 0.218, 3232/10016 datapoints
2025-03-06 20:23:33,484 - INFO - validation batch 151, loss: 0.132, 4832/10016 datapoints
2025-03-06 20:23:33,654 - INFO - validation batch 201, loss: 0.404, 6432/10016 datapoints
2025-03-06 20:23:33,822 - INFO - validation batch 251, loss: 0.091, 8032/10016 datapoints
2025-03-06 20:23:33,992 - INFO - validation batch 301, loss: 0.253, 9632/10016 datapoints
2025-03-06 20:23:34,035 - INFO - Epoch 700/800 done.
2025-03-06 20:23:34,035 - INFO - Final validation performance:
Loss: 0.205, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:23:34,036 - INFO - Beginning epoch 701/800
2025-03-06 20:23:34,043 - INFO - training batch 1, loss: 0.118, 32/60000 datapoints
2025-03-06 20:23:34,250 - INFO - training batch 51, loss: 0.168, 1632/60000 datapoints
2025-03-06 20:23:34,473 - INFO - training batch 101, loss: 0.127, 3232/60000 datapoints
2025-03-06 20:23:34,688 - INFO - training batch 151, loss: 0.467, 4832/60000 datapoints
2025-03-06 20:23:34,898 - INFO - training batch 201, loss: 0.082, 6432/60000 datapoints
2025-03-06 20:23:35,112 - INFO - training batch 251, loss: 0.256, 8032/60000 datapoints
2025-03-06 20:23:35,321 - INFO - training batch 301, loss: 0.272, 9632/60000 datapoints
2025-03-06 20:23:35,534 - INFO - training batch 351, loss: 0.217, 11232/60000 datapoints
2025-03-06 20:23:35,746 - INFO - training batch 401, loss: 0.177, 12832/60000 datapoints
2025-03-06 20:23:35,955 - INFO - training batch 451, loss: 0.224, 14432/60000 datapoints
2025-03-06 20:23:36,155 - INFO - training batch 501, loss: 0.135, 16032/60000 datapoints
2025-03-06 20:23:36,377 - INFO - training batch 551, loss: 0.360, 17632/60000 datapoints
2025-03-06 20:23:36,583 - INFO - training batch 601, loss: 0.101, 19232/60000 datapoints
2025-03-06 20:23:36,789 - INFO - training batch 651, loss: 0.190, 20832/60000 datapoints
2025-03-06 20:23:36,993 - INFO - training batch 701, loss: 0.450, 22432/60000 datapoints
2025-03-06 20:23:37,193 - INFO - training batch 751, loss: 0.330, 24032/60000 datapoints
2025-03-06 20:23:37,393 - INFO - training batch 801, loss: 0.082, 25632/60000 datapoints
2025-03-06 20:23:37,595 - INFO - training batch 851, loss: 0.174, 27232/60000 datapoints
2025-03-06 20:23:37,823 - INFO - training batch 901, loss: 0.185, 28832/60000 datapoints
2025-03-06 20:23:38,021 - INFO - training batch 951, loss: 0.256, 30432/60000 datapoints
2025-03-06 20:23:38,216 - INFO - training batch 1001, loss: 0.235, 32032/60000 datapoints
2025-03-06 20:23:38,409 - INFO - training batch 1051, loss: 0.099, 33632/60000 datapoints
2025-03-06 20:23:38,613 - INFO - training batch 1101, loss: 0.230, 35232/60000 datapoints
2025-03-06 20:23:38,809 - INFO - training batch 1151, loss: 0.182, 36832/60000 datapoints
2025-03-06 20:23:39,003 - INFO - training batch 1201, loss: 0.307, 38432/60000 datapoints
2025-03-06 20:23:39,201 - INFO - training batch 1251, loss: 0.333, 40032/60000 datapoints
2025-03-06 20:23:39,394 - INFO - training batch 1301, loss: 0.236, 41632/60000 datapoints
2025-03-06 20:23:39,605 - INFO - training batch 1351, loss: 0.074, 43232/60000 datapoints
2025-03-06 20:23:39,818 - INFO - training batch 1401, loss: 0.195, 44832/60000 datapoints
2025-03-06 20:23:40,014 - INFO - training batch 1451, loss: 0.325, 46432/60000 datapoints
2025-03-06 20:23:40,210 - INFO - training batch 1501, loss: 0.169, 48032/60000 datapoints
2025-03-06 20:23:40,403 - INFO - training batch 1551, loss: 0.289, 49632/60000 datapoints
2025-03-06 20:23:40,604 - INFO - training batch 1601, loss: 0.082, 51232/60000 datapoints
2025-03-06 20:23:40,810 - INFO - training batch 1651, loss: 0.288, 52832/60000 datapoints
2025-03-06 20:23:41,012 - INFO - training batch 1701, loss: 0.044, 54432/60000 datapoints
2025-03-06 20:23:41,221 - INFO - training batch 1751, loss: 0.234, 56032/60000 datapoints
2025-03-06 20:23:41,416 - INFO - training batch 1801, loss: 0.162, 57632/60000 datapoints
2025-03-06 20:23:41,612 - INFO - training batch 1851, loss: 0.100, 59232/60000 datapoints
2025-03-06 20:23:41,719 - INFO - validation batch 1, loss: 0.298, 32/10016 datapoints
2025-03-06 20:23:41,875 - INFO - validation batch 51, loss: 0.132, 1632/10016 datapoints
2025-03-06 20:23:42,030 - INFO - validation batch 101, loss: 0.148, 3232/10016 datapoints
2025-03-06 20:23:42,184 - INFO - validation batch 151, loss: 0.100, 4832/10016 datapoints
2025-03-06 20:23:42,337 - INFO - validation batch 201, loss: 0.219, 6432/10016 datapoints
2025-03-06 20:23:42,490 - INFO - validation batch 251, loss: 0.292, 8032/10016 datapoints
2025-03-06 20:23:42,650 - INFO - validation batch 301, loss: 0.287, 9632/10016 datapoints
2025-03-06 20:23:42,689 - INFO - Epoch 701/800 done.
2025-03-06 20:23:42,689 - INFO - Final validation performance:
Loss: 0.211, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:23:42,690 - INFO - Beginning epoch 702/800
2025-03-06 20:23:42,697 - INFO - training batch 1, loss: 0.088, 32/60000 datapoints
2025-03-06 20:23:42,927 - INFO - training batch 51, loss: 0.132, 1632/60000 datapoints
2025-03-06 20:23:43,122 - INFO - training batch 101, loss: 0.264, 3232/60000 datapoints
2025-03-06 20:23:43,327 - INFO - training batch 151, loss: 0.447, 4832/60000 datapoints
2025-03-06 20:23:43,527 - INFO - training batch 201, loss: 0.115, 6432/60000 datapoints
2025-03-06 20:23:43,727 - INFO - training batch 251, loss: 0.097, 8032/60000 datapoints
2025-03-06 20:23:43,923 - INFO - training batch 301, loss: 0.127, 9632/60000 datapoints
2025-03-06 20:23:44,118 - INFO - training batch 351, loss: 0.173, 11232/60000 datapoints
2025-03-06 20:23:44,313 - INFO - training batch 401, loss: 0.387, 12832/60000 datapoints
2025-03-06 20:23:44,508 - INFO - training batch 451, loss: 0.114, 14432/60000 datapoints
2025-03-06 20:23:44,706 - INFO - training batch 501, loss: 0.617, 16032/60000 datapoints
2025-03-06 20:23:44,910 - INFO - training batch 551, loss: 0.255, 17632/60000 datapoints
2025-03-06 20:23:45,105 - INFO - training batch 601, loss: 0.342, 19232/60000 datapoints
2025-03-06 20:23:45,299 - INFO - training batch 651, loss: 0.170, 20832/60000 datapoints
2025-03-06 20:23:45,495 - INFO - training batch 701, loss: 0.110, 22432/60000 datapoints
2025-03-06 20:23:45,698 - INFO - training batch 751, loss: 0.365, 24032/60000 datapoints
2025-03-06 20:23:45,892 - INFO - training batch 801, loss: 0.371, 25632/60000 datapoints
2025-03-06 20:23:46,091 - INFO - training batch 851, loss: 0.121, 27232/60000 datapoints
2025-03-06 20:23:46,285 - INFO - training batch 901, loss: 0.209, 28832/60000 datapoints
2025-03-06 20:23:46,500 - INFO - training batch 951, loss: 0.132, 30432/60000 datapoints
2025-03-06 20:23:46,700 - INFO - training batch 1001, loss: 0.122, 32032/60000 datapoints
2025-03-06 20:23:46,897 - INFO - training batch 1051, loss: 0.078, 33632/60000 datapoints
2025-03-06 20:23:47,095 - INFO - training batch 1101, loss: 0.217, 35232/60000 datapoints
2025-03-06 20:23:47,292 - INFO - training batch 1151, loss: 0.150, 36832/60000 datapoints
2025-03-06 20:23:47,485 - INFO - training batch 1201, loss: 0.260, 38432/60000 datapoints
2025-03-06 20:23:47,688 - INFO - training batch 1251, loss: 0.068, 40032/60000 datapoints
2025-03-06 20:23:47,882 - INFO - training batch 1301, loss: 0.080, 41632/60000 datapoints
2025-03-06 20:23:48,076 - INFO - training batch 1351, loss: 0.271, 43232/60000 datapoints
2025-03-06 20:23:48,269 - INFO - training batch 1401, loss: 0.139, 44832/60000 datapoints
2025-03-06 20:23:48,463 - INFO - training batch 1451, loss: 0.078, 46432/60000 datapoints
2025-03-06 20:23:48,665 - INFO - training batch 1501, loss: 0.128, 48032/60000 datapoints
2025-03-06 20:23:48,859 - INFO - training batch 1551, loss: 0.027, 49632/60000 datapoints
2025-03-06 20:23:49,054 - INFO - training batch 1601, loss: 0.129, 51232/60000 datapoints
2025-03-06 20:23:49,251 - INFO - training batch 1651, loss: 0.723, 52832/60000 datapoints
2025-03-06 20:23:49,446 - INFO - training batch 1701, loss: 0.158, 54432/60000 datapoints
2025-03-06 20:23:49,646 - INFO - training batch 1751, loss: 0.144, 56032/60000 datapoints
2025-03-06 20:23:49,841 - INFO - training batch 1801, loss: 0.565, 57632/60000 datapoints
2025-03-06 20:23:50,036 - INFO - training batch 1851, loss: 0.395, 59232/60000 datapoints
2025-03-06 20:23:50,139 - INFO - validation batch 1, loss: 0.265, 32/10016 datapoints
2025-03-06 20:23:50,292 - INFO - validation batch 51, loss: 0.155, 1632/10016 datapoints
2025-03-06 20:23:50,443 - INFO - validation batch 101, loss: 0.065, 3232/10016 datapoints
2025-03-06 20:23:50,598 - INFO - validation batch 151, loss: 0.158, 4832/10016 datapoints
2025-03-06 20:23:50,758 - INFO - validation batch 201, loss: 0.127, 6432/10016 datapoints
2025-03-06 20:23:50,911 - INFO - validation batch 251, loss: 0.246, 8032/10016 datapoints
2025-03-06 20:23:51,065 - INFO - validation batch 301, loss: 0.053, 9632/10016 datapoints
2025-03-06 20:23:51,103 - INFO - Epoch 702/800 done.
2025-03-06 20:23:51,103 - INFO - Final validation performance:
Loss: 0.153, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:23:51,104 - INFO - Beginning epoch 703/800
2025-03-06 20:23:51,112 - INFO - training batch 1, loss: 0.272, 32/60000 datapoints
2025-03-06 20:23:51,309 - INFO - training batch 51, loss: 0.163, 1632/60000 datapoints
2025-03-06 20:23:51,516 - INFO - training batch 101, loss: 0.326, 3232/60000 datapoints
2025-03-06 20:23:51,717 - INFO - training batch 151, loss: 0.186, 4832/60000 datapoints
2025-03-06 20:23:51,917 - INFO - training batch 201, loss: 0.296, 6432/60000 datapoints
2025-03-06 20:23:52,114 - INFO - training batch 251, loss: 0.110, 8032/60000 datapoints
2025-03-06 20:23:52,314 - INFO - training batch 301, loss: 0.478, 9632/60000 datapoints
2025-03-06 20:23:52,507 - INFO - training batch 351, loss: 0.226, 11232/60000 datapoints
2025-03-06 20:23:52,708 - INFO - training batch 401, loss: 0.114, 12832/60000 datapoints
2025-03-06 20:23:52,905 - INFO - training batch 451, loss: 0.124, 14432/60000 datapoints
2025-03-06 20:23:53,099 - INFO - training batch 501, loss: 0.153, 16032/60000 datapoints
2025-03-06 20:23:53,298 - INFO - training batch 551, loss: 0.179, 17632/60000 datapoints
2025-03-06 20:23:53,494 - INFO - training batch 601, loss: 0.120, 19232/60000 datapoints
2025-03-06 20:23:53,697 - INFO - training batch 651, loss: 0.340, 20832/60000 datapoints
2025-03-06 20:23:53,894 - INFO - training batch 701, loss: 0.087, 22432/60000 datapoints
2025-03-06 20:23:54,088 - INFO - training batch 751, loss: 0.527, 24032/60000 datapoints
2025-03-06 20:23:54,285 - INFO - training batch 801, loss: 0.162, 25632/60000 datapoints
2025-03-06 20:23:54,478 - INFO - training batch 851, loss: 0.329, 27232/60000 datapoints
2025-03-06 20:23:54,678 - INFO - training batch 901, loss: 0.247, 28832/60000 datapoints
2025-03-06 20:23:54,873 - INFO - training batch 951, loss: 0.263, 30432/60000 datapoints
2025-03-06 20:23:55,072 - INFO - training batch 1001, loss: 0.173, 32032/60000 datapoints
2025-03-06 20:23:55,269 - INFO - training batch 1051, loss: 0.340, 33632/60000 datapoints
2025-03-06 20:23:55,461 - INFO - training batch 1101, loss: 0.339, 35232/60000 datapoints
2025-03-06 20:23:55,661 - INFO - training batch 1151, loss: 0.578, 36832/60000 datapoints
2025-03-06 20:23:55,857 - INFO - training batch 1201, loss: 0.066, 38432/60000 datapoints
2025-03-06 20:23:56,050 - INFO - training batch 1251, loss: 0.237, 40032/60000 datapoints
2025-03-06 20:23:56,244 - INFO - training batch 1301, loss: 0.238, 41632/60000 datapoints
2025-03-06 20:23:56,452 - INFO - training batch 1351, loss: 0.265, 43232/60000 datapoints
2025-03-06 20:23:56,663 - INFO - training batch 1401, loss: 0.332, 44832/60000 datapoints
2025-03-06 20:23:56,856 - INFO - training batch 1451, loss: 0.200, 46432/60000 datapoints
2025-03-06 20:23:57,052 - INFO - training batch 1501, loss: 0.255, 48032/60000 datapoints
2025-03-06 20:23:57,248 - INFO - training batch 1551, loss: 0.215, 49632/60000 datapoints
2025-03-06 20:23:57,441 - INFO - training batch 1601, loss: 0.190, 51232/60000 datapoints
2025-03-06 20:23:57,637 - INFO - training batch 1651, loss: 0.524, 52832/60000 datapoints
2025-03-06 20:23:57,833 - INFO - training batch 1701, loss: 0.115, 54432/60000 datapoints
2025-03-06 20:23:58,031 - INFO - training batch 1751, loss: 0.072, 56032/60000 datapoints
2025-03-06 20:23:58,227 - INFO - training batch 1801, loss: 0.165, 57632/60000 datapoints
2025-03-06 20:23:58,421 - INFO - training batch 1851, loss: 0.151, 59232/60000 datapoints
2025-03-06 20:23:58,523 - INFO - validation batch 1, loss: 0.116, 32/10016 datapoints
2025-03-06 20:23:58,683 - INFO - validation batch 51, loss: 0.086, 1632/10016 datapoints
2025-03-06 20:23:58,839 - INFO - validation batch 101, loss: 0.247, 3232/10016 datapoints
2025-03-06 20:23:58,993 - INFO - validation batch 151, loss: 0.278, 4832/10016 datapoints
2025-03-06 20:23:59,147 - INFO - validation batch 201, loss: 0.188, 6432/10016 datapoints
2025-03-06 20:23:59,302 - INFO - validation batch 251, loss: 0.238, 8032/10016 datapoints
2025-03-06 20:23:59,457 - INFO - validation batch 301, loss: 0.154, 9632/10016 datapoints
2025-03-06 20:23:59,495 - INFO - Epoch 703/800 done.
2025-03-06 20:23:59,495 - INFO - Final validation performance:
Loss: 0.187, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:23:59,496 - INFO - Beginning epoch 704/800
2025-03-06 20:23:59,502 - INFO - training batch 1, loss: 0.301, 32/60000 datapoints
2025-03-06 20:23:59,703 - INFO - training batch 51, loss: 0.172, 1632/60000 datapoints
2025-03-06 20:23:59,899 - INFO - training batch 101, loss: 0.300, 3232/60000 datapoints
2025-03-06 20:24:00,105 - INFO - training batch 151, loss: 0.213, 4832/60000 datapoints
2025-03-06 20:24:00,305 - INFO - training batch 201, loss: 0.244, 6432/60000 datapoints
2025-03-06 20:24:00,498 - INFO - training batch 251, loss: 0.275, 8032/60000 datapoints
2025-03-06 20:24:00,705 - INFO - training batch 301, loss: 0.122, 9632/60000 datapoints
2025-03-06 20:24:00,907 - INFO - training batch 351, loss: 0.437, 11232/60000 datapoints
2025-03-06 20:24:01,115 - INFO - training batch 401, loss: 0.370, 12832/60000 datapoints
2025-03-06 20:24:01,310 - INFO - training batch 451, loss: 0.063, 14432/60000 datapoints
2025-03-06 20:24:01,505 - INFO - training batch 501, loss: 0.230, 16032/60000 datapoints
2025-03-06 20:24:01,709 - INFO - training batch 551, loss: 0.129, 17632/60000 datapoints
2025-03-06 20:24:01,915 - INFO - training batch 601, loss: 0.138, 19232/60000 datapoints
2025-03-06 20:24:02,110 - INFO - training batch 651, loss: 0.269, 20832/60000 datapoints
2025-03-06 20:24:02,311 - INFO - training batch 701, loss: 0.141, 22432/60000 datapoints
2025-03-06 20:24:02,505 - INFO - training batch 751, loss: 0.106, 24032/60000 datapoints
2025-03-06 20:24:02,705 - INFO - training batch 801, loss: 0.240, 25632/60000 datapoints
2025-03-06 20:24:02,900 - INFO - training batch 851, loss: 0.090, 27232/60000 datapoints
2025-03-06 20:24:03,094 - INFO - training batch 901, loss: 0.200, 28832/60000 datapoints
2025-03-06 20:24:03,291 - INFO - training batch 951, loss: 0.107, 30432/60000 datapoints
2025-03-06 20:24:03,488 - INFO - training batch 1001, loss: 0.124, 32032/60000 datapoints
2025-03-06 20:24:03,684 - INFO - training batch 1051, loss: 0.174, 33632/60000 datapoints
2025-03-06 20:24:03,879 - INFO - training batch 1101, loss: 0.270, 35232/60000 datapoints
2025-03-06 20:24:04,072 - INFO - training batch 1151, loss: 0.153, 36832/60000 datapoints
2025-03-06 20:24:04,268 - INFO - training batch 1201, loss: 0.423, 38432/60000 datapoints
2025-03-06 20:24:04,463 - INFO - training batch 1251, loss: 0.093, 40032/60000 datapoints
2025-03-06 20:24:04,661 - INFO - training batch 1301, loss: 0.155, 41632/60000 datapoints
2025-03-06 20:24:04,859 - INFO - training batch 1351, loss: 0.282, 43232/60000 datapoints
2025-03-06 20:24:05,059 - INFO - training batch 1401, loss: 0.306, 44832/60000 datapoints
2025-03-06 20:24:05,254 - INFO - training batch 1451, loss: 0.341, 46432/60000 datapoints
2025-03-06 20:24:05,448 - INFO - training batch 1501, loss: 0.231, 48032/60000 datapoints
2025-03-06 20:24:05,650 - INFO - training batch 1551, loss: 0.140, 49632/60000 datapoints
2025-03-06 20:24:05,844 - INFO - training batch 1601, loss: 0.296, 51232/60000 datapoints
2025-03-06 20:24:06,042 - INFO - training batch 1651, loss: 0.464, 52832/60000 datapoints
2025-03-06 20:24:06,238 - INFO - training batch 1701, loss: 0.126, 54432/60000 datapoints
2025-03-06 20:24:06,433 - INFO - training batch 1751, loss: 0.128, 56032/60000 datapoints
2025-03-06 20:24:06,650 - INFO - training batch 1801, loss: 0.126, 57632/60000 datapoints
2025-03-06 20:24:06,848 - INFO - training batch 1851, loss: 0.355, 59232/60000 datapoints
2025-03-06 20:24:06,951 - INFO - validation batch 1, loss: 0.174, 32/10016 datapoints
2025-03-06 20:24:07,105 - INFO - validation batch 51, loss: 0.098, 1632/10016 datapoints
2025-03-06 20:24:07,257 - INFO - validation batch 101, loss: 0.293, 3232/10016 datapoints
2025-03-06 20:24:07,411 - INFO - validation batch 151, loss: 0.345, 4832/10016 datapoints
2025-03-06 20:24:07,566 - INFO - validation batch 201, loss: 0.191, 6432/10016 datapoints
2025-03-06 20:24:07,725 - INFO - validation batch 251, loss: 0.180, 8032/10016 datapoints
2025-03-06 20:24:07,881 - INFO - validation batch 301, loss: 0.031, 9632/10016 datapoints
2025-03-06 20:24:07,918 - INFO - Epoch 704/800 done.
2025-03-06 20:24:07,918 - INFO - Final validation performance:
Loss: 0.187, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:24:07,919 - INFO - Beginning epoch 705/800
2025-03-06 20:24:07,926 - INFO - training batch 1, loss: 0.160, 32/60000 datapoints
2025-03-06 20:24:08,125 - INFO - training batch 51, loss: 0.236, 1632/60000 datapoints
2025-03-06 20:24:08,326 - INFO - training batch 101, loss: 0.123, 3232/60000 datapoints
2025-03-06 20:24:08,550 - INFO - training batch 151, loss: 0.111, 4832/60000 datapoints
2025-03-06 20:24:08,753 - INFO - training batch 201, loss: 0.266, 6432/60000 datapoints
2025-03-06 20:24:08,958 - INFO - training batch 251, loss: 0.125, 8032/60000 datapoints
2025-03-06 20:24:09,161 - INFO - training batch 301, loss: 0.378, 9632/60000 datapoints
2025-03-06 20:24:09,361 - INFO - training batch 351, loss: 0.367, 11232/60000 datapoints
2025-03-06 20:24:09,558 - INFO - training batch 401, loss: 0.160, 12832/60000 datapoints
2025-03-06 20:24:09,757 - INFO - training batch 451, loss: 0.289, 14432/60000 datapoints
2025-03-06 20:24:09,954 - INFO - training batch 501, loss: 0.990, 16032/60000 datapoints
2025-03-06 20:24:10,149 - INFO - training batch 551, loss: 0.094, 17632/60000 datapoints
2025-03-06 20:24:10,346 - INFO - training batch 601, loss: 0.099, 19232/60000 datapoints
2025-03-06 20:24:10,541 - INFO - training batch 651, loss: 0.419, 20832/60000 datapoints
2025-03-06 20:24:10,740 - INFO - training batch 701, loss: 0.391, 22432/60000 datapoints
2025-03-06 20:24:10,938 - INFO - training batch 751, loss: 0.124, 24032/60000 datapoints
2025-03-06 20:24:11,137 - INFO - training batch 801, loss: 0.438, 25632/60000 datapoints
2025-03-06 20:24:11,333 - INFO - training batch 851, loss: 0.191, 27232/60000 datapoints
2025-03-06 20:24:11,529 - INFO - training batch 901, loss: 0.178, 28832/60000 datapoints
2025-03-06 20:24:11,747 - INFO - training batch 951, loss: 0.136, 30432/60000 datapoints
2025-03-06 20:24:11,942 - INFO - training batch 1001, loss: 0.068, 32032/60000 datapoints
2025-03-06 20:24:12,139 - INFO - training batch 1051, loss: 0.125, 33632/60000 datapoints
2025-03-06 20:24:12,332 - INFO - training batch 1101, loss: 0.128, 35232/60000 datapoints
2025-03-06 20:24:12,528 - INFO - training batch 1151, loss: 0.159, 36832/60000 datapoints
2025-03-06 20:24:12,727 - INFO - training batch 1201, loss: 0.319, 38432/60000 datapoints
2025-03-06 20:24:12,927 - INFO - training batch 1251, loss: 0.150, 40032/60000 datapoints
2025-03-06 20:24:13,123 - INFO - training batch 1301, loss: 0.035, 41632/60000 datapoints
2025-03-06 20:24:13,319 - INFO - training batch 1351, loss: 0.206, 43232/60000 datapoints
2025-03-06 20:24:13,514 - INFO - training batch 1401, loss: 0.404, 44832/60000 datapoints
2025-03-06 20:24:13,712 - INFO - training batch 1451, loss: 0.167, 46432/60000 datapoints
2025-03-06 20:24:13,905 - INFO - training batch 1501, loss: 0.249, 48032/60000 datapoints
2025-03-06 20:24:14,104 - INFO - training batch 1551, loss: 0.097, 49632/60000 datapoints
2025-03-06 20:24:14,299 - INFO - training batch 1601, loss: 0.152, 51232/60000 datapoints
2025-03-06 20:24:14,496 - INFO - training batch 1651, loss: 0.101, 52832/60000 datapoints
2025-03-06 20:24:14,692 - INFO - training batch 1701, loss: 0.293, 54432/60000 datapoints
2025-03-06 20:24:14,891 - INFO - training batch 1751, loss: 0.109, 56032/60000 datapoints
2025-03-06 20:24:15,091 - INFO - training batch 1801, loss: 0.353, 57632/60000 datapoints
2025-03-06 20:24:15,299 - INFO - training batch 1851, loss: 0.211, 59232/60000 datapoints
2025-03-06 20:24:15,407 - INFO - validation batch 1, loss: 0.148, 32/10016 datapoints
2025-03-06 20:24:15,561 - INFO - validation batch 51, loss: 0.321, 1632/10016 datapoints
2025-03-06 20:24:15,721 - INFO - validation batch 101, loss: 0.224, 3232/10016 datapoints
2025-03-06 20:24:15,874 - INFO - validation batch 151, loss: 0.211, 4832/10016 datapoints
2025-03-06 20:24:16,031 - INFO - validation batch 201, loss: 0.392, 6432/10016 datapoints
2025-03-06 20:24:16,183 - INFO - validation batch 251, loss: 0.251, 8032/10016 datapoints
2025-03-06 20:24:16,337 - INFO - validation batch 301, loss: 0.115, 9632/10016 datapoints
2025-03-06 20:24:16,373 - INFO - Epoch 705/800 done.
2025-03-06 20:24:16,373 - INFO - Final validation performance:
Loss: 0.237, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:24:16,373 - INFO - Beginning epoch 706/800
2025-03-06 20:24:16,380 - INFO - training batch 1, loss: 0.116, 32/60000 datapoints
2025-03-06 20:24:16,592 - INFO - training batch 51, loss: 0.264, 1632/60000 datapoints
2025-03-06 20:24:16,808 - INFO - training batch 101, loss: 0.218, 3232/60000 datapoints
2025-03-06 20:24:17,007 - INFO - training batch 151, loss: 0.215, 4832/60000 datapoints
2025-03-06 20:24:17,203 - INFO - training batch 201, loss: 0.425, 6432/60000 datapoints
2025-03-06 20:24:17,397 - INFO - training batch 251, loss: 0.130, 8032/60000 datapoints
2025-03-06 20:24:17,592 - INFO - training batch 301, loss: 0.163, 9632/60000 datapoints
2025-03-06 20:24:17,787 - INFO - training batch 351, loss: 0.270, 11232/60000 datapoints
2025-03-06 20:24:17,983 - INFO - training batch 401, loss: 0.162, 12832/60000 datapoints
2025-03-06 20:24:18,181 - INFO - training batch 451, loss: 0.055, 14432/60000 datapoints
2025-03-06 20:24:18,372 - INFO - training batch 501, loss: 0.078, 16032/60000 datapoints
2025-03-06 20:24:18,564 - INFO - training batch 551, loss: 0.127, 17632/60000 datapoints
2025-03-06 20:24:18,763 - INFO - training batch 601, loss: 0.110, 19232/60000 datapoints
2025-03-06 20:24:18,956 - INFO - training batch 651, loss: 0.258, 20832/60000 datapoints
2025-03-06 20:24:19,148 - INFO - training batch 701, loss: 0.323, 22432/60000 datapoints
2025-03-06 20:24:19,340 - INFO - training batch 751, loss: 0.194, 24032/60000 datapoints
2025-03-06 20:24:19,536 - INFO - training batch 801, loss: 0.356, 25632/60000 datapoints
2025-03-06 20:24:19,731 - INFO - training batch 851, loss: 0.119, 27232/60000 datapoints
2025-03-06 20:24:19,924 - INFO - training batch 901, loss: 0.100, 28832/60000 datapoints
2025-03-06 20:24:20,119 - INFO - training batch 951, loss: 0.282, 30432/60000 datapoints
2025-03-06 20:24:20,312 - INFO - training batch 1001, loss: 0.103, 32032/60000 datapoints
2025-03-06 20:24:20,505 - INFO - training batch 1051, loss: 0.063, 33632/60000 datapoints
2025-03-06 20:24:20,703 - INFO - training batch 1101, loss: 0.269, 35232/60000 datapoints
2025-03-06 20:24:20,905 - INFO - training batch 1151, loss: 0.223, 36832/60000 datapoints
2025-03-06 20:24:21,101 - INFO - training batch 1201, loss: 0.165, 38432/60000 datapoints
2025-03-06 20:24:21,295 - INFO - training batch 1251, loss: 0.113, 40032/60000 datapoints
2025-03-06 20:24:21,488 - INFO - training batch 1301, loss: 0.161, 41632/60000 datapoints
2025-03-06 20:24:21,684 - INFO - training batch 1351, loss: 0.499, 43232/60000 datapoints
2025-03-06 20:24:21,878 - INFO - training batch 1401, loss: 0.093, 44832/60000 datapoints
2025-03-06 20:24:22,073 - INFO - training batch 1451, loss: 0.110, 46432/60000 datapoints
2025-03-06 20:24:22,266 - INFO - training batch 1501, loss: 0.158, 48032/60000 datapoints
2025-03-06 20:24:22,459 - INFO - training batch 1551, loss: 0.251, 49632/60000 datapoints
2025-03-06 20:24:22,654 - INFO - training batch 1601, loss: 0.265, 51232/60000 datapoints
2025-03-06 20:24:22,850 - INFO - training batch 1651, loss: 0.507, 52832/60000 datapoints
2025-03-06 20:24:23,044 - INFO - training batch 1701, loss: 0.257, 54432/60000 datapoints
2025-03-06 20:24:23,241 - INFO - training batch 1751, loss: 0.481, 56032/60000 datapoints
2025-03-06 20:24:23,434 - INFO - training batch 1801, loss: 0.112, 57632/60000 datapoints
2025-03-06 20:24:23,628 - INFO - training batch 1851, loss: 0.222, 59232/60000 datapoints
2025-03-06 20:24:23,736 - INFO - validation batch 1, loss: 0.131, 32/10016 datapoints
2025-03-06 20:24:23,902 - INFO - validation batch 51, loss: 0.139, 1632/10016 datapoints
2025-03-06 20:24:24,054 - INFO - validation batch 101, loss: 0.344, 3232/10016 datapoints
2025-03-06 20:24:24,207 - INFO - validation batch 151, loss: 0.092, 4832/10016 datapoints
2025-03-06 20:24:24,358 - INFO - validation batch 201, loss: 0.397, 6432/10016 datapoints
2025-03-06 20:24:24,510 - INFO - validation batch 251, loss: 0.185, 8032/10016 datapoints
2025-03-06 20:24:24,663 - INFO - validation batch 301, loss: 0.481, 9632/10016 datapoints
2025-03-06 20:24:24,699 - INFO - Epoch 706/800 done.
2025-03-06 20:24:24,699 - INFO - Final validation performance:
Loss: 0.253, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:24:24,700 - INFO - Beginning epoch 707/800
2025-03-06 20:24:24,706 - INFO - training batch 1, loss: 0.267, 32/60000 datapoints
2025-03-06 20:24:24,920 - INFO - training batch 51, loss: 0.277, 1632/60000 datapoints
2025-03-06 20:24:25,115 - INFO - training batch 101, loss: 0.224, 3232/60000 datapoints
2025-03-06 20:24:25,314 - INFO - training batch 151, loss: 0.117, 4832/60000 datapoints
2025-03-06 20:24:25,509 - INFO - training batch 201, loss: 0.217, 6432/60000 datapoints
2025-03-06 20:24:25,713 - INFO - training batch 251, loss: 0.194, 8032/60000 datapoints
2025-03-06 20:24:25,907 - INFO - training batch 301, loss: 0.089, 9632/60000 datapoints
2025-03-06 20:24:26,100 - INFO - training batch 351, loss: 0.208, 11232/60000 datapoints
2025-03-06 20:24:26,294 - INFO - training batch 401, loss: 0.482, 12832/60000 datapoints
2025-03-06 20:24:26,492 - INFO - training batch 451, loss: 0.160, 14432/60000 datapoints
2025-03-06 20:24:26,692 - INFO - training batch 501, loss: 0.080, 16032/60000 datapoints
2025-03-06 20:24:26,910 - INFO - training batch 551, loss: 0.307, 17632/60000 datapoints
2025-03-06 20:24:27,108 - INFO - training batch 601, loss: 0.229, 19232/60000 datapoints
2025-03-06 20:24:27,302 - INFO - training batch 651, loss: 0.716, 20832/60000 datapoints
2025-03-06 20:24:27,498 - INFO - training batch 701, loss: 0.153, 22432/60000 datapoints
2025-03-06 20:24:27,695 - INFO - training batch 751, loss: 0.188, 24032/60000 datapoints
2025-03-06 20:24:27,893 - INFO - training batch 801, loss: 0.168, 25632/60000 datapoints
2025-03-06 20:24:28,089 - INFO - training batch 851, loss: 0.268, 27232/60000 datapoints
2025-03-06 20:24:28,285 - INFO - training batch 901, loss: 0.513, 28832/60000 datapoints
2025-03-06 20:24:28,479 - INFO - training batch 951, loss: 0.132, 30432/60000 datapoints
2025-03-06 20:24:28,676 - INFO - training batch 1001, loss: 0.172, 32032/60000 datapoints
2025-03-06 20:24:28,873 - INFO - training batch 1051, loss: 0.071, 33632/60000 datapoints
2025-03-06 20:24:29,069 - INFO - training batch 1101, loss: 0.364, 35232/60000 datapoints
2025-03-06 20:24:29,263 - INFO - training batch 1151, loss: 0.119, 36832/60000 datapoints
2025-03-06 20:24:29,459 - INFO - training batch 1201, loss: 0.223, 38432/60000 datapoints
2025-03-06 20:24:29,657 - INFO - training batch 1251, loss: 0.180, 40032/60000 datapoints
2025-03-06 20:24:29,855 - INFO - training batch 1301, loss: 0.440, 41632/60000 datapoints
2025-03-06 20:24:30,052 - INFO - training batch 1351, loss: 0.094, 43232/60000 datapoints
2025-03-06 20:24:30,246 - INFO - training batch 1401, loss: 0.327, 44832/60000 datapoints
2025-03-06 20:24:30,441 - INFO - training batch 1451, loss: 0.177, 46432/60000 datapoints
2025-03-06 20:24:30,638 - INFO - training batch 1501, loss: 0.247, 48032/60000 datapoints
2025-03-06 20:24:30,837 - INFO - training batch 1551, loss: 0.238, 49632/60000 datapoints
2025-03-06 20:24:31,035 - INFO - training batch 1601, loss: 0.069, 51232/60000 datapoints
2025-03-06 20:24:31,231 - INFO - training batch 1651, loss: 0.299, 52832/60000 datapoints
2025-03-06 20:24:31,426 - INFO - training batch 1701, loss: 0.269, 54432/60000 datapoints
2025-03-06 20:24:31,621 - INFO - training batch 1751, loss: 0.354, 56032/60000 datapoints
2025-03-06 20:24:31,819 - INFO - training batch 1801, loss: 0.223, 57632/60000 datapoints
2025-03-06 20:24:32,015 - INFO - training batch 1851, loss: 0.140, 59232/60000 datapoints
2025-03-06 20:24:32,125 - INFO - validation batch 1, loss: 0.072, 32/10016 datapoints
2025-03-06 20:24:32,276 - INFO - validation batch 51, loss: 0.335, 1632/10016 datapoints
2025-03-06 20:24:32,426 - INFO - validation batch 101, loss: 0.213, 3232/10016 datapoints
2025-03-06 20:24:32,578 - INFO - validation batch 151, loss: 0.171, 4832/10016 datapoints
2025-03-06 20:24:32,731 - INFO - validation batch 201, loss: 0.146, 6432/10016 datapoints
2025-03-06 20:24:32,889 - INFO - validation batch 251, loss: 0.065, 8032/10016 datapoints
2025-03-06 20:24:33,041 - INFO - validation batch 301, loss: 0.280, 9632/10016 datapoints
2025-03-06 20:24:33,079 - INFO - Epoch 707/800 done.
2025-03-06 20:24:33,079 - INFO - Final validation performance:
Loss: 0.183, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:24:33,080 - INFO - Beginning epoch 708/800
2025-03-06 20:24:33,086 - INFO - training batch 1, loss: 0.152, 32/60000 datapoints
2025-03-06 20:24:33,295 - INFO - training batch 51, loss: 0.288, 1632/60000 datapoints
2025-03-06 20:24:33,487 - INFO - training batch 101, loss: 0.283, 3232/60000 datapoints
2025-03-06 20:24:33,684 - INFO - training batch 151, loss: 0.152, 4832/60000 datapoints
2025-03-06 20:24:33,883 - INFO - training batch 201, loss: 0.190, 6432/60000 datapoints
2025-03-06 20:24:34,082 - INFO - training batch 251, loss: 0.149, 8032/60000 datapoints
2025-03-06 20:24:34,279 - INFO - training batch 301, loss: 0.093, 9632/60000 datapoints
2025-03-06 20:24:34,472 - INFO - training batch 351, loss: 0.186, 11232/60000 datapoints
2025-03-06 20:24:34,668 - INFO - training batch 401, loss: 0.158, 12832/60000 datapoints
2025-03-06 20:24:34,865 - INFO - training batch 451, loss: 0.091, 14432/60000 datapoints
2025-03-06 20:24:35,068 - INFO - training batch 501, loss: 0.190, 16032/60000 datapoints
2025-03-06 20:24:35,261 - INFO - training batch 551, loss: 0.402, 17632/60000 datapoints
2025-03-06 20:24:35,453 - INFO - training batch 601, loss: 0.520, 19232/60000 datapoints
2025-03-06 20:24:35,649 - INFO - training batch 651, loss: 0.504, 20832/60000 datapoints
2025-03-06 20:24:35,847 - INFO - training batch 701, loss: 0.237, 22432/60000 datapoints
2025-03-06 20:24:36,040 - INFO - training batch 751, loss: 0.312, 24032/60000 datapoints
2025-03-06 20:24:36,234 - INFO - training batch 801, loss: 0.247, 25632/60000 datapoints
2025-03-06 20:24:36,426 - INFO - training batch 851, loss: 0.298, 27232/60000 datapoints
2025-03-06 20:24:36,621 - INFO - training batch 901, loss: 0.166, 28832/60000 datapoints
2025-03-06 20:24:36,840 - INFO - training batch 951, loss: 0.258, 30432/60000 datapoints
2025-03-06 20:24:37,037 - INFO - training batch 1001, loss: 0.312, 32032/60000 datapoints
2025-03-06 20:24:37,232 - INFO - training batch 1051, loss: 0.079, 33632/60000 datapoints
2025-03-06 20:24:37,427 - INFO - training batch 1101, loss: 0.051, 35232/60000 datapoints
2025-03-06 20:24:37,619 - INFO - training batch 1151, loss: 0.244, 36832/60000 datapoints
2025-03-06 20:24:37,828 - INFO - training batch 1201, loss: 0.111, 38432/60000 datapoints
2025-03-06 20:24:38,019 - INFO - training batch 1251, loss: 0.253, 40032/60000 datapoints
2025-03-06 20:24:38,214 - INFO - training batch 1301, loss: 0.215, 41632/60000 datapoints
2025-03-06 20:24:38,406 - INFO - training batch 1351, loss: 0.258, 43232/60000 datapoints
2025-03-06 20:24:38,596 - INFO - training batch 1401, loss: 0.257, 44832/60000 datapoints
2025-03-06 20:24:38,793 - INFO - training batch 1451, loss: 0.518, 46432/60000 datapoints
2025-03-06 20:24:38,993 - INFO - training batch 1501, loss: 0.201, 48032/60000 datapoints
2025-03-06 20:24:39,189 - INFO - training batch 1551, loss: 0.311, 49632/60000 datapoints
2025-03-06 20:24:39,382 - INFO - training batch 1601, loss: 0.055, 51232/60000 datapoints
2025-03-06 20:24:39,575 - INFO - training batch 1651, loss: 0.192, 52832/60000 datapoints
2025-03-06 20:24:39,771 - INFO - training batch 1701, loss: 0.135, 54432/60000 datapoints
2025-03-06 20:24:39,981 - INFO - training batch 1751, loss: 0.333, 56032/60000 datapoints
2025-03-06 20:24:40,191 - INFO - training batch 1801, loss: 0.687, 57632/60000 datapoints
2025-03-06 20:24:40,383 - INFO - training batch 1851, loss: 0.149, 59232/60000 datapoints
2025-03-06 20:24:40,483 - INFO - validation batch 1, loss: 0.106, 32/10016 datapoints
2025-03-06 20:24:40,634 - INFO - validation batch 51, loss: 0.080, 1632/10016 datapoints
2025-03-06 20:24:40,786 - INFO - validation batch 101, loss: 0.299, 3232/10016 datapoints
2025-03-06 20:24:40,940 - INFO - validation batch 151, loss: 0.276, 4832/10016 datapoints
2025-03-06 20:24:41,092 - INFO - validation batch 201, loss: 0.098, 6432/10016 datapoints
2025-03-06 20:24:41,244 - INFO - validation batch 251, loss: 0.228, 8032/10016 datapoints
2025-03-06 20:24:41,397 - INFO - validation batch 301, loss: 0.093, 9632/10016 datapoints
2025-03-06 20:24:41,433 - INFO - Epoch 708/800 done.
2025-03-06 20:24:41,433 - INFO - Final validation performance:
Loss: 0.168, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:24:41,434 - INFO - Beginning epoch 709/800
2025-03-06 20:24:41,440 - INFO - training batch 1, loss: 0.252, 32/60000 datapoints
2025-03-06 20:24:41,634 - INFO - training batch 51, loss: 0.193, 1632/60000 datapoints
2025-03-06 20:24:41,831 - INFO - training batch 101, loss: 0.121, 3232/60000 datapoints
2025-03-06 20:24:42,038 - INFO - training batch 151, loss: 0.053, 4832/60000 datapoints
2025-03-06 20:24:42,234 - INFO - training batch 201, loss: 0.338, 6432/60000 datapoints
2025-03-06 20:24:42,427 - INFO - training batch 251, loss: 0.247, 8032/60000 datapoints
2025-03-06 20:24:42,626 - INFO - training batch 301, loss: 0.380, 9632/60000 datapoints
2025-03-06 20:24:42,826 - INFO - training batch 351, loss: 0.221, 11232/60000 datapoints
2025-03-06 20:24:43,024 - INFO - training batch 401, loss: 0.409, 12832/60000 datapoints
2025-03-06 20:24:43,223 - INFO - training batch 451, loss: 0.414, 14432/60000 datapoints
2025-03-06 20:24:43,417 - INFO - training batch 501, loss: 0.143, 16032/60000 datapoints
2025-03-06 20:24:43,610 - INFO - training batch 551, loss: 0.195, 17632/60000 datapoints
2025-03-06 20:24:43,806 - INFO - training batch 601, loss: 0.268, 19232/60000 datapoints
2025-03-06 20:24:44,000 - INFO - training batch 651, loss: 0.230, 20832/60000 datapoints
2025-03-06 20:24:44,195 - INFO - training batch 701, loss: 0.125, 22432/60000 datapoints
2025-03-06 20:24:44,389 - INFO - training batch 751, loss: 0.235, 24032/60000 datapoints
2025-03-06 20:24:44,583 - INFO - training batch 801, loss: 0.189, 25632/60000 datapoints
2025-03-06 20:24:44,781 - INFO - training batch 851, loss: 0.167, 27232/60000 datapoints
2025-03-06 20:24:44,983 - INFO - training batch 901, loss: 0.091, 28832/60000 datapoints
2025-03-06 20:24:45,177 - INFO - training batch 951, loss: 0.358, 30432/60000 datapoints
2025-03-06 20:24:45,372 - INFO - training batch 1001, loss: 0.287, 32032/60000 datapoints
2025-03-06 20:24:45,565 - INFO - training batch 1051, loss: 0.405, 33632/60000 datapoints
2025-03-06 20:24:45,762 - INFO - training batch 1101, loss: 0.126, 35232/60000 datapoints
2025-03-06 20:24:45,958 - INFO - training batch 1151, loss: 0.164, 36832/60000 datapoints
2025-03-06 20:24:46,152 - INFO - training batch 1201, loss: 0.294, 38432/60000 datapoints
2025-03-06 20:24:46,347 - INFO - training batch 1251, loss: 0.166, 40032/60000 datapoints
2025-03-06 20:24:46,549 - INFO - training batch 1301, loss: 0.194, 41632/60000 datapoints
2025-03-06 20:24:46,746 - INFO - training batch 1351, loss: 0.383, 43232/60000 datapoints
2025-03-06 20:24:46,964 - INFO - training batch 1401, loss: 0.508, 44832/60000 datapoints
2025-03-06 20:24:47,163 - INFO - training batch 1451, loss: 0.124, 46432/60000 datapoints
2025-03-06 20:24:47,359 - INFO - training batch 1501, loss: 0.180, 48032/60000 datapoints
2025-03-06 20:24:47,552 - INFO - training batch 1551, loss: 0.287, 49632/60000 datapoints
2025-03-06 20:24:47,763 - INFO - training batch 1601, loss: 0.130, 51232/60000 datapoints
2025-03-06 20:24:47,988 - INFO - training batch 1651, loss: 0.179, 52832/60000 datapoints
2025-03-06 20:24:48,190 - INFO - training batch 1701, loss: 0.263, 54432/60000 datapoints
2025-03-06 20:24:48,384 - INFO - training batch 1751, loss: 0.189, 56032/60000 datapoints
2025-03-06 20:24:48,577 - INFO - training batch 1801, loss: 0.143, 57632/60000 datapoints
2025-03-06 20:24:48,774 - INFO - training batch 1851, loss: 0.397, 59232/60000 datapoints
2025-03-06 20:24:48,879 - INFO - validation batch 1, loss: 0.340, 32/10016 datapoints
2025-03-06 20:24:49,035 - INFO - validation batch 51, loss: 0.125, 1632/10016 datapoints
2025-03-06 20:24:49,189 - INFO - validation batch 101, loss: 0.365, 3232/10016 datapoints
2025-03-06 20:24:49,345 - INFO - validation batch 151, loss: 0.230, 4832/10016 datapoints
2025-03-06 20:24:49,501 - INFO - validation batch 201, loss: 0.180, 6432/10016 datapoints
2025-03-06 20:24:49,658 - INFO - validation batch 251, loss: 0.191, 8032/10016 datapoints
2025-03-06 20:24:49,814 - INFO - validation batch 301, loss: 0.198, 9632/10016 datapoints
2025-03-06 20:24:49,854 - INFO - Epoch 709/800 done.
2025-03-06 20:24:49,854 - INFO - Final validation performance:
Loss: 0.233, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:24:49,854 - INFO - Beginning epoch 710/800
2025-03-06 20:24:49,861 - INFO - training batch 1, loss: 0.152, 32/60000 datapoints
2025-03-06 20:24:50,060 - INFO - training batch 51, loss: 0.091, 1632/60000 datapoints
2025-03-06 20:24:50,267 - INFO - training batch 101, loss: 0.080, 3232/60000 datapoints
2025-03-06 20:24:50,466 - INFO - training batch 151, loss: 0.549, 4832/60000 datapoints
2025-03-06 20:24:50,679 - INFO - training batch 201, loss: 0.210, 6432/60000 datapoints
2025-03-06 20:24:50,904 - INFO - training batch 251, loss: 0.148, 8032/60000 datapoints
2025-03-06 20:24:51,102 - INFO - training batch 301, loss: 0.145, 9632/60000 datapoints
2025-03-06 20:24:51,298 - INFO - training batch 351, loss: 0.390, 11232/60000 datapoints
2025-03-06 20:24:51,495 - INFO - training batch 401, loss: 0.211, 12832/60000 datapoints
2025-03-06 20:24:51,695 - INFO - training batch 451, loss: 0.242, 14432/60000 datapoints
2025-03-06 20:24:51,893 - INFO - training batch 501, loss: 0.276, 16032/60000 datapoints
2025-03-06 20:24:52,088 - INFO - training batch 551, loss: 0.383, 17632/60000 datapoints
2025-03-06 20:24:52,282 - INFO - training batch 601, loss: 0.085, 19232/60000 datapoints
2025-03-06 20:24:52,479 - INFO - training batch 651, loss: 0.226, 20832/60000 datapoints
2025-03-06 20:24:52,680 - INFO - training batch 701, loss: 0.339, 22432/60000 datapoints
2025-03-06 20:24:52,879 - INFO - training batch 751, loss: 0.142, 24032/60000 datapoints
2025-03-06 20:24:53,075 - INFO - training batch 801, loss: 0.205, 25632/60000 datapoints
2025-03-06 20:24:53,275 - INFO - training batch 851, loss: 0.093, 27232/60000 datapoints
2025-03-06 20:24:53,472 - INFO - training batch 901, loss: 0.154, 28832/60000 datapoints
2025-03-06 20:24:53,668 - INFO - training batch 951, loss: 0.342, 30432/60000 datapoints
2025-03-06 20:24:53,864 - INFO - training batch 1001, loss: 0.147, 32032/60000 datapoints
2025-03-06 20:24:54,063 - INFO - training batch 1051, loss: 0.316, 33632/60000 datapoints
2025-03-06 20:24:54,261 - INFO - training batch 1101, loss: 0.179, 35232/60000 datapoints
2025-03-06 20:24:54,458 - INFO - training batch 1151, loss: 0.080, 36832/60000 datapoints
2025-03-06 20:24:54,658 - INFO - training batch 1201, loss: 0.241, 38432/60000 datapoints
2025-03-06 20:24:54,857 - INFO - training batch 1251, loss: 0.141, 40032/60000 datapoints
2025-03-06 20:24:55,058 - INFO - training batch 1301, loss: 0.088, 41632/60000 datapoints
2025-03-06 20:24:55,252 - INFO - training batch 1351, loss: 0.236, 43232/60000 datapoints
2025-03-06 20:24:55,449 - INFO - training batch 1401, loss: 0.102, 44832/60000 datapoints
2025-03-06 20:24:55,647 - INFO - training batch 1451, loss: 0.207, 46432/60000 datapoints
2025-03-06 20:24:55,845 - INFO - training batch 1501, loss: 0.261, 48032/60000 datapoints
2025-03-06 20:24:56,043 - INFO - training batch 1551, loss: 0.297, 49632/60000 datapoints
2025-03-06 20:24:56,238 - INFO - training batch 1601, loss: 0.377, 51232/60000 datapoints
2025-03-06 20:24:56,436 - INFO - training batch 1651, loss: 0.384, 52832/60000 datapoints
2025-03-06 20:24:56,634 - INFO - training batch 1701, loss: 0.159, 54432/60000 datapoints
2025-03-06 20:24:56,828 - INFO - training batch 1751, loss: 0.112, 56032/60000 datapoints
2025-03-06 20:24:57,045 - INFO - training batch 1801, loss: 0.125, 57632/60000 datapoints
2025-03-06 20:24:57,243 - INFO - training batch 1851, loss: 0.175, 59232/60000 datapoints
2025-03-06 20:24:57,345 - INFO - validation batch 1, loss: 0.231, 32/10016 datapoints
2025-03-06 20:24:57,499 - INFO - validation batch 51, loss: 0.598, 1632/10016 datapoints
2025-03-06 20:24:57,653 - INFO - validation batch 101, loss: 0.129, 3232/10016 datapoints
2025-03-06 20:24:57,805 - INFO - validation batch 151, loss: 0.288, 4832/10016 datapoints
2025-03-06 20:24:57,963 - INFO - validation batch 201, loss: 0.330, 6432/10016 datapoints
2025-03-06 20:24:58,117 - INFO - validation batch 251, loss: 0.175, 8032/10016 datapoints
2025-03-06 20:24:58,272 - INFO - validation batch 301, loss: 0.184, 9632/10016 datapoints
2025-03-06 20:24:58,309 - INFO - Epoch 710/800 done.
2025-03-06 20:24:58,309 - INFO - Final validation performance:
Loss: 0.276, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:24:58,310 - INFO - Beginning epoch 711/800
2025-03-06 20:24:58,317 - INFO - training batch 1, loss: 0.228, 32/60000 datapoints
2025-03-06 20:24:58,533 - INFO - training batch 51, loss: 0.254, 1632/60000 datapoints
2025-03-06 20:24:58,731 - INFO - training batch 101, loss: 0.197, 3232/60000 datapoints
2025-03-06 20:24:58,938 - INFO - training batch 151, loss: 0.279, 4832/60000 datapoints
2025-03-06 20:24:59,138 - INFO - training batch 201, loss: 0.322, 6432/60000 datapoints
2025-03-06 20:24:59,337 - INFO - training batch 251, loss: 0.321, 8032/60000 datapoints
2025-03-06 20:24:59,533 - INFO - training batch 301, loss: 0.463, 9632/60000 datapoints
2025-03-06 20:24:59,732 - INFO - training batch 351, loss: 0.200, 11232/60000 datapoints
2025-03-06 20:24:59,927 - INFO - training batch 401, loss: 0.084, 12832/60000 datapoints
2025-03-06 20:25:00,123 - INFO - training batch 451, loss: 0.199, 14432/60000 datapoints
2025-03-06 20:25:00,319 - INFO - training batch 501, loss: 0.255, 16032/60000 datapoints
2025-03-06 20:25:00,517 - INFO - training batch 551, loss: 0.255, 17632/60000 datapoints
2025-03-06 20:25:00,716 - INFO - training batch 601, loss: 0.153, 19232/60000 datapoints
2025-03-06 20:25:00,914 - INFO - training batch 651, loss: 0.097, 20832/60000 datapoints
2025-03-06 20:25:01,118 - INFO - training batch 701, loss: 0.190, 22432/60000 datapoints
2025-03-06 20:25:01,314 - INFO - training batch 751, loss: 0.312, 24032/60000 datapoints
2025-03-06 20:25:01,510 - INFO - training batch 801, loss: 0.281, 25632/60000 datapoints
2025-03-06 20:25:01,706 - INFO - training batch 851, loss: 0.150, 27232/60000 datapoints
2025-03-06 20:25:01,901 - INFO - training batch 901, loss: 0.325, 28832/60000 datapoints
2025-03-06 20:25:02,100 - INFO - training batch 951, loss: 0.219, 30432/60000 datapoints
2025-03-06 20:25:02,296 - INFO - training batch 1001, loss: 0.105, 32032/60000 datapoints
2025-03-06 20:25:02,493 - INFO - training batch 1051, loss: 0.067, 33632/60000 datapoints
2025-03-06 20:25:02,690 - INFO - training batch 1101, loss: 0.075, 35232/60000 datapoints
2025-03-06 20:25:02,888 - INFO - training batch 1151, loss: 0.410, 36832/60000 datapoints
2025-03-06 20:25:03,086 - INFO - training batch 1201, loss: 0.166, 38432/60000 datapoints
2025-03-06 20:25:03,289 - INFO - training batch 1251, loss: 0.097, 40032/60000 datapoints
2025-03-06 20:25:03,487 - INFO - training batch 1301, loss: 0.130, 41632/60000 datapoints
2025-03-06 20:25:03,687 - INFO - training batch 1351, loss: 0.302, 43232/60000 datapoints
2025-03-06 20:25:03,881 - INFO - training batch 1401, loss: 0.144, 44832/60000 datapoints
2025-03-06 20:25:04,150 - INFO - training batch 1451, loss: 0.259, 46432/60000 datapoints
2025-03-06 20:25:04,346 - INFO - training batch 1501, loss: 0.320, 48032/60000 datapoints
2025-03-06 20:25:04,542 - INFO - training batch 1551, loss: 0.316, 49632/60000 datapoints
2025-03-06 20:25:04,743 - INFO - training batch 1601, loss: 0.307, 51232/60000 datapoints
2025-03-06 20:25:04,947 - INFO - training batch 1651, loss: 0.130, 52832/60000 datapoints
2025-03-06 20:25:05,145 - INFO - training batch 1701, loss: 0.291, 54432/60000 datapoints
2025-03-06 20:25:05,340 - INFO - training batch 1751, loss: 0.145, 56032/60000 datapoints
2025-03-06 20:25:05,535 - INFO - training batch 1801, loss: 0.203, 57632/60000 datapoints
2025-03-06 20:25:05,732 - INFO - training batch 1851, loss: 0.547, 59232/60000 datapoints
2025-03-06 20:25:05,836 - INFO - validation batch 1, loss: 0.279, 32/10016 datapoints
2025-03-06 20:25:05,990 - INFO - validation batch 51, loss: 0.505, 1632/10016 datapoints
2025-03-06 20:25:06,147 - INFO - validation batch 101, loss: 0.187, 3232/10016 datapoints
2025-03-06 20:25:06,302 - INFO - validation batch 151, loss: 0.417, 4832/10016 datapoints
2025-03-06 20:25:06,453 - INFO - validation batch 201, loss: 0.140, 6432/10016 datapoints
2025-03-06 20:25:06,609 - INFO - validation batch 251, loss: 0.198, 8032/10016 datapoints
2025-03-06 20:25:06,763 - INFO - validation batch 301, loss: 0.341, 9632/10016 datapoints
2025-03-06 20:25:06,801 - INFO - Epoch 711/800 done.
2025-03-06 20:25:06,802 - INFO - Final validation performance:
Loss: 0.296, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:25:06,802 - INFO - Beginning epoch 712/800
2025-03-06 20:25:06,809 - INFO - training batch 1, loss: 0.186, 32/60000 datapoints
2025-03-06 20:25:07,030 - INFO - training batch 51, loss: 0.305, 1632/60000 datapoints
2025-03-06 20:25:07,245 - INFO - training batch 101, loss: 0.194, 3232/60000 datapoints
2025-03-06 20:25:07,447 - INFO - training batch 151, loss: 0.150, 4832/60000 datapoints
2025-03-06 20:25:07,654 - INFO - training batch 201, loss: 0.219, 6432/60000 datapoints
2025-03-06 20:25:07,853 - INFO - training batch 251, loss: 0.200, 8032/60000 datapoints
2025-03-06 20:25:08,063 - INFO - training batch 301, loss: 0.104, 9632/60000 datapoints
2025-03-06 20:25:08,261 - INFO - training batch 351, loss: 0.507, 11232/60000 datapoints
2025-03-06 20:25:08,456 - INFO - training batch 401, loss: 0.104, 12832/60000 datapoints
2025-03-06 20:25:08,655 - INFO - training batch 451, loss: 0.150, 14432/60000 datapoints
2025-03-06 20:25:08,848 - INFO - training batch 501, loss: 0.309, 16032/60000 datapoints
2025-03-06 20:25:09,048 - INFO - training batch 551, loss: 0.159, 17632/60000 datapoints
2025-03-06 20:25:09,247 - INFO - training batch 601, loss: 0.235, 19232/60000 datapoints
2025-03-06 20:25:09,444 - INFO - training batch 651, loss: 0.392, 20832/60000 datapoints
2025-03-06 20:25:09,642 - INFO - training batch 701, loss: 0.269, 22432/60000 datapoints
2025-03-06 20:25:09,839 - INFO - training batch 751, loss: 0.177, 24032/60000 datapoints
2025-03-06 20:25:10,032 - INFO - training batch 801, loss: 0.132, 25632/60000 datapoints
2025-03-06 20:25:10,232 - INFO - training batch 851, loss: 0.393, 27232/60000 datapoints
2025-03-06 20:25:10,428 - INFO - training batch 901, loss: 0.176, 28832/60000 datapoints
2025-03-06 20:25:10,623 - INFO - training batch 951, loss: 0.132, 30432/60000 datapoints
2025-03-06 20:25:10,822 - INFO - training batch 1001, loss: 0.283, 32032/60000 datapoints
2025-03-06 20:25:11,022 - INFO - training batch 1051, loss: 0.148, 33632/60000 datapoints
2025-03-06 20:25:11,220 - INFO - training batch 1101, loss: 0.202, 35232/60000 datapoints
2025-03-06 20:25:11,414 - INFO - training batch 1151, loss: 0.236, 36832/60000 datapoints
2025-03-06 20:25:11,610 - INFO - training batch 1201, loss: 0.106, 38432/60000 datapoints
2025-03-06 20:25:11,807 - INFO - training batch 1251, loss: 0.054, 40032/60000 datapoints
2025-03-06 20:25:12,003 - INFO - training batch 1301, loss: 0.476, 41632/60000 datapoints
2025-03-06 20:25:12,202 - INFO - training batch 1351, loss: 0.168, 43232/60000 datapoints
2025-03-06 20:25:12,397 - INFO - training batch 1401, loss: 0.110, 44832/60000 datapoints
2025-03-06 20:25:12,593 - INFO - training batch 1451, loss: 0.055, 46432/60000 datapoints
2025-03-06 20:25:12,790 - INFO - training batch 1501, loss: 0.214, 48032/60000 datapoints
2025-03-06 20:25:12,990 - INFO - training batch 1551, loss: 0.163, 49632/60000 datapoints
2025-03-06 20:25:13,187 - INFO - training batch 1601, loss: 0.107, 51232/60000 datapoints
2025-03-06 20:25:13,385 - INFO - training batch 1651, loss: 0.279, 52832/60000 datapoints
2025-03-06 20:25:13,583 - INFO - training batch 1701, loss: 0.295, 54432/60000 datapoints
2025-03-06 20:25:13,783 - INFO - training batch 1751, loss: 0.460, 56032/60000 datapoints
2025-03-06 20:25:13,976 - INFO - training batch 1801, loss: 0.138, 57632/60000 datapoints
2025-03-06 20:25:14,173 - INFO - training batch 1851, loss: 0.140, 59232/60000 datapoints
2025-03-06 20:25:14,279 - INFO - validation batch 1, loss: 0.157, 32/10016 datapoints
2025-03-06 20:25:14,433 - INFO - validation batch 51, loss: 0.073, 1632/10016 datapoints
2025-03-06 20:25:14,587 - INFO - validation batch 101, loss: 0.088, 3232/10016 datapoints
2025-03-06 20:25:14,745 - INFO - validation batch 151, loss: 0.347, 4832/10016 datapoints
2025-03-06 20:25:14,901 - INFO - validation batch 201, loss: 0.175, 6432/10016 datapoints
2025-03-06 20:25:15,057 - INFO - validation batch 251, loss: 0.110, 8032/10016 datapoints
2025-03-06 20:25:15,213 - INFO - validation batch 301, loss: 0.197, 9632/10016 datapoints
2025-03-06 20:25:15,251 - INFO - Epoch 712/800 done.
2025-03-06 20:25:15,251 - INFO - Final validation performance:
Loss: 0.164, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:25:15,252 - INFO - Beginning epoch 713/800
2025-03-06 20:25:15,258 - INFO - training batch 1, loss: 0.138, 32/60000 datapoints
2025-03-06 20:25:15,459 - INFO - training batch 51, loss: 0.097, 1632/60000 datapoints
2025-03-06 20:25:15,683 - INFO - training batch 101, loss: 0.542, 3232/60000 datapoints
2025-03-06 20:25:15,885 - INFO - training batch 151, loss: 0.347, 4832/60000 datapoints
2025-03-06 20:25:16,090 - INFO - training batch 201, loss: 0.139, 6432/60000 datapoints
2025-03-06 20:25:16,291 - INFO - training batch 251, loss: 0.242, 8032/60000 datapoints
2025-03-06 20:25:16,491 - INFO - training batch 301, loss: 0.204, 9632/60000 datapoints
2025-03-06 20:25:16,692 - INFO - training batch 351, loss: 0.125, 11232/60000 datapoints
2025-03-06 20:25:16,889 - INFO - training batch 401, loss: 0.505, 12832/60000 datapoints
2025-03-06 20:25:17,088 - INFO - training batch 451, loss: 0.197, 14432/60000 datapoints
2025-03-06 20:25:17,302 - INFO - training batch 501, loss: 0.208, 16032/60000 datapoints
2025-03-06 20:25:17,501 - INFO - training batch 551, loss: 0.216, 17632/60000 datapoints
2025-03-06 20:25:17,702 - INFO - training batch 601, loss: 0.154, 19232/60000 datapoints
2025-03-06 20:25:17,897 - INFO - training batch 651, loss: 0.167, 20832/60000 datapoints
2025-03-06 20:25:18,094 - INFO - training batch 701, loss: 0.504, 22432/60000 datapoints
2025-03-06 20:25:18,289 - INFO - training batch 751, loss: 0.279, 24032/60000 datapoints
2025-03-06 20:25:18,489 - INFO - training batch 801, loss: 0.157, 25632/60000 datapoints
2025-03-06 20:25:18,685 - INFO - training batch 851, loss: 0.301, 27232/60000 datapoints
2025-03-06 20:25:18,880 - INFO - training batch 901, loss: 0.178, 28832/60000 datapoints
2025-03-06 20:25:19,080 - INFO - training batch 951, loss: 0.053, 30432/60000 datapoints
2025-03-06 20:25:19,279 - INFO - training batch 1001, loss: 0.337, 32032/60000 datapoints
2025-03-06 20:25:19,474 - INFO - training batch 1051, loss: 0.311, 33632/60000 datapoints
2025-03-06 20:25:19,675 - INFO - training batch 1101, loss: 0.226, 35232/60000 datapoints
2025-03-06 20:25:19,871 - INFO - training batch 1151, loss: 0.199, 36832/60000 datapoints
2025-03-06 20:25:20,066 - INFO - training batch 1201, loss: 0.127, 38432/60000 datapoints
2025-03-06 20:25:20,263 - INFO - training batch 1251, loss: 0.143, 40032/60000 datapoints
2025-03-06 20:25:20,460 - INFO - training batch 1301, loss: 0.089, 41632/60000 datapoints
2025-03-06 20:25:20,655 - INFO - training batch 1351, loss: 0.542, 43232/60000 datapoints
2025-03-06 20:25:20,851 - INFO - training batch 1401, loss: 0.147, 44832/60000 datapoints
2025-03-06 20:25:21,050 - INFO - training batch 1451, loss: 0.161, 46432/60000 datapoints
2025-03-06 20:25:21,248 - INFO - training batch 1501, loss: 0.195, 48032/60000 datapoints
2025-03-06 20:25:21,448 - INFO - training batch 1551, loss: 0.279, 49632/60000 datapoints
2025-03-06 20:25:21,644 - INFO - training batch 1601, loss: 0.109, 51232/60000 datapoints
2025-03-06 20:25:21,840 - INFO - training batch 1651, loss: 0.282, 52832/60000 datapoints
2025-03-06 20:25:22,037 - INFO - training batch 1701, loss: 0.537, 54432/60000 datapoints
2025-03-06 20:25:22,235 - INFO - training batch 1751, loss: 0.145, 56032/60000 datapoints
2025-03-06 20:25:22,432 - INFO - training batch 1801, loss: 0.268, 57632/60000 datapoints
2025-03-06 20:25:22,633 - INFO - training batch 1851, loss: 0.239, 59232/60000 datapoints
2025-03-06 20:25:22,737 - INFO - validation batch 1, loss: 0.174, 32/10016 datapoints
2025-03-06 20:25:22,889 - INFO - validation batch 51, loss: 0.185, 1632/10016 datapoints
2025-03-06 20:25:23,044 - INFO - validation batch 101, loss: 0.153, 3232/10016 datapoints
2025-03-06 20:25:23,199 - INFO - validation batch 151, loss: 0.536, 4832/10016 datapoints
2025-03-06 20:25:23,356 - INFO - validation batch 201, loss: 0.200, 6432/10016 datapoints
2025-03-06 20:25:23,510 - INFO - validation batch 251, loss: 0.094, 8032/10016 datapoints
2025-03-06 20:25:23,666 - INFO - validation batch 301, loss: 0.175, 9632/10016 datapoints
2025-03-06 20:25:23,705 - INFO - Epoch 713/800 done.
2025-03-06 20:25:23,706 - INFO - Final validation performance:
Loss: 0.217, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:25:23,706 - INFO - Beginning epoch 714/800
2025-03-06 20:25:23,713 - INFO - training batch 1, loss: 0.106, 32/60000 datapoints
2025-03-06 20:25:23,913 - INFO - training batch 51, loss: 0.361, 1632/60000 datapoints
2025-03-06 20:25:24,108 - INFO - training batch 101, loss: 0.085, 3232/60000 datapoints
2025-03-06 20:25:24,311 - INFO - training batch 151, loss: 0.251, 4832/60000 datapoints
2025-03-06 20:25:24,508 - INFO - training batch 201, loss: 0.379, 6432/60000 datapoints
2025-03-06 20:25:24,706 - INFO - training batch 251, loss: 0.120, 8032/60000 datapoints
2025-03-06 20:25:24,908 - INFO - training batch 301, loss: 0.143, 9632/60000 datapoints
2025-03-06 20:25:25,106 - INFO - training batch 351, loss: 0.227, 11232/60000 datapoints
2025-03-06 20:25:25,306 - INFO - training batch 401, loss: 0.238, 12832/60000 datapoints
2025-03-06 20:25:25,498 - INFO - training batch 451, loss: 0.153, 14432/60000 datapoints
2025-03-06 20:25:25,705 - INFO - training batch 501, loss: 0.154, 16032/60000 datapoints
2025-03-06 20:25:25,910 - INFO - training batch 551, loss: 0.139, 17632/60000 datapoints
2025-03-06 20:25:26,108 - INFO - training batch 601, loss: 0.169, 19232/60000 datapoints
2025-03-06 20:25:26,307 - INFO - training batch 651, loss: 0.179, 20832/60000 datapoints
2025-03-06 20:25:26,506 - INFO - training batch 701, loss: 0.106, 22432/60000 datapoints
2025-03-06 20:25:26,704 - INFO - training batch 751, loss: 0.320, 24032/60000 datapoints
2025-03-06 20:25:26,898 - INFO - training batch 801, loss: 0.235, 25632/60000 datapoints
2025-03-06 20:25:27,099 - INFO - training batch 851, loss: 0.056, 27232/60000 datapoints
2025-03-06 20:25:27,316 - INFO - training batch 901, loss: 0.238, 28832/60000 datapoints
2025-03-06 20:25:27,512 - INFO - training batch 951, loss: 0.331, 30432/60000 datapoints
2025-03-06 20:25:27,707 - INFO - training batch 1001, loss: 0.176, 32032/60000 datapoints
2025-03-06 20:25:27,900 - INFO - training batch 1051, loss: 0.147, 33632/60000 datapoints
2025-03-06 20:25:28,096 - INFO - training batch 1101, loss: 0.149, 35232/60000 datapoints
2025-03-06 20:25:28,292 - INFO - training batch 1151, loss: 0.451, 36832/60000 datapoints
2025-03-06 20:25:28,486 - INFO - training batch 1201, loss: 0.095, 38432/60000 datapoints
2025-03-06 20:25:28,686 - INFO - training batch 1251, loss: 0.082, 40032/60000 datapoints
2025-03-06 20:25:28,883 - INFO - training batch 1301, loss: 0.529, 41632/60000 datapoints
2025-03-06 20:25:29,087 - INFO - training batch 1351, loss: 0.126, 43232/60000 datapoints
2025-03-06 20:25:29,284 - INFO - training batch 1401, loss: 0.126, 44832/60000 datapoints
2025-03-06 20:25:29,490 - INFO - training batch 1451, loss: 0.135, 46432/60000 datapoints
2025-03-06 20:25:29,691 - INFO - training batch 1501, loss: 0.197, 48032/60000 datapoints
2025-03-06 20:25:29,889 - INFO - training batch 1551, loss: 0.051, 49632/60000 datapoints
2025-03-06 20:25:30,108 - INFO - training batch 1601, loss: 0.134, 51232/60000 datapoints
2025-03-06 20:25:30,313 - INFO - training batch 1651, loss: 0.153, 52832/60000 datapoints
2025-03-06 20:25:30,507 - INFO - training batch 1701, loss: 0.432, 54432/60000 datapoints
2025-03-06 20:25:30,715 - INFO - training batch 1751, loss: 0.136, 56032/60000 datapoints
2025-03-06 20:25:30,915 - INFO - training batch 1801, loss: 0.138, 57632/60000 datapoints
2025-03-06 20:25:31,117 - INFO - training batch 1851, loss: 0.221, 59232/60000 datapoints
2025-03-06 20:25:31,220 - INFO - validation batch 1, loss: 0.072, 32/10016 datapoints
2025-03-06 20:25:31,376 - INFO - validation batch 51, loss: 0.069, 1632/10016 datapoints
2025-03-06 20:25:31,532 - INFO - validation batch 101, loss: 0.235, 3232/10016 datapoints
2025-03-06 20:25:31,689 - INFO - validation batch 151, loss: 0.144, 4832/10016 datapoints
2025-03-06 20:25:31,846 - INFO - validation batch 201, loss: 0.139, 6432/10016 datapoints
2025-03-06 20:25:32,000 - INFO - validation batch 251, loss: 0.290, 8032/10016 datapoints
2025-03-06 20:25:32,155 - INFO - validation batch 301, loss: 0.290, 9632/10016 datapoints
2025-03-06 20:25:32,193 - INFO - Epoch 714/800 done.
2025-03-06 20:25:32,193 - INFO - Final validation performance:
Loss: 0.177, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:25:32,194 - INFO - Beginning epoch 715/800
2025-03-06 20:25:32,200 - INFO - training batch 1, loss: 0.364, 32/60000 datapoints
2025-03-06 20:25:32,400 - INFO - training batch 51, loss: 0.076, 1632/60000 datapoints
2025-03-06 20:25:32,596 - INFO - training batch 101, loss: 0.233, 3232/60000 datapoints
2025-03-06 20:25:32,797 - INFO - training batch 151, loss: 0.089, 4832/60000 datapoints
2025-03-06 20:25:32,997 - INFO - training batch 201, loss: 0.176, 6432/60000 datapoints
2025-03-06 20:25:33,195 - INFO - training batch 251, loss: 0.190, 8032/60000 datapoints
2025-03-06 20:25:33,393 - INFO - training batch 301, loss: 0.319, 9632/60000 datapoints
2025-03-06 20:25:33,584 - INFO - training batch 351, loss: 0.151, 11232/60000 datapoints
2025-03-06 20:25:33,779 - INFO - training batch 401, loss: 0.060, 12832/60000 datapoints
2025-03-06 20:25:33,976 - INFO - training batch 451, loss: 0.254, 14432/60000 datapoints
2025-03-06 20:25:34,169 - INFO - training batch 501, loss: 0.050, 16032/60000 datapoints
2025-03-06 20:25:34,364 - INFO - training batch 551, loss: 0.085, 17632/60000 datapoints
2025-03-06 20:25:34,560 - INFO - training batch 601, loss: 0.054, 19232/60000 datapoints
2025-03-06 20:25:34,757 - INFO - training batch 651, loss: 0.139, 20832/60000 datapoints
2025-03-06 20:25:34,956 - INFO - training batch 701, loss: 0.170, 22432/60000 datapoints
2025-03-06 20:25:35,158 - INFO - training batch 751, loss: 0.110, 24032/60000 datapoints
2025-03-06 20:25:35,353 - INFO - training batch 801, loss: 0.156, 25632/60000 datapoints
2025-03-06 20:25:35,549 - INFO - training batch 851, loss: 0.490, 27232/60000 datapoints
2025-03-06 20:25:35,748 - INFO - training batch 901, loss: 0.186, 28832/60000 datapoints
2025-03-06 20:25:35,946 - INFO - training batch 951, loss: 0.174, 30432/60000 datapoints
2025-03-06 20:25:36,142 - INFO - training batch 1001, loss: 0.093, 32032/60000 datapoints
2025-03-06 20:25:36,338 - INFO - training batch 1051, loss: 0.182, 33632/60000 datapoints
2025-03-06 20:25:36,546 - INFO - training batch 1101, loss: 0.091, 35232/60000 datapoints
2025-03-06 20:25:36,745 - INFO - training batch 1151, loss: 0.399, 36832/60000 datapoints
2025-03-06 20:25:36,945 - INFO - training batch 1201, loss: 0.324, 38432/60000 datapoints
2025-03-06 20:25:37,150 - INFO - training batch 1251, loss: 0.119, 40032/60000 datapoints
2025-03-06 20:25:37,370 - INFO - training batch 1301, loss: 0.381, 41632/60000 datapoints
2025-03-06 20:25:37,566 - INFO - training batch 1351, loss: 0.299, 43232/60000 datapoints
2025-03-06 20:25:37,772 - INFO - training batch 1401, loss: 0.054, 44832/60000 datapoints
2025-03-06 20:25:37,970 - INFO - training batch 1451, loss: 0.289, 46432/60000 datapoints
2025-03-06 20:25:38,164 - INFO - training batch 1501, loss: 0.208, 48032/60000 datapoints
2025-03-06 20:25:38,359 - INFO - training batch 1551, loss: 0.420, 49632/60000 datapoints
2025-03-06 20:25:38,559 - INFO - training batch 1601, loss: 0.214, 51232/60000 datapoints
2025-03-06 20:25:38,756 - INFO - training batch 1651, loss: 0.229, 52832/60000 datapoints
2025-03-06 20:25:38,951 - INFO - training batch 1701, loss: 0.092, 54432/60000 datapoints
2025-03-06 20:25:39,154 - INFO - training batch 1751, loss: 0.132, 56032/60000 datapoints
2025-03-06 20:25:39,358 - INFO - training batch 1801, loss: 0.091, 57632/60000 datapoints
2025-03-06 20:25:39,557 - INFO - training batch 1851, loss: 0.445, 59232/60000 datapoints
2025-03-06 20:25:39,662 - INFO - validation batch 1, loss: 0.201, 32/10016 datapoints
2025-03-06 20:25:39,816 - INFO - validation batch 51, loss: 0.643, 1632/10016 datapoints
2025-03-06 20:25:39,970 - INFO - validation batch 101, loss: 0.085, 3232/10016 datapoints
2025-03-06 20:25:40,123 - INFO - validation batch 151, loss: 0.468, 4832/10016 datapoints
2025-03-06 20:25:40,304 - INFO - validation batch 201, loss: 0.137, 6432/10016 datapoints
2025-03-06 20:25:40,470 - INFO - validation batch 251, loss: 0.327, 8032/10016 datapoints
2025-03-06 20:25:40,629 - INFO - validation batch 301, loss: 0.108, 9632/10016 datapoints
2025-03-06 20:25:40,666 - INFO - Epoch 715/800 done.
2025-03-06 20:25:40,666 - INFO - Final validation performance:
Loss: 0.281, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:25:40,667 - INFO - Beginning epoch 716/800
2025-03-06 20:25:40,675 - INFO - training batch 1, loss: 0.173, 32/60000 datapoints
2025-03-06 20:25:40,880 - INFO - training batch 51, loss: 0.184, 1632/60000 datapoints
2025-03-06 20:25:41,088 - INFO - training batch 101, loss: 0.570, 3232/60000 datapoints
2025-03-06 20:25:41,291 - INFO - training batch 151, loss: 0.403, 4832/60000 datapoints
2025-03-06 20:25:41,489 - INFO - training batch 201, loss: 0.180, 6432/60000 datapoints
2025-03-06 20:25:41,689 - INFO - training batch 251, loss: 0.261, 8032/60000 datapoints
2025-03-06 20:25:41,885 - INFO - training batch 301, loss: 0.040, 9632/60000 datapoints
2025-03-06 20:25:42,082 - INFO - training batch 351, loss: 0.242, 11232/60000 datapoints
2025-03-06 20:25:42,277 - INFO - training batch 401, loss: 0.114, 12832/60000 datapoints
2025-03-06 20:25:42,473 - INFO - training batch 451, loss: 0.404, 14432/60000 datapoints
2025-03-06 20:25:42,683 - INFO - training batch 501, loss: 0.131, 16032/60000 datapoints
2025-03-06 20:25:42,877 - INFO - training batch 551, loss: 0.169, 17632/60000 datapoints
2025-03-06 20:25:43,080 - INFO - training batch 601, loss: 0.203, 19232/60000 datapoints
2025-03-06 20:25:43,281 - INFO - training batch 651, loss: 0.210, 20832/60000 datapoints
2025-03-06 20:25:43,481 - INFO - training batch 701, loss: 0.158, 22432/60000 datapoints
2025-03-06 20:25:43,679 - INFO - training batch 751, loss: 0.166, 24032/60000 datapoints
2025-03-06 20:25:43,871 - INFO - training batch 801, loss: 0.096, 25632/60000 datapoints
2025-03-06 20:25:44,067 - INFO - training batch 851, loss: 0.446, 27232/60000 datapoints
2025-03-06 20:25:44,264 - INFO - training batch 901, loss: 0.149, 28832/60000 datapoints
2025-03-06 20:25:44,459 - INFO - training batch 951, loss: 0.365, 30432/60000 datapoints
2025-03-06 20:25:44,653 - INFO - training batch 1001, loss: 0.068, 32032/60000 datapoints
2025-03-06 20:25:44,849 - INFO - training batch 1051, loss: 0.224, 33632/60000 datapoints
2025-03-06 20:25:45,055 - INFO - training batch 1101, loss: 0.358, 35232/60000 datapoints
2025-03-06 20:25:45,252 - INFO - training batch 1151, loss: 0.207, 36832/60000 datapoints
2025-03-06 20:25:45,448 - INFO - training batch 1201, loss: 0.147, 38432/60000 datapoints
2025-03-06 20:25:45,647 - INFO - training batch 1251, loss: 0.155, 40032/60000 datapoints
2025-03-06 20:25:45,845 - INFO - training batch 1301, loss: 0.420, 41632/60000 datapoints
2025-03-06 20:25:46,045 - INFO - training batch 1351, loss: 0.189, 43232/60000 datapoints
2025-03-06 20:25:46,244 - INFO - training batch 1401, loss: 0.098, 44832/60000 datapoints
2025-03-06 20:25:46,440 - INFO - training batch 1451, loss: 0.322, 46432/60000 datapoints
2025-03-06 20:25:46,638 - INFO - training batch 1501, loss: 0.128, 48032/60000 datapoints
2025-03-06 20:25:46,835 - INFO - training batch 1551, loss: 0.167, 49632/60000 datapoints
2025-03-06 20:25:47,031 - INFO - training batch 1601, loss: 0.368, 51232/60000 datapoints
2025-03-06 20:25:47,233 - INFO - training batch 1651, loss: 0.507, 52832/60000 datapoints
2025-03-06 20:25:47,448 - INFO - training batch 1701, loss: 0.328, 54432/60000 datapoints
2025-03-06 20:25:47,649 - INFO - training batch 1751, loss: 0.078, 56032/60000 datapoints
2025-03-06 20:25:47,843 - INFO - training batch 1801, loss: 0.169, 57632/60000 datapoints
2025-03-06 20:25:48,040 - INFO - training batch 1851, loss: 0.329, 59232/60000 datapoints
2025-03-06 20:25:48,141 - INFO - validation batch 1, loss: 0.142, 32/10016 datapoints
2025-03-06 20:25:48,300 - INFO - validation batch 51, loss: 0.106, 1632/10016 datapoints
2025-03-06 20:25:48,455 - INFO - validation batch 101, loss: 0.192, 3232/10016 datapoints
2025-03-06 20:25:48,610 - INFO - validation batch 151, loss: 0.215, 4832/10016 datapoints
2025-03-06 20:25:48,764 - INFO - validation batch 201, loss: 0.195, 6432/10016 datapoints
2025-03-06 20:25:48,919 - INFO - validation batch 251, loss: 0.297, 8032/10016 datapoints
2025-03-06 20:25:49,075 - INFO - validation batch 301, loss: 0.504, 9632/10016 datapoints
2025-03-06 20:25:49,112 - INFO - Epoch 716/800 done.
2025-03-06 20:25:49,112 - INFO - Final validation performance:
Loss: 0.236, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:25:49,113 - INFO - Beginning epoch 717/800
2025-03-06 20:25:49,120 - INFO - training batch 1, loss: 0.054, 32/60000 datapoints
2025-03-06 20:25:49,320 - INFO - training batch 51, loss: 0.047, 1632/60000 datapoints
2025-03-06 20:25:49,528 - INFO - training batch 101, loss: 0.103, 3232/60000 datapoints
2025-03-06 20:25:49,730 - INFO - training batch 151, loss: 0.071, 4832/60000 datapoints
2025-03-06 20:25:49,929 - INFO - training batch 201, loss: 0.145, 6432/60000 datapoints
2025-03-06 20:25:50,132 - INFO - training batch 251, loss: 0.113, 8032/60000 datapoints
2025-03-06 20:25:50,331 - INFO - training batch 301, loss: 0.082, 9632/60000 datapoints
2025-03-06 20:25:50,526 - INFO - training batch 351, loss: 0.052, 11232/60000 datapoints
2025-03-06 20:25:50,726 - INFO - training batch 401, loss: 0.110, 12832/60000 datapoints
2025-03-06 20:25:50,923 - INFO - training batch 451, loss: 0.305, 14432/60000 datapoints
2025-03-06 20:25:51,123 - INFO - training batch 501, loss: 0.092, 16032/60000 datapoints
2025-03-06 20:25:51,317 - INFO - training batch 551, loss: 0.185, 17632/60000 datapoints
2025-03-06 20:25:51,513 - INFO - training batch 601, loss: 0.178, 19232/60000 datapoints
2025-03-06 20:25:51,710 - INFO - training batch 651, loss: 0.246, 20832/60000 datapoints
2025-03-06 20:25:51,908 - INFO - training batch 701, loss: 0.126, 22432/60000 datapoints
2025-03-06 20:25:52,105 - INFO - training batch 751, loss: 0.091, 24032/60000 datapoints
2025-03-06 20:25:52,303 - INFO - training batch 801, loss: 0.054, 25632/60000 datapoints
2025-03-06 20:25:52,498 - INFO - training batch 851, loss: 0.215, 27232/60000 datapoints
2025-03-06 20:25:52,697 - INFO - training batch 901, loss: 0.135, 28832/60000 datapoints
2025-03-06 20:25:52,889 - INFO - training batch 951, loss: 0.259, 30432/60000 datapoints
2025-03-06 20:25:53,086 - INFO - training batch 1001, loss: 0.220, 32032/60000 datapoints
2025-03-06 20:25:53,285 - INFO - training batch 1051, loss: 0.053, 33632/60000 datapoints
2025-03-06 20:25:53,485 - INFO - training batch 1101, loss: 0.477, 35232/60000 datapoints
2025-03-06 20:25:53,687 - INFO - training batch 1151, loss: 0.177, 36832/60000 datapoints
2025-03-06 20:25:53,885 - INFO - training batch 1201, loss: 0.090, 38432/60000 datapoints
2025-03-06 20:25:54,080 - INFO - training batch 1251, loss: 0.125, 40032/60000 datapoints
2025-03-06 20:25:54,278 - INFO - training batch 1301, loss: 0.165, 41632/60000 datapoints
2025-03-06 20:25:54,474 - INFO - training batch 1351, loss: 0.250, 43232/60000 datapoints
2025-03-06 20:25:54,669 - INFO - training batch 1401, loss: 0.102, 44832/60000 datapoints
2025-03-06 20:25:54,865 - INFO - training batch 1451, loss: 0.294, 46432/60000 datapoints
2025-03-06 20:25:55,066 - INFO - training batch 1501, loss: 0.183, 48032/60000 datapoints
2025-03-06 20:25:55,264 - INFO - training batch 1551, loss: 0.087, 49632/60000 datapoints
2025-03-06 20:25:55,458 - INFO - training batch 1601, loss: 0.355, 51232/60000 datapoints
2025-03-06 20:25:55,655 - INFO - training batch 1651, loss: 0.437, 52832/60000 datapoints
2025-03-06 20:25:55,852 - INFO - training batch 1701, loss: 0.172, 54432/60000 datapoints
2025-03-06 20:25:56,053 - INFO - training batch 1751, loss: 0.296, 56032/60000 datapoints
2025-03-06 20:25:56,252 - INFO - training batch 1801, loss: 0.297, 57632/60000 datapoints
2025-03-06 20:25:56,448 - INFO - training batch 1851, loss: 0.235, 59232/60000 datapoints
2025-03-06 20:25:56,550 - INFO - validation batch 1, loss: 0.321, 32/10016 datapoints
2025-03-06 20:25:56,705 - INFO - validation batch 51, loss: 0.261, 1632/10016 datapoints
2025-03-06 20:25:56,858 - INFO - validation batch 101, loss: 0.235, 3232/10016 datapoints
2025-03-06 20:25:57,010 - INFO - validation batch 151, loss: 0.073, 4832/10016 datapoints
2025-03-06 20:25:57,168 - INFO - validation batch 201, loss: 0.388, 6432/10016 datapoints
2025-03-06 20:25:57,321 - INFO - validation batch 251, loss: 0.362, 8032/10016 datapoints
2025-03-06 20:25:57,482 - INFO - validation batch 301, loss: 0.300, 9632/10016 datapoints
2025-03-06 20:25:57,530 - INFO - Epoch 717/800 done.
2025-03-06 20:25:57,531 - INFO - Final validation performance:
Loss: 0.277, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:25:57,531 - INFO - Beginning epoch 718/800
2025-03-06 20:25:57,538 - INFO - training batch 1, loss: 0.049, 32/60000 datapoints
2025-03-06 20:25:57,740 - INFO - training batch 51, loss: 0.415, 1632/60000 datapoints
2025-03-06 20:25:57,936 - INFO - training batch 101, loss: 0.109, 3232/60000 datapoints
2025-03-06 20:25:58,145 - INFO - training batch 151, loss: 0.164, 4832/60000 datapoints
2025-03-06 20:25:58,343 - INFO - training batch 201, loss: 0.165, 6432/60000 datapoints
2025-03-06 20:25:58,546 - INFO - training batch 251, loss: 0.190, 8032/60000 datapoints
2025-03-06 20:25:58,752 - INFO - training batch 301, loss: 0.147, 9632/60000 datapoints
2025-03-06 20:25:58,948 - INFO - training batch 351, loss: 0.166, 11232/60000 datapoints
2025-03-06 20:25:59,144 - INFO - training batch 401, loss: 0.241, 12832/60000 datapoints
2025-03-06 20:25:59,339 - INFO - training batch 451, loss: 0.607, 14432/60000 datapoints
2025-03-06 20:25:59,534 - INFO - training batch 501, loss: 0.096, 16032/60000 datapoints
2025-03-06 20:25:59,733 - INFO - training batch 551, loss: 0.164, 17632/60000 datapoints
2025-03-06 20:25:59,930 - INFO - training batch 601, loss: 0.047, 19232/60000 datapoints
2025-03-06 20:26:00,126 - INFO - training batch 651, loss: 0.179, 20832/60000 datapoints
2025-03-06 20:26:00,321 - INFO - training batch 701, loss: 0.307, 22432/60000 datapoints
2025-03-06 20:26:00,516 - INFO - training batch 751, loss: 0.252, 24032/60000 datapoints
2025-03-06 20:26:00,716 - INFO - training batch 801, loss: 0.207, 25632/60000 datapoints
2025-03-06 20:26:00,906 - INFO - training batch 851, loss: 0.070, 27232/60000 datapoints
2025-03-06 20:26:01,103 - INFO - training batch 901, loss: 0.161, 28832/60000 datapoints
2025-03-06 20:26:01,304 - INFO - training batch 951, loss: 0.292, 30432/60000 datapoints
2025-03-06 20:26:01,498 - INFO - training batch 1001, loss: 0.101, 32032/60000 datapoints
2025-03-06 20:26:01,699 - INFO - training batch 1051, loss: 0.122, 33632/60000 datapoints
2025-03-06 20:26:01,894 - INFO - training batch 1101, loss: 0.121, 35232/60000 datapoints
2025-03-06 20:26:02,087 - INFO - training batch 1151, loss: 0.087, 36832/60000 datapoints
2025-03-06 20:26:02,282 - INFO - training batch 1201, loss: 0.234, 38432/60000 datapoints
2025-03-06 20:26:02,477 - INFO - training batch 1251, loss: 0.175, 40032/60000 datapoints
2025-03-06 20:26:02,672 - INFO - training batch 1301, loss: 0.070, 41632/60000 datapoints
2025-03-06 20:26:02,866 - INFO - training batch 1351, loss: 0.150, 43232/60000 datapoints
2025-03-06 20:26:03,059 - INFO - training batch 1401, loss: 0.096, 44832/60000 datapoints
2025-03-06 20:26:03,259 - INFO - training batch 1451, loss: 0.154, 46432/60000 datapoints
2025-03-06 20:26:03,455 - INFO - training batch 1501, loss: 0.312, 48032/60000 datapoints
2025-03-06 20:26:03,656 - INFO - training batch 1551, loss: 0.229, 49632/60000 datapoints
2025-03-06 20:26:03,849 - INFO - training batch 1601, loss: 0.161, 51232/60000 datapoints
2025-03-06 20:26:04,043 - INFO - training batch 1651, loss: 0.276, 52832/60000 datapoints
2025-03-06 20:26:04,239 - INFO - training batch 1701, loss: 0.216, 54432/60000 datapoints
2025-03-06 20:26:04,434 - INFO - training batch 1751, loss: 0.222, 56032/60000 datapoints
2025-03-06 20:26:04,626 - INFO - training batch 1801, loss: 0.363, 57632/60000 datapoints
2025-03-06 20:26:04,823 - INFO - training batch 1851, loss: 0.322, 59232/60000 datapoints
2025-03-06 20:26:04,931 - INFO - validation batch 1, loss: 0.313, 32/10016 datapoints
2025-03-06 20:26:05,085 - INFO - validation batch 51, loss: 0.460, 1632/10016 datapoints
2025-03-06 20:26:05,244 - INFO - validation batch 101, loss: 0.486, 3232/10016 datapoints
2025-03-06 20:26:05,400 - INFO - validation batch 151, loss: 0.176, 4832/10016 datapoints
2025-03-06 20:26:05,552 - INFO - validation batch 201, loss: 0.118, 6432/10016 datapoints
2025-03-06 20:26:05,709 - INFO - validation batch 251, loss: 0.153, 8032/10016 datapoints
2025-03-06 20:26:05,861 - INFO - validation batch 301, loss: 0.496, 9632/10016 datapoints
2025-03-06 20:26:05,898 - INFO - Epoch 718/800 done.
2025-03-06 20:26:05,899 - INFO - Final validation performance:
Loss: 0.314, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:26:05,899 - INFO - Beginning epoch 719/800
2025-03-06 20:26:05,906 - INFO - training batch 1, loss: 0.074, 32/60000 datapoints
2025-03-06 20:26:06,105 - INFO - training batch 51, loss: 0.153, 1632/60000 datapoints
2025-03-06 20:26:06,302 - INFO - training batch 101, loss: 0.097, 3232/60000 datapoints
2025-03-06 20:26:06,505 - INFO - training batch 151, loss: 0.046, 4832/60000 datapoints
2025-03-06 20:26:06,703 - INFO - training batch 201, loss: 0.173, 6432/60000 datapoints
2025-03-06 20:26:06,899 - INFO - training batch 251, loss: 0.140, 8032/60000 datapoints
2025-03-06 20:26:07,101 - INFO - training batch 301, loss: 0.398, 9632/60000 datapoints
2025-03-06 20:26:07,302 - INFO - training batch 351, loss: 0.193, 11232/60000 datapoints
2025-03-06 20:26:07,500 - INFO - training batch 401, loss: 0.109, 12832/60000 datapoints
2025-03-06 20:26:07,720 - INFO - training batch 451, loss: 0.412, 14432/60000 datapoints
2025-03-06 20:26:07,915 - INFO - training batch 501, loss: 0.482, 16032/60000 datapoints
2025-03-06 20:26:08,114 - INFO - training batch 551, loss: 0.195, 17632/60000 datapoints
2025-03-06 20:26:08,308 - INFO - training batch 601, loss: 0.071, 19232/60000 datapoints
2025-03-06 20:26:08,500 - INFO - training batch 651, loss: 0.106, 20832/60000 datapoints
2025-03-06 20:26:08,697 - INFO - training batch 701, loss: 0.306, 22432/60000 datapoints
2025-03-06 20:26:08,897 - INFO - training batch 751, loss: 0.189, 24032/60000 datapoints
2025-03-06 20:26:09,103 - INFO - training batch 801, loss: 0.075, 25632/60000 datapoints
2025-03-06 20:26:09,297 - INFO - training batch 851, loss: 0.150, 27232/60000 datapoints
2025-03-06 20:26:09,494 - INFO - training batch 901, loss: 0.119, 28832/60000 datapoints
2025-03-06 20:26:09,692 - INFO - training batch 951, loss: 0.121, 30432/60000 datapoints
2025-03-06 20:26:09,892 - INFO - training batch 1001, loss: 0.255, 32032/60000 datapoints
2025-03-06 20:26:10,087 - INFO - training batch 1051, loss: 0.150, 33632/60000 datapoints
2025-03-06 20:26:10,284 - INFO - training batch 1101, loss: 0.154, 35232/60000 datapoints
2025-03-06 20:26:10,482 - INFO - training batch 1151, loss: 0.126, 36832/60000 datapoints
2025-03-06 20:26:10,679 - INFO - training batch 1201, loss: 0.046, 38432/60000 datapoints
2025-03-06 20:26:10,875 - INFO - training batch 1251, loss: 0.208, 40032/60000 datapoints
2025-03-06 20:26:11,069 - INFO - training batch 1301, loss: 0.306, 41632/60000 datapoints
2025-03-06 20:26:11,266 - INFO - training batch 1351, loss: 0.417, 43232/60000 datapoints
2025-03-06 20:26:11,463 - INFO - training batch 1401, loss: 0.217, 44832/60000 datapoints
2025-03-06 20:26:11,660 - INFO - training batch 1451, loss: 0.265, 46432/60000 datapoints
2025-03-06 20:26:11,853 - INFO - training batch 1501, loss: 0.148, 48032/60000 datapoints
2025-03-06 20:26:12,050 - INFO - training batch 1551, loss: 0.191, 49632/60000 datapoints
2025-03-06 20:26:12,247 - INFO - training batch 1601, loss: 0.109, 51232/60000 datapoints
2025-03-06 20:26:12,440 - INFO - training batch 1651, loss: 0.142, 52832/60000 datapoints
2025-03-06 20:26:12,638 - INFO - training batch 1701, loss: 0.294, 54432/60000 datapoints
2025-03-06 20:26:12,833 - INFO - training batch 1751, loss: 0.354, 56032/60000 datapoints
2025-03-06 20:26:13,029 - INFO - training batch 1801, loss: 0.214, 57632/60000 datapoints
2025-03-06 20:26:13,231 - INFO - training batch 1851, loss: 0.099, 59232/60000 datapoints
2025-03-06 20:26:13,333 - INFO - validation batch 1, loss: 0.176, 32/10016 datapoints
2025-03-06 20:26:13,508 - INFO - validation batch 51, loss: 0.116, 1632/10016 datapoints
2025-03-06 20:26:13,680 - INFO - validation batch 101, loss: 0.142, 3232/10016 datapoints
2025-03-06 20:26:13,835 - INFO - validation batch 151, loss: 0.102, 4832/10016 datapoints
2025-03-06 20:26:13,989 - INFO - validation batch 201, loss: 0.300, 6432/10016 datapoints
2025-03-06 20:26:14,145 - INFO - validation batch 251, loss: 0.231, 8032/10016 datapoints
2025-03-06 20:26:14,299 - INFO - validation batch 301, loss: 0.305, 9632/10016 datapoints
2025-03-06 20:26:14,337 - INFO - Epoch 719/800 done.
2025-03-06 20:26:14,337 - INFO - Final validation performance:
Loss: 0.196, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:26:14,338 - INFO - Beginning epoch 720/800
2025-03-06 20:26:14,345 - INFO - training batch 1, loss: 0.162, 32/60000 datapoints
2025-03-06 20:26:14,553 - INFO - training batch 51, loss: 0.317, 1632/60000 datapoints
2025-03-06 20:26:14,753 - INFO - training batch 101, loss: 0.270, 3232/60000 datapoints
2025-03-06 20:26:14,956 - INFO - training batch 151, loss: 0.154, 4832/60000 datapoints
2025-03-06 20:26:15,159 - INFO - training batch 201, loss: 0.190, 6432/60000 datapoints
2025-03-06 20:26:15,361 - INFO - training batch 251, loss: 0.367, 8032/60000 datapoints
2025-03-06 20:26:15,557 - INFO - training batch 301, loss: 0.453, 9632/60000 datapoints
2025-03-06 20:26:15,754 - INFO - training batch 351, loss: 0.057, 11232/60000 datapoints
2025-03-06 20:26:15,954 - INFO - training batch 401, loss: 0.258, 12832/60000 datapoints
2025-03-06 20:26:16,157 - INFO - training batch 451, loss: 0.432, 14432/60000 datapoints
2025-03-06 20:26:16,352 - INFO - training batch 501, loss: 0.181, 16032/60000 datapoints
2025-03-06 20:26:16,548 - INFO - training batch 551, loss: 0.134, 17632/60000 datapoints
2025-03-06 20:26:16,744 - INFO - training batch 601, loss: 0.154, 19232/60000 datapoints
2025-03-06 20:26:16,942 - INFO - training batch 651, loss: 0.414, 20832/60000 datapoints
2025-03-06 20:26:17,142 - INFO - training batch 701, loss: 0.226, 22432/60000 datapoints
2025-03-06 20:26:17,340 - INFO - training batch 751, loss: 0.184, 24032/60000 datapoints
2025-03-06 20:26:17,535 - INFO - training batch 801, loss: 0.212, 25632/60000 datapoints
2025-03-06 20:26:17,756 - INFO - training batch 851, loss: 0.122, 27232/60000 datapoints
2025-03-06 20:26:17,954 - INFO - training batch 901, loss: 0.331, 28832/60000 datapoints
2025-03-06 20:26:18,150 - INFO - training batch 951, loss: 0.386, 30432/60000 datapoints
2025-03-06 20:26:18,349 - INFO - training batch 1001, loss: 0.332, 32032/60000 datapoints
2025-03-06 20:26:18,546 - INFO - training batch 1051, loss: 0.128, 33632/60000 datapoints
2025-03-06 20:26:18,741 - INFO - training batch 1101, loss: 0.216, 35232/60000 datapoints
2025-03-06 20:26:18,936 - INFO - training batch 1151, loss: 0.384, 36832/60000 datapoints
2025-03-06 20:26:19,133 - INFO - training batch 1201, loss: 0.284, 38432/60000 datapoints
2025-03-06 20:26:19,329 - INFO - training batch 1251, loss: 0.104, 40032/60000 datapoints
2025-03-06 20:26:19,522 - INFO - training batch 1301, loss: 0.401, 41632/60000 datapoints
2025-03-06 20:26:19,723 - INFO - training batch 1351, loss: 0.283, 43232/60000 datapoints
2025-03-06 20:26:19,921 - INFO - training batch 1401, loss: 0.192, 44832/60000 datapoints
2025-03-06 20:26:20,118 - INFO - training batch 1451, loss: 0.367, 46432/60000 datapoints
2025-03-06 20:26:20,313 - INFO - training batch 1501, loss: 0.073, 48032/60000 datapoints
2025-03-06 20:26:20,511 - INFO - training batch 1551, loss: 0.104, 49632/60000 datapoints
2025-03-06 20:26:20,709 - INFO - training batch 1601, loss: 0.591, 51232/60000 datapoints
2025-03-06 20:26:20,902 - INFO - training batch 1651, loss: 0.541, 52832/60000 datapoints
2025-03-06 20:26:21,096 - INFO - training batch 1701, loss: 0.071, 54432/60000 datapoints
2025-03-06 20:26:21,300 - INFO - training batch 1751, loss: 0.144, 56032/60000 datapoints
2025-03-06 20:26:21,497 - INFO - training batch 1801, loss: 0.076, 57632/60000 datapoints
2025-03-06 20:26:21,695 - INFO - training batch 1851, loss: 0.264, 59232/60000 datapoints
2025-03-06 20:26:21,797 - INFO - validation batch 1, loss: 0.058, 32/10016 datapoints
2025-03-06 20:26:21,954 - INFO - validation batch 51, loss: 0.158, 1632/10016 datapoints
2025-03-06 20:26:22,108 - INFO - validation batch 101, loss: 0.222, 3232/10016 datapoints
2025-03-06 20:26:22,262 - INFO - validation batch 151, loss: 0.204, 4832/10016 datapoints
2025-03-06 20:26:22,415 - INFO - validation batch 201, loss: 0.405, 6432/10016 datapoints
2025-03-06 20:26:22,569 - INFO - validation batch 251, loss: 0.165, 8032/10016 datapoints
2025-03-06 20:26:22,724 - INFO - validation batch 301, loss: 0.406, 9632/10016 datapoints
2025-03-06 20:26:22,760 - INFO - Epoch 720/800 done.
2025-03-06 20:26:22,760 - INFO - Final validation performance:
Loss: 0.231, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:26:22,761 - INFO - Beginning epoch 721/800
2025-03-06 20:26:22,768 - INFO - training batch 1, loss: 0.090, 32/60000 datapoints
2025-03-06 20:26:22,969 - INFO - training batch 51, loss: 0.254, 1632/60000 datapoints
2025-03-06 20:26:23,169 - INFO - training batch 101, loss: 0.082, 3232/60000 datapoints
2025-03-06 20:26:23,375 - INFO - training batch 151, loss: 0.411, 4832/60000 datapoints
2025-03-06 20:26:23,573 - INFO - training batch 201, loss: 0.140, 6432/60000 datapoints
2025-03-06 20:26:23,770 - INFO - training batch 251, loss: 0.275, 8032/60000 datapoints
2025-03-06 20:26:23,975 - INFO - training batch 301, loss: 0.339, 9632/60000 datapoints
2025-03-06 20:26:24,175 - INFO - training batch 351, loss: 0.338, 11232/60000 datapoints
2025-03-06 20:26:24,374 - INFO - training batch 401, loss: 0.150, 12832/60000 datapoints
2025-03-06 20:26:24,570 - INFO - training batch 451, loss: 0.155, 14432/60000 datapoints
2025-03-06 20:26:24,768 - INFO - training batch 501, loss: 0.184, 16032/60000 datapoints
2025-03-06 20:26:24,968 - INFO - training batch 551, loss: 0.210, 17632/60000 datapoints
2025-03-06 20:26:25,162 - INFO - training batch 601, loss: 0.165, 19232/60000 datapoints
2025-03-06 20:26:25,357 - INFO - training batch 651, loss: 0.080, 20832/60000 datapoints
2025-03-06 20:26:25,551 - INFO - training batch 701, loss: 0.129, 22432/60000 datapoints
2025-03-06 20:26:25,747 - INFO - training batch 751, loss: 0.072, 24032/60000 datapoints
2025-03-06 20:26:25,944 - INFO - training batch 801, loss: 0.294, 25632/60000 datapoints
2025-03-06 20:26:26,140 - INFO - training batch 851, loss: 0.295, 27232/60000 datapoints
2025-03-06 20:26:26,336 - INFO - training batch 901, loss: 0.261, 28832/60000 datapoints
2025-03-06 20:26:26,530 - INFO - training batch 951, loss: 0.274, 30432/60000 datapoints
2025-03-06 20:26:26,724 - INFO - training batch 1001, loss: 0.354, 32032/60000 datapoints
2025-03-06 20:26:26,920 - INFO - training batch 1051, loss: 0.306, 33632/60000 datapoints
2025-03-06 20:26:27,113 - INFO - training batch 1101, loss: 0.436, 35232/60000 datapoints
2025-03-06 20:26:27,311 - INFO - training batch 1151, loss: 0.277, 36832/60000 datapoints
2025-03-06 20:26:27,506 - INFO - training batch 1201, loss: 0.187, 38432/60000 datapoints
2025-03-06 20:26:27,704 - INFO - training batch 1251, loss: 0.197, 40032/60000 datapoints
2025-03-06 20:26:27,915 - INFO - training batch 1301, loss: 0.197, 41632/60000 datapoints
2025-03-06 20:26:28,109 - INFO - training batch 1351, loss: 0.190, 43232/60000 datapoints
2025-03-06 20:26:28,303 - INFO - training batch 1401, loss: 0.576, 44832/60000 datapoints
2025-03-06 20:26:28,500 - INFO - training batch 1451, loss: 0.218, 46432/60000 datapoints
2025-03-06 20:26:28,696 - INFO - training batch 1501, loss: 0.141, 48032/60000 datapoints
2025-03-06 20:26:28,894 - INFO - training batch 1551, loss: 0.057, 49632/60000 datapoints
2025-03-06 20:26:29,088 - INFO - training batch 1601, loss: 0.148, 51232/60000 datapoints
2025-03-06 20:26:29,286 - INFO - training batch 1651, loss: 0.289, 52832/60000 datapoints
2025-03-06 20:26:29,481 - INFO - training batch 1701, loss: 0.114, 54432/60000 datapoints
2025-03-06 20:26:29,676 - INFO - training batch 1751, loss: 0.135, 56032/60000 datapoints
2025-03-06 20:26:29,873 - INFO - training batch 1801, loss: 0.110, 57632/60000 datapoints
2025-03-06 20:26:30,068 - INFO - training batch 1851, loss: 0.304, 59232/60000 datapoints
2025-03-06 20:26:30,170 - INFO - validation batch 1, loss: 0.416, 32/10016 datapoints
2025-03-06 20:26:30,328 - INFO - validation batch 51, loss: 0.260, 1632/10016 datapoints
2025-03-06 20:26:30,482 - INFO - validation batch 101, loss: 0.253, 3232/10016 datapoints
2025-03-06 20:26:30,637 - INFO - validation batch 151, loss: 0.527, 4832/10016 datapoints
2025-03-06 20:26:30,791 - INFO - validation batch 201, loss: 0.368, 6432/10016 datapoints
2025-03-06 20:26:30,943 - INFO - validation batch 251, loss: 0.234, 8032/10016 datapoints
2025-03-06 20:26:31,094 - INFO - validation batch 301, loss: 0.218, 9632/10016 datapoints
2025-03-06 20:26:31,131 - INFO - Epoch 721/800 done.
2025-03-06 20:26:31,132 - INFO - Final validation performance:
Loss: 0.325, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:26:31,132 - INFO - Beginning epoch 722/800
2025-03-06 20:26:31,139 - INFO - training batch 1, loss: 0.166, 32/60000 datapoints
2025-03-06 20:26:31,348 - INFO - training batch 51, loss: 0.101, 1632/60000 datapoints
2025-03-06 20:26:31,553 - INFO - training batch 101, loss: 0.111, 3232/60000 datapoints
2025-03-06 20:26:31,767 - INFO - training batch 151, loss: 0.094, 4832/60000 datapoints
2025-03-06 20:26:31,970 - INFO - training batch 201, loss: 0.213, 6432/60000 datapoints
2025-03-06 20:26:32,166 - INFO - training batch 251, loss: 0.152, 8032/60000 datapoints
2025-03-06 20:26:32,360 - INFO - training batch 301, loss: 0.073, 9632/60000 datapoints
2025-03-06 20:26:32,566 - INFO - training batch 351, loss: 0.085, 11232/60000 datapoints
2025-03-06 20:26:32,785 - INFO - training batch 401, loss: 0.357, 12832/60000 datapoints
2025-03-06 20:26:32,979 - INFO - training batch 451, loss: 0.164, 14432/60000 datapoints
2025-03-06 20:26:33,173 - INFO - training batch 501, loss: 0.368, 16032/60000 datapoints
2025-03-06 20:26:33,369 - INFO - training batch 551, loss: 0.097, 17632/60000 datapoints
2025-03-06 20:26:33,569 - INFO - training batch 601, loss: 0.327, 19232/60000 datapoints
2025-03-06 20:26:33,768 - INFO - training batch 651, loss: 0.195, 20832/60000 datapoints
2025-03-06 20:26:33,964 - INFO - training batch 701, loss: 0.096, 22432/60000 datapoints
2025-03-06 20:26:34,158 - INFO - training batch 751, loss: 0.079, 24032/60000 datapoints
2025-03-06 20:26:34,352 - INFO - training batch 801, loss: 0.748, 25632/60000 datapoints
2025-03-06 20:26:34,548 - INFO - training batch 851, loss: 0.113, 27232/60000 datapoints
2025-03-06 20:26:34,746 - INFO - training batch 901, loss: 0.092, 28832/60000 datapoints
2025-03-06 20:26:34,946 - INFO - training batch 951, loss: 0.356, 30432/60000 datapoints
2025-03-06 20:26:35,144 - INFO - training batch 1001, loss: 0.227, 32032/60000 datapoints
2025-03-06 20:26:35,342 - INFO - training batch 1051, loss: 0.153, 33632/60000 datapoints
2025-03-06 20:26:35,541 - INFO - training batch 1101, loss: 0.065, 35232/60000 datapoints
2025-03-06 20:26:35,737 - INFO - training batch 1151, loss: 0.259, 36832/60000 datapoints
2025-03-06 20:26:35,929 - INFO - training batch 1201, loss: 0.088, 38432/60000 datapoints
2025-03-06 20:26:36,131 - INFO - training batch 1251, loss: 0.377, 40032/60000 datapoints
2025-03-06 20:26:36,329 - INFO - training batch 1301, loss: 0.078, 41632/60000 datapoints
2025-03-06 20:26:36,525 - INFO - training batch 1351, loss: 0.184, 43232/60000 datapoints
2025-03-06 20:26:36,723 - INFO - training batch 1401, loss: 0.190, 44832/60000 datapoints
2025-03-06 20:26:36,916 - INFO - training batch 1451, loss: 0.271, 46432/60000 datapoints
2025-03-06 20:26:37,109 - INFO - training batch 1501, loss: 0.147, 48032/60000 datapoints
2025-03-06 20:26:37,304 - INFO - training batch 1551, loss: 0.170, 49632/60000 datapoints
2025-03-06 20:26:37,499 - INFO - training batch 1601, loss: 0.663, 51232/60000 datapoints
2025-03-06 20:26:37,715 - INFO - training batch 1651, loss: 0.064, 52832/60000 datapoints
2025-03-06 20:26:37,931 - INFO - training batch 1701, loss: 0.136, 54432/60000 datapoints
2025-03-06 20:26:38,126 - INFO - training batch 1751, loss: 0.074, 56032/60000 datapoints
2025-03-06 20:26:38,320 - INFO - training batch 1801, loss: 0.168, 57632/60000 datapoints
2025-03-06 20:26:38,515 - INFO - training batch 1851, loss: 0.352, 59232/60000 datapoints
2025-03-06 20:26:38,616 - INFO - validation batch 1, loss: 0.435, 32/10016 datapoints
2025-03-06 20:26:38,770 - INFO - validation batch 51, loss: 0.311, 1632/10016 datapoints
2025-03-06 20:26:38,923 - INFO - validation batch 101, loss: 0.449, 3232/10016 datapoints
2025-03-06 20:26:39,077 - INFO - validation batch 151, loss: 0.210, 4832/10016 datapoints
2025-03-06 20:26:39,230 - INFO - validation batch 201, loss: 0.184, 6432/10016 datapoints
2025-03-06 20:26:39,387 - INFO - validation batch 251, loss: 0.082, 8032/10016 datapoints
2025-03-06 20:26:39,541 - INFO - validation batch 301, loss: 0.384, 9632/10016 datapoints
2025-03-06 20:26:39,584 - INFO - Epoch 722/800 done.
2025-03-06 20:26:39,585 - INFO - Final validation performance:
Loss: 0.294, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:26:39,585 - INFO - Beginning epoch 723/800
2025-03-06 20:26:39,593 - INFO - training batch 1, loss: 0.468, 32/60000 datapoints
2025-03-06 20:26:39,792 - INFO - training batch 51, loss: 0.181, 1632/60000 datapoints
2025-03-06 20:26:39,989 - INFO - training batch 101, loss: 0.061, 3232/60000 datapoints
2025-03-06 20:26:40,197 - INFO - training batch 151, loss: 0.294, 4832/60000 datapoints
2025-03-06 20:26:40,390 - INFO - training batch 201, loss: 0.037, 6432/60000 datapoints
2025-03-06 20:26:40,597 - INFO - training batch 251, loss: 0.498, 8032/60000 datapoints
2025-03-06 20:26:40,798 - INFO - training batch 301, loss: 0.116, 9632/60000 datapoints
2025-03-06 20:26:40,995 - INFO - training batch 351, loss: 0.282, 11232/60000 datapoints
2025-03-06 20:26:41,187 - INFO - training batch 401, loss: 0.144, 12832/60000 datapoints
2025-03-06 20:26:41,386 - INFO - training batch 451, loss: 0.085, 14432/60000 datapoints
2025-03-06 20:26:41,582 - INFO - training batch 501, loss: 0.191, 16032/60000 datapoints
2025-03-06 20:26:41,786 - INFO - training batch 551, loss: 0.088, 17632/60000 datapoints
2025-03-06 20:26:41,982 - INFO - training batch 601, loss: 0.148, 19232/60000 datapoints
2025-03-06 20:26:42,177 - INFO - training batch 651, loss: 0.278, 20832/60000 datapoints
2025-03-06 20:26:42,373 - INFO - training batch 701, loss: 0.410, 22432/60000 datapoints
2025-03-06 20:26:42,570 - INFO - training batch 751, loss: 0.257, 24032/60000 datapoints
2025-03-06 20:26:42,767 - INFO - training batch 801, loss: 0.407, 25632/60000 datapoints
2025-03-06 20:26:42,963 - INFO - training batch 851, loss: 0.277, 27232/60000 datapoints
2025-03-06 20:26:43,154 - INFO - training batch 901, loss: 0.548, 28832/60000 datapoints
2025-03-06 20:26:43,352 - INFO - training batch 951, loss: 0.295, 30432/60000 datapoints
2025-03-06 20:26:43,548 - INFO - training batch 1001, loss: 0.096, 32032/60000 datapoints
2025-03-06 20:26:43,747 - INFO - training batch 1051, loss: 0.084, 33632/60000 datapoints
2025-03-06 20:26:43,939 - INFO - training batch 1101, loss: 0.163, 35232/60000 datapoints
2025-03-06 20:26:44,137 - INFO - training batch 1151, loss: 0.224, 36832/60000 datapoints
2025-03-06 20:26:44,331 - INFO - training batch 1201, loss: 0.386, 38432/60000 datapoints
2025-03-06 20:26:44,526 - INFO - training batch 1251, loss: 0.344, 40032/60000 datapoints
2025-03-06 20:26:44,725 - INFO - training batch 1301, loss: 0.058, 41632/60000 datapoints
2025-03-06 20:26:44,925 - INFO - training batch 1351, loss: 0.475, 43232/60000 datapoints
2025-03-06 20:26:45,122 - INFO - training batch 1401, loss: 0.292, 44832/60000 datapoints
2025-03-06 20:26:45,318 - INFO - training batch 1451, loss: 0.134, 46432/60000 datapoints
2025-03-06 20:26:45,514 - INFO - training batch 1501, loss: 0.163, 48032/60000 datapoints
2025-03-06 20:26:45,713 - INFO - training batch 1551, loss: 0.121, 49632/60000 datapoints
2025-03-06 20:26:45,907 - INFO - training batch 1601, loss: 0.058, 51232/60000 datapoints
2025-03-06 20:26:46,106 - INFO - training batch 1651, loss: 0.269, 52832/60000 datapoints
2025-03-06 20:26:46,301 - INFO - training batch 1701, loss: 0.176, 54432/60000 datapoints
2025-03-06 20:26:46,496 - INFO - training batch 1751, loss: 0.168, 56032/60000 datapoints
2025-03-06 20:26:46,694 - INFO - training batch 1801, loss: 0.192, 57632/60000 datapoints
2025-03-06 20:26:46,887 - INFO - training batch 1851, loss: 0.189, 59232/60000 datapoints
2025-03-06 20:26:46,989 - INFO - validation batch 1, loss: 0.100, 32/10016 datapoints
2025-03-06 20:26:47,146 - INFO - validation batch 51, loss: 0.143, 1632/10016 datapoints
2025-03-06 20:26:47,302 - INFO - validation batch 101, loss: 0.244, 3232/10016 datapoints
2025-03-06 20:26:47,456 - INFO - validation batch 151, loss: 0.292, 4832/10016 datapoints
2025-03-06 20:26:47,608 - INFO - validation batch 201, loss: 0.228, 6432/10016 datapoints
2025-03-06 20:26:47,767 - INFO - validation batch 251, loss: 0.174, 8032/10016 datapoints
2025-03-06 20:26:47,925 - INFO - validation batch 301, loss: 0.123, 9632/10016 datapoints
2025-03-06 20:26:47,972 - INFO - Epoch 723/800 done.
2025-03-06 20:26:47,984 - INFO - Final validation performance:
Loss: 0.186, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:26:47,984 - INFO - Beginning epoch 724/800
2025-03-06 20:26:47,991 - INFO - training batch 1, loss: 0.164, 32/60000 datapoints
2025-03-06 20:26:48,192 - INFO - training batch 51, loss: 0.091, 1632/60000 datapoints
2025-03-06 20:26:48,396 - INFO - training batch 101, loss: 0.414, 3232/60000 datapoints
2025-03-06 20:26:48,592 - INFO - training batch 151, loss: 0.162, 4832/60000 datapoints
2025-03-06 20:26:48,802 - INFO - training batch 201, loss: 0.239, 6432/60000 datapoints
2025-03-06 20:26:49,000 - INFO - training batch 251, loss: 0.071, 8032/60000 datapoints
2025-03-06 20:26:49,198 - INFO - training batch 301, loss: 0.049, 9632/60000 datapoints
2025-03-06 20:26:49,395 - INFO - training batch 351, loss: 0.096, 11232/60000 datapoints
2025-03-06 20:26:49,590 - INFO - training batch 401, loss: 0.063, 12832/60000 datapoints
2025-03-06 20:26:49,789 - INFO - training batch 451, loss: 0.098, 14432/60000 datapoints
2025-03-06 20:26:49,981 - INFO - training batch 501, loss: 0.303, 16032/60000 datapoints
2025-03-06 20:26:50,180 - INFO - training batch 551, loss: 0.723, 17632/60000 datapoints
2025-03-06 20:26:50,375 - INFO - training batch 601, loss: 0.135, 19232/60000 datapoints
2025-03-06 20:26:50,569 - INFO - training batch 651, loss: 0.496, 20832/60000 datapoints
2025-03-06 20:26:50,764 - INFO - training batch 701, loss: 0.303, 22432/60000 datapoints
2025-03-06 20:26:50,958 - INFO - training batch 751, loss: 0.208, 24032/60000 datapoints
2025-03-06 20:26:51,151 - INFO - training batch 801, loss: 0.238, 25632/60000 datapoints
2025-03-06 20:26:51,348 - INFO - training batch 851, loss: 0.130, 27232/60000 datapoints
2025-03-06 20:26:51,542 - INFO - training batch 901, loss: 0.137, 28832/60000 datapoints
2025-03-06 20:26:51,741 - INFO - training batch 951, loss: 0.192, 30432/60000 datapoints
2025-03-06 20:26:51,936 - INFO - training batch 1001, loss: 0.298, 32032/60000 datapoints
2025-03-06 20:26:52,129 - INFO - training batch 1051, loss: 0.165, 33632/60000 datapoints
2025-03-06 20:26:52,324 - INFO - training batch 1101, loss: 0.381, 35232/60000 datapoints
2025-03-06 20:26:52,518 - INFO - training batch 1151, loss: 0.282, 36832/60000 datapoints
2025-03-06 20:26:52,714 - INFO - training batch 1201, loss: 0.046, 38432/60000 datapoints
2025-03-06 20:26:52,910 - INFO - training batch 1251, loss: 0.174, 40032/60000 datapoints
2025-03-06 20:26:53,104 - INFO - training batch 1301, loss: 0.437, 41632/60000 datapoints
2025-03-06 20:26:53,304 - INFO - training batch 1351, loss: 0.249, 43232/60000 datapoints
2025-03-06 20:26:53,497 - INFO - training batch 1401, loss: 0.138, 44832/60000 datapoints
2025-03-06 20:26:53,694 - INFO - training batch 1451, loss: 0.355, 46432/60000 datapoints
2025-03-06 20:26:53,888 - INFO - training batch 1501, loss: 0.304, 48032/60000 datapoints
2025-03-06 20:26:54,082 - INFO - training batch 1551, loss: 0.218, 49632/60000 datapoints
2025-03-06 20:26:54,280 - INFO - training batch 1601, loss: 0.250, 51232/60000 datapoints
2025-03-06 20:26:54,474 - INFO - training batch 1651, loss: 0.195, 52832/60000 datapoints
2025-03-06 20:26:54,673 - INFO - training batch 1701, loss: 0.260, 54432/60000 datapoints
2025-03-06 20:26:54,872 - INFO - training batch 1751, loss: 0.076, 56032/60000 datapoints
2025-03-06 20:26:55,071 - INFO - training batch 1801, loss: 0.151, 57632/60000 datapoints
2025-03-06 20:26:55,267 - INFO - training batch 1851, loss: 0.157, 59232/60000 datapoints
2025-03-06 20:26:55,372 - INFO - validation batch 1, loss: 0.119, 32/10016 datapoints
2025-03-06 20:26:55,525 - INFO - validation batch 51, loss: 0.349, 1632/10016 datapoints
2025-03-06 20:26:55,681 - INFO - validation batch 101, loss: 0.184, 3232/10016 datapoints
2025-03-06 20:26:55,835 - INFO - validation batch 151, loss: 0.536, 4832/10016 datapoints
2025-03-06 20:26:55,988 - INFO - validation batch 201, loss: 0.115, 6432/10016 datapoints
2025-03-06 20:26:56,146 - INFO - validation batch 251, loss: 0.193, 8032/10016 datapoints
2025-03-06 20:26:56,300 - INFO - validation batch 301, loss: 0.230, 9632/10016 datapoints
2025-03-06 20:26:56,338 - INFO - Epoch 724/800 done.
2025-03-06 20:26:56,338 - INFO - Final validation performance:
Loss: 0.247, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:26:56,339 - INFO - Beginning epoch 725/800
2025-03-06 20:26:56,345 - INFO - training batch 1, loss: 0.223, 32/60000 datapoints
2025-03-06 20:26:56,540 - INFO - training batch 51, loss: 0.173, 1632/60000 datapoints
2025-03-06 20:26:56,739 - INFO - training batch 101, loss: 0.187, 3232/60000 datapoints
2025-03-06 20:26:56,944 - INFO - training batch 151, loss: 0.027, 4832/60000 datapoints
2025-03-06 20:26:57,140 - INFO - training batch 201, loss: 0.608, 6432/60000 datapoints
2025-03-06 20:26:57,343 - INFO - training batch 251, loss: 0.544, 8032/60000 datapoints
2025-03-06 20:26:57,540 - INFO - training batch 301, loss: 0.198, 9632/60000 datapoints
2025-03-06 20:26:57,742 - INFO - training batch 351, loss: 0.332, 11232/60000 datapoints
2025-03-06 20:26:57,936 - INFO - training batch 401, loss: 0.069, 12832/60000 datapoints
2025-03-06 20:26:58,151 - INFO - training batch 451, loss: 0.375, 14432/60000 datapoints
2025-03-06 20:26:58,345 - INFO - training batch 501, loss: 0.223, 16032/60000 datapoints
2025-03-06 20:26:58,541 - INFO - training batch 551, loss: 0.160, 17632/60000 datapoints
2025-03-06 20:26:58,740 - INFO - training batch 601, loss: 0.399, 19232/60000 datapoints
2025-03-06 20:26:58,937 - INFO - training batch 651, loss: 0.352, 20832/60000 datapoints
2025-03-06 20:26:59,133 - INFO - training batch 701, loss: 0.197, 22432/60000 datapoints
2025-03-06 20:26:59,333 - INFO - training batch 751, loss: 0.162, 24032/60000 datapoints
2025-03-06 20:26:59,528 - INFO - training batch 801, loss: 0.079, 25632/60000 datapoints
2025-03-06 20:26:59,723 - INFO - training batch 851, loss: 0.288, 27232/60000 datapoints
2025-03-06 20:26:59,919 - INFO - training batch 901, loss: 0.087, 28832/60000 datapoints
2025-03-06 20:27:00,116 - INFO - training batch 951, loss: 0.098, 30432/60000 datapoints
2025-03-06 20:27:00,312 - INFO - training batch 1001, loss: 0.310, 32032/60000 datapoints
2025-03-06 20:27:00,509 - INFO - training batch 1051, loss: 0.324, 33632/60000 datapoints
2025-03-06 20:27:00,706 - INFO - training batch 1101, loss: 0.245, 35232/60000 datapoints
2025-03-06 20:27:00,902 - INFO - training batch 1151, loss: 0.286, 36832/60000 datapoints
2025-03-06 20:27:01,095 - INFO - training batch 1201, loss: 0.167, 38432/60000 datapoints
2025-03-06 20:27:01,290 - INFO - training batch 1251, loss: 0.169, 40032/60000 datapoints
2025-03-06 20:27:01,484 - INFO - training batch 1301, loss: 0.294, 41632/60000 datapoints
2025-03-06 20:27:01,682 - INFO - training batch 1351, loss: 0.163, 43232/60000 datapoints
2025-03-06 20:27:01,876 - INFO - training batch 1401, loss: 0.262, 44832/60000 datapoints
2025-03-06 20:27:02,069 - INFO - training batch 1451, loss: 0.253, 46432/60000 datapoints
2025-03-06 20:27:02,262 - INFO - training batch 1501, loss: 0.379, 48032/60000 datapoints
2025-03-06 20:27:02,459 - INFO - training batch 1551, loss: 0.117, 49632/60000 datapoints
2025-03-06 20:27:02,654 - INFO - training batch 1601, loss: 0.171, 51232/60000 datapoints
2025-03-06 20:27:02,851 - INFO - training batch 1651, loss: 0.074, 52832/60000 datapoints
2025-03-06 20:27:03,046 - INFO - training batch 1701, loss: 0.299, 54432/60000 datapoints
2025-03-06 20:27:03,244 - INFO - training batch 1751, loss: 0.157, 56032/60000 datapoints
2025-03-06 20:27:03,440 - INFO - training batch 1801, loss: 0.215, 57632/60000 datapoints
2025-03-06 20:27:03,638 - INFO - training batch 1851, loss: 0.214, 59232/60000 datapoints
2025-03-06 20:27:03,738 - INFO - validation batch 1, loss: 0.023, 32/10016 datapoints
2025-03-06 20:27:03,893 - INFO - validation batch 51, loss: 0.250, 1632/10016 datapoints
2025-03-06 20:27:04,046 - INFO - validation batch 101, loss: 0.180, 3232/10016 datapoints
2025-03-06 20:27:04,200 - INFO - validation batch 151, loss: 0.208, 4832/10016 datapoints
2025-03-06 20:27:04,354 - INFO - validation batch 201, loss: 0.062, 6432/10016 datapoints
2025-03-06 20:27:04,507 - INFO - validation batch 251, loss: 0.081, 8032/10016 datapoints
2025-03-06 20:27:04,663 - INFO - validation batch 301, loss: 0.430, 9632/10016 datapoints
2025-03-06 20:27:04,701 - INFO - Epoch 725/800 done.
2025-03-06 20:27:04,701 - INFO - Final validation performance:
Loss: 0.177, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:27:04,701 - INFO - Beginning epoch 726/800
2025-03-06 20:27:04,708 - INFO - training batch 1, loss: 0.140, 32/60000 datapoints
2025-03-06 20:27:04,911 - INFO - training batch 51, loss: 0.125, 1632/60000 datapoints
2025-03-06 20:27:05,109 - INFO - training batch 101, loss: 0.214, 3232/60000 datapoints
2025-03-06 20:27:05,313 - INFO - training batch 151, loss: 0.444, 4832/60000 datapoints
2025-03-06 20:27:05,518 - INFO - training batch 201, loss: 0.204, 6432/60000 datapoints
2025-03-06 20:27:05,715 - INFO - training batch 251, loss: 0.111, 8032/60000 datapoints
2025-03-06 20:27:05,916 - INFO - training batch 301, loss: 0.152, 9632/60000 datapoints
2025-03-06 20:27:06,110 - INFO - training batch 351, loss: 0.308, 11232/60000 datapoints
2025-03-06 20:27:06,310 - INFO - training batch 401, loss: 0.224, 12832/60000 datapoints
2025-03-06 20:27:06,505 - INFO - training batch 451, loss: 0.115, 14432/60000 datapoints
2025-03-06 20:27:06,701 - INFO - training batch 501, loss: 0.251, 16032/60000 datapoints
2025-03-06 20:27:06,893 - INFO - training batch 551, loss: 0.370, 17632/60000 datapoints
2025-03-06 20:27:07,085 - INFO - training batch 601, loss: 0.271, 19232/60000 datapoints
2025-03-06 20:27:07,281 - INFO - training batch 651, loss: 0.461, 20832/60000 datapoints
2025-03-06 20:27:07,478 - INFO - training batch 701, loss: 0.084, 22432/60000 datapoints
2025-03-06 20:27:07,678 - INFO - training batch 751, loss: 0.140, 24032/60000 datapoints
2025-03-06 20:27:07,874 - INFO - training batch 801, loss: 0.186, 25632/60000 datapoints
2025-03-06 20:27:08,087 - INFO - training batch 851, loss: 0.080, 27232/60000 datapoints
2025-03-06 20:27:08,305 - INFO - training batch 901, loss: 0.145, 28832/60000 datapoints
2025-03-06 20:27:08,501 - INFO - training batch 951, loss: 0.211, 30432/60000 datapoints
2025-03-06 20:27:08,697 - INFO - training batch 1001, loss: 0.047, 32032/60000 datapoints
2025-03-06 20:27:08,892 - INFO - training batch 1051, loss: 0.409, 33632/60000 datapoints
2025-03-06 20:27:09,089 - INFO - training batch 1101, loss: 0.068, 35232/60000 datapoints
2025-03-06 20:27:09,285 - INFO - training batch 1151, loss: 0.163, 36832/60000 datapoints
2025-03-06 20:27:09,482 - INFO - training batch 1201, loss: 0.242, 38432/60000 datapoints
2025-03-06 20:27:09,678 - INFO - training batch 1251, loss: 0.113, 40032/60000 datapoints
2025-03-06 20:27:09,874 - INFO - training batch 1301, loss: 0.117, 41632/60000 datapoints
2025-03-06 20:27:10,076 - INFO - training batch 1351, loss: 0.248, 43232/60000 datapoints
2025-03-06 20:27:10,273 - INFO - training batch 1401, loss: 0.227, 44832/60000 datapoints
2025-03-06 20:27:10,468 - INFO - training batch 1451, loss: 0.069, 46432/60000 datapoints
2025-03-06 20:27:10,667 - INFO - training batch 1501, loss: 0.203, 48032/60000 datapoints
2025-03-06 20:27:10,862 - INFO - training batch 1551, loss: 0.449, 49632/60000 datapoints
2025-03-06 20:27:11,057 - INFO - training batch 1601, loss: 0.175, 51232/60000 datapoints
2025-03-06 20:27:11,250 - INFO - training batch 1651, loss: 0.123, 52832/60000 datapoints
2025-03-06 20:27:11,449 - INFO - training batch 1701, loss: 0.246, 54432/60000 datapoints
2025-03-06 20:27:11,645 - INFO - training batch 1751, loss: 0.256, 56032/60000 datapoints
2025-03-06 20:27:11,840 - INFO - training batch 1801, loss: 0.142, 57632/60000 datapoints
2025-03-06 20:27:12,037 - INFO - training batch 1851, loss: 0.183, 59232/60000 datapoints
2025-03-06 20:27:12,137 - INFO - validation batch 1, loss: 0.126, 32/10016 datapoints
2025-03-06 20:27:12,292 - INFO - validation batch 51, loss: 0.075, 1632/10016 datapoints
2025-03-06 20:27:12,447 - INFO - validation batch 101, loss: 0.263, 3232/10016 datapoints
2025-03-06 20:27:12,598 - INFO - validation batch 151, loss: 0.333, 4832/10016 datapoints
2025-03-06 20:27:12,754 - INFO - validation batch 201, loss: 0.121, 6432/10016 datapoints
2025-03-06 20:27:12,907 - INFO - validation batch 251, loss: 0.269, 8032/10016 datapoints
2025-03-06 20:27:13,059 - INFO - validation batch 301, loss: 0.169, 9632/10016 datapoints
2025-03-06 20:27:13,096 - INFO - Epoch 726/800 done.
2025-03-06 20:27:13,096 - INFO - Final validation performance:
Loss: 0.194, top-1 acc: 0.935top-5 acc: 0.935
2025-03-06 20:27:13,097 - INFO - Beginning epoch 727/800
2025-03-06 20:27:13,103 - INFO - training batch 1, loss: 0.145, 32/60000 datapoints
2025-03-06 20:27:13,309 - INFO - training batch 51, loss: 0.253, 1632/60000 datapoints
2025-03-06 20:27:13,507 - INFO - training batch 101, loss: 0.162, 3232/60000 datapoints
2025-03-06 20:27:13,712 - INFO - training batch 151, loss: 0.106, 4832/60000 datapoints
2025-03-06 20:27:13,910 - INFO - training batch 201, loss: 0.083, 6432/60000 datapoints
2025-03-06 20:27:14,109 - INFO - training batch 251, loss: 0.528, 8032/60000 datapoints
2025-03-06 20:27:14,303 - INFO - training batch 301, loss: 0.120, 9632/60000 datapoints
2025-03-06 20:27:14,496 - INFO - training batch 351, loss: 0.243, 11232/60000 datapoints
2025-03-06 20:27:14,694 - INFO - training batch 401, loss: 0.236, 12832/60000 datapoints
2025-03-06 20:27:14,897 - INFO - training batch 451, loss: 0.393, 14432/60000 datapoints
2025-03-06 20:27:15,093 - INFO - training batch 501, loss: 0.148, 16032/60000 datapoints
2025-03-06 20:27:15,291 - INFO - training batch 551, loss: 0.576, 17632/60000 datapoints
2025-03-06 20:27:15,492 - INFO - training batch 601, loss: 0.208, 19232/60000 datapoints
2025-03-06 20:27:15,689 - INFO - training batch 651, loss: 0.322, 20832/60000 datapoints
2025-03-06 20:27:15,884 - INFO - training batch 701, loss: 0.180, 22432/60000 datapoints
2025-03-06 20:27:16,079 - INFO - training batch 751, loss: 0.248, 24032/60000 datapoints
2025-03-06 20:27:16,287 - INFO - training batch 801, loss: 0.520, 25632/60000 datapoints
2025-03-06 20:27:16,483 - INFO - training batch 851, loss: 0.076, 27232/60000 datapoints
2025-03-06 20:27:16,678 - INFO - training batch 901, loss: 0.259, 28832/60000 datapoints
2025-03-06 20:27:16,873 - INFO - training batch 951, loss: 0.163, 30432/60000 datapoints
2025-03-06 20:27:17,068 - INFO - training batch 1001, loss: 0.466, 32032/60000 datapoints
2025-03-06 20:27:17,259 - INFO - training batch 1051, loss: 0.130, 33632/60000 datapoints
2025-03-06 20:27:17,458 - INFO - training batch 1101, loss: 0.093, 35232/60000 datapoints
2025-03-06 20:27:17,654 - INFO - training batch 1151, loss: 0.126, 36832/60000 datapoints
2025-03-06 20:27:17,852 - INFO - training batch 1201, loss: 0.111, 38432/60000 datapoints
2025-03-06 20:27:18,046 - INFO - training batch 1251, loss: 0.096, 40032/60000 datapoints
2025-03-06 20:27:18,271 - INFO - training batch 1301, loss: 0.271, 41632/60000 datapoints
2025-03-06 20:27:18,465 - INFO - training batch 1351, loss: 0.073, 43232/60000 datapoints
2025-03-06 20:27:18,663 - INFO - training batch 1401, loss: 0.421, 44832/60000 datapoints
2025-03-06 20:27:18,858 - INFO - training batch 1451, loss: 0.257, 46432/60000 datapoints
2025-03-06 20:27:19,053 - INFO - training batch 1501, loss: 0.254, 48032/60000 datapoints
2025-03-06 20:27:19,256 - INFO - training batch 1551, loss: 0.215, 49632/60000 datapoints
2025-03-06 20:27:19,464 - INFO - training batch 1601, loss: 0.286, 51232/60000 datapoints
2025-03-06 20:27:19,662 - INFO - training batch 1651, loss: 0.422, 52832/60000 datapoints
2025-03-06 20:27:19,860 - INFO - training batch 1701, loss: 0.323, 54432/60000 datapoints
2025-03-06 20:27:20,056 - INFO - training batch 1751, loss: 0.422, 56032/60000 datapoints
2025-03-06 20:27:20,274 - INFO - training batch 1801, loss: 0.312, 57632/60000 datapoints
2025-03-06 20:27:20,479 - INFO - training batch 1851, loss: 0.171, 59232/60000 datapoints
2025-03-06 20:27:20,584 - INFO - validation batch 1, loss: 0.191, 32/10016 datapoints
2025-03-06 20:27:20,741 - INFO - validation batch 51, loss: 0.080, 1632/10016 datapoints
2025-03-06 20:27:20,893 - INFO - validation batch 101, loss: 0.121, 3232/10016 datapoints
2025-03-06 20:27:21,047 - INFO - validation batch 151, loss: 0.081, 4832/10016 datapoints
2025-03-06 20:27:21,200 - INFO - validation batch 201, loss: 0.119, 6432/10016 datapoints
2025-03-06 20:27:21,353 - INFO - validation batch 251, loss: 0.225, 8032/10016 datapoints
2025-03-06 20:27:21,510 - INFO - validation batch 301, loss: 0.171, 9632/10016 datapoints
2025-03-06 20:27:21,549 - INFO - Epoch 727/800 done.
2025-03-06 20:27:21,549 - INFO - Final validation performance:
Loss: 0.141, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:27:21,550 - INFO - Beginning epoch 728/800
2025-03-06 20:27:21,556 - INFO - training batch 1, loss: 0.116, 32/60000 datapoints
2025-03-06 20:27:21,753 - INFO - training batch 51, loss: 0.211, 1632/60000 datapoints
2025-03-06 20:27:21,960 - INFO - training batch 101, loss: 0.190, 3232/60000 datapoints
2025-03-06 20:27:22,158 - INFO - training batch 151, loss: 0.713, 4832/60000 datapoints
2025-03-06 20:27:22,353 - INFO - training batch 201, loss: 0.425, 6432/60000 datapoints
2025-03-06 20:27:22,554 - INFO - training batch 251, loss: 0.427, 8032/60000 datapoints
2025-03-06 20:27:22,755 - INFO - training batch 301, loss: 0.253, 9632/60000 datapoints
2025-03-06 20:27:22,952 - INFO - training batch 351, loss: 0.268, 11232/60000 datapoints
2025-03-06 20:27:23,149 - INFO - training batch 401, loss: 0.431, 12832/60000 datapoints
2025-03-06 20:27:23,344 - INFO - training batch 451, loss: 0.245, 14432/60000 datapoints
2025-03-06 20:27:23,541 - INFO - training batch 501, loss: 0.054, 16032/60000 datapoints
2025-03-06 20:27:23,735 - INFO - training batch 551, loss: 0.799, 17632/60000 datapoints
2025-03-06 20:27:23,932 - INFO - training batch 601, loss: 0.197, 19232/60000 datapoints
2025-03-06 20:27:24,129 - INFO - training batch 651, loss: 0.103, 20832/60000 datapoints
2025-03-06 20:27:24,322 - INFO - training batch 701, loss: 0.025, 22432/60000 datapoints
2025-03-06 20:27:24,519 - INFO - training batch 751, loss: 0.328, 24032/60000 datapoints
2025-03-06 20:27:24,716 - INFO - training batch 801, loss: 0.287, 25632/60000 datapoints
2025-03-06 20:27:24,915 - INFO - training batch 851, loss: 0.132, 27232/60000 datapoints
2025-03-06 20:27:25,111 - INFO - training batch 901, loss: 0.253, 28832/60000 datapoints
2025-03-06 20:27:25,310 - INFO - training batch 951, loss: 0.119, 30432/60000 datapoints
2025-03-06 20:27:25,507 - INFO - training batch 1001, loss: 0.315, 32032/60000 datapoints
2025-03-06 20:27:25,705 - INFO - training batch 1051, loss: 0.348, 33632/60000 datapoints
2025-03-06 20:27:25,900 - INFO - training batch 1101, loss: 0.107, 35232/60000 datapoints
2025-03-06 20:27:26,097 - INFO - training batch 1151, loss: 0.177, 36832/60000 datapoints
2025-03-06 20:27:26,297 - INFO - training batch 1201, loss: 0.215, 38432/60000 datapoints
2025-03-06 20:27:26,494 - INFO - training batch 1251, loss: 0.107, 40032/60000 datapoints
2025-03-06 20:27:26,691 - INFO - training batch 1301, loss: 0.123, 41632/60000 datapoints
2025-03-06 20:27:26,884 - INFO - training batch 1351, loss: 0.237, 43232/60000 datapoints
2025-03-06 20:27:27,079 - INFO - training batch 1401, loss: 0.305, 44832/60000 datapoints
2025-03-06 20:27:27,273 - INFO - training batch 1451, loss: 0.456, 46432/60000 datapoints
2025-03-06 20:27:27,472 - INFO - training batch 1501, loss: 0.133, 48032/60000 datapoints
2025-03-06 20:27:27,669 - INFO - training batch 1551, loss: 0.320, 49632/60000 datapoints
2025-03-06 20:27:27,865 - INFO - training batch 1601, loss: 0.308, 51232/60000 datapoints
2025-03-06 20:27:28,059 - INFO - training batch 1651, loss: 0.334, 52832/60000 datapoints
2025-03-06 20:27:28,256 - INFO - training batch 1701, loss: 0.061, 54432/60000 datapoints
2025-03-06 20:27:28,466 - INFO - training batch 1751, loss: 0.230, 56032/60000 datapoints
2025-03-06 20:27:28,660 - INFO - training batch 1801, loss: 0.323, 57632/60000 datapoints
2025-03-06 20:27:28,855 - INFO - training batch 1851, loss: 0.156, 59232/60000 datapoints
2025-03-06 20:27:28,957 - INFO - validation batch 1, loss: 0.138, 32/10016 datapoints
2025-03-06 20:27:29,111 - INFO - validation batch 51, loss: 0.252, 1632/10016 datapoints
2025-03-06 20:27:29,265 - INFO - validation batch 101, loss: 0.154, 3232/10016 datapoints
2025-03-06 20:27:29,417 - INFO - validation batch 151, loss: 0.132, 4832/10016 datapoints
2025-03-06 20:27:29,572 - INFO - validation batch 201, loss: 0.139, 6432/10016 datapoints
2025-03-06 20:27:29,728 - INFO - validation batch 251, loss: 0.067, 8032/10016 datapoints
2025-03-06 20:27:29,883 - INFO - validation batch 301, loss: 0.219, 9632/10016 datapoints
2025-03-06 20:27:29,922 - INFO - Epoch 728/800 done.
2025-03-06 20:27:29,923 - INFO - Final validation performance:
Loss: 0.157, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:27:29,923 - INFO - Beginning epoch 729/800
2025-03-06 20:27:29,930 - INFO - training batch 1, loss: 0.158, 32/60000 datapoints
2025-03-06 20:27:30,137 - INFO - training batch 51, loss: 0.360, 1632/60000 datapoints
2025-03-06 20:27:30,329 - INFO - training batch 101, loss: 0.139, 3232/60000 datapoints
2025-03-06 20:27:30,528 - INFO - training batch 151, loss: 0.200, 4832/60000 datapoints
2025-03-06 20:27:30,724 - INFO - training batch 201, loss: 0.252, 6432/60000 datapoints
2025-03-06 20:27:30,920 - INFO - training batch 251, loss: 0.231, 8032/60000 datapoints
2025-03-06 20:27:31,112 - INFO - training batch 301, loss: 0.098, 9632/60000 datapoints
2025-03-06 20:27:31,303 - INFO - training batch 351, loss: 0.057, 11232/60000 datapoints
2025-03-06 20:27:31,498 - INFO - training batch 401, loss: 0.132, 12832/60000 datapoints
2025-03-06 20:27:31,693 - INFO - training batch 451, loss: 0.500, 14432/60000 datapoints
2025-03-06 20:27:31,886 - INFO - training batch 501, loss: 0.256, 16032/60000 datapoints
2025-03-06 20:27:32,077 - INFO - training batch 551, loss: 0.135, 17632/60000 datapoints
2025-03-06 20:27:32,270 - INFO - training batch 601, loss: 0.178, 19232/60000 datapoints
2025-03-06 20:27:32,464 - INFO - training batch 651, loss: 0.180, 20832/60000 datapoints
2025-03-06 20:27:32,658 - INFO - training batch 701, loss: 0.238, 22432/60000 datapoints
2025-03-06 20:27:32,849 - INFO - training batch 751, loss: 0.389, 24032/60000 datapoints
2025-03-06 20:27:33,041 - INFO - training batch 801, loss: 0.245, 25632/60000 datapoints
2025-03-06 20:27:33,234 - INFO - training batch 851, loss: 0.256, 27232/60000 datapoints
2025-03-06 20:27:33,426 - INFO - training batch 901, loss: 0.820, 28832/60000 datapoints
2025-03-06 20:27:33,619 - INFO - training batch 951, loss: 0.078, 30432/60000 datapoints
2025-03-06 20:27:33,814 - INFO - training batch 1001, loss: 0.400, 32032/60000 datapoints
2025-03-06 20:27:34,011 - INFO - training batch 1051, loss: 0.171, 33632/60000 datapoints
2025-03-06 20:27:34,203 - INFO - training batch 1101, loss: 0.108, 35232/60000 datapoints
2025-03-06 20:27:34,395 - INFO - training batch 1151, loss: 0.066, 36832/60000 datapoints
2025-03-06 20:27:34,586 - INFO - training batch 1201, loss: 0.525, 38432/60000 datapoints
2025-03-06 20:27:34,779 - INFO - training batch 1251, loss: 0.091, 40032/60000 datapoints
2025-03-06 20:27:34,979 - INFO - training batch 1301, loss: 0.139, 41632/60000 datapoints
2025-03-06 20:27:35,173 - INFO - training batch 1351, loss: 0.233, 43232/60000 datapoints
2025-03-06 20:27:35,364 - INFO - training batch 1401, loss: 0.076, 44832/60000 datapoints
2025-03-06 20:27:35,560 - INFO - training batch 1451, loss: 0.057, 46432/60000 datapoints
2025-03-06 20:27:35,755 - INFO - training batch 1501, loss: 0.264, 48032/60000 datapoints
2025-03-06 20:27:35,950 - INFO - training batch 1551, loss: 0.088, 49632/60000 datapoints
2025-03-06 20:27:36,141 - INFO - training batch 1601, loss: 0.051, 51232/60000 datapoints
2025-03-06 20:27:36,340 - INFO - training batch 1651, loss: 0.141, 52832/60000 datapoints
2025-03-06 20:27:36,532 - INFO - training batch 1701, loss: 0.148, 54432/60000 datapoints
2025-03-06 20:27:36,724 - INFO - training batch 1751, loss: 0.324, 56032/60000 datapoints
2025-03-06 20:27:36,917 - INFO - training batch 1801, loss: 0.126, 57632/60000 datapoints
2025-03-06 20:27:37,109 - INFO - training batch 1851, loss: 0.414, 59232/60000 datapoints
2025-03-06 20:27:37,209 - INFO - validation batch 1, loss: 0.124, 32/10016 datapoints
2025-03-06 20:27:37,359 - INFO - validation batch 51, loss: 0.478, 1632/10016 datapoints
2025-03-06 20:27:37,512 - INFO - validation batch 101, loss: 0.297, 3232/10016 datapoints
2025-03-06 20:27:37,680 - INFO - validation batch 151, loss: 0.187, 4832/10016 datapoints
2025-03-06 20:27:37,831 - INFO - validation batch 201, loss: 0.085, 6432/10016 datapoints
2025-03-06 20:27:37,983 - INFO - validation batch 251, loss: 0.170, 8032/10016 datapoints
2025-03-06 20:27:38,134 - INFO - validation batch 301, loss: 0.202, 9632/10016 datapoints
2025-03-06 20:27:38,170 - INFO - Epoch 729/800 done.
2025-03-06 20:27:38,170 - INFO - Final validation performance:
Loss: 0.221, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:27:38,170 - INFO - Beginning epoch 730/800
2025-03-06 20:27:38,177 - INFO - training batch 1, loss: 0.106, 32/60000 datapoints
2025-03-06 20:27:38,399 - INFO - training batch 51, loss: 0.333, 1632/60000 datapoints
2025-03-06 20:27:38,595 - INFO - training batch 101, loss: 0.316, 3232/60000 datapoints
2025-03-06 20:27:38,793 - INFO - training batch 151, loss: 0.221, 4832/60000 datapoints
2025-03-06 20:27:38,985 - INFO - training batch 201, loss: 0.369, 6432/60000 datapoints
2025-03-06 20:27:39,185 - INFO - training batch 251, loss: 0.135, 8032/60000 datapoints
2025-03-06 20:27:39,376 - INFO - training batch 301, loss: 0.058, 9632/60000 datapoints
2025-03-06 20:27:39,570 - INFO - training batch 351, loss: 0.245, 11232/60000 datapoints
2025-03-06 20:27:39,765 - INFO - training batch 401, loss: 0.268, 12832/60000 datapoints
2025-03-06 20:27:39,958 - INFO - training batch 451, loss: 0.053, 14432/60000 datapoints
2025-03-06 20:27:40,157 - INFO - training batch 501, loss: 0.251, 16032/60000 datapoints
2025-03-06 20:27:40,350 - INFO - training batch 551, loss: 0.316, 17632/60000 datapoints
2025-03-06 20:27:40,542 - INFO - training batch 601, loss: 0.128, 19232/60000 datapoints
2025-03-06 20:27:40,739 - INFO - training batch 651, loss: 0.130, 20832/60000 datapoints
2025-03-06 20:27:40,934 - INFO - training batch 701, loss: 0.412, 22432/60000 datapoints
2025-03-06 20:27:41,133 - INFO - training batch 751, loss: 0.081, 24032/60000 datapoints
2025-03-06 20:27:41,347 - INFO - training batch 801, loss: 0.321, 25632/60000 datapoints
2025-03-06 20:27:41,541 - INFO - training batch 851, loss: 0.303, 27232/60000 datapoints
2025-03-06 20:27:41,735 - INFO - training batch 901, loss: 0.252, 28832/60000 datapoints
2025-03-06 20:27:41,930 - INFO - training batch 951, loss: 0.372, 30432/60000 datapoints
2025-03-06 20:27:42,126 - INFO - training batch 1001, loss: 0.056, 32032/60000 datapoints
2025-03-06 20:27:42,320 - INFO - training batch 1051, loss: 0.148, 33632/60000 datapoints
2025-03-06 20:27:42,512 - INFO - training batch 1101, loss: 0.144, 35232/60000 datapoints
2025-03-06 20:27:42,706 - INFO - training batch 1151, loss: 0.107, 36832/60000 datapoints
2025-03-06 20:27:42,900 - INFO - training batch 1201, loss: 0.227, 38432/60000 datapoints
2025-03-06 20:27:43,093 - INFO - training batch 1251, loss: 0.340, 40032/60000 datapoints
2025-03-06 20:27:43,284 - INFO - training batch 1301, loss: 0.098, 41632/60000 datapoints
2025-03-06 20:27:43,474 - INFO - training batch 1351, loss: 0.111, 43232/60000 datapoints
2025-03-06 20:27:43,688 - INFO - training batch 1401, loss: 0.218, 44832/60000 datapoints
2025-03-06 20:27:43,880 - INFO - training batch 1451, loss: 0.312, 46432/60000 datapoints
2025-03-06 20:27:44,070 - INFO - training batch 1501, loss: 0.284, 48032/60000 datapoints
2025-03-06 20:27:44,260 - INFO - training batch 1551, loss: 0.423, 49632/60000 datapoints
2025-03-06 20:27:44,451 - INFO - training batch 1601, loss: 0.298, 51232/60000 datapoints
2025-03-06 20:27:44,644 - INFO - training batch 1651, loss: 0.202, 52832/60000 datapoints
2025-03-06 20:27:44,836 - INFO - training batch 1701, loss: 0.210, 54432/60000 datapoints
2025-03-06 20:27:45,032 - INFO - training batch 1751, loss: 0.092, 56032/60000 datapoints
2025-03-06 20:27:45,225 - INFO - training batch 1801, loss: 0.095, 57632/60000 datapoints
2025-03-06 20:27:45,418 - INFO - training batch 1851, loss: 0.135, 59232/60000 datapoints
2025-03-06 20:27:45,520 - INFO - validation batch 1, loss: 0.165, 32/10016 datapoints
2025-03-06 20:27:45,670 - INFO - validation batch 51, loss: 0.354, 1632/10016 datapoints
2025-03-06 20:27:45,823 - INFO - validation batch 101, loss: 0.113, 3232/10016 datapoints
2025-03-06 20:27:45,976 - INFO - validation batch 151, loss: 0.139, 4832/10016 datapoints
2025-03-06 20:27:46,128 - INFO - validation batch 201, loss: 0.410, 6432/10016 datapoints
2025-03-06 20:27:46,285 - INFO - validation batch 251, loss: 0.185, 8032/10016 datapoints
2025-03-06 20:27:46,436 - INFO - validation batch 301, loss: 0.405, 9632/10016 datapoints
2025-03-06 20:27:46,472 - INFO - Epoch 730/800 done.
2025-03-06 20:27:46,472 - INFO - Final validation performance:
Loss: 0.253, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:27:46,473 - INFO - Beginning epoch 731/800
2025-03-06 20:27:46,480 - INFO - training batch 1, loss: 0.228, 32/60000 datapoints
2025-03-06 20:27:46,686 - INFO - training batch 51, loss: 0.096, 1632/60000 datapoints
2025-03-06 20:27:46,880 - INFO - training batch 101, loss: 0.244, 3232/60000 datapoints
2025-03-06 20:27:47,077 - INFO - training batch 151, loss: 0.173, 4832/60000 datapoints
2025-03-06 20:27:47,271 - INFO - training batch 201, loss: 0.160, 6432/60000 datapoints
2025-03-06 20:27:47,467 - INFO - training batch 251, loss: 0.426, 8032/60000 datapoints
2025-03-06 20:27:47,669 - INFO - training batch 301, loss: 0.079, 9632/60000 datapoints
2025-03-06 20:27:47,882 - INFO - training batch 351, loss: 0.278, 11232/60000 datapoints
2025-03-06 20:27:48,077 - INFO - training batch 401, loss: 0.097, 12832/60000 datapoints
2025-03-06 20:27:48,275 - INFO - training batch 451, loss: 0.146, 14432/60000 datapoints
2025-03-06 20:27:48,487 - INFO - training batch 501, loss: 0.229, 16032/60000 datapoints
2025-03-06 20:27:48,692 - INFO - training batch 551, loss: 0.080, 17632/60000 datapoints
2025-03-06 20:27:48,887 - INFO - training batch 601, loss: 0.425, 19232/60000 datapoints
2025-03-06 20:27:49,082 - INFO - training batch 651, loss: 0.166, 20832/60000 datapoints
2025-03-06 20:27:49,283 - INFO - training batch 701, loss: 0.182, 22432/60000 datapoints
2025-03-06 20:27:49,480 - INFO - training batch 751, loss: 0.365, 24032/60000 datapoints
2025-03-06 20:27:49,681 - INFO - training batch 801, loss: 0.195, 25632/60000 datapoints
2025-03-06 20:27:49,877 - INFO - training batch 851, loss: 0.184, 27232/60000 datapoints
2025-03-06 20:27:50,073 - INFO - training batch 901, loss: 0.580, 28832/60000 datapoints
2025-03-06 20:27:50,269 - INFO - training batch 951, loss: 0.206, 30432/60000 datapoints
2025-03-06 20:27:50,464 - INFO - training batch 1001, loss: 0.269, 32032/60000 datapoints
2025-03-06 20:27:50,659 - INFO - training batch 1051, loss: 0.427, 33632/60000 datapoints
2025-03-06 20:27:50,856 - INFO - training batch 1101, loss: 0.070, 35232/60000 datapoints
2025-03-06 20:27:51,049 - INFO - training batch 1151, loss: 0.188, 36832/60000 datapoints
2025-03-06 20:27:51,243 - INFO - training batch 1201, loss: 0.099, 38432/60000 datapoints
2025-03-06 20:27:51,440 - INFO - training batch 1251, loss: 0.112, 40032/60000 datapoints
2025-03-06 20:27:51,637 - INFO - training batch 1301, loss: 0.171, 41632/60000 datapoints
2025-03-06 20:27:51,828 - INFO - training batch 1351, loss: 0.306, 43232/60000 datapoints
2025-03-06 20:27:52,025 - INFO - training batch 1401, loss: 0.110, 44832/60000 datapoints
2025-03-06 20:27:52,226 - INFO - training batch 1451, loss: 0.452, 46432/60000 datapoints
2025-03-06 20:27:52,422 - INFO - training batch 1501, loss: 0.353, 48032/60000 datapoints
2025-03-06 20:27:52,616 - INFO - training batch 1551, loss: 0.117, 49632/60000 datapoints
2025-03-06 20:27:52,812 - INFO - training batch 1601, loss: 0.151, 51232/60000 datapoints
2025-03-06 20:27:53,005 - INFO - training batch 1651, loss: 0.031, 52832/60000 datapoints
2025-03-06 20:27:53,200 - INFO - training batch 1701, loss: 0.115, 54432/60000 datapoints
2025-03-06 20:27:53,393 - INFO - training batch 1751, loss: 0.228, 56032/60000 datapoints
2025-03-06 20:27:53,588 - INFO - training batch 1801, loss: 0.143, 57632/60000 datapoints
2025-03-06 20:27:53,783 - INFO - training batch 1851, loss: 0.172, 59232/60000 datapoints
2025-03-06 20:27:53,885 - INFO - validation batch 1, loss: 0.215, 32/10016 datapoints
2025-03-06 20:27:54,041 - INFO - validation batch 51, loss: 0.198, 1632/10016 datapoints
2025-03-06 20:27:54,199 - INFO - validation batch 101, loss: 0.116, 3232/10016 datapoints
2025-03-06 20:27:54,352 - INFO - validation batch 151, loss: 0.187, 4832/10016 datapoints
2025-03-06 20:27:54,508 - INFO - validation batch 201, loss: 0.195, 6432/10016 datapoints
2025-03-06 20:27:54,674 - INFO - validation batch 251, loss: 0.251, 8032/10016 datapoints
2025-03-06 20:27:54,894 - INFO - validation batch 301, loss: 0.196, 9632/10016 datapoints
2025-03-06 20:27:54,947 - INFO - Epoch 731/800 done.
2025-03-06 20:27:54,947 - INFO - Final validation performance:
Loss: 0.194, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:27:54,948 - INFO - Beginning epoch 732/800
2025-03-06 20:27:54,956 - INFO - training batch 1, loss: 0.138, 32/60000 datapoints
2025-03-06 20:27:55,231 - INFO - training batch 51, loss: 0.183, 1632/60000 datapoints
2025-03-06 20:27:55,462 - INFO - training batch 101, loss: 0.126, 3232/60000 datapoints
2025-03-06 20:27:55,663 - INFO - training batch 151, loss: 0.266, 4832/60000 datapoints
2025-03-06 20:27:55,872 - INFO - training batch 201, loss: 0.250, 6432/60000 datapoints
2025-03-06 20:27:56,072 - INFO - training batch 251, loss: 0.188, 8032/60000 datapoints
2025-03-06 20:27:56,286 - INFO - training batch 301, loss: 0.508, 9632/60000 datapoints
2025-03-06 20:27:56,483 - INFO - training batch 351, loss: 0.398, 11232/60000 datapoints
2025-03-06 20:27:56,682 - INFO - training batch 401, loss: 0.733, 12832/60000 datapoints
2025-03-06 20:27:56,876 - INFO - training batch 451, loss: 0.212, 14432/60000 datapoints
2025-03-06 20:27:57,075 - INFO - training batch 501, loss: 0.135, 16032/60000 datapoints
2025-03-06 20:27:57,268 - INFO - training batch 551, loss: 0.108, 17632/60000 datapoints
2025-03-06 20:27:57,466 - INFO - training batch 601, loss: 0.234, 19232/60000 datapoints
2025-03-06 20:27:57,665 - INFO - training batch 651, loss: 0.069, 20832/60000 datapoints
2025-03-06 20:27:57,860 - INFO - training batch 701, loss: 0.071, 22432/60000 datapoints
2025-03-06 20:27:58,056 - INFO - training batch 751, loss: 0.096, 24032/60000 datapoints
2025-03-06 20:27:58,252 - INFO - training batch 801, loss: 0.141, 25632/60000 datapoints
2025-03-06 20:27:58,446 - INFO - training batch 851, loss: 0.077, 27232/60000 datapoints
2025-03-06 20:27:58,664 - INFO - training batch 901, loss: 0.227, 28832/60000 datapoints
2025-03-06 20:27:58,858 - INFO - training batch 951, loss: 0.180, 30432/60000 datapoints
2025-03-06 20:27:59,055 - INFO - training batch 1001, loss: 0.346, 32032/60000 datapoints
2025-03-06 20:27:59,252 - INFO - training batch 1051, loss: 0.281, 33632/60000 datapoints
2025-03-06 20:27:59,448 - INFO - training batch 1101, loss: 0.116, 35232/60000 datapoints
2025-03-06 20:27:59,647 - INFO - training batch 1151, loss: 0.083, 36832/60000 datapoints
2025-03-06 20:27:59,841 - INFO - training batch 1201, loss: 0.282, 38432/60000 datapoints
2025-03-06 20:28:00,037 - INFO - training batch 1251, loss: 0.245, 40032/60000 datapoints
2025-03-06 20:28:00,230 - INFO - training batch 1301, loss: 0.208, 41632/60000 datapoints
2025-03-06 20:28:00,423 - INFO - training batch 1351, loss: 0.310, 43232/60000 datapoints
2025-03-06 20:28:00,617 - INFO - training batch 1401, loss: 0.144, 44832/60000 datapoints
2025-03-06 20:28:00,811 - INFO - training batch 1451, loss: 0.234, 46432/60000 datapoints
2025-03-06 20:28:01,007 - INFO - training batch 1501, loss: 0.150, 48032/60000 datapoints
2025-03-06 20:28:01,201 - INFO - training batch 1551, loss: 0.103, 49632/60000 datapoints
2025-03-06 20:28:01,393 - INFO - training batch 1601, loss: 0.105, 51232/60000 datapoints
2025-03-06 20:28:01,589 - INFO - training batch 1651, loss: 0.077, 52832/60000 datapoints
2025-03-06 20:28:01,785 - INFO - training batch 1701, loss: 0.325, 54432/60000 datapoints
2025-03-06 20:28:01,984 - INFO - training batch 1751, loss: 0.547, 56032/60000 datapoints
2025-03-06 20:28:02,177 - INFO - training batch 1801, loss: 0.096, 57632/60000 datapoints
2025-03-06 20:28:02,371 - INFO - training batch 1851, loss: 0.121, 59232/60000 datapoints
2025-03-06 20:28:02,474 - INFO - validation batch 1, loss: 0.067, 32/10016 datapoints
2025-03-06 20:28:02,627 - INFO - validation batch 51, loss: 0.153, 1632/10016 datapoints
2025-03-06 20:28:02,781 - INFO - validation batch 101, loss: 0.217, 3232/10016 datapoints
2025-03-06 20:28:02,933 - INFO - validation batch 151, loss: 0.239, 4832/10016 datapoints
2025-03-06 20:28:03,094 - INFO - validation batch 201, loss: 0.137, 6432/10016 datapoints
2025-03-06 20:28:03,251 - INFO - validation batch 251, loss: 0.218, 8032/10016 datapoints
2025-03-06 20:28:03,403 - INFO - validation batch 301, loss: 0.315, 9632/10016 datapoints
2025-03-06 20:28:03,444 - INFO - Epoch 732/800 done.
2025-03-06 20:28:03,444 - INFO - Final validation performance:
Loss: 0.192, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:28:03,445 - INFO - Beginning epoch 733/800
2025-03-06 20:28:03,452 - INFO - training batch 1, loss: 0.218, 32/60000 datapoints
2025-03-06 20:28:03,662 - INFO - training batch 51, loss: 0.113, 1632/60000 datapoints
2025-03-06 20:28:03,855 - INFO - training batch 101, loss: 0.122, 3232/60000 datapoints
2025-03-06 20:28:04,057 - INFO - training batch 151, loss: 0.165, 4832/60000 datapoints
2025-03-06 20:28:04,253 - INFO - training batch 201, loss: 0.150, 6432/60000 datapoints
2025-03-06 20:28:04,453 - INFO - training batch 251, loss: 0.364, 8032/60000 datapoints
2025-03-06 20:28:04,650 - INFO - training batch 301, loss: 0.220, 9632/60000 datapoints
2025-03-06 20:28:04,845 - INFO - training batch 351, loss: 0.276, 11232/60000 datapoints
2025-03-06 20:28:05,050 - INFO - training batch 401, loss: 0.040, 12832/60000 datapoints
2025-03-06 20:28:05,245 - INFO - training batch 451, loss: 0.093, 14432/60000 datapoints
2025-03-06 20:28:05,440 - INFO - training batch 501, loss: 0.223, 16032/60000 datapoints
2025-03-06 20:28:05,650 - INFO - training batch 551, loss: 0.818, 17632/60000 datapoints
2025-03-06 20:28:05,858 - INFO - training batch 601, loss: 0.102, 19232/60000 datapoints
2025-03-06 20:28:06,061 - INFO - training batch 651, loss: 0.208, 20832/60000 datapoints
2025-03-06 20:28:06,256 - INFO - training batch 701, loss: 0.189, 22432/60000 datapoints
2025-03-06 20:28:06,455 - INFO - training batch 751, loss: 0.124, 24032/60000 datapoints
2025-03-06 20:28:06,653 - INFO - training batch 801, loss: 0.326, 25632/60000 datapoints
2025-03-06 20:28:06,845 - INFO - training batch 851, loss: 0.449, 27232/60000 datapoints
2025-03-06 20:28:07,042 - INFO - training batch 901, loss: 0.314, 28832/60000 datapoints
2025-03-06 20:28:07,237 - INFO - training batch 951, loss: 0.585, 30432/60000 datapoints
2025-03-06 20:28:07,431 - INFO - training batch 1001, loss: 0.189, 32032/60000 datapoints
2025-03-06 20:28:07,629 - INFO - training batch 1051, loss: 0.570, 33632/60000 datapoints
2025-03-06 20:28:07,823 - INFO - training batch 1101, loss: 0.305, 35232/60000 datapoints
2025-03-06 20:28:08,020 - INFO - training batch 1151, loss: 0.299, 36832/60000 datapoints
2025-03-06 20:28:08,219 - INFO - training batch 1201, loss: 0.190, 38432/60000 datapoints
2025-03-06 20:28:08,414 - INFO - training batch 1251, loss: 0.348, 40032/60000 datapoints
2025-03-06 20:28:08,615 - INFO - training batch 1301, loss: 0.233, 41632/60000 datapoints
2025-03-06 20:28:08,826 - INFO - training batch 1351, loss: 0.054, 43232/60000 datapoints
2025-03-06 20:28:09,023 - INFO - training batch 1401, loss: 0.280, 44832/60000 datapoints
2025-03-06 20:28:09,218 - INFO - training batch 1451, loss: 0.146, 46432/60000 datapoints
2025-03-06 20:28:09,413 - INFO - training batch 1501, loss: 0.239, 48032/60000 datapoints
2025-03-06 20:28:09,609 - INFO - training batch 1551, loss: 0.178, 49632/60000 datapoints
2025-03-06 20:28:09,805 - INFO - training batch 1601, loss: 0.058, 51232/60000 datapoints
2025-03-06 20:28:10,003 - INFO - training batch 1651, loss: 0.210, 52832/60000 datapoints
2025-03-06 20:28:10,197 - INFO - training batch 1701, loss: 0.105, 54432/60000 datapoints
2025-03-06 20:28:10,391 - INFO - training batch 1751, loss: 0.078, 56032/60000 datapoints
2025-03-06 20:28:10,586 - INFO - training batch 1801, loss: 0.155, 57632/60000 datapoints
2025-03-06 20:28:10,785 - INFO - training batch 1851, loss: 0.316, 59232/60000 datapoints
2025-03-06 20:28:10,893 - INFO - validation batch 1, loss: 0.069, 32/10016 datapoints
2025-03-06 20:28:11,054 - INFO - validation batch 51, loss: 0.259, 1632/10016 datapoints
2025-03-06 20:28:11,209 - INFO - validation batch 101, loss: 0.312, 3232/10016 datapoints
2025-03-06 20:28:11,365 - INFO - validation batch 151, loss: 0.310, 4832/10016 datapoints
2025-03-06 20:28:11,519 - INFO - validation batch 201, loss: 0.351, 6432/10016 datapoints
2025-03-06 20:28:11,677 - INFO - validation batch 251, loss: 0.209, 8032/10016 datapoints
2025-03-06 20:28:11,829 - INFO - validation batch 301, loss: 0.368, 9632/10016 datapoints
2025-03-06 20:28:11,866 - INFO - Epoch 733/800 done.
2025-03-06 20:28:11,866 - INFO - Final validation performance:
Loss: 0.268, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:28:11,867 - INFO - Beginning epoch 734/800
2025-03-06 20:28:11,874 - INFO - training batch 1, loss: 0.107, 32/60000 datapoints
2025-03-06 20:28:12,079 - INFO - training batch 51, loss: 0.122, 1632/60000 datapoints
2025-03-06 20:28:12,284 - INFO - training batch 101, loss: 0.148, 3232/60000 datapoints
2025-03-06 20:28:12,481 - INFO - training batch 151, loss: 0.183, 4832/60000 datapoints
2025-03-06 20:28:12,687 - INFO - training batch 201, loss: 0.183, 6432/60000 datapoints
2025-03-06 20:28:12,885 - INFO - training batch 251, loss: 0.294, 8032/60000 datapoints
2025-03-06 20:28:13,085 - INFO - training batch 301, loss: 0.209, 9632/60000 datapoints
2025-03-06 20:28:13,280 - INFO - training batch 351, loss: 0.160, 11232/60000 datapoints
2025-03-06 20:28:13,474 - INFO - training batch 401, loss: 0.181, 12832/60000 datapoints
2025-03-06 20:28:13,675 - INFO - training batch 451, loss: 0.170, 14432/60000 datapoints
2025-03-06 20:28:13,869 - INFO - training batch 501, loss: 0.310, 16032/60000 datapoints
2025-03-06 20:28:14,068 - INFO - training batch 551, loss: 0.275, 17632/60000 datapoints
2025-03-06 20:28:14,266 - INFO - training batch 601, loss: 0.260, 19232/60000 datapoints
2025-03-06 20:28:14,460 - INFO - training batch 651, loss: 0.098, 20832/60000 datapoints
2025-03-06 20:28:14,658 - INFO - training batch 701, loss: 0.357, 22432/60000 datapoints
2025-03-06 20:28:14,853 - INFO - training batch 751, loss: 0.064, 24032/60000 datapoints
2025-03-06 20:28:15,054 - INFO - training batch 801, loss: 0.173, 25632/60000 datapoints
2025-03-06 20:28:15,249 - INFO - training batch 851, loss: 0.078, 27232/60000 datapoints
2025-03-06 20:28:15,444 - INFO - training batch 901, loss: 0.213, 28832/60000 datapoints
2025-03-06 20:28:15,644 - INFO - training batch 951, loss: 0.061, 30432/60000 datapoints
2025-03-06 20:28:15,839 - INFO - training batch 1001, loss: 0.220, 32032/60000 datapoints
2025-03-06 20:28:16,033 - INFO - training batch 1051, loss: 0.549, 33632/60000 datapoints
2025-03-06 20:28:16,230 - INFO - training batch 1101, loss: 0.255, 35232/60000 datapoints
2025-03-06 20:28:16,427 - INFO - training batch 1151, loss: 0.108, 36832/60000 datapoints
2025-03-06 20:28:16,632 - INFO - training batch 1201, loss: 0.238, 38432/60000 datapoints
2025-03-06 20:28:16,825 - INFO - training batch 1251, loss: 0.315, 40032/60000 datapoints
2025-03-06 20:28:17,019 - INFO - training batch 1301, loss: 0.135, 41632/60000 datapoints
2025-03-06 20:28:17,215 - INFO - training batch 1351, loss: 0.189, 43232/60000 datapoints
2025-03-06 20:28:17,412 - INFO - training batch 1401, loss: 0.307, 44832/60000 datapoints
2025-03-06 20:28:17,607 - INFO - training batch 1451, loss: 0.091, 46432/60000 datapoints
2025-03-06 20:28:17,803 - INFO - training batch 1501, loss: 0.202, 48032/60000 datapoints
2025-03-06 20:28:17,998 - INFO - training batch 1551, loss: 0.090, 49632/60000 datapoints
2025-03-06 20:28:18,195 - INFO - training batch 1601, loss: 0.182, 51232/60000 datapoints
2025-03-06 20:28:18,389 - INFO - training batch 1651, loss: 0.170, 52832/60000 datapoints
2025-03-06 20:28:18,599 - INFO - training batch 1701, loss: 0.719, 54432/60000 datapoints
2025-03-06 20:28:18,826 - INFO - training batch 1751, loss: 0.256, 56032/60000 datapoints
2025-03-06 20:28:19,021 - INFO - training batch 1801, loss: 0.216, 57632/60000 datapoints
2025-03-06 20:28:19,214 - INFO - training batch 1851, loss: 0.152, 59232/60000 datapoints
2025-03-06 20:28:19,315 - INFO - validation batch 1, loss: 0.280, 32/10016 datapoints
2025-03-06 20:28:19,471 - INFO - validation batch 51, loss: 0.101, 1632/10016 datapoints
2025-03-06 20:28:19,629 - INFO - validation batch 101, loss: 0.210, 3232/10016 datapoints
2025-03-06 20:28:19,784 - INFO - validation batch 151, loss: 0.197, 4832/10016 datapoints
2025-03-06 20:28:19,937 - INFO - validation batch 201, loss: 0.259, 6432/10016 datapoints
2025-03-06 20:28:20,094 - INFO - validation batch 251, loss: 0.120, 8032/10016 datapoints
2025-03-06 20:28:20,248 - INFO - validation batch 301, loss: 0.152, 9632/10016 datapoints
2025-03-06 20:28:20,286 - INFO - Epoch 734/800 done.
2025-03-06 20:28:20,286 - INFO - Final validation performance:
Loss: 0.188, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:28:20,287 - INFO - Beginning epoch 735/800
2025-03-06 20:28:20,293 - INFO - training batch 1, loss: 0.443, 32/60000 datapoints
2025-03-06 20:28:20,498 - INFO - training batch 51, loss: 0.128, 1632/60000 datapoints
2025-03-06 20:28:20,698 - INFO - training batch 101, loss: 0.415, 3232/60000 datapoints
2025-03-06 20:28:20,898 - INFO - training batch 151, loss: 0.058, 4832/60000 datapoints
2025-03-06 20:28:21,093 - INFO - training batch 201, loss: 0.259, 6432/60000 datapoints
2025-03-06 20:28:21,292 - INFO - training batch 251, loss: 0.254, 8032/60000 datapoints
2025-03-06 20:28:21,486 - INFO - training batch 301, loss: 0.061, 9632/60000 datapoints
2025-03-06 20:28:21,687 - INFO - training batch 351, loss: 0.223, 11232/60000 datapoints
2025-03-06 20:28:21,882 - INFO - training batch 401, loss: 0.056, 12832/60000 datapoints
2025-03-06 20:28:22,078 - INFO - training batch 451, loss: 0.117, 14432/60000 datapoints
2025-03-06 20:28:22,273 - INFO - training batch 501, loss: 0.097, 16032/60000 datapoints
2025-03-06 20:28:22,468 - INFO - training batch 551, loss: 0.179, 17632/60000 datapoints
2025-03-06 20:28:22,666 - INFO - training batch 601, loss: 0.559, 19232/60000 datapoints
2025-03-06 20:28:22,862 - INFO - training batch 651, loss: 0.123, 20832/60000 datapoints
2025-03-06 20:28:23,056 - INFO - training batch 701, loss: 0.153, 22432/60000 datapoints
2025-03-06 20:28:23,251 - INFO - training batch 751, loss: 0.028, 24032/60000 datapoints
2025-03-06 20:28:23,443 - INFO - training batch 801, loss: 0.262, 25632/60000 datapoints
2025-03-06 20:28:23,642 - INFO - training batch 851, loss: 0.104, 27232/60000 datapoints
2025-03-06 20:28:23,836 - INFO - training batch 901, loss: 0.283, 28832/60000 datapoints
2025-03-06 20:28:24,031 - INFO - training batch 951, loss: 0.194, 30432/60000 datapoints
2025-03-06 20:28:24,225 - INFO - training batch 1001, loss: 0.096, 32032/60000 datapoints
2025-03-06 20:28:24,421 - INFO - training batch 1051, loss: 0.164, 33632/60000 datapoints
2025-03-06 20:28:24,618 - INFO - training batch 1101, loss: 0.402, 35232/60000 datapoints
2025-03-06 20:28:24,814 - INFO - training batch 1151, loss: 0.474, 36832/60000 datapoints
2025-03-06 20:28:25,014 - INFO - training batch 1201, loss: 0.362, 38432/60000 datapoints
2025-03-06 20:28:25,210 - INFO - training batch 1251, loss: 0.209, 40032/60000 datapoints
2025-03-06 20:28:25,406 - INFO - training batch 1301, loss: 0.058, 41632/60000 datapoints
2025-03-06 20:28:25,600 - INFO - training batch 1351, loss: 0.243, 43232/60000 datapoints
2025-03-06 20:28:25,797 - INFO - training batch 1401, loss: 0.223, 44832/60000 datapoints
2025-03-06 20:28:25,990 - INFO - training batch 1451, loss: 0.490, 46432/60000 datapoints
2025-03-06 20:28:26,186 - INFO - training batch 1501, loss: 0.428, 48032/60000 datapoints
2025-03-06 20:28:26,388 - INFO - training batch 1551, loss: 0.554, 49632/60000 datapoints
2025-03-06 20:28:26,581 - INFO - training batch 1601, loss: 0.224, 51232/60000 datapoints
2025-03-06 20:28:26,780 - INFO - training batch 1651, loss: 0.303, 52832/60000 datapoints
2025-03-06 20:28:26,973 - INFO - training batch 1701, loss: 0.234, 54432/60000 datapoints
2025-03-06 20:28:27,170 - INFO - training batch 1751, loss: 0.152, 56032/60000 datapoints
2025-03-06 20:28:27,365 - INFO - training batch 1801, loss: 0.309, 57632/60000 datapoints
2025-03-06 20:28:27,560 - INFO - training batch 1851, loss: 0.370, 59232/60000 datapoints
2025-03-06 20:28:27,667 - INFO - validation batch 1, loss: 0.242, 32/10016 datapoints
2025-03-06 20:28:27,821 - INFO - validation batch 51, loss: 0.094, 1632/10016 datapoints
2025-03-06 20:28:27,978 - INFO - validation batch 101, loss: 0.132, 3232/10016 datapoints
2025-03-06 20:28:28,133 - INFO - validation batch 151, loss: 0.173, 4832/10016 datapoints
2025-03-06 20:28:28,287 - INFO - validation batch 201, loss: 0.176, 6432/10016 datapoints
2025-03-06 20:28:28,441 - INFO - validation batch 251, loss: 0.118, 8032/10016 datapoints
2025-03-06 20:28:28,594 - INFO - validation batch 301, loss: 0.228, 9632/10016 datapoints
2025-03-06 20:28:28,634 - INFO - Epoch 735/800 done.
2025-03-06 20:28:28,634 - INFO - Final validation performance:
Loss: 0.166, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:28:28,635 - INFO - Beginning epoch 736/800
2025-03-06 20:28:28,642 - INFO - training batch 1, loss: 0.152, 32/60000 datapoints
2025-03-06 20:28:28,858 - INFO - training batch 51, loss: 0.149, 1632/60000 datapoints
2025-03-06 20:28:29,054 - INFO - training batch 101, loss: 0.144, 3232/60000 datapoints
2025-03-06 20:28:29,268 - INFO - training batch 151, loss: 0.142, 4832/60000 datapoints
2025-03-06 20:28:29,466 - INFO - training batch 201, loss: 0.415, 6432/60000 datapoints
2025-03-06 20:28:29,683 - INFO - training batch 251, loss: 0.143, 8032/60000 datapoints
2025-03-06 20:28:29,889 - INFO - training batch 301, loss: 0.160, 9632/60000 datapoints
2025-03-06 20:28:30,090 - INFO - training batch 351, loss: 0.405, 11232/60000 datapoints
2025-03-06 20:28:30,285 - INFO - training batch 401, loss: 0.089, 12832/60000 datapoints
2025-03-06 20:28:30,479 - INFO - training batch 451, loss: 0.186, 14432/60000 datapoints
2025-03-06 20:28:30,677 - INFO - training batch 501, loss: 0.153, 16032/60000 datapoints
2025-03-06 20:28:30,871 - INFO - training batch 551, loss: 0.325, 17632/60000 datapoints
2025-03-06 20:28:31,065 - INFO - training batch 601, loss: 0.298, 19232/60000 datapoints
2025-03-06 20:28:31,261 - INFO - training batch 651, loss: 0.308, 20832/60000 datapoints
2025-03-06 20:28:31,456 - INFO - training batch 701, loss: 0.150, 22432/60000 datapoints
2025-03-06 20:28:31,654 - INFO - training batch 751, loss: 0.275, 24032/60000 datapoints
2025-03-06 20:28:31,851 - INFO - training batch 801, loss: 0.142, 25632/60000 datapoints
2025-03-06 20:28:32,048 - INFO - training batch 851, loss: 0.373, 27232/60000 datapoints
2025-03-06 20:28:32,243 - INFO - training batch 901, loss: 0.186, 28832/60000 datapoints
2025-03-06 20:28:32,439 - INFO - training batch 951, loss: 0.284, 30432/60000 datapoints
2025-03-06 20:28:32,636 - INFO - training batch 1001, loss: 0.324, 32032/60000 datapoints
2025-03-06 20:28:32,831 - INFO - training batch 1051, loss: 0.288, 33632/60000 datapoints
2025-03-06 20:28:33,026 - INFO - training batch 1101, loss: 0.137, 35232/60000 datapoints
2025-03-06 20:28:33,219 - INFO - training batch 1151, loss: 0.203, 36832/60000 datapoints
2025-03-06 20:28:33,416 - INFO - training batch 1201, loss: 0.354, 38432/60000 datapoints
2025-03-06 20:28:33,610 - INFO - training batch 1251, loss: 0.187, 40032/60000 datapoints
2025-03-06 20:28:33,809 - INFO - training batch 1301, loss: 0.229, 41632/60000 datapoints
2025-03-06 20:28:34,004 - INFO - training batch 1351, loss: 0.184, 43232/60000 datapoints
2025-03-06 20:28:34,203 - INFO - training batch 1401, loss: 0.273, 44832/60000 datapoints
2025-03-06 20:28:34,397 - INFO - training batch 1451, loss: 0.079, 46432/60000 datapoints
2025-03-06 20:28:34,589 - INFO - training batch 1501, loss: 0.159, 48032/60000 datapoints
2025-03-06 20:28:34,786 - INFO - training batch 1551, loss: 0.178, 49632/60000 datapoints
2025-03-06 20:28:34,985 - INFO - training batch 1601, loss: 0.268, 51232/60000 datapoints
2025-03-06 20:28:35,181 - INFO - training batch 1651, loss: 0.326, 52832/60000 datapoints
2025-03-06 20:28:35,376 - INFO - training batch 1701, loss: 0.261, 54432/60000 datapoints
2025-03-06 20:28:35,569 - INFO - training batch 1751, loss: 0.134, 56032/60000 datapoints
2025-03-06 20:28:35,776 - INFO - training batch 1801, loss: 0.142, 57632/60000 datapoints
2025-03-06 20:28:35,973 - INFO - training batch 1851, loss: 0.781, 59232/60000 datapoints
2025-03-06 20:28:36,074 - INFO - validation batch 1, loss: 0.494, 32/10016 datapoints
2025-03-06 20:28:36,229 - INFO - validation batch 51, loss: 0.170, 1632/10016 datapoints
2025-03-06 20:28:36,388 - INFO - validation batch 101, loss: 0.147, 3232/10016 datapoints
2025-03-06 20:28:36,539 - INFO - validation batch 151, loss: 0.038, 4832/10016 datapoints
2025-03-06 20:28:36,698 - INFO - validation batch 201, loss: 0.206, 6432/10016 datapoints
2025-03-06 20:28:36,855 - INFO - validation batch 251, loss: 0.119, 8032/10016 datapoints
2025-03-06 20:28:37,005 - INFO - validation batch 301, loss: 0.127, 9632/10016 datapoints
2025-03-06 20:28:37,043 - INFO - Epoch 736/800 done.
2025-03-06 20:28:37,043 - INFO - Final validation performance:
Loss: 0.186, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:28:37,044 - INFO - Beginning epoch 737/800
2025-03-06 20:28:37,051 - INFO - training batch 1, loss: 0.206, 32/60000 datapoints
2025-03-06 20:28:37,259 - INFO - training batch 51, loss: 0.256, 1632/60000 datapoints
2025-03-06 20:28:37,455 - INFO - training batch 101, loss: 0.249, 3232/60000 datapoints
2025-03-06 20:28:37,671 - INFO - training batch 151, loss: 0.249, 4832/60000 datapoints
2025-03-06 20:28:37,904 - INFO - training batch 201, loss: 0.241, 6432/60000 datapoints
2025-03-06 20:28:38,101 - INFO - training batch 251, loss: 0.115, 8032/60000 datapoints
2025-03-06 20:28:38,298 - INFO - training batch 301, loss: 0.227, 9632/60000 datapoints
2025-03-06 20:28:38,490 - INFO - training batch 351, loss: 0.224, 11232/60000 datapoints
2025-03-06 20:28:38,690 - INFO - training batch 401, loss: 0.218, 12832/60000 datapoints
2025-03-06 20:28:38,900 - INFO - training batch 451, loss: 0.260, 14432/60000 datapoints
2025-03-06 20:28:39,105 - INFO - training batch 501, loss: 0.156, 16032/60000 datapoints
2025-03-06 20:28:39,299 - INFO - training batch 551, loss: 0.312, 17632/60000 datapoints
2025-03-06 20:28:39,495 - INFO - training batch 601, loss: 0.123, 19232/60000 datapoints
2025-03-06 20:28:39,696 - INFO - training batch 651, loss: 0.176, 20832/60000 datapoints
2025-03-06 20:28:39,892 - INFO - training batch 701, loss: 0.052, 22432/60000 datapoints
2025-03-06 20:28:40,090 - INFO - training batch 751, loss: 0.153, 24032/60000 datapoints
2025-03-06 20:28:40,285 - INFO - training batch 801, loss: 0.396, 25632/60000 datapoints
2025-03-06 20:28:40,486 - INFO - training batch 851, loss: 0.184, 27232/60000 datapoints
2025-03-06 20:28:40,683 - INFO - training batch 901, loss: 0.170, 28832/60000 datapoints
2025-03-06 20:28:40,880 - INFO - training batch 951, loss: 0.300, 30432/60000 datapoints
2025-03-06 20:28:41,075 - INFO - training batch 1001, loss: 0.102, 32032/60000 datapoints
2025-03-06 20:28:41,269 - INFO - training batch 1051, loss: 0.154, 33632/60000 datapoints
2025-03-06 20:28:41,492 - INFO - training batch 1101, loss: 0.106, 35232/60000 datapoints
2025-03-06 20:28:41,700 - INFO - training batch 1151, loss: 0.141, 36832/60000 datapoints
2025-03-06 20:28:41,897 - INFO - training batch 1201, loss: 0.119, 38432/60000 datapoints
2025-03-06 20:28:42,090 - INFO - training batch 1251, loss: 0.448, 40032/60000 datapoints
2025-03-06 20:28:42,282 - INFO - training batch 1301, loss: 0.304, 41632/60000 datapoints
2025-03-06 20:28:42,477 - INFO - training batch 1351, loss: 0.579, 43232/60000 datapoints
2025-03-06 20:28:42,674 - INFO - training batch 1401, loss: 0.155, 44832/60000 datapoints
2025-03-06 20:28:42,871 - INFO - training batch 1451, loss: 0.203, 46432/60000 datapoints
2025-03-06 20:28:43,067 - INFO - training batch 1501, loss: 0.197, 48032/60000 datapoints
2025-03-06 20:28:43,262 - INFO - training batch 1551, loss: 0.091, 49632/60000 datapoints
2025-03-06 20:28:43,457 - INFO - training batch 1601, loss: 0.304, 51232/60000 datapoints
2025-03-06 20:28:43,655 - INFO - training batch 1651, loss: 0.127, 52832/60000 datapoints
2025-03-06 20:28:43,927 - INFO - training batch 1701, loss: 0.162, 54432/60000 datapoints
2025-03-06 20:28:44,121 - INFO - training batch 1751, loss: 0.121, 56032/60000 datapoints
2025-03-06 20:28:44,313 - INFO - training batch 1801, loss: 0.268, 57632/60000 datapoints
2025-03-06 20:28:44,508 - INFO - training batch 1851, loss: 0.339, 59232/60000 datapoints
2025-03-06 20:28:44,609 - INFO - validation batch 1, loss: 0.068, 32/10016 datapoints
2025-03-06 20:28:44,765 - INFO - validation batch 51, loss: 0.147, 1632/10016 datapoints
2025-03-06 20:28:44,924 - INFO - validation batch 101, loss: 0.179, 3232/10016 datapoints
2025-03-06 20:28:45,079 - INFO - validation batch 151, loss: 0.237, 4832/10016 datapoints
2025-03-06 20:28:45,233 - INFO - validation batch 201, loss: 0.076, 6432/10016 datapoints
2025-03-06 20:28:45,389 - INFO - validation batch 251, loss: 0.435, 8032/10016 datapoints
2025-03-06 20:28:45,544 - INFO - validation batch 301, loss: 0.183, 9632/10016 datapoints
2025-03-06 20:28:45,582 - INFO - Epoch 737/800 done.
2025-03-06 20:28:45,582 - INFO - Final validation performance:
Loss: 0.189, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:28:45,583 - INFO - Beginning epoch 738/800
2025-03-06 20:28:45,589 - INFO - training batch 1, loss: 0.260, 32/60000 datapoints
2025-03-06 20:28:45,801 - INFO - training batch 51, loss: 0.330, 1632/60000 datapoints
2025-03-06 20:28:45,998 - INFO - training batch 101, loss: 0.060, 3232/60000 datapoints
2025-03-06 20:28:46,196 - INFO - training batch 151, loss: 0.264, 4832/60000 datapoints
2025-03-06 20:28:46,398 - INFO - training batch 201, loss: 0.722, 6432/60000 datapoints
2025-03-06 20:28:46,593 - INFO - training batch 251, loss: 0.099, 8032/60000 datapoints
2025-03-06 20:28:46,792 - INFO - training batch 301, loss: 0.278, 9632/60000 datapoints
2025-03-06 20:28:46,986 - INFO - training batch 351, loss: 0.214, 11232/60000 datapoints
2025-03-06 20:28:47,180 - INFO - training batch 401, loss: 0.247, 12832/60000 datapoints
2025-03-06 20:28:47,378 - INFO - training batch 451, loss: 0.161, 14432/60000 datapoints
2025-03-06 20:28:47,574 - INFO - training batch 501, loss: 0.113, 16032/60000 datapoints
2025-03-06 20:28:47,773 - INFO - training batch 551, loss: 0.031, 17632/60000 datapoints
2025-03-06 20:28:47,968 - INFO - training batch 601, loss: 0.318, 19232/60000 datapoints
2025-03-06 20:28:48,160 - INFO - training batch 651, loss: 0.157, 20832/60000 datapoints
2025-03-06 20:28:48,357 - INFO - training batch 701, loss: 0.135, 22432/60000 datapoints
2025-03-06 20:28:48,552 - INFO - training batch 751, loss: 0.263, 24032/60000 datapoints
2025-03-06 20:28:48,747 - INFO - training batch 801, loss: 0.361, 25632/60000 datapoints
2025-03-06 20:28:48,944 - INFO - training batch 851, loss: 0.255, 27232/60000 datapoints
2025-03-06 20:28:49,154 - INFO - training batch 901, loss: 0.104, 28832/60000 datapoints
2025-03-06 20:28:49,347 - INFO - training batch 951, loss: 0.119, 30432/60000 datapoints
2025-03-06 20:28:49,549 - INFO - training batch 1001, loss: 0.122, 32032/60000 datapoints
2025-03-06 20:28:49,750 - INFO - training batch 1051, loss: 0.070, 33632/60000 datapoints
2025-03-06 20:28:49,946 - INFO - training batch 1101, loss: 0.528, 35232/60000 datapoints
2025-03-06 20:28:50,144 - INFO - training batch 1151, loss: 0.050, 36832/60000 datapoints
2025-03-06 20:28:50,339 - INFO - training batch 1201, loss: 0.239, 38432/60000 datapoints
2025-03-06 20:28:50,536 - INFO - training batch 1251, loss: 0.135, 40032/60000 datapoints
2025-03-06 20:28:50,732 - INFO - training batch 1301, loss: 0.093, 41632/60000 datapoints
2025-03-06 20:28:50,926 - INFO - training batch 1351, loss: 0.177, 43232/60000 datapoints
2025-03-06 20:28:51,119 - INFO - training batch 1401, loss: 0.132, 44832/60000 datapoints
2025-03-06 20:28:51,316 - INFO - training batch 1451, loss: 0.154, 46432/60000 datapoints
2025-03-06 20:28:51,510 - INFO - training batch 1501, loss: 0.487, 48032/60000 datapoints
2025-03-06 20:28:51,706 - INFO - training batch 1551, loss: 0.220, 49632/60000 datapoints
2025-03-06 20:28:51,901 - INFO - training batch 1601, loss: 0.349, 51232/60000 datapoints
2025-03-06 20:28:52,096 - INFO - training batch 1651, loss: 0.103, 52832/60000 datapoints
2025-03-06 20:28:52,294 - INFO - training batch 1701, loss: 0.270, 54432/60000 datapoints
2025-03-06 20:28:52,491 - INFO - training batch 1751, loss: 0.253, 56032/60000 datapoints
2025-03-06 20:28:52,687 - INFO - training batch 1801, loss: 0.837, 57632/60000 datapoints
2025-03-06 20:28:52,881 - INFO - training batch 1851, loss: 0.125, 59232/60000 datapoints
2025-03-06 20:28:52,983 - INFO - validation batch 1, loss: 0.107, 32/10016 datapoints
2025-03-06 20:28:53,135 - INFO - validation batch 51, loss: 0.153, 1632/10016 datapoints
2025-03-06 20:28:53,289 - INFO - validation batch 101, loss: 0.160, 3232/10016 datapoints
2025-03-06 20:28:53,443 - INFO - validation batch 151, loss: 0.267, 4832/10016 datapoints
2025-03-06 20:28:53,597 - INFO - validation batch 201, loss: 0.133, 6432/10016 datapoints
2025-03-06 20:28:53,755 - INFO - validation batch 251, loss: 0.379, 8032/10016 datapoints
2025-03-06 20:28:53,909 - INFO - validation batch 301, loss: 0.151, 9632/10016 datapoints
2025-03-06 20:28:53,949 - INFO - Epoch 738/800 done.
2025-03-06 20:28:53,949 - INFO - Final validation performance:
Loss: 0.193, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:28:53,950 - INFO - Beginning epoch 739/800
2025-03-06 20:28:53,956 - INFO - training batch 1, loss: 0.139, 32/60000 datapoints
2025-03-06 20:28:54,169 - INFO - training batch 51, loss: 0.095, 1632/60000 datapoints
2025-03-06 20:28:54,417 - INFO - training batch 101, loss: 0.355, 3232/60000 datapoints
2025-03-06 20:28:54,631 - INFO - training batch 151, loss: 0.136, 4832/60000 datapoints
2025-03-06 20:28:54,828 - INFO - training batch 201, loss: 0.117, 6432/60000 datapoints
2025-03-06 20:28:55,035 - INFO - training batch 251, loss: 0.105, 8032/60000 datapoints
2025-03-06 20:28:55,230 - INFO - training batch 301, loss: 0.472, 9632/60000 datapoints
2025-03-06 20:28:55,425 - INFO - training batch 351, loss: 0.161, 11232/60000 datapoints
2025-03-06 20:28:55,626 - INFO - training batch 401, loss: 0.116, 12832/60000 datapoints
2025-03-06 20:28:55,827 - INFO - training batch 451, loss: 0.196, 14432/60000 datapoints
2025-03-06 20:28:56,024 - INFO - training batch 501, loss: 0.218, 16032/60000 datapoints
2025-03-06 20:28:56,218 - INFO - training batch 551, loss: 0.265, 17632/60000 datapoints
2025-03-06 20:28:56,421 - INFO - training batch 601, loss: 0.169, 19232/60000 datapoints
2025-03-06 20:28:56,618 - INFO - training batch 651, loss: 0.033, 20832/60000 datapoints
2025-03-06 20:28:56,816 - INFO - training batch 701, loss: 0.089, 22432/60000 datapoints
2025-03-06 20:28:57,010 - INFO - training batch 751, loss: 0.442, 24032/60000 datapoints
2025-03-06 20:28:57,202 - INFO - training batch 801, loss: 0.225, 25632/60000 datapoints
2025-03-06 20:28:57,395 - INFO - training batch 851, loss: 0.232, 27232/60000 datapoints
2025-03-06 20:28:57,590 - INFO - training batch 901, loss: 0.177, 28832/60000 datapoints
2025-03-06 20:28:57,790 - INFO - training batch 951, loss: 0.306, 30432/60000 datapoints
2025-03-06 20:28:57,986 - INFO - training batch 1001, loss: 0.266, 32032/60000 datapoints
2025-03-06 20:28:58,182 - INFO - training batch 1051, loss: 0.245, 33632/60000 datapoints
2025-03-06 20:28:58,376 - INFO - training batch 1101, loss: 0.140, 35232/60000 datapoints
2025-03-06 20:28:58,575 - INFO - training batch 1151, loss: 0.257, 36832/60000 datapoints
2025-03-06 20:28:58,771 - INFO - training batch 1201, loss: 0.183, 38432/60000 datapoints
2025-03-06 20:28:58,970 - INFO - training batch 1251, loss: 0.066, 40032/60000 datapoints
2025-03-06 20:28:59,189 - INFO - training batch 1301, loss: 0.257, 41632/60000 datapoints
2025-03-06 20:28:59,384 - INFO - training batch 1351, loss: 0.238, 43232/60000 datapoints
2025-03-06 20:28:59,583 - INFO - training batch 1401, loss: 0.111, 44832/60000 datapoints
2025-03-06 20:28:59,784 - INFO - training batch 1451, loss: 0.205, 46432/60000 datapoints
2025-03-06 20:28:59,981 - INFO - training batch 1501, loss: 0.183, 48032/60000 datapoints
2025-03-06 20:29:00,178 - INFO - training batch 1551, loss: 0.071, 49632/60000 datapoints
2025-03-06 20:29:00,371 - INFO - training batch 1601, loss: 0.106, 51232/60000 datapoints
2025-03-06 20:29:00,569 - INFO - training batch 1651, loss: 0.206, 52832/60000 datapoints
2025-03-06 20:29:00,767 - INFO - training batch 1701, loss: 0.121, 54432/60000 datapoints
2025-03-06 20:29:00,961 - INFO - training batch 1751, loss: 0.399, 56032/60000 datapoints
2025-03-06 20:29:01,158 - INFO - training batch 1801, loss: 0.111, 57632/60000 datapoints
2025-03-06 20:29:01,352 - INFO - training batch 1851, loss: 0.290, 59232/60000 datapoints
2025-03-06 20:29:01,456 - INFO - validation batch 1, loss: 0.119, 32/10016 datapoints
2025-03-06 20:29:01,611 - INFO - validation batch 51, loss: 0.362, 1632/10016 datapoints
2025-03-06 20:29:01,772 - INFO - validation batch 101, loss: 0.269, 3232/10016 datapoints
2025-03-06 20:29:01,933 - INFO - validation batch 151, loss: 0.143, 4832/10016 datapoints
2025-03-06 20:29:02,087 - INFO - validation batch 201, loss: 0.219, 6432/10016 datapoints
2025-03-06 20:29:02,241 - INFO - validation batch 251, loss: 0.242, 8032/10016 datapoints
2025-03-06 20:29:02,397 - INFO - validation batch 301, loss: 0.190, 9632/10016 datapoints
2025-03-06 20:29:02,437 - INFO - Epoch 739/800 done.
2025-03-06 20:29:02,437 - INFO - Final validation performance:
Loss: 0.221, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:29:02,438 - INFO - Beginning epoch 740/800
2025-03-06 20:29:02,446 - INFO - training batch 1, loss: 0.056, 32/60000 datapoints
2025-03-06 20:29:02,649 - INFO - training batch 51, loss: 0.155, 1632/60000 datapoints
2025-03-06 20:29:02,862 - INFO - training batch 101, loss: 0.165, 3232/60000 datapoints
2025-03-06 20:29:03,057 - INFO - training batch 151, loss: 0.371, 4832/60000 datapoints
2025-03-06 20:29:03,257 - INFO - training batch 201, loss: 0.090, 6432/60000 datapoints
2025-03-06 20:29:03,455 - INFO - training batch 251, loss: 0.073, 8032/60000 datapoints
2025-03-06 20:29:03,655 - INFO - training batch 301, loss: 0.411, 9632/60000 datapoints
2025-03-06 20:29:03,852 - INFO - training batch 351, loss: 0.236, 11232/60000 datapoints
2025-03-06 20:29:04,043 - INFO - training batch 401, loss: 0.371, 12832/60000 datapoints
2025-03-06 20:29:04,238 - INFO - training batch 451, loss: 0.383, 14432/60000 datapoints
2025-03-06 20:29:04,431 - INFO - training batch 501, loss: 0.302, 16032/60000 datapoints
2025-03-06 20:29:04,626 - INFO - training batch 551, loss: 0.371, 17632/60000 datapoints
2025-03-06 20:29:04,823 - INFO - training batch 601, loss: 0.214, 19232/60000 datapoints
2025-03-06 20:29:05,024 - INFO - training batch 651, loss: 0.203, 20832/60000 datapoints
2025-03-06 20:29:05,222 - INFO - training batch 701, loss: 0.059, 22432/60000 datapoints
2025-03-06 20:29:05,418 - INFO - training batch 751, loss: 0.304, 24032/60000 datapoints
2025-03-06 20:29:05,612 - INFO - training batch 801, loss: 0.140, 25632/60000 datapoints
2025-03-06 20:29:05,815 - INFO - training batch 851, loss: 0.198, 27232/60000 datapoints
2025-03-06 20:29:06,010 - INFO - training batch 901, loss: 0.529, 28832/60000 datapoints
2025-03-06 20:29:06,205 - INFO - training batch 951, loss: 0.366, 30432/60000 datapoints
2025-03-06 20:29:06,401 - INFO - training batch 1001, loss: 0.089, 32032/60000 datapoints
2025-03-06 20:29:06,599 - INFO - training batch 1051, loss: 0.158, 33632/60000 datapoints
2025-03-06 20:29:06,796 - INFO - training batch 1101, loss: 0.136, 35232/60000 datapoints
2025-03-06 20:29:06,994 - INFO - training batch 1151, loss: 0.160, 36832/60000 datapoints
2025-03-06 20:29:07,263 - INFO - training batch 1201, loss: 0.249, 38432/60000 datapoints
2025-03-06 20:29:07,459 - INFO - training batch 1251, loss: 0.382, 40032/60000 datapoints
2025-03-06 20:29:07,658 - INFO - training batch 1301, loss: 0.268, 41632/60000 datapoints
2025-03-06 20:29:07,859 - INFO - training batch 1351, loss: 0.354, 43232/60000 datapoints
2025-03-06 20:29:08,058 - INFO - training batch 1401, loss: 0.187, 44832/60000 datapoints
2025-03-06 20:29:08,255 - INFO - training batch 1451, loss: 0.127, 46432/60000 datapoints
2025-03-06 20:29:08,449 - INFO - training batch 1501, loss: 0.065, 48032/60000 datapoints
2025-03-06 20:29:08,647 - INFO - training batch 1551, loss: 0.172, 49632/60000 datapoints
2025-03-06 20:29:08,842 - INFO - training batch 1601, loss: 0.273, 51232/60000 datapoints
2025-03-06 20:29:09,037 - INFO - training batch 1651, loss: 0.266, 52832/60000 datapoints
2025-03-06 20:29:09,258 - INFO - training batch 1701, loss: 0.149, 54432/60000 datapoints
2025-03-06 20:29:09,453 - INFO - training batch 1751, loss: 0.488, 56032/60000 datapoints
2025-03-06 20:29:09,650 - INFO - training batch 1801, loss: 0.087, 57632/60000 datapoints
2025-03-06 20:29:09,847 - INFO - training batch 1851, loss: 0.153, 59232/60000 datapoints
2025-03-06 20:29:09,950 - INFO - validation batch 1, loss: 0.161, 32/10016 datapoints
2025-03-06 20:29:10,104 - INFO - validation batch 51, loss: 0.104, 1632/10016 datapoints
2025-03-06 20:29:10,257 - INFO - validation batch 101, loss: 0.197, 3232/10016 datapoints
2025-03-06 20:29:10,409 - INFO - validation batch 151, loss: 0.408, 4832/10016 datapoints
2025-03-06 20:29:10,562 - INFO - validation batch 201, loss: 0.253, 6432/10016 datapoints
2025-03-06 20:29:10,717 - INFO - validation batch 251, loss: 0.132, 8032/10016 datapoints
2025-03-06 20:29:10,871 - INFO - validation batch 301, loss: 0.308, 9632/10016 datapoints
2025-03-06 20:29:10,909 - INFO - Epoch 740/800 done.
2025-03-06 20:29:10,909 - INFO - Final validation performance:
Loss: 0.224, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:29:10,909 - INFO - Beginning epoch 741/800
2025-03-06 20:29:10,916 - INFO - training batch 1, loss: 0.124, 32/60000 datapoints
2025-03-06 20:29:11,109 - INFO - training batch 51, loss: 0.186, 1632/60000 datapoints
2025-03-06 20:29:11,308 - INFO - training batch 101, loss: 0.148, 3232/60000 datapoints
2025-03-06 20:29:11,518 - INFO - training batch 151, loss: 0.200, 4832/60000 datapoints
2025-03-06 20:29:11,716 - INFO - training batch 201, loss: 0.057, 6432/60000 datapoints
2025-03-06 20:29:11,923 - INFO - training batch 251, loss: 0.218, 8032/60000 datapoints
2025-03-06 20:29:12,123 - INFO - training batch 301, loss: 0.165, 9632/60000 datapoints
2025-03-06 20:29:12,320 - INFO - training batch 351, loss: 0.102, 11232/60000 datapoints
2025-03-06 20:29:12,531 - INFO - training batch 401, loss: 0.105, 12832/60000 datapoints
2025-03-06 20:29:12,731 - INFO - training batch 451, loss: 0.134, 14432/60000 datapoints
2025-03-06 20:29:12,926 - INFO - training batch 501, loss: 0.224, 16032/60000 datapoints
2025-03-06 20:29:13,119 - INFO - training batch 551, loss: 0.187, 17632/60000 datapoints
2025-03-06 20:29:13,316 - INFO - training batch 601, loss: 0.274, 19232/60000 datapoints
2025-03-06 20:29:13,510 - INFO - training batch 651, loss: 0.034, 20832/60000 datapoints
2025-03-06 20:29:13,708 - INFO - training batch 701, loss: 0.249, 22432/60000 datapoints
2025-03-06 20:29:13,907 - INFO - training batch 751, loss: 0.298, 24032/60000 datapoints
2025-03-06 20:29:14,101 - INFO - training batch 801, loss: 0.276, 25632/60000 datapoints
2025-03-06 20:29:14,297 - INFO - training batch 851, loss: 0.200, 27232/60000 datapoints
2025-03-06 20:29:14,493 - INFO - training batch 901, loss: 0.257, 28832/60000 datapoints
2025-03-06 20:29:14,688 - INFO - training batch 951, loss: 0.258, 30432/60000 datapoints
2025-03-06 20:29:14,884 - INFO - training batch 1001, loss: 0.316, 32032/60000 datapoints
2025-03-06 20:29:15,084 - INFO - training batch 1051, loss: 0.177, 33632/60000 datapoints
2025-03-06 20:29:15,277 - INFO - training batch 1101, loss: 0.116, 35232/60000 datapoints
2025-03-06 20:29:15,472 - INFO - training batch 1151, loss: 0.198, 36832/60000 datapoints
2025-03-06 20:29:15,668 - INFO - training batch 1201, loss: 0.186, 38432/60000 datapoints
2025-03-06 20:29:15,864 - INFO - training batch 1251, loss: 0.322, 40032/60000 datapoints
2025-03-06 20:29:16,060 - INFO - training batch 1301, loss: 0.129, 41632/60000 datapoints
2025-03-06 20:29:16,250 - INFO - training batch 1351, loss: 0.137, 43232/60000 datapoints
2025-03-06 20:29:16,445 - INFO - training batch 1401, loss: 0.081, 44832/60000 datapoints
2025-03-06 20:29:16,643 - INFO - training batch 1451, loss: 0.167, 46432/60000 datapoints
2025-03-06 20:29:16,836 - INFO - training batch 1501, loss: 0.349, 48032/60000 datapoints
2025-03-06 20:29:17,036 - INFO - training batch 1551, loss: 0.409, 49632/60000 datapoints
2025-03-06 20:29:17,230 - INFO - training batch 1601, loss: 0.082, 51232/60000 datapoints
2025-03-06 20:29:17,431 - INFO - training batch 1651, loss: 0.156, 52832/60000 datapoints
2025-03-06 20:29:17,627 - INFO - training batch 1701, loss: 0.171, 54432/60000 datapoints
2025-03-06 20:29:17,825 - INFO - training batch 1751, loss: 0.139, 56032/60000 datapoints
2025-03-06 20:29:18,020 - INFO - training batch 1801, loss: 0.270, 57632/60000 datapoints
2025-03-06 20:29:18,213 - INFO - training batch 1851, loss: 0.143, 59232/60000 datapoints
2025-03-06 20:29:18,314 - INFO - validation batch 1, loss: 0.164, 32/10016 datapoints
2025-03-06 20:29:18,467 - INFO - validation batch 51, loss: 0.244, 1632/10016 datapoints
2025-03-06 20:29:18,620 - INFO - validation batch 101, loss: 0.378, 3232/10016 datapoints
2025-03-06 20:29:18,781 - INFO - validation batch 151, loss: 0.164, 4832/10016 datapoints
2025-03-06 20:29:18,936 - INFO - validation batch 201, loss: 0.271, 6432/10016 datapoints
2025-03-06 20:29:19,087 - INFO - validation batch 251, loss: 0.254, 8032/10016 datapoints
2025-03-06 20:29:19,246 - INFO - validation batch 301, loss: 0.184, 9632/10016 datapoints
2025-03-06 20:29:19,295 - INFO - Epoch 741/800 done.
2025-03-06 20:29:19,295 - INFO - Final validation performance:
Loss: 0.237, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:29:19,296 - INFO - Beginning epoch 742/800
2025-03-06 20:29:19,303 - INFO - training batch 1, loss: 0.036, 32/60000 datapoints
2025-03-06 20:29:19,502 - INFO - training batch 51, loss: 0.132, 1632/60000 datapoints
2025-03-06 20:29:19,709 - INFO - training batch 101, loss: 0.332, 3232/60000 datapoints
2025-03-06 20:29:19,910 - INFO - training batch 151, loss: 0.105, 4832/60000 datapoints
2025-03-06 20:29:20,112 - INFO - training batch 201, loss: 0.195, 6432/60000 datapoints
2025-03-06 20:29:20,306 - INFO - training batch 251, loss: 0.116, 8032/60000 datapoints
2025-03-06 20:29:20,505 - INFO - training batch 301, loss: 0.324, 9632/60000 datapoints
2025-03-06 20:29:20,703 - INFO - training batch 351, loss: 0.137, 11232/60000 datapoints
2025-03-06 20:29:20,896 - INFO - training batch 401, loss: 0.071, 12832/60000 datapoints
2025-03-06 20:29:21,090 - INFO - training batch 451, loss: 0.221, 14432/60000 datapoints
2025-03-06 20:29:21,284 - INFO - training batch 501, loss: 0.120, 16032/60000 datapoints
2025-03-06 20:29:21,480 - INFO - training batch 551, loss: 0.107, 17632/60000 datapoints
2025-03-06 20:29:21,674 - INFO - training batch 601, loss: 0.074, 19232/60000 datapoints
2025-03-06 20:29:21,872 - INFO - training batch 651, loss: 0.698, 20832/60000 datapoints
2025-03-06 20:29:22,071 - INFO - training batch 701, loss: 0.318, 22432/60000 datapoints
2025-03-06 20:29:22,263 - INFO - training batch 751, loss: 0.166, 24032/60000 datapoints
2025-03-06 20:29:22,459 - INFO - training batch 801, loss: 0.245, 25632/60000 datapoints
2025-03-06 20:29:22,655 - INFO - training batch 851, loss: 0.154, 27232/60000 datapoints
2025-03-06 20:29:22,849 - INFO - training batch 901, loss: 0.350, 28832/60000 datapoints
2025-03-06 20:29:23,044 - INFO - training batch 951, loss: 0.328, 30432/60000 datapoints
2025-03-06 20:29:23,235 - INFO - training batch 1001, loss: 0.288, 32032/60000 datapoints
2025-03-06 20:29:23,430 - INFO - training batch 1051, loss: 0.086, 33632/60000 datapoints
2025-03-06 20:29:23,624 - INFO - training batch 1101, loss: 0.104, 35232/60000 datapoints
2025-03-06 20:29:23,817 - INFO - training batch 1151, loss: 0.234, 36832/60000 datapoints
2025-03-06 20:29:24,016 - INFO - training batch 1201, loss: 0.297, 38432/60000 datapoints
2025-03-06 20:29:24,210 - INFO - training batch 1251, loss: 0.142, 40032/60000 datapoints
2025-03-06 20:29:24,401 - INFO - training batch 1301, loss: 0.142, 41632/60000 datapoints
2025-03-06 20:29:24,593 - INFO - training batch 1351, loss: 0.121, 43232/60000 datapoints
2025-03-06 20:29:24,790 - INFO - training batch 1401, loss: 0.165, 44832/60000 datapoints
2025-03-06 20:29:24,994 - INFO - training batch 1451, loss: 0.146, 46432/60000 datapoints
2025-03-06 20:29:25,188 - INFO - training batch 1501, loss: 0.045, 48032/60000 datapoints
2025-03-06 20:29:25,381 - INFO - training batch 1551, loss: 0.140, 49632/60000 datapoints
2025-03-06 20:29:25,575 - INFO - training batch 1601, loss: 0.047, 51232/60000 datapoints
2025-03-06 20:29:25,773 - INFO - training batch 1651, loss: 0.353, 52832/60000 datapoints
2025-03-06 20:29:25,971 - INFO - training batch 1701, loss: 0.215, 54432/60000 datapoints
2025-03-06 20:29:26,165 - INFO - training batch 1751, loss: 0.135, 56032/60000 datapoints
2025-03-06 20:29:26,360 - INFO - training batch 1801, loss: 0.203, 57632/60000 datapoints
2025-03-06 20:29:26,561 - INFO - training batch 1851, loss: 0.213, 59232/60000 datapoints
2025-03-06 20:29:26,664 - INFO - validation batch 1, loss: 0.124, 32/10016 datapoints
2025-03-06 20:29:26,814 - INFO - validation batch 51, loss: 0.171, 1632/10016 datapoints
2025-03-06 20:29:26,976 - INFO - validation batch 101, loss: 0.082, 3232/10016 datapoints
2025-03-06 20:29:27,130 - INFO - validation batch 151, loss: 0.332, 4832/10016 datapoints
2025-03-06 20:29:27,283 - INFO - validation batch 201, loss: 0.073, 6432/10016 datapoints
2025-03-06 20:29:27,437 - INFO - validation batch 251, loss: 0.154, 8032/10016 datapoints
2025-03-06 20:29:27,589 - INFO - validation batch 301, loss: 0.235, 9632/10016 datapoints
2025-03-06 20:29:27,627 - INFO - Epoch 742/800 done.
2025-03-06 20:29:27,627 - INFO - Final validation performance:
Loss: 0.167, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:29:27,628 - INFO - Beginning epoch 743/800
2025-03-06 20:29:27,636 - INFO - training batch 1, loss: 0.198, 32/60000 datapoints
2025-03-06 20:29:27,828 - INFO - training batch 51, loss: 0.162, 1632/60000 datapoints
2025-03-06 20:29:28,023 - INFO - training batch 101, loss: 0.122, 3232/60000 datapoints
2025-03-06 20:29:28,226 - INFO - training batch 151, loss: 0.224, 4832/60000 datapoints
2025-03-06 20:29:28,422 - INFO - training batch 201, loss: 0.202, 6432/60000 datapoints
2025-03-06 20:29:28,622 - INFO - training batch 251, loss: 0.329, 8032/60000 datapoints
2025-03-06 20:29:28,848 - INFO - training batch 301, loss: 0.153, 9632/60000 datapoints
2025-03-06 20:29:29,079 - INFO - training batch 351, loss: 0.265, 11232/60000 datapoints
2025-03-06 20:29:29,272 - INFO - training batch 401, loss: 0.080, 12832/60000 datapoints
2025-03-06 20:29:29,486 - INFO - training batch 451, loss: 0.263, 14432/60000 datapoints
2025-03-06 20:29:29,684 - INFO - training batch 501, loss: 0.341, 16032/60000 datapoints
2025-03-06 20:29:29,884 - INFO - training batch 551, loss: 0.363, 17632/60000 datapoints
2025-03-06 20:29:30,088 - INFO - training batch 601, loss: 0.127, 19232/60000 datapoints
2025-03-06 20:29:30,281 - INFO - training batch 651, loss: 0.182, 20832/60000 datapoints
2025-03-06 20:29:30,475 - INFO - training batch 701, loss: 0.155, 22432/60000 datapoints
2025-03-06 20:29:30,672 - INFO - training batch 751, loss: 0.228, 24032/60000 datapoints
2025-03-06 20:29:30,865 - INFO - training batch 801, loss: 0.191, 25632/60000 datapoints
2025-03-06 20:29:31,063 - INFO - training batch 851, loss: 0.240, 27232/60000 datapoints
2025-03-06 20:29:31,255 - INFO - training batch 901, loss: 0.140, 28832/60000 datapoints
2025-03-06 20:29:31,448 - INFO - training batch 951, loss: 0.281, 30432/60000 datapoints
2025-03-06 20:29:31,646 - INFO - training batch 1001, loss: 0.161, 32032/60000 datapoints
2025-03-06 20:29:31,840 - INFO - training batch 1051, loss: 0.188, 33632/60000 datapoints
2025-03-06 20:29:32,041 - INFO - training batch 1101, loss: 0.027, 35232/60000 datapoints
2025-03-06 20:29:32,235 - INFO - training batch 1151, loss: 0.399, 36832/60000 datapoints
2025-03-06 20:29:32,428 - INFO - training batch 1201, loss: 0.305, 38432/60000 datapoints
2025-03-06 20:29:32,624 - INFO - training batch 1251, loss: 0.225, 40032/60000 datapoints
2025-03-06 20:29:32,819 - INFO - training batch 1301, loss: 0.274, 41632/60000 datapoints
2025-03-06 20:29:33,013 - INFO - training batch 1351, loss: 0.533, 43232/60000 datapoints
2025-03-06 20:29:33,205 - INFO - training batch 1401, loss: 0.054, 44832/60000 datapoints
2025-03-06 20:29:33,399 - INFO - training batch 1451, loss: 0.189, 46432/60000 datapoints
2025-03-06 20:29:33,595 - INFO - training batch 1501, loss: 0.195, 48032/60000 datapoints
2025-03-06 20:29:33,791 - INFO - training batch 1551, loss: 0.171, 49632/60000 datapoints
2025-03-06 20:29:33,989 - INFO - training batch 1601, loss: 0.242, 51232/60000 datapoints
2025-03-06 20:29:34,185 - INFO - training batch 1651, loss: 0.290, 52832/60000 datapoints
2025-03-06 20:29:34,380 - INFO - training batch 1701, loss: 0.103, 54432/60000 datapoints
2025-03-06 20:29:34,577 - INFO - training batch 1751, loss: 0.350, 56032/60000 datapoints
2025-03-06 20:29:34,775 - INFO - training batch 1801, loss: 0.099, 57632/60000 datapoints
2025-03-06 20:29:34,975 - INFO - training batch 1851, loss: 0.081, 59232/60000 datapoints
2025-03-06 20:29:35,080 - INFO - validation batch 1, loss: 0.247, 32/10016 datapoints
2025-03-06 20:29:35,233 - INFO - validation batch 51, loss: 0.375, 1632/10016 datapoints
2025-03-06 20:29:35,387 - INFO - validation batch 101, loss: 0.306, 3232/10016 datapoints
2025-03-06 20:29:35,540 - INFO - validation batch 151, loss: 0.174, 4832/10016 datapoints
2025-03-06 20:29:35,696 - INFO - validation batch 201, loss: 0.486, 6432/10016 datapoints
2025-03-06 20:29:35,848 - INFO - validation batch 251, loss: 0.303, 8032/10016 datapoints
2025-03-06 20:29:36,006 - INFO - validation batch 301, loss: 0.364, 9632/10016 datapoints
2025-03-06 20:29:36,045 - INFO - Epoch 743/800 done.
2025-03-06 20:29:36,045 - INFO - Final validation performance:
Loss: 0.322, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:29:36,046 - INFO - Beginning epoch 744/800
2025-03-06 20:29:36,052 - INFO - training batch 1, loss: 0.063, 32/60000 datapoints
2025-03-06 20:29:36,263 - INFO - training batch 51, loss: 0.092, 1632/60000 datapoints
2025-03-06 20:29:36,457 - INFO - training batch 101, loss: 0.202, 3232/60000 datapoints
2025-03-06 20:29:36,666 - INFO - training batch 151, loss: 0.053, 4832/60000 datapoints
2025-03-06 20:29:36,864 - INFO - training batch 201, loss: 0.347, 6432/60000 datapoints
2025-03-06 20:29:37,064 - INFO - training batch 251, loss: 0.088, 8032/60000 datapoints
2025-03-06 20:29:37,260 - INFO - training batch 301, loss: 0.238, 9632/60000 datapoints
2025-03-06 20:29:37,453 - INFO - training batch 351, loss: 0.239, 11232/60000 datapoints
2025-03-06 20:29:37,672 - INFO - training batch 401, loss: 0.304, 12832/60000 datapoints
2025-03-06 20:29:37,867 - INFO - training batch 451, loss: 0.258, 14432/60000 datapoints
2025-03-06 20:29:38,065 - INFO - training batch 501, loss: 0.274, 16032/60000 datapoints
2025-03-06 20:29:38,258 - INFO - training batch 551, loss: 0.141, 17632/60000 datapoints
2025-03-06 20:29:38,452 - INFO - training batch 601, loss: 0.111, 19232/60000 datapoints
2025-03-06 20:29:38,650 - INFO - training batch 651, loss: 0.096, 20832/60000 datapoints
2025-03-06 20:29:38,847 - INFO - training batch 701, loss: 0.261, 22432/60000 datapoints
2025-03-06 20:29:39,043 - INFO - training batch 751, loss: 0.156, 24032/60000 datapoints
2025-03-06 20:29:39,237 - INFO - training batch 801, loss: 0.195, 25632/60000 datapoints
2025-03-06 20:29:39,448 - INFO - training batch 851, loss: 0.246, 27232/60000 datapoints
2025-03-06 20:29:39,651 - INFO - training batch 901, loss: 0.215, 28832/60000 datapoints
2025-03-06 20:29:39,845 - INFO - training batch 951, loss: 0.320, 30432/60000 datapoints
2025-03-06 20:29:40,046 - INFO - training batch 1001, loss: 0.250, 32032/60000 datapoints
2025-03-06 20:29:40,243 - INFO - training batch 1051, loss: 0.167, 33632/60000 datapoints
2025-03-06 20:29:40,436 - INFO - training batch 1101, loss: 0.248, 35232/60000 datapoints
2025-03-06 20:29:40,634 - INFO - training batch 1151, loss: 0.172, 36832/60000 datapoints
2025-03-06 20:29:40,832 - INFO - training batch 1201, loss: 0.454, 38432/60000 datapoints
2025-03-06 20:29:41,028 - INFO - training batch 1251, loss: 0.252, 40032/60000 datapoints
2025-03-06 20:29:41,223 - INFO - training batch 1301, loss: 0.445, 41632/60000 datapoints
2025-03-06 20:29:41,419 - INFO - training batch 1351, loss: 0.059, 43232/60000 datapoints
2025-03-06 20:29:41,617 - INFO - training batch 1401, loss: 0.116, 44832/60000 datapoints
2025-03-06 20:29:41,824 - INFO - training batch 1451, loss: 0.307, 46432/60000 datapoints
2025-03-06 20:29:42,037 - INFO - training batch 1501, loss: 0.295, 48032/60000 datapoints
2025-03-06 20:29:42,238 - INFO - training batch 1551, loss: 0.207, 49632/60000 datapoints
2025-03-06 20:29:42,433 - INFO - training batch 1601, loss: 0.179, 51232/60000 datapoints
2025-03-06 20:29:42,633 - INFO - training batch 1651, loss: 0.156, 52832/60000 datapoints
2025-03-06 20:29:42,827 - INFO - training batch 1701, loss: 0.265, 54432/60000 datapoints
2025-03-06 20:29:43,020 - INFO - training batch 1751, loss: 0.484, 56032/60000 datapoints
2025-03-06 20:29:43,214 - INFO - training batch 1801, loss: 0.073, 57632/60000 datapoints
2025-03-06 20:29:43,409 - INFO - training batch 1851, loss: 0.217, 59232/60000 datapoints
2025-03-06 20:29:43,509 - INFO - validation batch 1, loss: 0.144, 32/10016 datapoints
2025-03-06 20:29:43,667 - INFO - validation batch 51, loss: 0.340, 1632/10016 datapoints
2025-03-06 20:29:43,820 - INFO - validation batch 101, loss: 0.202, 3232/10016 datapoints
2025-03-06 20:29:43,977 - INFO - validation batch 151, loss: 0.201, 4832/10016 datapoints
2025-03-06 20:29:44,130 - INFO - validation batch 201, loss: 0.140, 6432/10016 datapoints
2025-03-06 20:29:44,283 - INFO - validation batch 251, loss: 0.307, 8032/10016 datapoints
2025-03-06 20:29:44,436 - INFO - validation batch 301, loss: 0.255, 9632/10016 datapoints
2025-03-06 20:29:44,473 - INFO - Epoch 744/800 done.
2025-03-06 20:29:44,473 - INFO - Final validation performance:
Loss: 0.227, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:29:44,474 - INFO - Beginning epoch 745/800
2025-03-06 20:29:44,481 - INFO - training batch 1, loss: 0.085, 32/60000 datapoints
2025-03-06 20:29:44,678 - INFO - training batch 51, loss: 0.136, 1632/60000 datapoints
2025-03-06 20:29:44,883 - INFO - training batch 101, loss: 0.065, 3232/60000 datapoints
2025-03-06 20:29:45,081 - INFO - training batch 151, loss: 0.252, 4832/60000 datapoints
2025-03-06 20:29:45,277 - INFO - training batch 201, loss: 0.223, 6432/60000 datapoints
2025-03-06 20:29:45,472 - INFO - training batch 251, loss: 0.097, 8032/60000 datapoints
2025-03-06 20:29:45,674 - INFO - training batch 301, loss: 0.489, 9632/60000 datapoints
2025-03-06 20:29:45,872 - INFO - training batch 351, loss: 0.195, 11232/60000 datapoints
2025-03-06 20:29:46,068 - INFO - training batch 401, loss: 0.096, 12832/60000 datapoints
2025-03-06 20:29:46,261 - INFO - training batch 451, loss: 0.203, 14432/60000 datapoints
2025-03-06 20:29:46,454 - INFO - training batch 501, loss: 0.134, 16032/60000 datapoints
2025-03-06 20:29:46,654 - INFO - training batch 551, loss: 0.153, 17632/60000 datapoints
2025-03-06 20:29:46,847 - INFO - training batch 601, loss: 0.388, 19232/60000 datapoints
2025-03-06 20:29:47,040 - INFO - training batch 651, loss: 0.224, 20832/60000 datapoints
2025-03-06 20:29:47,236 - INFO - training batch 701, loss: 0.074, 22432/60000 datapoints
2025-03-06 20:29:47,430 - INFO - training batch 751, loss: 0.147, 24032/60000 datapoints
2025-03-06 20:29:47,623 - INFO - training batch 801, loss: 0.095, 25632/60000 datapoints
2025-03-06 20:29:47,816 - INFO - training batch 851, loss: 0.331, 27232/60000 datapoints
2025-03-06 20:29:48,010 - INFO - training batch 901, loss: 0.282, 28832/60000 datapoints
2025-03-06 20:29:48,203 - INFO - training batch 951, loss: 0.387, 30432/60000 datapoints
2025-03-06 20:29:48,394 - INFO - training batch 1001, loss: 0.214, 32032/60000 datapoints
2025-03-06 20:29:48,591 - INFO - training batch 1051, loss: 0.252, 33632/60000 datapoints
2025-03-06 20:29:48,786 - INFO - training batch 1101, loss: 0.103, 35232/60000 datapoints
2025-03-06 20:29:48,982 - INFO - training batch 1151, loss: 0.393, 36832/60000 datapoints
2025-03-06 20:29:49,176 - INFO - training batch 1201, loss: 0.212, 38432/60000 datapoints
2025-03-06 20:29:49,369 - INFO - training batch 1251, loss: 0.068, 40032/60000 datapoints
2025-03-06 20:29:49,586 - INFO - training batch 1301, loss: 0.135, 41632/60000 datapoints
2025-03-06 20:29:49,785 - INFO - training batch 1351, loss: 0.129, 43232/60000 datapoints
2025-03-06 20:29:49,989 - INFO - training batch 1401, loss: 0.175, 44832/60000 datapoints
2025-03-06 20:29:50,185 - INFO - training batch 1451, loss: 0.297, 46432/60000 datapoints
2025-03-06 20:29:50,380 - INFO - training batch 1501, loss: 0.183, 48032/60000 datapoints
2025-03-06 20:29:50,574 - INFO - training batch 1551, loss: 0.102, 49632/60000 datapoints
2025-03-06 20:29:50,773 - INFO - training batch 1601, loss: 0.225, 51232/60000 datapoints
2025-03-06 20:29:50,969 - INFO - training batch 1651, loss: 0.425, 52832/60000 datapoints
2025-03-06 20:29:51,164 - INFO - training batch 1701, loss: 0.268, 54432/60000 datapoints
2025-03-06 20:29:51,359 - INFO - training batch 1751, loss: 0.144, 56032/60000 datapoints
2025-03-06 20:29:51,556 - INFO - training batch 1801, loss: 0.271, 57632/60000 datapoints
2025-03-06 20:29:51,754 - INFO - training batch 1851, loss: 0.116, 59232/60000 datapoints
2025-03-06 20:29:51,856 - INFO - validation batch 1, loss: 0.216, 32/10016 datapoints
2025-03-06 20:29:52,013 - INFO - validation batch 51, loss: 0.516, 1632/10016 datapoints
2025-03-06 20:29:52,167 - INFO - validation batch 101, loss: 0.292, 3232/10016 datapoints
2025-03-06 20:29:52,319 - INFO - validation batch 151, loss: 0.432, 4832/10016 datapoints
2025-03-06 20:29:52,473 - INFO - validation batch 201, loss: 0.162, 6432/10016 datapoints
2025-03-06 20:29:52,627 - INFO - validation batch 251, loss: 0.310, 8032/10016 datapoints
2025-03-06 20:29:52,781 - INFO - validation batch 301, loss: 0.190, 9632/10016 datapoints
2025-03-06 20:29:52,819 - INFO - Epoch 745/800 done.
2025-03-06 20:29:52,819 - INFO - Final validation performance:
Loss: 0.303, top-1 acc: 0.936top-5 acc: 0.936
2025-03-06 20:29:52,820 - INFO - Beginning epoch 746/800
2025-03-06 20:29:52,826 - INFO - training batch 1, loss: 0.291, 32/60000 datapoints
2025-03-06 20:29:53,043 - INFO - training batch 51, loss: 0.298, 1632/60000 datapoints
2025-03-06 20:29:53,240 - INFO - training batch 101, loss: 0.163, 3232/60000 datapoints
2025-03-06 20:29:53,437 - INFO - training batch 151, loss: 0.220, 4832/60000 datapoints
2025-03-06 20:29:53,639 - INFO - training batch 201, loss: 0.168, 6432/60000 datapoints
2025-03-06 20:29:53,841 - INFO - training batch 251, loss: 0.226, 8032/60000 datapoints
2025-03-06 20:29:54,043 - INFO - training batch 301, loss: 0.138, 9632/60000 datapoints
2025-03-06 20:29:54,239 - INFO - training batch 351, loss: 0.177, 11232/60000 datapoints
2025-03-06 20:29:54,431 - INFO - training batch 401, loss: 0.087, 12832/60000 datapoints
2025-03-06 20:29:54,626 - INFO - training batch 451, loss: 0.156, 14432/60000 datapoints
2025-03-06 20:29:54,824 - INFO - training batch 501, loss: 0.695, 16032/60000 datapoints
2025-03-06 20:29:55,025 - INFO - training batch 551, loss: 0.265, 17632/60000 datapoints
2025-03-06 20:29:55,222 - INFO - training batch 601, loss: 0.088, 19232/60000 datapoints
2025-03-06 20:29:55,419 - INFO - training batch 651, loss: 0.100, 20832/60000 datapoints
2025-03-06 20:29:55,618 - INFO - training batch 701, loss: 0.397, 22432/60000 datapoints
2025-03-06 20:29:55,815 - INFO - training batch 751, loss: 0.200, 24032/60000 datapoints
2025-03-06 20:29:56,012 - INFO - training batch 801, loss: 0.460, 25632/60000 datapoints
2025-03-06 20:29:56,205 - INFO - training batch 851, loss: 0.191, 27232/60000 datapoints
2025-03-06 20:29:56,404 - INFO - training batch 901, loss: 0.548, 28832/60000 datapoints
2025-03-06 20:29:56,601 - INFO - training batch 951, loss: 0.327, 30432/60000 datapoints
2025-03-06 20:29:56,799 - INFO - training batch 1001, loss: 0.114, 32032/60000 datapoints
2025-03-06 20:29:56,993 - INFO - training batch 1051, loss: 0.452, 33632/60000 datapoints
2025-03-06 20:29:57,188 - INFO - training batch 1101, loss: 0.178, 35232/60000 datapoints
2025-03-06 20:29:57,382 - INFO - training batch 1151, loss: 0.607, 36832/60000 datapoints
2025-03-06 20:29:57,579 - INFO - training batch 1201, loss: 0.210, 38432/60000 datapoints
2025-03-06 20:29:57,778 - INFO - training batch 1251, loss: 0.191, 40032/60000 datapoints
2025-03-06 20:29:57,973 - INFO - training batch 1301, loss: 0.086, 41632/60000 datapoints
2025-03-06 20:29:58,168 - INFO - training batch 1351, loss: 0.403, 43232/60000 datapoints
2025-03-06 20:29:58,362 - INFO - training batch 1401, loss: 0.136, 44832/60000 datapoints
2025-03-06 20:29:58,559 - INFO - training batch 1451, loss: 0.347, 46432/60000 datapoints
2025-03-06 20:29:58,756 - INFO - training batch 1501, loss: 0.266, 48032/60000 datapoints
2025-03-06 20:29:58,950 - INFO - training batch 1551, loss: 0.384, 49632/60000 datapoints
2025-03-06 20:29:59,159 - INFO - training batch 1601, loss: 0.118, 51232/60000 datapoints
2025-03-06 20:29:59,380 - INFO - training batch 1651, loss: 0.191, 52832/60000 datapoints
2025-03-06 20:29:59,617 - INFO - training batch 1701, loss: 0.074, 54432/60000 datapoints
2025-03-06 20:29:59,848 - INFO - training batch 1751, loss: 0.065, 56032/60000 datapoints
2025-03-06 20:30:00,065 - INFO - training batch 1801, loss: 0.355, 57632/60000 datapoints
2025-03-06 20:30:00,261 - INFO - training batch 1851, loss: 0.050, 59232/60000 datapoints
2025-03-06 20:30:00,362 - INFO - validation batch 1, loss: 0.155, 32/10016 datapoints
2025-03-06 20:30:00,516 - INFO - validation batch 51, loss: 0.101, 1632/10016 datapoints
2025-03-06 20:30:00,673 - INFO - validation batch 101, loss: 0.068, 3232/10016 datapoints
2025-03-06 20:30:00,827 - INFO - validation batch 151, loss: 0.215, 4832/10016 datapoints
2025-03-06 20:30:00,979 - INFO - validation batch 201, loss: 0.444, 6432/10016 datapoints
2025-03-06 20:30:01,132 - INFO - validation batch 251, loss: 0.180, 8032/10016 datapoints
2025-03-06 20:30:01,284 - INFO - validation batch 301, loss: 0.193, 9632/10016 datapoints
2025-03-06 20:30:01,328 - INFO - Epoch 746/800 done.
2025-03-06 20:30:01,328 - INFO - Final validation performance:
Loss: 0.194, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:30:01,329 - INFO - Beginning epoch 747/800
2025-03-06 20:30:01,335 - INFO - training batch 1, loss: 0.122, 32/60000 datapoints
2025-03-06 20:30:01,547 - INFO - training batch 51, loss: 0.285, 1632/60000 datapoints
2025-03-06 20:30:01,747 - INFO - training batch 101, loss: 0.125, 3232/60000 datapoints
2025-03-06 20:30:01,953 - INFO - training batch 151, loss: 0.376, 4832/60000 datapoints
2025-03-06 20:30:02,155 - INFO - training batch 201, loss: 0.113, 6432/60000 datapoints
2025-03-06 20:30:02,356 - INFO - training batch 251, loss: 0.261, 8032/60000 datapoints
2025-03-06 20:30:02,551 - INFO - training batch 301, loss: 0.232, 9632/60000 datapoints
2025-03-06 20:30:02,750 - INFO - training batch 351, loss: 0.059, 11232/60000 datapoints
2025-03-06 20:30:02,943 - INFO - training batch 401, loss: 0.190, 12832/60000 datapoints
2025-03-06 20:30:03,137 - INFO - training batch 451, loss: 0.169, 14432/60000 datapoints
2025-03-06 20:30:03,333 - INFO - training batch 501, loss: 0.236, 16032/60000 datapoints
2025-03-06 20:30:03,531 - INFO - training batch 551, loss: 0.108, 17632/60000 datapoints
2025-03-06 20:30:03,726 - INFO - training batch 601, loss: 0.186, 19232/60000 datapoints
2025-03-06 20:30:03,922 - INFO - training batch 651, loss: 0.603, 20832/60000 datapoints
2025-03-06 20:30:04,120 - INFO - training batch 701, loss: 0.074, 22432/60000 datapoints
2025-03-06 20:30:04,313 - INFO - training batch 751, loss: 0.321, 24032/60000 datapoints
2025-03-06 20:30:04,507 - INFO - training batch 801, loss: 0.257, 25632/60000 datapoints
2025-03-06 20:30:04,703 - INFO - training batch 851, loss: 0.074, 27232/60000 datapoints
2025-03-06 20:30:04,897 - INFO - training batch 901, loss: 0.048, 28832/60000 datapoints
2025-03-06 20:30:05,097 - INFO - training batch 951, loss: 0.232, 30432/60000 datapoints
2025-03-06 20:30:05,293 - INFO - training batch 1001, loss: 0.140, 32032/60000 datapoints
2025-03-06 20:30:05,490 - INFO - training batch 1051, loss: 0.244, 33632/60000 datapoints
2025-03-06 20:30:05,687 - INFO - training batch 1101, loss: 0.175, 35232/60000 datapoints
2025-03-06 20:30:05,882 - INFO - training batch 1151, loss: 0.198, 36832/60000 datapoints
2025-03-06 20:30:06,082 - INFO - training batch 1201, loss: 0.209, 38432/60000 datapoints
2025-03-06 20:30:06,279 - INFO - training batch 1251, loss: 0.235, 40032/60000 datapoints
2025-03-06 20:30:06,476 - INFO - training batch 1301, loss: 0.258, 41632/60000 datapoints
2025-03-06 20:30:06,674 - INFO - training batch 1351, loss: 0.243, 43232/60000 datapoints
2025-03-06 20:30:06,867 - INFO - training batch 1401, loss: 0.363, 44832/60000 datapoints
2025-03-06 20:30:07,061 - INFO - training batch 1451, loss: 0.166, 46432/60000 datapoints
2025-03-06 20:30:07,255 - INFO - training batch 1501, loss: 0.194, 48032/60000 datapoints
2025-03-06 20:30:07,447 - INFO - training batch 1551, loss: 0.492, 49632/60000 datapoints
2025-03-06 20:30:07,646 - INFO - training batch 1601, loss: 0.097, 51232/60000 datapoints
2025-03-06 20:30:07,860 - INFO - training batch 1651, loss: 0.201, 52832/60000 datapoints
2025-03-06 20:30:08,062 - INFO - training batch 1701, loss: 0.505, 54432/60000 datapoints
2025-03-06 20:30:08,255 - INFO - training batch 1751, loss: 0.092, 56032/60000 datapoints
2025-03-06 20:30:08,448 - INFO - training batch 1801, loss: 0.202, 57632/60000 datapoints
2025-03-06 20:30:08,646 - INFO - training batch 1851, loss: 0.137, 59232/60000 datapoints
2025-03-06 20:30:08,749 - INFO - validation batch 1, loss: 0.202, 32/10016 datapoints
2025-03-06 20:30:08,904 - INFO - validation batch 51, loss: 0.247, 1632/10016 datapoints
2025-03-06 20:30:09,060 - INFO - validation batch 101, loss: 0.250, 3232/10016 datapoints
2025-03-06 20:30:09,214 - INFO - validation batch 151, loss: 0.269, 4832/10016 datapoints
2025-03-06 20:30:09,376 - INFO - validation batch 201, loss: 0.215, 6432/10016 datapoints
2025-03-06 20:30:09,530 - INFO - validation batch 251, loss: 0.069, 8032/10016 datapoints
2025-03-06 20:30:09,695 - INFO - validation batch 301, loss: 0.124, 9632/10016 datapoints
2025-03-06 20:30:09,744 - INFO - Epoch 747/800 done.
2025-03-06 20:30:09,744 - INFO - Final validation performance:
Loss: 0.197, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:30:09,744 - INFO - Beginning epoch 748/800
2025-03-06 20:30:09,751 - INFO - training batch 1, loss: 0.289, 32/60000 datapoints
2025-03-06 20:30:09,967 - INFO - training batch 51, loss: 0.357, 1632/60000 datapoints
2025-03-06 20:30:10,169 - INFO - training batch 101, loss: 0.162, 3232/60000 datapoints
2025-03-06 20:30:10,370 - INFO - training batch 151, loss: 0.099, 4832/60000 datapoints
2025-03-06 20:30:10,569 - INFO - training batch 201, loss: 0.130, 6432/60000 datapoints
2025-03-06 20:30:10,772 - INFO - training batch 251, loss: 0.302, 8032/60000 datapoints
2025-03-06 20:30:10,971 - INFO - training batch 301, loss: 0.347, 9632/60000 datapoints
2025-03-06 20:30:11,166 - INFO - training batch 351, loss: 0.167, 11232/60000 datapoints
2025-03-06 20:30:11,363 - INFO - training batch 401, loss: 0.106, 12832/60000 datapoints
2025-03-06 20:30:11,559 - INFO - training batch 451, loss: 0.163, 14432/60000 datapoints
2025-03-06 20:30:11,756 - INFO - training batch 501, loss: 0.059, 16032/60000 datapoints
2025-03-06 20:30:11,954 - INFO - training batch 551, loss: 0.544, 17632/60000 datapoints
2025-03-06 20:30:12,154 - INFO - training batch 601, loss: 0.177, 19232/60000 datapoints
2025-03-06 20:30:12,348 - INFO - training batch 651, loss: 0.168, 20832/60000 datapoints
2025-03-06 20:30:12,547 - INFO - training batch 701, loss: 0.168, 22432/60000 datapoints
2025-03-06 20:30:12,749 - INFO - training batch 751, loss: 0.176, 24032/60000 datapoints
2025-03-06 20:30:12,944 - INFO - training batch 801, loss: 0.225, 25632/60000 datapoints
2025-03-06 20:30:13,139 - INFO - training batch 851, loss: 0.075, 27232/60000 datapoints
2025-03-06 20:30:13,334 - INFO - training batch 901, loss: 0.113, 28832/60000 datapoints
2025-03-06 20:30:13,528 - INFO - training batch 951, loss: 0.157, 30432/60000 datapoints
2025-03-06 20:30:13,726 - INFO - training batch 1001, loss: 0.171, 32032/60000 datapoints
2025-03-06 20:30:13,921 - INFO - training batch 1051, loss: 0.069, 33632/60000 datapoints
2025-03-06 20:30:14,118 - INFO - training batch 1101, loss: 0.236, 35232/60000 datapoints
2025-03-06 20:30:14,313 - INFO - training batch 1151, loss: 0.203, 36832/60000 datapoints
2025-03-06 20:30:14,508 - INFO - training batch 1201, loss: 0.335, 38432/60000 datapoints
2025-03-06 20:30:14,706 - INFO - training batch 1251, loss: 0.129, 40032/60000 datapoints
2025-03-06 20:30:14,902 - INFO - training batch 1301, loss: 0.062, 41632/60000 datapoints
2025-03-06 20:30:15,101 - INFO - training batch 1351, loss: 0.515, 43232/60000 datapoints
2025-03-06 20:30:15,299 - INFO - training batch 1401, loss: 0.149, 44832/60000 datapoints
2025-03-06 20:30:15,493 - INFO - training batch 1451, loss: 0.671, 46432/60000 datapoints
2025-03-06 20:30:15,690 - INFO - training batch 1501, loss: 0.159, 48032/60000 datapoints
2025-03-06 20:30:15,883 - INFO - training batch 1551, loss: 0.185, 49632/60000 datapoints
2025-03-06 20:30:16,084 - INFO - training batch 1601, loss: 0.354, 51232/60000 datapoints
2025-03-06 20:30:16,276 - INFO - training batch 1651, loss: 0.273, 52832/60000 datapoints
2025-03-06 20:30:16,470 - INFO - training batch 1701, loss: 0.070, 54432/60000 datapoints
2025-03-06 20:30:16,674 - INFO - training batch 1751, loss: 0.258, 56032/60000 datapoints
2025-03-06 20:30:16,870 - INFO - training batch 1801, loss: 0.162, 57632/60000 datapoints
2025-03-06 20:30:17,065 - INFO - training batch 1851, loss: 0.121, 59232/60000 datapoints
2025-03-06 20:30:17,166 - INFO - validation batch 1, loss: 0.348, 32/10016 datapoints
2025-03-06 20:30:17,325 - INFO - validation batch 51, loss: 0.185, 1632/10016 datapoints
2025-03-06 20:30:17,480 - INFO - validation batch 101, loss: 0.170, 3232/10016 datapoints
2025-03-06 20:30:17,637 - INFO - validation batch 151, loss: 0.141, 4832/10016 datapoints
2025-03-06 20:30:17,789 - INFO - validation batch 201, loss: 0.196, 6432/10016 datapoints
2025-03-06 20:30:17,944 - INFO - validation batch 251, loss: 0.058, 8032/10016 datapoints
2025-03-06 20:30:18,100 - INFO - validation batch 301, loss: 0.435, 9632/10016 datapoints
2025-03-06 20:30:18,138 - INFO - Epoch 748/800 done.
2025-03-06 20:30:18,138 - INFO - Final validation performance:
Loss: 0.219, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:30:18,138 - INFO - Beginning epoch 749/800
2025-03-06 20:30:18,145 - INFO - training batch 1, loss: 0.110, 32/60000 datapoints
2025-03-06 20:30:18,356 - INFO - training batch 51, loss: 0.277, 1632/60000 datapoints
2025-03-06 20:30:18,553 - INFO - training batch 101, loss: 0.541, 3232/60000 datapoints
2025-03-06 20:30:18,754 - INFO - training batch 151, loss: 0.095, 4832/60000 datapoints
2025-03-06 20:30:18,953 - INFO - training batch 201, loss: 0.261, 6432/60000 datapoints
2025-03-06 20:30:19,150 - INFO - training batch 251, loss: 0.293, 8032/60000 datapoints
2025-03-06 20:30:19,344 - INFO - training batch 301, loss: 0.298, 9632/60000 datapoints
2025-03-06 20:30:19,539 - INFO - training batch 351, loss: 0.257, 11232/60000 datapoints
2025-03-06 20:30:19,735 - INFO - training batch 401, loss: 0.086, 12832/60000 datapoints
2025-03-06 20:30:19,954 - INFO - training batch 451, loss: 0.042, 14432/60000 datapoints
2025-03-06 20:30:20,154 - INFO - training batch 501, loss: 0.300, 16032/60000 datapoints
2025-03-06 20:30:20,348 - INFO - training batch 551, loss: 0.142, 17632/60000 datapoints
2025-03-06 20:30:20,544 - INFO - training batch 601, loss: 0.319, 19232/60000 datapoints
2025-03-06 20:30:20,740 - INFO - training batch 651, loss: 0.074, 20832/60000 datapoints
2025-03-06 20:30:20,935 - INFO - training batch 701, loss: 0.207, 22432/60000 datapoints
2025-03-06 20:30:21,130 - INFO - training batch 751, loss: 0.071, 24032/60000 datapoints
2025-03-06 20:30:21,328 - INFO - training batch 801, loss: 0.168, 25632/60000 datapoints
2025-03-06 20:30:21,521 - INFO - training batch 851, loss: 0.248, 27232/60000 datapoints
2025-03-06 20:30:21,718 - INFO - training batch 901, loss: 0.137, 28832/60000 datapoints
2025-03-06 20:30:21,915 - INFO - training batch 951, loss: 0.297, 30432/60000 datapoints
2025-03-06 20:30:22,113 - INFO - training batch 1001, loss: 0.201, 32032/60000 datapoints
2025-03-06 20:30:22,307 - INFO - training batch 1051, loss: 0.171, 33632/60000 datapoints
2025-03-06 20:30:22,503 - INFO - training batch 1101, loss: 0.093, 35232/60000 datapoints
2025-03-06 20:30:22,705 - INFO - training batch 1151, loss: 0.146, 36832/60000 datapoints
2025-03-06 20:30:22,903 - INFO - training batch 1201, loss: 0.179, 38432/60000 datapoints
2025-03-06 20:30:23,096 - INFO - training batch 1251, loss: 0.273, 40032/60000 datapoints
2025-03-06 20:30:23,294 - INFO - training batch 1301, loss: 0.343, 41632/60000 datapoints
2025-03-06 20:30:23,487 - INFO - training batch 1351, loss: 0.212, 43232/60000 datapoints
2025-03-06 20:30:23,687 - INFO - training batch 1401, loss: 0.241, 44832/60000 datapoints
2025-03-06 20:30:23,882 - INFO - training batch 1451, loss: 0.104, 46432/60000 datapoints
2025-03-06 20:30:24,079 - INFO - training batch 1501, loss: 0.192, 48032/60000 datapoints
2025-03-06 20:30:24,275 - INFO - training batch 1551, loss: 0.161, 49632/60000 datapoints
2025-03-06 20:30:24,470 - INFO - training batch 1601, loss: 0.136, 51232/60000 datapoints
2025-03-06 20:30:24,666 - INFO - training batch 1651, loss: 0.190, 52832/60000 datapoints
2025-03-06 20:30:24,863 - INFO - training batch 1701, loss: 0.099, 54432/60000 datapoints
2025-03-06 20:30:25,062 - INFO - training batch 1751, loss: 0.072, 56032/60000 datapoints
2025-03-06 20:30:25,257 - INFO - training batch 1801, loss: 0.191, 57632/60000 datapoints
2025-03-06 20:30:25,451 - INFO - training batch 1851, loss: 0.188, 59232/60000 datapoints
2025-03-06 20:30:25,552 - INFO - validation batch 1, loss: 0.340, 32/10016 datapoints
2025-03-06 20:30:25,709 - INFO - validation batch 51, loss: 0.138, 1632/10016 datapoints
2025-03-06 20:30:25,860 - INFO - validation batch 101, loss: 0.117, 3232/10016 datapoints
2025-03-06 20:30:26,013 - INFO - validation batch 151, loss: 0.143, 4832/10016 datapoints
2025-03-06 20:30:26,169 - INFO - validation batch 201, loss: 0.074, 6432/10016 datapoints
2025-03-06 20:30:26,323 - INFO - validation batch 251, loss: 0.378, 8032/10016 datapoints
2025-03-06 20:30:26,476 - INFO - validation batch 301, loss: 0.100, 9632/10016 datapoints
2025-03-06 20:30:26,513 - INFO - Epoch 749/800 done.
2025-03-06 20:30:26,513 - INFO - Final validation performance:
Loss: 0.184, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:30:26,514 - INFO - Beginning epoch 750/800
2025-03-06 20:30:26,521 - INFO - training batch 1, loss: 0.045, 32/60000 datapoints
2025-03-06 20:30:26,727 - INFO - training batch 51, loss: 0.171, 1632/60000 datapoints
2025-03-06 20:30:26,922 - INFO - training batch 101, loss: 0.242, 3232/60000 datapoints
2025-03-06 20:30:27,126 - INFO - training batch 151, loss: 0.327, 4832/60000 datapoints
2025-03-06 20:30:27,325 - INFO - training batch 201, loss: 0.373, 6432/60000 datapoints
2025-03-06 20:30:27,524 - INFO - training batch 251, loss: 0.342, 8032/60000 datapoints
2025-03-06 20:30:27,728 - INFO - training batch 301, loss: 0.206, 9632/60000 datapoints
2025-03-06 20:30:27,932 - INFO - training batch 351, loss: 0.270, 11232/60000 datapoints
2025-03-06 20:30:28,130 - INFO - training batch 401, loss: 0.225, 12832/60000 datapoints
2025-03-06 20:30:28,323 - INFO - training batch 451, loss: 0.403, 14432/60000 datapoints
2025-03-06 20:30:28,517 - INFO - training batch 501, loss: 0.184, 16032/60000 datapoints
2025-03-06 20:30:28,715 - INFO - training batch 551, loss: 0.285, 17632/60000 datapoints
2025-03-06 20:30:28,910 - INFO - training batch 601, loss: 0.222, 19232/60000 datapoints
2025-03-06 20:30:29,102 - INFO - training batch 651, loss: 0.236, 20832/60000 datapoints
2025-03-06 20:30:29,297 - INFO - training batch 701, loss: 0.123, 22432/60000 datapoints
2025-03-06 20:30:29,493 - INFO - training batch 751, loss: 0.190, 24032/60000 datapoints
2025-03-06 20:30:29,689 - INFO - training batch 801, loss: 0.185, 25632/60000 datapoints
2025-03-06 20:30:29,897 - INFO - training batch 851, loss: 0.417, 27232/60000 datapoints
2025-03-06 20:30:30,101 - INFO - training batch 901, loss: 0.255, 28832/60000 datapoints
2025-03-06 20:30:30,297 - INFO - training batch 951, loss: 0.153, 30432/60000 datapoints
2025-03-06 20:30:30,491 - INFO - training batch 1001, loss: 0.054, 32032/60000 datapoints
2025-03-06 20:30:30,687 - INFO - training batch 1051, loss: 0.212, 33632/60000 datapoints
2025-03-06 20:30:30,881 - INFO - training batch 1101, loss: 0.133, 35232/60000 datapoints
2025-03-06 20:30:31,074 - INFO - training batch 1151, loss: 0.107, 36832/60000 datapoints
2025-03-06 20:30:31,272 - INFO - training batch 1201, loss: 0.365, 38432/60000 datapoints
2025-03-06 20:30:31,467 - INFO - training batch 1251, loss: 0.523, 40032/60000 datapoints
2025-03-06 20:30:31,671 - INFO - training batch 1301, loss: 0.219, 41632/60000 datapoints
2025-03-06 20:30:31,867 - INFO - training batch 1351, loss: 0.120, 43232/60000 datapoints
2025-03-06 20:30:32,061 - INFO - training batch 1401, loss: 0.206, 44832/60000 datapoints
2025-03-06 20:30:32,256 - INFO - training batch 1451, loss: 0.603, 46432/60000 datapoints
2025-03-06 20:30:32,449 - INFO - training batch 1501, loss: 0.109, 48032/60000 datapoints
2025-03-06 20:30:32,645 - INFO - training batch 1551, loss: 0.146, 49632/60000 datapoints
2025-03-06 20:30:32,842 - INFO - training batch 1601, loss: 0.196, 51232/60000 datapoints
2025-03-06 20:30:33,036 - INFO - training batch 1651, loss: 0.162, 52832/60000 datapoints
2025-03-06 20:30:33,232 - INFO - training batch 1701, loss: 0.273, 54432/60000 datapoints
2025-03-06 20:30:33,428 - INFO - training batch 1751, loss: 0.288, 56032/60000 datapoints
2025-03-06 20:30:33,623 - INFO - training batch 1801, loss: 0.226, 57632/60000 datapoints
2025-03-06 20:30:33,819 - INFO - training batch 1851, loss: 0.189, 59232/60000 datapoints
2025-03-06 20:30:33,921 - INFO - validation batch 1, loss: 0.218, 32/10016 datapoints
2025-03-06 20:30:34,074 - INFO - validation batch 51, loss: 0.129, 1632/10016 datapoints
2025-03-06 20:30:34,232 - INFO - validation batch 101, loss: 0.110, 3232/10016 datapoints
2025-03-06 20:30:34,384 - INFO - validation batch 151, loss: 0.093, 4832/10016 datapoints
2025-03-06 20:30:34,537 - INFO - validation batch 201, loss: 0.180, 6432/10016 datapoints
2025-03-06 20:30:34,695 - INFO - validation batch 251, loss: 0.085, 8032/10016 datapoints
2025-03-06 20:30:34,850 - INFO - validation batch 301, loss: 0.378, 9632/10016 datapoints
2025-03-06 20:30:34,891 - INFO - Epoch 750/800 done.
2025-03-06 20:30:34,891 - INFO - Final validation performance:
Loss: 0.170, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:30:34,892 - INFO - Beginning epoch 751/800
2025-03-06 20:30:34,900 - INFO - training batch 1, loss: 0.323, 32/60000 datapoints
2025-03-06 20:30:35,109 - INFO - training batch 51, loss: 0.062, 1632/60000 datapoints
2025-03-06 20:30:35,308 - INFO - training batch 101, loss: 0.078, 3232/60000 datapoints
2025-03-06 20:30:35,508 - INFO - training batch 151, loss: 0.099, 4832/60000 datapoints
2025-03-06 20:30:35,709 - INFO - training batch 201, loss: 0.402, 6432/60000 datapoints
2025-03-06 20:30:35,906 - INFO - training batch 251, loss: 0.178, 8032/60000 datapoints
2025-03-06 20:30:36,106 - INFO - training batch 301, loss: 0.228, 9632/60000 datapoints
2025-03-06 20:30:36,304 - INFO - training batch 351, loss: 0.097, 11232/60000 datapoints
2025-03-06 20:30:36,499 - INFO - training batch 401, loss: 0.188, 12832/60000 datapoints
2025-03-06 20:30:36,699 - INFO - training batch 451, loss: 0.269, 14432/60000 datapoints
2025-03-06 20:30:36,894 - INFO - training batch 501, loss: 0.212, 16032/60000 datapoints
2025-03-06 20:30:37,089 - INFO - training batch 551, loss: 0.201, 17632/60000 datapoints
2025-03-06 20:30:37,285 - INFO - training batch 601, loss: 0.102, 19232/60000 datapoints
2025-03-06 20:30:37,481 - INFO - training batch 651, loss: 0.101, 20832/60000 datapoints
2025-03-06 20:30:37,694 - INFO - training batch 701, loss: 0.208, 22432/60000 datapoints
2025-03-06 20:30:37,888 - INFO - training batch 751, loss: 0.128, 24032/60000 datapoints
2025-03-06 20:30:38,084 - INFO - training batch 801, loss: 0.086, 25632/60000 datapoints
2025-03-06 20:30:38,281 - INFO - training batch 851, loss: 0.299, 27232/60000 datapoints
2025-03-06 20:30:38,476 - INFO - training batch 901, loss: 0.482, 28832/60000 datapoints
2025-03-06 20:30:38,673 - INFO - training batch 951, loss: 0.625, 30432/60000 datapoints
2025-03-06 20:30:38,875 - INFO - training batch 1001, loss: 0.251, 32032/60000 datapoints
2025-03-06 20:30:39,071 - INFO - training batch 1051, loss: 0.315, 33632/60000 datapoints
2025-03-06 20:30:39,265 - INFO - training batch 1101, loss: 0.458, 35232/60000 datapoints
2025-03-06 20:30:39,459 - INFO - training batch 1151, loss: 0.377, 36832/60000 datapoints
2025-03-06 20:30:39,655 - INFO - training batch 1201, loss: 0.232, 38432/60000 datapoints
2025-03-06 20:30:39,851 - INFO - training batch 1251, loss: 0.245, 40032/60000 datapoints
2025-03-06 20:30:40,068 - INFO - training batch 1301, loss: 0.374, 41632/60000 datapoints
2025-03-06 20:30:40,265 - INFO - training batch 1351, loss: 0.168, 43232/60000 datapoints
2025-03-06 20:30:40,459 - INFO - training batch 1401, loss: 0.120, 44832/60000 datapoints
2025-03-06 20:30:40,655 - INFO - training batch 1451, loss: 0.072, 46432/60000 datapoints
2025-03-06 20:30:40,850 - INFO - training batch 1501, loss: 0.207, 48032/60000 datapoints
2025-03-06 20:30:41,051 - INFO - training batch 1551, loss: 0.160, 49632/60000 datapoints
2025-03-06 20:30:41,245 - INFO - training batch 1601, loss: 0.399, 51232/60000 datapoints
2025-03-06 20:30:41,439 - INFO - training batch 1651, loss: 0.260, 52832/60000 datapoints
2025-03-06 20:30:41,633 - INFO - training batch 1701, loss: 0.157, 54432/60000 datapoints
2025-03-06 20:30:41,829 - INFO - training batch 1751, loss: 0.075, 56032/60000 datapoints
2025-03-06 20:30:42,027 - INFO - training batch 1801, loss: 0.152, 57632/60000 datapoints
2025-03-06 20:30:42,252 - INFO - training batch 1851, loss: 0.347, 59232/60000 datapoints
2025-03-06 20:30:42,354 - INFO - validation batch 1, loss: 0.185, 32/10016 datapoints
2025-03-06 20:30:42,506 - INFO - validation batch 51, loss: 0.108, 1632/10016 datapoints
2025-03-06 20:30:42,662 - INFO - validation batch 101, loss: 0.236, 3232/10016 datapoints
2025-03-06 20:30:42,814 - INFO - validation batch 151, loss: 0.357, 4832/10016 datapoints
2025-03-06 20:30:42,970 - INFO - validation batch 201, loss: 0.226, 6432/10016 datapoints
2025-03-06 20:30:43,129 - INFO - validation batch 251, loss: 0.249, 8032/10016 datapoints
2025-03-06 20:30:43,284 - INFO - validation batch 301, loss: 0.151, 9632/10016 datapoints
2025-03-06 20:30:43,322 - INFO - Epoch 751/800 done.
2025-03-06 20:30:43,323 - INFO - Final validation performance:
Loss: 0.216, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:30:43,323 - INFO - Beginning epoch 752/800
2025-03-06 20:30:43,331 - INFO - training batch 1, loss: 0.133, 32/60000 datapoints
2025-03-06 20:30:43,524 - INFO - training batch 51, loss: 0.389, 1632/60000 datapoints
2025-03-06 20:30:43,719 - INFO - training batch 101, loss: 0.174, 3232/60000 datapoints
2025-03-06 20:30:43,926 - INFO - training batch 151, loss: 0.138, 4832/60000 datapoints
2025-03-06 20:30:44,116 - INFO - training batch 201, loss: 0.407, 6432/60000 datapoints
2025-03-06 20:30:44,317 - INFO - training batch 251, loss: 0.181, 8032/60000 datapoints
2025-03-06 20:30:44,514 - INFO - training batch 301, loss: 0.178, 9632/60000 datapoints
2025-03-06 20:30:44,707 - INFO - training batch 351, loss: 0.246, 11232/60000 datapoints
2025-03-06 20:30:44,906 - INFO - training batch 401, loss: 0.152, 12832/60000 datapoints
2025-03-06 20:30:45,100 - INFO - training batch 451, loss: 0.306, 14432/60000 datapoints
2025-03-06 20:30:45,292 - INFO - training batch 501, loss: 0.086, 16032/60000 datapoints
2025-03-06 20:30:45,483 - INFO - training batch 551, loss: 0.420, 17632/60000 datapoints
2025-03-06 20:30:45,679 - INFO - training batch 601, loss: 0.102, 19232/60000 datapoints
2025-03-06 20:30:45,875 - INFO - training batch 651, loss: 0.142, 20832/60000 datapoints
2025-03-06 20:30:46,069 - INFO - training batch 701, loss: 0.299, 22432/60000 datapoints
2025-03-06 20:30:46,260 - INFO - training batch 751, loss: 0.259, 24032/60000 datapoints
2025-03-06 20:30:46,453 - INFO - training batch 801, loss: 0.299, 25632/60000 datapoints
2025-03-06 20:30:46,645 - INFO - training batch 851, loss: 0.074, 27232/60000 datapoints
2025-03-06 20:30:46,844 - INFO - training batch 901, loss: 0.193, 28832/60000 datapoints
2025-03-06 20:30:47,036 - INFO - training batch 951, loss: 0.175, 30432/60000 datapoints
2025-03-06 20:30:47,225 - INFO - training batch 1001, loss: 0.236, 32032/60000 datapoints
2025-03-06 20:30:47,419 - INFO - training batch 1051, loss: 0.062, 33632/60000 datapoints
2025-03-06 20:30:47,614 - INFO - training batch 1101, loss: 0.270, 35232/60000 datapoints
2025-03-06 20:30:47,810 - INFO - training batch 1151, loss: 0.141, 36832/60000 datapoints
2025-03-06 20:30:48,002 - INFO - training batch 1201, loss: 0.111, 38432/60000 datapoints
2025-03-06 20:30:48,195 - INFO - training batch 1251, loss: 0.123, 40032/60000 datapoints
2025-03-06 20:30:48,387 - INFO - training batch 1301, loss: 0.099, 41632/60000 datapoints
2025-03-06 20:30:48,579 - INFO - training batch 1351, loss: 0.114, 43232/60000 datapoints
2025-03-06 20:30:48,772 - INFO - training batch 1401, loss: 0.083, 44832/60000 datapoints
2025-03-06 20:30:48,975 - INFO - training batch 1451, loss: 0.169, 46432/60000 datapoints
2025-03-06 20:30:49,173 - INFO - training batch 1501, loss: 0.429, 48032/60000 datapoints
2025-03-06 20:30:49,371 - INFO - training batch 1551, loss: 0.319, 49632/60000 datapoints
2025-03-06 20:30:49,565 - INFO - training batch 1601, loss: 0.103, 51232/60000 datapoints
2025-03-06 20:30:49,760 - INFO - training batch 1651, loss: 0.133, 52832/60000 datapoints
2025-03-06 20:30:49,959 - INFO - training batch 1701, loss: 0.216, 54432/60000 datapoints
2025-03-06 20:30:50,185 - INFO - training batch 1751, loss: 0.406, 56032/60000 datapoints
2025-03-06 20:30:50,382 - INFO - training batch 1801, loss: 0.164, 57632/60000 datapoints
2025-03-06 20:30:50,579 - INFO - training batch 1851, loss: 0.220, 59232/60000 datapoints
2025-03-06 20:30:50,685 - INFO - validation batch 1, loss: 0.201, 32/10016 datapoints
2025-03-06 20:30:50,839 - INFO - validation batch 51, loss: 0.077, 1632/10016 datapoints
2025-03-06 20:30:50,994 - INFO - validation batch 101, loss: 0.224, 3232/10016 datapoints
2025-03-06 20:30:51,147 - INFO - validation batch 151, loss: 0.148, 4832/10016 datapoints
2025-03-06 20:30:51,301 - INFO - validation batch 201, loss: 0.098, 6432/10016 datapoints
2025-03-06 20:30:51,456 - INFO - validation batch 251, loss: 0.324, 8032/10016 datapoints
2025-03-06 20:30:51,609 - INFO - validation batch 301, loss: 0.237, 9632/10016 datapoints
2025-03-06 20:30:51,648 - INFO - Epoch 752/800 done.
2025-03-06 20:30:51,648 - INFO - Final validation performance:
Loss: 0.187, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:30:51,649 - INFO - Beginning epoch 753/800
2025-03-06 20:30:51,656 - INFO - training batch 1, loss: 0.060, 32/60000 datapoints
2025-03-06 20:30:51,862 - INFO - training batch 51, loss: 0.265, 1632/60000 datapoints
2025-03-06 20:30:52,062 - INFO - training batch 101, loss: 0.401, 3232/60000 datapoints
2025-03-06 20:30:52,266 - INFO - training batch 151, loss: 0.224, 4832/60000 datapoints
2025-03-06 20:30:52,467 - INFO - training batch 201, loss: 0.072, 6432/60000 datapoints
2025-03-06 20:30:52,668 - INFO - training batch 251, loss: 0.106, 8032/60000 datapoints
2025-03-06 20:30:52,861 - INFO - training batch 301, loss: 0.257, 9632/60000 datapoints
2025-03-06 20:30:53,058 - INFO - training batch 351, loss: 0.208, 11232/60000 datapoints
2025-03-06 20:30:53,256 - INFO - training batch 401, loss: 0.149, 12832/60000 datapoints
2025-03-06 20:30:53,449 - INFO - training batch 451, loss: 0.133, 14432/60000 datapoints
2025-03-06 20:30:53,646 - INFO - training batch 501, loss: 0.330, 16032/60000 datapoints
2025-03-06 20:30:53,839 - INFO - training batch 551, loss: 0.128, 17632/60000 datapoints
2025-03-06 20:30:54,036 - INFO - training batch 601, loss: 0.154, 19232/60000 datapoints
2025-03-06 20:30:54,235 - INFO - training batch 651, loss: 0.190, 20832/60000 datapoints
2025-03-06 20:30:54,429 - INFO - training batch 701, loss: 0.137, 22432/60000 datapoints
2025-03-06 20:30:54,627 - INFO - training batch 751, loss: 0.082, 24032/60000 datapoints
2025-03-06 20:30:54,822 - INFO - training batch 801, loss: 0.112, 25632/60000 datapoints
2025-03-06 20:30:55,022 - INFO - training batch 851, loss: 0.306, 27232/60000 datapoints
2025-03-06 20:30:55,218 - INFO - training batch 901, loss: 0.220, 28832/60000 datapoints
2025-03-06 20:30:55,410 - INFO - training batch 951, loss: 0.191, 30432/60000 datapoints
2025-03-06 20:30:55,607 - INFO - training batch 1001, loss: 0.250, 32032/60000 datapoints
2025-03-06 20:30:55,806 - INFO - training batch 1051, loss: 0.336, 33632/60000 datapoints
2025-03-06 20:30:56,001 - INFO - training batch 1101, loss: 0.076, 35232/60000 datapoints
2025-03-06 20:30:56,196 - INFO - training batch 1151, loss: 0.196, 36832/60000 datapoints
2025-03-06 20:30:56,391 - INFO - training batch 1201, loss: 0.129, 38432/60000 datapoints
2025-03-06 20:30:56,589 - INFO - training batch 1251, loss: 0.223, 40032/60000 datapoints
2025-03-06 20:30:56,784 - INFO - training batch 1301, loss: 0.052, 41632/60000 datapoints
2025-03-06 20:30:56,980 - INFO - training batch 1351, loss: 0.153, 43232/60000 datapoints
2025-03-06 20:30:57,173 - INFO - training batch 1401, loss: 0.278, 44832/60000 datapoints
2025-03-06 20:30:57,367 - INFO - training batch 1451, loss: 0.341, 46432/60000 datapoints
2025-03-06 20:30:57,562 - INFO - training batch 1501, loss: 0.353, 48032/60000 datapoints
2025-03-06 20:30:57,759 - INFO - training batch 1551, loss: 0.216, 49632/60000 datapoints
2025-03-06 20:30:57,955 - INFO - training batch 1601, loss: 0.434, 51232/60000 datapoints
2025-03-06 20:30:58,149 - INFO - training batch 1651, loss: 0.345, 52832/60000 datapoints
2025-03-06 20:30:58,348 - INFO - training batch 1701, loss: 0.100, 54432/60000 datapoints
2025-03-06 20:30:58,550 - INFO - training batch 1751, loss: 0.257, 56032/60000 datapoints
2025-03-06 20:30:58,752 - INFO - training batch 1801, loss: 0.277, 57632/60000 datapoints
2025-03-06 20:30:58,955 - INFO - training batch 1851, loss: 0.187, 59232/60000 datapoints
2025-03-06 20:30:59,062 - INFO - validation batch 1, loss: 0.150, 32/10016 datapoints
2025-03-06 20:30:59,213 - INFO - validation batch 51, loss: 0.250, 1632/10016 datapoints
2025-03-06 20:30:59,366 - INFO - validation batch 101, loss: 0.322, 3232/10016 datapoints
2025-03-06 20:30:59,520 - INFO - validation batch 151, loss: 0.256, 4832/10016 datapoints
2025-03-06 20:30:59,675 - INFO - validation batch 201, loss: 0.142, 6432/10016 datapoints
2025-03-06 20:30:59,827 - INFO - validation batch 251, loss: 0.083, 8032/10016 datapoints
2025-03-06 20:30:59,980 - INFO - validation batch 301, loss: 0.170, 9632/10016 datapoints
2025-03-06 20:31:00,019 - INFO - Epoch 753/800 done.
2025-03-06 20:31:00,019 - INFO - Final validation performance:
Loss: 0.196, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:31:00,020 - INFO - Beginning epoch 754/800
2025-03-06 20:31:00,028 - INFO - training batch 1, loss: 0.296, 32/60000 datapoints
2025-03-06 20:31:00,248 - INFO - training batch 51, loss: 0.391, 1632/60000 datapoints
2025-03-06 20:31:00,455 - INFO - training batch 101, loss: 0.212, 3232/60000 datapoints
2025-03-06 20:31:00,653 - INFO - training batch 151, loss: 0.230, 4832/60000 datapoints
2025-03-06 20:31:00,853 - INFO - training batch 201, loss: 0.148, 6432/60000 datapoints
2025-03-06 20:31:01,050 - INFO - training batch 251, loss: 0.337, 8032/60000 datapoints
2025-03-06 20:31:01,246 - INFO - training batch 301, loss: 0.130, 9632/60000 datapoints
2025-03-06 20:31:01,436 - INFO - training batch 351, loss: 0.152, 11232/60000 datapoints
2025-03-06 20:31:01,630 - INFO - training batch 401, loss: 0.359, 12832/60000 datapoints
2025-03-06 20:31:01,824 - INFO - training batch 451, loss: 0.118, 14432/60000 datapoints
2025-03-06 20:31:02,017 - INFO - training batch 501, loss: 0.115, 16032/60000 datapoints
2025-03-06 20:31:02,213 - INFO - training batch 551, loss: 0.191, 17632/60000 datapoints
2025-03-06 20:31:02,407 - INFO - training batch 601, loss: 0.052, 19232/60000 datapoints
2025-03-06 20:31:02,600 - INFO - training batch 651, loss: 0.206, 20832/60000 datapoints
2025-03-06 20:31:02,796 - INFO - training batch 701, loss: 0.289, 22432/60000 datapoints
2025-03-06 20:31:02,988 - INFO - training batch 751, loss: 0.143, 24032/60000 datapoints
2025-03-06 20:31:03,181 - INFO - training batch 801, loss: 0.233, 25632/60000 datapoints
2025-03-06 20:31:03,372 - INFO - training batch 851, loss: 0.121, 27232/60000 datapoints
2025-03-06 20:31:03,566 - INFO - training batch 901, loss: 0.102, 28832/60000 datapoints
2025-03-06 20:31:03,760 - INFO - training batch 951, loss: 0.159, 30432/60000 datapoints
2025-03-06 20:31:03,951 - INFO - training batch 1001, loss: 0.143, 32032/60000 datapoints
2025-03-06 20:31:04,145 - INFO - training batch 1051, loss: 0.469, 33632/60000 datapoints
2025-03-06 20:31:04,340 - INFO - training batch 1101, loss: 0.595, 35232/60000 datapoints
2025-03-06 20:31:04,531 - INFO - training batch 1151, loss: 0.096, 36832/60000 datapoints
2025-03-06 20:31:04,727 - INFO - training batch 1201, loss: 0.151, 38432/60000 datapoints
2025-03-06 20:31:04,924 - INFO - training batch 1251, loss: 0.086, 40032/60000 datapoints
2025-03-06 20:31:05,123 - INFO - training batch 1301, loss: 0.491, 41632/60000 datapoints
2025-03-06 20:31:05,326 - INFO - training batch 1351, loss: 0.260, 43232/60000 datapoints
2025-03-06 20:31:05,517 - INFO - training batch 1401, loss: 0.144, 44832/60000 datapoints
2025-03-06 20:31:05,710 - INFO - training batch 1451, loss: 0.372, 46432/60000 datapoints
2025-03-06 20:31:05,904 - INFO - training batch 1501, loss: 0.032, 48032/60000 datapoints
2025-03-06 20:31:06,093 - INFO - training batch 1551, loss: 0.236, 49632/60000 datapoints
2025-03-06 20:31:06,291 - INFO - training batch 1601, loss: 0.087, 51232/60000 datapoints
2025-03-06 20:31:06,482 - INFO - training batch 1651, loss: 0.373, 52832/60000 datapoints
2025-03-06 20:31:06,678 - INFO - training batch 1701, loss: 0.241, 54432/60000 datapoints
2025-03-06 20:31:06,873 - INFO - training batch 1751, loss: 0.235, 56032/60000 datapoints
2025-03-06 20:31:07,065 - INFO - training batch 1801, loss: 0.149, 57632/60000 datapoints
2025-03-06 20:31:07,256 - INFO - training batch 1851, loss: 0.075, 59232/60000 datapoints
2025-03-06 20:31:07,356 - INFO - validation batch 1, loss: 0.152, 32/10016 datapoints
2025-03-06 20:31:07,509 - INFO - validation batch 51, loss: 0.385, 1632/10016 datapoints
2025-03-06 20:31:07,666 - INFO - validation batch 101, loss: 0.144, 3232/10016 datapoints
2025-03-06 20:31:07,819 - INFO - validation batch 151, loss: 0.081, 4832/10016 datapoints
2025-03-06 20:31:07,972 - INFO - validation batch 201, loss: 0.186, 6432/10016 datapoints
2025-03-06 20:31:08,127 - INFO - validation batch 251, loss: 0.337, 8032/10016 datapoints
2025-03-06 20:31:08,281 - INFO - validation batch 301, loss: 0.222, 9632/10016 datapoints
2025-03-06 20:31:08,317 - INFO - Epoch 754/800 done.
2025-03-06 20:31:08,317 - INFO - Final validation performance:
Loss: 0.215, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:31:08,318 - INFO - Beginning epoch 755/800
2025-03-06 20:31:08,324 - INFO - training batch 1, loss: 0.111, 32/60000 datapoints
2025-03-06 20:31:08,517 - INFO - training batch 51, loss: 0.167, 1632/60000 datapoints
2025-03-06 20:31:08,711 - INFO - training batch 101, loss: 0.229, 3232/60000 datapoints
2025-03-06 20:31:08,919 - INFO - training batch 151, loss: 0.129, 4832/60000 datapoints
2025-03-06 20:31:09,121 - INFO - training batch 201, loss: 0.193, 6432/60000 datapoints
2025-03-06 20:31:09,319 - INFO - training batch 251, loss: 0.067, 8032/60000 datapoints
2025-03-06 20:31:09,518 - INFO - training batch 301, loss: 0.157, 9632/60000 datapoints
2025-03-06 20:31:09,723 - INFO - training batch 351, loss: 0.158, 11232/60000 datapoints
2025-03-06 20:31:09,919 - INFO - training batch 401, loss: 0.329, 12832/60000 datapoints
2025-03-06 20:31:10,114 - INFO - training batch 451, loss: 0.120, 14432/60000 datapoints
2025-03-06 20:31:10,328 - INFO - training batch 501, loss: 0.173, 16032/60000 datapoints
2025-03-06 20:31:10,523 - INFO - training batch 551, loss: 0.310, 17632/60000 datapoints
2025-03-06 20:31:10,718 - INFO - training batch 601, loss: 0.107, 19232/60000 datapoints
2025-03-06 20:31:10,913 - INFO - training batch 651, loss: 0.149, 20832/60000 datapoints
2025-03-06 20:31:11,105 - INFO - training batch 701, loss: 0.289, 22432/60000 datapoints
2025-03-06 20:31:11,301 - INFO - training batch 751, loss: 0.432, 24032/60000 datapoints
2025-03-06 20:31:11,496 - INFO - training batch 801, loss: 0.070, 25632/60000 datapoints
2025-03-06 20:31:11,693 - INFO - training batch 851, loss: 0.202, 27232/60000 datapoints
2025-03-06 20:31:11,888 - INFO - training batch 901, loss: 0.178, 28832/60000 datapoints
2025-03-06 20:31:12,083 - INFO - training batch 951, loss: 0.302, 30432/60000 datapoints
2025-03-06 20:31:12,281 - INFO - training batch 1001, loss: 0.090, 32032/60000 datapoints
2025-03-06 20:31:12,477 - INFO - training batch 1051, loss: 0.136, 33632/60000 datapoints
2025-03-06 20:31:12,679 - INFO - training batch 1101, loss: 0.073, 35232/60000 datapoints
2025-03-06 20:31:12,874 - INFO - training batch 1151, loss: 0.210, 36832/60000 datapoints
2025-03-06 20:31:13,072 - INFO - training batch 1201, loss: 0.167, 38432/60000 datapoints
2025-03-06 20:31:13,268 - INFO - training batch 1251, loss: 0.220, 40032/60000 datapoints
2025-03-06 20:31:13,465 - INFO - training batch 1301, loss: 0.104, 41632/60000 datapoints
2025-03-06 20:31:13,665 - INFO - training batch 1351, loss: 0.191, 43232/60000 datapoints
2025-03-06 20:31:13,863 - INFO - training batch 1401, loss: 0.314, 44832/60000 datapoints
2025-03-06 20:31:14,056 - INFO - training batch 1451, loss: 0.250, 46432/60000 datapoints
2025-03-06 20:31:14,253 - INFO - training batch 1501, loss: 0.030, 48032/60000 datapoints
2025-03-06 20:31:14,449 - INFO - training batch 1551, loss: 0.302, 49632/60000 datapoints
2025-03-06 20:31:14,645 - INFO - training batch 1601, loss: 0.108, 51232/60000 datapoints
2025-03-06 20:31:14,842 - INFO - training batch 1651, loss: 0.205, 52832/60000 datapoints
2025-03-06 20:31:15,044 - INFO - training batch 1701, loss: 0.187, 54432/60000 datapoints
2025-03-06 20:31:15,240 - INFO - training batch 1751, loss: 0.137, 56032/60000 datapoints
2025-03-06 20:31:15,436 - INFO - training batch 1801, loss: 0.137, 57632/60000 datapoints
2025-03-06 20:31:15,633 - INFO - training batch 1851, loss: 0.281, 59232/60000 datapoints
2025-03-06 20:31:15,736 - INFO - validation batch 1, loss: 0.377, 32/10016 datapoints
2025-03-06 20:31:15,891 - INFO - validation batch 51, loss: 0.363, 1632/10016 datapoints
2025-03-06 20:31:16,045 - INFO - validation batch 101, loss: 0.196, 3232/10016 datapoints
2025-03-06 20:31:16,198 - INFO - validation batch 151, loss: 0.140, 4832/10016 datapoints
2025-03-06 20:31:16,354 - INFO - validation batch 201, loss: 0.082, 6432/10016 datapoints
2025-03-06 20:31:16,508 - INFO - validation batch 251, loss: 0.217, 8032/10016 datapoints
2025-03-06 20:31:16,663 - INFO - validation batch 301, loss: 0.439, 9632/10016 datapoints
2025-03-06 20:31:16,700 - INFO - Epoch 755/800 done.
2025-03-06 20:31:16,700 - INFO - Final validation performance:
Loss: 0.259, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:31:16,701 - INFO - Beginning epoch 756/800
2025-03-06 20:31:16,708 - INFO - training batch 1, loss: 0.082, 32/60000 datapoints
2025-03-06 20:31:16,910 - INFO - training batch 51, loss: 0.292, 1632/60000 datapoints
2025-03-06 20:31:17,106 - INFO - training batch 101, loss: 0.142, 3232/60000 datapoints
2025-03-06 20:31:17,310 - INFO - training batch 151, loss: 0.468, 4832/60000 datapoints
2025-03-06 20:31:17,516 - INFO - training batch 201, loss: 0.170, 6432/60000 datapoints
2025-03-06 20:31:17,749 - INFO - training batch 251, loss: 0.155, 8032/60000 datapoints
2025-03-06 20:31:17,974 - INFO - training batch 301, loss: 0.304, 9632/60000 datapoints
2025-03-06 20:31:18,172 - INFO - training batch 351, loss: 0.084, 11232/60000 datapoints
2025-03-06 20:31:18,370 - INFO - training batch 401, loss: 0.066, 12832/60000 datapoints
2025-03-06 20:31:18,565 - INFO - training batch 451, loss: 0.448, 14432/60000 datapoints
2025-03-06 20:31:18,766 - INFO - training batch 501, loss: 0.076, 16032/60000 datapoints
2025-03-06 20:31:18,962 - INFO - training batch 551, loss: 0.156, 17632/60000 datapoints
2025-03-06 20:31:19,159 - INFO - training batch 601, loss: 0.222, 19232/60000 datapoints
2025-03-06 20:31:19,355 - INFO - training batch 651, loss: 0.224, 20832/60000 datapoints
2025-03-06 20:31:19,551 - INFO - training batch 701, loss: 0.235, 22432/60000 datapoints
2025-03-06 20:31:19,748 - INFO - training batch 751, loss: 0.031, 24032/60000 datapoints
2025-03-06 20:31:19,945 - INFO - training batch 801, loss: 0.365, 25632/60000 datapoints
2025-03-06 20:31:20,143 - INFO - training batch 851, loss: 0.083, 27232/60000 datapoints
2025-03-06 20:31:20,362 - INFO - training batch 901, loss: 0.111, 28832/60000 datapoints
2025-03-06 20:31:20,560 - INFO - training batch 951, loss: 0.285, 30432/60000 datapoints
2025-03-06 20:31:20,761 - INFO - training batch 1001, loss: 0.346, 32032/60000 datapoints
2025-03-06 20:31:20,959 - INFO - training batch 1051, loss: 0.123, 33632/60000 datapoints
2025-03-06 20:31:21,153 - INFO - training batch 1101, loss: 0.197, 35232/60000 datapoints
2025-03-06 20:31:21,351 - INFO - training batch 1151, loss: 0.401, 36832/60000 datapoints
2025-03-06 20:31:21,545 - INFO - training batch 1201, loss: 0.228, 38432/60000 datapoints
2025-03-06 20:31:21,743 - INFO - training batch 1251, loss: 0.204, 40032/60000 datapoints
2025-03-06 20:31:21,938 - INFO - training batch 1301, loss: 0.054, 41632/60000 datapoints
2025-03-06 20:31:22,132 - INFO - training batch 1351, loss: 0.181, 43232/60000 datapoints
2025-03-06 20:31:22,329 - INFO - training batch 1401, loss: 0.812, 44832/60000 datapoints
2025-03-06 20:31:22,524 - INFO - training batch 1451, loss: 0.145, 46432/60000 datapoints
2025-03-06 20:31:22,724 - INFO - training batch 1501, loss: 0.229, 48032/60000 datapoints
2025-03-06 20:31:22,921 - INFO - training batch 1551, loss: 0.091, 49632/60000 datapoints
2025-03-06 20:31:23,118 - INFO - training batch 1601, loss: 0.236, 51232/60000 datapoints
2025-03-06 20:31:23,314 - INFO - training batch 1651, loss: 0.170, 52832/60000 datapoints
2025-03-06 20:31:23,510 - INFO - training batch 1701, loss: 0.059, 54432/60000 datapoints
2025-03-06 20:31:23,707 - INFO - training batch 1751, loss: 0.147, 56032/60000 datapoints
2025-03-06 20:31:23,903 - INFO - training batch 1801, loss: 0.059, 57632/60000 datapoints
2025-03-06 20:31:24,094 - INFO - training batch 1851, loss: 0.318, 59232/60000 datapoints
2025-03-06 20:31:24,196 - INFO - validation batch 1, loss: 0.375, 32/10016 datapoints
2025-03-06 20:31:24,353 - INFO - validation batch 51, loss: 0.140, 1632/10016 datapoints
2025-03-06 20:31:24,506 - INFO - validation batch 101, loss: 0.273, 3232/10016 datapoints
2025-03-06 20:31:24,661 - INFO - validation batch 151, loss: 0.235, 4832/10016 datapoints
2025-03-06 20:31:24,815 - INFO - validation batch 201, loss: 0.169, 6432/10016 datapoints
2025-03-06 20:31:24,974 - INFO - validation batch 251, loss: 0.143, 8032/10016 datapoints
2025-03-06 20:31:25,129 - INFO - validation batch 301, loss: 0.574, 9632/10016 datapoints
2025-03-06 20:31:25,165 - INFO - Epoch 756/800 done.
2025-03-06 20:31:25,166 - INFO - Final validation performance:
Loss: 0.273, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:31:25,166 - INFO - Beginning epoch 757/800
2025-03-06 20:31:25,174 - INFO - training batch 1, loss: 0.196, 32/60000 datapoints
2025-03-06 20:31:25,374 - INFO - training batch 51, loss: 0.098, 1632/60000 datapoints
2025-03-06 20:31:25,580 - INFO - training batch 101, loss: 0.197, 3232/60000 datapoints
2025-03-06 20:31:25,774 - INFO - training batch 151, loss: 0.220, 4832/60000 datapoints
2025-03-06 20:31:25,975 - INFO - training batch 201, loss: 0.101, 6432/60000 datapoints
2025-03-06 20:31:26,171 - INFO - training batch 251, loss: 0.077, 8032/60000 datapoints
2025-03-06 20:31:26,372 - INFO - training batch 301, loss: 0.383, 9632/60000 datapoints
2025-03-06 20:31:26,567 - INFO - training batch 351, loss: 0.105, 11232/60000 datapoints
2025-03-06 20:31:26,767 - INFO - training batch 401, loss: 0.144, 12832/60000 datapoints
2025-03-06 20:31:26,965 - INFO - training batch 451, loss: 0.284, 14432/60000 datapoints
2025-03-06 20:31:27,160 - INFO - training batch 501, loss: 0.182, 16032/60000 datapoints
2025-03-06 20:31:27,356 - INFO - training batch 551, loss: 0.098, 17632/60000 datapoints
2025-03-06 20:31:27,550 - INFO - training batch 601, loss: 0.078, 19232/60000 datapoints
2025-03-06 20:31:27,755 - INFO - training batch 651, loss: 0.329, 20832/60000 datapoints
2025-03-06 20:31:27,953 - INFO - training batch 701, loss: 0.264, 22432/60000 datapoints
2025-03-06 20:31:28,148 - INFO - training batch 751, loss: 0.255, 24032/60000 datapoints
2025-03-06 20:31:28,346 - INFO - training batch 801, loss: 0.255, 25632/60000 datapoints
2025-03-06 20:31:28,543 - INFO - training batch 851, loss: 0.097, 27232/60000 datapoints
2025-03-06 20:31:28,748 - INFO - training batch 901, loss: 0.298, 28832/60000 datapoints
2025-03-06 20:31:28,947 - INFO - training batch 951, loss: 0.355, 30432/60000 datapoints
2025-03-06 20:31:29,148 - INFO - training batch 1001, loss: 0.051, 32032/60000 datapoints
2025-03-06 20:31:29,342 - INFO - training batch 1051, loss: 0.097, 33632/60000 datapoints
2025-03-06 20:31:29,540 - INFO - training batch 1101, loss: 0.134, 35232/60000 datapoints
2025-03-06 20:31:29,737 - INFO - training batch 1151, loss: 0.384, 36832/60000 datapoints
2025-03-06 20:31:29,933 - INFO - training batch 1201, loss: 0.226, 38432/60000 datapoints
2025-03-06 20:31:30,132 - INFO - training batch 1251, loss: 0.156, 40032/60000 datapoints
2025-03-06 20:31:30,337 - INFO - training batch 1301, loss: 0.213, 41632/60000 datapoints
2025-03-06 20:31:30,555 - INFO - training batch 1351, loss: 0.066, 43232/60000 datapoints
2025-03-06 20:31:30,754 - INFO - training batch 1401, loss: 0.340, 44832/60000 datapoints
2025-03-06 20:31:30,948 - INFO - training batch 1451, loss: 0.063, 46432/60000 datapoints
2025-03-06 20:31:31,142 - INFO - training batch 1501, loss: 0.345, 48032/60000 datapoints
2025-03-06 20:31:31,338 - INFO - training batch 1551, loss: 0.121, 49632/60000 datapoints
2025-03-06 20:31:31,536 - INFO - training batch 1601, loss: 0.105, 51232/60000 datapoints
2025-03-06 20:31:31,733 - INFO - training batch 1651, loss: 0.041, 52832/60000 datapoints
2025-03-06 20:31:31,931 - INFO - training batch 1701, loss: 0.263, 54432/60000 datapoints
2025-03-06 20:31:32,123 - INFO - training batch 1751, loss: 0.452, 56032/60000 datapoints
2025-03-06 20:31:32,319 - INFO - training batch 1801, loss: 0.179, 57632/60000 datapoints
2025-03-06 20:31:32,513 - INFO - training batch 1851, loss: 0.102, 59232/60000 datapoints
2025-03-06 20:31:32,613 - INFO - validation batch 1, loss: 0.216, 32/10016 datapoints
2025-03-06 20:31:32,770 - INFO - validation batch 51, loss: 0.061, 1632/10016 datapoints
2025-03-06 20:31:32,925 - INFO - validation batch 101, loss: 0.147, 3232/10016 datapoints
2025-03-06 20:31:33,080 - INFO - validation batch 151, loss: 0.154, 4832/10016 datapoints
2025-03-06 20:31:33,240 - INFO - validation batch 201, loss: 0.328, 6432/10016 datapoints
2025-03-06 20:31:33,400 - INFO - validation batch 251, loss: 0.090, 8032/10016 datapoints
2025-03-06 20:31:33,553 - INFO - validation batch 301, loss: 0.362, 9632/10016 datapoints
2025-03-06 20:31:33,591 - INFO - Epoch 757/800 done.
2025-03-06 20:31:33,591 - INFO - Final validation performance:
Loss: 0.194, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:31:33,592 - INFO - Beginning epoch 758/800
2025-03-06 20:31:33,599 - INFO - training batch 1, loss: 0.106, 32/60000 datapoints
2025-03-06 20:31:33,797 - INFO - training batch 51, loss: 0.065, 1632/60000 datapoints
2025-03-06 20:31:33,997 - INFO - training batch 101, loss: 0.061, 3232/60000 datapoints
2025-03-06 20:31:34,203 - INFO - training batch 151, loss: 0.099, 4832/60000 datapoints
2025-03-06 20:31:34,405 - INFO - training batch 201, loss: 0.185, 6432/60000 datapoints
2025-03-06 20:31:34,601 - INFO - training batch 251, loss: 0.355, 8032/60000 datapoints
2025-03-06 20:31:34,803 - INFO - training batch 301, loss: 0.419, 9632/60000 datapoints
2025-03-06 20:31:35,011 - INFO - training batch 351, loss: 0.081, 11232/60000 datapoints
2025-03-06 20:31:35,225 - INFO - training batch 401, loss: 0.181, 12832/60000 datapoints
2025-03-06 20:31:35,441 - INFO - training batch 451, loss: 0.138, 14432/60000 datapoints
2025-03-06 20:31:35,639 - INFO - training batch 501, loss: 0.133, 16032/60000 datapoints
2025-03-06 20:31:35,832 - INFO - training batch 551, loss: 0.178, 17632/60000 datapoints
2025-03-06 20:31:36,028 - INFO - training batch 601, loss: 0.036, 19232/60000 datapoints
2025-03-06 20:31:36,224 - INFO - training batch 651, loss: 0.246, 20832/60000 datapoints
2025-03-06 20:31:36,423 - INFO - training batch 701, loss: 0.200, 22432/60000 datapoints
2025-03-06 20:31:36,616 - INFO - training batch 751, loss: 0.108, 24032/60000 datapoints
2025-03-06 20:31:36,814 - INFO - training batch 801, loss: 0.171, 25632/60000 datapoints
2025-03-06 20:31:37,010 - INFO - training batch 851, loss: 0.232, 27232/60000 datapoints
2025-03-06 20:31:37,204 - INFO - training batch 901, loss: 0.531, 28832/60000 datapoints
2025-03-06 20:31:37,400 - INFO - training batch 951, loss: 0.324, 30432/60000 datapoints
2025-03-06 20:31:37,596 - INFO - training batch 1001, loss: 0.196, 32032/60000 datapoints
2025-03-06 20:31:37,808 - INFO - training batch 1051, loss: 0.222, 33632/60000 datapoints
2025-03-06 20:31:38,005 - INFO - training batch 1101, loss: 0.291, 35232/60000 datapoints
2025-03-06 20:31:38,204 - INFO - training batch 1151, loss: 0.193, 36832/60000 datapoints
2025-03-06 20:31:38,401 - INFO - training batch 1201, loss: 0.204, 38432/60000 datapoints
2025-03-06 20:31:38,597 - INFO - training batch 1251, loss: 0.353, 40032/60000 datapoints
2025-03-06 20:31:38,795 - INFO - training batch 1301, loss: 0.190, 41632/60000 datapoints
2025-03-06 20:31:38,992 - INFO - training batch 1351, loss: 0.126, 43232/60000 datapoints
2025-03-06 20:31:39,188 - INFO - training batch 1401, loss: 0.055, 44832/60000 datapoints
2025-03-06 20:31:39,382 - INFO - training batch 1451, loss: 0.196, 46432/60000 datapoints
2025-03-06 20:31:39,580 - INFO - training batch 1501, loss: 0.243, 48032/60000 datapoints
2025-03-06 20:31:39,777 - INFO - training batch 1551, loss: 0.112, 49632/60000 datapoints
2025-03-06 20:31:39,973 - INFO - training batch 1601, loss: 0.174, 51232/60000 datapoints
2025-03-06 20:31:40,169 - INFO - training batch 1651, loss: 0.113, 52832/60000 datapoints
2025-03-06 20:31:40,376 - INFO - training batch 1701, loss: 0.175, 54432/60000 datapoints
2025-03-06 20:31:40,590 - INFO - training batch 1751, loss: 0.209, 56032/60000 datapoints
2025-03-06 20:31:40,787 - INFO - training batch 1801, loss: 0.183, 57632/60000 datapoints
2025-03-06 20:31:40,982 - INFO - training batch 1851, loss: 0.089, 59232/60000 datapoints
2025-03-06 20:31:41,085 - INFO - validation batch 1, loss: 0.189, 32/10016 datapoints
2025-03-06 20:31:41,238 - INFO - validation batch 51, loss: 0.151, 1632/10016 datapoints
2025-03-06 20:31:41,392 - INFO - validation batch 101, loss: 0.140, 3232/10016 datapoints
2025-03-06 20:31:41,552 - INFO - validation batch 151, loss: 0.160, 4832/10016 datapoints
2025-03-06 20:31:41,709 - INFO - validation batch 201, loss: 0.221, 6432/10016 datapoints
2025-03-06 20:31:41,863 - INFO - validation batch 251, loss: 0.081, 8032/10016 datapoints
2025-03-06 20:31:42,017 - INFO - validation batch 301, loss: 0.192, 9632/10016 datapoints
2025-03-06 20:31:42,056 - INFO - Epoch 758/800 done.
2025-03-06 20:31:42,057 - INFO - Final validation performance:
Loss: 0.162, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:31:42,057 - INFO - Beginning epoch 759/800
2025-03-06 20:31:42,064 - INFO - training batch 1, loss: 0.132, 32/60000 datapoints
2025-03-06 20:31:42,276 - INFO - training batch 51, loss: 0.240, 1632/60000 datapoints
2025-03-06 20:31:42,489 - INFO - training batch 101, loss: 0.053, 3232/60000 datapoints
2025-03-06 20:31:42,720 - INFO - training batch 151, loss: 0.112, 4832/60000 datapoints
2025-03-06 20:31:42,915 - INFO - training batch 201, loss: 0.208, 6432/60000 datapoints
2025-03-06 20:31:43,113 - INFO - training batch 251, loss: 0.045, 8032/60000 datapoints
2025-03-06 20:31:43,310 - INFO - training batch 301, loss: 0.234, 9632/60000 datapoints
2025-03-06 20:31:43,504 - INFO - training batch 351, loss: 0.102, 11232/60000 datapoints
2025-03-06 20:31:43,702 - INFO - training batch 401, loss: 0.288, 12832/60000 datapoints
2025-03-06 20:31:43,897 - INFO - training batch 451, loss: 0.139, 14432/60000 datapoints
2025-03-06 20:31:44,097 - INFO - training batch 501, loss: 0.140, 16032/60000 datapoints
2025-03-06 20:31:44,294 - INFO - training batch 551, loss: 0.214, 17632/60000 datapoints
2025-03-06 20:31:44,491 - INFO - training batch 601, loss: 0.255, 19232/60000 datapoints
2025-03-06 20:31:44,690 - INFO - training batch 651, loss: 0.127, 20832/60000 datapoints
2025-03-06 20:31:44,885 - INFO - training batch 701, loss: 0.216, 22432/60000 datapoints
2025-03-06 20:31:45,081 - INFO - training batch 751, loss: 0.152, 24032/60000 datapoints
2025-03-06 20:31:45,278 - INFO - training batch 801, loss: 0.255, 25632/60000 datapoints
2025-03-06 20:31:45,473 - INFO - training batch 851, loss: 0.341, 27232/60000 datapoints
2025-03-06 20:31:45,671 - INFO - training batch 901, loss: 0.118, 28832/60000 datapoints
2025-03-06 20:31:45,863 - INFO - training batch 951, loss: 0.237, 30432/60000 datapoints
2025-03-06 20:31:46,057 - INFO - training batch 1001, loss: 0.171, 32032/60000 datapoints
2025-03-06 20:31:46,254 - INFO - training batch 1051, loss: 0.110, 33632/60000 datapoints
2025-03-06 20:31:46,454 - INFO - training batch 1101, loss: 0.390, 35232/60000 datapoints
2025-03-06 20:31:46,687 - INFO - training batch 1151, loss: 0.271, 36832/60000 datapoints
2025-03-06 20:31:46,914 - INFO - training batch 1201, loss: 0.248, 38432/60000 datapoints
2025-03-06 20:31:47,124 - INFO - training batch 1251, loss: 0.149, 40032/60000 datapoints
2025-03-06 20:31:47,334 - INFO - training batch 1301, loss: 0.362, 41632/60000 datapoints
2025-03-06 20:31:47,555 - INFO - training batch 1351, loss: 0.353, 43232/60000 datapoints
2025-03-06 20:31:47,755 - INFO - training batch 1401, loss: 0.256, 44832/60000 datapoints
2025-03-06 20:31:47,952 - INFO - training batch 1451, loss: 0.224, 46432/60000 datapoints
2025-03-06 20:31:48,150 - INFO - training batch 1501, loss: 0.250, 48032/60000 datapoints
2025-03-06 20:31:48,348 - INFO - training batch 1551, loss: 0.147, 49632/60000 datapoints
2025-03-06 20:31:48,546 - INFO - training batch 1601, loss: 0.204, 51232/60000 datapoints
2025-03-06 20:31:48,745 - INFO - training batch 1651, loss: 0.503, 52832/60000 datapoints
2025-03-06 20:31:48,938 - INFO - training batch 1701, loss: 0.086, 54432/60000 datapoints
2025-03-06 20:31:49,134 - INFO - training batch 1751, loss: 0.190, 56032/60000 datapoints
2025-03-06 20:31:49,332 - INFO - training batch 1801, loss: 0.352, 57632/60000 datapoints
2025-03-06 20:31:49,533 - INFO - training batch 1851, loss: 0.365, 59232/60000 datapoints
2025-03-06 20:31:49,641 - INFO - validation batch 1, loss: 0.233, 32/10016 datapoints
2025-03-06 20:31:49,794 - INFO - validation batch 51, loss: 0.404, 1632/10016 datapoints
2025-03-06 20:31:49,947 - INFO - validation batch 101, loss: 0.111, 3232/10016 datapoints
2025-03-06 20:31:50,102 - INFO - validation batch 151, loss: 0.224, 4832/10016 datapoints
2025-03-06 20:31:50,256 - INFO - validation batch 201, loss: 0.239, 6432/10016 datapoints
2025-03-06 20:31:50,412 - INFO - validation batch 251, loss: 0.290, 8032/10016 datapoints
2025-03-06 20:31:50,578 - INFO - validation batch 301, loss: 0.115, 9632/10016 datapoints
2025-03-06 20:31:50,626 - INFO - Epoch 759/800 done.
2025-03-06 20:31:50,626 - INFO - Final validation performance:
Loss: 0.231, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:31:50,629 - INFO - Beginning epoch 760/800
2025-03-06 20:31:50,638 - INFO - training batch 1, loss: 0.280, 32/60000 datapoints
2025-03-06 20:31:50,853 - INFO - training batch 51, loss: 0.127, 1632/60000 datapoints
2025-03-06 20:31:51,048 - INFO - training batch 101, loss: 0.182, 3232/60000 datapoints
2025-03-06 20:31:51,255 - INFO - training batch 151, loss: 0.116, 4832/60000 datapoints
2025-03-06 20:31:51,456 - INFO - training batch 201, loss: 0.221, 6432/60000 datapoints
2025-03-06 20:31:51,657 - INFO - training batch 251, loss: 0.087, 8032/60000 datapoints
2025-03-06 20:31:51,851 - INFO - training batch 301, loss: 0.252, 9632/60000 datapoints
2025-03-06 20:31:52,043 - INFO - training batch 351, loss: 0.161, 11232/60000 datapoints
2025-03-06 20:31:52,242 - INFO - training batch 401, loss: 0.319, 12832/60000 datapoints
2025-03-06 20:31:52,438 - INFO - training batch 451, loss: 0.359, 14432/60000 datapoints
2025-03-06 20:31:52,637 - INFO - training batch 501, loss: 0.378, 16032/60000 datapoints
2025-03-06 20:31:52,830 - INFO - training batch 551, loss: 0.313, 17632/60000 datapoints
2025-03-06 20:31:53,027 - INFO - training batch 601, loss: 0.153, 19232/60000 datapoints
2025-03-06 20:31:53,224 - INFO - training batch 651, loss: 0.182, 20832/60000 datapoints
2025-03-06 20:31:53,421 - INFO - training batch 701, loss: 0.294, 22432/60000 datapoints
2025-03-06 20:31:53,619 - INFO - training batch 751, loss: 0.216, 24032/60000 datapoints
2025-03-06 20:31:53,815 - INFO - training batch 801, loss: 0.367, 25632/60000 datapoints
2025-03-06 20:31:54,013 - INFO - training batch 851, loss: 0.072, 27232/60000 datapoints
2025-03-06 20:31:54,209 - INFO - training batch 901, loss: 0.182, 28832/60000 datapoints
2025-03-06 20:31:54,408 - INFO - training batch 951, loss: 0.139, 30432/60000 datapoints
2025-03-06 20:31:54,605 - INFO - training batch 1001, loss: 0.288, 32032/60000 datapoints
2025-03-06 20:31:54,802 - INFO - training batch 1051, loss: 0.244, 33632/60000 datapoints
2025-03-06 20:31:55,003 - INFO - training batch 1101, loss: 0.145, 35232/60000 datapoints
2025-03-06 20:31:55,200 - INFO - training batch 1151, loss: 0.152, 36832/60000 datapoints
2025-03-06 20:31:55,394 - INFO - training batch 1201, loss: 0.176, 38432/60000 datapoints
2025-03-06 20:31:55,588 - INFO - training batch 1251, loss: 0.216, 40032/60000 datapoints
2025-03-06 20:31:55,790 - INFO - training batch 1301, loss: 0.161, 41632/60000 datapoints
2025-03-06 20:31:55,984 - INFO - training batch 1351, loss: 0.049, 43232/60000 datapoints
2025-03-06 20:31:56,180 - INFO - training batch 1401, loss: 0.025, 44832/60000 datapoints
2025-03-06 20:31:56,377 - INFO - training batch 1451, loss: 0.095, 46432/60000 datapoints
2025-03-06 20:31:56,573 - INFO - training batch 1501, loss: 0.077, 48032/60000 datapoints
2025-03-06 20:31:56,771 - INFO - training batch 1551, loss: 0.152, 49632/60000 datapoints
2025-03-06 20:31:56,972 - INFO - training batch 1601, loss: 0.214, 51232/60000 datapoints
2025-03-06 20:31:57,166 - INFO - training batch 1651, loss: 0.164, 52832/60000 datapoints
2025-03-06 20:31:57,361 - INFO - training batch 1701, loss: 0.067, 54432/60000 datapoints
2025-03-06 20:31:57,557 - INFO - training batch 1751, loss: 0.173, 56032/60000 datapoints
2025-03-06 20:31:57,757 - INFO - training batch 1801, loss: 0.220, 57632/60000 datapoints
2025-03-06 20:31:57,952 - INFO - training batch 1851, loss: 0.346, 59232/60000 datapoints
2025-03-06 20:31:58,053 - INFO - validation batch 1, loss: 0.207, 32/10016 datapoints
2025-03-06 20:31:58,206 - INFO - validation batch 51, loss: 0.278, 1632/10016 datapoints
2025-03-06 20:31:58,358 - INFO - validation batch 101, loss: 0.129, 3232/10016 datapoints
2025-03-06 20:31:58,513 - INFO - validation batch 151, loss: 0.527, 4832/10016 datapoints
2025-03-06 20:31:58,672 - INFO - validation batch 201, loss: 0.215, 6432/10016 datapoints
2025-03-06 20:31:58,825 - INFO - validation batch 251, loss: 0.468, 8032/10016 datapoints
2025-03-06 20:31:58,980 - INFO - validation batch 301, loss: 0.093, 9632/10016 datapoints
2025-03-06 20:31:59,016 - INFO - Epoch 760/800 done.
2025-03-06 20:31:59,016 - INFO - Final validation performance:
Loss: 0.274, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:31:59,017 - INFO - Beginning epoch 761/800
2025-03-06 20:31:59,024 - INFO - training batch 1, loss: 0.097, 32/60000 datapoints
2025-03-06 20:31:59,242 - INFO - training batch 51, loss: 0.066, 1632/60000 datapoints
2025-03-06 20:31:59,443 - INFO - training batch 101, loss: 0.316, 3232/60000 datapoints
2025-03-06 20:31:59,649 - INFO - training batch 151, loss: 0.079, 4832/60000 datapoints
2025-03-06 20:31:59,845 - INFO - training batch 201, loss: 0.192, 6432/60000 datapoints
2025-03-06 20:32:00,044 - INFO - training batch 251, loss: 0.037, 8032/60000 datapoints
2025-03-06 20:32:00,239 - INFO - training batch 301, loss: 0.249, 9632/60000 datapoints
2025-03-06 20:32:00,438 - INFO - training batch 351, loss: 0.331, 11232/60000 datapoints
2025-03-06 20:32:00,635 - INFO - training batch 401, loss: 0.253, 12832/60000 datapoints
2025-03-06 20:32:00,849 - INFO - training batch 451, loss: 0.278, 14432/60000 datapoints
2025-03-06 20:32:01,047 - INFO - training batch 501, loss: 0.525, 16032/60000 datapoints
2025-03-06 20:32:01,241 - INFO - training batch 551, loss: 0.568, 17632/60000 datapoints
2025-03-06 20:32:01,439 - INFO - training batch 601, loss: 0.514, 19232/60000 datapoints
2025-03-06 20:32:01,636 - INFO - training batch 651, loss: 0.138, 20832/60000 datapoints
2025-03-06 20:32:01,832 - INFO - training batch 701, loss: 0.308, 22432/60000 datapoints
2025-03-06 20:32:02,028 - INFO - training batch 751, loss: 0.319, 24032/60000 datapoints
2025-03-06 20:32:02,224 - INFO - training batch 801, loss: 0.216, 25632/60000 datapoints
2025-03-06 20:32:02,422 - INFO - training batch 851, loss: 0.175, 27232/60000 datapoints
2025-03-06 20:32:02,615 - INFO - training batch 901, loss: 0.105, 28832/60000 datapoints
2025-03-06 20:32:02,814 - INFO - training batch 951, loss: 0.334, 30432/60000 datapoints
2025-03-06 20:32:03,011 - INFO - training batch 1001, loss: 0.284, 32032/60000 datapoints
2025-03-06 20:32:03,205 - INFO - training batch 1051, loss: 0.321, 33632/60000 datapoints
2025-03-06 20:32:03,400 - INFO - training batch 1101, loss: 0.215, 35232/60000 datapoints
2025-03-06 20:32:03,595 - INFO - training batch 1151, loss: 0.259, 36832/60000 datapoints
2025-03-06 20:32:03,792 - INFO - training batch 1201, loss: 0.283, 38432/60000 datapoints
2025-03-06 20:32:03,986 - INFO - training batch 1251, loss: 0.056, 40032/60000 datapoints
2025-03-06 20:32:04,183 - INFO - training batch 1301, loss: 0.159, 41632/60000 datapoints
2025-03-06 20:32:04,379 - INFO - training batch 1351, loss: 0.118, 43232/60000 datapoints
2025-03-06 20:32:04,579 - INFO - training batch 1401, loss: 0.160, 44832/60000 datapoints
2025-03-06 20:32:04,775 - INFO - training batch 1451, loss: 0.076, 46432/60000 datapoints
2025-03-06 20:32:04,977 - INFO - training batch 1501, loss: 0.181, 48032/60000 datapoints
2025-03-06 20:32:05,172 - INFO - training batch 1551, loss: 0.112, 49632/60000 datapoints
2025-03-06 20:32:05,369 - INFO - training batch 1601, loss: 0.544, 51232/60000 datapoints
2025-03-06 20:32:05,562 - INFO - training batch 1651, loss: 0.126, 52832/60000 datapoints
2025-03-06 20:32:05,760 - INFO - training batch 1701, loss: 0.218, 54432/60000 datapoints
2025-03-06 20:32:05,957 - INFO - training batch 1751, loss: 0.149, 56032/60000 datapoints
2025-03-06 20:32:06,152 - INFO - training batch 1801, loss: 0.071, 57632/60000 datapoints
2025-03-06 20:32:06,349 - INFO - training batch 1851, loss: 0.138, 59232/60000 datapoints
2025-03-06 20:32:06,455 - INFO - validation batch 1, loss: 0.278, 32/10016 datapoints
2025-03-06 20:32:06,607 - INFO - validation batch 51, loss: 0.347, 1632/10016 datapoints
2025-03-06 20:32:06,762 - INFO - validation batch 101, loss: 0.187, 3232/10016 datapoints
2025-03-06 20:32:06,919 - INFO - validation batch 151, loss: 0.065, 4832/10016 datapoints
2025-03-06 20:32:07,073 - INFO - validation batch 201, loss: 0.399, 6432/10016 datapoints
2025-03-06 20:32:07,224 - INFO - validation batch 251, loss: 0.436, 8032/10016 datapoints
2025-03-06 20:32:07,381 - INFO - validation batch 301, loss: 0.166, 9632/10016 datapoints
2025-03-06 20:32:07,417 - INFO - Epoch 761/800 done.
2025-03-06 20:32:07,418 - INFO - Final validation performance:
Loss: 0.268, top-1 acc: 0.937top-5 acc: 0.937
2025-03-06 20:32:07,418 - INFO - Beginning epoch 762/800
2025-03-06 20:32:07,426 - INFO - training batch 1, loss: 0.046, 32/60000 datapoints
2025-03-06 20:32:07,649 - INFO - training batch 51, loss: 0.130, 1632/60000 datapoints
2025-03-06 20:32:07,852 - INFO - training batch 101, loss: 0.110, 3232/60000 datapoints
2025-03-06 20:32:08,062 - INFO - training batch 151, loss: 0.168, 4832/60000 datapoints
2025-03-06 20:32:08,269 - INFO - training batch 201, loss: 0.053, 6432/60000 datapoints
2025-03-06 20:32:08,479 - INFO - training batch 251, loss: 0.242, 8032/60000 datapoints
2025-03-06 20:32:08,682 - INFO - training batch 301, loss: 0.339, 9632/60000 datapoints
2025-03-06 20:32:08,889 - INFO - training batch 351, loss: 0.322, 11232/60000 datapoints
2025-03-06 20:32:09,109 - INFO - training batch 401, loss: 0.110, 12832/60000 datapoints
2025-03-06 20:32:09,316 - INFO - training batch 451, loss: 0.321, 14432/60000 datapoints
2025-03-06 20:32:09,515 - INFO - training batch 501, loss: 0.351, 16032/60000 datapoints
2025-03-06 20:32:09,719 - INFO - training batch 551, loss: 0.128, 17632/60000 datapoints
2025-03-06 20:32:09,920 - INFO - training batch 601, loss: 0.383, 19232/60000 datapoints
2025-03-06 20:32:10,124 - INFO - training batch 651, loss: 0.198, 20832/60000 datapoints
2025-03-06 20:32:10,332 - INFO - training batch 701, loss: 0.297, 22432/60000 datapoints
2025-03-06 20:32:10,531 - INFO - training batch 751, loss: 0.046, 24032/60000 datapoints
2025-03-06 20:32:10,737 - INFO - training batch 801, loss: 0.224, 25632/60000 datapoints
2025-03-06 20:32:10,958 - INFO - training batch 851, loss: 0.253, 27232/60000 datapoints
2025-03-06 20:32:11,159 - INFO - training batch 901, loss: 0.074, 28832/60000 datapoints
2025-03-06 20:32:11,360 - INFO - training batch 951, loss: 0.137, 30432/60000 datapoints
2025-03-06 20:32:11,563 - INFO - training batch 1001, loss: 0.116, 32032/60000 datapoints
2025-03-06 20:32:11,766 - INFO - training batch 1051, loss: 0.087, 33632/60000 datapoints
2025-03-06 20:32:11,966 - INFO - training batch 1101, loss: 0.329, 35232/60000 datapoints
2025-03-06 20:32:12,170 - INFO - training batch 1151, loss: 0.100, 36832/60000 datapoints
2025-03-06 20:32:12,375 - INFO - training batch 1201, loss: 0.116, 38432/60000 datapoints
2025-03-06 20:32:12,583 - INFO - training batch 1251, loss: 0.184, 40032/60000 datapoints
2025-03-06 20:32:12,784 - INFO - training batch 1301, loss: 0.148, 41632/60000 datapoints
2025-03-06 20:32:12,982 - INFO - training batch 1351, loss: 0.133, 43232/60000 datapoints
2025-03-06 20:32:13,177 - INFO - training batch 1401, loss: 0.416, 44832/60000 datapoints
2025-03-06 20:32:13,373 - INFO - training batch 1451, loss: 0.266, 46432/60000 datapoints
2025-03-06 20:32:13,570 - INFO - training batch 1501, loss: 0.096, 48032/60000 datapoints
2025-03-06 20:32:13,767 - INFO - training batch 1551, loss: 0.340, 49632/60000 datapoints
2025-03-06 20:32:13,961 - INFO - training batch 1601, loss: 0.111, 51232/60000 datapoints
2025-03-06 20:32:14,155 - INFO - training batch 1651, loss: 0.279, 52832/60000 datapoints
2025-03-06 20:32:14,349 - INFO - training batch 1701, loss: 0.382, 54432/60000 datapoints
2025-03-06 20:32:14,549 - INFO - training batch 1751, loss: 0.079, 56032/60000 datapoints
2025-03-06 20:32:14,747 - INFO - training batch 1801, loss: 0.243, 57632/60000 datapoints
2025-03-06 20:32:14,948 - INFO - training batch 1851, loss: 0.437, 59232/60000 datapoints
2025-03-06 20:32:15,050 - INFO - validation batch 1, loss: 0.077, 32/10016 datapoints
2025-03-06 20:32:15,204 - INFO - validation batch 51, loss: 0.225, 1632/10016 datapoints
2025-03-06 20:32:15,360 - INFO - validation batch 101, loss: 0.077, 3232/10016 datapoints
2025-03-06 20:32:15,516 - INFO - validation batch 151, loss: 0.231, 4832/10016 datapoints
2025-03-06 20:32:15,672 - INFO - validation batch 201, loss: 0.107, 6432/10016 datapoints
2025-03-06 20:32:15,825 - INFO - validation batch 251, loss: 0.086, 8032/10016 datapoints
2025-03-06 20:32:15,980 - INFO - validation batch 301, loss: 0.069, 9632/10016 datapoints
2025-03-06 20:32:16,017 - INFO - Epoch 762/800 done.
2025-03-06 20:32:16,017 - INFO - Final validation performance:
Loss: 0.124, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:32:16,018 - INFO - Beginning epoch 763/800
2025-03-06 20:32:16,025 - INFO - training batch 1, loss: 0.104, 32/60000 datapoints
2025-03-06 20:32:16,223 - INFO - training batch 51, loss: 0.240, 1632/60000 datapoints
2025-03-06 20:32:16,420 - INFO - training batch 101, loss: 0.217, 3232/60000 datapoints
2025-03-06 20:32:16,631 - INFO - training batch 151, loss: 0.311, 4832/60000 datapoints
2025-03-06 20:32:16,831 - INFO - training batch 201, loss: 0.388, 6432/60000 datapoints
2025-03-06 20:32:17,030 - INFO - training batch 251, loss: 0.288, 8032/60000 datapoints
2025-03-06 20:32:17,229 - INFO - training batch 301, loss: 0.338, 9632/60000 datapoints
2025-03-06 20:32:17,427 - INFO - training batch 351, loss: 0.354, 11232/60000 datapoints
2025-03-06 20:32:17,625 - INFO - training batch 401, loss: 0.071, 12832/60000 datapoints
2025-03-06 20:32:17,824 - INFO - training batch 451, loss: 0.140, 14432/60000 datapoints
2025-03-06 20:32:18,027 - INFO - training batch 501, loss: 0.065, 16032/60000 datapoints
2025-03-06 20:32:18,222 - INFO - training batch 551, loss: 0.078, 17632/60000 datapoints
2025-03-06 20:32:18,417 - INFO - training batch 601, loss: 0.071, 19232/60000 datapoints
2025-03-06 20:32:18,613 - INFO - training batch 651, loss: 0.163, 20832/60000 datapoints
2025-03-06 20:32:18,810 - INFO - training batch 701, loss: 0.057, 22432/60000 datapoints
2025-03-06 20:32:19,007 - INFO - training batch 751, loss: 0.253, 24032/60000 datapoints
2025-03-06 20:32:19,208 - INFO - training batch 801, loss: 0.150, 25632/60000 datapoints
2025-03-06 20:32:19,408 - INFO - training batch 851, loss: 0.186, 27232/60000 datapoints
2025-03-06 20:32:19,604 - INFO - training batch 901, loss: 0.130, 28832/60000 datapoints
2025-03-06 20:32:19,804 - INFO - training batch 951, loss: 0.235, 30432/60000 datapoints
2025-03-06 20:32:20,000 - INFO - training batch 1001, loss: 0.164, 32032/60000 datapoints
2025-03-06 20:32:20,197 - INFO - training batch 1051, loss: 0.114, 33632/60000 datapoints
2025-03-06 20:32:20,393 - INFO - training batch 1101, loss: 0.676, 35232/60000 datapoints
2025-03-06 20:32:20,591 - INFO - training batch 1151, loss: 0.577, 36832/60000 datapoints
2025-03-06 20:32:20,788 - INFO - training batch 1201, loss: 0.458, 38432/60000 datapoints
2025-03-06 20:32:21,006 - INFO - training batch 1251, loss: 0.213, 40032/60000 datapoints
2025-03-06 20:32:21,205 - INFO - training batch 1301, loss: 0.228, 41632/60000 datapoints
2025-03-06 20:32:21,400 - INFO - training batch 1351, loss: 0.206, 43232/60000 datapoints
2025-03-06 20:32:21,594 - INFO - training batch 1401, loss: 0.133, 44832/60000 datapoints
2025-03-06 20:32:21,791 - INFO - training batch 1451, loss: 0.148, 46432/60000 datapoints
2025-03-06 20:32:21,986 - INFO - training batch 1501, loss: 0.227, 48032/60000 datapoints
2025-03-06 20:32:22,179 - INFO - training batch 1551, loss: 0.332, 49632/60000 datapoints
2025-03-06 20:32:22,375 - INFO - training batch 1601, loss: 0.485, 51232/60000 datapoints
2025-03-06 20:32:22,571 - INFO - training batch 1651, loss: 0.542, 52832/60000 datapoints
2025-03-06 20:32:22,770 - INFO - training batch 1701, loss: 0.262, 54432/60000 datapoints
2025-03-06 20:32:22,964 - INFO - training batch 1751, loss: 0.146, 56032/60000 datapoints
2025-03-06 20:32:23,160 - INFO - training batch 1801, loss: 0.157, 57632/60000 datapoints
2025-03-06 20:32:23,356 - INFO - training batch 1851, loss: 0.499, 59232/60000 datapoints
2025-03-06 20:32:23,457 - INFO - validation batch 1, loss: 0.135, 32/10016 datapoints
2025-03-06 20:32:23,611 - INFO - validation batch 51, loss: 0.453, 1632/10016 datapoints
2025-03-06 20:32:23,766 - INFO - validation batch 101, loss: 0.266, 3232/10016 datapoints
2025-03-06 20:32:23,918 - INFO - validation batch 151, loss: 0.362, 4832/10016 datapoints
2025-03-06 20:32:24,073 - INFO - validation batch 201, loss: 0.312, 6432/10016 datapoints
2025-03-06 20:32:24,225 - INFO - validation batch 251, loss: 0.100, 8032/10016 datapoints
2025-03-06 20:32:24,380 - INFO - validation batch 301, loss: 0.146, 9632/10016 datapoints
2025-03-06 20:32:24,417 - INFO - Epoch 763/800 done.
2025-03-06 20:32:24,418 - INFO - Final validation performance:
Loss: 0.254, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:32:24,419 - INFO - Beginning epoch 764/800
2025-03-06 20:32:24,426 - INFO - training batch 1, loss: 0.345, 32/60000 datapoints
2025-03-06 20:32:24,654 - INFO - training batch 51, loss: 0.086, 1632/60000 datapoints
2025-03-06 20:32:24,850 - INFO - training batch 101, loss: 0.154, 3232/60000 datapoints
2025-03-06 20:32:25,059 - INFO - training batch 151, loss: 0.308, 4832/60000 datapoints
2025-03-06 20:32:25,257 - INFO - training batch 201, loss: 0.333, 6432/60000 datapoints
2025-03-06 20:32:25,454 - INFO - training batch 251, loss: 0.240, 8032/60000 datapoints
2025-03-06 20:32:25,652 - INFO - training batch 301, loss: 0.271, 9632/60000 datapoints
2025-03-06 20:32:25,846 - INFO - training batch 351, loss: 0.173, 11232/60000 datapoints
2025-03-06 20:32:26,042 - INFO - training batch 401, loss: 0.146, 12832/60000 datapoints
2025-03-06 20:32:26,234 - INFO - training batch 451, loss: 0.232, 14432/60000 datapoints
2025-03-06 20:32:26,428 - INFO - training batch 501, loss: 0.238, 16032/60000 datapoints
2025-03-06 20:32:26,626 - INFO - training batch 551, loss: 0.124, 17632/60000 datapoints
2025-03-06 20:32:26,823 - INFO - training batch 601, loss: 0.419, 19232/60000 datapoints
2025-03-06 20:32:27,023 - INFO - training batch 651, loss: 0.375, 20832/60000 datapoints
2025-03-06 20:32:27,218 - INFO - training batch 701, loss: 0.185, 22432/60000 datapoints
2025-03-06 20:32:27,413 - INFO - training batch 751, loss: 0.244, 24032/60000 datapoints
2025-03-06 20:32:27,607 - INFO - training batch 801, loss: 0.201, 25632/60000 datapoints
2025-03-06 20:32:27,806 - INFO - training batch 851, loss: 0.342, 27232/60000 datapoints
2025-03-06 20:32:28,000 - INFO - training batch 901, loss: 0.161, 28832/60000 datapoints
2025-03-06 20:32:28,196 - INFO - training batch 951, loss: 0.310, 30432/60000 datapoints
2025-03-06 20:32:28,389 - INFO - training batch 1001, loss: 0.203, 32032/60000 datapoints
2025-03-06 20:32:28,586 - INFO - training batch 1051, loss: 0.195, 33632/60000 datapoints
2025-03-06 20:32:28,783 - INFO - training batch 1101, loss: 0.088, 35232/60000 datapoints
2025-03-06 20:32:28,978 - INFO - training batch 1151, loss: 0.154, 36832/60000 datapoints
2025-03-06 20:32:29,174 - INFO - training batch 1201, loss: 0.110, 38432/60000 datapoints
2025-03-06 20:32:29,370 - INFO - training batch 1251, loss: 0.195, 40032/60000 datapoints
2025-03-06 20:32:29,565 - INFO - training batch 1301, loss: 0.051, 41632/60000 datapoints
2025-03-06 20:32:29,767 - INFO - training batch 1351, loss: 0.037, 43232/60000 datapoints
2025-03-06 20:32:29,961 - INFO - training batch 1401, loss: 0.126, 44832/60000 datapoints
2025-03-06 20:32:30,159 - INFO - training batch 1451, loss: 0.230, 46432/60000 datapoints
2025-03-06 20:32:30,354 - INFO - training batch 1501, loss: 0.232, 48032/60000 datapoints
2025-03-06 20:32:30,551 - INFO - training batch 1551, loss: 0.195, 49632/60000 datapoints
2025-03-06 20:32:30,748 - INFO - training batch 1601, loss: 0.252, 51232/60000 datapoints
2025-03-06 20:32:30,950 - INFO - training batch 1651, loss: 0.547, 52832/60000 datapoints
2025-03-06 20:32:31,159 - INFO - training batch 1701, loss: 0.395, 54432/60000 datapoints
2025-03-06 20:32:31,354 - INFO - training batch 1751, loss: 0.056, 56032/60000 datapoints
2025-03-06 20:32:31,548 - INFO - training batch 1801, loss: 0.032, 57632/60000 datapoints
2025-03-06 20:32:31,748 - INFO - training batch 1851, loss: 0.394, 59232/60000 datapoints
2025-03-06 20:32:31,849 - INFO - validation batch 1, loss: 0.098, 32/10016 datapoints
2025-03-06 20:32:32,003 - INFO - validation batch 51, loss: 0.126, 1632/10016 datapoints
2025-03-06 20:32:32,159 - INFO - validation batch 101, loss: 0.191, 3232/10016 datapoints
2025-03-06 20:32:32,312 - INFO - validation batch 151, loss: 0.181, 4832/10016 datapoints
2025-03-06 20:32:32,465 - INFO - validation batch 201, loss: 0.208, 6432/10016 datapoints
2025-03-06 20:32:32,621 - INFO - validation batch 251, loss: 0.130, 8032/10016 datapoints
2025-03-06 20:32:32,776 - INFO - validation batch 301, loss: 0.158, 9632/10016 datapoints
2025-03-06 20:32:32,812 - INFO - Epoch 764/800 done.
2025-03-06 20:32:32,812 - INFO - Final validation performance:
Loss: 0.156, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:32:32,813 - INFO - Beginning epoch 765/800
2025-03-06 20:32:32,820 - INFO - training batch 1, loss: 0.262, 32/60000 datapoints
2025-03-06 20:32:33,034 - INFO - training batch 51, loss: 0.308, 1632/60000 datapoints
2025-03-06 20:32:33,231 - INFO - training batch 101, loss: 0.149, 3232/60000 datapoints
2025-03-06 20:32:33,431 - INFO - training batch 151, loss: 0.176, 4832/60000 datapoints
2025-03-06 20:32:33,628 - INFO - training batch 201, loss: 0.244, 6432/60000 datapoints
2025-03-06 20:32:33,827 - INFO - training batch 251, loss: 0.122, 8032/60000 datapoints
2025-03-06 20:32:34,020 - INFO - training batch 301, loss: 0.086, 9632/60000 datapoints
2025-03-06 20:32:34,217 - INFO - training batch 351, loss: 0.438, 11232/60000 datapoints
2025-03-06 20:32:34,410 - INFO - training batch 401, loss: 0.147, 12832/60000 datapoints
2025-03-06 20:32:34,607 - INFO - training batch 451, loss: 0.160, 14432/60000 datapoints
2025-03-06 20:32:34,805 - INFO - training batch 501, loss: 0.135, 16032/60000 datapoints
2025-03-06 20:32:35,005 - INFO - training batch 551, loss: 0.195, 17632/60000 datapoints
2025-03-06 20:32:35,212 - INFO - training batch 601, loss: 0.290, 19232/60000 datapoints
2025-03-06 20:32:35,408 - INFO - training batch 651, loss: 0.176, 20832/60000 datapoints
2025-03-06 20:32:35,602 - INFO - training batch 701, loss: 0.169, 22432/60000 datapoints
2025-03-06 20:32:35,801 - INFO - training batch 751, loss: 0.179, 24032/60000 datapoints
2025-03-06 20:32:35,999 - INFO - training batch 801, loss: 0.106, 25632/60000 datapoints
2025-03-06 20:32:36,195 - INFO - training batch 851, loss: 0.327, 27232/60000 datapoints
2025-03-06 20:32:36,389 - INFO - training batch 901, loss: 0.104, 28832/60000 datapoints
2025-03-06 20:32:36,590 - INFO - training batch 951, loss: 0.698, 30432/60000 datapoints
2025-03-06 20:32:36,787 - INFO - training batch 1001, loss: 0.137, 32032/60000 datapoints
2025-03-06 20:32:36,982 - INFO - training batch 1051, loss: 0.234, 33632/60000 datapoints
2025-03-06 20:32:37,184 - INFO - training batch 1101, loss: 0.098, 35232/60000 datapoints
2025-03-06 20:32:37,378 - INFO - training batch 1151, loss: 0.092, 36832/60000 datapoints
2025-03-06 20:32:37,574 - INFO - training batch 1201, loss: 0.187, 38432/60000 datapoints
2025-03-06 20:32:37,792 - INFO - training batch 1251, loss: 0.272, 40032/60000 datapoints
2025-03-06 20:32:37,987 - INFO - training batch 1301, loss: 0.295, 41632/60000 datapoints
2025-03-06 20:32:38,185 - INFO - training batch 1351, loss: 0.242, 43232/60000 datapoints
2025-03-06 20:32:38,380 - INFO - training batch 1401, loss: 0.077, 44832/60000 datapoints
2025-03-06 20:32:38,577 - INFO - training batch 1451, loss: 0.151, 46432/60000 datapoints
2025-03-06 20:32:38,775 - INFO - training batch 1501, loss: 0.044, 48032/60000 datapoints
2025-03-06 20:32:38,969 - INFO - training batch 1551, loss: 0.213, 49632/60000 datapoints
2025-03-06 20:32:39,167 - INFO - training batch 1601, loss: 0.213, 51232/60000 datapoints
2025-03-06 20:32:39,360 - INFO - training batch 1651, loss: 0.108, 52832/60000 datapoints
2025-03-06 20:32:39,557 - INFO - training batch 1701, loss: 0.358, 54432/60000 datapoints
2025-03-06 20:32:39,760 - INFO - training batch 1751, loss: 0.203, 56032/60000 datapoints
2025-03-06 20:32:39,954 - INFO - training batch 1801, loss: 0.277, 57632/60000 datapoints
2025-03-06 20:32:40,152 - INFO - training batch 1851, loss: 0.113, 59232/60000 datapoints
2025-03-06 20:32:40,256 - INFO - validation batch 1, loss: 0.178, 32/10016 datapoints
2025-03-06 20:32:40,410 - INFO - validation batch 51, loss: 0.133, 1632/10016 datapoints
2025-03-06 20:32:40,566 - INFO - validation batch 101, loss: 0.137, 3232/10016 datapoints
2025-03-06 20:32:40,726 - INFO - validation batch 151, loss: 0.262, 4832/10016 datapoints
2025-03-06 20:32:40,880 - INFO - validation batch 201, loss: 0.183, 6432/10016 datapoints
2025-03-06 20:32:41,044 - INFO - validation batch 251, loss: 0.386, 8032/10016 datapoints
2025-03-06 20:32:41,205 - INFO - validation batch 301, loss: 0.231, 9632/10016 datapoints
2025-03-06 20:32:41,244 - INFO - Epoch 765/800 done.
2025-03-06 20:32:41,244 - INFO - Final validation performance:
Loss: 0.216, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:32:41,245 - INFO - Beginning epoch 766/800
2025-03-06 20:32:41,251 - INFO - training batch 1, loss: 0.183, 32/60000 datapoints
2025-03-06 20:32:41,463 - INFO - training batch 51, loss: 0.051, 1632/60000 datapoints
2025-03-06 20:32:41,660 - INFO - training batch 101, loss: 0.173, 3232/60000 datapoints
2025-03-06 20:32:41,873 - INFO - training batch 151, loss: 0.197, 4832/60000 datapoints
2025-03-06 20:32:42,072 - INFO - training batch 201, loss: 0.176, 6432/60000 datapoints
2025-03-06 20:32:42,278 - INFO - training batch 251, loss: 0.122, 8032/60000 datapoints
2025-03-06 20:32:42,476 - INFO - training batch 301, loss: 0.171, 9632/60000 datapoints
2025-03-06 20:32:42,675 - INFO - training batch 351, loss: 0.087, 11232/60000 datapoints
2025-03-06 20:32:42,902 - INFO - training batch 401, loss: 0.603, 12832/60000 datapoints
2025-03-06 20:32:43,098 - INFO - training batch 451, loss: 0.196, 14432/60000 datapoints
2025-03-06 20:32:43,294 - INFO - training batch 501, loss: 0.227, 16032/60000 datapoints
2025-03-06 20:32:43,488 - INFO - training batch 551, loss: 0.339, 17632/60000 datapoints
2025-03-06 20:32:43,689 - INFO - training batch 601, loss: 0.216, 19232/60000 datapoints
2025-03-06 20:32:43,886 - INFO - training batch 651, loss: 0.399, 20832/60000 datapoints
2025-03-06 20:32:44,081 - INFO - training batch 701, loss: 0.146, 22432/60000 datapoints
2025-03-06 20:32:44,275 - INFO - training batch 751, loss: 0.160, 24032/60000 datapoints
2025-03-06 20:32:44,470 - INFO - training batch 801, loss: 0.102, 25632/60000 datapoints
2025-03-06 20:32:44,668 - INFO - training batch 851, loss: 0.199, 27232/60000 datapoints
2025-03-06 20:32:44,863 - INFO - training batch 901, loss: 0.101, 28832/60000 datapoints
2025-03-06 20:32:45,066 - INFO - training batch 951, loss: 0.302, 30432/60000 datapoints
2025-03-06 20:32:45,263 - INFO - training batch 1001, loss: 0.103, 32032/60000 datapoints
2025-03-06 20:32:45,461 - INFO - training batch 1051, loss: 0.250, 33632/60000 datapoints
2025-03-06 20:32:45,658 - INFO - training batch 1101, loss: 0.185, 35232/60000 datapoints
2025-03-06 20:32:45,853 - INFO - training batch 1151, loss: 0.327, 36832/60000 datapoints
2025-03-06 20:32:46,046 - INFO - training batch 1201, loss: 0.222, 38432/60000 datapoints
2025-03-06 20:32:46,243 - INFO - training batch 1251, loss: 0.057, 40032/60000 datapoints
2025-03-06 20:32:46,438 - INFO - training batch 1301, loss: 0.084, 41632/60000 datapoints
2025-03-06 20:32:46,638 - INFO - training batch 1351, loss: 0.113, 43232/60000 datapoints
2025-03-06 20:32:46,834 - INFO - training batch 1401, loss: 0.115, 44832/60000 datapoints
2025-03-06 20:32:47,029 - INFO - training batch 1451, loss: 0.376, 46432/60000 datapoints
2025-03-06 20:32:47,226 - INFO - training batch 1501, loss: 0.167, 48032/60000 datapoints
2025-03-06 20:32:47,422 - INFO - training batch 1551, loss: 0.197, 49632/60000 datapoints
2025-03-06 20:32:47,617 - INFO - training batch 1601, loss: 0.088, 51232/60000 datapoints
2025-03-06 20:32:47,817 - INFO - training batch 1651, loss: 0.237, 52832/60000 datapoints
2025-03-06 20:32:48,012 - INFO - training batch 1701, loss: 0.201, 54432/60000 datapoints
2025-03-06 20:32:48,210 - INFO - training batch 1751, loss: 0.104, 56032/60000 datapoints
2025-03-06 20:32:48,406 - INFO - training batch 1801, loss: 0.331, 57632/60000 datapoints
2025-03-06 20:32:48,602 - INFO - training batch 1851, loss: 0.328, 59232/60000 datapoints
2025-03-06 20:32:48,705 - INFO - validation batch 1, loss: 0.351, 32/10016 datapoints
2025-03-06 20:32:48,859 - INFO - validation batch 51, loss: 0.170, 1632/10016 datapoints
2025-03-06 20:32:49,011 - INFO - validation batch 101, loss: 0.891, 3232/10016 datapoints
2025-03-06 20:32:49,163 - INFO - validation batch 151, loss: 0.260, 4832/10016 datapoints
2025-03-06 20:32:49,318 - INFO - validation batch 201, loss: 0.282, 6432/10016 datapoints
2025-03-06 20:32:49,473 - INFO - validation batch 251, loss: 0.400, 8032/10016 datapoints
2025-03-06 20:32:49,626 - INFO - validation batch 301, loss: 0.472, 9632/10016 datapoints
2025-03-06 20:32:49,666 - INFO - Epoch 766/800 done.
2025-03-06 20:32:49,666 - INFO - Final validation performance:
Loss: 0.404, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:32:49,667 - INFO - Beginning epoch 767/800
2025-03-06 20:32:49,674 - INFO - training batch 1, loss: 0.207, 32/60000 datapoints
2025-03-06 20:32:49,890 - INFO - training batch 51, loss: 0.117, 1632/60000 datapoints
2025-03-06 20:32:50,086 - INFO - training batch 101, loss: 0.050, 3232/60000 datapoints
2025-03-06 20:32:50,287 - INFO - training batch 151, loss: 0.300, 4832/60000 datapoints
2025-03-06 20:32:50,487 - INFO - training batch 201, loss: 0.550, 6432/60000 datapoints
2025-03-06 20:32:50,690 - INFO - training batch 251, loss: 0.255, 8032/60000 datapoints
2025-03-06 20:32:50,888 - INFO - training batch 301, loss: 0.105, 9632/60000 datapoints
2025-03-06 20:32:51,085 - INFO - training batch 351, loss: 0.082, 11232/60000 datapoints
2025-03-06 20:32:51,297 - INFO - training batch 401, loss: 0.290, 12832/60000 datapoints
2025-03-06 20:32:51,493 - INFO - training batch 451, loss: 0.101, 14432/60000 datapoints
2025-03-06 20:32:51,689 - INFO - training batch 501, loss: 0.108, 16032/60000 datapoints
2025-03-06 20:32:51,888 - INFO - training batch 551, loss: 0.132, 17632/60000 datapoints
2025-03-06 20:32:52,082 - INFO - training batch 601, loss: 0.285, 19232/60000 datapoints
2025-03-06 20:32:52,275 - INFO - training batch 651, loss: 0.076, 20832/60000 datapoints
2025-03-06 20:32:52,471 - INFO - training batch 701, loss: 0.110, 22432/60000 datapoints
2025-03-06 20:32:52,669 - INFO - training batch 751, loss: 0.091, 24032/60000 datapoints
2025-03-06 20:32:52,866 - INFO - training batch 801, loss: 0.100, 25632/60000 datapoints
2025-03-06 20:32:53,060 - INFO - training batch 851, loss: 0.113, 27232/60000 datapoints
2025-03-06 20:32:53,256 - INFO - training batch 901, loss: 0.346, 28832/60000 datapoints
2025-03-06 20:32:53,453 - INFO - training batch 951, loss: 0.444, 30432/60000 datapoints
2025-03-06 20:32:53,650 - INFO - training batch 1001, loss: 0.250, 32032/60000 datapoints
2025-03-06 20:32:53,846 - INFO - training batch 1051, loss: 0.341, 33632/60000 datapoints
2025-03-06 20:32:54,043 - INFO - training batch 1101, loss: 0.238, 35232/60000 datapoints
2025-03-06 20:32:54,238 - INFO - training batch 1151, loss: 0.479, 36832/60000 datapoints
2025-03-06 20:32:54,433 - INFO - training batch 1201, loss: 0.040, 38432/60000 datapoints
2025-03-06 20:32:54,632 - INFO - training batch 1251, loss: 0.228, 40032/60000 datapoints
2025-03-06 20:32:54,828 - INFO - training batch 1301, loss: 0.185, 41632/60000 datapoints
2025-03-06 20:32:55,029 - INFO - training batch 1351, loss: 0.298, 43232/60000 datapoints
2025-03-06 20:32:55,227 - INFO - training batch 1401, loss: 0.107, 44832/60000 datapoints
2025-03-06 20:32:55,420 - INFO - training batch 1451, loss: 0.179, 46432/60000 datapoints
2025-03-06 20:32:55,615 - INFO - training batch 1501, loss: 0.107, 48032/60000 datapoints
2025-03-06 20:32:55,813 - INFO - training batch 1551, loss: 0.379, 49632/60000 datapoints
2025-03-06 20:32:56,009 - INFO - training batch 1601, loss: 0.189, 51232/60000 datapoints
2025-03-06 20:32:56,205 - INFO - training batch 1651, loss: 0.337, 52832/60000 datapoints
2025-03-06 20:32:56,400 - INFO - training batch 1701, loss: 0.159, 54432/60000 datapoints
2025-03-06 20:32:56,610 - INFO - training batch 1751, loss: 0.061, 56032/60000 datapoints
2025-03-06 20:32:56,831 - INFO - training batch 1801, loss: 0.405, 57632/60000 datapoints
2025-03-06 20:32:57,049 - INFO - training batch 1851, loss: 0.330, 59232/60000 datapoints
2025-03-06 20:32:57,168 - INFO - validation batch 1, loss: 0.142, 32/10016 datapoints
2025-03-06 20:32:57,322 - INFO - validation batch 51, loss: 0.193, 1632/10016 datapoints
2025-03-06 20:32:57,476 - INFO - validation batch 101, loss: 0.160, 3232/10016 datapoints
2025-03-06 20:32:57,632 - INFO - validation batch 151, loss: 0.383, 4832/10016 datapoints
2025-03-06 20:32:57,786 - INFO - validation batch 201, loss: 0.247, 6432/10016 datapoints
2025-03-06 20:32:57,941 - INFO - validation batch 251, loss: 0.162, 8032/10016 datapoints
2025-03-06 20:32:58,094 - INFO - validation batch 301, loss: 0.176, 9632/10016 datapoints
2025-03-06 20:32:58,132 - INFO - Epoch 767/800 done.
2025-03-06 20:32:58,132 - INFO - Final validation performance:
Loss: 0.209, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:32:58,133 - INFO - Beginning epoch 768/800
2025-03-06 20:32:58,140 - INFO - training batch 1, loss: 0.317, 32/60000 datapoints
2025-03-06 20:32:58,359 - INFO - training batch 51, loss: 0.310, 1632/60000 datapoints
2025-03-06 20:32:58,569 - INFO - training batch 101, loss: 0.122, 3232/60000 datapoints
2025-03-06 20:32:58,781 - INFO - training batch 151, loss: 0.076, 4832/60000 datapoints
2025-03-06 20:32:58,982 - INFO - training batch 201, loss: 0.368, 6432/60000 datapoints
2025-03-06 20:32:59,191 - INFO - training batch 251, loss: 0.169, 8032/60000 datapoints
2025-03-06 20:32:59,424 - INFO - training batch 301, loss: 0.100, 9632/60000 datapoints
2025-03-06 20:32:59,669 - INFO - training batch 351, loss: 0.196, 11232/60000 datapoints
2025-03-06 20:33:00,005 - INFO - training batch 401, loss: 0.309, 12832/60000 datapoints
2025-03-06 20:33:00,268 - INFO - training batch 451, loss: 0.223, 14432/60000 datapoints
2025-03-06 20:33:00,506 - INFO - training batch 501, loss: 0.129, 16032/60000 datapoints
2025-03-06 20:33:00,735 - INFO - training batch 551, loss: 0.133, 17632/60000 datapoints
2025-03-06 20:33:01,084 - INFO - training batch 601, loss: 0.222, 19232/60000 datapoints
2025-03-06 20:33:01,382 - INFO - training batch 651, loss: 0.358, 20832/60000 datapoints
2025-03-06 20:33:01,622 - INFO - training batch 701, loss: 0.262, 22432/60000 datapoints
2025-03-06 20:33:01,837 - INFO - training batch 751, loss: 0.116, 24032/60000 datapoints
2025-03-06 20:33:02,048 - INFO - training batch 801, loss: 0.208, 25632/60000 datapoints
2025-03-06 20:33:02,261 - INFO - training batch 851, loss: 0.207, 27232/60000 datapoints
2025-03-06 20:33:02,554 - INFO - training batch 901, loss: 0.062, 28832/60000 datapoints
2025-03-06 20:33:02,778 - INFO - training batch 951, loss: 0.365, 30432/60000 datapoints
2025-03-06 20:33:03,000 - INFO - training batch 1001, loss: 0.324, 32032/60000 datapoints
2025-03-06 20:33:03,223 - INFO - training batch 1051, loss: 0.329, 33632/60000 datapoints
2025-03-06 20:33:03,462 - INFO - training batch 1101, loss: 0.249, 35232/60000 datapoints
2025-03-06 20:33:03,697 - INFO - training batch 1151, loss: 0.226, 36832/60000 datapoints
2025-03-06 20:33:03,915 - INFO - training batch 1201, loss: 0.250, 38432/60000 datapoints
2025-03-06 20:33:04,131 - INFO - training batch 1251, loss: 0.171, 40032/60000 datapoints
2025-03-06 20:33:04,347 - INFO - training batch 1301, loss: 0.232, 41632/60000 datapoints
2025-03-06 20:33:04,562 - INFO - training batch 1351, loss: 0.393, 43232/60000 datapoints
2025-03-06 20:33:04,785 - INFO - training batch 1401, loss: 0.160, 44832/60000 datapoints
2025-03-06 20:33:05,004 - INFO - training batch 1451, loss: 0.415, 46432/60000 datapoints
2025-03-06 20:33:05,225 - INFO - training batch 1501, loss: 0.355, 48032/60000 datapoints
2025-03-06 20:33:05,441 - INFO - training batch 1551, loss: 0.155, 49632/60000 datapoints
2025-03-06 20:33:05,659 - INFO - training batch 1601, loss: 0.133, 51232/60000 datapoints
2025-03-06 20:33:05,876 - INFO - training batch 1651, loss: 0.030, 52832/60000 datapoints
2025-03-06 20:33:06,084 - INFO - training batch 1701, loss: 0.069, 54432/60000 datapoints
2025-03-06 20:33:06,291 - INFO - training batch 1751, loss: 0.214, 56032/60000 datapoints
2025-03-06 20:33:06,499 - INFO - training batch 1801, loss: 0.348, 57632/60000 datapoints
2025-03-06 20:33:06,715 - INFO - training batch 1851, loss: 0.258, 59232/60000 datapoints
2025-03-06 20:33:06,824 - INFO - validation batch 1, loss: 0.261, 32/10016 datapoints
2025-03-06 20:33:06,992 - INFO - validation batch 51, loss: 0.255, 1632/10016 datapoints
2025-03-06 20:33:07,156 - INFO - validation batch 101, loss: 0.227, 3232/10016 datapoints
2025-03-06 20:33:07,319 - INFO - validation batch 151, loss: 0.429, 4832/10016 datapoints
2025-03-06 20:33:07,479 - INFO - validation batch 201, loss: 0.166, 6432/10016 datapoints
2025-03-06 20:33:07,649 - INFO - validation batch 251, loss: 0.115, 8032/10016 datapoints
2025-03-06 20:33:07,811 - INFO - validation batch 301, loss: 0.254, 9632/10016 datapoints
2025-03-06 20:33:07,851 - INFO - Epoch 768/800 done.
2025-03-06 20:33:07,851 - INFO - Final validation performance:
Loss: 0.244, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:33:07,852 - INFO - Beginning epoch 769/800
2025-03-06 20:33:07,859 - INFO - training batch 1, loss: 0.123, 32/60000 datapoints
2025-03-06 20:33:08,076 - INFO - training batch 51, loss: 0.204, 1632/60000 datapoints
2025-03-06 20:33:08,275 - INFO - training batch 101, loss: 0.282, 3232/60000 datapoints
2025-03-06 20:33:08,475 - INFO - training batch 151, loss: 0.179, 4832/60000 datapoints
2025-03-06 20:33:08,681 - INFO - training batch 201, loss: 0.163, 6432/60000 datapoints
2025-03-06 20:33:08,878 - INFO - training batch 251, loss: 0.476, 8032/60000 datapoints
2025-03-06 20:33:09,075 - INFO - training batch 301, loss: 0.611, 9632/60000 datapoints
2025-03-06 20:33:09,270 - INFO - training batch 351, loss: 0.239, 11232/60000 datapoints
2025-03-06 20:33:09,466 - INFO - training batch 401, loss: 0.112, 12832/60000 datapoints
2025-03-06 20:33:09,664 - INFO - training batch 451, loss: 0.196, 14432/60000 datapoints
2025-03-06 20:33:09,859 - INFO - training batch 501, loss: 0.098, 16032/60000 datapoints
2025-03-06 20:33:10,055 - INFO - training batch 551, loss: 0.392, 17632/60000 datapoints
2025-03-06 20:33:10,266 - INFO - training batch 601, loss: 0.300, 19232/60000 datapoints
2025-03-06 20:33:10,481 - INFO - training batch 651, loss: 0.393, 20832/60000 datapoints
2025-03-06 20:33:10,695 - INFO - training batch 701, loss: 0.152, 22432/60000 datapoints
2025-03-06 20:33:10,897 - INFO - training batch 751, loss: 0.072, 24032/60000 datapoints
2025-03-06 20:33:11,105 - INFO - training batch 801, loss: 0.160, 25632/60000 datapoints
2025-03-06 20:33:11,323 - INFO - training batch 851, loss: 0.277, 27232/60000 datapoints
2025-03-06 20:33:11,536 - INFO - training batch 901, loss: 0.119, 28832/60000 datapoints
2025-03-06 20:33:11,747 - INFO - training batch 951, loss: 0.142, 30432/60000 datapoints
2025-03-06 20:33:12,006 - INFO - training batch 1001, loss: 0.245, 32032/60000 datapoints
2025-03-06 20:33:12,203 - INFO - training batch 1051, loss: 0.241, 33632/60000 datapoints
2025-03-06 20:33:12,400 - INFO - training batch 1101, loss: 0.197, 35232/60000 datapoints
2025-03-06 20:33:12,599 - INFO - training batch 1151, loss: 0.155, 36832/60000 datapoints
2025-03-06 20:33:12,803 - INFO - training batch 1201, loss: 0.208, 38432/60000 datapoints
2025-03-06 20:33:13,013 - INFO - training batch 1251, loss: 0.553, 40032/60000 datapoints
2025-03-06 20:33:13,209 - INFO - training batch 1301, loss: 0.136, 41632/60000 datapoints
2025-03-06 20:33:13,403 - INFO - training batch 1351, loss: 0.181, 43232/60000 datapoints
2025-03-06 20:33:13,598 - INFO - training batch 1401, loss: 0.078, 44832/60000 datapoints
2025-03-06 20:33:13,797 - INFO - training batch 1451, loss: 0.212, 46432/60000 datapoints
2025-03-06 20:33:14,011 - INFO - training batch 1501, loss: 0.432, 48032/60000 datapoints
2025-03-06 20:33:14,222 - INFO - training batch 1551, loss: 0.110, 49632/60000 datapoints
2025-03-06 20:33:14,415 - INFO - training batch 1601, loss: 0.111, 51232/60000 datapoints
2025-03-06 20:33:14,608 - INFO - training batch 1651, loss: 0.210, 52832/60000 datapoints
2025-03-06 20:33:14,809 - INFO - training batch 1701, loss: 0.094, 54432/60000 datapoints
2025-03-06 20:33:15,013 - INFO - training batch 1751, loss: 0.336, 56032/60000 datapoints
2025-03-06 20:33:15,215 - INFO - training batch 1801, loss: 0.181, 57632/60000 datapoints
2025-03-06 20:33:15,415 - INFO - training batch 1851, loss: 0.176, 59232/60000 datapoints
2025-03-06 20:33:15,519 - INFO - validation batch 1, loss: 0.097, 32/10016 datapoints
2025-03-06 20:33:15,676 - INFO - validation batch 51, loss: 0.450, 1632/10016 datapoints
2025-03-06 20:33:15,834 - INFO - validation batch 101, loss: 0.151, 3232/10016 datapoints
2025-03-06 20:33:15,988 - INFO - validation batch 151, loss: 0.287, 4832/10016 datapoints
2025-03-06 20:33:16,151 - INFO - validation batch 201, loss: 0.344, 6432/10016 datapoints
2025-03-06 20:33:16,306 - INFO - validation batch 251, loss: 0.434, 8032/10016 datapoints
2025-03-06 20:33:16,460 - INFO - validation batch 301, loss: 0.131, 9632/10016 datapoints
2025-03-06 20:33:16,498 - INFO - Epoch 769/800 done.
2025-03-06 20:33:16,498 - INFO - Final validation performance:
Loss: 0.271, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:33:16,499 - INFO - Beginning epoch 770/800
2025-03-06 20:33:16,505 - INFO - training batch 1, loss: 0.390, 32/60000 datapoints
2025-03-06 20:33:16,718 - INFO - training batch 51, loss: 0.100, 1632/60000 datapoints
2025-03-06 20:33:16,917 - INFO - training batch 101, loss: 0.264, 3232/60000 datapoints
2025-03-06 20:33:17,114 - INFO - training batch 151, loss: 0.067, 4832/60000 datapoints
2025-03-06 20:33:17,317 - INFO - training batch 201, loss: 0.103, 6432/60000 datapoints
2025-03-06 20:33:17,515 - INFO - training batch 251, loss: 0.392, 8032/60000 datapoints
2025-03-06 20:33:17,716 - INFO - training batch 301, loss: 0.164, 9632/60000 datapoints
2025-03-06 20:33:17,911 - INFO - training batch 351, loss: 0.142, 11232/60000 datapoints
2025-03-06 20:33:18,105 - INFO - training batch 401, loss: 0.165, 12832/60000 datapoints
2025-03-06 20:33:18,305 - INFO - training batch 451, loss: 0.202, 14432/60000 datapoints
2025-03-06 20:33:18,503 - INFO - training batch 501, loss: 0.174, 16032/60000 datapoints
2025-03-06 20:33:18,703 - INFO - training batch 551, loss: 0.167, 17632/60000 datapoints
2025-03-06 20:33:18,899 - INFO - training batch 601, loss: 0.236, 19232/60000 datapoints
2025-03-06 20:33:19,094 - INFO - training batch 651, loss: 0.243, 20832/60000 datapoints
2025-03-06 20:33:19,289 - INFO - training batch 701, loss: 0.189, 22432/60000 datapoints
2025-03-06 20:33:19,484 - INFO - training batch 751, loss: 0.274, 24032/60000 datapoints
2025-03-06 20:33:19,682 - INFO - training batch 801, loss: 0.089, 25632/60000 datapoints
2025-03-06 20:33:19,886 - INFO - training batch 851, loss: 0.281, 27232/60000 datapoints
2025-03-06 20:33:20,108 - INFO - training batch 901, loss: 0.159, 28832/60000 datapoints
2025-03-06 20:33:20,305 - INFO - training batch 951, loss: 0.134, 30432/60000 datapoints
2025-03-06 20:33:20,503 - INFO - training batch 1001, loss: 0.209, 32032/60000 datapoints
2025-03-06 20:33:20,704 - INFO - training batch 1051, loss: 0.091, 33632/60000 datapoints
2025-03-06 20:33:20,900 - INFO - training batch 1101, loss: 0.225, 35232/60000 datapoints
2025-03-06 20:33:21,094 - INFO - training batch 1151, loss: 0.172, 36832/60000 datapoints
2025-03-06 20:33:21,293 - INFO - training batch 1201, loss: 0.302, 38432/60000 datapoints
2025-03-06 20:33:21,511 - INFO - training batch 1251, loss: 0.293, 40032/60000 datapoints
2025-03-06 20:33:21,714 - INFO - training batch 1301, loss: 0.155, 41632/60000 datapoints
2025-03-06 20:33:21,909 - INFO - training batch 1351, loss: 0.204, 43232/60000 datapoints
2025-03-06 20:33:22,104 - INFO - training batch 1401, loss: 0.448, 44832/60000 datapoints
2025-03-06 20:33:22,301 - INFO - training batch 1451, loss: 0.249, 46432/60000 datapoints
2025-03-06 20:33:22,495 - INFO - training batch 1501, loss: 0.234, 48032/60000 datapoints
2025-03-06 20:33:22,694 - INFO - training batch 1551, loss: 0.180, 49632/60000 datapoints
2025-03-06 20:33:22,890 - INFO - training batch 1601, loss: 0.197, 51232/60000 datapoints
2025-03-06 20:33:23,087 - INFO - training batch 1651, loss: 0.117, 52832/60000 datapoints
2025-03-06 20:33:23,287 - INFO - training batch 1701, loss: 0.258, 54432/60000 datapoints
2025-03-06 20:33:23,482 - INFO - training batch 1751, loss: 0.273, 56032/60000 datapoints
2025-03-06 20:33:23,679 - INFO - training batch 1801, loss: 0.172, 57632/60000 datapoints
2025-03-06 20:33:23,875 - INFO - training batch 1851, loss: 0.342, 59232/60000 datapoints
2025-03-06 20:33:23,976 - INFO - validation batch 1, loss: 0.090, 32/10016 datapoints
2025-03-06 20:33:24,128 - INFO - validation batch 51, loss: 0.324, 1632/10016 datapoints
2025-03-06 20:33:24,284 - INFO - validation batch 101, loss: 0.319, 3232/10016 datapoints
2025-03-06 20:33:24,436 - INFO - validation batch 151, loss: 0.072, 4832/10016 datapoints
2025-03-06 20:33:24,590 - INFO - validation batch 201, loss: 0.123, 6432/10016 datapoints
2025-03-06 20:33:24,746 - INFO - validation batch 251, loss: 0.086, 8032/10016 datapoints
2025-03-06 20:33:24,905 - INFO - validation batch 301, loss: 0.084, 9632/10016 datapoints
2025-03-06 20:33:24,943 - INFO - Epoch 770/800 done.
2025-03-06 20:33:24,943 - INFO - Final validation performance:
Loss: 0.157, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:33:24,944 - INFO - Beginning epoch 771/800
2025-03-06 20:33:24,951 - INFO - training batch 1, loss: 0.126, 32/60000 datapoints
2025-03-06 20:33:25,149 - INFO - training batch 51, loss: 0.194, 1632/60000 datapoints
2025-03-06 20:33:25,346 - INFO - training batch 101, loss: 0.050, 3232/60000 datapoints
2025-03-06 20:33:25,552 - INFO - training batch 151, loss: 0.100, 4832/60000 datapoints
2025-03-06 20:33:25,748 - INFO - training batch 201, loss: 0.172, 6432/60000 datapoints
2025-03-06 20:33:25,949 - INFO - training batch 251, loss: 0.170, 8032/60000 datapoints
2025-03-06 20:33:26,148 - INFO - training batch 301, loss: 0.218, 9632/60000 datapoints
2025-03-06 20:33:26,348 - INFO - training batch 351, loss: 0.165, 11232/60000 datapoints
2025-03-06 20:33:26,544 - INFO - training batch 401, loss: 0.183, 12832/60000 datapoints
2025-03-06 20:33:26,747 - INFO - training batch 451, loss: 0.081, 14432/60000 datapoints
2025-03-06 20:33:26,950 - INFO - training batch 501, loss: 0.281, 16032/60000 datapoints
2025-03-06 20:33:27,147 - INFO - training batch 551, loss: 0.384, 17632/60000 datapoints
2025-03-06 20:33:27,343 - INFO - training batch 601, loss: 0.061, 19232/60000 datapoints
2025-03-06 20:33:27,540 - INFO - training batch 651, loss: 0.102, 20832/60000 datapoints
2025-03-06 20:33:27,738 - INFO - training batch 701, loss: 0.255, 22432/60000 datapoints
2025-03-06 20:33:27,934 - INFO - training batch 751, loss: 0.392, 24032/60000 datapoints
2025-03-06 20:33:28,131 - INFO - training batch 801, loss: 0.341, 25632/60000 datapoints
2025-03-06 20:33:28,327 - INFO - training batch 851, loss: 0.269, 27232/60000 datapoints
2025-03-06 20:33:28,530 - INFO - training batch 901, loss: 0.240, 28832/60000 datapoints
2025-03-06 20:33:28,732 - INFO - training batch 951, loss: 0.130, 30432/60000 datapoints
2025-03-06 20:33:28,935 - INFO - training batch 1001, loss: 0.360, 32032/60000 datapoints
2025-03-06 20:33:29,130 - INFO - training batch 1051, loss: 0.502, 33632/60000 datapoints
2025-03-06 20:33:29,326 - INFO - training batch 1101, loss: 0.298, 35232/60000 datapoints
2025-03-06 20:33:29,521 - INFO - training batch 1151, loss: 0.194, 36832/60000 datapoints
2025-03-06 20:33:29,722 - INFO - training batch 1201, loss: 0.113, 38432/60000 datapoints
2025-03-06 20:33:29,918 - INFO - training batch 1251, loss: 0.337, 40032/60000 datapoints
2025-03-06 20:33:30,114 - INFO - training batch 1301, loss: 0.115, 41632/60000 datapoints
2025-03-06 20:33:30,312 - INFO - training batch 1351, loss: 0.397, 43232/60000 datapoints
2025-03-06 20:33:30,507 - INFO - training batch 1401, loss: 0.246, 44832/60000 datapoints
2025-03-06 20:33:30,703 - INFO - training batch 1451, loss: 0.304, 46432/60000 datapoints
2025-03-06 20:33:30,905 - INFO - training batch 1501, loss: 0.264, 48032/60000 datapoints
2025-03-06 20:33:31,102 - INFO - training batch 1551, loss: 0.428, 49632/60000 datapoints
2025-03-06 20:33:31,299 - INFO - training batch 1601, loss: 0.059, 51232/60000 datapoints
2025-03-06 20:33:31,525 - INFO - training batch 1651, loss: 0.362, 52832/60000 datapoints
2025-03-06 20:33:31,733 - INFO - training batch 1701, loss: 0.166, 54432/60000 datapoints
2025-03-06 20:33:31,928 - INFO - training batch 1751, loss: 0.234, 56032/60000 datapoints
2025-03-06 20:33:32,121 - INFO - training batch 1801, loss: 0.488, 57632/60000 datapoints
2025-03-06 20:33:32,314 - INFO - training batch 1851, loss: 0.195, 59232/60000 datapoints
2025-03-06 20:33:32,417 - INFO - validation batch 1, loss: 0.108, 32/10016 datapoints
2025-03-06 20:33:32,572 - INFO - validation batch 51, loss: 0.183, 1632/10016 datapoints
2025-03-06 20:33:32,731 - INFO - validation batch 101, loss: 0.203, 3232/10016 datapoints
2025-03-06 20:33:32,887 - INFO - validation batch 151, loss: 0.201, 4832/10016 datapoints
2025-03-06 20:33:33,041 - INFO - validation batch 201, loss: 0.296, 6432/10016 datapoints
2025-03-06 20:33:33,196 - INFO - validation batch 251, loss: 0.143, 8032/10016 datapoints
2025-03-06 20:33:33,349 - INFO - validation batch 301, loss: 0.144, 9632/10016 datapoints
2025-03-06 20:33:33,387 - INFO - Epoch 771/800 done.
2025-03-06 20:33:33,387 - INFO - Final validation performance:
Loss: 0.182, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:33:33,387 - INFO - Beginning epoch 772/800
2025-03-06 20:33:33,394 - INFO - training batch 1, loss: 0.230, 32/60000 datapoints
2025-03-06 20:33:33,603 - INFO - training batch 51, loss: 0.217, 1632/60000 datapoints
2025-03-06 20:33:33,800 - INFO - training batch 101, loss: 0.323, 3232/60000 datapoints
2025-03-06 20:33:34,003 - INFO - training batch 151, loss: 0.163, 4832/60000 datapoints
2025-03-06 20:33:34,203 - INFO - training batch 201, loss: 0.277, 6432/60000 datapoints
2025-03-06 20:33:34,401 - INFO - training batch 251, loss: 0.206, 8032/60000 datapoints
2025-03-06 20:33:34,596 - INFO - training batch 301, loss: 0.360, 9632/60000 datapoints
2025-03-06 20:33:34,793 - INFO - training batch 351, loss: 0.561, 11232/60000 datapoints
2025-03-06 20:33:34,991 - INFO - training batch 401, loss: 0.258, 12832/60000 datapoints
2025-03-06 20:33:35,187 - INFO - training batch 451, loss: 0.084, 14432/60000 datapoints
2025-03-06 20:33:35,386 - INFO - training batch 501, loss: 0.223, 16032/60000 datapoints
2025-03-06 20:33:35,582 - INFO - training batch 551, loss: 0.171, 17632/60000 datapoints
2025-03-06 20:33:35,781 - INFO - training batch 601, loss: 0.233, 19232/60000 datapoints
2025-03-06 20:33:35,975 - INFO - training batch 651, loss: 0.188, 20832/60000 datapoints
2025-03-06 20:33:36,171 - INFO - training batch 701, loss: 0.179, 22432/60000 datapoints
2025-03-06 20:33:36,367 - INFO - training batch 751, loss: 0.334, 24032/60000 datapoints
2025-03-06 20:33:36,564 - INFO - training batch 801, loss: 0.043, 25632/60000 datapoints
2025-03-06 20:33:36,765 - INFO - training batch 851, loss: 0.200, 27232/60000 datapoints
2025-03-06 20:33:36,964 - INFO - training batch 901, loss: 0.082, 28832/60000 datapoints
2025-03-06 20:33:37,164 - INFO - training batch 951, loss: 0.159, 30432/60000 datapoints
2025-03-06 20:33:37,361 - INFO - training batch 1001, loss: 0.166, 32032/60000 datapoints
2025-03-06 20:33:37,556 - INFO - training batch 1051, loss: 0.140, 33632/60000 datapoints
2025-03-06 20:33:37,793 - INFO - training batch 1101, loss: 0.106, 35232/60000 datapoints
2025-03-06 20:33:37,987 - INFO - training batch 1151, loss: 0.288, 36832/60000 datapoints
2025-03-06 20:33:38,183 - INFO - training batch 1201, loss: 0.370, 38432/60000 datapoints
2025-03-06 20:33:38,381 - INFO - training batch 1251, loss: 0.152, 40032/60000 datapoints
2025-03-06 20:33:38,576 - INFO - training batch 1301, loss: 0.093, 41632/60000 datapoints
2025-03-06 20:33:38,775 - INFO - training batch 1351, loss: 0.254, 43232/60000 datapoints
2025-03-06 20:33:38,969 - INFO - training batch 1401, loss: 0.286, 44832/60000 datapoints
2025-03-06 20:33:39,167 - INFO - training batch 1451, loss: 0.198, 46432/60000 datapoints
2025-03-06 20:33:39,363 - INFO - training batch 1501, loss: 0.144, 48032/60000 datapoints
2025-03-06 20:33:39,558 - INFO - training batch 1551, loss: 0.382, 49632/60000 datapoints
2025-03-06 20:33:39,756 - INFO - training batch 1601, loss: 0.195, 51232/60000 datapoints
2025-03-06 20:33:39,950 - INFO - training batch 1651, loss: 0.291, 52832/60000 datapoints
2025-03-06 20:33:40,144 - INFO - training batch 1701, loss: 0.143, 54432/60000 datapoints
2025-03-06 20:33:40,340 - INFO - training batch 1751, loss: 0.107, 56032/60000 datapoints
2025-03-06 20:33:40,536 - INFO - training batch 1801, loss: 0.080, 57632/60000 datapoints
2025-03-06 20:33:40,731 - INFO - training batch 1851, loss: 0.296, 59232/60000 datapoints
2025-03-06 20:33:40,836 - INFO - validation batch 1, loss: 0.095, 32/10016 datapoints
2025-03-06 20:33:40,988 - INFO - validation batch 51, loss: 0.154, 1632/10016 datapoints
2025-03-06 20:33:41,143 - INFO - validation batch 101, loss: 0.359, 3232/10016 datapoints
2025-03-06 20:33:41,294 - INFO - validation batch 151, loss: 0.263, 4832/10016 datapoints
2025-03-06 20:33:41,449 - INFO - validation batch 201, loss: 0.167, 6432/10016 datapoints
2025-03-06 20:33:41,619 - INFO - validation batch 251, loss: 0.138, 8032/10016 datapoints
2025-03-06 20:33:41,780 - INFO - validation batch 301, loss: 0.161, 9632/10016 datapoints
2025-03-06 20:33:41,816 - INFO - Epoch 772/800 done.
2025-03-06 20:33:41,817 - INFO - Final validation performance:
Loss: 0.191, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:33:41,817 - INFO - Beginning epoch 773/800
2025-03-06 20:33:41,825 - INFO - training batch 1, loss: 0.268, 32/60000 datapoints
2025-03-06 20:33:42,035 - INFO - training batch 51, loss: 0.109, 1632/60000 datapoints
2025-03-06 20:33:42,231 - INFO - training batch 101, loss: 0.237, 3232/60000 datapoints
2025-03-06 20:33:42,431 - INFO - training batch 151, loss: 0.223, 4832/60000 datapoints
2025-03-06 20:33:42,627 - INFO - training batch 201, loss: 0.094, 6432/60000 datapoints
2025-03-06 20:33:42,833 - INFO - training batch 251, loss: 0.094, 8032/60000 datapoints
2025-03-06 20:33:43,028 - INFO - training batch 301, loss: 0.415, 9632/60000 datapoints
2025-03-06 20:33:43,221 - INFO - training batch 351, loss: 0.099, 11232/60000 datapoints
2025-03-06 20:33:43,413 - INFO - training batch 401, loss: 0.149, 12832/60000 datapoints
2025-03-06 20:33:43,604 - INFO - training batch 451, loss: 0.248, 14432/60000 datapoints
2025-03-06 20:33:43,797 - INFO - training batch 501, loss: 0.126, 16032/60000 datapoints
2025-03-06 20:33:43,992 - INFO - training batch 551, loss: 0.213, 17632/60000 datapoints
2025-03-06 20:33:44,187 - INFO - training batch 601, loss: 0.086, 19232/60000 datapoints
2025-03-06 20:33:44,377 - INFO - training batch 651, loss: 0.405, 20832/60000 datapoints
2025-03-06 20:33:44,571 - INFO - training batch 701, loss: 0.144, 22432/60000 datapoints
2025-03-06 20:33:44,792 - INFO - training batch 751, loss: 0.166, 24032/60000 datapoints
2025-03-06 20:33:44,989 - INFO - training batch 801, loss: 0.139, 25632/60000 datapoints
2025-03-06 20:33:45,184 - INFO - training batch 851, loss: 0.211, 27232/60000 datapoints
2025-03-06 20:33:45,380 - INFO - training batch 901, loss: 0.116, 28832/60000 datapoints
2025-03-06 20:33:45,573 - INFO - training batch 951, loss: 0.108, 30432/60000 datapoints
2025-03-06 20:33:45,768 - INFO - training batch 1001, loss: 0.155, 32032/60000 datapoints
2025-03-06 20:33:45,962 - INFO - training batch 1051, loss: 0.129, 33632/60000 datapoints
2025-03-06 20:33:46,158 - INFO - training batch 1101, loss: 0.128, 35232/60000 datapoints
2025-03-06 20:33:46,352 - INFO - training batch 1151, loss: 0.185, 36832/60000 datapoints
2025-03-06 20:33:46,548 - INFO - training batch 1201, loss: 0.131, 38432/60000 datapoints
2025-03-06 20:33:46,742 - INFO - training batch 1251, loss: 0.193, 40032/60000 datapoints
2025-03-06 20:33:46,939 - INFO - training batch 1301, loss: 0.077, 41632/60000 datapoints
2025-03-06 20:33:47,131 - INFO - training batch 1351, loss: 0.081, 43232/60000 datapoints
2025-03-06 20:33:47,327 - INFO - training batch 1401, loss: 0.234, 44832/60000 datapoints
2025-03-06 20:33:47,523 - INFO - training batch 1451, loss: 0.092, 46432/60000 datapoints
2025-03-06 20:33:47,717 - INFO - training batch 1501, loss: 0.229, 48032/60000 datapoints
2025-03-06 20:33:47,911 - INFO - training batch 1551, loss: 0.243, 49632/60000 datapoints
2025-03-06 20:33:48,104 - INFO - training batch 1601, loss: 0.288, 51232/60000 datapoints
2025-03-06 20:33:48,295 - INFO - training batch 1651, loss: 0.149, 52832/60000 datapoints
2025-03-06 20:33:48,489 - INFO - training batch 1701, loss: 0.196, 54432/60000 datapoints
2025-03-06 20:33:48,685 - INFO - training batch 1751, loss: 0.084, 56032/60000 datapoints
2025-03-06 20:33:48,879 - INFO - training batch 1801, loss: 0.227, 57632/60000 datapoints
2025-03-06 20:33:49,071 - INFO - training batch 1851, loss: 0.147, 59232/60000 datapoints
2025-03-06 20:33:49,170 - INFO - validation batch 1, loss: 0.099, 32/10016 datapoints
2025-03-06 20:33:49,321 - INFO - validation batch 51, loss: 0.535, 1632/10016 datapoints
2025-03-06 20:33:49,471 - INFO - validation batch 101, loss: 0.244, 3232/10016 datapoints
2025-03-06 20:33:49,622 - INFO - validation batch 151, loss: 0.218, 4832/10016 datapoints
2025-03-06 20:33:49,775 - INFO - validation batch 201, loss: 0.126, 6432/10016 datapoints
2025-03-06 20:33:49,934 - INFO - validation batch 251, loss: 0.184, 8032/10016 datapoints
2025-03-06 20:33:50,091 - INFO - validation batch 301, loss: 0.078, 9632/10016 datapoints
2025-03-06 20:33:50,130 - INFO - Epoch 773/800 done.
2025-03-06 20:33:50,130 - INFO - Final validation performance:
Loss: 0.212, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:33:50,131 - INFO - Beginning epoch 774/800
2025-03-06 20:33:50,137 - INFO - training batch 1, loss: 0.562, 32/60000 datapoints
2025-03-06 20:33:50,333 - INFO - training batch 51, loss: 0.160, 1632/60000 datapoints
2025-03-06 20:33:50,543 - INFO - training batch 101, loss: 0.288, 3232/60000 datapoints
2025-03-06 20:33:50,740 - INFO - training batch 151, loss: 0.109, 4832/60000 datapoints
2025-03-06 20:33:50,945 - INFO - training batch 201, loss: 0.212, 6432/60000 datapoints
2025-03-06 20:33:51,149 - INFO - training batch 251, loss: 0.151, 8032/60000 datapoints
2025-03-06 20:33:51,350 - INFO - training batch 301, loss: 0.111, 9632/60000 datapoints
2025-03-06 20:33:51,548 - INFO - training batch 351, loss: 0.251, 11232/60000 datapoints
2025-03-06 20:33:51,765 - INFO - training batch 401, loss: 0.179, 12832/60000 datapoints
2025-03-06 20:33:51,962 - INFO - training batch 451, loss: 0.294, 14432/60000 datapoints
2025-03-06 20:33:52,158 - INFO - training batch 501, loss: 0.344, 16032/60000 datapoints
2025-03-06 20:33:52,352 - INFO - training batch 551, loss: 0.217, 17632/60000 datapoints
2025-03-06 20:33:52,548 - INFO - training batch 601, loss: 0.150, 19232/60000 datapoints
2025-03-06 20:33:52,746 - INFO - training batch 651, loss: 0.262, 20832/60000 datapoints
2025-03-06 20:33:52,948 - INFO - training batch 701, loss: 0.103, 22432/60000 datapoints
2025-03-06 20:33:53,143 - INFO - training batch 751, loss: 0.458, 24032/60000 datapoints
2025-03-06 20:33:53,337 - INFO - training batch 801, loss: 0.349, 25632/60000 datapoints
2025-03-06 20:33:53,532 - INFO - training batch 851, loss: 0.287, 27232/60000 datapoints
2025-03-06 20:33:53,731 - INFO - training batch 901, loss: 0.145, 28832/60000 datapoints
2025-03-06 20:33:53,928 - INFO - training batch 951, loss: 0.091, 30432/60000 datapoints
2025-03-06 20:33:54,124 - INFO - training batch 1001, loss: 0.224, 32032/60000 datapoints
2025-03-06 20:33:54,319 - INFO - training batch 1051, loss: 0.571, 33632/60000 datapoints
2025-03-06 20:33:54,517 - INFO - training batch 1101, loss: 0.165, 35232/60000 datapoints
2025-03-06 20:33:54,714 - INFO - training batch 1151, loss: 0.077, 36832/60000 datapoints
2025-03-06 20:33:54,917 - INFO - training batch 1201, loss: 0.288, 38432/60000 datapoints
2025-03-06 20:33:55,116 - INFO - training batch 1251, loss: 0.265, 40032/60000 datapoints
2025-03-06 20:33:55,313 - INFO - training batch 1301, loss: 0.144, 41632/60000 datapoints
2025-03-06 20:33:55,507 - INFO - training batch 1351, loss: 0.077, 43232/60000 datapoints
2025-03-06 20:33:55,704 - INFO - training batch 1401, loss: 0.281, 44832/60000 datapoints
2025-03-06 20:33:55,900 - INFO - training batch 1451, loss: 0.055, 46432/60000 datapoints
2025-03-06 20:33:56,096 - INFO - training batch 1501, loss: 0.271, 48032/60000 datapoints
2025-03-06 20:33:56,290 - INFO - training batch 1551, loss: 0.252, 49632/60000 datapoints
2025-03-06 20:33:56,486 - INFO - training batch 1601, loss: 0.176, 51232/60000 datapoints
2025-03-06 20:33:56,683 - INFO - training batch 1651, loss: 0.153, 52832/60000 datapoints
2025-03-06 20:33:56,882 - INFO - training batch 1701, loss: 0.129, 54432/60000 datapoints
2025-03-06 20:33:57,077 - INFO - training batch 1751, loss: 0.258, 56032/60000 datapoints
2025-03-06 20:33:57,278 - INFO - training batch 1801, loss: 0.213, 57632/60000 datapoints
2025-03-06 20:33:57,473 - INFO - training batch 1851, loss: 0.096, 59232/60000 datapoints
2025-03-06 20:33:57,574 - INFO - validation batch 1, loss: 0.178, 32/10016 datapoints
2025-03-06 20:33:57,729 - INFO - validation batch 51, loss: 0.253, 1632/10016 datapoints
2025-03-06 20:33:57,884 - INFO - validation batch 101, loss: 0.206, 3232/10016 datapoints
2025-03-06 20:33:58,037 - INFO - validation batch 151, loss: 0.061, 4832/10016 datapoints
2025-03-06 20:33:58,191 - INFO - validation batch 201, loss: 0.196, 6432/10016 datapoints
2025-03-06 20:33:58,345 - INFO - validation batch 251, loss: 0.149, 8032/10016 datapoints
2025-03-06 20:33:58,504 - INFO - validation batch 301, loss: 0.074, 9632/10016 datapoints
2025-03-06 20:33:58,543 - INFO - Epoch 774/800 done.
2025-03-06 20:33:58,543 - INFO - Final validation performance:
Loss: 0.160, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:33:58,544 - INFO - Beginning epoch 775/800
2025-03-06 20:33:58,550 - INFO - training batch 1, loss: 0.093, 32/60000 datapoints
2025-03-06 20:33:58,754 - INFO - training batch 51, loss: 0.180, 1632/60000 datapoints
2025-03-06 20:33:58,964 - INFO - training batch 101, loss: 0.428, 3232/60000 datapoints
2025-03-06 20:33:59,160 - INFO - training batch 151, loss: 0.179, 4832/60000 datapoints
2025-03-06 20:33:59,359 - INFO - training batch 201, loss: 0.432, 6432/60000 datapoints
2025-03-06 20:33:59,559 - INFO - training batch 251, loss: 0.227, 8032/60000 datapoints
2025-03-06 20:33:59,763 - INFO - training batch 301, loss: 0.353, 9632/60000 datapoints
2025-03-06 20:33:59,959 - INFO - training batch 351, loss: 0.139, 11232/60000 datapoints
2025-03-06 20:34:00,155 - INFO - training batch 401, loss: 0.099, 12832/60000 datapoints
2025-03-06 20:34:00,350 - INFO - training batch 451, loss: 0.159, 14432/60000 datapoints
2025-03-06 20:34:00,546 - INFO - training batch 501, loss: 0.581, 16032/60000 datapoints
2025-03-06 20:34:00,744 - INFO - training batch 551, loss: 0.191, 17632/60000 datapoints
2025-03-06 20:34:00,941 - INFO - training batch 601, loss: 0.304, 19232/60000 datapoints
2025-03-06 20:34:01,135 - INFO - training batch 651, loss: 0.131, 20832/60000 datapoints
2025-03-06 20:34:01,337 - INFO - training batch 701, loss: 0.229, 22432/60000 datapoints
2025-03-06 20:34:01,535 - INFO - training batch 751, loss: 0.198, 24032/60000 datapoints
2025-03-06 20:34:01,741 - INFO - training batch 801, loss: 0.272, 25632/60000 datapoints
2025-03-06 20:34:01,954 - INFO - training batch 851, loss: 0.203, 27232/60000 datapoints
2025-03-06 20:34:02,149 - INFO - training batch 901, loss: 0.339, 28832/60000 datapoints
2025-03-06 20:34:02,344 - INFO - training batch 951, loss: 0.200, 30432/60000 datapoints
2025-03-06 20:34:02,538 - INFO - training batch 1001, loss: 0.148, 32032/60000 datapoints
2025-03-06 20:34:02,739 - INFO - training batch 1051, loss: 0.234, 33632/60000 datapoints
2025-03-06 20:34:02,942 - INFO - training batch 1101, loss: 0.119, 35232/60000 datapoints
2025-03-06 20:34:03,137 - INFO - training batch 1151, loss: 0.135, 36832/60000 datapoints
2025-03-06 20:34:03,333 - INFO - training batch 1201, loss: 0.498, 38432/60000 datapoints
2025-03-06 20:34:03,537 - INFO - training batch 1251, loss: 0.303, 40032/60000 datapoints
2025-03-06 20:34:03,737 - INFO - training batch 1301, loss: 0.417, 41632/60000 datapoints
2025-03-06 20:34:03,933 - INFO - training batch 1351, loss: 0.168, 43232/60000 datapoints
2025-03-06 20:34:04,128 - INFO - training batch 1401, loss: 0.199, 44832/60000 datapoints
2025-03-06 20:34:04,324 - INFO - training batch 1451, loss: 0.140, 46432/60000 datapoints
2025-03-06 20:34:04,519 - INFO - training batch 1501, loss: 0.070, 48032/60000 datapoints
2025-03-06 20:34:04,717 - INFO - training batch 1551, loss: 0.162, 49632/60000 datapoints
2025-03-06 20:34:04,918 - INFO - training batch 1601, loss: 0.124, 51232/60000 datapoints
2025-03-06 20:34:05,115 - INFO - training batch 1651, loss: 0.180, 52832/60000 datapoints
2025-03-06 20:34:05,313 - INFO - training batch 1701, loss: 0.245, 54432/60000 datapoints
2025-03-06 20:34:05,508 - INFO - training batch 1751, loss: 0.406, 56032/60000 datapoints
2025-03-06 20:34:05,704 - INFO - training batch 1801, loss: 0.254, 57632/60000 datapoints
2025-03-06 20:34:05,899 - INFO - training batch 1851, loss: 0.413, 59232/60000 datapoints
2025-03-06 20:34:06,001 - INFO - validation batch 1, loss: 0.320, 32/10016 datapoints
2025-03-06 20:34:06,155 - INFO - validation batch 51, loss: 0.231, 1632/10016 datapoints
2025-03-06 20:34:06,311 - INFO - validation batch 101, loss: 0.280, 3232/10016 datapoints
2025-03-06 20:34:06,464 - INFO - validation batch 151, loss: 0.219, 4832/10016 datapoints
2025-03-06 20:34:06,620 - INFO - validation batch 201, loss: 0.270, 6432/10016 datapoints
2025-03-06 20:34:06,776 - INFO - validation batch 251, loss: 0.083, 8032/10016 datapoints
2025-03-06 20:34:06,933 - INFO - validation batch 301, loss: 0.046, 9632/10016 datapoints
2025-03-06 20:34:06,970 - INFO - Epoch 775/800 done.
2025-03-06 20:34:06,970 - INFO - Final validation performance:
Loss: 0.207, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:34:06,971 - INFO - Beginning epoch 776/800
2025-03-06 20:34:06,978 - INFO - training batch 1, loss: 0.103, 32/60000 datapoints
2025-03-06 20:34:07,178 - INFO - training batch 51, loss: 0.089, 1632/60000 datapoints
2025-03-06 20:34:07,378 - INFO - training batch 101, loss: 0.066, 3232/60000 datapoints
2025-03-06 20:34:07,585 - INFO - training batch 151, loss: 0.199, 4832/60000 datapoints
2025-03-06 20:34:07,790 - INFO - training batch 201, loss: 0.274, 6432/60000 datapoints
2025-03-06 20:34:07,993 - INFO - training batch 251, loss: 0.200, 8032/60000 datapoints
2025-03-06 20:34:08,230 - INFO - training batch 301, loss: 0.252, 9632/60000 datapoints
2025-03-06 20:34:08,438 - INFO - training batch 351, loss: 0.281, 11232/60000 datapoints
2025-03-06 20:34:08,637 - INFO - training batch 401, loss: 0.130, 12832/60000 datapoints
2025-03-06 20:34:08,836 - INFO - training batch 451, loss: 0.262, 14432/60000 datapoints
2025-03-06 20:34:09,037 - INFO - training batch 501, loss: 0.119, 16032/60000 datapoints
2025-03-06 20:34:09,236 - INFO - training batch 551, loss: 0.141, 17632/60000 datapoints
2025-03-06 20:34:09,435 - INFO - training batch 601, loss: 0.276, 19232/60000 datapoints
2025-03-06 20:34:09,637 - INFO - training batch 651, loss: 0.097, 20832/60000 datapoints
2025-03-06 20:34:09,833 - INFO - training batch 701, loss: 0.173, 22432/60000 datapoints
2025-03-06 20:34:10,030 - INFO - training batch 751, loss: 0.261, 24032/60000 datapoints
2025-03-06 20:34:10,226 - INFO - training batch 801, loss: 0.150, 25632/60000 datapoints
2025-03-06 20:34:10,424 - INFO - training batch 851, loss: 0.072, 27232/60000 datapoints
2025-03-06 20:34:10,619 - INFO - training batch 901, loss: 0.161, 28832/60000 datapoints
2025-03-06 20:34:10,818 - INFO - training batch 951, loss: 0.188, 30432/60000 datapoints
2025-03-06 20:34:11,018 - INFO - training batch 1001, loss: 0.123, 32032/60000 datapoints
2025-03-06 20:34:11,212 - INFO - training batch 1051, loss: 0.423, 33632/60000 datapoints
2025-03-06 20:34:11,410 - INFO - training batch 1101, loss: 0.084, 35232/60000 datapoints
2025-03-06 20:34:11,605 - INFO - training batch 1151, loss: 0.275, 36832/60000 datapoints
2025-03-06 20:34:11,826 - INFO - training batch 1201, loss: 0.207, 38432/60000 datapoints
2025-03-06 20:34:12,078 - INFO - training batch 1251, loss: 0.117, 40032/60000 datapoints
2025-03-06 20:34:12,279 - INFO - training batch 1301, loss: 0.239, 41632/60000 datapoints
2025-03-06 20:34:12,474 - INFO - training batch 1351, loss: 0.699, 43232/60000 datapoints
2025-03-06 20:34:12,671 - INFO - training batch 1401, loss: 0.282, 44832/60000 datapoints
2025-03-06 20:34:12,873 - INFO - training batch 1451, loss: 0.055, 46432/60000 datapoints
2025-03-06 20:34:13,071 - INFO - training batch 1501, loss: 0.210, 48032/60000 datapoints
2025-03-06 20:34:13,267 - INFO - training batch 1551, loss: 0.278, 49632/60000 datapoints
2025-03-06 20:34:13,478 - INFO - training batch 1601, loss: 0.308, 51232/60000 datapoints
2025-03-06 20:34:13,684 - INFO - training batch 1651, loss: 0.261, 52832/60000 datapoints
2025-03-06 20:34:13,880 - INFO - training batch 1701, loss: 0.061, 54432/60000 datapoints
2025-03-06 20:34:14,075 - INFO - training batch 1751, loss: 0.111, 56032/60000 datapoints
2025-03-06 20:34:14,271 - INFO - training batch 1801, loss: 0.268, 57632/60000 datapoints
2025-03-06 20:34:14,468 - INFO - training batch 1851, loss: 0.071, 59232/60000 datapoints
2025-03-06 20:34:14,569 - INFO - validation batch 1, loss: 0.188, 32/10016 datapoints
2025-03-06 20:34:14,724 - INFO - validation batch 51, loss: 0.093, 1632/10016 datapoints
2025-03-06 20:34:14,883 - INFO - validation batch 101, loss: 0.229, 3232/10016 datapoints
2025-03-06 20:34:15,042 - INFO - validation batch 151, loss: 0.093, 4832/10016 datapoints
2025-03-06 20:34:15,197 - INFO - validation batch 201, loss: 0.135, 6432/10016 datapoints
2025-03-06 20:34:15,354 - INFO - validation batch 251, loss: 0.102, 8032/10016 datapoints
2025-03-06 20:34:15,509 - INFO - validation batch 301, loss: 0.207, 9632/10016 datapoints
2025-03-06 20:34:15,548 - INFO - Epoch 776/800 done.
2025-03-06 20:34:15,548 - INFO - Final validation performance:
Loss: 0.150, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:34:15,549 - INFO - Beginning epoch 777/800
2025-03-06 20:34:15,556 - INFO - training batch 1, loss: 0.229, 32/60000 datapoints
2025-03-06 20:34:15,768 - INFO - training batch 51, loss: 0.156, 1632/60000 datapoints
2025-03-06 20:34:15,964 - INFO - training batch 101, loss: 0.227, 3232/60000 datapoints
2025-03-06 20:34:16,166 - INFO - training batch 151, loss: 0.335, 4832/60000 datapoints
2025-03-06 20:34:16,364 - INFO - training batch 201, loss: 0.370, 6432/60000 datapoints
2025-03-06 20:34:16,563 - INFO - training batch 251, loss: 0.263, 8032/60000 datapoints
2025-03-06 20:34:16,768 - INFO - training batch 301, loss: 0.050, 9632/60000 datapoints
2025-03-06 20:34:16,969 - INFO - training batch 351, loss: 0.163, 11232/60000 datapoints
2025-03-06 20:34:17,165 - INFO - training batch 401, loss: 0.317, 12832/60000 datapoints
2025-03-06 20:34:17,364 - INFO - training batch 451, loss: 0.182, 14432/60000 datapoints
2025-03-06 20:34:17,561 - INFO - training batch 501, loss: 0.064, 16032/60000 datapoints
2025-03-06 20:34:17,758 - INFO - training batch 551, loss: 0.303, 17632/60000 datapoints
2025-03-06 20:34:17,957 - INFO - training batch 601, loss: 0.485, 19232/60000 datapoints
2025-03-06 20:34:18,153 - INFO - training batch 651, loss: 0.318, 20832/60000 datapoints
2025-03-06 20:34:18,349 - INFO - training batch 701, loss: 0.089, 22432/60000 datapoints
2025-03-06 20:34:18,547 - INFO - training batch 751, loss: 0.162, 24032/60000 datapoints
2025-03-06 20:34:18,749 - INFO - training batch 801, loss: 0.135, 25632/60000 datapoints
2025-03-06 20:34:18,948 - INFO - training batch 851, loss: 0.148, 27232/60000 datapoints
2025-03-06 20:34:19,145 - INFO - training batch 901, loss: 0.353, 28832/60000 datapoints
2025-03-06 20:34:19,341 - INFO - training batch 951, loss: 0.371, 30432/60000 datapoints
2025-03-06 20:34:19,540 - INFO - training batch 1001, loss: 0.070, 32032/60000 datapoints
2025-03-06 20:34:19,739 - INFO - training batch 1051, loss: 0.253, 33632/60000 datapoints
2025-03-06 20:34:19,936 - INFO - training batch 1101, loss: 0.106, 35232/60000 datapoints
2025-03-06 20:34:20,133 - INFO - training batch 1151, loss: 0.181, 36832/60000 datapoints
2025-03-06 20:34:20,329 - INFO - training batch 1201, loss: 0.177, 38432/60000 datapoints
2025-03-06 20:34:20,526 - INFO - training batch 1251, loss: 0.197, 40032/60000 datapoints
2025-03-06 20:34:20,725 - INFO - training batch 1301, loss: 0.320, 41632/60000 datapoints
2025-03-06 20:34:20,924 - INFO - training batch 1351, loss: 0.352, 43232/60000 datapoints
2025-03-06 20:34:21,120 - INFO - training batch 1401, loss: 0.291, 44832/60000 datapoints
2025-03-06 20:34:21,314 - INFO - training batch 1451, loss: 0.216, 46432/60000 datapoints
2025-03-06 20:34:21,512 - INFO - training batch 1501, loss: 0.282, 48032/60000 datapoints
2025-03-06 20:34:21,711 - INFO - training batch 1551, loss: 0.284, 49632/60000 datapoints
2025-03-06 20:34:21,909 - INFO - training batch 1601, loss: 0.221, 51232/60000 datapoints
2025-03-06 20:34:22,126 - INFO - training batch 1651, loss: 0.145, 52832/60000 datapoints
2025-03-06 20:34:22,321 - INFO - training batch 1701, loss: 0.320, 54432/60000 datapoints
2025-03-06 20:34:22,520 - INFO - training batch 1751, loss: 0.130, 56032/60000 datapoints
2025-03-06 20:34:22,719 - INFO - training batch 1801, loss: 0.169, 57632/60000 datapoints
2025-03-06 20:34:22,920 - INFO - training batch 1851, loss: 0.613, 59232/60000 datapoints
2025-03-06 20:34:23,024 - INFO - validation batch 1, loss: 0.135, 32/10016 datapoints
2025-03-06 20:34:23,179 - INFO - validation batch 51, loss: 0.267, 1632/10016 datapoints
2025-03-06 20:34:23,336 - INFO - validation batch 101, loss: 0.329, 3232/10016 datapoints
2025-03-06 20:34:23,491 - INFO - validation batch 151, loss: 0.113, 4832/10016 datapoints
2025-03-06 20:34:23,647 - INFO - validation batch 201, loss: 0.118, 6432/10016 datapoints
2025-03-06 20:34:23,802 - INFO - validation batch 251, loss: 0.179, 8032/10016 datapoints
2025-03-06 20:34:23,957 - INFO - validation batch 301, loss: 0.239, 9632/10016 datapoints
2025-03-06 20:34:23,995 - INFO - Epoch 777/800 done.
2025-03-06 20:34:23,995 - INFO - Final validation performance:
Loss: 0.197, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:34:23,996 - INFO - Beginning epoch 778/800
2025-03-06 20:34:24,002 - INFO - training batch 1, loss: 0.098, 32/60000 datapoints
2025-03-06 20:34:24,199 - INFO - training batch 51, loss: 0.128, 1632/60000 datapoints
2025-03-06 20:34:24,393 - INFO - training batch 101, loss: 0.215, 3232/60000 datapoints
2025-03-06 20:34:24,604 - INFO - training batch 151, loss: 0.203, 4832/60000 datapoints
2025-03-06 20:34:24,802 - INFO - training batch 201, loss: 0.183, 6432/60000 datapoints
2025-03-06 20:34:25,008 - INFO - training batch 251, loss: 0.271, 8032/60000 datapoints
2025-03-06 20:34:25,208 - INFO - training batch 301, loss: 0.087, 9632/60000 datapoints
2025-03-06 20:34:25,419 - INFO - training batch 351, loss: 0.197, 11232/60000 datapoints
2025-03-06 20:34:25,616 - INFO - training batch 401, loss: 0.300, 12832/60000 datapoints
2025-03-06 20:34:25,816 - INFO - training batch 451, loss: 0.131, 14432/60000 datapoints
2025-03-06 20:34:26,013 - INFO - training batch 501, loss: 0.289, 16032/60000 datapoints
2025-03-06 20:34:26,209 - INFO - training batch 551, loss: 0.201, 17632/60000 datapoints
2025-03-06 20:34:26,404 - INFO - training batch 601, loss: 0.460, 19232/60000 datapoints
2025-03-06 20:34:26,601 - INFO - training batch 651, loss: 0.103, 20832/60000 datapoints
2025-03-06 20:34:26,801 - INFO - training batch 701, loss: 0.149, 22432/60000 datapoints
2025-03-06 20:34:27,000 - INFO - training batch 751, loss: 0.096, 24032/60000 datapoints
2025-03-06 20:34:27,195 - INFO - training batch 801, loss: 0.483, 25632/60000 datapoints
2025-03-06 20:34:27,398 - INFO - training batch 851, loss: 0.116, 27232/60000 datapoints
2025-03-06 20:34:27,594 - INFO - training batch 901, loss: 0.192, 28832/60000 datapoints
2025-03-06 20:34:27,793 - INFO - training batch 951, loss: 0.134, 30432/60000 datapoints
2025-03-06 20:34:27,988 - INFO - training batch 1001, loss: 0.087, 32032/60000 datapoints
2025-03-06 20:34:28,183 - INFO - training batch 1051, loss: 0.353, 33632/60000 datapoints
2025-03-06 20:34:28,374 - INFO - training batch 1101, loss: 0.169, 35232/60000 datapoints
2025-03-06 20:34:28,571 - INFO - training batch 1151, loss: 0.067, 36832/60000 datapoints
2025-03-06 20:34:28,770 - INFO - training batch 1201, loss: 0.116, 38432/60000 datapoints
2025-03-06 20:34:28,968 - INFO - training batch 1251, loss: 0.348, 40032/60000 datapoints
2025-03-06 20:34:29,164 - INFO - training batch 1301, loss: 0.292, 41632/60000 datapoints
2025-03-06 20:34:29,359 - INFO - training batch 1351, loss: 0.379, 43232/60000 datapoints
2025-03-06 20:34:29,554 - INFO - training batch 1401, loss: 0.243, 44832/60000 datapoints
2025-03-06 20:34:29,750 - INFO - training batch 1451, loss: 0.061, 46432/60000 datapoints
2025-03-06 20:34:29,948 - INFO - training batch 1501, loss: 0.436, 48032/60000 datapoints
2025-03-06 20:34:30,145 - INFO - training batch 1551, loss: 0.400, 49632/60000 datapoints
2025-03-06 20:34:30,342 - INFO - training batch 1601, loss: 0.197, 51232/60000 datapoints
2025-03-06 20:34:30,538 - INFO - training batch 1651, loss: 0.210, 52832/60000 datapoints
2025-03-06 20:34:30,745 - INFO - training batch 1701, loss: 0.221, 54432/60000 datapoints
2025-03-06 20:34:30,944 - INFO - training batch 1751, loss: 0.082, 56032/60000 datapoints
2025-03-06 20:34:31,139 - INFO - training batch 1801, loss: 0.190, 57632/60000 datapoints
2025-03-06 20:34:31,333 - INFO - training batch 1851, loss: 0.187, 59232/60000 datapoints
2025-03-06 20:34:31,438 - INFO - validation batch 1, loss: 0.164, 32/10016 datapoints
2025-03-06 20:34:31,593 - INFO - validation batch 51, loss: 0.338, 1632/10016 datapoints
2025-03-06 20:34:31,750 - INFO - validation batch 101, loss: 0.110, 3232/10016 datapoints
2025-03-06 20:34:31,908 - INFO - validation batch 151, loss: 0.221, 4832/10016 datapoints
2025-03-06 20:34:32,079 - INFO - validation batch 201, loss: 0.348, 6432/10016 datapoints
2025-03-06 20:34:32,231 - INFO - validation batch 251, loss: 0.306, 8032/10016 datapoints
2025-03-06 20:34:32,386 - INFO - validation batch 301, loss: 0.157, 9632/10016 datapoints
2025-03-06 20:34:32,423 - INFO - Epoch 778/800 done.
2025-03-06 20:34:32,423 - INFO - Final validation performance:
Loss: 0.235, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:34:32,424 - INFO - Beginning epoch 779/800
2025-03-06 20:34:32,432 - INFO - training batch 1, loss: 0.113, 32/60000 datapoints
2025-03-06 20:34:32,641 - INFO - training batch 51, loss: 0.122, 1632/60000 datapoints
2025-03-06 20:34:32,833 - INFO - training batch 101, loss: 0.525, 3232/60000 datapoints
2025-03-06 20:34:33,035 - INFO - training batch 151, loss: 0.138, 4832/60000 datapoints
2025-03-06 20:34:33,232 - INFO - training batch 201, loss: 0.272, 6432/60000 datapoints
2025-03-06 20:34:33,427 - INFO - training batch 251, loss: 0.068, 8032/60000 datapoints
2025-03-06 20:34:33,619 - INFO - training batch 301, loss: 0.055, 9632/60000 datapoints
2025-03-06 20:34:33,811 - INFO - training batch 351, loss: 0.203, 11232/60000 datapoints
2025-03-06 20:34:34,004 - INFO - training batch 401, loss: 0.175, 12832/60000 datapoints
2025-03-06 20:34:34,193 - INFO - training batch 451, loss: 0.387, 14432/60000 datapoints
2025-03-06 20:34:34,387 - INFO - training batch 501, loss: 0.128, 16032/60000 datapoints
2025-03-06 20:34:34,578 - INFO - training batch 551, loss: 0.306, 17632/60000 datapoints
2025-03-06 20:34:34,771 - INFO - training batch 601, loss: 0.156, 19232/60000 datapoints
2025-03-06 20:34:34,971 - INFO - training batch 651, loss: 0.062, 20832/60000 datapoints
2025-03-06 20:34:35,164 - INFO - training batch 701, loss: 0.068, 22432/60000 datapoints
2025-03-06 20:34:35,356 - INFO - training batch 751, loss: 0.095, 24032/60000 datapoints
2025-03-06 20:34:35,549 - INFO - training batch 801, loss: 0.294, 25632/60000 datapoints
2025-03-06 20:34:35,748 - INFO - training batch 851, loss: 0.070, 27232/60000 datapoints
2025-03-06 20:34:35,942 - INFO - training batch 901, loss: 0.261, 28832/60000 datapoints
2025-03-06 20:34:36,135 - INFO - training batch 951, loss: 0.100, 30432/60000 datapoints
2025-03-06 20:34:36,327 - INFO - training batch 1001, loss: 0.411, 32032/60000 datapoints
2025-03-06 20:34:36,517 - INFO - training batch 1051, loss: 0.210, 33632/60000 datapoints
2025-03-06 20:34:36,714 - INFO - training batch 1101, loss: 0.421, 35232/60000 datapoints
2025-03-06 20:34:36,906 - INFO - training batch 1151, loss: 0.400, 36832/60000 datapoints
2025-03-06 20:34:37,106 - INFO - training batch 1201, loss: 0.173, 38432/60000 datapoints
2025-03-06 20:34:37,299 - INFO - training batch 1251, loss: 0.232, 40032/60000 datapoints
2025-03-06 20:34:37,495 - INFO - training batch 1301, loss: 0.416, 41632/60000 datapoints
2025-03-06 20:34:37,701 - INFO - training batch 1351, loss: 0.217, 43232/60000 datapoints
2025-03-06 20:34:37,893 - INFO - training batch 1401, loss: 0.248, 44832/60000 datapoints
2025-03-06 20:34:38,088 - INFO - training batch 1451, loss: 0.339, 46432/60000 datapoints
2025-03-06 20:34:38,280 - INFO - training batch 1501, loss: 0.088, 48032/60000 datapoints
2025-03-06 20:34:38,473 - INFO - training batch 1551, loss: 0.071, 49632/60000 datapoints
2025-03-06 20:34:38,669 - INFO - training batch 1601, loss: 0.113, 51232/60000 datapoints
2025-03-06 20:34:38,867 - INFO - training batch 1651, loss: 0.118, 52832/60000 datapoints
2025-03-06 20:34:39,065 - INFO - training batch 1701, loss: 0.224, 54432/60000 datapoints
2025-03-06 20:34:39,259 - INFO - training batch 1751, loss: 0.250, 56032/60000 datapoints
2025-03-06 20:34:39,448 - INFO - training batch 1801, loss: 0.168, 57632/60000 datapoints
2025-03-06 20:34:39,643 - INFO - training batch 1851, loss: 0.078, 59232/60000 datapoints
2025-03-06 20:34:39,742 - INFO - validation batch 1, loss: 0.361, 32/10016 datapoints
2025-03-06 20:34:39,894 - INFO - validation batch 51, loss: 0.109, 1632/10016 datapoints
2025-03-06 20:34:40,044 - INFO - validation batch 101, loss: 0.289, 3232/10016 datapoints
2025-03-06 20:34:40,197 - INFO - validation batch 151, loss: 0.265, 4832/10016 datapoints
2025-03-06 20:34:40,349 - INFO - validation batch 201, loss: 0.116, 6432/10016 datapoints
2025-03-06 20:34:40,499 - INFO - validation batch 251, loss: 0.092, 8032/10016 datapoints
2025-03-06 20:34:40,652 - INFO - validation batch 301, loss: 0.248, 9632/10016 datapoints
2025-03-06 20:34:40,688 - INFO - Epoch 779/800 done.
2025-03-06 20:34:40,688 - INFO - Final validation performance:
Loss: 0.212, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:34:40,688 - INFO - Beginning epoch 780/800
2025-03-06 20:34:40,695 - INFO - training batch 1, loss: 0.103, 32/60000 datapoints
2025-03-06 20:34:40,885 - INFO - training batch 51, loss: 0.133, 1632/60000 datapoints
2025-03-06 20:34:41,079 - INFO - training batch 101, loss: 0.134, 3232/60000 datapoints
2025-03-06 20:34:41,282 - INFO - training batch 151, loss: 0.059, 4832/60000 datapoints
2025-03-06 20:34:41,477 - INFO - training batch 201, loss: 0.468, 6432/60000 datapoints
2025-03-06 20:34:41,677 - INFO - training batch 251, loss: 0.068, 8032/60000 datapoints
2025-03-06 20:34:41,874 - INFO - training batch 301, loss: 0.245, 9632/60000 datapoints
2025-03-06 20:34:42,071 - INFO - training batch 351, loss: 0.201, 11232/60000 datapoints
2025-03-06 20:34:42,282 - INFO - training batch 401, loss: 0.226, 12832/60000 datapoints
2025-03-06 20:34:42,480 - INFO - training batch 451, loss: 0.227, 14432/60000 datapoints
2025-03-06 20:34:42,676 - INFO - training batch 501, loss: 0.268, 16032/60000 datapoints
2025-03-06 20:34:42,875 - INFO - training batch 551, loss: 0.225, 17632/60000 datapoints
2025-03-06 20:34:43,087 - INFO - training batch 601, loss: 0.613, 19232/60000 datapoints
2025-03-06 20:34:43,283 - INFO - training batch 651, loss: 0.438, 20832/60000 datapoints
2025-03-06 20:34:43,475 - INFO - training batch 701, loss: 0.311, 22432/60000 datapoints
2025-03-06 20:34:43,704 - INFO - training batch 751, loss: 0.125, 24032/60000 datapoints
2025-03-06 20:34:43,898 - INFO - training batch 801, loss: 0.147, 25632/60000 datapoints
2025-03-06 20:34:44,091 - INFO - training batch 851, loss: 0.147, 27232/60000 datapoints
2025-03-06 20:34:44,283 - INFO - training batch 901, loss: 0.115, 28832/60000 datapoints
2025-03-06 20:34:44,474 - INFO - training batch 951, loss: 0.125, 30432/60000 datapoints
2025-03-06 20:34:44,668 - INFO - training batch 1001, loss: 0.302, 32032/60000 datapoints
2025-03-06 20:34:44,861 - INFO - training batch 1051, loss: 0.360, 33632/60000 datapoints
2025-03-06 20:34:45,057 - INFO - training batch 1101, loss: 0.091, 35232/60000 datapoints
2025-03-06 20:34:45,249 - INFO - training batch 1151, loss: 0.348, 36832/60000 datapoints
2025-03-06 20:34:45,441 - INFO - training batch 1201, loss: 0.354, 38432/60000 datapoints
2025-03-06 20:34:45,635 - INFO - training batch 1251, loss: 0.133, 40032/60000 datapoints
2025-03-06 20:34:45,826 - INFO - training batch 1301, loss: 0.140, 41632/60000 datapoints
2025-03-06 20:34:46,018 - INFO - training batch 1351, loss: 0.220, 43232/60000 datapoints
2025-03-06 20:34:46,210 - INFO - training batch 1401, loss: 0.091, 44832/60000 datapoints
2025-03-06 20:34:46,400 - INFO - training batch 1451, loss: 0.270, 46432/60000 datapoints
2025-03-06 20:34:46,592 - INFO - training batch 1501, loss: 0.260, 48032/60000 datapoints
2025-03-06 20:34:46,785 - INFO - training batch 1551, loss: 0.202, 49632/60000 datapoints
2025-03-06 20:34:46,978 - INFO - training batch 1601, loss: 0.192, 51232/60000 datapoints
2025-03-06 20:34:47,176 - INFO - training batch 1651, loss: 0.264, 52832/60000 datapoints
2025-03-06 20:34:47,375 - INFO - training batch 1701, loss: 0.339, 54432/60000 datapoints
2025-03-06 20:34:47,571 - INFO - training batch 1751, loss: 0.176, 56032/60000 datapoints
2025-03-06 20:34:47,766 - INFO - training batch 1801, loss: 0.218, 57632/60000 datapoints
2025-03-06 20:34:47,959 - INFO - training batch 1851, loss: 0.361, 59232/60000 datapoints
2025-03-06 20:34:48,057 - INFO - validation batch 1, loss: 0.220, 32/10016 datapoints
2025-03-06 20:34:48,209 - INFO - validation batch 51, loss: 0.089, 1632/10016 datapoints
2025-03-06 20:34:48,360 - INFO - validation batch 101, loss: 0.326, 3232/10016 datapoints
2025-03-06 20:34:48,509 - INFO - validation batch 151, loss: 0.236, 4832/10016 datapoints
2025-03-06 20:34:48,662 - INFO - validation batch 201, loss: 0.113, 6432/10016 datapoints
2025-03-06 20:34:48,814 - INFO - validation batch 251, loss: 0.278, 8032/10016 datapoints
2025-03-06 20:34:48,966 - INFO - validation batch 301, loss: 0.056, 9632/10016 datapoints
2025-03-06 20:34:49,004 - INFO - Epoch 780/800 done.
2025-03-06 20:34:49,004 - INFO - Final validation performance:
Loss: 0.188, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:34:49,005 - INFO - Beginning epoch 781/800
2025-03-06 20:34:49,011 - INFO - training batch 1, loss: 0.089, 32/60000 datapoints
2025-03-06 20:34:49,216 - INFO - training batch 51, loss: 0.256, 1632/60000 datapoints
2025-03-06 20:34:49,410 - INFO - training batch 101, loss: 0.357, 3232/60000 datapoints
2025-03-06 20:34:49,607 - INFO - training batch 151, loss: 0.237, 4832/60000 datapoints
2025-03-06 20:34:49,808 - INFO - training batch 201, loss: 0.146, 6432/60000 datapoints
2025-03-06 20:34:50,006 - INFO - training batch 251, loss: 0.369, 8032/60000 datapoints
2025-03-06 20:34:50,205 - INFO - training batch 301, loss: 0.293, 9632/60000 datapoints
2025-03-06 20:34:50,401 - INFO - training batch 351, loss: 0.277, 11232/60000 datapoints
2025-03-06 20:34:50,597 - INFO - training batch 401, loss: 0.156, 12832/60000 datapoints
2025-03-06 20:34:50,797 - INFO - training batch 451, loss: 0.141, 14432/60000 datapoints
2025-03-06 20:34:50,992 - INFO - training batch 501, loss: 0.384, 16032/60000 datapoints
2025-03-06 20:34:51,189 - INFO - training batch 551, loss: 0.239, 17632/60000 datapoints
2025-03-06 20:34:51,384 - INFO - training batch 601, loss: 0.283, 19232/60000 datapoints
2025-03-06 20:34:51,585 - INFO - training batch 651, loss: 0.217, 20832/60000 datapoints
2025-03-06 20:34:51,790 - INFO - training batch 701, loss: 0.196, 22432/60000 datapoints
2025-03-06 20:34:51,987 - INFO - training batch 751, loss: 0.272, 24032/60000 datapoints
2025-03-06 20:34:52,189 - INFO - training batch 801, loss: 0.181, 25632/60000 datapoints
2025-03-06 20:34:52,399 - INFO - training batch 851, loss: 0.202, 27232/60000 datapoints
2025-03-06 20:34:52,594 - INFO - training batch 901, loss: 0.531, 28832/60000 datapoints
2025-03-06 20:34:52,791 - INFO - training batch 951, loss: 0.316, 30432/60000 datapoints
2025-03-06 20:34:52,986 - INFO - training batch 1001, loss: 0.206, 32032/60000 datapoints
2025-03-06 20:34:53,181 - INFO - training batch 1051, loss: 0.391, 33632/60000 datapoints
2025-03-06 20:34:53,375 - INFO - training batch 1101, loss: 0.240, 35232/60000 datapoints
2025-03-06 20:34:53,569 - INFO - training batch 1151, loss: 0.110, 36832/60000 datapoints
2025-03-06 20:34:53,766 - INFO - training batch 1201, loss: 0.517, 38432/60000 datapoints
2025-03-06 20:34:53,961 - INFO - training batch 1251, loss: 0.163, 40032/60000 datapoints
2025-03-06 20:34:54,157 - INFO - training batch 1301, loss: 0.176, 41632/60000 datapoints
2025-03-06 20:34:54,350 - INFO - training batch 1351, loss: 0.319, 43232/60000 datapoints
2025-03-06 20:34:54,544 - INFO - training batch 1401, loss: 0.204, 44832/60000 datapoints
2025-03-06 20:34:54,742 - INFO - training batch 1451, loss: 0.180, 46432/60000 datapoints
2025-03-06 20:34:54,940 - INFO - training batch 1501, loss: 0.216, 48032/60000 datapoints
2025-03-06 20:34:55,137 - INFO - training batch 1551, loss: 0.252, 49632/60000 datapoints
2025-03-06 20:34:55,332 - INFO - training batch 1601, loss: 0.066, 51232/60000 datapoints
2025-03-06 20:34:55,527 - INFO - training batch 1651, loss: 0.245, 52832/60000 datapoints
2025-03-06 20:34:55,724 - INFO - training batch 1701, loss: 0.131, 54432/60000 datapoints
2025-03-06 20:34:55,917 - INFO - training batch 1751, loss: 0.137, 56032/60000 datapoints
2025-03-06 20:34:56,112 - INFO - training batch 1801, loss: 0.238, 57632/60000 datapoints
2025-03-06 20:34:56,303 - INFO - training batch 1851, loss: 0.069, 59232/60000 datapoints
2025-03-06 20:34:56,407 - INFO - validation batch 1, loss: 0.442, 32/10016 datapoints
2025-03-06 20:34:56,561 - INFO - validation batch 51, loss: 0.526, 1632/10016 datapoints
2025-03-06 20:34:56,716 - INFO - validation batch 101, loss: 0.164, 3232/10016 datapoints
2025-03-06 20:34:56,869 - INFO - validation batch 151, loss: 0.102, 4832/10016 datapoints
2025-03-06 20:34:57,025 - INFO - validation batch 201, loss: 0.108, 6432/10016 datapoints
2025-03-06 20:34:57,176 - INFO - validation batch 251, loss: 0.266, 8032/10016 datapoints
2025-03-06 20:34:57,330 - INFO - validation batch 301, loss: 0.180, 9632/10016 datapoints
2025-03-06 20:34:57,367 - INFO - Epoch 781/800 done.
2025-03-06 20:34:57,368 - INFO - Final validation performance:
Loss: 0.255, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:34:57,368 - INFO - Beginning epoch 782/800
2025-03-06 20:34:57,376 - INFO - training batch 1, loss: 0.490, 32/60000 datapoints
2025-03-06 20:34:57,583 - INFO - training batch 51, loss: 0.387, 1632/60000 datapoints
2025-03-06 20:34:57,777 - INFO - training batch 101, loss: 0.157, 3232/60000 datapoints
2025-03-06 20:34:57,977 - INFO - training batch 151, loss: 0.426, 4832/60000 datapoints
2025-03-06 20:34:58,172 - INFO - training batch 201, loss: 0.218, 6432/60000 datapoints
2025-03-06 20:34:58,367 - INFO - training batch 251, loss: 0.540, 8032/60000 datapoints
2025-03-06 20:34:58,560 - INFO - training batch 301, loss: 0.066, 9632/60000 datapoints
2025-03-06 20:34:58,754 - INFO - training batch 351, loss: 0.092, 11232/60000 datapoints
2025-03-06 20:34:58,947 - INFO - training batch 401, loss: 0.122, 12832/60000 datapoints
2025-03-06 20:34:59,142 - INFO - training batch 451, loss: 0.164, 14432/60000 datapoints
2025-03-06 20:34:59,333 - INFO - training batch 501, loss: 0.186, 16032/60000 datapoints
2025-03-06 20:34:59,525 - INFO - training batch 551, loss: 0.275, 17632/60000 datapoints
2025-03-06 20:34:59,722 - INFO - training batch 601, loss: 0.304, 19232/60000 datapoints
2025-03-06 20:34:59,914 - INFO - training batch 651, loss: 0.264, 20832/60000 datapoints
2025-03-06 20:35:00,109 - INFO - training batch 701, loss: 0.105, 22432/60000 datapoints
2025-03-06 20:35:00,300 - INFO - training batch 751, loss: 0.499, 24032/60000 datapoints
2025-03-06 20:35:00,494 - INFO - training batch 801, loss: 0.214, 25632/60000 datapoints
2025-03-06 20:35:00,688 - INFO - training batch 851, loss: 0.217, 27232/60000 datapoints
2025-03-06 20:35:00,879 - INFO - training batch 901, loss: 0.103, 28832/60000 datapoints
2025-03-06 20:35:01,075 - INFO - training batch 951, loss: 0.531, 30432/60000 datapoints
2025-03-06 20:35:01,265 - INFO - training batch 1001, loss: 0.173, 32032/60000 datapoints
2025-03-06 20:35:01,458 - INFO - training batch 1051, loss: 0.126, 33632/60000 datapoints
2025-03-06 20:35:01,652 - INFO - training batch 1101, loss: 0.080, 35232/60000 datapoints
2025-03-06 20:35:01,842 - INFO - training batch 1151, loss: 0.069, 36832/60000 datapoints
2025-03-06 20:35:02,036 - INFO - training batch 1201, loss: 0.107, 38432/60000 datapoints
2025-03-06 20:35:02,227 - INFO - training batch 1251, loss: 0.277, 40032/60000 datapoints
2025-03-06 20:35:02,441 - INFO - training batch 1301, loss: 0.355, 41632/60000 datapoints
2025-03-06 20:35:02,638 - INFO - training batch 1351, loss: 0.131, 43232/60000 datapoints
2025-03-06 20:35:02,831 - INFO - training batch 1401, loss: 0.452, 44832/60000 datapoints
2025-03-06 20:35:03,025 - INFO - training batch 1451, loss: 0.172, 46432/60000 datapoints
2025-03-06 20:35:03,219 - INFO - training batch 1501, loss: 0.243, 48032/60000 datapoints
2025-03-06 20:35:03,411 - INFO - training batch 1551, loss: 0.207, 49632/60000 datapoints
2025-03-06 20:35:03,603 - INFO - training batch 1601, loss: 0.093, 51232/60000 datapoints
2025-03-06 20:35:03,796 - INFO - training batch 1651, loss: 0.052, 52832/60000 datapoints
2025-03-06 20:35:03,989 - INFO - training batch 1701, loss: 0.119, 54432/60000 datapoints
2025-03-06 20:35:04,177 - INFO - training batch 1751, loss: 0.274, 56032/60000 datapoints
2025-03-06 20:35:04,378 - INFO - training batch 1801, loss: 0.139, 57632/60000 datapoints
2025-03-06 20:35:04,571 - INFO - training batch 1851, loss: 0.058, 59232/60000 datapoints
2025-03-06 20:35:04,672 - INFO - validation batch 1, loss: 0.319, 32/10016 datapoints
2025-03-06 20:35:04,822 - INFO - validation batch 51, loss: 0.158, 1632/10016 datapoints
2025-03-06 20:35:04,979 - INFO - validation batch 101, loss: 0.150, 3232/10016 datapoints
2025-03-06 20:35:05,132 - INFO - validation batch 151, loss: 0.646, 4832/10016 datapoints
2025-03-06 20:35:05,283 - INFO - validation batch 201, loss: 0.410, 6432/10016 datapoints
2025-03-06 20:35:05,435 - INFO - validation batch 251, loss: 0.387, 8032/10016 datapoints
2025-03-06 20:35:05,585 - INFO - validation batch 301, loss: 0.074, 9632/10016 datapoints
2025-03-06 20:35:05,620 - INFO - Epoch 782/800 done.
2025-03-06 20:35:05,620 - INFO - Final validation performance:
Loss: 0.306, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:35:05,621 - INFO - Beginning epoch 783/800
2025-03-06 20:35:05,628 - INFO - training batch 1, loss: 0.296, 32/60000 datapoints
2025-03-06 20:35:05,837 - INFO - training batch 51, loss: 0.095, 1632/60000 datapoints
2025-03-06 20:35:06,033 - INFO - training batch 101, loss: 0.333, 3232/60000 datapoints
2025-03-06 20:35:06,226 - INFO - training batch 151, loss: 0.181, 4832/60000 datapoints
2025-03-06 20:35:06,424 - INFO - training batch 201, loss: 0.222, 6432/60000 datapoints
2025-03-06 20:35:06,619 - INFO - training batch 251, loss: 0.150, 8032/60000 datapoints
2025-03-06 20:35:06,817 - INFO - training batch 301, loss: 0.334, 9632/60000 datapoints
2025-03-06 20:35:07,011 - INFO - training batch 351, loss: 0.120, 11232/60000 datapoints
2025-03-06 20:35:07,208 - INFO - training batch 401, loss: 0.313, 12832/60000 datapoints
2025-03-06 20:35:07,401 - INFO - training batch 451, loss: 0.294, 14432/60000 datapoints
2025-03-06 20:35:07,600 - INFO - training batch 501, loss: 0.143, 16032/60000 datapoints
2025-03-06 20:35:07,797 - INFO - training batch 551, loss: 0.088, 17632/60000 datapoints
2025-03-06 20:35:07,991 - INFO - training batch 601, loss: 0.325, 19232/60000 datapoints
2025-03-06 20:35:08,189 - INFO - training batch 651, loss: 0.161, 20832/60000 datapoints
2025-03-06 20:35:08,380 - INFO - training batch 701, loss: 0.234, 22432/60000 datapoints
2025-03-06 20:35:08,572 - INFO - training batch 751, loss: 0.202, 24032/60000 datapoints
2025-03-06 20:35:08,766 - INFO - training batch 801, loss: 0.121, 25632/60000 datapoints
2025-03-06 20:35:08,955 - INFO - training batch 851, loss: 0.134, 27232/60000 datapoints
2025-03-06 20:35:09,162 - INFO - training batch 901, loss: 0.616, 28832/60000 datapoints
2025-03-06 20:35:09,353 - INFO - training batch 951, loss: 0.380, 30432/60000 datapoints
2025-03-06 20:35:09,544 - INFO - training batch 1001, loss: 0.478, 32032/60000 datapoints
2025-03-06 20:35:09,745 - INFO - training batch 1051, loss: 0.249, 33632/60000 datapoints
2025-03-06 20:35:09,936 - INFO - training batch 1101, loss: 0.149, 35232/60000 datapoints
2025-03-06 20:35:10,133 - INFO - training batch 1151, loss: 0.533, 36832/60000 datapoints
2025-03-06 20:35:10,329 - INFO - training batch 1201, loss: 0.311, 38432/60000 datapoints
2025-03-06 20:35:10,526 - INFO - training batch 1251, loss: 0.322, 40032/60000 datapoints
2025-03-06 20:35:10,722 - INFO - training batch 1301, loss: 0.396, 41632/60000 datapoints
2025-03-06 20:35:10,918 - INFO - training batch 1351, loss: 0.325, 43232/60000 datapoints
2025-03-06 20:35:11,115 - INFO - training batch 1401, loss: 0.195, 44832/60000 datapoints
2025-03-06 20:35:11,311 - INFO - training batch 1451, loss: 0.138, 46432/60000 datapoints
2025-03-06 20:35:11,507 - INFO - training batch 1501, loss: 0.060, 48032/60000 datapoints
2025-03-06 20:35:11,705 - INFO - training batch 1551, loss: 0.231, 49632/60000 datapoints
2025-03-06 20:35:11,899 - INFO - training batch 1601, loss: 0.328, 51232/60000 datapoints
2025-03-06 20:35:12,097 - INFO - training batch 1651, loss: 0.323, 52832/60000 datapoints
2025-03-06 20:35:12,292 - INFO - training batch 1701, loss: 0.072, 54432/60000 datapoints
2025-03-06 20:35:12,513 - INFO - training batch 1751, loss: 0.264, 56032/60000 datapoints
2025-03-06 20:35:12,711 - INFO - training batch 1801, loss: 0.228, 57632/60000 datapoints
2025-03-06 20:35:12,906 - INFO - training batch 1851, loss: 0.057, 59232/60000 datapoints
2025-03-06 20:35:13,008 - INFO - validation batch 1, loss: 0.235, 32/10016 datapoints
2025-03-06 20:35:13,164 - INFO - validation batch 51, loss: 0.307, 1632/10016 datapoints
2025-03-06 20:35:13,318 - INFO - validation batch 101, loss: 0.192, 3232/10016 datapoints
2025-03-06 20:35:13,471 - INFO - validation batch 151, loss: 0.128, 4832/10016 datapoints
2025-03-06 20:35:13,626 - INFO - validation batch 201, loss: 0.272, 6432/10016 datapoints
2025-03-06 20:35:13,784 - INFO - validation batch 251, loss: 0.345, 8032/10016 datapoints
2025-03-06 20:35:13,940 - INFO - validation batch 301, loss: 0.168, 9632/10016 datapoints
2025-03-06 20:35:13,976 - INFO - Epoch 783/800 done.
2025-03-06 20:35:13,976 - INFO - Final validation performance:
Loss: 0.235, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:35:13,977 - INFO - Beginning epoch 784/800
2025-03-06 20:35:13,985 - INFO - training batch 1, loss: 0.332, 32/60000 datapoints
2025-03-06 20:35:14,183 - INFO - training batch 51, loss: 0.093, 1632/60000 datapoints
2025-03-06 20:35:14,394 - INFO - training batch 101, loss: 0.205, 3232/60000 datapoints
2025-03-06 20:35:14,589 - INFO - training batch 151, loss: 0.168, 4832/60000 datapoints
2025-03-06 20:35:14,791 - INFO - training batch 201, loss: 0.105, 6432/60000 datapoints
2025-03-06 20:35:14,995 - INFO - training batch 251, loss: 0.245, 8032/60000 datapoints
2025-03-06 20:35:15,196 - INFO - training batch 301, loss: 0.110, 9632/60000 datapoints
2025-03-06 20:35:15,390 - INFO - training batch 351, loss: 0.283, 11232/60000 datapoints
2025-03-06 20:35:15,587 - INFO - training batch 401, loss: 0.332, 12832/60000 datapoints
2025-03-06 20:35:15,785 - INFO - training batch 451, loss: 0.395, 14432/60000 datapoints
2025-03-06 20:35:15,979 - INFO - training batch 501, loss: 0.151, 16032/60000 datapoints
2025-03-06 20:35:16,175 - INFO - training batch 551, loss: 0.148, 17632/60000 datapoints
2025-03-06 20:35:16,367 - INFO - training batch 601, loss: 0.210, 19232/60000 datapoints
2025-03-06 20:35:16,562 - INFO - training batch 651, loss: 0.230, 20832/60000 datapoints
2025-03-06 20:35:16,771 - INFO - training batch 701, loss: 0.128, 22432/60000 datapoints
2025-03-06 20:35:16,966 - INFO - training batch 751, loss: 0.190, 24032/60000 datapoints
2025-03-06 20:35:17,165 - INFO - training batch 801, loss: 0.114, 25632/60000 datapoints
2025-03-06 20:35:17,358 - INFO - training batch 851, loss: 0.294, 27232/60000 datapoints
2025-03-06 20:35:17,558 - INFO - training batch 901, loss: 0.159, 28832/60000 datapoints
2025-03-06 20:35:17,759 - INFO - training batch 951, loss: 0.368, 30432/60000 datapoints
2025-03-06 20:35:17,954 - INFO - training batch 1001, loss: 0.060, 32032/60000 datapoints
2025-03-06 20:35:18,149 - INFO - training batch 1051, loss: 0.114, 33632/60000 datapoints
2025-03-06 20:35:18,343 - INFO - training batch 1101, loss: 0.277, 35232/60000 datapoints
2025-03-06 20:35:18,536 - INFO - training batch 1151, loss: 0.118, 36832/60000 datapoints
2025-03-06 20:35:18,734 - INFO - training batch 1201, loss: 0.136, 38432/60000 datapoints
2025-03-06 20:35:18,929 - INFO - training batch 1251, loss: 0.129, 40032/60000 datapoints
2025-03-06 20:35:19,135 - INFO - training batch 1301, loss: 0.386, 41632/60000 datapoints
2025-03-06 20:35:19,329 - INFO - training batch 1351, loss: 0.175, 43232/60000 datapoints
2025-03-06 20:35:19,524 - INFO - training batch 1401, loss: 0.290, 44832/60000 datapoints
2025-03-06 20:35:19,724 - INFO - training batch 1451, loss: 0.403, 46432/60000 datapoints
2025-03-06 20:35:19,920 - INFO - training batch 1501, loss: 0.119, 48032/60000 datapoints
2025-03-06 20:35:20,118 - INFO - training batch 1551, loss: 0.093, 49632/60000 datapoints
2025-03-06 20:35:20,314 - INFO - training batch 1601, loss: 0.302, 51232/60000 datapoints
2025-03-06 20:35:20,508 - INFO - training batch 1651, loss: 0.088, 52832/60000 datapoints
2025-03-06 20:35:20,703 - INFO - training batch 1701, loss: 0.098, 54432/60000 datapoints
2025-03-06 20:35:20,899 - INFO - training batch 1751, loss: 0.464, 56032/60000 datapoints
2025-03-06 20:35:21,094 - INFO - training batch 1801, loss: 0.056, 57632/60000 datapoints
2025-03-06 20:35:21,294 - INFO - training batch 1851, loss: 0.102, 59232/60000 datapoints
2025-03-06 20:35:21,395 - INFO - validation batch 1, loss: 0.136, 32/10016 datapoints
2025-03-06 20:35:21,548 - INFO - validation batch 51, loss: 0.161, 1632/10016 datapoints
2025-03-06 20:35:21,705 - INFO - validation batch 101, loss: 0.083, 3232/10016 datapoints
2025-03-06 20:35:21,858 - INFO - validation batch 151, loss: 0.450, 4832/10016 datapoints
2025-03-06 20:35:22,012 - INFO - validation batch 201, loss: 0.230, 6432/10016 datapoints
2025-03-06 20:35:22,167 - INFO - validation batch 251, loss: 0.338, 8032/10016 datapoints
2025-03-06 20:35:22,318 - INFO - validation batch 301, loss: 0.275, 9632/10016 datapoints
2025-03-06 20:35:22,355 - INFO - Epoch 784/800 done.
2025-03-06 20:35:22,355 - INFO - Final validation performance:
Loss: 0.239, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:35:22,356 - INFO - Beginning epoch 785/800
2025-03-06 20:35:22,363 - INFO - training batch 1, loss: 0.476, 32/60000 datapoints
2025-03-06 20:35:22,592 - INFO - training batch 51, loss: 0.106, 1632/60000 datapoints
2025-03-06 20:35:22,794 - INFO - training batch 101, loss: 0.440, 3232/60000 datapoints
2025-03-06 20:35:22,996 - INFO - training batch 151, loss: 0.197, 4832/60000 datapoints
2025-03-06 20:35:23,197 - INFO - training batch 201, loss: 0.203, 6432/60000 datapoints
2025-03-06 20:35:23,395 - INFO - training batch 251, loss: 0.280, 8032/60000 datapoints
2025-03-06 20:35:23,589 - INFO - training batch 301, loss: 0.110, 9632/60000 datapoints
2025-03-06 20:35:23,785 - INFO - training batch 351, loss: 0.059, 11232/60000 datapoints
2025-03-06 20:35:23,979 - INFO - training batch 401, loss: 0.059, 12832/60000 datapoints
2025-03-06 20:35:24,177 - INFO - training batch 451, loss: 0.144, 14432/60000 datapoints
2025-03-06 20:35:24,371 - INFO - training batch 501, loss: 0.165, 16032/60000 datapoints
2025-03-06 20:35:24,564 - INFO - training batch 551, loss: 0.253, 17632/60000 datapoints
2025-03-06 20:35:24,761 - INFO - training batch 601, loss: 0.211, 19232/60000 datapoints
2025-03-06 20:35:24,962 - INFO - training batch 651, loss: 0.265, 20832/60000 datapoints
2025-03-06 20:35:25,158 - INFO - training batch 701, loss: 0.054, 22432/60000 datapoints
2025-03-06 20:35:25,355 - INFO - training batch 751, loss: 0.081, 24032/60000 datapoints
2025-03-06 20:35:25,550 - INFO - training batch 801, loss: 0.118, 25632/60000 datapoints
2025-03-06 20:35:25,747 - INFO - training batch 851, loss: 0.183, 27232/60000 datapoints
2025-03-06 20:35:25,941 - INFO - training batch 901, loss: 0.342, 28832/60000 datapoints
2025-03-06 20:35:26,134 - INFO - training batch 951, loss: 0.319, 30432/60000 datapoints
2025-03-06 20:35:26,331 - INFO - training batch 1001, loss: 0.147, 32032/60000 datapoints
2025-03-06 20:35:26,526 - INFO - training batch 1051, loss: 0.256, 33632/60000 datapoints
2025-03-06 20:35:26,725 - INFO - training batch 1101, loss: 0.072, 35232/60000 datapoints
2025-03-06 20:35:26,919 - INFO - training batch 1151, loss: 0.262, 36832/60000 datapoints
2025-03-06 20:35:27,115 - INFO - training batch 1201, loss: 0.515, 38432/60000 datapoints
2025-03-06 20:35:27,309 - INFO - training batch 1251, loss: 0.138, 40032/60000 datapoints
2025-03-06 20:35:27,506 - INFO - training batch 1301, loss: 0.071, 41632/60000 datapoints
2025-03-06 20:35:27,706 - INFO - training batch 1351, loss: 0.125, 43232/60000 datapoints
2025-03-06 20:35:27,902 - INFO - training batch 1401, loss: 0.165, 44832/60000 datapoints
2025-03-06 20:35:28,096 - INFO - training batch 1451, loss: 0.184, 46432/60000 datapoints
2025-03-06 20:35:28,291 - INFO - training batch 1501, loss: 0.164, 48032/60000 datapoints
2025-03-06 20:35:28,486 - INFO - training batch 1551, loss: 0.452, 49632/60000 datapoints
2025-03-06 20:35:28,681 - INFO - training batch 1601, loss: 0.159, 51232/60000 datapoints
2025-03-06 20:35:28,873 - INFO - training batch 1651, loss: 0.326, 52832/60000 datapoints
2025-03-06 20:35:29,064 - INFO - training batch 1701, loss: 0.044, 54432/60000 datapoints
2025-03-06 20:35:29,259 - INFO - training batch 1751, loss: 0.243, 56032/60000 datapoints
2025-03-06 20:35:29,453 - INFO - training batch 1801, loss: 0.279, 57632/60000 datapoints
2025-03-06 20:35:29,649 - INFO - training batch 1851, loss: 0.754, 59232/60000 datapoints
2025-03-06 20:35:29,753 - INFO - validation batch 1, loss: 0.367, 32/10016 datapoints
2025-03-06 20:35:29,907 - INFO - validation batch 51, loss: 0.100, 1632/10016 datapoints
2025-03-06 20:35:30,060 - INFO - validation batch 101, loss: 0.281, 3232/10016 datapoints
2025-03-06 20:35:30,214 - INFO - validation batch 151, loss: 0.330, 4832/10016 datapoints
2025-03-06 20:35:30,370 - INFO - validation batch 201, loss: 0.199, 6432/10016 datapoints
2025-03-06 20:35:30,524 - INFO - validation batch 251, loss: 0.138, 8032/10016 datapoints
2025-03-06 20:35:30,678 - INFO - validation batch 301, loss: 0.252, 9632/10016 datapoints
2025-03-06 20:35:30,714 - INFO - Epoch 785/800 done.
2025-03-06 20:35:30,714 - INFO - Final validation performance:
Loss: 0.238, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:35:30,715 - INFO - Beginning epoch 786/800
2025-03-06 20:35:30,724 - INFO - training batch 1, loss: 0.095, 32/60000 datapoints
2025-03-06 20:35:30,936 - INFO - training batch 51, loss: 0.150, 1632/60000 datapoints
2025-03-06 20:35:31,130 - INFO - training batch 101, loss: 0.145, 3232/60000 datapoints
2025-03-06 20:35:31,334 - INFO - training batch 151, loss: 0.183, 4832/60000 datapoints
2025-03-06 20:35:31,528 - INFO - training batch 201, loss: 0.087, 6432/60000 datapoints
2025-03-06 20:35:31,728 - INFO - training batch 251, loss: 0.092, 8032/60000 datapoints
2025-03-06 20:35:31,924 - INFO - training batch 301, loss: 0.253, 9632/60000 datapoints
2025-03-06 20:35:32,118 - INFO - training batch 351, loss: 0.472, 11232/60000 datapoints
2025-03-06 20:35:32,317 - INFO - training batch 401, loss: 0.186, 12832/60000 datapoints
2025-03-06 20:35:32,509 - INFO - training batch 451, loss: 0.272, 14432/60000 datapoints
2025-03-06 20:35:32,725 - INFO - training batch 501, loss: 0.079, 16032/60000 datapoints
2025-03-06 20:35:32,924 - INFO - training batch 551, loss: 0.461, 17632/60000 datapoints
2025-03-06 20:35:33,120 - INFO - training batch 601, loss: 0.283, 19232/60000 datapoints
2025-03-06 20:35:33,318 - INFO - training batch 651, loss: 0.169, 20832/60000 datapoints
2025-03-06 20:35:33,510 - INFO - training batch 701, loss: 0.144, 22432/60000 datapoints
2025-03-06 20:35:33,706 - INFO - training batch 751, loss: 0.380, 24032/60000 datapoints
2025-03-06 20:35:33,902 - INFO - training batch 801, loss: 0.599, 25632/60000 datapoints
2025-03-06 20:35:34,097 - INFO - training batch 851, loss: 0.106, 27232/60000 datapoints
2025-03-06 20:35:34,293 - INFO - training batch 901, loss: 0.243, 28832/60000 datapoints
2025-03-06 20:35:34,490 - INFO - training batch 951, loss: 0.111, 30432/60000 datapoints
2025-03-06 20:35:34,688 - INFO - training batch 1001, loss: 0.225, 32032/60000 datapoints
2025-03-06 20:35:34,889 - INFO - training batch 1051, loss: 0.108, 33632/60000 datapoints
2025-03-06 20:35:35,084 - INFO - training batch 1101, loss: 0.232, 35232/60000 datapoints
2025-03-06 20:35:35,284 - INFO - training batch 1151, loss: 0.064, 36832/60000 datapoints
2025-03-06 20:35:35,478 - INFO - training batch 1201, loss: 0.167, 38432/60000 datapoints
2025-03-06 20:35:35,674 - INFO - training batch 1251, loss: 0.314, 40032/60000 datapoints
2025-03-06 20:35:35,873 - INFO - training batch 1301, loss: 0.150, 41632/60000 datapoints
2025-03-06 20:35:36,068 - INFO - training batch 1351, loss: 0.126, 43232/60000 datapoints
2025-03-06 20:35:36,261 - INFO - training batch 1401, loss: 0.091, 44832/60000 datapoints
2025-03-06 20:35:36,455 - INFO - training batch 1451, loss: 0.327, 46432/60000 datapoints
2025-03-06 20:35:36,653 - INFO - training batch 1501, loss: 0.158, 48032/60000 datapoints
2025-03-06 20:35:36,848 - INFO - training batch 1551, loss: 0.198, 49632/60000 datapoints
2025-03-06 20:35:37,042 - INFO - training batch 1601, loss: 0.278, 51232/60000 datapoints
2025-03-06 20:35:37,242 - INFO - training batch 1651, loss: 0.080, 52832/60000 datapoints
2025-03-06 20:35:37,446 - INFO - training batch 1701, loss: 0.391, 54432/60000 datapoints
2025-03-06 20:35:37,659 - INFO - training batch 1751, loss: 0.141, 56032/60000 datapoints
2025-03-06 20:35:37,856 - INFO - training batch 1801, loss: 0.279, 57632/60000 datapoints
2025-03-06 20:35:38,051 - INFO - training batch 1851, loss: 0.538, 59232/60000 datapoints
2025-03-06 20:35:38,154 - INFO - validation batch 1, loss: 0.095, 32/10016 datapoints
2025-03-06 20:35:38,309 - INFO - validation batch 51, loss: 0.378, 1632/10016 datapoints
2025-03-06 20:35:38,463 - INFO - validation batch 101, loss: 0.287, 3232/10016 datapoints
2025-03-06 20:35:38,614 - INFO - validation batch 151, loss: 0.360, 4832/10016 datapoints
2025-03-06 20:35:38,770 - INFO - validation batch 201, loss: 0.372, 6432/10016 datapoints
2025-03-06 20:35:38,925 - INFO - validation batch 251, loss: 0.225, 8032/10016 datapoints
2025-03-06 20:35:39,080 - INFO - validation batch 301, loss: 0.210, 9632/10016 datapoints
2025-03-06 20:35:39,116 - INFO - Epoch 786/800 done.
2025-03-06 20:35:39,116 - INFO - Final validation performance:
Loss: 0.275, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:35:39,117 - INFO - Beginning epoch 787/800
2025-03-06 20:35:39,124 - INFO - training batch 1, loss: 0.449, 32/60000 datapoints
2025-03-06 20:35:39,323 - INFO - training batch 51, loss: 0.141, 1632/60000 datapoints
2025-03-06 20:35:39,519 - INFO - training batch 101, loss: 0.655, 3232/60000 datapoints
2025-03-06 20:35:39,726 - INFO - training batch 151, loss: 0.467, 4832/60000 datapoints
2025-03-06 20:35:39,924 - INFO - training batch 201, loss: 0.495, 6432/60000 datapoints
2025-03-06 20:35:40,118 - INFO - training batch 251, loss: 0.204, 8032/60000 datapoints
2025-03-06 20:35:40,320 - INFO - training batch 301, loss: 0.096, 9632/60000 datapoints
2025-03-06 20:35:40,518 - INFO - training batch 351, loss: 0.425, 11232/60000 datapoints
2025-03-06 20:35:40,719 - INFO - training batch 401, loss: 0.107, 12832/60000 datapoints
2025-03-06 20:35:40,921 - INFO - training batch 451, loss: 0.110, 14432/60000 datapoints
2025-03-06 20:35:41,116 - INFO - training batch 501, loss: 0.139, 16032/60000 datapoints
2025-03-06 20:35:41,312 - INFO - training batch 551, loss: 0.338, 17632/60000 datapoints
2025-03-06 20:35:41,508 - INFO - training batch 601, loss: 0.224, 19232/60000 datapoints
2025-03-06 20:35:41,705 - INFO - training batch 651, loss: 0.278, 20832/60000 datapoints
2025-03-06 20:35:41,902 - INFO - training batch 701, loss: 0.129, 22432/60000 datapoints
2025-03-06 20:35:42,097 - INFO - training batch 751, loss: 0.210, 24032/60000 datapoints
2025-03-06 20:35:42,293 - INFO - training batch 801, loss: 0.282, 25632/60000 datapoints
2025-03-06 20:35:42,488 - INFO - training batch 851, loss: 0.398, 27232/60000 datapoints
2025-03-06 20:35:42,702 - INFO - training batch 901, loss: 0.167, 28832/60000 datapoints
2025-03-06 20:35:42,903 - INFO - training batch 951, loss: 0.199, 30432/60000 datapoints
2025-03-06 20:35:43,099 - INFO - training batch 1001, loss: 0.109, 32032/60000 datapoints
2025-03-06 20:35:43,297 - INFO - training batch 1051, loss: 0.465, 33632/60000 datapoints
2025-03-06 20:35:43,490 - INFO - training batch 1101, loss: 0.109, 35232/60000 datapoints
2025-03-06 20:35:43,688 - INFO - training batch 1151, loss: 0.187, 36832/60000 datapoints
2025-03-06 20:35:43,883 - INFO - training batch 1201, loss: 0.141, 38432/60000 datapoints
2025-03-06 20:35:44,116 - INFO - training batch 1251, loss: 0.152, 40032/60000 datapoints
2025-03-06 20:35:44,311 - INFO - training batch 1301, loss: 0.141, 41632/60000 datapoints
2025-03-06 20:35:44,504 - INFO - training batch 1351, loss: 0.094, 43232/60000 datapoints
2025-03-06 20:35:44,701 - INFO - training batch 1401, loss: 0.209, 44832/60000 datapoints
2025-03-06 20:35:44,918 - INFO - training batch 1451, loss: 0.256, 46432/60000 datapoints
2025-03-06 20:35:45,112 - INFO - training batch 1501, loss: 0.067, 48032/60000 datapoints
2025-03-06 20:35:45,313 - INFO - training batch 1551, loss: 0.158, 49632/60000 datapoints
2025-03-06 20:35:45,510 - INFO - training batch 1601, loss: 0.320, 51232/60000 datapoints
2025-03-06 20:35:45,706 - INFO - training batch 1651, loss: 0.262, 52832/60000 datapoints
2025-03-06 20:35:45,901 - INFO - training batch 1701, loss: 0.198, 54432/60000 datapoints
2025-03-06 20:35:46,095 - INFO - training batch 1751, loss: 0.054, 56032/60000 datapoints
2025-03-06 20:35:46,288 - INFO - training batch 1801, loss: 0.105, 57632/60000 datapoints
2025-03-06 20:35:46,480 - INFO - training batch 1851, loss: 0.179, 59232/60000 datapoints
2025-03-06 20:35:46,582 - INFO - validation batch 1, loss: 0.243, 32/10016 datapoints
2025-03-06 20:35:46,738 - INFO - validation batch 51, loss: 0.429, 1632/10016 datapoints
2025-03-06 20:35:46,891 - INFO - validation batch 101, loss: 0.445, 3232/10016 datapoints
2025-03-06 20:35:47,045 - INFO - validation batch 151, loss: 0.376, 4832/10016 datapoints
2025-03-06 20:35:47,198 - INFO - validation batch 201, loss: 0.294, 6432/10016 datapoints
2025-03-06 20:35:47,353 - INFO - validation batch 251, loss: 0.379, 8032/10016 datapoints
2025-03-06 20:35:47,507 - INFO - validation batch 301, loss: 0.230, 9632/10016 datapoints
2025-03-06 20:35:47,549 - INFO - Epoch 787/800 done.
2025-03-06 20:35:47,549 - INFO - Final validation performance:
Loss: 0.342, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:35:47,550 - INFO - Beginning epoch 788/800
2025-03-06 20:35:47,557 - INFO - training batch 1, loss: 0.265, 32/60000 datapoints
2025-03-06 20:35:47,770 - INFO - training batch 51, loss: 0.250, 1632/60000 datapoints
2025-03-06 20:35:47,969 - INFO - training batch 101, loss: 0.092, 3232/60000 datapoints
2025-03-06 20:35:48,174 - INFO - training batch 151, loss: 0.150, 4832/60000 datapoints
2025-03-06 20:35:48,372 - INFO - training batch 201, loss: 0.140, 6432/60000 datapoints
2025-03-06 20:35:48,571 - INFO - training batch 251, loss: 0.361, 8032/60000 datapoints
2025-03-06 20:35:48,767 - INFO - training batch 301, loss: 0.078, 9632/60000 datapoints
2025-03-06 20:35:48,962 - INFO - training batch 351, loss: 0.288, 11232/60000 datapoints
2025-03-06 20:35:49,157 - INFO - training batch 401, loss: 0.010, 12832/60000 datapoints
2025-03-06 20:35:49,355 - INFO - training batch 451, loss: 0.217, 14432/60000 datapoints
2025-03-06 20:35:49,551 - INFO - training batch 501, loss: 0.111, 16032/60000 datapoints
2025-03-06 20:35:49,748 - INFO - training batch 551, loss: 0.212, 17632/60000 datapoints
2025-03-06 20:35:49,943 - INFO - training batch 601, loss: 0.080, 19232/60000 datapoints
2025-03-06 20:35:50,140 - INFO - training batch 651, loss: 0.275, 20832/60000 datapoints
2025-03-06 20:35:50,338 - INFO - training batch 701, loss: 0.243, 22432/60000 datapoints
2025-03-06 20:35:50,537 - INFO - training batch 751, loss: 0.337, 24032/60000 datapoints
2025-03-06 20:35:50,733 - INFO - training batch 801, loss: 0.169, 25632/60000 datapoints
2025-03-06 20:35:50,928 - INFO - training batch 851, loss: 0.195, 27232/60000 datapoints
2025-03-06 20:35:51,124 - INFO - training batch 901, loss: 0.054, 28832/60000 datapoints
2025-03-06 20:35:51,323 - INFO - training batch 951, loss: 0.161, 30432/60000 datapoints
2025-03-06 20:35:51,519 - INFO - training batch 1001, loss: 0.089, 32032/60000 datapoints
2025-03-06 20:35:51,714 - INFO - training batch 1051, loss: 0.207, 33632/60000 datapoints
2025-03-06 20:35:51,908 - INFO - training batch 1101, loss: 0.246, 35232/60000 datapoints
2025-03-06 20:35:52,109 - INFO - training batch 1151, loss: 0.244, 36832/60000 datapoints
2025-03-06 20:35:52,304 - INFO - training batch 1201, loss: 0.253, 38432/60000 datapoints
2025-03-06 20:35:52,497 - INFO - training batch 1251, loss: 0.071, 40032/60000 datapoints
2025-03-06 20:35:52,694 - INFO - training batch 1301, loss: 0.201, 41632/60000 datapoints
2025-03-06 20:35:52,911 - INFO - training batch 1351, loss: 0.138, 43232/60000 datapoints
2025-03-06 20:35:53,107 - INFO - training batch 1401, loss: 0.261, 44832/60000 datapoints
2025-03-06 20:35:53,303 - INFO - training batch 1451, loss: 0.346, 46432/60000 datapoints
2025-03-06 20:35:53,497 - INFO - training batch 1501, loss: 0.254, 48032/60000 datapoints
2025-03-06 20:35:53,695 - INFO - training batch 1551, loss: 0.069, 49632/60000 datapoints
2025-03-06 20:35:53,891 - INFO - training batch 1601, loss: 0.167, 51232/60000 datapoints
2025-03-06 20:35:54,088 - INFO - training batch 1651, loss: 0.206, 52832/60000 datapoints
2025-03-06 20:35:54,282 - INFO - training batch 1701, loss: 0.056, 54432/60000 datapoints
2025-03-06 20:35:54,476 - INFO - training batch 1751, loss: 0.204, 56032/60000 datapoints
2025-03-06 20:35:54,674 - INFO - training batch 1801, loss: 0.183, 57632/60000 datapoints
2025-03-06 20:35:54,870 - INFO - training batch 1851, loss: 0.057, 59232/60000 datapoints
2025-03-06 20:35:54,972 - INFO - validation batch 1, loss: 0.142, 32/10016 datapoints
2025-03-06 20:35:55,127 - INFO - validation batch 51, loss: 0.065, 1632/10016 datapoints
2025-03-06 20:35:55,284 - INFO - validation batch 101, loss: 0.082, 3232/10016 datapoints
2025-03-06 20:35:55,438 - INFO - validation batch 151, loss: 0.066, 4832/10016 datapoints
2025-03-06 20:35:55,591 - INFO - validation batch 201, loss: 0.231, 6432/10016 datapoints
2025-03-06 20:35:55,748 - INFO - validation batch 251, loss: 0.102, 8032/10016 datapoints
2025-03-06 20:35:55,899 - INFO - validation batch 301, loss: 0.181, 9632/10016 datapoints
2025-03-06 20:35:55,938 - INFO - Epoch 788/800 done.
2025-03-06 20:35:55,938 - INFO - Final validation performance:
Loss: 0.124, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:35:55,939 - INFO - Beginning epoch 789/800
2025-03-06 20:35:55,946 - INFO - training batch 1, loss: 0.268, 32/60000 datapoints
2025-03-06 20:35:56,157 - INFO - training batch 51, loss: 0.302, 1632/60000 datapoints
2025-03-06 20:35:56,353 - INFO - training batch 101, loss: 0.157, 3232/60000 datapoints
2025-03-06 20:35:56,552 - INFO - training batch 151, loss: 0.116, 4832/60000 datapoints
2025-03-06 20:35:56,750 - INFO - training batch 201, loss: 0.063, 6432/60000 datapoints
2025-03-06 20:35:56,949 - INFO - training batch 251, loss: 0.328, 8032/60000 datapoints
2025-03-06 20:35:57,145 - INFO - training batch 301, loss: 0.044, 9632/60000 datapoints
2025-03-06 20:35:57,344 - INFO - training batch 351, loss: 0.206, 11232/60000 datapoints
2025-03-06 20:35:57,537 - INFO - training batch 401, loss: 0.137, 12832/60000 datapoints
2025-03-06 20:35:57,737 - INFO - training batch 451, loss: 0.180, 14432/60000 datapoints
2025-03-06 20:35:57,932 - INFO - training batch 501, loss: 0.071, 16032/60000 datapoints
2025-03-06 20:35:58,127 - INFO - training batch 551, loss: 0.153, 17632/60000 datapoints
2025-03-06 20:35:58,321 - INFO - training batch 601, loss: 0.157, 19232/60000 datapoints
2025-03-06 20:35:58,516 - INFO - training batch 651, loss: 0.214, 20832/60000 datapoints
2025-03-06 20:35:58,714 - INFO - training batch 701, loss: 0.117, 22432/60000 datapoints
2025-03-06 20:35:58,906 - INFO - training batch 751, loss: 0.181, 24032/60000 datapoints
2025-03-06 20:35:59,102 - INFO - training batch 801, loss: 0.082, 25632/60000 datapoints
2025-03-06 20:35:59,297 - INFO - training batch 851, loss: 0.388, 27232/60000 datapoints
2025-03-06 20:35:59,490 - INFO - training batch 901, loss: 0.268, 28832/60000 datapoints
2025-03-06 20:35:59,687 - INFO - training batch 951, loss: 0.227, 30432/60000 datapoints
2025-03-06 20:35:59,880 - INFO - training batch 1001, loss: 0.125, 32032/60000 datapoints
2025-03-06 20:36:00,074 - INFO - training batch 1051, loss: 0.216, 33632/60000 datapoints
2025-03-06 20:36:00,268 - INFO - training batch 1101, loss: 0.274, 35232/60000 datapoints
2025-03-06 20:36:00,464 - INFO - training batch 1151, loss: 0.290, 36832/60000 datapoints
2025-03-06 20:36:00,661 - INFO - training batch 1201, loss: 0.194, 38432/60000 datapoints
2025-03-06 20:36:00,852 - INFO - training batch 1251, loss: 0.145, 40032/60000 datapoints
2025-03-06 20:36:01,045 - INFO - training batch 1301, loss: 0.191, 41632/60000 datapoints
2025-03-06 20:36:01,236 - INFO - training batch 1351, loss: 0.102, 43232/60000 datapoints
2025-03-06 20:36:01,432 - INFO - training batch 1401, loss: 0.548, 44832/60000 datapoints
2025-03-06 20:36:01,627 - INFO - training batch 1451, loss: 0.176, 46432/60000 datapoints
2025-03-06 20:36:01,822 - INFO - training batch 1501, loss: 0.110, 48032/60000 datapoints
2025-03-06 20:36:02,018 - INFO - training batch 1551, loss: 0.114, 49632/60000 datapoints
2025-03-06 20:36:02,211 - INFO - training batch 1601, loss: 0.322, 51232/60000 datapoints
2025-03-06 20:36:02,407 - INFO - training batch 1651, loss: 0.195, 52832/60000 datapoints
2025-03-06 20:36:02,601 - INFO - training batch 1701, loss: 0.095, 54432/60000 datapoints
2025-03-06 20:36:02,800 - INFO - training batch 1751, loss: 0.206, 56032/60000 datapoints
2025-03-06 20:36:03,016 - INFO - training batch 1801, loss: 0.144, 57632/60000 datapoints
2025-03-06 20:36:03,214 - INFO - training batch 1851, loss: 0.276, 59232/60000 datapoints
2025-03-06 20:36:03,323 - INFO - validation batch 1, loss: 0.107, 32/10016 datapoints
2025-03-06 20:36:03,478 - INFO - validation batch 51, loss: 0.040, 1632/10016 datapoints
2025-03-06 20:36:03,634 - INFO - validation batch 101, loss: 0.177, 3232/10016 datapoints
2025-03-06 20:36:03,787 - INFO - validation batch 151, loss: 0.168, 4832/10016 datapoints
2025-03-06 20:36:03,941 - INFO - validation batch 201, loss: 0.085, 6432/10016 datapoints
2025-03-06 20:36:04,095 - INFO - validation batch 251, loss: 0.129, 8032/10016 datapoints
2025-03-06 20:36:04,251 - INFO - validation batch 301, loss: 0.290, 9632/10016 datapoints
2025-03-06 20:36:04,289 - INFO - Epoch 789/800 done.
2025-03-06 20:36:04,289 - INFO - Final validation performance:
Loss: 0.142, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:36:04,289 - INFO - Beginning epoch 790/800
2025-03-06 20:36:04,296 - INFO - training batch 1, loss: 0.265, 32/60000 datapoints
2025-03-06 20:36:04,504 - INFO - training batch 51, loss: 0.220, 1632/60000 datapoints
2025-03-06 20:36:04,703 - INFO - training batch 101, loss: 0.087, 3232/60000 datapoints
2025-03-06 20:36:04,908 - INFO - training batch 151, loss: 0.341, 4832/60000 datapoints
2025-03-06 20:36:05,106 - INFO - training batch 201, loss: 0.187, 6432/60000 datapoints
2025-03-06 20:36:05,306 - INFO - training batch 251, loss: 0.081, 8032/60000 datapoints
2025-03-06 20:36:05,502 - INFO - training batch 301, loss: 0.120, 9632/60000 datapoints
2025-03-06 20:36:05,699 - INFO - training batch 351, loss: 0.107, 11232/60000 datapoints
2025-03-06 20:36:05,894 - INFO - training batch 401, loss: 0.262, 12832/60000 datapoints
2025-03-06 20:36:06,087 - INFO - training batch 451, loss: 0.108, 14432/60000 datapoints
2025-03-06 20:36:06,281 - INFO - training batch 501, loss: 0.323, 16032/60000 datapoints
2025-03-06 20:36:06,476 - INFO - training batch 551, loss: 0.380, 17632/60000 datapoints
2025-03-06 20:36:06,675 - INFO - training batch 601, loss: 0.248, 19232/60000 datapoints
2025-03-06 20:36:06,870 - INFO - training batch 651, loss: 0.097, 20832/60000 datapoints
2025-03-06 20:36:07,066 - INFO - training batch 701, loss: 0.168, 22432/60000 datapoints
2025-03-06 20:36:07,260 - INFO - training batch 751, loss: 0.116, 24032/60000 datapoints
2025-03-06 20:36:07,459 - INFO - training batch 801, loss: 0.119, 25632/60000 datapoints
2025-03-06 20:36:07,662 - INFO - training batch 851, loss: 0.316, 27232/60000 datapoints
2025-03-06 20:36:07,856 - INFO - training batch 901, loss: 0.238, 28832/60000 datapoints
2025-03-06 20:36:08,057 - INFO - training batch 951, loss: 0.161, 30432/60000 datapoints
2025-03-06 20:36:08,252 - INFO - training batch 1001, loss: 0.391, 32032/60000 datapoints
2025-03-06 20:36:08,448 - INFO - training batch 1051, loss: 0.158, 33632/60000 datapoints
2025-03-06 20:36:08,645 - INFO - training batch 1101, loss: 0.240, 35232/60000 datapoints
2025-03-06 20:36:08,839 - INFO - training batch 1151, loss: 0.186, 36832/60000 datapoints
2025-03-06 20:36:09,034 - INFO - training batch 1201, loss: 0.132, 38432/60000 datapoints
2025-03-06 20:36:09,231 - INFO - training batch 1251, loss: 0.149, 40032/60000 datapoints
2025-03-06 20:36:09,431 - INFO - training batch 1301, loss: 0.479, 41632/60000 datapoints
2025-03-06 20:36:09,626 - INFO - training batch 1351, loss: 0.202, 43232/60000 datapoints
2025-03-06 20:36:09,824 - INFO - training batch 1401, loss: 0.086, 44832/60000 datapoints
2025-03-06 20:36:10,017 - INFO - training batch 1451, loss: 0.059, 46432/60000 datapoints
2025-03-06 20:36:10,215 - INFO - training batch 1501, loss: 0.294, 48032/60000 datapoints
2025-03-06 20:36:10,411 - INFO - training batch 1551, loss: 0.121, 49632/60000 datapoints
2025-03-06 20:36:10,606 - INFO - training batch 1601, loss: 0.227, 51232/60000 datapoints
2025-03-06 20:36:10,804 - INFO - training batch 1651, loss: 0.420, 52832/60000 datapoints
2025-03-06 20:36:10,997 - INFO - training batch 1701, loss: 0.091, 54432/60000 datapoints
2025-03-06 20:36:11,193 - INFO - training batch 1751, loss: 0.233, 56032/60000 datapoints
2025-03-06 20:36:11,395 - INFO - training batch 1801, loss: 0.205, 57632/60000 datapoints
2025-03-06 20:36:11,592 - INFO - training batch 1851, loss: 0.100, 59232/60000 datapoints
2025-03-06 20:36:11,695 - INFO - validation batch 1, loss: 0.235, 32/10016 datapoints
2025-03-06 20:36:11,850 - INFO - validation batch 51, loss: 0.197, 1632/10016 datapoints
2025-03-06 20:36:12,002 - INFO - validation batch 101, loss: 0.075, 3232/10016 datapoints
2025-03-06 20:36:12,155 - INFO - validation batch 151, loss: 0.260, 4832/10016 datapoints
2025-03-06 20:36:12,309 - INFO - validation batch 201, loss: 0.106, 6432/10016 datapoints
2025-03-06 20:36:12,462 - INFO - validation batch 251, loss: 0.395, 8032/10016 datapoints
2025-03-06 20:36:12,616 - INFO - validation batch 301, loss: 0.121, 9632/10016 datapoints
2025-03-06 20:36:12,655 - INFO - Epoch 790/800 done.
2025-03-06 20:36:12,655 - INFO - Final validation performance:
Loss: 0.198, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:36:12,656 - INFO - Beginning epoch 791/800
2025-03-06 20:36:12,662 - INFO - training batch 1, loss: 0.352, 32/60000 datapoints
2025-03-06 20:36:12,871 - INFO - training batch 51, loss: 0.317, 1632/60000 datapoints
2025-03-06 20:36:13,086 - INFO - training batch 101, loss: 0.131, 3232/60000 datapoints
2025-03-06 20:36:13,287 - INFO - training batch 151, loss: 0.186, 4832/60000 datapoints
2025-03-06 20:36:13,491 - INFO - training batch 201, loss: 0.156, 6432/60000 datapoints
2025-03-06 20:36:13,693 - INFO - training batch 251, loss: 0.208, 8032/60000 datapoints
2025-03-06 20:36:13,890 - INFO - training batch 301, loss: 0.143, 9632/60000 datapoints
2025-03-06 20:36:14,084 - INFO - training batch 351, loss: 0.449, 11232/60000 datapoints
2025-03-06 20:36:14,281 - INFO - training batch 401, loss: 0.262, 12832/60000 datapoints
2025-03-06 20:36:14,474 - INFO - training batch 451, loss: 0.300, 14432/60000 datapoints
2025-03-06 20:36:14,671 - INFO - training batch 501, loss: 0.231, 16032/60000 datapoints
2025-03-06 20:36:14,869 - INFO - training batch 551, loss: 0.316, 17632/60000 datapoints
2025-03-06 20:36:15,067 - INFO - training batch 601, loss: 0.150, 19232/60000 datapoints
2025-03-06 20:36:15,264 - INFO - training batch 651, loss: 0.149, 20832/60000 datapoints
2025-03-06 20:36:15,464 - INFO - training batch 701, loss: 0.174, 22432/60000 datapoints
2025-03-06 20:36:15,672 - INFO - training batch 751, loss: 0.100, 24032/60000 datapoints
2025-03-06 20:36:15,867 - INFO - training batch 801, loss: 0.217, 25632/60000 datapoints
2025-03-06 20:36:16,062 - INFO - training batch 851, loss: 0.333, 27232/60000 datapoints
2025-03-06 20:36:16,257 - INFO - training batch 901, loss: 0.199, 28832/60000 datapoints
2025-03-06 20:36:16,454 - INFO - training batch 951, loss: 0.066, 30432/60000 datapoints
2025-03-06 20:36:16,650 - INFO - training batch 1001, loss: 0.214, 32032/60000 datapoints
2025-03-06 20:36:16,848 - INFO - training batch 1051, loss: 0.223, 33632/60000 datapoints
2025-03-06 20:36:17,044 - INFO - training batch 1101, loss: 0.272, 35232/60000 datapoints
2025-03-06 20:36:17,238 - INFO - training batch 1151, loss: 0.345, 36832/60000 datapoints
2025-03-06 20:36:17,436 - INFO - training batch 1201, loss: 0.227, 38432/60000 datapoints
2025-03-06 20:36:17,635 - INFO - training batch 1251, loss: 0.260, 40032/60000 datapoints
2025-03-06 20:36:17,829 - INFO - training batch 1301, loss: 0.483, 41632/60000 datapoints
2025-03-06 20:36:18,025 - INFO - training batch 1351, loss: 0.106, 43232/60000 datapoints
2025-03-06 20:36:18,219 - INFO - training batch 1401, loss: 0.069, 44832/60000 datapoints
2025-03-06 20:36:18,416 - INFO - training batch 1451, loss: 0.080, 46432/60000 datapoints
2025-03-06 20:36:18,617 - INFO - training batch 1501, loss: 0.133, 48032/60000 datapoints
2025-03-06 20:36:18,814 - INFO - training batch 1551, loss: 0.583, 49632/60000 datapoints
2025-03-06 20:36:19,009 - INFO - training batch 1601, loss: 0.141, 51232/60000 datapoints
2025-03-06 20:36:19,204 - INFO - training batch 1651, loss: 0.369, 52832/60000 datapoints
2025-03-06 20:36:19,407 - INFO - training batch 1701, loss: 0.409, 54432/60000 datapoints
2025-03-06 20:36:19,602 - INFO - training batch 1751, loss: 0.108, 56032/60000 datapoints
2025-03-06 20:36:19,802 - INFO - training batch 1801, loss: 0.356, 57632/60000 datapoints
2025-03-06 20:36:19,996 - INFO - training batch 1851, loss: 0.439, 59232/60000 datapoints
2025-03-06 20:36:20,098 - INFO - validation batch 1, loss: 0.316, 32/10016 datapoints
2025-03-06 20:36:20,250 - INFO - validation batch 51, loss: 0.069, 1632/10016 datapoints
2025-03-06 20:36:20,404 - INFO - validation batch 101, loss: 0.134, 3232/10016 datapoints
2025-03-06 20:36:20,558 - INFO - validation batch 151, loss: 0.081, 4832/10016 datapoints
2025-03-06 20:36:20,713 - INFO - validation batch 201, loss: 0.115, 6432/10016 datapoints
2025-03-06 20:36:20,866 - INFO - validation batch 251, loss: 0.080, 8032/10016 datapoints
2025-03-06 20:36:21,019 - INFO - validation batch 301, loss: 0.134, 9632/10016 datapoints
2025-03-06 20:36:21,056 - INFO - Epoch 791/800 done.
2025-03-06 20:36:21,056 - INFO - Final validation performance:
Loss: 0.133, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:36:21,057 - INFO - Beginning epoch 792/800
2025-03-06 20:36:21,063 - INFO - training batch 1, loss: 0.230, 32/60000 datapoints
2025-03-06 20:36:21,278 - INFO - training batch 51, loss: 0.455, 1632/60000 datapoints
2025-03-06 20:36:21,481 - INFO - training batch 101, loss: 0.205, 3232/60000 datapoints
2025-03-06 20:36:21,689 - INFO - training batch 151, loss: 0.263, 4832/60000 datapoints
2025-03-06 20:36:21,888 - INFO - training batch 201, loss: 0.243, 6432/60000 datapoints
2025-03-06 20:36:22,087 - INFO - training batch 251, loss: 0.339, 8032/60000 datapoints
2025-03-06 20:36:22,283 - INFO - training batch 301, loss: 0.396, 9632/60000 datapoints
2025-03-06 20:36:22,478 - INFO - training batch 351, loss: 0.235, 11232/60000 datapoints
2025-03-06 20:36:22,678 - INFO - training batch 401, loss: 0.142, 12832/60000 datapoints
2025-03-06 20:36:22,874 - INFO - training batch 451, loss: 0.570, 14432/60000 datapoints
2025-03-06 20:36:23,091 - INFO - training batch 501, loss: 0.393, 16032/60000 datapoints
2025-03-06 20:36:23,288 - INFO - training batch 551, loss: 0.139, 17632/60000 datapoints
2025-03-06 20:36:23,488 - INFO - training batch 601, loss: 0.273, 19232/60000 datapoints
2025-03-06 20:36:23,686 - INFO - training batch 651, loss: 0.293, 20832/60000 datapoints
2025-03-06 20:36:23,882 - INFO - training batch 701, loss: 0.425, 22432/60000 datapoints
2025-03-06 20:36:24,077 - INFO - training batch 751, loss: 0.099, 24032/60000 datapoints
2025-03-06 20:36:24,272 - INFO - training batch 801, loss: 0.157, 25632/60000 datapoints
2025-03-06 20:36:24,467 - INFO - training batch 851, loss: 0.080, 27232/60000 datapoints
2025-03-06 20:36:24,664 - INFO - training batch 901, loss: 0.129, 28832/60000 datapoints
2025-03-06 20:36:24,859 - INFO - training batch 951, loss: 0.283, 30432/60000 datapoints
2025-03-06 20:36:25,058 - INFO - training batch 1001, loss: 0.408, 32032/60000 datapoints
2025-03-06 20:36:25,254 - INFO - training batch 1051, loss: 0.041, 33632/60000 datapoints
2025-03-06 20:36:25,457 - INFO - training batch 1101, loss: 0.119, 35232/60000 datapoints
2025-03-06 20:36:25,659 - INFO - training batch 1151, loss: 0.165, 36832/60000 datapoints
2025-03-06 20:36:25,853 - INFO - training batch 1201, loss: 0.114, 38432/60000 datapoints
2025-03-06 20:36:26,047 - INFO - training batch 1251, loss: 0.221, 40032/60000 datapoints
2025-03-06 20:36:26,243 - INFO - training batch 1301, loss: 0.202, 41632/60000 datapoints
2025-03-06 20:36:26,438 - INFO - training batch 1351, loss: 0.738, 43232/60000 datapoints
2025-03-06 20:36:26,632 - INFO - training batch 1401, loss: 0.149, 44832/60000 datapoints
2025-03-06 20:36:26,826 - INFO - training batch 1451, loss: 0.111, 46432/60000 datapoints
2025-03-06 20:36:27,020 - INFO - training batch 1501, loss: 0.272, 48032/60000 datapoints
2025-03-06 20:36:27,215 - INFO - training batch 1551, loss: 0.185, 49632/60000 datapoints
2025-03-06 20:36:27,412 - INFO - training batch 1601, loss: 0.084, 51232/60000 datapoints
2025-03-06 20:36:27,606 - INFO - training batch 1651, loss: 0.103, 52832/60000 datapoints
2025-03-06 20:36:27,803 - INFO - training batch 1701, loss: 0.331, 54432/60000 datapoints
2025-03-06 20:36:28,001 - INFO - training batch 1751, loss: 0.246, 56032/60000 datapoints
2025-03-06 20:36:28,195 - INFO - training batch 1801, loss: 0.223, 57632/60000 datapoints
2025-03-06 20:36:28,388 - INFO - training batch 1851, loss: 0.147, 59232/60000 datapoints
2025-03-06 20:36:28,492 - INFO - validation batch 1, loss: 0.075, 32/10016 datapoints
2025-03-06 20:36:28,648 - INFO - validation batch 51, loss: 0.156, 1632/10016 datapoints
2025-03-06 20:36:28,801 - INFO - validation batch 101, loss: 0.107, 3232/10016 datapoints
2025-03-06 20:36:28,953 - INFO - validation batch 151, loss: 0.066, 4832/10016 datapoints
2025-03-06 20:36:29,105 - INFO - validation batch 201, loss: 0.208, 6432/10016 datapoints
2025-03-06 20:36:29,258 - INFO - validation batch 251, loss: 0.295, 8032/10016 datapoints
2025-03-06 20:36:29,415 - INFO - validation batch 301, loss: 0.273, 9632/10016 datapoints
2025-03-06 20:36:29,453 - INFO - Epoch 792/800 done.
2025-03-06 20:36:29,453 - INFO - Final validation performance:
Loss: 0.168, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:36:29,454 - INFO - Beginning epoch 793/800
2025-03-06 20:36:29,460 - INFO - training batch 1, loss: 0.108, 32/60000 datapoints
2025-03-06 20:36:29,672 - INFO - training batch 51, loss: 0.270, 1632/60000 datapoints
2025-03-06 20:36:29,868 - INFO - training batch 101, loss: 0.178, 3232/60000 datapoints
2025-03-06 20:36:30,072 - INFO - training batch 151, loss: 0.742, 4832/60000 datapoints
2025-03-06 20:36:30,270 - INFO - training batch 201, loss: 0.146, 6432/60000 datapoints
2025-03-06 20:36:30,470 - INFO - training batch 251, loss: 0.217, 8032/60000 datapoints
2025-03-06 20:36:30,667 - INFO - training batch 301, loss: 0.184, 9632/60000 datapoints
2025-03-06 20:36:30,862 - INFO - training batch 351, loss: 0.162, 11232/60000 datapoints
2025-03-06 20:36:31,059 - INFO - training batch 401, loss: 0.108, 12832/60000 datapoints
2025-03-06 20:36:31,253 - INFO - training batch 451, loss: 0.335, 14432/60000 datapoints
2025-03-06 20:36:31,451 - INFO - training batch 501, loss: 0.592, 16032/60000 datapoints
2025-03-06 20:36:31,648 - INFO - training batch 551, loss: 0.462, 17632/60000 datapoints
2025-03-06 20:36:31,844 - INFO - training batch 601, loss: 0.153, 19232/60000 datapoints
2025-03-06 20:36:32,042 - INFO - training batch 651, loss: 0.230, 20832/60000 datapoints
2025-03-06 20:36:32,235 - INFO - training batch 701, loss: 0.072, 22432/60000 datapoints
2025-03-06 20:36:32,430 - INFO - training batch 751, loss: 0.242, 24032/60000 datapoints
2025-03-06 20:36:32,626 - INFO - training batch 801, loss: 0.168, 25632/60000 datapoints
2025-03-06 20:36:32,823 - INFO - training batch 851, loss: 0.053, 27232/60000 datapoints
2025-03-06 20:36:33,020 - INFO - training batch 901, loss: 0.203, 28832/60000 datapoints
2025-03-06 20:36:33,233 - INFO - training batch 951, loss: 0.393, 30432/60000 datapoints
2025-03-06 20:36:33,431 - INFO - training batch 1001, loss: 0.199, 32032/60000 datapoints
2025-03-06 20:36:33,623 - INFO - training batch 1051, loss: 0.071, 33632/60000 datapoints
2025-03-06 20:36:33,828 - INFO - training batch 1101, loss: 0.185, 35232/60000 datapoints
2025-03-06 20:36:34,025 - INFO - training batch 1151, loss: 0.107, 36832/60000 datapoints
2025-03-06 20:36:34,220 - INFO - training batch 1201, loss: 0.074, 38432/60000 datapoints
2025-03-06 20:36:34,414 - INFO - training batch 1251, loss: 0.193, 40032/60000 datapoints
2025-03-06 20:36:34,608 - INFO - training batch 1301, loss: 0.177, 41632/60000 datapoints
2025-03-06 20:36:34,803 - INFO - training batch 1351, loss: 0.248, 43232/60000 datapoints
2025-03-06 20:36:35,003 - INFO - training batch 1401, loss: 0.140, 44832/60000 datapoints
2025-03-06 20:36:35,197 - INFO - training batch 1451, loss: 0.086, 46432/60000 datapoints
2025-03-06 20:36:35,390 - INFO - training batch 1501, loss: 0.196, 48032/60000 datapoints
2025-03-06 20:36:35,593 - INFO - training batch 1551, loss: 0.240, 49632/60000 datapoints
2025-03-06 20:36:35,790 - INFO - training batch 1601, loss: 0.266, 51232/60000 datapoints
2025-03-06 20:36:35,983 - INFO - training batch 1651, loss: 0.123, 52832/60000 datapoints
2025-03-06 20:36:36,181 - INFO - training batch 1701, loss: 0.395, 54432/60000 datapoints
2025-03-06 20:36:36,381 - INFO - training batch 1751, loss: 0.099, 56032/60000 datapoints
2025-03-06 20:36:36,578 - INFO - training batch 1801, loss: 0.130, 57632/60000 datapoints
2025-03-06 20:36:36,776 - INFO - training batch 1851, loss: 0.117, 59232/60000 datapoints
2025-03-06 20:36:36,878 - INFO - validation batch 1, loss: 0.191, 32/10016 datapoints
2025-03-06 20:36:37,031 - INFO - validation batch 51, loss: 0.363, 1632/10016 datapoints
2025-03-06 20:36:37,183 - INFO - validation batch 101, loss: 0.462, 3232/10016 datapoints
2025-03-06 20:36:37,337 - INFO - validation batch 151, loss: 0.287, 4832/10016 datapoints
2025-03-06 20:36:37,493 - INFO - validation batch 201, loss: 0.225, 6432/10016 datapoints
2025-03-06 20:36:37,682 - INFO - validation batch 251, loss: 0.114, 8032/10016 datapoints
2025-03-06 20:36:37,835 - INFO - validation batch 301, loss: 0.302, 9632/10016 datapoints
2025-03-06 20:36:37,872 - INFO - Epoch 793/800 done.
2025-03-06 20:36:37,872 - INFO - Final validation performance:
Loss: 0.278, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:36:37,873 - INFO - Beginning epoch 794/800
2025-03-06 20:36:37,880 - INFO - training batch 1, loss: 0.078, 32/60000 datapoints
2025-03-06 20:36:38,096 - INFO - training batch 51, loss: 0.084, 1632/60000 datapoints
2025-03-06 20:36:38,289 - INFO - training batch 101, loss: 0.027, 3232/60000 datapoints
2025-03-06 20:36:38,489 - INFO - training batch 151, loss: 0.181, 4832/60000 datapoints
2025-03-06 20:36:38,689 - INFO - training batch 201, loss: 0.535, 6432/60000 datapoints
2025-03-06 20:36:38,885 - INFO - training batch 251, loss: 0.239, 8032/60000 datapoints
2025-03-06 20:36:39,080 - INFO - training batch 301, loss: 0.092, 9632/60000 datapoints
2025-03-06 20:36:39,269 - INFO - training batch 351, loss: 0.056, 11232/60000 datapoints
2025-03-06 20:36:39,463 - INFO - training batch 401, loss: 0.422, 12832/60000 datapoints
2025-03-06 20:36:39,659 - INFO - training batch 451, loss: 0.278, 14432/60000 datapoints
2025-03-06 20:36:39,851 - INFO - training batch 501, loss: 0.046, 16032/60000 datapoints
2025-03-06 20:36:40,044 - INFO - training batch 551, loss: 0.096, 17632/60000 datapoints
2025-03-06 20:36:40,239 - INFO - training batch 601, loss: 0.419, 19232/60000 datapoints
2025-03-06 20:36:40,430 - INFO - training batch 651, loss: 0.352, 20832/60000 datapoints
2025-03-06 20:36:40,624 - INFO - training batch 701, loss: 0.189, 22432/60000 datapoints
2025-03-06 20:36:40,818 - INFO - training batch 751, loss: 0.207, 24032/60000 datapoints
2025-03-06 20:36:41,011 - INFO - training batch 801, loss: 0.265, 25632/60000 datapoints
2025-03-06 20:36:41,201 - INFO - training batch 851, loss: 0.266, 27232/60000 datapoints
2025-03-06 20:36:41,394 - INFO - training batch 901, loss: 0.339, 28832/60000 datapoints
2025-03-06 20:36:41,588 - INFO - training batch 951, loss: 0.089, 30432/60000 datapoints
2025-03-06 20:36:41,782 - INFO - training batch 1001, loss: 0.210, 32032/60000 datapoints
2025-03-06 20:36:41,975 - INFO - training batch 1051, loss: 0.123, 33632/60000 datapoints
2025-03-06 20:36:42,167 - INFO - training batch 1101, loss: 0.162, 35232/60000 datapoints
2025-03-06 20:36:42,362 - INFO - training batch 1151, loss: 0.083, 36832/60000 datapoints
2025-03-06 20:36:42,556 - INFO - training batch 1201, loss: 0.125, 38432/60000 datapoints
2025-03-06 20:36:42,751 - INFO - training batch 1251, loss: 0.144, 40032/60000 datapoints
2025-03-06 20:36:42,944 - INFO - training batch 1301, loss: 0.173, 41632/60000 datapoints
2025-03-06 20:36:43,141 - INFO - training batch 1351, loss: 0.056, 43232/60000 datapoints
2025-03-06 20:36:43,360 - INFO - training batch 1401, loss: 0.302, 44832/60000 datapoints
2025-03-06 20:36:43,556 - INFO - training batch 1451, loss: 0.306, 46432/60000 datapoints
2025-03-06 20:36:43,752 - INFO - training batch 1501, loss: 0.125, 48032/60000 datapoints
2025-03-06 20:36:43,947 - INFO - training batch 1551, loss: 0.254, 49632/60000 datapoints
2025-03-06 20:36:44,141 - INFO - training batch 1601, loss: 0.166, 51232/60000 datapoints
2025-03-06 20:36:44,340 - INFO - training batch 1651, loss: 0.270, 52832/60000 datapoints
2025-03-06 20:36:44,558 - INFO - training batch 1701, loss: 0.221, 54432/60000 datapoints
2025-03-06 20:36:44,753 - INFO - training batch 1751, loss: 0.114, 56032/60000 datapoints
2025-03-06 20:36:44,951 - INFO - training batch 1801, loss: 0.188, 57632/60000 datapoints
2025-03-06 20:36:45,145 - INFO - training batch 1851, loss: 0.124, 59232/60000 datapoints
2025-03-06 20:36:45,246 - INFO - validation batch 1, loss: 0.209, 32/10016 datapoints
2025-03-06 20:36:45,397 - INFO - validation batch 51, loss: 0.189, 1632/10016 datapoints
2025-03-06 20:36:45,554 - INFO - validation batch 101, loss: 0.085, 3232/10016 datapoints
2025-03-06 20:36:45,711 - INFO - validation batch 151, loss: 0.336, 4832/10016 datapoints
2025-03-06 20:36:45,863 - INFO - validation batch 201, loss: 0.283, 6432/10016 datapoints
2025-03-06 20:36:46,012 - INFO - validation batch 251, loss: 0.307, 8032/10016 datapoints
2025-03-06 20:36:46,163 - INFO - validation batch 301, loss: 0.052, 9632/10016 datapoints
2025-03-06 20:36:46,199 - INFO - Epoch 794/800 done.
2025-03-06 20:36:46,199 - INFO - Final validation performance:
Loss: 0.209, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:36:46,200 - INFO - Beginning epoch 795/800
2025-03-06 20:36:46,207 - INFO - training batch 1, loss: 0.285, 32/60000 datapoints
2025-03-06 20:36:46,400 - INFO - training batch 51, loss: 0.130, 1632/60000 datapoints
2025-03-06 20:36:46,592 - INFO - training batch 101, loss: 0.336, 3232/60000 datapoints
2025-03-06 20:36:46,794 - INFO - training batch 151, loss: 0.143, 4832/60000 datapoints
2025-03-06 20:36:46,991 - INFO - training batch 201, loss: 0.276, 6432/60000 datapoints
2025-03-06 20:36:47,183 - INFO - training batch 251, loss: 0.134, 8032/60000 datapoints
2025-03-06 20:36:47,382 - INFO - training batch 301, loss: 0.174, 9632/60000 datapoints
2025-03-06 20:36:47,581 - INFO - training batch 351, loss: 0.217, 11232/60000 datapoints
2025-03-06 20:36:47,785 - INFO - training batch 401, loss: 0.177, 12832/60000 datapoints
2025-03-06 20:36:47,981 - INFO - training batch 451, loss: 0.070, 14432/60000 datapoints
2025-03-06 20:36:48,174 - INFO - training batch 501, loss: 0.063, 16032/60000 datapoints
2025-03-06 20:36:48,368 - INFO - training batch 551, loss: 0.396, 17632/60000 datapoints
2025-03-06 20:36:48,561 - INFO - training batch 601, loss: 0.054, 19232/60000 datapoints
2025-03-06 20:36:48,754 - INFO - training batch 651, loss: 0.435, 20832/60000 datapoints
2025-03-06 20:36:48,948 - INFO - training batch 701, loss: 0.157, 22432/60000 datapoints
2025-03-06 20:36:49,143 - INFO - training batch 751, loss: 0.188, 24032/60000 datapoints
2025-03-06 20:36:49,349 - INFO - training batch 801, loss: 0.424, 25632/60000 datapoints
2025-03-06 20:36:49,546 - INFO - training batch 851, loss: 0.098, 27232/60000 datapoints
2025-03-06 20:36:49,742 - INFO - training batch 901, loss: 0.085, 28832/60000 datapoints
2025-03-06 20:36:49,935 - INFO - training batch 951, loss: 0.067, 30432/60000 datapoints
2025-03-06 20:36:50,129 - INFO - training batch 1001, loss: 0.146, 32032/60000 datapoints
2025-03-06 20:36:50,322 - INFO - training batch 1051, loss: 0.120, 33632/60000 datapoints
2025-03-06 20:36:50,515 - INFO - training batch 1101, loss: 0.059, 35232/60000 datapoints
2025-03-06 20:36:50,713 - INFO - training batch 1151, loss: 0.219, 36832/60000 datapoints
2025-03-06 20:36:50,908 - INFO - training batch 1201, loss: 0.073, 38432/60000 datapoints
2025-03-06 20:36:51,102 - INFO - training batch 1251, loss: 0.039, 40032/60000 datapoints
2025-03-06 20:36:51,296 - INFO - training batch 1301, loss: 0.175, 41632/60000 datapoints
2025-03-06 20:36:51,496 - INFO - training batch 1351, loss: 0.086, 43232/60000 datapoints
2025-03-06 20:36:51,694 - INFO - training batch 1401, loss: 0.152, 44832/60000 datapoints
2025-03-06 20:36:51,892 - INFO - training batch 1451, loss: 0.107, 46432/60000 datapoints
2025-03-06 20:36:52,088 - INFO - training batch 1501, loss: 0.637, 48032/60000 datapoints
2025-03-06 20:36:52,283 - INFO - training batch 1551, loss: 0.107, 49632/60000 datapoints
2025-03-06 20:36:52,512 - INFO - training batch 1601, loss: 0.167, 51232/60000 datapoints
2025-03-06 20:36:52,725 - INFO - training batch 1651, loss: 0.040, 52832/60000 datapoints
2025-03-06 20:36:52,921 - INFO - training batch 1701, loss: 0.287, 54432/60000 datapoints
2025-03-06 20:36:53,119 - INFO - training batch 1751, loss: 0.095, 56032/60000 datapoints
2025-03-06 20:36:53,335 - INFO - training batch 1801, loss: 0.219, 57632/60000 datapoints
2025-03-06 20:36:53,533 - INFO - training batch 1851, loss: 0.116, 59232/60000 datapoints
2025-03-06 20:36:53,639 - INFO - validation batch 1, loss: 0.184, 32/10016 datapoints
2025-03-06 20:36:53,791 - INFO - validation batch 51, loss: 0.234, 1632/10016 datapoints
2025-03-06 20:36:53,945 - INFO - validation batch 101, loss: 0.119, 3232/10016 datapoints
2025-03-06 20:36:54,098 - INFO - validation batch 151, loss: 0.440, 4832/10016 datapoints
2025-03-06 20:36:54,250 - INFO - validation batch 201, loss: 0.290, 6432/10016 datapoints
2025-03-06 20:36:54,403 - INFO - validation batch 251, loss: 0.277, 8032/10016 datapoints
2025-03-06 20:36:54,558 - INFO - validation batch 301, loss: 0.274, 9632/10016 datapoints
2025-03-06 20:36:54,596 - INFO - Epoch 795/800 done.
2025-03-06 20:36:54,596 - INFO - Final validation performance:
Loss: 0.260, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:36:54,597 - INFO - Beginning epoch 796/800
2025-03-06 20:36:54,603 - INFO - training batch 1, loss: 0.260, 32/60000 datapoints
2025-03-06 20:36:54,806 - INFO - training batch 51, loss: 0.274, 1632/60000 datapoints
2025-03-06 20:36:55,002 - INFO - training batch 101, loss: 0.153, 3232/60000 datapoints
2025-03-06 20:36:55,207 - INFO - training batch 151, loss: 0.224, 4832/60000 datapoints
2025-03-06 20:36:55,405 - INFO - training batch 201, loss: 0.107, 6432/60000 datapoints
2025-03-06 20:36:55,605 - INFO - training batch 251, loss: 0.331, 8032/60000 datapoints
2025-03-06 20:36:55,809 - INFO - training batch 301, loss: 0.118, 9632/60000 datapoints
2025-03-06 20:36:56,006 - INFO - training batch 351, loss: 0.246, 11232/60000 datapoints
2025-03-06 20:36:56,203 - INFO - training batch 401, loss: 0.161, 12832/60000 datapoints
2025-03-06 20:36:56,398 - INFO - training batch 451, loss: 0.086, 14432/60000 datapoints
2025-03-06 20:36:56,595 - INFO - training batch 501, loss: 0.158, 16032/60000 datapoints
2025-03-06 20:36:56,802 - INFO - training batch 551, loss: 0.217, 17632/60000 datapoints
2025-03-06 20:36:56,999 - INFO - training batch 601, loss: 0.326, 19232/60000 datapoints
2025-03-06 20:36:57,193 - INFO - training batch 651, loss: 0.197, 20832/60000 datapoints
2025-03-06 20:36:57,391 - INFO - training batch 701, loss: 0.368, 22432/60000 datapoints
2025-03-06 20:36:57,591 - INFO - training batch 751, loss: 0.300, 24032/60000 datapoints
2025-03-06 20:36:57,794 - INFO - training batch 801, loss: 0.149, 25632/60000 datapoints
2025-03-06 20:36:57,993 - INFO - training batch 851, loss: 0.125, 27232/60000 datapoints
2025-03-06 20:36:58,188 - INFO - training batch 901, loss: 0.118, 28832/60000 datapoints
2025-03-06 20:36:58,387 - INFO - training batch 951, loss: 0.124, 30432/60000 datapoints
2025-03-06 20:36:58,585 - INFO - training batch 1001, loss: 0.106, 32032/60000 datapoints
2025-03-06 20:36:58,789 - INFO - training batch 1051, loss: 0.148, 33632/60000 datapoints
2025-03-06 20:36:58,987 - INFO - training batch 1101, loss: 0.193, 35232/60000 datapoints
2025-03-06 20:36:59,183 - INFO - training batch 1151, loss: 0.168, 36832/60000 datapoints
2025-03-06 20:36:59,379 - INFO - training batch 1201, loss: 0.268, 38432/60000 datapoints
2025-03-06 20:36:59,576 - INFO - training batch 1251, loss: 0.169, 40032/60000 datapoints
2025-03-06 20:36:59,775 - INFO - training batch 1301, loss: 0.212, 41632/60000 datapoints
2025-03-06 20:36:59,970 - INFO - training batch 1351, loss: 0.078, 43232/60000 datapoints
2025-03-06 20:37:00,166 - INFO - training batch 1401, loss: 0.244, 44832/60000 datapoints
2025-03-06 20:37:00,359 - INFO - training batch 1451, loss: 0.284, 46432/60000 datapoints
2025-03-06 20:37:00,555 - INFO - training batch 1501, loss: 0.242, 48032/60000 datapoints
2025-03-06 20:37:00,755 - INFO - training batch 1551, loss: 0.366, 49632/60000 datapoints
2025-03-06 20:37:01,029 - INFO - training batch 1601, loss: 0.183, 51232/60000 datapoints
2025-03-06 20:37:01,222 - INFO - training batch 1651, loss: 0.363, 52832/60000 datapoints
2025-03-06 20:37:01,419 - INFO - training batch 1701, loss: 0.086, 54432/60000 datapoints
2025-03-06 20:37:01,616 - INFO - training batch 1751, loss: 0.114, 56032/60000 datapoints
2025-03-06 20:37:01,813 - INFO - training batch 1801, loss: 0.317, 57632/60000 datapoints
2025-03-06 20:37:02,010 - INFO - training batch 1851, loss: 0.244, 59232/60000 datapoints
2025-03-06 20:37:02,114 - INFO - validation batch 1, loss: 0.205, 32/10016 datapoints
2025-03-06 20:37:02,267 - INFO - validation batch 51, loss: 0.073, 1632/10016 datapoints
2025-03-06 20:37:02,421 - INFO - validation batch 101, loss: 0.157, 3232/10016 datapoints
2025-03-06 20:37:02,575 - INFO - validation batch 151, loss: 0.202, 4832/10016 datapoints
2025-03-06 20:37:02,733 - INFO - validation batch 201, loss: 0.230, 6432/10016 datapoints
2025-03-06 20:37:02,887 - INFO - validation batch 251, loss: 0.179, 8032/10016 datapoints
2025-03-06 20:37:03,044 - INFO - validation batch 301, loss: 0.229, 9632/10016 datapoints
2025-03-06 20:37:03,082 - INFO - Epoch 796/800 done.
2025-03-06 20:37:03,082 - INFO - Final validation performance:
Loss: 0.182, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:37:03,082 - INFO - Beginning epoch 797/800
2025-03-06 20:37:03,089 - INFO - training batch 1, loss: 0.379, 32/60000 datapoints
2025-03-06 20:37:03,300 - INFO - training batch 51, loss: 0.065, 1632/60000 datapoints
2025-03-06 20:37:03,523 - INFO - training batch 101, loss: 0.161, 3232/60000 datapoints
2025-03-06 20:37:03,725 - INFO - training batch 151, loss: 0.051, 4832/60000 datapoints
2025-03-06 20:37:03,926 - INFO - training batch 201, loss: 0.052, 6432/60000 datapoints
2025-03-06 20:37:04,123 - INFO - training batch 251, loss: 0.126, 8032/60000 datapoints
2025-03-06 20:37:04,322 - INFO - training batch 301, loss: 0.324, 9632/60000 datapoints
2025-03-06 20:37:04,519 - INFO - training batch 351, loss: 0.145, 11232/60000 datapoints
2025-03-06 20:37:04,719 - INFO - training batch 401, loss: 0.064, 12832/60000 datapoints
2025-03-06 20:37:04,921 - INFO - training batch 451, loss: 0.054, 14432/60000 datapoints
2025-03-06 20:37:05,118 - INFO - training batch 501, loss: 0.401, 16032/60000 datapoints
2025-03-06 20:37:05,315 - INFO - training batch 551, loss: 0.208, 17632/60000 datapoints
2025-03-06 20:37:05,516 - INFO - training batch 601, loss: 0.275, 19232/60000 datapoints
2025-03-06 20:37:05,717 - INFO - training batch 651, loss: 0.085, 20832/60000 datapoints
2025-03-06 20:37:05,912 - INFO - training batch 701, loss: 0.259, 22432/60000 datapoints
2025-03-06 20:37:06,107 - INFO - training batch 751, loss: 0.485, 24032/60000 datapoints
2025-03-06 20:37:06,301 - INFO - training batch 801, loss: 0.111, 25632/60000 datapoints
2025-03-06 20:37:06,499 - INFO - training batch 851, loss: 0.056, 27232/60000 datapoints
2025-03-06 20:37:06,697 - INFO - training batch 901, loss: 0.087, 28832/60000 datapoints
2025-03-06 20:37:06,892 - INFO - training batch 951, loss: 0.428, 30432/60000 datapoints
2025-03-06 20:37:07,088 - INFO - training batch 1001, loss: 0.175, 32032/60000 datapoints
2025-03-06 20:37:07,284 - INFO - training batch 1051, loss: 0.202, 33632/60000 datapoints
2025-03-06 20:37:07,480 - INFO - training batch 1101, loss: 0.054, 35232/60000 datapoints
2025-03-06 20:37:07,683 - INFO - training batch 1151, loss: 0.195, 36832/60000 datapoints
2025-03-06 20:37:07,883 - INFO - training batch 1201, loss: 0.565, 38432/60000 datapoints
2025-03-06 20:37:08,089 - INFO - training batch 1251, loss: 0.366, 40032/60000 datapoints
2025-03-06 20:37:08,282 - INFO - training batch 1301, loss: 0.114, 41632/60000 datapoints
2025-03-06 20:37:08,479 - INFO - training batch 1351, loss: 0.318, 43232/60000 datapoints
2025-03-06 20:37:08,675 - INFO - training batch 1401, loss: 0.169, 44832/60000 datapoints
2025-03-06 20:37:08,872 - INFO - training batch 1451, loss: 0.245, 46432/60000 datapoints
2025-03-06 20:37:09,074 - INFO - training batch 1501, loss: 0.253, 48032/60000 datapoints
2025-03-06 20:37:09,268 - INFO - training batch 1551, loss: 0.251, 49632/60000 datapoints
2025-03-06 20:37:09,463 - INFO - training batch 1601, loss: 0.375, 51232/60000 datapoints
2025-03-06 20:37:09,664 - INFO - training batch 1651, loss: 0.227, 52832/60000 datapoints
2025-03-06 20:37:09,857 - INFO - training batch 1701, loss: 0.297, 54432/60000 datapoints
2025-03-06 20:37:10,054 - INFO - training batch 1751, loss: 0.287, 56032/60000 datapoints
2025-03-06 20:37:10,246 - INFO - training batch 1801, loss: 0.293, 57632/60000 datapoints
2025-03-06 20:37:10,441 - INFO - training batch 1851, loss: 0.442, 59232/60000 datapoints
2025-03-06 20:37:10,544 - INFO - validation batch 1, loss: 0.484, 32/10016 datapoints
2025-03-06 20:37:10,699 - INFO - validation batch 51, loss: 0.130, 1632/10016 datapoints
2025-03-06 20:37:10,852 - INFO - validation batch 101, loss: 0.058, 3232/10016 datapoints
2025-03-06 20:37:11,005 - INFO - validation batch 151, loss: 0.061, 4832/10016 datapoints
2025-03-06 20:37:11,160 - INFO - validation batch 201, loss: 0.149, 6432/10016 datapoints
2025-03-06 20:37:11,313 - INFO - validation batch 251, loss: 0.238, 8032/10016 datapoints
2025-03-06 20:37:11,466 - INFO - validation batch 301, loss: 0.124, 9632/10016 datapoints
2025-03-06 20:37:11,505 - INFO - Epoch 797/800 done.
2025-03-06 20:37:11,505 - INFO - Final validation performance:
Loss: 0.178, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:37:11,506 - INFO - Beginning epoch 798/800
2025-03-06 20:37:11,512 - INFO - training batch 1, loss: 0.087, 32/60000 datapoints
2025-03-06 20:37:11,724 - INFO - training batch 51, loss: 0.058, 1632/60000 datapoints
2025-03-06 20:37:11,920 - INFO - training batch 101, loss: 0.274, 3232/60000 datapoints
2025-03-06 20:37:12,118 - INFO - training batch 151, loss: 0.066, 4832/60000 datapoints
2025-03-06 20:37:12,317 - INFO - training batch 201, loss: 0.267, 6432/60000 datapoints
2025-03-06 20:37:12,514 - INFO - training batch 251, loss: 0.094, 8032/60000 datapoints
2025-03-06 20:37:12,712 - INFO - training batch 301, loss: 0.129, 9632/60000 datapoints
2025-03-06 20:37:12,907 - INFO - training batch 351, loss: 0.103, 11232/60000 datapoints
2025-03-06 20:37:13,105 - INFO - training batch 401, loss: 0.424, 12832/60000 datapoints
2025-03-06 20:37:13,304 - INFO - training batch 451, loss: 0.066, 14432/60000 datapoints
2025-03-06 20:37:13,524 - INFO - training batch 501, loss: 0.280, 16032/60000 datapoints
2025-03-06 20:37:13,724 - INFO - training batch 551, loss: 0.177, 17632/60000 datapoints
2025-03-06 20:37:13,919 - INFO - training batch 601, loss: 0.315, 19232/60000 datapoints
2025-03-06 20:37:14,116 - INFO - training batch 651, loss: 0.338, 20832/60000 datapoints
2025-03-06 20:37:14,312 - INFO - training batch 701, loss: 0.398, 22432/60000 datapoints
2025-03-06 20:37:14,508 - INFO - training batch 751, loss: 0.080, 24032/60000 datapoints
2025-03-06 20:37:14,707 - INFO - training batch 801, loss: 0.123, 25632/60000 datapoints
2025-03-06 20:37:14,904 - INFO - training batch 851, loss: 0.147, 27232/60000 datapoints
2025-03-06 20:37:15,103 - INFO - training batch 901, loss: 0.061, 28832/60000 datapoints
2025-03-06 20:37:15,298 - INFO - training batch 951, loss: 0.525, 30432/60000 datapoints
2025-03-06 20:37:15,498 - INFO - training batch 1001, loss: 0.268, 32032/60000 datapoints
2025-03-06 20:37:15,704 - INFO - training batch 1051, loss: 0.153, 33632/60000 datapoints
2025-03-06 20:37:15,914 - INFO - training batch 1101, loss: 0.281, 35232/60000 datapoints
2025-03-06 20:37:16,109 - INFO - training batch 1151, loss: 0.315, 36832/60000 datapoints
2025-03-06 20:37:16,306 - INFO - training batch 1201, loss: 0.143, 38432/60000 datapoints
2025-03-06 20:37:16,502 - INFO - training batch 1251, loss: 0.055, 40032/60000 datapoints
2025-03-06 20:37:16,700 - INFO - training batch 1301, loss: 0.185, 41632/60000 datapoints
2025-03-06 20:37:16,897 - INFO - training batch 1351, loss: 0.112, 43232/60000 datapoints
2025-03-06 20:37:17,093 - INFO - training batch 1401, loss: 0.322, 44832/60000 datapoints
2025-03-06 20:37:17,287 - INFO - training batch 1451, loss: 0.529, 46432/60000 datapoints
2025-03-06 20:37:17,481 - INFO - training batch 1501, loss: 0.078, 48032/60000 datapoints
2025-03-06 20:37:17,679 - INFO - training batch 1551, loss: 0.214, 49632/60000 datapoints
2025-03-06 20:37:17,878 - INFO - training batch 1601, loss: 0.184, 51232/60000 datapoints
2025-03-06 20:37:18,077 - INFO - training batch 1651, loss: 0.197, 52832/60000 datapoints
2025-03-06 20:37:18,270 - INFO - training batch 1701, loss: 0.219, 54432/60000 datapoints
2025-03-06 20:37:18,464 - INFO - training batch 1751, loss: 0.177, 56032/60000 datapoints
2025-03-06 20:37:18,662 - INFO - training batch 1801, loss: 0.074, 57632/60000 datapoints
2025-03-06 20:37:18,859 - INFO - training batch 1851, loss: 0.316, 59232/60000 datapoints
2025-03-06 20:37:18,961 - INFO - validation batch 1, loss: 0.145, 32/10016 datapoints
2025-03-06 20:37:19,114 - INFO - validation batch 51, loss: 0.519, 1632/10016 datapoints
2025-03-06 20:37:19,268 - INFO - validation batch 101, loss: 0.154, 3232/10016 datapoints
2025-03-06 20:37:19,424 - INFO - validation batch 151, loss: 0.060, 4832/10016 datapoints
2025-03-06 20:37:19,583 - INFO - validation batch 201, loss: 0.425, 6432/10016 datapoints
2025-03-06 20:37:19,744 - INFO - validation batch 251, loss: 0.164, 8032/10016 datapoints
2025-03-06 20:37:19,897 - INFO - validation batch 301, loss: 0.166, 9632/10016 datapoints
2025-03-06 20:37:19,934 - INFO - Epoch 798/800 done.
2025-03-06 20:37:19,934 - INFO - Final validation performance:
Loss: 0.233, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:37:19,935 - INFO - Beginning epoch 799/800
2025-03-06 20:37:19,942 - INFO - training batch 1, loss: 0.235, 32/60000 datapoints
2025-03-06 20:37:20,154 - INFO - training batch 51, loss: 0.257, 1632/60000 datapoints
2025-03-06 20:37:20,348 - INFO - training batch 101, loss: 0.183, 3232/60000 datapoints
2025-03-06 20:37:20,552 - INFO - training batch 151, loss: 0.259, 4832/60000 datapoints
2025-03-06 20:37:20,754 - INFO - training batch 201, loss: 0.304, 6432/60000 datapoints
2025-03-06 20:37:20,954 - INFO - training batch 251, loss: 0.199, 8032/60000 datapoints
2025-03-06 20:37:21,149 - INFO - training batch 301, loss: 0.236, 9632/60000 datapoints
2025-03-06 20:37:21,344 - INFO - training batch 351, loss: 0.125, 11232/60000 datapoints
2025-03-06 20:37:21,538 - INFO - training batch 401, loss: 0.276, 12832/60000 datapoints
2025-03-06 20:37:21,740 - INFO - training batch 451, loss: 0.143, 14432/60000 datapoints
2025-03-06 20:37:21,935 - INFO - training batch 501, loss: 0.369, 16032/60000 datapoints
2025-03-06 20:37:22,132 - INFO - training batch 551, loss: 0.167, 17632/60000 datapoints
2025-03-06 20:37:22,327 - INFO - training batch 601, loss: 0.104, 19232/60000 datapoints
2025-03-06 20:37:22,524 - INFO - training batch 651, loss: 0.101, 20832/60000 datapoints
2025-03-06 20:37:22,726 - INFO - training batch 701, loss: 0.087, 22432/60000 datapoints
2025-03-06 20:37:22,921 - INFO - training batch 751, loss: 0.120, 24032/60000 datapoints
2025-03-06 20:37:23,118 - INFO - training batch 801, loss: 0.454, 25632/60000 datapoints
2025-03-06 20:37:23,314 - INFO - training batch 851, loss: 0.114, 27232/60000 datapoints
2025-03-06 20:37:23,511 - INFO - training batch 901, loss: 0.239, 28832/60000 datapoints
2025-03-06 20:37:23,728 - INFO - training batch 951, loss: 0.415, 30432/60000 datapoints
2025-03-06 20:37:23,922 - INFO - training batch 1001, loss: 0.047, 32032/60000 datapoints
2025-03-06 20:37:24,116 - INFO - training batch 1051, loss: 0.299, 33632/60000 datapoints
2025-03-06 20:37:24,311 - INFO - training batch 1101, loss: 0.238, 35232/60000 datapoints
2025-03-06 20:37:24,504 - INFO - training batch 1151, loss: 0.232, 36832/60000 datapoints
2025-03-06 20:37:24,700 - INFO - training batch 1201, loss: 0.147, 38432/60000 datapoints
2025-03-06 20:37:24,900 - INFO - training batch 1251, loss: 0.153, 40032/60000 datapoints
2025-03-06 20:37:25,098 - INFO - training batch 1301, loss: 0.172, 41632/60000 datapoints
2025-03-06 20:37:25,293 - INFO - training batch 1351, loss: 0.126, 43232/60000 datapoints
2025-03-06 20:37:25,489 - INFO - training batch 1401, loss: 0.080, 44832/60000 datapoints
2025-03-06 20:37:25,696 - INFO - training batch 1451, loss: 0.037, 46432/60000 datapoints
2025-03-06 20:37:25,895 - INFO - training batch 1501, loss: 0.262, 48032/60000 datapoints
2025-03-06 20:37:26,091 - INFO - training batch 1551, loss: 0.141, 49632/60000 datapoints
2025-03-06 20:37:26,288 - INFO - training batch 1601, loss: 0.209, 51232/60000 datapoints
2025-03-06 20:37:26,481 - INFO - training batch 1651, loss: 0.186, 52832/60000 datapoints
2025-03-06 20:37:26,680 - INFO - training batch 1701, loss: 0.069, 54432/60000 datapoints
2025-03-06 20:37:26,876 - INFO - training batch 1751, loss: 0.218, 56032/60000 datapoints
2025-03-06 20:37:27,072 - INFO - training batch 1801, loss: 0.189, 57632/60000 datapoints
2025-03-06 20:37:27,265 - INFO - training batch 1851, loss: 0.072, 59232/60000 datapoints
2025-03-06 20:37:27,366 - INFO - validation batch 1, loss: 0.353, 32/10016 datapoints
2025-03-06 20:37:27,518 - INFO - validation batch 51, loss: 0.434, 1632/10016 datapoints
2025-03-06 20:37:27,677 - INFO - validation batch 101, loss: 0.050, 3232/10016 datapoints
2025-03-06 20:37:27,833 - INFO - validation batch 151, loss: 0.146, 4832/10016 datapoints
2025-03-06 20:37:27,988 - INFO - validation batch 201, loss: 0.130, 6432/10016 datapoints
2025-03-06 20:37:28,143 - INFO - validation batch 251, loss: 0.037, 8032/10016 datapoints
2025-03-06 20:37:28,297 - INFO - validation batch 301, loss: 0.148, 9632/10016 datapoints
2025-03-06 20:37:28,334 - INFO - Epoch 799/800 done.
2025-03-06 20:37:28,334 - INFO - Final validation performance:
Loss: 0.185, top-1 acc: 0.938top-5 acc: 0.938
2025-03-06 20:37:28,335 - INFO - Beginning epoch 800/800
2025-03-06 20:37:28,341 - INFO - training batch 1, loss: 0.334, 32/60000 datapoints
2025-03-06 20:37:28,553 - INFO - training batch 51, loss: 0.589, 1632/60000 datapoints
2025-03-06 20:37:28,751 - INFO - training batch 101, loss: 0.350, 3232/60000 datapoints
2025-03-06 20:37:28,953 - INFO - training batch 151, loss: 0.117, 4832/60000 datapoints
2025-03-06 20:37:29,151 - INFO - training batch 201, loss: 0.101, 6432/60000 datapoints
2025-03-06 20:37:29,349 - INFO - training batch 251, loss: 0.359, 8032/60000 datapoints
2025-03-06 20:37:29,543 - INFO - training batch 301, loss: 0.086, 9632/60000 datapoints
2025-03-06 20:37:29,741 - INFO - training batch 351, loss: 0.227, 11232/60000 datapoints
2025-03-06 20:37:29,936 - INFO - training batch 401, loss: 0.102, 12832/60000 datapoints
2025-03-06 20:37:30,131 - INFO - training batch 451, loss: 0.162, 14432/60000 datapoints
2025-03-06 20:37:30,327 - INFO - training batch 501, loss: 0.108, 16032/60000 datapoints
2025-03-06 20:37:30,522 - INFO - training batch 551, loss: 0.178, 17632/60000 datapoints
2025-03-06 20:37:30,720 - INFO - training batch 601, loss: 0.083, 19232/60000 datapoints
2025-03-06 20:37:30,915 - INFO - training batch 651, loss: 0.265, 20832/60000 datapoints
2025-03-06 20:37:31,120 - INFO - training batch 701, loss: 0.257, 22432/60000 datapoints
2025-03-06 20:37:31,314 - INFO - training batch 751, loss: 0.359, 24032/60000 datapoints
2025-03-06 20:37:31,509 - INFO - training batch 801, loss: 0.140, 25632/60000 datapoints
2025-03-06 20:37:31,709 - INFO - training batch 851, loss: 0.420, 27232/60000 datapoints
2025-03-06 20:37:31,904 - INFO - training batch 901, loss: 0.368, 28832/60000 datapoints
2025-03-06 20:37:32,099 - INFO - training batch 951, loss: 0.050, 30432/60000 datapoints
2025-03-06 20:37:32,293 - INFO - training batch 1001, loss: 0.227, 32032/60000 datapoints
2025-03-06 20:37:32,486 - INFO - training batch 1051, loss: 0.090, 33632/60000 datapoints
2025-03-06 20:37:32,688 - INFO - training batch 1101, loss: 0.377, 35232/60000 datapoints
2025-03-06 20:37:32,888 - INFO - training batch 1151, loss: 0.066, 36832/60000 datapoints
2025-03-06 20:37:33,085 - INFO - training batch 1201, loss: 0.080, 38432/60000 datapoints
2025-03-06 20:37:33,283 - INFO - training batch 1251, loss: 0.252, 40032/60000 datapoints
2025-03-06 20:37:33,476 - INFO - training batch 1301, loss: 0.270, 41632/60000 datapoints
2025-03-06 20:37:33,717 - INFO - training batch 1351, loss: 0.093, 43232/60000 datapoints
2025-03-06 20:37:33,922 - INFO - training batch 1401, loss: 0.144, 44832/60000 datapoints
2025-03-06 20:37:34,117 - INFO - training batch 1451, loss: 0.153, 46432/60000 datapoints
2025-03-06 20:37:34,310 - INFO - training batch 1501, loss: 0.198, 48032/60000 datapoints
2025-03-06 20:37:34,504 - INFO - training batch 1551, loss: 0.190, 49632/60000 datapoints
2025-03-06 20:37:34,701 - INFO - training batch 1601, loss: 0.270, 51232/60000 datapoints
2025-03-06 20:37:34,907 - INFO - training batch 1651, loss: 0.291, 52832/60000 datapoints
2025-03-06 20:37:35,101 - INFO - training batch 1701, loss: 0.321, 54432/60000 datapoints
2025-03-06 20:37:35,296 - INFO - training batch 1751, loss: 0.145, 56032/60000 datapoints
2025-03-06 20:37:35,490 - INFO - training batch 1801, loss: 0.260, 57632/60000 datapoints
2025-03-06 20:37:35,690 - INFO - training batch 1851, loss: 0.141, 59232/60000 datapoints
2025-03-06 20:37:35,789 - INFO - validation batch 1, loss: 0.161, 32/10016 datapoints
2025-03-06 20:37:35,945 - INFO - validation batch 51, loss: 0.577, 1632/10016 datapoints
2025-03-06 20:37:36,098 - INFO - validation batch 101, loss: 0.206, 3232/10016 datapoints
2025-03-06 20:37:36,251 - INFO - validation batch 151, loss: 0.193, 4832/10016 datapoints
2025-03-06 20:37:36,408 - INFO - validation batch 201, loss: 0.143, 6432/10016 datapoints
2025-03-06 20:37:36,564 - INFO - validation batch 251, loss: 0.223, 8032/10016 datapoints
2025-03-06 20:37:36,719 - INFO - validation batch 301, loss: 0.209, 9632/10016 datapoints
2025-03-06 20:37:36,756 - INFO - Epoch 800/800 done.
2025-03-06 20:37:36,757 - INFO - Final validation performance:
Loss: 0.244, top-1 acc: 0.939top-5 acc: 0.939
2025-03-06 20:37:36,757 - INFO - Finished training in 7217.64 seconds.
2025-03-06 20:37:36,758 - INFO - Model trained in {train_time:.2f} s
2025-03-06 20:37:36,758 - INFO - Evaluating model...
2025-03-06 20:37:36,762 - INFO - validation batch 1, loss: 0.035, 32/10016 datapoints
2025-03-06 20:37:36,934 - INFO - validation batch 51, loss: 0.115, 1632/10016 datapoints
2025-03-06 20:37:37,088 - INFO - validation batch 101, loss: 0.186, 3232/10016 datapoints
2025-03-06 20:37:37,248 - INFO - validation batch 151, loss: 0.185, 4832/10016 datapoints
2025-03-06 20:37:37,408 - INFO - validation batch 201, loss: 0.274, 6432/10016 datapoints
2025-03-06 20:37:37,563 - INFO - validation batch 251, loss: 0.119, 8032/10016 datapoints
2025-03-06 20:37:37,741 - INFO - validation batch 301, loss: 0.135, 9632/10016 datapoints
2025-03-06 20:37:37,777 - INFO - Done evaluating.
2025-03-06 20:37:37,777 - INFO - Average final validation loss: 0.150
2025-03-06 20:37:37,777 - INFO - Saving...
2025-03-06 20:37:38,965 - INFO - Done saving.
2025-03-06 20:37:38,965 - INFO - Successfully completed hyperparameter combination 1 of 1