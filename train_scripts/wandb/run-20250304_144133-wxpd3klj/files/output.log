2025-03-04 14:41:39,221 - INFO - Running hyperparameter combination 1 of 1
2025-03-04 14:41:39,221 - INFO - 0_CTCNet_finetuning_ThalReadout_2
2025-03-04 14:41:43,453 - INFO - Training...
2025-03-04 14:41:43,454 - INFO - Beginning epoch 1/50
2025-03-04 14:41:43,570 - INFO - training batch 1, loss: 0.337, 32/56000 datapoints
2025-03-04 14:41:44,371 - INFO - training batch 51, loss: 0.324, 1632/56000 datapoints
2025-03-04 14:41:44,758 - INFO - training batch 101, loss: 0.285, 3232/56000 datapoints
2025-03-04 14:41:45,732 - INFO - training batch 151, loss: 0.304, 4832/56000 datapoints
2025-03-04 14:41:46,054 - INFO - training batch 201, loss: 0.274, 6432/56000 datapoints
2025-03-04 14:41:46,340 - INFO - training batch 251, loss: 0.248, 8032/56000 datapoints
2025-03-04 14:41:46,492 - INFO - training batch 301, loss: 0.261, 9632/56000 datapoints
2025-03-04 14:41:46,607 - INFO - training batch 351, loss: 0.265, 11232/56000 datapoints
2025-03-04 14:41:46,727 - INFO - training batch 401, loss: 0.327, 12832/56000 datapoints
2025-03-04 14:41:46,837 - INFO - training batch 451, loss: 0.290, 14432/56000 datapoints
2025-03-04 14:41:46,969 - INFO - training batch 501, loss: 0.304, 16032/56000 datapoints
2025-03-04 14:41:47,079 - INFO - training batch 551, loss: 0.213, 17632/56000 datapoints
2025-03-04 14:41:47,198 - INFO - training batch 601, loss: 0.165, 19232/56000 datapoints
2025-03-04 14:41:47,341 - INFO - training batch 651, loss: 0.201, 20832/56000 datapoints
2025-03-04 14:41:47,445 - INFO - training batch 701, loss: 0.243, 22432/56000 datapoints
2025-03-04 14:41:47,575 - INFO - training batch 751, loss: 0.236, 24032/56000 datapoints
2025-03-04 14:41:47,686 - INFO - training batch 801, loss: 0.269, 25632/56000 datapoints
2025-03-04 14:41:47,799 - INFO - training batch 851, loss: 0.185, 27232/56000 datapoints
2025-03-04 14:41:47,973 - INFO - training batch 901, loss: 0.241, 28832/56000 datapoints
2025-03-04 14:41:48,095 - INFO - training batch 951, loss: 0.236, 30432/56000 datapoints
2025-03-04 14:41:48,243 - INFO - training batch 1001, loss: 0.167, 32032/56000 datapoints
2025-03-04 14:41:48,366 - INFO - training batch 1051, loss: 0.179, 33632/56000 datapoints
2025-03-04 14:41:48,480 - INFO - training batch 1101, loss: 0.207, 35232/56000 datapoints
2025-03-04 14:41:48,684 - INFO - training batch 1151, loss: 0.132, 36832/56000 datapoints
2025-03-04 14:41:48,868 - INFO - training batch 1201, loss: 0.207, 38432/56000 datapoints
2025-03-04 14:41:49,029 - INFO - training batch 1251, loss: 0.264, 40032/56000 datapoints
2025-03-04 14:41:49,204 - INFO - training batch 1301, loss: 0.200, 41632/56000 datapoints
2025-03-04 14:41:49,391 - INFO - training batch 1351, loss: 0.139, 43232/56000 datapoints
2025-03-04 14:41:49,504 - INFO - training batch 1401, loss: 0.137, 44832/56000 datapoints
2025-03-04 14:41:49,630 - INFO - training batch 1451, loss: 0.164, 46432/56000 datapoints
2025-03-04 14:41:49,776 - INFO - training batch 1501, loss: 0.303, 48032/56000 datapoints
2025-03-04 14:41:49,948 - INFO - training batch 1551, loss: 0.177, 49632/56000 datapoints
2025-03-04 14:41:50,114 - INFO - training batch 1601, loss: 0.236, 51232/56000 datapoints
2025-03-04 14:41:50,251 - INFO - training batch 1651, loss: 0.145, 52832/56000 datapoints
2025-03-04 14:41:50,837 - INFO - training batch 1701, loss: 0.199, 54432/56000 datapoints
2025-03-04 14:41:51,309 - INFO - validation batch 1, loss: 0.135, 32/13984 datapoints
2025-03-04 14:41:51,445 - INFO - validation batch 51, loss: 0.079, 1632/13984 datapoints
2025-03-04 14:41:51,575 - INFO - validation batch 101, loss: 0.192, 3232/13984 datapoints
2025-03-04 14:41:51,657 - INFO - validation batch 151, loss: 0.110, 4832/13984 datapoints
2025-03-04 14:41:51,749 - INFO - validation batch 201, loss: 0.189, 6432/13984 datapoints
2025-03-04 14:41:51,891 - INFO - validation batch 251, loss: 0.219, 8032/13984 datapoints
2025-03-04 14:41:51,948 - INFO - validation batch 301, loss: 0.276, 9632/13984 datapoints
2025-03-04 14:41:52,005 - INFO - validation batch 351, loss: 0.135, 11232/13984 datapoints
2025-03-04 14:41:52,079 - INFO - validation batch 401, loss: 0.252, 12832/13984 datapoints
2025-03-04 14:41:52,158 - INFO - Epoch 1/50 done
2025-03-04 14:41:52,158 - INFO - Beginning epoch 2/50
2025-03-04 14:41:52,168 - INFO - training batch 1, loss: 0.189, 32/56000 datapoints
2025-03-04 14:41:52,327 - INFO - training batch 51, loss: 0.242, 1632/56000 datapoints
2025-03-04 14:41:52,470 - INFO - training batch 101, loss: 0.139, 3232/56000 datapoints
2025-03-04 14:41:52,577 - INFO - training batch 151, loss: 0.212, 4832/56000 datapoints
2025-03-04 14:41:52,673 - INFO - training batch 201, loss: 0.126, 6432/56000 datapoints
2025-03-04 14:41:52,822 - INFO - training batch 251, loss: 0.154, 8032/56000 datapoints
2025-03-04 14:41:53,040 - INFO - training batch 301, loss: 0.175, 9632/56000 datapoints
2025-03-04 14:41:53,171 - INFO - training batch 351, loss: 0.176, 11232/56000 datapoints
2025-03-04 14:41:53,290 - INFO - training batch 401, loss: 0.274, 12832/56000 datapoints
2025-03-04 14:41:53,389 - INFO - training batch 451, loss: 0.243, 14432/56000 datapoints
2025-03-04 14:41:54,268 - INFO - training batch 501, loss: 0.239, 16032/56000 datapoints
2025-03-04 14:41:54,671 - INFO - training batch 551, loss: 0.153, 17632/56000 datapoints
2025-03-04 14:41:54,775 - INFO - training batch 601, loss: 0.110, 19232/56000 datapoints
2025-03-04 14:41:54,975 - INFO - training batch 651, loss: 0.124, 20832/56000 datapoints
2025-03-04 14:41:55,171 - INFO - training batch 701, loss: 0.207, 22432/56000 datapoints
2025-03-04 14:41:55,435 - INFO - training batch 751, loss: 0.157, 24032/56000 datapoints
2025-03-04 14:41:55,679 - INFO - training batch 801, loss: 0.226, 25632/56000 datapoints
2025-03-04 14:41:56,003 - INFO - training batch 851, loss: 0.122, 27232/56000 datapoints
2025-03-04 14:41:56,382 - INFO - training batch 901, loss: 0.180, 28832/56000 datapoints
2025-03-04 14:41:56,800 - INFO - training batch 951, loss: 0.189, 30432/56000 datapoints
2025-03-04 14:41:56,921 - INFO - training batch 1001, loss: 0.108, 32032/56000 datapoints
2025-03-04 14:41:57,058 - INFO - training batch 1051, loss: 0.119, 33632/56000 datapoints
2025-03-04 14:41:57,202 - INFO - training batch 1101, loss: 0.176, 35232/56000 datapoints
2025-03-04 14:41:57,867 - INFO - training batch 1151, loss: 0.075, 36832/56000 datapoints
2025-03-04 14:41:58,397 - INFO - training batch 1201, loss: 0.176, 38432/56000 datapoints
2025-03-04 14:41:58,790 - INFO - training batch 1251, loss: 0.241, 40032/56000 datapoints
2025-03-04 14:41:59,374 - INFO - training batch 1301, loss: 0.164, 41632/56000 datapoints
2025-03-04 14:41:59,758 - INFO - training batch 1351, loss: 0.100, 43232/56000 datapoints
2025-03-04 14:42:00,164 - INFO - training batch 1401, loss: 0.092, 44832/56000 datapoints
2025-03-04 14:42:00,321 - INFO - training batch 1451, loss: 0.117, 46432/56000 datapoints
2025-03-04 14:42:00,494 - INFO - training batch 1501, loss: 0.264, 48032/56000 datapoints
2025-03-04 14:42:00,622 - INFO - training batch 1551, loss: 0.146, 49632/56000 datapoints
2025-03-04 14:42:00,720 - INFO - training batch 1601, loss: 0.196, 51232/56000 datapoints
2025-03-04 14:42:00,803 - INFO - training batch 1651, loss: 0.107, 52832/56000 datapoints
2025-03-04 14:42:00,886 - INFO - training batch 1701, loss: 0.161, 54432/56000 datapoints
2025-03-04 14:42:01,003 - INFO - validation batch 1, loss: 0.092, 32/13984 datapoints
2025-03-04 14:42:01,359 - INFO - validation batch 51, loss: 0.046, 1632/13984 datapoints
2025-03-04 14:42:01,409 - INFO - validation batch 101, loss: 0.158, 3232/13984 datapoints
2025-03-04 14:42:01,691 - INFO - validation batch 151, loss: 0.073, 4832/13984 datapoints
2025-03-04 14:42:01,933 - INFO - validation batch 201, loss: 0.150, 6432/13984 datapoints
2025-03-04 14:42:02,020 - INFO - validation batch 251, loss: 0.181, 8032/13984 datapoints
2025-03-04 14:42:02,077 - INFO - validation batch 301, loss: 0.247, 9632/13984 datapoints
2025-03-04 14:42:02,339 - INFO - validation batch 351, loss: 0.096, 11232/13984 datapoints
2025-03-04 14:42:02,463 - INFO - validation batch 401, loss: 0.226, 12832/13984 datapoints
2025-03-04 14:42:02,502 - INFO - Epoch 2/50 done
2025-03-04 14:42:02,503 - INFO - Beginning epoch 3/50
2025-03-04 14:42:02,509 - INFO - training batch 1, loss: 0.161, 32/56000 datapoints
2025-03-04 14:42:02,591 - INFO - training batch 51, loss: 0.209, 1632/56000 datapoints
2025-03-04 14:42:02,671 - INFO - training batch 101, loss: 0.103, 3232/56000 datapoints
2025-03-04 14:42:02,745 - INFO - training batch 151, loss: 0.176, 4832/56000 datapoints
2025-03-04 14:42:02,812 - INFO - training batch 201, loss: 0.091, 6432/56000 datapoints
2025-03-04 14:42:02,876 - INFO - training batch 251, loss: 0.122, 8032/56000 datapoints
2025-03-04 14:42:02,944 - INFO - training batch 301, loss: 0.144, 9632/56000 datapoints
2025-03-04 14:42:03,027 - INFO - training batch 351, loss: 0.148, 11232/56000 datapoints
2025-03-04 14:42:03,103 - INFO - training batch 401, loss: 0.246, 12832/56000 datapoints
2025-03-04 14:42:03,171 - INFO - training batch 451, loss: 0.215, 14432/56000 datapoints
2025-03-04 14:42:03,241 - INFO - training batch 501, loss: 0.204, 16032/56000 datapoints
2025-03-04 14:42:03,308 - INFO - training batch 551, loss: 0.136, 17632/56000 datapoints
2025-03-04 14:42:03,387 - INFO - training batch 601, loss: 0.087, 19232/56000 datapoints
2025-03-04 14:42:03,464 - INFO - training batch 651, loss: 0.098, 20832/56000 datapoints
2025-03-04 14:42:03,539 - INFO - training batch 701, loss: 0.201, 22432/56000 datapoints
2025-03-04 14:42:03,641 - INFO - training batch 751, loss: 0.126, 24032/56000 datapoints
2025-03-04 14:42:03,728 - INFO - training batch 801, loss: 0.197, 25632/56000 datapoints
2025-03-04 14:42:03,820 - INFO - training batch 851, loss: 0.096, 27232/56000 datapoints
2025-03-04 14:42:03,902 - INFO - training batch 901, loss: 0.147, 28832/56000 datapoints
2025-03-04 14:42:03,984 - INFO - training batch 951, loss: 0.158, 30432/56000 datapoints
2025-03-04 14:42:04,061 - INFO - training batch 1001, loss: 0.085, 32032/56000 datapoints
2025-03-04 14:42:04,138 - INFO - training batch 1051, loss: 0.094, 33632/56000 datapoints
2025-03-04 14:42:04,201 - INFO - training batch 1101, loss: 0.159, 35232/56000 datapoints
2025-03-04 14:42:04,261 - INFO - training batch 1151, loss: 0.051, 36832/56000 datapoints
2025-03-04 14:42:04,321 - INFO - training batch 1201, loss: 0.161, 38432/56000 datapoints
2025-03-04 14:42:04,381 - INFO - training batch 1251, loss: 0.236, 40032/56000 datapoints
2025-03-04 14:42:04,442 - INFO - training batch 1301, loss: 0.137, 41632/56000 datapoints
2025-03-04 14:42:04,518 - INFO - training batch 1351, loss: 0.081, 43232/56000 datapoints
2025-03-04 14:42:04,596 - INFO - training batch 1401, loss: 0.072, 44832/56000 datapoints
2025-03-04 14:42:04,678 - INFO - training batch 1451, loss: 0.092, 46432/56000 datapoints
2025-03-04 14:42:04,764 - INFO - training batch 1501, loss: 0.234, 48032/56000 datapoints
2025-03-04 14:42:04,837 - INFO - training batch 1551, loss: 0.125, 49632/56000 datapoints
2025-03-04 14:42:04,906 - INFO - training batch 1601, loss: 0.172, 51232/56000 datapoints
2025-03-04 14:42:04,974 - INFO - training batch 1651, loss: 0.087, 52832/56000 datapoints
2025-03-04 14:42:05,053 - INFO - training batch 1701, loss: 0.137, 54432/56000 datapoints
2025-03-04 14:42:05,147 - INFO - validation batch 1, loss: 0.069, 32/13984 datapoints
2025-03-04 14:42:05,177 - INFO - validation batch 51, loss: 0.032, 1632/13984 datapoints
2025-03-04 14:42:05,238 - INFO - validation batch 101, loss: 0.137, 3232/13984 datapoints
2025-03-04 14:42:05,279 - INFO - validation batch 151, loss: 0.055, 4832/13984 datapoints
2025-03-04 14:42:05,306 - INFO - validation batch 201, loss: 0.126, 6432/13984 datapoints
2025-03-04 14:42:05,334 - INFO - validation batch 251, loss: 0.158, 8032/13984 datapoints
2025-03-04 14:42:05,373 - INFO - validation batch 301, loss: 0.220, 9632/13984 datapoints
2025-03-04 14:42:05,400 - INFO - validation batch 351, loss: 0.077, 11232/13984 datapoints
2025-03-04 14:42:05,429 - INFO - validation batch 401, loss: 0.209, 12832/13984 datapoints
2025-03-04 14:42:05,452 - INFO - Epoch 3/50 done
2025-03-04 14:42:05,452 - INFO - Beginning epoch 4/50
2025-03-04 14:42:05,458 - INFO - training batch 1, loss: 0.148, 32/56000 datapoints
2025-03-04 14:42:05,549 - INFO - training batch 51, loss: 0.183, 1632/56000 datapoints
2025-03-04 14:42:05,650 - INFO - training batch 101, loss: 0.085, 3232/56000 datapoints
2025-03-04 14:42:05,782 - INFO - training batch 151, loss: 0.152, 4832/56000 datapoints
2025-03-04 14:42:05,909 - INFO - training batch 201, loss: 0.074, 6432/56000 datapoints
2025-03-04 14:42:06,013 - INFO - training batch 251, loss: 0.101, 8032/56000 datapoints
2025-03-04 14:42:06,128 - INFO - training batch 301, loss: 0.124, 9632/56000 datapoints
2025-03-04 14:42:06,241 - INFO - training batch 351, loss: 0.130, 11232/56000 datapoints
2025-03-04 14:42:06,364 - INFO - training batch 401, loss: 0.226, 12832/56000 datapoints
2025-03-04 14:42:06,525 - INFO - training batch 451, loss: 0.195, 14432/56000 datapoints
2025-03-04 14:42:06,675 - INFO - training batch 501, loss: 0.179, 16032/56000 datapoints
2025-03-04 14:42:06,864 - INFO - training batch 551, loss: 0.130, 17632/56000 datapoints
2025-03-04 14:42:07,165 - INFO - training batch 601, loss: 0.073, 19232/56000 datapoints
2025-03-04 14:42:07,459 - INFO - training batch 651, loss: 0.081, 20832/56000 datapoints
2025-03-04 14:42:07,751 - INFO - training batch 701, loss: 0.207, 22432/56000 datapoints
2025-03-04 14:42:08,052 - INFO - training batch 751, loss: 0.109, 24032/56000 datapoints
2025-03-04 14:42:08,239 - INFO - training batch 801, loss: 0.173, 25632/56000 datapoints
2025-03-04 14:42:08,418 - INFO - training batch 851, loss: 0.081, 27232/56000 datapoints
2025-03-04 14:42:08,621 - INFO - training batch 901, loss: 0.130, 28832/56000 datapoints
2025-03-04 14:42:08,791 - INFO - training batch 951, loss: 0.137, 30432/56000 datapoints
2025-03-04 14:42:08,926 - INFO - training batch 1001, loss: 0.071, 32032/56000 datapoints
2025-03-04 14:42:09,082 - INFO - training batch 1051, loss: 0.080, 33632/56000 datapoints
2025-03-04 14:42:09,330 - INFO - training batch 1101, loss: 0.148, 35232/56000 datapoints
2025-03-04 14:42:09,512 - INFO - training batch 1151, loss: 0.039, 36832/56000 datapoints
2025-03-04 14:42:09,666 - INFO - training batch 1201, loss: 0.149, 38432/56000 datapoints
2025-03-04 14:42:09,823 - INFO - training batch 1251, loss: 0.238, 40032/56000 datapoints
2025-03-04 14:42:10,016 - INFO - training batch 1301, loss: 0.118, 41632/56000 datapoints
2025-03-04 14:42:10,185 - INFO - training batch 1351, loss: 0.069, 43232/56000 datapoints
2025-03-04 14:42:10,496 - INFO - training batch 1401, loss: 0.061, 44832/56000 datapoints
2025-03-04 14:42:10,760 - INFO - training batch 1451, loss: 0.075, 46432/56000 datapoints
2025-03-04 14:42:11,448 - INFO - training batch 1501, loss: 0.214, 48032/56000 datapoints
2025-03-04 14:42:11,638 - INFO - training batch 1551, loss: 0.111, 49632/56000 datapoints
2025-03-04 14:42:11,769 - INFO - training batch 1601, loss: 0.155, 51232/56000 datapoints
2025-03-04 14:42:11,926 - INFO - training batch 1651, loss: 0.075, 52832/56000 datapoints
2025-03-04 14:42:12,050 - INFO - training batch 1701, loss: 0.120, 54432/56000 datapoints
2025-03-04 14:42:12,203 - INFO - validation batch 1, loss: 0.055, 32/13984 datapoints
2025-03-04 14:42:12,259 - INFO - validation batch 51, loss: 0.025, 1632/13984 datapoints
2025-03-04 14:42:12,312 - INFO - validation batch 101, loss: 0.121, 3232/13984 datapoints
2025-03-04 14:42:12,359 - INFO - validation batch 151, loss: 0.045, 4832/13984 datapoints
2025-03-04 14:42:12,391 - INFO - validation batch 201, loss: 0.109, 6432/13984 datapoints
2025-03-04 14:42:12,427 - INFO - validation batch 251, loss: 0.141, 8032/13984 datapoints
2025-03-04 14:42:12,461 - INFO - validation batch 301, loss: 0.198, 9632/13984 datapoints
2025-03-04 14:42:12,502 - INFO - validation batch 351, loss: 0.066, 11232/13984 datapoints
2025-03-04 14:42:12,533 - INFO - validation batch 401, loss: 0.197, 12832/13984 datapoints
2025-03-04 14:42:12,559 - INFO - Epoch 4/50 done
2025-03-04 14:42:12,559 - INFO - Beginning epoch 5/50
2025-03-04 14:42:12,570 - INFO - training batch 1, loss: 0.142, 32/56000 datapoints
2025-03-04 14:42:12,670 - INFO - training batch 51, loss: 0.162, 1632/56000 datapoints
2025-03-04 14:42:12,784 - INFO - training batch 101, loss: 0.075, 3232/56000 datapoints
2025-03-04 14:42:12,887 - INFO - training batch 151, loss: 0.136, 4832/56000 datapoints
2025-03-04 14:42:12,989 - INFO - training batch 201, loss: 0.065, 6432/56000 datapoints
2025-03-04 14:42:13,122 - INFO - training batch 251, loss: 0.088, 8032/56000 datapoints
2025-03-04 14:42:13,230 - INFO - training batch 301, loss: 0.109, 9632/56000 datapoints
2025-03-04 14:42:13,333 - INFO - training batch 351, loss: 0.118, 11232/56000 datapoints
2025-03-04 14:42:13,441 - INFO - training batch 401, loss: 0.213, 12832/56000 datapoints
2025-03-04 14:42:13,534 - INFO - training batch 451, loss: 0.179, 14432/56000 datapoints
2025-03-04 14:42:13,655 - INFO - training batch 501, loss: 0.160, 16032/56000 datapoints
2025-03-04 14:42:13,791 - INFO - training batch 551, loss: 0.127, 17632/56000 datapoints
2025-03-04 14:42:13,921 - INFO - training batch 601, loss: 0.065, 19232/56000 datapoints
2025-03-04 14:42:14,041 - INFO - training batch 651, loss: 0.070, 20832/56000 datapoints
2025-03-04 14:42:14,162 - INFO - training batch 701, loss: 0.217, 22432/56000 datapoints
2025-03-04 14:42:14,444 - INFO - training batch 751, loss: 0.098, 24032/56000 datapoints
2025-03-04 14:42:14,536 - INFO - training batch 801, loss: 0.152, 25632/56000 datapoints
2025-03-04 14:42:14,614 - INFO - training batch 851, loss: 0.071, 27232/56000 datapoints
2025-03-04 14:42:14,693 - INFO - training batch 901, loss: 0.117, 28832/56000 datapoints
2025-03-04 14:42:14,770 - INFO - training batch 951, loss: 0.124, 30432/56000 datapoints
2025-03-04 14:42:14,850 - INFO - training batch 1001, loss: 0.061, 32032/56000 datapoints
2025-03-04 14:42:14,942 - INFO - training batch 1051, loss: 0.071, 33632/56000 datapoints
2025-03-04 14:42:15,019 - INFO - training batch 1101, loss: 0.139, 35232/56000 datapoints
2025-03-04 14:42:15,104 - INFO - training batch 1151, loss: 0.031, 36832/56000 datapoints
2025-03-04 14:42:15,180 - INFO - training batch 1201, loss: 0.137, 38432/56000 datapoints
2025-03-04 14:42:15,255 - INFO - training batch 1251, loss: 0.241, 40032/56000 datapoints
2025-03-04 14:42:15,335 - INFO - training batch 1301, loss: 0.104, 41632/56000 datapoints
2025-03-04 14:42:15,420 - INFO - training batch 1351, loss: 0.060, 43232/56000 datapoints
2025-03-04 14:42:15,497 - INFO - training batch 1401, loss: 0.054, 44832/56000 datapoints
2025-03-04 14:42:15,581 - INFO - training batch 1451, loss: 0.065, 46432/56000 datapoints
2025-03-04 14:42:15,681 - INFO - training batch 1501, loss: 0.199, 48032/56000 datapoints
2025-03-04 14:42:15,785 - INFO - training batch 1551, loss: 0.101, 49632/56000 datapoints
2025-03-04 14:42:15,878 - INFO - training batch 1601, loss: 0.142, 51232/56000 datapoints
2025-03-04 14:42:15,994 - INFO - training batch 1651, loss: 0.067, 52832/56000 datapoints
2025-03-04 14:42:16,271 - INFO - training batch 1701, loss: 0.110, 54432/56000 datapoints
2025-03-04 14:42:16,826 - INFO - validation batch 1, loss: 0.045, 32/13984 datapoints
2025-03-04 14:42:16,973 - INFO - validation batch 51, loss: 0.022, 1632/13984 datapoints
2025-03-04 14:42:17,112 - INFO - validation batch 101, loss: 0.110, 3232/13984 datapoints
2025-03-04 14:42:17,226 - INFO - validation batch 151, loss: 0.040, 4832/13984 datapoints
2025-03-04 14:42:17,579 - INFO - validation batch 201, loss: 0.096, 6432/13984 datapoints
2025-03-04 14:42:17,713 - INFO - validation batch 251, loss: 0.128, 8032/13984 datapoints
2025-03-04 14:42:17,753 - INFO - validation batch 301, loss: 0.183, 9632/13984 datapoints
2025-03-04 14:42:17,793 - INFO - validation batch 351, loss: 0.059, 11232/13984 datapoints
2025-03-04 14:42:18,010 - INFO - validation batch 401, loss: 0.188, 12832/13984 datapoints
2025-03-04 14:42:18,198 - INFO - Epoch 5/50 done
2025-03-04 14:42:18,198 - INFO - Beginning epoch 6/50
2025-03-04 14:42:18,248 - INFO - training batch 1, loss: 0.139, 32/56000 datapoints
2025-03-04 14:42:18,785 - INFO - training batch 51, loss: 0.147, 1632/56000 datapoints
2025-03-04 14:42:19,011 - INFO - training batch 101, loss: 0.069, 3232/56000 datapoints
2025-03-04 14:42:19,156 - INFO - training batch 151, loss: 0.127, 4832/56000 datapoints
2025-03-04 14:42:19,363 - INFO - training batch 201, loss: 0.061, 6432/56000 datapoints
2025-03-04 14:42:20,428 - INFO - training batch 251, loss: 0.079, 8032/56000 datapoints
2025-03-04 14:42:21,004 - INFO - training batch 301, loss: 0.098, 9632/56000 datapoints
2025-03-04 14:42:22,615 - INFO - training batch 351, loss: 0.109, 11232/56000 datapoints
2025-03-04 14:42:22,905 - INFO - training batch 401, loss: 0.201, 12832/56000 datapoints
2025-03-04 14:42:23,123 - INFO - training batch 451, loss: 0.165, 14432/56000 datapoints
2025-03-04 14:42:23,375 - INFO - training batch 501, loss: 0.146, 16032/56000 datapoints
2025-03-04 14:42:23,801 - INFO - training batch 551, loss: 0.125, 17632/56000 datapoints
2025-03-04 14:42:23,960 - INFO - training batch 601, loss: 0.060, 19232/56000 datapoints
2025-03-04 14:42:24,240 - INFO - training batch 651, loss: 0.063, 20832/56000 datapoints
2025-03-04 14:42:24,917 - INFO - training batch 701, loss: 0.226, 22432/56000 datapoints
2025-03-04 14:42:25,183 - INFO - training batch 751, loss: 0.090, 24032/56000 datapoints
2025-03-04 14:42:25,524 - INFO - training batch 801, loss: 0.136, 25632/56000 datapoints
2025-03-04 14:42:25,657 - INFO - training batch 851, loss: 0.065, 27232/56000 datapoints
2025-03-04 14:42:25,797 - INFO - training batch 901, loss: 0.108, 28832/56000 datapoints
2025-03-04 14:42:26,091 - INFO - training batch 951, loss: 0.113, 30432/56000 datapoints
2025-03-04 14:42:26,663 - INFO - training batch 1001, loss: 0.054, 32032/56000 datapoints
2025-03-04 14:42:27,753 - INFO - training batch 1051, loss: 0.065, 33632/56000 datapoints
2025-03-04 14:42:28,338 - INFO - training batch 1101, loss: 0.132, 35232/56000 datapoints
2025-03-04 14:42:29,469 - INFO - training batch 1151, loss: 0.027, 36832/56000 datapoints
2025-03-04 14:42:30,397 - INFO - training batch 1201, loss: 0.126, 38432/56000 datapoints
2025-03-04 14:42:30,625 - INFO - training batch 1251, loss: 0.246, 40032/56000 datapoints
2025-03-04 14:42:30,865 - INFO - training batch 1301, loss: 0.094, 41632/56000 datapoints
2025-03-04 14:42:31,069 - INFO - training batch 1351, loss: 0.053, 43232/56000 datapoints
2025-03-04 14:42:31,401 - INFO - training batch 1401, loss: 0.050, 44832/56000 datapoints
2025-03-04 14:42:31,753 - INFO - training batch 1451, loss: 0.058, 46432/56000 datapoints
2025-03-04 14:42:32,247 - INFO - training batch 1501, loss: 0.187, 48032/56000 datapoints
2025-03-04 14:42:32,481 - INFO - training batch 1551, loss: 0.093, 49632/56000 datapoints
2025-03-04 14:42:32,805 - INFO - training batch 1601, loss: 0.132, 51232/56000 datapoints
2025-03-04 14:42:33,111 - INFO - training batch 1651, loss: 0.061, 52832/56000 datapoints
2025-03-04 14:42:33,263 - INFO - training batch 1701, loss: 0.103, 54432/56000 datapoints
2025-03-04 14:42:33,416 - INFO - validation batch 1, loss: 0.039, 32/13984 datapoints
2025-03-04 14:42:33,495 - INFO - validation batch 51, loss: 0.020, 1632/13984 datapoints
2025-03-04 14:42:33,574 - INFO - validation batch 101, loss: 0.101, 3232/13984 datapoints
2025-03-04 14:42:33,653 - INFO - validation batch 151, loss: 0.036, 4832/13984 datapoints
2025-03-04 14:42:33,708 - INFO - validation batch 201, loss: 0.086, 6432/13984 datapoints
2025-03-04 14:42:33,781 - INFO - validation batch 251, loss: 0.118, 8032/13984 datapoints
2025-03-04 14:42:33,838 - INFO - validation batch 301, loss: 0.171, 9632/13984 datapoints
2025-03-04 14:42:33,886 - INFO - validation batch 351, loss: 0.054, 11232/13984 datapoints
2025-03-04 14:42:33,952 - INFO - validation batch 401, loss: 0.180, 12832/13984 datapoints
2025-03-04 14:42:34,007 - INFO - Epoch 6/50 done
2025-03-04 14:42:34,007 - INFO - Beginning epoch 7/50
2025-03-04 14:42:34,015 - INFO - training batch 1, loss: 0.137, 32/56000 datapoints
2025-03-04 14:42:34,182 - INFO - training batch 51, loss: 0.136, 1632/56000 datapoints
2025-03-04 14:42:34,345 - INFO - training batch 101, loss: 0.066, 3232/56000 datapoints
2025-03-04 14:42:34,500 - INFO - training batch 151, loss: 0.121, 4832/56000 datapoints
2025-03-04 14:42:34,633 - INFO - training batch 201, loss: 0.058, 6432/56000 datapoints
2025-03-04 14:42:34,814 - INFO - training batch 251, loss: 0.072, 8032/56000 datapoints
2025-03-04 14:42:34,999 - INFO - training batch 301, loss: 0.090, 9632/56000 datapoints
2025-03-04 14:42:35,215 - INFO - training batch 351, loss: 0.102, 11232/56000 datapoints
2025-03-04 14:42:35,428 - INFO - training batch 401, loss: 0.192, 12832/56000 datapoints
2025-03-04 14:42:35,582 - INFO - training batch 451, loss: 0.154, 14432/56000 datapoints
2025-03-04 14:42:35,728 - INFO - training batch 501, loss: 0.135, 16032/56000 datapoints
2025-03-04 14:42:35,865 - INFO - training batch 551, loss: 0.124, 17632/56000 datapoints
2025-03-04 14:42:36,000 - INFO - training batch 601, loss: 0.056, 19232/56000 datapoints
2025-03-04 14:42:36,127 - INFO - training batch 651, loss: 0.058, 20832/56000 datapoints
2025-03-04 14:42:36,288 - INFO - training batch 701, loss: 0.234, 22432/56000 datapoints
2025-03-04 14:42:36,423 - INFO - training batch 751, loss: 0.084, 24032/56000 datapoints
2025-03-04 14:42:36,541 - INFO - training batch 801, loss: 0.123, 25632/56000 datapoints
2025-03-04 14:42:36,649 - INFO - training batch 851, loss: 0.061, 27232/56000 datapoints
2025-03-04 14:42:36,741 - INFO - training batch 901, loss: 0.101, 28832/56000 datapoints
2025-03-04 14:42:36,862 - INFO - training batch 951, loss: 0.106, 30432/56000 datapoints
2025-03-04 14:42:36,987 - INFO - training batch 1001, loss: 0.049, 32032/56000 datapoints
2025-03-04 14:42:37,108 - INFO - training batch 1051, loss: 0.060, 33632/56000 datapoints
2025-03-04 14:42:37,197 - INFO - training batch 1101, loss: 0.128, 35232/56000 datapoints
2025-03-04 14:42:37,269 - INFO - training batch 1151, loss: 0.023, 36832/56000 datapoints
2025-03-04 14:42:37,344 - INFO - training batch 1201, loss: 0.115, 38432/56000 datapoints
2025-03-04 14:42:37,433 - INFO - training batch 1251, loss: 0.250, 40032/56000 datapoints
2025-03-04 14:42:37,519 - INFO - training batch 1301, loss: 0.086, 41632/56000 datapoints
2025-03-04 14:42:37,616 - INFO - training batch 1351, loss: 0.048, 43232/56000 datapoints
2025-03-04 14:42:37,720 - INFO - training batch 1401, loss: 0.047, 44832/56000 datapoints
2025-03-04 14:42:37,814 - INFO - training batch 1451, loss: 0.053, 46432/56000 datapoints
2025-03-04 14:42:37,884 - INFO - training batch 1501, loss: 0.178, 48032/56000 datapoints
2025-03-04 14:42:37,963 - INFO - training batch 1551, loss: 0.089, 49632/56000 datapoints
2025-03-04 14:42:38,036 - INFO - training batch 1601, loss: 0.124, 51232/56000 datapoints
2025-03-04 14:42:38,111 - INFO - training batch 1651, loss: 0.057, 52832/56000 datapoints
2025-03-04 14:42:38,184 - INFO - training batch 1701, loss: 0.097, 54432/56000 datapoints
2025-03-04 14:42:38,265 - INFO - validation batch 1, loss: 0.035, 32/13984 datapoints
2025-03-04 14:42:38,291 - INFO - validation batch 51, loss: 0.018, 1632/13984 datapoints
2025-03-04 14:42:38,316 - INFO - validation batch 101, loss: 0.094, 3232/13984 datapoints
2025-03-04 14:42:38,346 - INFO - validation batch 151, loss: 0.034, 4832/13984 datapoints
2025-03-04 14:42:38,368 - INFO - validation batch 201, loss: 0.078, 6432/13984 datapoints
2025-03-04 14:42:38,395 - INFO - validation batch 251, loss: 0.111, 8032/13984 datapoints
2025-03-04 14:42:38,420 - INFO - validation batch 301, loss: 0.162, 9632/13984 datapoints
2025-03-04 14:42:38,459 - INFO - validation batch 351, loss: 0.050, 11232/13984 datapoints
2025-03-04 14:42:38,485 - INFO - validation batch 401, loss: 0.175, 12832/13984 datapoints
2025-03-04 14:42:38,506 - INFO - Epoch 7/50 done
2025-03-04 14:42:38,506 - INFO - Beginning epoch 8/50
2025-03-04 14:42:38,510 - INFO - training batch 1, loss: 0.135, 32/56000 datapoints
2025-03-04 14:42:38,586 - INFO - training batch 51, loss: 0.128, 1632/56000 datapoints
2025-03-04 14:42:38,658 - INFO - training batch 101, loss: 0.063, 3232/56000 datapoints
2025-03-04 14:42:38,731 - INFO - training batch 151, loss: 0.116, 4832/56000 datapoints
2025-03-04 14:42:38,805 - INFO - training batch 201, loss: 0.056, 6432/56000 datapoints
2025-03-04 14:42:38,892 - INFO - training batch 251, loss: 0.068, 8032/56000 datapoints
2025-03-04 14:42:38,967 - INFO - training batch 301, loss: 0.084, 9632/56000 datapoints
2025-03-04 14:42:39,042 - INFO - training batch 351, loss: 0.097, 11232/56000 datapoints
2025-03-04 14:42:39,114 - INFO - training batch 401, loss: 0.185, 12832/56000 datapoints
2025-03-04 14:42:39,199 - INFO - training batch 451, loss: 0.145, 14432/56000 datapoints
2025-03-04 14:42:39,270 - INFO - training batch 501, loss: 0.127, 16032/56000 datapoints
2025-03-04 14:42:39,347 - INFO - training batch 551, loss: 0.124, 17632/56000 datapoints
2025-03-04 14:42:39,438 - INFO - training batch 601, loss: 0.053, 19232/56000 datapoints
2025-03-04 14:42:39,522 - INFO - training batch 651, loss: 0.054, 20832/56000 datapoints
2025-03-04 14:42:39,612 - INFO - training batch 701, loss: 0.243, 22432/56000 datapoints
2025-03-04 14:42:39,702 - INFO - training batch 751, loss: 0.081, 24032/56000 datapoints
2025-03-04 14:42:39,788 - INFO - training batch 801, loss: 0.114, 25632/56000 datapoints
2025-03-04 14:42:39,892 - INFO - training batch 851, loss: 0.058, 27232/56000 datapoints
2025-03-04 14:42:39,987 - INFO - training batch 901, loss: 0.097, 28832/56000 datapoints
2025-03-04 14:42:40,081 - INFO - training batch 951, loss: 0.100, 30432/56000 datapoints
2025-03-04 14:42:40,183 - INFO - training batch 1001, loss: 0.046, 32032/56000 datapoints
2025-03-04 14:42:40,278 - INFO - training batch 1051, loss: 0.057, 33632/56000 datapoints
2025-03-04 14:42:40,623 - INFO - training batch 1101, loss: 0.126, 35232/56000 datapoints
2025-03-04 14:42:40,706 - INFO - training batch 1151, loss: 0.022, 36832/56000 datapoints
2025-03-04 14:42:40,793 - INFO - training batch 1201, loss: 0.107, 38432/56000 datapoints
2025-03-04 14:42:40,892 - INFO - training batch 1251, loss: 0.255, 40032/56000 datapoints
2025-03-04 14:42:40,991 - INFO - training batch 1301, loss: 0.082, 41632/56000 datapoints
2025-03-04 14:42:41,074 - INFO - training batch 1351, loss: 0.045, 43232/56000 datapoints
2025-03-04 14:42:41,151 - INFO - training batch 1401, loss: 0.045, 44832/56000 datapoints
2025-03-04 14:42:41,229 - INFO - training batch 1451, loss: 0.049, 46432/56000 datapoints
2025-03-04 14:42:41,317 - INFO - training batch 1501, loss: 0.174, 48032/56000 datapoints
2025-03-04 14:42:41,407 - INFO - training batch 1551, loss: 0.086, 49632/56000 datapoints
2025-03-04 14:42:41,496 - INFO - training batch 1601, loss: 0.119, 51232/56000 datapoints
2025-03-04 14:42:41,585 - INFO - training batch 1651, loss: 0.054, 52832/56000 datapoints
2025-03-04 14:42:41,663 - INFO - training batch 1701, loss: 0.093, 54432/56000 datapoints
2025-03-04 14:42:41,752 - INFO - validation batch 1, loss: 0.032, 32/13984 datapoints
2025-03-04 14:42:41,775 - INFO - validation batch 51, loss: 0.017, 1632/13984 datapoints
2025-03-04 14:42:41,808 - INFO - validation batch 101, loss: 0.090, 3232/13984 datapoints
2025-03-04 14:42:41,833 - INFO - validation batch 151, loss: 0.032, 4832/13984 datapoints
2025-03-04 14:42:41,864 - INFO - validation batch 201, loss: 0.073, 6432/13984 datapoints
2025-03-04 14:42:41,895 - INFO - validation batch 251, loss: 0.106, 8032/13984 datapoints
2025-03-04 14:42:41,925 - INFO - validation batch 301, loss: 0.157, 9632/13984 datapoints
2025-03-04 14:42:41,949 - INFO - validation batch 351, loss: 0.048, 11232/13984 datapoints
2025-03-04 14:42:41,973 - INFO - validation batch 401, loss: 0.172, 12832/13984 datapoints
2025-03-04 14:42:41,995 - INFO - Epoch 8/50 done
2025-03-04 14:42:41,996 - INFO - Beginning epoch 9/50
2025-03-04 14:42:42,000 - INFO - training batch 1, loss: 0.134, 32/56000 datapoints
2025-03-04 14:42:42,103 - INFO - training batch 51, loss: 0.123, 1632/56000 datapoints
2025-03-04 14:42:42,202 - INFO - training batch 101, loss: 0.062, 3232/56000 datapoints
2025-03-04 14:42:42,280 - INFO - training batch 151, loss: 0.114, 4832/56000 datapoints
2025-03-04 14:42:42,356 - INFO - training batch 201, loss: 0.054, 6432/56000 datapoints
2025-03-04 14:42:42,438 - INFO - training batch 251, loss: 0.066, 8032/56000 datapoints
2025-03-04 14:42:42,513 - INFO - training batch 301, loss: 0.081, 9632/56000 datapoints
2025-03-04 14:42:42,587 - INFO - training batch 351, loss: 0.095, 11232/56000 datapoints
2025-03-04 14:42:42,673 - INFO - training batch 401, loss: 0.182, 12832/56000 datapoints
2025-03-04 14:42:42,763 - INFO - training batch 451, loss: 0.139, 14432/56000 datapoints
2025-03-04 14:42:42,845 - INFO - training batch 501, loss: 0.122, 16032/56000 datapoints
2025-03-04 14:42:42,941 - INFO - training batch 551, loss: 0.124, 17632/56000 datapoints
2025-03-04 14:42:43,023 - INFO - training batch 601, loss: 0.052, 19232/56000 datapoints
2025-03-04 14:42:43,106 - INFO - training batch 651, loss: 0.052, 20832/56000 datapoints
2025-03-04 14:42:43,203 - INFO - training batch 701, loss: 0.250, 22432/56000 datapoints
2025-03-04 14:42:43,301 - INFO - training batch 751, loss: 0.078, 24032/56000 datapoints
2025-03-04 14:42:43,419 - INFO - training batch 801, loss: 0.108, 25632/56000 datapoints
2025-03-04 14:42:43,521 - INFO - training batch 851, loss: 0.056, 27232/56000 datapoints
2025-03-04 14:42:43,608 - INFO - training batch 901, loss: 0.095, 28832/56000 datapoints
2025-03-04 14:42:43,682 - INFO - training batch 951, loss: 0.097, 30432/56000 datapoints
2025-03-04 14:42:43,766 - INFO - training batch 1001, loss: 0.044, 32032/56000 datapoints
2025-03-04 14:42:43,841 - INFO - training batch 1051, loss: 0.055, 33632/56000 datapoints
2025-03-04 14:42:43,918 - INFO - training batch 1101, loss: 0.125, 35232/56000 datapoints
2025-03-04 14:42:43,986 - INFO - training batch 1151, loss: 0.020, 36832/56000 datapoints
2025-03-04 14:42:44,056 - INFO - training batch 1201, loss: 0.099, 38432/56000 datapoints
2025-03-04 14:42:44,123 - INFO - training batch 1251, loss: 0.258, 40032/56000 datapoints
2025-03-04 14:42:44,190 - INFO - training batch 1301, loss: 0.079, 41632/56000 datapoints
2025-03-04 14:42:44,257 - INFO - training batch 1351, loss: 0.043, 43232/56000 datapoints
2025-03-04 14:42:44,325 - INFO - training batch 1401, loss: 0.043, 44832/56000 datapoints
2025-03-04 14:42:45,095 - INFO - training batch 1451, loss: 0.047, 46432/56000 datapoints
2025-03-04 14:42:45,183 - INFO - training batch 1501, loss: 0.171, 48032/56000 datapoints
2025-03-04 14:42:45,265 - INFO - training batch 1551, loss: 0.085, 49632/56000 datapoints
2025-03-04 14:42:45,346 - INFO - training batch 1601, loss: 0.116, 51232/56000 datapoints
2025-03-04 14:42:45,425 - INFO - training batch 1651, loss: 0.052, 52832/56000 datapoints
2025-03-04 14:42:45,698 - INFO - training batch 1701, loss: 0.090, 54432/56000 datapoints
2025-03-04 14:42:45,787 - INFO - validation batch 1, loss: 0.030, 32/13984 datapoints
2025-03-04 14:42:45,813 - INFO - validation batch 51, loss: 0.016, 1632/13984 datapoints
2025-03-04 14:42:45,841 - INFO - validation batch 101, loss: 0.088, 3232/13984 datapoints
2025-03-04 14:42:45,871 - INFO - validation batch 151, loss: 0.031, 4832/13984 datapoints
2025-03-04 14:42:45,898 - INFO - validation batch 201, loss: 0.069, 6432/13984 datapoints
2025-03-04 14:42:45,925 - INFO - validation batch 251, loss: 0.102, 8032/13984 datapoints
2025-03-04 14:42:45,952 - INFO - validation batch 301, loss: 0.154, 9632/13984 datapoints
2025-03-04 14:42:45,979 - INFO - validation batch 351, loss: 0.046, 11232/13984 datapoints
2025-03-04 14:42:46,008 - INFO - validation batch 401, loss: 0.171, 12832/13984 datapoints
2025-03-04 14:42:46,028 - INFO - Epoch 9/50 done
2025-03-04 14:42:46,028 - INFO - Beginning epoch 10/50
2025-03-04 14:42:46,031 - INFO - training batch 1, loss: 0.132, 32/56000 datapoints
2025-03-04 14:42:46,126 - INFO - training batch 51, loss: 0.120, 1632/56000 datapoints
2025-03-04 14:42:46,217 - INFO - training batch 101, loss: 0.061, 3232/56000 datapoints
2025-03-04 14:42:46,290 - INFO - training batch 151, loss: 0.113, 4832/56000 datapoints
2025-03-04 14:42:46,362 - INFO - training batch 201, loss: 0.053, 6432/56000 datapoints
2025-03-04 14:42:46,429 - INFO - training batch 251, loss: 0.064, 8032/56000 datapoints
2025-03-04 14:42:46,495 - INFO - training batch 301, loss: 0.078, 9632/56000 datapoints
2025-03-04 14:42:46,561 - INFO - training batch 351, loss: 0.093, 11232/56000 datapoints
2025-03-04 14:42:46,638 - INFO - training batch 401, loss: 0.181, 12832/56000 datapoints
2025-03-04 14:42:46,710 - INFO - training batch 451, loss: 0.135, 14432/56000 datapoints
2025-03-04 14:42:46,774 - INFO - training batch 501, loss: 0.118, 16032/56000 datapoints
2025-03-04 14:42:46,838 - INFO - training batch 551, loss: 0.125, 17632/56000 datapoints
2025-03-04 14:42:46,910 - INFO - training batch 601, loss: 0.051, 19232/56000 datapoints
2025-03-04 14:42:46,988 - INFO - training batch 651, loss: 0.050, 20832/56000 datapoints
2025-03-04 14:42:47,058 - INFO - training batch 701, loss: 0.256, 22432/56000 datapoints
2025-03-04 14:42:47,125 - INFO - training batch 751, loss: 0.077, 24032/56000 datapoints
2025-03-04 14:42:47,200 - INFO - training batch 801, loss: 0.105, 25632/56000 datapoints
2025-03-04 14:42:47,282 - INFO - training batch 851, loss: 0.054, 27232/56000 datapoints
2025-03-04 14:42:47,349 - INFO - training batch 901, loss: 0.093, 28832/56000 datapoints
2025-03-04 14:42:47,416 - INFO - training batch 951, loss: 0.095, 30432/56000 datapoints
2025-03-04 14:42:47,482 - INFO - training batch 1001, loss: 0.043, 32032/56000 datapoints
2025-03-04 14:42:47,549 - INFO - training batch 1051, loss: 0.053, 33632/56000 datapoints
2025-03-04 14:42:47,620 - INFO - training batch 1101, loss: 0.124, 35232/56000 datapoints
2025-03-04 14:42:47,684 - INFO - training batch 1151, loss: 0.020, 36832/56000 datapoints
2025-03-04 14:42:47,761 - INFO - training batch 1201, loss: 0.092, 38432/56000 datapoints
2025-03-04 14:42:47,835 - INFO - training batch 1251, loss: 0.260, 40032/56000 datapoints
2025-03-04 14:42:47,914 - INFO - training batch 1301, loss: 0.077, 41632/56000 datapoints
2025-03-04 14:42:47,981 - INFO - training batch 1351, loss: 0.041, 43232/56000 datapoints
2025-03-04 14:42:48,043 - INFO - training batch 1401, loss: 0.042, 44832/56000 datapoints
2025-03-04 14:42:48,113 - INFO - training batch 1451, loss: 0.045, 46432/56000 datapoints
2025-03-04 14:42:48,178 - INFO - training batch 1501, loss: 0.169, 48032/56000 datapoints
2025-03-04 14:42:48,248 - INFO - training batch 1551, loss: 0.084, 49632/56000 datapoints
2025-03-04 14:42:48,315 - INFO - training batch 1601, loss: 0.113, 51232/56000 datapoints
2025-03-04 14:42:48,378 - INFO - training batch 1651, loss: 0.050, 52832/56000 datapoints
2025-03-04 14:42:48,443 - INFO - training batch 1701, loss: 0.088, 54432/56000 datapoints
2025-03-04 14:42:48,505 - INFO - validation batch 1, loss: 0.029, 32/13984 datapoints
2025-03-04 14:42:48,523 - INFO - validation batch 51, loss: 0.015, 1632/13984 datapoints
2025-03-04 14:42:48,549 - INFO - validation batch 101, loss: 0.086, 3232/13984 datapoints
2025-03-04 14:42:48,573 - INFO - validation batch 151, loss: 0.030, 4832/13984 datapoints
2025-03-04 14:42:48,596 - INFO - validation batch 201, loss: 0.067, 6432/13984 datapoints
2025-03-04 14:42:48,622 - INFO - validation batch 251, loss: 0.098, 8032/13984 datapoints
2025-03-04 14:42:48,646 - INFO - validation batch 301, loss: 0.152, 9632/13984 datapoints
2025-03-04 14:42:48,668 - INFO - validation batch 351, loss: 0.044, 11232/13984 datapoints
2025-03-04 14:42:48,688 - INFO - validation batch 401, loss: 0.170, 12832/13984 datapoints
2025-03-04 14:42:48,705 - INFO - Epoch 10/50 done
2025-03-04 14:42:48,706 - INFO - Beginning epoch 11/50
2025-03-04 14:42:48,708 - INFO - training batch 1, loss: 0.131, 32/56000 datapoints
2025-03-04 14:42:48,780 - INFO - training batch 51, loss: 0.119, 1632/56000 datapoints
2025-03-04 14:42:48,845 - INFO - training batch 101, loss: 0.061, 3232/56000 datapoints
2025-03-04 14:42:48,913 - INFO - training batch 151, loss: 0.112, 4832/56000 datapoints
2025-03-04 14:42:48,977 - INFO - training batch 201, loss: 0.052, 6432/56000 datapoints
2025-03-04 14:42:49,042 - INFO - training batch 251, loss: 0.063, 8032/56000 datapoints
2025-03-04 14:42:49,112 - INFO - training batch 301, loss: 0.077, 9632/56000 datapoints
2025-03-04 14:42:49,187 - INFO - training batch 351, loss: 0.092, 11232/56000 datapoints
2025-03-04 14:42:49,253 - INFO - training batch 401, loss: 0.179, 12832/56000 datapoints
2025-03-04 14:42:49,321 - INFO - training batch 451, loss: 0.132, 14432/56000 datapoints
2025-03-04 14:42:49,400 - INFO - training batch 501, loss: 0.114, 16032/56000 datapoints
2025-03-04 14:42:49,475 - INFO - training batch 551, loss: 0.125, 17632/56000 datapoints
2025-03-04 14:42:49,590 - INFO - training batch 601, loss: 0.050, 19232/56000 datapoints
2025-03-04 14:42:49,749 - INFO - training batch 651, loss: 0.049, 20832/56000 datapoints
2025-03-04 14:42:49,932 - INFO - training batch 701, loss: 0.260, 22432/56000 datapoints
2025-03-04 14:42:50,019 - INFO - training batch 751, loss: 0.075, 24032/56000 datapoints
2025-03-04 14:42:50,084 - INFO - training batch 801, loss: 0.102, 25632/56000 datapoints
2025-03-04 14:42:50,152 - INFO - training batch 851, loss: 0.053, 27232/56000 datapoints
2025-03-04 14:42:50,217 - INFO - training batch 901, loss: 0.092, 28832/56000 datapoints
2025-03-04 14:42:50,286 - INFO - training batch 951, loss: 0.093, 30432/56000 datapoints
2025-03-04 14:42:50,348 - INFO - training batch 1001, loss: 0.042, 32032/56000 datapoints
2025-03-04 14:42:50,413 - INFO - training batch 1051, loss: 0.051, 33632/56000 datapoints
2025-03-04 14:42:50,476 - INFO - training batch 1101, loss: 0.123, 35232/56000 datapoints
2025-03-04 14:42:50,589 - INFO - training batch 1151, loss: 0.019, 36832/56000 datapoints
2025-03-04 14:42:50,663 - INFO - training batch 1201, loss: 0.087, 38432/56000 datapoints
2025-03-04 14:42:50,730 - INFO - training batch 1251, loss: 0.261, 40032/56000 datapoints
2025-03-04 14:42:50,802 - INFO - training batch 1301, loss: 0.075, 41632/56000 datapoints
2025-03-04 14:42:50,871 - INFO - training batch 1351, loss: 0.040, 43232/56000 datapoints
2025-03-04 14:42:50,955 - INFO - training batch 1401, loss: 0.041, 44832/56000 datapoints
2025-03-04 14:42:51,043 - INFO - training batch 1451, loss: 0.043, 46432/56000 datapoints
2025-03-04 14:42:51,119 - INFO - training batch 1501, loss: 0.167, 48032/56000 datapoints
2025-03-04 14:42:51,198 - INFO - training batch 1551, loss: 0.083, 49632/56000 datapoints
2025-03-04 14:42:51,264 - INFO - training batch 1601, loss: 0.110, 51232/56000 datapoints
2025-03-04 14:42:51,331 - INFO - training batch 1651, loss: 0.049, 52832/56000 datapoints
2025-03-04 14:42:51,398 - INFO - training batch 1701, loss: 0.087, 54432/56000 datapoints
2025-03-04 14:42:51,466 - INFO - validation batch 1, loss: 0.028, 32/13984 datapoints
2025-03-04 14:42:51,485 - INFO - validation batch 51, loss: 0.014, 1632/13984 datapoints
2025-03-04 14:42:51,509 - INFO - validation batch 101, loss: 0.085, 3232/13984 datapoints
2025-03-04 14:42:51,534 - INFO - validation batch 151, loss: 0.029, 4832/13984 datapoints
2025-03-04 14:42:51,554 - INFO - validation batch 201, loss: 0.064, 6432/13984 datapoints
2025-03-04 14:42:51,579 - INFO - validation batch 251, loss: 0.096, 8032/13984 datapoints
2025-03-04 14:42:51,601 - INFO - validation batch 301, loss: 0.150, 9632/13984 datapoints
2025-03-04 14:42:51,628 - INFO - validation batch 351, loss: 0.043, 11232/13984 datapoints
2025-03-04 14:42:51,678 - INFO - validation batch 401, loss: 0.168, 12832/13984 datapoints
2025-03-04 14:42:51,693 - INFO - Epoch 11/50 done
2025-03-04 14:42:51,694 - INFO - Beginning epoch 12/50
2025-03-04 14:42:51,697 - INFO - training batch 1, loss: 0.130, 32/56000 datapoints
2025-03-04 14:42:51,773 - INFO - training batch 51, loss: 0.117, 1632/56000 datapoints
2025-03-04 14:42:51,840 - INFO - training batch 101, loss: 0.061, 3232/56000 datapoints
2025-03-04 14:42:51,910 - INFO - training batch 151, loss: 0.111, 4832/56000 datapoints
2025-03-04 14:42:51,974 - INFO - training batch 201, loss: 0.051, 6432/56000 datapoints
2025-03-04 14:42:52,041 - INFO - training batch 251, loss: 0.062, 8032/56000 datapoints
2025-03-04 14:42:52,109 - INFO - training batch 301, loss: 0.075, 9632/56000 datapoints
2025-03-04 14:42:52,184 - INFO - training batch 351, loss: 0.091, 11232/56000 datapoints
2025-03-04 14:42:52,257 - INFO - training batch 401, loss: 0.178, 12832/56000 datapoints
2025-03-04 14:42:52,325 - INFO - training batch 451, loss: 0.129, 14432/56000 datapoints
2025-03-04 14:42:52,388 - INFO - training batch 501, loss: 0.111, 16032/56000 datapoints
2025-03-04 14:42:52,452 - INFO - training batch 551, loss: 0.124, 17632/56000 datapoints
2025-03-04 14:42:52,519 - INFO - training batch 601, loss: 0.049, 19232/56000 datapoints
2025-03-04 14:42:52,582 - INFO - training batch 651, loss: 0.048, 20832/56000 datapoints
2025-03-04 14:42:52,651 - INFO - training batch 701, loss: 0.264, 22432/56000 datapoints
2025-03-04 14:42:52,721 - INFO - training batch 751, loss: 0.074, 24032/56000 datapoints
2025-03-04 14:42:52,789 - INFO - training batch 801, loss: 0.099, 25632/56000 datapoints
2025-03-04 14:42:52,851 - INFO - training batch 851, loss: 0.053, 27232/56000 datapoints
2025-03-04 14:42:52,924 - INFO - training batch 901, loss: 0.091, 28832/56000 datapoints
2025-03-04 14:42:53,006 - INFO - training batch 951, loss: 0.091, 30432/56000 datapoints
2025-03-04 14:42:53,077 - INFO - training batch 1001, loss: 0.041, 32032/56000 datapoints
2025-03-04 14:42:53,141 - INFO - training batch 1051, loss: 0.050, 33632/56000 datapoints
2025-03-04 14:42:53,209 - INFO - training batch 1101, loss: 0.122, 35232/56000 datapoints
2025-03-04 14:42:53,289 - INFO - training batch 1151, loss: 0.019, 36832/56000 datapoints
2025-03-04 14:42:53,362 - INFO - training batch 1201, loss: 0.082, 38432/56000 datapoints
2025-03-04 14:42:53,432 - INFO - training batch 1251, loss: 0.262, 40032/56000 datapoints
2025-03-04 14:42:53,497 - INFO - training batch 1301, loss: 0.073, 41632/56000 datapoints
2025-03-04 14:42:53,560 - INFO - training batch 1351, loss: 0.039, 43232/56000 datapoints
2025-03-04 14:42:53,631 - INFO - training batch 1401, loss: 0.040, 44832/56000 datapoints
2025-03-04 14:42:53,700 - INFO - training batch 1451, loss: 0.042, 46432/56000 datapoints
2025-03-04 14:42:53,768 - INFO - training batch 1501, loss: 0.165, 48032/56000 datapoints
2025-03-04 14:42:53,837 - INFO - training batch 1551, loss: 0.082, 49632/56000 datapoints
2025-03-04 14:42:53,908 - INFO - training batch 1601, loss: 0.107, 51232/56000 datapoints
2025-03-04 14:42:53,972 - INFO - training batch 1651, loss: 0.048, 52832/56000 datapoints
2025-03-04 14:42:54,037 - INFO - training batch 1701, loss: 0.086, 54432/56000 datapoints
2025-03-04 14:42:54,137 - INFO - validation batch 1, loss: 0.027, 32/13984 datapoints
2025-03-04 14:42:54,166 - INFO - validation batch 51, loss: 0.014, 1632/13984 datapoints
2025-03-04 14:42:54,197 - INFO - validation batch 101, loss: 0.084, 3232/13984 datapoints
2025-03-04 14:42:54,231 - INFO - validation batch 151, loss: 0.028, 4832/13984 datapoints
2025-03-04 14:42:54,263 - INFO - validation batch 201, loss: 0.062, 6432/13984 datapoints
2025-03-04 14:42:54,296 - INFO - validation batch 251, loss: 0.093, 8032/13984 datapoints
2025-03-04 14:42:54,342 - INFO - validation batch 301, loss: 0.148, 9632/13984 datapoints
2025-03-04 14:42:54,369 - INFO - validation batch 351, loss: 0.042, 11232/13984 datapoints
2025-03-04 14:42:54,392 - INFO - validation batch 401, loss: 0.167, 12832/13984 datapoints
2025-03-04 14:42:54,410 - INFO - Epoch 12/50 done
2025-03-04 14:42:54,411 - INFO - Beginning epoch 13/50
2025-03-04 14:42:54,413 - INFO - training batch 1, loss: 0.129, 32/56000 datapoints
2025-03-04 14:42:54,483 - INFO - training batch 51, loss: 0.116, 1632/56000 datapoints
2025-03-04 14:42:54,554 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-04 14:42:54,632 - INFO - training batch 151, loss: 0.110, 4832/56000 datapoints
2025-03-04 14:42:54,704 - INFO - training batch 201, loss: 0.050, 6432/56000 datapoints
2025-03-04 14:42:54,773 - INFO - training batch 251, loss: 0.061, 8032/56000 datapoints
2025-03-04 14:42:54,845 - INFO - training batch 301, loss: 0.074, 9632/56000 datapoints
2025-03-04 14:42:54,917 - INFO - training batch 351, loss: 0.089, 11232/56000 datapoints
2025-03-04 14:42:54,983 - INFO - training batch 401, loss: 0.177, 12832/56000 datapoints
2025-03-04 14:42:55,057 - INFO - training batch 451, loss: 0.126, 14432/56000 datapoints
2025-03-04 14:42:55,142 - INFO - training batch 501, loss: 0.108, 16032/56000 datapoints
2025-03-04 14:42:55,216 - INFO - training batch 551, loss: 0.124, 17632/56000 datapoints
2025-03-04 14:42:55,285 - INFO - training batch 601, loss: 0.049, 19232/56000 datapoints
2025-03-04 14:42:55,356 - INFO - training batch 651, loss: 0.047, 20832/56000 datapoints
2025-03-04 14:42:55,420 - INFO - training batch 701, loss: 0.266, 22432/56000 datapoints
2025-03-04 14:42:55,506 - INFO - training batch 751, loss: 0.073, 24032/56000 datapoints
2025-03-04 14:42:55,649 - INFO - training batch 801, loss: 0.097, 25632/56000 datapoints
2025-03-04 14:42:55,746 - INFO - training batch 851, loss: 0.052, 27232/56000 datapoints
2025-03-04 14:42:55,951 - INFO - training batch 901, loss: 0.089, 28832/56000 datapoints
2025-03-04 14:42:56,228 - INFO - training batch 951, loss: 0.090, 30432/56000 datapoints
2025-03-04 14:42:56,410 - INFO - training batch 1001, loss: 0.040, 32032/56000 datapoints
2025-03-04 14:42:56,562 - INFO - training batch 1051, loss: 0.049, 33632/56000 datapoints
2025-03-04 14:42:56,741 - INFO - training batch 1101, loss: 0.122, 35232/56000 datapoints
2025-03-04 14:42:56,924 - INFO - training batch 1151, loss: 0.018, 36832/56000 datapoints
2025-03-04 14:42:57,138 - INFO - training batch 1201, loss: 0.078, 38432/56000 datapoints
2025-03-04 14:42:57,326 - INFO - training batch 1251, loss: 0.261, 40032/56000 datapoints
2025-03-04 14:42:57,477 - INFO - training batch 1301, loss: 0.072, 41632/56000 datapoints
2025-03-04 14:42:57,590 - INFO - training batch 1351, loss: 0.038, 43232/56000 datapoints
2025-03-04 14:42:57,794 - INFO - training batch 1401, loss: 0.039, 44832/56000 datapoints
2025-03-04 14:42:58,041 - INFO - training batch 1451, loss: 0.041, 46432/56000 datapoints
2025-03-04 14:42:58,185 - INFO - training batch 1501, loss: 0.163, 48032/56000 datapoints
2025-03-04 14:42:58,302 - INFO - training batch 1551, loss: 0.081, 49632/56000 datapoints
2025-03-04 14:42:58,926 - INFO - training batch 1601, loss: 0.105, 51232/56000 datapoints
2025-03-04 14:42:59,192 - INFO - training batch 1651, loss: 0.046, 52832/56000 datapoints
2025-03-04 14:42:59,322 - INFO - training batch 1701, loss: 0.085, 54432/56000 datapoints
2025-03-04 14:42:59,433 - INFO - validation batch 1, loss: 0.026, 32/13984 datapoints
2025-03-04 14:42:59,488 - INFO - validation batch 51, loss: 0.013, 1632/13984 datapoints
2025-03-04 14:42:59,673 - INFO - validation batch 101, loss: 0.083, 3232/13984 datapoints
2025-03-04 14:42:59,706 - INFO - validation batch 151, loss: 0.028, 4832/13984 datapoints
2025-03-04 14:42:59,768 - INFO - validation batch 201, loss: 0.060, 6432/13984 datapoints
2025-03-04 14:42:59,826 - INFO - validation batch 251, loss: 0.091, 8032/13984 datapoints
2025-03-04 14:42:59,862 - INFO - validation batch 301, loss: 0.146, 9632/13984 datapoints
2025-03-04 14:42:59,898 - INFO - validation batch 351, loss: 0.041, 11232/13984 datapoints
2025-03-04 14:42:59,929 - INFO - validation batch 401, loss: 0.166, 12832/13984 datapoints
2025-03-04 14:42:59,958 - INFO - Epoch 13/50 done
2025-03-04 14:42:59,958 - INFO - Beginning epoch 14/50
2025-03-04 14:42:59,962 - INFO - training batch 1, loss: 0.128, 32/56000 datapoints
2025-03-04 14:43:00,073 - INFO - training batch 51, loss: 0.114, 1632/56000 datapoints
2025-03-04 14:43:00,172 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-04 14:43:00,272 - INFO - training batch 151, loss: 0.109, 4832/56000 datapoints
2025-03-04 14:43:00,372 - INFO - training batch 201, loss: 0.050, 6432/56000 datapoints
2025-03-04 14:43:00,475 - INFO - training batch 251, loss: 0.060, 8032/56000 datapoints
2025-03-04 14:43:00,582 - INFO - training batch 301, loss: 0.072, 9632/56000 datapoints
2025-03-04 14:43:00,719 - INFO - training batch 351, loss: 0.089, 11232/56000 datapoints
2025-03-04 14:43:00,851 - INFO - training batch 401, loss: 0.176, 12832/56000 datapoints
2025-03-04 14:43:00,950 - INFO - training batch 451, loss: 0.124, 14432/56000 datapoints
2025-03-04 14:43:01,033 - INFO - training batch 501, loss: 0.105, 16032/56000 datapoints
2025-03-04 14:43:01,103 - INFO - training batch 551, loss: 0.123, 17632/56000 datapoints
2025-03-04 14:43:01,174 - INFO - training batch 601, loss: 0.048, 19232/56000 datapoints
2025-03-04 14:43:01,246 - INFO - training batch 651, loss: 0.046, 20832/56000 datapoints
2025-03-04 14:43:01,320 - INFO - training batch 701, loss: 0.268, 22432/56000 datapoints
2025-03-04 14:43:01,390 - INFO - training batch 751, loss: 0.071, 24032/56000 datapoints
2025-03-04 14:43:01,462 - INFO - training batch 801, loss: 0.095, 25632/56000 datapoints
2025-03-04 14:43:01,550 - INFO - training batch 851, loss: 0.051, 27232/56000 datapoints
2025-03-04 14:43:01,640 - INFO - training batch 901, loss: 0.088, 28832/56000 datapoints
2025-03-04 14:43:01,754 - INFO - training batch 951, loss: 0.089, 30432/56000 datapoints
2025-03-04 14:43:01,836 - INFO - training batch 1001, loss: 0.039, 32032/56000 datapoints
2025-03-04 14:43:01,935 - INFO - training batch 1051, loss: 0.048, 33632/56000 datapoints
2025-03-04 14:43:02,019 - INFO - training batch 1101, loss: 0.121, 35232/56000 datapoints
2025-03-04 14:43:02,103 - INFO - training batch 1151, loss: 0.018, 36832/56000 datapoints
2025-03-04 14:43:02,178 - INFO - training batch 1201, loss: 0.075, 38432/56000 datapoints
2025-03-04 14:43:02,253 - INFO - training batch 1251, loss: 0.261, 40032/56000 datapoints
2025-03-04 14:43:02,326 - INFO - training batch 1301, loss: 0.070, 41632/56000 datapoints
2025-03-04 14:43:02,399 - INFO - training batch 1351, loss: 0.037, 43232/56000 datapoints
2025-03-04 14:43:02,473 - INFO - training batch 1401, loss: 0.038, 44832/56000 datapoints
2025-03-04 14:43:02,545 - INFO - training batch 1451, loss: 0.040, 46432/56000 datapoints
2025-03-04 14:43:02,631 - INFO - training batch 1501, loss: 0.162, 48032/56000 datapoints
2025-03-04 14:43:02,713 - INFO - training batch 1551, loss: 0.080, 49632/56000 datapoints
2025-03-04 14:43:02,790 - INFO - training batch 1601, loss: 0.103, 51232/56000 datapoints
2025-03-04 14:43:02,881 - INFO - training batch 1651, loss: 0.046, 52832/56000 datapoints
2025-03-04 14:43:02,980 - INFO - training batch 1701, loss: 0.084, 54432/56000 datapoints
2025-03-04 14:43:03,072 - INFO - validation batch 1, loss: 0.026, 32/13984 datapoints
2025-03-04 14:43:03,099 - INFO - validation batch 51, loss: 0.013, 1632/13984 datapoints
2025-03-04 14:43:03,134 - INFO - validation batch 101, loss: 0.082, 3232/13984 datapoints
2025-03-04 14:43:03,164 - INFO - validation batch 151, loss: 0.027, 4832/13984 datapoints
2025-03-04 14:43:03,202 - INFO - validation batch 201, loss: 0.059, 6432/13984 datapoints
2025-03-04 14:43:03,240 - INFO - validation batch 251, loss: 0.089, 8032/13984 datapoints
2025-03-04 14:43:03,277 - INFO - validation batch 301, loss: 0.144, 9632/13984 datapoints
2025-03-04 14:43:03,306 - INFO - validation batch 351, loss: 0.040, 11232/13984 datapoints
2025-03-04 14:43:03,336 - INFO - validation batch 401, loss: 0.164, 12832/13984 datapoints
2025-03-04 14:43:03,361 - INFO - Epoch 14/50 done
2025-03-04 14:43:03,362 - INFO - Beginning epoch 15/50
2025-03-04 14:43:03,365 - INFO - training batch 1, loss: 0.127, 32/56000 datapoints
2025-03-04 14:43:03,460 - INFO - training batch 51, loss: 0.113, 1632/56000 datapoints
2025-03-04 14:43:03,571 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-04 14:43:03,667 - INFO - training batch 151, loss: 0.108, 4832/56000 datapoints
2025-03-04 14:43:03,762 - INFO - training batch 201, loss: 0.049, 6432/56000 datapoints
2025-03-04 14:43:03,872 - INFO - training batch 251, loss: 0.059, 8032/56000 datapoints
2025-03-04 14:43:03,992 - INFO - training batch 301, loss: 0.071, 9632/56000 datapoints
2025-03-04 14:43:04,107 - INFO - training batch 351, loss: 0.088, 11232/56000 datapoints
2025-03-04 14:43:04,214 - INFO - training batch 401, loss: 0.174, 12832/56000 datapoints
2025-03-04 14:43:04,336 - INFO - training batch 451, loss: 0.122, 14432/56000 datapoints
2025-03-04 14:43:04,450 - INFO - training batch 501, loss: 0.104, 16032/56000 datapoints
2025-03-04 14:43:04,549 - INFO - training batch 551, loss: 0.123, 17632/56000 datapoints
2025-03-04 14:43:04,646 - INFO - training batch 601, loss: 0.048, 19232/56000 datapoints
2025-03-04 14:43:04,743 - INFO - training batch 651, loss: 0.046, 20832/56000 datapoints
2025-03-04 14:43:04,829 - INFO - training batch 701, loss: 0.270, 22432/56000 datapoints
2025-03-04 14:43:04,935 - INFO - training batch 751, loss: 0.070, 24032/56000 datapoints
2025-03-04 14:43:05,018 - INFO - training batch 801, loss: 0.094, 25632/56000 datapoints
2025-03-04 14:43:05,099 - INFO - training batch 851, loss: 0.050, 27232/56000 datapoints
2025-03-04 14:43:05,175 - INFO - training batch 901, loss: 0.087, 28832/56000 datapoints
2025-03-04 14:43:05,279 - INFO - training batch 951, loss: 0.087, 30432/56000 datapoints
2025-03-04 14:43:05,373 - INFO - training batch 1001, loss: 0.039, 32032/56000 datapoints
2025-03-04 14:43:05,454 - INFO - training batch 1051, loss: 0.047, 33632/56000 datapoints
2025-03-04 14:43:05,533 - INFO - training batch 1101, loss: 0.120, 35232/56000 datapoints
2025-03-04 14:43:05,615 - INFO - training batch 1151, loss: 0.018, 36832/56000 datapoints
2025-03-04 14:43:05,693 - INFO - training batch 1201, loss: 0.071, 38432/56000 datapoints
2025-03-04 14:43:05,769 - INFO - training batch 1251, loss: 0.260, 40032/56000 datapoints
2025-03-04 14:43:05,850 - INFO - training batch 1301, loss: 0.069, 41632/56000 datapoints
2025-03-04 14:43:05,932 - INFO - training batch 1351, loss: 0.037, 43232/56000 datapoints
2025-03-04 14:43:06,014 - INFO - training batch 1401, loss: 0.037, 44832/56000 datapoints
2025-03-04 14:43:06,100 - INFO - training batch 1451, loss: 0.039, 46432/56000 datapoints
2025-03-04 14:43:06,188 - INFO - training batch 1501, loss: 0.160, 48032/56000 datapoints
2025-03-04 14:43:06,266 - INFO - training batch 1551, loss: 0.079, 49632/56000 datapoints
2025-03-04 14:43:06,352 - INFO - training batch 1601, loss: 0.101, 51232/56000 datapoints
2025-03-04 14:43:06,428 - INFO - training batch 1651, loss: 0.045, 52832/56000 datapoints
2025-03-04 14:43:06,497 - INFO - training batch 1701, loss: 0.083, 54432/56000 datapoints
2025-03-04 14:43:06,568 - INFO - validation batch 1, loss: 0.025, 32/13984 datapoints
2025-03-04 14:43:06,589 - INFO - validation batch 51, loss: 0.013, 1632/13984 datapoints
2025-03-04 14:43:06,613 - INFO - validation batch 101, loss: 0.081, 3232/13984 datapoints
2025-03-04 14:43:06,636 - INFO - validation batch 151, loss: 0.026, 4832/13984 datapoints
2025-03-04 14:43:06,660 - INFO - validation batch 201, loss: 0.057, 6432/13984 datapoints
2025-03-04 14:43:06,687 - INFO - validation batch 251, loss: 0.087, 8032/13984 datapoints
2025-03-04 14:43:06,710 - INFO - validation batch 301, loss: 0.143, 9632/13984 datapoints
2025-03-04 14:43:06,738 - INFO - validation batch 351, loss: 0.039, 11232/13984 datapoints
2025-03-04 14:43:06,760 - INFO - validation batch 401, loss: 0.163, 12832/13984 datapoints
2025-03-04 14:43:06,784 - INFO - Epoch 15/50 done
2025-03-04 14:43:06,785 - INFO - Beginning epoch 16/50
2025-03-04 14:43:06,788 - INFO - training batch 1, loss: 0.127, 32/56000 datapoints
2025-03-04 14:43:06,862 - INFO - training batch 51, loss: 0.111, 1632/56000 datapoints
2025-03-04 14:43:07,059 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-04 14:43:07,255 - INFO - training batch 151, loss: 0.107, 4832/56000 datapoints
2025-03-04 14:43:07,387 - INFO - training batch 201, loss: 0.048, 6432/56000 datapoints
2025-03-04 14:43:07,678 - INFO - training batch 251, loss: 0.059, 8032/56000 datapoints
2025-03-04 14:43:07,817 - INFO - training batch 301, loss: 0.070, 9632/56000 datapoints
2025-03-04 14:43:07,984 - INFO - training batch 351, loss: 0.087, 11232/56000 datapoints
2025-03-04 14:43:08,123 - INFO - training batch 401, loss: 0.173, 12832/56000 datapoints
2025-03-04 14:43:08,240 - INFO - training batch 451, loss: 0.121, 14432/56000 datapoints
2025-03-04 14:43:08,337 - INFO - training batch 501, loss: 0.102, 16032/56000 datapoints
2025-03-04 14:43:08,464 - INFO - training batch 551, loss: 0.122, 17632/56000 datapoints
2025-03-04 14:43:08,563 - INFO - training batch 601, loss: 0.047, 19232/56000 datapoints
2025-03-04 14:43:08,654 - INFO - training batch 651, loss: 0.045, 20832/56000 datapoints
2025-03-04 14:43:08,758 - INFO - training batch 701, loss: 0.271, 22432/56000 datapoints
2025-03-04 14:43:08,872 - INFO - training batch 751, loss: 0.069, 24032/56000 datapoints
2025-03-04 14:43:08,988 - INFO - training batch 801, loss: 0.092, 25632/56000 datapoints
2025-03-04 14:43:09,102 - INFO - training batch 851, loss: 0.050, 27232/56000 datapoints
2025-03-04 14:43:09,190 - INFO - training batch 901, loss: 0.086, 28832/56000 datapoints
2025-03-04 14:43:09,303 - INFO - training batch 951, loss: 0.086, 30432/56000 datapoints
2025-03-04 14:43:09,397 - INFO - training batch 1001, loss: 0.038, 32032/56000 datapoints
2025-03-04 14:43:09,524 - INFO - training batch 1051, loss: 0.046, 33632/56000 datapoints
2025-03-04 14:43:09,640 - INFO - training batch 1101, loss: 0.119, 35232/56000 datapoints
2025-03-04 14:43:09,871 - INFO - training batch 1151, loss: 0.017, 36832/56000 datapoints
2025-03-04 14:43:09,977 - INFO - training batch 1201, loss: 0.069, 38432/56000 datapoints
2025-03-04 14:43:10,076 - INFO - training batch 1251, loss: 0.258, 40032/56000 datapoints
2025-03-04 14:43:10,168 - INFO - training batch 1301, loss: 0.068, 41632/56000 datapoints
2025-03-04 14:43:10,257 - INFO - training batch 1351, loss: 0.036, 43232/56000 datapoints
2025-03-04 14:43:10,356 - INFO - training batch 1401, loss: 0.037, 44832/56000 datapoints
2025-03-04 14:43:10,448 - INFO - training batch 1451, loss: 0.038, 46432/56000 datapoints
2025-03-04 14:43:10,553 - INFO - training batch 1501, loss: 0.158, 48032/56000 datapoints
2025-03-04 14:43:10,646 - INFO - training batch 1551, loss: 0.078, 49632/56000 datapoints
2025-03-04 14:43:10,737 - INFO - training batch 1601, loss: 0.100, 51232/56000 datapoints
2025-03-04 14:43:10,831 - INFO - training batch 1651, loss: 0.044, 52832/56000 datapoints
2025-03-04 14:43:11,196 - INFO - training batch 1701, loss: 0.083, 54432/56000 datapoints
2025-03-04 14:43:11,297 - INFO - validation batch 1, loss: 0.025, 32/13984 datapoints
2025-03-04 14:43:11,337 - INFO - validation batch 51, loss: 0.012, 1632/13984 datapoints
2025-03-04 14:43:11,374 - INFO - validation batch 101, loss: 0.080, 3232/13984 datapoints
2025-03-04 14:43:11,414 - INFO - validation batch 151, loss: 0.026, 4832/13984 datapoints
2025-03-04 14:43:11,464 - INFO - validation batch 201, loss: 0.056, 6432/13984 datapoints
2025-03-04 14:43:11,520 - INFO - validation batch 251, loss: 0.085, 8032/13984 datapoints
2025-03-04 14:43:11,570 - INFO - validation batch 301, loss: 0.141, 9632/13984 datapoints
2025-03-04 14:43:11,613 - INFO - validation batch 351, loss: 0.038, 11232/13984 datapoints
2025-03-04 14:43:11,659 - INFO - validation batch 401, loss: 0.162, 12832/13984 datapoints
2025-03-04 14:43:11,691 - INFO - Epoch 16/50 done
2025-03-04 14:43:11,692 - INFO - Beginning epoch 17/50
2025-03-04 14:43:11,694 - INFO - training batch 1, loss: 0.126, 32/56000 datapoints
2025-03-04 14:43:11,809 - INFO - training batch 51, loss: 0.110, 1632/56000 datapoints
2025-03-04 14:43:11,904 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-04 14:43:12,012 - INFO - training batch 151, loss: 0.106, 4832/56000 datapoints
2025-03-04 14:43:12,111 - INFO - training batch 201, loss: 0.048, 6432/56000 datapoints
2025-03-04 14:43:12,191 - INFO - training batch 251, loss: 0.058, 8032/56000 datapoints
2025-03-04 14:43:12,266 - INFO - training batch 301, loss: 0.069, 9632/56000 datapoints
2025-03-04 14:43:12,342 - INFO - training batch 351, loss: 0.086, 11232/56000 datapoints
2025-03-04 14:43:12,417 - INFO - training batch 401, loss: 0.171, 12832/56000 datapoints
2025-03-04 14:43:12,502 - INFO - training batch 451, loss: 0.119, 14432/56000 datapoints
2025-03-04 14:43:12,583 - INFO - training batch 501, loss: 0.100, 16032/56000 datapoints
2025-03-04 14:43:12,653 - INFO - training batch 551, loss: 0.121, 17632/56000 datapoints
2025-03-04 14:43:12,724 - INFO - training batch 601, loss: 0.047, 19232/56000 datapoints
2025-03-04 14:43:12,803 - INFO - training batch 651, loss: 0.044, 20832/56000 datapoints
2025-03-04 14:43:12,881 - INFO - training batch 701, loss: 0.272, 22432/56000 datapoints
2025-03-04 14:43:12,961 - INFO - training batch 751, loss: 0.068, 24032/56000 datapoints
2025-03-04 14:43:13,067 - INFO - training batch 801, loss: 0.091, 25632/56000 datapoints
2025-03-04 14:43:13,199 - INFO - training batch 851, loss: 0.049, 27232/56000 datapoints
2025-03-04 14:43:13,311 - INFO - training batch 901, loss: 0.085, 28832/56000 datapoints
2025-03-04 14:43:13,428 - INFO - training batch 951, loss: 0.085, 30432/56000 datapoints
2025-03-04 14:43:13,556 - INFO - training batch 1001, loss: 0.038, 32032/56000 datapoints
2025-03-04 14:43:13,661 - INFO - training batch 1051, loss: 0.045, 33632/56000 datapoints
2025-03-04 14:43:13,764 - INFO - training batch 1101, loss: 0.118, 35232/56000 datapoints
2025-03-04 14:43:13,847 - INFO - training batch 1151, loss: 0.017, 36832/56000 datapoints
2025-03-04 14:43:13,932 - INFO - training batch 1201, loss: 0.066, 38432/56000 datapoints
2025-03-04 14:43:14,004 - INFO - training batch 1251, loss: 0.257, 40032/56000 datapoints
2025-03-04 14:43:14,070 - INFO - training batch 1301, loss: 0.067, 41632/56000 datapoints
2025-03-04 14:43:14,135 - INFO - training batch 1351, loss: 0.035, 43232/56000 datapoints
2025-03-04 14:43:14,208 - INFO - training batch 1401, loss: 0.036, 44832/56000 datapoints
2025-03-04 14:43:14,283 - INFO - training batch 1451, loss: 0.037, 46432/56000 datapoints
2025-03-04 14:43:14,359 - INFO - training batch 1501, loss: 0.157, 48032/56000 datapoints
2025-03-04 14:43:14,430 - INFO - training batch 1551, loss: 0.077, 49632/56000 datapoints
2025-03-04 14:43:14,496 - INFO - training batch 1601, loss: 0.098, 51232/56000 datapoints
2025-03-04 14:43:14,570 - INFO - training batch 1651, loss: 0.043, 52832/56000 datapoints
2025-03-04 14:43:14,637 - INFO - training batch 1701, loss: 0.082, 54432/56000 datapoints
2025-03-04 14:43:14,706 - INFO - validation batch 1, loss: 0.024, 32/13984 datapoints
2025-03-04 14:43:14,730 - INFO - validation batch 51, loss: 0.012, 1632/13984 datapoints
2025-03-04 14:43:14,756 - INFO - validation batch 101, loss: 0.079, 3232/13984 datapoints
2025-03-04 14:43:14,781 - INFO - validation batch 151, loss: 0.025, 4832/13984 datapoints
2025-03-04 14:43:14,802 - INFO - validation batch 201, loss: 0.055, 6432/13984 datapoints
2025-03-04 14:43:14,828 - INFO - validation batch 251, loss: 0.084, 8032/13984 datapoints
2025-03-04 14:43:14,850 - INFO - validation batch 301, loss: 0.139, 9632/13984 datapoints
2025-03-04 14:43:14,877 - INFO - validation batch 351, loss: 0.038, 11232/13984 datapoints
2025-03-04 14:43:14,907 - INFO - validation batch 401, loss: 0.161, 12832/13984 datapoints
2025-03-04 14:43:14,923 - INFO - Epoch 17/50 done
2025-03-04 14:43:14,923 - INFO - Beginning epoch 18/50
2025-03-04 14:43:14,925 - INFO - training batch 1, loss: 0.125, 32/56000 datapoints
2025-03-04 14:43:14,997 - INFO - training batch 51, loss: 0.109, 1632/56000 datapoints
2025-03-04 14:43:15,070 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-04 14:43:15,138 - INFO - training batch 151, loss: 0.106, 4832/56000 datapoints
2025-03-04 14:43:15,204 - INFO - training batch 201, loss: 0.047, 6432/56000 datapoints
2025-03-04 14:43:15,274 - INFO - training batch 251, loss: 0.057, 8032/56000 datapoints
2025-03-04 14:43:15,341 - INFO - training batch 301, loss: 0.068, 9632/56000 datapoints
2025-03-04 14:43:15,413 - INFO - training batch 351, loss: 0.085, 11232/56000 datapoints
2025-03-04 14:43:15,484 - INFO - training batch 401, loss: 0.170, 12832/56000 datapoints
2025-03-04 14:43:15,561 - INFO - training batch 451, loss: 0.117, 14432/56000 datapoints
2025-03-04 14:43:15,631 - INFO - training batch 501, loss: 0.099, 16032/56000 datapoints
2025-03-04 14:43:15,702 - INFO - training batch 551, loss: 0.120, 17632/56000 datapoints
2025-03-04 14:43:15,778 - INFO - training batch 601, loss: 0.046, 19232/56000 datapoints
2025-03-04 14:43:15,844 - INFO - training batch 651, loss: 0.044, 20832/56000 datapoints
2025-03-04 14:43:15,919 - INFO - training batch 701, loss: 0.272, 22432/56000 datapoints
2025-03-04 14:43:15,989 - INFO - training batch 751, loss: 0.067, 24032/56000 datapoints
2025-03-04 14:43:16,063 - INFO - training batch 801, loss: 0.090, 25632/56000 datapoints
2025-03-04 14:43:16,134 - INFO - training batch 851, loss: 0.048, 27232/56000 datapoints
2025-03-04 14:43:16,201 - INFO - training batch 901, loss: 0.084, 28832/56000 datapoints
2025-03-04 14:43:16,269 - INFO - training batch 951, loss: 0.084, 30432/56000 datapoints
2025-03-04 14:43:16,332 - INFO - training batch 1001, loss: 0.037, 32032/56000 datapoints
2025-03-04 14:43:16,396 - INFO - training batch 1051, loss: 0.044, 33632/56000 datapoints
2025-03-04 14:43:16,463 - INFO - training batch 1101, loss: 0.118, 35232/56000 datapoints
2025-03-04 14:43:16,538 - INFO - training batch 1151, loss: 0.017, 36832/56000 datapoints
2025-03-04 14:43:16,624 - INFO - training batch 1201, loss: 0.064, 38432/56000 datapoints
2025-03-04 14:43:16,696 - INFO - training batch 1251, loss: 0.256, 40032/56000 datapoints
2025-03-04 14:43:16,764 - INFO - training batch 1301, loss: 0.065, 41632/56000 datapoints
2025-03-04 14:43:16,831 - INFO - training batch 1351, loss: 0.035, 43232/56000 datapoints
2025-03-04 14:43:16,901 - INFO - training batch 1401, loss: 0.035, 44832/56000 datapoints
2025-03-04 14:43:16,976 - INFO - training batch 1451, loss: 0.037, 46432/56000 datapoints
2025-03-04 14:43:17,051 - INFO - training batch 1501, loss: 0.155, 48032/56000 datapoints
2025-03-04 14:43:17,115 - INFO - training batch 1551, loss: 0.076, 49632/56000 datapoints
2025-03-04 14:43:17,178 - INFO - training batch 1601, loss: 0.096, 51232/56000 datapoints
2025-03-04 14:43:17,244 - INFO - training batch 1651, loss: 0.043, 52832/56000 datapoints
2025-03-04 14:43:17,308 - INFO - training batch 1701, loss: 0.081, 54432/56000 datapoints
2025-03-04 14:43:17,373 - INFO - validation batch 1, loss: 0.024, 32/13984 datapoints
2025-03-04 14:43:17,396 - INFO - validation batch 51, loss: 0.012, 1632/13984 datapoints
2025-03-04 14:43:17,420 - INFO - validation batch 101, loss: 0.078, 3232/13984 datapoints
2025-03-04 14:43:17,449 - INFO - validation batch 151, loss: 0.025, 4832/13984 datapoints
2025-03-04 14:43:17,474 - INFO - validation batch 201, loss: 0.054, 6432/13984 datapoints
2025-03-04 14:43:17,503 - INFO - validation batch 251, loss: 0.082, 8032/13984 datapoints
2025-03-04 14:43:17,524 - INFO - validation batch 301, loss: 0.138, 9632/13984 datapoints
2025-03-04 14:43:17,549 - INFO - validation batch 351, loss: 0.037, 11232/13984 datapoints
2025-03-04 14:43:17,572 - INFO - validation batch 401, loss: 0.160, 12832/13984 datapoints
2025-03-04 14:43:17,609 - INFO - Epoch 18/50 done
2025-03-04 14:43:17,610 - INFO - Beginning epoch 19/50
2025-03-04 14:43:17,612 - INFO - training batch 1, loss: 0.125, 32/56000 datapoints
2025-03-04 14:43:17,764 - INFO - training batch 51, loss: 0.108, 1632/56000 datapoints
2025-03-04 14:43:17,874 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-04 14:43:18,068 - INFO - training batch 151, loss: 0.105, 4832/56000 datapoints
2025-03-04 14:43:18,223 - INFO - training batch 201, loss: 0.047, 6432/56000 datapoints
2025-03-04 14:43:18,610 - INFO - training batch 251, loss: 0.057, 8032/56000 datapoints
2025-03-04 14:43:18,727 - INFO - training batch 301, loss: 0.067, 9632/56000 datapoints
2025-03-04 14:43:18,822 - INFO - training batch 351, loss: 0.085, 11232/56000 datapoints
2025-03-04 14:43:18,934 - INFO - training batch 401, loss: 0.169, 12832/56000 datapoints
2025-03-04 14:43:19,029 - INFO - training batch 451, loss: 0.116, 14432/56000 datapoints
2025-03-04 14:43:19,112 - INFO - training batch 501, loss: 0.098, 16032/56000 datapoints
2025-03-04 14:43:19,204 - INFO - training batch 551, loss: 0.119, 17632/56000 datapoints
2025-03-04 14:43:19,295 - INFO - training batch 601, loss: 0.045, 19232/56000 datapoints
2025-03-04 14:43:19,391 - INFO - training batch 651, loss: 0.043, 20832/56000 datapoints
2025-03-04 14:43:19,477 - INFO - training batch 701, loss: 0.273, 22432/56000 datapoints
2025-03-04 14:43:19,565 - INFO - training batch 751, loss: 0.066, 24032/56000 datapoints
2025-03-04 14:43:19,657 - INFO - training batch 801, loss: 0.089, 25632/56000 datapoints
2025-03-04 14:43:19,740 - INFO - training batch 851, loss: 0.048, 27232/56000 datapoints
2025-03-04 14:43:19,835 - INFO - training batch 901, loss: 0.083, 28832/56000 datapoints
2025-03-04 14:43:19,929 - INFO - training batch 951, loss: 0.083, 30432/56000 datapoints
2025-03-04 14:43:20,016 - INFO - training batch 1001, loss: 0.037, 32032/56000 datapoints
2025-03-04 14:43:20,115 - INFO - training batch 1051, loss: 0.044, 33632/56000 datapoints
2025-03-04 14:43:20,238 - INFO - training batch 1101, loss: 0.117, 35232/56000 datapoints
2025-03-04 14:43:20,331 - INFO - training batch 1151, loss: 0.017, 36832/56000 datapoints
2025-03-04 14:43:20,420 - INFO - training batch 1201, loss: 0.062, 38432/56000 datapoints
2025-03-04 14:43:20,515 - INFO - training batch 1251, loss: 0.254, 40032/56000 datapoints
2025-03-04 14:43:20,611 - INFO - training batch 1301, loss: 0.064, 41632/56000 datapoints
2025-03-04 14:43:20,700 - INFO - training batch 1351, loss: 0.034, 43232/56000 datapoints
2025-03-04 14:43:20,786 - INFO - training batch 1401, loss: 0.035, 44832/56000 datapoints
2025-03-04 14:43:20,876 - INFO - training batch 1451, loss: 0.036, 46432/56000 datapoints
2025-03-04 14:43:20,973 - INFO - training batch 1501, loss: 0.154, 48032/56000 datapoints
2025-03-04 14:43:21,062 - INFO - training batch 1551, loss: 0.075, 49632/56000 datapoints
2025-03-04 14:43:21,148 - INFO - training batch 1601, loss: 0.095, 51232/56000 datapoints
2025-03-04 14:43:21,234 - INFO - training batch 1651, loss: 0.042, 52832/56000 datapoints
2025-03-04 14:43:21,321 - INFO - training batch 1701, loss: 0.081, 54432/56000 datapoints
2025-03-04 14:43:21,432 - INFO - validation batch 1, loss: 0.023, 32/13984 datapoints
2025-03-04 14:43:21,490 - INFO - validation batch 51, loss: 0.012, 1632/13984 datapoints
2025-03-04 14:43:21,521 - INFO - validation batch 101, loss: 0.077, 3232/13984 datapoints
2025-03-04 14:43:21,589 - INFO - validation batch 151, loss: 0.024, 4832/13984 datapoints
2025-03-04 14:43:21,669 - INFO - validation batch 201, loss: 0.053, 6432/13984 datapoints
2025-03-04 14:43:21,710 - INFO - validation batch 251, loss: 0.081, 8032/13984 datapoints
2025-03-04 14:43:21,745 - INFO - validation batch 301, loss: 0.136, 9632/13984 datapoints
2025-03-04 14:43:21,781 - INFO - validation batch 351, loss: 0.036, 11232/13984 datapoints
2025-03-04 14:43:21,815 - INFO - validation batch 401, loss: 0.159, 12832/13984 datapoints
2025-03-04 14:43:21,841 - INFO - Epoch 19/50 done
2025-03-04 14:43:21,842 - INFO - Beginning epoch 20/50
2025-03-04 14:43:21,845 - INFO - training batch 1, loss: 0.124, 32/56000 datapoints
2025-03-04 14:43:21,947 - INFO - training batch 51, loss: 0.106, 1632/56000 datapoints
2025-03-04 14:43:22,038 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-04 14:43:22,126 - INFO - training batch 151, loss: 0.104, 4832/56000 datapoints
2025-03-04 14:43:22,224 - INFO - training batch 201, loss: 0.046, 6432/56000 datapoints
2025-03-04 14:43:22,353 - INFO - training batch 251, loss: 0.056, 8032/56000 datapoints
2025-03-04 14:43:22,448 - INFO - training batch 301, loss: 0.066, 9632/56000 datapoints
2025-03-04 14:43:22,550 - INFO - training batch 351, loss: 0.084, 11232/56000 datapoints
2025-03-04 14:43:22,646 - INFO - training batch 401, loss: 0.167, 12832/56000 datapoints
2025-03-04 14:43:22,734 - INFO - training batch 451, loss: 0.114, 14432/56000 datapoints
2025-03-04 14:43:22,807 - INFO - training batch 501, loss: 0.096, 16032/56000 datapoints
2025-03-04 14:43:22,884 - INFO - training batch 551, loss: 0.118, 17632/56000 datapoints
2025-03-04 14:43:22,977 - INFO - training batch 601, loss: 0.045, 19232/56000 datapoints
2025-03-04 14:43:23,051 - INFO - training batch 651, loss: 0.043, 20832/56000 datapoints
2025-03-04 14:43:23,134 - INFO - training batch 701, loss: 0.273, 22432/56000 datapoints
2025-03-04 14:43:23,211 - INFO - training batch 751, loss: 0.065, 24032/56000 datapoints
2025-03-04 14:43:23,288 - INFO - training batch 801, loss: 0.088, 25632/56000 datapoints
2025-03-04 14:43:23,360 - INFO - training batch 851, loss: 0.047, 27232/56000 datapoints
2025-03-04 14:43:23,432 - INFO - training batch 901, loss: 0.082, 28832/56000 datapoints
2025-03-04 14:43:23,514 - INFO - training batch 951, loss: 0.082, 30432/56000 datapoints
2025-03-04 14:43:23,588 - INFO - training batch 1001, loss: 0.036, 32032/56000 datapoints
2025-03-04 14:43:23,666 - INFO - training batch 1051, loss: 0.043, 33632/56000 datapoints
2025-03-04 14:43:23,746 - INFO - training batch 1101, loss: 0.116, 35232/56000 datapoints
2025-03-04 14:43:23,840 - INFO - training batch 1151, loss: 0.016, 36832/56000 datapoints
2025-03-04 14:43:23,924 - INFO - training batch 1201, loss: 0.060, 38432/56000 datapoints
2025-03-04 14:43:24,010 - INFO - training batch 1251, loss: 0.252, 40032/56000 datapoints
2025-03-04 14:43:24,112 - INFO - training batch 1301, loss: 0.064, 41632/56000 datapoints
2025-03-04 14:43:24,223 - INFO - training batch 1351, loss: 0.034, 43232/56000 datapoints
2025-03-04 14:43:24,347 - INFO - training batch 1401, loss: 0.034, 44832/56000 datapoints
2025-03-04 14:43:24,462 - INFO - training batch 1451, loss: 0.035, 46432/56000 datapoints
2025-03-04 14:43:24,544 - INFO - training batch 1501, loss: 0.152, 48032/56000 datapoints
2025-03-04 14:43:24,614 - INFO - training batch 1551, loss: 0.074, 49632/56000 datapoints
2025-03-04 14:43:24,688 - INFO - training batch 1601, loss: 0.093, 51232/56000 datapoints
2025-03-04 14:43:24,757 - INFO - training batch 1651, loss: 0.041, 52832/56000 datapoints
2025-03-04 14:43:24,829 - INFO - training batch 1701, loss: 0.080, 54432/56000 datapoints
2025-03-04 14:43:24,917 - INFO - validation batch 1, loss: 0.023, 32/13984 datapoints
2025-03-04 14:43:24,947 - INFO - validation batch 51, loss: 0.012, 1632/13984 datapoints
2025-03-04 14:43:24,986 - INFO - validation batch 101, loss: 0.077, 3232/13984 datapoints
2025-03-04 14:43:25,015 - INFO - validation batch 151, loss: 0.024, 4832/13984 datapoints
2025-03-04 14:43:25,046 - INFO - validation batch 201, loss: 0.052, 6432/13984 datapoints
2025-03-04 14:43:25,083 - INFO - validation batch 251, loss: 0.079, 8032/13984 datapoints
2025-03-04 14:43:25,110 - INFO - validation batch 301, loss: 0.135, 9632/13984 datapoints
2025-03-04 14:43:25,147 - INFO - validation batch 351, loss: 0.036, 11232/13984 datapoints
2025-03-04 14:43:25,176 - INFO - validation batch 401, loss: 0.158, 12832/13984 datapoints
2025-03-04 14:43:25,208 - INFO - Epoch 20/50 done
2025-03-04 14:43:25,208 - INFO - Beginning epoch 21/50
2025-03-04 14:43:25,219 - INFO - training batch 1, loss: 0.124, 32/56000 datapoints
2025-03-04 14:43:25,322 - INFO - training batch 51, loss: 0.105, 1632/56000 datapoints
2025-03-04 14:43:25,413 - INFO - training batch 101, loss: 0.060, 3232/56000 datapoints
2025-03-04 14:43:25,498 - INFO - training batch 151, loss: 0.103, 4832/56000 datapoints
2025-03-04 14:43:25,588 - INFO - training batch 201, loss: 0.046, 6432/56000 datapoints
2025-03-04 14:43:25,666 - INFO - training batch 251, loss: 0.055, 8032/56000 datapoints
2025-03-04 14:43:25,747 - INFO - training batch 301, loss: 0.066, 9632/56000 datapoints
2025-03-04 14:43:25,840 - INFO - training batch 351, loss: 0.083, 11232/56000 datapoints
2025-03-04 14:43:25,915 - INFO - training batch 401, loss: 0.165, 12832/56000 datapoints
2025-03-04 14:43:25,985 - INFO - training batch 451, loss: 0.113, 14432/56000 datapoints
2025-03-04 14:43:26,066 - INFO - training batch 501, loss: 0.095, 16032/56000 datapoints
2025-03-04 14:43:26,156 - INFO - training batch 551, loss: 0.117, 17632/56000 datapoints
2025-03-04 14:43:26,229 - INFO - training batch 601, loss: 0.044, 19232/56000 datapoints
2025-03-04 14:43:26,300 - INFO - training batch 651, loss: 0.043, 20832/56000 datapoints
2025-03-04 14:43:26,370 - INFO - training batch 701, loss: 0.273, 22432/56000 datapoints
2025-03-04 14:43:26,449 - INFO - training batch 751, loss: 0.065, 24032/56000 datapoints
2025-03-04 14:43:26,537 - INFO - training batch 801, loss: 0.087, 25632/56000 datapoints
2025-03-04 14:43:26,607 - INFO - training batch 851, loss: 0.046, 27232/56000 datapoints
2025-03-04 14:43:26,675 - INFO - training batch 901, loss: 0.081, 28832/56000 datapoints
2025-03-04 14:43:26,761 - INFO - training batch 951, loss: 0.081, 30432/56000 datapoints
2025-03-04 14:43:26,863 - INFO - training batch 1001, loss: 0.036, 32032/56000 datapoints
2025-03-04 14:43:26,955 - INFO - training batch 1051, loss: 0.042, 33632/56000 datapoints
2025-03-04 14:43:27,035 - INFO - training batch 1101, loss: 0.115, 35232/56000 datapoints
2025-03-04 14:43:27,110 - INFO - training batch 1151, loss: 0.016, 36832/56000 datapoints
2025-03-04 14:43:27,194 - INFO - training batch 1201, loss: 0.059, 38432/56000 datapoints
2025-03-04 14:43:27,275 - INFO - training batch 1251, loss: 0.251, 40032/56000 datapoints
2025-03-04 14:43:27,357 - INFO - training batch 1301, loss: 0.063, 41632/56000 datapoints
2025-03-04 14:43:27,512 - INFO - training batch 1351, loss: 0.033, 43232/56000 datapoints
2025-03-04 14:43:27,604 - INFO - training batch 1401, loss: 0.034, 44832/56000 datapoints
2025-03-04 14:43:27,678 - INFO - training batch 1451, loss: 0.035, 46432/56000 datapoints
2025-03-04 14:43:27,759 - INFO - training batch 1501, loss: 0.150, 48032/56000 datapoints
2025-03-04 14:43:27,844 - INFO - training batch 1551, loss: 0.074, 49632/56000 datapoints
2025-03-04 14:43:27,926 - INFO - training batch 1601, loss: 0.092, 51232/56000 datapoints
2025-03-04 14:43:28,005 - INFO - training batch 1651, loss: 0.041, 52832/56000 datapoints
2025-03-04 14:43:28,092 - INFO - training batch 1701, loss: 0.080, 54432/56000 datapoints
2025-03-04 14:43:28,180 - INFO - validation batch 1, loss: 0.022, 32/13984 datapoints
2025-03-04 14:43:28,216 - INFO - validation batch 51, loss: 0.012, 1632/13984 datapoints
2025-03-04 14:43:28,257 - INFO - validation batch 101, loss: 0.076, 3232/13984 datapoints
2025-03-04 14:43:28,280 - INFO - validation batch 151, loss: 0.024, 4832/13984 datapoints
2025-03-04 14:43:28,308 - INFO - validation batch 201, loss: 0.051, 6432/13984 datapoints
2025-03-04 14:43:28,334 - INFO - validation batch 251, loss: 0.078, 8032/13984 datapoints
2025-03-04 14:43:28,362 - INFO - validation batch 301, loss: 0.133, 9632/13984 datapoints
2025-03-04 14:43:28,390 - INFO - validation batch 351, loss: 0.035, 11232/13984 datapoints
2025-03-04 14:43:28,419 - INFO - validation batch 401, loss: 0.157, 12832/13984 datapoints
2025-03-04 14:43:28,437 - INFO - Epoch 21/50 done
2025-03-04 14:43:28,438 - INFO - Beginning epoch 22/50
2025-03-04 14:43:28,441 - INFO - training batch 1, loss: 0.123, 32/56000 datapoints
2025-03-04 14:43:28,527 - INFO - training batch 51, loss: 0.104, 1632/56000 datapoints
2025-03-04 14:43:28,616 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:43:28,692 - INFO - training batch 151, loss: 0.102, 4832/56000 datapoints
2025-03-04 14:43:28,770 - INFO - training batch 201, loss: 0.045, 6432/56000 datapoints
2025-03-04 14:43:28,877 - INFO - training batch 251, loss: 0.055, 8032/56000 datapoints
2025-03-04 14:43:28,972 - INFO - training batch 301, loss: 0.065, 9632/56000 datapoints
2025-03-04 14:43:29,043 - INFO - training batch 351, loss: 0.083, 11232/56000 datapoints
2025-03-04 14:43:29,136 - INFO - training batch 401, loss: 0.164, 12832/56000 datapoints
2025-03-04 14:43:29,239 - INFO - training batch 451, loss: 0.112, 14432/56000 datapoints
2025-03-04 14:43:29,329 - INFO - training batch 501, loss: 0.094, 16032/56000 datapoints
2025-03-04 14:43:29,418 - INFO - training batch 551, loss: 0.116, 17632/56000 datapoints
2025-03-04 14:43:29,501 - INFO - training batch 601, loss: 0.044, 19232/56000 datapoints
2025-03-04 14:43:29,585 - INFO - training batch 651, loss: 0.042, 20832/56000 datapoints
2025-03-04 14:43:29,666 - INFO - training batch 701, loss: 0.272, 22432/56000 datapoints
2025-03-04 14:43:29,746 - INFO - training batch 751, loss: 0.064, 24032/56000 datapoints
2025-03-04 14:43:29,836 - INFO - training batch 801, loss: 0.086, 25632/56000 datapoints
2025-03-04 14:43:29,913 - INFO - training batch 851, loss: 0.046, 27232/56000 datapoints
2025-03-04 14:43:29,989 - INFO - training batch 901, loss: 0.080, 28832/56000 datapoints
2025-03-04 14:43:30,063 - INFO - training batch 951, loss: 0.080, 30432/56000 datapoints
2025-03-04 14:43:30,136 - INFO - training batch 1001, loss: 0.035, 32032/56000 datapoints
2025-03-04 14:43:30,212 - INFO - training batch 1051, loss: 0.042, 33632/56000 datapoints
2025-03-04 14:43:30,320 - INFO - training batch 1101, loss: 0.114, 35232/56000 datapoints
2025-03-04 14:43:30,417 - INFO - training batch 1151, loss: 0.016, 36832/56000 datapoints
2025-03-04 14:43:30,551 - INFO - training batch 1201, loss: 0.058, 38432/56000 datapoints
2025-03-04 14:43:30,670 - INFO - training batch 1251, loss: 0.249, 40032/56000 datapoints
2025-03-04 14:43:30,802 - INFO - training batch 1301, loss: 0.062, 41632/56000 datapoints
2025-03-04 14:43:30,900 - INFO - training batch 1351, loss: 0.033, 43232/56000 datapoints
2025-03-04 14:43:30,994 - INFO - training batch 1401, loss: 0.033, 44832/56000 datapoints
2025-03-04 14:43:31,079 - INFO - training batch 1451, loss: 0.034, 46432/56000 datapoints
2025-03-04 14:43:31,162 - INFO - training batch 1501, loss: 0.149, 48032/56000 datapoints
2025-03-04 14:43:31,270 - INFO - training batch 1551, loss: 0.073, 49632/56000 datapoints
2025-03-04 14:43:31,375 - INFO - training batch 1601, loss: 0.091, 51232/56000 datapoints
2025-03-04 14:43:31,472 - INFO - training batch 1651, loss: 0.040, 52832/56000 datapoints
2025-03-04 14:43:31,573 - INFO - training batch 1701, loss: 0.080, 54432/56000 datapoints
2025-03-04 14:43:31,667 - INFO - validation batch 1, loss: 0.022, 32/13984 datapoints
2025-03-04 14:43:31,708 - INFO - validation batch 51, loss: 0.012, 1632/13984 datapoints
2025-03-04 14:43:31,757 - INFO - validation batch 101, loss: 0.075, 3232/13984 datapoints
2025-03-04 14:43:31,788 - INFO - validation batch 151, loss: 0.023, 4832/13984 datapoints
2025-03-04 14:43:31,823 - INFO - validation batch 201, loss: 0.050, 6432/13984 datapoints
2025-03-04 14:43:31,854 - INFO - validation batch 251, loss: 0.077, 8032/13984 datapoints
2025-03-04 14:43:31,877 - INFO - validation batch 301, loss: 0.132, 9632/13984 datapoints
2025-03-04 14:43:31,916 - INFO - validation batch 351, loss: 0.035, 11232/13984 datapoints
2025-03-04 14:43:31,943 - INFO - validation batch 401, loss: 0.156, 12832/13984 datapoints
2025-03-04 14:43:31,994 - INFO - Epoch 22/50 done
2025-03-04 14:43:31,994 - INFO - Beginning epoch 23/50
2025-03-04 14:43:32,005 - INFO - training batch 1, loss: 0.123, 32/56000 datapoints
2025-03-04 14:43:32,277 - INFO - training batch 51, loss: 0.103, 1632/56000 datapoints
2025-03-04 14:43:32,928 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:43:33,388 - INFO - training batch 151, loss: 0.102, 4832/56000 datapoints
2025-03-04 14:43:33,555 - INFO - training batch 201, loss: 0.045, 6432/56000 datapoints
2025-03-04 14:43:33,690 - INFO - training batch 251, loss: 0.054, 8032/56000 datapoints
2025-03-04 14:43:33,946 - INFO - training batch 301, loss: 0.064, 9632/56000 datapoints
2025-03-04 14:43:34,050 - INFO - training batch 351, loss: 0.082, 11232/56000 datapoints
2025-03-04 14:43:34,191 - INFO - training batch 401, loss: 0.162, 12832/56000 datapoints
2025-03-04 14:43:34,328 - INFO - training batch 451, loss: 0.110, 14432/56000 datapoints
2025-03-04 14:43:34,474 - INFO - training batch 501, loss: 0.093, 16032/56000 datapoints
2025-03-04 14:43:34,641 - INFO - training batch 551, loss: 0.115, 17632/56000 datapoints
2025-03-04 14:43:35,150 - INFO - training batch 601, loss: 0.043, 19232/56000 datapoints
2025-03-04 14:43:35,279 - INFO - training batch 651, loss: 0.042, 20832/56000 datapoints
2025-03-04 14:43:35,379 - INFO - training batch 701, loss: 0.272, 22432/56000 datapoints
2025-03-04 14:43:35,506 - INFO - training batch 751, loss: 0.063, 24032/56000 datapoints
2025-03-04 14:43:35,661 - INFO - training batch 801, loss: 0.085, 25632/56000 datapoints
2025-03-04 14:43:35,858 - INFO - training batch 851, loss: 0.045, 27232/56000 datapoints
2025-03-04 14:43:36,021 - INFO - training batch 901, loss: 0.080, 28832/56000 datapoints
2025-03-04 14:43:36,219 - INFO - training batch 951, loss: 0.079, 30432/56000 datapoints
2025-03-04 14:43:36,414 - INFO - training batch 1001, loss: 0.035, 32032/56000 datapoints
2025-03-04 14:43:36,576 - INFO - training batch 1051, loss: 0.041, 33632/56000 datapoints
2025-03-04 14:43:36,704 - INFO - training batch 1101, loss: 0.113, 35232/56000 datapoints
2025-03-04 14:43:36,796 - INFO - training batch 1151, loss: 0.016, 36832/56000 datapoints
2025-03-04 14:43:36,919 - INFO - training batch 1201, loss: 0.056, 38432/56000 datapoints
2025-03-04 14:43:37,012 - INFO - training batch 1251, loss: 0.247, 40032/56000 datapoints
2025-03-04 14:43:37,116 - INFO - training batch 1301, loss: 0.061, 41632/56000 datapoints
2025-03-04 14:43:37,231 - INFO - training batch 1351, loss: 0.032, 43232/56000 datapoints
2025-03-04 14:43:37,327 - INFO - training batch 1401, loss: 0.033, 44832/56000 datapoints
2025-03-04 14:43:37,521 - INFO - training batch 1451, loss: 0.034, 46432/56000 datapoints
2025-03-04 14:43:37,594 - INFO - training batch 1501, loss: 0.147, 48032/56000 datapoints
2025-03-04 14:43:37,668 - INFO - training batch 1551, loss: 0.072, 49632/56000 datapoints
2025-03-04 14:43:37,740 - INFO - training batch 1601, loss: 0.090, 51232/56000 datapoints
2025-03-04 14:43:37,819 - INFO - training batch 1651, loss: 0.040, 52832/56000 datapoints
2025-03-04 14:43:37,915 - INFO - training batch 1701, loss: 0.079, 54432/56000 datapoints
2025-03-04 14:43:38,012 - INFO - validation batch 1, loss: 0.022, 32/13984 datapoints
2025-03-04 14:43:38,036 - INFO - validation batch 51, loss: 0.011, 1632/13984 datapoints
2025-03-04 14:43:38,072 - INFO - validation batch 101, loss: 0.074, 3232/13984 datapoints
2025-03-04 14:43:38,099 - INFO - validation batch 151, loss: 0.023, 4832/13984 datapoints
2025-03-04 14:43:38,175 - INFO - validation batch 201, loss: 0.050, 6432/13984 datapoints
2025-03-04 14:43:38,311 - INFO - validation batch 251, loss: 0.076, 8032/13984 datapoints
2025-03-04 14:43:38,392 - INFO - validation batch 301, loss: 0.130, 9632/13984 datapoints
2025-03-04 14:43:38,511 - INFO - validation batch 351, loss: 0.034, 11232/13984 datapoints
2025-03-04 14:43:38,568 - INFO - validation batch 401, loss: 0.155, 12832/13984 datapoints
2025-03-04 14:43:38,603 - INFO - Epoch 23/50 done
2025-03-04 14:43:38,608 - INFO - Beginning epoch 24/50
2025-03-04 14:43:38,613 - INFO - training batch 1, loss: 0.122, 32/56000 datapoints
2025-03-04 14:43:39,060 - INFO - training batch 51, loss: 0.101, 1632/56000 datapoints
2025-03-04 14:43:39,302 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:43:39,410 - INFO - training batch 151, loss: 0.101, 4832/56000 datapoints
2025-03-04 14:43:39,523 - INFO - training batch 201, loss: 0.045, 6432/56000 datapoints
2025-03-04 14:43:39,636 - INFO - training batch 251, loss: 0.054, 8032/56000 datapoints
2025-03-04 14:43:39,757 - INFO - training batch 301, loss: 0.063, 9632/56000 datapoints
2025-03-04 14:43:39,883 - INFO - training batch 351, loss: 0.082, 11232/56000 datapoints
2025-03-04 14:43:39,978 - INFO - training batch 401, loss: 0.161, 12832/56000 datapoints
2025-03-04 14:43:40,065 - INFO - training batch 451, loss: 0.109, 14432/56000 datapoints
2025-03-04 14:43:40,157 - INFO - training batch 501, loss: 0.091, 16032/56000 datapoints
2025-03-04 14:43:40,251 - INFO - training batch 551, loss: 0.114, 17632/56000 datapoints
2025-03-04 14:43:40,339 - INFO - training batch 601, loss: 0.043, 19232/56000 datapoints
2025-03-04 14:43:40,416 - INFO - training batch 651, loss: 0.041, 20832/56000 datapoints
2025-03-04 14:43:40,496 - INFO - training batch 701, loss: 0.271, 22432/56000 datapoints
2025-03-04 14:43:40,566 - INFO - training batch 751, loss: 0.062, 24032/56000 datapoints
2025-03-04 14:43:40,636 - INFO - training batch 801, loss: 0.084, 25632/56000 datapoints
2025-03-04 14:43:40,719 - INFO - training batch 851, loss: 0.045, 27232/56000 datapoints
2025-03-04 14:43:40,853 - INFO - training batch 901, loss: 0.079, 28832/56000 datapoints
2025-03-04 14:43:40,980 - INFO - training batch 951, loss: 0.078, 30432/56000 datapoints
2025-03-04 14:43:41,172 - INFO - training batch 1001, loss: 0.034, 32032/56000 datapoints
2025-03-04 14:43:41,306 - INFO - training batch 1051, loss: 0.040, 33632/56000 datapoints
2025-03-04 14:43:41,412 - INFO - training batch 1101, loss: 0.112, 35232/56000 datapoints
2025-03-04 14:43:41,503 - INFO - training batch 1151, loss: 0.015, 36832/56000 datapoints
2025-03-04 14:43:41,604 - INFO - training batch 1201, loss: 0.055, 38432/56000 datapoints
2025-03-04 14:43:41,703 - INFO - training batch 1251, loss: 0.245, 40032/56000 datapoints
2025-03-04 14:43:41,791 - INFO - training batch 1301, loss: 0.060, 41632/56000 datapoints
2025-03-04 14:43:41,885 - INFO - training batch 1351, loss: 0.032, 43232/56000 datapoints
2025-03-04 14:43:42,040 - INFO - training batch 1401, loss: 0.032, 44832/56000 datapoints
2025-03-04 14:43:42,748 - INFO - training batch 1451, loss: 0.033, 46432/56000 datapoints
2025-03-04 14:43:42,975 - INFO - training batch 1501, loss: 0.146, 48032/56000 datapoints
2025-03-04 14:43:43,154 - INFO - training batch 1551, loss: 0.071, 49632/56000 datapoints
2025-03-04 14:43:43,336 - INFO - training batch 1601, loss: 0.089, 51232/56000 datapoints
2025-03-04 14:43:43,470 - INFO - training batch 1651, loss: 0.039, 52832/56000 datapoints
2025-03-04 14:43:43,596 - INFO - training batch 1701, loss: 0.079, 54432/56000 datapoints
2025-03-04 14:43:43,998 - INFO - validation batch 1, loss: 0.021, 32/13984 datapoints
2025-03-04 14:43:44,149 - INFO - validation batch 51, loss: 0.011, 1632/13984 datapoints
2025-03-04 14:43:44,321 - INFO - validation batch 101, loss: 0.073, 3232/13984 datapoints
2025-03-04 14:43:44,450 - INFO - validation batch 151, loss: 0.022, 4832/13984 datapoints
2025-03-04 14:43:44,760 - INFO - validation batch 201, loss: 0.049, 6432/13984 datapoints
2025-03-04 14:43:44,871 - INFO - validation batch 251, loss: 0.075, 8032/13984 datapoints
2025-03-04 14:43:44,964 - INFO - validation batch 301, loss: 0.129, 9632/13984 datapoints
2025-03-04 14:43:45,136 - INFO - validation batch 351, loss: 0.034, 11232/13984 datapoints
2025-03-04 14:43:45,614 - INFO - validation batch 401, loss: 0.154, 12832/13984 datapoints
2025-03-04 14:43:45,800 - INFO - Epoch 24/50 done
2025-03-04 14:43:45,801 - INFO - Beginning epoch 25/50
2025-03-04 14:43:45,805 - INFO - training batch 1, loss: 0.122, 32/56000 datapoints
2025-03-04 14:43:45,985 - INFO - training batch 51, loss: 0.100, 1632/56000 datapoints
2025-03-04 14:43:46,087 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:43:46,163 - INFO - training batch 151, loss: 0.100, 4832/56000 datapoints
2025-03-04 14:43:46,240 - INFO - training batch 201, loss: 0.044, 6432/56000 datapoints
2025-03-04 14:43:46,315 - INFO - training batch 251, loss: 0.053, 8032/56000 datapoints
2025-03-04 14:43:46,390 - INFO - training batch 301, loss: 0.063, 9632/56000 datapoints
2025-03-04 14:43:46,465 - INFO - training batch 351, loss: 0.081, 11232/56000 datapoints
2025-03-04 14:43:46,541 - INFO - training batch 401, loss: 0.159, 12832/56000 datapoints
2025-03-04 14:43:46,615 - INFO - training batch 451, loss: 0.108, 14432/56000 datapoints
2025-03-04 14:43:46,691 - INFO - training batch 501, loss: 0.090, 16032/56000 datapoints
2025-03-04 14:43:46,781 - INFO - training batch 551, loss: 0.113, 17632/56000 datapoints
2025-03-04 14:43:46,928 - INFO - training batch 601, loss: 0.042, 19232/56000 datapoints
2025-03-04 14:43:47,036 - INFO - training batch 651, loss: 0.041, 20832/56000 datapoints
2025-03-04 14:43:47,137 - INFO - training batch 701, loss: 0.271, 22432/56000 datapoints
2025-03-04 14:43:47,580 - INFO - training batch 751, loss: 0.061, 24032/56000 datapoints
2025-03-04 14:43:47,675 - INFO - training batch 801, loss: 0.083, 25632/56000 datapoints
2025-03-04 14:43:47,769 - INFO - training batch 851, loss: 0.044, 27232/56000 datapoints
2025-03-04 14:43:47,872 - INFO - training batch 901, loss: 0.078, 28832/56000 datapoints
2025-03-04 14:43:47,968 - INFO - training batch 951, loss: 0.077, 30432/56000 datapoints
2025-03-04 14:43:48,044 - INFO - training batch 1001, loss: 0.034, 32032/56000 datapoints
2025-03-04 14:43:48,116 - INFO - training batch 1051, loss: 0.040, 33632/56000 datapoints
2025-03-04 14:43:48,191 - INFO - training batch 1101, loss: 0.111, 35232/56000 datapoints
2025-03-04 14:43:48,277 - INFO - training batch 1151, loss: 0.015, 36832/56000 datapoints
2025-03-04 14:43:48,352 - INFO - training batch 1201, loss: 0.055, 38432/56000 datapoints
2025-03-04 14:43:48,426 - INFO - training batch 1251, loss: 0.243, 40032/56000 datapoints
2025-03-04 14:43:48,506 - INFO - training batch 1301, loss: 0.059, 41632/56000 datapoints
2025-03-04 14:43:48,620 - INFO - training batch 1351, loss: 0.031, 43232/56000 datapoints
2025-03-04 14:43:48,732 - INFO - training batch 1401, loss: 0.032, 44832/56000 datapoints
2025-03-04 14:43:48,825 - INFO - training batch 1451, loss: 0.033, 46432/56000 datapoints
2025-03-04 14:43:48,916 - INFO - training batch 1501, loss: 0.144, 48032/56000 datapoints
2025-03-04 14:43:49,017 - INFO - training batch 1551, loss: 0.070, 49632/56000 datapoints
2025-03-04 14:43:49,109 - INFO - training batch 1601, loss: 0.087, 51232/56000 datapoints
2025-03-04 14:43:49,213 - INFO - training batch 1651, loss: 0.039, 52832/56000 datapoints
2025-03-04 14:43:49,307 - INFO - training batch 1701, loss: 0.078, 54432/56000 datapoints
2025-03-04 14:43:49,412 - INFO - validation batch 1, loss: 0.021, 32/13984 datapoints
2025-03-04 14:43:49,575 - INFO - validation batch 51, loss: 0.011, 1632/13984 datapoints
2025-03-04 14:43:49,609 - INFO - validation batch 101, loss: 0.072, 3232/13984 datapoints
2025-03-04 14:43:49,645 - INFO - validation batch 151, loss: 0.022, 4832/13984 datapoints
2025-03-04 14:43:49,680 - INFO - validation batch 201, loss: 0.048, 6432/13984 datapoints
2025-03-04 14:43:49,712 - INFO - validation batch 251, loss: 0.075, 8032/13984 datapoints
2025-03-04 14:43:49,760 - INFO - validation batch 301, loss: 0.127, 9632/13984 datapoints
2025-03-04 14:43:49,789 - INFO - validation batch 351, loss: 0.033, 11232/13984 datapoints
2025-03-04 14:43:49,830 - INFO - validation batch 401, loss: 0.154, 12832/13984 datapoints
2025-03-04 14:43:49,849 - INFO - Epoch 25/50 done
2025-03-04 14:43:49,850 - INFO - Beginning epoch 26/50
2025-03-04 14:43:49,854 - INFO - training batch 1, loss: 0.122, 32/56000 datapoints
2025-03-04 14:43:49,949 - INFO - training batch 51, loss: 0.099, 1632/56000 datapoints
2025-03-04 14:43:50,026 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:43:50,105 - INFO - training batch 151, loss: 0.099, 4832/56000 datapoints
2025-03-04 14:43:50,181 - INFO - training batch 201, loss: 0.044, 6432/56000 datapoints
2025-03-04 14:43:50,261 - INFO - training batch 251, loss: 0.053, 8032/56000 datapoints
2025-03-04 14:43:50,363 - INFO - training batch 301, loss: 0.062, 9632/56000 datapoints
2025-03-04 14:43:50,441 - INFO - training batch 351, loss: 0.080, 11232/56000 datapoints
2025-03-04 14:43:50,515 - INFO - training batch 401, loss: 0.158, 12832/56000 datapoints
2025-03-04 14:43:50,627 - INFO - training batch 451, loss: 0.107, 14432/56000 datapoints
2025-03-04 14:43:50,734 - INFO - training batch 501, loss: 0.089, 16032/56000 datapoints
2025-03-04 14:43:50,868 - INFO - training batch 551, loss: 0.111, 17632/56000 datapoints
2025-03-04 14:43:51,039 - INFO - training batch 601, loss: 0.042, 19232/56000 datapoints
2025-03-04 14:43:51,302 - INFO - training batch 651, loss: 0.041, 20832/56000 datapoints
2025-03-04 14:43:51,476 - INFO - training batch 701, loss: 0.270, 22432/56000 datapoints
2025-03-04 14:43:51,618 - INFO - training batch 751, loss: 0.061, 24032/56000 datapoints
2025-03-04 14:43:51,713 - INFO - training batch 801, loss: 0.082, 25632/56000 datapoints
2025-03-04 14:43:51,814 - INFO - training batch 851, loss: 0.044, 27232/56000 datapoints
2025-03-04 14:43:51,922 - INFO - training batch 901, loss: 0.077, 28832/56000 datapoints
2025-03-04 14:43:52,013 - INFO - training batch 951, loss: 0.076, 30432/56000 datapoints
2025-03-04 14:43:52,095 - INFO - training batch 1001, loss: 0.033, 32032/56000 datapoints
2025-03-04 14:43:52,216 - INFO - training batch 1051, loss: 0.039, 33632/56000 datapoints
2025-03-04 14:43:52,339 - INFO - training batch 1101, loss: 0.111, 35232/56000 datapoints
2025-03-04 14:43:52,475 - INFO - training batch 1151, loss: 0.015, 36832/56000 datapoints
2025-03-04 14:43:52,609 - INFO - training batch 1201, loss: 0.054, 38432/56000 datapoints
2025-03-04 14:43:52,799 - INFO - training batch 1251, loss: 0.241, 40032/56000 datapoints
2025-03-04 14:43:53,062 - INFO - training batch 1301, loss: 0.058, 41632/56000 datapoints
2025-03-04 14:43:53,463 - INFO - training batch 1351, loss: 0.031, 43232/56000 datapoints
2025-03-04 14:43:53,652 - INFO - training batch 1401, loss: 0.031, 44832/56000 datapoints
2025-03-04 14:43:53,754 - INFO - training batch 1451, loss: 0.032, 46432/56000 datapoints
2025-03-04 14:43:53,873 - INFO - training batch 1501, loss: 0.143, 48032/56000 datapoints
2025-03-04 14:43:54,023 - INFO - training batch 1551, loss: 0.069, 49632/56000 datapoints
2025-03-04 14:43:54,203 - INFO - training batch 1601, loss: 0.086, 51232/56000 datapoints
2025-03-04 14:43:54,374 - INFO - training batch 1651, loss: 0.038, 52832/56000 datapoints
2025-03-04 14:43:54,520 - INFO - training batch 1701, loss: 0.078, 54432/56000 datapoints
2025-03-04 14:43:54,632 - INFO - validation batch 1, loss: 0.021, 32/13984 datapoints
2025-03-04 14:43:54,671 - INFO - validation batch 51, loss: 0.011, 1632/13984 datapoints
2025-03-04 14:43:54,728 - INFO - validation batch 101, loss: 0.071, 3232/13984 datapoints
2025-03-04 14:43:54,775 - INFO - validation batch 151, loss: 0.022, 4832/13984 datapoints
2025-03-04 14:43:54,816 - INFO - validation batch 201, loss: 0.048, 6432/13984 datapoints
2025-03-04 14:43:54,852 - INFO - validation batch 251, loss: 0.074, 8032/13984 datapoints
2025-03-04 14:43:54,945 - INFO - validation batch 301, loss: 0.126, 9632/13984 datapoints
2025-03-04 14:43:55,017 - INFO - validation batch 351, loss: 0.033, 11232/13984 datapoints
2025-03-04 14:43:55,060 - INFO - validation batch 401, loss: 0.153, 12832/13984 datapoints
2025-03-04 14:43:55,080 - INFO - Epoch 26/50 done
2025-03-04 14:43:55,081 - INFO - Beginning epoch 27/50
2025-03-04 14:43:55,084 - INFO - training batch 1, loss: 0.121, 32/56000 datapoints
2025-03-04 14:43:55,202 - INFO - training batch 51, loss: 0.098, 1632/56000 datapoints
2025-03-04 14:43:55,331 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:43:55,468 - INFO - training batch 151, loss: 0.099, 4832/56000 datapoints
2025-03-04 14:43:55,567 - INFO - training batch 201, loss: 0.043, 6432/56000 datapoints
2025-03-04 14:43:55,674 - INFO - training batch 251, loss: 0.052, 8032/56000 datapoints
2025-03-04 14:43:55,778 - INFO - training batch 301, loss: 0.061, 9632/56000 datapoints
2025-03-04 14:43:55,902 - INFO - training batch 351, loss: 0.080, 11232/56000 datapoints
2025-03-04 14:43:56,047 - INFO - training batch 401, loss: 0.156, 12832/56000 datapoints
2025-03-04 14:43:56,158 - INFO - training batch 451, loss: 0.106, 14432/56000 datapoints
2025-03-04 14:43:56,291 - INFO - training batch 501, loss: 0.088, 16032/56000 datapoints
2025-03-04 14:43:56,389 - INFO - training batch 551, loss: 0.110, 17632/56000 datapoints
2025-03-04 14:43:56,557 - INFO - training batch 601, loss: 0.041, 19232/56000 datapoints
2025-03-04 14:43:56,710 - INFO - training batch 651, loss: 0.040, 20832/56000 datapoints
2025-03-04 14:43:56,890 - INFO - training batch 701, loss: 0.269, 22432/56000 datapoints
2025-03-04 14:43:57,058 - INFO - training batch 751, loss: 0.060, 24032/56000 datapoints
2025-03-04 14:43:57,216 - INFO - training batch 801, loss: 0.081, 25632/56000 datapoints
2025-03-04 14:43:57,331 - INFO - training batch 851, loss: 0.043, 27232/56000 datapoints
2025-03-04 14:43:57,488 - INFO - training batch 901, loss: 0.076, 28832/56000 datapoints
2025-03-04 14:43:57,656 - INFO - training batch 951, loss: 0.075, 30432/56000 datapoints
2025-03-04 14:43:57,792 - INFO - training batch 1001, loss: 0.033, 32032/56000 datapoints
2025-03-04 14:43:57,986 - INFO - training batch 1051, loss: 0.039, 33632/56000 datapoints
2025-03-04 14:43:58,118 - INFO - training batch 1101, loss: 0.110, 35232/56000 datapoints
2025-03-04 14:43:58,221 - INFO - training batch 1151, loss: 0.015, 36832/56000 datapoints
2025-03-04 14:43:58,347 - INFO - training batch 1201, loss: 0.053, 38432/56000 datapoints
2025-03-04 14:43:58,459 - INFO - training batch 1251, loss: 0.239, 40032/56000 datapoints
2025-03-04 14:43:58,594 - INFO - training batch 1301, loss: 0.058, 41632/56000 datapoints
2025-03-04 14:43:58,750 - INFO - training batch 1351, loss: 0.030, 43232/56000 datapoints
2025-03-04 14:43:58,871 - INFO - training batch 1401, loss: 0.031, 44832/56000 datapoints
2025-03-04 14:43:59,099 - INFO - training batch 1451, loss: 0.032, 46432/56000 datapoints
2025-03-04 14:43:59,281 - INFO - training batch 1501, loss: 0.141, 48032/56000 datapoints
2025-03-04 14:43:59,452 - INFO - training batch 1551, loss: 0.069, 49632/56000 datapoints
2025-03-04 14:43:59,583 - INFO - training batch 1601, loss: 0.085, 51232/56000 datapoints
2025-03-04 14:43:59,692 - INFO - training batch 1651, loss: 0.038, 52832/56000 datapoints
2025-03-04 14:43:59,795 - INFO - training batch 1701, loss: 0.078, 54432/56000 datapoints
2025-03-04 14:43:59,920 - INFO - validation batch 1, loss: 0.020, 32/13984 datapoints
2025-03-04 14:43:59,972 - INFO - validation batch 51, loss: 0.011, 1632/13984 datapoints
2025-03-04 14:44:00,017 - INFO - validation batch 101, loss: 0.071, 3232/13984 datapoints
2025-03-04 14:44:00,106 - INFO - validation batch 151, loss: 0.021, 4832/13984 datapoints
2025-03-04 14:44:00,346 - INFO - validation batch 201, loss: 0.047, 6432/13984 datapoints
2025-03-04 14:44:00,470 - INFO - validation batch 251, loss: 0.073, 8032/13984 datapoints
2025-03-04 14:44:00,546 - INFO - validation batch 301, loss: 0.124, 9632/13984 datapoints
2025-03-04 14:44:00,868 - INFO - validation batch 351, loss: 0.032, 11232/13984 datapoints
2025-03-04 14:44:01,174 - INFO - validation batch 401, loss: 0.152, 12832/13984 datapoints
2025-03-04 14:44:01,263 - INFO - Epoch 27/50 done
2025-03-04 14:44:01,263 - INFO - Beginning epoch 28/50
2025-03-04 14:44:01,266 - INFO - training batch 1, loss: 0.121, 32/56000 datapoints
2025-03-04 14:44:01,421 - INFO - training batch 51, loss: 0.097, 1632/56000 datapoints
2025-03-04 14:44:01,528 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:44:01,646 - INFO - training batch 151, loss: 0.098, 4832/56000 datapoints
2025-03-04 14:44:01,761 - INFO - training batch 201, loss: 0.043, 6432/56000 datapoints
2025-03-04 14:44:01,867 - INFO - training batch 251, loss: 0.052, 8032/56000 datapoints
2025-03-04 14:44:02,012 - INFO - training batch 301, loss: 0.061, 9632/56000 datapoints
2025-03-04 14:44:02,118 - INFO - training batch 351, loss: 0.079, 11232/56000 datapoints
2025-03-04 14:44:02,229 - INFO - training batch 401, loss: 0.154, 12832/56000 datapoints
2025-03-04 14:44:02,354 - INFO - training batch 451, loss: 0.105, 14432/56000 datapoints
2025-03-04 14:44:02,527 - INFO - training batch 501, loss: 0.087, 16032/56000 datapoints
2025-03-04 14:44:02,799 - INFO - training batch 551, loss: 0.109, 17632/56000 datapoints
2025-03-04 14:44:03,054 - INFO - training batch 601, loss: 0.041, 19232/56000 datapoints
2025-03-04 14:44:03,316 - INFO - training batch 651, loss: 0.040, 20832/56000 datapoints
2025-03-04 14:44:03,467 - INFO - training batch 701, loss: 0.269, 22432/56000 datapoints
2025-03-04 14:44:03,624 - INFO - training batch 751, loss: 0.059, 24032/56000 datapoints
2025-03-04 14:44:03,787 - INFO - training batch 801, loss: 0.080, 25632/56000 datapoints
2025-03-04 14:44:03,925 - INFO - training batch 851, loss: 0.043, 27232/56000 datapoints
2025-03-04 14:44:04,193 - INFO - training batch 901, loss: 0.075, 28832/56000 datapoints
2025-03-04 14:44:04,358 - INFO - training batch 951, loss: 0.074, 30432/56000 datapoints
2025-03-04 14:44:04,556 - INFO - training batch 1001, loss: 0.033, 32032/56000 datapoints
2025-03-04 14:44:04,706 - INFO - training batch 1051, loss: 0.038, 33632/56000 datapoints
2025-03-04 14:44:04,865 - INFO - training batch 1101, loss: 0.109, 35232/56000 datapoints
2025-03-04 14:44:05,098 - INFO - training batch 1151, loss: 0.015, 36832/56000 datapoints
2025-03-04 14:44:05,254 - INFO - training batch 1201, loss: 0.052, 38432/56000 datapoints
2025-03-04 14:44:05,402 - INFO - training batch 1251, loss: 0.237, 40032/56000 datapoints
2025-03-04 14:44:05,552 - INFO - training batch 1301, loss: 0.057, 41632/56000 datapoints
2025-03-04 14:44:05,692 - INFO - training batch 1351, loss: 0.030, 43232/56000 datapoints
2025-03-04 14:44:05,814 - INFO - training batch 1401, loss: 0.030, 44832/56000 datapoints
2025-03-04 14:44:06,104 - INFO - training batch 1451, loss: 0.031, 46432/56000 datapoints
2025-03-04 14:44:06,270 - INFO - training batch 1501, loss: 0.140, 48032/56000 datapoints
2025-03-04 14:44:06,434 - INFO - training batch 1551, loss: 0.068, 49632/56000 datapoints
2025-03-04 14:44:06,590 - INFO - training batch 1601, loss: 0.084, 51232/56000 datapoints
2025-03-04 14:44:06,705 - INFO - training batch 1651, loss: 0.037, 52832/56000 datapoints
2025-03-04 14:44:06,843 - INFO - training batch 1701, loss: 0.077, 54432/56000 datapoints
2025-03-04 14:44:06,974 - INFO - validation batch 1, loss: 0.020, 32/13984 datapoints
2025-03-04 14:44:07,037 - INFO - validation batch 51, loss: 0.011, 1632/13984 datapoints
2025-03-04 14:44:07,088 - INFO - validation batch 101, loss: 0.070, 3232/13984 datapoints
2025-03-04 14:44:07,146 - INFO - validation batch 151, loss: 0.021, 4832/13984 datapoints
2025-03-04 14:44:07,214 - INFO - validation batch 201, loss: 0.047, 6432/13984 datapoints
2025-03-04 14:44:07,270 - INFO - validation batch 251, loss: 0.072, 8032/13984 datapoints
2025-03-04 14:44:07,341 - INFO - validation batch 301, loss: 0.123, 9632/13984 datapoints
2025-03-04 14:44:07,398 - INFO - validation batch 351, loss: 0.032, 11232/13984 datapoints
2025-03-04 14:44:07,439 - INFO - validation batch 401, loss: 0.151, 12832/13984 datapoints
2025-03-04 14:44:07,475 - INFO - Epoch 28/50 done
2025-03-04 14:44:07,476 - INFO - Beginning epoch 29/50
2025-03-04 14:44:07,482 - INFO - training batch 1, loss: 0.121, 32/56000 datapoints
2025-03-04 14:44:07,599 - INFO - training batch 51, loss: 0.096, 1632/56000 datapoints
2025-03-04 14:44:07,708 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:44:07,810 - INFO - training batch 151, loss: 0.097, 4832/56000 datapoints
2025-03-04 14:44:07,929 - INFO - training batch 201, loss: 0.042, 6432/56000 datapoints
2025-03-04 14:44:08,048 - INFO - training batch 251, loss: 0.051, 8032/56000 datapoints
2025-03-04 14:44:08,155 - INFO - training batch 301, loss: 0.060, 9632/56000 datapoints
2025-03-04 14:44:08,253 - INFO - training batch 351, loss: 0.079, 11232/56000 datapoints
2025-03-04 14:44:08,355 - INFO - training batch 401, loss: 0.153, 12832/56000 datapoints
2025-03-04 14:44:08,445 - INFO - training batch 451, loss: 0.103, 14432/56000 datapoints
2025-03-04 14:44:08,539 - INFO - training batch 501, loss: 0.086, 16032/56000 datapoints
2025-03-04 14:44:08,620 - INFO - training batch 551, loss: 0.107, 17632/56000 datapoints
2025-03-04 14:44:08,697 - INFO - training batch 601, loss: 0.040, 19232/56000 datapoints
2025-03-04 14:44:08,775 - INFO - training batch 651, loss: 0.039, 20832/56000 datapoints
2025-03-04 14:44:08,873 - INFO - training batch 701, loss: 0.268, 22432/56000 datapoints
2025-03-04 14:44:08,968 - INFO - training batch 751, loss: 0.058, 24032/56000 datapoints
2025-03-04 14:44:09,065 - INFO - training batch 801, loss: 0.079, 25632/56000 datapoints
2025-03-04 14:44:09,159 - INFO - training batch 851, loss: 0.042, 27232/56000 datapoints
2025-03-04 14:44:09,250 - INFO - training batch 901, loss: 0.074, 28832/56000 datapoints
2025-03-04 14:44:09,329 - INFO - training batch 951, loss: 0.073, 30432/56000 datapoints
2025-03-04 14:44:09,412 - INFO - training batch 1001, loss: 0.032, 32032/56000 datapoints
2025-03-04 14:44:09,489 - INFO - training batch 1051, loss: 0.038, 33632/56000 datapoints
2025-03-04 14:44:09,577 - INFO - training batch 1101, loss: 0.108, 35232/56000 datapoints
2025-03-04 14:44:09,667 - INFO - training batch 1151, loss: 0.015, 36832/56000 datapoints
2025-03-04 14:44:09,754 - INFO - training batch 1201, loss: 0.051, 38432/56000 datapoints
2025-03-04 14:44:09,838 - INFO - training batch 1251, loss: 0.235, 40032/56000 datapoints
2025-03-04 14:44:09,918 - INFO - training batch 1301, loss: 0.056, 41632/56000 datapoints
2025-03-04 14:44:09,994 - INFO - training batch 1351, loss: 0.030, 43232/56000 datapoints
2025-03-04 14:44:10,081 - INFO - training batch 1401, loss: 0.030, 44832/56000 datapoints
2025-03-04 14:44:10,159 - INFO - training batch 1451, loss: 0.031, 46432/56000 datapoints
2025-03-04 14:44:10,240 - INFO - training batch 1501, loss: 0.138, 48032/56000 datapoints
2025-03-04 14:44:10,313 - INFO - training batch 1551, loss: 0.067, 49632/56000 datapoints
2025-03-04 14:44:10,386 - INFO - training batch 1601, loss: 0.083, 51232/56000 datapoints
2025-03-04 14:44:10,473 - INFO - training batch 1651, loss: 0.037, 52832/56000 datapoints
2025-03-04 14:44:10,557 - INFO - training batch 1701, loss: 0.077, 54432/56000 datapoints
2025-03-04 14:44:10,647 - INFO - validation batch 1, loss: 0.020, 32/13984 datapoints
2025-03-04 14:44:10,675 - INFO - validation batch 51, loss: 0.011, 1632/13984 datapoints
2025-03-04 14:44:10,707 - INFO - validation batch 101, loss: 0.069, 3232/13984 datapoints
2025-03-04 14:44:10,733 - INFO - validation batch 151, loss: 0.021, 4832/13984 datapoints
2025-03-04 14:44:10,766 - INFO - validation batch 201, loss: 0.046, 6432/13984 datapoints
2025-03-04 14:44:10,794 - INFO - validation batch 251, loss: 0.071, 8032/13984 datapoints
2025-03-04 14:44:10,821 - INFO - validation batch 301, loss: 0.121, 9632/13984 datapoints
2025-03-04 14:44:10,851 - INFO - validation batch 351, loss: 0.031, 11232/13984 datapoints
2025-03-04 14:44:10,875 - INFO - validation batch 401, loss: 0.150, 12832/13984 datapoints
2025-03-04 14:44:10,983 - INFO - Epoch 29/50 done
2025-03-04 14:44:10,984 - INFO - Beginning epoch 30/50
2025-03-04 14:44:11,026 - INFO - training batch 1, loss: 0.120, 32/56000 datapoints
2025-03-04 14:44:11,149 - INFO - training batch 51, loss: 0.095, 1632/56000 datapoints
2025-03-04 14:44:11,221 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:44:11,300 - INFO - training batch 151, loss: 0.096, 4832/56000 datapoints
2025-03-04 14:44:11,387 - INFO - training batch 201, loss: 0.042, 6432/56000 datapoints
2025-03-04 14:44:11,459 - INFO - training batch 251, loss: 0.051, 8032/56000 datapoints
2025-03-04 14:44:11,550 - INFO - training batch 301, loss: 0.059, 9632/56000 datapoints
2025-03-04 14:44:11,656 - INFO - training batch 351, loss: 0.078, 11232/56000 datapoints
2025-03-04 14:44:11,745 - INFO - training batch 401, loss: 0.151, 12832/56000 datapoints
2025-03-04 14:44:11,830 - INFO - training batch 451, loss: 0.102, 14432/56000 datapoints
2025-03-04 14:44:11,940 - INFO - training batch 501, loss: 0.085, 16032/56000 datapoints
2025-03-04 14:44:12,022 - INFO - training batch 551, loss: 0.106, 17632/56000 datapoints
2025-03-04 14:44:12,111 - INFO - training batch 601, loss: 0.040, 19232/56000 datapoints
2025-03-04 14:44:12,185 - INFO - training batch 651, loss: 0.039, 20832/56000 datapoints
2025-03-04 14:44:12,256 - INFO - training batch 701, loss: 0.267, 22432/56000 datapoints
2025-03-04 14:44:12,326 - INFO - training batch 751, loss: 0.058, 24032/56000 datapoints
2025-03-04 14:44:12,399 - INFO - training batch 801, loss: 0.078, 25632/56000 datapoints
2025-03-04 14:44:12,474 - INFO - training batch 851, loss: 0.042, 27232/56000 datapoints
2025-03-04 14:44:12,556 - INFO - training batch 901, loss: 0.073, 28832/56000 datapoints
2025-03-04 14:44:12,637 - INFO - training batch 951, loss: 0.073, 30432/56000 datapoints
2025-03-04 14:44:12,707 - INFO - training batch 1001, loss: 0.032, 32032/56000 datapoints
2025-03-04 14:44:12,775 - INFO - training batch 1051, loss: 0.037, 33632/56000 datapoints
2025-03-04 14:44:12,851 - INFO - training batch 1101, loss: 0.108, 35232/56000 datapoints
2025-03-04 14:44:12,931 - INFO - training batch 1151, loss: 0.014, 36832/56000 datapoints
2025-03-04 14:44:13,006 - INFO - training batch 1201, loss: 0.050, 38432/56000 datapoints
2025-03-04 14:44:13,088 - INFO - training batch 1251, loss: 0.233, 40032/56000 datapoints
2025-03-04 14:44:13,160 - INFO - training batch 1301, loss: 0.055, 41632/56000 datapoints
2025-03-04 14:44:13,228 - INFO - training batch 1351, loss: 0.029, 43232/56000 datapoints
2025-03-04 14:44:13,299 - INFO - training batch 1401, loss: 0.030, 44832/56000 datapoints
2025-03-04 14:44:13,366 - INFO - training batch 1451, loss: 0.030, 46432/56000 datapoints
2025-03-04 14:44:13,434 - INFO - training batch 1501, loss: 0.137, 48032/56000 datapoints
2025-03-04 14:44:13,505 - INFO - training batch 1551, loss: 0.066, 49632/56000 datapoints
2025-03-04 14:44:13,576 - INFO - training batch 1601, loss: 0.082, 51232/56000 datapoints
2025-03-04 14:44:13,644 - INFO - training batch 1651, loss: 0.036, 52832/56000 datapoints
2025-03-04 14:44:13,710 - INFO - training batch 1701, loss: 0.077, 54432/56000 datapoints
2025-03-04 14:44:13,776 - INFO - validation batch 1, loss: 0.019, 32/13984 datapoints
2025-03-04 14:44:13,799 - INFO - validation batch 51, loss: 0.011, 1632/13984 datapoints
2025-03-04 14:44:13,820 - INFO - validation batch 101, loss: 0.068, 3232/13984 datapoints
2025-03-04 14:44:13,846 - INFO - validation batch 151, loss: 0.020, 4832/13984 datapoints
2025-03-04 14:44:13,869 - INFO - validation batch 201, loss: 0.046, 6432/13984 datapoints
2025-03-04 14:44:13,889 - INFO - validation batch 251, loss: 0.070, 8032/13984 datapoints
2025-03-04 14:44:13,921 - INFO - validation batch 301, loss: 0.120, 9632/13984 datapoints
2025-03-04 14:44:13,944 - INFO - validation batch 351, loss: 0.031, 11232/13984 datapoints
2025-03-04 14:44:13,972 - INFO - validation batch 401, loss: 0.149, 12832/13984 datapoints
2025-03-04 14:44:13,990 - INFO - Epoch 30/50 done
2025-03-04 14:44:13,992 - INFO - Beginning epoch 31/50
2025-03-04 14:44:14,013 - INFO - training batch 1, loss: 0.120, 32/56000 datapoints
2025-03-04 14:44:14,115 - INFO - training batch 51, loss: 0.093, 1632/56000 datapoints
2025-03-04 14:44:14,181 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:44:14,249 - INFO - training batch 151, loss: 0.096, 4832/56000 datapoints
2025-03-04 14:44:14,316 - INFO - training batch 201, loss: 0.042, 6432/56000 datapoints
2025-03-04 14:44:14,386 - INFO - training batch 251, loss: 0.050, 8032/56000 datapoints
2025-03-04 14:44:14,457 - INFO - training batch 301, loss: 0.059, 9632/56000 datapoints
2025-03-04 14:44:14,523 - INFO - training batch 351, loss: 0.078, 11232/56000 datapoints
2025-03-04 14:44:14,590 - INFO - training batch 401, loss: 0.150, 12832/56000 datapoints
2025-03-04 14:44:14,655 - INFO - training batch 451, loss: 0.101, 14432/56000 datapoints
2025-03-04 14:44:14,718 - INFO - training batch 501, loss: 0.084, 16032/56000 datapoints
2025-03-04 14:44:14,783 - INFO - training batch 551, loss: 0.104, 17632/56000 datapoints
2025-03-04 14:44:14,848 - INFO - training batch 601, loss: 0.040, 19232/56000 datapoints
2025-03-04 14:44:14,917 - INFO - training batch 651, loss: 0.039, 20832/56000 datapoints
2025-03-04 14:44:15,007 - INFO - training batch 701, loss: 0.266, 22432/56000 datapoints
2025-03-04 14:44:15,091 - INFO - training batch 751, loss: 0.057, 24032/56000 datapoints
2025-03-04 14:44:15,184 - INFO - training batch 801, loss: 0.077, 25632/56000 datapoints
2025-03-04 14:44:15,253 - INFO - training batch 851, loss: 0.041, 27232/56000 datapoints
2025-03-04 14:44:15,322 - INFO - training batch 901, loss: 0.073, 28832/56000 datapoints
2025-03-04 14:44:15,396 - INFO - training batch 951, loss: 0.072, 30432/56000 datapoints
2025-03-04 14:44:15,469 - INFO - training batch 1001, loss: 0.031, 32032/56000 datapoints
2025-03-04 14:44:15,644 - INFO - training batch 1051, loss: 0.037, 33632/56000 datapoints
2025-03-04 14:44:15,759 - INFO - training batch 1101, loss: 0.107, 35232/56000 datapoints
2025-03-04 14:44:15,874 - INFO - training batch 1151, loss: 0.014, 36832/56000 datapoints
2025-03-04 14:44:15,995 - INFO - training batch 1201, loss: 0.050, 38432/56000 datapoints
2025-03-04 14:44:16,082 - INFO - training batch 1251, loss: 0.231, 40032/56000 datapoints
2025-03-04 14:44:16,165 - INFO - training batch 1301, loss: 0.055, 41632/56000 datapoints
2025-03-04 14:44:16,265 - INFO - training batch 1351, loss: 0.029, 43232/56000 datapoints
2025-03-04 14:44:16,396 - INFO - training batch 1401, loss: 0.029, 44832/56000 datapoints
2025-03-04 14:44:16,591 - INFO - training batch 1451, loss: 0.030, 46432/56000 datapoints
2025-03-04 14:44:16,690 - INFO - training batch 1501, loss: 0.135, 48032/56000 datapoints
2025-03-04 14:44:16,788 - INFO - training batch 1551, loss: 0.065, 49632/56000 datapoints
2025-03-04 14:44:16,884 - INFO - training batch 1601, loss: 0.081, 51232/56000 datapoints
2025-03-04 14:44:17,141 - INFO - training batch 1651, loss: 0.036, 52832/56000 datapoints
2025-03-04 14:44:17,305 - INFO - training batch 1701, loss: 0.077, 54432/56000 datapoints
2025-03-04 14:44:17,442 - INFO - validation batch 1, loss: 0.019, 32/13984 datapoints
2025-03-04 14:44:17,498 - INFO - validation batch 51, loss: 0.011, 1632/13984 datapoints
2025-03-04 14:44:17,529 - INFO - validation batch 101, loss: 0.067, 3232/13984 datapoints
2025-03-04 14:44:17,579 - INFO - validation batch 151, loss: 0.020, 4832/13984 datapoints
2025-03-04 14:44:17,612 - INFO - validation batch 201, loss: 0.045, 6432/13984 datapoints
2025-03-04 14:44:17,653 - INFO - validation batch 251, loss: 0.069, 8032/13984 datapoints
2025-03-04 14:44:17,700 - INFO - validation batch 301, loss: 0.119, 9632/13984 datapoints
2025-03-04 14:44:17,777 - INFO - validation batch 351, loss: 0.030, 11232/13984 datapoints
2025-03-04 14:44:17,848 - INFO - validation batch 401, loss: 0.148, 12832/13984 datapoints
2025-03-04 14:44:17,873 - INFO - Epoch 31/50 done
2025-03-04 14:44:17,874 - INFO - Beginning epoch 32/50
2025-03-04 14:44:17,879 - INFO - training batch 1, loss: 0.120, 32/56000 datapoints
2025-03-04 14:44:18,030 - INFO - training batch 51, loss: 0.092, 1632/56000 datapoints
2025-03-04 14:44:18,153 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:44:18,251 - INFO - training batch 151, loss: 0.095, 4832/56000 datapoints
2025-03-04 14:44:18,359 - INFO - training batch 201, loss: 0.041, 6432/56000 datapoints
2025-03-04 14:44:18,442 - INFO - training batch 251, loss: 0.050, 8032/56000 datapoints
2025-03-04 14:44:18,532 - INFO - training batch 301, loss: 0.058, 9632/56000 datapoints
2025-03-04 14:44:18,631 - INFO - training batch 351, loss: 0.077, 11232/56000 datapoints
2025-03-04 14:44:18,731 - INFO - training batch 401, loss: 0.148, 12832/56000 datapoints
2025-03-04 14:44:18,824 - INFO - training batch 451, loss: 0.100, 14432/56000 datapoints
2025-03-04 14:44:18,945 - INFO - training batch 501, loss: 0.083, 16032/56000 datapoints
2025-03-04 14:44:19,075 - INFO - training batch 551, loss: 0.103, 17632/56000 datapoints
2025-03-04 14:44:19,184 - INFO - training batch 601, loss: 0.039, 19232/56000 datapoints
2025-03-04 14:44:19,284 - INFO - training batch 651, loss: 0.038, 20832/56000 datapoints
2025-03-04 14:44:19,423 - INFO - training batch 701, loss: 0.266, 22432/56000 datapoints
2025-03-04 14:44:19,559 - INFO - training batch 751, loss: 0.056, 24032/56000 datapoints
2025-03-04 14:44:19,673 - INFO - training batch 801, loss: 0.076, 25632/56000 datapoints
2025-03-04 14:44:19,779 - INFO - training batch 851, loss: 0.041, 27232/56000 datapoints
2025-03-04 14:44:19,859 - INFO - training batch 901, loss: 0.072, 28832/56000 datapoints
2025-03-04 14:44:19,952 - INFO - training batch 951, loss: 0.071, 30432/56000 datapoints
2025-03-04 14:44:20,024 - INFO - training batch 1001, loss: 0.031, 32032/56000 datapoints
2025-03-04 14:44:20,095 - INFO - training batch 1051, loss: 0.036, 33632/56000 datapoints
2025-03-04 14:44:20,165 - INFO - training batch 1101, loss: 0.106, 35232/56000 datapoints
2025-03-04 14:44:20,231 - INFO - training batch 1151, loss: 0.014, 36832/56000 datapoints
2025-03-04 14:44:20,298 - INFO - training batch 1201, loss: 0.049, 38432/56000 datapoints
2025-03-04 14:44:20,365 - INFO - training batch 1251, loss: 0.229, 40032/56000 datapoints
2025-03-04 14:44:20,438 - INFO - training batch 1301, loss: 0.054, 41632/56000 datapoints
2025-03-04 14:44:20,506 - INFO - training batch 1351, loss: 0.028, 43232/56000 datapoints
2025-03-04 14:44:20,584 - INFO - training batch 1401, loss: 0.029, 44832/56000 datapoints
2025-03-04 14:44:20,651 - INFO - training batch 1451, loss: 0.029, 46432/56000 datapoints
2025-03-04 14:44:20,720 - INFO - training batch 1501, loss: 0.133, 48032/56000 datapoints
2025-03-04 14:44:20,785 - INFO - training batch 1551, loss: 0.065, 49632/56000 datapoints
2025-03-04 14:44:20,855 - INFO - training batch 1601, loss: 0.080, 51232/56000 datapoints
2025-03-04 14:44:20,927 - INFO - training batch 1651, loss: 0.036, 52832/56000 datapoints
2025-03-04 14:44:21,001 - INFO - training batch 1701, loss: 0.076, 54432/56000 datapoints
2025-03-04 14:44:21,071 - INFO - validation batch 1, loss: 0.019, 32/13984 datapoints
2025-03-04 14:44:21,093 - INFO - validation batch 51, loss: 0.011, 1632/13984 datapoints
2025-03-04 14:44:21,114 - INFO - validation batch 101, loss: 0.067, 3232/13984 datapoints
2025-03-04 14:44:21,139 - INFO - validation batch 151, loss: 0.020, 4832/13984 datapoints
2025-03-04 14:44:21,161 - INFO - validation batch 201, loss: 0.044, 6432/13984 datapoints
2025-03-04 14:44:21,187 - INFO - validation batch 251, loss: 0.069, 8032/13984 datapoints
2025-03-04 14:44:21,208 - INFO - validation batch 301, loss: 0.117, 9632/13984 datapoints
2025-03-04 14:44:21,231 - INFO - validation batch 351, loss: 0.030, 11232/13984 datapoints
2025-03-04 14:44:21,252 - INFO - validation batch 401, loss: 0.147, 12832/13984 datapoints
2025-03-04 14:44:21,272 - INFO - Epoch 32/50 done
2025-03-04 14:44:21,273 - INFO - Beginning epoch 33/50
2025-03-04 14:44:21,276 - INFO - training batch 1, loss: 0.120, 32/56000 datapoints
2025-03-04 14:44:21,345 - INFO - training batch 51, loss: 0.091, 1632/56000 datapoints
2025-03-04 14:44:21,425 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:44:21,517 - INFO - training batch 151, loss: 0.094, 4832/56000 datapoints
2025-03-04 14:44:21,596 - INFO - training batch 201, loss: 0.041, 6432/56000 datapoints
2025-03-04 14:44:21,684 - INFO - training batch 251, loss: 0.049, 8032/56000 datapoints
2025-03-04 14:44:21,767 - INFO - training batch 301, loss: 0.058, 9632/56000 datapoints
2025-03-04 14:44:21,854 - INFO - training batch 351, loss: 0.077, 11232/56000 datapoints
2025-03-04 14:44:21,942 - INFO - training batch 401, loss: 0.147, 12832/56000 datapoints
2025-03-04 14:44:22,023 - INFO - training batch 451, loss: 0.099, 14432/56000 datapoints
2025-03-04 14:44:22,106 - INFO - training batch 501, loss: 0.082, 16032/56000 datapoints
2025-03-04 14:44:22,191 - INFO - training batch 551, loss: 0.102, 17632/56000 datapoints
2025-03-04 14:44:22,289 - INFO - training batch 601, loss: 0.039, 19232/56000 datapoints
2025-03-04 14:44:22,380 - INFO - training batch 651, loss: 0.038, 20832/56000 datapoints
2025-03-04 14:44:22,483 - INFO - training batch 701, loss: 0.265, 22432/56000 datapoints
2025-03-04 14:44:22,579 - INFO - training batch 751, loss: 0.056, 24032/56000 datapoints
2025-03-04 14:44:22,674 - INFO - training batch 801, loss: 0.075, 25632/56000 datapoints
2025-03-04 14:44:22,789 - INFO - training batch 851, loss: 0.040, 27232/56000 datapoints
2025-03-04 14:44:22,875 - INFO - training batch 901, loss: 0.071, 28832/56000 datapoints
2025-03-04 14:44:22,960 - INFO - training batch 951, loss: 0.070, 30432/56000 datapoints
2025-03-04 14:44:23,055 - INFO - training batch 1001, loss: 0.031, 32032/56000 datapoints
2025-03-04 14:44:23,176 - INFO - training batch 1051, loss: 0.036, 33632/56000 datapoints
2025-03-04 14:44:23,268 - INFO - training batch 1101, loss: 0.106, 35232/56000 datapoints
2025-03-04 14:44:23,348 - INFO - training batch 1151, loss: 0.014, 36832/56000 datapoints
2025-03-04 14:44:23,422 - INFO - training batch 1201, loss: 0.048, 38432/56000 datapoints
2025-03-04 14:44:23,499 - INFO - training batch 1251, loss: 0.227, 40032/56000 datapoints
2025-03-04 14:44:23,581 - INFO - training batch 1301, loss: 0.053, 41632/56000 datapoints
2025-03-04 14:44:23,684 - INFO - training batch 1351, loss: 0.028, 43232/56000 datapoints
2025-03-04 14:44:23,789 - INFO - training batch 1401, loss: 0.028, 44832/56000 datapoints
2025-03-04 14:44:23,938 - INFO - training batch 1451, loss: 0.029, 46432/56000 datapoints
2025-03-04 14:44:24,235 - INFO - training batch 1501, loss: 0.132, 48032/56000 datapoints
2025-03-04 14:44:24,414 - INFO - training batch 1551, loss: 0.064, 49632/56000 datapoints
2025-03-04 14:44:24,869 - INFO - training batch 1601, loss: 0.079, 51232/56000 datapoints
2025-03-04 14:44:25,101 - INFO - training batch 1651, loss: 0.035, 52832/56000 datapoints
2025-03-04 14:44:25,423 - INFO - training batch 1701, loss: 0.076, 54432/56000 datapoints
2025-03-04 14:44:25,916 - INFO - validation batch 1, loss: 0.019, 32/13984 datapoints
2025-03-04 14:44:25,964 - INFO - validation batch 51, loss: 0.011, 1632/13984 datapoints
2025-03-04 14:44:26,000 - INFO - validation batch 101, loss: 0.066, 3232/13984 datapoints
2025-03-04 14:44:26,039 - INFO - validation batch 151, loss: 0.020, 4832/13984 datapoints
2025-03-04 14:44:26,081 - INFO - validation batch 201, loss: 0.044, 6432/13984 datapoints
2025-03-04 14:44:26,131 - INFO - validation batch 251, loss: 0.068, 8032/13984 datapoints
2025-03-04 14:44:26,181 - INFO - validation batch 301, loss: 0.116, 9632/13984 datapoints
2025-03-04 14:44:26,226 - INFO - validation batch 351, loss: 0.030, 11232/13984 datapoints
2025-03-04 14:44:26,265 - INFO - validation batch 401, loss: 0.146, 12832/13984 datapoints
2025-03-04 14:44:26,284 - INFO - Epoch 33/50 done
2025-03-04 14:44:26,284 - INFO - Beginning epoch 34/50
2025-03-04 14:44:26,287 - INFO - training batch 1, loss: 0.120, 32/56000 datapoints
2025-03-04 14:44:26,371 - INFO - training batch 51, loss: 0.090, 1632/56000 datapoints
2025-03-04 14:44:26,445 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:44:26,519 - INFO - training batch 151, loss: 0.093, 4832/56000 datapoints
2025-03-04 14:44:26,593 - INFO - training batch 201, loss: 0.040, 6432/56000 datapoints
2025-03-04 14:44:26,664 - INFO - training batch 251, loss: 0.049, 8032/56000 datapoints
2025-03-04 14:44:26,734 - INFO - training batch 301, loss: 0.057, 9632/56000 datapoints
2025-03-04 14:44:26,804 - INFO - training batch 351, loss: 0.076, 11232/56000 datapoints
2025-03-04 14:44:27,374 - INFO - training batch 401, loss: 0.145, 12832/56000 datapoints
2025-03-04 14:44:27,652 - INFO - training batch 451, loss: 0.098, 14432/56000 datapoints
2025-03-04 14:44:28,453 - INFO - training batch 501, loss: 0.081, 16032/56000 datapoints
2025-03-04 14:44:28,614 - INFO - training batch 551, loss: 0.101, 17632/56000 datapoints
2025-03-04 14:44:28,772 - INFO - training batch 601, loss: 0.038, 19232/56000 datapoints
2025-03-04 14:44:29,326 - INFO - training batch 651, loss: 0.038, 20832/56000 datapoints
2025-03-04 14:44:29,593 - INFO - training batch 701, loss: 0.264, 22432/56000 datapoints
2025-03-04 14:44:29,763 - INFO - training batch 751, loss: 0.055, 24032/56000 datapoints
2025-03-04 14:44:29,875 - INFO - training batch 801, loss: 0.074, 25632/56000 datapoints
2025-03-04 14:44:29,988 - INFO - training batch 851, loss: 0.040, 27232/56000 datapoints
2025-03-04 14:44:30,097 - INFO - training batch 901, loss: 0.070, 28832/56000 datapoints
2025-03-04 14:44:30,219 - INFO - training batch 951, loss: 0.069, 30432/56000 datapoints
2025-03-04 14:44:30,315 - INFO - training batch 1001, loss: 0.030, 32032/56000 datapoints
2025-03-04 14:44:30,399 - INFO - training batch 1051, loss: 0.035, 33632/56000 datapoints
2025-03-04 14:44:30,542 - INFO - training batch 1101, loss: 0.105, 35232/56000 datapoints
2025-03-04 14:44:30,633 - INFO - training batch 1151, loss: 0.014, 36832/56000 datapoints
2025-03-04 14:44:30,773 - INFO - training batch 1201, loss: 0.048, 38432/56000 datapoints
2025-03-04 14:44:30,882 - INFO - training batch 1251, loss: 0.225, 40032/56000 datapoints
2025-03-04 14:44:31,124 - INFO - training batch 1301, loss: 0.053, 41632/56000 datapoints
2025-03-04 14:44:31,290 - INFO - training batch 1351, loss: 0.028, 43232/56000 datapoints
2025-03-04 14:44:31,682 - INFO - training batch 1401, loss: 0.028, 44832/56000 datapoints
2025-03-04 14:44:31,777 - INFO - training batch 1451, loss: 0.029, 46432/56000 datapoints
2025-03-04 14:44:31,869 - INFO - training batch 1501, loss: 0.130, 48032/56000 datapoints
2025-03-04 14:44:32,081 - INFO - training batch 1551, loss: 0.063, 49632/56000 datapoints
2025-03-04 14:44:32,230 - INFO - training batch 1601, loss: 0.079, 51232/56000 datapoints
2025-03-04 14:44:32,348 - INFO - training batch 1651, loss: 0.035, 52832/56000 datapoints
2025-03-04 14:44:32,479 - INFO - training batch 1701, loss: 0.076, 54432/56000 datapoints
2025-03-04 14:44:32,618 - INFO - validation batch 1, loss: 0.018, 32/13984 datapoints
2025-03-04 14:44:32,698 - INFO - validation batch 51, loss: 0.011, 1632/13984 datapoints
2025-03-04 14:44:32,752 - INFO - validation batch 101, loss: 0.065, 3232/13984 datapoints
2025-03-04 14:44:32,805 - INFO - validation batch 151, loss: 0.019, 4832/13984 datapoints
2025-03-04 14:44:32,851 - INFO - validation batch 201, loss: 0.043, 6432/13984 datapoints
2025-03-04 14:44:32,912 - INFO - validation batch 251, loss: 0.067, 8032/13984 datapoints
2025-03-04 14:44:32,947 - INFO - validation batch 301, loss: 0.115, 9632/13984 datapoints
2025-03-04 14:44:32,984 - INFO - validation batch 351, loss: 0.029, 11232/13984 datapoints
2025-03-04 14:44:33,036 - INFO - validation batch 401, loss: 0.145, 12832/13984 datapoints
2025-03-04 14:44:33,065 - INFO - Epoch 34/50 done
2025-03-04 14:44:33,066 - INFO - Beginning epoch 35/50
2025-03-04 14:44:33,069 - INFO - training batch 1, loss: 0.119, 32/56000 datapoints
2025-03-04 14:44:33,170 - INFO - training batch 51, loss: 0.089, 1632/56000 datapoints
2025-03-04 14:44:33,248 - INFO - training batch 101, loss: 0.059, 3232/56000 datapoints
2025-03-04 14:44:33,358 - INFO - training batch 151, loss: 0.092, 4832/56000 datapoints
2025-03-04 14:44:33,449 - INFO - training batch 201, loss: 0.040, 6432/56000 datapoints
2025-03-04 14:44:33,565 - INFO - training batch 251, loss: 0.048, 8032/56000 datapoints
2025-03-04 14:44:33,687 - INFO - training batch 301, loss: 0.056, 9632/56000 datapoints
2025-03-04 14:44:33,822 - INFO - training batch 351, loss: 0.076, 11232/56000 datapoints
2025-03-04 14:44:33,998 - INFO - training batch 401, loss: 0.144, 12832/56000 datapoints
2025-03-04 14:44:34,248 - INFO - training batch 451, loss: 0.097, 14432/56000 datapoints
2025-03-04 14:44:34,485 - INFO - training batch 501, loss: 0.080, 16032/56000 datapoints
2025-03-04 14:44:34,642 - INFO - training batch 551, loss: 0.099, 17632/56000 datapoints
2025-03-04 14:44:34,772 - INFO - training batch 601, loss: 0.038, 19232/56000 datapoints
2025-03-04 14:44:35,180 - INFO - training batch 651, loss: 0.037, 20832/56000 datapoints
2025-03-04 14:44:36,047 - INFO - training batch 701, loss: 0.263, 22432/56000 datapoints
2025-03-04 14:44:36,831 - INFO - training batch 751, loss: 0.054, 24032/56000 datapoints
2025-03-04 14:44:37,073 - INFO - training batch 801, loss: 0.073, 25632/56000 datapoints
2025-03-04 14:44:37,369 - INFO - training batch 851, loss: 0.039, 27232/56000 datapoints
2025-03-04 14:44:37,524 - INFO - training batch 901, loss: 0.069, 28832/56000 datapoints
2025-03-04 14:44:37,964 - INFO - training batch 951, loss: 0.068, 30432/56000 datapoints
2025-03-04 14:44:38,496 - INFO - training batch 1001, loss: 0.030, 32032/56000 datapoints
2025-03-04 14:44:39,307 - INFO - training batch 1051, loss: 0.035, 33632/56000 datapoints
2025-03-04 14:44:39,555 - ERROR - Traceback (most recent call last):
2025-03-04 14:44:39,555 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/train_scripts/train_ff_finetune.py", line 195, in <module>
    ohe_targets=hyperparams["ohe_targets"],
2025-03-04 14:44:39,555 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 420, in train_thalreadout
    train_losses, state_dict = train_one_epoch_thalreadout(
2025-03-04 14:44:39,556 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 371, in train_one_epoch_thalreadout
    _, y_est = model(X)
2025-03-04 14:44:39,556 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
2025-03-04 14:44:39,557 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/models.py", line 299, in forward
    _, thal, _ = self.subforward(input)
2025-03-04 14:44:39,557 - ERROR - File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/models.py", line 363, in subforward
    thal_output = self.thal_readout(thal)
2025-03-04 14:44:39,558 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
2025-03-04 14:44:39,558 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
2025-03-04 14:44:39,558 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
2025-03-04 14:44:39,558 - ERROR - File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
2025-03-04 14:44:39,560 - ERROR - KeyboardInterrupt
2025-03-04 14:44:39,565 - ERROR - Traceback (most recent call last):
  File "/Users/patmccarthy/Documents/ThalamoCortex/train_scripts/train_ff_finetune.py", line 195, in <module>
    ohe_targets=hyperparams["ohe_targets"],
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 420, in train_thalreadout
    train_losses, state_dict = train_one_epoch_thalreadout(
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/utils.py", line 371, in train_one_epoch_thalreadout
    _, y_est = model(X)
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/models.py", line 299, in forward
    _, thal, _ = self.subforward(input)
  File "/Users/patmccarthy/Documents/ThalamoCortex/thalamocortex/models.py", line 363, in subforward
    thal_output = self.thal_readout(thal)
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt