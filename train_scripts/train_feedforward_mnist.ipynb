{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patmccarthy/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import traceback\n",
    "from datetime import date\n",
    "from copy import copy, deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from thalamocortex.models import CTCNet\n",
    "from thalamocortex.utils import make_grid, create_data_loaders, train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom class to redirect stdout to logger\n",
    "class LoggerWriter:\n",
    "    def __init__(self, level):\n",
    "        self.level = level  # Logging level (INFO, ERROR, etc.)\n",
    "\n",
    "    def write(self, message):\n",
    "        if message.strip():  # Avoid logging empty lines\n",
    "            self.level(message.strip())\n",
    "\n",
    "    def flush(self):\n",
    "        pass  # Needed for compatibility with sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter grid for driver-type model\n",
    "hyperparam_grid = {\n",
    "    # data hyperparams\n",
    "    \"norm\" : [\"normalise\"],\n",
    "    \"dataset\" : [\"MNIST\"],\n",
    "    \"save_path\" : [\"/Users/patmccarthy/Documents/thalamocortex/data\"],\n",
    "    \"batch_size\" : [32],\n",
    "    # model hyperparams\n",
    "    \"input_size\" : [28 * 28],\n",
    "    \"output_size\" : [10],\n",
    "    \"ctx_layer_size\" : [32, 64],\n",
    "    \"thal_layer_size\" : [16],\n",
    "    \"thalamocortical_type\" : [None],\n",
    "    \"thal_reciprocal\" : [False], \n",
    "    \"thal_to_readout\" : [False], \n",
    "    \"thal_per_layer\" : [False],\n",
    "    # training hyperparams\n",
    "    \"lr\" : [1e-6],\n",
    "    \"loss\" : [torch.nn.CrossEntropyLoss()],\n",
    "    \"epochs\": [800],\n",
    "    \"ohe_targets\": [True],\n",
    "    \"track_loss_step\": [50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make parameter grid\n",
    "model_param_grid = make_grid(hyperparam_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set backend\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Linear: 2-1                       1,040\n",
      "|    └─ReLU: 2-2                         --\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─Linear: 2-3                       25,120\n",
      "|    └─ReLU: 2-4                         --\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─Linear: 2-5                       1,056\n",
      "|    └─ReLU: 2-6                         --\n",
      "├─Sequential: 1-4                        --\n",
      "|    └─Linear: 2-7                       330\n",
      "=================================================================\n",
      "Total params: 27,546\n",
      "Trainable params: 27,546\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Training...\n",
      "Beginning epoch 1/800\n",
      "training batch 1, loss: 2.337, 32/60000 datapoints\n",
      "training batch 51, loss: 2.324, 1632/60000 datapoints\n",
      "training batch 101, loss: 2.324, 3232/60000 datapoints\n",
      "training batch 151, loss: 2.356, 4832/60000 datapoints\n",
      "training batch 201, loss: 2.332, 6432/60000 datapoints\n",
      "training batch 251, loss: 2.293, 8032/60000 datapoints\n",
      "training batch 301, loss: 2.297, 9632/60000 datapoints\n",
      "training batch 351, loss: 2.324, 11232/60000 datapoints\n",
      "training batch 401, loss: 2.328, 12832/60000 datapoints\n",
      "training batch 451, loss: 2.304, 14432/60000 datapoints\n",
      "training batch 501, loss: 2.305, 16032/60000 datapoints\n",
      "training batch 551, loss: 2.320, 17632/60000 datapoints\n",
      "training batch 601, loss: 2.287, 19232/60000 datapoints\n",
      "training batch 651, loss: 2.306, 20832/60000 datapoints\n",
      "training batch 701, loss: 2.305, 22432/60000 datapoints\n",
      "training batch 751, loss: 2.312, 24032/60000 datapoints\n",
      "training batch 801, loss: 2.327, 25632/60000 datapoints\n",
      "training batch 851, loss: 2.272, 27232/60000 datapoints\n",
      "training batch 901, loss: 2.277, 28832/60000 datapoints\n",
      "training batch 951, loss: 2.317, 30432/60000 datapoints\n",
      "training batch 1001, loss: 2.297, 32032/60000 datapoints\n",
      "training batch 1051, loss: 2.303, 33632/60000 datapoints\n",
      "training batch 1101, loss: 2.298, 35232/60000 datapoints\n",
      "training batch 1151, loss: 2.289, 36832/60000 datapoints\n",
      "training batch 1201, loss: 2.338, 38432/60000 datapoints\n",
      "training batch 1251, loss: 2.319, 40032/60000 datapoints\n",
      "training batch 1301, loss: 2.285, 41632/60000 datapoints\n",
      "training batch 1351, loss: 2.341, 43232/60000 datapoints\n",
      "training batch 1401, loss: 2.285, 44832/60000 datapoints\n",
      "training batch 1451, loss: 2.312, 46432/60000 datapoints\n",
      "training batch 1501, loss: 2.278, 48032/60000 datapoints\n",
      "training batch 1551, loss: 2.320, 49632/60000 datapoints\n",
      "training batch 1601, loss: 2.320, 51232/60000 datapoints\n",
      "training batch 1651, loss: 2.319, 52832/60000 datapoints\n",
      "training batch 1701, loss: 2.272, 54432/60000 datapoints\n",
      "training batch 1751, loss: 2.313, 56032/60000 datapoints\n",
      "training batch 1801, loss: 2.311, 57632/60000 datapoints\n",
      "training batch 1851, loss: 2.302, 59232/60000 datapoints\n",
      "validation batch 1, loss: 2.343, 32/10016 datapoints\n",
      "validation batch 51, loss: 2.312, 1632/10016 datapoints\n",
      "validation batch 101, loss: 2.319, 3232/10016 datapoints\n",
      "validation batch 151, loss: 2.305, 4832/10016 datapoints\n",
      "validation batch 201, loss: 2.321, 6432/10016 datapoints\n",
      "validation batch 251, loss: 2.304, 8032/10016 datapoints\n",
      "validation batch 301, loss: 2.288, 9632/10016 datapoints\n",
      "train_topk_accs={1: 0.1474, 5: 0.49921666666666664}\n",
      "Epoch 1/800 done.\n",
      "Final validation performance:\n",
      "Loss: 2.313, top-1 acc: 0.148top-5 acc: 0.148\n",
      "Beginning epoch 2/800\n",
      "training batch 1, loss: 2.303, 32/60000 datapoints\n",
      "training batch 51, loss: 2.308, 1632/60000 datapoints\n",
      "training batch 101, loss: 2.317, 3232/60000 datapoints\n",
      "training batch 151, loss: 2.298, 4832/60000 datapoints\n",
      "training batch 201, loss: 2.301, 6432/60000 datapoints\n",
      "training batch 251, loss: 2.306, 8032/60000 datapoints\n",
      "training batch 301, loss: 2.319, 9632/60000 datapoints\n",
      "training batch 351, loss: 2.325, 11232/60000 datapoints\n",
      "training batch 401, loss: 2.295, 12832/60000 datapoints\n",
      "training batch 451, loss: 2.328, 14432/60000 datapoints\n",
      "training batch 501, loss: 2.307, 16032/60000 datapoints\n",
      "training batch 551, loss: 2.350, 17632/60000 datapoints\n",
      "training batch 601, loss: 2.294, 19232/60000 datapoints\n",
      "training batch 651, loss: 2.296, 20832/60000 datapoints\n",
      "training batch 701, loss: 2.313, 22432/60000 datapoints\n",
      "training batch 751, loss: 2.277, 24032/60000 datapoints\n",
      "training batch 801, loss: 2.295, 25632/60000 datapoints\n",
      "training batch 851, loss: 2.261, 27232/60000 datapoints\n",
      "training batch 901, loss: 2.311, 28832/60000 datapoints\n",
      "training batch 951, loss: 2.260, 30432/60000 datapoints\n",
      "training batch 1001, loss: 2.292, 32032/60000 datapoints\n",
      "training batch 1051, loss: 2.288, 33632/60000 datapoints\n",
      "training batch 1101, loss: 2.228, 35232/60000 datapoints\n",
      "training batch 1151, loss: 2.279, 36832/60000 datapoints\n",
      "training batch 1201, loss: 2.259, 38432/60000 datapoints\n",
      "training batch 1251, loss: 2.274, 40032/60000 datapoints\n",
      "training batch 1301, loss: 2.308, 41632/60000 datapoints\n",
      "training batch 1351, loss: 2.341, 43232/60000 datapoints\n",
      "training batch 1401, loss: 2.311, 44832/60000 datapoints\n",
      "training batch 1451, loss: 2.256, 46432/60000 datapoints\n",
      "training batch 1501, loss: 2.299, 48032/60000 datapoints\n",
      "training batch 1551, loss: 2.334, 49632/60000 datapoints\n",
      "training batch 1601, loss: 2.284, 51232/60000 datapoints\n",
      "training batch 1651, loss: 2.322, 52832/60000 datapoints\n",
      "training batch 1701, loss: 2.282, 54432/60000 datapoints\n",
      "training batch 1751, loss: 2.234, 56032/60000 datapoints\n",
      "training batch 1801, loss: 2.290, 57632/60000 datapoints\n",
      "training batch 1851, loss: 2.268, 59232/60000 datapoints\n",
      "validation batch 1, loss: 2.295, 32/10016 datapoints\n",
      "validation batch 51, loss: 2.325, 1632/10016 datapoints\n",
      "validation batch 101, loss: 2.277, 3232/10016 datapoints\n",
      "validation batch 151, loss: 2.258, 4832/10016 datapoints\n",
      "validation batch 201, loss: 2.302, 6432/10016 datapoints\n",
      "validation batch 251, loss: 2.276, 8032/10016 datapoints\n",
      "validation batch 301, loss: 2.309, 9632/10016 datapoints\n",
      "train_topk_accs={1: 0.16293333333333335, 5: 0.4994}\n",
      "Epoch 2/800 done.\n",
      "Final validation performance:\n",
      "Loss: 2.292, top-1 acc: 0.159top-5 acc: 0.159\n",
      "Beginning epoch 3/800\n",
      "training batch 1, loss: 2.272, 32/60000 datapoints\n",
      "training batch 51, loss: 2.268, 1632/60000 datapoints\n",
      "training batch 101, loss: 2.283, 3232/60000 datapoints\n",
      "training batch 151, loss: 2.282, 4832/60000 datapoints\n",
      "training batch 201, loss: 2.262, 6432/60000 datapoints\n",
      "training batch 251, loss: 2.270, 8032/60000 datapoints\n",
      "training batch 301, loss: 2.287, 9632/60000 datapoints\n",
      "training batch 351, loss: 2.328, 11232/60000 datapoints\n",
      "training batch 401, loss: 2.305, 12832/60000 datapoints\n",
      "training batch 451, loss: 2.274, 14432/60000 datapoints\n",
      "training batch 501, loss: 2.293, 16032/60000 datapoints\n",
      "training batch 551, loss: 2.256, 17632/60000 datapoints\n",
      "training batch 601, loss: 2.247, 19232/60000 datapoints\n",
      "training batch 651, loss: 2.278, 20832/60000 datapoints\n",
      "training batch 701, loss: 2.258, 22432/60000 datapoints\n",
      "training batch 751, loss: 2.254, 24032/60000 datapoints\n",
      "training batch 801, loss: 2.301, 25632/60000 datapoints\n",
      "training batch 851, loss: 2.279, 27232/60000 datapoints\n",
      "training batch 901, loss: 2.271, 28832/60000 datapoints\n",
      "training batch 951, loss: 2.278, 30432/60000 datapoints\n",
      "training batch 1001, loss: 2.267, 32032/60000 datapoints\n",
      "training batch 1051, loss: 2.257, 33632/60000 datapoints\n",
      "training batch 1101, loss: 2.305, 35232/60000 datapoints\n",
      "training batch 1151, loss: 2.271, 36832/60000 datapoints\n",
      "training batch 1201, loss: 2.251, 38432/60000 datapoints\n",
      "training batch 1251, loss: 2.301, 40032/60000 datapoints\n",
      "training batch 1301, loss: 2.312, 41632/60000 datapoints\n",
      "training batch 1351, loss: 2.264, 43232/60000 datapoints\n",
      "training batch 1401, loss: 2.285, 44832/60000 datapoints\n",
      "training batch 1451, loss: 2.250, 46432/60000 datapoints\n",
      "training batch 1501, loss: 2.287, 48032/60000 datapoints\n",
      "training batch 1551, loss: 2.272, 49632/60000 datapoints\n",
      "training batch 1601, loss: 2.250, 51232/60000 datapoints\n",
      "training batch 1651, loss: 2.259, 52832/60000 datapoints\n",
      "training batch 1701, loss: 2.280, 54432/60000 datapoints\n",
      "training batch 1751, loss: 2.264, 56032/60000 datapoints\n",
      "training batch 1801, loss: 2.230, 57632/60000 datapoints\n",
      "training batch 1851, loss: 2.301, 59232/60000 datapoints\n",
      "validation batch 1, loss: 2.296, 32/10016 datapoints\n",
      "validation batch 51, loss: 2.292, 1632/10016 datapoints\n",
      "validation batch 101, loss: 2.239, 3232/10016 datapoints\n",
      "validation batch 151, loss: 2.304, 4832/10016 datapoints\n",
      "validation batch 201, loss: 2.284, 6432/10016 datapoints\n",
      "validation batch 251, loss: 2.220, 8032/10016 datapoints\n",
      "validation batch 301, loss: 2.308, 9632/10016 datapoints\n",
      "train_topk_accs={1: 0.17128333333333334, 5: 0.5079}\n",
      "Epoch 3/800 done.\n",
      "Final validation performance:\n",
      "Loss: 2.277, top-1 acc: 0.168top-5 acc: 0.168\n",
      "Beginning epoch 4/800\n",
      "training batch 1, loss: 2.221, 32/60000 datapoints\n",
      "training batch 51, loss: 2.264, 1632/60000 datapoints\n",
      "training batch 101, loss: 2.287, 3232/60000 datapoints\n",
      "training batch 151, loss: 2.258, 4832/60000 datapoints\n",
      "training batch 201, loss: 2.257, 6432/60000 datapoints\n",
      "training batch 251, loss: 2.280, 8032/60000 datapoints\n",
      "training batch 301, loss: 2.280, 9632/60000 datapoints\n",
      "training batch 351, loss: 2.341, 11232/60000 datapoints\n",
      "training batch 401, loss: 2.302, 12832/60000 datapoints\n",
      "training batch 451, loss: 2.270, 14432/60000 datapoints\n",
      "training batch 501, loss: 2.277, 16032/60000 datapoints\n",
      "training batch 551, loss: 2.226, 17632/60000 datapoints\n",
      "training batch 601, loss: 2.315, 19232/60000 datapoints\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m     42\u001b[0m                             lr \u001b[38;5;241m=\u001b[39m hyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m train_losses_epochs, val_losses_epochs, train_topk_accs, val_topk_accs, state_dicts, train_time  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtrainset_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainset_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalset_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestset_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mohe_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mohe_targets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclasses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mloss_track_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrack_loss_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ThalamoCortex/thalamocortex/utils.py:184\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, trainset_loader, valset_loader, optimizer, loss_fn, ohe_targets, num_classes, num_epochs, device, loss_track_step, get_state_dict, wandb_run, topk)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeginning epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 184\u001b[0m     train_losses, train_topk_accs, state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m       \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m       \u001b[49m\u001b[43mtrainset_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m       \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m       \u001b[49m\u001b[43mohe_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m       \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m       \u001b[49m\u001b[43mloss_track_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m       \u001b[49m\u001b[43mget_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m       \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopk\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     val_losses, val_topk_accs \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m    197\u001b[0m        model,\n\u001b[1;32m    198\u001b[0m        valset_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m        loss_track_step,\n\u001b[1;32m    205\u001b[0m     )\n\u001b[1;32m    206\u001b[0m     train_losses_epochs\u001b[38;5;241m.\u001b[39mappend(train_losses)\n",
      "File \u001b[0;32m~/Documents/ThalamoCortex/thalamocortex/utils.py:258\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, optimizer, loss_fn, ohe_targets, num_classes, device, loss_track_step, get_state_dict, topk)\u001b[0m\n\u001b[1;32m    255\u001b[0m y_est \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_est\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_one_hot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# compute top-k accuracy\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m topk:\n",
      "File \u001b[0;32m~/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thalamocortex/lib/python3.10/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# number of parameter combinations\n",
    "num_comb = len(model_param_grid)\n",
    "for hp_comb_idx, hyperparams in enumerate(model_param_grid):\n",
    "\n",
    "    # create readable tag for saving\n",
    "    if hp_comb_idx < 10:\n",
    "        comb_id = f\"0{hp_comb_idx}\"\n",
    "    else:\n",
    "        comb_id = copy(hp_comb_idx)\n",
    "    tag = f\"{hp_comb_idx}_CTCNet\"\n",
    "    if hyperparams[\"thalamocortical_type\"] is None:\n",
    "        tag += \"_TC_none\"\n",
    "    else:\n",
    "        tag += f\"_TC_{hyperparams['thalamocortical_type']}\"\n",
    "    if hyperparams[\"thal_reciprocal\"]:\n",
    "        tag += \"_reciprocal\"\n",
    "    if hyperparams[\"thal_to_readout\"]:\n",
    "        tag += \"_readout\"\n",
    "    if hyperparams[\"thal_per_layer\"]:\n",
    "        tag += \"_per_layer\"\n",
    "\n",
    "    # create data loaders\n",
    "    trainset_loader, testset_loader, metadata = create_data_loaders(dataset=hyperparams[\"dataset\"],\n",
    "                                                                    norm=hyperparams[\"norm\"],\n",
    "                                                                    save_path=hyperparams[\"save_path\"],\n",
    "                                                                    batch_size=hyperparams[\"batch_size\"])\n",
    "\n",
    "    # create model\n",
    "    model = CTCNet(input_size=hyperparams[\"input_size\"],\n",
    "                    output_size=hyperparams[\"output_size\"],\n",
    "                    ctx_layer_size=hyperparams[\"ctx_layer_size\"],\n",
    "                    thal_layer_size=hyperparams[\"thal_layer_size\"],\n",
    "                    thalamocortical_type=hyperparams[\"thalamocortical_type\"],\n",
    "                    thal_reciprocal=hyperparams[\"thal_reciprocal\"],\n",
    "                    thal_to_readout=hyperparams[\"thal_to_readout\"], \n",
    "                    thal_per_layer=hyperparams[\"thal_per_layer\"])\n",
    "    model.summary()\n",
    "\n",
    "    # define loss and optimiser\n",
    "    loss_fn = deepcopy(hyperparams[\"loss\"])\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr = hyperparams[\"lr\"])\n",
    "    \n",
    "    # train model\n",
    "    train_losses_epochs, val_losses_epochs, train_topk_accs, val_topk_accs, state_dicts, train_time  = train(model=model,\n",
    "                                    trainset_loader=trainset_loader,\n",
    "                                    valset_loader=testset_loader,\n",
    "                                    optimizer=optimizer,\n",
    "                                    loss_fn=loss_fn,\n",
    "                                    ohe_targets=hyperparams[\"ohe_targets\"],\n",
    "                                    num_classes=len(metadata[\"classes\"]),\n",
    "                                    num_epochs=hyperparams[\"epochs\"],\n",
    "                                    device=device,\n",
    "                                    loss_track_step=hyperparams[\"track_loss_step\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thalamocortex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
